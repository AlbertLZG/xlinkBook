arxiv-11700-1 | Spatial Transformer Networks | http://arxiv.org/abs/1506.02025 | author:Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu category:cs.CV published:2015-06-05 summary:Convolutional Neural Networks define an exceptionally powerful class ofmodels, but are still limited by the lack of ability to be spatially invariantto the input data in a computationally and parameter efficient manner. In thiswork we introduce a new learnable module, the Spatial Transformer, whichexplicitly allows the spatial manipulation of data within the network. Thisdifferentiable module can be inserted into existing convolutionalarchitectures, giving neural networks the ability to actively spatiallytransform feature maps, conditional on the feature map itself, without anyextra training supervision or modification to the optimisation process. We showthat the use of spatial transformers results in models which learn invarianceto translation, scale, rotation and more generic warping, resulting instate-of-the-art performance on several benchmarks, and for a number of classesof transformations.
arxiv-11700-2 | Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives | http://arxiv.org/abs/1506.01972 | author:Zeyuan Allen-Zhu, Yang Yuan category:cs.LG cs.DS math.OC stat.ML published:2015-06-05 summary:Many classical algorithms are found until several years later to outlive theconfines in which they were conceived, and continue to be relevant inunforeseen settings. In this paper, we show that SVRG is one such method:originally designed for strongly convex objectives, is also very robust undernon-strongly convex or sum-of-non-convex settings. If $f(x)$ is a sum of smooth, convex functions but $f$ is not strongly convex(such as Lasso or logistic regression), we propose a variant SVRG++ that makesa novel choice of growing epoch length on top of SVRG. SVRG++ is a direct,faster variant of SVRG in this setting. If $f(x)$ is a sum of non-convex functions but $f$ is strongly convex, weshow that the convergence of SVRG linearly depends on the non-convexityparameter of the summands. This improves the best known result in this setting,and gives better running time for stochastic PCA.
arxiv-11700-3 | Programs as Polypeptides | http://arxiv.org/abs/1506.01573 | author:Lance R. Williams category:cs.NE cs.ET cs.PL published:2015-06-04 summary:We describe a visual programming language for defining behaviors manifestedby reified actors in a 2D virtual world that can be compiled into programscomprised of sequences of combinators that are themselves reified as actors.This makes it possible to build programs that build programs from components ofa few fixed types delivered by diffusion using processes that resemblechemistry as much as computation.
arxiv-11700-4 | Rivalry of Two Families of Algorithms for Memory-Restricted Streaming PCA | http://arxiv.org/abs/1506.01490 | author:Chun-Liang Li, Hsuan-Tien Lin, Chi-Jen Lu category:stat.ML cs.LG published:2015-06-04 summary:We study the problem of recovering the subspace spanned by the first $k$principal components of $d$-dimensional data under the streaming setting, witha memory bound of $O(kd)$. Two families of algorithms are known for thisproblem. The first family is based on the framework of stochastic gradientdescent. Nevertheless, the convergence rate of the family can be seriouslyaffected by the learning rate of the descent steps and deserves more seriousstudy. The second family is based on the power method over blocks of data, butsetting the block size for its existing algorithms is not an easy task. In thispaper, we analyze the convergence rate of a representative algorithm withdecayed learning rate (Oja and Karhunen, 1985) in the first family for thegeneral $k>1$ case. Moreover, we propose a novel algorithm for the secondfamily that sets the block sizes automatically and dynamically with fasterconvergence rate. We then conduct empirical studies that fairly compare the twofamilies on real-world data. The studies reveal the advantages anddisadvantages of these two families.
arxiv-11700-5 | Feature selection and classification of high-dimensional normal vectors with possibly large number of classes | http://arxiv.org/abs/1506.01567 | author:Felix Abramovich, Marianna Pensky category:math.ST stat.ME stat.ML stat.TH published:2015-06-04 summary:We consider high-dimensional multi-class classification of normal vectors,where unlike standard assumptions, the number of classes may be also large. Wederive the (non-asymptotic) conditions on effects of significant features, andthe low and the upper bounds for distances between classes required forsuccessful feature selection and classification with a given accuracy. Inparticular, we present an interesting and, at first glance, somewhatcounter-intuitive phenomenon that the precision of classification can improveas a number of classes grows. This is due to more accurate feature selectionsince even weak significant features, which are not sufficiently strong to bemanifested in a coarse classification, can nevertheless have a strong impactwhen the number of classes is large. The presented simulation study illustratesthe performance of the procedure.
arxiv-11700-6 | A Novel Approach Towards Clustering Based Image Segmentation | http://arxiv.org/abs/1506.01710 | author:Dibya Jyoti Bora, Anil Kumar Gupta category:cs.CV published:2015-06-04 summary:In computer vision, image segmentation is always selected as a major researchtopic by researchers. Due to its vital rule in image processing, there alwaysarises the need of a better image segmentation method. Clustering is anunsupervised study with its application in almost every field of science andengineering. Many researchers used clustering in image segmentation process.But still there requires improvement of such approaches. In this paper, a novelapproach for clustering based image segmentation is proposed. Here, we giveimportance on color space and choose lab for this task. The famous hardclustering algorithm K-means is used, but as its performance is dependent onchoosing a proper distance measure, so, we go for cosine distance measure. Thenthe segmented image is filtered with sobel filter. The filtered image isanalyzed with marker watershed algorithm to have the final segmented result ofour original image. The MSE and PSNR values are evaluated to observe theperformance.
arxiv-11700-7 | Multilayer Structured NMF for Spectral Unmixing of Hyperspectral Images | http://arxiv.org/abs/1506.01596 | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2015-06-04 summary:One of the challenges in hyperspectral data analysis is the presence of mixedpixels. Mixed pixels are the result of low spatial resolution of hyperspectralsensors. Spectral unmixing methods decompose a mixed pixel into a set ofendmembers and abundance fractions. Due to nonnegativity constraint onabundance fraction values, NMF based methods are well suited to this problem.In this paper multilayer NMF has been used to improve the results of NMFmethods for spectral unmixing of hyperspectral data under the linear mixingframework. Sparseness constraint on both spectral signatures and abundancefractions matrices are used in this paper. Evaluation of the proposed algorithmis done using synthetic and real datasets in terms of spectral angle andabundance angle distances. Results show that the proposed algorithm outperformsother previously proposed methods.
arxiv-11700-8 | Comparing the Performance of L*A*B* and HSV Color Spaces with Respect to Color Image Segmentation | http://arxiv.org/abs/1506.01472 | author:Dibya Jyoti Bora, Anil Kumar Gupta, Fayaz Ahmad Khan category:cs.CV published:2015-06-04 summary:Color image segmentation is a very emerging topic for image processingresearch. Since it has the ability to present the result in a way that is muchmore close to the human yes perceive, so todays more research is going on thisarea. Choosing a proper color space is a very important issue for color imagesegmentation process. Generally LAB and HSV are the two frequently chosen colorspaces. In this paper a comparative analysis is performed between these twocolor spaces with respect to color image segmentation. For measuring theirperformance, we consider the parameters: mse and psnr . It is found that HSVcolor space is performing better than LAB.
arxiv-11700-9 | The Long-Short Story of Movie Description | http://arxiv.org/abs/1506.01698 | author:Anna Rohrbach, Marcus Rohrbach, Bernt Schiele category:cs.CV cs.CL published:2015-06-04 summary:Generating descriptions for videos has many applications including assistingblind people and human-robot interaction. The recent advances in imagecaptioning as well as the release of large-scale movie description datasetssuch as MPII Movie Description allow to study this task in more depth. Many ofthe proposed methods for image captioning rely on pre-trained object classifierCNNs and Long-Short Term Memory recurrent networks (LSTMs) for generatingdescriptions. While image description focuses on objects, we argue that it isimportant to distinguish verbs, objects, and places in the challenging settingof movie description. In this work we show how to learn robust visualclassifiers from the weak annotations of the sentence descriptions. Based onthese visual classifiers we learn how to generate a description using an LSTM.We explore different design choices to build and train the LSTM and achieve thebest performance to date on the challenging MPII-MD dataset. We compare andanalyze our approach and prior work along various dimensions to betterunderstand the key challenges of the movie description task.
arxiv-11700-10 | The Preference Learning Toolbox | http://arxiv.org/abs/1506.01709 | author:Vincent E. Farrugia, Héctor P. Martínez, Georgios N. Yannakakis category:stat.ML cs.IR cs.LG published:2015-06-04 summary:Preference learning (PL) is a core area of machine learning that handlesdatasets with ordinal relations. As the number of generated data of ordinalnature is increasing, the importance and role of the PL field becomes centralwithin machine learning research and practice. This paper introduces an opensource, scalable, efficient and accessible preference learning toolbox thatsupports the key phases of the data training process incorporating variouspopular data preprocessing, feature selection and preference learning methods.
arxiv-11700-11 | Monocular SLAM Supported Object Recognition | http://arxiv.org/abs/1506.01732 | author:Sudeep Pillai, John Leonard category:cs.RO cs.CV published:2015-06-04 summary:In this work, we develop a monocular SLAM-aware object recognition systemthat is able to achieve considerably stronger recognition performance, ascompared to classical object recognition systems that function on aframe-by-frame basis. By incorporating several key ideas including multi-viewobject proposals and efficient feature encoding methods, our proposed system isable to detect and robustly recognize objects in its environment using a singleRGB camera in near-constant time. Through experiments, we illustrate theutility of using such a system to effectively detect and recognize objects,incorporating multiple object viewpoint detections into a unified predictionhypothesis. The performance of the proposed recognition system is evaluated onthe UW RGB-D Dataset, showing strong recognition performance and scalablerun-time performance compared to current state-of-the-art recognition systems.
arxiv-11700-12 | Spectral Learning of Large Structured HMMs for Comparative Epigenomics | http://arxiv.org/abs/1506.01744 | author:Chicheng Zhang, Jimin Song, Kevin C Chen, Kamalika Chaudhuri category:stat.ML cs.LG math.ST q-bio.GN stat.TH published:2015-06-04 summary:We develop a latent variable model and an efficient spectral algorithmmotivated by the recent emergence of very large data sets of chromatin marksfrom multiple human cell types. A natural model for chromatin data in one celltype is a Hidden Markov Model (HMM); we model the relationship between multiplecell types by connecting their hidden states by a fixed tree of knownstructure. The main challenge with learning parameters of such models is thatiterative methods such as EM are very slow, while naive spectral methods resultin time and space complexity exponential in the number of cell types. Weexploit properties of the tree structure of the hidden states to providespectral algorithms that are more computationally efficient for currentbiological datasets. We provide sample complexity bounds for our algorithm andevaluate it experimentally on biological data from nine human cell types.Finally, we show that beyond our specific model, some of our algorithmic ideascan be applied to other graphical models.
arxiv-11700-13 | An Average Classification Algorithm | http://arxiv.org/abs/1506.01520 | author:Brendan van Rooyen, Aditya Krishna Menon, Robert C. Williamson category:stat.ML cs.LG published:2015-06-04 summary:Many classification algorithms produce a classifier that is a weightedaverage of kernel evaluations. When working with a high or infinite dimensionalkernel, it is imperative for speed of evaluation and storage issues that as fewtraining samples as possible are used in the kernel expansion. Popular existingapproaches focus on altering standard learning algorithms, such as the SupportVector Machine, to induce sparsity, as well as post-hoc procedures for sparseapproximations. Here we adopt the latter approach. We begin with a very simpleclassifier, given by the kernel mean $$ f(x) = \frac{1}{n}\sum\limits_{i=i}^{n} y_i K(x_i,x) $$ We then find a sparse approximation tothis kernel mean via herding. The result is an accurate, easily parallelizedalgorithm for learning classifiers.
arxiv-11700-14 | ShapeFit: Exact location recovery from corrupted pairwise directions | http://arxiv.org/abs/1506.01437 | author:Paul Hand, Choongbum Lee, Vladislav Voroninski category:cs.CV cs.IT math.CO math.IT math.OC published:2015-06-04 summary:Let $t_1,\ldots,t_n \in \mathbb{R}^d$ and consider the location recoveryproblem: given a subset of pairwise direction observations $\{(t_i - t_j) /\t_i - t_j\_2\}_{i<j \in [n] \times [n]}$, where a constant fraction of theseobservations are arbitrarily corrupted, find $\{t_i\}_{i=1}^n$ up to a globaltranslation and scale. We propose a novel algorithm for the location recoveryproblem, which consists of a simple convex program over $dn$ real variables. Weprove that this program recovers a set of $n$ i.i.d. Gaussian locations exactlyand with high probability if the observations are given by an \erdosrenyigraph, $d$ is large enough, and provided that at most a constant fraction ofobservations involving any particular location are adversarially corrupted. Wealso prove that the program exactly recovers Gaussian locations for $d=3$ ifthe fraction of corrupted observations at each location is, up topoly-logarithmic factors, at most a constant. Both of these recovery theoremsare based on a set of deterministic conditions that we prove are sufficient forexact recovery.
arxiv-11700-15 | Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks | http://arxiv.org/abs/1506.01497 | author:Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun category:cs.CV published:2015-06-04 summary:State-of-the-art object detection networks depend on region proposalalgorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNNhave reduced the running time of these detection networks, exposing regionproposal computation as a bottleneck. In this work, we introduce a RegionProposal Network (RPN) that shares full-image convolutional features with thedetection network, thus enabling nearly cost-free region proposals. An RPN is afully convolutional network that simultaneously predicts object bounds andobjectness scores at each position. The RPN is trained end-to-end to generatehigh-quality region proposals, which are used by Fast R-CNN for detection. Wefurther merge RPN and Fast R-CNN into a single network by sharing theirconvolutional features---using the recently popular terminology of neuralnetworks with 'attention' mechanisms, the RPN component tells the unifiednetwork where to look. For the very deep VGG-16 model, our detection system hasa frame rate of 5fps (including all steps) on a GPU, while achievingstate-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MSCOCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015competitions, Faster R-CNN and RPN are the foundations of the 1st-place winningentries in several tracks. Code has been made publicly available.
arxiv-11700-16 | Abstractive Multi-Document Summarization via Phrase Selection and Merging | http://arxiv.org/abs/1506.01597 | author:Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo, Rebecca J. Passonneau category:cs.CL cs.AI published:2015-06-04 summary:We propose an abstraction-based multi-document summarization framework thatcan construct new sentences by exploring more fine-grained syntactic units thansentences, namely, noun/verb phrases. Different from existing abstraction-basedapproaches, our method first constructs a pool of concepts and factsrepresented by phrases from the input documents. Then new sentences aregenerated by selecting and merging informative phrases to maximize the salienceof phrases and meanwhile satisfy the sentence construction constraints. Weemploy integer linear optimization for conducting phrase selection and mergingsimultaneously in order to achieve the global optimal solution for a summary.Experimental results on the benchmark data set TAC 2011 show that our frameworkoutperforms the state-of-the-art models under automated pyramid evaluationmetric, and achieves reasonably well results on manual linguistic qualityevaluation.
arxiv-11700-17 | Optimal change point detection in Gaussian processes | http://arxiv.org/abs/1506.01338 | author:Hossein Keshavarz, Clayton Scott, XuanLong Nguyen category:math.ST cs.IT cs.LG math.IT stat.ML stat.TH published:2015-06-03 summary:We study the problem of detecting a change in the mean of one-dimensionalGaussian process data. This problem is investigated in the setting ofincreasing domain (customarily employed in time series analysis) and in thesetting of fixed domain (typically arising in spatial data analysis). Wepropose a detection method based on generalized likelihood ratio test (GLRT),and show that our method achieves asymptotically optimal rate in the minimaxsense, in both settings. The salient feature of the proposed method is that itexploits in an efficient way the data dependence captured by the Gaussianprocess covariance structure. When the covariance is not known, we proposeplug-in GLRT method and derive conditions under which the method remainsasymptotically optimal. By contrast, the standard CUSUM method, which does notaccount for the covariance structure, is shown to be asymptotically optimalonly in the increasing domain. Our algorithms and accompanying theory areapplicable to a wide variety of covariance structures, including the Maternclass, the powered exponential class, and others. The plug-in GLRT method isshown to perform well for a number of covariance estimators, including maximumlikelihood estimators with a dense or a tapered covariance matrix.
arxiv-11700-18 | Unsupervised Feature Analysis with Class Margin Optimization | http://arxiv.org/abs/1506.01330 | author:Sen Wang, Feiping Nie, Xiaojun Chang, Lina Yao, Xue Li, Quan Z. Sheng category:cs.LG published:2015-06-03 summary:Unsupervised feature selection has been always attracting research attentionin the communities of machine learning and data mining for decades. In thispaper, we propose an unsupervised feature selection method seeking a featurecoefficient matrix to select the most distinctive features. Specifically, ourproposed algorithm integrates the Maximum Margin Criterion with asparsity-based model into a joint framework, where the class margin and featurecorrelation are taken into account at the same time. To maximize the total dataseparability while preserving minimized within-class scatter simultaneously, wepropose to embed Kmeans into the framework generating pseudo class labelinformation in a scenario of unsupervised feature selection. Meanwhile, asparsity-based model, ` 2 ,p-norm, is imposed to the regularization term toeffectively discover the sparse structures of the feature coefficient matrix.In this way, noisy and irrelevant features are removed by ruling out thosefeatures whose corresponding coefficients are zeros. To alleviate the localoptimum problem that is caused by random initializations of K-means, aconvergence guaranteed algorithm with an updating strategy for the clusteringindicator matrix, is proposed to iteractively chase the optimal solution.Performance evaluation is extensively conducted over six benchmark data sets.From plenty of experimental results, it is demonstrated that our method hassuperior performance against all other compared approaches.
arxiv-11700-19 | Probabilistic Numerics and Uncertainty in Computations | http://arxiv.org/abs/1506.01326 | author:Philipp Hennig, Michael A Osborne, Mark Girolami category:math.NA cs.AI cs.LG stat.CO stat.ML published:2015-06-03 summary:We deliver a call to arms for probabilistic numerical methods: algorithms fornumerical tasks, including linear algebra, integration, optimization andsolving differential equations, that return uncertainties in theircalculations. Such uncertainties, arising from the loss of precision induced bynumerical calculation with limited time or hardware, are important for muchcontemporary science and industry. Within applications such as climate scienceand astrophysics, the need to make decisions on the basis of computations withlarge and complex data has led to a renewed focus on the management ofnumerical uncertainty. We describe how several seminal classic numericalmethods can be interpreted naturally as probabilistic inference. We then showthat the probabilistic view suggests new algorithms that can flexibly beadapted to suit application specifics, while delivering improved empiricalperformance. We provide concrete illustrations of the benefits of probabilisticnumeric algorithms on real scientific problems from astrometry and astronomicalimaging, while highlighting open problems with these new algorithms. Finally,we describe how probabilistic numerical methods provide a coherent frameworkfor identifying the uncertainty in calculations performed with a combination ofnumerical algorithms (e.g. both numerical optimisers and differential equationsolvers), potentially allowing the diagnosis (and control) of error sources incomputations.
arxiv-11700-20 | PeakSegJoint: fast supervised peak detection via joint segmentation of multiple count data samples | http://arxiv.org/abs/1506.01286 | author:Toby Dylan Hocking, Guillaume Bourque category:stat.ML q-bio.GN published:2015-06-03 summary:Joint peak detection is a central problem when comparing samples in genomicdata analysis, but current algorithms for this task are unsupervised andlimited to at most 2 sample types. We propose PeakSegJoint, a new constrainedmaximum likelihood segmentation model for any number of sample types. To selectthe number of peaks in the segmentation, we propose a supervised penaltylearning model. To infer the parameters of these two models, we propose to usea discrete optimization heuristic for the segmentation, and convex optimizationfor the penalty learning. In comparisons with state-of-the-art peak detectionalgorithms, PeakSegJoint achieves similar accuracy, faster speeds, and a moreinterpretable model with overlapping peaks that occur in exactly the samepositions across all samples.
arxiv-11700-21 | Personalizing a Universal Recurrent Neural Network Language Model with User Characteristic Features by Crowdsouring over Social Networks | http://arxiv.org/abs/1506.01192 | author:Bo-Hsiang Tseng, Hung-Yi Lee, Lin-Shan Lee category:cs.CL cs.LG published:2015-06-03 summary:With the popularity of mobile devices, personalized speech recognizer becomesmore realizable today and highly attractive. Each mobile device is primarilyused by a single user, so it's possible to have a personalized recognizer wellmatching to the characteristics of individual user. Although acoustic modelpersonalization has been investigated for decades, much less work have beenreported on personalizing language model, probably because of the difficultiesin collecting enough personalized corpora. Previous work used the corporacollected from social networks to solve the problem, but constructing apersonalized model for each user is troublesome. In this paper, we propose auniversal recurrent neural network language model with user characteristicfeatures, so all users share the same model, except each with different usercharacteristic features. These user characteristic features can be obtained bycrowdsouring over social networks, which include huge quantity of texts postedby users with known friend relationships, who may share some subject topics andwording patterns. The preliminary experiments on Facebook corpus showed thatthis proposed approach not only drastically reduced the model perplexity, butoffered very good improvement in recognition accuracy in n-best rescoringtests. This approach also mitigated the data sparseness problem forpersonalized language models.
arxiv-11700-22 | A Hybrid Model for Enhancing Lexical Statistical Machine Translation (SMT) | http://arxiv.org/abs/1506.01171 | author:Ahmed G. M. ElSayed, Ahmed S. Salama, Alaa El-Din M. El-Ghazali category:cs.CL published:2015-06-03 summary:The interest in statistical machine translation systems increases currentlydue to political and social events in the world. A proposed Statistical MachineTranslation (SMT) based model that can be used to translate a sentence from thesource Language (English) to the target language (Arabic) automatically throughefficiently incorporating different statistical and Natural Language Processing(NLP) models such as language model, alignment model, phrase based model,reordering model, and translation model. These models are combined to enhancethe performance of statistical machine translation (SMT). Many implementationtools have been used in this work such as Moses, Gizaa++, IRSTLM, KenLM, andBLEU. Based on the implementation, evaluation of this model, and comparing thegenerated translation with other implemented machine translation systems likeGoogle Translate, it was proved that this proposed model has enhanced theresults of the statistical machine translation, and forms a reliable andefficient model in this field of research.
arxiv-11700-23 | One-to-many face recognition with bilinear CNNs | http://arxiv.org/abs/1506.01342 | author:Aruni RoyChowdhury, Tsung-Yu Lin, Subhransu Maji, Erik Learned-Miller category:cs.CV published:2015-06-03 summary:The recent explosive growth in convolutional neural network (CNN) researchhas produced a variety of new architectures for deep learning. One intriguingnew architecture is the bilinear CNN (B-CNN), which has shown dramaticperformance gains on certain fine-grained recognition problems [15]. We applythis new CNN to the challenging new face recognition benchmark, the IARPA JanusBenchmark A (IJB-A) [12]. It features faces from a large number of identitiesin challenging real-world conditions. Because the face images were notidentified automatically using a computerized face detection system, it doesnot have the bias inherent in such a database. We demonstrate the performanceof the B-CNN model beginning from an AlexNet-style network pre-trained onImageNet. We then show results for fine-tuning using a moderate-sized andpublic external database, FaceScrub [17]. We also present results withadditional fine-tuning on the limited training data provided by the protocol.In each case, the fine-tuned bilinear model shows substantial improvements overthe standard CNN. Finally, we demonstrate how a standard CNN pre-trained on alarge face database, the recently released VGG-Face model [20], can beconverted into a B-CNN without any additional feature training. This B-CNNimproves upon the CNN performance on the IJB-A benchmark, achieving 89.5%rank-1 recall.
arxiv-11700-24 | Exploiting an Oracle that Reports AUC Scores in Machine Learning Contests | http://arxiv.org/abs/1506.01339 | author:Jacob Whitehill category:cs.LG published:2015-06-03 summary:In machine learning contests such as the ImageNet Large Scale VisualRecognition Challenge and the KDD Cup, contestants can submit candidatesolutions and receive from an oracle (typically the organizers of thecompetition) the accuracy of their guesses compared to the ground-truth labels.One of the most commonly used accuracy metrics for binary classification tasksis the Area Under the Receiver Operating Characteristics Curve (AUC). In thispaper we provide proofs-of-concept of how knowledge of the AUC of a set ofguesses can be used, in two different kinds of attacks, to improve the accuracyof those guesses. On the other hand, we also demonstrate the intractability ofone kind of AUC exploit by proving that the number of possible binary labelingsof $n$ examples for which a candidate solution obtains a AUC score of $c$ growsexponentially in $n$, for every $c\in (0,1)$.
arxiv-11700-25 | Color Image Retrieval Using Fuzzy Measure Hamming and S-Tree | http://arxiv.org/abs/1506.01166 | author:Thanh The Van, Thanh Manh Le category:cs.CV H.2.8; H.3.3 published:2015-06-03 summary:This chapter approaches the image retrieval system on the base of the colorsof image. It creates fuzzy signature to describe the color of image on colorspace HSV and builds fuzzy Hamming distance (FHD) to evaluate the similaritybetween the images. In order to reduce the storage space and speed up thesearch of similar images, it aims to create S-tree to store fuzzy signaturerelies on FHD and builds image retrieval algorithm on S-tree. Then, it providesthe content-based image retrieval (CBIR) and an image retrieval method on FHDand S-tree. Last but not least, based on this theory, it also presents anapplication and experimental assessment of the process of querying similarimage on the database system over 10,000 images.
arxiv-11700-26 | Image Retrieval System Base on EMD Similarity Measure and S-Tree | http://arxiv.org/abs/1506.01165 | author:Thanh Manh Le, Thanh The Van category:cs.CV cs.IR H.2.8; H.3.3 published:2015-06-03 summary:The paper approaches the binary signature for each image based on thepercentage of the pixels in each color images, at the same time the paperbuilds a similar measure between images based on EMD (Earth Mover's Distance).Besides, the paper proceeded to create the S-tree based on the similar measureEMD to store the image's binary signatures to quickly query image signaturedata. From there, the paper build an image retrieval algorithm and CBIR(Content-Based Image Retrieval) based on a similar measure EMD and S-tree.Based on this theory, the paper proceeded to build application and experimentalassessment of the process of querying image on the database system which haveover 10,000 images.
arxiv-11700-27 | Towards Structured Deep Neural Network for Automatic Speech Recognition | http://arxiv.org/abs/1506.01163 | author:Yi-Hsiu Liao, Hung-Yi Lee, Lin-shan Lee category:cs.LG published:2015-06-03 summary:In this paper we propose the Structured Deep Neural Network (Structured DNN)as a structured and deep learning algorithm, learning to find the beststructured object (such as a label sequence) given a structured input (such asa vector sequence) by globally considering the mapping relationships betweenthe structure rather than item by item. When automatic speech recognition is viewed as a special case of such astructured learning problem, where we have the acoustic vector sequence as theinput and the phoneme label sequence as the output, it becomes possible tocomprehensively learned utterance by utterance as a whole, rather than frame byframe. Structured Support Vector Machine (structured SVM) was proposed to performASR with structured learning previously, but limited by the linear nature ofSVM. Here we propose structured DNN to use nonlinear transformations inmulti-layers as a structured and deep learning algorithm. It was shown to beatstructured SVM in preliminary experiments on TIMIT.
arxiv-11700-28 | Understanding deep features with computer-generated imagery | http://arxiv.org/abs/1506.01151 | author:Mathieu Aubry, Bryan Russell category:cs.CV published:2015-06-03 summary:We introduce an approach for analyzing the variation of features generated byconvolutional neural networks (CNNs) with respect to scene factors that occurin natural images. Such factors may include object style, 3D viewpoint, color,and scene lighting configuration. Our approach analyzes CNN feature responsescorresponding to different scene factors by controlling for them via renderingusing a large database of 3D CAD models. The rendered images are presented to atrained CNN and responses for different layers are studied with respect to theinput scene factors. We perform a decomposition of the responses based onknowledge of the input scene factors and analyze the resulting components. Inparticular, we quantify their relative importance in the CNN responses andvisualize them using principal component analysis. We show qualitative andquantitative results of our study on three CNNs trained on large imagedatasets: AlexNet, Places, and Oxford VGG. We observe important differencesacross the networks and CNN layers for different scene factors and objectcategories. Finally, we demonstrate that our analysis based oncomputer-generated imagery translates to the network representation of naturalimages.
arxiv-11700-29 | Implementation of Training Convolutional Neural Networks | http://arxiv.org/abs/1506.01195 | author:Tianyi Liu, Shuangsang Fang, Yuehui Zhao, Peng Wang, Jun Zhang category:cs.CV cs.LG cs.NE published:2015-06-03 summary:Deep learning refers to the shining branch of machine learning that is basedon learning levels of representations. Convolutional Neural Networks (CNN) isone kind of deep neural network. It can study concurrently. In this article, wegave a detailed analysis of the process of CNN algorithm both the forwardprocess and back propagation. Then we applied the particular convolutionalneural network to implement the typical face recognition problem by java. Then,a parallel strategy was proposed in section4. In addition, by measuring theactual time of forward and backward computing, we analysed the maximal speed upand parallel efficiency theoretically.
arxiv-11700-30 | Recognition of Changes in SAR Images Based on Gauss-Log Ratio and MRFFCM | http://arxiv.org/abs/1506.01398 | author:Jismy Alphonse, Biju V. G. category:cs.CV published:2015-06-03 summary:A modified version of MRFFCM (Markov Random Field Fuzzy C means) based SAR(Synthetic aperture Radar) image change detection method is proposed in thispaper. It involves three steps: Difference Image (DI) generation by usingGauss-log ratio operator, speckle noise reduction by SRAD (Speckle ReducingAnisotropic Diffusion), and the detection of changed regions by using MRFFCM.The proposed method is compared with existing methods such as FCM and MRFFCMusing simulated and real SAR images. The measures used for evaluation includesOverall Error (OE), Percentage Correct Classification (PCC), Kappa Coefficient(KC), Root Mean Square Error (RMSE), and Peak Signal to Noise Ratio (PSNR). Theresults show that the proposed method is better compared to FCM and MRFFCMbased change detection method.
arxiv-11700-31 | Unsupervised domain adaption dictionary learning for visual recognition | http://arxiv.org/abs/1506.01125 | author:Zhun Zhong, Zongmin Li, Runlin Li, Xiaoxia Sun category:cs.CV published:2015-06-03 summary:Over the last years, dictionary learning method has been extensively appliedto deal with various computer vision recognition applications, and producedstate-of-the-art results. However, when the data instances of a target domainhave a different distribution than that of a source domain, the dictionarylearning method may fail to perform well. In this paper, we address thecross-domain visual recognition problem and propose a simple but effectiveunsupervised domain adaption approach, where labeled data are only from sourcedomain. In order to bring the original data in source and target domain intothe same distribution, the proposed method forcing nearest coupled data betweensource and target domain to have identical sparse representations while jointlylearning dictionaries for each domain, where the learned dictionaries canreconstruct original data in source and target domain respectively. So thatsparse representations of original data can be used to perform visualrecognition tasks. We demonstrate the effectiveness of our approach on standarddatasets. Our method performs on par or better than competitivestate-of-the-art methods.
arxiv-11700-32 | Bayesian optimization for materials design | http://arxiv.org/abs/1506.01349 | author:Peter I. Frazier, Jialei Wang category:stat.ML math.OC published:2015-06-03 summary:We introduce Bayesian optimization, a technique developed for optimizingtime-consuming engineering simulations and for fitting machine learning modelson large datasets. Bayesian optimization guides the choice of experimentsduring materials design and discovery to find good material designs in as fewexperiments as possible. We focus on the case when materials designs areparameterized by a low-dimensional vector. Bayesian optimization is built on astatistical technique called Gaussian process regression, which allowspredicting the performance of a new design based on previously tested designs.After providing a detailed introduction to Gaussian process regression, weintroduce two Bayesian optimization methods: expected improvement, for designproblems with noise-free evaluations; and the knowledge-gradient method, whichgeneralizes expected improvement and may be used in design problems with noisyevaluations. Both methods are derived using a value-of-information analysis,and enjoy one-step Bayes-optimality.
arxiv-11700-33 | Hyperspectral Image Classification and Clutter Detection via Multiple Structural Embeddings and Dimension Reductions | http://arxiv.org/abs/1506.01115 | author:Alexandros-Stavros Iliopoulos, Tiancheng Liu, Xiaobai Sun category:cs.CV published:2015-06-03 summary:We present a new and effective approach for Hyperspectral Image (HSI)classification and clutter detection, overcoming a few long-standing challengespresented by HSI data characteristics. Residing in a high-dimensional spectralattribute space, HSI data samples are known to be strongly correlated in theirspectral signatures, exhibit nonlinear structure due to several physical laws,and contain uncertainty and noise from multiple sources. In the presentedapproach, we generate an adaptive, structurally enriched representationenvironment, and employ the locally linear embedding (LLE) in it. There are twostructure layers external to LLE. One is feature space embedding: the HSI dataattributes are embedded into a discriminatory feature space wherespatio-spectral coherence and distinctive structures are distilled andexploited to mitigate various difficulties encountered in the nativehyperspectral attribute space. The other structure layer encloses the ranges ofalgorithmic parameters for LLE and feature embedding, and supports amultiplexing and integrating scheme for contending with multi-sourceuncertainty. Experiments on two commonly used HSI datasets with a small numberof learning samples have rendered remarkably high-accuracy classificationresults, as well as distinctive maps of detected clutter regions.
arxiv-11700-34 | Multi-view Machines | http://arxiv.org/abs/1506.01110 | author:Bokai Cao, Hucheng Zhou, Philip S. Yu category:cs.LG stat.ML H.2.8 published:2015-06-03 summary:For a learning task, data can usually be collected from different sources orbe represented from multiple views. For example, laboratory results fromdifferent medical examinations are available for disease diagnosis, and each ofthem can only reflect the health state of a person from a particularaspect/view. Therefore, different views provide complementary information forlearning tasks. An effective integration of the multi-view information isexpected to facilitate the learning performance. In this paper, we propose ageneral predictor, named multi-view machines (MVMs), that can effectivelyinclude all the possible interactions between features from multiple views. Ajoint factorization is embedded for the full-order interaction parameters whichallows parameter estimation under sparsity. Moreover, MVMs can work inconjunction with different loss functions for a variety of machine learningtasks. A stochastic gradient descent method is presented to learn the MVMmodel. We further illustrate the advantages of MVMs through comparison withother methods for multi-view classification, including support vector machines(SVMs), support tensor machines (STMs) and factorization machines (FMs).
arxiv-11700-35 | Bilinear Random Projections for Locality-Sensitive Binary Codes | http://arxiv.org/abs/1506.01092 | author:Saehoon Kim, Seungjin Choi category:cs.CV published:2015-06-03 summary:Locality-sensitive hashing (LSH) is a popular data-independent indexingmethod for approximate similarity search, where random projections followed byquantization hash the points from the database so as to ensure that theprobability of collision is much higher for objects that are close to eachother than for those that are far apart. Most of high-dimensional visualdescriptors for images exhibit a natural matrix structure. When visualdescriptors are represented by high-dimensional feature vectors and long binarycodes are assigned, a random projection matrix requires expensive complexitiesin both space and time. In this paper we analyze a bilinear random projectionmethod where feature matrices are transformed to binary codes by two smallerrandom projection matrices. We base our theoretical analysis on extendingRaginsky and Lazebnik's result where random Fourier features are composed withrandom binary quantizers to form locality sensitive binary codes. To this end,we answer the following two questions: (1) whether a bilinear random projectionalso yields similarity-preserving binary codes; (2) whether a bilinear randomprojection yields performance gain or loss, compared to a large linearprojection. Regarding the first question, we present upper and lower bounds onthe expected Hamming distance between binary codes produced by bilinear randomprojections. In regards to the second question, we analyze the upper and lowerbounds on covariance between two bits of binary codes, showing that thecorrelation between two bits is small. Numerical experiments on MNIST andFlickr45K datasets confirm the validity of our method.
arxiv-11700-36 | Summarization of Films and Documentaries Based on Subtitles and Scripts | http://arxiv.org/abs/1506.01273 | author:Marta Aparício, Paulo Figueiredo, Francisco Raposo, David Martins de Matos, Ricardo Ribeiro, Luís Marujo category:cs.CL cs.AI cs.IR I.2.7 published:2015-06-03 summary:We assess the performance of generic text summarization algorithms applied tofilms and documentaries, using the well-known behavior of summarization of newsarticles as reference. We use three datasets: (i) news articles, (ii) filmscripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metricsare used for comparing generated summaries against news abstracts, plotsummaries, and synopses. We show that the best performing algorithms are LSA,for news articles and documentaries, and LexRank and Support Sets, for films.Despite the different nature of films and documentaries, their relativebehavior is in accordance with that obtained for news articles.
arxiv-11700-37 | Celeste: Variational inference for a generative model of astronomical images | http://arxiv.org/abs/1506.01351 | author:Jeffrey Regier, Andrew Miller, Jon McAuliffe, Ryan Adams, Matt Hoffman, Dustin Lang, David Schlegel, Prabhat category:astro-ph.IM stat.ML G.3 published:2015-06-03 summary:We present a new, fully generative model of optical telescope image sets,along with a variational procedure for inference. Each pixel intensity istreated as a Poisson random variable, with a rate parameter dependent on latentproperties of stars and galaxies. Key latent properties are themselves random,with scientific prior distributions constructed from large ancillary data sets.We check our approach on synthetic images. We also run it on images from amajor sky survey, where it exceeds the performance of the currentstate-of-the-art method for locating celestial bodies and measuring theircolors.
arxiv-11700-38 | A Nearly Optimal and Agnostic Algorithm for Properly Learning a Mixture of k Gaussians, for any Constant k | http://arxiv.org/abs/1506.01367 | author:Jerry Li, Ludwig Schmidt category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH published:2015-06-03 summary:Learning a Gaussian mixture model (GMM) is a fundamental problem in machinelearning, learning theory, and statistics. One notion of learning a GMM isproper learning: here, the goal is to find a mixture of $k$ Gaussians$\mathcal{M}$ that is close to the density $f$ of the unknown distribution fromwhich we draw samples. The distance between $\mathcal{M}$ and $f$ is typicallymeasured in the total variation or $L_1$-norm. We give an algorithm for learning a mixture of $k$ univariate Gaussians thatis nearly optimal for any fixed $k$. The sample complexity of our algorithm is$\tilde{O}(\frac{k}{\epsilon^2})$ and the running time is $(k \cdot\log\frac{1}{\epsilon})^{O(k^4)} + \tilde{O}(\frac{k}{\epsilon^2})$. It iswell-known that this sample complexity is optimal (up to logarithmic factors),and it was already achieved by prior work. However, the best known timecomplexity for proper learning a $k$-GMM was$\tilde{O}(\frac{1}{\epsilon^{3k-1}})$. In particular, the dependence between$\frac{1}{\epsilon}$ and $k$ was exponential. We significantly improve thisdependence by replacing the $\frac{1}{\epsilon}$ term with a $\log\frac{1}{\epsilon}$ while only increasing the exponent moderately. Hence, forany fixed $k$, the $\tilde{O} (\frac{k}{\epsilon^2})$ term dominates ourrunning time, and thus our algorithm runs in time which is nearly-linear in thenumber of samples drawn. Achieving a running time of $\textrm{poly}(k,\frac{1}{\epsilon})$ for proper learning of $k$-GMMs has recently been statedas an open problem by multiple researchers, and we make progress on thisquestion. Moreover, our approach offers an agnostic learning guarantee: our algorithmreturns a good GMM even if the distribution we are sampling from is not amixture of Gaussians. To the best of our knowledge, our algorithm is the firstagnostic proper learning algorithm for GMMs.
arxiv-11700-39 | Traversing Knowledge Graphs in Vector Space | http://arxiv.org/abs/1506.01094 | author:Kelvin Guu, John Miller, Percy Liang category:cs.CL cs.AI cs.DB stat.ML published:2015-06-03 summary:Path queries on a knowledge graph can be used to answer compositionalquestions such as "What languages are spoken by people living in Lisbon?".However, knowledge graphs often have missing facts (edges) which disrupts pathqueries. Recent models for knowledge base completion impute missing facts byembedding knowledge graphs in vector spaces. We show that these models can berecursively applied to answer path queries, but that they suffer from cascadingerrors. This motivates a new "compositional" training objective, whichdramatically improves all models' ability to answer path queries, in some casesmore than doubling accuracy. On a standard knowledge base completion task, wealso demonstrate that compositional training acts as a novel form of structuralregularization, reliably improving performance across all base models (reducingerrors by up to 43%) and achieving new state-of-the-art results.
arxiv-11700-40 | What value do explicit high level concepts have in vision to language problems? | http://arxiv.org/abs/1506.01144 | author:Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, Anton van den Hengel category:cs.CV published:2015-06-03 summary:Much of the recent progress in Vision-to-Language (V2L) problems has beenachieved through a combination of Convolutional Neural Networks (CNNs) andRecurrent Neural Networks (RNNs). This approach does not explicitly representhigh-level semantic concepts, but rather seeks to progress directly from imagefeatures to text. We propose here a method of incorporating high-level conceptsinto the very successful CNN-RNN approach, and show that it achieves asignificant improvement on the state-of-the-art performance in both imagecaptioning and visual question answering. We also show that the same mechanismcan be used to introduce external semantic information and that doing sofurther improves performance. In doing so we provide an analysis of the valueof high level semantic information in V2L problems.
arxiv-11700-41 | Parallel Stochastic Gradient Markov Chain Monte Carlo for Matrix Factorisation Models | http://arxiv.org/abs/1506.01418 | author:Umut Şimşekli, Hazal Koptagel, Hakan Güldaş, A. Taylan Cemgil, Figen Öztoprak, Ş. İlker Birbil category:stat.ML published:2015-06-03 summary:For large matrix factorisation problems, we develop a distributed MarkovChain Monte Carlo (MCMC) method based on stochastic gradient Langevin dynamics(SGLD) that we call Parallel SGLD (PSGLD). PSGLD has very favourable scalingproperties with increasing data size and is comparable in terms ofcomputational requirements to optimisation methods based on stochastic gradientdescent. PSGLD achieves high performance by exploiting the conditionalindependence structure of the MF models to sub-sample data in a systematicmanner as to allow parallelisation and distributed computation. We provide aconvergence proof of the algorithm and verify its superior performance onvarious architectures such as Graphics Processing Units, shared memorymulti-core systems and multi-computer clusters.
arxiv-11700-42 | Multi-Objective Optimization for Self-Adjusting Weighted Gradient in Machine Learning Tasks | http://arxiv.org/abs/1506.01113 | author:Conrado Silva Miranda, Fernando José Von Zuben category:stat.ML cs.LG published:2015-06-03 summary:Much of the focus in machine learning research is placed in creating newarchitectures and optimization methods, but the overall loss function is seldomquestioned. This paper interprets machine learning from a multi-objectiveoptimization perspective, showing the limitations of the default linearcombination of loss functions over a data set and introducing the hypervolumeindicator as an alternative. It is shown that the gradient of the hypervolumeis defined by a self-adjusting weighted mean of the individual loss gradients,making it similar to the gradient of a weighted mean loss but without requiringthe weights to be defined a priori. This enables an inner boosting-likebehavior, where the current model is used to automatically place higher weightson samples with higher losses but without requiring the use of multiple models.Results on a denoising autoencoder show that the new formulation is able toachieve better mean loss than the direct optimization of the mean loss,providing evidence to the conjecture that self-adjusting the weights creates asmoother loss surface.
arxiv-11700-43 | No More Pesky Learning Rate Guessing Games | http://arxiv.org/abs/1506.01186 | author:Leslie N. Smith category:cs.CV cs.LG cs.NE published:2015-06-03 summary:It is known that the learning rate is the most important hyper-parameter totune for training deep convolutional neural networks (i.e., a "guessing game").This report describes a new method for setting the learning rate, namedcyclical learning rates, that eliminates the need to experimentally find thebest values and schedule for the learning rates. Instead of setting thelearning rate to fixed values, this method lets the learning rate cyclicallyvary within reasonable boundary values. This report shows that training withcyclical learning rates achieves near optimal classification accuracy withouttuning and often in many fewer iterations. This report also describes a simpleway to estimate "reasonable bounds" - by linearly increasing the learning ratein one training run of the network for only a few epochs. In addition, cyclicallearning rates are demonstrated on training with the CIFAR-10 dataset and theAlexNet and GoogLeNet architectures on the ImageNet dataset. These methods arepractical tools for everyone who trains convolutional neural networks.
arxiv-11700-44 | The Influence of Context on Dialogue Act Recognition | http://arxiv.org/abs/1506.00839 | author:Eugénio Ribeiro, Ricardo Ribeiro, David Martins de Matos category:cs.CL published:2015-06-02 summary:This article presents a deep analysis of the influence of context informationon dialogue act recognition. We performed experiments on the annotated subsetsof three different corpora: the widely explored Switchboard Dialog Act Corpus,as well as the unexplored LEGO and Cambridge Restaurant corpora. In contrast with previous work, especially in what concerns the Switchboardcorpus, we used an event-based classification approach, using SVMs, instead ofthe more common sequential approaches, such as HMMs. We opted for such anapproach so that we could control the amount of provided context informationand, thus, explore its range of influence. Our base features consist ofn-grams, punctuation, and wh-words. Context information is obtained fromprevious utterances and provided in three ways -- n-grams, n-grams tagged withrelative position, and dialogue act classifications. A comparative study wasconducted to evaluate the performance of the three approaches. From it, we wereable to assess the importance of context information on dialogue actrecognition, as well as its range of influence for each of the three selectedrepresentations. In addition to the conclusions originated by the analysis, this work alsoproduced results that advance the state-of-the-art, especially consideringprevious work on the Switchboard corpus. Furthermore, since, to our knowledge,the remaining datasets had not been previously explored for this task, ourexperiments can be used as baselines for future work on those corpora.
arxiv-11700-45 | Learning Speech Rate in Speech Recognition | http://arxiv.org/abs/1506.00799 | author:Xiangyu Zeng, Shi Yin, Dong Wang category:cs.CL cs.LG published:2015-06-02 summary:A significant performance reduction is often observed in speech recognitionwhen the rate of speech (ROS) is too low or too high. Most of presentapproaches to addressing the ROS variation focus on the change of speechsignals in dynamic properties caused by ROS, and accordingly modify the dynamicmodel, e.g., the transition probabilities of the hidden Markov model (HMM).However, an abnormal ROS changes not only the dynamic but also the staticproperty of speech signals, and thus can not be compensated for purely bymodifying the dynamic model. This paper proposes an ROS learning approach basedon deep neural networks (DNN), which involves an ROS feature as the input ofthe DNN model and so the spectrum distortion caused by ROS can be learned andcompensated for. The experimental results show that this approach can deliverbetter performance for too slow and too fast utterances, demonstrating ourconjecture that ROS impacts both the dynamic and the static property of speech.In addition, the proposed approach can be combined with the conventional HMMtransition adaptation method, offering additional performance gains.
arxiv-11700-46 | Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays | http://arxiv.org/abs/1506.00779 | author:Junpei Komiyama, Junya Honda, Hiroshi Nakagawa category:stat.ML cs.LG published:2015-06-02 summary:We discuss a multiple-play multi-armed bandit (MAB) problem in which severalarms are selected at each round. Recently, Thompson sampling (TS), a randomizedalgorithm with a Bayesian spirit, has attracted much attention for itsempirically excellent performance, and it is revealed to have an optimal regretbound in the standard single-play MAB problem. In this paper, we propose themultiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to themultiple-play MAB problem, and discuss its regret analysis. We prove that MP-TSfor binary rewards has the optimal regret upper bound that matches the regretlower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the firstcomputationally efficient algorithm with optimal regret. A set of computersimulations was also conducted, which compared MP-TS with state-of-the-artalgorithms. We also propose a modification of MP-TS, which is shown to havebetter empirical performance.
arxiv-11700-47 | Soft Computing Techniques for Change Detection in remotely sensed images : A Review | http://arxiv.org/abs/1506.00768 | author:Madhu Khurana, Vikas Saxena category:cs.NE cs.CV published:2015-06-02 summary:With the advent of remote sensing satellites, a huge repository of remotelysensed images is available. Change detection in remotely sensed images has beenan active research area as it helps us understand the transitions that aretaking place on the Earths surface. This paper discusses the methods and theirclassifications proposed by various researchers for change detection. Since useof soft computing based techniques are now very popular among researchcommunity, this paper also presents a classification based on learningtechniques used in soft-computing methods for change detection.
arxiv-11700-48 | Video (GIF) Sentiment Analysis using Large-Scale Mid-Level Ontology | http://arxiv.org/abs/1506.00765 | author:Zheng Cai, Donglin Cao, Rongrong Ji category:cs.MM cs.CL cs.IR published:2015-06-02 summary:With faster connection speed, Internet users are now making social network ahuge reservoir of texts, images and video clips (GIF). Sentiment analysis forsuch online platform can be used to predict political elections, evaluateseconomic indicators and so on. However, GIF sentiment analysis is quitechallenging, not only because it hinges on spatio-temporal visualcontentabstraction, but also for the relationship between such abstraction andfinal sentiment remains unknown.In this paper, we dedicated to find out suchrelationship.We proposed a SentiPairSequence basedspatiotemporal visualsentiment ontology, which forms the midlevel representations for GIFsentiment.The establishment process of SentiPair contains two steps. First, we constructthe Synset Forest to define the semantic tree structure of visual sentimentlabel elements. Then, through theSynset Forest, we organically select andcombine sentiment label elements to form a mid-level visual sentimentrepresentation. Our experiments indicate that SentiPair outperforms othercompeting mid-level attributes. Using SentiPair, our analysis frameworkcanachieve satisfying prediction accuracy (72.6%). We also opened ourdataset(GSO-2015) to the research community. GSO-2015 contains more than 6,000manually annotated GIFs out of more than 40,000 candidates. Each is labeledwith both sentiment and SentiPair Sequence.
arxiv-11700-49 | Image Retrieval Based on Binary Signature ang S-kGraph | http://arxiv.org/abs/1506.00761 | author:Thanh The Van, Thanh Manh Le category:cs.CV H.2.8; H.3.3 published:2015-06-02 summary:In this paper, we introduce an optimum approach for querying similar imageson large digital-image databases. Our work is based on RBIR (region-based imageretrieval) method which uses multiple regions as the key to retrieval images.This method significantly improves the accuracy of queries. However, this alsoincreases the cost of computing. To reduce this expensive computational cost,we implement binary signature encoder which maps an image to its identificationin binary. In order to fasten the lookup, binary signatures of images areclassified by the help of S-kGraph. Finally, our work is evaluated on COREL'simages.
arxiv-11700-50 | What Makes Kevin Spacey Look Like Kevin Spacey | http://arxiv.org/abs/1506.00752 | author:Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, Steve Seitz category:cs.CV published:2015-06-02 summary:We reconstruct a controllable model of a person from a large photo collectionthat captures his or her {\em persona}, i.e., physical appearance and behavior.The ability to operate on unstructured photo collections enables modeling ahuge number of people, including celebrities and other well photographed peoplewithout requiring them to be scanned. Moreover, we show the ability to drive or{\em puppeteer} the captured person B using any other video of a differentperson A. In this scenario, B acts out the role of person A, but retainshis/her own personality and character. Our system is based on a novelcombination of 3D face reconstruction, tracking, alignment, and multi-texturemodeling, applied to the puppeteering problem. We demonstrate convincingresults on a large variety of celebrities derived from Internet imagery andvideo.
arxiv-11700-51 | A Generalized Labeled Multi-Bernoulli Filter Implementation using Gibbs Sampling | http://arxiv.org/abs/1506.00821 | author:Hung Gia Hoang, Ba-Tuong Vo, Ba-Ngu Vo category:stat.CO cs.LG published:2015-06-02 summary:This paper proposes an efficient implementation of the generalized labeledmulti-Bernoulli (GLMB) filter by combining the prediction and update into asingle step. In contrast to the original approach which involves separatetruncations in the prediction and update steps, the proposed implementationrequires only one single truncation for each iteration, which can be performedusing a standard ranked optimal assignment algorithm. Furthermore, we propose anew truncation technique based on Markov Chain Monte Carlo methods such asGibbs sampling, which drastically reduces the complexity of the filter. Thesuperior performance of the proposed approach is demonstrated through extensivenumerical studies.
arxiv-11700-52 | Quantifying Creativity in Art Networks | http://arxiv.org/abs/1506.00711 | author:Ahmed Elgammal, Babak Saleh category:cs.AI cs.CV cs.CY cs.MM cs.SI published:2015-06-02 summary:Can we develop a computer algorithm that assesses the creativity of apainting given its context within art history? This paper proposes a novelcomputational framework for assessing the creativity of creative products, suchas paintings, sculptures, poetry, etc. We use the most common definition ofcreativity, which emphasizes the originality of the product and its influentialvalue. The proposed computational framework is based on constructing a networkbetween creative products and using this network to infer about the originalityand influence of its nodes. Through a series of transformations, we construct aCreativity Implication Network. We show that inference about creativity in thisnetwork reduces to a variant of network centrality problems which can be solvedefficiently. We apply the proposed framework to the task of quantifyingcreativity of paintings (and sculptures). We experimented on two datasets withover 62K paintings to illustrate the behavior of the proposed framework. Wealso propose a methodology for quantitatively validating the results of theproposed algorithm, which we call the "time machine experiment".
arxiv-11700-53 | Facial Expressions Tracking and Recognition: Database Protocols for Systems Validation and Evaluation | http://arxiv.org/abs/1506.00925 | author:Catarina Runa Miranda, Pedro Mendes, Pedro Coelho, Xenxo Alvarez, João Freitas, Miguel Sales Dias, Verónica Costa Orvalho category:cs.CV published:2015-06-02 summary:Each human face is unique. It has its own shape, topology, and distinguishingfeatures. As such, developing and testing facial tracking systems arechallenging tasks. The existing face recognition and tracking algorithms inComputer Vision mainly specify concrete situations according to particulargoals and applications, requiring validation methodologies with data that fitstheir purposes. However, a database that covers all possible variations ofexternal and factors does not exist, increasing researchers' work in acquiringtheir own data or compiling groups of databases. To address this shortcoming, we propose a methodology for facial dataacquisition through definition of fundamental variables, such as subjectcharacteristics, acquisition hardware, and performance parameters. Followingthis methodology, we also propose two protocols that allow the capturing offacial behaviors under uncontrolled and real-life situations. As validation, weexecuted both protocols which lead to creation of two sample databases: FdMiee(Facial database with Multi input, expressions, and environments) and FACIA(Facial Multimodal database driven by emotional induced acting). Using different types of hardware, FdMiee captures facial information underenvironmental and facial behaviors variations. FACIA is an extension of FdMieeintroducing a pipeline to acquire additional facial behaviors and speech usingan emotion-acting method. Therefore, this work eases the creation of adaptabledatabase according to algorithm's requirements and applications, leading tosimplified validation and testing processes.
arxiv-11700-54 | Discovering Valuable Items from Massive Data | http://arxiv.org/abs/1506.00935 | author:Hastagiri P. Vanchinathan, Andreas Marfurt, Charles-Antoine Robelin, Donald Kossmann, Andreas Krause category:cs.LG cs.AI cs.IT math.IT H.2.8 published:2015-06-02 summary:Suppose there is a large collection of items, each with an associated costand an inherent utility that is revealed only once we commit to selecting it.Given a budget on the cumulative cost of the selected items, how can we pick asubset of maximal value? This task generalizes several important problems suchas multi-arm bandits, active search and the knapsack problem. We present analgorithm, GP-Select, which utilizes prior knowledge about similarity be- tweenitems, expressed as a kernel function. GP-Select uses Gaussian processprediction to balance exploration (estimating the unknown value of items) andexploitation (selecting items of high value). We extend GP-Select to be able todiscover sets that simultaneously have high utility and are diverse. Ourpreference for diversity can be specified as an arbitrary monotone submodularfunction that quantifies the diminishing returns obtained when selectingsimilar items. Furthermore, we exploit the structure of the model updates toachieve an order of magnitude (up to 40X) speedup in our experiments withoutresorting to approximations. We provide strong guarantees on the performance ofGP-Select and apply it to three real-world case studies of industrialrelevance: (1) Refreshing a repository of prices in a Global DistributionSystem for the travel industry, (2) Identifying diverse, binding-affinepeptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scalerecommender system by recommending items to users.
arxiv-11700-55 | Global and Local Structure Preserving Sparse Subspace Learning: An Iterative Approach to Unsupervised Feature Selection | http://arxiv.org/abs/1506.01060 | author:Nan Zhou, Yangyang Xu, Hong Cheng, Jun Fang, Witold Pedrycz category:cs.LG 05-04 E.0 published:2015-06-02 summary:As we aim at alleviating the curse of high-dimensionality, subspace learningis becoming more popular. Existing approaches use either information aboutglobal or local structure of the data, and few studies simultaneously focus onglobal and local structures as the both of them contain important information.In this paper, we propose a global and local structure preserving sparsesubspace learning (GLoSS) model for unsupervised feature selection. The modelcan simultaneously realize feature selection and subspace learning. Inaddition, we develop a greedy algorithm to establish a generic combinatorialmodel, and an iterative strategy based on an accelerated block coordinatedescent is used to solve the GLoSS problem. We also provide whole iteratesequence convergence analysis of the proposed iterative algorithm. Extensiveexperiments are conducted on real-world datasets to show the superiority of theproposed approach over several state-of-the-art unsupervised feature selectionapproaches.
arxiv-11700-56 | Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases | http://arxiv.org/abs/1506.00999 | author:Alberto Garcia-Duran, Antoine Bordes, Nicolas Usunier, Yves Grandvalet category:cs.AI cs.CL cs.LG published:2015-06-02 summary:This paper tackles the problem of endogenous link prediction for KnowledgeBase completion. Knowledge Bases can be represented as directed graphs whosenodes correspond to entities and edges to relationships. Previous attemptseither consist of powerful systems with high capacity to model complexconnectivity patterns, which unfortunately usually end up overfitting on rarerelationships, or in approaches that trade capacity for simplicity in order tofairly model all relationships, frequent or not. In this paper, we proposeTatec a happy medium obtained by complementing a high-capacity model with asimpler one, both pre-trained separately and then combined. We present severalvariants of this model with different kinds of regularization and combinationstrategies and show that this approach outperforms existing methods ondifferent types of relationships by achieving state-of-the-art results on fourbenchmarks of the literature.
arxiv-11700-57 | Do Multi-Sense Embeddings Improve Natural Language Understanding? | http://arxiv.org/abs/1506.01070 | author:Jiwei Li, Dan Jurafsky category:cs.CL published:2015-06-02 summary:Learning a distinct representation for each sense of an ambiguous word couldlead to more powerful and fine-grained models of vector-space representations.Yet while `multi-sense' methods have been proposed and tested on artificialword-similarity tasks, we don't know if they improve real natural languageunderstanding tasks. In this paper we introduce a multi-sense embedding modelbased on Chinese Restaurant Processes that achieves state of the artperformance on matching human word similarity judgments, and propose apipelined architecture for incorporating multi-sense embeddings into languageunderstanding. We then test the performance of our model on part-of-speech tagging, namedentity recognition, sentiment analysis, semantic relation identification andsemantic relatedness, controlling for embedding dimensionality. We find thatmulti-sense embeddings do improve performance on some tasks (part-of-speechtagging, semantic relation identification, semantic relatedness) but not onothers (named entity recognition, various forms of sentiment analysis). Wediscuss how these differences may be caused by the different role of word senseinformation in each of the tasks. The results highlight the importance oftesting embedding models in real applications.
arxiv-11700-58 | Unsupervised Learning on Neural Network Outputs | http://arxiv.org/abs/1506.00990 | author:Yao Lu category:cs.LG published:2015-06-02 summary:The outputs of a trained neural network contain much richer information thanjust an one-hot classifier. For example, a neural network might give an imageof a dog the probability of one in a million of being a cat but it is stillmuch larger than the probability of being a car. To reveal the hidden structurein them, we apply two unsupervised learning algorithms, PCA and ICA, to theoutputs of a deep Convolutional Neural Network trained on the ImageNet of 1000classes. The PCA/ICA embedding of the object classes reveals their visualsimilarity and the PCA/ICA components can be interpreted as common visualfeatures shared by similar object classes. For an application, we proposed anew zero-shot learning method, in which the visual features learned by PCA/ICAare employed. Our zero-shot learning method achieves the state-of-the-artresults on the ImageNet of over 20000 classes.
arxiv-11700-59 | Peer Grading in a Course on Algorithms and Data Structures: Machine Learning Algorithms do not Improve over Simple Baselines | http://arxiv.org/abs/1506.00852 | author:Mehdi S. M. Sajjadi, Morteza Alamgir, Ulrike von Luxburg category:cs.LG stat.ML published:2015-06-02 summary:Peer grading is the process of students reviewing each others' work, such ashomework submissions, and has lately become a popular mechanism used in massiveopen online courses (MOOCs). Intrigued by this idea, we used it in a course onalgorithms and data structures at the University of Hamburg. Throughout thewhole semester, students repeatedly handed in submissions to exercises, whichwere then evaluated both by teaching assistants and by a peer gradingmechanism, yielding a large dataset of teacher and peer grades. We applieddifferent statistical and machine learning methods to aggregate the peer gradesin order to come up with accurate final grades for the submissions (supervisedand unsupervised, methods based on numeric scores and ordinal rankings).Surprisingly, none of them improves over the baseline of using the mean peergrade as the final grade. We discuss a number of possible explanations forthese results and present a thorough analysis of the generated dataset.
arxiv-11700-60 | Classify Images with Conceptor Network | http://arxiv.org/abs/1506.00815 | author:Yuhuang Hu, M. S. Ishwarya, Chu Kiong Loo category:cs.CV published:2015-06-02 summary:This article demonstrates a new conceptor network based classifier inclassifying images. Mathematical descriptions and analysis are presented.Various tests are experimented using three benchmark datasets: MNIST, CIFAR-10and CIFAR-100. The experiments displayed that conceptor network can offersuperior results and flexible configurations than conventional classifiers suchas Softmax Regression and Support Vector Machine (SVM).
arxiv-11700-61 | Homogeneous Spiking Neuromorphic System for Real-World Pattern Recognition | http://arxiv.org/abs/1506.01072 | author:Xinyu Wu, Vishal Saxena, Kehan Zhu category:cs.NE cs.AI cs.CV cs.ET published:2015-06-02 summary:A neuromorphic chip that combines CMOS analog spiking neurons and memristivesynapses offers a promising solution to brain-inspired computing, as it canprovide massive neural network parallelism and density. Previous hybrid analogCMOS-memristor approaches required extensive CMOS circuitry for training, andthus eliminated most of the density advantages gained by the adoption ofmemristor synapses. Further, they used different waveforms for pre andpost-synaptic spikes that added undesirable circuit overhead. Here we describea hardware architecture that can feature a large number of memristor synapsesto learn real-world patterns. We present a versatile CMOS neuron that combinesintegrate-and-fire behavior, drives passive memristors and implementscompetitive learning in a compact circuit module, and enables in-situplasticity in the memristor synapses. We demonstrate handwritten-digitsrecognition using the proposed architecture using transistor-level circuitsimulations. As the described neuromorphic architecture is homogeneous, itrealizes a fundamental building block for large-scale energy-efficientbrain-inspired silicon chips that could lead to next-generation cognitivecomputing.
arxiv-11700-62 | Toward a generic representation of random variables for machine learning | http://arxiv.org/abs/1506.00976 | author:Gautier Marti, Philippe Very, Philippe Donnat category:cs.LG stat.ML published:2015-06-02 summary:This paper presents a pre-processing and a distance which improve theperformance of machine learning algorithms working on independent andidentically distributed stochastic processes. We introduce a novelnon-parametric approach to represent random variables which splits apartdependency and distribution without losing any information. We also propound anassociated metric leveraging this representation and its statistical estimate.Besides experiments on synthetic datasets, the benefits of our contribution isillustrated through the example of clustering financial time series, forinstance prices from the credit default swaps market. Results are available onthe website www.datagrapple.com and an IPython Notebook tutorial is availableat www.datagrapple.com/Tech for reproducible research.
arxiv-11700-63 | An objective prior that unifies objective Bayes and information-based inference | http://arxiv.org/abs/1506.00745 | author:Colin H. LaMont, Paul A. Wiggins category:stat.ML cs.LG published:2015-06-02 summary:There are three principle paradigms of statistical inference: (i) Bayesian,(ii) information-based and (iii) frequentist inference. We describe anobjective prior (the weighting or $w$-prior) which unifies objective Bayes andinformation-based inference. The $w$-prior is chosen to make the marginalprobability an unbiased estimator of the predictive performance of the model.This definition has several other natural interpretations. From the perspectiveof the information content of the prior, the $w$-prior is both uniformly andmaximally uninformative. The $w$-prior can also be understood to result in auniform density of distinguishable models in parameter space. Finally wedemonstrate the the $w$-prior is equivalent to the Akaike Information Criterion(AIC) for regular models in the asymptotic limit. The $w$-prior appears to begenerically applicable to statistical inference and is free of {\it ad hoc}regularization. The mechanism for suppressing complexity is analogous to AIC:model complexity reduces model predictivity. We expect this new objective-Bayesapproach to inference to be widely-applicable to machine-learning problemsincluding singular models.
arxiv-11700-64 | Visualizing and Understanding Neural Models in NLP | http://arxiv.org/abs/1506.01066 | author:Jiwei Li, Xinlei Chen, Eduard Hovy, Dan Jurafsky category:cs.CL published:2015-06-02 summary:While neural networks have been successfully applied to many NLP tasks theresulting vector-based models are very difficult to interpret. For example it'snot clear how they achieve {\em compositionality}, building sentence meaningfrom the meanings of words and phrases. In this paper we describe fourstrategies for visualizing compositionality in neural models for NLP, inspiredby similar work in computer vision. We first plot unit values to visualizecompositionality of negation, intensification, and concessive clauses, allow usto see well-known markedness asymmetries in negation. We then introduce threesimple and straightforward methods for visualizing a unit's {\em salience}, theamount it contributes to the final composed meaning: (1) gradientback-propagation, (2) the variance of a token from the average word node, (3)LSTM-style gates that measure information flow. We test our methods onsentiment using simple recurrent nets and LSTMs. Our general-purpose methodsmay have wide applications for understanding compositionality and othersemantic properties of deep networks , and also shed light on why LSTMsoutperform simple recurrent nets,
arxiv-11700-65 | A CMOS Spiking Neuron for Dense Memristor-Synapse Connectivity for Brain-Inspired Computing | http://arxiv.org/abs/1506.01069 | author:Xinyu Wu, Vishal Saxena, Kehan Zhu category:cs.NE cs.ET published:2015-06-02 summary:Neuromorphic systems that densely integrate CMOS spiking neurons andnano-scale memristor synapses open a new avenue of brain-inspired computing.Existing silicon neurons have molded neural biophysical dynamics but areincompatible with memristor synapses, or used extra training circuitry thuseliminating much of the density advantages gained by using memristors, or wereenergy inefficient. Here we describe a novel CMOS spiking leakyintegrate-and-fire neuron circuit. Building on a reconfigurable architecturewith a single opamp, the described neuron accommodates a large number ofmemristor synapses, and enables online spike timing dependent plasticity (STDP)learning with optimized power consumption. Simulation results of an 180nm CMOSdesign showed 97% power efficiency metric when realizing STDP learning in10,000 memristor synapses with a nominal 1M{\Omega} memristance, and only13{\mu}A current consumption when integrating input spikes. Therefore, thedescribed CMOS neuron contributes a generalized building block for large-scalebrain-inspired neuromorphic systems.
arxiv-11700-66 | Extreme Compressive Sampling for Covariance Estimation | http://arxiv.org/abs/1506.00898 | author:Martin Azizyan, Akshay Krishnamurthy, Aarti Singh category:stat.ML cs.IT math.IT published:2015-06-02 summary:This paper studies the problem of estimating the covariance of a collectionof vectors using only extremely compressed measurements of each vector. Anestimator based on back-projections of these compressive samples is proposedand analyzed. A distribution-free analysis shows that by observing just asingle compressive measurement of each vector, one can consistently estimatethe covariance matrix, in both infinity and spectral norm, and this sameanalysis leads to precise rates of convergence in both norms. Viainformation-theoretic techniques, lower bounds showing that this estimator isminimax-optimal for both infinity and spectral norm estimation problems areestablished. These results are also specialized to give matching upper andlower bounds for estimating the population covariance of a collection ofGaussian vectors, again in the compressive measurement model. The analysisconducted in this paper shows that the effective sample complexity for thisproblem is scaled by a factor of $m^2/d^2$ where $m$ is the compressiondimension and $d$ is the ambient dimension. Applications to subspace learning(Principal Components Analysis) and learning over distributed sensor networksare also discussed.
arxiv-11700-67 | A Hierarchical Neural Autoencoder for Paragraphs and Documents | http://arxiv.org/abs/1506.01057 | author:Jiwei Li, Minh-Thang Luong, Dan Jurafsky category:cs.CL published:2015-06-02 summary:Natural language generation of coherent long texts like paragraphs or longerdocuments is a challenging problem for recurrent networks models. In thispaper, we explore an important step toward this generation task: training anLSTM (Long-short term memory) auto-encoder to preserve and reconstructmulti-sentence paragraphs. We introduce an LSTM model that hierarchicallybuilds an embedding for a paragraph from embeddings for sentences and words,then decodes this embedding to reconstruct the original paragraph. We evaluatethe reconstructed paragraph using standard metrics like ROUGE and Entity Grid,showing that neural models are able to encode texts in a way that preservesyntactic, semantic, and discourse coherence. While only a first step towardgenerating coherent text units from neural models, our work has the potentialto significantly impact natural language generation andsummarization\footnote{Code for the three models described in this paper can befound at www.stanford.edu/~jiweil/ .
arxiv-11700-68 | On bicluster aggregation and its benefits for enumerative solutions | http://arxiv.org/abs/1506.01077 | author:Saullo Haniell Galvão de Oliveira, Rosana Veroneze, Fernando José Von Zuben category:cs.LG published:2015-06-02 summary:Biclustering involves the simultaneous clustering of objects and theirattributes, thus defining local two-way clustering models. Recently, efficientalgorithms were conceived to enumerate all biclusters in real-valued datasets.In this case, the solution composes a complete set of maximal and non-redundantbiclusters. However, the ability to enumerate biclusters revealed a challengingscenario: in noisy datasets, each true bicluster may become highly fragmentedand with a high degree of overlapping. It prevents a direct analysis of theobtained results. To revert the fragmentation, we propose here two approachesfor properly aggregating the whole set of enumerated biclusters: one based onsingle linkage and the other directly exploring the rate of overlapping. Bothproposals were compared with each other and with the actual state-of-the-art inseveral experiments, and they not only significantly reduced the number ofbiclusters but also consistently increased the quality of the solution.
arxiv-11700-69 | RBIR using Interest Regions and Binary Signatures | http://arxiv.org/abs/1506.00368 | author:Thanh The Van, Thanh Manh Le category:cs.CV H.2.8; H.3.3 published:2015-06-01 summary:In this paper, we introduce an approach to overcome the low accuracy of theContent-Based Image Retrieval (CBIR) (when using the global features). Toincrease the accuracy, we use Harris-Laplace detector to identify the interestregions of image. Then, we build the Region-Based Image Retrieval (RBIR). Forthe efficient image storage and retrieval, we encode images into binarysignatures. The binary signature of a image is created from its interestregions. Furthermore, this paper also provides an algorithm for image retrievalon S-tree by comparing the images' signatures on a metric similarly to EMD(earth mover's distance). Finally, we evaluate the created models on COREL'simages.
arxiv-11700-70 | Imaging Time-Series to Improve Classification and Imputation | http://arxiv.org/abs/1506.00327 | author:Zhiguang Wang, Tim Oates category:cs.LG cs.NE stat.ML published:2015-06-01 summary:Inspired by recent successes of deep learning in computer vision, we proposea novel framework for encoding time series as different types of images,namely, Gramian Angular Summation/Difference Fields (GASF/GADF) and MarkovTransition Fields (MTF). This enables the use of techniques from computervision for time series classification and imputation. We used TiledConvolutional Neural Networks (tiled CNNs) on 20 standard datasets to learnhigh-level features from the individual and compound GASF-GADF-MTF images. Ourapproaches achieve highly competitive results when compared to nine of thecurrent best time series classification approaches. Inspired by the bijectionproperty of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) onthe GASF images of four standard and one synthesized compound dataset. Theimputation MSE on test data is reduced by 12.18%-48.02% when compared to usingthe raw data. An analysis of the features and weights learned via tiled CNNsand DAs explains why the approaches work.
arxiv-11700-71 | Hierarchical structure-and-motion recovery from uncalibrated images | http://arxiv.org/abs/1506.00395 | author:Roberto Toldo, Riccardo Gherardi, Michela Farenzena, Andrea Fusiello category:cs.CV published:2015-06-01 summary:This paper addresses the structure-and-motion problem, that requires to findcamera motion and 3D struc- ture from point matches. A new pipeline, dubbedSamantha, is presented, that departs from the prevailing sequential paradigmand embraces instead a hierarchical approach. This method has severaladvantages, like a provably lower computational complexity, which is necessaryto achieve true scalability, and better error containment, leading to morestability and less drift. Moreover, a practical autocalibration procedureallows to process images without ancillary information. Experiments with realdata assess the accuracy and the computational efficiency of the method.
arxiv-11700-72 | Robust Face Recognition with Structural Binary Gradient Patterns | http://arxiv.org/abs/1506.00481 | author:Weilin Huang, Hujun Yin category:cs.CV published:2015-06-01 summary:This paper presents a computationally efficient yet powerful binary frameworkfor robust facial representation based on image gradients. It is termed asstructural binary gradient patterns (SBGP). To discover underlying localstructures in the gradient domain, we compute image gradients from multipledirections and simplify them into a set of binary strings. The SBGP is derivedfrom certain types of these binary strings that have meaningful localstructures and are capable of resembling fundamental textural information. Theydetect micro orientational edges and possess strong orientation and localitycapabilities, thus enabling great discrimination. The SBGP also benefits fromthe advantages of the gradient domain and exhibits profound robustness againstillumination variations. The binary strategy realized by pixel correlations ina small neighborhood substantially simplifies the computational complexity andachieves extremely efficient processing with only 0.0032s in Matlab for atypical face image. Furthermore, the discrimination power of the SBGP can beenhanced on a set of defined orientational image gradient magnitudes, furtherenforcing locality and orientation. Results of extensive experiments on variousbenchmark databases illustrate significant improvements of the SBGP basedrepresentations over the existing state-of-the-art local descriptors in theterms of discrimination, robustness and complexity. Codes for the SBGP methodswill be available athttp://www.eee.manchester.ac.uk/research/groups/sisp/software/.
arxiv-11700-73 | User Preferences Modeling and Learning for Pleasing Photo Collage Generation | http://arxiv.org/abs/1506.00527 | author:Simone Bianco, Gianluigi Ciocca category:cs.MM cs.CV cs.HC published:2015-06-01 summary:In this paper we consider how to automatically create pleasing photo collagescreated by placing a set of images on a limited canvas area. The task isformulated as an optimization problem. Differently from existingstate-of-the-art approaches, we here exploit subjective experiments to modeland learn pleasantness from user preferences. To this end, we design anexperimental framework for the identification of the criteria that need to betaken into account to generate a pleasing photo collage. Five differentthematic photo datasets are used to create collages using state-of-the-artcriteria. A first subjective experiment where several subjects evaluated thecollages, emphasizes that different criteria are involved in the subjectivedefinition of pleasantness. We then identify new global and local criteria anddesign algorithms to quantify them. The relative importance of these criteriaare automatically learned by exploiting the user preferences, and new collagesare generated. To validate our framework, we performed several psycho-visualexperiments involving different users. The results shows that the proposedframework allows to learn a novel computational model which effectively encodesan inter-user definition of pleasantness. The learned definition ofpleasantness generalizes well to new photo datasets of different themes andsizes not used in the learning. Moreover, compared with two state of the artapproaches, the collages created using our framework are preferred by themajority of the users.
arxiv-11700-74 | How much is said in a microblog? A multilingual inquiry based on Weibo and Twitter | http://arxiv.org/abs/1506.00572 | author:Han-Teng Liao, King-wa Fu, Scott A. Hale category:cs.SI cs.CL cs.CY H.5.3, H.5.4 published:2015-06-01 summary:This paper presents a multilingual study on, per single post of microblogtext, (a) how much can be said, (b) how much is written in terms of charactersand bytes, and (c) how much is said in terms of information content in posts bydifferent organizations in different languages. Focusing on three differentlanguages (English, Chinese, and Japanese), this research analyses Weibo andTwitter accounts of major embassies and news agencies. We first establish ourcriterion for quantifying "how much can be said" in a digital text based on theopenly available Universal Declaration of Human Rights and the translatedsubtitles from TED talks. These parallel corpora allow us to determine thenumber of characters and bits needed to represent the same content in differentlanguages and character encodings. We then derive the amount of informationthat is actually contained in microblog posts authored by selected accounts onWeibo and Twitter. Our results confirm that languages with larger charactersets such as Chinese and Japanese contain more information per character thanEnglish, but the actual information content contained within a microblog textvaries depending on both the type of organization and the language of the post.We conclude with a discussion on the design implications of microblog textlimits for different languages.
arxiv-11700-75 | Modeling Relation Paths for Representation Learning of Knowledge Bases | http://arxiv.org/abs/1506.00379 | author:Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, Song Liu category:cs.CL published:2015-06-01 summary:Representation learning of knowledge bases (KBs) aims to embed both entitiesand relations into a low-dimensional space. Most existing methods only considerdirect relations in representation learning. We argue that multiple-steprelation paths also contain rich inference patterns between entities, andpropose a path-based representation learning model. This model considersrelation paths as translations between entities for representation learning,and addresses two key challenges: (1) Since not all relation paths arereliable, we design a path-constraint resource allocation algorithm to measurethe reliability of relation paths. (2) We represent relation paths via semanticcomposition of relation embeddings. Experimental results on real-world datasetsshow that, as compared with baselines, our model achieves significant andconsistent improvements on knowledge base completion and relation extractionfrom text.
arxiv-11700-76 | Robust PCA: Optimization of the Robust Reconstruction Error over the Stiefel Manifold | http://arxiv.org/abs/1506.00323 | author:Anastasia Podosinnikova, Simon Setzer, Matthias Hein category:stat.ML cs.LG published:2015-06-01 summary:It is well known that Principal Component Analysis (PCA) is strongly affectedby outliers and a lot of effort has been put into robustification of PCA. Inthis paper we present a new algorithm for robust PCA minimizing the trimmedreconstruction error. By directly minimizing over the Stiefel manifold, weavoid deflation as often used by projection pursuit methods. In distinction toother methods for robust PCA, our method has no free parameter and iscomputationally very efficient. We illustrate the performance on variousdatasets including an application to background modeling and subtraction. Ourmethod performs better or similar to current state-of-the-art methods whilebeing faster.
arxiv-11700-77 | Copeland Dueling Bandits | http://arxiv.org/abs/1506.00312 | author:Masrour Zoghi, Zohar Karnin, Shimon Whiteson, Maarten de Rijke category:cs.LG published:2015-06-01 summary:A version of the dueling bandit problem is addressed in which a Condorcetwinner may not exist. Two algorithms are proposed that instead seek to minimizeregret with respect to the Copeland winner, which, unlike the Condorcet winner,is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designedfor small numbers of arms, while the second, Scalable Copeland Bandits (SCB),works better for large-scale problems. We provide theoretical results boundingthe regret accumulated by CCB and SCB, both substantially improving existingresults. Such existing results either offer bounds of the form $O(K \log T)$but require restrictive assumptions, or offer bounds of the form $O(K^2 \logT)$ without requiring such assumptions. Our results offer the best of bothworlds: $O(K \log T)$ bounds without restrictive assumptions.
arxiv-11700-78 | A Riemannian low-rank method for optimization over semidefinite matrices with block-diagonal constraints | http://arxiv.org/abs/1506.00575 | author:Nicolas Boumal category:math.OC cs.CV stat.CO published:2015-06-01 summary:We propose a new algorithm to solve optimization problems of the form $\minf(X)$ for a smooth function $f$ under the constraints that $X$ is positivesemidefinite and the diagonal blocks of $X$ are small identity matrices. Suchproblems often arise as the result of relaxing a rank constraint (lifting). Inparticular, many estimation tasks involving phases, rotations, orthonormalbases or permutations fit in this framework, and so do certain relaxations ofcombinatorial problems such as Max-Cut. The proposed algorithm exploits thefacts that (1) such formulations admit low-rank solutions, and (2) theirrank-restricted versions are smooth optimization problems on a Riemannianmanifold. Combining insights from both the Riemannian and the convex geometriesof the problem, we characterize when second-order critical points of the smoothproblem reveal KKT points of the semidefinite problem. We compare against stateof the art, mature software and find that, on certain interesting probleminstances, what we call the staircase method is orders of magnitude faster, ismore accurate and scales better. Code is available.
arxiv-11700-79 | Learning to Answer Questions From Image Using Convolutional Neural Network | http://arxiv.org/abs/1506.00333 | author:Lin Ma, Zhengdong Lu, Hang Li category:cs.CL cs.CV cs.LG cs.NE published:2015-06-01 summary:In this paper, we propose to employ the convolutional neural network (CNN)for the image question answering (QA). Our proposed CNN provides an end-to-endframework with convolutional architectures for learning not only the image andquestion representations, but also their inter-modal interactions to producethe answer. More specifically, our model consists of three CNNs: one image CNNto encode the image content, one sentence CNN to compose the words of thequestion, and one multimodal convolution layer to learn their jointrepresentation for the classification in the space of candidate answer words.We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QAdatasets, which are two benchmark datasets for the image QA, with theperformances significantly outperforming the state-of-the-art.
arxiv-11700-80 | Learning with hidden variables | http://arxiv.org/abs/1506.00354 | author:Yasser Roudi, Graham Taylor category:q-bio.NC cs.LG cs.NE stat.ML published:2015-06-01 summary:Learning and inferring features that generate sensory input is a taskcontinuously performed by cortex. In recent years, novel algorithms andlearning rules have been proposed that allow neural network models to learnsuch features from natural images, written text, audio signals, etc. Thesenetworks usually involve deep architectures with many layers of hidden neurons.Here we review recent advancements in this area emphasizing, amongst otherthings, the processing of dynamical inputs by networks with hidden nodes andthe role of single neuron models. These points and the questions they arise canprovide conceptual advancements in understanding of learning in the cortex andthe relationship between machine learning approaches to learning with hiddennodes and those in cortical circuits.
arxiv-11700-81 | An Efficient Algorithm for Video Super-Resolution Based On a Sequential Model | http://arxiv.org/abs/1506.00473 | author:Patrick Héas, Angélique Drémeau, Cédric Herzet category:cs.CV published:2015-06-01 summary:In this work, we propose a novel procedure for video super-resolution, thatis the recovery of a sequence of high-resolution images from its low-resolutioncounterpart. Our approach is based on a "sequential" model (i.e., eachhigh-resolution frame is supposed to be a displaced version of the precedingone) and considers the use of sparsity-enforcing priors. Both the recovery ofthe high-resolution images and the motion fields relating them is tackled. Thisleads to a large-dimensional, non-convex and non-smooth problem. We propose analgorithmic framework to address the latter. Our approach relies on fastgradient evaluation methods and modern optimization techniques fornon-differentiable/non-convex problems. Unlike some other previous works, weshow that there exists a provably-convergent method with a complexity linear inthe problem dimensions. We assess the proposed optimization method on {severalvideo benchmarks and emphasize its good performance with respect to the stateof the art.}
arxiv-11700-82 | Mutual Dependence: A Novel Method for Computing Dependencies Between Random Variables | http://arxiv.org/abs/1506.00673 | author:Rahul Agarwal, Pierre Sacre, Sridevi V. Sarma category:math.ST stat.ML stat.TH published:2015-06-01 summary:In data science, it is often required to estimate dependencies betweendifferent data sources. These dependencies are typically calculated usingPearson's correlation, distance correlation, and/or mutual information.However, none of these measures satisfy all the Granger's axioms for an "idealmeasure". One such ideal measure, proposed by Granger himself, calculates theBhattacharyya distance between the joint probability density function (pdf) andthe product of marginal pdfs. We call this measure the mutual dependence.However, to date this measure has not been directly computable from data. Inthis paper, we use our recently introduced maximum likelihood non-parametricestimator for band-limited pdfs, to compute the mutual dependence directly fromthe data. We construct the estimator of mutual dependence and compare itsperformance to standard measures (Pearson's and distance correlation) fordifferent known pdfs by computing convergence rates, computational complexity,and the ability to capture nonlinear dependencies. Our mutual dependenceestimator requires fewer samples to converge to theoretical values, is fasterto compute, and captures more complex dependencies than standard measures.
arxiv-11700-83 | Statistical Machine Translation Features with Multitask Tensor Networks | http://arxiv.org/abs/1506.00698 | author:Hendra Setiawan, Zhongqiang Huang, Jacob Devlin, Thomas Lamar, Rabih Zbib, Richard Schwartz, John Makhoul category:cs.CL published:2015-06-01 summary:We present a three-pronged approach to improving Statistical MachineTranslation (SMT), building on recent success in the application of neuralnetworks to SMT. First, we propose new features based on neural networks tomodel various non-local translation phenomena. Second, we augment thearchitecture of the neural network with tensor layers that capture importanthigher-order interaction among the network units. Third, we apply multitasklearning to estimate the neural network parameters jointly. Each of ourproposed methods results in significant improvements that are complementary.The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English andChinese-English translation over a state-of-the-art system that alreadyincludes neural network features.
arxiv-11700-84 | Medical Synonym Extraction with Concept Space Models | http://arxiv.org/abs/1506.00528 | author:Chang Wang, Liangliang Cao, Bowen Zhou category:cs.CL published:2015-06-01 summary:In this paper, we present a novel approach for medical synonym extraction. Weaim to integrate the term embedding with the medical domain knowledge forhealthcare applications. One advantage of our method is that it is veryscalable. Experiments on a dataset with more than 1M term pairs show that theproposed approach outperforms the baseline approaches by a large margin.
arxiv-11700-85 | Monolingually Derived Phrase Scores for Phrase Based SMT Using Neural Networks Vector Representations | http://arxiv.org/abs/1506.00406 | author:Amir Pouya Aghasadeghi, Mohadeseh Bastan, Shahram Khadivi category:cs.CL published:2015-06-01 summary:In this paper, we propose two new features for estimating phrase-basedmachine translation parameters from mainly monolingual data. Our method isbased on two recently introduced neural network vector representation modelsfor words and sentences. It is the first time that these models have been usedin an end to end phrase-based machine translation system. Scores obtained fromour method can recover more than 80% of BLEU loss caused by removing phrasetable probabilities. We also show that our features combined with the phrasetable probabilities improve the BLEU score by absolute 0.74 points.
arxiv-11700-86 | Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection | http://arxiv.org/abs/1506.00552 | author:Julie Nutini, Mark Schmidt, Issam H. Laradji, Michael Friedlander, Hoyt Koepke category:math.OC cs.LG stat.CO stat.ML published:2015-06-01 summary:There has been significant recent work on the theory and application ofrandomized coordinate descent algorithms, beginning with the work of Nesterov[SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selectionrule achieves the same convergence rate as the Gauss-Southwell selection rule.This result suggests that we should never use the Gauss-Southwell rule, as itis typically much more expensive than random selection. However, the empiricalbehaviours of these algorithms contradict this theoretical result: inapplications where the computational costs of the selection rules arecomparable, the Gauss-Southwell selection rule tends to perform substantiallybetter than random coordinate selection. We give a simple analysis of theGauss-Southwell rule showing that---except in extreme cases---it's convergencerate is faster than choosing random coordinates. Further, in this work we (i)show that exact coordinate optimization improves the convergence rate forcertain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule thatgives an even faster convergence rate given knowledge of the Lipschitzconstants of the partial derivatives, (iii) analyze the effect of approximateGauss-Southwell rules, and (iv) analyze proximal-gradient variants of theGauss-Southwell rule.
arxiv-11700-87 | Bootstrap Bias Corrections for Ensemble Methods | http://arxiv.org/abs/1506.00553 | author:Giles Hooker, Lucas Mentch category:stat.ML published:2015-06-01 summary:This paper examines the use of a residual bootstrap for bias correction inmachine learning regression methods. Accounting for bias is an importantobstacle in recent efforts to develop statistical inference for machinelearning methods. We demonstrate empirically that the proposed bootstrap biascorrection can lead to substantial improvements in both bias and predictiveaccuracy. In the context of ensembles of trees, we show that this correctioncan be approximated at only double the cost of training the original ensemblewithout introducing additional variance. Our method is shown to improvetest-set accuracy over random forests by up to 70\% on example problems fromthe UCI repository.
arxiv-11700-88 | Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions | http://arxiv.org/abs/1506.00511 | author:Jimmy Ba, Kevin Swersky, Sanja Fidler, Ruslan Salakhutdinov category:cs.LG cs.CV cs.NE published:2015-06-01 summary:One of the main challenges in Zero-Shot Learning of visual categories isgathering semantic attributes to accompany images. Recent work has shown thatlearning from textual descriptions, such as Wikipedia articles, avoids theproblem of having to explicitly define these attributes. We present a new modelthat can classify unseen categories from their textual description.Specifically, we use text features to predict the output weights of both theconvolutional and the fully connected layers in a deep convolutional neuralnetwork (CNN). We take advantage of the architecture of CNNs and learn featuresat different layers, rather than just learning an embedding space for bothmodalities, as is common with existing approaches. The proposed model alsoallows us to automatically generate a list of pseudo- attributes for eachvisual category consisting of words from Wikipedia articles. We train ourmodels end-to-end us- ing the Caltech-UCSD bird and flower datasets andevaluate both ROC and Precision-Recall curves. Our empirical results show thatthe proposed model significantly outperforms previous methods.
arxiv-11700-89 | On Quantum Generalizations of Information-Theoretic Measures and their Contribution to Distributional Semantics | http://arxiv.org/abs/1506.00578 | author:William Blacoe category:cs.IT cs.CL math.IT published:2015-06-01 summary:Information-theoretic measures such as relative entropy and correlation areextremely useful when modeling or analyzing the interaction of probabilisticsystems. We survey the quantum generalization of 5 such measures and point outsome of their commonalities and interpretations. In particular we find theapplication of information theory to distributional semantics useful. Bymodeling the distributional meaning of words as density operators rather thanvectors, more of their semantic structure may be exploited. Furthermore,properties of and interactions between words such as ambiguity, similarity andentailment can be simulated more richly and intuitively when using methods fromquantum information theory.
arxiv-11700-90 | Sample-Optimal Density Estimation in Nearly-Linear Time | http://arxiv.org/abs/1506.00671 | author:Jayadev Acharya, Ilias Diakonikolas, Jerry Li, Ludwig Schmidt category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH published:2015-06-01 summary:We design a new, fast algorithm for agnostically learning univariateprobability distributions whose densities are well approximated by piecewisepolynomial functions. Let $f$ be the density function of an arbitraryunivariate distribution, and suppose that $f$ is $\mathrm{OPT}$-close in$L_1$-distance to an unknown piecewise polynomial function with $t$ intervalpieces and degree $d$. Our algorithm draws $n = O(t(d+1)/\epsilon^2)$ samplesfrom $f$, runs in time $\tilde{O}(n \cdot \mathrm{poly}(d))$, and withprobability at least $9/10$ outputs an $O(t)$-piecewise degree-$d$ hypothesis$h$ that is $4 \cdot \mathrm{OPT} +\epsilon$ close to $f$. Our general algorithm yields (nearly) sample-optimal and nearly-linear timeestimators for a wide range of structured distribution families over bothcontinuous and discrete domains in a unified way. For most of our applications,these are the first sample-optimal and nearly-linear time estimators in theliterature. As a consequence, our work resolves the sample and computationalcomplexities of a broad class of inference tasks via a single "meta-algorithm".Moreover, we experimentally demonstrate that our algorithm performs very wellin practice. Our algorithm consists of three "levels": (i) At the top level, we employ aniterative greedy algorithm for finding a good partition of the real line intothe pieces of a piecewise polynomial. (ii) For each piece, we show that thesub-problem of finding a good polynomial fit on the current interval can besolved efficiently with a separation oracle method. (iii) We reduce the task offinding a separating hyperplane to a combinatorial problem and give anefficient algorithm for this problem. Combining these three procedures gives adensity estimation algorithm with the claimed guarantees.
arxiv-11700-91 | Blocks and Fuel: Frameworks for deep learning | http://arxiv.org/abs/1506.00619 | author:Bart van Merriënboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley, Jan Chorowski, Yoshua Bengio category:cs.LG cs.NE stat.ML published:2015-06-01 summary:We introduce two Python frameworks to train neural networks on largedatasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compilerwith CUDA-support. It facilitates the training of complex neural network modelsby providing parametrized Theano operations, attaching metadata to Theano'ssymbolic computational graph, and providing an extensive set of utilities toassist training the networks, e.g. training algorithms, logging, monitoring,visualization, and serialization. Fuel provides a standard format for machinelearning datasets. It allows the user to easily iterate over large datasets,performing many types of pre-processing on the fly.
arxiv-11700-92 | Classifying Tweet Level Judgements of Rumours in Social Media | http://arxiv.org/abs/1506.00468 | author:Michal Lukasik, Trevor Cohn, Kalina Bontcheva category:cs.SI cs.CL cs.LG published:2015-06-01 summary:Social media is a rich source of rumours and corresponding communityreactions. Rumours reflect different characteristics, some shared and someindividual. We formulate the problem of classifying tweet level judgements ofrumours as a supervised learning task. Both supervised and unsupervised domainadaptation are considered, in which tweets from a rumour are classified on thebasis of other annotated rumours. We demonstrate how multi-task learning helpsachieve good results on rumours from the 2011 England riots.
arxiv-11700-93 | Network Topology Identification using PCA and its Graph Theoretic Interpretations | http://arxiv.org/abs/1506.00438 | author:Aravind Rajeswaran, Shankar Narasimhan category:cs.LG cs.DM cs.SY stat.ME published:2015-06-01 summary:We solve the problem of identifying (reconstructing) network topology fromsteady state network measurements. Concretely, given only a data matrix$\mathbf{X}$ where the $X_{ij}$ entry corresponds to flow in edge $i$ inconfiguration (steady-state) $j$, we wish to find a network structure for whichflow conservation is obeyed at all the nodes. This models many network problemsinvolving conserved quantities like water, power, and metabolic networks. Weshow that identification is equivalent to learning a model $\mathbf{A_n}$ whichcaptures the approximate linear relationships between the different variablescomprising $\mathbf{X}$ (i.e. of the form $\mathbf{A_n X \approx 0}$) such that$\mathbf{A_n}$ is full rank (highest possible) and consistent with a networknode-edge incidence structure. The problem is solved through a sequence ofsteps like estimating approximate linear relationships using PrincipalComponent Analysis, obtaining f-cut-sets from these approximate relationships,and graph realization from f-cut-sets (or equivalently f-circuits). Each stepand the overall process is polynomial time. The method is illustrated byidentifying topology of a water distribution network. We also study the extentof identifiability from steady-state data.
arxiv-11700-94 | Parallel Spectral Clustering Algorithm Based on Hadoop | http://arxiv.org/abs/1506.00227 | author:Yajun Cui, Yang Zhao, Kafei Xiao, Chenglong Zhang, Lei Wang category:cs.DC cs.DS cs.LG published:2015-05-31 summary:Spectral clustering and cloud computing is emerging branch of computerscience or related discipline. It overcome the shortcomings of some traditionalclustering algorithm and guarantee the convergence to the optimal solution,thus have to the widespread attention. This article first introduced theparallel spectral clustering algorithm research background and significance,and then to Hadoop the cloud computing Framework has carried on the detailedintroduction, then has carried on the related to spectral clustering isintroduced, then introduces the spectral clustering arithmetic Method ofparallel and relevant steps, finally made the related experiments, and theexperiment are summarized.
arxiv-11700-95 | Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion | http://arxiv.org/abs/1506.00196 | author:Kaisheng Yao, Geoffrey Zweig category:cs.CL published:2015-05-31 summary:Sequence-to-sequence translation methods based on generation with aside-conditioned language model have recently shown promising results inseveral tasks. In machine translation, models conditioned on source side wordshave been used to produce target-language text, and in image captioning, modelsconditioned images have been used to generate caption text. Past work with thisapproach has focused on large vocabulary tasks, and measured quality in termsof BLEU. In this paper, we explore the applicability of such models to thequalitatively different grapheme-to-phoneme task. Here, the input and outputside vocabularies are small, plain n-gram models do well, and credit is onlygiven when the output is exactly correct. We find that the simpleside-conditioned generation approach is able to rival the state-of-the-art, andwe are able to significantly advance the stat-of-the-art with bi-directionallong short-term memory (LSTM) neural networks that use the same alignmentinformation that is used in conventional approaches.
arxiv-11700-96 | Visual Madlibs: Fill in the blank Image Generation and Question Answering | http://arxiv.org/abs/1506.00278 | author:Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg category:cs.CV cs.CL published:2015-05-31 summary:In this paper, we introduce a new dataset consisting of 360,001 focusednatural language descriptions for 10,738 images. This dataset, the VisualMadlibs dataset, is collected using automatically produced fill-in-the-blanktemplates designed to gather targeted descriptions about: people and objects,their appearances, activities, and interactions, as well as inferences aboutthe general scene or its broader context. We provide several analyses of theVisual Madlibs dataset and demonstrate its applicability to two new descriptiongeneration tasks: focused description generation, and multiple-choicequestion-answering for images. Experiments using joint-embedding and deeplearning methods show promising results on these tasks.
arxiv-11700-97 | Interactive Knowledge Base Population | http://arxiv.org/abs/1506.00301 | author:Travis Wolfe, Mark Dredze, James Mayfield, Paul McNamee, Craig Harman, Tim Finin, Benjamin Van Durme category:cs.AI cs.CL published:2015-05-31 summary:Most work on building knowledge bases has focused on collecting entities andfacts from as large a collection of documents as possible. We argue for anddescribe a new paradigm where the focus is on a high-recall extraction over asmall collection of documents under the supervision of a human expert, that wecall Interactive Knowledge Base Population (IKBP).
arxiv-11700-98 | Automatic Inference for Inverting Software Simulators via Probabilistic Programming | http://arxiv.org/abs/1506.00308 | author:Ardavan Saeedi, Vlad Firoiu, Vikash Mansinghka category:stat.ML published:2015-05-31 summary:Models of complex systems are often formalized as sequential softwaresimulators: computationally intensive programs that iteratively build upprobable system configurations given parameters and initial conditions. Thesesimulators enable modelers to capture effects that are difficult tocharacterize analytically or summarize statistically. However, in manyreal-world applications, these simulations need to be inverted to match theobserved data. This typically requires the custom design, derivation andimplementation of sophisticated inversion algorithms. Here we give a frameworkfor inverting a broad class of complex software simulators via probabilisticprogramming and automatic inference, using under 20 lines of probabilisticcode. Our approach is based on a formulation of inversion as approximateinference in a simple sequential probabilistic model. We implement fourinference strategies, including Metropolis-Hastings, a sequentializedMetropolis-Hastings scheme, and a particle Markov chain Monte Carlo scheme,requiring 4 or fewer lines of probabilistic code each. We demonstrate ourframework by applying it to invert a real geological software simulator fromthe oil and gas industry.
arxiv-11700-99 | Recurrent Neural Networks with External Memory for Language Understanding | http://arxiv.org/abs/1506.00195 | author:Baolin Peng, Kaisheng Yao category:cs.CL cs.AI cs.LG cs.NE published:2015-05-31 summary:Recurrent Neural Networks (RNNs) have become increasingly popular for thetask of language understanding. In this task, a semantic tagger is deployed toassociate a semantic label to each word in an input sequence. The success ofRNN may be attributed to its ability to memorize long-term dependence thatrelates the current-time semantic label prediction to the observations manytime instances away. However, the memory capacity of simple RNNs is limitedbecause of the gradient vanishing and exploding problem. We propose to use anexternal memory to improve memorization capability of RNNs. We conductedexperiments on the ATIS dataset, and observed that the proposed model was ableto achieve the state-of-the-art results. We compare our proposed model withalternative models and report analysis results that may provide insights forfuture research.
arxiv-11700-100 | Diversity in Spectral Learning for Natural Language Parsing | http://arxiv.org/abs/1506.00275 | author:Shashi Narayan, Shay B. Cohen category:cs.CL published:2015-05-31 summary:We describe an approach to create a diverse set of predictions with spectrallearning of latent-variable PCFGs (L-PCFGs). Our approach works by creatingmultiple spectral models where noise is added to the underlying features in thetraining set before the estimation of each model. We describe three ways todecode with multiple models. In addition, we describe a simple variant of thespectral algorithm for L-PCFGs that is fast and leads to compact models. Ourexperiments for natural language parsing, for English and German, show that weget a significant improvement over baselines comparable to state of the art.For English, we achieve the $F_1$ score of 90.18, and for German we achieve the$F_1$ score of 83.38.
arxiv-11700-101 | Learning quantitative sequence-function relationships from massively parallel experiments | http://arxiv.org/abs/1506.00054 | author:Gurinder S. Atwal, Justin B. Kinney category:q-bio.QM math.ST physics.bio-ph stat.ML stat.TH published:2015-05-30 summary:A fundamental aspect of biological information processing is the ubiquity ofsequence-function relationships -- functions that map the sequence of DNA, RNA,or protein to a biochemically relevant activity. Most sequence-functionrelationships in biology are quantitative, but only recently have experimentaltechniques for effectively measuring these relationships been developed. Theadvent of such "massively parallel" experiments presents an excitingopportunity for the concepts and methods of statistical physics to inform thestudy of biological systems. After reviewing these recent experimentaladvances, we focus on the problem of how to infer parametric models ofsequence-function relationships from the data produced by these experiments.Specifically, we retrace and extend recent theoretical work showing thatinference based on mutual information, not the standard likelihood-basedapproach, is often necessary for accurately learning the parameters of thesemodels. Closely connected with this result is the emergence of "diffeomorphicmodes" -- directions in parameter space that are far less constrained by datathan likelihood-based inference would suggest. Analogous to Goldstone modes inphysics, diffeomorphic modes arise from an arbitrarily broken symmetry of theinference problem. An analytically tractable model of a massively parallelexperiment is then described, providing an explicit demonstration of thesefundamental aspects of statistical inference. This paper concludes with anoutlook on the theoretical and computational challenges currently facingstudies of quantitative sequence-function relationships.
arxiv-11700-102 | IDSA: Intelligent Distributed Sensor Activation Algorithm For Target Tracking With Wireless Sensor Network | http://arxiv.org/abs/1506.00122 | author:Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hoseini category:cs.NI cs.NE published:2015-05-30 summary:One important application of the Wireless Sensor Network(WSN) is targettracking, the aim of this application is converging to an event or object in anarea. In this paper, we propose an energy-efficient distributed sensoractivation protocol based on predicted location technique, called IntelligentDistributed Sensor Activation Algorithm (IDSA). The proposed algorithm predictsthe location of target in the next time interval, by analyzing current locationand movement history of the target, this prediction is done by computationalintelligence. The fewest essential number of sensor nodes within the predictedlocation will be activated to cover the target. The results show that theproposed method outperforms the existing methods such as Na\"ive and DSA interms of energy consumption and the number of nodes that was involved intracking the target.
arxiv-11700-103 | Using curvature to distinguish between surface reflections and vessel contents in computer vision based recognition of materials in transparent vessels | http://arxiv.org/abs/1506.00168 | author:Sagi Eppel category:cs.CV published:2015-05-30 summary:The recognition of materials and objects inside transparent containers usingcomputer vision has a wide range of applications, ranging from industrialbottles filling to the automation of chemistry laboratory. One of the mainchallenges in such recognition is the ability to distinguish between imagefeatures resulting from the vessels surface and image features resulting fromthe material inside the vessel. Reflections and the functional parts of avessels surface can create strong edges that can be mistakenly identified ascorresponding to the vessel contents, and cause recognition errors. The abilityto evaluate whether a specific edge in an image stems from the vessels surfaceor from its contents can considerably improve the ability to identify materialsinside transparent vessels. This work will suggest a method for suchevaluation, based on the following two assumptions: 1) Areas of high curvatureon the vessel surface are likely to cause strong edges due to changes inreflectivity, as is the appearance of functional parts (e.g. corks or valves).2) Most transparent vessels (bottles, glasses) have high symmetry(cylindrical). As a result the curvature angle of the vessels surface at eachpoint of the image is similar to the curvature angle of the contour line of thevessel in the same row in the image. These assumptions, allow theidentification of image regions with strong edges corresponding to the vesselsurface reflections. Combining this method with existing image analysis methodsfor detecting materials inside transparent containers allows considerableimprovement in accuracy.
arxiv-11700-104 | Efficient combination of pairswise feature networks | http://arxiv.org/abs/1506.00102 | author:Pau Bellot, Patrick E. Meyer category:stat.ML cs.LG published:2015-05-30 summary:This paper presents a novel method for the reconstruction of a neural networkconnectivity using calcium fluorescence data. We introduce a fast unsupervisedmethod to integrate different networks that reconstructs structuralconnectivity from neuron activity. Our method improves the state-of-the-artreconstruction method General Transfer Entropy (GTE). We are able to bettereliminate indirect links, improving therefore the quality of the network via anormalization and ensemble process of GTE and three new informative features.The approach is based on a simple combination of networks, which is remarkablyfast. The performance of our approach is benchmarked on simulated time seriesprovided at the connectomics challenge and also submitted at the publiccompetition.
arxiv-11700-105 | An Open Source Testing Tool for Evaluating Handwriting Input Methods | http://arxiv.org/abs/1506.00176 | author:Liquan Qiu, Lianwen Jin, Ruifen Dai, Yuxiang Zhang, Lei Li category:cs.HC cs.CV published:2015-05-30 summary:This paper presents an open source tool for testing the recognition accuracyof Chinese handwriting input methods. The tool consists of two modules, namelythe PC and Android mobile client. The PC client reads handwritten samples inthe computer, and transfers them individually to the Android client inaccordance with the socket communication protocol. After the Android clientreceives the data, it simulates the handwriting on screen of client device, andtriggers the corresponding handwriting recognition method. The recognitionaccuracy is recorded by the Android client. We present the design principlesand describe the implementation of the test platform. We construct several testdatasets for evaluating different handwriting recognition systems, and conductan objective and comprehensive test using six Chinese handwriting input methodswith five datasets. The test results for the recognition accuracy are thencompared and analyzed.
arxiv-11700-106 | A Review of Feature and Data Fusion with Medical Images | http://arxiv.org/abs/1506.00097 | author:Alex Pappachen James, Belur Dasarathy category:cs.CV published:2015-05-30 summary:The fusion techniques that utilize multiple feature sets to form new featuresthat are often more robust and contain useful information for future processingare referred to as feature fusion. The term data fusion is applied to the classof techniques used for combining decisions obtained from multiple feature setsto form global decisions. Feature and data fusion interchangeably represent twoimportant classes of techniques that have proved to be of practical importancein a wide range of medical imaging problems
arxiv-11700-107 | Recognition of convolutional neural network based on CUDA Technology | http://arxiv.org/abs/1506.00074 | author:Yi-bin Huang, Kang Li, Ge Wang, Min Cao, Pin Li, Yu-jia Zhang category:cs.DC cs.NE published:2015-05-30 summary:For the problem whether Graphic Processing Unit(GPU),the stream processorwith high performance of floating-point computing is applicable to neuralnetworks, this paper proposes the parallel recognition algorithm ofConvolutional Neural Networks(CNNs).It adopts Compute Unified DeviceArchitecture(CUDA)technology, definite the parallel data structures, anddescribes the mapping mechanism for computing tasks on CUDA. It compares theparallel recognition algorithm achieved on GPU of GTX200 hardware architecturewith the serial algorithm on CPU. It improves speed by nearly 60 times. Resultshows that GPU based the stream processor architecture ate more applicable tosome related applications about neural networks than CPU.
arxiv-11700-108 | A Three-stage Approach for Segmenting Degraded Color Images: Smoothing, Lifting and Thresholding (SLaT) | http://arxiv.org/abs/1506.00060 | author:Xiaohao Cai, Raymond Chan, Mila Nikolova, Tieyong Zeng category:cs.CV math.NA 65F22 I.4.6 published:2015-05-30 summary:In this paper, we propose a SLaT (Smoothing, Lifting and Thresholding) methodwith three stages for multiphase segmentation of color images corrupted bydifferent degradations: noise, information loss, and blur. At the first stage,a convex variant of the Mumford-Shah model is applied to each channel to obtaina smooth image. We show that the model has unique solution under the differentdegradations. In order to properly handle the color information, the secondstage is dimension lifting where we consider a new vector-valued image composedof the restored image and its transform in the secondary color space withadditional information. This ensures that even if the first color space hashighly correlated channels, we can still have enough information to give goodsegmentation results. In the last stage, we apply multichannel thresholding tothe combined vector-valued image to find the segmentation. The number of phasesis only required in the last stage, so users can choose or change it allwithout the need of solving the previous stages again. Experiments demonstratethat our SLaT method gives excellent results in terms of segmentation qualityand CPU time in comparison with other state-of-the-art segmentation methods.
arxiv-11700-109 | Efficient Bayesian experimentation using an expected information gain lower bound | http://arxiv.org/abs/1506.00053 | author:Panagiotis Tsilifis, Roger G. Ghanem, Paris Hajali category:stat.ML physics.geo-ph stat.CO stat.ME published:2015-05-30 summary:Experimental design is crucial for inference where limitations in the datacollection procedure are present due to cost or other restrictions. Optimalexperimental designs determine parameters that in some appropriate sense makethe data the most informative possible. In a Bayesian setting this istranslated to updating to the best possible posterior. Information theoreticarguments have led to the formation of the expected information gain as adesign criterion. This can be evaluated mainly by Monte Carlo sampling andmaximized by using stochastic approximation methods, both known for beingcomputationally expensive tasks. We propose a framework where a lower bound ofthe expected information gain is used as an alternative design criterion. Inaddition to alleviating the computational burden, this also addresses issuesconcerning estimation bias. The problem of permeability inference in a largecontaminated area is used to demonstrate the validity of our approach where weemploy the massively parallel version of the multiphase multicomponentsimulator TOUGH2 to simulate contaminant transport and a Polynomial Chaosapproximation of the forward model that further accelerates the objectivefunction evaluations. The proposed methodology is demonstrated to a settingwhere field measurements are available.
arxiv-11700-110 | Labeled compression schemes for extremal classes | http://arxiv.org/abs/1506.00165 | author:Shay Moran, Manfred K. Warmuth category:cs.LG cs.DM math.CO published:2015-05-30 summary:It is a long-standing open problem whether there always exists a compressionscheme whose size is of the order of the Vapnik-Chervonienkis (VC) dimension$d$. Recently compression schemes of size exponential in $d$ have been foundfor any concept class of VC dimension $d$. Previously size $d$ unlabeledcompression scheme have been given for maximum classes, which are specialconcept classes whose size equals an upper bound due to Sauer-Shelah. Weconsider a natural generalization of the maximum classes called extremalclasses. Their definition is based on a generalization of the Sauer-Shelahbound called the Sandwich Theorem which has applications in many areas ofcombinatorics. The key result of the paper is the construction of a labeledcompression scheme for extremal classes of size equal to their VC dimension. Wealso give a number of open problems concerning the combinatorial structure ofextremal classes and the existence of unlabeled compression schemes for them.
arxiv-11700-111 | Bag-of-Genres for Video Genre Retrieval | http://arxiv.org/abs/1506.00051 | author:Leonardo A. Duarte, Otávio A. B. Penatti, Jurandy Almeida category:cs.CV published:2015-05-30 summary:This paper presents a higher level representation for videos aiming at videogenre retrieval. In video genre retrieval, there is a challenge that videos maycomprise multiple categories, for instance, news videos may be composed ofsports, documentary, and action. Therefore, it is interesting to encode thedistribution of such genres in a compact and effective manner. We propose tocreate a visual dictionary using a genre classifier. Each visual word in theproposed model corresponds to a region in the classification space determinedby the classifier's model learned on the training frames. Therefore, the videofeature vector contains a summary of the activations of each genre in itscontents. We evaluate the bag-of-genres model for video genre retrieval, usingthe dataset of MediaEval Tagging Task of 2012. Results show that the proposedmodel increases the quality of the representation being more compact thanexisting features.
arxiv-11700-112 | Using Syntactic Features for Phishing Detection | http://arxiv.org/abs/1506.00037 | author:Gilchan Park, Julia M. Taylor category:cs.CL published:2015-05-29 summary:This paper reports on the comparison of the subject and object of verbs intheir usage between phishing emails and legitimate emails. The purpose of thisresearch is to explore whether the syntactic structures and subjects andobjects of verbs can be distinguishable features for phishing detection. Toachieve the objective, we have conducted two series of experiments: thesyntactic similarity for sentences, and the subject and object of verbcomparison. The results of the experiments indicated that both features can beused for some verbs, but more work has to be done for others.
arxiv-11700-113 | Feature Representation for Online Signature Verification | http://arxiv.org/abs/1505.08153 | author:Mohsen Fayyaz, Mohammad Hajizadeh_Saffar, Mohammad Sabokrou, Mahmood Fathy category:cs.CV cs.AI published:2015-05-29 summary:Biometrics systems have been used in a wide range of applications and haveimproved people authentication. Signature verification is one of the mostcommon biometric methods with techniques that employ various specifications ofa signature. Recently, deep learning has achieved great success in many fields,such as image, sounds and text processing. In this paper, deep learning methodhas been used for feature extraction and feature selection.
arxiv-11700-114 | Modeling of the meaning: computational interpreting and understanding of natural language fragments | http://arxiv.org/abs/1505.08149 | author:Michael Kapustin, Pavlo Kapustin category:cs.CL published:2015-05-29 summary:In this introductory article we present the basics of an approach toimplementing computational interpreting of natural language aiming to model themeanings of words and phrases. Unlike other approaches, we attempt to definethe meanings of text fragments in a composable and computer interpretable way.We discuss models and ideas for detecting different types of semanticincomprehension and choosing the interpretation that makes most sense in agiven context. Knowledge representation is designed for handlingcontext-sensitive and uncertain / imprecise knowledge, and for easyaccommodation of new information. It stores quantitative information capturingthe essence of the concepts, because it is crucial for working with naturallanguage understanding and reasoning. Still, the representation is generalenough to allow for new knowledge to be learned, and even generated by thesystem. The article concludes by discussing some reasoning-related topics:possible approaches to generation of new abstract concepts, and describingsituations and concepts in words (e.g. for specifying interpretationdifficulties).
arxiv-11700-115 | Learning to count with deep object features | http://arxiv.org/abs/1505.08082 | author:Santi Seguí, Oriol Pujol, Jordi Vitrià category:cs.CV published:2015-05-29 summary:Learning to count is a learning strategy that has been recently proposed inthe literature for dealing with problems where estimating the number of objectinstances in a scene is the final objective. In this framework, the task oflearning to detect and localize individual object instances is seen as a hardertask that can be evaded by casting the problem as that of computing aregression value from hand-crafted image features. In this paper we explore thefeatures that are learned when training a counting convolutional neural networkin order to understand their underlying representation. To this end we define acounting problem for MNIST data and show that the internal representation ofthe network is able to classify digits in spite of the fact that no directsupervision was provided for them during training. We also present preliminaryresults about a deep network that is able to count the number of pedestrians ina scene.
arxiv-11700-116 | Transition-Based Dependency Parsing with Stack Long Short-Term Memory | http://arxiv.org/abs/1505.08075 | author:Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, Noah A. Smith category:cs.CL cs.LG cs.NE published:2015-05-29 summary:We propose a technique for learning representations of parser states intransition-based dependency parsers. Our primary innovation is a new controlstructure for sequence-to-sequence neural networks---the stack LSTM. Like theconventional stack data structures used in transition-based parsing, elementscan be pushed to or popped from the top of the stack in constant time, but, inaddition, an LSTM maintains a continuous space embedding of the stack contents.This lets us formulate an efficient parsing model that captures three facets ofa parser's state: (i) unbounded look-ahead into the buffer of incoming words,(ii) the complete history of actions taken by the parser, and (iii) thecomplete contents of the stack of partially built tree fragments, includingtheir internal structures. Standard backpropagation techniques are used fortraining and yield state-of-the-art parsing performance.
arxiv-11700-117 | Geometry of Graph Edit Distance Spaces | http://arxiv.org/abs/1505.08071 | author:Brijnesh J. Jain category:cs.CV math.MG published:2015-05-29 summary:In this paper we study the geometry of graph spaces endowed with a specialclass of graph edit distances. The focus is on geometrical results useful forstatistical pattern recognition. The main result is the Graph RepresentationTheorem. It states that a graph is a point in some geometrical space, calledorbit space. Orbit spaces are well investigated and easier to explore than theoriginal graph space. We derive a number of geometrical results from the orbitspace representation, translate them to the graph space, and indicate theirsignificance and usefulness in statistical pattern recognition.
arxiv-11700-118 | A Critical Review of Recurrent Neural Networks for Sequence Learning | http://arxiv.org/abs/1506.00019 | author:Zachary C. Lipton, John Berkowitz, Charles Elkan category:cs.LG cs.NE published:2015-05-29 summary:Countless learning tasks require dealing with sequential data. Imagecaptioning, speech synthesis, and music generation all require that a modelproduce outputs that are sequences. In other domains, such as time seriesprediction, video analysis, and musical information retrieval, a model mustlearn from inputs that are sequences. Interactive tasks, such as translatingnatural language, engaging in dialogue, and controlling a robot, often demandboth capabilities. Recurrent neural networks (RNNs) are connectionist modelsthat capture the dynamics of sequences via cycles in the network of nodes.Unlike standard feedforward neural networks, recurrent networks retain a statethat can represent information from an arbitrarily long context window.Although recurrent neural networks have traditionally been difficult to train,and often contain millions of parameters, recent advances in networkarchitectures, optimization techniques, and parallel computation have enabledsuccessful large-scale learning with them. In recent years, systems based onlong short-term memory (LSTM) and bidirectional (BRNN) architectures havedemonstrated ground-breaking performance on tasks as varied as imagecaptioning, language translation, and handwriting recognition. In this survey,we review and synthesize the research that over the past three decades firstyielded and then made practical these powerful learning models. Whenappropriate, we reconcile conflicting notation and nomenclature. Our goal is toprovide a self-contained explication of the state of the art together with ahistorical perspective and references to primary research.
arxiv-11700-119 | Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding | http://arxiv.org/abs/1505.07909 | author:Huazheng Wang, Fei Tian, Bin Gao, Jiang Bian, Tie-Yan Liu category:cs.CL cs.IR cs.LG published:2015-05-29 summary:Intelligence Quotient (IQ) Test is a set of standardized questions designedto evaluate human intelligence. Verbal comprehension questions appear veryfrequently in IQ tests, which measure human's verbal ability including theunderstanding of the words with multiple senses, the synonyms and antonyms, andthe analogies among words. In this work, we explore whether such tests can besolved automatically by artificial intelligence technologies, especially thedeep learning technologies that are recently developed and successfully appliedin a number of fields. However, we found that the task was quite challenging,and simply applying existing technologies (e.g., word embedding) could notachieve a good performance, mainly due to the multiple senses of words and thecomplex relations among words. To tackle these challenges, we propose a novelframework consisting of three components. First, we build a classifier torecognize the specific type of a verbal question (e.g., analogy,classification, synonym, or antonym). Second, we obtain distributedrepresentations of words and relations by leveraging a novel word embeddingmethod that considers the multi-sense nature of words and the relationalknowledge among words (or their senses) contained in dictionaries. Third, foreach type of questions, we propose a specific solver based on the obtaineddistributed word representations and relation representations. Experimentalresults have shown that the proposed framework can not only outperform existingmethods for solving verbal comprehension questions but also exceed the averageperformance of the Amazon Mechanical Turk workers involved in the study. Theresults indicate that with appropriate uses of the deep learning technologieswe might be a further step closer to the human intelligence.
arxiv-11700-120 | General Deformations of Point Configurations Viewed By a Pinhole Model Camera | http://arxiv.org/abs/1505.08070 | author:Yirmeyahu Kaminski, Mike Werman category:cs.CV math.AG published:2015-05-29 summary:This paper is a theoretical study of the following Non-Rigid Structure fromMotion problem. What can be computed from a monocular view of a parametricallydeforming set of points? We treat various variations of this problem for affineand polynomial deformations with calibrated and uncalibrated cameras. We showthat in general at least three images with quasi-identical two deformations areneeded in order to have a finite set of solutions of the points' structure andcalculate some simple examples.
arxiv-11700-121 | Research on the fast Fourier transform of image based on GPU | http://arxiv.org/abs/1505.08019 | author:Feifei Shen, Zhenjian Song, Congrui Wu, Jiaqi Geng, Qingyun Wang category:cs.MS cs.CV published:2015-05-29 summary:Study of general purpose computation by GPU (Graphics Processing Unit) canimprove the image processing capability of micro-computer system. This paperstudies the parallelism of the different stages of decimation in time radix 2FFT algorithm, designs the butterfly and scramble kernels and implements 2D FFTon GPU. The experiment result demonstrates the validity and advantage overgeneral CPU, especially in the condition of large input size. The approach canalso be generalized to other transforms alike.
arxiv-11700-122 | Symbolic Segmentation Using Algorithm Selection | http://arxiv.org/abs/1505.07934 | author:Martin Lukac, Kamila Abdiyeva, Michitaka Kameyama category:cs.CV published:2015-05-29 summary:In this paper we present an alternative approach to symbolic segmentation;instead of implementing a new method we approach symbolic segmentation as analgorithm selection problem. That is, let there be $n$ available algorithms forsymbolic segmentation, a selection mechanism forms a set of input features andimage attributes and selects on a case by case basis the best algorithm. Theselection mechanism is demonstrated from within an algorithm framework wherethe selection is done in a set of various algorithm networks. Two sets ofexperiments are performed and in both cases we demonstrate that the algorithmselection allows to increase the result of the symbolic segmentation by aconsiderable amount.
arxiv-11700-123 | Supervised Fine Tuning for Word Embedding with Integrated Knowledge | http://arxiv.org/abs/1505.07931 | author:Xuefeng Yang, Kezhi Mao category:cs.CL published:2015-05-29 summary:Learning vector representation for words is an important research field whichmay benefit many natural language processing tasks. Two limitations exist innearly all available models, which are the bias caused by the contextdefinition and the lack of knowledge utilization. They are difficult to tacklebecause these algorithms are essentially unsupervised learning approaches.Inspired by deep learning, the authors propose a supervised framework forlearning vector representation of words to provide additional supervised finetuning after unsupervised learning. The framework is knowledge rich approacherand compatible with any numerical vectors word representation. The authorsperform both intrinsic evaluation like attributional and relational similarityprediction and extrinsic evaluations like the sentence completion and sentimentanalysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets showthat the proposed fine tuning framework may significantly improve the qualityof the vector representation of words.
arxiv-11700-124 | Batch Bayesian Optimization via Local Penalization | http://arxiv.org/abs/1505.08052 | author:Javier González, Zhenwen Dai, Philipp Hennig, Neil D. Lawrence category:stat.ML published:2015-05-29 summary:The popularity of Bayesian optimization methods for efficient exploration ofparameter spaces has lead to a series of papers applying Gaussian processes assurrogates in the optimization of functions. However, most proposed approachesonly allow the exploration of the parameter space to occur sequentially. Often,it is desirable to simultaneously propose batches of parameter values toexplore. This is particularly the case when large parallel processingfacilities are available. These facilities could be computational or physicalfacets of the process being optimized. E.g. in biological experiments manyexperimental set ups allow several samples to be simultaneously processed.Batch methods, however, require modeling of the interaction between theevaluations in the batch, which can be expensive in complex scenarios. Weinvestigate a simple heuristic based on an estimate of the Lipschitz constantthat captures the most important aspect of this interaction (i.e. localrepulsion) at negligible computational overhead. The resulting algorithmcompares well, in running time, with much more elaborate alternatives. Theapproach assumes that the function of interest, $f$, is a Lipschitz continuousfunction. A wrap-loop around the acquisition function is used to collectbatches of points of certain size minimizing the non-parallelizablecomputational effort. The speed-up of our method with respect to previousapproaches is significant in a set of computationally expensive experiments.
arxiv-11700-125 | CURL: Co-trained Unsupervised Representation Learning for Image Classification | http://arxiv.org/abs/1505.08098 | author:Simone Bianco, Gianluigi Ciocca, Claudio Cusano category:cs.LG cs.CV stat.ML I.2.6 published:2015-05-29 summary:In this paper we propose a strategy for semi-supervised image classificationthat leverages unsupervised representation learning and co-training. Thestrategy, that is called CURL from Co-trained Unsupervised RepresentationLearning, iteratively builds two classifiers on two different views of thedata. The two views correspond to different representations learned from bothlabeled and unlabeled data and differ in the fusion scheme used to combine theimage features. To assess the performance of our proposal, we conducted severalexperiments on widely used data sets for scene and object recognition. Weconsidered three scenarios (inductive, transductive and self-taught learning)that differ in the strategy followed to exploit the unlabeled data. As imagefeatures we considered a combination of GIST, PHOG, and LBP as well as featuresextracted from a Convolutional Neural Network. Moreover, two embodiments ofCURL are investigated: one using Ensemble Projection as unsupervisedrepresentation learning coupled with Logistic Regression, and one based onLapSVM. The results show that CURL clearly outperforms other supervised andsemi-supervised learning methods in the state of the art.
arxiv-11700-126 | Salient Object Detection via Augmented Hypotheses | http://arxiv.org/abs/1505.07930 | author:Tam V. Nguyen, Jose Sepulveda category:cs.CV published:2015-05-29 summary:In this paper, we propose using \textit{augmented hypotheses} which considerobjectness, foreground and compactness for salient object detection. Ouralgorithm consists of four basic steps. First, our method generates theobjectness map via objectness hypotheses. Based on the objectness map, weestimate the foreground margin and compute the corresponding foreground mapwhich prefers the foreground objects. From the objectness map and theforeground map, the compactness map is formed to favor the compact objects. Wethen derive a saliency measure that produces a pixel-accurate saliency mapwhich uniformly covers the objects of interest and consistently separates fore-and background. We finally evaluate the proposed framework on two challengingdatasets, MSRA-1000 and iCoSeg. Our extensive experimental results show thatour method outperforms state-of-the-art approaches.
arxiv-11700-127 | On the Computational Complexity of High-Dimensional Bayesian Variable Selection | http://arxiv.org/abs/1505.07925 | author:Yun Yang, Martin J. Wainwright, Michael I. Jordan category:math.ST cs.LG stat.CO stat.ME stat.ML stat.TH published:2015-05-29 summary:We study the computational complexity of Markov chain Monte Carlo (MCMC)methods for high-dimensional Bayesian linear regression under sparsityconstraints. We first show that a Bayesian approach can achievevariable-selection consistency under relatively mild conditions on the designmatrix. We then demonstrate that the statistical criterion of posteriorconcentration need not imply the computational desideratum of rapid mixing ofthe MCMC algorithm. By introducing a truncated sparsity prior for variableselection, we provide a set of conditions that guarantee bothvariable-selection consistency and rapid mixing of a particularMetropolis-Hastings algorithm. The mixing time is linear in the number ofcovariates up to a logarithmic factor. Our proof controls the spectral gap ofthe Markov chain by constructing a canonical path ensemble that is inspired bythe steps taken by greedy algorithms for variable selection.
arxiv-11700-128 | Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network | http://arxiv.org/abs/1505.07922 | author:Junshi Huang, Rogerio S. Feris, Qiang Chen, Shuicheng Yan category:cs.CV published:2015-05-29 summary:We address the problem of cross-domain image retrieval, considering thefollowing practical application: given a user photo depicting a clothing image,our goal is to retrieve the same or attribute-similar clothing items fromonline shopping stores. This is a challenging problem due to the largediscrepancy between online shopping images, usually taken in ideallighting/pose/background conditions, and user photos captured in uncontrolledconditions. To address this problem, we propose a Dual Attribute-aware RankingNetwork (DARN) for retrieval feature learning. More specifically, DARN consistsof two sub-networks, one for each domain, whose retrieval featurerepresentations are driven by semantic attribute learning. We show that thisattribute-guided learning is a key factor for retrieval accuracy improvement.In addition, to further align with the nature of the retrieval problem, weimpose a triplet visual similarity constraint for learning to rank across thetwo sub-networks. Another contribution of our work is a large-scale datasetwhich makes the network learning feasible. We exploit customer review websitesto crawl a large set of online shopping images and corresponding offline userphotos with fine-grained clothing attributes, i.e., around 450,000 onlineshopping images and about 90,000 exact offline counterpart images of thoseonline ones. All these images are collected from real-world consumer websitesreflecting the diversity of the data modality, which makes this dataset uniqueand rare in the academic community. We extensively evaluate the retrievalperformance of networks in different configurations. The top-20 retrievalaccuracy is doubled when using the proposed DARN other than the current popularsolution using pre-trained CNN features only (0.570 vs. 0.268).
arxiv-11700-129 | Fast Computation of PERCLOS and Saccadic Ratio | http://arxiv.org/abs/1505.07923 | author:Anirban Dasgupta, Aurobinda Routray category:cs.CV published:2015-05-29 summary:This thesis describes the development of fast algorithms for the computationof PERcentage CLOSure of eyes (PERCLOS) and Saccadic Ratio (SR). PERCLOS and SRare two ocular parameters reported to be measures of alertness levels in humanbeings. PERCLOS is the percentage of time in which at least 80% of the eyelidremains closed over the pupil. Saccades are fast and simultaneous movement ofboth the eyes in the same direction. SR is the ratio of peak saccadic velocityto the saccadic duration. This thesis addresses the issues of image basedestimation of PERCLOS and SR, prevailing in the literature such as illuminationvariation, poor illumination conditions, head rotations etc. In this work,algorithms for real-time PERCLOS computation has been developed and implementedon an embedded platform. The platform has been used as a case study forassessment of loss of attention in automotive drivers. The SR estimation hasbeen carried out offline as real-time implementation requires high frame ratesof processing which is difficult to achieve due to hardware limitations. Theaccuracy in estimation of the loss of attention using PERCLOS and SR has beenvalidated using brain signals, which are reported to be an authentic cue forestimating the state of alertness in human beings. The major contributions ofthis thesis include database creation, design and implementation of fastalgorithms for estimating PERCLOS and SR on embedded computing platforms.
arxiv-11700-130 | A Bounded $p$-norm Approximation of Max-Convolution for Sub-Quadratic Bayesian Inference on Additive Factors | http://arxiv.org/abs/1505.07519 | author:Julianus Pfeuffer, Oliver Serang category:stat.CO cs.NA stat.ML published:2015-05-28 summary:Max-convolution is an important problem closely resembling standardconvolution; as such, max-convolution occurs frequently across many fields.Here we extend the method with fastest known worst-case runtime, which can beapplied to nonnegative vectors by numerically approximating the Chebyshev norm$\ \cdot \_\infty$, and use this approach to derive two numerically stablemethods based on the idea of computing $p$-norms via fast convolution: Thefirst method proposed, with runtime in $O( k \log(k) \log(\log(k)) )$ (which isless than $18 k \log(k)$ for any vectors that can be practically realized),uses the $p$-norm as a direct approximation of the Chebyshev norm. The secondapproach proposed, with runtime in $O( k \log(k) )$ (although in practice bothperform similarly), uses a novel null space projection method, which extractsinformation from a sequence of $p$-norms to estimate the maximum value in thevector (this is equivalent to querying a small number of moments from adistribution of bounded support in order to estimate the maximum). The $p$-normapproaches are compared to one another and are shown to compute anapproximation of the Viterbi path in a hidden Markov model where the transitionmatrix is a Toeplitz matrix; the runtime of approximating the Viterbi path isthus reduced from $O( n k^2 )$ steps to $O( n $k \log(k))$ steps in practice,and is demonstrated by inferring the U.S. unemployment rate from the S&P 500stock index.
arxiv-11700-131 | A new Semi-Markov model based clustering for a Clustering-Scheduling Integrated framework for Patient Flow Modeling and Optimization | http://arxiv.org/abs/1505.07752 | author:Chitta Ranjan, Kamran Paynabar, Jonathan E. Helm category:stat.ME stat.AP stat.ML published:2015-05-28 summary:The ability to accurately forecast and control inpatient census, and therebyworkloads, is a critical and longstanding problem in hospital management. Themajority of current literature focuses on optimal scheduling of electiveinpatients, but largely ignores the process of accurate estimation of thetrajectory of patients throughout the treatment and recovery process. Theresult is that current elective scheduling models are optimizing based oninaccurate input data. In this paper, we develop a Clustering and SchedulingIntegrated (CSI) approach to capture patient flows through a network ofhospital services. CSI functions by clustering patients into groups based onsimilarity of trajectory using a Semi-Markov model-based clustering scheme, asopposed to clustering by admit type or condition as in previous literature. Themethodology is validated by simulation and then applied to real patient datafrom a partner hospital where we see it outperforms current methods. Further,we demonstrate that optimization methods achieve significantly better resultson key hospital performance measure under CSI, compared with traditionalestimation approaches, increasing elective admissions by 97% and utilization by22% compared to 30% and 8% using traditional estimation techniques. From atheoretical standpoint, the SMM-clustering is a novel approach applicable toany temporal-spatial stochastic data that is prevalent in many industries.
arxiv-11700-132 | A CMOS Spiking Neuron for Brain-Inspired Neural Networks with Resistive Synapses and In-Situ Learning | http://arxiv.org/abs/1505.07814 | author:Xinyu Wu, Vishal Saxena, Kehan Zhu, Sakkarapani Balagopal category:cs.NE published:2015-05-28 summary:Nanoscale resistive memories are expected to fuel dense integration ofelectronic synapses for large-scale neuromorphic system. To realize such abrain-inspired computing chip, a compact CMOS spiking neuron that performsin-situ learning and computing while driving a large number of resistivesynapses is desired. This work presents a novel leaky integrate-and-fire neurondesign which implements the dual-mode operation of current integration andsynaptic drive, with a single opamp and enables in-situ learning with crossbarresistive synapses. The proposed design was implemented in a 0.18 $\mu$m CMOStechnology. Measurements show neuron's ability to drive a thousand resistivesynapses, and demonstrate an in-situ associative learning. The neuron circuitoccupies a small area of 0.01 mm$^2$ and has an energy-efficiency of 9.3pJ$/$spike$/$synapse.
arxiv-11700-133 | Overview of the NLPCC 2015 Shared Task: Chinese Word Segmentation and POS Tagging for Micro-blog Texts | http://arxiv.org/abs/1505.07599 | author:Xipeng Qiu, Peng Qian, Liusong Yin, Shiyu Wu, Xuanjing Huang category:cs.CL published:2015-05-28 summary:In this paper, we give an overview for the shared task at the 4th CCFConference on Natural Language Processing \& Chinese Computing (NLPCC 2015):Chinese word segmentation and part-of-speech (POS) tagging for micro-blogtexts. Different with the popular used newswire datasets, the dataset of thisshared task consists of the relatively informal micro-texts. The shared taskhas two sub-tasks: (1) individual Chinese word segmentation and (2) jointChinese word segmentation and POS Tagging. Each subtask has three tracks todistinguish the systems with different resources. We first introduce thedataset and task, then we characterize the different approaches of theparticipating systems, report the test results, and provide a overview analysisof these results. An online system is available for open registration andevaluation at http://nlp.fudan.edu.cn/nlpcc2015.
arxiv-11700-134 | Domain-Adversarial Training of Neural Networks | http://arxiv.org/abs/1505.07818 | author:Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, Victor Lempitsky category:stat.ML cs.LG cs.NE published:2015-05-28 summary:We introduce a new representation learning approach for domain adaptation, inwhich data at training and test time come from similar but differentdistributions. Our approach is directlyinspired by the theory on domainadaptation suggesting that, for effective domain transfer to be achieved,predictions must be made based on features that cannot discriminate between thetraining (source) and test (target) domains. The approach implements this idea in the context of neural networkarchitectures that are trained on labeled data from the source domain andunlabeled data from the target domain (no labeled target-domain data isnecessary). As the training progresses, the approach promotes the emergence offeatures that are (i) discriminative for the main learning task on the sourcedomain and (ii) indiscriminate with respect to the shift between the domains.We show that this adaptation behaviour can be achieved in almost anyfeed-forward model by augmenting it with few standard layers and a new gradientreversal layer. The resulting augmented architecture can be trained usingstandard backpropagation and stochastic gradient descent, and can thus beimplemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classificationproblems (document sentiment analysis and image classification), wherestate-of-the-art domain adaptation performance on standard benchmarks isachieved. We also validate the approach for descriptor learning task in thecontext of person re-identification application.
arxiv-11700-135 | Automatic Relevance Determination For Deep Generative Models | http://arxiv.org/abs/1505.07765 | author:Theofanis Karaletsos, Gunnar Rätsch category:stat.ML published:2015-05-28 summary:A recurring problem when building probabilistic latent variable models isregularization and model selection, for instance, the choice of thedimensionality of the latent space. In the context of belief networks withlatent variables, this problem has been adressed with Automatic RelevanceDetermination (ARD) employing Monte Carlo inference. We present a variationalinference approach to ARD for Deep Generative Models using doubly stochasticvariational inference to provide fast and scalable learning. We show empiricalresults on a standard dataset illustrating the effects of contracting thelatent space automatically. We show that the resulting latent representationsare significantly more compact without loss of expressive power of the learnedmodels.
arxiv-11700-136 | Query by String word spotting based on character bi-gram indexing | http://arxiv.org/abs/1505.07778 | author:Suman K. Ghosh, Ernest Valveny category:cs.CV published:2015-05-28 summary:In this paper we propose a segmentation-free query by string word spottingmethod. Both the documents and query strings are encoded using a recentlyproposed word representa- tion that projects images and strings into a commonatribute space based on a pyramidal histogram of characters(PHOC). Theseattribute models are learned using linear SVMs over the Fisher Vectorrepresentation of the images along with the PHOC labels of the correspondingstrings. In order to search through the whole page, document regions areindexed per character bi- gram using a similar attribute representation. On topof that, we propose an integral image representation of the document using asimplified version of the attribute model for efficient computation. Finally weintroduce a re-ranking step in order to boost retrieval performance. We showstate-of-the-art results for segmentation-free query by string word spotting insingle-writer and multi-writer standard datasets
arxiv-11700-137 | A Category Theory of Communication Theory | http://arxiv.org/abs/1505.07712 | author:Eric Werner category:cs.IT cs.CL cs.LO math.IT published:2015-05-28 summary:A theory of how agents can come to understand a language is presented. Ifunderstanding a sentence $\alpha$ is to associate an operator with $\alpha$that transforms the representational state of the agent as intended by thesender, then coming to know a language involves coming to know the operatorsthat correspond to the meaning of any sentence. This involves a higher orderoperator that operates on the possible transformations that operate on therepresentational capacity of the agent. We formalize these constructs usingconcepts and diagrams analogous to category theory.
arxiv-11700-138 | Invertible Orientation Scores of 3D Images | http://arxiv.org/abs/1505.07690 | author:Michiel Janssen, Remco Duits, Marcel Breeuwer category:math.NA cs.CV published:2015-05-28 summary:The enhancement and detection of elongated structures in noisy image data isrelevant for many biomedical applications. To handle complex crossingstructures in 2D images, 2D orientation scores were introduced, which alreadyshowed their use in a variety of applications. Here we extend this work to 3Dorientation scores. First, we construct the orientation score from a givendataset, which is achieved by an invertible coherent state type of transform.For this transformation we introduce 3D versions of the 2D cake-wavelets, whichare complex wavelets that can simultaneously detect oriented structures andoriented edges. For efficient implementation of the different steps in thewavelet creation we use a spherical harmonic transform. Finally, we show somefirst results of practical applications of 3D orientation scores.
arxiv-11700-139 | Improved Deep Convolutional Neural Network For Online Handwritten Chinese Character Recognition using Domain-Specific Knowledge | http://arxiv.org/abs/1505.07675 | author:Weixin Yang, Lianwen Jin, Zecheng Xie, Ziyong Feng category:cs.CV published:2015-05-28 summary:Deep convolutional neural networks (DCNNs) have achieved great success invarious computer vision and pattern recognition applications, including thosefor handwritten Chinese character recognition (HCCR). However, most currentDCNN-based HCCR approaches treat the handwritten sample simply as an imagebitmap, ignoring some vital domain-specific information that may be useful butthat cannot be learnt by traditional networks. In this paper, we propose anenhancement of the DCNN approach to online HCCR by incorporating a variety ofdomain-specific knowledge, including deformation, non-linear normalization,imaginary strokes, path signature, and 8-directional features. Our contributionis twofold. First, these domain-specific technologies are investigated andintegrated with a DCNN to form a composite network to achieve improvedperformance. Second, the resulting DCNNs with diversity in their domainknowledge are combined using a hybrid serial-parallel (HSP) strategy.Consequently, we achieve a promising accuracy of 97.20% and 96.87% onCASIA-OLHWDB1.0 and CASIA-OLHWDB1.1, respectively, outperforming the bestresults previously reported in the literature.
arxiv-11700-140 | A Generative Model of Natural Texture Surrogates | http://arxiv.org/abs/1505.07672 | author:Niklas Ludtke, Debapriya Das, Lucas Theis, Matthias Bethge category:cs.CV published:2015-05-28 summary:Natural images can be viewed as patchworks of different textures, where thelocal image statistics is roughly stationary within a small neighborhood butotherwise varies from region to region. In order to model this variability, wefirst applied the parametric texture algorithm of Portilla and Simoncelli toimage patches of 64X64 pixels in a large database of natural images such thateach image patch is then described by 655 texture parameters which specifycertain statistics, such as variances and covariances of wavelet coefficientsor coefficient magnitudes within that patch. To model the statistics of these texture parameters, we then developedsuitable nonlinear transformations of the parameters that allowed us to fittheir joint statistics with a multivariate Gaussian distribution. We find thatthe first 200 principal components contain more than 99% of the variance andare sufficient to generate textures that are perceptually extremely close tothose generated with all 655 components. We demonstrate the usefulness of themodel in several ways: (1) We sample ensembles of texture patches that can bedirectly compared to samples of patches from the natural image database and canto a high degree reproduce their perceptual appearance. (2) We furtherdeveloped an image compression algorithm which generates surprisingly accurateimages at bit rates as low as 0.14 bits/pixel. Finally, (3) We demonstrate howour approach can be used for an efficient and objective evaluation of samplesgenerated with probabilistic models of natural images.
arxiv-11700-141 | A Practical Guide to Randomized Matrix Computations with MATLAB Implementations | http://arxiv.org/abs/1505.07570 | author:Shusen Wang category:cs.MS cs.LG published:2015-05-28 summary:Matrix operations such as matrix inversion, eigenvalue decomposition,singular value decomposition are ubiquitous in real-world applications.Unfortunately, many of these matrix operations so time and memory expensivethat they are prohibitive when the scale of data is large. In real-worldapplications, since the data themselves are noisy, machine-precision matrixoperations are not necessary at all, and one can sacrifice a reasonable amountof accuracy for computational efficiency. In recent years, a bunch of randomized algorithms have been devised to makematrix computations more scalable. Mahoney (2011) and Woodruff (2014) havewritten excellent but very technical reviews of the randomized algorithms.Differently, the focus of this manuscript is on intuition, algorithmderivation, and implementation. This manuscript should be accessible to peoplewith knowledge in elementary matrix algebra but unfamiliar with randomizedmatrix computations. The algorithms introduced in this manuscript are allsummarized in a user-friendly way, and they can be implemented in lines ofMATLAB code. The readers can easily follow the implementations even if they donot understand the maths and algorithms.
arxiv-11700-142 | A trust-region method for stochastic variational inference with applications to streaming data | http://arxiv.org/abs/1505.07649 | author:Lucas Theis, Matthew D. Hoffman category:stat.ML stat.AP published:2015-05-28 summary:Stochastic variational inference allows for fast posterior inference incomplex Bayesian models. However, the algorithm is prone to local optima whichcan make the quality of the posterior approximation sensitive to the choice ofhyperparameters and initialization. We address this problem by replacing thenatural gradient step of stochastic varitional inference with a trust-regionupdate. We show that this leads to generally better results and reducedsensitivity to hyperparameters. We also describe a new strategy for variationalinference on streaming data and show that here our trust-region method iscrucial for getting good performance.
arxiv-11700-143 | Learning with Symmetric Label Noise: The Importance of Being Unhinged | http://arxiv.org/abs/1505.07634 | author:Brendan van Rooyen, Aditya Krishna Menon, Robert C. Williamson category:cs.LG published:2015-05-28 summary:Convex potential minimisation is the de facto approach to binaryclassification. However, Long and Servedio [2010] proved that under symmetriclabel noise (SLN), minimisation of any convex potential over a linear functionclass can result in classification performance equivalent to random guessing.This ostensibly shows that convex losses are not SLN-robust. In this paper, wepropose a convex, classification-calibrated loss and prove that it isSLN-robust. The loss avoids the Long and Servedio [2010] result by virtue ofbeing negatively unbounded. The loss is a modification of the hinge loss, whereone does not clamp at zero; hence, we call it the unhinged loss. We show thatthe optimal unhinged solution is equivalent to that of a strongly regularisedSVM, and is the limiting solution for any convex potential; this implies thatstrong l2 regularisation makes most standard learners SLN-robust. Experimentsconfirm the SLN-robustness of the unhinged loss.
arxiv-11700-144 | Like Partying? Your Face Says It All. Predicting the Ambiance of Places with Profile Pictures | http://arxiv.org/abs/1505.07522 | author:Miriam Redi, Daniele Quercia, Lindsay T. Graham, Samuel D. Gosling category:cs.HC cs.CV cs.CY published:2015-05-28 summary:To choose restaurants and coffee shops, people are increasingly relying onsocial-networking sites. In a popular site such as Foursquare or Yelp, a placecomes with descriptions and reviews, and with profile pictures of people whofrequent them. Descriptions and reviews have been widely explored in theresearch area of data mining. By contrast, profile pictures have receivedlittle attention. Previous work showed that people are able to partly guess aplace's ambiance, clientele, and activities not only by observing the placeitself but also by observing the profile pictures of its visitors. Here wefurther that work by determining which visual cues people may have relied uponto make their guesses; showing that a state-of-the-art algorithm could makepredictions more accurately than humans at times; and demonstrating that thevisual cues people relied upon partly differ from those of the algorithm.
arxiv-11700-145 | Visual Search at Pinterest | http://arxiv.org/abs/1505.07647 | author:Yushi Jing, David Liu, Dmitry Kislyuk, Andrew Zhai, Jiajing Xu, Jeff Donahue, Sarah Tavel category:cs.CV published:2015-05-28 summary:We demonstrate that, with the availability of distributed computationplatforms such as Amazon Web Services and open-source tools, it is possible fora small engineering team to build, launch and maintain a cost-effective,large-scale visual search system with widely available tools. We alsodemonstrate, through a comprehensive set of live experiments at Pinterest, thatcontent recommendation powered by visual search improve user engagement. Bysharing our implementation details and the experiences learned from launching acommercial visual search engines from scratch, we hope visual search are morewidely incorporated into today's commercial applications.
arxiv-11700-146 | Inner and Inter Label Propagation: Salient Object Detection in the Wild | http://arxiv.org/abs/1505.07192 | author:Hongyang Li, Huchuan Lu, Zhe Lin, Xiaohui Shen, Brian Price category:cs.CV published:2015-05-27 summary:In this paper, we propose a novel label propagation based method for saliencydetection. A key observation is that saliency in an image can be estimated bypropagating the labels extracted from the most certain background and objectregions. For most natural images, some boundary superpixels serve as thebackground labels and the saliency of other superpixels are determined byranking their similarities to the boundary labels based on an inner propagationscheme. For images of complex scenes, we further deploy a 3-cue-center-biasedobjectness measure to pick out and propagate foreground labels. Aco-transduction algorithm is devised to fuse both boundary and objectnesslabels based on an inter propagation scheme. The compactness criterion decideswhether the incorporation of objectness labels is necessary, thus greatlyenhancing computational efficiency. Results on five benchmark datasets withpixel-wise accurate annotations show that the proposed method achieves superiorperformance compared with the newest state-of-the-arts in terms of differentevaluation metrics.
arxiv-11700-147 | New characterizations of minimum spanning trees and of saliency maps based on quasi-flat zones | http://arxiv.org/abs/1505.07203 | author:Jean Cousty, Laurent Najman, Yukiko Kenmochi, Silvio GuimarÃ£es category:cs.CV cs.DS published:2015-05-27 summary:We study three representations of hierarchies of partitions: dendrograms(direct representations), saliency maps, and minimum spanning trees. We providea new bijection between saliency maps and hierarchies based on quasi-flat zonesas used in image processing and characterize saliency maps and minimum spanningtrees as solutions to constrained minimization problems where the constraint isquasi-flat zones preservation. In practice, these results form a toolkit fornew hierarchical methods where one can choose the most convenientrepresentation. They also invite us to process non-image data withmorphological hierarchies.
arxiv-11700-148 | Unsupervised Cross-Domain Word Representation Learning | http://arxiv.org/abs/1505.07184 | author:Danushka Bollegala, Takanori Maehara, Ken-ichi Kawarabayashi category:cs.CL published:2015-05-27 summary:Meaning of a word varies from one domain to another. Despite this importantdomain dependence in word semantics, existing word representation learningmethods are bound to a single domain. Given a pair of\emph{source}-\emph{target} domains, we propose an unsupervised method forlearning domain-specific word representations that accurately capture thedomain-specific aspects of word semantics. First, we select a subset offrequent words that occur in both domains as \emph{pivots}. Next, we optimizean objective function that enforces two constraints: (a) for both source andtarget domain documents, pivots that appear in a document must accuratelypredict the co-occurring non-pivots, and (b) word representations learnt forpivots must be similar in the two domains. Moreover, we propose a method toperform domain adaptation using the learnt word representations. Our proposedmethod significantly outperforms competitive baselines including thestate-of-the-art domain-insensitive word representations, and reports bestsentiment classification accuracies for all domain-pairs in a benchmarkdataset.
arxiv-11700-149 | SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling | http://arxiv.org/abs/1505.07293 | author:Vijay Badrinarayanan, Ankur Handa, Roberto Cipolla category:cs.CV published:2015-05-27 summary:We propose a novel deep architecture, SegNet, for semantic pixel wise imagelabelling. SegNet has several attractive properties; (i) it only requiresforward evaluation of a fully learnt function to obtain smooth labelpredictions, (ii) with increasing depth, a larger context is considered forpixel labelling which improves accuracy, and (iii) it is easy to visualise theeffect of feature activation(s) in the pixel label space at any depth. SegNetis composed of a stack of encoders followed by a corresponding decoder stackwhich feeds into a soft-max classification layer. The decoders help map lowresolution feature maps at the output of the encoder stack to full input imagesize feature maps. This addresses an important drawback of recent deep learningapproaches which have adopted networks designed for object categorization forpixel wise labelling. These methods lack a mechanism to map deep layer featuremaps to input dimensions. They resort to ad hoc methods to upsample features,e.g. by replication. This results in noisy predictions and also restricts thenumber of pooling layers in order to avoid too much upsampling and thus reducesspatial context. SegNet overcomes these problems by learning to map encoderoutputs to image pixel labels. We test the performance of SegNet on outdoor RGBscenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our resultsshow that SegNet achieves state-of-the-art performance even without use ofadditional cues such as depth, video frames or post-processing with CRF models.
arxiv-11700-150 | Improving Spatial Codification in Semantic Segmentation | http://arxiv.org/abs/1505.07409 | author:Carles Ventura, Xavier Giró-i-Nieto, Verónica Vilaplana, Kevin McGuinness, Ferran Marqués, Noel E. O'Connor category:cs.CV published:2015-05-27 summary:This paper explores novel approaches for improving the spatial codificationfor the pooling of local descriptors to solve the semantic segmentationproblem. We propose to partition the image into three regions for each objectto be described: Figure, Border and Ground. This partition aims at minimizingthe influence of the image context on the object description and vice versa byintroducing an intermediate zone around the object contour. Furthermore, wealso propose a richer visual descriptor of the object by applying a SpatialPyramid over the Figure region. Two novel Spatial Pyramid configurations areexplored: Cartesian-based and crown-based Spatial Pyramids. We test theseapproaches with state-of-the-art techniques and show that they improve theFigure-Ground based pooling in the Pascal VOC 2011 and 2012 semanticsegmentation challenges.
arxiv-11700-151 | Sufficient Forecasting Using Factor Models | http://arxiv.org/abs/1505.07414 | author:Jianqing Fan, Lingzhou Xue, Jiawei Yao category:math.ST stat.ME stat.ML stat.TH published:2015-05-27 summary:We consider forecasting a single time series when there is a large number ofpredictors and a possible nonlinear effect. The dimensionality was firstreduced via a high-dimensional (approximate) factor model implemented by theprincipal component analysis. Using the extracted factors, we develop a novelforecasting method called the sufficient forecasting, which provides a set ofsufficient predictive indices, inferred from high-dimensional predictors, todeliver additional predictive power. The projected principal component analysiswill be employed to enhance the accuracy of inferred factors when asemi-parametric (approximate) factor model is assumed. Our method is alsoapplicable to cross-sectional sufficient regression using extracted factors.The connection between the sufficient forecasting and the deep learningarchitecture is explicitly stated. The sufficient forecasting correctlyestimates projection indices of the underlying factors even in the presence ofa nonparametric forecasting function. The proposed method extends thesufficient dimension reduction to high-dimensional regimes by condensing thecross-sectional information through factor models. We derive asymptoticproperties for the estimate of the central subspace spanned by these projectiondirections as well as the estimates of the sufficient predictive indices. Wefurther show that the natural method of running multiple regression of targeton estimated factors yields a linear estimate that actually falls into thiscentral subspace. Our method and theory allow the number of predictors to belarger than the number of observations. We finally demonstrate that thesufficient forecasting improves upon the linear forecasting in both simulationstudies and an empirical study of forecasting macroeconomic variables.
arxiv-11700-152 | Training a Convolutional Neural Network for Appearance-Invariant Place Recognition | http://arxiv.org/abs/1505.07428 | author:Ruben Gomez-Ojeda, Manuel Lopez-Antequera, Nicolai Petkov, Javier Gonzalez-Jimenez category:cs.CV cs.LG cs.RO published:2015-05-27 summary:Place recognition is one of the most challenging problems in computer vision,and has become a key part in mobile robotics and autonomous drivingapplications for performing loop closure in visual SLAM systems. Moreover, thedifficulty of recognizing a revisited location increases with appearancechanges caused, for instance, by weather or illumination variations, whichhinders the long-term application of such algorithms in real environments. Inthis paper we present a convolutional neural network (CNN), trained for thefirst time with the purpose of recognizing revisited locations under severeappearance changes, which maps images to a low dimensional space whereEuclidean distances represent place dissimilarity. In order for the network tolearn the desired invariances, we train it with triplets of images selectedfrom datasets which present a challenging variability in visual appearance. Thetriplets are selected in such way that two samples are from the same locationand the third one is taken from a different place. We validate our systemthrough extensive experimentation, where we demonstrate better performance thanstate-of-art algorithms in a number of popular datasets.
arxiv-11700-153 | PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization | http://arxiv.org/abs/1505.07427 | author:Alex Kendall, Matthew Grimes, Roberto Cipolla category:cs.CV cs.NE cs.RO published:2015-05-27 summary:We present a robust and real-time monocular six degree of freedomrelocalization system. Our system trains a convolutional neural network toregress the 6-DOF camera pose from a single RGB image in an end-to-end mannerwith no need of additional engineering or graph optimisation. The algorithm canoperate indoors and outdoors in real time, taking 5ms per frame to compute. Itobtains approximately 2m and 6 degree accuracy for large scale outdoor scenesand 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23layer deep convnet, demonstrating that convnets can be used to solvecomplicated out of image plane regression problems. This was made possible byleveraging transfer learning from large scale classification data. We show theconvnet localizes from high level features and is robust to difficult lighting,motion blur and different camera intrinsics where point based SIFT registrationfails. Furthermore we show how the pose feature that is produced generalizes toother scenes allowing us to regress pose with only a few dozen trainingexamples. PoseNet code, dataset and an online demonstration is available on ourproject webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/
arxiv-11700-154 | Texture Synthesis Using Convolutional Neural Networks | http://arxiv.org/abs/1505.07376 | author:Leon A. Gatys, Alexander S. Ecker, Matthias Bethge category:cs.CV cs.NE q-bio.NC published:2015-05-27 summary:Here we introduce a new model of natural textures based on the feature spacesof convolutional neural networks optimised for object recognition. Samples fromthe model are of high perceptual quality demonstrating the generative power ofneural networks trained in a purely discriminative fashion. Within the model,textures are represented by the correlations between feature maps in severallayers of the network. We show that across layers the texture representationsincreasingly capture the statistical properties of natural images while makingobject information more and more explicit. The model provides a new tool togenerate stimuli for neuroscience and might offer insights into the deeprepresentations learned by convolutional neural networks.
arxiv-11700-155 | Unveiling the Political Agenda of the European Parliament Plenary: A Topical Analysis | http://arxiv.org/abs/1505.07302 | author:Derek Greene, James P. Cross category:cs.CL cs.CY published:2015-05-27 summary:This study analyzes political interactions in the European Parliament (EP) byconsidering how the political agenda of the plenary sessions has evolved overtime and the manner in which Members of the European Parliament (MEPs) havereacted to external and internal stimuli when making Parliamentary speeches. Itdoes so by considering the context in which speeches are made, and the contentof those speeches. To detect latent themes in legislative speeches over time,speech content is analyzed using a new dynamic topic modeling method, based ontwo layers of matrix factorization. This method is applied to a new corpus ofall English language legislative speeches in the EP plenary from the period1999-2014. Our findings suggest that the political agenda of the EP has evolvedsignificantly over time, is impacted upon by the committee structure of theParliament, and reacts to exogenous events such as EU Treaty referenda and theemergence of the Euro-crisis have a significant impact on what is beingdiscussed in Parliament.
arxiv-11700-156 | Belief Flows of Robust Online Learning | http://arxiv.org/abs/1505.07067 | author:Pedro A. Ortega, Koby Crammer, Daniel D. Lee category:stat.ML cs.LG published:2015-05-26 summary:This paper introduces a new probabilistic model for online learning whichdynamically incorporates information from stochastic gradients of an arbitraryloss function. Similar to probabilistic filtering, the model maintains aGaussian belief over the optimal weight parameters. Unlike traditional Bayesianupdates, the model incorporates a small number of gradient evaluations atlocations chosen using Thompson sampling, making it computationally tractable.The belief is then transformed via a linear flow field which optimally updatesthe belief distribution using rules derived from information theoreticprinciples. Several versions of the algorithm are shown using differentconstraints on the flow field and compared with conventional online learningalgorithms. Results are given for several classification tasks includinglogistic regression and multilayer neural networks.
arxiv-11700-157 | Using Dimension Reduction to Improve the Classification of High-dimensional Data | http://arxiv.org/abs/1505.06907 | author:Andreas Grünauer, Markus Vincze category:cs.LG cs.CV published:2015-05-26 summary:In this work we show that the classification performance of high-dimensionalstructural MRI data with only a small set of training examples is improved bythe usage of dimension reduction methods. We assessed two different dimensionreduction variants: feature selection by ANOVA F-test and featuretransformation by PCA. On the reduced datasets, we applied common learningalgorithms using 5-fold cross-validation. Training, tuning of thehyperparameters, as well as the performance evaluation of the classifiers wasconducted using two different performance measures: Accuracy, and ReceiverOperating Characteristic curve (AUC). Our hypothesis is supported byexperimental results.
arxiv-11700-158 | An Overview of the Asymptotic Performance of the Family of the FastICA Algorithms | http://arxiv.org/abs/1505.07008 | author:Tianwen Wei category:stat.ML cs.LG published:2015-05-26 summary:This contribution summarizes the results on the asymptotic performance ofseveral variants of the FastICA algorithm. A number of new closed-formexpressions are presented.
arxiv-11700-159 | Some Open Problems in Optimal AdaBoost and Decision Stumps | http://arxiv.org/abs/1505.06999 | author:Joshua Belanich, Luis E. Ortiz category:cs.LG stat.ML published:2015-05-26 summary:The significance of the study of the theoretical and practical properties ofAdaBoost is unquestionable, given its simplicity, wide practical use, andeffectiveness on real-world datasets. Here we present a few open problemsregarding the behavior of "Optimal AdaBoost," a term coined by Rudin,Daubechies, and Schapire in 2004 to label the simple version of the standardAdaBoost algorithm in which the weak learner that AdaBoost uses always outputsthe weak classifier with lowest weighted error among the respective hypothesisclass of weak classifiers implicit in the weak learner. We concentrate on thestandard, "vanilla" version of Optimal AdaBoost for binary classification thatresults from using an exponential-loss upper bound on the misclassificationtraining error. We present two types of open problems. One deals with generalweak hypotheses. The other deals with the particular case of decision stumps,as often and commonly used in practice. Answers to the open problems can haveimmediate significant impact to (1) cementing previously established results onasymptotic convergence properties of Optimal AdaBoost, for finite datasets,which in turn can be the start to any convergence-rate analysis; (2)understanding the weak-hypotheses class of effective decision stumps generatedfrom data, which we have empirically observed to be significantly smaller thanthe typically obtained class, as well as the effect on the weak learner'srunning time and previously established improved bounds on the generalizationperformance of Optimal AdaBoost classifiers; and (3) shedding some light on the"self control" that AdaBoost tends to exhibit in practice.
arxiv-11700-160 | An Empirical Evaluation of Current Convolutional Architectures' Ability to Manage Nuisance Location and Scale Variability | http://arxiv.org/abs/1505.06795 | author:Nikolaos Karianakis, Jingming Dong, Stefano Soatto category:cs.CV cs.LG cs.NE published:2015-05-26 summary:We conduct an empirical study to test the ability of Convolutional NeuralNetworks (CNNs) to reduce the effects of nuisance transformations of the inputdata, such as location, scale and aspect ratio. We isolate factors by adoptinga common convolutional architecture either deployed globally on the image tocompute class posterior distributions, or restricted locally to compute classconditional distributions given location, scale and aspect ratios of boundingboxes determined by proposal heuristics. In theory, averaging the latter shouldyield inferior performance compared to proper marginalization. Yet empiricalevidence suggests the converse, leading us to conclude that - at the currentlevel of complexity of convolutional architectures and scale of the data setsused to train them - CNNs are not very effective at marginalizing nuisancevariability. We also quantify the effects of context on the overallclassification task and its impact on the performance of CNNs, and proposeimproved sampling techniques for heuristic proposal schemes that improveend-to-end performance to state-of-the-art levels. We test our hypothesis on aclassification task using the ImageNet Challenge benchmark and on awide-baseline matching task using the Oxford and Fischer's datasets.
arxiv-11700-161 | Representing Meaning with a Combination of Logical Form and Vectors | http://arxiv.org/abs/1505.06816 | author:I. Beltagy, Stephen Roller, Pengxiang Cheng, Katrin Erk, Raymond J. Mooney category:cs.CL published:2015-05-26 summary:NLP tasks differ in the semantic information they require, and at this timeno single se- mantic representation fulfills all requirements. Logic-basedrepresentations characterize sentence structure, but do not capture the gradedaspect of meaning. Distributional models give graded similarity ratings forwords and phrases, but do not capture sentence structure in the same detail aslogic-based approaches. So it has been argued that the two are complementary.We adopt a hybrid approach that combines logic-based and distributionalsemantics through probabilistic logic inference in Markov Logic Networks(MLNs). In this paper, we focus on the three components of a practical systemintegrating logical and distributional models: 1) Parsing and taskrepresentation is the logic-based part where input problems are represented inprobabilistic logic. This is quite different from representing them in standardfirst-order logic. 2) For knowledge base construction we form weightedinference rules. We integrate and compare distributional information with othersources, notably WordNet and an existing paraphrase collection. In particular,we use our system to evaluate distributional lexical entailment approaches. Weuse a variant of Robinson resolution to determine the necessary inferencerules. More sources can easily be added by mapping them to logical rules; oursystem learns a resource-specific weight that corrects for scaling differencesbetween resources. 3) In discussing probabilistic inference, we show how tosolve the inference problems efficiently. To evaluate our approach, we use thetask of textual entailment (RTE), which can utilize the strengths of bothlogic-based and distributional representations. In particular we focus on theSICK dataset, where we achieve state-of-the-art results.
arxiv-11700-162 | Sequential Dimensionality Reduction for Extracting Localized Features | http://arxiv.org/abs/1505.06957 | author:Gabriella Casalino, Nicolas Gillis category:cs.CV cs.LG cs.NA math.NA stat.ML published:2015-05-26 summary:Linear dimensionality reduction techniques are powerful tools for imageanalysis as they allow the identification of important features in a data set.In particular, nonnegative matrix factorization (NMF) has become very popularas it is able to extract sparse, localized and easily interpretable features byimposing an additive combination of nonnegative basis elements. Nonnegativematrix underapproximation (NMU) is a closely related technique that has theadvantage to identify features sequentially. In this paper, we propose avariant of NMU that is particularly well suited for image analysis as itincorporates the spatial information, that is, it takes into account the factthat neighboring pixels are more likely to be contained in the same features,and favors the extraction of localized features by looking for sparse basiselements. We show that our new approach competes favorably with comparablestate-of-the-art techniques on several facial and hyperspectral image datasets.
arxiv-11700-163 | Approximate Joint Diagonalization and Geometric Mean of Symmetric Positive Definite Matrices | http://arxiv.org/abs/1505.07343 | author:Marco Congedo, Bijan Afsari, Alexandre Barachant, Maher Moakher category:math.DG stat.ML published:2015-05-26 summary:We explore the connection between two problems that have arisen independentlyin the signal processing and related fields: the estimation of the geometricmean of a set of symmetric positive definite (SPD) matrices and theirapproximate joint diagonalization (AJD). Today there is a considerable interestin estimating the geometric mean of a SPD matrix set in the manifold of SPDmatrices endowed with the Fisher information metric. The resulting mean hasseveral important invariance properties and has proven very useful in diverseengineering applications such as biomedical and image data processing. Whilefor two SPD matrices the mean has an algebraic closed form solution, for a setof more than two SPD matrices it can only be estimated by iterative algorithms.However, none of the existing iterative algorithms feature at the same timefast convergence, low computational complexity per iteration and guarantee ofconvergence. For this reason, recently other definitions of geometric meanbased on symmetric divergence measures, such as the Bhattacharyya divergence,have been considered. The resulting means, although possibly useful inpractice, do not satisfy all desirable invariance properties. In this paper weconsider geometric means of co-variance matrices estimated on high-dimensionaltime-series, assuming that the data is generated according to an instantaneousmixing model, which is very common in signal processing. We show that in thesecircumstances we can approximate the Fisher information geometric mean byemploying an efficient AJD algorithm. Our approximation is in general muchcloser to the Fisher information geometric mean as compared to its competitorsand verifies many invariance properties. Furthermore, convergence isguaranteed, the computational complexity is low and the convergence rate isquadratic. The accuracy of this new geometric mean approximation isdemonstrated by means of simulations.
arxiv-11700-164 | Surrogate Functions for Maximizing Precision at the Top | http://arxiv.org/abs/1505.06813 | author:Purushottam Kar, Harikrishna Narasimhan, Prateek Jain category:stat.ML cs.LG published:2015-05-26 summary:The problem of maximizing precision at the top of a ranked list, often dubbedPrecision@k (prec@k), finds relevance in myriad learning applications such asranking, multi-label classification, and learning with severe label imbalance.However, despite its popularity, there exist significant gaps in ourunderstanding of this problem and its associated performance measure. The most notable of these is the lack of a convex upper bounding surrogatefor prec@k. We also lack scalable perceptron and stochastic gradient descentalgorithms for optimizing this performance measure. In this paper we make keycontributions in these directions. At the heart of our results is a family oftruly upper bounding surrogates for prec@k. These surrogates are motivated in aprincipled manner and enjoy attractive properties such as consistency to prec@kunder various natural margin/noise conditions. These surrogates are then used to design a class of novel perceptronalgorithms for optimizing prec@k with provable mistake bounds. We also devisescalable stochastic gradient descent style methods for this problem withprovable convergence bounds. Our proofs rely on novel uniform convergencebounds which require an in-depth analysis of the structural properties ofprec@k and its surrogates. We conclude with experimental results comparing ouralgorithms with state-of-the-art cutting plane and stochastic gradientalgorithms for maximizing prec@k.
arxiv-11700-165 | Fantasy Football Prediction | http://arxiv.org/abs/1505.06918 | author:Roman Lutz category:cs.LG I.2.6 published:2015-05-26 summary:The ubiquity of professional sports and specifically the NFL have lead to anincrease in popularity for Fantasy Football. Users have many tools at theirdisposal: statistics, predictions, rankings of experts and even recommendationsof peers. There are issues with all of these, though. Especially since manypeople pay money to play, the prediction tools should be enhanced as theyprovide unbiased and easy-to-use assistance for users. This paper provides anddiscusses approaches to predict Fantasy Football scores of Quarterbacks withrelatively limited data. In addition to that, it includes several suggestionson how the data could be enhanced to achieve better results. The datasetconsists only of game data from the last six NFL seasons. I used two differentmethods to predict the Fantasy Football scores of NFL players: Support VectorRegression (SVR) and Neural Networks. The results of both are promising giventhe limited data that was used.
arxiv-11700-166 | Accelerating Very Deep Convolutional Networks for Classification and Detection | http://arxiv.org/abs/1505.06798 | author:Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun category:cs.CV cs.LG cs.NE published:2015-05-26 summary:This paper aims to accelerate the test-time computation of convolutionalneural networks (CNNs), especially very deep CNNs that have substantiallyimpacted the computer vision community. Unlike previous methods that aredesigned for approximating linear filters or linear responses, our method takesthe nonlinear units into account. We develop an effective solution to theresulting nonlinear optimization problem without the need of stochasticgradient descent (SGD). More importantly, while previous methods mainly focuson optimizing one or two layers, our nonlinear method enables an asymmetricreconstruction that reduces the rapidly accumulated error when multiple (e.g.,>=10) layers are approximated. For the widely used very deep VGG-16 model, ourmethod achieves a whole-model speedup of 4x with merely a 0.3% increase oftop-5 error in ImageNet classification. Our 4x accelerated VGG-16 model alsoshows a graceful accuracy degradation for object detection when plugged intothe Fast R-CNN detector.
arxiv-11700-167 | Large-scale Machine Learning for Metagenomics Sequence Classification | http://arxiv.org/abs/1505.06915 | author:Kévin Vervier, Pierre Mahé, Maud Tournoud, Jean-Baptiste Veyrieras, Jean-Philippe Vert category:q-bio.QM cs.CE cs.LG q-bio.GN stat.ML published:2015-05-26 summary:Metagenomics characterizes the taxonomic diversity of microbial communitiesby sequencing DNA directly from an environmental sample. One of the mainchallenges in metagenomics data analysis is the binning step, where eachsequenced read is assigned to a taxonomic clade. Due to the large volume ofmetagenomics datasets, binning methods need fast and accurate algorithms thatcan operate with reasonable computing requirements. While standardalignment-based methods provide state-of-the-art performance, compositionalapproaches that assign a taxonomic class to a DNA read based on the k-mers itcontains have the potential to provide faster solutions. In this work, weinvestigate the potential of modern, large-scale machine learningimplementations for taxonomic affectation of next-generation sequencing readsbased on their k-mers profile. We show that machine learning-basedcompositional approaches benefit from increasing the number of fragmentssampled from reference genome to tune their parameters, up to a coverage ofabout 10, and from increasing the k-mer size to about 12. Tuning these modelsinvolves training a machine learning model on about 10 8 samples in 10 7dimensions, which is out of reach of standard soft-wares but can be doneefficiently with modern implementations for large-scale machine learning. Theresulting models are competitive in terms of accuracy with well-establishedalignment tools for problems involving a small to moderate number of candidatespecies, and for reasonable amounts of sequencing errors. We show, however,that compositional approaches are still limited in their ability to deal withproblems involving a greater number of species, and more sensitive tosequencing errors. We finally confirm that compositional approach achievefaster prediction times, with a gain of 3 to 15 times with respect to theBWA-MEM short read mapper, depending on the number of candidate species and thelevel of sequencing noise.
arxiv-11700-168 | Discrete Independent Component Analysis (DICA) with Belief Propagation | http://arxiv.org/abs/1505.06814 | author:Francesco A. N. Palmieri, Amedeo Buonanno category:cs.CV cs.LG stat.ML published:2015-05-26 summary:We apply belief propagation to a Bayesian bipartite graph composed ofdiscrete independent hidden variables and discrete visible variables. Thenetwork is the Discrete counterpart of Independent Component Analysis (DICA)and it is manipulated in a factor graph form for inference and learning. A fullset of simulations is reported for character images from the MNIST dataset. Theresults show that the factorial code implemented by the sources contributes tobuild a good generative model for the data that can be used in variousinference modes.
arxiv-11700-169 | Optimizing Non-decomposable Performance Measures: A Tale of Two Classes | http://arxiv.org/abs/1505.06812 | author:Harikrishna Narasimhan, Purushottam Kar, Prateek Jain category:stat.ML cs.LG published:2015-05-26 summary:Modern classification problems frequently present mild to severe labelimbalance as well as specific requirements on classification characteristics,and require optimizing performance measures that are non-decomposable over thedataset, such as F-measure. Such measures have spurred much interest and posespecific challenges to learning algorithms since their non-additive natureprecludes a direct application of well-studied large scale optimization methodssuch as stochastic gradient descent. In this paper we reveal that for two large families of performance measuresthat can be expressed as functions of true positive/negative rates, it isindeed possible to implement point stochastic updates. The families we considerare concave and pseudo-linear functions of TPR, TNR which cover severalpopularly used performance measures such as F-measure, G-mean and H-mean. Our core contribution is an adaptive linearization scheme for these families,using which we develop optimization techniques that enable truly point-basedstochastic updates. For concave performance measures we propose SPADE, astochastic primal dual solver; for pseudo-linear measures we propose STAMP, astochastic alternate maximization procedure. Both methods have crispconvergence guarantees, demonstrate significant speedups over existing methods- often by an order of magnitude or more, and give similar or more accuratepredictions on test data.
arxiv-11700-170 | Times series averaging from a probabilistic interpretation of time-elastic kernel | http://arxiv.org/abs/1505.06897 | author:Pierre-François Marteau category:cs.LG cs.DS published:2015-05-26 summary:At the light of regularized dynamic time warping kernels, this paperreconsider the concept of time elastic centroid (TEC) for a set of time series.From this perspective, we show first how TEC can easily be addressed as apreimage problem. Unfortunately this preimage problem is ill-posed, may sufferfrom over-fitting especially for long time series and getting a sub-optimalsolution involves heavy computational costs. We then derive two new algorithmsbased on a probabilistic interpretation of kernel alignment matrices thatexpresses in terms of probabilistic distributions over sets of alignment paths.The first algorithm is an iterative agglomerative heuristics inspired from thestate of the art DTW barycenter averaging (DBA) algorithm proposed specificallyfor the Dynamic Time Warping measure. The second proposed algorithm achieves aclassical averaging of the aligned samples but also implements an averaging ofthe time of occurrences of the aligned samples. It exploits a straightforwardprogressive agglomerative heuristics. An experimentation that compares for 45time series datasets classification error rates obtained by first nearneighbors classifiers exploiting a single medoid or centroid estimate torepresent each categories show that: i) centroids based approachessignificantly outperform medoids based approaches, ii) on the consideredexperience, the two proposed algorithms outperform the state of the art DBAalgorithm, and iii) the second proposed algorithm that implements an averagingjointly in the sample space and along the time axes emerges as the mostsignificantly robust time elastic averaging heuristic with an interesting noisereduction capability. Index Terms-Time series averaging Time elastic kernelDynamic Time Warping Time series clustering and classification.
arxiv-11700-171 | MLlib: Machine Learning in Apache Spark | http://arxiv.org/abs/1505.06807 | author:Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, Doris Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh, Matei Zaharia, Ameet Talwalkar category:cs.LG cs.DC cs.MS stat.ML published:2015-05-26 summary:Apache Spark is a popular open-source platform for large-scale dataprocessing that is well-suited for iterative machine learning tasks. In thispaper we present MLlib, Spark's open-source distributed machine learninglibrary. MLlib provides efficient functionality for a wide range of learningsettings and includes several underlying statistical, optimization, and linearalgebra primitives. Shipped with Spark, MLlib supports several languages andprovides a high-level API that leverages Spark's rich ecosystem to simplify thedevelopment of end-to-end machine learning pipelines. MLlib has experienced arapid growth due to its vibrant open-source community of over 140 contributors,and includes extensive documentation to support further growth and to let usersquickly get up to speed.
arxiv-11700-172 | Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts | http://arxiv.org/abs/1505.06973 | author:Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guillaume Lavoué, Thomas Brox, Bjoern Andres category:cs.CV published:2015-05-26 summary:Formulations of the Image Decomposition Problem as a Multicut Problem (MP)w.r.t. a superpixel graph have received considerable attention. In contrast,instances of the MP w.r.t. a pixel grid graph have received little attention,firstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph arehard to solve in practice, and, secondly, due to the lack of long-range termsin the objective function of the MP. We propose a generalization of the MP withlong-range terms (LMP). We design and implement two efficient algorithms(primal feasible heuristics) for the MP and LMP which allow us to studyinstances of both problems w.r.t. the pixel grid graphs of the images in theBSDS-500 benchmark. The decompositions we obtain do not differ significantlyfrom the state of the art, suggesting that the LMP is a competitive formulationof the Image Decomposition Problem. To demonstrate the generality of the LMP,we apply it also to the Mesh Decomposition Problem posed by the Princetonbenchmark, obtaining state-of-the-art decompositions.
arxiv-11700-173 | Deep Ranking for Person Re-identification via Joint Representation Learning | http://arxiv.org/abs/1505.06821 | author:Shi-Zhe Chen, Chun-Chao Guo, Jian-Huang Lai category:cs.CV published:2015-05-26 summary:This paper proposes a novel approach to person re-identification, afundamental task in distributed multi-camera surveillance systems. Although avariety of powerful algorithms have been presented in the past few years, mostof them usually focus on designing hand-crafted features and learning metricseither individually or sequentially. Different from previous works, weformulate a unified deep ranking framework that jointly tackles both of thesekey components to maximize their strengths. We start from the principle thatthe correct match of the probe image should be positioned in the top rankwithin the whole gallery set. An effective learning-to-rank algorithm isproposed to minimize the cost corresponding to the ranking disorders of thegallery. The ranking model is solved with a deep convolutional neural network(CNN) that builds the relation between input image pairs and their similarityscores through joint representation learning directly from raw image pixels.The proposed framework allows us to get rid of feature engineering and does notrely on any assumption. An extensive comparative evaluation is given,demonstrating that our approach significantly outperforms all state-of-the-artapproaches, including both traditional and CNN-based methods on the challengingVIPeR, CUHK-01 and CAVIAR4REID datasets. Additionally, our approach has betterability to generalize across datasets without fine-tuning.
arxiv-11700-174 | Boosting-like Deep Learning For Pedestrian Detection | http://arxiv.org/abs/1505.06800 | author:Lei Wang, Baochang Zhang category:cs.CV cs.LG cs.NE published:2015-05-26 summary:This paper proposes boosting-like deep learning (BDL) framework forpedestrian detection. Due to overtraining on the limited training samples,overfitting is a major problem of deep learning. We incorporate a boosting-liketechnique into deep learning to weigh the training samples, and thus preventovertraining in the iterative process. We theoretically give the details ofderivation of our algorithm, and report the experimental results on open datasets showing that BDL achieves a better stable performance than thestate-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in theaverage miss rate compared with ACF and JointDeep on the largest Caltechbenchmark dataset, respectively.
arxiv-11700-175 | Sketching for Sequential Change-Point Detection | http://arxiv.org/abs/1505.06770 | author:Yang Cao, Andrew Thompson, Meng Wang, Yao Xie category:cs.LG stat.ML published:2015-05-25 summary:We study sequential change-point detection using sketches (linearprojections) of high-dimensional signal vectors, by presenting the sketchingprocedures that are derived based on the generalized likelihood ratiostatistic. We consider both fixed and time-varying projections, and derivetheoretical approximations to two fundamental performance metrics: the averagerun length (ARL) and the expected detection delay (EDD); these approximationsare shown to be highly accurate by numerical simulations. We also characterizethe performance of the procedure when the projection is a Gaussian randomprojection or a sparse 0-1 matrix (in particular, an expander graph). Finally,we demonstrate the good performance of the sketching performance usingsimulation and real-data examples on solar flare detection and failuredetection in power networks.
arxiv-11700-176 | Robust Optimization for Deep Regression | http://arxiv.org/abs/1505.06606 | author:Vasileios Belagiannis, Christian Rupprecht, Gustavo Carneiro, Nassir Navab category:cs.CV published:2015-05-25 summary:Convolutional Neural Networks (ConvNets) have successfully contributed toimprove the accuracy of regression-based methods for computer vision tasks suchas human pose estimation, landmark localization, and object detection. Thenetwork optimization has been usually performed with L2 loss and withoutconsidering the impact of outliers on the training process, where an outlier inthis context is defined by a sample estimation that lies at an abnormaldistance from the other training sample estimations in the objective space. Inthis work, we propose a regression model with ConvNets that achieves robustnessto such outliers by minimizing Tukey's biweight function, an M-estimator robustto outliers, as the loss function for the ConvNet. In addition to the robustloss, we introduce a coarse-to-fine model, which processes input images ofprogressively higher resolutions for improving the accuracy of the regressedvalues. In our experiments, we demonstrate faster convergence and bettergeneralization of our robust loss function for the tasks of human poseestimation and age estimation from face images. We also show that thecombination of the robust loss function with the coarse-to-fine model producescomparable or better results than current state-of-the-art approaches in fourpublicly available human pose estimation datasets.
arxiv-11700-177 | Smooth PARAFAC Decomposition for Tensor Completion | http://arxiv.org/abs/1505.06611 | author:Tatsuya Yokota, Qibin Zhao, Andrzej Cichocki category:cs.CV published:2015-05-25 summary:In recent years, low-rank based tensor completion, which is a higher-orderextension of matrix completion, has received considerable attention. However,the low-rank assumption is not sufficient for the recovery of visual data, suchas color and 3D images, where the ratio of missing data is extremely high. Inthis paper, we consider "smoothness" constraints as well as low-rankapproximations, and propose an efficient algorithm for performing tensorcompletion that is particularly powerful regarding visual data. The proposedmethod admits significant advantages, owing to the integration of smoothPARAFAC decomposition for incomplete tensors and the efficient selection ofmodels in order to minimize the tensor rank. Thus, our proposed method istermed as "smooth PARAFAC tensor completion (SPC)." In order to impose thesmoothness constraints, we employ two strategies, total variation (SPC-TV) andquadratic variation (SPC-QV), and invoke the corresponding algorithms for modellearning. Extensive experimental evaluations on both synthetic and real-worldvisual data illustrate the significant improvements of our method, in terms ofboth prediction performance and efficiency, compared with many state-of-the-arttensor completion methods.
arxiv-11700-178 | VeinPLUS: A Transillumination and Reflection-based Hand Vein Database | http://arxiv.org/abs/1505.06769 | author:Alexander Gruschina category:cs.CV published:2015-05-25 summary:This paper gives a short summary of work related to the creation of adepartment-hosted hand vein database. After the introducing section, specialproperties of the hand vein acquisition are explained, followed by a comparisontable, which shows key differences to existing well-known hand vein databases.At the end, the ROI extraction process is described and sample images and ROIsare presented.
arxiv-11700-179 | Stochastic Annealing for Variational Inference | http://arxiv.org/abs/1505.06723 | author:San Gultekin, Aonan Zhang, John Paisley category:stat.ML published:2015-05-25 summary:We empirically evaluate a stochastic annealing strategy for Bayesianposterior optimization with variational inference. Variational inference is adeterministic approach to approximate posterior inference in Bayesian models inwhich a typically non-convex objective function is locally optimized over theparameters of the approximating distribution. We investigate an annealingmethod for optimizing this objective with the aim of finding a better localoptimal solution and compare with deterministic annealing methods and noannealing. We show that stochastic annealing can provide clear improvement onthe GMM and HMM, while performance on LDA tends to favor deterministicannealing methods.
arxiv-11700-180 | Smooth and iteratively Restore: A simple and fast edge-preserving smoothing model | http://arxiv.org/abs/1505.06702 | author:Philipp Kniefacz, Walter Kropatsch category:cs.CV published:2015-05-25 summary:In image processing, it can be a useful pre-processing step to smooth awaysmall structures, such as noise or unimportant details, while retaining theoverall structure of the image by keeping edges, which separate objects, sharp.Typically this edge-preserving smoothing process is achieved using edge-awarefilters. However such filters may preserve unwanted small structures as well ifthey contain edges. In this work we present a novel framework foredge-preserving smoothing which separates the process into two different steps:First the image is smoothed using a blurring filter and in the second step theimportant edges are restored using a guided edge-aware filter. The presentedmethod proves to deliver very good results, compared to state-of-the-artedge-preserving smoothing filters, especially at removing unwanted smallstructures. Furthermore it is very versatile and can easily be adapted todifferent fields of applications while at the same time being very fast tocompute and therefore well-suited for real time applications.
arxiv-11700-181 | Statistical and Algorithmic Perspectives on Randomized Sketching for Ordinary Least-Squares -- ICML | http://arxiv.org/abs/1505.06659 | author:Garvesh Raskutti, Michael Mahoney category:stat.ML published:2015-05-25 summary:We consider statistical and algorithmic aspects of solving large-scaleleast-squares (LS) problems using randomized sketching algorithms. Priorresults show that, from an \emph{algorithmic perspective}, when using sketchingmatrices constructed from random projections and leverage-score sampling, ifthe number of samples $r$ much smaller than the original sample size $n$, thenthe worst-case (WC) error is the same as solving the original problem, up to avery small relative error. From a \emph{statistical perspective}, one typicallyconsiders the mean-squared error performance of randomized sketchingalgorithms, when data are generated according to a statistical linear model. Inthis paper, we provide a rigorous comparison of both perspectives leading toinsights on how they differ. To do this, we first develop a framework forassessing, in a unified manner, algorithmic and statistical aspects ofrandomized sketching methods. We then consider the statistical predictionefficiency (PE) and the statistical residual efficiency (RE) of the sketched LSestimator; and we use our framework to provide upper bounds for several typesof random projection and random sampling algorithms. Among other results, weshow that the RE can be upper bounded when $r$ is much smaller than $n$, whilethe PE typically requires the number of samples $r$ to be substantially larger.Lower bounds developed in subsequent work show that our upper bounds on PE cannot be improved.
arxiv-11700-182 | Recognition Confidence Analysis of Handwritten Chinese Character with CNN | http://arxiv.org/abs/1505.06623 | author:Meijun He, Shuye Zhang, Huiyun Mao, Lianwen Jin category:cs.CV published:2015-05-25 summary:In this paper, we present an effective method to analyze the recognitionconfidence of handwritten Chinese character, based on the softmax regressionscore of a high performance convolutional neural networks (CNN). Throughcareful and thorough statistics of 827,685 testing samples that randomlyselected from total 8836 different classes of Chinese characters, we find thatthe confidence measurement based on CNN is an useful metric to know howreliable the recognition results are. Furthermore, we find by experiments thatthe recognition confidence can be used to find out similar and confusablecharacter-pairs, to check wrongly or cursively written samples, and even todiscover and correct mis-labelled samples. Many interesting observations andstatistics are given and analyzed in this study.
arxiv-11700-183 | Machine learning based data mining for Milky Way filamentary structures reconstruction | http://arxiv.org/abs/1505.06621 | author:Giuseppe Riccio, Stefano Cavuoti, Eugenio Schisano, Massimo Brescia, Amata Mercurio, Davide Elia, Milena Benedettini, Stefano Pezzuto, Sergio Molinari, Anna Maria Di Giorgio category:astro-ph.IM cs.CV published:2015-05-25 summary:We present an innovative method called FilExSeC (Filaments Extraction,Selection and Classification), a data mining tool developed to investigate thepossibility to refine and optimize the shape reconstruction of filamentarystructures detected with a consolidated method based on the flux derivativeanalysis, through the column-density maps computed from Herschel infraredGalactic Plane Survey (Hi-GAL) observations of the Galactic plane. The presentmethodology is based on a feature extraction module followed by a machinelearning model (Random Forest) dedicated to select features and to classify thepixels of the input images. From tests on both simulations and realobservations the method appears reliable and robust with respect to thevariability of shape and distribution of filaments. In the cases of highlydefined filament structures, the presented method is able to bridge the gapsamong the detected fragments, thus improving their shape reconstruction. From apreliminary "a posteriori" analysis of derived filament physical parameters,the method appears potentially able to add a sufficient contribution tocomplete and refine the filament reconstruction.
arxiv-11700-184 | Electre Tri-Machine Learning Approach to the Record Linkage Problem | http://arxiv.org/abs/1505.06614 | author:Renato De Leone, Valentina Minnetti category:stat.ML cs.LG published:2015-05-25 summary:In this short paper, the Electre Tri-Machine Learning Method, generally usedto solve ordinal classification problems, is proposed for solving the RecordLinkage problem. Preliminary experimental results show that, using the ElectreTri method, high accuracy can be achieved and more than 99% of the matches andnonmatches were correctly identified by the procedure.
arxiv-11700-185 | Fast Detection of Curved Edges at Low SNR | http://arxiv.org/abs/1505.06600 | author:Nati Ofir, Meirav Galun, Boaz Nadler, Ronen Basri category:cs.CV published:2015-05-25 summary:Detecting edges is a fundamental problem in computer vision with manyapplications, some involving very noisy images. While most edge detectionmethods are fast, they perform well only on relatively clean images. Indeed,edges in such images can be reliably detected using only local filters.Detecting faint edges under high levels of noise cannot be done locally at theindividual pixel level, and requires more sophisticated global processing.Unfortunately, existing methods that achieve this goal are quite slow. In thispaper we develop a novel multiscale method to detect curved edges in noisyimages. While our algorithm searches for edges over a huge set of candidatecurves, it does so in a practical runtime, nearly linear in the total number ofimage pixels. As we demonstrate experimentally, our algorithm is orders ofmagnitude faster than previous methods designed to deal with high noise levels.Nevertheless, it obtains comparable, if not better, edge detection quality on avariety of challenging noisy images.
arxiv-11700-186 | Affine and Regional Dynamic Time Warpng | http://arxiv.org/abs/1505.06531 | author:Tsu-Wei Chen, Meena Abdelmaseeh, Daniel Stashuk category:cs.CV cs.CE cs.LG published:2015-05-25 summary:Pointwise matches between two time series are of great importance in timeseries analysis, and dynamic time warping (DTW) is known to provide generallyreasonable matches. There are situations where time series alignment should beinvariant to scaling and offset in amplitude or where local regions of theconsidered time series should be strongly reflected in pointwise matches. Twodifferent variants of DTW, affine DTW (ADTW) and regional DTW (RDTW), areproposed to handle scaling and offset in amplitude and provide regionalemphasis respectively. Furthermore, ADTW and RDTW can be combined in twodifferent ways to generate alignments that incorporate advantages from bothmethods, where the affine model can be applied either globally to the entiretime series or locally to each region. The proposed alignment methodsoutperform DTW on specific simulated datasets, and one-nearest-neighborclassifiers using their associated difference measures are competitive with thedifference measures associated with state-of-the-art alignment methods on realdatasets.
arxiv-11700-187 | Clustering via Content-Augmented Stochastic Blockmodels | http://arxiv.org/abs/1505.06538 | author:J. Massey Cashore, Xiaoting Zhao, Alexander A. Alemi, Yujia Liu, Peter I. Frazier category:stat.ML cs.LG cs.SI published:2015-05-25 summary:Much of the data being created on the web contains interactions between usersand items. Stochastic blockmodels, and other methods for community detectionand clustering of bipartite graphs, can infer latent user communities andlatent item clusters from this interaction data. These methods, however,typically ignore the items' contents and the information they provide aboutitem clusters, despite the tendency of items in the same latent cluster toshare commonalities in content. We introduce content-augmented stochasticblockmodels (CASB), which use item content together with user-item interactiondata to enhance the user communities and item clusters learned. Comparisons toseveral state-of-the-art benchmark methods, on datasets arising from scientistsinteracting with scientific articles, show that content-augmented stochasticblockmodels provide highly accurate clusters with respect to metricsrepresentative of the underlying community structure.
arxiv-11700-188 | A Simple Yet Effective Improvement to the Bilateral Filter for Image Denoising | http://arxiv.org/abs/1505.06578 | author:Kollipara Rithwik, Kunal Narayan Chaudhury category:cs.CV published:2015-05-25 summary:The bilateral filter has diverse applications in image processing, computervision, and computational photography. In particular, this non-linear filter isquite effective in denoising images corrupted with additive Gaussian noise. Thefilter, however, is known to perform poorly at large noise levels. Severaladaptations of the filter have been proposed in the literature to address thisshortcoming, but often at an added computational cost. In this paper, we reporta simple yet effective modification that improves the denoising performance ofthe bilateral filter at almost no additional cost. We provide visual andquantitative results on standard test images which show that this improvementis significant both visually and in terms of PSNR and SSIM (often as large as 5dB). We also demonstrate how the proposed filtering can be implemented atreduced complexity by adapting a recent idea for fast bilateral filtering.
arxiv-11700-189 | Reply to Garcia et al.: Common mistakes in measuring frequency dependent word characteristics | http://arxiv.org/abs/1505.06750 | author:P. S. Dodds, E. M. Clark, S. Desu, M. R. Frank, A. J. Reagan, J. R. Williams, L. Mitchell, K. D. Harris, I. M. Kloumann, J. P. Bagrow, K. Megerdoomian, M. T. McMahon, B. F. Tivnan, C. M. Danforth category:physics.soc-ph cs.CL published:2015-05-25 summary:We demonstrate that the concerns expressed by Garcia et al. are misplaced,due to (1) a misreading of our findings in [1]; (2) a widespread failure toexamine and present words in support of asserted summary quantities based onword usage frequencies; and (3) a range of misconceptions about word usagefrequency, word rank, and expert-constructed word lists. In particular, we showthat the English component of our study compares well statistically with tworelated surveys, that no survey design influence is apparent, and thatestimates of measurement error do not explain the positivity biases reported inour work and that of others. We further demonstrate that for the frequencydependence of positivity---of which we explored the nuances in great detail in[1]---Garcia et al. did not perform a reanalysis of our data---they insteadcarried out an analysis of a different, statistically improper data set andintroduced a nonlinearity before performing linear regression.
arxiv-11700-190 | Expresso : A user-friendly GUI for Designing, Training and Exploring Convolutional Neural Networks | http://arxiv.org/abs/1505.06605 | author:Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu category:cs.CV cs.NE published:2015-05-25 summary:With a view to provide a user-friendly interface for designing, training anddeveloping deep learning frameworks, we have developed Expresso, a GUI toolwritten in Python. Expresso is built atop Caffe, the open-source, prize-winningframework popularly used to develop Convolutional Neural Networks. Expressoprovides a convenient wizard-like graphical interface which guides the userthrough various common scenarios -- data import, construction and training ofdeep networks, performing various experiments, analyzing and visualizing theresults of these experiments. The multi-threaded nature of Expresso enablesconcurrent execution and notification of events related to the aforementionedscenarios. The GUI sub-components and inter-component interfaces in Expressohave been designed with extensibility in mind. We believe Expresso'sflexibility and ease of use will come in handy to researchers, newcomers andseasoned alike, in their explorations related to deep learning.
arxiv-11700-191 | Differentially Private Distributed Online Learning | http://arxiv.org/abs/1505.06556 | author:Chencheng Li, Pan Zhou category:cs.LG published:2015-05-25 summary:Online learning has been in the spotlight from the machine learning societyfor a long time. To handle massive data in Big Data era, one single learnercould never efficiently finish this heavy task. Hence, in this paper, wepropose a novel distributed online learning algorithm to solve the problem.Comparing to typical centralized online learner, the distributed learnersoptimize their own learning parameters based on local data sources and timelycommunicate with neighbors. However, communication may lead to a privacybreach. Thus, we use differential privacy to preserve the privacy of learners,and study the influence of guaranteeing differential privacy on the utility ofthe distributed online learning algorithm. Furthermore, by using the resultsfrom Kakade and Tewari (2009), we use the regret bounds of online learning toachieve fast convergence rates for offline learning algorithms in distributedscenarios, which provides tighter utility performance than the existingstate-of-the-art results. In simulation, we demonstrate that the differentiallyprivate offline learning algorithm has high variance, but we can use mini-batchto improve the performance. Finally, the simulations show that the analyticalresults of our proposed theorems are right and our private distributed onlinelearning algorithm is a general framework.
arxiv-11700-192 | Deep Speaker Vectors for Semi Text-independent Speaker Verification | http://arxiv.org/abs/1505.06427 | author:Lantian Li, Dong Wang, Zhiyong Zhang, Thomas Fang Zheng category:cs.CL cs.LG cs.NE published:2015-05-24 summary:Recent research shows that deep neural networks (DNNs) can be used to extractdeep speaker vectors (d-vectors) that preserve speaker characteristics and canbe used in speaker verification. This new method has been tested ontext-dependent speaker verification tasks, and improvement was reported whencombined with the conventional i-vector method. This paper extends the d-vector approach to semi text-independent speakerverification tasks, i.e., the text of the speech is in a limited set of shortphrases. We explore various settings of the DNN structure used for d-vectorextraction, and present a phone-dependent training which employs the posteriorfeatures obtained from an ASR system. The experimental results show that it ispossible to apply d-vectors on semi text-independent speaker recognition, andthe phone-dependent training improves system performance.
arxiv-11700-193 | Detecting bird sound in unknown acoustic background using crowdsourced training data | http://arxiv.org/abs/1505.06443 | author:Timos Papadopoulos, Stephen Roberts, Kathy Willis category:stat.ML cs.LG cs.SD published:2015-05-24 summary:Biodiversity monitoring using audio recordings is achievable at a trulyglobal scale via large-scale deployment of inexpensive, unattended recordingstations or by large-scale crowdsourcing using recording and speciesrecognition on mobile devices. The ability, however, to reliably identifyvocalising animal species is limited by the fact that acoustic signatures ofinterest in such recordings are typically embedded in a diverse and complexacoustic background. To avoid the problems associated with modelling suchbackgrounds, we build generative models of bird sounds and use the concept ofnovelty detection to screen recordings to detect sections of data which arelikely bird vocalisations. We present detection results against variousacoustic environments and different signal-to-noise ratios. We discuss theissues related to selecting the cost function and setting detection thresholdsin such algorithms. Our methods are designed to be scalable and automaticallyapplicable to arbitrary selections of species depending on the specificgeographic region and time period of deployment.
arxiv-11700-194 | Tight Continuous Relaxation of the Balanced $k$-Cut Problem | http://arxiv.org/abs/1505.06478 | author:Syama Sundar Rangapuram, Pramod Kaushik Mudrakarta, Matthias Hein category:stat.ML cs.LG published:2015-05-24 summary:Spectral Clustering as a relaxation of the normalized/ratio cut has becomeone of the standard graph-based clustering methods. Existing methods for thecomputation of multiple clusters, corresponding to a balanced $k$-cut of thegraph, are either based on greedy techniques or heuristics which have weakconnection to the original motivation of minimizing the normalized cut. In thispaper we propose a new tight continuous relaxation for any balanced $k$-cutproblem and show that a related recently proposed relaxation is in most casesloose leading to poor performance in practice. For the optimization of ourtight continuous relaxation we propose a new algorithm for the difficultsum-of-ratios minimization problem which achieves monotonic descent. Extensivecomparisons show that our method outperforms all existing approaches for ratiocut and other balanced $k$-cut criteria.
arxiv-11700-195 | Constrained 1-Spectral Clustering | http://arxiv.org/abs/1505.06485 | author:Syama Sundar Rangapuram, Matthias Hein category:stat.ML cs.LG published:2015-05-24 summary:An important form of prior information in clustering comes in form ofcannot-link and must-link constraints. We present a generalization of thepopular spectral clustering technique which integrates such constraints.Motivated by the recently proposed $1$-spectral clustering for theunconstrained problem, our method is based on a tight relaxation of theconstrained normalized cut into a continuous optimization problem. Opposite toall other methods which have been suggested for constrained spectralclustering, we can always guarantee to satisfy all constraints. Moreover, oursoft formulation allows to optimize a trade-off between normalized cut and thenumber of violated constraints. An efficient implementation is provided whichscales to large datasets. We outperform consistently all other proposed methodsin the experiments.
arxiv-11700-196 | Efficient Elastic Net Regularization for Sparse Linear Models | http://arxiv.org/abs/1505.06449 | author:Zachary C. Lipton, Charles Elkan category:cs.LG published:2015-05-24 summary:This paper presents an algorithm for efficient training of sparse linearmodels with elastic net regularization. Extending previous work on delayedupdates, the new algorithm applies stochastic gradient updates to non-zerofeatures only, bringing weights current as needed with closed-form updates.Closed-form delayed updates for the $\ell_1$, $\ell_{\infty}$, and rarely used$\ell_2$ regularizers have been described previously. This paper providesclosed-form updates for the popular squared norm $\ell^2_2$ and elastic netregularizers. We provide dynamic programming algorithms that perform each delayed update inconstant time. The new $\ell^2_2$ and elastic net methods handle both fixed andvarying learning rates, and both standard {stochastic gradient descent} (SGD)and {forward backward splitting (FoBoS)}. Experimental results show that on abag-of-words dataset with $260,941$ features, but only $88$ nonzero features onaverage per training example, the dynamic programming method trains a logisticregression classifier with elastic net regularization over $2000$ times fasterthan otherwise.
arxiv-11700-197 | Image Segmentation Using Hierarchical Merge Tree | http://arxiv.org/abs/1505.06389 | author:Ting Liu, Mojtaba Seyedhosseini, Tolga Tasdizen category:cs.CV published:2015-05-24 summary:This paper investigates one of the most fundamental computer vision problems:image segmentation. We propose a supervised hierarchical approach toobject-independent image segmentation. Starting with over-segmentingsuperpixels, we use a tree structure to represent the hierarchy of regionmerging, by which we reduce the problem of segmenting image regions to findinga set of label assignment to tree nodes. We formulate the tree structure as aconstrained conditional model to associate region merging with likelihoodspredicted using an ensemble boundary classifier. Final segmentations can thenbe inferred by finding globally optimal solutions to the model efficiently. Wealso present an iterative training and testing algorithm that generates varioustree structures and combines them to emphasize accurate boundaries bysegmentation accumulation. Experiment results and comparisons with other veryrecent methods on six public data sets demonstrate that our approach achievesthe state-of-the-art region accuracy and is very competitive in imagesegmentation without semantic priors.
arxiv-11700-198 | Domain Adaptation Extreme Learning Machines for Drift Compensation in E-nose Systems | http://arxiv.org/abs/1505.06405 | author:Lei Zhang, David Zhang category:cs.LG published:2015-05-24 summary:This paper addresses an important issue, known as sensor drift that behaves anonlinear dynamic property in electronic nose (E-nose), from the viewpoint ofmachine learning. Traditional methods for drift compensation are laborious andcostly due to the frequent acquisition and labeling process for gases samplesrecalibration. Extreme learning machines (ELMs) have been confirmed to beefficient and effective learning techniques for pattern recognition andregression. However, ELMs primarily focus on the supervised, semi-supervisedand unsupervised learning problems in single domain (i.e. source domain). Toour best knowledge, ELM with cross-domain learning capability has never beenstudied. This paper proposes a unified framework, referred to as DomainAdaptation Extreme Learning Machine (DAELM), which learns a robust classifierby leveraging a limited number of labeled data from target domain for driftcompensation as well as gases recognition in E-nose systems, without loss ofthe computational efficiency and learning ability of traditional ELM. In theunified framework, two algorithms called DAELM-S and DAELM-T are proposed forthe purpose of this paper, respectively. In order to percept the differencesamong ELM, DAELM-S and DAELM-T, two remarks are provided. Experiments on thepopular sensor drift data with multiple batches collected by E-nose systemclearly demonstrate that the proposed DAELM significantly outperforms existingdrift compensation methods without cumbersome measures, and also bring newperspectives for ELM.
arxiv-11700-199 | A Fast and Flexible Algorithm for the Graph-Fused Lasso | http://arxiv.org/abs/1505.06475 | author:Wesley Tansey, James G. Scott category:stat.ML stat.CO published:2015-05-24 summary:We propose a new algorithm for solving the graph-fused lasso (GFL), a methodfor parameter estimation that operates under the assumption that the signaltends to be locally constant over a predefined graph structure. Our key insightis to decompose the graph into a set of trails which can then each be solvedefficiently using techniques for the ordinary (1D) fused lasso. We leveragethese trails in a proximal algorithm that alternates between closed form primalupdates and fast dual trail updates. The resulting techinque is both fasterthan previous GFL methods and more flexible in the choice of loss function andgraph structure. Furthermore, we present two algorithms for constructing trailsets and show empirically that they offer a tradeoff between preprocessing timeand convergence rate.
arxiv-11700-200 | Monotonic Calibrated Interpolated Look-Up Tables | http://arxiv.org/abs/1505.06378 | author:Maya Gupta, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojtek Moczydlowski, Alex van Esbroeck category:cs.LG published:2015-05-23 summary:Real-world machine learning applications may require functions that arefast-to-evaluate and interpretable. In particular, guaranteed monotonicity ofthe learned function can be critical to user trust. We propose meeting thesegoals for low-dimensional machine learning problems by learning flexible,monotonic functions using calibrated interpolated look-up tables. We extend thestructural risk minimization framework of lattice regression to train monotoniclook-up tables by solving a convex problem with appropriate linear inequalityconstraints. In addition, we propose jointly learning interpretablecalibrations of each feature to normalize continuous features and handlecategorical or missing data, at the cost of making the objective non-convex. Weaddress large-scale learning through parallelization, mini-batching, andpropose random sampling of additive regularizer terms. Case studies withreal-world problems with five to sixteen features and thousands to millions oftraining samples demonstrate the proposed monotonic functions can achievestate-of-the-art accuracy on practical problems while providing greatertransparency to users.
arxiv-11700-201 | Exposing ambiguities in a relation-extraction gold standard with crowdsourcing | http://arxiv.org/abs/1505.06256 | author:Tong Shu Li, Benjamin M. Good, Andrew I. Su category:cs.CL q-bio.QM published:2015-05-23 summary:Semantic relation extraction is one of the frontiers of biomedical naturallanguage processing research. Gold standards are key tools for advancing thisresearch. It is challenging to generate these standards because of the highcost of expert time and the difficulty in establishing agreement betweenannotators. We implemented and evaluated a microtask crowdsourcing approachthat can produce a gold standard for extracting drug-disease relations. Theaggregated crowd judgment agreed with expert annotations from a pre-existingcorpus on 43 of 60 sentences tested. The levels of crowd agreement varied in asimilar manner to the levels of agreement among the original expert annotators.This work rein-forces the power of crowdsourcing in the process of assemblinggold standards for relation extraction. Further, it high-lights the importanceof exposing the levels of agreement between human annotators, expert or crowd,in gold standard corpora as these are reproducible signals indicatingambiguities in the data or in the annotation guidelines.
arxiv-11700-202 | Low-Rank Matrix Recovery from Row-and-Column Affine Measurements | http://arxiv.org/abs/1505.06292 | author:Avishai Wagner, Or Zuk category:cs.LG cs.IT math.IT math.ST stat.CO stat.ML stat.TH 15A83 published:2015-05-23 summary:We propose and study a row-and-column affine measurement scheme for low-rankmatrix recovery. Each measurement is a linear combination of elements in onerow or one column of a matrix $X$. This setting arises naturally inapplications from different domains. However, current algorithms developed forstandard matrix recovery problems do not perform well in our case, hence theneed for developing new algorithms and theory for our problem. We propose asimple algorithm for the problem based on Singular Value Decomposition ($SVD$)and least-squares ($LS$), which we term \alg. We prove that (a simplifiedversion of) our algorithm can recover $X$ exactly with the minimum possiblenumber of measurements in the noiseless case. In the general noisy case, weprove performance guarantees on the reconstruction accuracy under the Frobeniusnorm. In simulations, our row-and-column design and \alg algorithm showimproved speed, and comparable and in some cases better accuracy compared tostandard measurements designs and algorithms. Our theoretical and experimentalresults suggest that the proposed row-and-column affine measurements scheme,together with our recovery algorithm, may provide a powerful framework foraffine matrix reconstruction.
arxiv-11700-203 | A Frobenius Model of Information Structure in Categorical Compositional Distributional Semantics | http://arxiv.org/abs/1505.06294 | author:Dimitri Kartsaklis, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT math.RA published:2015-05-23 summary:The categorical compositional distributional model of Coecke, Sadrzadeh andClark provides a linguistically motivated procedure for computing the meaningof a sentence as a function of the distributional meaning of the words therein.The theoretical framework allows for reasoning about compositional aspects oflanguage and offers structural ways of studying the underlying relationships.While the model so far has been applied on the level of syntactic structures, asentence can bring extra information conveyed in utterances via intonationalmeans. In the current paper we extend the framework in order to accommodatethis additional information, using Frobenius algebraic structures canonicallyinduced over the basis of finite-dimensional vector spaces. We detail thetheory, provide truth-theoretic and distributional semantics for meanings ofintonationally-marked utterances, and present justifications and extensiveexamples.
arxiv-11700-204 | The Minimum Spanning Tree of Maximum Entropy | http://arxiv.org/abs/1505.06319 | author:Samuel de Sousa, Walter G. Kropatsch category:cs.CV published:2015-05-23 summary:In computer vision, we have the problem of creating graphs out ofunstructured point-sets, i.e. the data graph. A common approach for thisproblem consists of building a triangulation which might not always lead to thebest solution. Small changes in the location of the points might generategraphs with unstable configurations and the topology of the graph could changesignificantly. After building the data-graph, one could apply Graph Matchingtechniques to register the original point-sets. In this paper, we propose adata graph technique based on the Minimum Spanning Tree of Maximum Entropty(MSTME). We aim at a data graph construction which could be more stable thanthe Delaunay triangulation with respect to small variations in the neighborhoodof points. Our technique aims at creating data graphs which could help thepoint-set registration process. We propose an algorithm with a single freeparameter that weighs the importance between the total weight cost and theentropy of the current spanning tree. We compare our algorithm on a number ofdifferent databases with the Delaunay triangulation.
arxiv-11700-205 | The evolutionary origins of hierarchy | http://arxiv.org/abs/1505.06353 | author:Henok Mengistu, Joost Huizinga, Jean-Baptiste Mouret, Jeff Clune category:cs.NE published:2015-05-23 summary:Hierarchical organization -- the recursive composition of sub-modules -- isubiquitous in biological networks, including neural, metabolic, ecological, andgenetic regulatory networks, and in human-made systems, such as largeorganizations and the Internet. To date, most research on hierarchy in networkshas been limited to quantifying this property. However, an open, importantquestion in evolutionary biology is why hierarchical organization evolves inthe first place. It has recently been shown that modularity evolves because ofthe presence of a cost for network connections. Here we investigate whethersuch connection costs also tend to cause a hierarchical organization of suchmodules. In computational simulations, we find that networks without aconnection cost do not evolve to be hierarchical, even when the task has ahierarchical structure. However, with a connection cost, networks evolve to beboth modular and hierarchical, and these networks exhibit higher overallperformance and evolvability (i.e. faster adaptation to new environments).Additional analyses confirm that hierarchy independently improves adaptabilityafter controlling for modularity. Overall, our results suggest that the sameforce--the cost of connections--promotes the evolution of both hierarchy andmodularity, and that these properties are important drivers of networkperformance and adaptability. In addition to shedding light on the emergence ofhierarchy across the many domains in which it appears, these findings will alsoaccelerate future research into evolving more complex, intelligentcomputational brains in the fields of artificial intelligence and robotics.
arxiv-11700-206 | Text to 3D Scene Generation with Rich Lexical Grounding | http://arxiv.org/abs/1505.06289 | author:Angel Chang, Will Monroe, Manolis Savva, Christopher Potts, Christopher D. Manning category:cs.CL cs.GR published:2015-05-23 summary:The ability to map descriptions of scenes to 3D geometric representations hasmany applications in areas such as art, education, and robotics. However, priorwork on the text to 3D scene generation task has used manually specified objectcategories and language that identifies them. We introduce a dataset of 3Dscenes annotated with natural language descriptions and learn from this datahow to ground textual descriptions to physical objects. Our method successfullygrounds a variety of lexical terms to concrete referents, and we showquantitatively that our method improves 3D scene generation over previous workusing purely rule-based methods. We evaluate the fidelity and plausibility of3D scenes generated with our grounding approach through human judgments. Toease evaluation on this task, we also introduce an automated metric thatstrongly correlates with human judgments.
arxiv-11700-207 | The Benefit of Multitask Representation Learning | http://arxiv.org/abs/1505.06279 | author:Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes category:stat.ML cs.LG published:2015-05-23 summary:We discuss a general method to learn data representations from multipletasks. We provide a justification for this method in both settings of multitasklearning and learning-to-learn. The method is illustrated in detail in thespecial case of linear feature learning. Conditions on the theoreticaladvantage offered by multitask representation learning over independent tasklearning are established. In particular, focusing on the important example ofhalf-space learning, we derive the regime in which multitask representationlearning is beneficial over independent task learning, as a function of thesample size, the number of tasks and the intrinsic data dimensionality. Otherpotential applications of our results include multitask feature learning inreproducing kernel Hilbert spaces and multilayer, deep networks.
arxiv-11700-208 | Instant Learning: Parallel Deep Neural Networks and Convolutional Bootstrapping | http://arxiv.org/abs/1505.05972 | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2015-05-22 summary:Although deep neural networks (DNN) are able to scale with direct advances incomputational power (e.g., memory and processing speed), they are not wellsuited to exploit the recent trends for parallel architectures. In particular,gradient descent is a sequential process and the resulting serial dependenciesmean that DNN training cannot be parallelized effectively. Here, we show that aDNN may be replicated over a massive parallel architecture and used to providea cumulative sampling of local solution space which results in rapid and robustlearning. We introduce a complimentary convolutional bootstrapping approachthat enhances performance of the parallel architecture further. Ourparallelized convolutional bootstrapping DNN out-performs an identicalfully-trained traditional DNN after only a single iteration of training.
arxiv-11700-209 | Direct Variational Perspective Shape from Shading with Cartesian Depth Parametrisation | http://arxiv.org/abs/1505.06163 | author:Yong Chul Ju, Daniel Maurer, Michael Breuß, Andrés Bruhn category:cs.CV published:2015-05-22 summary:Most of today's state-of-the-art methods for perspective shape from shadingare modelled in terms of partial differential equations (PDEs) ofHamilton-Jacobi type. To improve the robustness of such methods w.r.t. noiseand missing data, first approaches have recently been proposed that seek toembed the underlying PDE into a variational framework with data and smoothnessterm. So far, however, such methods either make use of a radial depthparametrisation that makes the regularisation hard to interpret from ageometrical viewpoint or they consider indirect smoothness terms that requireadditional consistency constraints to provide valid solutions. Moreover theminimisation of such frameworks is an intricate task, since the underlyingenergy is typically non-convex. In our paper we address all three of theaforementioned issues. First, we propose a novel variational model thatoperates directly on the Cartesian depth. In this context, we also point out acommon mistake in the derivation of the surface normal. Moreover, we employ adirect second-order regulariser with edge-preservation property. This directregulariser yields by construction valid solutions without requiring additionalconsistency constraints. Finally, we also propose a novel coarse-to-fineminimisation framework based on an alternating explicit scheme. This frameworkallows us to avoid local minima during the minimisation and thus to improve theaccuracy of the reconstruction. Experiments show the good quality of our modelas well as the usefulness of the proposed numerical scheme.
arxiv-11700-210 | Learning Program Embeddings to Propagate Feedback on Student Code | http://arxiv.org/abs/1505.05969 | author:Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, Leonidas Guibas category:cs.LG cs.NE cs.SE published:2015-05-22 summary:Providing feedback, both assessing final work and giving hints to stuckstudents, is difficult for open-ended assignments in massive online classeswhich can range from thousands to millions of students. We introduce a neuralnetwork method to encode programs as a linear mapping from an embeddedprecondition space to an embedded postcondition space and propose an algorithmfor feedback at scale using these linear maps as features. We apply ouralgorithm to assessments from the Code.org Hour of Code and StanfordUniversity's CS1 course, where we propagate human comments on studentassignments to orders of magnitude more submissions.
arxiv-11700-211 | Diffusion Methods for Classification with Pairwise Relationships | http://arxiv.org/abs/1505.06072 | author:Pedro F. Felzenszwalb, Benar F. Svaiter category:cs.AI cs.CV published:2015-05-22 summary:We define two algorithms for propagating information in classificationproblems with pairwise relationships. The algorithms are based on contractionmaps and are related to non-linear diffusion and random walks on graphs. Theapproach is also related to message passing algorithms, including beliefpropagation and mean field methods. The algorithms we describe are guaranteedto converge on graphs with arbitrary topology. Moreover they always converge toa unique fixed point, independent of initialization. We prove that the fixedpoints of the algorithms under consideration define lower-bounds on the energyfunction and the max-marginals of a Markov random field. The theoreticalresults also illustrate a relationship between message passing algorithms andvalue iteration for an infinite horizon Markov decision process. We illustratethe practical application of the algorithms under study with numericalexperiments in image restoration, stereo depth estimation and binaryclassification on a grid.
arxiv-11700-212 | Joint Inference of Groups, Events and Human Roles in Aerial Videos | http://arxiv.org/abs/1505.05957 | author:Tianmin Shu, Dan Xie, Brandon Rothrock, Sinisa Todorovic, Song-Chun Zhu category:cs.CV published:2015-05-22 summary:With the advent of drones, aerial video analysis becomes increasinglyimportant; yet, it has received scant attention in the literature. This paperaddresses a new problem of parsing low-resolution aerial videos of largespatial areas, in terms of 1) grouping, 2) recognizing events and 3) assigningroles to people engaged in events. We propose a novel framework aimed atconducting joint inference of the above tasks, as reasoning about each inisolation typically fails in our setting. Given noisy tracklets of people anddetections of large objects and scene surfaces (e.g., building, grass), we usea spatiotemporal AND-OR graph to drive our joint inference, using Markov ChainMonte Carlo and dynamic programming. We also introduce a new formalism ofspatiotemporal templates characterizing latent sub-events. For evaluation, wehave collected and released a new aerial videos dataset using a hex-rotorflying over picnic areas rich with group events. Our results demonstrate thatwe successfully address above inference tasks under challenging conditions.
arxiv-11700-213 | Design and Implementation of Real-time Algorithms for Eye Tracking and PERCLOS Measurement for on board Estimation of Alertness of Drivers | http://arxiv.org/abs/1505.06162 | author:Anjith George, Aurobinda Routray category:cs.CV published:2015-05-22 summary:The alertness level of drivers can be estimated with the use of computervision based methods. The level of fatigue can be found from the value ofPERCLOS. It is the ratio of closed eye frames to the total frames processed.The main objective of the thesis is the design and implementation of real-timealgorithms for measurement of PERCLOS. In this work we have developed areal-time system which is able to process the video onboard and to alarm thedriver in case the driver is in alert. For accurate estimation of PERCLOS theframe rate should be greater than 4 and accuracy should be greater than 90%.For eye detection we have used mainly two approaches Haar classifier basedmethod and Principal Component Analysis (PCA) based method for day time. Duringnight time active Near Infra Red (NIR) illumination is used. Local BinaryPattern (LBP) histogram based method is used for the detection of eyes at nighttime. The accuracy rate of the algorithms was found to be more than 90% atframe rates more than 5 fps which was suitable for the application.
arxiv-11700-214 | Learning Dynamic Feature Selection for Fast Sequential Prediction | http://arxiv.org/abs/1505.06169 | author:Emma Strubell, Luke Vilnis, Kate Silverstein, Andrew McCallum category:cs.CL cs.LG published:2015-05-22 summary:We present paired learning and inference algorithms for significantlyreducing computation and increasing speed of the vector dot products in theclassifiers that are at the heart of many NLP components. This is accomplishedby partitioning the features into a sequence of templates which are orderedsuch that high confidence can often be reached using only a small fraction ofall features. Parameter estimation is arranged to maximize accuracy and earlyconfidence in this sequence. Our approach is simpler and better suited to NLPthan other related cascade methods. We present experiments in left-to-rightpart-of-speech tagging, named entity recognition, and transition-baseddependency parsing. On the typical benchmarking datasets we can preserve POStagging accuracy above 97% and parsing LAS above 88.5% both with over afive-fold reduction in run-time, and NER F1 above 88 with more than 2x increasein speed.
arxiv-11700-215 | Weakly-Supervised Alignment of Video With Text | http://arxiv.org/abs/1505.06027 | author:Piotr Bojanowski, Rémi Lajugie, Edouard Grave, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid category:cs.CV cs.CL published:2015-05-22 summary:Suppose that we are given a set of videos, along with natural languagedescriptions in the form of multiple sentences (e.g., manual annotations, moviescripts, sport summaries etc.), and that these sentences appear in the sametemporal order as their visual counterparts. We propose in this paper a methodfor aligning the two modalities, i.e., automatically providing a time stamp forevery sentence. Given vectorial features for both video and text, we propose tocast this task as a temporal assignment problem, with an implicit linearmapping between the two feature modalities. We formulate this problem as aninteger quadratic program, and solve its continuous convex relaxation using anefficient conditional gradient algorithm. Several rounding procedures areproposed to construct the final integer solution. After demonstratingsignificant improvements over the state of the art on the related task ofaligning video with symbolic labels [7], we evaluate our method on achallenging dataset of videos with associated textual descriptions [36], usingboth bag-of-words and continuous representations for text.
arxiv-11700-216 | Efficient Large Scale Video Classification | http://arxiv.org/abs/1505.06250 | author:Balakrishnan Varadarajan, George Toderici, Sudheendra Vijayanarasimhan, Apostol Natsev category:cs.CV cs.MM cs.NE published:2015-05-22 summary:Video classification has advanced tremendously over the recent years. A largepart of the improvements in video classification had to do with the work doneby the image classification community and the use of deep convolutionalnetworks (CNNs) which produce competitive results with hand- crafted motionfeatures. These networks were adapted to use video frames in various ways andhave yielded state of the art classification results. We present two methodsthat build on this work, and scale it up to work with millions of videos andhundreds of thousands of classes while maintaining a low computational cost. Inthe context of large scale video processing, training CNNs on video frames isextremely time consuming, due to the large number of frames involved. Wepropose to avoid this problem by training CNNs on either YouTube thumbnails orFlickr images, and then using these networks' outputs as features for otherhigher level classifiers. We discuss the challenges of achieving this andpropose two models for frame-level and video-level classification. The first isa highly efficient mixture of experts while the latter is based on long shortterm memory neural networks. We present results on the Sports-1M video dataset(1 million videos, 487 classes) and on a new dataset which has 12 millionvideos and 150,000 labels.
arxiv-11700-217 | Keyphrase Based Evaluation of Automatic Text Summarization | http://arxiv.org/abs/1505.06228 | author:Fatma Elghannam, Tarek El-Shishtawy category:cs.CL 94AXX published:2015-05-22 summary:The development of methods to deal with the informative contents of the textunits in the matching process is a major challenge in automatic summaryevaluation systems that use fixed n-gram matching. The limitation causesinaccurate matching between units in a peer and reference summaries. Thepresent study introduces a new Keyphrase based Summary Evaluator KpEval forevaluating automatic summaries. The KpEval relies on the keyphrases since theyconvey the most important concepts of a text. In the evaluation process, thekeyphrases are used in their lemma form as the matching text unit. The systemwas applied to evaluate different summaries of Arabic multi-document data setpresented at TAC2011. The results showed that the new evaluation techniquecorrelates well with the known evaluation systems: Rouge1, Rouge2, RougeSU4,and AutoSummENG MeMoG. KpEval has the strongest correlation with AutoSummENGMeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667respectively.
arxiv-11700-218 | Machine Learning for Indoor Localization Using Mobile Phone-Based Sensors | http://arxiv.org/abs/1505.06125 | author:David Mascharka, Eric Manley category:cs.LG cs.NI published:2015-05-22 summary:In this paper we investigate the problem of localizing a mobile device basedon readings from its embedded sensors utilizing machine learning methodologies.We consider a real-world environment, collect a large dataset of 3110datapoints, and examine the performance of a substantial number of machinelearning algorithms in localizing a mobile device. We have found algorithmsthat give a mean error as accurate as 0.76 meters, outperforming other indoorlocalization systems reported in the literature. We also propose a hybridinstance-based approach that results in a speed increase by a factor of tenwith no loss of accuracy in a live deployment over standard instance-basedmethods, allowing for fast and accurate localization. Further, we determine howsmaller datasets collected with less density affect accuracy of localization,important for use in real-world environments. Finally, we demonstrate thatthese approaches are appropriate for real-world deployment by evaluating theirperformance in an online, in-motion experiment.
arxiv-11700-219 | Tunnel Surface 3D Reconstruction from Unoriented Image Sequences | http://arxiv.org/abs/1505.06237 | author:Arnold Bauer, Karlheinz Gutjahr, Gerhard Paar, Heiner Kontrus, Robert Glatzl category:cs.CV published:2015-05-22 summary:The 3D documentation of the tunnel surface during construction requires fastand robust measurement systems. In the solution proposed in this paper, duringtunnel advance a single camera is taking pictures of the tunnel surface fromseveral positions. The recorded images are automatically processed to gain a 3Dtunnel surface model. Image acquisition is realized by thetunneling/advance/driving personnel close to the tunnel face (= the front endof the advance). Based on the following fully automatic analysis/evaluation, adecision on the quality of the outbreak can be made within a few minutes. Thispaper describes the image recording system and conditions as well as thestereo-photogrammetry based workflow for the continuously merged dense 3Dreconstruction of the entire advance region. Geo-reference is realized by meansof signalized targets that are automatically detected in the images. We reporton the results of recent testing under real construction conditions, andconclude with prospects for further development in terms of on-siteperformance.
arxiv-11700-220 | A Bottom-up Approach for Pancreas Segmentation using Cascaded Superpixels and (Deep) Image Patch Labeling | http://arxiv.org/abs/1505.06236 | author:Amal Farag, Le Lu, Holger R. Roth, Jiamin Liu, Evrim Turkbey, Ronald M. Summers category:cs.CV published:2015-05-22 summary:Robust automated organ segmentation is a prerequisite for computer-aideddiagnosis (CAD), quantitative imaging analysis and surgical assistance. Forhigh-variability organs such as the pancreas, previous approaches reportundesirably low accuracies. We present a bottom-up approach for pancreassegmentation in abdominal CT scans that is based on a hierarchy of informationpropagation by classifying image patches at different resolutions; andcascading superpixels. There are four stages: 1) decomposing CT slice images asa set of disjoint boundary-preserving superpixels; 2) computing pancreas classprobability maps via dense patch labeling; 3) classifying superpixels bypooling both intensity and probability features to form empirical statistics incascaded random forest frameworks; and 4) simple connectivity basedpost-processing. The dense image patch labeling are conducted by: efficientrandom forest classifier on image histogram, location and texture features; andmore expensive (but with better specificity) deep convolutional neural networkclassification on larger image windows (with more spatial contexts). Evaluationof the approach is performed on a database of 80 manually segmented CT volumesin six-fold cross-validation (CV). Our achieved results are comparable, orbetter than the state-of-the-art methods (evaluated by"leave-one-patient-out"), with Dice 70.7% and Jaccard 57.9%. The computationalefficiency has been drastically improved in the order of 6~8 minutes, comparingwith others of ~10 hours per case. Finally, we implement a multi-atlas labelfusion (MALF) approach for pancreas segmentation using the same datasets. Undersix-fold CV, our bottom-up segmentation method significantly outperforms itsMALF counterpart: (70.7 +/- 13.0%) versus (52.5 +/- 20.8%) in Dice. Deep CNNpatch labeling confidences offer more numerical stability, reflected by smallerstandard deviations.
arxiv-11700-221 | Greedy Biomarker Discovery in the Genome with Applications to Antimicrobial Resistance | http://arxiv.org/abs/1505.06249 | author:Alexandre Drouin, Sébastien Giguère, Maxime Déraspe, François Laviolette, Mario Marchand, Jacques Corbeil category:q-bio.GN cs.LG stat.ML published:2015-05-22 summary:The Set Covering Machine (SCM) is a greedy learning algorithm that producessparse classifiers. We extend the SCM for datasets that contain a huge numberof features. The whole genetic material of living organisms is an example ofsuch a case, where the number of feature exceeds 10^7. Three human pathogenswere used to evaluate the performance of the SCM at predicting antimicrobialresistance. Our results show that the SCM compares favorably in terms ofsparsity and accuracy against L1 and L2 regularized Support Vector Machines andCART decision trees. Moreover, the SCM was the only algorithm that couldconsider the full feature space. For all other algorithms, the latter had to befiltered as a preprocessing step.
arxiv-11700-222 | Robust Rotation Synchronization via Low-rank and Sparse Matrix Decomposition | http://arxiv.org/abs/1505.06079 | author:Federica Arrigoni, Andrea Fusiello, Beatrice Rossi, Pasqualina Fragneto category:cs.CV published:2015-05-22 summary:This paper deals with the rotation synchronization problem, which arises inglobal registration of 3D point-sets and in structure from motion. The problemis formulated in an unprecedented way as a "low-rank and sparse" matrixdecomposition that handles both outliers and missing data. A minimizationstrategy, dubbed R-GoDec, is also proposed and evaluated experimentally againststate-of-the-art algorithms on simulated and real data. The results show thatR-GoDec is the fastest among the robust algorithms.
arxiv-11700-223 | Watch and Learn: Semi-Supervised Learning of Object Detectors from Videos | http://arxiv.org/abs/1505.05769 | author:Ishan Misra, Abhinav Shrivastava, Martial Hebert category:cs.CV published:2015-05-21 summary:We present a semi-supervised approach that localizes multiple unknown objectinstances in long videos. We start with a handful of labeled boxes anditeratively learn and label hundreds of thousands of object instances. Wepropose criteria for reliable object detection and tracking for constrainingthe semi-supervised learning process and minimizing semantic drift. Ourapproach does not assume exhaustive labeling of each object instance in anysingle frame, or any explicit annotation of negative data. Working in such ageneric setting allow us to tackle multiple object instances in video, many ofwhich are static. In contrast, existing approaches either do not considermultiple object instances per video, or rely heavily on the motion of theobjects present. The experiments demonstrate the effectiveness of our approachby evaluating the automatically labeled data on a variety of metrics likequality, coverage (recall), diversity, and relevance to training an objectdetector.
arxiv-11700-224 | GazeDPM: Early Integration of Gaze Information in Deformable Part Models | http://arxiv.org/abs/1505.05753 | author:Iaroslav Shcherbatyi, Andreas Bulling, Mario Fritz category:cs.CV cs.HC published:2015-05-21 summary:An increasing number of works explore collaborative human-computer systems inwhich human gaze is used to enhance computer vision systems. For objectdetection these efforts were so far restricted to late integration approachesthat have inherent limitations, such as increased precision without increase inrecall. We propose an early integration approach in a deformable part model,which constitutes a joint formulation over gaze and visual data. We show thatour GazeDPM method improves over the state-of-the-art DPM baseline by 4% and arecent method for gaze-supported object detection by 3% on the public POETdataset. Our approach additionally provides introspection of the learnt models,can reveal salient image structures, and allows us to investigate the interplaybetween gaze attracting and repelling areas, the importance of view-specificmodels, as well as viewers' personal biases in gaze patterns. We finally studyimportant practical aspects of our approach, such as the impact of usingsaliency maps instead of real fixations, the impact of the number of fixations,as well as robustness to gaze estimation error.
arxiv-11700-225 | Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret | http://arxiv.org/abs/1505.05798 | author:Haitham Bou Ammar, Rasul Tutunov, Eric Eaton category:cs.LG published:2015-05-21 summary:Lifelong reinforcement learning provides a promising framework for developingversatile agents that can accumulate knowledge over a lifetime of experienceand rapidly learn new tasks by building upon prior knowledge. However, currentlifelong learning methods exhibit non-vanishing regret as the amount ofexperience increases and include limitations that can lead to suboptimal orunsafe control policies. To address these issues, we develop a lifelong policygradient learner that operates in an adversarial set- ting to learn multipletasks online while enforcing safety constraints on the learned policies. Wedemonstrate, for the first time, sublinear regret for lifelong policy search,and validate our algorithm on several benchmark dynamical systems and anapplication to quadrotor control.
arxiv-11700-226 | Translation Memory Retrieval Methods | http://arxiv.org/abs/1505.05841 | author:Michael Bloodgood, Benjamin Strauss category:cs.CL I.2.7 published:2015-05-21 summary:Translation Memory (TM) systems are one of the most widely used translationtechnologies. An important part of TM systems is the matching algorithm thatdetermines what translations get retrieved from the bank of availabletranslations to assist the human translator. Although detailed accounts of thematching algorithms used in commercial systems can't be found in theliterature, it is widely believed that edit distance algorithms are used. Thispaper investigates and evaluates the use of several matching algorithms,including the edit distance algorithm that is believed to be at the heart ofmost modern commercial TM systems. This paper presents results showing how wellvarious matching algorithms correlate with human judgments of helpfulness(collected via crowdsourcing with Amazon's Mechanical Turk). A new algorithmbased on weighted n-gram precision that can be adjusted for translator lengthpreferences consistently returns translations judged to be most helpful bytranslators for multiple domains and language pairs.
arxiv-11700-227 | Graph edit distance : a new binary linear programming formulation | http://arxiv.org/abs/1505.05740 | author:Julien Lerouge, Zeina Abu-Aisheh, Romain Raveaux, Pierre Héroux, Sébastien Adam category:cs.DS cs.CV published:2015-05-21 summary:Graph edit distance (GED) is a powerful and flexible graph matching paradigmthat can be used to address different tasks in structural pattern recognition,machine learning, and data mining. In this paper, some new binary linearprogramming formulations for computing the exact GED between two graphs areproposed. A major strength of the formulations lies in their genericity sincethe GED can be computed between directed or undirected fully attributed graphs(i.e. with attributes on both vertices and edges). Moreover, a relaxation ofthe domain constraints in the formulations provides efficient lower boundapproximations of the GED. A complete experimental study comparing the proposedformulations with 4 state-of-the-art algorithms for exact and approximate graphedit distances is provided. By considering both the quality of the proposedsolution and the efficiency of the algorithms as performance criteria, theresults show that none of the compared methods dominates the others in thePareto sense. As a consequence, faced to a given real-world problem, atrade-off between quality and efficiency has to be chosen w.r.t. theapplication constraints. In this context, this paper provides a guide that canbe used to choose the appropriate method.
arxiv-11700-228 | On the relation between accuracy and fairness in binary classification | http://arxiv.org/abs/1505.05723 | author:Indre Zliobaite category:cs.LG cs.AI published:2015-05-21 summary:Our study revisits the problem of accuracy-fairness tradeoff in binaryclassification. We argue that comparison of non-discriminatory classifiersneeds to account for different rates of positive predictions, otherwiseconclusions about performance may be misleading, because accuracy anddiscrimination of naive baselines on the same dataset vary with different ratesof positive predictions. We provide methodological recommendations for soundcomparison of non-discriminatory classifiers, and present a brief theoreticaland empirical analysis of tradeoffs between accuracy and non-discrimination.
arxiv-11700-229 | A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network | http://arxiv.org/abs/1505.05667 | author:Chenxi Zhu, Xipeng Qiu, Xinchi Chen, Xuanjing Huang category:cs.CL cs.LG cs.NE published:2015-05-21 summary:In this work, we address the problem to model all the nodes (words orphrases) in a dependency tree with the dense representations. We propose arecursive convolutional neural network (RCNN) architecture to capture syntacticand compositional-semantic representations of phrases and words in a dependencytree. Different with the original recursive neural network, we introduce theconvolution and pooling layers, which can model a variety of compositions bythe feature maps and choose the most informative compositions by the poolinglayers. Based on RCNN, we use a discriminative model to re-rank a $k$-best listof candidate dependency parsing trees. The experiments show that RCNN is veryeffective to improve the state-of-the-art dependency parsing on both Englishand Chinese datasets.
arxiv-11700-230 | Inferring Graphs from Cascades: A Sparse Recovery Framework | http://arxiv.org/abs/1505.05663 | author:Jean Pouget-Abadie, Thibaut Horel category:cs.SI cs.LG stat.ML published:2015-05-21 summary:In the Network Inference problem, one seeks to recover the edges of anunknown graph from the observations of cascades propagating over this graph. Inthis paper, we approach this problem from the sparse recovery perspective. Weintroduce a general model of cascades, including the voter model and theindependent cascade model, for which we provide the first algorithm whichrecovers the graph's edges with high probability and $O(s\log m)$ measurementswhere $s$ is the maximum degree of the graph and $m$ is the number of nodes.Furthermore, we show that our algorithm also recovers the edge weights (theparameters of the diffusion process) and is robust in the context ofapproximate sparsity. Finally we prove an almost matching lower bound of$\Omega(s\log\frac{m}{s})$ and validate our approach empirically on syntheticgraphs.
arxiv-11700-231 | Complexity Theoretic Limitations on Learning Halfspaces | http://arxiv.org/abs/1505.05800 | author:Amit Daniely category:cs.CC cs.LG published:2015-05-21 summary:We study the problem of agnostically learning halfspaces which is defined bya fixed but unknown distribution $\mathcal{D}$ on $\mathbb{Q}^n\times \{\pm1\}$. We define $\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D})$ as the least errorof a halfspace classifier for $\mathcal{D}$. A learner who can access$\mathcal{D}$ has to return a hypothesis whose error is small compared to$\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D})$. Using the recently developed method of the author, Linial and Shalev-Shwartzwe prove hardness of learning results under a natural assumption on thecomplexity of refuting random $K$-$\mathrm{XOR}$ formulas. We show that noefficient learning algorithm has non-trivial worst-case performance even underthe guarantees that $\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D}) \le \eta$ forarbitrarily small constant $\eta>0$, and that $\mathcal{D}$ is supported in$\{\pm 1\}^n\times \{\pm 1\}$. Namely, even under these favorable conditionsits error must be $\ge \frac{1}{2}-\frac{1}{n^c}$ for every $c>0$. Inparticular, no efficient algorithm can achieve a constant approximation ratio.Under a stronger version of the assumption (where $K$ can be poly-logarithmicin $n$), we can take $\eta = 2^{-\log^{1-\nu}(n)}$ for arbitrarily small$\nu>0$. Interestingly, this is even stronger than the best known lower bounds(Arora et. al. 1993, Feldamn et. al. 2006, Guruswami and Raghavendra 2006) forthe case that the learner is restricted to return a halfspace classifier (i.e.proper learning).
arxiv-11700-232 | Object Modelling with a Handheld RGB-D Camera | http://arxiv.org/abs/1505.05643 | author:Aitor Aldoma, Johann Prankl, Alexander Svejda, Markus Vincze category:cs.CV published:2015-05-21 summary:This work presents a flexible system to reconstruct 3D models of objectscaptured with an RGB-D sensor. A major advantage of the method is that ourreconstruction pipeline allows the user to acquire a full 3D model of theobject. This is achieved by acquiring several partial 3D models in differentsessions that are automatically merged together to reconstruct a full model. Inaddition, the 3D models acquired by our system can be directly used bystate-of-the-art object instance recognition and object tracking modules,providing object-perception capabilities for different applications, such ashuman-object interaction analysis or robot grasping. The system does not imposeconstraints in the appearance of objects (textured, untextured) nor in themodelling setup (moving camera with static object or a turn-table setup). Theproposed reconstruction system has been used to model a large number of objectsresulting in metrically accurate and visually appealing 3D models.
arxiv-11700-233 | Locally Adaptive Dynamic Networks | http://arxiv.org/abs/1505.05668 | author:Daniele Durante, David B. Dunson category:stat.AP stat.ML published:2015-05-21 summary:Our focus is on realistically modeling and forecasting dynamic networks offace-to-face contacts among individuals. Important aspects of such data thatlead to problems with current methods include the tendency to move betweenperiods of slow and rapid changes and the dynamic heterogeneity in theconnectivity behaviors across nodes. Motivated by this application, we developa novel methodology for Locally Adaptive DYnamic (LADY) network inference. Theproposed model relies on a dynamic latent space representation in which eachsubjects' position evolves over time via a stochastic differential equation.Using a state space representation for these stochastic processes andP\'olya-gamma data augmentation, we develop an efficient MCMC algorithm forposterior inference along with tractable online updating and predictionprocedures for forecasting of future networks. We evaluate performance viasimulation studies, and consider an application to face-to-face contacts amongstudents in a primary school.
arxiv-11700-234 | Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views | http://arxiv.org/abs/1505.05641 | author:Hao Su, Charles R. Qi, Yangyan Li, Leonidas Guibas category:cs.CV published:2015-05-21 summary:Object viewpoint estimation from 2D images is an essential task in computervision. However, two issues hinder its progress: scarcity of training data withviewpoint annotations, and a lack of powerful features. Inspired by the growingavailability of 3D models, we propose a framework to address both issues bycombining render-based image synthesis and CNNs. We believe that 3D models havethe potential in generating a large number of images of high variation, whichcan be well exploited by deep CNN with a high learning capacity. Towards thisgoal, we propose a scalable and overfit-resistant image synthesis pipeline,together with a novel CNN specifically tailored for the viewpoint estimationtask. Experimentally, we show that the viewpoint estimation from our pipelinecan significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.
arxiv-11700-235 | Unsupervised Segmentation of Overlapping Cervical Cell Cytoplasm | http://arxiv.org/abs/1505.05601 | author:S L Happy, Swarnadip Chatterjee, Debdoot Sheet category:cs.CV published:2015-05-21 summary:Overlapping of cervical cells and poor contrast of cell cytoplasm are themajor issues in accurate detection and segmentation of cervical cells. Anunsupervised cell segmentation approach is presented here. Cell clumpsegmentation was carried out using the extended depth of field (EDF) imagecreated from the images of different focal planes. A modified Otsu method withprior class weights is proposed for accurate segmentation of nuclei from thecell clumps. The cell cytoplasm was further segmented from cell clump dependingupon the number of nucleus detected in that cell clump. Level set model wasused for cytoplasm segmentation.
arxiv-11700-236 | The development of an information criterion for Change-Point Analysis | http://arxiv.org/abs/1505.05572 | author:Paul A. Wiggins, Colin H. LaMont category:cs.LG stat.ML published:2015-05-21 summary:Change-point analysis is a flexible and computationally tractable tool forthe analysis of times series data from systems that transition between discretestates and whose observables are corrupted by noise. The change-point algorithmis used to identify the time indices (change points) at which the systemtransitions between these discrete states. We present a unifiedinformation-based approach to testing for the existence of change points. Thisnew approach reconciles two previously disparate approaches to Change-PointAnalysis (frequentist and information-based) for testing transitions betweenstates. The resulting method is statistically principled, parameter and priorfree and widely applicable to a wide range of change-point problems.
arxiv-11700-237 | The IBM 2015 English Conversational Telephone Speech Recognition System | http://arxiv.org/abs/1505.05899 | author:George Saon, Hong-Kwang J. Kuo, Steven Rennie, Michael Picheny category:cs.CL published:2015-05-21 summary:We describe the latest improvements to the IBM English conversationaltelephone speech recognition system. Some of the techniques that were foundbeneficial are: maxout networks with annealed dropout rates; networks with avery large number of outputs trained on 2000 hours of data; joint modeling ofpartially unfolded recurrent neural networks and convolutional nets bycombining the bottleneck and output layers and retraining the resulting model;and lastly, sophisticated language model rescoring with exponential and neuralnetwork LMs. These techniques result in an 8.0% word error rate on theSwitchboard part of the Hub5-2000 evaluation test set which is 23% relativebetter than our previous best published result.
arxiv-11700-238 | Why Regularized Auto-Encoders learn Sparse Representation? | http://arxiv.org/abs/1505.05561 | author:Devansh Arpit, Yingbo Zhou, Hung Ngo, Venu Govindaraju category:stat.ML cs.CV cs.LG published:2015-05-21 summary:Sparse Distributed representation is the key to learning useful features indeep learning algorithms not just because it is an efficient mode of datarepresentation, but more importantly, because it captures the generationprocess of most real world data. Although a number of regularized auto-encoders(AE) enforce sparsity explicitly in their learned representation while othersdon't, there has been little formal analysis on what encourages sparsity inthese models in general. Therefore, our objective here is to formally studythis general problem for regularized auto-encoders. We show the properties ofboth regularization and activation function that play an important role inencouraging sparsity. We provide sufficient conditions on both these criteriaand show that multiple popular models-- eg. De-noising and Contractive autoencoders-- and activations-- eg. Rectified Linear and Sigmoid-- satisfy theseconditions; thus explaining sparsity in their learned representation. Ourtheoretical and empirical analysis together, throws light on the properties ofregularization/activation that are conducive to sparsity, but also bringstogether a number of existing auto-encoder models and activation functionsunder a unified analytical framework thereby yielding deeper insights intounsupervised representation learning.
arxiv-11700-239 | Object-Proposal Evaluation Protocol is 'Gameable' | http://arxiv.org/abs/1505.05836 | author:Neelima Chavali, Harsh Agrawal, Aroma Mahendru, Dhruv Batra category:cs.CV published:2015-05-21 summary:Object proposals have quickly become the de-facto pre-processing step in anumber of vision pipelines (for object detection, object discovery, and othertasks). Their performance is usually evaluated on partially annotated datasets.In this paper, we argue that the choice of using a partially annotated datasetfor evaluation of object proposals is problematic -- as we demonstrate via athought experiment, the evaluation protocol is 'gameable', in the sense thatprogress under this protocol does not necessarily correspond to a "better"category independent object proposal algorithm. To alleviate this problem, we: (1) Introduce a nearly-fully annotated versionof PASCAL VOC dataset, which serves as a test-bed to check if object proposaltechniques are overfitting to a particular list of categories. (2) Perform anexhaustive evaluation of object proposal methods on our introduced nearly-fullyannotated PASCAL dataset and perform cross-dataset generalization experiments;and (3) Introduce a diagnostic experiment to detect the bias capacity in anobject proposal algorithm. This tool circumvents the need to collect a denselyannotated dataset, which can be expensive and cumbersome to collect. Finally,we plan to release an easy-to-use toolbox which combines various publiclyavailable implementations of object proposal algorithms which standardizes theproposal generation and evaluation so that new methods can be added andevaluated on different datasets. We hope that the results presented in thepaper will motivate the community to test the category independence of variousobject proposal methods by carefully choosing the evaluation protocol.
arxiv-11700-240 | Rendering of Eyes for Eye-Shape Registration and Gaze Estimation | http://arxiv.org/abs/1505.05916 | author:Erroll Wood, Tadas Baltrusaitis, Xucong Zhang, Yusuke Sugano, Peter Robinson, Andreas Bulling category:cs.CV published:2015-05-21 summary:Images of the eye are key in several computer vision problems, such as shaperegistration and gaze estimation. Recent large-scale supervised methods forthese problems require time-consuming data collection and manual annotation,which can be unreliable. We propose synthesizing perfectly labelledphoto-realistic training data in a fraction of the time. We used computergraphics techniques to build a collection of dynamic eye-region models fromhead scan geometry. These were randomly posed to synthesize close-up eye imagesfor a wide range of head poses, gaze directions, and illumination conditions.We used our model's controllability to verify the importance of realisticillumination and shape variations in eye-region training data. Finally, wedemonstrate the benefits of our synthesized training data (SynthesEyes) byout-performing state-of-the-art methods for eye-shape registration as well ascross-dataset appearance-based gaze estimation in the wild.
arxiv-11700-241 | Variational Inference with Normalizing Flows | http://arxiv.org/abs/1505.05770 | author:Danilo Jimenez Rezende, Shakir Mohamed category:stat.ML cs.AI cs.LG stat.CO stat.ME published:2015-05-21 summary:The choice of approximate posterior distribution is one of the core problemsin variational inference. Most applications of variational inference employsimple families of posterior approximations in order to allow for efficientinference, focusing on mean-field or other simple structured approximations.This restriction has a significant impact on the quality of inferences madeusing variational methods. We introduce a new approach for specifying flexible,arbitrarily complex and scalable approximate posterior distributions. Ourapproximations are distributions constructed through a normalizing flow,whereby a simple initial density is transformed into a more complex one byapplying a sequence of invertible transformations until a desired level ofcomplexity is attained. We use this view of normalizing flows to developcategories of finite and infinitesimal flows and provide a unified view ofapproaches for constructing rich posterior approximations. We demonstrate thatthe theoretical advantages of having posteriors that better match the trueposterior, combined with the scalability of amortized variational approaches,provides a clear improvement in performance and applicability of variationalinference.
arxiv-11700-242 | Regulating Greed Over Time | http://arxiv.org/abs/1505.05629 | author:Stefano Tracà, Cynthia Rudin category:stat.ML cs.LG published:2015-05-21 summary:In retail, there are predictable yet dramatic time-dependent patterns incustomer behavior, such as periodic changes in the number of visitors, orincreases in visitors just before major holidays (e.g., Christmas). The currentparadigm of multi-armed bandit analysis does not take these known patterns intoaccount, which means that despite the firm theoretical foundation of thesemethods, they are fundamentally flawed when it comes to real applications. Thiswork provides a remedy that takes the time-dependent patterns into account, andwe show how this remedy is implemented in the UCB and {\epsilon}-greedymethods. In the corrected methods, exploitation (greed) is regulated over time,so that more exploitation occurs during higher reward periods, and moreexploration occurs in periods of low reward. In order to understand why regretis reduced with the corrected methods, we present a set of bounds that provideinsight into why we would want to exploit during periods of high reward, anddiscuss the impact on regret. Our proposed methods have excellent performancein experiments, and were inspired by a high-scoring entry in the Explorationand Exploitation 3 contest using data from Yahoo! Front Page. That entryheavily used time-series methods to regulate greed over time, which wassubstantially more effective than other contextual bandit methods.
arxiv-11700-243 | A Multi-scale Multiple Instance Video Description Network | http://arxiv.org/abs/1505.05914 | author:Huijuan Xu, Subhashini Venugopalan, Vasili Ramanishka, Marcus Rohrbach, Kate Saenko category:cs.CV published:2015-05-21 summary:Generating natural language descriptions for in-the-wild videos is achallenging task. Most state-of-the-art methods for solving this problem borrowexisting deep convolutional neural network (CNN) architectures (AlexNet,GoogLeNet) to extract a visual representation of the input video. However,these deep CNN architectures are designed for single-label centered-positionedobject classification. While they generate strong semantic features, they haveno inherent structure allowing them to detect multiple objects of differentsizes and locations in the frame. Our paper tries to solve this problem byintegrating the base CNN into several fully convolutional neural networks(FCNs) to form a multi-scale network that handles multiple receptive fieldsizes in the original image. FCNs, previously applied to image segmentation,can generate class heat-maps efficiently compared to sliding window mechanisms,and can easily handle multiple scales. To further handle the ambiguity overmultiple objects and locations, we incorporate the Multiple Instance Learningmechanism (MIL) to consider objects in different positions and at differentscales simultaneously. We integrate our multi-scale multi-instance architecturewith a sequence-to-sequence recurrent neural network to generate sentencedescriptions based on the visual representation. Ours is the first end-to-endtrainable architecture that is capable of multi-scale region processing.Evaluation on a Youtube video dataset shows the advantage of our approachcompared to the original single-scale whole frame CNN model. Our flexible andefficient architecture can potentially be extended to support other videoprocessing tasks.
arxiv-11700-244 | Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering | http://arxiv.org/abs/1505.05612 | author:Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu category:cs.CV cs.CL cs.LG published:2015-05-21 summary:In this paper, we present the mQA model, which is able to answer questionsabout the content of an image. The answer can be a sentence, a phrase or asingle word. Our model contains four components: a Long Short-Term Memory(LSTM) to extract the question representation, a Convolutional Neural Network(CNN) to extract the visual representation, an LSTM for storing the linguisticcontext in an answer, and a fusing component to combine the information fromthe first three components and generate the answer. We construct a FreestyleMultilingual Image Question Answering (FM-IQA) dataset to train and evaluateour mQA model. It contains over 150,000 images and 310,000 freestyle Chinesequestion-answer pairs and their English translations. The quality of thegenerated answers of our mQA model on this dataset is evaluated by human judgesthrough a Turing Test. Specifically, we mix the answers provided by humans andour model. The human judges need to distinguish our model from the human. Theywill also provide a score (i.e. 0, 1, 2, the larger the better) indicating thequality of the answer. We propose strategies to monitor the quality of thisevaluation process. The experiments show that in 64.7% of cases, the humanjudges cannot distinguish our model from humans. The average score is 1.454(1.918 for human). The details of this work, including the FM-IQA dataset, canbe found on the project page: http://idl.baidu.com/FM-IQA.html
arxiv-11700-245 | Randomized Robust Subspace Recovery for High Dimensional Data Matrices | http://arxiv.org/abs/1505.05901 | author:Mostafa Rahmani, George Atia category:stat.ML cs.CV published:2015-05-21 summary:This paper explores and analyzes two randomized designs for robust PrincipalComponent Analysis (PCA) employing low-dimensional data sketching. In onedesign, a data sketch is constructed using random column sampling followed bylow dimensional embedding, while in the other, sketching is based on randomcolumn and row sampling. Both designs are shown to bring about substantialsavings in complexity and memory requirements for robust subspace learning overconventional approaches that use the full scale data. A characterization of thesample and computational complexity of both designs is derived in the contextof two distinct outlier models, namely, sparse and independent outlier models.The proposed randomized approach can provably recover the correct subspace withcomputational and sample complexity that are almost independent of the size ofthe data. The results of the mathematical analysis are confirmed throughnumerical simulations using both synthetic and real data.
arxiv-11700-246 | A Sparse Gaussian Process Framework for Photometric Redshift Estimation | http://arxiv.org/abs/1505.05489 | author:Ibrahim A. Almosallam, Sam N. Lindsay, Matt J. Jarvis, Stephen J. Roberts category:astro-ph.IM astro-ph.GA cs.CV published:2015-05-20 summary:Accurate photometric redshifts are a lynchpin for many future experiments topin down the cosmological model and for studies of galaxy evolution. In thisstudy, a novel sparse regression framework for photometric redshift estimationis presented. Simulated and real data from SDSS DR12 were used to train andtest the proposed models. We show that approaches which include careful datapreparation and model design offer a significant improvement in comparison withseveral competing machine learning algorithms. Standard implementations of mostregression algorithms have as the objective the minimization of the sum ofsquared errors. For redshift inference, however, this induces a bias in theposterior mean of the output distribution, which can be problematic. In thispaper we directly target minimizing $\Delta z = (z_\textrm{s} -z_\textrm{p})/(1+z_\textrm{s})$ and address the bias problem via adistribution-based weighting scheme, incorporated as part of the optimizationobjective. The results are compared with other machine learning algorithms inthe field such as Artificial Neural Networks (ANN), Gaussian Processes (GPs)and sparse GPs. The proposed framework reaches a mean absolute $\Delta z =0.0026(1+z_\textrm{s})$, over the redshift range of $0 \le z_\textrm{s} \le 2$on the simulated data, and $\Delta z = 0.0178(1+z_\textrm{s})$ over the entireredshift range on the SDSS DR12 survey, outperforming the standard ANNz used inthe literature. We also investigate how the relative size of the training setaffects the photometric redshift accuracy. We find that a training set of\textgreater 30 per cent of total sample size, provides little additionalconstraint on the photometric redshifts, and note that our GP formalismstrongly outperforms ANNz in the sparse data regime for the simulated data set.
arxiv-11700-247 | Visual Understanding via Multi-Feature Shared Learning with Global Consistency | http://arxiv.org/abs/1505.05233 | author:Lei Zhang, David Zhang category:cs.CV cs.LG published:2015-05-20 summary:Image/video data is usually represented with multiple visual features. Fusionof multi-source information for establishing the attributes has been widelyrecognized. Multi-feature visual recognition has recently received muchattention in multimedia applications. This paper studies visual understandingvia a newly proposed l_2-norm based multi-feature shared learning framework,which can simultaneously learn a global label matrix and multiplesub-classifiers with the labeled multi-feature data. Additionally, a groupgraph manifold regularizer composed of the Laplacian and Hessian graph isproposed for better preserving the manifold structure of each feature, suchthat the label prediction power is much improved through the semi-supervisedlearning with global label consistency. For convenience, we call the proposedapproach Global-Label-Consistent Classifier (GLCC). The merits of the proposedmethod include: 1) the manifold structure information of each feature isexploited in learning, resulting in a more faithful classification owing to theglobal label consistency; 2) a group graph manifold regularizer based on theLaplacian and Hessian regularization is constructed; 3) an efficientalternative optimization method is introduced as a fast solver owing to theconvex sub-problems. Experiments on several benchmark visual datasets formultimedia understanding, such as the 17-category Oxford Flower dataset, thechallenging 101-category Caltech dataset, the YouTube & Consumer Videos datasetand the large-scale NUS-WIDE dataset, demonstrate that the proposed approachcompares favorably with the state-of-the-art algorithms. An extensiveexperiment on the deep convolutional activation features also show theeffectiveness of the proposed approach. The code is available onhttp://www.escience.cn/people/lei/index.html
arxiv-11700-248 | Live Video Synopsis for Multiple Cameras | http://arxiv.org/abs/1505.05254 | author:Yedid Hoshen, Shmuel Peleg category:cs.CV published:2015-05-20 summary:Video surveillance cameras generate most of recorded video, and there is farmore recorded video than operators can watch. Much progress has recently beenmade using summarization of recorded video, but such techniques do not havemuch impact on live video surveillance. We assume a camera hierarchy where a Master camera observes thedecision-critical region, and one or more Slave cameras observe regions wherepast activity is important for making the current decision. We propose thatwhen people appear in the live Master camera, the Slave cameras will displaytheir past activities, and the operator could use past information forreal-time decision making. The basic units of our method are action tubes, representing objects andtheir trajectories over time. Our object-based method has advantages over framebased methods, as it can handle multiple people, multiple activities for eachperson, and can address re-identification uncertainty.
arxiv-11700-249 | Knowlege Graph Embedding by Flexible Translation | http://arxiv.org/abs/1505.05253 | author:Jun Feng, Mantong Zhou, Yu Hao, Minlie Huang, Xiaoyan Zhu category:cs.CL published:2015-05-20 summary:Knowledge graph embedding refers to projecting entities and relations inknowledge graph into continuous vector spaces. State-of-the-art methods, suchas TransE, TransH, and TransR build embeddings by treating relation astranslation from head entity to tail entity. However, previous models can notdeal with reflexive/one-to-many/many-to-one/many-to-many relations properly, orlack of scalability and efficiency. Thus, we propose a novel method, flexibletranslation, named TransF, to address the above issues. TransF regards relationas translation between head entity vector and tail entity vector with flexiblemagnitude. To evaluate the proposed model, we conduct link prediction andtriple classification on benchmark datasets. Experimental results show that ourmethod remarkably improve the performance compared with severalstate-of-the-art baselines.
arxiv-11700-250 | Measuring Visibility using Atmospheric Transmission and Digital Surface Model | http://arxiv.org/abs/1505.05286 | author:Jean-Philippe Andreu, Stefan Mayer, Karlheinz Gutjahr, Harald Ganster category:cs.CV published:2015-05-20 summary:Reliable and exact assessment of visibility is essential for safe airtraffic. In order to overcome the drawbacks of the currently subjective reportsfrom human observers, we present an approach to automatically derive visibilitymeasures by means of image processing. It first exploits image based estimationof the atmospheric transmission describing the portion of the light that is notscattered by atmospheric phenomena (e.g., haze, fog, smoke) and reaches thecamera. Once the atmospheric transmission is estimated, a 3D representation ofthe vicinity (digital surface model: DMS) is used to compute depth measurementsfor the haze-free pixels and then derive a global visibility estimation for theairport. Results on foggy images demonstrate the validity of the proposedmethod.
arxiv-11700-251 | Algorithmic Analysis of Edge Ranking and Profiling for MTF Determination of an Imaging System | http://arxiv.org/abs/1505.05338 | author:Poorna Banerjee Dasgupta category:cs.CV published:2015-05-20 summary:Edge detection is one of the most principal techniques for detectingdiscontinuities in the gray levels of image pixels. The Modulation TransferFunction (MTF) is one of the main criteria for assessing imaging quality and isa parameter frequently used for measuring the sharpness of an imaging system.In order to determine the MTF, it is essential to determine the best edge fromthe target image so that an edge profile can be developed and then the linespread function and hence the MTF, can be computed accordingly. For regularimage sizes, the human visual system is adept enough to identify suitable edgesfrom the image. But considering huge image datasets, such as those obtainedfrom satellites, the image size may range in few gigabytes and in such a case,manual inspection of images for determination of the best suitable edge is notplausible and hence, edge profiling tasks have to be automated. This paperpresents a novel, yet simple, algorithm for edge ranking and detection fromimage data-sets for MTF computation, which is ideal for automation onvectorised graphical processing units.
arxiv-11700-252 | Weight Uncertainty in Neural Networks | http://arxiv.org/abs/1505.05424 | author:Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra category:stat.ML cs.LG published:2015-05-20 summary:We introduce a new, efficient, principled and backpropagation-compatiblealgorithm for learning a probability distribution on the weights of a neuralnetwork, called Bayes by Backprop. It regularises the weights by minimising acompression cost, known as the variational free energy or the expected lowerbound on the marginal likelihood. We show that this principled kind ofregularisation yields comparable performance to dropout on MNISTclassification. We then demonstrate how the learnt uncertainty in the weightscan be used to improve generalisation in non-linear regression problems, andhow this weight uncertainty can be used to drive the exploration-exploitationtrade-off in reinforcement learning.
arxiv-11700-253 | Benchmarking KAZE and MCM for Multiclass Classification | http://arxiv.org/abs/1505.05240 | author:Siddharth Srivastava, Prerana Mukherjee, Brejesh Lall category:cs.CV cs.IR published:2015-05-20 summary:In this paper, we propose a novel approach for feature generation byappropriately fusing KAZE and SIFT features. We then use this feature set alongwith Minimal Complexity Machine(MCM) for object classification. We show thatKAZE and SIFT features are complementary. Experimental results indicate that anelementary integration of these techniques can outperform the state-of-the-artapproaches.
arxiv-11700-254 | Multi-scale recognition with DAG-CNNs | http://arxiv.org/abs/1505.05232 | author:Songfan Yang, Deva Ramanan category:cs.CV published:2015-05-20 summary:We explore multi-scale convolutional neural nets (CNNs) for imageclassification. Contemporary approaches extract features from a single outputlayer. By extracting features from multiple layers, one can simultaneouslyreason about high, mid, and low-level features during classification. Theresulting multi-scale architecture can itself be seen as a feed-forward modelthat is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs tolearn a set of multiscale features that can be effectively shared betweencoarse and fine-grained classification tasks. While fine-tuning such modelshelps performance, we show that even "off-the-self" multiscale features performquite well. We present extensive analysis and demonstrate state-of-the-artclassification performance on three standard scene benchmarks (SUN397, MIT67,and Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets,our results reduce the lowest previously-reported error by 23.9% and 9.5%,respectively.
arxiv-11700-255 | Bounds on the Minimax Rate for Estimating a Prior over a VC Class from Independent Learning Tasks | http://arxiv.org/abs/1505.05231 | author:Liu Yang, Steve Hanneke, Jaime Carbonell category:cs.LG published:2015-05-20 summary:We study the optimal rates of convergence for estimating a prior distributionover a VC class from a sequence of independent data sets respectively labeledby independent target functions sampled from the prior. We specifically deriveupper and lower bounds on the optimal rates under a smoothness condition on thecorrect prior, with the number of samples per data set equal the VC dimension.These results have implications for the improvements achievable via transferlearning. We additionally extend this setting to real-valued function, where weestablish consistency of an estimator for the prior, and discuss an additionalapplication to a preference elicitation problem in algorithmic economics.
arxiv-11700-256 | Variable subset selection via GA and information complexity in mixtures of Poisson and negative binomial regression models | http://arxiv.org/abs/1505.05229 | author:T. J. Massaro, H. Bozdogan category:stat.ML stat.ME published:2015-05-20 summary:Count data, for example the number of observed cases of a disease in a city,often arise in the fields of healthcare analytics and epidemiology. In thispaper, we consider performing regression on multivariate data in which ouroutcome is a count. Specifically, we derive log-likelihood functions for finitemixtures of regression models involving counts that come from a Poissondistribution, as well as a negative binomial distribution when the counts aresignificantly overdispersed. Within our proposed modeling framework, we carryout optimal component selection using the information criteria scores AIC, BIC,CAIC, and ICOMP. We demonstrate applications of our approach on simulated data,as well as on a real data set of HIV cases in Tennessee counties from the year2010. Finally, using a genetic algorithm within our framework, we performvariable subset selection to determine the covariates that are most responsiblefor categorizing Tennessee counties. This leads to some interesting insightsinto the traits of counties that have high HIV counts.
arxiv-11700-257 | Image aesthetic evaluation using paralleled deep convolution neural network | http://arxiv.org/abs/1505.05225 | author:Guo Lihua, Li Fudi category:cs.CV cs.MM 68U10 I.4.7 published:2015-05-20 summary:Image aesthetic evaluation has attracted much attention in recent years.Image aesthetic evaluation methods heavily depend on the effective aestheticfeature. Traditional meth-ods always extract hand-crafted features. However,these hand-crafted features are always designed to adapt particu-lar datasets,and extraction of them needs special design. Rather than extractinghand-crafted features, an automati-cally learn of aesthetic features based ondeep convolutional neural network (DCNN) is first adopt in this paper. As weall know, when the training dataset is given, the DCNN architecture with highcomplexity may meet the over-fitting problem. On the other side, the DCNNarchitecture with low complexity would not efficiently extract effectivefeatures. For these reasons, we further propose a paralleled convolutionalneural network (PDCNN) with multi-level structures to automatically adapt tothe training dataset. Experimental results show that our proposed PDCNNarchitecture achieves better performance than other traditional methods.
arxiv-11700-258 | DropSample: A New Training Method to Enhance Deep Convolutional Neural Networks for Large-Scale Unconstrained Handwritten Chinese Character Recognition | http://arxiv.org/abs/1505.05354 | author:Weixin Yang, Lianwen Jin, Dacheng Tao, Zecheng Xie, Ziyong Feng category:cs.CV published:2015-05-20 summary:Inspired by the theory of Leitners learning box from the field of psychology,we propose DropSample, a new method for training deep convolutional neuralnetworks (DCNNs), and apply it to large-scale online handwritten Chinesecharacter recognition (HCCR). According to the principle of DropSample, eachtraining sample is associated with a quota function that is dynamicallyadjusted on the basis of the classification confidence given by the DCNNsoftmax output. After a learning iteration, samples with low confidence willhave a higher probability of being selected as training data in the nextiteration; in contrast, well-trained and well-recognized samples with very highconfidence will have a lower probability of being involved in the next trainingiteration and can be gradually eliminated. As a result, the learning processbecomes more efficient as it progresses. Furthermore, we investigate the use ofdomain-specific knowledge to enhance the performance of DCNN by adding a domainknowledge layer before the traditional CNN. By adopting DropSample togetherwith different types of domain-specific knowledge, the accuracy of HCCR can beimproved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1,and ICDAR 2013 online HCCR competition datasets yield outstanding recognitionrates of 97.33%, 97.06%, and 97.51% respectively, all of which aresignificantly better than the previous best results reported in the literature.
arxiv-11700-259 | Convergence Analysis of Policy Iteration | http://arxiv.org/abs/1505.05216 | author:Ali Heydari category:cs.SY math.OC stat.ML published:2015-05-20 summary:Adaptive optimal control of nonlinear dynamic systems with deterministic andknown dynamics under a known undiscounted infinite-horizon cost function isinvestigated. Policy iteration scheme initiated using a stabilizing initialcontrol is analyzed in solving the problem. The convergence of the iterationsand the optimality of the limit functions, which follows from the establisheduniqueness of the solution to the Bellman equation, are the main results ofthis study. Furthermore, a theoretical comparison between the speed ofconvergence of policy iteration versus value iteration is presented. Finally,the convergence results are extended to the case of multi-step look-aheadpolicy iteration.
arxiv-11700-260 | Kinect Range Sensing: Structured-Light versus Time-of-Flight Kinect | http://arxiv.org/abs/1505.05459 | author:Hamed Sarbolandi, Damien Lefloch, Andreas Kolb category:cs.CV published:2015-05-20 summary:Recently, the new Kinect One has been issued by Microsoft, providing the nextgeneration of real-time range sensing devices based on the Time-of-Flight (ToF)principle. As the first Kinect version was using a structured light approach,one would expect various differences in the characteristics of the range datadelivered by both devices. This paper presents a detailed and in-depthcomparison between both devices. In order to conduct the comparison, we proposea framework of seven different experimental setups, which is a generic basisfor evaluating range cameras such as Kinect. The experiments have been designedwith the goal to capture individual effects of the Kinect devices as isolatedlyas possible and in a way, that they can also be adopted, in order to apply themto any other range sensing device. The overall goal of this paper is to providea solid insight into the pros and cons of either device. Thus, scientists thatare interested in using Kinect range sensing cameras in their specificapplication scenario can directly assess the expected, specific benefits andpotential problem of either device.
arxiv-11700-261 | Learning with a Drifting Target Concept | http://arxiv.org/abs/1505.05215 | author:Steve Hanneke, Varun Kanade, Liu Yang category:cs.LG published:2015-05-20 summary:We study the problem of learning in the presence of a drifting targetconcept. Specifically, we provide bounds on the error rate at a given time,given a learner with access to a history of independent samples labeledaccording to a target concept that can change on each round. One of our maincontributions is a refinement of the best previous results for polynomial-timealgorithms for the space of linear separators under a uniform distribution. Wealso provide general results for an algorithm capable of adapting to a variablerate of drift of the target concept. Some of the results also describe anactive learning variant of this setting, and provide bounds on the number ofqueries for the labels of points in the sequence sufficient to obtain thestated bounds on the error rates.
arxiv-11700-262 | Network driven sampling; a critical threshold for design effects | http://arxiv.org/abs/1505.05461 | author:Karl Rohe category:math.ST stat.ME stat.ML stat.TH published:2015-05-20 summary:Web crawling, snowball sampling, and respondent-driven sampling (RDS) arethree types of network driven sampling techniques that are popular when it isdifficult to contact individuals in the population of interest. This paperstudies network driven sampling as a Markov process on the social network thatis indexed by a tree. Each node in this tree corresponds to an observation andeach edge in the tree corresponds to a referral. Indexing with a tree, insteadof a chain, allows for the sampled units to refer multiple future units intothe sample. In survey sampling, the design effect characterizes the additional varianceinduced by a novel sampling strategy. If the design effect is DE, thenconstructing an estimator from the novel design makes the variance of theestimator DE times greater than it would be under a simple random sample. Undercertain assumptions on the referral tree, the design effect of network drivensampling has a critical threshold that is a function of the referral rate $m$and the clustering structure in the social network, represented by the secondeigenvalue of the Markov transition matrix, $\lambda_2$. If $m <1/\lambda_2^2$, then the design effect is finite (i.e. the standard estimatoris $\sqrt{n}$-consistent). However, if $m > 1/\lambda_2^2$, then the designeffect grows with $n$ (i.e. the standard estimator is no longer$\sqrt{n}$-consistent). Past the critical threshold, the estimator converges atthe slower rate of $\log_m \lambda_2$. The Markov model allows for nodes to beresampled. Under certain conditions, the rate of resampling is not affected bythe critical threshold, so long as $n = o(\sqrt{N})$, where $n$ is the samplesize and $N$ is the population size.
arxiv-11700-263 | A Max-Sum algorithm for training discrete neural networks | http://arxiv.org/abs/1505.05401 | author:Carlo Baldassi, Alfredo Braunstein category:cs.LG cs.NE published:2015-05-20 summary:We present an efficient learning algorithm for the problem of training neuralnetworks with discrete synapses, a well-known hard (NP-complete) discreteoptimization problem. The algorithm is a variant of the so-called Max-Sum (MS)algorithm. In particular, we show how, for bounded integer weights with $q$distinct states and independent concave a priori distribution (e.g. $l_{1}$regularization), the algorithm's time complexity can be made to scale as$O\left(N\log N\right)$ per node update, thus putting it on par withalternative schemes, such as Belief Propagation (BP), without resorting toapproximations. Two special cases are of particular interest: binary synapses$W\in\{-1,1\}$ and ternary synapses $W\in\{-1,0,1\}$ with $l_{0}$regularization. The algorithm we present performs as well as BP on binaryperceptron learning problems, and may be better suited to address the problemon fully-connected two-layer networks, since inherent symmetries in two layernetworks are naturally broken using the MS approach.
arxiv-11700-264 | Fuzzy Least Squares Twin Support Vector Machines | http://arxiv.org/abs/1505.05451 | author:Javad Salimi Sartakhti, Nasser Ghadiri, Homayun Afrabandpey category:cs.AI cs.LG published:2015-05-20 summary:Least Squares Twin Support Vector Machine (LSTSVM) is an extremely efficientand fast version of SVM algorithm for binary classification. LSTSVM combinesthe idea of Least Squares SVM and Twin SVM in which two non-parallelhyperplanes are found by solving two systems of linear equations. Although, thealgorithm is very fast and efficient in many classification tasks, it is unableto cope with two features of real-world problems. First, in many real-worldclassification problems, it is almost impossible to assign data points to asingle class. Second, data points in real-world problems may have differentimportance. In this study, we propose a novel version of LSTSVM based on fuzzyconcepts to deal with these two characteristics of real-world data. Thealgorithm is called Fuzzy LSTSVM (FLSTSVM) which provides more flexibility thanbinary classification of LSTSVM. Two models are proposed for the algorithm. Inthe first model, a fuzzy membership value is assigned to each data point andthe hyperplanes are optimized based on these fuzzy samples. In the second modelwe construct fuzzy hyperplanes to classify data. Finally, we apply our proposedFLSTSVM to an artificial as well as three real-world datasets. Resultsdemonstrate that FLSTSVM obtains better performance than SVM and LSTSVM.
arxiv-11700-265 | Supervised Learning for Dynamical System Learning | http://arxiv.org/abs/1505.05310 | author:Ahmed Hefny, Carlton Downey, Geoffrey Gordon category:stat.ML cs.LG published:2015-05-20 summary:Recently there has been substantial interest in spectral methods for learningdynamical systems. These methods are popular since they often offer a goodtradeoff between computational and statistical efficiency. Unfortunately, theycan be difficult to use and extend in practice: e.g., they can make itdifficult to incorporate prior information such as sparsity or structure. Toaddress this problem, we present a new view of dynamical system learning: weshow how to learn dynamical systems by solving a sequence of ordinarysupervised learning problems, thereby allowing users to incorporate priorknowledge via standard techniques such as L1 regularization. Many existingspectral methods are special cases of this new framework, using linearregression as the supervised learner. We demonstrate the effectiveness of ourframework by showing examples where nonlinear regression or lasso let us learnbetter state representations than plain linear regression does; the correctnessof these instances follows directly from our general analysis.
arxiv-11700-266 | Learning Better Word Embedding by Asymmetric Low-Rank Projection of Knowledge Graph | http://arxiv.org/abs/1505.04891 | author:Fei Tian, Bin Gao, Enhong Chen, Tie-Yan Liu category:cs.CL published:2015-05-19 summary:Word embedding, which refers to low-dimensional dense vector representationsof natural words, has demonstrated its power in many natural languageprocessing tasks. However, it may suffer from the inaccurate and incompleteinformation contained in the free text corpus as training data. To tackle thischallenge, there have been quite a few works that leverage knowledge graphs asan additional information source to improve the quality of word embedding.Although these works have achieved certain success, they have neglected someimportant facts about knowledge graphs: (i) many relationships in knowledgegraphs are \emph{many-to-one}, \emph{one-to-many} or even \emph{many-to-many},rather than simply \emph{one-to-one}; (ii) most head entities and tail entitiesin knowledge graphs come from very different semantic spaces. To address theseissues, in this paper, we propose a new algorithm named ProjectNet. ProjecNetmodels the relationships between head and tail entities after transforming themwith different low-rank projection matrices. The low-rank projection can allownon \emph{one-to-one} relationships between entities, while differentprojection matrices for head and tail entities allow them to originate indifferent semantic spaces. The experimental results demonstrate that ProjectNetyields more accurate word embedding than previous works, thus leads to clearimprovements in various natural language processing tasks.
arxiv-11700-267 | Boosting Named Entity Recognition with Neural Character Embeddings | http://arxiv.org/abs/1505.05008 | author:Cicero Nogueira dos Santos, Victor Guimarães category:cs.CL published:2015-05-19 summary:Most state-of-the-art named entity recognition (NER) systems rely onhandcrafted features and on the output of other NLP tasks such aspart-of-speech (POS) tagging and text chunking. In this work we propose alanguage-independent NER system that uses automatically learned features only.Our approach is based on the CharWNN deep neural network, which uses word-leveland character-level representations (embeddings) to perform sequentialclassification. We perform an extensive number of experiments using twoannotated corpora in two different languages: HAREM I corpus, which containstexts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts inSpanish. Our experimental results shade light on the contribution of neuralcharacter embeddings for NER. Moreover, we demonstrate that the same neuralnetwork which has been successfully applied to POS tagging can also achievestate-of-the-art results for language-independet NER, using the samehyperparameters, and without any handcrafted features. For the HAREM I corpus,CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-scorefor the total scenario (ten NE classes), and by 7.2 points in the F1 for theselective scenario (five NE classes).
arxiv-11700-268 | Multi-Image Matching via Fast Alternating Minimization | http://arxiv.org/abs/1505.04845 | author:Xiaowei Zhou, Menglong Zhu, Kostas Daniilidis category:cs.CV published:2015-05-19 summary:In this paper we propose a global optimization-based approach to jointlymatching a set of images. The estimated correspondences simultaneously maximizepairwise feature affinities and cycle consistency across multiple images.Unlike previous convex methods relying on semidefinite programming, weformulate the problem as a low-rank matrix recovery problem and show that thedesired semidefiniteness of a solution can be spontaneously fulfilled. Thelow-rank formulation enables us to derive a fast alternating minimizationalgorithm in order to handle practical problems with thousands of features.Both simulation and real experiments demonstrate that the proposed algorithmcan achieve a competitive performance with an order of magnitude speedupcompared to the state-of-the-art algorithm. In the end, we demonstrate theapplicability of the proposed method to match the images of different objectinstances and as a result the potential to reconstruct category-specific objectmodels from those images.
arxiv-11700-269 | Modelling-based experiment retrieval: A case study with gene expression clustering | http://arxiv.org/abs/1505.05007 | author:Paul Blomstedt, Ritabrata Dutta, Sohan Seth, Alvis Brazma, Samuel Kaski category:stat.ML cs.IR cs.LG published:2015-05-19 summary:Motivation: Public and private repositories of experimental data are growingto sizes that require dedicated methods for finding relevant data. To improveon the state of the art of keyword searches from annotations, methods forcontent-based retrieval have been proposed. In the context of gene expressionexperiments, most methods retrieve gene expression profiles, requiring eachexperiment to be expressed as a single profile, typically of case vs. control.A more general, recently suggested alternative is to retrieve experiments whosemodels are good for modelling the query dataset. However, for very noisy andhigh-dimensional query data, this retrieval criterion turns out to be verynoisy as well. Results: We propose doing retrieval using a denoised model of the querydataset, instead of the original noisy dataset itself. To this end, weintroduce a general probabilistic framework, where each experiment is modelledseparately and the retrieval is done by finding related models. For retrievalof gene expression experiments, we use a probabilistic model called productpartition model, which induces a clustering of genes that show similarexpression patterns across a number of samples. The suggested metric forretrieval using clusterings is the normalized information distance. Empiricalresults finally suggest that inference for the full probabilistic model can beapproximated with good performance using computationally faster heuristicclustering approaches (e.g. $k$-means). The method is highly scalable andstraightforward to apply to construct a general-purpose gene expressionexperiment retrieval method. Availability: The method can be implemented using standard clusteringalgorithms and normalized information distance, available in many statisticalsoftware packages.
arxiv-11700-270 | An Experimental Comparison of Hybrid Algorithms for Bayesian Network Structure Learning | http://arxiv.org/abs/1505.05004 | author:Maxime Gasse, Alex Aussem, Haytham Elghazel category:stat.ML cs.AI cs.LG published:2015-05-19 summary:We present a novel hybrid algorithm for Bayesian network structure learning,called Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesiannetwork and then performs a Bayesian-scoring greedy hill-climbing search toorient the edges. It is based on a subroutine called HPC, that combines ideasfrom incremental and divide-and-conquer constraint-based methods to learn theparents and children of a target variable. We conduct an experimentalcomparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently themost powerful state-of-the-art algorithm for Bayesian network structurelearning, on several benchmarks with various data sizes. Our extensiveexperiments show that H2PC outperforms MMHC both in terms of goodness of fit tonew data and in terms of the quality of the network structure itself, which iscloser to the true dependence structure of the data. The source code (in R) ofH2PC as well as all data sets used for the empirical tests are publiclyavailable.
arxiv-11700-271 | Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems | http://arxiv.org/abs/1505.05114 | author:Yuxin Chen, Emmanuel J. Candes category:cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH published:2015-05-19 summary:We consider the fundamental problem of solving quadratic systems of equationsin $n$ variables, where $y_i = \langle \boldsymbol{a}_i, \boldsymbol{x}\rangle^2$, $i = 1, \ldots, m$ and $\boldsymbol{x} \in \mathbb{R}^n$ isunknown. We propose a novel method, which starting with an initial guesscomputed by means of a spectral method, proceeds by minimizing a nonconvexfunctional as in the Wirtinger flow approach. There are several keydistinguishing features, most notably, a distinct objective functional andnovel update rules, which operate in an adaptive fashion and drop terms bearingtoo much influence on the search direction. These careful selection rulesprovide a tighter initial guess, better descent directions, and thus enhancedpractical performance. On the theoretical side, we prove that for certainunstructured models of quadratic systems, our algorithms return the correctsolution in linear time, i.e. in time proportional to reading the data$\{\boldsymbol{a}_i\}$ and $\{y_i\}$ as soon as the ratio $m/n$ between thenumber of equations and unknowns exceeds a fixed numerical constant. We extendthe theory to deal with noisy systems in which we only have $y_i \approx\langle \boldsymbol{a}_i, \boldsymbol{x} \rangle^2$ and prove that ouralgorithms achieve a statistical accuracy, which is nearly un-improvable. Wecomplement our theoretical study with numerical examples showing that solvingrandom quadratic systems is both computationally and statistically not muchharder than solving linear systems of the same size---hence the title of thispaper. For instance, we demonstrate empirically that the computational cost ofour algorithm is about four times that of solving a least-squares problem ofthe same size.
arxiv-11700-272 | Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models | http://arxiv.org/abs/1505.04870 | author:Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, Svetlana Lazebnik category:cs.CV cs.CL published:2015-05-19 summary:The Flickr30k dataset has become a standard benchmark for sentence-basedimage description. This paper presents Flickr30k Entities, which augments the158k captions from Flickr30k with 244k coreference chains, linking mentions ofthe same entities across different captions for the same image, and associatingthem with 276k manually annotated bounding boxes. Such annotations areessential for continued progress in automatic image description and groundedlanguage understanding. They enable us to define a new benchmark forlocalization of textual entity mentions in an image. We present a strongbaseline for this task that combines an image-text embedding, detectors forcommon objects, a color classifier, and a bias towards selecting largerobjects. While our baseline rivals in accuracy more complex state-of-the-artmodels, we show that its gains cannot be easily parlayed into improvements onsuch tasks as image-sentence retrieval, thus underlining the limitations ofcurrent methods and the need for further research.
arxiv-11700-273 | Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors | http://arxiv.org/abs/1505.04868 | author:Limin Wang, Yu Qiao, Xiaoou Tang category:cs.CV published:2015-05-19 summary:Visual features are of vital importance for human action understanding invideos. This paper presents a new video representation, calledtrajectory-pooled deep-convolutional descriptor (TDD), which shares the meritsof both hand-crafted features and deep-learned features. Specifically, weutilize deep architectures to learn discriminative convolutional feature maps,and conduct trajectory-constrained pooling to aggregate these convolutionalfeatures into effective descriptors. To enhance the robustness of TDDs, wedesign two normalization methods to transform convolutional feature maps,namely spatiotemporal normalization and channel normalization. The advantagesof our features come from (i) TDDs are automatically learned and contain highdiscriminative capacity compared with those hand-crafted features; (ii) TDDstake account of the intrinsic characteristics of temporal dimension andintroduce the strategies of trajectory-constrained sampling and pooling foraggregating deep-learned features. We conduct experiments on two challengingdatasets: HMDB51 and UCF101. Experimental results show that TDDs outperformprevious hand-crafted features and deep-learned features. Our method alsoachieves superior performance to the state of the art on these datasets (HMDB5165.9%, UCF101 91.5%).
arxiv-11700-274 | Barcode Annotations for Medical Image Retrieval: A Preliminary Investigation | http://arxiv.org/abs/1505.05212 | author:Hamid R. Tizhoosh category:cs.CV published:2015-05-19 summary:This paper proposes to generate and to use barcodes to annotate medicalimages and/or their regions of interest such as organs, tumors and tissuetypes. A multitude of efficient feature-based image retrieval methods alreadyexist that can assign a query image to a certain image class. Visualannotations may help to increase the retrieval accuracy if combined withexisting feature-based classification paradigms. Whereas with annotations weusually mean textual descriptions, in this paper barcode annotations areproposed. In particular, Radon barcodes (RBC) are introduced. As well, localbinary patterns (LBP) and local Radon binary patterns (LRBP) are implemented asbarcodes. The IRMA x-ray dataset with 12,677 training images and 1,733 testimages is used to verify how barcodes could facilitate image retrieval.
arxiv-11700-275 | oASIS: Adaptive Column Sampling for Kernel Matrix Approximation | http://arxiv.org/abs/1505.05208 | author:Raajen Patel, Thomas A. Goldstein, Eva L. Dyer, Azalia Mirhoseini, Richard G. Baraniuk category:stat.ML cs.LG G.1.0; G.4 published:2015-05-19 summary:Kernel matrices (e.g. Gram or similarity matrices) are essential for manystate-of-the-art approaches to classification, clustering, and dimensionalityreduction. For large datasets, the cost of forming and factoring such kernelmatrices becomes intractable. To address this challenge, we introduce a newadaptive sampling algorithm called Accelerated Sequential Incoherence Selection(oASIS) that samples columns without explicitly computing the entire kernelmatrix. We provide conditions under which oASIS is guaranteed to exactlyrecover the kernel matrix with an optimal number of columns selected. Numericalexperiments on both synthetic and real-world datasets demonstrate that oASISachieves performance comparable to state-of-the-art adaptive sampling methodsat a fraction of the computational cost. The low runtime complexity of oASISand its low memory footprint enable the solution of large problems that aresimply intractable using other adaptive methods.
arxiv-11700-276 | Image Reconstruction from Bag-of-Visual-Words | http://arxiv.org/abs/1505.05190 | author:Hiroharu Kato, Tatsuya Harada category:cs.CV cs.AI published:2015-05-19 summary:The objective of this work is to reconstruct an original image fromBag-of-Visual-Words (BoVW). Image reconstruction from features can be a meansof identifying the characteristics of features. Additionally, it enables us togenerate novel images via features. Although BoVW is the de facto standardfeature for image recognition and retrieval, successful image reconstructionfrom BoVW has not been reported yet. What complicates this task is that BoVWlacks the spatial information for including visual words. As described in thispaper, to estimate an original arrangement, we propose an evaluation functionthat incorporates the naturalness of local adjacency and the global position,with a method to obtain related parameters using an external image database. Toevaluate the performance of our method, we reconstruct images of objects of 101kinds. Additionally, we apply our method to analyze object classifiers and togenerate novel images via BoVW.
arxiv-11700-277 | Unsupervised Visual Representation Learning by Context Prediction | http://arxiv.org/abs/1505.05192 | author:Carl Doersch, Abhinav Gupta, Alexei A. Efros category:cs.CV published:2015-05-19 summary:This work explores the use of spatial context as a source of free andplentiful supervisory signal for training a rich visual representation. Givenonly a large, unlabeled image collection, we extract random pairs of patchesfrom each image and train a convolutional neural net to predict the position ofthe second patch relative to the first. We argue that doing well on this taskrequires the model to learn to recognize objects and their parts. Wedemonstrate that the feature representation learned using this within-imagecontext indeed captures visual similarity across images. For example, thisrepresentation allows us to perform unsupervised visual discovery of objectslike cats, people, and even birds from the Pascal VOC 2011 detection dataset.Furthermore, we show that the learned ConvNet can be used in the R-CNNframework and provides a significant boost over a randomly-initialized ConvNet,resulting in state-of-the-art performance among algorithms which use onlyPascal-provided training set annotations.
arxiv-11700-278 | Vector-Space Markov Random Fields via Exponential Families | http://arxiv.org/abs/1505.05117 | author:Wesley Tansey, Oscar Hernan Madrid Padilla, Arun Sai Suggala, Pradeep Ravikumar category:stat.ML published:2015-05-19 summary:We present Vector-Space Markov Random Fields (VS-MRFs), a novel class ofundirected graphical models where each variable can belong to an arbitraryvector space. VS-MRFs generalize a recent line of work on scalar-valued,uni-parameter exponential family and mixed graphical models, thereby greatlybroadening the class of exponential families available (e.g., allowingmultinomial and Dirichlet distributions). Specifically, VS-MRFs are the jointgraphical model distributions where the node-conditional distributions belongto generic exponential families with general vector space domains. We alsopresent a sparsistent $M$-estimator for learning our class of MRFs thatrecovers the correct set of edges with high probability. We validate ourapproach via a set of synthetic data experiments as well as a real-world casestudy of over four million foods from the popular diet tracking appMyFitnessPal. Our results demonstrate that our algorithm performs wellempirically and that VS-MRFs are capable of capturing and highlightinginteresting structure in complex, real-world data. All code for our algorithmis open source and publicly available.
arxiv-11700-279 | Risk and Regret of Hierarchical Bayesian Learners | http://arxiv.org/abs/1505.04984 | author:Jonathan H. Huggins, Joshua B. Tenenbaum category:cs.LG stat.ML published:2015-05-19 summary:Common statistical practice has shown that the full power of Bayesian methodsis not realized until hierarchical priors are used, as these allow for greater"robustness" and the ability to "share statistical strength." Yet it is anongoing challenge to provide a learning-theoretically sound formalism of suchnotions that: offers practical guidance concerning when and how best to utilizehierarchical models; provides insights into what makes for a good hierarchicalprior; and, when the form of the prior has been chosen, can guide the choice ofhyperparameter settings. We present a set of analytical tools for understandinghierarchical priors in both the online and batch learning settings. We provideregret bounds under log-loss, which show how certain hierarchical modelscompare, in retrospect, to the best single model in the model class. We alsoshow how to convert a Bayesian log-loss regret bound into a Bayesian risk boundfor any bounded loss, a result which may be of independent interest. Risk andregret bounds for Student's $t$ and hierarchical Gaussian priors allow us toformalize the concepts of "robustness" and "sharing statistical strength."Priors for feature selection are investigated as well. Our results suggest thatthe learning-theoretic benefits of using hierarchical priors can often come atlittle cost on practical problems.
arxiv-11700-280 | Multi-task additive models with shared transfer functions based on dictionary learning | http://arxiv.org/abs/1505.04966 | author:Alhussein Fawzi, Mathieu Sinn, Pascal Frossard category:stat.ML cs.LG published:2015-05-19 summary:Additive models form a widely popular class of regression models whichrepresent the relation between covariates and response variables as the sum oflow-dimensional transfer functions. Besides flexibility and accuracy, a keybenefit of these models is their interpretability: the transfer functionsprovide visual means for inspecting the models and identifying domain-specificrelations between inputs and outputs. However, in large-scale problemsinvolving the prediction of many related tasks, learning independently additivemodels results in a loss of model interpretability, and can cause overfittingwhen training data is scarce. We introduce a novel multi-task learning approachwhich provides a corpus of accurate and interpretable additive models for alarge number of related forecasting tasks. Our key idea is to share transferfunctions across models in order to reduce the model complexity and ease theexploration of the corpus. We establish a connection with sparse dictionarylearning and propose a new efficient fitting algorithm which alternates betweensparse coding and transfer function updates. The former step is solved via anextension of Orthogonal Matching Pursuit, whose properties are analyzed using anovel recovery condition which extends existing results in the literature. Thelatter step is addressed using a traditional dictionary update rule.Experiments on real-world data demonstrate that our approach compares favorablyto baseline methods while yielding an interpretable corpus of models, revealingstructure among the individual tasks and being more robust when training datais scarce. Our framework therefore extends the well-known benefits of additivemodels to common regression settings possibly involving thousands of tasks.
arxiv-11700-281 | Convective regularization for optical flow | http://arxiv.org/abs/1505.04938 | author:José A. Iglesias, Clemens Kirisits category:math.OC cs.CV published:2015-05-19 summary:We argue that the time derivative in a fixed coordinate frame may not be themost appropriate measure of time regularity of an optical flow field. Instead,for a given velocity field $v$ we consider the convective acceleration $v_t +\nabla v v$ which describes the acceleration of objects moving according to$v$. Consequently we investigate the suitability of the nonconvex functional$\v_t + \nabla v v\^2_{L^2}$ as a regularization term for optical flow. Wedemonstrate that this term acts as both a spatial and a temporal regularizerand has an intrinsic edge-preserving property. We incorporate it into acontrast invariant and time-regularized variant of the Horn-Schunck functional,prove existence of minimizers and verify experimentally that it addresses someof the problems of basic quadratic models. For the minimization we use aniterative scheme that approximates the original nonlinear problem with asequence of linear ones. We believe that the convective acceleration may begainfully introduced in a variety of optical flow models.
arxiv-11700-282 | Towards Data-Driven Autonomics in Data Centers | http://arxiv.org/abs/1505.04935 | author:Alina Sîrbu, Ozalp Babaoglu category:cs.DC cs.AI stat.ML published:2015-05-19 summary:Continued reliance on human operators for managing data centers is a majorimpediment for them from ever reaching extreme dimensions. Large computersystems in general, and data centers in particular, will ultimately be managedusing predictive computational and executable models obtained throughdata-science tools, and at that point, the intervention of humans will belimited to setting high-level goals and policies rather than performinglow-level operations. Data-driven autonomics, where management and control arebased on holistic predictive models that are built and updated using generateddata, opens one possible path towards limiting the role of operators in datacenters. In this paper, we present a data-science study of a public Googledataset collected in a 12K-node cluster with the goal of building andevaluating a predictive model for node failures. We use BigQuery, the big dataSQL platform from the Google Cloud suite, to process massive amounts of dataand generate a rich feature set characterizing machine state over time. Wedescribe how an ensemble classifier can be built out of many Random Forestclassifiers each trained on these features, to predict if machines will fail ina future 24-hour window. Our evaluation reveals that if we limit false positiverates to 5%, we can achieve true positive rates between 27% and 88% withprecision varying between 50% and 72%. We discuss the practicality of includingour predictive model as the central component of a data-driven autonomicmanager and operating it on-line with live data streams (rather than off-lineon data logs). All of the scripts used for BigQuery and classification analysesare publicly available from the authors' website.
arxiv-11700-283 | High Performance Offline Handwritten Chinese Character Recognition Using GoogLeNet and Directional Feature Maps | http://arxiv.org/abs/1505.04925 | author:Zhuoyao Zhong, Lianwen Jin, Zecheng Xie category:cs.CV published:2015-05-19 summary:Just like its great success in solving many computer vision problems, theconvolutional neural networks (CNN) provided new end-to-end approach tohandwritten Chinese character recognition (HCCR) with very promising results inrecent years. However, previous CNNs so far proposed for HCCR were neither deepenough nor slim enough. We show in this paper that, a deeper architecture canbenefit HCCR a lot to achieve higher performance, meanwhile can be designedwith less parameters. We also show that the traditional feature extractionmethods, such as Gabor or gradient feature maps, are still useful for enhancingthe performance of CNN. We design a streamlined version of GoogLeNet [13],which was original proposed for image classification in recent years with verydeep architecture, for HCCR (denoted as HCCR-GoogLeNet). The HCCR-GoogLeNet weused is 19 layers deep but involves with only 7.26 million parameters.Experiments were conducted using the ICDAR 2013 offline HCCR competitiondataset. It has been shown that with the proper incorporation with traditionaldirectional feature maps, the proposed single and ensemble HCCR-GoogLeNetmodels achieve new state of the art recognition accuracy of 96.35% and 96.74%,respectively, outperforming previous best result with significant gap.
arxiv-11700-284 | Character-level Chinese Writer Identification using Path Signature Feature, DropStroke and Deep CNN | http://arxiv.org/abs/1505.04922 | author:Weixin Yang, Lianwen Jin, Manfei Liu category:cs.CV published:2015-05-19 summary:Most existing online writer-identification systems require that the textcontent is supplied in advance and rely on separately designed features andclassifiers. The identifications are based on lines of text, entire paragraphs,or entire documents; however, these materials are not always available. In thispaper, we introduce a path-signature feature to an end-to-end text-independentwriter-identification system with a deep convolutional neural network (DCNN).Because deep models require a considerable amount of data to achieve goodperformance, we propose a data-augmentation method named DropStroke to enrichpersonal handwriting. Experiments were conducted on online handwritten Chinesecharacters from the CASIA-OLHWDB1.0 dataset, which consists of 3,866 classesfrom 420 writers. For each writer, we only used 200 samples for training andthe remaining 3,666. The results reveal that the path-signature feature isuseful for writer identification, and the proposed DropStroke techniqueenhances the generalization and significantly improves performance.
arxiv-11700-285 | Have a Look at What I See | http://arxiv.org/abs/1505.04873 | author:Lior Talker, Yael Moses, Ilan Shimshoni category:cs.CV published:2015-05-19 summary:We propose a method for guiding a photographer to rotate her/his smartphonecamera to obtain an image that overlaps with another image of the same scene.The other image is taken by another photographer from a different viewpoint.Our method is applicable even when the images do not have overlapping fields ofview. Straightforward applications of our method include sharing attention toregions of interest for social purposes, or adding missing images to improvestructure for motion results. Our solution uses additional images of the scene,which are often available since many people use their smartphone camerasregularly. These images may be available online from other photographers whoare present at the scene. Our method avoids 3D scene reconstruction; it reliesinstead on a new representation that consists of the spatial orders of thescene points on two axes, x and y. This representation allows a sequence ofpoints to be chosen efficiently and projected onto the photographers images,using epipolar point transfer. Overlaying these epipolar lines on the livepreview of the camera produces a convenient interface to guide the user. Themethod was tested on challenging datasets of images and succeeded in guiding aphotographer from one view to a non-overlapping destination view.
arxiv-11700-286 | Ensemble of Example-Dependent Cost-Sensitive Decision Trees | http://arxiv.org/abs/1505.04637 | author:Alejandro Correa Bahnsen, Djamila Aouada, Bjorn Ottersten category:cs.LG published:2015-05-18 summary:Several real-world classification problems are example-dependentcost-sensitive in nature, where the costs due to misclassification vary betweenexamples and not only within classes. However, standard classification methodsdo not take these costs into account, and assume a constant cost ofmisclassification errors. In previous works, some methods that take intoaccount the financial costs into the training of different algorithms have beenproposed, with the example-dependent cost-sensitive decision tree algorithmbeing the one that gives the highest savings. In this paper we propose a newframework of ensembles of example-dependent cost-sensitive decision-trees. Theframework consists in creating different example-dependent cost-sensitivedecision trees on random subsamples of the training set, and then combiningthem using three different combination approaches. Moreover, we propose two newcost-sensitive combination approaches; cost-sensitive weighted voting andcost-sensitive stacking, the latter being based on the cost-sensitive logisticregression method. Finally, using five different databases, from fourreal-world applications: credit card fraud detection, churn modeling, creditscoring and direct marketing, we evaluate the proposed method againststate-of-the-art example-dependent cost-sensitive techniques, namely,cost-proportionate sampling, Bayes minimum risk and cost-sensitive decisiontrees. The results show that the proposed algorithms have better results forall databases, in the sense of higher savings.
arxiv-11700-287 | Mining User Opinions in Mobile App Reviews: A Keyword-based Approach | http://arxiv.org/abs/1505.04657 | author:Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, Tung Thanh Nguyen category:cs.IR cs.CL published:2015-05-18 summary:User reviews of mobile apps often contain complaints or suggestions which arevaluable for app developers to improve user experience and satisfaction.However, due to the large volume and noisy-nature of those reviews, manuallyanalyzing them for useful opinions is inherently challenging. To address thisproblem, we propose MARK, a keyword-based framework for semi-automated reviewanalysis. MARK allows an analyst describing his interests in one or some mobileapps by a set of keywords. It then finds and lists the reviews most relevant tothose keywords for further analysis. It can also draw the trends over time ofthose keywords and detect their sudden changes, which might indicate theoccurrences of serious issues. To help analysts describe their interests moreeffectively, MARK can automatically extract keywords from raw reviews and rankthem by their associations with negative reviews. In addition, based on avector-based semantic representation of keywords, MARK can divide a large setof keywords into more cohesive subsets, or suggest keywords similar to theselected ones.
arxiv-11700-288 | Towards Faster Rates and Oracle Property for Low-Rank Matrix Estimation | http://arxiv.org/abs/1505.04780 | author:Huan Gui, Quanquan Gu category:stat.ML published:2015-05-18 summary:We present a unified framework for low-rank matrix estimation with nonconvexpenalties. We first prove that the proposed estimator attains a fasterstatistical rate than the traditional low-rank matrix estimator with nuclearnorm penalty. Moreover, we rigorously show that under a certain condition onthe magnitude of the nonzero singular values, the proposed estimator enjoysoracle property (i.e., exactly recovers the true rank of the matrix), besidesattaining a faster rate. As far as we know, this is the first work thatestablishes the theory of low-rank matrix estimation with nonconvex penalties,confirming the advantages of nonconvex penalties for matrix completion.Numerical experiments on both synthetic and real world datasets corroborate ourtheory.
arxiv-11700-289 | Graph Partitioning via Parallel Submodular Approximation to Accelerate Distributed Machine Learning | http://arxiv.org/abs/1505.04636 | author:Mu Li, Dave G. Andersen, Alexander J. Smola category:cs.DC cs.AI cs.LG published:2015-05-18 summary:Distributed computing excels at processing large scale data, but thecommunication cost for synchronizing the shared parameters may slow down theoverall performance. Fortunately, the interactions between parameter and datain many problems are sparse, which admits efficient partition in order toreduce the communication overhead. In this paper, we formulate data placement as a graph partitioning problem.We propose a distributed partitioning algorithm. We give both theoreticalguarantees and a highly efficient implementation. We also provide a highlyefficient implementation of the algorithm and demonstrate its promising resultson both text datasets and social networks. We show that the proposed algorithmleads to 1.6x speedup of a state-of-the-start distributed machine learningsystem by eliminating 90\% of the network communication.
arxiv-11700-290 | Place Recognition with Event-based Cameras and a Neural Implementation of SeqSLAM | http://arxiv.org/abs/1505.04548 | author:Michael Milford, Hanme Kim, Michael Mangan, Stefan Leutenegger, Tom Stone, Barbara Webb, Andrew Davison category:cs.RO cs.CV published:2015-05-18 summary:Event-based cameras offer much potential to the fields of robotics andcomputer vision, in part due to their large dynamic range and extremely high"frame rates". These attributes make them, at least in theory, particularlysuitable for enabling tasks like navigation and mapping on high speed roboticplatforms under challenging lighting conditions, a task which has beenparticularly challenging for traditional algorithms and camera sensors. Beforethese tasks become feasible however, progress must be made towards adapting andinnovating current RGB-camera-based algorithms to work with event-basedcameras. In this paper we present ongoing research investigating two distinctapproaches to incorporating event-based cameras for robotic navigation: theinvestigation of suitable place recognition / loop closure techniques, and thedevelopment of efficient neural implementations of place recognition techniquesthat enable the possibility of place recognition using event-based cameras atvery high frame rates using neuromorphic computing hardware.
arxiv-11700-291 | Simple regret for infinitely many armed bandits | http://arxiv.org/abs/1505.04627 | author:Alexandra Carpentier, Michal Valko category:cs.LG stat.ML published:2015-05-18 summary:We consider a stochastic bandit problem with infinitely many arms. In thissetting, the learner has no chance of trying all the arms even once and has todedicate its limited number of samples only to a certain number of arms. Allprevious algorithms for this setting were designed for minimizing thecumulative regret of the learner. In this paper, we propose an algorithm aimingat minimizing the simple regret. As in the cumulative regret setting ofinfinitely many armed bandits, the rate of the simple regret will depend on aparameter $\beta$ characterizing the distribution of the near-optimal arms. Weprove that depending on $\beta$, our algorithm is minimax optimal either up toa multiplicative constant or up to a $\log(n)$ factor. We also provideextensions to several important cases: when $\beta$ is unknown, in a naturalsetting where the near-optimal arms have a small variance, and in the case ofunknown time horizon.
arxiv-11700-292 | Layered Adaptive Importance Sampling | http://arxiv.org/abs/1505.04732 | author:L. Martino, V. Elvira, D. Luengo, J. Corander category:stat.CO cs.LG stat.ML published:2015-05-18 summary:Monte Carlo methods represent the "de facto" standard for approximatingcomplicated integrals involving multidimensional target distributions. In orderto generate random realizations from the target distribution, Monte Carlotechniques use simpler proposal probability densities to draw candidatesamples. The performance of any such method is strictly related to thespecification of the proposal distribution, such that unfortunate choiceseasily wreak havoc on the resulting estimators. In this work, we introduce alayered (i.e., hierarchical) procedure to generate samples employed within aMonte Carlo scheme. This approach ensures that an appropriate equivalentproposal density is always obtained automatically (thus eliminating the risk ofa catastrophic performance), although at the expense of a moderate increase inthe complexity. Furthermore, we provide a general unified importance sampling(IS) framework, where multiple proposal densities are employed and several ISschemes are introduced by applying the so-called deterministic mixtureapproach. Finally, given these schemes, we also propose a novel class ofadaptive importance samplers using a population of proposals, where theadaptation is driven by independent parallel or interacting Markov Chain MonteCarlo (MCMC) chains. The resulting algorithms efficiently combine the benefitsof both IS and MCMC methods.
arxiv-11700-293 | Compressed Nonnegative Matrix Factorization is Fast and Accurate | http://arxiv.org/abs/1505.04650 | author:Mariano Tepper, Guillermo Sapiro category:cs.LG stat.ML published:2015-05-18 summary:Nonnegative matrix factorization (NMF) has an established reputation as auseful data analysis technique in numerous applications. However, its usage inpractical situations is undergoing challenges in recent years. The fundamentalfactor to this is the increasingly growing size of the datasets available andneeded in the information sciences. To address this, in this work we propose touse structured random compression, that is, random projections that exploit thedata structure, for two NMF variants: classical and separable. In separable NMF(SNMF) the left factors are a subset of the columns of the input matrix. Wepresent suitable formulations for each problem, dealing with differentrepresentative algorithms within each one. We show that the resultingcompressed techniques are faster than their uncompressed variants, vastlyreduce memory demands, and do not encompass any significant deterioration inperformance. The proposed structured random projections for SNMF allow to dealwith arbitrarily shaped large matrices, beyond the standard limit oftall-and-skinny matrices, granting access to very efficient computations inthis general setting. We accompany the algorithmic presentation withtheoretical foundations and numerous and diverse examples, showing thesuitability of the proposed approaches.
arxiv-11700-294 | Reproducible Evaluation of Pan-Tilt-Zoom Tracking | http://arxiv.org/abs/1505.04502 | author:Gengjie Chen, Pierre-Luc St-Charles, Wassim Bouachir, Thomas Joeisseint, Guillaume-Alexandre Bilodeau, Robert Bergevin category:cs.CV published:2015-05-18 summary:Tracking with a Pan-Tilt-Zoom (PTZ) camera has been a research topic incomputer vision for many years. However, it is very difficult to assess theprogress that has been made on this topic because there is no standardevaluation methodology. The difficulty in evaluating PTZ tracking algorithmsarises from their dynamic nature. In contrast to other forms of tracking, PTZtracking involves both locating the target in the image and controlling themotors of the camera to aim it so that the target stays in its field of view.This type of tracking can only be performed online. In this paper, we propose anew evaluation framework based on a virtual PTZ camera. With this framework,tracking scenarios do not change for each experiment and we are able toreplicate online PTZ camera control and behavior including camera positioningdelays, tracker processing delays, and numerical zoom. We tested our evaluationframework with the Camshift tracker to show its viability and to establishbaseline results.
arxiv-11700-295 | Global Variational Method for Fingerprint Segmentation by Three-part Decomposition | http://arxiv.org/abs/1505.04585 | author:Duy Hoang Thai, Carsten Gottschlich category:cs.CV published:2015-05-18 summary:Verifying an identity claim by fingerprint recognition is a commonplaceexperience for millions of people in their daily life, e.g. for unlocking atablet computer or smartphone. The first processing step after fingerprintimage acquisition is segmentation, i.e. dividing a fingerprint image into aforeground region which contains the relevant features for the comparisonalgorithm, and a background region. We propose a novel segmentation method byglobal three-part decomposition (G3PD). Based on global variational analysis,the G3PD method decomposes a fingerprint image into cartoon, texture and noiseparts. After decomposition, the foreground region is obtained from the non-zerocoefficients in the texture image using morphological processing. Thesegmentation performance of the G3PD method is compared to fivestate-of-the-art methods on a benchmark which comprises manually marked groundtruth segmentation for 10560 images. Performance evaluations show that the G3PDmethod consistently outperforms existing methods in terms of segmentationaccuracy.
arxiv-11700-296 | Recurrent Neural Network Training with Dark Knowledge Transfer | http://arxiv.org/abs/1505.04630 | author:Zhiyuan Tang, Dong Wang, Zhiyong Zhang category:stat.ML cs.CL cs.LG cs.NE published:2015-05-18 summary:Recurrent neural networks (RNNs), particularly long short-term memory (LSTM),have gained much attention in automatic speech recognition (ASR). Although somesuccessful stories have been reported, training RNNs remains highlychallenging, especially with limited training data. Recent research found thata well-trained model can be used as a teacher to train other child models, byusing the predictions generated by the teacher model as supervision. Thisknowledge transfer learning has been employed to train simple neural nets witha complex one, so that the final performance can reach a level that isinfeasible to obtain by regular training. In this paper, we employ theknowledge transfer learning approach to train RNNs (precisely LSTM) using adeep neural network (DNN) model as the teacher. This is different from most ofthe existing research on knowledge transfer learning, since the teacher (DNN)is assumed to be weaker than the child (RNN); however, our experiments on anASR task showed that it works fairly well: without applying any tricks on thelearning scheme, this approach can train RNNs successfully even with limitedtraining data.
arxiv-11700-297 | Predicting Important Objects for Egocentric Video Summarization | http://arxiv.org/abs/1505.04803 | author:Yong Jae Lee, Kristen Grauman category:cs.CV published:2015-05-18 summary:We present a video summarization approach for egocentric or "wearable" cameradata. Given hours of video, the proposed method produces a compact storyboardsummary of the camera wearer's day. In contrast to traditional keyframeselection techniques, the resulting summary focuses on the most importantobjects and people with which the camera wearer interacts. To accomplish this,we develop region cues indicative of high-level saliency in egocentricvideo---such as the nearness to hands, gaze, and frequency of occurrence---andlearn a regressor to predict the relative importance of any new region based onthese cues. Using these predictions and a simple form of temporal eventdetection, our method selects frames for the storyboard that reflect the keyobject-driven happenings. We adjust the compactness of the final summary giveneither an importance selection criterion or a length budget; for the latter, wedesign an efficient dynamic programming solution that accounts for importance,visual uniqueness, and temporal displacement. Critically, the approach isneither camera-wearer-specific nor object-specific; that means the learnedimportance metric need not be trained for a given user or context, and it canpredict the importance of objects and people that have never been seenpreviously. Our results on two egocentric video datasets show the method'spromise relative to existing techniques for saliency and summarization.
arxiv-11700-298 | Emergence-focused design in complex system simulation | http://arxiv.org/abs/1505.04518 | author:Chris Marriott, Jobran Chebib category:q-bio.PE cs.AI cs.NE published:2015-05-18 summary:Emergence is a phenomenon taken for granted in science but also still notwell understood. We have developed a model of artificial genetic evolutionintended to allow for emergence on genetic, population and social levels. Wepresent the details of the current state of our environment, agent, andreproductive models. In developing our models we have relied on a principle ofusing non-linear systems to model as many systems as possible includingmutation and recombination, gene-environment interaction, agent metabolism,agent survival, resource gathering and sexual reproduction. In this paper wereview the genetic dynamics that have emerged in our system includinggenotype-phenotype divergence, genetic drift, pseudogenes, and geneduplication. We conclude that emergence-focused design in complex systemsimulation is necessary to reproduce the multilevel emergence seen in thenatural world.
arxiv-11700-299 | On the tightness of an SDP relaxation of k-means | http://arxiv.org/abs/1505.04778 | author:Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar category:cs.IT cs.DS cs.LG math.IT math.ST stat.ML stat.TH published:2015-05-18 summary:Recently, Awasthi et al. introduced an SDP relaxation of the $k$-meansproblem in $\mathbb R^m$. In this work, we consider a random model for the datapoints in which $k$ balls of unit radius are deterministically distributedthroughout $\mathbb R^m$, and then in each ball, $n$ points are drawn accordingto a common rotationally invariant probability distribution. For any fixed ballconfiguration and probability distribution, we prove that the SDP relaxation ofthe $k$-means problem exactly recovers these planted clusters with probability$1-e^{-\Omega(n)}$ provided the distance between any two of the ball centers is$>2+\epsilon$, where $\epsilon$ is an explicit function of the configuration ofthe ball centers, and can be arbitrarily small when $m$ is large.
arxiv-11700-300 | U-Net: Convolutional Networks for Biomedical Image Segmentation | http://arxiv.org/abs/1505.04597 | author:Olaf Ronneberger, Philipp Fischer, Thomas Brox category:cs.CV published:2015-05-18 summary:There is large consent that successful training of deep networks requiresmany thousand annotated training samples. In this paper, we present a networkand training strategy that relies on the strong use of data augmentation to usethe available annotated samples more efficiently. The architecture consists ofa contracting path to capture context and a symmetric expanding path thatenables precise localization. We show that such a network can be trainedend-to-end from very few images and outperforms the prior best method (asliding-window convolutional network) on the ISBI challenge for segmentation ofneuronal structures in electron microscopic stacks. Using the same networktrained on transmitted light microscopy images (phase contrast and DIC) we wonthe ISBI cell tracking challenge 2015 in these categories by a large margin.Moreover, the network is fast. Segmentation of a 512x512 image takes less thana second on a recent GPU. The full implementation (based on Caffe) and thetrained networks are available athttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .
