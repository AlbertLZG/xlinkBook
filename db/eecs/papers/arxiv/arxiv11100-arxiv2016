arxiv-11100-1 | Factorized Asymptotic Bayesian Inference for Factorial Hidden Markov Models | http://arxiv.org/pdf/1506.07959v1.pdf | author:Shaohua Li, Ryohei Fujimaki, Chunyan Miao category:stat.ML published:2015-06-26 summary:Factorial hidden Markov models (FHMMs) are powerful tools of modelingsequential data. Learning FHMMs yields a challenging simultaneous modelselection issue, i.e., selecting the number of multiple Markov chains and thedimensionality of each chain. Our main contribution is to address this modelselection issue by extending Factorized Asymptotic Bayesian (FAB) inference toFHMMs. First, we offer a better approximation of marginal log-likelihood thanthe previous FAB inference. Our key idea is to integrate out transitionprobabilities, yet still apply the Laplace approximation to emissionprobabilities. Second, we prove that if there are two very similar hiddenstates in an FHMM, i.e. one is redundant, then FAB will almost surely shrinkand eliminate one of them, making the model parsimonious. Experimental resultsshow that FAB for FHMMs significantly outperforms state-of-the-artnonparametric Bayesian iFHMM and Variational FHMM in model selection accuracy,with competitive held-out perplexity.
arxiv-11100-2 | Bag-of-Features Image Indexing and Classification in Microsoft SQL Server Relational Database | http://arxiv.org/pdf/1506.07950v1.pdf | author:Marcin Korytkowski, Rafal Scherer, Pawel Staszewski, Piotr Woldan category:cs.DB cs.CV published:2015-06-26 summary:This paper presents a novel relational database architecture aimed to visualobjects classification and retrieval. The framework is based on thebag-of-features image representation model combined with the Support VectorMachine classification and is integrated in a Microsoft SQL Server database.
arxiv-11100-3 | Collaboratively Learning Preferences from Ordinal Data | http://arxiv.org/pdf/1506.07947v1.pdf | author:Sewoong Oh, Kiran K. Thekumparampil, Jiaming Xu category:cs.LG cs.IT math.IT stat.ML published:2015-06-26 summary:In applications such as recommendation systems and revenue management, it isimportant to predict preferences on items that have not been seen by a user orpredict outcomes of comparisons among those that have never been compared. Apopular discrete choice model of multinomial logit model captures the structureof the hidden preferences with a low-rank matrix. In order to predict thepreferences, we want to learn the underlying model from noisy observations ofthe low-rank matrix, collected as revealed preferences in various forms ofordinal data. A natural approach to learn such a model is to solve a convexrelaxation of nuclear norm minimization. We present the convex relaxationapproach in two contexts of interest: collaborative ranking and bundled choicemodeling. In both cases, we show that the convex relaxation is minimax optimal.We prove an upper bound on the resulting error with finite samples, and providea matching information-theoretic lower bound.
arxiv-11100-4 | Clustering categorical data via ensembling dissimilarity matrices | http://arxiv.org/pdf/1506.07930v1.pdf | author:Saeid Amiri, Bertrand Clarke, Jennifer Clarke category:stat.ML published:2015-06-26 summary:We present a technique for clustering categorical data by generating manydissimilarity matrices and averaging over them. We begin by demonstrating ourtechnique on low dimensional categorical data and comparing it to several othertechniques that have been proposed. Then we give conditions under which ourmethod should yield good results in general. Our method extends to highdimensional categorical data of equal lengths by ensembling over many choicesof explanatory variables. In this context we compare our method with two othermethods. Finally, we extend our method to high dimensional categorical datavectors of unequal length by using alignment techniques to equalize thelengths. We give examples to show that our method continues to provide goodresults, in particular, better in the context of genome sequences thanclusterings suggested by phylogenetic trees.
arxiv-11100-5 | Analyzing statistical and computational tradeoffs of estimation procedures | http://arxiv.org/pdf/1506.07925v1.pdf | author:Daniel L. Sussman, Alexander Volfovsky, Edoardo M. Airoldi category:stat.CO stat.ML published:2015-06-25 summary:The recent explosion in the amount and dimensionality of data has exacerbatedthe need of trading off computational and statistical efficiency carefully, sothat inference is both tractable and meaningful. We propose a framework thatprovides an explicit opportunity for practitioners to specify how muchstatistical risk they are willing to accept for a given computational cost, andleads to a theoretical risk-computation frontier for any given inferenceproblem. We illustrate the tradeoff between risk and computation and illustratethe frontier in three distinct settings. First, we derive analytic forms forthe risk of estimating parameters in the classical setting of estimating themean and variance for normally distributed data and for the more generalsetting of parameters of an exponential family. The second example concentrateson computationally constrained Hodges-Lehmann estimators. We conclude with anevaluation of risk associated with early termination of iterative matrixinversion algorithms in the context of linear regression.
arxiv-11100-6 | Diffusion Nets | http://arxiv.org/pdf/1506.07840v1.pdf | author:Gal Mishne, Uri Shaham, Alexander Cloninger, Israel Cohen category:stat.ML cs.LG math.CA published:2015-06-25 summary:Non-linear manifold learning enables high-dimensional data analysis, butrequires out-of-sample-extension methods to process new data points. In thispaper, we propose a manifold learning algorithm based on deep learning tocreate an encoder, which maps a high-dimensional dataset and itslow-dimensional embedding, and a decoder, which takes the embedded data back tothe high-dimensional space. Stacking the encoder and decoder togetherconstructs an autoencoder, which we term a diffusion net, that performsout-of-sample-extension as well as outlier detection. We introduce new neuralnet constraints for the encoder, which preserves the local geometry of thepoints, and we prove rates of convergence for the encoder. Also, our approachis efficient in both computational complexity and memory requirements, asopposed to previous methods that require storage of all training points in boththe high-dimensional and the low-dimensional spaces to calculate theout-of-sample-extension and the pre-image.
arxiv-11100-7 | Deep Learning by Scattering | http://arxiv.org/pdf/1306.5532v2.pdf | author:Stéphane Mallat, Irène Waldspurger category:cs.LG stat.ML published:2013-06-24 summary:We introduce general scattering transforms as mathematical models of deepneural networks with l2 pooling. Scattering networks iteratively apply complexvalued unitary operators, and the pooling is performed by a complex modulus. Anexpected scattering defines a contractive representation of a high-dimensionalprobability distribution, which preserves its mean-square norm. We show thatunsupervised learning can be casted as an optimization of the space contractionto preserve the volume occupied by unlabeled examples, at each layer of thenetwork. Supervised learning and classification are performed with an averagedscattering, which provides scattering estimations for multiple classes.
arxiv-11100-8 | Sparse multi-view matrix factorisation: a multivariate approach to multiple tissue comparisons | http://arxiv.org/pdf/1503.01291v2.pdf | author:Zi Wang, Wei Yuan, Giovanni Montana category:stat.ML stat.AP published:2015-03-04 summary:Gene expression levels in a population vary extensively across tissues. Suchheterogeneity is caused by genetic variability and environmental factors, andis expected to be linked to disease development. The abundance of experimentaldata now enables the identification of features of gene expression profilesthat are shared across tissues, and those that are tissue-specific. While mostcurrent research is concerned with characterising differential expression bycomparing mean expression profiles across tissues, it is also believed that asignificant difference in a gene expression's variance across tissues may alsobe associated to molecular mechanisms that are important for tissue developmentand function. We propose a sparse multi-view matrix factorisation (sMVMF)algorithm to jointly analyse gene expression measurements in multiple tissues,where each tissue provides a different "view" of the underlying organism. Theproposed methodology can be interpreted as an extension of principal componentanalysis in that it provides the means to decompose the total sample variancein each tissue into the sum of two components: one capturing the variance thatis shared across tissues, and one isolating the tissue-specific variances.sMVMF has been used to jointly model mRNA expression profiles in three tissues- adipose, skin and LCL - which are available for a large and well-phenotypedtwins cohort, TwinsUK. Using sMVMF, we are able to prioritise genes based onwhether their variation patterns are specific to each tissue. Furthermore,using DNA methylation profiles available, we provide supporting evidence thatadipose-specific gene expression patterns may be driven by epigenetic effects.
arxiv-11100-9 | Deep Neural Networks for Anatomical Brain Segmentation | http://arxiv.org/pdf/1502.02445v2.pdf | author:Alexandre de Brebisson, Giovanni Montana category:cs.CV cs.LG stat.AP stat.ML published:2015-02-09 summary:We present a novel approach to automatically segment magnetic resonance (MR)images of the human brain into anatomical regions. Our methodology is based ona deep artificial neural network that assigns each voxel in an MR image of thebrain to its corresponding anatomical region. The inputs of the network captureinformation at different scales around the voxel of interest: 3D and orthogonal2D intensity patches capture the local spatial context while large, compressed2D orthogonal patches and distances to the regional centroids enforce globalspatial consistency. Contrary to commonly used segmentation methods, ourtechnique does not require any non-linear registration of the MR images. Tobenchmark our model, we used the dataset provided for the MICCAI 2012 challengeon multi-atlas labelling, which consists of 35 manually segmented MR images ofthe brain. We obtained competitive results (mean dice coefficient 0.725, errorrate 0.163) showing the potential of our approach. To our knowledge, ourtechnique is the first to tackle the anatomical segmentation of the whole brainusing deep neural networks.
arxiv-11100-10 | Diffusion Fingerprints | http://arxiv.org/pdf/1408.4966v2.pdf | author:Jimmy Dubuisson, Jean-Pierre Eckmann, Andrea Agazzi category:stat.ML cs.IR cs.LG published:2014-08-21 summary:We introduce, test and discuss a method for classifying and clustering datamodeled as directed graphs. The idea is to start diffusion processes from anysubset of a data collection, generating corresponding distributions forreaching points in the network. These distributions take the form ofhigh-dimensional numerical vectors and capture essential topological propertiesof the original dataset. We show how these diffusion vectors can besuccessfully applied for getting state-of-the-art accuracies in the problem ofextracting pathways from metabolic networks. We also provide a guideline toillustrate how to use our method for classification problems, and discussimportant details of its implementation. In particular, we present a simpledimensionality reduction technique that lowers the computational cost ofclassifying diffusion vectors, while leaving the predictive power of theclassification process substantially unaltered. Although the method has veryfew parameters, the results we obtain show its flexibility and power. Thisshould make it helpful in many other contexts.
arxiv-11100-11 | How to improve robustness in Kohonen maps and display additional information in Factorial Analysis: application to text mining | http://arxiv.org/pdf/1506.07732v1.pdf | author:Nicolas Bourgeois, Marie Cottrell, Benjamin Déruelle, Stéphane Lamassé, Patrick Letrémy category:math.ST cs.CL stat.TH published:2015-06-25 summary:This article is an extended version of a paper presented in the WSOM'2012conference [1]. We display a combination of factorial projections, SOMalgorithm and graph techniques applied to a text mining problem. The corpuscontains 8 medieval manuscripts which were used to teach arithmetic techniquesto merchants. Among the techniques for Data Analysis, those used forLexicometry (such as Factorial Analysis) highlight the discrepancies betweenmanuscripts. The reason for this is that they focus on the deviation from theindependence between words and manuscripts. Still, we also want to discover andcharacterize the common vocabulary among the whole corpus. Using the propertiesof stochastic Kohonen maps, which define neighborhood between inputs in anon-deterministic way, we highlight the words which seem to play a special rolein the vocabulary. We call them fickle and use them to improve both Kohonen maprobustness and significance of FCA visualization. Finally we use graphalgorithmic to exploit this fickleness for classification of words.
arxiv-11100-12 | Fairness-Aware Learning with Restriction of Universal Dependency using f-Divergences | http://arxiv.org/pdf/1506.07721v1.pdf | author:Kazuto Fukuchi, Jun Sakuma category:stat.ML cs.LG published:2015-06-25 summary:Fairness-aware learning is a novel framework for classification tasks. Likeregular empirical risk minimization (ERM), it aims to learn a classifier with alow error rate, and at the same time, for the predictions of the classifier tobe independent of sensitive features, such as gender, religion, race, andethnicity. Existing methods can achieve low dependencies on given samples, butthis is not guaranteed on unseen samples. The existing fairness-aware learningalgorithms employ different dependency measures, and each algorithm isspecifically designed for a particular one. Such diversity makes it difficultto theoretically analyze and compare them. In this paper, we propose a generalframework for fairness-aware learning that uses f-divergences and that coversmost of the dependency measures employed in the existing methods. We introducea way to estimate the f-divergences that allows us to give a unified analysisfor the upper bound of the estimation error; this bound is tighter than that ofthe existing convergence rate analysis of the divergence estimation. With ourdivergence estimate, we propose a fairness-aware learning algorithm, andperform a theoretical analysis of its generalization error. Our analysisreveals that, under mild assumptions and even with enforcement of fairness, thegeneralization error of our method is $O(\sqrt{1/n})$, which is the same asthat of the regular ERM. In addition, and more importantly, we show that, forany f-divergence, the upper bound of the estimation error of the divergence is$O(\sqrt{1/n})$. This indicates that our fairness-aware learning algorithmguarantees low dependencies on unseen samples for any dependency measurerepresented by an f-divergence.
arxiv-11100-13 | Role of normalization in spectral clustering for stochastic blockmodels | http://arxiv.org/pdf/1310.1495v2.pdf | author:Purnamrita Sarkar, Peter J. Bickel category:stat.ML published:2013-10-05 summary:Spectral clustering is a technique that clusters elements using the top feweigenvectors of their (possibly normalized) similarity matrix. The quality ofspectral clustering is closely tied to the convergence properties of theseprincipal eigenvectors. This rate of convergence has been shown to be identicalfor both the normalized and unnormalized variants in recent random matrixtheory literature. However, normalization for spectral clustering is commonlybelieved to be beneficial [Stat. Comput. 17 (2007) 395-416]. Indeed, ourexperiments show that normalization improves prediction accuracy. In thispaper, for the popular stochastic blockmodel, we theoretically show thatnormalization shrinks the spread of points in a class by a constant fractionunder a broad parameter regime. As a byproduct of our work, we also obtainsharp deviation bounds of empirical principal eigenvalues of graphs generatedfrom a stochastic blockmodel.
arxiv-11100-14 | Manifold Optimization for Gaussian Mixture Models | http://arxiv.org/pdf/1506.07677v1.pdf | author:Reshad Hosseini, Suvrit Sra category:stat.ML cs.LG math.OC published:2015-06-25 summary:We take a new look at parameter estimation for Gaussian Mixture Models(GMMs). In particular, we propose using \emph{Riemannian manifold optimization}as a powerful counterpart to Expectation Maximization (EM). An out-of-the-boxinvocation of manifold optimization, however, fails spectacularly: it convergesto the same solution but vastly slower. Driven by intuition from manifoldconvexity, we then propose a reparamerization that has remarkable empiricalconsequences. It makes manifold optimization not only match EM---a highlyencouraging result in itself given the poor record nonlinear programmingmethods have had against EM so far---but also outperform EM in many practicalsettings, while displaying much less variability in running times. We furtherhighlight the strengths of manifold optimization by developing a somewhat tunedmanifold LBFGS method that proves even more competitive and reliable thanexisting manifold optimization tools. We hope that our results encourage awider consideration of manifold optimization for parameter estimation problems.
arxiv-11100-15 | Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling | http://arxiv.org/pdf/1506.07650v1.pdf | author:Kun Xu, Yansong Feng, Songfang Huang, Dongyan Zhao category:cs.CL cs.LG published:2015-06-25 summary:Syntactic features play an essential role in identifying relationship in asentence. Previous neural network models often suffer from irrelevantinformation introduced when subjects and objects are in a long distance. Inthis paper, we propose to learn more robust relation representations from theshortest dependency path through a convolution neural network. We furtherpropose a straightforward negative sampling strategy to improve the assignmentof subjects and objects. Experimental results show that our method outperformsthe state-of-the-art methods on the SemEval-2010 Task 8 dataset.
arxiv-11100-16 | A PTAS for Agnostically Learning Halfspaces | http://arxiv.org/pdf/1410.7050v3.pdf | author:Amit Daniely category:cs.DS cs.LG published:2014-10-26 summary:We present a PTAS for agnostically learning halfspaces w.r.t. the uniformdistribution on the $d$ dimensional sphere. Namely, we show that for every$\mu>0$ there is an algorithm that runs in time$\mathrm{poly}(d,\frac{1}{\epsilon})$, and is guaranteed to return a classifierwith error at most $(1+\mu)\mathrm{opt}+\epsilon$, where $\mathrm{opt}$ is theerror of the best halfspace classifier. This improves on Awasthi, Balcan andLong [ABL14] who showed an algorithm with an (unspecified) constantapproximation ratio. Our algorithm combines the classical technique ofpolynomial regression (e.g. [LMN89, KKMS05]), together with the newlocalization technique of [ABL14].
arxiv-11100-17 | Completing Low-Rank Matrices with Corrupted Samples from Few Coefficients in General Basis | http://arxiv.org/pdf/1506.07615v1.pdf | author:Hongyang Zhang, Zhouchen Lin, Chao Zhang category:cs.IT cs.LG cs.NA math.IT math.NA stat.ML 68T05 G.1.6; K.3.2 published:2015-06-25 summary:Subspace recovery from corrupted and missing data is crucial for variousapplications in signal processing and information theory. To complete missingvalues and detect column corruptions, existing robust Matrix Completion (MC)methods mostly concentrate on recovering a low-rank matrix from few corruptedcoefficients w.r.t. the standard basis, which, however, does not apply to moregeneral basis, e.g., the Fourier basis. In this paper, we prove that the rangespace of an $m\times n$ matrix with rank $r$ can be exactly recovered from fewcoefficients w.r.t. general basis, though the rank $r$ and the number ofcorrupted samples are both as high as $O(\min\{m,n\}/\log^3 (m+n))$. Thus ourresults cover previous work as special cases, and robust MC can recover theintrinsic matrix with a higher rank. Moreover, we suggest a universal choice ofthe regularization parameter, which is $\lambda=1/\sqrt{\log n}$. By our$\ell_{2,1}$ filtering algorithm, which has theoretical guarantees, we canfurther reduce the computational cost of our model. As an application, we alsofind that the solutions to extended robust Low-Rank Representation and to ourextended robust MC are mutually expressible, so both our theory and algorithmcan be immediately applied to the subspace clustering problem with missingvalues. Experiments verify our theories.
arxiv-11100-18 | Generalized Majorization-Minimization | http://arxiv.org/pdf/1506.07613v1.pdf | author:Sobhan Naderi Parizi, Kun He, Stan Sclaroff, Pedro Felzenszwalb category:cs.CV cs.IT cs.LG math.IT stat.ML published:2015-06-25 summary:Non-convex optimization is ubiquitous in machine learning. TheMajorization-Minimization (MM) procedure systematically optimizes non-convexfunctions through an iterative construction and optimization of upper bounds onthe objective function. The bound at each iteration is required to \emph{touch}the objective function at the optimizer of the previous bound. We show thatthis touching constraint is unnecessary and overly restrictive. We generalizeMM by relaxing this constraint, and propose a new framework for designingoptimization algorithms, named Generalized Majorization-Minimization (G-MM).Compared to MM, G-MM is much more flexible. For instance, it can incorporateapplication-specific biases into the optimization procedure without changingthe objective function. We derive G-MM algorithms for several latent variablemodels and show that they consistently outperform their MM counterparts inoptimizing non-convex objectives. In particular, G-MM algorithms appear to beless sensitive to initialization.
arxiv-11100-19 | Joint community and anomaly tracking in dynamic networks | http://arxiv.org/pdf/1506.07611v1.pdf | author:Brian Baingana, Georgios B. Giannakis category:stat.ML cs.SI physics.soc-ph published:2015-06-25 summary:Most real-world networks exhibit community structure, a phenomenoncharacterized by existence of node clusters whose intra-edge connectivity isstronger than edge connectivities between nodes belonging to differentclusters. In addition to facilitating a better understanding of networkbehavior, community detection finds many practical applications in diversesettings. Communities in online social networks are indicative of sharedfunctional roles, or affiliation to a common socio-economic status, theknowledge of which is vital for targeted advertisement. In buyer-sellernetworks, community detection facilitates better product recommendations.Unfortunately, reliability of community assignments is hindered by anomaloususer behavior often observed as unfair self-promotion, or "fake"highly-connected accounts created to promote fraud. The present paper advocatesa novel approach for jointly tracking communities while detecting suchanomalous nodes in time-varying networks. By postulating edge creation as theresult of mutual community participation by node pairs, a dynamic factor modelwith anomalous memberships captured through a sparse outlier matrix is putforth. Efficient tracking algorithms suitable for both online and decentralizedoperation are developed. Experiments conducted on both synthetic and realnetwork time series successfully unveil underlying communities and anomalousnodes.
arxiv-11100-20 | CRAFT: ClusteR-specific Assorted Feature selecTion | http://arxiv.org/pdf/1506.07609v1.pdf | author:Vikas K. Garg, Cynthia Rudin, Tommi Jaakkola category:cs.LG stat.ML published:2015-06-25 summary:We present a framework for clustering with cluster-specific featureselection. The framework, CRAFT, is derived from asymptotic log posteriorformulations of nonparametric MAP-based clustering models. CRAFT handlesassorted data, i.e., both numeric and categorical data, and the underlyingobjective functions are intuitively appealing. The resulting algorithm issimple to implement and scales nicely, requires minimal parameter tuning,obviates the need to specify the number of clusters a priori, and comparesfavorably with other methods on real datasets.
arxiv-11100-21 | Incremental RANSAC for Online Relocation in Large Dynamic Environments | http://arxiv.org/pdf/1506.07236v2.pdf | author:Kanji Tanaka, Eiji Kondo category:cs.RO cs.CV published:2015-06-24 summary:Vehicle relocation is the problem in which a mobile robot has to estimate theself-position with respect to an a priori map of landmarks using the perceptionand the motion measurements without using any knowledge of the initialself-position. Recently, RANdom SAmple Consensus (RANSAC), a robustmulti-hypothesis estimator, has been successfully applied to offline relocationin static environments. On the other hand, online relocation in dynamicenvironments is still a difficult problem, for available computation time isalways limited, and for measurement include many outliers. To realize real timealgorithm for such an online process, we have developed an incremental versionof RANSAC algorithm by extending an efficient preemption RANSAC scheme. Thisnovel scheme named incremental RANSAC is able to find inlier hypotheses ofself-positions out of large number of outlier hypotheses contaminated byoutlier measurements.
arxiv-11100-22 | An objective prior that unifies objective Bayes and information-based inference | http://arxiv.org/pdf/1506.00745v2.pdf | author:Colin H. LaMont, Paul A. Wiggins category:stat.ML cs.LG published:2015-06-02 summary:There are three principle paradigms of statistical inference: (i) Bayesian,(ii) information-based and (iii) frequentist inference. We describe anobjective prior (the weighting or $w$-prior) which unifies objective Bayes andinformation-based inference. The $w$-prior is chosen to make the marginalprobability an unbiased estimator of the predictive performance of the model.This definition has several other natural interpretations. From the perspectiveof the information content of the prior, the $w$-prior is both uniformly andmaximally uninformative. The $w$-prior can also be understood to result in auniform density of distinguishable models in parameter space. Finally wedemonstrate the the $w$-prior is equivalent to the Akaike Information Criterion(AIC) for regular models in the asymptotic limit. The $w$-prior appears to begenerically applicable to statistical inference and is free of {\it ad hoc}regularization. The mechanism for suppressing complexity is analogous to AIC:model complexity reduces model predictivity. We expect this new objective-Bayesapproach to inference to be widely-applicable to machine-learning problemsincluding singular models.
arxiv-11100-23 | Degenerate Motions in Multicamera Cluster SLAM with Non-overlapping Fields of View | http://arxiv.org/pdf/1506.07597v1.pdf | author:Michael J. Tribou, David W. L. Wang, Steven L. Waslander category:cs.CV cs.RO published:2015-06-25 summary:An analysis of the relative motion and point feature model configurationsleading to solution degeneracy is presented, for the case of a SimultaneousLocalization and Mapping system using multicamera clusters with non-overlappingfields-of-view. The SLAM optimization system seeks to minimize image spacereprojection error and is formulated for a cluster containing any number ofcomponent cameras, observing any number of point features over two keyframes.The measurement Jacobian is transformed to expose a reduced-dimensionrepresentation such that the degeneracy of the system can be determined by therank of a dense submatrix. A set of relative motions sufficient for degeneracyare identified for certain cluster configurations, independent of target modelgeometry. Furthermore, it is shown that increasing the number of cameras withinthe cluster and observing features across different cameras over the twokeyframes reduces the size of the degenerate motion sets significantly.
arxiv-11100-24 | Multiresolution Approach to Acceleration of Iterative Image Reconstruction for X-Ray Imaging for Security Applications | http://arxiv.org/pdf/1508.04458v1.pdf | author:S. Degirmenci, Joseph A. O'Sullivan, David G. Politte category:cs.CV published:2015-06-24 summary:Three-dimensional x-ray CT image reconstruction in baggage scanning insecurity applications is an important research field. The variety of materialsto be reconstructed is broader than medical x-ray imaging. Presence of highattenuating materials such as metal may cause artifacts if analyticalreconstruction methods are used. Statistical modeling and the resultantiterative algorithms are known to reduce these artifacts and present goodquantitative accuracy in estimates of linear attenuation coefficients. However,iterative algorithms may require computations in order to achievequantitatively accurate results. For the case of baggage scanning, in order toprovide fast accurate inspection throughput, they must be accelerateddrastically. There are many approaches proposed in the literature to increasespeed of convergence. This paper presents a new method that estimates thewavelet coefficients of the images in the discrete wavelet transform domaininstead of the image space itself. Initially, surrogate functions are createdaround approximation coefficients only. As the iterations proceed, the wavelettree on which the updates are made is expanded based on a criterion and detailcoefficients at each level are updated and the tree is expanded this way. Forexample, in the smooth regions of the image the detail coefficients are notupdated while the coefficients that represent the high-frequency componentaround edges are being updated, thus saving time by focusing computations wherethey are needed. This approach is implemented on real data from a SureScan (TM)x1000 Explosive Detection System and compared to straightforward implementationof the unregularized alternating minimization of O'Sullivan and Benac [1].
arxiv-11100-25 | CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique Splits | http://arxiv.org/pdf/1506.06155v2.pdf | author:Mohammad Norouzi, Maxwell D. Collins, David J. Fleet, Pushmeet Kohli category:cs.LG cs.CV published:2015-06-19 summary:We propose a novel algorithm for optimizing multivariate linear thresholdfunctions as split functions of decision trees to create improved Random Forestclassifiers. Standard tree induction methods resort to sampling and exhaustivesearch to find good univariate split functions. In contrast, our methodcomputes a linear combination of the features at each node, and optimizes theparameters of the linear combination (oblique) split functions by adopting avariant of latent variable SVM formulation. We develop a convex-concave upperbound on the classification loss for a one-level decision tree, and optimizethe bound by stochastic gradient descent at each internal node of the tree.Forests of up to 1000 Continuously Optimized Oblique (CO2) decision trees arecreated, which significantly outperform Random Forest with univariate splitsand previous techniques for constructing oblique trees. Experimental resultsare reported on multi-class classification benchmarks and on Labeled Faces inthe Wild (LFW) dataset.
arxiv-11100-26 | Learning Representations from Deep Networks Using Mode Synthesizers | http://arxiv.org/pdf/1506.07545v1.pdf | author:N. E. Osegi, P. Enyindah category:cs.NE published:2015-06-24 summary:Deep learning Networks play a crucial role in the evolution of a vast numberof current machine learning models for solving a variety of real worldnon-trivial tasks. Such networks use big data which is generally unlabeledunsupervised and multi-layered requiring no form of supervision for trainingand learning data and has been used to successfully build automatic supervisoryneural networks. However the question still remains how well the learned datarepresents interestingness, and their effectiveness i.e. efficiency in deeplearning models or applications. If the output of a network of deep learningmodels can be beamed unto a scene of observables, we could learn thevariational frequencies of these stacked networks in a parallel anddistributive way.This paper seeks to discover and represent interestingpatterns in an efficient and less complex way by incorporating the concept ofMode synthesizers in the deep learning process models
arxiv-11100-27 | Global Optimality in Tensor Factorization, Deep Learning, and Beyond | http://arxiv.org/pdf/1506.07540v1.pdf | author:Benjamin D. Haeffele, Rene Vidal category:cs.NA cs.LG stat.ML published:2015-06-24 summary:Techniques involving factorization are found in a wide range of applicationsand have enjoyed significant empirical success in many fields. However, commonto a vast majority of these problems is the significant disadvantage that theassociated optimization problems are typically non-convex due to a multilinearform or other convexity destroying transformation. Here we build on ideas fromconvex relaxations of matrix factorizations and present a very generalframework which allows for the analysis of a wide range of non-convexfactorization problems - including matrix factorization, tensor factorization,and deep neural network training formulations. We derive sufficient conditionsto guarantee that a local minimum of the non-convex optimization problem is aglobal minimum and show that if the size of the factorized variables is largeenough then from any initialization it is possible to find a global minimizerusing a purely local descent algorithm. Our framework also provides a partialtheoretical justification for the increasingly common use of Rectified LinearUnits (ReLUs) in deep neural networks and offers guidance on deep networkarchitectures and regularization strategies to facilitate efficientoptimization.
arxiv-11100-28 | Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization | http://arxiv.org/pdf/1506.07512v1.pdf | author:Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford category:stat.ML cs.DS cs.LG published:2015-06-24 summary:We develop a family of accelerated stochastic algorithms that minimize sumsof convex functions. Our algorithms improve upon the fastest running time forempirical risk minimization (ERM), and in particular linear least-squaresregression, across a wide range of problem settings. To achieve this, weestablish a framework based on the classical proximal point algorithm. Namely,we provide several algorithms that reduce the minimization of a strongly convexfunction to approximate minimizations of regularizations of the function. Usingthese results, we accelerate recent fast stochastic algorithms in a black-boxfashion. Empirically, we demonstrate that the resulting algorithms exhibitnotions of stability that are advantageous in practice. Both in theory and inpractice, the provided algorithms reap the computational benefits of adding alarge strongly convex regularization term, without incurring a correspondingbias to the original problem.
arxiv-11100-29 | Objective Variables for Probabilistic Revenue Maximization in Second-Price Auctions with Reserve | http://arxiv.org/pdf/1506.07504v1.pdf | author:Maja R. Rudolph, Joseph G. Ellis, David M. Blei category:stat.ML cs.AI cs.GT cs.LG stat.AP published:2015-06-24 summary:Many online companies sell advertisement space in second-price auctions withreserve. In this paper, we develop a probabilistic method to learn a profitablestrategy to set the reserve price. We use historical auction data with featuresto fit a predictor of the best reserve price. This problem is delicate - thestructure of the auction is such that a reserve price set too high is muchworse than a reserve price set too low. To address this we develop objectivevariables, a new framework for combining probabilistic modeling with optimaldecision-making. Objective variables are "hallucinated observations" thattransform the revenue maximization task into a regularized maximum likelihoodestimation problem, which we solve with an EM algorithm. This framework enablesa variety of prediction mechanisms to set the reserve price. As examples, westudy objective variable methods with regression, kernelized regression, andneural networks on simulated and real data. Our methods outperform previousapproaches both in terms of scalability and profit.
arxiv-11100-30 | Attention-Based Models for Speech Recognition | http://arxiv.org/pdf/1506.07503v1.pdf | author:Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, Yoshua Bengio category:cs.CL cs.LG cs.NE stat.ML published:2015-06-24 summary:Recurrent sequence generators conditioned on input data through an attentionmechanism have recently shown very good performance on a range of tasks in-cluding machine translation, handwriting synthesis and image caption gen-eration. We extend the attention-mechanism with features needed for speechrecognition. We show that while an adaptation of the model used for machinetranslation in reaches a competitive 18.7% phoneme error rate (PER) on theTIMIT phoneme recognition task, it can only be applied to utterances which areroughly as long as the ones it was trained on. We offer a qualitativeexplanation of this failure and propose a novel and generic method of addinglocation-awareness to the attention mechanism to alleviate this issue. The newmethod yields a model that is robust to long inputs and achieves 18% PER insingle utterances and 20% in 10-times longer (repeated) utterances. Finally, wepropose a change to the at- tention mechanism that prevents it fromconcentrating too much on single frames, which further reduces PER to 17.6%level.
arxiv-11100-31 | Efficient Learning for Undirected Topic Models | http://arxiv.org/pdf/1506.07477v1.pdf | author:Jiatao Gu, Victor O. K. Li category:cs.LG cs.CL cs.IR stat.ML published:2015-06-24 summary:Replicated Softmax model, a well-known undirected topic model, is powerful inextracting semantic representations of documents. Traditional learningstrategies such as Contrastive Divergence are very inefficient. This paperprovides a novel estimator to speed up the learning based on Noise ContrastiveEstimate, extended for documents of variant lengths and weighted inputs.Experiments on two benchmarks show that the new estimator achieves greatlearning efficiency and high accuracy on document retrieval and classification.
arxiv-11100-32 | Nonnegative Matrix Factorization applied to reordered pixels of single images based on patches to achieve structured nonnegative dictionaries | http://arxiv.org/pdf/1506.08110v1.pdf | author:Richard M. Charles, Kye M. Taylor, James H. Curry category:cs.CV math.NA 65K02 published:2015-06-24 summary:Recent improvements in computing allow for the processing and analysis ofvery large datasets in a variety of fields. Often the analysis requires thecreation of low-rank approximations to the datasets leading to efficientstorage. This article presents and analyzes a novel approach for creatingnonnegative, structured dictionaries using NMF applied to reordered pixels ofsingle, natural images. We reorder the pixels based on patches and present ourapproach in general. We investigate our approach when using the Singular ValueDecomposition (SVD) and Nonnegative Matrix Factorizations (NMF) as low-rankapproximations. Peak Signal-to-Noise Ratio (PSNR) and Mean StructuralSimilarity Index (MSSIM) are used to evaluate the algorithm. We report thatwhile the SVD provides the best reconstructions, its dictionary of vectors loseboth the sign structure of the original image and details of localized imagecontent. In contrast, the dictionaries produced using NMF preserves the signstructure of the original image matrix and offer a nonnegative, parts-baseddictionary.
arxiv-11100-33 | Highly Accurate Multispectral Palmprint Recognition Using Statistical and Wavelet Features | http://arxiv.org/pdf/1408.3772v2.pdf | author:Shervin Minaee, AmirAli Abdolrashidi category:cs.CV published:2014-08-16 summary:Palmprint is one of the most useful physiological biometrics that can be usedas a powerful means in personal recognition systems. The major features of thepalmprints are palm lines, wrinkles and ridges, and many approaches use them indifferent ways towards solving the palmprint recognition problem. Here we haveproposed to use a set of statistical and wavelet-based features; statistical tocapture the general characteristics of palmprints; and wavelet-based to findthose information not evident in the spatial domain. Also we use two differentclassification approaches, minimum distance classifier scheme and weightedmajority voting algorithm, to perform palmprint matching. The proposed methodis tested on a well-known palmprint dataset of 6000 samples and has shown animpressive accuracy rate of 99.65\%-100\% for most scenarios.
arxiv-11100-34 | Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation | http://arxiv.org/pdf/1506.07452v1.pdf | author:Marijn F. Stollenga, Wonmin Byeon, Marcus Liwicki, Juergen Schmidhuber category:cs.CV cs.LG published:2015-06-24 summary:Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3Dvideos to segment them. They have a fixed input size and typically perceiveonly small local contexts of the pixels to be classified as foreground orbackground. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceivethe entire spatio-temporal context of each pixel in a few sweeps through allpixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despitethese theoretical advantages, however, unlike CNNs, previous MD-LSTM variantswere hard to parallelize on GPUs. Here we re-arrange the traditional cuboidorder of computations in MD-LSTM in pyramidal fashion. The resultingPyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks ofbrain slice images. PyraMiD-LSTM achieved best known pixel-wise brain imagesegmentation results on MRBrainS13 (and competitive results on EM-ISBI12).
arxiv-11100-35 | Unshredding of Shredded Documents: Computational Framework and Implementation | http://arxiv.org/pdf/1506.07440v1.pdf | author:Lei Kristoffer R. Lactuan, Jaderick P. Pabico category:cs.CV published:2015-06-24 summary:A shredded document $D$ is a document whose pages have been cut into stripsfor the purpose of destroying private, confidential, or sensitive information$I$ contained in $D$. Shredding has become a standard means of governmentorganizations, businesses, and private individuals to destroy archival recordsthat have been officially classified for disposal. It can also be used todestroy documentary evidence of wrongdoings by entities who are trying to hide$I$. In this paper, we present an optimal $O((n\times m)^2)$ algorithm $A$ thatreconstructs an $n$-page $D$, where each page $p$ is shredded into $m$ strips.We also present the efficacy of $A$ in reconstructing three document types:hand-written, machine typed-set, and images.
arxiv-11100-36 | Secrets of GrabCut and Kernel K-means | http://arxiv.org/pdf/1506.07439v1.pdf | author:Meng Tang, Ismail Ben Ayed, Dmitrii Marin, Yuri Boykov category:cs.CV published:2015-06-24 summary:The log-likelihood energy term in popular model-fitting segmentation methods,e.g. Zhu-Yuille, Chan-Vese, GrabCut, etc., is presented as a generalized"probabilistic" K-means energy for color space clustering. This interpretationreveals some limitations, e.g. over-fitting. We propose an alternative approachto color clustering using kernel K-means energy with well-known properties suchas non-linear separation and scalability to higher-dimensional feature spaces.Similarly to log-likelihoods, our kernel energy term for color space clusteringcan be combined with image grid regularization, e.g. boundary smoothness, andminimized using (pseudo-) bound optimization and max-flow algorithm. Unlikehistogram or GMM fitting and implicit entropy minimization, our approach isclosely related to general pairwise clustering such as average association andnormalized cut. But, in contrast to previous pairwise clustering algorithms,our approach can incorporate any standard geometric regularization in the imagedomain. We analyze extreme cases for kernel bandwidth (e.g. Gini bias) andpropose adaptive strategies. Our general kernel-based approach opens the doorfor many extensions/applications.
arxiv-11100-37 | Salient Object Detection via Objectness Measure | http://arxiv.org/pdf/1506.07363v1.pdf | author:Sai Srivatsa R, R. Venkatesh Babu category:cs.CV published:2015-06-24 summary:Salient object detection has become an important task in many imageprocessing applications. The existing approaches exploit background prior andcontrast prior to attain state of the art results. In this paper, instead ofusing background cues, we estimate the foreground regions in an image usingobjectness proposals and utilize it to obtain smooth and accurate saliencymaps. We propose a novel saliency measure called `foreground connectivity'which determines how tightly a pixel or a region is connected to the estimatedforeground. We use the values assigned by this measure as foreground weightsand integrate these in an optimization framework to obtain the final saliencymaps. We extensively evaluate the proposed approach on two benchmark databasesand demonstrate that the results obtained are better than the existing state ofthe art approaches.
arxiv-11100-38 | Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT | http://arxiv.org/pdf/1405.5769v2.pdf | author:Philipp Fischer, Alexey Dosovitskiy, Thomas Brox category:cs.CV cs.LG published:2014-05-22 summary:Latest results indicate that features learned via convolutional neuralnetworks outperform previous descriptors on classification tasks by a largemargin. It has been shown that these networks still work well when they areapplied to datasets or recognition tasks different from those they were trainedon. However, descriptors like SIFT are not only used in recognition but alsofor many correspondence problems that rely on descriptor matching. In thispaper we compare features from various layers of convolutional neural nets tostandard SIFT descriptors. We consider a network that was trained on ImageNetand another one that was trained without supervision. Surprisingly,convolutional neural networks clearly outperform SIFT on descriptor matching.This paper has been merged with arXiv:1406.6909
arxiv-11100-39 | Natural Scene Recognition Based on Superpixels and Deep Boltzmann Machines | http://arxiv.org/pdf/1506.07271v1.pdf | author:Jinfu Yang, Jingyu Gao, Guanghui Wang, Shanshan Zhang category:cs.CV published:2015-06-24 summary:The Deep Boltzmann Machines (DBM) is a state-of-the-art unsupervised learningmodel, which has been successfully applied to handwritten digit recognitionand, as well as object recognition. However, the DBM is limited in scenerecognition due to the fact that natural scene images are usually very large.In this paper, an efficient scene recognition approach is proposed based onsuperpixels and the DBMs. First, a simple linear iterative clustering (SLIC)algorithm is employed to generate superpixels of input images, where eachsuperpixel is regarded as an input of a learning model. Then, a two-layer DBMmodel is constructed by stacking two restricted Boltzmann machines (RBMs), anda greedy layer-wise algorithm is applied to train the DBM model. Finally, asoftmax regression is utilized to categorize scene images. The proposedtechnique can effectively reduce the computational complexity and enhance theperformance for large natural image recognition. The approach is verified andevaluated by extensive experiments, including the fifteen-scene categoriesdataset the UIUC eight-sports dataset, and the SIFT flow dataset, are used toevaluate the proposed method. The experimental results show that the proposedapproach outperforms other state-of-the-art methods in terms of recognitionrate.
arxiv-11100-40 | A Novel Feature Extraction Method for Scene Recognition Based on Centered Convolutional Restricted Boltzmann Machines | http://arxiv.org/pdf/1506.07257v1.pdf | author:Jingyu Gao, Jinfu Yang, Guanghui Wang, Mingai Li category:cs.CV published:2015-06-24 summary:Scene recognition is an important research topic in computer vision, whilefeature extraction is a key step of object recognition. Although classicalRestricted Boltzmann machines (RBM) can efficiently represent complicated data,it is hard to handle large images due to its complexity in computation. In thispaper, a novel feature extraction method, named Centered ConvolutionalRestricted Boltzmann Machines (CCRBM), is proposed for scene recognition. Theproposed model is an improved Convolutional Restricted Boltzmann Machines(CRBM) by introducing centered factors in its learning strategy to reduce thesource of instabilities. First, the visible units of the network are redefinedusing centered factors. Then, the hidden units are learned with a modifiedenergy function by utilizing a distribution function, and the visible units arereconstructed using the learned hidden units. In order to achieve bettergenerative ability, the Centered Convolutional Deep Belief Networks (CCDBN) istrained in a greedy layer-wise way. Finally, a softmax regression isincorporated for scene recognition. Extensive experimental evaluations usingnatural scenes, MIT-indoor scenes, and Caltech 101 datasets show that theproposed approach performs better than other counterparts in terms ofstability, generalization, and discrimination. The CCDBN model is more suitablefor natural scene image recognition by virtue of convolutional property.
arxiv-11100-41 | Unconfused ultraconservative multiclass algorithms | http://arxiv.org/pdf/1506.07254v1.pdf | author:Ugo Louche, Liva Ralaivola category:cs.LG published:2015-06-24 summary:We tackle the problem of learning linear classifiers from noisy datasets in amulticlass setting. The two-class version of this problem was studied a fewyears ago where the proposed approaches to combat the noise revolve around aPer-ceptron learning scheme fed with peculiar examples computed through aweighted average of points from the noisy training set. We propose to buildupon these approaches and we introduce a new algorithm called UMA (forUnconfused Multiclass additive Algorithm) which may be seen as a generalizationto the multiclass setting of the previous approaches. In order to characterizethe noise we use the confusion matrix as a multiclass extension of theclassification noise studied in the aforemen-tioned literature. Theoreticallywell-founded, UMA furthermore displays very good empirical noise robustness, asevidenced by numerical simulations conducted on both synthetic and real data.
arxiv-11100-42 | Benchmark of structured machine learning methods for microbial identification from mass-spectrometry data | http://arxiv.org/pdf/1506.07251v1.pdf | author:Kévin Vervier, Pierre Mahé, Jean-Baptiste Veyrieras, Jean-Philippe Vert category:stat.ML cs.LG q-bio.QM published:2015-06-24 summary:Microbial identification is a central issue in microbiology, in particular inthe fields of infectious diseases diagnosis and industrial quality control. Theconcept of species is tightly linked to the concept of biological and clinicalclassification where the proximity between species is generally measured interms of evolutionary distances and/or clinical phenotypes. Surprisingly, theinformation provided by this well-known hierarchical structure is rarely usedby machine learning-based automatic microbial identification systems.Structured machine learning methods were recently proposed for taking intoaccount the structure embedded in a hierarchy and using it as additional apriori information, and could therefore allow to improve microbialidentification systems. We test and compare several state-of-the-art machinelearning methods for microbial identification on a new Matrix-Assisted LaserDesorption/Ionization Time-of-Flight mass spectrometry (MALDI-TOF MS) dataset.We include in the benchmark standard and structured methods, that leverage theknowledge of the underlying hierarchical structure in the learning process. Ourresults show that although some methods perform better than others, structuredmethods do not consistently perform better than their "flat" counterparts. Wepostulate that this is partly due to the fact that standard methods alreadyreach a high level of accuracy in this context, and that they mainly confusespecies close to each other in the tree, a case where using the known hierarchyis not helpful.
arxiv-11100-43 | Deep CNN Ensemble with Data Augmentation for Object Detection | http://arxiv.org/pdf/1506.07224v1.pdf | author:Jian Guo, Stephen Gould category:cs.CV published:2015-06-24 summary:We report on the methods used in our recent DeepEnsembleCoco submission tothe PASCAL VOC 2012 challenge, which achieves state-of-the-art performance onthe object detection task. Our method is a variant of the R-CNN model proposedGirshick:CVPR14 with two key improvements to training and evaluation. First,our method constructs an ensemble of deep CNN models with differentarchitectures that are complementary to each other. Second, we augment thePASCAL VOC training set with images from the Microsoft COCO dataset tosignificantly enlarge the amount training data. Importantly, we select a subsetof the Microsoft COCO images to be consistent with the PASCAL VOC task. Resultson the PASCAL VOC evaluation server show that our proposed method outperformall previous methods on the PASCAL VOC 2012 detection task at time ofsubmission.
arxiv-11100-44 | Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks | http://arxiv.org/pdf/1506.07220v1.pdf | author:Yangtuo Peng, Hui Jiang category:cs.CE cs.AI cs.CL published:2015-06-24 summary:Financial news contains useful information on public companies and themarket. In this paper we apply the popular word embedding methods and deepneural networks to leverage financial news to predict stock price movements inthe market. Experimental results have shown that our proposed methods aresimple but very effective, which can significantly improve the stock predictionaccuracy on a standard financial database over the baseline system using onlythe historical price information.
arxiv-11100-45 | deltaBLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets | http://arxiv.org/pdf/1506.06863v2.pdf | author:Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao, Bill Dolan category:cs.CL published:2015-06-23 summary:We introduce Discriminative BLEU (deltaBLEU), a novel metric for intrinsicevaluation of generated text in tasks that admit a diverse range of possibleoutputs. Reference strings are scored for quality by human raters on a scale of[-1, +1] to weight multi-reference BLEU. In tasks involving generation ofconversational responses, deltaBLEU correlates reasonably with human judgmentsand outperforms sentence-level and IBM BLEU in terms of both Spearman's rho andKendall's tau.
arxiv-11100-46 | Context-Dependent Translation Selection Using Convolutional Neural Network | http://arxiv.org/pdf/1503.02357v2.pdf | author:Zhaopeng Tu, Baotian Hu, Zhengdong Lu, Hang Li category:cs.CL cs.LG cs.NE published:2015-03-09 summary:We propose a novel method for translation selection in statistical machinetranslation, in which a convolutional neural network is employed to judge thesimilarity between a phrase pair in two languages. The specifically designedconvolutional architecture encodes not only the semantic similarity of thetranslation pair, but also the context containing the phrase in the sourcelanguage. Therefore, our approach is able to capture context-dependent semanticsimilarities of translation pairs. We adopt a curriculum learning strategy totrain the model: we classify the training examples into easy, medium, anddifficult categories, and gradually build the ability of representing phraseand sentence level context by using training examples from easy to difficult.Experimental results show that our approach significantly outperforms thebaseline system by up to 1.4 BLEU points.
arxiv-11100-47 | On Elicitation Complexity and Conditional Elicitation | http://arxiv.org/pdf/1506.07212v1.pdf | author:Rafael Frongillo, Ian A. Kash category:cs.LG math.OC math.ST q-fin.MF stat.TH published:2015-06-23 summary:Elicitation is the study of statistics or properties which are computable viaempirical risk minimization. While several recent papers have approached thegeneral question of which properties are elicitable, we suggest that this isthe wrong question---all properties are elicitable by first eliciting theentire distribution or data set, and thus the important question is howelicitable. Specifically, what is the minimum number of regression parametersneeded to compute the property? Building on previous work, we introduce a new notion of elicitationcomplexity and lay the foundations for a calculus of elicitation. We establishseveral general results and techniques for proving upper and lower bounds onelicitation complexity. These results provide tight bounds for eliciting theBayes risk of any loss, a large class of properties which includes spectralrisk measures and several new properties of interest. Finally, we extend ourcalculus to conditionally elicitable properties, which are elicitableconditioned on knowing the value of another property, giving a necessarycondition for the elicitability of both properties together.
arxiv-11100-48 | Clustering, Classification, Discriminant Analysis, and Dimension Reduction via Generalized Hyperbolic Mixtures | http://arxiv.org/pdf/1308.6315v3.pdf | author:Katherine Morris, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2013-08-28 summary:A method for dimension reduction with clustering, classification, ordiscriminant analysis is introduced. This mixture model-based approach is basedon fitting generalized hyperbolic mixtures on a reduced subspace within theparadigm of model-based clustering, classification, or discriminant analysis. Areduced subspace of the data is derived by considering the extent to whichgroup means and group covariances vary. The members of the subspace arisethrough linear combinations of the original data, and are ordered by importancevia the associated eigenvalues. The observations can be projected onto thesubspace, resulting in a set of variables that captures most of the clusteringinformation available. The use of generalized hyperbolic mixtures gives arobust framework capable of dealing with skewed clusters. Although dimensionreduction is increasingly in demand across many application areas, the authorsare most familiar with biological applications and so two of the five real dataexamples are within that sphere. Simulated data are also used for illustration.The approach introduced herein can be considered the most general such approachavailable, and so we compare results to three special and limiting cases.Comparisons with several well established techniques illustrate its promisingperformance.
arxiv-11100-49 | Multi-domain Dialog State Tracking using Recurrent Neural Networks | http://arxiv.org/pdf/1506.07190v1.pdf | author:Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gašić, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, Steve Young category:cs.CL cs.LG published:2015-06-23 summary:Dialog state tracking is a key component of many modern dialog systems, mostof which are designed with a single, well-defined domain in mind. This papershows that dialog data drawn from different dialog domains can be used to traina general belief tracking model which can operate across all of these domains,exhibiting superior performance to each of the domain-specific models. Wepropose a training procedure which uses out-of-domain data to initialise belieftracking models for entirely new domains. This procedure leads to improvementsin belief tracking performance regardless of the amount of in-domain dataavailable for training the model.
arxiv-11100-50 | Segmentation of Three-dimensional Images with Parametric Active Surfaces and Topology Changes | http://arxiv.org/pdf/1506.07136v1.pdf | author:Heike Benninghoff, Harald Garcke category:cs.CV published:2015-06-23 summary:In this paper, we introduce a novel parametric method for segmentation ofthree-dimensional images. We consider a piecewise constant version of theMumford-Shah and the Chan-Vese functionals and perform a region-basedsegmentation of 3D image data. An evolution law is derived from energyminimization problems which push the surfaces to the boundaries of 3D objectsin the image. We propose a parametric scheme which describes the evolution ofparametric surfaces. An efficient finite element scheme is proposed for anumerical approximation of the evolution equations. Since standard parametricmethods cannot handle topology changes automatically, an efficient method ispresented to detect, identify and perform changes in the topology of thesurfaces. One main focus of this paper are the algorithmic details to handletopology changes like splitting and merging of surfaces and change of the genusof a surface. Different artificial images are studied to demonstrate theability to detect the different types of topology changes. Finally, theparametric method is applied to segmentation of medical 3D images.
arxiv-11100-51 | Coercive functions from a topological viewpoint and properties of minimizing sets of convex functions appearing in image restoration | http://arxiv.org/pdf/1506.08615v1.pdf | author:René Ciak category:math.OC cs.CV math.CA math.FA published:2015-06-23 summary:Many tasks in image processing can be tackled by modeling an appropriate datafidelity term $\Phi: \mathbb{R}^n \rightarrow \mathbb{R} \cup \{+\infty\}$ andthen solve one of the regularized minimization problems \begin{align*} &{}(P_{1,\tau}) \qquad \mathop{\rm argmin}_{x \in \mathbb R^n} \big\{ \Phi(x)\;{\rm s.t.}\; \Psi(x) \leq \tau \big\} \\ &{}(P_{2,\lambda}) \qquad\mathop{\rm argmin}_{x \in \mathbb R^n} \{ \Phi(x) + \lambda \Psi(x) \}, \;\lambda > 0 \end{align*} with some function $\Psi: \mathbb{R}^n \rightarrow\mathbb{R} \cup \{+\infty\}$ and a good choice of the parameter(s). Two tasksarise naturally here: \begin{align*} {}& \text{1. Study the solver sets ${\rmSOL}(P_{1,\tau})$ and ${\rm SOL}(P_{2,\lambda})$ of the minimization problems.} \\ {}& \text{2.Ensure that the minimization problems have solutions.} \end{align*} This thesisprovides contributions to both tasks: Regarding the first task for a morespecial setting we prove that there are intervals $(0,c)$ and $(0,d)$ such thatthe setvalued curves \begin{align*} \tau \mapsto {}& {\rm SOL}(P_{1,\tau}), \; \tau \in (0,c) \\ {} \lambda\mapsto {}& {\rm SOL}(P_{2,\lambda}), \; \lambda \in (0,d) \end{align*} are thesame, besides an order reversing parameter change $g: (0,c) \rightarrow (0,d)$.Moreover we show that the solver sets are changing all the time while $\tau$runs from $0$ to $c$ and $\lambda$ runs from $d$ to $0$. In the presence of lower semicontinuity the second task is done if we haveadditionally coercivity. We regard lower semicontinuity and coercivity from atopological point of view and develop a new technique for proving lowersemicontinuity plus coercivity. Dropping any lower semicontinuity assumption we also prove a theorem on thecoercivity of a sum of functions.
arxiv-11100-52 | Improving Fiber Alignment in HARDI by Combining Contextual PDE Flow with Constrained Spherical Deconvolution | http://arxiv.org/pdf/1506.07062v1.pdf | author:J. M. Portegies, R. H. J. Fick, G. R. Sanguinetti, S. P. L. Meesters, G. Girard, R. Duits category:cs.CV published:2015-06-23 summary:We propose two strategies to improve the quality of tractography resultscomputed from diffusion weighted magnetic resonance imaging (DW-MRI) data. Bothmethods are based on the same PDE framework, defined in the coupled space ofpositions and orientations, associated with a stochastic process describing theenhancement of elongated structures while preserving crossing structures. Inthe first method we use the enhancement PDE for contextual regularization of afiber orientation distribution (FOD) that is obtained on individual voxels fromhigh angular resolution diffusion imaging (HARDI) data via constrainedspherical deconvolution (CSD). Thereby we improve the FOD as input forsubsequent tractography. Secondly, we introduce the fiber to bundle coherence(FBC), a measure for quantification of fiber alignment. The FBC is computedfrom a tractography result using the same PDE framework and provides acriterion for removing the spurious fibers. We validate the proposedcombination of CSD and enhancement on phantom data and on human data, acquiredwith different scanning protocols. On the phantom data we find that PDEenhancements improve both local metrics and global metrics of tractographyresults, compared to CSD without enhancements. On the human data we show thatthe enhancements allow for a better reconstruction of crossing fiber bundlesand they reduce the variability of the tractography output with respect to theacquisition parameters. Finally, we show that both the enhancement of the FODsand the use of the FBC measure on the tractography improve the stability withrespect to different stochastic realizations of probabilistic tractography.This is shown in a clinical application: the reconstruction of the opticradiation for epilepsy surgery planning.
arxiv-11100-53 | R-CNN minus R | http://arxiv.org/pdf/1506.06981v1.pdf | author:Karel Lenc, Andrea Vedaldi category:cs.CV published:2015-06-23 summary:Deep convolutional neural networks (CNNs) have had a major impact in mostareas of image understanding, including object category detection. In objectdetection, methods such as R-CNN have obtained excellent results by integratingCNNs with region proposal generation algorithms such as selective search. Inthis paper, we investigate the role of proposal generation in CNN-baseddetectors in order to determine whether it is a necessary modelling component,carrying essential geometric information not contained in the CNN, or whetherit is merely a way of accelerating detection. We do so by designing andevaluating a detector that uses a trivial region generation scheme, constantfor each image. Combined with SPP, this results in an excellent and fastdetector that does not require to process an image with algorithms other thanthe CNN itself. We also streamline and simplify the training of CNN-baseddetectors by integrating several learning steps in a single algorithm, as wellas by proposing a number of improvements that accelerate detection.
arxiv-11100-54 | Efficient approximate Bayesian inference for models with intractable likelihoods | http://arxiv.org/pdf/1506.06975v1.pdf | author:Johan Dahlin, Mattias Villani, Thomas B. Schön category:stat.CO q-fin.RM stat.ML published:2015-06-23 summary:We consider the problem of approximate Bayesian parameter inference innonlinear state space models with intractable likelihoods. Sequential MonteCarlo with approximate Bayesian computations (SMC-ABC) is an approach toapproximate the likelihood in this type of models. However, such approximationscan be noisy and computationally costly which hinders efficient implementationsusing standard methods based on optimisation and statistical simulation. Wepropose a novel method based on the combination of Gaussian processoptimisation (GPO) and SMC-ABC to create a Laplace approximation of theintractable posterior. The properties of the resulting GPO-ABC method arestudied using stochastic volatility (SV) models with both synthetic andreal-world data. We conclude that the algorithm enjoys: good accuracycomparable to particle Markov chain Monte Carlo with a significant reduction incomputational cost and better robustness to noise in the estimates comparedwith a gradient-based optimisation algorithm. Finally, we make use of GPO-ABCto estimate the Value-at-Risk for a portfolio using a copula model with SVmodels for the margins.
arxiv-11100-55 | GEFCOM 2014 - Probabilistic Electricity Price Forecasting | http://arxiv.org/pdf/1506.06972v1.pdf | author:Gergo Barta, Gyula Borbely, Gabor Nagy, Sandor Kazi, Tamas Henk category:stat.ML cs.CE cs.LG stat.AP published:2015-06-23 summary:Energy price forecasting is a relevant yet hard task in the field ofmulti-step time series forecasting. In this paper we compare a well-known andestablished method, ARMA with exogenous variables with a relatively newtechnique Gradient Boosting Regression. The method was tested on data fromGlobal Energy Forecasting Competition 2014 with a year long rolling windowforecast. The results from the experiment reveal that a multi-model approach issignificantly better performing in terms of error metrics. Gradient Boostingcan deal with seasonality and auto-correlation out-of-the box and achieve lowerrate of normalized mean absolute error on real-world data.
arxiv-11100-56 | Graphs in machine learning: an introduction | http://arxiv.org/pdf/1506.06962v1.pdf | author:Pierre Latouche, Fabrice Rossi category:stat.ML cs.LG cs.SI physics.soc-ph published:2015-06-23 summary:Graphs are commonly used to characterise interactions between objects ofinterest. Because they are based on a straightforward formalism, they are usedin many scientific fields from computer science to historical sciences. In thispaper, we give an introduction to some methods relying on graphs for learning.This includes both unsupervised and supervised methods. Unsupervised learningalgorithms usually aim at visualising graphs in latent spaces and/or clusteringthe nodes. Both focus on extracting knowledge from graph topologies. While mostexisting techniques are only applicable to static graphs, where edges do notevolve through time, recent developments have shown that they could be extendedto deal with evolving networks. In a supervised context, one generally aims atinferring labels or numerical values attached to nodes using both the graphand, when they are available, node characteristics. Balancing the two sourcesof information can be challenging, especially as they can disagree locally orglobally. In both contexts, supervised and un-supervised, data can berelational (augmented with one or several global graphs) as described above, orgraph valued. In this latter case, each object of interest is given as a fullgraph (possibly completed by other characteristics). In this context, naturaltasks include graph clustering (as in producing clusters of graphs rather thanclusters of nodes in a single graph), graph classification, etc. 1 Realnetworks One of the first practical studies on graphs can be dated back to theoriginal work of Moreno [51] in the 30s. Since then, there has been a growinginterest in graph analysis associated with strong developments in the modellingand the processing of these data. Graphs are now used in many scientificfields. In Biology [54, 2, 7], for instance, metabolic networks can describepathways of biochemical reactions [41], while in social sciences networks areused to represent relation ties between actors [66, 56, 36, 34]. Other examplesinclude powergrids [71] and the web [75]. Recently, networks have also beenconsidered in other areas such as geography [22] and history [59, 39]. Inmachine learning, networks are seen as powerful tools to model problems inorder to extract information from data and for prediction purposes. This is theobject of this paper. For more complete surveys, we refer to [28, 62, 49, 45].In this section, we introduce notations and highlight properties shared by mostreal networks. In Section 2, we then consider methods aiming at extractinginformation from a unique network. We will particularly focus on clusteringmethods where the goal is to find clusters of vertices. Finally, in Section 3,techniques that take a series of networks into account, where each network is
arxiv-11100-57 | Individual Biases, Cultural Evolution, and the Statistical Nature of Language Universals: The Case of Colour Naming Systems | http://arxiv.org/pdf/1310.7782v2.pdf | author:Andrea Baronchelli, Vittorio Loreto, Andrea Puglisi category:physics.soc-ph cs.CL cs.MA q-bio.PE published:2013-10-29 summary:Language universals have long been attributed to an innate Universal Grammar.An alternative explanation states that linguistic universals emergedindependently in every language in response to shared cognitive or perceptualbiases. A computational model has recently shown how this could be the case,focusing on the paradigmatic example of the universal properties of colournaming patterns, and producing results in quantitative agreement with theexperimental data. Here we investigate the role of an individual perceptualbias in the framework of the model. We study how, and to what extent, thestructure of the bias influences the corresponding linguistic universalpatterns. We show that the cultural history of a group of speakers introducespopulation-specific constraints that act against the pressure for uniformityarising from the individual bias, and we clarify the interplay between thesetwo forces.
arxiv-11100-58 | Random Forest for the Contextual Bandit Problem - extended version | http://arxiv.org/pdf/1504.06952v19.pdf | author:Raphaël Féraud, Robin Allesiardo, Tanguy Urvoy, Fabrice Clérot category:cs.LG published:2015-04-27 summary:To address the contextual bandit problem, we propose an online random forestalgorithm. The analysis of the proposed algorithm is based on the samplecomplexity needed to find the optimal decision stump. Then, the decision stumpsare assembled in a random collection of decision trees, Bandit Forest. We showthat the proposed algorithm is optimal up to logarithmic factors. Thedependence of the sample complexity upon the number of contextual variables islogarithmic. The computational cost of the proposed algorithm with respect tothe time horizon is linear. These analytical results allow the proposedalgorithm to be efficient in real applications, where the number of events toprocess is huge, and where we expect that some contextual variables, chosenfrom a large set, have potentially non- linear dependencies with the rewards.In the experiments done to illustrate the theoretical analysis, Bandit Forestobtain promising results in comparison with state-of-the-art algorithms.
arxiv-11100-59 | Differentially Private Distributed Online Learning | http://arxiv.org/pdf/1505.06556v2.pdf | author:Chencheng Li, Pan Zhou category:cs.LG published:2015-05-25 summary:Online learning has been in the spotlight from the machine learning societyfor a long time. To handle massive data in Big Data era, one single learnercould never efficiently finish this heavy task. Hence, in this paper, wepropose a novel distributed online learning algorithm to solve the problem.Comparing to typical centralized online learner, the distributed learnersoptimize their own learning parameters based on local data sources and timelycommunicate with neighbors. However, communication may lead to a privacybreach. Thus, we use differential privacy to preserve the privacy of learners,and study the influence of guaranteeing differential privacy on the utility ofthe distributed online learning algorithm. Furthermore, by using the resultsfrom Kakade and Tewari (2009), we use the regret bounds of online learning toachieve fast convergence rates for offline learning algorithms in distributedscenarios, which provides tighter utility performance than the existingstate-of-the-art results. In simulation, we demonstrate that the differentiallyprivate offline learning algorithm has high variance, but we can use mini-batchto improve the performance. Finally, the simulations show that the analyticalresults of our proposed theorems are right and our private distributed onlinelearning algorithm is a general framework.
arxiv-11100-60 | Person re-identification via efficient inference in fully connected CRF | http://arxiv.org/pdf/1506.06905v1.pdf | author:Jiuqing Wan, Menglin Xing category:cs.CV published:2015-06-23 summary:In this paper, we address the problem of person re-identification problem,i.e., retrieving instances from gallery which are generated by the same personas the given probe image. This is very challenging because the person'sappearance usually undergoes significant variations due to changes inillumination, camera angle and view, background clutter, and occlusion over thecamera network. In this paper, we assume that the matched gallery images shouldnot only be similar to the probe, but also be similar to each other, undersuitable metric. We express this assumption with a fully connected CRF model inwhich each node corresponds to a gallery and every pair of nodes are connectedby an edge. A label variable is associated with each node to indicate whetherthe corresponding image is from target person. We define unary potential foreach node using existing feature calculation and matching techniques, whichreflect the similarity between probe and gallery image, and define pairwisepotential for each edge in terms of a weighed combination of Gaussian kernels,which encode appearance similarity between pair of gallery images. The specificform of pairwise potential allows us to exploit an efficient inferencealgorithm to calculate the marginal distribution of each label variable forthis dense connected CRF. We show the superiority of our method by applying itto public datasets and comparing with the state of the art.
arxiv-11100-61 | New Approach to translation of Isolated Units in English-Korean Machine Translation | http://arxiv.org/pdf/1506.06904v1.pdf | author:Kim Song Jon, An Hae Gum category:cs.CL published:2015-06-23 summary:It is the most effective way for quick translation of tremendous amount ofexplosively increasing science and technique information material to develop apracticable machine translation system and introduce it into translationpractice. This essay treats problems arising from translation of isolated unitson the basis of the practical materials and experiments obtained in thedevelopment and introduction of English-Korean machine translation system. Inother words, this essay considers establishment of information for isolatedunits and their Korean equivalents and word order.
arxiv-11100-62 | SALSA: A Novel Dataset for Multimodal Group Behavior Analysis | http://arxiv.org/pdf/1506.06882v1.pdf | author:Xavier Alameda-Pineda, Jacopo Staiano, Ramanathan Subramanian, Ligia Batrinca, Elisa Ricci, Bruno Lepri, Oswald Lanz, Nicu Sebe category:cs.CV published:2015-06-23 summary:Studying free-standing conversational groups (FCGs) in unstructured socialsettings (e.g., cocktail party ) is gratifying due to the wealth of informationavailable at the group (mining social networks) and individual (recognizingnative behavioral and personality traits) levels. However, analyzing socialscenes involving FCGs is also highly challenging due to the difficulty inextracting behavioral cues such as target locations, their speaking activityand head/body pose due to crowdedness and presence of extreme occlusions. Tothis end, we propose SALSA, a novel dataset facilitating multimodal andSynergetic sociAL Scene Analysis, and make two main contributions to researchon automated social interaction analysis: (1) SALSA records social interactionsamong 18 participants in a natural, indoor environment for over 60 minutes,under the poster presentation and cocktail party contexts presentingdifficulties in the form of low-resolution images, lighting variations,numerous occlusions, reverberations and interfering sound sources; (2) Toalleviate these problems we facilitate multimodal analysis by recording thesocial interplay using four static surveillance cameras and sociometric badgesworn by each participant, comprising the microphone, accelerometer, bluetoothand infrared sensors. In addition to raw data, we also provide annotationsconcerning individuals' personality as well as their position, head, bodyorientation and F-formation information over the entire event duration. Throughextensive experiments with state-of-the-art approaches, we show (a) thelimitations of current methods and (b) how the recorded multiple cuessynergetically aid automatic analysis of social interactions. SALSA isavailable at http://tev.fbk.eu/salsa.
arxiv-11100-63 | Automatic vehicle tracking and recognition from aerial image sequences | http://arxiv.org/pdf/1506.06881v1.pdf | author:Ognjen Arandjelovic category:cs.CV published:2015-06-23 summary:This paper addresses the problem of automated vehicle tracking andrecognition from aerial image sequences. Motivated by its successes in theexisting literature focus on the use of linear appearance subspaces to describemulti-view object appearance and highlight the challenges involved in theirapplication as a part of a practical system. A working solution which includessteps for data extraction and normalization is described. In experiments onreal-world data the proposed methodology achieved promising results with a highcorrect recognition rate and few, meaningful errors (type II errors wherebygenuinely similar targets are sometimes being confused with one another).Directions for future research and possible improvements of the proposed methodare discussed.
arxiv-11100-64 | Autonomous 3D Reconstruction Using a MAV | http://arxiv.org/pdf/1506.06876v1.pdf | author:Alexander Popov, Dimitrios Zermas, Nikolaos Papanikolopoulos category:cs.CV published:2015-06-23 summary:An approach is proposed for high resolution 3D reconstruction of an objectusing a Micro Air Vehicle (MAV). A system is described which autonomouslycaptures images and performs a dense 3D reconstruction via structure frommotion with no prior knowledge of the environment. Only the MAVs own sensors,the front facing camera and the Inertial Measurement Unit (IMU) are utilized.Precision agriculture is considered as an example application for the system.
arxiv-11100-65 | Learning Discriminative Bayesian Networks from High-dimensional Continuous Neuroimaging Data | http://arxiv.org/pdf/1506.06868v1.pdf | author:Luping Zhou, Lei Wang, Lingqiao Liu, Philip Ogunbona, Dinggang Shen category:cs.CV cs.LG published:2015-06-23 summary:Due to its causal semantics, Bayesian networks (BN) have been widely employedto discover the underlying data relationship in exploratory studies, such asbrain research. Despite its success in modeling the probability distribution ofvariables, BN is naturally a generative model, which is not necessarilydiscriminative. This may cause the ignorance of subtle but critical networkchanges that are of investigation values across populations. In this paper, wepropose to improve the discriminative power of BN models for continuousvariables from two different perspectives. This brings two generaldiscriminative learning frameworks for Gaussian Bayesian networks (GBN). In thefirst framework, we employ Fisher kernel to bridge the generative models of GBNand the discriminative classifiers of SVMs, and convert the GBN parameterlearning to Fisher kernel learning via minimizing a generalization error boundof SVMs. In the second framework, we employ the max-margin criterion and buildit directly upon GBN models to explicitly optimize the classificationperformance of the GBNs. The advantages and disadvantages of the two frameworksare discussed and experimentally compared. Both of them demonstrate strongpower in learning discriminative parameters of GBNs for neuroimaging basedbrain network analysis, as well as maintaining reasonable representationcapacity. The contributions of this paper also include a new Directed AcyclicGraph (DAG) constraint with theoretical guarantee to ensure the graph validityof GBN.
arxiv-11100-66 | A Feature-Based Analysis on the Impact of Set of Constraints for e-Constrained Differential Evolution | http://arxiv.org/pdf/1506.06848v1.pdf | author:Shayan Poursoltan, FranK Neumann category:cs.NE published:2015-06-23 summary:Different types of evolutionary algorithms have been developed forconstrained continuous optimization. We carry out a feature-based analysis ofevolved constrained continuous optimization instances to understand thecharacteristics of constraints that make problems hard for evolutionaryalgorithm. In our study, we examine how various sets of constraints caninfluence the behaviour of e-Constrained Differential Evolution. Investigatingthe evolved instances, we obtain knowledge of what type of constraints andtheir features make a problem difficult for the examined algorithm.
arxiv-11100-67 | Modality-dependent Cross-media Retrieval | http://arxiv.org/pdf/1506.06628v2.pdf | author:Yunchao Wei, Yao Zhao, Zhenfeng Zhu, Shikui Wei, Yanhui Xiao, Jiashi Feng, Shuicheng Yan category:cs.CV cs.IR cs.LG published:2015-06-22 summary:In this paper, we investigate the cross-media retrieval between images andtext, i.e., using image to search text (I2T) and using text to search images(T2I). Existing cross-media retrieval methods usually learn one couple ofprojections, by which the original features of images and text can be projectedinto a common latent space to measure the content similarity. However, usingthe same projections for the two different retrieval tasks (I2T and T2I) maylead to a tradeoff between their respective performances, rather than theirbest performances. Different from previous works, we propose amodality-dependent cross-media retrieval (MDCR) model, where two couples ofprojections are learned for different cross-media retrieval tasks instead ofone couple of projections. Specifically, by jointly optimizing the correlationbetween images and text and the linear regression from one modal space (imageor text) to the semantic space, two couples of mappings are learned to projectimages and text from their original feature spaces into two common latentsubspaces (one for I2T and the other for T2I). Extensive experiments show thesuperiority of the proposed MDCR compared with other methods. In particular,based the 4,096 dimensional convolutional neural network (CNN) visual featureand 100 dimensional LDA textual feature, the mAP of the proposed methodachieves 41.5\%, which is a new state-of-the-art performance on the Wikipediadataset.
arxiv-11100-68 | Detection and Analysis of Emotion From Speech Signals | http://arxiv.org/pdf/1506.06832v1.pdf | author:Assel Davletcharova, Sherin Sugathan, Bibia Abraham, Alex Pappachen James category:cs.SD cs.CL cs.HC published:2015-06-23 summary:Recognizing emotion from speech has become one the active research themes inspeech processing and in applications based on human-computer interaction. Thispaper conducts an experimental study on recognizing emotions from human speech.The emotions considered for the experiments include neutral, anger, joy andsadness. The distinuishability of emotional features in speech were studiedfirst followed by emotion classification performed on a custom dataset. Theclassification was performed for different classifiers. One of the main featureattribute considered in the prepared dataset was the peak-to-peak distanceobtained from the graphical representation of the speech signals. Afterperforming the classification tests on a dataset formed from 30 differentsubjects, it was found that for getting better accuracy, one should considerthe data collected from one person rather than considering the data from agroup of people.
arxiv-11100-69 | DeepStereo: Learning to Predict New Views from the World's Imagery | http://arxiv.org/pdf/1506.06825v1.pdf | author:John Flynn, Ivan Neulander, James Philbin, Noah Snavely category:cs.CV published:2015-06-22 summary:Deep networks have recently enjoyed enormous success when applied torecognition and classification problems in computer vision, but their use ingraphics problems has been limited. In this work, we present a novel deeparchitecture that performs new view synthesis directly from pixels, trainedfrom a large number of posed image sets. In contrast to traditional approacheswhich consist of multiple complex stages of processing, each of which requirecareful tuning and can fail in unexpected ways, our system is trainedend-to-end. The pixels from neighboring views of a scene are presented to thenetwork which then directly produces the pixels of the unseen view. Thebenefits of our approach include generality (we only require posed image setsand can easily apply our method to different domains), and high quality resultson traditionally difficult scenes. We believe this is due to the end-to-endnature of our system which is able to plausibly generate pixels according tocolor, depth, and texture priors learnt automatically from the training data.To verify our method we show that it can convincingly reproduce known testviews from nearby imagery. Additionally we show images rendered from novelviewpoints. To our knowledge, our work is the first to apply deep learning tothe problem of new view synthesis from sets of real-world, natural imagery.
arxiv-11100-70 | Parallel training of DNNs with Natural Gradient and Parameter Averaging | http://arxiv.org/pdf/1410.7455v8.pdf | author:Daniel Povey, Xiaohui Zhang, Sanjeev Khudanpur category:cs.NE cs.LG stat.ML published:2014-10-27 summary:We describe the neural-network training framework used in the Kaldi speechrecognition toolkit, which is geared towards training DNNs with large amountsof training data using multiple GPU-equipped or multi-core machines. In orderto be as hardware-agnostic as possible, we needed a way to use multiplemachines without generating excessive network traffic. Our method is to averagethe neural network parameters periodically (typically every minute or two), andredistribute the averaged parameters to the machines for further training. Eachmachine sees different data. By itself, this method does not work very well.However, we have another method, an approximate and efficient implementation ofNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allowour periodic-averaging method to work well, as well as substantially improvingthe convergence of SGD on a single machine.
arxiv-11100-71 | Skip-Thought Vectors | http://arxiv.org/pdf/1506.06726v1.pdf | author:Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler category:cs.CL cs.LG published:2015-06-22 summary:We describe an approach for unsupervised learning of a generic, distributedsentence encoder. Using the continuity of text from books, we train anencoder-decoder model that tries to reconstruct the surrounding sentences of anencoded passage. Sentences that share semantic and syntactic properties arethus mapped to similar vector representations. We next introduce a simplevocabulary expansion method to encode words that were not seen as part oftraining, allowing us to expand our vocabulary to a million words. Aftertraining our model, we extract and evaluate our vectors with linear models on 8tasks: semantic relatedness, paraphrase detection, image-sentence ranking,question-type classification and 4 benchmark sentiment and subjectivitydatasets. The end result is an off-the-shelf encoder that can produce highlygeneric sentence representations that are robust and perform well in practice.We will make our encoder publicly available.
arxiv-11100-72 | Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books | http://arxiv.org/pdf/1506.06724v1.pdf | author:Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler category:cs.CV cs.CL published:2015-06-22 summary:Books are a rich source of both fine-grained information, how a character, anobject or a scene looks like, as well as high-level semantics, what someone isthinking, feeling and how these states evolve through a story. This paper aimsto align books to their movie releases in order to provide rich descriptiveexplanations for visual content that go semantically far beyond the captionsavailable in current datasets. To align movies and books we exploit a neuralsentence embedding that is trained in an unsupervised way from a large corpusof books, as well as a video-text neural embedding for computing similaritiesbetween movie clips and sentences in the book. We propose a context-aware CNNto combine information from multiple sources. We demonstrate good quantitativeperformance for movie/book alignment and show several qualitative examples thatshowcase the diversity of tasks our model can be used for.
arxiv-11100-73 | Variational Inference with Normalizing Flows | http://arxiv.org/pdf/1505.05770v4.pdf | author:Danilo Jimenez Rezende, Shakir Mohamed category:stat.ML cs.AI cs.LG stat.CO stat.ME published:2015-05-21 summary:The choice of approximate posterior distribution is one of the core problemsin variational inference. Most applications of variational inference employsimple families of posterior approximations in order to allow for efficientinference, focusing on mean-field or other simple structured approximations.This restriction has a significant impact on the quality of inferences madeusing variational methods. We introduce a new approach for specifying flexible,arbitrarily complex and scalable approximate posterior distributions. Ourapproximations are distributions constructed through a normalizing flow,whereby a simple initial density is transformed into a more complex one byapplying a sequence of invertible transformations until a desired level ofcomplexity is attained. We use this view of normalizing flows to developcategories of finite and infinitesimal flows and provide a unified view ofapproaches for constructing rich posterior approximations. We demonstrate thatthe theoretical advantages of having posteriors that better match the trueposterior, combined with the scalability of amortized variational approaches,provides a clear improvement in performance and applicability of variationalinference.
arxiv-11100-74 | A Neural Network Approach to Context-Sensitive Generation of Conversational Responses | http://arxiv.org/pdf/1506.06714v1.pdf | author:Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, Bill Dolan category:cs.CL cs.AI cs.LG cs.NE published:2015-06-22 summary:We present a novel response generation system that can be trained end to endon large quantities of unstructured Twitter conversations. A neural networkarchitecture is used to address sparsity issues that arise when integratingcontextual information into classic statistical models, allowing the system totake into account previous dialog utterances. Our dynamic-context generativemodels show consistent gains over both context-sensitive andnon-context-sensitive Machine Translation and Information Retrieval baselines.
arxiv-11100-75 | A Rank-Corrected Procedure for Matrix Completion with Fixed Basis Coefficients | http://arxiv.org/pdf/1210.3709v3.pdf | author:Weimin Miao, Shaohua Pan, Defeng Sun category:math.OC cs.IT cs.NA math.IT stat.ML published:2012-10-13 summary:For the problems of low-rank matrix completion, the efficiency of thewidely-used nuclear norm technique may be challenged under many circumstances,especially when certain basis coefficients are fixed, for example, the low-rankcorrelation matrix completion in various fields such as the financial marketand the low-rank density matrix completion from the quantum state tomography.To seek a solution of high recovery quality beyond the reach of the nuclearnorm, in this paper, we propose a rank-corrected procedure using a nuclearsemi-norm to generate a new estimator. For this new estimator, we establish anon-asymptotic recovery error bound. More importantly, we quantify thereduction of the recovery error bound for this rank-corrected procedure.Compared with the one obtained for the nuclear norm penalized least squaresestimator, this reduction can be substantial (around 50%). We also providenecessary and sufficient conditions for rank consistency in the sense of Bach(2008). Very interestingly, these conditions are highly related to the conceptof constraint nondegeneracy in matrix optimization. As a byproduct, our resultsprovide a theoretical foundation for the majorized penalty method of Gao andSun (2010) and Gao (2010) for structured low-rank matrix optimization problems.Extensive numerical experiments demonstrate that our proposed rank-correctedprocedure can simultaneously achieve a high recovery accuracy and capture thelow-rank structure.
arxiv-11100-76 | Adaptive Digital Scan Variable Pixels | http://arxiv.org/pdf/1506.06681v1.pdf | author:Sherin Sugathan, Reshma Scaria, Alex Pappachen James category:cs.CV published:2015-06-22 summary:The square and rectangular shape of the pixels in the digital images forsensing and display purposes introduces several inaccuracies in therepresentation of digital images. The major disadvantage of square pixel shapesis the inability to accurately capture and display the details in the objectshaving variable orientations to edges, shapes and regions. This effect can beobserved by the inaccurate representation of diagonal edges in low resolutionsquare pixel images. This paper explores a less investigated idea of usingvariable shaped pixels for improving visual quality of image scans withoutincreasing the square pixel resolution. The proposed adaptive filteringtechnique reports an improvement in image PSNR.
arxiv-11100-77 | A cognitive neural architecture able to learn and communicate through natural language | http://arxiv.org/pdf/1506.03229v3.pdf | author:Bruno Golosio, Angelo Cangelosi, Olesya Gamotina, Giovanni Luca Masala category:cs.CL published:2015-06-10 summary:Communicative interactions involve a kind of procedural knowledge that isused by the human brain for processing verbal and nonverbal inputs and forlanguage production. Although considerable work has been done on modeling humanlanguage abilities, it has been difficult to bring them together to acomprehensive tabula rasa system compatible with current knowledge of howverbal information is processed in the brain. This work presents a cognitivesystem, entirely based on a large-scale neural architecture, which wasdeveloped to shed light on the procedural knowledge involved in languageelaboration. The main component of this system is the central executive, whichis a supervising system that coordinates the other components of the workingmemory. In our model, the central executive is a neural network that takes asinput the neural activation states of the short-term memory and yields asoutput mental actions, which control the flow of information among the workingmemory components through neural gating mechanisms. The proposed system iscapable of learning to communicate through natural language starting fromtabula rasa, without any a priori knowledge of the structure of phrases,meaning of words, role of the different classes of words, only by interactingwith a human through a text-based interface, using an open-ended incrementallearning process. It is able to learn nouns, verbs, adjectives, pronouns andother word classes, and to use them in expressive language. The model wasvalidated on a corpus of 1587 input sentences, based on literature on earlylanguage assessment, at the level of about 4-years old child, and produced 521output sentences, expressing a broad range of language processingfunctionalities.
arxiv-11100-78 | Multi-path Convolutional Neural Networks for Complex Image Classification | http://arxiv.org/pdf/1506.04701v3.pdf | author:Mingming Wang category:cs.CV published:2015-06-15 summary:Convolutional Neural Networks demonstrate high performance on ImageNetLarge-Scale Visual Recognition Challenges contest. Nevertheless, the publishedresults only show the overall performance for all image classes. There is nofurther analysis why certain images get worse results and how they could beimproved. In this paper, we provide deep performance analysis based ondifferent types of images and point out the weaknesses of convolutional neuralnetworks through experiment. We design a novel multiple paths convolutionalneural network, which feeds different versions of images into separated pathsto learn more comprehensive features. This model has better presentation forimage than the traditional single path model. We acquire better classificationresults on complex validation set on both top 1 and top 5 scores than the bestILSVRC 2013 classification model.
arxiv-11100-79 | Target Tracking In Real Time Surveillance Cameras and Videos | http://arxiv.org/pdf/1506.06659v1.pdf | author:Nayyab Naseem, Mehreen Sirshar category:cs.CV published:2015-06-22 summary:Security concerns has been kept on increasing, so it is important foreveryone to keep their property safe from thefts and destruction. So the needfor surveillance techniques are also increasing. The system has been developedto detect the motion in a video. A system has been developed for real timeapplications by using the techniques of background subtraction and framedifferencing. In this system, motion is detected from the webcam or from thereal time video. Background subtraction and frames differencing method has beenused to detect the moving target. In background subtraction method, currentframe is subtracted from the referenced frame and then the threshold isapplied. If the difference is greater than the threshold then it is consideredas the pixel from the moving object, otherwise it is considered as backgroundpixel. Similarly, two frames difference method takes difference between twocontinuous frames. Then that resultant difference frame is thresholded and theamount of difference pixels is calculated.
arxiv-11100-80 | Multi-Modulus Algorithms Using Hyperbolic and Givens Rotations for MIMO Deconvolution | http://arxiv.org/pdf/1506.06650v1.pdf | author:Syed A. W. Shah, Karim Abed-Meraim, Tareq Y. Al-Naffouri category:cs.IT math.IT stat.ML published:2015-06-22 summary:This paper addresses the problem of blind multiple-input multiple-outputdeconvolution of a communication system. Two new iterative blind sourceseparation (BSS) algorithms are presented, based on the minimization ofmulti-modulus criterion. Further, we show that the design of algorithm in thecomplex domain is quite complicated, so a special structure of real filteringmatrix is suggested and maintained throughout the design. Then, a firstmulti-modulus algorithm based on data whitening and Givens rotations isproposed. An improved version of the latter is introduced for small samplesizes by combining Hyperbolic (Shear) with Givens rotations to compensate forthe ill whitening that occurs in this case. Proposed methods are finallycompared with several BSS algorithms in terms of signal-to-interference andnoise ratio, symbol error rate and convergence rate. Simulation results showthat the proposed methods outperform the contemporary BSS algorithms.
arxiv-11100-81 | Representation Learning for cold-start recommendation | http://arxiv.org/pdf/1412.7156v5.pdf | author:Gabriella Contardo, Ludovic Denoyer, Thierry Artieres category:cs.IR cs.LG published:2014-12-22 summary:A standard approach to Collaborative Filtering (CF), i.e. prediction of userratings on items, relies on Matrix Factorization techniques. Representationsfor both users and items are computed from the observed ratings and used forprediction. Unfortunatly, these transductive approaches cannot handle the caseof new users arriving in the system, with no known rating, a problem known asuser cold-start. A common approach in this context is to ask these incomingusers for a few initialization ratings. This paper presents a model to tacklethis twofold problem of (i) finding good questions to ask, (ii) buildingefficient representations from this small amount of information. The model canalso be used in a more standard (warm) context. Our approach is evaluated onthe classical CF problem and on the cold-start problem on four differentdatasets showing its ability to improve baseline performance in both cases.
arxiv-11100-82 | Task-Oriented Learning of Word Embeddings for Semantic Relation Classification | http://arxiv.org/pdf/1503.00095v3.pdf | author:Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa, Yoshimasa Tsuruoka category:cs.CL published:2015-02-28 summary:We present a novel learning method for word embeddings designed for relationclassification. Our word embeddings are trained by predicting words betweennoun pairs using lexical relation-specific features on a large unlabeledcorpus. This allows us to explicitly incorporate relation-specific informationinto the word embeddings. The learned word embeddings are then used toconstruct feature vectors for a relation classification model. On awell-established semantic relation classification task, our methodsignificantly outperforms a baseline based on a previously introduced wordembedding method, and compares favorably to previous state-of-the-art modelsthat use syntactic information or manually constructed external resources.
arxiv-11100-83 | Understanding Neural Networks Through Deep Visualization | http://arxiv.org/pdf/1506.06579v1.pdf | author:Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson category:cs.CV cs.LG cs.NE published:2015-06-22 summary:Recent years have produced great advances in training large, deep neuralnetworks (DNNs), including notable successes in training convolutional neuralnetworks (convnets) to recognize natural images. However, our understanding ofhow these models work, especially what computations they perform atintermediate layers, has lagged behind. Progress in the field will be furtheraccelerated by the development of better tools for visualizing and interpretingneural nets. We introduce two such tools here. The first is a tool thatvisualizes the activations produced on each layer of a trained convnet as itprocesses an image or video (e.g. a live webcam stream). We have found thatlooking at live activations that change in response to user input helps buildvaluable intuitions about how convnets work. The second tool enablesvisualizing features at each layer of a DNN via regularized optimization inimage space. Because previous versions of this idea produced less recognizableimages, here we introduce several new regularization methods that combine toproduce qualitatively clearer, more interpretable visualizations. Both toolsare open source and work on a pre-trained convnet with minimal setup.
arxiv-11100-84 | PAC-Bayes Iterated Logarithm Bounds for Martingale Mixtures | http://arxiv.org/pdf/1506.06573v1.pdf | author:Akshay Balsubramani category:cs.LG math.PR stat.ML published:2015-06-22 summary:We give tight concentration bounds for mixtures of martingales that aresimultaneously uniform over (a) mixture distributions, in a PAC-Bayes sense;and (b) all finite times. These bounds are proved in terms of the martingalevariance, extending classical Bernstein inequalities, and sharpening andsimplifying prior work.
arxiv-11100-85 | Regularization Path of Cross-Validation Error Lower Bounds | http://arxiv.org/pdf/1502.02344v2.pdf | author:Atsushi Shibagaki, Yoshiki Suzuki, Masayuki Karasuyama, Ichiro Takeuchi category:stat.ML published:2015-02-09 summary:Careful tuning of a regularization parameter is indispensable in many machinelearning tasks because it has a significant impact on generalizationperformances. Nevertheless, current practice of regularization parameter tuningis more of an art than a science, e.g., it is hard to tell how many grid-pointswould be needed in cross-validation (CV) for obtaining a solution withsufficiently small CV error. In this paper we propose a novel framework forcomputing a lower bound of the CV errors as a function of the regularizationparameter, which we call regularization path of CV error lower bounds. Theproposed framework can be used for providing a theoretical approximationguarantee on a set of solutions in the sense that how far the CV error of thecurrent best solution could be away from best possible CV error in the entirerange of the regularization parameters. We demonstrate through numericalexperiments that a theoretically guaranteed a choice of regularizationparameter in the above sense is possible with reasonable computational costs.
arxiv-11100-86 | Answer Sequence Learning with Neural Networks for Answer Selection in Community Question Answering | http://arxiv.org/pdf/1506.06490v1.pdf | author:Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou Tang, Xiaolong Wang category:cs.CL cs.IR cs.LG published:2015-06-22 summary:In this paper, the answer selection problem in community question answering(CQA) is regarded as an answer sequence labeling task, and a novel approach isproposed based on the recurrent architecture for this problem. Our approachapplies convolution neural networks (CNNs) to learning the joint representationof question-answer pair firstly, and then uses the joint representation asinput of the long short-term memory (LSTM) to learn the answer sequence of aquestion for labeling the matching quality of each answer. Experimentsconducted on the SemEval 2015 CQA dataset shows the effectiveness of ourapproach.
arxiv-11100-87 | The Ebb and Flow of Deep Learning: a Theory of Local Learning | http://arxiv.org/pdf/1506.06472v1.pdf | author:Pierre Baldi, Peter Sadowski category:cs.LG cs.NE stat.ML published:2015-06-22 summary:In a physical neural system, where storage and processing are intimatelyintertwined, the rules for adjusting the synaptic weights can only depend onvariables that are available locally, such as the activity of the pre- andpost-synaptic neurons, resulting in local learning rules. A systematicframework for studying the space of local learning rules must first define thenature of the local variables, and then the functional form that ties themtogether into each learning rule. We consider polynomial local learning rulesand analyze their behavior and capabilities in both linear and non-linearnetworks. As a byproduct, this framework enables also the discovery of newlearning rules as well as important relationships between learning rules andgroup symmetries. Stacking local learning rules in deep feedforward networksleads to deep local learning. While deep local learning can learn interestingrepresentations, it cannot learn complex input-output functions, even whentargets are available for the top layer. Learning complex input-outputfunctions requires local deep learning where target information is propagatedto the deep layers through a backward channel. The nature of the propagatedinformation about the targets, and the backward channel through which thisinformation is propagated, partition the space of learning algorithms. For anylearning algorithm, the capacity of the backward channel can be defined as thenumber of bits provided about the gradient per weight, divided by the number ofrequired operations per weight. We estimate the capacity associated withseveral learning algorithms and show that backpropagation outperforms them andachieves the maximum possible capacity. The theory clarifies the concept ofHebbian learning, what is learnable by Hebbian learning, and explains thesparsity of the space of learning rules discovered so far.
arxiv-11100-88 | On the Equivalence of CoCoA+ and DisDCA | http://arxiv.org/pdf/1506.04217v2.pdf | author:Ching-pei Lee category:cs.LG published:2015-06-13 summary:In this document, we show that the algorithm CoCoA+ (Ma et al., ICML, 2015)under the setting used in their experiments, which is also the best settingsuggested by the authors that proposed this algorithm, is equivalent to thepractical variant of DisDCA (Yang, NIPS, 2013).
arxiv-11100-89 | DeepOrgan: Multi-level Deep Convolutional Networks for Automated Pancreas Segmentation | http://arxiv.org/pdf/1506.06448v1.pdf | author:Holger R. Roth, Le Lu, Amal Farag, Hoo-Chang Shin, Jiamin Liu, Evrim Turkbey, Ronald M. Summers category:cs.CV published:2015-06-22 summary:Automatic organ segmentation is an important yet challenging problem formedical image analysis. The pancreas is an abdominal organ with very highanatomical variability. This inhibits previous segmentation methods fromachieving high accuracies, especially compared to other organs such as theliver, heart or kidneys. In this paper, we present a probabilistic bottom-upapproach for pancreas segmentation in abdominal computed tomography (CT) scans,using multi-level deep convolutional networks (ConvNets). We propose andevaluate several variations of deep ConvNets in the context of hierarchical,coarse-to-fine classification on image patches and regions, i.e. superpixels.We first present a dense labeling of local image patches via$P{-}\mathrm{ConvNet}$ and nearest neighbor fusion. Then we describe a regionalConvNet ($R_1{-}\mathrm{ConvNet}$) that samples a set of bounding boxes aroundeach image superpixel at different scales of contexts in a "zoom-out" fashion.Our ConvNets learn to assign class probabilities for each superpixel region ofbeing pancreas. Last, we study a stacked $R_2{-}\mathrm{ConvNet}$ leveragingthe joint space of CT intensities and the $P{-}\mathrm{ConvNet}$ denseprobability maps. Both 3D Gaussian smoothing and 2D conditional random fieldsare exploited as structured predictions for post-processing. We evaluate on CTimages of 82 patients in 4-fold cross-validation. We achieve a Dice SimilarityCoefficient of 83.6$\pm$6.3% in training and 71.8$\pm$10.7% in testing.
arxiv-11100-90 | Deep Convolutional Inverse Graphics Network | http://arxiv.org/pdf/1503.03167v4.pdf | author:Tejas D. Kulkarni, Will Whitney, Pushmeet Kohli, Joshua B. Tenenbaum category:cs.CV cs.GR cs.LG cs.NE published:2015-03-11 summary:This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), amodel that learns an interpretable representation of images. Thisrepresentation is disentangled with respect to transformations such asout-of-plane rotations and lighting variations. The DC-IGN model is composed ofmultiple layers of convolution and de-convolution operators and is trainedusing the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose atraining procedure to encourage neurons in the graphics code layer to representa specific transformation (e.g. pose or light). Given a single input image, ourmodel can generate new images of the same object with variations in pose andlighting. We present qualitative and quantitative results of the model'sefficacy at learning a 3D rendering engine.
arxiv-11100-91 | Extreme Extraction: Only One Hour per Relation | http://arxiv.org/pdf/1506.06418v1.pdf | author:Raphael Hoffmann, Luke Zettlemoyer, Daniel S. Weld category:cs.CL cs.AI cs.IR published:2015-06-21 summary:Information Extraction (IE) aims to automatically generate a large knowledgebase from natural language text, but progress remains slow. Supervised learningrequires copious human annotation, while unsupervised and weakly supervisedapproaches do not deliver competitive accuracy. As a result, most fieldedapplications of IE, as well as the leading TAC-KBP systems, rely on significantamounts of manual engineering. Even "Extreme" methods, such as those reportedin Freedman et al. 2011, require about 10 hours of expert labor per relation. This paper shows how to reduce that effort by an order of magnitude. Wepresent a novel system, InstaRead, that streamlines authoring with an ensembleof methods: 1) encoding extraction rules in an expressive and compositionalrepresentation, 2) guiding the user to promising rules based on corpusstatistics and mined resources, and 3) introducing a new interactivedevelopment cycle that provides immediate feedback --- even on large datasets.Experiments show that experts can create quality extractors in under an hourand even NLP novices can author good extractors. These extractors equal oroutperform ones obtained by comparably supervised and state-of-the-artdistantly supervised approaches.
arxiv-11100-92 | Permutation Search Methods are Efficient, Yet Faster Search is Possible | http://arxiv.org/pdf/1506.03163v3.pdf | author:Bilegsaikhan Naidan, Leonid Boytsov, Eric Nyberg category:cs.LG cs.DB cs.DS published:2015-06-10 summary:We survey permutation-based methods for approximate k-nearest neighborsearch. In these methods, every data point is represented by a ranked list ofpivots sorted by the distance to this point. Such ranked lists are calledpermutations. The underpinning assumption is that, for both metric andnon-metric spaces, the distance between permutations is a good proxy for thedistance between original points. Thus, it should be possible to efficientlyretrieve most true nearest neighbors by examining only a tiny subset of datapoints whose permutations are similar to the permutation of a query. We furthertest this assumption by carrying out an extensive experimental evaluation wherepermutation methods are pitted against state-of-the art benchmarks (themulti-probe LSH, the VP-tree, and proximity-graph based retrieval) on a varietyof realistically large data set from the image and textual domain. The focus ison the high-accuracy retrieval methods for generic spaces. Additionally, weassume that both data and indices are stored in main memory. We findpermutation methods to be reasonably efficient and describe a setup where thesemethods are most useful. To ease reproducibility, we make our software and datasets publicly available.
arxiv-11100-93 | Puzzle Imaging: Using Large-scale Dimensionality Reduction Algorithms for Localization | http://arxiv.org/pdf/1502.07816v3.pdf | author:Joshua I. Glaser, Bradley M. Zamft, George M. Church, Konrad P. Kording category:q-bio.NC cs.CE cs.CV q-bio.QM published:2015-02-27 summary:Current high-resolution imaging techniques require an intact sample thatpreserves spatial relationships. We here present a novel approach, "puzzleimaging," that allows imaging a spatially scrambled sample. This techniquetakes many spatially disordered samples, and then pieces them back togetherusing local properties embedded within the sample. We show that puzzle imagingcan efficiently produce high-resolution images using dimensionality reductionalgorithms. We demonstrate the theoretical capabilities of puzzle imaging inthree biological scenarios, showing that (1) relatively precise 3-dimensionalbrain imaging is possible; (2) the physical structure of a neural network canoften be recovered based only on the neural connectivity matrix; and (3) achemical map could be reproduced using bacteria with chemosensitive DNA andconjugative transfer. The ability to reconstruct scrambled images promises toenable imaging based on DNA sequencing of homogenized tissue samples.
arxiv-11100-94 | A Novel Method for Stock Forecasting based on Fuzzy Time Series Combined with the Longest Common/Repeated Sub-sequence | http://arxiv.org/pdf/1506.06366v1.pdf | author:He-Wen Chen, Zih-Ci Wang, Shu-Yu Kuo, Yao-Hsin Chou category:cs.CE cs.AI cs.NE published:2015-06-21 summary:Stock price forecasting is an important issue for investors since extremeaccuracy in forecasting can bring about high profits. Fuzzy Time Series (FTS)and Longest Common/Repeated Sub-sequence (LCS/LRS) are two important issues forforecasting prices. However, to the best of our knowledge, there are nosignificant studies using LCS/LRS to predict stock prices. It is impossiblethat prices stay exactly the same as historic prices. Therefore, this paperproposes a state-of-the-art method which combines FTS and LCS/LRS to predictstock prices. This method is based on the principle that history will repeatitself. It uses different interval lengths in FTS to fuzzify the prices, andLCS/LRS to look for the same pattern in the historical prices to predict futurestock prices. In the experiment, we examine various intervals of fuzzy timesets in order to achieve high prediction accuracy. The proposed methodoutperforms traditional methods in terms of prediction accuracy and,furthermore, it is easy to implement.
arxiv-11100-95 | Communication Efficient Distributed Agnostic Boosting | http://arxiv.org/pdf/1506.06318v1.pdf | author:Shang-Tse Chen, Maria-Florina Balcan, Duen Horng Chau category:cs.LG stat.ML published:2015-06-21 summary:We consider the problem of learning from distributed data in the agnosticsetting, i.e., in the presence of arbitrary forms of noise. Our maincontribution is a general distributed boosting-based procedure for learning anarbitrary concept space, that is simultaneously noise tolerant, communicationefficient, and computationally efficient. This improves significantly overprior works that were either communication efficient only in noise-freescenarios or computationally prohibitive. Empirical results on large syntheticand real-world datasets demonstrate the effectiveness and scalability of theproposed approach.
arxiv-11100-96 | Soft-Deep Boltzmann Machines | http://arxiv.org/pdf/1505.02462v3.pdf | author:Taichi Kiwaki category:cs.NE cs.LG stat.ML published:2015-05-11 summary:We present a layered Boltzmann machine (BM) that can better exploit theadvantages of a distributed representation. It is widely believed that deep BMs(DBMs) have far greater representational power than its shallow counterpart,restricted Boltzmann machines (RBMs). However, this expectation on thesupremacy of DBMs over RBMs has not ever been validated in a theoreticalfashion. In this paper, we provide both theoretical and empirical evidencesthat the representational power of DBMs can be actually rather limited intaking advantages of distributed representations. We propose an approximatemeasure for the representational power of a BM regarding to the efficiency of adistributed representation. With this measure, we show a surprising fact thatDBMs can make inefficient use of distributed representations. Based on theseobservations, we propose an alternative BM architecture, which we dub soft-deepBMs (sDBMs). We show that sDBMs can more efficiently exploit the distributedrepresentations in terms of the measure. Experiments demonstrate that sDBMsoutperform several state-of-the-art models, including DBMs, in generative taskson binarized MNIST and Caltech-101 silhouettes.
arxiv-11100-97 | Metacarpal Bones Localization in X-ray Imagery Using Particle Filter Segmentation | http://arxiv.org/pdf/1412.8197v2.pdf | author:Z. Bardosi, D. Granata, G. Lugos, A. P. Tafti, S. Saxena category:cs.CV published:2014-12-28 summary:Statistical methods such as sequential Monte Carlo Methods were proposed fordetection, segmentation and tracking of objects in digital images. A similarapproach, called Shape Particle Filters was introduced for the segmentation ofvertebra, lungs and hearts [1]. In this contribution, a global shape and alocal appearance model are derived from specific object annotated X-ray imagesof the metacarpal bones. In the test data a unique labeling of the boneboundary and the background points and a manual annotation is given. Using aset of local features (Haar-like) in the neighborhood of each pixel aprobabilistic pixel classifier is built using the random forest algorithm. Tofit the shape model to a new image, a label probability map is extracted andthen the optimal shape is obtained by maximizing the probability of eachlandmark with the Differential Evolution algorithm.
arxiv-11100-98 | Understanding image representations by measuring their equivariance and equivalence | http://arxiv.org/pdf/1411.5908v2.pdf | author:Karel Lenc, Andrea Vedaldi category:cs.CV cs.LG cs.NE published:2014-11-21 summary:Despite the importance of image representations such as histograms oforiented gradients and deep Convolutional Neural Networks (CNN), ourtheoretical understanding of them remains limited. Aiming at filling this gap,we investigate three key mathematical properties of representations:equivariance, invariance, and equivalence. Equivariance studies howtransformations of the input image are encoded by the representation,invariance being a special case where a transformation has no effect.Equivalence studies whether two representations, for example two differentparametrisations of a CNN, capture the same visual information or not. A numberof methods to establish these properties empirically are proposed, includingintroducing transformation and stitching layers in CNNs. These methods are thenapplied to popular representations to reveal insightful aspects of theirstructure, including clarifying at which layers in a CNN certain geometricinvariances are achieved. While the focus of the paper is theoretical, directapplications to structured-output regression are demonstrated too.
arxiv-11100-99 | Pose Estimation Based on 3D Models | http://arxiv.org/pdf/1506.06274v1.pdf | author:Chuiwen Ma, Hao Su, Liang Shi category:cs.CV cs.LG cs.RO published:2015-06-20 summary:In this paper, we proposed a pose estimation system based on rendered imagetraining set, which predicts the pose of objects in real image, with knowledgeof object category and tight bounding box. We developed a patch-basedmulti-class classification algorithm, and an iterative approach to improve theaccuracy. We achieved state-of-the-art performance on pose estimation task.
arxiv-11100-100 | 3D Reconstruction from Full-view Fisheye Camera | http://arxiv.org/pdf/1506.06273v1.pdf | author:Chuiwen Ma, Liang Shi, Hanlu Huang, Mengyuan Yan category:cs.CV published:2015-06-20 summary:In this report, we proposed a 3D reconstruction method for the full-viewfisheye camera. The camera we used is Ricoh Theta, which captures sphericalimages and has a wide field of view (FOV). The conventional stereo apporachbased on perspective camera model cannot be directly applied and instead weused a spherical camera model to depict the relation between 3D point and itscorresponding observation in the image. We implemented a system that canreconstruct the 3D scene using captures from two or more cameras. A GUI is alsocreated to allow users to control the view perspective and obtain a betterintuition of how the scene is rebuilt. Experiments showed that ourreconstruction results well preserved the structure of the scene in the realworld.
arxiv-11100-101 | Aligning where to see and what to tell: image caption with region-based attention and scene factorization | http://arxiv.org/pdf/1506.06272v1.pdf | author:Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha, Changshui Zhang category:cs.CV cs.LG stat.ML published:2015-06-20 summary:Recent progress on automatic generation of image captions has shown that itis possible to describe the most salient information conveyed by images withaccurate and meaningful sentences. In this paper, we propose an image captionsystem that exploits the parallel structures between images and sentences. Inour model, the process of generating the next word, given the previouslygenerated ones, is aligned with the visual perception experience where theattention shifting among the visual regions imposes a thread of visualordering. This alignment characterizes the flow of "abstract meaning", encodingwhat is semantically shared by both the visual scene and the text description.Our system also makes another novel modeling contribution by introducingscene-specific contexts that capture higher-level semantic information encodedin an image. The contexts adapt language models for word generation to specificscene types. We benchmark our system and contrast to published results onseveral popular datasets. We show that using either region-based attention orscene-specific contexts improves systems without those components. Furthermore,combining these two modeling ingredients attains the state-of-the-artperformance.
arxiv-11100-102 | Can deep learning help you find the perfect match? | http://arxiv.org/pdf/1505.00359v2.pdf | author:Harm de Vries, Jason Yosinski category:cs.LG published:2015-05-02 summary:Is he/she my type or not? The answer to this question depends on the personalpreferences of the one asking it. The individual process of obtaining a fullanswer may generally be difficult and time consuming, but often an approximateanswer can be obtained simply by looking at a photo of the potential match.Such approximate answers based on visual cues can be produced in a fraction ofa second, a phenomenon that has led to a series of recently successful datingapps in which users rate others positively or negatively using primarily asingle photo. In this paper we explore using convolutional networks to create amodel of an individual's personal preferences based on rated photos. Thisintroduced task is difficult due to the large number of variations in profilepictures and the noise in attractiveness labels. Toward this task we collect adataset comprised of $9364$ pictures and binary labels for each. We compareperformance of convolutional models trained in three ways: first directly onthe collected dataset, second with features transferred from a network trainedto predict gender, and third with features transferred from a network trainedon ImageNet. Our findings show that ImageNet features transfer best, producinga model that attains $68.1\%$ accuracy on the test set and is moderatelysuccessful at predicting matches.
arxiv-11100-103 | Collective Mind, Part II: Towards Performance- and Cost-Aware Software Engineering as a Natural Science | http://arxiv.org/pdf/1506.06256v1.pdf | author:Grigori Fursin, Abdul Memon, Christophe Guillon, Anton Lokhmotov category:cs.SE cs.LG cs.PF published:2015-06-20 summary:Nowadays, engineers have to develop software often without even knowing whichhardware it will eventually run on in numerous mobile phones, tablets,desktops, laptops, data centers, supercomputers and cloud services.Unfortunately, optimizing compilers are not keeping pace with ever increasingcomplexity of computer systems anymore and may produce severely underperformingexecutable codes while wasting expensive resources and energy. We present our practical and collaborative solution to this problem vialight-weight wrappers around any software piece when more than oneimplementation or optimization choice available. These wrappers are connectedwith a public Collective Mind autotuning infrastructure and repository ofknowledge (c-mind.org/repo) to continuously monitor various importantcharacteristics of these pieces (computational species) across numerousexisting hardware configurations together with randomly selected optimizations.Similar to natural sciences, we can now continuously track winning solutions(optimizations for a given hardware) that minimize all costs of a computation(execution time, energy spent, code size, failures, memory and storagefootprint, optimization time, faults, contentions, inaccuracy and so on) of agiven species on a Pareto frontier along with any unexpected behavior. Thecommunity can then collaboratively classify solutions, prune redundant ones,and correlate them with various features of software, its inputs (data sets)and used hardware either manually or using powerful predictive analyticstechniques. Our approach can then help create a large, realistic, diverse,representative, and continuously evolving benchmark with related optimizationknowledge while gradually covering all possible software and hardware to beable to predict best optimizations and improve compilers and hardware dependingon usage scenarios and requirements.
arxiv-11100-104 | Fast Sampling for Bayesian Max-Margin Models | http://arxiv.org/pdf/1504.07107v4.pdf | author:Wenbo Hu, Jun Zhu, Minjie Xu, Bo Zhang category:stat.ML cs.AI cs.LG published:2015-04-27 summary:Bayesian max-margin models have shown great superiority in various machinelearning tasks with a likelihood regularization, while the probabilistic MonteCarlo sampling for these models still remains challenging, especially forlarge-scale settings. In analogy to the data augmentation technique to tacklewith non-smoothness of the hinge loss, we present a stochastic subgradient MCMCmethod which is easy to implement and computationally efficient. We investigatethe variants that use adaptive stepsizes and thermostats to improve mixingspeeds for Bayesian linear SVM. Furthermore, we design a stochastic subgradientHMC within Gibbs method and a doubly stochastic HMC algorithm for mixture ofSVMs, a popular extension of linear classifiers. Experimental results on a widerange of problems demonstrate the effectiveness of our approach.
arxiv-11100-105 | Influence Functions for Machine Learning: Nonparametric Estimators for Entropies, Divergences and Mutual Informations | http://arxiv.org/pdf/1411.4342v3.pdf | author:Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry Wasserman, James M. Robins category:stat.ML cs.AI cs.LG published:2014-11-17 summary:We propose and analyze estimators for statistical functionals of one or moredistributions under nonparametric assumptions. Our estimators are based on thetheory of influence functions, which appear in the semiparametric statisticsliterature. We show that estimators based either on data-splitting or aleave-one-out technique enjoy fast rates of convergence and other favorabletheoretical properties. We apply this framework to derive estimators forseveral popular information theoretic quantities, and via empirical evaluation,show the advantage of this approach over existing estimators.
arxiv-11100-106 | Detectability thresholds and optimal algorithms for community structure in dynamic networks | http://arxiv.org/pdf/1506.06179v1.pdf | author:Amir Ghasemian, Pan Zhang, Aaron Clauset, Cristopher Moore, Leto Peel category:stat.ML cs.LG cs.SI published:2015-06-19 summary:We study the fundamental limits on learning latent community structure indynamic networks. Specifically, we study dynamic stochastic block models wherenodes change their community membership over time, but where edges aregenerated independently at each time step. In this setting (which is a specialcase of several existing models), we are able to derive the detectabilitythreshold exactly, as a function of the rate of change and the strength of thecommunities. Below this threshold, we claim that no algorithm can identify thecommunities better than chance. We then give two algorithms that are optimal inthe sense that they succeed all the way down to this limit. The first usesbelief propagation (BP), which gives asymptotically optimal accuracy, and thesecond is a fast spectral clustering algorithm, based on linearizing the BPequations. We verify our analytic and algorithmic results via numericalsimulation, and close with a brief discussion of extensions and open questions.
arxiv-11100-107 | Structured Training for Neural Network Transition-Based Parsing | http://arxiv.org/pdf/1506.06158v1.pdf | author:David Weiss, Chris Alberti, Michael Collins, Slav Petrov category:cs.CL published:2015-06-19 summary:We present structured perceptron training for neural network transition-baseddependency parsing. We learn the neural network representation using a goldcorpus augmented by a large number of automatically parsed sentences. Giventhis fixed network representation, we learn a final layer using the structuredperceptron with beam-search decoding. On the Penn Treebank, our parser reaches94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledgeis the best accuracy on Stanford Dependencies to date. We also provide in-depthablative analysis to determine which aspects of our model provide the largestgains in accuracy.
arxiv-11100-108 | LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop | http://arxiv.org/pdf/1506.03365v2.pdf | author:Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, Jianxiong Xiao category:cs.CV published:2015-06-10 summary:The state-of-the-art visual recognition algorithms are all data-hungry,requiring a huge amount of labeled image data to optimize millions ofparameters. While there has been remarkable progress in algorithm and systemdesign, the labeled datasets used by these models are quickly becoming outdatedin terms of size. To overcome the bottleneck of human labeling speed duringdataset construction, we propose to amplify human effort using deep learningwith humans in the loop. Our procedure comes equipped with precision and recallguarantees to ensure labeling quality, reaching the same level of performanceas fully manual annotation. To demonstrate the power of our annotationprocedure and enable further progress of visual recognition, we construct ascene-centric database called "LSUN" containing millions of labeled images ineach scene category. We experiment with popular deep nets using our dataset andobtain a substantial performance gain with the same model trained using ourlarger training set. All data and source code will be available online uponacceptance of the paper.
arxiv-11100-109 | Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample Complexity | http://arxiv.org/pdf/1412.2954v3.pdf | author:Santosh S. Vempala, Ying Xiao category:cs.DS cs.LG stat.ML published:2014-12-09 summary:We present a simple, general technique for reducing the sample complexity ofmatrix and tensor decomposition algorithms applied to distributions. We use thetechnique to give a polynomial-time algorithm for standard ICA with samplecomplexity nearly linear in the dimension, thereby improving substantially onprevious bounds. The analysis is based on properties of random polynomials,namely the spacings of an ensemble of polynomials. Our technique also appliesto other applications of tensor decompositions, including spherical Gaussianmixture models.
arxiv-11100-110 | Learning language through pictures | http://arxiv.org/pdf/1506.03694v2.pdf | author:Grzegorz Chrupała, Ákos Kádár, Afra Alishahi category:cs.CL published:2015-06-11 summary:We propose Imaginet, a model of learning visually grounded representations oflanguage from coupled textual and visual input. The model consists of two GatedRecurrent Unit networks with shared word embeddings, and uses a multi-taskobjective by receiving a textual description of a scene and trying toconcurrently predict its visual representation and the next word in thesentence. Mimicking an important aspect of human language learning, it acquiresmeaning representations for individual words from descriptions of visualscenes. Moreover, it learns to effectively use sequential structure in semanticinterpretation of multi-word phrases.
arxiv-11100-111 | Approximate Inference with the Variational Holder Bound | http://arxiv.org/pdf/1506.06100v1.pdf | author:Guillaume Bouchard, Balaji Lakshminarayanan category:stat.ML cs.LG math.FA published:2015-06-19 summary:We introduce the Variational Holder (VH) bound as an alternative toVariational Bayes (VB) for approximate Bayesian inference. Unlike VB whichtypically involves maximization of a non-convex lower bound with respect to thevariational parameters, the VH bound involves minimization of a convex upperbound to the intractable integral with respect to the variational parameters.Minimization of the VH bound is a convex optimization problem; hence the VHmethod can be applied using off-the-shelf convex optimization algorithms andthe approximation error of the VH bound can also be analyzed using tools fromconvex optimization literature. We present experiments on the task ofintegrating a truncated multivariate Gaussian distribution and compare ourmethod to VB, EP and a state-of-the-art numerical integration method for thisproblem.
arxiv-11100-112 | Graph-based compression of dynamic 3D point cloud sequences | http://arxiv.org/pdf/1506.06096v1.pdf | author:Dorina Thanou, Philip A. Chou, Pascal Frossard category:cs.CV cs.GR published:2015-06-19 summary:This paper addresses the problem of compression of 3D point cloud sequencesthat are characterized by moving 3D positions and color attributes. Astemporally successive point cloud frames are similar, motion estimation is keyto effective compression of these sequences. It however remains a challengingproblem as the point cloud frames have varying numbers of points withoutexplicit correspondence information. We represent the time-varying geometry ofthese sequences with a set of graphs, and consider 3D positions and colorattributes of the points clouds as signals on the vertices of the graphs. Wethen cast motion estimation as a feature matching problem between successivegraphs. The motion is estimated on a sparse set of representative verticesusing new spectral graph wavelet descriptors. A dense motion field iseventually interpolated by solving a graph-based regularization problem. Theestimated motion is finally used for removing the temporal redundancy in thepredictive coding of the 3D positions and the color characteristics of thepoint cloud sequences. Experimental results demonstrate that our method is ableto accurately estimate the motion between consecutive frames. Moreover, motionestimation is shown to bring significant improvement in terms of the overallcompression performance of the sequence. To the best of our knowledge, this isthe first paper that exploits both the spatial correlation inside each frame(through the graph) and the temporal correlation between the frames (throughthe motion estimation) to compress the color and the geometry of 3D point cloudsequences in an efficient way.
arxiv-11100-113 | Quantifying the Effect of Sentiment on Information Diffusion in Social Media | http://arxiv.org/pdf/1506.06072v1.pdf | author:Emilio Ferrara, Zeyao Yang category:cs.SI cs.LG physics.soc-ph published:2015-06-19 summary:Social media have become the main vehicle of information production andconsumption online. Millions of users every day log on their Facebook orTwitter accounts to get updates and news, read about their topics of interest,and become exposed to new opportunities and interactions. Although recentstudies suggest that the contents users produce will affect the emotions oftheir readers, we still lack a rigorous understanding of the role and effectsof contents sentiment on the dynamics of information diffusion. This work aimsat quantifying the effect of sentiment on information diffusion, to understand:(i) whether positive conversations spread faster and/or broader than negativeones (or vice-versa); (ii) what kind of emotions are more typical of popularconversations on social media; and, (iii) what type of sentiment is expressedin conversations characterized by different temporal dynamics. Our findingsshow that, at the level of contents, negative messages spread faster thanpositive ones, but positive ones reach larger audiences, suggesting that peopleare more inclined to share and favorite positive contents, the so-calledpositive bias. As for the entire conversations, we highlight how differenttemporal dynamics exhibit different sentiment patterns: for example, positivesentiment builds up for highly-anticipated events, while unexpected events aremainly characterized by negative sentiment. Our contribution is a milestone tounderstand how the emotions expressed in short texts affect their spreading inonline social ecosystems, and may help to craft effective policies andstrategies for content generation and diffusion.
arxiv-11100-114 | A general framework for the IT-based clustering methods | http://arxiv.org/pdf/1506.06068v1.pdf | author:Teng Qiu, Yongjie Li category:cs.CV cs.LG stat.ML published:2015-06-19 summary:Previously, we proposed a physically inspired rule to organize the datapoints in a sparse yet effective structure, called the in-tree (IT) graph,which is able to capture a wide class of underlying cluster structures in thedatasets, especially for the density-based datasets. Although there are someredundant edges or lines between clusters requiring to be removed by computer,this IT graph has a big advantage compared with the k-nearest-neighborhood(k-NN) or the minimal spanning tree (MST) graph, in that the redundant edges inthe IT graph are much more distinguishable and thus can be easily determined byseveral methods previously proposed by us. In this paper, we propose a general framework to re-construct the IT graph,based on an initial neighborhood graph, such as the k-NN or MST, etc, and thecorresponding graph distances. For this general framework, our previous way ofconstructing the IT graph turns out to be a special case of it. This generalframework 1) can make the IT graph capture a wider class of underlying clusterstructures in the datasets, especially for the manifolds, and 2) should be moreeffective to cluster the sparse or graph-based datasets.
arxiv-11100-115 | Tensor Analysis and Fusion of Multimodal Brain Images | http://arxiv.org/pdf/1506.06040v1.pdf | author:Esin Karahan, Pedro A. Rojas-Lopez, Maria L. Bringas-Vega, Pedro A. Valdes-Hernandez, Pedro A. Valdes-Sosa category:stat.ME cs.NA stat.AP stat.ML published:2015-06-19 summary:Current high-throughput data acquisition technologies probe dynamical systemswith different imaging modalities, generating massive data sets at differentspatial and temporal resolutions posing challenging problems in multimodal datafusion. A case in point is the attempt to parse out the brain structures andnetworks that underpin human cognitive processes by analysis of differentneuroimaging modalities (functional MRI, EEG, NIRS etc.). We emphasize that themultimodal, multi-scale nature of neuroimaging data is well reflected by amulti-way (tensor) structure where the underlying processes can be summarizedby a relatively small number of components or "atoms". We introduceMarkov-Penrose diagrams - an integration of Bayesian DAG and tensor networknotation in order to analyze these models. These diagrams not only clarifymatrix and tensor EEG and fMRI time/frequency analysis and inverse problems,but also help understand multimodal fusion via Multiway Partial Least Squaresand Coupled Matrix-Tensor Factorization. We show here, for the first time, thatGranger causal analysis of brain networks is a tensor regression problem, thusallowing the atomic decomposition of brain networks. Analysis of EEG and fMRIrecordings shows the potential of the methods and suggests their use in otherscientific domains.
arxiv-11100-116 | moco: Fast Motion Correction for Calcium Imaging | http://arxiv.org/pdf/1506.06039v1.pdf | author:Alexander Dubbs, James Guevara, Darcy S. Peterka, Rafael Yuste category:cs.CV published:2015-06-19 summary:Motion correction is the first in a pipeline of algorithms to analyze calciumimaging videos and extract biologically relevant information, for example thenetwork structure of the neurons therein. Fast motion correction would beespecially critical for closed-loop activity triggered stimulation experiments,where accurate detection and targeting of specific cells in necessary. Ouralgorithm uses a Fourier-transform approach, and its efficiency derives from acombination of judicious downsampling and the accelerated computation of many$L_2$ norms using dynamic programming and two-dimensional, fft-acceleratedconvolutions. Its accuracy is comparable to that of established community-usedalgorithms, and it is more stable to large translational motions. It isprogrammed in Java and is compatible with ImageJ.
arxiv-11100-117 | Measuring Emotional Contagion in Social Media | http://arxiv.org/pdf/1506.06021v1.pdf | author:Emilio Ferrara, Zeyao Yang category:cs.SI cs.LG physics.soc-ph published:2015-06-19 summary:Social media are used as main discussion channels by millions of individualsevery day. The content individuals produce in daily social-media-basedmicro-communications, and the emotions therein expressed, may impact theemotional states of others. A recent experiment performed on Facebookhypothesized that emotions spread online, even in absence of non-verbal cuestypical of in-person interactions, and that individuals are more likely toadopt positive or negative emotions if these are over-expressed in their socialnetwork. Experiments of this type, however, raise ethical concerns, as theyrequire massive-scale content manipulation with unknown consequences for theindividuals therein involved. Here, we study the dynamics of emotionalcontagion using Twitter. Rather than manipulating content, we devise a nullmodel that discounts some confounding factors (including the effect ofemotional contagion). We measure the emotional valence of content the users areexposed to before posting their own tweets. We determine that on average anegative post follows an over-exposure to 4.34% more negative content thanbaseline, while positive posts occur after an average over-exposure to 4.50%more positive contents. We highlight the presence of a linear relationshipbetween the average emotional valence of the stimuli users are exposed to, andthat of the responses they produce. We also identify two different classes ofindividuals: highly and scarcely susceptible to emotional contagion. Highlysusceptible users are significantly less inclined to adopt negative emotionsthan the scarcely susceptible ones, but equally likely to adopt positiveemotions. In general, the likelihood of adopting positive emotions is muchgreater than that of negative emotions.
arxiv-11100-118 | Crowd Flow Segmentation in Compressed Domain using CRF | http://arxiv.org/pdf/1506.06006v1.pdf | author:Srinivas S. S. Kruthiventi, R. Venkatesh Babu category:cs.CV published:2015-06-19 summary:Crowd flow segmentation is an important step in many video surveillancetasks. In this work, we propose an algorithm for segmenting flows in H.264compressed videos in a completely unsupervised manner. Our algorithm works onmotion vectors which can be obtained by partially decoding the compressed videowithout extracting any additional features. Our approach is based on modellingthe motion vector field as a Conditional Random Field (CRF) and obtainingoriented motion segments by finding the optimal labelling which minimises theglobal energy of CRF. These oriented motion segments are recursively mergedbased on gradient across their boundaries to obtain the final flow segments.This work in compressed domain can be easily extended to pixel domain bysubstituting motion vectors with motion based features like optical flow. Theproposed algorithm is experimentally evaluated on a standard crowd flow datasetand its superior performance in both accuracy and computational time aredemonstrated through quantitative results.
arxiv-11100-119 | Design of OFDM radar pulses using genetic algorithm based techniques | http://arxiv.org/pdf/1507.01889v1.pdf | author:Gabriel Lellouch, Amit Kumar Mishra, Michael Inggs category:cs.NE published:2015-06-19 summary:The merit of evolutionary algorithms (EA) to solve convex optimizationproblems is widely acknowledged. In this paper, a genetic algorithm (GA)optimization based waveform design framework is used to improve the features ofradar pulses relying on the orthogonal frequency division multiplexing (OFDM)structure. Our optimization techniques focus on finding optimal phase codesequences for the OFDM signal. Several optimality criteria are used since weconsider two different radar processing solutions which call either for singleor multiple-objective optimizations. When minimization of the so-calledpeak-to-mean envelope power ratio (PMEPR) single-objective is tackled, wecompare our findings with existing methods and emphasize on the merit of ourapproach. In the scope of the two-objective optimization, we first addressPMEPR and peak-to-sidelobe level ratio (PSLR) and show that our approach basedon the non-dominated sorting genetic algorithm-II (NSGA-II) provides designsolutions with noticeable improvements as opposed to random sets of phasecodes. We then look at another case of interest where the objective functionsare two measures of the sidelobe level, namely PSLR and the integrated-sidelobelevel ratio (ISLR) and propose to modify the NSGA-II to include a constrain onthe PMEPR instead. In the last part, we illustrate via a case study how ourencoding solution makes it possible to minimize the single objective PMEPRwhile enabling a target detection enhancement strategy, when the SNR metricwould be chosen for the detection framework.
arxiv-11100-120 | Stereoscopic Cinema | http://arxiv.org/pdf/1506.06001v1.pdf | author:Frédéric Devernay, Paul Beardsley category:cs.CV published:2015-06-19 summary:Stereoscopic cinema has seen a surge of activity in recent years, and for thefirst time all of the major Hollywood studios released 3-D movies in 2009. Thisis happening alongside the adoption of 3-D technology for sports broadcasting,and the arrival of 3-D TVs for the home. Two previous attempts to introduce 3-Dcinema in the 1950s and the 1980s failed because the contemporary technologywas immature and resulted in viewer discomfort. But current technologies --such as accurately-adjustable 3-D camera rigs with onboard computers toautomatically inform a camera operator of inappropriate stereoscopic shots,digital processing for post-shooting rectification of the 3-D imagery, digitalprojectors for accurate positioning of the two stereo projections on the cinemascreen, and polarized silver screens to reduce cross-talk between the viewersleft- and right-eyes -- mean that the viewer experience is at a much higherlevel of quality than in the past. Even so, creation of stereoscopic cinema isan open, active research area, and there are many challenges from acquisitionto post-production to automatic adaptation for different-sized display. Thischapter describes the current state-of-the-art in stereoscopic cinema, anddirections of future work.
arxiv-11100-121 | Enhanced Lasso Recovery on Graph | http://arxiv.org/pdf/1506.05985v1.pdf | author:Xavier Bresson, Thomas Laurent, James von Brecht category:cs.LG stat.ML published:2015-06-19 summary:This work aims at recovering signals that are sparse on graphs. Compressedsensing offers techniques for signal recovery from a few linear measurementsand graph Fourier analysis provides a signal representation on graph. In thispaper, we leverage these two frameworks to introduce a new Lasso recoveryalgorithm on graphs. More precisely, we present a non-convex, non-smoothalgorithm that outperforms the standard convex Lasso technique. We carry outnumerical experiments on three benchmark graph datasets.
arxiv-11100-122 | Corpus sp{é}cialis{é} et ressource de sp{é}cialit{é} | http://arxiv.org/pdf/0801.1179v2.pdf | author:Bernard Jacquemin, Sabine Ploux category:cs.IR cs.CL published:2008-01-08 summary:"Semantic Atlas" is a mathematic and statistic model to visualise word sensesaccording to relations between words. The model, that has been applied toproximity relations from a corpus, has shown its ability to distinguish wordsenses as the corpus' contributors comprehend them. We propose to use the modeland a specialised corpus in order to create automatically a specialiseddictionary relative to the corpus' domain. A morpho-syntactic analysisperformed on the corpus makes it possible to create the dictionary fromsyntactic relations between lexical units. The semantic resource can be used tonavigate semantically - and not only lexically - through the corpus, to createclassical dictionaries or for diachronic studies of the language.
arxiv-11100-123 | Montblanc: GPU accelerated Radio Interferometer Measurement Equations in support of Bayesian Inference for Radio Observations | http://arxiv.org/pdf/1501.07719v3.pdf | author:Simon Perkins, Patrick Marais, Jonathan Zwart, Iniyan Natarajan, Cyril Tasse, Oleg Smirnov category:cs.DC astro-ph.IM cs.CV published:2015-01-30 summary:We present Montblanc, a GPU implementation of the Radio interferometermeasurement equation (RIME) in support of the Bayesian inference for radioobservations (BIRO) technique. BIRO uses Bayesian inference to select skymodels that best match the visibilities observed by a radio interferometer. Toaccomplish this, BIRO evaluates the RIME multiple times, varying sky modelparameters to produce multiple model visibilities. Chi-squared values computedfrom the model and observed visibilities are used as likelihood values to drivethe Bayesian sampling process and select the best sky model. As most of the elements of the RIME and chi-squared calculation areindependent of one another, they are highly amenable to parallel computation.Additionally, Montblanc caters for iterative RIME evaluation to producemultiple chi-squared values. Modified model parameters are transferred to theGPU between each iteration. We implemented Montblanc as a Python package based upon NVIDIA's CUDAarchitecture. As such, it is easy to extend and implement different pipelines.At present, Montblanc supports point and Gaussian morphologies, but is designedfor easy addition of new source profiles. Montblanc's RIME implementation is performant: On an NVIDIA K40, it isapproximately 250 times faster than MeqTrees on a dual hexacore Intel E5-2620v2CPU. Compared to the OSKAR simulator's GPU-implemented RIME components it is7.7 and 12 times faster on the same K40 for single and double-precisionfloating point respectively. However, OSKAR's RIME implementation is moregeneral than Montblanc's BIRO-tailored RIME. Theoretical analysis of Montblanc's dominant CUDA kernel suggests that it ismemory bound. In practice, profiling shows that is balanced between compute andmemory, as much of the data required by the problem is retained in L1 and L2cache.
arxiv-11100-124 | Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks | http://arxiv.org/pdf/1406.6909v2.pdf | author:Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox category:cs.LG cs.CV cs.NE published:2014-06-26 summary:Deep convolutional networks have proven to be very successful in learningtask specific features that allow for unprecedented performance on variouscomputer vision tasks. Training of such networks follows mostly the supervisedlearning paradigm, where sufficiently many input-output pairs are required fortraining. Acquisition of large training sets is one of the key challenges, whenapproaching a new task. In this paper, we aim for generic feature learning andpresent an approach for training a convolutional network using only unlabeleddata. To this end, we train the network to discriminate between a set ofsurrogate classes. Each surrogate class is formed by applying a variety oftransformations to a randomly sampled 'seed' image patch. In contrast tosupervised network training, the resulting feature representation is not classspecific. It rather provides robustness to the transformations that have beenapplied during training. This generic feature representation allows forclassification results that outperform the state of the art for unsupervisedlearning on several popular datasets (STL-10, CIFAR-10, Caltech-101,Caltech-256). While such generic features cannot compete with class specificfeatures from supervised training on a classification task, we show that theyare advantageous on geometric matching problems, where they also outperform theSIFT descriptor.
arxiv-11100-125 | A new Initial Centroid finding Method based on Dissimilarity Tree for K-means Algorithm | http://arxiv.org/pdf/1509.03200v1.pdf | author:Abhishek Kumar, Suresh Chandra Gupta category:cs.LG published:2015-06-19 summary:Cluster analysis is one of the primary data analysis technique in data miningand K-means is one of the commonly used partitioning clustering algorithm. InK-means algorithm, resulting set of clusters depend on the choice of initialcentroids. If we can find initial centroids which are coherent with thearrangement of data, the better set of clusters can be obtained. This paperproposes a method based on the Dissimilarity Tree to find, the better initialcentroid as well as every bit more accurate cluster with less computationaltime. Theory analysis and experimental results indicate that the proposedmethod can effectively improve the accuracy of clusters and reduce thecomputational complexity of the K-means algorithm.
arxiv-11100-126 | Spectral Analysis of Symmetric and Anti-Symmetric Pairwise Kernels | http://arxiv.org/pdf/1506.05950v1.pdf | author:Tapio Pahikkala, Markus Viljanen, Antti Airola, Willem Waegeman category:cs.LG stat.ML published:2015-06-19 summary:We consider the problem of learning regression functions from pairwise datawhen there exists prior knowledge that the relation to be learned is symmetricor anti-symmetric. Such prior knowledge is commonly enforced by symmetrizing oranti-symmetrizing pairwise kernel functions. Through spectral analysis, we showthat these transformations reduce the kernel's effective dimension. Further, weprovide an analysis of the approximation properties of the resulting kernels,and bound the regularization bias of the kernels in terms of the correspondingbias of the original kernel.
arxiv-11100-127 | A Tight Runtime Analysis of the $(1+(λ, λ))$ Genetic Algorithm on OneMax | http://arxiv.org/pdf/1506.05937v1.pdf | author:Benjamin Doerr, Carola Doerr category:cs.NE published:2015-06-19 summary:Understanding how crossover works is still one of the big challenges inevolutionary computation research, and making our understanding precise andproven by mathematical means might be an even bigger one. As one of fewexamples where crossover provably is useful, the $(1+(\lambda, \lambda))$Genetic Algorithm (GA) was proposed recently in [Doerr, Doerr, Ebel: TCS 2015].Using the fitness level method, the expected optimization time on generalOneMax functions was analyzed and a $O(\max\{n\log(n)/\lambda, \lambda n\})$bound was proven for any offspring population size $\lambda \in [1..n]$. We improve this work in several ways, leading to sharper bounds and a betterunderstanding of how the use of crossover speeds up the runtime in thisalgorithm. We first improve the upper bound on the runtime to$O(\max\{n\log(n)/\lambda, n\lambda \log\log(\lambda)/\log(\lambda)\})$. Thisimprovement is made possible from observing that in the parallel generation of$\lambda$ offspring via crossover (but not mutation), the best of these oftenis better than the expected value, and hence several fitness levels can begained in one iteration. We then present the first lower bound for this problem. It matches our upperbound for all values of $\lambda$. This allows to determine the asymptoticallyoptimal value for the population size. It is $\lambda =\Theta(\sqrt{\log(n)\log\log(n)/\log\log\log(n)})$, which gives an optimizationtime of $\Theta(n \sqrt{\log(n)\log\log\log(n)/\log\log(n)})$. Hence theimproved runtime analysis gives a better runtime guarantee along with a bettersuggestion for the parameter $\lambda$. We finally give a tail bound for the upper tail of the runtime distribution,which shows that the actual runtime exceeds our runtime guarantee by a factorof $(1+\delta)$ with probability $O((n/\lambda^2)^{-\delta})$ only.
arxiv-11100-128 | Sampling constrained probability distributions using Spherical Augmentation | http://arxiv.org/pdf/1506.05936v1.pdf | author:Shiwei Lan, Babak Shahbaba category:stat.CO stat.ML published:2015-06-19 summary:Statistical models with constrained probability distributions are abundant inmachine learning. Some examples include regression models with norm constraints(e.g., Lasso), probit, many copula models, and latent Dirichlet allocation(LDA). Bayesian inference involving probability distributions confined toconstrained domains could be quite challenging for commonly used samplingalgorithms. In this paper, we propose a novel augmentation technique thathandles a wide range of constraints by mapping the constrained domain to asphere in the augmented space. By moving freely on the surface of this sphere,sampling algorithms handle constraints implicitly and generate proposals thatremain within boundaries when mapped back to the original space. Our proposedmethod, called {Spherical Augmentation}, provides a mathematically natural andcomputationally efficient framework for sampling from constrained probabilitydistributions. We show the advantages of our method over state-of-the-artsampling algorithms, such as exact Hamiltonian Monte Carlo, using severalexamples including truncated Gaussian distributions, Bayesian Lasso, Bayesianbridge regression, reconstruction of quantized stationary Gaussian process, andLDA for topic modeling.
arxiv-11100-129 | Expectation Particle Belief Propagation | http://arxiv.org/pdf/1506.05934v1.pdf | author:Thibaut Lienart, Yee Whye Teh, Arnaud Doucet category:stat.CO cs.AI stat.ML published:2015-06-19 summary:We propose an original particle-based implementation of the Loopy BeliefPropagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on acontinuous state space. The algorithm constructs adaptively efficient proposaldistributions approximating the local beliefs at each note of the MRF. This isachieved by considering proposal distributions in the exponential family whoseparameters are updated iterately in an Expectation Propagation (EP) framework.The proposed particle scheme provides consistent estimation of the LBPmarginals as the number of particles increases. We demonstrate that it providesmore accurate results than the Particle Belief Propagation (PBP) algorithm ofIhler and McAllester (2009) at a fraction of the computational cost and isadditionally more robust empirically. The computational complexity of ouralgorithm at each iteration is quadratic in the number of particles. We alsopropose an accelerated implementation with sub-quadratic computationalcomplexity which still provides consistent estimates of the loopy BP marginaldistributions and performs almost as well as the original procedure.
arxiv-11100-130 | Exploring the influence of scale on artist attribution | http://arxiv.org/pdf/1506.05929v1.pdf | author:Nanne van Noord, Eric Postma category:cs.CV published:2015-06-19 summary:Previous work has shown that the artist of an artwork can be identified byuse of computational methods that analyse digital images. However, thedigitised artworks are often investigated at a coarse scale discarding many ofthe important details that may define an artist's style. In recent years highresolution images of artworks have become available, which, combined withincreased processing power and new computational techniques, allow us toanalyse digital images of artworks at a very fine scale. In this work we trainand evaluate a Convolutional Neural Network (CNN) on the task of artistattribution using artwork images of varying resolutions. To this end, wecombine two existing methods to enable the application of high resolutionimages to CNNs. By comparing the attribution performances obtained at differentscales, we find that in most cases finer scales are beneficial to theattribution performance, whereas for a minority of the artists, coarser scalesappear to be preferable. We conclude that artist attribution would benefit froma multi-scale CNN approach which vastly expands the possibilities forcomputational art forensics.
arxiv-11100-131 | New Descriptor for Glomerulus Detection in Kidney Microscopy Image | http://arxiv.org/pdf/1506.05920v1.pdf | author:Tsuyoshi Kato, Raissa Relator, Hayliang Ngouv, Yoshihiro Hirohashi, Tetsuhiro Kakimoto, Kinya Okada category:cs.CV published:2015-06-19 summary:Glomerulus detection is a key step in histopathological evaluation ofmicroscopy images of kidneys. However, the task of automatic detection ofglomeruli poses challenges due to the disparity in sizes and shapes ofglomeruli in renal sections. Moreover, extensive variations of theirintensities due to heterogeneity in immunohistochemistry staining are alsoencountered. Despite being widely recognized as a powerful descriptor forgeneral object detection, the rectangular histogram of oriented gradients(Rectangular HOG) suffers from many false positives due to the aforementioneddifficulties in the context of glomerulus detection. A new descriptor referred to as Segmental HOG is developed to perform acomprehensive detection of hundreds of glomeruli in images of whole kidneysections. The new descriptor possesses flexible blocks that can be adaptivelyfitted to input images to acquire robustness to deformations of glomeruli.Moreover, the novel segmentation technique employed herewith generates highquality segmentation outputs and the algorithm is assured to converge to anoptimal solution. Consequently, experiments using real world image data revealthat Segmental HOG achieves significant improvements in detection performancecompared to Rectangular HOG. The proposed descriptor and method for glomeruli detection present promisingresults and is expected to be useful in pathological evaluation.
arxiv-11100-132 | Solving Problems with Unknown Solution Length at (Almost) No Extra Cost | http://arxiv.org/pdf/1506.05913v1.pdf | author:Benjamin Doerr, Carola Doerr, Timo Kötzing category:cs.NE published:2015-06-19 summary:Most research in the theory of evolutionary computation assumes that theproblem at hand has a fixed problem size. This assumption does not always applyto real-world optimization challenges, where the length of an optimal solutionmay be unknown a priori. Following up on previous work of Cathabard, Lehre, and Yao [FOGA 2011] weanalyze variants of the (1+1) evolutionary algorithm for problems with unknownsolution length. For their setting, in which the solution length is sampledfrom a geometric distribution, we provide mutation rates that yield an expectedoptimization time that is of the same order as that of the (1+1) EA knowing thesolution length. We then show that almost the same run times can be achieved even if \emph{no}a priori information on the solution length is available. Finally, we provide mutation rates suitable for settings in which neither thesolution length nor the positions of the relevant bits are known. Again weobtain almost optimal run times for the \textsc{OneMax} and\textsc{LeadingOnes} test functions, thus solving an open problem fromCathabard et al.
arxiv-11100-133 | Deep Knowledge Tracing | http://arxiv.org/pdf/1506.05908v1.pdf | author:Chris Piech, Jonathan Spencer, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas Guibas, Jascha Sohl-Dickstein category:cs.AI cs.CY cs.LG K.3.1 published:2015-06-19 summary:Knowledge tracing---where a machine models the knowledge of a student as theyinteract with coursework---is a well established problem in computer supportededucation. Though effectively modeling student knowledge would have higheducational impact, the task has many inherent challenges. In this paper weexplore the utility of using Recurrent Neural Networks (RNNs) to model studentlearning. The RNN family of models have important advantages over previousmethods in that they do not require the explicit encoding of human domainknowledge, and can capture more complex representations of student knowledge.Using neural networks results in substantial improvements in predictionperformance on a range of knowledge tracing datasets. Moreover the learnedmodel can be used for intelligent curriculum design and allows straightforwardinterpretation and discovery of structure in student tasks. These resultssuggest a promising new line of research for knowledge tracing and an exemplaryapplication task for RNNs.
arxiv-11100-134 | Representation Learning for Clustering: A Statistical Framework | http://arxiv.org/pdf/1506.05900v1.pdf | author:Hassan Ashtiani, Shai Ben-David category:stat.ML cs.LG published:2015-06-19 summary:We address the problem of communicating domain knowledge from a user to thedesigner of a clustering algorithm. We propose a protocol in which the userprovides a clustering of a relatively small random sample of a data set. Thealgorithm designer then uses that sample to come up with a data representationunder which $k$-means clustering results in a clustering (of the full data set)that is aligned with the user's clustering. We provide a formal statisticalmodel for analyzing the sample complexity of learning a clusteringrepresentation with this paradigm. We then introduce a notion of capacity of aclass of possible representations, in the spirit of the VC-dimension, showingthat classes of representations that have finite such dimension can besuccessfully learned with sample size error bounds, and end our discussion withan analysis of that dimension for classes of representations induced by linearembeddings.
arxiv-11100-135 | Novel Bernstein-like Concentration Inequalities for the Missing Mass | http://arxiv.org/pdf/1503.02768v2.pdf | author:Bahman Yari Saeed Khanloo, Gholamreza Haffari category:stat.ML published:2015-03-10 summary:We are concerned with obtaining novel concentration inequalities for themissing mass, i.e. the total probability mass of the outcomes not observed inthe sample. We not only derive - for the first time - distribution-freeBernstein-like deviation bounds with sublinear exponents in deviation size formissing mass, but also improve the results of McAllester and Ortiz (2003)andBerend and Kontorovich (2013, 2012) for small deviations which is the mostinteresting case in learning theory. It is known that the majority of standardinequalities cannot be directly used to analyze heterogeneous sums i.e. sumswhose terms have large difference in magnitude. Our generic and intuitiveapproach shows that the heterogeneity issue introduced in McAllester and Ortiz(2003) is resolvable at least in the case of missing mass via regulating theterms using our novel thresholding technique.
arxiv-11100-136 | Strongly Adaptive Online Learning | http://arxiv.org/pdf/1502.07073v3.pdf | author:Amit Daniely, Alon Gonen, Shai Shalev-Shwartz category:cs.LG published:2015-02-25 summary:Strongly adaptive algorithms are algorithms whose performance on every timeinterval is close to optimal. We present a reduction that can transformstandard low-regret algorithms to strongly adaptive. As a consequence, wederive simple, yet efficient, strongly adaptive algorithms for a handful ofproblems.
arxiv-11100-137 | To Know Where We Are: Vision-Based Positioning in Outdoor Environments | http://arxiv.org/pdf/1506.05870v1.pdf | author:Kuan-Wen Chen, Chun-Hsin Wang, Xiao Wei, Qiao Liang, Ming-Hsuan Yang, Chu-Song Chen, Yi-Ping Hung category:cs.CV published:2015-06-19 summary:Augmented reality (AR) displays become more and more popular recently,because of its high intuitiveness for humans and high-quality head-mounteddisplay have rapidly developed. To achieve such displays with augmentedinformation, highly accurate image registration or ego-positioning arerequired, but little attention have been paid for out-door environments. Thispaper presents a method for ego-positioning in outdoor environments with lowcost monocular cameras. To reduce the computational and memory requirements aswell as the communication overheads, we formulate the model compressionalgorithm as a weighted k-cover problem for better preserving model structures.Specifically for real-world vision-based positioning applications, we considerthe issues with large scene change and propose a model update algorithm totackle these problems. A long- term positioning dataset with more than onemonth, 106 sessions, and 14,275 images is constructed. Based on both local andup-to-date models constructed in our approach, extensive experimental resultsshow that high positioning accuracy (mean ~ 30.9cm, stdev. ~ 15.4cm) can beachieved, which outperforms existing vision-based algorithms.
arxiv-11100-138 | Stay on path: PCA along graph paths | http://arxiv.org/pdf/1506.02344v2.pdf | author:Megasthenis Asteris, Anastasios Kyrillidis, Alexandros G. Dimakis, Han-Gyol Yi and, Bharath Chandrasekaran category:stat.ML cs.IT cs.LG math.IT math.OC published:2015-06-08 summary:We introduce a variant of (sparse) PCA in which the set of feasible supportsets is determined by a graph. In particular, we consider the followingsetting: given a directed acyclic graph $G$ on $p$ vertices corresponding tovariables, the non-zero entries of the extracted principal component mustcoincide with vertices lying along a path in $G$. From a statistical perspective, information on the underlying network maypotentially reduce the number of observations required to recover thepopulation principal component. We consider the canonical estimator whichoptimally exploits the prior knowledge by solving a non-convex quadraticmaximization on the empirical covariance. We introduce a simple network andanalyze the estimator under the spiked covariance model. We show that sideinformation potentially improves the statistical complexity. We propose two algorithms to approximate the solution of the constrainedquadratic maximization, and recover a component with the desired properties. Weempirically evaluate our schemes on synthetic and real datasets.
arxiv-11100-139 | A simple application of FIC to model selection | http://arxiv.org/pdf/1506.06129v1.pdf | author:Paul A. Wiggins category:cs.LG stat.ML published:2015-06-19 summary:We have recently proposed a new information-based approach to modelselection, the Frequentist Information Criterion (FIC), that reconcilesinformation-based and frequentist inference. The purpose of this current paperis to provide a simple example of the application of this criterion and ademonstration of the natural emergence of model complexities with both AIC-like($N^0$) and BIC-like ($\log N$) scaling with observation number $N$. Theapplication developed is deliberately simplified to make the analysisanalytically tractable.
arxiv-11100-140 | An Iterative Convolutional Neural Network Algorithm Improves Electron Microscopy Image Segmentation | http://arxiv.org/pdf/1506.05849v1.pdf | author:Xundong Wu category:cs.NE cs.LG published:2015-06-18 summary:To build the connectomics map of the brain, we developed a new algorithm thatcan automatically refine the Membrane Detection Probability Maps (MDPM)generated to perform automatic segmentation of electron microscopy (EM) images.To achieve this, we executed supervised training of a convolutional neuralnetwork to recover the removed center pixel label of patches sampled from aMDPM. MDPM can be generated from other machine learning based algorithmsrecognizing whether a pixel in an image corresponds to the cell membrane. Byiteratively applying this network over MDPM for multiple rounds, we were ableto significantly improve membrane segmentation results.
arxiv-11100-141 | Dependent Multinomial Models Made Easy: Stick Breaking with the Pólya-Gamma Augmentation | http://arxiv.org/pdf/1506.05843v1.pdf | author:Scott W. Linderman, Matthew J. Johnson, Ryan P. Adams category:stat.ML published:2015-06-18 summary:Many practical modeling problems involve discrete data that are bestrepresented as draws from multinomial or categorical distributions. Forexample, nucleotides in a DNA sequence, children's names in a given state andyear, and text documents are all commonly modeled with multinomialdistributions. In all of these cases, we expect some form of dependency betweenthe draws: the nucleotide at one position in the DNA strand may depend on thepreceding nucleotides, children's names are highly correlated from year toyear, and topics in text may be correlated and dynamic. These dependencies arenot naturally captured by the typical Dirichlet-multinomial formulation. Here,we leverage a logistic stick-breaking representation and recent innovations inP\'olya-gamma augmentation to reformulate the multinomial distribution in termsof latent variables with jointly Gaussian likelihoods, enabling us to takeadvantage of a host of Bayesian inference techniques for Gaussian models withminimal overhead.
arxiv-11100-142 | Cheap Bandits | http://arxiv.org/pdf/1506.04782v2.pdf | author:Manjesh Kumar Hanawal, Venkatesh Saligrama, Michal Valko, R\' emi Munos category:cs.LG published:2015-06-15 summary:We consider stochastic sequential learning problems where the learner canobserve the \textit{average reward of several actions}. Such a setting isinteresting in many applications involving monitoring and surveillance, wherethe set of the actions to observe represent some (geographical) area. Theimportance of this setting is that in these applications, it is actually\textit{cheaper} to observe average reward of a group of actions rather thanthe reward of a single action. We show that when the reward is \textit{smooth}over a given graph representing the neighboring actions, we can maximize thecumulative reward of learning while \textit{minimizing the sensing cost}. Inthis paper we propose CheapUCB, an algorithm that matches the regret guaranteesof the known algorithms for this setting and at the same time guarantees alinear cost again over them. As a by-product of our analysis, we establish a$\Omega(\sqrt{dT})$ lower bound on the cumulative regret of spectral banditsfor a class of graphs with effective dimension $d$.
arxiv-11100-143 | Optimal model-free prediction from multivariate time series | http://arxiv.org/pdf/1506.05822v1.pdf | author:Jakob Runge, Reik V. Donner, Jürgen Kurths category:stat.ML stat.ME published:2015-06-18 summary:Forecasting a time series from multivariate predictors constitutes achallenging problem, especially using model-free approaches. Most techniques,such as nearest-neighbor prediction, quickly suffer from the curse ofdimensionality and overfitting for more than a few predictors which has limitedtheir application mostly to the univariate case. Therefore, selectionstrategies are needed that harness the available information as efficiently aspossible. Since often the right combination of predictors matters, ideally allsubsets of possible predictors should be tested for their predictive power, butthe exponentially growing number of combinations makes such an approachcomputationally prohibitive. Here a prediction scheme that overcomes thisstrong limitation is introduced utilizing a causal pre-selection step whichdrastically reduces the number of possible predictors to the most predictiveset of causal drivers making a globally optimal search scheme tractable. Theinformation-theoretic optimality is derived and practical selection criteriaare discussed. As demonstrated for multivariate nonlinear stochastic delayprocesses, the optimal scheme can even be less computationally expensive thancommonly used sub-optimal schemes like forward selection. The method suggests ageneral framework to apply the optimal model-free approach to select variablesand subsequently fit a model to further improve a prediction or learnstatistical dependencies. The performance of this framework is illustrated on aclimatological index of El Ni\~no Southern Oscillation.
arxiv-11100-144 | Convolutional Dictionary Learning through Tensor Factorization | http://arxiv.org/pdf/1506.03509v3.pdf | author:Furong Huang, Animashree Anandkumar category:cs.LG stat.ML published:2015-06-10 summary:Tensor methods have emerged as a powerful paradigm for consistent learning ofmany latent variable models such as topic models, independent componentanalysis and dictionary learning. Model parameters are estimated via CPdecomposition of the observed higher order input moments. However, in manydomains, additional invariances such as shift invariances exist, enforced viamodels such as convolutional dictionary learning. In this paper, we developnovel tensor decomposition algorithms for parameter estimation of convolutionalmodels. Our algorithm is based on the popular alternating least squares method,but with efficient projections onto the space of stacked circulant matrices.Our method is embarrassingly parallel and consists of simple operations such asfast Fourier transforms and matrix multiplications. Our algorithm converges tothe dictionary much faster and more accurately compared to the alternatingminimization over filters and activation maps.
arxiv-11100-145 | Optimally Combining Classifiers Using Unlabeled Data | http://arxiv.org/pdf/1503.01811v3.pdf | author:Akshay Balsubramani, Yoav Freund category:cs.LG stat.ML published:2015-03-05 summary:We develop a worst-case analysis of aggregation of classifier ensembles forbinary classification. The task of predicting to minimize error is formulatedas a game played over a given set of unlabeled data (a transductive setting),where prior label information is encoded as constraints on the game. Theminimax solution of this game identifies cases where a weighted combination ofthe classifiers can perform significantly better than any single classifier.
arxiv-11100-146 | A tree augmented naive Bayesian network experiment for breast cancer prediction | http://arxiv.org/pdf/1506.05776v1.pdf | author:Ping Ren category:stat.ML q-bio.QM published:2015-06-18 summary:In order to investigate the breast cancer prediction problem on the agingpopulation with the grades of DCIS, we conduct a tree augmented naive Bayesiannetwork experiment trained and tested on a large clinical dataset includingconsecutive diagnostic mammography examinations, consequent biopsy outcomes andrelated cancer registry records in the population of women across all ages. Theaggregated results of our ten-fold cross validation method recommend a biopsythreshold higher than 2% for the aging population.
arxiv-11100-147 | Multi-Context Models for Reasoning under Partial Knowledge: Generative Process and Inference Grammar | http://arxiv.org/pdf/1412.4271v2.pdf | author:Ardavan Salehi Nobandegani, Ioannis N. Psaromiligkos category:cs.AI math.LO math.PR stat.ML published:2014-12-13 summary:Arriving at the complete probabilistic knowledge of a domain, i.e., learninghow all variables interact, is indeed a demanding task. In reality, settingsoften arise for which an individual merely possesses partial knowledge of thedomain, and yet, is expected to give adequate answers to a variety of posedqueries. That is, although precise answers to some queries, in principle,cannot be achieved, a range of plausible answers is attainable for each querygiven the available partial knowledge. In this paper, we propose theMulti-Context Model (MCM), a new graphical model to represent the state ofpartial knowledge as to a domain. MCM is a middle ground between ProbabilisticLogic, Bayesian Logic, and Probabilistic Graphical Models. For this model wediscuss: (i) the dynamics of constructing a contradiction-free MCM, i.e., toform partial beliefs regarding a domain in a gradual and probabilisticallyconsistent way, and (ii) how to perform inference, i.e., to evaluate aprobability of interest involving some variables of the domain.
arxiv-11100-148 | Integer Programming Ensemble of Classifiers for Temporal Relations | http://arxiv.org/pdf/1412.1866v2.pdf | author:Catherine Kerr, Terri Hoare, Jakub Marecek, Paula Carroll category:cs.CL cs.LG math.OC published:2014-12-05 summary:Extraction of events and understanding related temporal expression among themis a major challenge in natural language processing. In longer texts,processing on sentence-by-sentence or expression-by-expression basis oftenfails, in part due to the disregard for the consistency of the processed data.We present an ensemble method, which reconciles the output of multipleclassifiers for temporal expressions, subject to consistency constraints acrossthe whole text. The use of integer programming to enforce the consistencyconstraints globally improves upon the best published results from theTempEval-3 Challenge considerably.
arxiv-11100-149 | A Fast Incremental Gaussian Mixture Model | http://arxiv.org/pdf/1506.04422v2.pdf | author:Rafael Pinto, Paulo Engel category:cs.LG I.2.6 published:2015-06-14 summary:This work builds upon previous efforts in online incremental learning, namelythe Incremental Gaussian Mixture Network (IGMN). The IGMN is capable oflearning from data streams in a single-pass by improving its model afteranalyzing each data point and discarding it thereafter. Nevertheless, itsuffers from the scalability point-of-view, due to its asymptotic timecomplexity of $\operatorname{O}\bigl(NKD^3\bigr)$ for $N$ data points, $K$Gaussian components and $D$ dimensions, rendering it inadequate forhigh-dimensional data. In this paper, we manage to reduce this complexity to$\operatorname{O}\bigl(NKD^2\bigr)$ by deriving formulas for working directlywith precision matrices instead of covariance matrices. The final result is amuch faster and scalable algorithm which can be applied to high dimensionaltasks. This is confirmed by applying the modified algorithm to high-dimensionalclassification datasets.
arxiv-11100-150 | Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks | http://arxiv.org/pdf/1506.05751v1.pdf | author:Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus category:cs.CV published:2015-06-18 summary:In this paper we introduce a generative parametric model capable of producinghigh quality samples of natural images. Our approach uses a cascade ofconvolutional networks within a Laplacian pyramid framework to generate imagesin a coarse-to-fine fashion. At each level of the pyramid, a separategenerative convnet model is trained using the Generative Adversarial Nets (GAN)approach (Goodfellow et al.). Samples drawn from our model are of significantlyhigher quality than alternate approaches. In a quantitative assessment by humanevaluators, our CIFAR10 samples were mistaken for real images around 40% of thetime, compared to 10% for samples drawn from a GAN baseline model. We also showsamples from models trained on the higher resolution images of the LSUN scenedataset.
arxiv-11100-151 | From Pixels to Torques: Policy Learning with Deep Dynamical Models | http://arxiv.org/pdf/1502.02251v3.pdf | author:Niklas Wahlström, Thomas B. Schön, Marc Peter Deisenroth category:stat.ML cs.LG cs.RO cs.SY published:2015-02-08 summary:Data-efficient learning in continuous state-action spaces using veryhigh-dimensional observations remains a key challenge in developing fullyautonomous systems. In this paper, we consider one instance of this challenge,the pixels to torques problem, where an agent must learn a closed-loop controlpolicy from pixel information only. We introduce a data-efficient, model-basedreinforcement learning algorithm that learns such a closed-loop policy directlyfrom pixel information. The key ingredient is a deep dynamical model that usesdeep auto-encoders to learn a low-dimensional embedding of images jointly witha predictive model in this low-dimensional feature space. Joint learningensures that not only static but also dynamic properties of the data areaccounted for. This is crucial for long-term predictions, which lie at the coreof the adaptive model predictive control strategy that we use for closed-loopcontrol. Compared to state-of-the-art reinforcement learning methods forcontinuous states and actions, our approach learns quickly, scales tohigh-dimensional state spaces and is an important step toward fully autonomouslearning from pixels to torques.
arxiv-11100-152 | On the accuracy of self-normalized log-linear models | http://arxiv.org/pdf/1506.04147v2.pdf | author:Jacob Andreas, Maxim Rabinovich, Dan Klein, Michael I. Jordan category:stat.ML cs.CL cs.LG stat.ME published:2015-06-12 summary:Calculation of the log-normalizer is a major computational obstacle inapplications of log-linear models with large output spaces. The problem of fastnormalizer computation has therefore attracted significant attention in thetheoretical and applied machine learning literature. In this paper, we analyzea recently proposed technique known as "self-normalization", which introduces aregularization term in training to penalize log normalizers for deviating fromzero. This makes it possible to use unnormalized model scores as approximateprobabilities. Empirical evidence suggests that self-normalization is extremelyeffective, but a theoretical understanding of why it should work, and howgenerally it can be applied, is largely lacking. We prove generalization boundson the estimated variance of normalizers and upper bounds on the loss inaccuracy due to self-normalization, describe classes of input distributionsthat self-normalize easily, and construct explicit examples of high-varianceinput distributions. Our theoretical results make predictions about thedifficulty of fitting self-normalized models to several classes ofdistributions, and we conclude with empirical validation of these predictions.
arxiv-11100-153 | "The Sum of Its Parts": Joint Learning of Word and Phrase Representations with Autoencoders | http://arxiv.org/pdf/1506.05703v1.pdf | author:Rémi Lebret, Ronan Collobert category:cs.CL published:2015-06-18 summary:Recently, there has been a lot of effort to represent words in continuousvector spaces. Those representations have been shown to capture both semanticand syntactic information about words. However, distributed representations ofphrases remain a challenge. We introduce a novel model that jointly learns wordvector representations and their summation. Word representations are learntusing the word co-occurrence statistical information. To embed sequences ofwords (i.e. phrases) with different sizes into a common semantic space, wepropose to average word vector representations. In contrast with previousmethods which reported a posteriori some compositionality aspects by simplesummation, we simultaneously train words to sum, while keeping the maximuminformation from the original vectors. We evaluate the quality of the wordrepresentations on several classical word evaluation tasks, and we introduce anovel task to evaluate the quality of the phrase representations. While ourdistributed representations compete with other methods of learning wordrepresentations on word evaluations, we show that they give better performanceon the phrase evaluation. Such representations of phrases could be interestingfor many tasks in natural language processing.
arxiv-11100-154 | A hybrid algorithm for Bayesian network structure learning with application to multi-label learning | http://arxiv.org/pdf/1506.05692v1.pdf | author:Maxime Gasse, Alex Aussem, Haytham Elghazel category:stat.ML cs.AI cs.LG published:2015-06-18 summary:We present a novel hybrid algorithm for Bayesian network structure learning,called H2PC. It first reconstructs the skeleton of a Bayesian network and thenperforms a Bayesian-scoring greedy hill-climbing search to orient the edges.The algorithm is based on divide-and-conquer constraint-based subroutines tolearn the local structure around a target variable. We conduct two series ofexperimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which iscurrently the most powerful state-of-the-art algorithm for Bayesian networkstructure learning. First, we use eight well-known Bayesian network benchmarkswith various data sizes to assess the quality of the learned structure returnedby the algorithms. Our extensive experiments show that H2PC outperforms MMHC interms of goodness of fit to new data and quality of the network structure withrespect to the true dependence structure of the data. Second, we investigateH2PC's ability to solve the multi-label learning problem. We providetheoretical results to characterize and identify graphically the so-calledminimal label powersets that appear as irreducible factors in the jointdistribution under the faithfulness condition. The multi-label learning problemis then decomposed into a series of multi-class classification problems, whereeach multi-class variable encodes a label powerset. H2PC is shown to comparefavorably to MMHC in terms of global classification accuracy over tenmulti-label data sets covering different application domains. Overall, ourexperiments support the conclusions that local structural learning with H2PC inthe form of local neighborhood induction is a theoretically well-motivated andempirically effective learning framework that is well suited to multi-labellearning. The source code (in R) of H2PC as well as all data sets used for theempirical tests are publicly available.
arxiv-11100-155 | Concentric network symmetry grasps authors' styles in word adjacency networks | http://arxiv.org/pdf/1504.02162v2.pdf | author:Diego R. Amancio, Filipi N. Silva, Luciano da F. Costa category:cs.CL published:2015-04-09 summary:Several characteristics of written texts have been inferred from statisticalanalysis derived from networked models. Even though many network measurementshave been adapted to study textual properties at several levels of complexity,some textual aspects have been disregarded. In this paper, we study thesymmetry of word adjacency networks, a well-known representation of text as agraph. A statistical analysis of the symmetry distribution performed in severalnovels showed that most of the words do not display symmetric patterns ofconnectivity. More specifically, the merged symmetry displayed a distributionsimilar to the ubiquitous power-law distribution. Our experiments also revealedthat the studied metrics do not correlate with other traditional networkmeasurements, such as the degree or betweenness centrality. The effectivenessof the symmetry measurements was verified in the authorship attribution task.Interestingly, we found that specific authors prefer particular types ofsymmetric motifs. As a consequence, the authorship of books could be accuratelyidentified in 82.5% of the cases, in a dataset comprising books written by 8authors. Because the proposed measurements for text analysis are complementaryto the traditional approach, they can be used to improve the characterizationof text networks, which might be useful for related applications, such as thoserelying on the identification of topical words and information retrieval.
arxiv-11100-156 | Simultaneous Estimation of Non-Gaussian Components and their Correlation Structure | http://arxiv.org/pdf/1506.05666v1.pdf | author:Hiroaki Sasaki, Michael U. Gutmann, Hayaru Shouno, Aapo Hyvärinen category:stat.ML published:2015-06-18 summary:The statistical dependencies which independent component analysis (ICA)cannot remove often provide rich information beyond the linear independentcomponents. It would thus be very useful to estimate the dependency structurefrom data. While such models have been proposed, they usually concentrated onhigher-order correlations such as energy (square) correlations. Yet, linearcorrelations are a fundamental and informative form of dependency in many realdata sets. Linear correlations are usually completely removed by ICA andrelated methods, so they can only be analyzed by developing new methods whichexplicitly allow for linearly correlated components. In this paper, we proposea probabilistic model of linear non-Gaussian components which are allowed tohave both linear and energy correlations. The precision matrix of the linearcomponents is assumed to be randomly generated by a higher-order process andexplicitly parametrized by a parameter matrix. The estimation of the parametermatrix is shown to be particularly simple because using score matching, theobjective function is a quadratic form. Using simulations with artificial data,we demonstrate that the proposed method is able to estimate non-Gaussiancomponents and their correlation structure simultaneously. Applications onsimulated complex cells with natural image input, as well as spectrograms ofnatural audio data show that the method finds new kinds of dependencies betweenthe components.
arxiv-11100-157 | FastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample Test | http://arxiv.org/pdf/1405.2664v2.pdf | author:Ji Zhao, Deyu Meng category:cs.AI cs.LG stat.ML published:2014-05-12 summary:The maximum mean discrepancy (MMD) is a recently proposed test statistic fortwo-sample test. Its quadratic time complexity, however, greatly hampers itsavailability to large-scale applications. To accelerate the MMD calculation, inthis study we propose an efficient method called FastMMD. The core idea ofFastMMD is to equivalently transform the MMD with shift-invariant kernels intothe amplitude expectation of a linear combination of sinusoid components basedon Bochner's theorem and Fourier transform (Rahimi & Recht, 2007). Takingadvantage of sampling of Fourier transform, FastMMD decreases the timecomplexity for MMD calculation from $O(N^2 d)$ to $O(L N d)$, where $N$ and $d$are the size and dimension of the sample set, respectively. Here $L$ is thenumber of basis functions for approximating kernels which determines theapproximation accuracy. For kernels that are spherically invariant, thecomputation can be further accelerated to $O(L N \log d)$ by using the Fastfoodtechnique (Le et al., 2013). The uniform convergence of our method has alsobeen theoretically proved in both unbiased and biased estimates. We havefurther provided a geometric explanation for our method, namely ensemble ofcircular discrepancy, which facilitates us to understand the insight of MMD,and is hopeful to help arouse more extensive metrics for assessing two-sampletest. Experimental results substantiate that FastMMD is with similar accuracyas exact MMD, while with faster computation speed and lower variance than theexisting MMD approximation methods.
arxiv-11100-158 | Point-wise Map Recovery and Refinement from Functional Correspondence | http://arxiv.org/pdf/1506.05603v1.pdf | author:Emanuele Rodolà, Michael Moeller, Daniel Cremers category:cs.CV cs.CG published:2015-06-18 summary:Since their introduction in the shape analysis community, functional mapshave met with considerable success due to their ability to compactly representdense correspondences between deformable shapes, with applications ranging fromshape matching and image segmentation, to exploration of large shapecollections. Despite the numerous advantages of such representation, however,the problem of converting a given functional map back to a point-to-point maphas received a surprisingly limited interest. In this paper we analyze thegeneral problem of point-wise map recovery from arbitrary functional maps. Indoing so, we rule out many of the assumptions required by the currentlyestablished approach -- most notably, the limiting requirement of the inputshapes being nearly-isometric. We devise an efficient recovery process based ona simple probabilistic model. Experiments confirm that this approach achievesremarkable accuracy improvements in very challenging cases.
arxiv-11100-159 | Collaborative Deep Learning for Recommender Systems | http://arxiv.org/pdf/1409.2944v2.pdf | author:Hao Wang, Naiyan Wang, Dit-Yan Yeung category:cs.LG cs.CL cs.IR cs.NE stat.ML published:2014-09-10 summary:Collaborative filtering (CF) is a successful approach commonly used by manyrecommender systems. Conventional CF-based methods use the ratings given toitems by users as the sole source of information for learning to makerecommendation. However, the ratings are often very sparse in manyapplications, causing CF-based methods to degrade significantly in theirrecommendation performance. To address this sparsity problem, auxiliaryinformation such as item content information may be utilized. Collaborativetopic regression (CTR) is an appealing recent method taking this approach whichtightly couples the two components that learn from two different sources ofinformation. Nevertheless, the latent representation learned by CTR may not bevery effective when the auxiliary information is very sparse. To address thisproblem, we generalize recent advances in deep learning from i.i.d. input tonon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesianmodel called collaborative deep learning (CDL), which jointly performs deeprepresentation learning for the content information and collaborative filteringfor the ratings (feedback) matrix. Extensive experiments on three real-worlddatasets from different domains show that CDL can significantly advance thestate of the art.
arxiv-11100-160 | Bethe Projections for Non-Local Inference | http://arxiv.org/pdf/1503.01397v2.pdf | author:Luke Vilnis, David Belanger, Daniel Sheldon, Andrew McCallum category:stat.ML cs.CL cs.LG published:2015-03-04 summary:Many inference problems in structured prediction are naturally solved byaugmenting a tractable dependency structure with complex, non-local auxiliaryobjectives. This includes the mean field family of variational inferencealgorithms, soft- or hard-constrained inference using Lagrangian relaxation orlinear programming, collective graphical models, and forms of semi-supervisedlearning such as posterior regularization. We present a method todiscriminatively learn broad families of inference objectives, capturingpowerful non-local statistics of the latent variables, while maintainingtractable and provably fast inference using non-Euclidean projected gradientdescent with a distance-generating function given by the Bethe entropy. Wedemonstrate the performance and flexibility of our method by (1) extractingstructured citations from research papers by learning soft global constraints,(2) achieving state-of-the-art results on a widely-used handwriting recognitiontask using a novel learned non-convex inference procedure, and (3) providing afast and highly scalable algorithm for the challenging problem of inference ina collective graphical model applied to bird migration.
arxiv-11100-161 | Comparing and evaluating extended Lambek calculi | http://arxiv.org/pdf/1506.05561v1.pdf | author:Richard Moot category:cs.CL cs.LO published:2015-06-18 summary:Lambeks Syntactic Calculus, commonly referred to as the Lambek calculus, wasinnovative in many ways, notably as a precursor of linear logic. But it alsoshowed that we could treat our grammatical framework as a logic (as opposed toa logical theory). However, though it was successful in giving at least a basictreatment of many linguistic phenomena, it was also clear that a slightly moreexpressive logical calculus was needed for many other cases. Therefore, manyextensions and variants of the Lambek calculus have been proposed, since theeighties and up until the present day. As a result, there is now a large classof calculi, each with its own empirical successes and theoretical results, butalso each with its own logical primitives. This raises the question: how do wecompare and evaluate these different logical formalisms? To answer thisquestion, I present two unifying frameworks for these extended Lambek calculi.Both are proof net calculi with graph contraction criteria. The first calculusis a very general system: you specify the structure of your sequents and itgives you the connectives and contractions which correspond to it. The calculuscan be extended with structural rules, which translate directly into graphrewrite rules. The second calculus is first-order (multiplicativeintuitionistic) linear logic, which turns out to have several other,independently proposed extensions of the Lambek calculus as fragments. I willillustrate the use of each calculus in building bridges between analysesproposed in different frameworks, in highlighting differences and in helping toidentify problems.
arxiv-11100-162 | Learning Scale-Free Networks by Dynamic Node-Specific Degree Prior | http://arxiv.org/pdf/1503.02129v3.pdf | author:Qingming Tang, Siqi Sun, Jinbo Xu category:cs.LG cs.AI stat.ML published:2015-03-07 summary:Learning the network structure underlying data is an important problem inmachine learning. This paper introduces a novel prior to study the inference ofscale-free networks, which are widely used to model social and biologicalnetworks. The prior not only favors a desirable global node degreedistribution, but also takes into consideration the relative strength of allthe possible edges adjacent to the same node and the estimated degree of eachindividual node. To fulfill this, ranking is incorporated into the prior, which makes theproblem challenging to solve. We employ an ADMM (alternating direction methodof multipliers) framework to solve the Gaussian Graphical model regularized bythis prior. Our experiments on both synthetic and real data show that our priornot only yields a scale-free network, but also produces many more correctlypredicted edges than the others such as the scale-free inducing prior, thehub-inducing prior and the $l_1$ norm.
arxiv-11100-163 | Exact Hybrid Covariance Thresholding for Joint Graphical Lasso | http://arxiv.org/pdf/1503.02128v2.pdf | author:Qingming Tang, Chao Yang, Jian Peng, Jinbo Xu category:cs.LG cs.AI stat.ML published:2015-03-07 summary:This paper considers the problem of estimating multiple related Gaussiangraphical models from a $p$-dimensional dataset consisting of differentclasses. Our work is based upon the formulation of this problem as groupgraphical lasso. This paper proposes a novel hybrid covariance thresholdingalgorithm that can effectively identify zero entries in the precision matricesand split a large joint graphical lasso problem into small subproblems. Ourhybrid covariance thresholding method is superior to existing uniformthresholding methods in that our method can split the precision matrix of eachindividual class using different partition schemes and thus split groupgraphical lasso into much smaller subproblems, each of which can be solved veryfast. In addition, this paper establishes necessary and sufficient conditionsfor our hybrid covariance thresholding algorithm. The superior performance ofour thresholding method is thoroughly analyzed and illustrated by a fewexperiments on simulated data and real gene expression data.
arxiv-11100-164 | FaceNet: A Unified Embedding for Face Recognition and Clustering | http://arxiv.org/pdf/1503.03832v3.pdf | author:Florian Schroff, Dmitry Kalenichenko, James Philbin category:cs.CV published:2015-03-12 summary:Despite significant recent advances in the field of face recognition,implementing face verification and recognition efficiently at scale presentsserious challenges to current approaches. In this paper we present a system,called FaceNet, that directly learns a mapping from face images to a compactEuclidean space where distances directly correspond to a measure of facesimilarity. Once this space has been produced, tasks such as face recognition,verification and clustering can be easily implemented using standard techniqueswith FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize theembedding itself, rather than an intermediate bottleneck layer as in previousdeep learning approaches. To train, we use triplets of roughly aligned matching/ non-matching face patches generated using a novel online triplet miningmethod. The benefit of our approach is much greater representationalefficiency: we achieve state-of-the-art face recognition performance using only128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our systemachieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves95.12%. Our system cuts the error rate in comparison to the best publishedresult by 30% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic tripletloss, which describe different versions of face embeddings (produced bydifferent networks) that are compatible to each other and allow for directcomparison between each other.
arxiv-11100-165 | Learning Contextualized Semantics from Co-occurring Terms via a Siamese Architecture | http://arxiv.org/pdf/1506.05514v1.pdf | author:Ubai Sandouk, Ke Chen category:cs.IR cs.CL cs.LG I.2.6 published:2015-06-17 summary:One of the biggest challenges in Multimedia information retrieval andunderstanding is to bridge the semantic gap by properly modeling conceptsemantics in context. The presence of out of vocabulary (OOV) conceptsexacerbates this difficulty. To address the semantic gap issues, we formulate aproblem on learning contextualized semantics from descriptive terms and proposea novel Siamese architecture to model the contextualized semantics fromdescriptive terms. By means of pattern aggregation and probabilistic topicmodels, our Siamese architecture captures contextualized semantics from theco-occurring descriptive terms via unsupervised learning, which leads to aconcept embedding space of the terms in context. Furthermore, the co-occurringOOV concepts can be easily represented in the learnt concept embedding space.The main properties of the concept embedding space are demonstrated viavisualization. Using various settings in semantic priming, we have carried outa thorough evaluation by comparing our approach to a number of state-of-the-artmethods on six annotation corpora in different domains, i.e., MagTag5K, CAL500and Million Song Dataset in the music domain as well as Corel5K, LabelMe andSUNDatabase in the image domain. Experimental results on semantic primingsuggest that our approach outperforms those state-of-the-art methodsconsiderably in various aspects.
arxiv-11100-166 | Pragmatic Side Effects | http://arxiv.org/pdf/1506.05676v1.pdf | author:Jiri Marsik, Maxime Amblard category:cs.CL published:2015-06-17 summary:In the quest to give a formal compositional semantics to natural languages,semanticists have started turning their attention to phenomena that have beenalso considered as parts of pragmatics (e.g., discourse anaphora andpresupposition projection). To account for these phenomena, the very kinds ofmeanings assigned to words and phrases are often revisited. To be morespecific, in the prevalent paradigm of modeling natural language denotationsusing the simply-typed lambda calculus (higher-order logic) this meansrevisiting the types of denotations assigned to individual parts of speech.However, the lambda calculus also serves as a fundamental theory ofcomputation, and in the study of computation, similar type shifts have beenemployed to give a meaning to side effects. Side effects in programminglanguages correspond to actions that go beyond the lexical scope of anexpression (a thrown exception might propagate throughout a program, a variablemodified at one point might later be read at an another) or even beyond thescope of the program itself (a program might interact with the outside world bye.g., printing documents, making sounds, operating robotic limbs...).
arxiv-11100-167 | Real time unsupervised learning of visual stimuli in neuromorphic VLSI systems | http://arxiv.org/pdf/1506.05427v1.pdf | author:Massimiliano Giulioni, Federico Corradi, Vittorio Dante, Paolo del Giudice category:cs.NE q-bio.NC published:2015-06-17 summary:Neuromorphic chips embody computational principles operating in the nervoussystem, into microelectronic devices. In this domain it is important toidentify computational primitives that theory and experiments suggest asgeneric and reusable cognitive elements. One such element is provided byattractor dynamics in recurrent networks. Point attractors are equilibriumstates of the dynamics (up to fluctuations), determined by the synapticstructure of the network; a `basin' of attraction comprises all initial statesleading to a given attractor upon relaxation, hence making attractor dynamicssuitable to implement robust associative memory. The initial network state isdictated by the stimulus, and relaxation to the attractor state implements theretrieval of the corresponding memorized prototypical pattern. In a previouswork we demonstrated that a neuromorphic recurrent network of spiking neuronsand suitably chosen, fixed synapses supports attractor dynamics. Here we focuson learning: activating on-chip synaptic plasticity and using a theory-drivenstrategy for choosing network parameters, we show that autonomous learning,following repeated presentation of simple visual stimuli, shapes a synapticconnectivity supporting stimulus-selective attractors. Associative memorydevelops on chip as the result of the coupled stimulus-driven neural activityand ensuing synaptic dynamics, with no artificial separation between learningand retrieval phases.
arxiv-11100-168 | Hybrid Algorithm for Multi-Objective Optimization by Greedy Hypervolume Maximization | http://arxiv.org/pdf/1506.05424v1.pdf | author:Conrado Silva Miranda, Fernando José Von Zuben category:cs.NE cs.AI published:2015-06-17 summary:This paper introduces a high-performance hybrid algorithm, called HybridHypervolume Maximization Algorithm (H2MA), for multi-objective optimizationthat alternates between exploring the decision space and exploiting the alreadyobtained non-dominated solutions. The proposal is centered on maximizing thehypervolume indicator, thus converting the multi-objective problem into asingle-objective one. The exploitation employs gradient-based methods, butconsidering a single candidate efficient solution at a time, to overcomelimitations associated with population-based approaches and also to allow aneasy control of the number of solutions provided. There is an interchangebetween two steps. The first step is a deterministic local exploration, endowedwith an automatic procedure to detect stagnation. When stagnation is detected,the search is switched to a second step characterized by a stochastic globalexploration using an evolutionary algorithm. Using five ZDT benchmarks with 30variables, the performance of the new algorithm is compared to state-of-the-artalgorithms for multi-objective optimization, more specifically NSGA-II, SPEA2,and SMS-EMOA. The solutions found by the H2MA guide to higher hypervolume andsmaller distance to the true Pareto frontier with significantly less functionevaluations, even when the gradient is estimated numerically. Furthermore,although only continuous decision spaces have been considered here, discretedecision spaces could also have been treated, replacing gradient-based searchby hill-climbing. Finally, a thorough explanation is provided to support theexpressive gain in performance that was achieved.
arxiv-11100-169 | Editorial for the First Workshop on Mining Scientific Papers: Computational Linguistics and Bibliometrics | http://arxiv.org/pdf/1506.05402v1.pdf | author:Iana Atanassova, Marc Bertin, Philipp Mayr category:cs.CL cs.DL cs.IR published:2015-06-17 summary:The workshop "Mining Scientific Papers: Computational Linguistics andBibliometrics" (CLBib 2015), co-located with the 15th International Society ofScientometrics and Informetrics Conference (ISSI 2015), brought togetherresearchers in Bibliometrics and Computational Linguistics in order to studythe ways Bibliometrics can benefit from large-scale text analytics and sensemining of scientific papers, thus exploring the interdisciplinarity ofBibliometrics and Natural Language Processing (NLP). The goals of the workshopwere to answer questions like: How can we enhance author network analysis andBibliometrics using data obtained by text analytics? What insights can NLPprovide on the structure of scientific writing, on citation networks, and onin-text citation analysis? This workshop is the first step to foster thereflection on the interdisciplinarity and the benefits that the two disciplinesBibliometrics and Natural Language Processing can drive from it.
arxiv-11100-170 | MRF-ZOOM: A Fast Dictionary Searching Algorithm for Magnetic Resonance Fingerprinting | http://arxiv.org/pdf/1506.05393v1.pdf | author:Ze Wang category:cs.DS cs.CV published:2015-06-17 summary:Magnetic resonance fingerprinting (MRF) is a new technique for simultaneouslyquantifying multiple MR parameters using one temporally resolved MR scan. Butits brute-force dictionary generating and searching (DGS) process causes a hugedisk space demand and computational burden, prohibiting it from a practicalmultiple slice high-definition imaging. The purpose of this paper was toprovide a fast and space efficient DGS algorithm for MRF. Based on an empiricalanalysis of properties of the distance function of the acquired MRF signal andthe pre-defined MRF dictionary entries, we proposed a parameter separable MRFDGS method, which breaks the multiplicative computation complexity into anadditive one and enabling a resolution scalable multi-resolution DGS process,which was dubbed as MRF ZOOM. The evaluation results showed that MRF ZOOM washundreds or thousands of times faster than the original brute-force DGS method.The acceleration was even higher when considering the time difference forgenerating the dictionary. Using a high precision quantification, MRF can findthe right parameter values for a 64x64 imaging slice in 117 secs. Our data alsoshowed that spatial constraints can be used to further speed up MRF ZOOM.
arxiv-11100-171 | An Incentive Compatible Multi-Armed-Bandit Crowdsourcing Mechanism with Quality Assurance | http://arxiv.org/pdf/1406.7157v3.pdf | author:Shweta Jain, Sujit Gujar, Satyanath Bhat, Onno Zoeter, Y. Narahari category:cs.GT cs.LG published:2014-06-27 summary:Consider a requester who wishes to crowdsource a series of identical binarylabeling tasks to a pool of workers so as to achieve an assured accuracy foreach task, in a cost optimal way. The workers are heterogeneous with unknownbut fixed qualities and their costs are private. The problem is to select foreach task an optimal subset of workers so that the outcome obtained from theselected workers guarantees a target accuracy level. The problem is achallenging one even in a non strategic setting since the accuracy ofaggregated label depends on unknown qualities. We develop a novel multi-armedbandit (MAB) mechanism for solving this problem. First, we propose a framework,Assured Accuracy Bandit (AAB), which leads to an MAB algorithm, ConstrainedConfidence Bound for a Non Strategic setting (CCB-NS). We derive an upper boundon the number of time steps the algorithm chooses a sub-optimal set thatdepends on the target accuracy level and true qualities. A more challengingsituation arises when the requester not only has to learn the qualities of theworkers but also elicit their true costs. We modify the CCB-NS algorithm toobtain an adaptive exploration separated algorithm which we call { \emConstrained Confidence Bound for a Strategic setting (CCB-S)}. CCB-S algorithmproduces an ex-post monotone allocation rule and thus can be transformed intoan ex-post incentive compatible and ex-post individually rational mechanismthat learns the qualities of the workers and guarantees a given target accuracylevel in a cost optimal way. We provide a lower bound on the number of timesany algorithm should select a sub-optimal set and we see that the lower boundmatches our upper bound upto a constant factor. We provide insights on thepractical implementation of this framework through an illustrative example andwe show the efficacy of our algorithms through simulations.
arxiv-11100-172 | MLitB: Machine Learning in the Browser | http://arxiv.org/pdf/1412.2432v2.pdf | author:Edward Meeds, Remco Hendriks, Said Al Faraby, Magiel Bruntink, Max Welling category:cs.DC cs.LG stat.ML published:2014-12-08 summary:With few exceptions, the field of Machine Learning (ML) research has largelyignored the browser as a computational engine. Beyond an educational resourcefor ML, the browser has vast potential to not only improve the state-of-the-artin ML research, but also, inexpensively and on a massive scale, to bringsophisticated ML learning and prediction to the public at large. This paperintroduces MLitB, a prototype ML framework written entirely in JavaScript,capable of performing large-scale distributed computing with heterogeneousclasses of devices. The development of MLitB has been driven by severalunderlying objectives whose aim is to make ML learning and usage ubiquitous (byusing ubiquitous compute devices), cheap and effortlessly distributed, andcollaborative. This is achieved by allowing every internet capable device torun training algorithms and predictive models with no software installation andby saving models in universally readable formats. Our prototype library iscapable of training deep neural networks with synchronized, distributedstochastic gradient descent. MLitB offers several important opportunities fornovel ML research, including: development of distributed learning algorithms,advancement of web GPU algorithms, novel field and mobile applications, privacypreserving computing, and green grid-computing. MLitB is available as opensource software.
arxiv-11100-173 | How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets | http://arxiv.org/pdf/1411.4000v2.pdf | author:Zhiyun Lu, Avner May, Kuan Liu, Alireza Bagheri Garakani, Dong Guo, Aurélien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael Picheny, Fei Sha category:cs.LG stat.ML published:2014-11-14 summary:The computational complexity of kernel methods has often been a major barrierfor applying them to large-scale learning problems. We argue that this barriercan be effectively overcome. In particular, we develop methods to scale upkernel models to successfully tackle large-scale learning problems that are sofar only approachable by deep learning architectures. Based on the seminal workby Rahimi and Recht on approximating kernel functions with features derivedfrom random projections, we advance the state-of-the-art by proposing methodsthat can efficiently train models with hundreds of millions of parameters, andlearn optimal representations from multiple kernels. We conduct extensiveempirical studies on problems from image recognition and automatic speechrecognition, and show that the performance of our kernel models matches that ofwell-engineered deep neural nets (DNNs). To the best of our knowledge, this isthe first time that a direct comparison between these two methods onlarge-scale problems is reported. Our kernel methods have several appealingproperties: training with convex optimization, cost for training a single modelcomparable to DNNs, and significantly reduced total cost due to fewerhyperparameters to tune for model selection. Our contrastive study betweenthese two very different but equally competitive models sheds light onfundamental questions such as how to learn good representations.
arxiv-11100-174 | Deep Denoising Auto-encoder for Statistical Speech Synthesis | http://arxiv.org/pdf/1506.05268v1.pdf | author:Zhenzhou Wu, Shinji Takaki, Junichi Yamagishi category:cs.SD cs.LG published:2015-06-17 summary:This paper proposes a deep denoising auto-encoder technique to extract betteracoustic features for speech synthesis. The technique allows us toautomatically extract low-dimensional features from high dimensional spectralfeatures in a non-linear, data-driven, unsupervised way. We compared the newstochastic feature extractor with conventional mel-cepstral analysis inanalysis-by-synthesis and text-to-speech experiments. Our results confirm thatthe proposed method increases the quality of synthetic speech in bothexperiments.
arxiv-11100-175 | Leveraging Textual Features for Best Answer Prediction in Community-based Question Answering | http://arxiv.org/pdf/1506.02816v2.pdf | author:George Gkotsis, Maria Liakata, Carlos Pedrinaci, John Domingue category:cs.CL cs.IR H.3.1 published:2015-06-09 summary:This paper addresses the problem of determining the best answer inCommunity-based Question Answering (CQA) websites by focussing on the content.In particular, we present a system, ACQUA [http://acqua.kmi.open.ac.uk], thatcan be installed onto the majority of browsers as a plugin. The service offersa seamless and accurate prediction of the answer to be accepted. Previousresearch on this topic relies on the exploitation of community feedback on theanswers, which involves rating of either users (e.g., reputation) or answers(e.g. scores manually assigned to answers). We propose a new technique thatleverages the content/textual features of answers in a novel way. Our approachdelivers better results than related linguistics-based solutions and manages tomatch rating-based approaches. More specifically, the gain in performance isachieved by rendering the values of these features into a discretised form. Wealso show how our technique manages to deliver equally good results inreal-time settings, as opposed to having to rely on information not alwaysreadily available, such as user ratings and answer scores. We ran an evaluationon 21 StackExchange websites covering around 4 million questions and more than8 million answers. We obtain 84% average precision and 70% recall, which showsthat our technique is robust, effective, and widely applicable.
arxiv-11100-176 | CFORB: Circular FREAK-ORB Visual Odometry | http://arxiv.org/pdf/1506.05257v1.pdf | author:Daniel J. Mankowitz, Ehud Rivlin category:cs.CV published:2015-06-17 summary:We present a novel Visual Odometry algorithm entitled Circular FREAK-ORB(CFORB). This algorithm detects features using the well-known ORB algorithm[12] and computes feature descriptors using the FREAK algorithm [14]. CFORB isinvariant to both rotation and scale changes, and is suitable for use inenvironments with uneven terrain. Two visual geometric constraints have beenutilized in order to remove invalid feature descriptor matches. Theseconstraints have not previously been utilized in a Visual Odometry algorithm. Avariation to circular matching [16] has also been implemented. This allowsfeatures to be matched between images without having to be dependent upon theepipolar constraint. This algorithm has been run on the KITTI benchmark datasetand achieves a competitive average translational error of $3.73 \%$ and averagerotational error of $0.0107 deg/m$. CFORB has also been run in an indoorenvironment and achieved an average translational error of $3.70 \%$. Afterrunning CFORB in a highly textured environment with an approximately uniformfeature spread across the images, the algorithm achieves an averagetranslational error of $2.4 \%$ and an average rotational error of $0.009deg/m$.
arxiv-11100-177 | Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation | http://arxiv.org/pdf/1506.04924v2.pdf | author:Seunghoon Hong, Hyeonwoo Noh, Bohyung Han category:cs.CV published:2015-06-16 summary:We propose a novel deep neural network architecture for semi-supervisedsemantic segmentation using heterogeneous annotations. Contrary to existingapproaches posing semantic segmentation as a single task of region-basedclassification, our algorithm decouples classification and segmentation, andlearns a separate network for each task. In this architecture, labelsassociated with an image are identified by classification network, and binarysegmentation is subsequently performed for each identified label insegmentation network. The decoupled architecture enables us to learnclassification and segmentation networks separately based on the training datawith image-level and pixel-wise class labels, respectively. It facilitates toreduce search space for segmentation effectively by exploiting class-specificactivation maps obtained from bridging layers. Our algorithm shows outstandingperformance compared to other semi-supervised approaches even with much lesstraining images with strong annotations in PASCAL VOC dataset.
arxiv-11100-178 | Novel Super-Resolution Method Based on High Order Nonlocal-Means | http://arxiv.org/pdf/1503.04253v3.pdf | author:Kang Yong-Rim, Kim Yong-Jin category:cs.IT cs.CV math.IT published:2015-03-14 summary:Super-resolution without explicit sub-pixel motion estimation is a veryactive subject of image reconstruction containing general motion. The Non-LocalMeans (NLM) method is a simple image reconstruction method without explicitmotion estimation. In this paper we generalize NLM method to higher ordersusing kernel regression can apply to super-resolution reconstruction. Theperformance of the generalized method is compared with other methods.
arxiv-11100-179 | Non-distributional Word Vector Representations | http://arxiv.org/pdf/1506.05230v1.pdf | author:Manaal Faruqui, Chris Dyer category:cs.CL published:2015-06-17 summary:Data-driven representation learning for words is a technique of centralimportance in NLP. While indisputably useful as a source of features indownstream tasks, such vectors tend to consist of uninterpretable componentswhose relationship to the categories of traditional lexical semantic theoriesis tenuous at best. We present a method for constructing interpretable wordvectors from hand-crafted linguistic resources like WordNet, FrameNet etc.These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. Weanalyze their performance on state-of-the-art evaluation methods fordistributional models of word vectors and find they are competitive to standarddistributional approaches.
arxiv-11100-180 | Discovering Hidden Factors of Variation in Deep Networks | http://arxiv.org/pdf/1412.6583v4.pdf | author:Brian Cheung, Jesse A. Livezey, Arjun K. Bansal, Bruno A. Olshausen category:cs.LG cs.CV cs.NE published:2014-12-20 summary:Deep learning has enjoyed a great deal of success because of its ability tolearn useful features for tasks such as classification. But there has been lessexploration in learning the factors of variation apart from the classificationsignal. By augmenting autoencoders with simple regularization terms duringtraining, we demonstrate that standard deep architectures can discover andexplicitly represent factors of variation beyond those relevant forcategorization. We introduce a cross-covariance penalty (XCov) as a method todisentangle factors like handwriting style for digits and subject identity infaces. We demonstrate this on the MNIST handwritten digit database, the TorontoFaces Database (TFD) and the Multi-PIE dataset by generating manipulatedinstances of the data. Furthermore, we demonstrate these deep networks canextrapolate `hidden' variation in the supervised signal.
arxiv-11100-181 | Gated Feedback Recurrent Neural Networks | http://arxiv.org/pdf/1502.02367v4.pdf | author:Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio category:cs.NE cs.LG stat.ML published:2015-02-09 summary:In this work, we propose a novel recurrent neural network (RNN) architecture.The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach ofstacking multiple recurrent layers by allowing and controlling signals flowingfrom upper recurrent layers to lower layers using a global gating unit for eachpair of layers. The recurrent signals exchanged between layers are gatedadaptively based on the previous hidden states and the current input. Weevaluated the proposed GF-RNN with different types of recurrent units, such astanh, long short-term memory and gated recurrent units, on the tasks ofcharacter-level language modeling and Python program evaluation. Our empiricalevaluation of different RNN units, revealed that in both tasks, the GF-RNNoutperforms the conventional approaches to build deep stacked RNNs. We suggestthat the improvement arises because the GF-RNN can adaptively assign differentlayers to different timescales and layer-to-layer interactions (including thetop-down ones which are not usually present in a stacked RNN) by learning togate these interactions.
arxiv-11100-182 | Robust Estimation of Structured Covariance Matrix for Heavy-Tailed Elliptical Distributions | http://arxiv.org/pdf/1506.05215v1.pdf | author:Ying Sun, Prabhu Babu, Daniel P. Palomar category:stat.AP stat.ML published:2015-06-17 summary:This paper considers the problem of robustly estimating a structuredcovariance matrix with an elliptical underlying distribution with known mean.In applications where the covariance matrix naturally possesses a certainstructure, taking the prior structure information into account in theestimation procedure is beneficial to improve the estimation accuracy. Wepropose incorporating the prior structure information into Tyler's M-estimatorand formulate the problem as minimizing the cost function of Tyler's estimatorunder the prior structural constraint. First, the estimation under a generalconvex structural constraint is introduced with an efficient algorithm forfinding the estimator derived based on the majorization minimization (MM)algorithm framework. Then, the algorithm is tailored to several specialstructures that enjoy a wide range of applications in signal processing relatedfields, namely, sum of rank-one matrices, Toeplitz, and banded Toeplitzstructure. In addition, two types of non-convex structures, i.e., the Kroneckerstructure and the spiked covariance structure, are also discussed, where it isshown that simple algorithms can be derived under the guidelines of MM.Numerical results show that the proposed estimator achieves a smallerestimation error than the benchmark estimators at a lower computational cost.
arxiv-11100-183 | Learning Spike time codes through Morphological Learning with Binary Synapses | http://arxiv.org/pdf/1506.05212v1.pdf | author:Subhrajit Roy, Phyo Phyo San, Shaista Hussain, Lee Wang Wei, Arindam Basu category:cs.NE published:2015-06-17 summary:In this paper, a neuron with nonlinear dendrites (NNLD) and binary synapsesthat is able to learn temporal features of spike input patterns is considered.Since binary synapses are considered, learning happens through formation andelimination of connections between the inputs and the dendritic branches tomodify the structure or "morphology" of the NNLD. A morphological learningalgorithm inspired by the 'Tempotron', i.e., a recently proposed temporallearning algorithm-is presented in this work. Unlike 'Tempotron', the proposedlearning rule uses a technique to automatically adapt the NNLD threshold duringtraining. Experimental results indicate that our NNLD with 1-bit synapses canobtain similar accuracy as a traditional Tempotron with 4-bit synapses inclassifying single spike random latency and pair-wise synchrony patterns.Hence, the proposed method is better suited for robust hardware implementationin the presence of statistical variations. We also present results of applyingthis rule to real life spike classification problems from the field of tactilesensing.
arxiv-11100-184 | A Discriminative Representation of Convolutional Features for Indoor Scene Recognition | http://arxiv.org/pdf/1506.05196v1.pdf | author:Salman H. Khan, Munawar Hayat, Mohammed Bennamoun, Roberto Togneri, Ferdous Sohel category:cs.CV published:2015-06-17 summary:Indoor scene recognition is a multi-faceted and challenging problem due tothe diverse intra-class variations and the confusing inter-class similarities.This paper presents a novel approach which exploits rich mid-levelconvolutional features to categorize indoor scenes. Traditionally usedconvolutional features preserve the global spatial structure, which is adesirable property for general object recognition. However, we argue that thisstructuredness is not much helpful when we have large variations in scenelayouts, e.g., in indoor scenes. We propose to transform the structuredconvolutional activations to another highly discriminative feature space. Therepresentation in the transformed space not only incorporates thediscriminative aspects of the target dataset, but it also encodes the featuresin terms of the general object categories that are present in indoor scenes. Tothis end, we introduce a new large-scale dataset of 1300 object categorieswhich are commonly present in indoor scenes. Our proposed approach achieves asignificant performance boost over previous state of the art approaches on fivemajor scene classification datasets.
arxiv-11100-185 | Robust High Quality Image Guided Depth Upsampling | http://arxiv.org/pdf/1506.05187v1.pdf | author:Wei Liu, Yijun Li, Xiaogang Chen, Jie Yang, Qiang Wu, Jingyi Yu category:cs.CV published:2015-06-17 summary:Time-of-Flight (ToF) depth sensing camera is able to obtain depth maps at ahigh frame rate. However, its low resolution and sensitivity to the noise arealways a concern. A popular solution is upsampling the obtained noisy lowresolution depth map with the guidance of the companion high resolution colorimage. However, due to the constrains in the existing upsampling models, thehigh resolution depth map obtained in such way may suffer from either texturecopy artifacts or blur of depth discontinuity. In this paper, a noveloptimization framework is proposed with the brand new data term and smoothnessterm. The comprehensive experiments using both synthetic data and real datashow that the proposed method well tackles the problem of texture copyartifacts and blur of depth discontinuity. It also demonstrates sufficientrobustness to the noise. Moreover, a data driven scheme is proposed toadaptively estimate the parameter in the upsampling optimization framework. Theencouraging performance is maintained even in the case of large upsampling e.g.$8\times$ and $16\times$.
arxiv-11100-186 | Generalized Additive Model Selection | http://arxiv.org/pdf/1506.03850v2.pdf | author:Alexandra Chouldechova, Trevor Hastie category:stat.ML published:2015-06-11 summary:We introduce GAMSEL (Generalized Additive Model Selection), a penalizedlikelihood approach for fitting sparse generalized additive models in highdimension. Our method interpolates between null, linear and additive models byallowing the effect of each variable to be estimated as being either zero,linear, or a low-complexity curve, as determined by the data. We present ablockwise coordinate descent procedure for efficiently optimizing the penalizedlikelihood objective over a dense grid of the tuning parameter, producing aregularization path of additive models. We demonstrate the performance of ourmethod on both real and simulated data examples, and compare it with existingtechniques for additive model selection.
arxiv-11100-187 | Connotation Frames: Typed Relations of Implied Sentiment in Predicate-Argument Structure | http://arxiv.org/pdf/1506.02739v2.pdf | author:Hannah Rashkin, Sameer Singh, Yejin Choi category:cs.CL published:2015-06-09 summary:Through a choice of a predicate (e.g., "violate"), a writer can convey subtlesentiments and value judgements toward the arguments of a verb (e.g.,projecting the agent as an "antagonist" and the theme as a "victim"). Weintroduce connotation frames to encode the rich dimensions of impliedsentiment, value judgements, and effect evaluation as typed relations thatthese choices influence, and propose a factor graph formulation that capturesthe inter-play among different types of connotative relations at thelexicon-level. Experimental results confirm that our model is effective inpredicting connotative sentiments compared to strong baselines and existingsentiment lexicons.
arxiv-11100-188 | Geometric Inference for General High-Dimensional Linear Inverse Problems | http://arxiv.org/pdf/1404.4408v3.pdf | author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST stat.ML stat.TH published:2014-04-17 summary:This paper presents a unified geometric framework for the statisticalanalysis of a general ill-posed linear inverse model which includes as specialcases noisy compressed sensing, sign vector recovery, trace regression,orthogonal matrix estimation, and noisy matrix completion. We proposecomputationally feasible convex programs for statistical inference includingestimation, confidence intervals and hypothesis testing. A theoreticalframework is developed to characterize the local estimation rate of convergenceand to provide statistical inference guarantees. Our results are built based onthe local conic geometry and duality. The difficulty of statistical inferenceis captured by the geometric characterization of the local tangent cone throughthe Gaussian width and Sudakov minoration estimate.
arxiv-11100-189 | Deep Convolutional Networks on Graph-Structured Data | http://arxiv.org/pdf/1506.05163v1.pdf | author:Mikael Henaff, Joan Bruna, Yann LeCun category:cs.LG cs.CV cs.NE published:2015-06-16 summary:Deep Learning's recent successes have mostly relied on ConvolutionalNetworks, which exploit fundamental statistical properties of images, soundsand video data: the local stationarity and multi-scale compositional structure,that allows expressing long range interactions in terms of shorter, localizedinteractions. However, there exist other important examples, such as textdocuments or bioinformatic data, that may lack some or all of these strongstatistical regularities. In this paper we consider the general question of how to construct deeparchitectures with small learning complexity on general non-Euclidean domains,which are typically unknown and need to be estimated from the data. Inparticular, we develop an extension of Spectral Networks which incorporates aGraph Estimation procedure, that we test on large-scale classificationproblems, matching or improving over Dropout Networks with far less parametersto estimate.
arxiv-11100-190 | A discussion on the validation tests employed to compare human action recognition methods using the MSR Action3D dataset | http://arxiv.org/pdf/1407.7390v3.pdf | author:José Ramón Padilla-López, Alexandros André Chaaraoui, Francisco Flórez-Revuelta category:cs.CV published:2014-07-28 summary:This paper aims to determine which is the best human action recognitionmethod based on features extracted from RGB-D devices, such as the MicrosoftKinect. A review of all the papers that make reference to MSR Action3D, themost used dataset that includes depth information acquired from a RGB-D device,has been performed. We found that the validation method used by each workdiffers from the others. So, a direct comparison among works cannot be made.However, almost all the works present their results comparing them withouttaking into account this issue. Therefore, we present different rankingsaccording to the methodology used for the validation in orden to clarify theexisting confusion.
arxiv-11100-191 | Combinatorial Energy Learning for Image Segmentation | http://arxiv.org/pdf/1506.04304v2.pdf | author:Jeremy Maitin-Shepard, Viren Jain, Michal Januszewski, Peter Li, Jörgen Kornfeld, Julia Buhmann, Pieter Abbeel category:cs.CV published:2015-06-13 summary:We introduce a new machine learning approach for image segmentation, based ona joint energy model over image features and novel local binary shapedescriptors. These descriptors compactly represent rich shape information atmultiple scales, including interactions between multiple objects. Our approach,which does not rely on any hand-designed features, reflects the inherentcombinatorial nature of dense image segmentation problems. We propose efficientalgorithms for learning deep neural networks to model the joint energy, and forlocal optimization of this energy in the space of supervoxel agglomerations. We demonstrate the effectiveness of our approach on 3-D biological data,where rich shape information appears to be critical for resolving ambiguities.On two challenging 3-D electron microscopy datasets highly relevant to ongoingefforts towards large-scale dense mapping of neural circuits, we achievestate-of-the-art segmentation accuracy.
arxiv-11100-192 | A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models | http://arxiv.org/pdf/1505.01504v2.pdf | author:Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai category:cs.NE cs.CL cs.LG published:2015-05-06 summary:In this paper, we propose the new fixed-size ordinally-forgetting encoding(FOFE) method, which can almost uniquely encode any variable-length sequence ofwords into a fixed-size representation. FOFE can model the word order in asequence using a simple ordinally-forgetting mechanism according to thepositions of words. In this work, we have applied FOFE to feedforward neuralnetwork language models (FNN-LMs). Experimental results have shown that withoutusing any recurrent feedbacks, FOFE based FNN-LMs can significantly outperformnot only the standard fixed-input FNN-LMs but also the popular RNN-LMs.
arxiv-11100-193 | Numeric Input Relations for Relational Learning with Applications to Community Structure Analysis | http://arxiv.org/pdf/1506.05055v1.pdf | author:Jiuchuan Jiang, Manfred Jaeger category:cs.LG published:2015-06-16 summary:Most work in the area of statistical relational learning (SRL) is focussed ondiscrete data, even though a few approaches for hybrid SRL models have beenproposed that combine numerical and discrete variables. In this paper wedistinguish numerical random variables for which a probability distribution isdefined by the model from numerical input variables that are only used forconditioning the distribution of discrete response variables. We show hownumerical input relations can very easily be used in the Relational BayesianNetwork framework, and that existing inference and learning methods need onlyminor adjustments to be applied in this generalized setting. The resultingframework provides natural relational extensions of classical probabilisticmodels for categorical data. We demonstrate the usefulness of RBN models withnumeric input relations by several examples. In particular, we use the augmented RBN framework to define probabilisticmodels for multi-relational (social) networks in which the probability of alink between two nodes depends on numeric latent feature vectors associatedwith the nodes. A generic learning procedure can be used to obtain amaximum-likelihood fit of model parameters and latent feature values for avariety of models that can be expressed in the high-level RBN representation.Specifically, we propose a model that allows us to interpret learned latentfeature values as community centrality degrees by which we can identify nodesthat are central for one community, that are hubs between communities, or thatare isolated nodes. In a multi-relational setting, the model also provides acharacterization of how different relations are associated with each community.
arxiv-11100-194 | Depth Perception in Autostereograms: 1/f-Noise is Best | http://arxiv.org/pdf/1506.05036v1.pdf | author:Yael Yankelevsky, Ishai Shvartz, Tamar Avraham, Alfred M. Bruckstein category:cs.CV published:2015-06-16 summary:An autostereogram is a single image that encodes depth information that popsout when looking at it. The trick is achieved by replicating a vertical stripthat sets a basic two-dimensional pattern with disparity shifts that encode athree-dimensional scene. It is of interest to explore the dependency betweenthe ease of perceiving depth in autostereograms and the choice of the basicpattern used for generating them. In this work we confirm a theory proposed byBruckstein et al. to explain the process of autostereographic depth perception,providing a measure for the ease of "locking into" the depth profile, based onthe spectral properties of the basic pattern used. We report the results ofthree sets of psychophysical experiments using autostereograms generated fromtwo-dimensional random noise patterns having power spectra of the form$1/f^\beta$. The experiments were designed to test the ability of humansubjects to identify smooth, low resolution surfaces, as well as detail, in theform of higher resolution objects in the depth profile, and to determine limitsin identifying small objects as a function of their size. In accordance withthe theory, we discover a significant advantage of the $1/f$ noise pattern(pink noise) for fast depth lock-in and fine detail detection, showing thatsuch patterns are optimal choices for autostereogram design. Validating thetheoretical model predictions strengthens its underlying assumptions, andcontributes to a better understanding of the visual system's binoculardisparity mechanisms.
arxiv-11100-195 | Emotion Analysis of Songs Based on Lyrical and Audio Features | http://arxiv.org/pdf/1506.05012v1.pdf | author:Adit Jamdar, Jessica Abraham, Karishma Khanna, Rahul Dubey category:cs.CL cs.AI cs.SD published:2015-06-16 summary:In this paper, a method is proposed to detect the emotion of a song based onits lyrical and audio features. Lyrical features are generated by segmentationof lyrics during the process of data extraction. ANEW and WordNet knowledge isthen incorporated to compute Valence and Arousal values. In addition to this,linguistic association rules are applied to ensure that the issue of ambiguityis properly addressed. Audio features are used to supplement the lyrical onesand include attributes like energy, tempo, and danceability. These features areextracted from The Echo Nest, a widely used music intelligence platform.Construction of training and test sets is done on the basis of social tagsextracted from the last.fm website. The classification is done by applyingfeature weighting and stepwise threshold reduction on the k-Nearest Neighborsalgorithm to provide fuzziness in the classification.
arxiv-11100-196 | Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and Pain Detection | http://arxiv.org/pdf/1506.05001v1.pdf | author:Liliana Lo Presti, Marco La Cascia category:cs.CV cs.AI cs.RO published:2015-06-16 summary:This paper proposes a new approach to model the temporal dynamics of asequence of facial expressions. To this purpose, a sequence of Face ImageDescriptors (FID) is regarded as the output of a Linear Time Invariant (LTI)system. The temporal dynamics of such sequence of descriptors are representedby means of a Hankel matrix. The paper presents different strategies to computedynamics-based representation of a sequence of FID, and reports classificationaccuracy values of the proposed representations within different standardclassification frameworks. The representations have been validated in two verychallenging application domains: emotion recognition and pain detection.Experiments on two publicly available benchmarks and comparison withstate-of-the-art approaches demonstrate that the dynamics-based FIDrepresentation attains competitive performance when off-the-shelfclassification tools are adopted.
arxiv-11100-197 | Recognize Foreign Low-Frequency Words with Similar Pairs | http://arxiv.org/pdf/1506.04940v1.pdf | author:Xi Ma, Xiaoxi Wang, Dong Wang, Zhiyong Zhang category:cs.CL published:2015-06-16 summary:Low-frequency words place a major challenge for automatic speech recognition(ASR). The probabilities of these words, which are often important nameentities, are generally under-estimated by the language model (LM) due to theirlimited occurrences in the training data. Recently, we proposed a word-pairapproach to deal with the problem, which borrows information of frequent wordsto enhance the probabilities of low-frequency words. This paper presents anextension to the word-pair method by involving multiple `predicting words' toproduce better estimation for low-frequency words. We also employ this approachto deal with out-of-language words in the task of multi-lingual speechrecognition.
arxiv-11100-198 | Post-Reconstruction Deconvolution of PET Images by Total Generalized Variation Regularization | http://arxiv.org/pdf/1506.04935v1.pdf | author:Stéphanie Guérit, Laurent Jacques, Benoît Macq, John A. Lee category:cs.CV math.OC published:2015-06-16 summary:Improving the quality of positron emission tomography (PET) images, affectedby low resolution and high level of noise, is a challenging task in nuclearmedicine and radiotherapy. This work proposes a restoration method, achievedafter tomographic reconstruction of the images and targeting clinicalsituations where raw data are often not accessible. Based on inverse problemmethods, our contribution introduces the recently developed total generalizedvariation (TGV) norm to regularize PET image deconvolution. Moreover, westabilize this procedure with additional image constraints such as positivityand photometry invariance. A criterion for updating and adjusting automaticallythe regularization parameter in case of Poisson noise is also presented.Experiments are conducted on both synthetic data and real patient images.
arxiv-11100-199 | Subsampled terahertz data reconstruction based on spatio-temporal dictionary learning | http://arxiv.org/pdf/1506.04912v1.pdf | author:Vahid Abolghasemi, Hao Shen, Yaochun Shen, Lu Gan category:cs.CV published:2015-06-16 summary:In this paper, the problem of terahertz pulsed imaging and reconstruction isaddressed. It is assumed that an incomplete (subsampled) three dimensional THzdata set has been acquired and the aim is to recover all missing samples. Asparsity-inducing approach is proposed for this purpose. First, a simpleinterpolation is applied to incomplete noisy data. Then, we propose aspatio-temporal dictionary learning method to obtain an appropriate sparserepresentation of data based on a joint sparse recovery algorithm. Then, usingthe sparse coefficients and the learned dictionary, the 3D data is effectivelydenoised by minimizing a simple cost function. We consider two types ofterahertz data to evaluate the performance of the proposed approach; THz dataacquired for a model sample with clear layered structures (e.g., a T-shapeplastic sheet buried in a polythene pellet), and pharmaceutical tablet data(with low spatial resolution). The achieved signal-to-noise-ratio forreconstruction of T-shape data, from only 5% observation was 19 dB. Moreover,the accuracies of obtained thickness and depth measurements for pharmaceuticaltablet data after reconstruction from 10% observation were 98.8%, and 99.9%,respectively. These results, along with chemical mapping analysis, presented atthe end of this paper, confirm the accuracy of the proposed method.
arxiv-11100-200 | Parsing Natural Language Sentences by Semi-supervised Methods | http://arxiv.org/pdf/1506.04897v1.pdf | author:Rudolf Rosa category:cs.CL I.2.7 published:2015-06-16 summary:We present our work on semi-supervised parsing of natural language sentences,focusing on multi-source crosslingual transfer of delexicalized dependencyparsers. We first evaluate the influence of treebank annotation styles onparsing performance, focusing on adposition attachment style. Then, we presentKLcpos3, an empirical language similarity measure, designed and tuned forsource parser weighting in multi-source delexicalized parser transfer. Andfinally, we introduce a novel resource combination method, based oninterpolation of trained parser models.
arxiv-11100-201 | Author Identification using Multi-headed Recurrent Neural Networks | http://arxiv.org/pdf/1506.04891v1.pdf | author:Douglas Bagnall category:cs.CL cs.LG cs.NE 68T50 published:2015-06-16 summary:Recurrent neural networks (RNNs) are very good at modelling the flow of text,but typically need to be trained on a far larger corpus than is available forthe PAN 2015 Author Identification task. This paper describes a novel approachwhere the output layer of a character-level RNN language model is split intoseveral independent predictive sub-models, each representing an author, whilethe recurrent layer is shared by all. This allows the recurrent layer to modelthe language as a whole without over-fitting, while the outputs select aspectsof the underlying model that reflect their author's style. The method provescompetitive, ranking first in two of the four languages.
arxiv-11100-202 | Learning Super-Resolution Jointly from External and Internal Examples | http://arxiv.org/pdf/1503.01138v3.pdf | author:Zhangyang Wang, Yingzhen Yang, Zhaowen Wang, Shiyu Chang, Jianchao Yang, Thomas S. Huang category:cs.CV published:2015-03-03 summary:Single image super-resolution (SR) aims to estimate a high-resolution (HR)image from a lowresolution (LR) input. Image priors are commonly learned toregularize the otherwise seriously ill-posed SR problem, either using externalLR-HR pairs or internal similar patterns. We propose joint SR to adaptivelycombine the advantages of both external and internal SR methods. We define twoloss functions using sparse coding based external examples, and epitomicmatching based on internal examples, as well as a corresponding adaptive weightto automatically balance their contributions according to their reconstructionerrors. Extensive SR results demonstrate the effectiveness of the proposedmethod over the existing state-of-the-art methods, and is also verified by oursubjective evaluation study.
arxiv-11100-203 | Detection and Estimation of Iris Centre | http://arxiv.org/pdf/1506.04843v1.pdf | author:Anirban Dasgupta, Aurobinda Routray, Sai Nataraj Mallavollu category:cs.CV published:2015-06-16 summary:Detection of iris center is an active area of research in the field ofcomputer vision and human-machine interaction systems. The major issuesinvolved in the detection of iris involves glint on the corneal region,occlusion of the iris by eye-lids, occlusions due to eye gaze, high speed ofprocessing etc. This paper presents an algorithm for detecting and estimatingthe iris center thereby addressing some of these issues.
arxiv-11100-204 | Spectral Sparsification and Regret Minimization Beyond Matrix Multiplicative Updates | http://arxiv.org/pdf/1506.04838v1.pdf | author:Zeyuan Allen-Zhu, Zhenyu Liao, Lorenzo Orecchia category:cs.LG cs.DS math.OC stat.ML published:2015-06-16 summary:In this paper, we provide a novel construction of the linear-sized spectralsparsifiers of Batson, Spielman and Srivastava [BSS14]. While previousconstructions required $\Omega(n^4)$ running time [BSS14, Zou12], oursparsification routine can be implemented in almost-quadratic running time$O(n^{2+\varepsilon})$. The fundamental conceptual novelty of our work is the leveraging of a strongconnection between sparsification and a regret minimization problem overdensity matrices. This connection was known to provide an interpretation of therandomized sparsifiers of Spielman and Srivastava [SS11] via the application ofmatrix multiplicative weight updates (MWU) [CHS11, Vis14]. In this paper, weexplain how matrix MWU naturally arises as an instance of theFollow-the-Regularized-Leader framework and generalize this approach to yield alarger class of updates. This new class allows us to accelerate theconstruction of linear-sized spectral sparsifiers, and give novel insights onthe motivation behind Batson, Spielman and Srivastava [BSS14].
arxiv-11100-205 | Semi-Stochastic Gradient Descent Methods | http://arxiv.org/pdf/1312.1666v2.pdf | author:Jakub Konečný, Peter Richtárik category:stat.ML cs.LG cs.NA math.NA math.OC published:2013-12-05 summary:In this paper we study the problem of minimizing the average of a largenumber ($n$) of smooth convex loss functions. We propose a new method, S2GD(Semi-Stochastic Gradient Descent), which runs for one or several epochs ineach of which a single full gradient and a random number of stochasticgradients is computed, following a geometric law. The total work needed for themethod to output an $\varepsilon$-accurate solution in expectation, measured inthe number of passes over data, or equivalently, in units equivalent to thecomputation of a single gradient of the loss, is$O((\kappa/n)\log(1/\varepsilon))$, where $\kappa$ is the condition number.This is achieved by running the method for $O(\log(1/\varepsilon))$ epochs,with a single gradient evaluation and $O(\kappa)$ stochastic gradientevaluations in each. The SVRG method of Johnson and Zhang arises as a specialcase. If our method is limited to a single epoch only, it needs to evaluate atmost $O((\kappa/\varepsilon)\log(1/\varepsilon))$ stochastic gradients. Incontrast, SVRG requires $O(\kappa/\varepsilon^2)$ stochastic gradients. Toillustrate our theoretical results, S2GD only needs the workload equivalent toabout 2.1 full gradient evaluations to find an $10^{-6}$-accurate solution fora problem with $n=10^9$ and $\kappa=10^3$.
arxiv-11100-206 | Exploiting Text and Network Context for Geolocation of Social Media Users | http://arxiv.org/pdf/1506.04803v1.pdf | author:Afshin Rahimi, Duy Vu, Trevor Cohn, Timothy Baldwin category:cs.CL cs.SI published:2015-06-16 summary:Research on automatically geolocating social media users has conventionallybeen based on the text content of posts from a given user or the social networkof the user, with very little crossover between the two, and no bench-markingof the two approaches over compara- ble datasets. We bring the two threads ofresearch together in first proposing a text-based method based on adaptivegrids, followed by a hybrid network- and text-based method. Evaluating overthree Twitter datasets, we show that the empirical difference between text- andnetwork-based methods is not great, and that hybridisation of the two issuperior to the component methods, especially in contexts where the user graphis not well connected. We achieve state-of-the-art results on all threedatasets.
arxiv-11100-207 | Is Joint Training Better for Deep Auto-Encoders? | http://arxiv.org/pdf/1405.1380v4.pdf | author:Yingbo Zhou, Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju category:stat.ML cs.LG cs.NE published:2014-05-06 summary:Traditionally, when generative models of data are developed via deeparchitectures, greedy layer-wise pre-training is employed. In a well-trainedmodel, the lower layer of the architecture models the data distributionconditional upon the hidden variables, while the higher layers model the hiddendistribution prior. But due to the greedy scheme of the layerwise trainingtechnique, the parameters of lower layers are fixed when training higherlayers. This makes it extremely challenging for the model to learn the hiddendistribution prior, which in turn leads to a suboptimal model for the datadistribution. We therefore investigate joint training of deep autoencoders,where the architecture is viewed as one stack of two or more single-layerautoencoders. A single global reconstruction objective is jointly optimized,such that the objective for the single autoencoders at each layer acts as alocal, layer-level regularizer. We empirically evaluate the performance of thisjoint training scheme and observe that it not only learns a better data model,but also learns better higher layer representations, which highlights itspotential for unsupervised feature learning. In addition, we find that theusage of regularizations in the joint training scheme is crucial in achievinggood performance. In the supervised setting, joint training also shows superiorperformance when training deeper models. The joint training framework can thusprovide a platform for investigating more efficient usage of different types ofregularizers, especially in light of the growing volumes of available unlabeleddata.
arxiv-11100-208 | Incremental Knowledge Base Construction Using DeepDive | http://arxiv.org/pdf/1502.00731v4.pdf | author:Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang, Christopher Ré category:cs.DB cs.CL cs.LG published:2015-02-03 summary:Populating a database with unstructured information is a long-standingproblem in industry and research that encompasses problems of extraction,cleaning, and integration. Recent names used for this problem include dealingwith dark data and knowledge base construction (KBC). In this work, we describeDeepDive, a system that combines database and machine learning ideas to helpdevelop KBC systems, and we present techniques to make the KBC process moreefficient. We observe that the KBC process is iterative, and we developtechniques to incrementally produce inference results for KBC systems. Wepropose two methods for incremental inference, based respectively on samplingand variational techniques. We also study the tradeoff space of these methodsand develop a simple rule-based optimizer. DeepDive includes all of thesecontributions, and we evaluate DeepDive on five KBC systems, showing that itcan speed up KBC inference tasks by up to two orders of magnitude withnegligible impact on quality.
arxiv-11100-209 | Encog: Library of Interchangeable Machine Learning Models for Java and C# | http://arxiv.org/pdf/1506.04776v1.pdf | author:Jeff Heaton category:cs.MS cs.LG 68T01 I.2 published:2015-06-15 summary:This paper introduces the Encog library for Java and C#, a scalable,adaptable, multiplatform machine learning framework that was 1st released in2008. Encog allows a variety of machine learning models to be applied todatasets using regression, classification, and clustering. Various supportedmachine learning models can be used interchangeably with minimal recoding.Encog uses efficient multithreaded code to reduce training time by exploitingmodern multicore processors. The current version of Encog can be downloadedfrom http://www.encog.org.
arxiv-11100-210 | Image-based Recommendations on Styles and Substitutes | http://arxiv.org/pdf/1506.04757v1.pdf | author:Julian McAuley, Christopher Targett, Qinfeng Shi, Anton van den Hengel category:cs.CV cs.IR published:2015-06-15 summary:Humans inevitably develop a sense of the relationships between objects, someof which are based on their appearance. Some pairs of objects might be seen asbeing alternatives to each other (such as two pairs of jeans), while others maybe seen as being complementary (such as a pair of jeans and a matching shirt).This information guides many of the choices that people make, from buyingclothes to their interactions with each other. We seek here to model this humansense of the relationships between objects based on their appearance. Ourapproach is not based on fine-grained modeling of user annotations but ratheron capturing the largest dataset possible and developing a scalable method foruncovering human notions of the visual relationships within. We cast this as anetwork inference problem defined on graphs of related images, and provide alarge-scale dataset for the training and evaluation of the same. The system wedevelop is capable of recommending which clothes and accessories will go welltogether (and which will not), amongst a host of other applications.
arxiv-11100-211 | Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy Game | http://arxiv.org/pdf/1506.04744v1.pdf | author:Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber, Cristian Danescu-Niculescu-Mizil category:cs.CL cs.AI cs.SI physics.soc-ph stat.ML published:2015-06-15 summary:Interpersonal relations are fickle, with close friendships often dissolvinginto enmity. In this work, we explore linguistic cues that presage suchtransitions by studying dyadic interactions in an online strategy game whereplayers form alliances and break those alliances through betrayal. Wecharacterize friendships that are unlikely to last and examine temporalpatterns that foretell betrayal. We reveal that subtle signs of imminent betrayal are encoded in theconversational patterns of the dyad, even if the victim is not aware of therelationship's fate. In particular, we find that lasting friendships exhibit aform of balance that manifests itself through language. In contrast, suddenchanges in the balance of certain conversational attributes---such as positivesentiment, politeness, or focus on future planning---signal impending betrayal.
arxiv-11100-212 | Fast Two-Sample Testing with Analytic Representations of Probability Measures | http://arxiv.org/pdf/1506.04725v1.pdf | author:Kacper Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, Arthur Gretton category:stat.ML 62G10 G.3 published:2015-06-15 summary:We propose a class of nonparametric two-sample tests with a cost linear inthe sample size. Two tests are given, both based on an ensemble of distancesbetween analytic functions representing each of the distributions. The firsttest uses smoothed empirical characteristic functions to represent thedistributions, the second uses distribution embeddings in a reproducing kernelHilbert space. Analyticity implies that differences in the distributions may bedetected almost surely at a finite number of randomly chosenlocations/frequencies. The new tests are consistent against a larger class ofalternatives than the previous linear-time tests based on the (non-smoothed)empirical characteristic functions, while being much faster than the currentstate-of-the-art quadratic-time kernel-based or energy distance-based tests.Experiments on artificial benchmarks and on challenging real-world testingproblems demonstrate that our tests give a better power/time tradeoff thancompeting approaches, and in some cases, better outright power than even themost expensive quadratic-time tests. This performance advantage is retainedeven in high dimensions, and in cases where the difference in distributions isnot observable with low order statistics.
arxiv-11100-213 | Automatic Layer Separation using Light Field Imaging | http://arxiv.org/pdf/1506.04721v1.pdf | author:Qiaosong Wang, Haiting Lin, Yi Ma, Sing Bing Kang, Jingyi Yu category:cs.CV published:2015-06-15 summary:We propose a novel approach that jointly removes reflection or translucentlayer from a scene and estimates scene depth. The input data are captured vialight field imaging. The problem is couched as minimizing the rank of thetransmitted scene layer via Robust Principle Component Analysis (RPCA). We alsoimpose regularization based on piecewise smoothness, gradient sparsity, andlayer independence to simultaneously recover 3D geometry of the transmittedlayer. Experimental results on synthetic and real data show that our techniqueis robust and reliable, and can handle a broad range of layer separationproblems.
arxiv-11100-214 | Latent Regression Bayesian Network for Data Representation | http://arxiv.org/pdf/1506.04720v1.pdf | author:Siqi Nie, Qiang Ji category:cs.LG published:2015-06-15 summary:Deep directed generative models have attracted much attention recently due totheir expressive representation power and the ability of ancestral sampling.One major difficulty of learning directed models with many latent variables isthe intractable inference. To address this problem, most existing algorithmsmake assumptions to render the latent variables independent of each other,either by designing specific priors, or by approximating the true posteriorusing a factorized distribution. We believe the correlations among latentvariables are crucial for faithful data representation. Driven by this idea, wepropose an inference method based on the conditional pseudo-likelihood thatpreserves the dependencies among the latent variables. For learning, we proposeto employ the hard Expectation Maximization (EM) algorithm, which avoids theintractability of the traditional EM by max-out instead of sum-out to computethe data likelihood. Qualitative and quantitative evaluations of our modelagainst state of the art deep models on benchmark datasets demonstrate theeffectiveness of the proposed algorithm in data representation andreconstruction.
arxiv-11100-215 | Leveraging the Power of Gabor Phase for Face Identification: A Block Matching Approach | http://arxiv.org/pdf/1506.04655v1.pdf | author:Yang Zhong, Haibo Li category:cs.CV published:2015-06-15 summary:Different from face verification, face identification is much more demanding.To reach comparable performance, an identifier needs to be roughly N timesbetter than a verifier. To expect a breakthrough in face identification, weneed a fresh look at the fundamental building blocks of face recognition. Inthis paper we focus on the selection of a suitable signal representation andbetter matching strategy for face identification. We demonstrate how Gaborphase could be leveraged to improve the performance of face identification byusing the Block Matching method. Compared to the existing approaches, theproposed method features much lower algorithmic complexity: face images areonly filtered by a single-scale Gabor filter pair and the matching is performedbetween any pairs of face images at hand without involving any trainingprocess. Benchmark evaluations show that the proposed approach is totallycomparable to and even better than state-of-the-art algorithms, which aretypically based on more features extracted from a large set of Gabor facesand/or rely on heavy training processes.
arxiv-11100-216 | Coordinate Descent with Arbitrary Sampling I: Algorithms and Complexity | http://arxiv.org/pdf/1412.8060v2.pdf | author:Zheng Qu, Peter Richtárik category:math.OC cs.LG cs.NA math.NA published:2014-12-27 summary:We study the problem of minimizing the sum of a smooth convex function and aconvex block-separable regularizer and propose a new randomized coordinatedescent method, which we call ALPHA. Our method at every iteration updates arandom subset of coordinates, following an arbitrary distribution. Nocoordinate descent methods capable to handle an arbitrary sampling have beenstudied in the literature before for this problem. ALPHA is a remarkablyflexible algorithm: in special cases, it reduces to deterministic andrandomized methods such as gradient descent, coordinate descent, parallelcoordinate descent and distributed coordinate descent -- both in nonacceleratedand accelerated variants. The variants with arbitrary (or importance) samplingare new. We provide a complexity analysis of ALPHA, from which we deduce as adirect corollary complexity bounds for its many variants, all matching orimproving best known bounds.
arxiv-11100-217 | Learning with Square Loss: Localization through Offset Rademacher Complexity | http://arxiv.org/pdf/1502.06134v3.pdf | author:Tengyuan Liang, Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.LG math.ST stat.TH published:2015-02-21 summary:We consider regression with square loss and general classes of functionswithout the boundedness assumption. We introduce a notion of offset Rademachercomplexity that provides a transparent way to study localization both inexpectation and in high probability. For any (possibly non-convex) class, theexcess loss of a two-step estimator is shown to be upper bounded by this offsetcomplexity through a novel geometric inequality. In the convex case, theestimator reduces to an empirical risk minimizer. The method recovers theresults of \citep{RakSriTsy15} for the bounded case while also providingguarantees without the boundedness assumption.
arxiv-11100-218 | Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions | http://arxiv.org/pdf/1501.07242v2.pdf | author:Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, Alexander Rakhlin category:cs.NA cs.LG math.OC published:2015-01-28 summary:We consider the problem of optimizing an approximately convex function over abounded convex set in $\mathbb{R}^n$ using only function evaluations. Theproblem is reduced to sampling from an \emph{approximately} log-concavedistribution using the Hit-and-Run method, which is shown to have the same$\mathcal{O}^*$ complexity as sampling from log-concave distributions. Inaddition to extend the analysis for log-concave distributions to approximatelog-concave distributions, the implementation of the 1-dimensional sampler ofthe Hit-and-Run walk requires new methods and analysis. The algorithm then isbased on simulated annealing which does not relies on first order conditionswhich makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context ofzeroth order stochastic convex optimization, the proposed method produces an$\epsilon$-minimizer after $\mathcal{O}^*(n^{7.5}\epsilon^{-2})$ noisy functionevaluations by inducing a $\mathcal{O}(\epsilon/n)$-approximately log concavedistribution. We also consider in detail the case when the "amount ofnon-convexity" decays towards the optimum of the function. Other applicationsof the method discussed in this work include private computation of empiricalrisk minimizers, two-stage stochastic programming, and approximate dynamicprogramming for online learning.
arxiv-11100-219 | Transfer Learning for Video Recognition with Scarce Training Data for Deep Convolutional Neural Network | http://arxiv.org/pdf/1409.4127v2.pdf | author:Yu-Chuan Su, Tzu-Hsuan Chiu, Chun-Yen Yeh, Hsin-Fu Huang, Winston H. Hsu category:cs.CV cs.LG published:2014-09-15 summary:Unconstrained video recognition and Deep Convolution Network (DCN) are twoactive topics in computer vision recently. In this work, we apply DCNs asframe-based recognizers for video recognition. Our preliminary studies,however, show that video corpora with complete ground truth are usually notlarge and diverse enough to learn a robust model. The networks trained directlyon the video data set suffer from significant overfitting and have poorrecognition rate on the test set. The same lack-of-training-sample problemlimits the usage of deep models on a wide range of computer vision problemswhere obtaining training data are difficult. To overcome the problem, weperform transfer learning from images to videos to utilize the knowledge in theweakly labeled image corpus for video recognition. The image corpus help tolearn important visual patterns for natural images, while these patterns areignored by models trained only on the video corpus. Therefore, the resultantnetworks have better generalizability and better recognition rate. We show thatby means of transfer learning from image to video, we can learn a frame-basedrecognizer with only 4k videos. Because the image corpus is weakly labeled, theentire learning process requires only 4k annotated instances, which is far lessthan the million scale image data sets required by previous works. The sameapproach may be applied to other visual recognition tasks where only scarcetraining data is available, and it improves the applicability of DCNs invarious computer vision problems. Our experiments also reveal the correlationbetween meta-parameters and the performance of DCNs, given the properties ofthe target problem and data. These results lead to a heuristic formeta-parameter selection for future researches, which does not rely on the timeconsuming meta-parameter search.
arxiv-11100-220 | Flow Segmentation in Dense Crowds | http://arxiv.org/pdf/1506.04608v1.pdf | author:Javairia Nazir, Mehreen Sirshar category:cs.CV published:2015-06-15 summary:A framework is proposed in this paper that is used to segment flow of densecrowds. The flow field that is generated by the movement in the crowd istreated just like an aperiodic dynamic system. On this flow field a grid ofparticles is put over for particle advection by the use of a numericalintegration scheme. Then flow maps are generated which associates the initialposition of the particles with final position. The gradient of the flow mapsgives the amount of divergence of the neighboring particles. For forwardintegration and analysis forward Finite time Lyapunov Exponent is calculatedand backward Finite time Lyapunov Exponent is also calculated it gives theLagrangian coherent structures of the flow in crowd. Lagrangian CoherentStructures basically divides the flow in crowd into regions and these regionshave different dynamics. These regions are then used to get the boundary in thedifferent flow segments by using water shed algorithm. The experiment isconducted on the crowd dataset of UCF (University of central Florida).
arxiv-11100-221 | Learning with incremental iterative regularization | http://arxiv.org/pdf/1405.0042v2.pdf | author:Lorenzo Rosasco, Silvia Villa category:stat.ML cs.LG math.OC math.PR published:2014-04-30 summary:Within a statistical learning setting, we propose and study an iterativeregularization algorithm for least squares defined by an incremental gradientmethod. In particular, we show that, if all other parameters are fixed apriori, the number of passes over the data (epochs) acts as a regularizationparameter, and prove strong universal consistency, i.e. almost sure convergenceof the risk, as well as sharp finite sample bounds for the iterates. Ourresults are a step towards understanding the effect of multiple epochs instochastic gradient techniques in machine learning and rely on integratingstatistical and optimization results.
arxiv-11100-222 | Re-scale AdaBoost for Attack Detection in Collaborative Filtering Recommender Systems | http://arxiv.org/pdf/1506.04584v1.pdf | author:Zhihai Yang, Lin Xu, Zhongmin Cai category:cs.IR cs.CR cs.LG published:2015-06-15 summary:Collaborative filtering recommender systems (CFRSs) are the key components ofsuccessful e-commerce systems. Actually, CFRSs are highly vulnerable to attackssince its openness. However, since attack size is far smaller than that ofgenuine users, conventional supervised learning based detection methods couldbe too "dull" to handle such imbalanced classification. In this paper, weimprove detection performance from following two aspects. First, we extractwell-designed features from user profiles based on the statistical propertiesof the diverse attack models, making hard classification task becomes easier toperform. Then, refer to the general idea of re-scale Boosting (RBoosting) andAdaBoost, we apply a variant of AdaBoost, called the re-scale AdaBoost(RAdaBoost) as our detection method based on extracted features. RAdaBoost iscomparable to the optimal Boosting-type algorithm and can effectively improvethe performance in some hard scenarios. Finally, a series of experiments on theMovieLens-100K data set are conducted to demonstrate the outperformance ofRAdaBoost comparing with some classical techniques such as SVM, kNN andAdaBoost.
arxiv-11100-223 | Variational Recurrent Auto-Encoders | http://arxiv.org/pdf/1412.6581v6.pdf | author:Otto Fabius, Joost R. van Amersfoort category:stat.ML cs.LG cs.NE published:2014-12-20 summary:In this paper we propose a model that combines the strengths of RNNs andSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be usedfor efficient, large scale unsupervised learning on time series data, mappingthe time series data to a latent vector representation. The model isgenerative, such that data can be generated from samples of the latent space.An important contribution of this work is that the model can make use ofunlabeled data in order to facilitate supervised training of RNNs byinitialising the weights and network state.
arxiv-11100-224 | Optimising Spatial and Tonal Data for PDE-based Inpainting | http://arxiv.org/pdf/1506.04566v1.pdf | author:Laurent Hoeltgen, Markus Mainberger, Sebastian Hoffmann, Joachim Weickert, Ching Hoo Tang, Simon Setzer, Daniel Johannsen, Frank Neumann, Benjamin Doerr category:cs.CV math.OC published:2015-06-15 summary:Some recent methods for lossy signal and image compression store only a fewselected pixels and fill in the missing structures by inpainting with a partialdifferential equation (PDE). Suitable operators include the Laplacian, thebiharmonic operator, and edge-enhancing anisotropic diffusion (EED). Thequality of such approaches depends substantially on the selection of the datathat is kept. Optimising this data in the domain and codomain gives rise tochallenging mathematical problems that shall be addressed in our work. In the 1D case, we prove results that provide insights into the difficulty ofthis problem, and we give evidence that a splitting into spatial and tonal(i.e. function value) optimisation does hardly deteriorate the results. In the2D setting, we present generic algorithms that achieve a high reconstructionquality even if the specified data is very sparse. To optimise the spatialdata, we use a probabilistic sparsification, followed by a nonlocal pixelexchange that avoids getting trapped in bad local optima. After this spatialoptimisation we perform a tonal optimisation that modifies the function valuesin order to reduce the global reconstruction error. For homogeneous diffusioninpainting, this comes down to a least squares problem for which we prove thatit has a unique solution. We demonstrate that it can be found efficiently witha gradient descent approach that is accelerated with fast explicit diffusion(FED) cycles. Our framework allows to specify the desired density of theinpainting mask a priori. Moreover, is more generic than other dataoptimisation approaches for the sparse inpainting problem, since it can also beextended to nonlinear inpainting operators such as EED. This is exploited toachieve reconstructions with state-of-the-art quality. We also give an extensive literature survey on PDE-based image compressionmethods.
arxiv-11100-225 | Big Data Analytics in Bioinformatics: A Machine Learning Perspective | http://arxiv.org/pdf/1506.05101v1.pdf | author:Hirak Kashyap, Hasin Afzal Ahmed, Nazrul Hoque, Swarup Roy, Dhruba Kumar Bhattacharyya category:cs.CE cs.LG published:2015-06-15 summary:Bioinformatics research is characterized by voluminous and incrementaldatasets and complex data analytics methods. The machine learning methods usedin bioinformatics are iterative and parallel. These methods can be scaled tohandle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch-mode and are notoptimized for iterative processing and high data dependency among operations.In the recent years, parallel, incremental, and multi-view machine learningalgorithms have been proposed. Similarly, graph-based architectures andin-memory big data tools have been developed to minimize I/O cost and optimizeiterative processing. However, there lack standard big data architectures and tools for manyimportant bioinformatics problems, such as fast construction of co-expressionand regulatory networks and salient module identification, detection ofcomplexes over growing protein-protein interaction data, fast analysis ofmassive DNA, RNA, and protein sequence data, and fast querying on incrementaland heterogeneous disease networks. This paper addresses the issues andchallenges posed by several big data problems in bioinformatics, and gives anoverview of the state of the art and the future research opportunities.
arxiv-11100-226 | On the Generalization of the C-Bound to Structured Output Ensemble Methods | http://arxiv.org/pdf/1408.1336v2.pdf | author:François Laviolette, Emilie Morvant, Liva Ralaivola, Jean-Francis Roy category:stat.ML published:2014-08-06 summary:This paper generalizes an important result from the PAC-Bayesian literaturefor binary classification to the case of ensemble methods for structuredoutputs. We prove a generic version of the \Cbound, an upper bound over therisk of models expressed as a weighted majority vote that is based on the firstand second statistical moments of the vote's margin. This bound mayadvantageously $(i)$ be applied on more complex outputs such as multiclasslabels and multilabel, and $(ii)$ allow to consider margin relaxations. Theseresults open the way to develop new ensemble methods for structured outputprediction with PAC-Bayesian guarantees.
arxiv-11100-227 | Convex Risk Minimization and Conditional Probability Estimation | http://arxiv.org/pdf/1506.04513v1.pdf | author:Matus Telgarsky, Miroslav Dudík, Robert Schapire category:cs.LG stat.ML published:2015-06-15 summary:This paper proves, in very general settings, that convex risk minimization isa procedure to select a unique conditional probability model determined by theclassification problem. Unlike most previous work, we give results that aregeneral enough to include cases in which no minimum exists, as occurstypically, for instance, with standard boosting algorithms. Concretely, wefirst show that any sequence of predictors minimizing convex risk over thesource distribution will converge to this unique model when the class ofpredictors is linear (but potentially of infinite dimension). Secondly, we showthe same result holds for \emph{empirical} risk minimization whenever thisclass of predictors is finite dimensional, where the essential technicalcontribution is a norm-free generalization bound.
arxiv-11100-228 | Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy | http://arxiv.org/pdf/1506.02914v2.pdf | author:Marylou Gabrié, Eric W. Tramel, Florent Krzakala category:cs.LG cs.NE stat.ML published:2015-06-09 summary:Restricted Boltzmann machines are undirected neural networks which have beenshown to be effective in many applications, including serving asinitializations for training deep multi-layer neural networks. One of the mainreasons for their success is the existence of efficient and practicalstochastic algorithms, such as contrastive divergence, for unsupervisedtraining. We propose an alternative deterministic iterative procedure based onan improved mean field method from statistical physics known as theThouless-Anderson-Palmer approach. We demonstrate that our algorithm providesperformance equal to, and sometimes superior to, persistent contrastivedivergence, while also providing a clear and easy to evaluate objectivefunction. We believe that this strategy can be easily generalized to othermodels as well as to more accurate higher-order approximations, paving the wayfor systematic improvements in training Boltzmann machines with hidden units.
arxiv-11100-229 | On the properties of variational approximations of Gibbs posteriors | http://arxiv.org/pdf/1506.04091v2.pdf | author:Pierre Alquier, James Ridgway, Nicolas Chopin category:stat.ML math.ST stat.TH published:2015-06-12 summary:The PAC-Bayesian approach is a powerful set of techniques to derive non-asymptotic risk bounds for random estimators. The corresponding optimaldistribution of estimators, usually called the Gibbs posterior, isunfortunately intractable. One may sample from it using Markov chain MonteCarlo, but this is often too slow for big datasets. We consider insteadvariational approximations of the Gibbs posterior, which are fast to compute.We undertake a general study of the properties of such approximations. Our mainfinding is that such a variational approximation has often the same rate ofconvergence as the original PAC-Bayesian procedure it approximates. Wespecialise our results to several learning tasks (classification, ranking,matrix completion),discuss how to implement a variational approximation in eachcase, and illustrate the good properties of said approximation on realdatasets.
arxiv-11100-230 | Distilling Word Embeddings: An Encoding Approach | http://arxiv.org/pdf/1506.04488v1.pdf | author:Lili Mou, Ge Li, Yan Xu, Lu Zhang, Zhi Jin category:cs.CL cs.LG published:2015-06-15 summary:Distilling knowledge from a well-trained cumbersome network to a small onehas become a new research topic recently, as lightweight neural networks withhigh performance are particularly in need in various resource-restrictedsystems. This paper addresses the problem of distilling embeddings for NLPtasks. We propose an encoding approach to distill task-specific knowledge fromhigh-dimensional embeddings, which can retain high performance and reduce modelcomplexity to a large extent. Experimental results show our method is betterthan directly training neural networks with small embeddings.
arxiv-11100-231 | Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy | http://arxiv.org/pdf/1506.04477v1.pdf | author:Sang-Woo Lee, Min-Oh Heo, Jiwon Kim, Jeonghee Kim, Byoung-Tak Zhang category:cs.LG published:2015-06-15 summary:The online learning of deep neural networks is an interesting problem ofmachine learning because, for example, major IT companies want to manage theinformation of the massive data uploaded on the web daily, and this technologycan contribute to the next generation of lifelong learning. We aim to traindeep models from new data that consists of new classes, distributions, andtasks at minimal computational cost, which we call online deep learning.Unfortunately, deep neural network learning through classical online andincremental methods does not work well in both theory and practice. In thispaper, we introduce dual memory architectures for online incremental deeplearning. The proposed architecture consists of deep representation learnersand fast learnable shallow kernel networks, both of which synergize to trackthe information of new data. During the training phase, we use various online,incremental ensemble, and transfer learning techniques in order to achievelower error of the architecture. On the MNIST, CIFAR-10, and ImageNet imagerecognition tasks, the proposed dual memory architectures performs much betterthan the classical online and incremental ensemble algorithm, and theiraccuracies are similar to that of the batch learner.
arxiv-11100-232 | Scoring and Classifying with Gated Auto-encoders | http://arxiv.org/pdf/1412.6610v5.pdf | author:Daniel Jiwoong Im, Graham W. Taylor category:cs.LG cs.NE published:2014-12-20 summary:Auto-encoders are perhaps the best-known non-probabilistic methods forrepresentation learning. They are conceptually simple and easy to train. Recenttheoretical work has shed light on their ability to capture manifold structure,and drawn connections to density modelling. This has motivated researchers toseek ways of auto-encoder scoring, which has furthered their use inclassification. Gated auto-encoders (GAEs) are an interesting and flexibleextension of auto-encoders which can learn transformations among differentimages or pixel covariances within images. However, they have been much lessstudied, theoretically or empirically. In this work, we apply a dynamicalsystems view to GAEs, deriving a scoring function, and drawing connections toRestricted Boltzmann Machines. On a set of deep learning benchmarks, we alsodemonstrate their effectiveness for single and multi-label classification.
arxiv-11100-233 | Compressing Convolutional Neural Networks | http://arxiv.org/pdf/1506.04449v1.pdf | author:Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, Yixin Chen category:cs.LG cs.CV cs.NE published:2015-06-14 summary:Convolutional neural networks (CNN) are increasingly used in many areas ofcomputer vision. They are particularly attractive because of their ability to"absorb" great quantities of labeled data through millions of parameters.However, as model sizes increase, so do the storage and memory requirements ofthe classifiers. We present a novel network architecture, Frequency-SensitiveHashed Nets (FreshNets), which exploits inherent redundancy in bothconvolutional layers and fully-connected layers of a deep learning model,leading to dramatic savings in memory and storage consumption. Based on the keyobservation that the weights of learned convolutional filters are typicallysmooth and low-frequency, we first convert filter weights to the frequencydomain with a discrete cosine transform (DCT) and use a low-cost hash functionto randomly group frequency parameters into hash buckets. All parametersassigned the same hash bucket share a single value learned with standardback-propagation. To further reduce model size we allocate fewer hash bucketsto high-frequency components, which are generally less important. We evaluateFreshNets on eight data sets, and show that it leads to drastically bettercompressed performance than several relevant baselines.
arxiv-11100-234 | Linguistics and some aspects of its underlying dynamics | http://arxiv.org/pdf/1506.08663v1.pdf | author:Massimo Piattelli-Palmarini, Giuseppe Vitiello category:cs.CL quant-ph published:2015-06-14 summary:In recent years, central components of a new approach to linguistics, theMinimalist Program (MP) have come closer to physics. Features of the MinimalistProgram, such as the unconstrained nature of recursive Merge, the operation ofthe Labeling Algorithm that only operates at the interface of Narrow Syntaxwith the Conceptual-Intentional and the Sensory-Motor interfaces, thedifference between pronounced and un-pronounced copies of elements in asentence and the build-up of the Fibonacci sequence in the syntactic derivationof sentence structures, are directly accessible to representation in terms ofalgebraic formalism. Although in our scheme linguistic structures are classicalones, we find that an interesting and productive isomorphism can be establishedbetween the MP structure, algebraic structures and many-body field theoryopening new avenues of inquiry on the dynamics underlying some central aspectsof linguistics.
arxiv-11100-235 | Leveraging Word Embeddings for Spoken Document Summarization | http://arxiv.org/pdf/1506.04365v1.pdf | author:Kuan-Yu Chen, Shih-Hung Liu, Hsin-Min Wang, Berlin Chen, Hsin-Hsi Chen category:cs.CL cs.AI published:2015-06-14 summary:Owing to the rapidly growing multimedia content available on the Internet,extractive spoken document summarization, with the purpose of automaticallyselecting a set of representative sentences from a spoken document to conciselyexpress the most important theme of the document, has been an active area ofresearch and experimentation. On the other hand, word embedding has emerged asa newly favorite research subject because of its excellent performance in manynatural language processing (NLP)-related tasks. However, as far as we areaware, there are relatively few studies investigating its use in extractivetext or speech summarization. A common thread of leveraging word embeddings inthe summarization process is to represent the document (or sentence) byaveraging the word embeddings of the words occurring in the document (orsentence). Then, intuitively, the cosine similarity measure can be employed todetermine the relevance degree between a pair of representations. Beyond thecontinued efforts made to improve the representation of words, this paperfocuses on building novel and efficient ranking models based on the generalword embedding methods for extractive speech summarization. Experimentalresults demonstrate the effectiveness of our proposed methods, compared toexisting state-of-the-art methods.
arxiv-11100-236 | Localized Multiple Kernel Learning---A Convex Approach | http://arxiv.org/pdf/1506.04364v1.pdf | author:Yunwen Lei, Alexander Binder, Ürün Dogan, Marius Kloft category:cs.LG published:2015-06-14 summary:We propose a localized approach to multiple kernel learning that, in contrastto prevalent approaches, can be formulated as a convex optimization problemover a given cluster structure. From which we obtain the first generalizationerror bounds for localized multiple kernel learning and derive an efficientoptimization algorithm based on the Fenchel dual representation. Experiments onreal-world datasets from the application domains of computational biology andcomputer vision show that the convex approach to localized multiple kernellearning can achieve higher prediction accuracies than its global andnon-convex local counterparts.
arxiv-11100-237 | Learning Better Word Embedding by Asymmetric Low-Rank Projection of Knowledge Graph | http://arxiv.org/pdf/1505.04891v2.pdf | author:Fei Tian, Bin Gao, Enhong Chen, Tie-Yan Liu category:cs.CL published:2015-05-19 summary:Word embedding, which refers to low-dimensional dense vector representationsof natural words, has demonstrated its power in many natural languageprocessing tasks. However, it may suffer from the inaccurate and incompleteinformation contained in the free text corpus as training data. To tackle thischallenge, there have been quite a few works that leverage knowledge graphs asan additional information source to improve the quality of word embedding.Although these works have achieved certain success, they have neglected someimportant facts about knowledge graphs: (i) many relationships in knowledgegraphs are \emph{many-to-one}, \emph{one-to-many} or even \emph{many-to-many},rather than simply \emph{one-to-one}; (ii) most head entities and tail entitiesin knowledge graphs come from very different semantic spaces. To address theseissues, in this paper, we propose a new algorithm named ProjectNet. ProjecNetmodels the relationships between head and tail entities after transforming themwith different low-rank projection matrices. The low-rank projection can allownon \emph{one-to-one} relationships between entities, while differentprojection matrices for head and tail entities allow them to originate indifferent semantic spaces. The experimental results demonstrate that ProjectNetyields more accurate word embedding than previous works, thus leads to clearimprovements in various natural language processing tasks.
arxiv-11100-238 | Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms | http://arxiv.org/pdf/1506.04359v1.pdf | author:Yunwen Lei, Ürün Dogan, Alexander Binder, Marius Kloft category:cs.LG published:2015-06-14 summary:This paper studies the generalization performance of multi-classclassification algorithms, for which we obtain, for the first time, adata-dependent generalization error bound with a logarithmic dependence on theclass size, substantially improving the state-of-the-art linear dependence inthe existing data-dependent generalization analysis. The theoretical analysismotivates us to introduce a new multi-class classification machine based on$\ell_p$-norm regularization, where the parameter $p$ controls the complexityof the corresponding bounds. We derive an efficient optimization algorithmbased on Fenchel duality theory. Benchmarks on several real-world datasets showthat the proposed algorithm can achieve significant accuracy gains over thestate of the art.
arxiv-11100-239 | The Artists who Forged Themselves: Detecting Creativity in Art | http://arxiv.org/pdf/1506.04356v1.pdf | author:Milan Rajković, Miloš Milovanović category:cs.CV q-bio.NC published:2015-06-14 summary:Creativity and the understanding of cognitive processes involved in thecreative process are relevant to all of human activities. Comprehension ofcreativity in the arts is of special interest due to the involvement of manyscientific and non scientific disciplines. Using digital representation ofpaintings, we show that creative process in painting art may be objectivelyrecognized within the mathematical framework of self organization, a processcharacteristic of nonlinear dynamic systems and occurring in natural and socialsciences. Unlike the artist identification process or the recognition offorgery, which presupposes the knowledge of the original work, our methodrequires no prior knowledge on the originality of the work of art. The originalpaintings are recognized as realizations of the creative process which, ingeneral, is shown to correspond to self-organization of texture features whichdetermine the aesthetic complexity of the painting. The method consists of thewavelet based statistical digital image processing and the measure ofstatistical complexity which represents the minimal (average) informationnecessary for optimal prediction. The statistical complexity is based on theproperly defined causal states with optimal predictive properties. Twodifferent time concepts related to the works of art are introduced: theinternal time and the artistic time. The internal time of the artwork isdetermined by the span of causal dependencies between wavelet coefficientswhile the artistic time refers to the internal time during which complexityincreases where complexity refers to compositional, aesthetic and structuralarrangement of texture features. The method is illustrated by recognizing theoriginal paintings from the copies made by the artists themselves, includingthe works of the famous surrealist painter Ren\'{e} Magritte.
arxiv-11100-240 | Deep Secure Encoding: An Application to Face Recognition | http://arxiv.org/pdf/1506.04340v1.pdf | author:Rohit Pandey, Yingbo Zhou, Venu Govindaraju category:cs.CV published:2015-06-14 summary:In this paper we present Deep Secure Encoding: a framework for secureclassification using deep neural networks, and apply it to the task ofbiometric template protection for faces. Using deep convolutional neuralnetworks (CNNs), we learn a robust mapping of face classes to high entropysecure codes. These secure codes are then hashed using standard hash functionslike SHA-256 to generate secure face templates. The efficacy of the approach isshown on two face databases, namely, CMU-PIE and Extended Yale B, where weachieve state of the art matching performance, along with cancelability andhigh security with no unrealistic assumptions. Furthermore, the scheme can workin both identification and verification modes.
arxiv-11100-241 | Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis | http://arxiv.org/pdf/1506.04338v1.pdf | author:Wei Yang, Haiting Lin, Sing Bing Kang, Jingyi Yu category:cs.CV published:2015-06-14 summary:In perspective cameras, images of a frontal-parallel 3D object preserve itsaspect ratio invariant to its depth. Such an invariance is useful inphotography but is unique to perspective projection. In this paper, we showthat alternative non-perspective cameras such as the crossed-slit or XSlitcameras exhibit a different depth-dependent aspect ratio (DDAR) property thatcan be used to 3D recovery. We first conduct a comprehensive analysis tocharacterize DDAR, infer object depth from its AR, and model recoverable depthrange, sensitivity, and error. We show that repeated shape patterns in realManhattan World scenes can be used for 3D reconstruction using a single XSlitimage. We also extend our analysis to model slopes of lines. Specifically,parallel 3D lines exhibit depth-dependent slopes (DDS) on their images whichcan also be used to infer their depths. We validate our analyses using realXSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show thatDDAR and DDS provide important depth cues and enable effective single-imagescene reconstruction.
arxiv-11100-242 | Contextual Dueling Bandits | http://arxiv.org/pdf/1502.06362v2.pdf | author:Miroslav Dudík, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, Masrour Zoghi category:cs.LG published:2015-02-23 summary:We consider the problem of learning to choose actions using contextualinformation when provided with limited feedback in the form of relativepairwise comparisons. We study this problem in the dueling-bandits framework ofYue et al. (2009), which we extend to incorporate context. Roughly, thelearner's goal is to find the best policy, or way of behaving, in some space ofpolicies, although "best" is not always so clearly defined. Here, we propose anew and natural solution concept, rooted in game theory, called a von Neumannwinner, a randomized policy that beats or ties every other policy. We show thatthis notion overcomes important limitations of existing solutions, particularlythe Condorcet winner which has typically been used in the past, but whichrequires strong and often unrealistic assumptions. We then present threeefficient algorithms for online learning in our setting, and for approximatinga von Neumann winner from batch-like data. The first of these algorithmsachieves particularly low regret, even when data is adversarial, although itstime and space requirements are linear in the size of the policy space. Theother two algorithms require time and space only logarithmic in the size of thepolicy space when provided access to an oracle for solving classificationproblems on the space.
arxiv-11100-243 | A General Framework for Fast Stagewise Algorithms | http://arxiv.org/pdf/1408.5801v2.pdf | author:Ryan J. Tibshirani category:stat.ML stat.CO published:2014-08-25 summary:Forward stagewise regression follows a very simple strategy for constructinga sequence of sparse regression estimates: it starts with all coefficientsequal to zero, and iteratively updates the coefficient (by a small amount$\epsilon$) of the variable that achieves the maximal absolute inner productwith the current residual. This procedure has an interesting connection to thelasso: under some conditions, it is known that the sequence of forwardstagewise estimates exactly coincides with the lasso path, as the step size$\epsilon$ goes to zero. Furthermore, essentially the same equivalence holdsoutside of least squares regression, with the minimization of a differentiableconvex loss function subject to an $\ell_1$ norm constraint (the stagewisealgorithm now updates the coefficient corresponding to the maximal absolutecomponent of the gradient). Even when they do not match their $\ell_1$-constrained analogues, stagewiseestimates provide a useful approximation, and are computationally appealing.Their success in sparse modeling motivates the question: can a simple,effective strategy like forward stagewise be applied more broadly in otherregularization settings, beyond the $\ell_1$ norm and sparsity? The currentpaper is an attempt to do just this. We present a general framework forstagewise estimation, which yields fast algorithms for problems such asgroup-structured learning, matrix completion, image denoising, and more.
arxiv-11100-244 | On the Optimality of Averaging in Distributed Statistical Learning | http://arxiv.org/pdf/1407.2724v2.pdf | author:Jonathan Rosenblatt, Boaz Nadler category:stat.ML math.ST stat.TH published:2014-07-10 summary:A common approach to statistical learning with big-data is to randomly splitit among $m$ machines and learn the parameter of interest by averaging the $m$individual estimates. In this paper, focusing on empirical risk minimization,or equivalently M-estimation, we study the statistical error incurred by thisstrategy. We consider two large-sample settings: First, a classical settingwhere the number of parameters $p$ is fixed, and the number of samples permachine $n\to\infty$. Second, a high-dimensional regime where both$p,n\to\infty$ with $p/n \to \kappa \in (0,1)$. For both regimes and undersuitable assumptions, we present asymptotically exact expressions for thisestimation error. In the fixed-$p$ setting, under suitable assumptions, weprove that to leading order averaging is as accurate as the centralizedsolution. We also derive the second order error terms, and show that these canbe non-negligible, notably for non-linear models. The high-dimensional setting,in contrast, exhibits a qualitatively different behavior: data splitting incursa first-order accuracy loss, which to leading order increases linearly with thenumber of machines. The dependence of our error approximations on the number ofmachines traces an interesting accuracy-complexity tradeoff, allowing thepractitioner an informed choice on the number of machines to deploy. Finally,we confirm our theoretical analysis with several simulations.
arxiv-11100-245 | How much is said in a microblog? A multilingual inquiry based on Weibo and Twitter | http://arxiv.org/pdf/1506.00572v2.pdf | author:Han-Teng Liao, King-wa Fu, Scott A. Hale category:cs.SI cs.CL cs.CY H.5.3, H.5.4 published:2015-06-01 summary:This paper presents a multilingual study on, per single post of microblogtext, (a) how much can be said, (b) how much is written in terms of charactersand bytes, and (c) how much is said in terms of information content in posts bydifferent organizations in different languages. Focusing on three differentlanguages (English, Chinese, and Japanese), this research analyses Weibo andTwitter accounts of major embassies and news agencies. We first establish ourcriterion for quantifying "how much can be said" in a digital text based on theopenly available Universal Declaration of Human Rights and the translatedsubtitles from TED talks. These parallel corpora allow us to determine thenumber of characters and bits needed to represent the same content in differentlanguages and character encodings. We then derive the amount of informationthat is actually contained in microblog posts authored by selected accounts onWeibo and Twitter. Our results confirm that languages with larger charactersets such as Chinese and Japanese contain more information per character thanEnglish, but the actual information content contained within a microblog textvaries depending on both the type of organization and the language of the post.We conclude with a discussion on the design implications of microblog textlimits for different languages.
arxiv-11100-246 | Extract an essential skeleton of a character as a graph from a character image | http://arxiv.org/pdf/1506.05068v1.pdf | author:Kazuhisa Fujita category:cs.CV published:2015-06-13 summary:This paper aims to make a graph representing an essential skeleton of acharacter from an image that includes a machine printed or a handwrittencharacter using the growing neural gas (GNG) method and the relativeneighborhood graph (RNG) algorithm. The visual system in our brain canrecognize printed characters and handwritten characters easily, robustly, andprecisely. How can our brains robustly recognize characters? In the visualprocessing in our brain, essential features of an object will be used forrecognition. The essential features are crosses, corners, junctions and so on.These features may be useful for character recognition by a computer. However,extraction of the features is difficult. If the skeleton of a character isrepresented as a graph, the features can be more easily extracted. To extractthe skeleton of a character as a graph from a character image, we used the GNGmethod and the RNG algorithm. We achieved to extract skeleton graphs fromimages including distorted, noisy, and handwritten characters.
arxiv-11100-247 | Fusing Continuous-valued Medical Labels using a Bayesian Model | http://arxiv.org/pdf/1503.06619v2.pdf | author:Tingting Zhu, Nic Dunkley, Joachim Behar, David A. Clifton, Gari D. Clifford category:cs.LG published:2015-03-23 summary:With the rapid increase in volume of time series medical data availablethrough wearable devices, there is a need to employ automated algorithms tolabel data. Examples of labels include interventions, changes in activity (e.g.sleep) and changes in physiology (e.g. arrhythmias). However, automatedalgorithms tend to be unreliable resulting in lower quality care. Expertannotations are scarce, expensive, and prone to significant inter- andintra-observer variance. To address these problems, a BayesianContinuous-valued Label Aggregator(BCLA) is proposed to provide a reliableestimation of label aggregation while accurately infer the precision and biasof each algorithm. The BCLA was applied to QT interval (pro-arrhythmicindicator) estimation from the electrocardiogram using labels from the 2006PhysioNet/Computing in Cardiology Challenge database. It was compared to themean, median, and a previously proposed Expectation Maximization (EM) labelaggregation approaches. While accurately predicting each labelling algorithm'sbias and precision, the root-mean-square error of the BCLA was11.78$\pm$0.63ms, significantly outperforming the best Challenge entry(15.37$\pm$2.13ms) as well as the EM, mean, and median voting strategies(14.76$\pm$0.52ms, 17.61$\pm$0.55ms, and 14.43$\pm$0.57ms respectively with$p<0.0001$).
arxiv-11100-248 | Contamination Estimation via Convex Relaxations | http://arxiv.org/pdf/1506.04257v1.pdf | author:Matthew L. Malloy, Scott Alfeld, Paul Barford category:cs.IT cs.LG math.IT math.OC published:2015-06-13 summary:Identifying anomalies and contamination in datasets is important in a widevariety of settings. In this paper, we describe a new technique for estimatingcontamination in large, discrete valued datasets. Our approach considers thenormal condition of the data to be specified by a model consisting of a set ofdistributions. Our key contribution is in our approach to contaminationestimation. Specifically, we develop a technique that identifies the minimumnumber of data points that must be discarded (i.e., the level of contamination)from an empirical data set in order to match the model to within a specifiedgoodness-of-fit, controlled by a p-value. Appealing to results from largedeviations theory, we show a lower bound on the level of contamination isobtained by solving a series of convex programs. Theoretical results guaranteethe bound converges at a rate of $O(\sqrt{\log(p)/p})$, where p is the size ofthe empirical data set.
arxiv-11100-249 | Evaluation of the Accuracy of the BGLemmatizer | http://arxiv.org/pdf/1506.04229v1.pdf | author:Elena Karashtranova, Grigor Iliev, Nadezhda Borisova, Yana Chankova, Irena Atanasova category:cs.CL published:2015-06-13 summary:This paper reveals the results of an analysis of the accuracy of developedsoftware for automatic lemmatization for the Bulgarian language. Thislemmatization software is written entirely in Java and is distributed as a GATEplugin. Certain statistical methods are used to define the accuracy of thissoftware. The results of the analysis show 95% lemmatization accuracy.
arxiv-11100-250 | A Publicly Available Cross-Platform Lemmatizer for Bulgarian | http://arxiv.org/pdf/1506.04228v1.pdf | author:Grigor Iliev, Nadezhda Borisova, Elena Karashtranova, Dafina Kostadinova category:cs.CL published:2015-06-13 summary:Our dictionary-based lemmatizer for the Bulgarian language presented here isdistributed as free software, publicly available to download and use under theGPL v3 license. The presented software is written entirely in Java and isdistributed as a GATE plugin. To our best knowledge, at the time of writingthis article, there are not any other free lemmatization tools specificallytargeting the Bulgarian language. The presented lemmatizer is a work inprogress and currently yields an accuracy of about 95% in comparison to themanually annotated corpus BulTreeBank-Morph, which contains 273933 tokens.
arxiv-11100-251 | Deep Structured Models For Group Activity Recognition | http://arxiv.org/pdf/1506.04191v1.pdf | author:Zhiwei Deng, Mengyao Zhai, Lei Chen, Yuhao Liu, Srikanth Muralidharan, Mehrsan Javan Roshtkhari, Greg Mori category:cs.CV published:2015-06-12 summary:This paper presents a deep neural-network-based hierarchical graphical modelfor individual and group activity recognition in surveillance scenes. Deepnetworks are used to recognize the actions of individual people in a scene.Next, a neural-network-based hierarchical graphical model refines the predictedlabels for each class by considering dependencies between the classes. Thisrefinement step mimics a message-passing step similar to inference in aprobabilistic graphical model. We show that this approach can be effective ingroup activity recognition, with the deep graphical model improving recognitionrates over baseline methods.
arxiv-11100-252 | Search Strategies for Binary Feature Selection for a Naive Bayes Classifier | http://arxiv.org/pdf/1506.04177v1.pdf | author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG published:2015-06-12 summary:We compare in this paper several feature selection methods for the NaiveBayes Classifier (NBC) when the data under study are described by a largenumber of redundant binary indicators. Wrapper approaches guided by the NBCestimation of the classification error probability out-perform filterapproaches while retaining a reasonable computational cost.
arxiv-11100-253 | Using the Mean Absolute Percentage Error for Regression Models | http://arxiv.org/pdf/1506.04176v1.pdf | author:Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, Fabrice Rossi category:stat.ML cs.LG published:2015-06-12 summary:We study in this paper the consequences of using the Mean Absolute PercentageError (MAPE) as a measure of quality for regression models. We show thatfinding the best model under the MAPE is equivalent to doing weighted MeanAbsolute Error (MAE) regression. We show that universal consistency ofEmpirical Risk Minimization remains possible using the MAPE instead of the MAE.
arxiv-11100-254 | Exact ICL maximization in a non-stationary time extension of the latent block model for dynamic networks | http://arxiv.org/pdf/1506.04138v1.pdf | author:Marco Corneli, Pierre Latouche, Fabrice Rossi category:stat.ML published:2015-06-12 summary:The latent block model (LBM) is a flexible probabilistic tool to describeinteractions between node sets in bipartite networks, but it does not accountfor interactions of time varying intensity between nodes in unknown classes. Inthis paper we propose a non stationary temporal extension of the LBM thatclusters simultaneously the two node sets of a bipartite network and constructsclasses of time intervals on which interactions are stationary. The number ofclusters as well as the membership to classes are obtained by maximizing theexact complete-data integrated likelihood relying on a greedy search approach.Experiments on simulated and real data are carried out in order to assess theproposed methodology.
arxiv-11100-255 | Reducing offline evaluation bias of collaborative filtering algorithms | http://arxiv.org/pdf/1506.04135v1.pdf | author:Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, Fabrice Rossi category:cs.IR cs.LG stat.ML published:2015-06-12 summary:Recommendation systems have been integrated into the majority of large onlinesystems to filter and rank information according to user profiles. It thusinfluences the way users interact with the system and, as a consequence, biasthe evaluation of the performance of a recommendation algorithm computed usinghistorical data (via offline evaluation). This paper presents a new applicationof a weighted offline evaluation to reduce this bias for collaborativefiltering algorithms.
arxiv-11100-256 | CloudCV: Large Scale Distributed Computer Vision as a Cloud Service | http://arxiv.org/pdf/1506.04130v1.pdf | author:Harsh Agrawal, Clint Solomon Mathialagan, Yash Goyal, Neelima Chavali, Prakriti Banik, Akrit Mohapatra, Ahmed Osman, Dhruv Batra category:cs.CV cs.DC published:2015-06-12 summary:We are witnessing a proliferation of massive visual data. Unfortunatelyscaling existing computer vision algorithms to large datasets leavesresearchers repeatedly solving the same algorithmic, logistical, andinfrastructural problems. Our goal is to democratize computer vision; oneshould not have to be a computer vision, big data and distributed computingexpert to have access to state-of-the-art distributed computer visionalgorithms. We present CloudCV, a comprehensive system to provide access tostate-of-the-art distributed computer vision algorithms as a cloud servicethrough a Web Interface and APIs.
arxiv-11100-257 | Random Maxout Features | http://arxiv.org/pdf/1506.03705v2.pdf | author:Youssef Mroueh, Steven Rennie, Vaibhava Goel category:cs.LG stat.ML published:2015-06-11 summary:In this paper, we propose and study random maxout features, which areconstructed by first projecting the input data onto sets of randomly generatedvectors with Gaussian elements, and then outputing the maximum projection valuefor each set. We show that the resulting random feature map, when used inconjunction with linear models, allows for the locally linear estimation of thefunction of interest in classification tasks, and for the locally linearembedding of points when used for dimensionality reduction or datavisualization. We derive generalization bounds for learning that assess theerror in approximating locally linear functions by linear functions in themaxout feature space, and empirically evaluate the efficacy of the approach onthe MNIST and TIMIT classification tasks.
arxiv-11100-258 | Automatic Variational Inference in Stan | http://arxiv.org/pdf/1506.03431v2.pdf | author:Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, David M. Blei category:stat.ML published:2015-06-10 summary:Variational inference is a scalable technique for approximate Bayesianinference. Deriving variational inference algorithms requires tediousmodel-specific calculations; this makes it difficult to automate. We propose anautomatic variational inference algorithm, automatic differentiationvariational inference (ADVI). The user only provides a Bayesian model and adataset; nothing else. We make no conjugacy assumptions and support a broadclass of models. The algorithm automatically determines an appropriatevariational family and optimizes the variational objective. We implement ADVIin Stan (code available now), a probabilistic programming framework. We compareADVI to MCMC sampling across hierarchical generalized linear models,nonconjugate matrix factorization, and a mixture model. We train the mixturemodel on a quarter million images. With ADVI we can use variational inferenceon any model we write in Stan.
arxiv-11100-259 | Adaptive Stochastic Primal-Dual Coordinate Descent for Separable Saddle Point Problems | http://arxiv.org/pdf/1506.04093v1.pdf | author:Zhanxing Zhu, Amos J. Storkey category:stat.ML cs.LG published:2015-06-12 summary:We consider a generic convex-concave saddle point problem with separablestructure, a form that covers a wide-ranged machine learning applications.Under this problem structure, we follow the framework of primal-dual updatesfor saddle point problems, and incorporate stochastic block coordinate descentwith adaptive stepsize into this framework. We theoretically show that ourproposal of adaptive stepsize potentially achieves a sharper linear convergencerate compared with the existing methods. Additionally, since we can select"mini-batch" of block coordinates to update, our method is also amenable toparallel processing for large-scale data. We apply the proposed method toregularized empirical risk minimization and show that it performs comparablyor, more often, better than state-of-the-art methods on both synthetic andreal-world data sets.
arxiv-11100-260 | Towards Benchmarking Scene Background Initialization | http://arxiv.org/pdf/1506.04051v1.pdf | author:Lucia Maddalena, Alfredo Petrosino category:cs.CV published:2015-06-12 summary:Given a set of images of a scene taken at different times, the availabilityof an initial background model that describes the scene without foregroundobjects is the prerequisite for a wide range of applications, ranging fromvideo surveillance to computational photography. Even though several methodshave been proposed for scene background initialization, the lack of a commongroundtruthed dataset and of a common set of metrics makes it difficult tocompare their performance. To move first steps towards an easy and faircomparison of these methods, we assembled a dataset of sequences frequentlyadopted for background initialization, selected or created ground truths forquantitative evaluation through a selected suite of metrics, and comparedresults obtained by some existing methods, making all the material publiclyavailable.
arxiv-11100-261 | Multi-Atlas Segmentation of Biomedical Images: A Survey | http://arxiv.org/pdf/1412.3421v2.pdf | author:Juan Eugenio Iglesias, Mert Rory Sabuncu category:cs.CV published:2014-12-10 summary:Multi-atlas segmentation (MAS), first introduced and popularized by thepioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh,Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckertand Hammers (2006), is becoming one of the most widely-used and successfulimage segmentation techniques in biomedical applications. By manipulating andutilizing the entire dataset of "atlases" (training images that have beenpreviously labeled, e.g., manually by an expert), rather than some model-basedaverage representation, MAS has the flexibility to better capture anatomicalvariation, thus offering superior segmentation accuracy. This benefit, however,typically comes at a high computational cost. Recent advancements in computerhardware and image processing software have been instrumental in addressingthis challenge and facilitated the wide adoption of MAS. Today, MAS has come along way and the approach includes a wide array of sophisticated algorithmsthat employ ideas from machine learning, probabilistic modeling, optimization,and computer vision, among other fields. This paper presents a survey ofpublished MAS algorithms and studies that have applied these methods to variousbiomedical problems. In writing this survey, we have three distinct aims. Ourprimary goal is to document how MAS was originally conceived, later evolved,and now relates to alternative methods. Second, this paper is intended to be adetailed reference of past research activity in MAS, which now spans over adecade (2003 - 2014) and entails novel methodological developments andapplication-specific solutions. Finally, our goal is to also present aperspective on the future of MAS, which, we believe, will be one of thedominant approaches in biomedical image segmentation.
arxiv-11100-262 | On Using Monolingual Corpora in Neural Machine Translation | http://arxiv.org/pdf/1503.03535v2.pdf | author:Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, Yoshua Bengio category:cs.CL published:2015-03-11 summary:Recent work on end-to-end neural network-based architectures for machinetranslation has shown promising results for En-Fr and En-De translation.Arguably, one of the major factors behind this success has been theavailability of high quality parallel corpora. In this work, we investigate howto leverage abundant monolingual corpora for neural machine translation.Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$BLEU improvement on the low-resource language pair Turkish-English, and $1.59$BLEU on the focused domain task of Chinese-English chat messages. While ourmethod was initially targeted toward such tasks with less parallel data, weshow that it also extends to high resource languages such as Cs-En and De-Enwhere we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neuralmachine translation baselines, respectively.
arxiv-11100-263 | Knowledge Representation in Learning Classifier Systems: A Review | http://arxiv.org/pdf/1506.04002v1.pdf | author:Farzaneh Shoeleh, Mahshid Majd, Ali Hamzeh, Sattar Hashemi category:cs.NE cs.LG published:2015-06-12 summary:Knowledge representation is a key component to the success of all rule basedsystems including learning classifier systems (LCSs). This component bringsinsight into how to partition the problem space what in turn seeks prominentrole in generalization capacity of the system as a whole. Recently, knowledgerepresentation component has received great deal of attention within datamining communities due to its impacts on rule based systems in terms ofefficiency and efficacy. The current work is an attempt to find a comprehensiveand yet elaborate view into the existing knowledge representation techniques inLCS domain in general and XCS in specific. To achieve the objectives, knowledgerepresentation techniques are grouped into different categories based on theclassification approach in which they are incorporated. In each category, theunderlying rule representation schema and the format of classifier condition tosupport the corresponding representation are presented. Furthermore, a preciseexplanation on the way that each technique partitions the problem space alongwith the extensive experimental results is provided. To have an elaborated viewon the functionality of each technique, a comparative analysis of existingtechniques on some conventional problems is provided. We expect this survey tobe of interest to the LCS researchers and practitioners since it provides aguideline for choosing a proper knowledge representation technique for a givenproblem and also opens up new streams of research on this topic.
arxiv-11100-264 | MCMC for Variationally Sparse Gaussian Processes | http://arxiv.org/pdf/1506.04000v1.pdf | author:James Hensman, Alexander G. de G. Matthews, Maurizio Filippone, Zoubin Ghahramani category:stat.ML published:2015-06-12 summary:Gaussian process (GP) models form a core part of probabilistic machinelearning. Considerable research effort has been made into attacking threeissues with GP models: how to compute efficiently when the number of data islarge; how to approximate the posterior when the likelihood is not Gaussian andhow to estimate covariance function parameter posteriors. This papersimultaneously addresses these, using a variational approximation to theposterior which is sparse in support of the function but otherwise free-form.The result is a Hybrid Monte-Carlo sampling scheme which allows for anon-Gaussian approximation over the function values and covariance parameterssimultaneously, with efficient computations based on inducing-point sparse GPs.Code to replicate each experiment in this paper will be available shortly.
arxiv-11100-265 | Sparse Multi-layer Image Approximation: Facial Image Compression | http://arxiv.org/pdf/1506.03998v1.pdf | author:Sohrab Ferdowsi, Svyatoslav Voloshynovskiy, Dimche Kostadinov category:cs.CV cs.IT math.IT published:2015-06-12 summary:We propose a scheme for multi-layer representation of images. The problem isfirst treated from an information-theoretic viewpoint where we analyze thebehavior of different sources of information under a multi-layer datacompression framework and compare it with a single-stage (shallow) structure.We then consider the image data as the source of information and link theproposed representation scheme to the problem of multi-layer dictionarylearning for visual data. For the current work we focus on the problem of imagecompression for a special class of images where we report a considerableperformance boost in terms of PSNR at high compression ratios in comparisonwith the JPEG2000 codec.
arxiv-11100-266 | Technical Report: Image Captioning with Semantically Similar Images | http://arxiv.org/pdf/1506.03995v1.pdf | author:Martin Kolář, Michal Hradiš, Pavel Zemčík category:cs.CV published:2015-06-12 summary:This report presents our submission to the MS COCO Captioning Challenge 2015.The method uses Convolutional Neural Network activations as an embedding tofind semantically similar images. From these images, the most typical captionis selected based on unigram frequencies. Although the method received lowscores with automated evaluation metrics and in human assessed averagecorrectness, it is competitive in the ratio of captions which pass the Turingtest and which are assessed as better or equal to human captions.
arxiv-11100-267 | Phase Retrieval using Alternating Minimization | http://arxiv.org/pdf/1306.0160v2.pdf | author:Praneeth Netrapalli, Prateek Jain, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT published:2013-06-02 summary:Phase retrieval problems involve solving linear equations, but with missingsign (or phase, for complex numbers) information. More than four decades afterit was first proposed, the seminal error reduction algorithm of (Gerchberg andSaxton 1972) and (Fienup 1982) is still the popular choice for solving manyvariants of this problem. The algorithm is based on alternating minimization;i.e. it alternates between estimating the missing phase information, and thecandidate solution. Despite its wide usage in practice, no global convergenceguarantees for this algorithm are known. In this paper, we show that a(resampling) variant of this approach converges geometrically to the solutionof one such problem -- finding a vector $\mathbf{x}$ from$\mathbf{y},\mathbf{A}$, where $\mathbf{y} =\left\mathbf{A}^{\top}\mathbf{x}\right$ and $\mathbf{z}$ denotes a vectorof element-wise magnitudes of $\mathbf{z}$ -- under the assumption that$\mathbf{A}$ is Gaussian. Empirically, we demonstrate that alternating minimization performs similar torecently proposed convex techniques for this problem (which are based on"lifting" to a convex matrix problem) in sample complexity and robustness tonoise. However, it is much more efficient and can scale to large problems.Analytically, for a resampling version of alternating minimization, we showgeometric convergence to the solution, and sample complexity that is off by logfactors from obvious lower bounds. We also establish close to optimal scalingfor the case when the unknown vector is sparse. Our work represents the firsttheoretical guarantee for alternating minimization (albeit with resampling) forany variant of phase retrieval problems in the non-convex setting.
arxiv-11100-268 | Robust Structured Low-Rank Approximation on the Grassmannian | http://arxiv.org/pdf/1506.03958v1.pdf | author:Clemens Hage, Martin Kleinsteuber category:stat.ML published:2015-06-12 summary:Over the past years Robust PCA has been established as a standard tool forreliable low-rank approximation of matrices in the presence of outliers.Recently, the Robust PCA approach via nuclear norm minimization has beenextended to matrices with linear structures which appear in applications suchas system identification and data series analysis. At the same time it has beenshown how to control the rank of a structured approximation via matrixfactorization approaches. The drawbacks of these methods either lie in the lackof robustness against outliers or in their static nature of repeatedbatch-processing. We present a Robust Structured Low-Rank Approximation methodon the Grassmannian that on the one hand allows for fast re-initialization inan online setting due to subspace identification with manifolds, and that isrobust against outliers due to a smooth approximation of the $\ell_p$-norm costfunction on the other hand. The method is evaluated in online time seriesforecasting tasks on simulated and real-world data.
arxiv-11100-269 | Optimal $γ$ and $C$ for $ε$-Support Vector Regression with RBF Kernels | http://arxiv.org/pdf/1506.03942v1.pdf | author:Longfei Lu category:cs.LG stat.ML published:2015-06-12 summary:The objective of this study is to investigate the efficient determination of$C$ and $\gamma$ for Support Vector Regression with RBF or mahalanobis kernelbased on numerical and statistician considerations, which indicates theconnection between $C$ and kernels and demonstrates that the deviation ofgeometric distance of neighbour observation in mapped space effects the predictaccuracy of $\epsilon$-SVR. We determinate the arrange of $\gamma$ & $C$ andpropose our method to choose their best values.
arxiv-11100-270 | A Novel Hybrid Approach for Cephalometric Landmark Detection | http://arxiv.org/pdf/1506.03936v1.pdf | author:Mahshid Majd, Farzaneh Shoeleh category:cs.CV published:2015-06-12 summary:Cephalometric analysis has an important role in dentistry and especially inorthodontics as a treatment planning tool to gauge the size and specialrelationships of the teeth, jaws and cranium. The first step of using suchanalyses is localizing some important landmarks known as cephalometriclandmarks on craniofacial in x-ray image. The past decade has seen a growinginterest in automating this process. In this paper, a novel hybrid approach isproposed for automatic detection of cephalometric landmarks. Here, thelandmarks are categorized into three main sets according to their anatomicalcharacteristics and usage in well-known cephalometric analyses. Consequently,to have a reliable and accurate detection system, three methods named edgetracing, weighted template matching, and analysis based estimation aredesigned, each of which is consistent and well-suited for one category. Edgetracing method is suggested to predict those landmarks which are located onedges. Weighted template matching method is well-suited for landmarks locatedin an obvious and specific structure which can be extracted or searchable in agiven x-ray image. The last but not the least method is named analysis basedestimation. This method is based on the fact that in cephalometric analyses therelations between landmarks are used and the locations of some landmarks arenever used individually. Therefore the third suggested method has a novelty inestimating the desired relations directly. The effectiveness of the proposedapproach is compared with the state of the art methods and the results werepromising especially in real world applications.
arxiv-11100-271 | Syntax-based Deep Matching of Short Texts | http://arxiv.org/pdf/1503.02427v6.pdf | author:Mingxuan Wang, Zhengdong Lu, Hang Li, Qun Liu category:cs.CL cs.LG cs.NE published:2015-03-09 summary:Many tasks in natural language processing, ranging from machine translationto question answering, can be reduced to the problem of matching two sentencesor more generally two short texts. We propose a new approach to the problem,called Deep Match Tree (DeepMatch$_{tree}$), under a general setting. Theapproach consists of two components, 1) a mining algorithm to discover patternsfor matching two short-texts, defined in the product space of dependency trees,and 2) a deep neural network for matching short texts using the mined patterns,as well as a learning algorithm to build the network having a sparse structure.We test our algorithm on the problem of matching a tweet and a response insocial media, a hard matching problem proposed in [Wang et al., 2013], and showthat DeepMatch$_{tree}$ can outperform a number of competitor models includingone without using dependency trees and one based on word-embedding, all withlarge margins
arxiv-11100-272 | MCMC Learning | http://arxiv.org/pdf/1307.3617v2.pdf | author:Varun Kanade, Elchanan Mossel category:cs.LG stat.ML published:2013-07-13 summary:The theory of learning under the uniform distribution is rich and deep, withconnections to cryptography, computational complexity, and the analysis ofboolean functions to name a few areas. This theory however is very limited dueto the fact that the uniform distribution and the corresponding Fourier basisare rarely encountered as a statistical model. A family of distributions that vastly generalizes the uniform distribution onthe Boolean cube is that of distributions represented by Markov Random Fields(MRF). Markov Random Fields are one of the main tools for modeling highdimensional data in many areas of statistics and machine learning. In this paper we initiate the investigation of extending central ideas,methods and algorithms from the theory of learning under the uniformdistribution to the setup of learning concepts given examples from MRFdistributions. In particular, our results establish a novel connection betweenproperties of MCMC sampling of MRFs and learning under the MRF distribution.
arxiv-11100-273 | Consistency and fluctuations for stochastic gradient Langevin dynamics | http://arxiv.org/pdf/1409.0578v2.pdf | author:Yee Whye Teh, Alexandre Thiéry, Sebastian Vollmer category:stat.ML 60J22, 65C40 published:2014-09-01 summary:Applying standard Markov chain Monte Carlo (MCMC) algorithms to large datasets is computationally expensive. Both the calculation of the acceptanceprobability and the creation of informed proposals usually require an iterationthrough the whole data set. The recently proposed stochastic gradient Langevindynamics (SGLD) method circumvents this problem by generating proposals whichare only based on a subset of the data, by skipping the accept-reject step andby using decreasing step-sizes sequence $(\delta_m)_{m \geq 0}$. %Under appropriate Lyapunov conditions, We provide in this article a rigorousmathematical framework for analysing this algorithm. We prove that, underverifiable assumptions, the algorithm is consistent, satisfies a central limittheorem (CLT) and its asymptotic bias-variance decomposition can becharacterized by an explicit functional of the step-sizes sequence$(\delta_m)_{m \geq 0}$. We leverage this analysis to give practicalrecommendations for the notoriously difficult tuning of this algorithm: it isasymptotically optimal to use a step-size sequence of the type $\delta_m \asympm^{-1/3}$, leading to an algorithm whose mean squared error (MSE) decreases atrate $\mathcal{O}(m^{-1/3})$
arxiv-11100-274 | Place classification with a graph regularized deep neural network model | http://arxiv.org/pdf/1506.03899v1.pdf | author:Yiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi, Yong Liu category:cs.RO cs.CV cs.LG cs.NE published:2015-06-12 summary:Place classification is a fundamental ability that a robot should possess tocarry out effective human-robot interactions. It is a nontrivial classificationproblem which has attracted many research. In recent years, there is a highexploitation of Artificial Intelligent algorithms in robotics applications.Inspired by the recent successes of deep learning methods, we propose anend-to-end learning approach for the place classification problem. With thedeep architectures, this methodology automatically discovers features andcontributes in general to higher classification accuracies. The pipeline of ourapproach is composed of three parts. Firstly, we construct multiple layers oflaser range data to represent the environment information in different levelsof granularity. Secondly, each layer of data is fed into a deep neural networkmodel for classification, where a graph regularization is imposed to the deeparchitecture for keeping local consistency between adjacent samples. Finally,the predicted labels obtained from all the layers are fused based on confidencetrees to maximize the overall confidence. Experimental results validate theeffective- ness of our end-to-end place classification framework in which boththe multi-layer structure and the graph regularization promote theclassification performance. Furthermore, results show that the featuresautomatically learned from the raw input range data can achieve competitiveresults to the features constructed based on statistical and geometricalinformation.
arxiv-11100-275 | Classify Images with Conceptor Network | http://arxiv.org/pdf/1506.00815v4.pdf | author:Yuhuang Hu, M. S. Ishwarya, Chu Kiong Loo category:cs.CV published:2015-06-02 summary:This article demonstrates a new conceptor network based classifier inclassifying images. Mathematical descriptions and analysis are presented.Various tests are experimented using three benchmark datasets: MNIST, CIFAR-10and CIFAR-100. The experiments displayed that conceptor network can offersuperior results and flexible configurations than conventional classifiers suchas Softmax Regression and Support Vector Machine (SVM).
arxiv-11100-276 | Causal inference via algebraic geometry: necessary and sufficient conditions for the feasibility of discrete causal models | http://arxiv.org/pdf/1506.03880v1.pdf | author:Ciarán M. Lee, Robert W. Spekkens category:stat.ML quant-ph published:2015-06-12 summary:We provide a scheme for inferring causal relations from uncontrolledstatistical data which makes use of all of the information in the jointprobability distribution over the observed variables rather than just theconditional independence relations. We focus on causal models containing justtwo observed variables, each of which is binary. We allow any number of latentvariables and we do not impose any restriction on the manner in which theobserved variables may depend functionally on the latent ones. In particular,the noise need not be additive. We provide an inductive scheme for classifyingcausal models into distinct observational equivalence classes. For eachobservational equivalence class, we provide a procedure for deriving, usingtechniques from algebraic geometry, necessary and sufficient conditions on thejoint distribution for the feasibility of the class. Connections andapplications of these results to the emerging field of quantum causal modelsare also discussed.
arxiv-11100-277 | Hierarchies of Relaxations for Online Prediction Problems with Evolving Constraints | http://arxiv.org/pdf/1503.01212v2.pdf | author:Alexander Rakhlin, Karthik Sridharan category:cs.LG cs.DS stat.ML published:2015-03-04 summary:We study online prediction where regret of the algorithm is measured againsta benchmark defined via evolving constraints. This framework captures onlineprediction on graphs, as well as other prediction problems with combinatorialstructure. A key aspect here is that finding the optimal benchmark predictor(even in hindsight, given all the data) might be computationally hard due tothe combinatorial nature of the constraints. Despite this, we providepolynomial-time \emph{prediction} algorithms that achieve low regret againstcombinatorial benchmark sets. We do so by building improper learning algorithmsbased on two ideas that work together. The first is to alleviate part of thecomputational burden through random playout, and the second is to employLasserre semidefinite hierarchies to approximate the resulting integer program.Interestingly, for our prediction algorithms, we only need to compute thevalues of the semidefinite programs and not the rounded solutions. However, theintegrality gap for Lasserre hierarchy \emph{does} enter the generic regretbound in terms of Rademacher complexity of the benchmark set. This establishesa trade-off between the computation time and the regret bound of the algorithm.
arxiv-11100-278 | Tree-Cut for Probabilistic Image Segmentation | http://arxiv.org/pdf/1506.03852v1.pdf | author:Shell X. Hu, Christopher K. I. Williams, Sinisa Todorovic category:stat.ML cs.CV published:2015-06-11 summary:This paper presents a new probabilistic generative model for imagesegmentation, i.e. the task of partitioning an image into homogeneous regions.Our model is grounded on a mid-level image representation, called a regiontree, in which regions are recursively split into subregions until superpixelsare reached. Given the region tree, image segmentation is formalized assampling cuts in the tree from the model. Inference for the cuts is exact, andformulated using dynamic programming. Our tree-cut model can be tuned to samplesegmentations at a particular scale of interest out of many possible multiscaleimage segmentations. This generalizes the common notion that there should beonly one correct segmentation per image. Also, it allows moving beyond thestandard single-scale evaluation, where the segmentation result for an image isaveraged against the corresponding set of coarse and fine human annotations, toconduct a scale-specific evaluation. Our quantitative results are comparable tothose of the leading gPb-owt-ucm method, with the notable advantage that weadditionally produce a distribution over all possible tree-consistentsegmentations of the image.
arxiv-11100-279 | Rectified Factor Networks | http://arxiv.org/pdf/1502.06464v2.pdf | author:Djork-Arné Clevert, Andreas Mayr, Thomas Unterthiner, Sepp Hochreiter category:cs.LG cs.CV cs.NE stat.ML published:2015-02-23 summary:We propose rectified factor networks (RFNs) to efficiently construct verysparse, non-linear, high-dimensional representations of the input. RFN modelsidentify rare and small events in the input, have a low interference betweencode units, have a small reconstruction error, and explain the data covariancestructure. RFN learning is a generalized alternating minimization algorithmderived from the posterior regularization method which enforces non-negativeand normalized posterior means. We proof convergence and correctness of the RFNlearning algorithm. On benchmarks, RFNs are compared to other unsupervisedmethods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast toprevious sparse coding methods, RFNs yield sparser codes, capture the data'scovariance structure more precisely, and have a significantly smallerreconstruction error. We test RFNs as pretraining technique for deep networkson different vision datasets, where RFNs were superior to RBMs andautoencoders. On gene expression data from two pharmaceutical drug discoverystudies, RFNs detected small and rare gene modules that revealed highlyrelevant new biological insights which were so far missed by other unsupervisedmethods.
arxiv-11100-280 | Pose-Invariant 3D Face Alignment | http://arxiv.org/pdf/1506.03799v1.pdf | author:Amin Jourabloo, Xiaoming Liu category:cs.CV published:2015-06-11 summary:Face alignment aims to estimate the locations of a set of landmarks for agiven image. This problem has received much attention as evidenced by therecent advancement in both the methodology and performance. However, most ofthe existing works neither explicitly handle face images with arbitrary poses,nor perform large-scale experiments on non-frontal and profile face images. Inorder to address these limitations, this paper proposes a novel face alignmentalgorithm that estimates both 2D and 3D landmarks and their 2D visibilities fora face image with an arbitrary pose. By integrating a 3D deformable model, acascaded coupled-regressor approach is designed to estimate both the cameraprojection matrix and the 3D landmarks. Furthermore, the 3D model also allowsus to automatically estimate the 2D landmark visibilities via surface normals.We gather a substantially larger collection of all-pose face images to evaluateour algorithm and demonstrate superior performances than the state-of-the-artmethods.
arxiv-11100-281 | Parallelizing LDA using Partially Collapsed Gibbs Sampling | http://arxiv.org/pdf/1506.03784v1.pdf | author:Måns Magnusson, Leif Jonsson, Mattias Villani, David Broman category:stat.ML stat.ME published:2015-06-11 summary:Latent dirichlet allocation (LDA) is a model widely used for unsupervisedprobabilistic modeling of text and images. MCMC sampling from the posteriordistribution is typically performed using a collapsed Gibbs sampler thatintegrates out all model parameters except the topic indicators for each word.The topic indicators are Gibbs sampled iteratively by drawing each topic fromits conditional posterior. The popularity of this sampler stems from itsbalanced combination of simplicity and efficiency, but its inherentlysequential nature is an obstacle for parallel implementations. Growing corpussizes and increasing model complexity are making inference in LDA modelscomputationally infeasible without parallel sampling. We propose a parallelimplementation of LDA that only collapses over the topic proportions in eachdocument and therefore allows independent sampling of the topic indicators indifferent documents. We develop several modifications of the basic algorithmthat exploits sparsity and structure to further improve the performance of thepartially collapsed sampler. Contrary to other parallel LDA implementations,the partially collapsed sampler guarantees convergence to the true posterior.We show on several well-known corpora that the expected increase in statisticalinefficiency from only partial collapsing is smaller than commonly assumed, andcan be more than compensated by the speed-up from parallelization for largercorpora.
arxiv-11100-282 | Entity-Specific Sentiment Classification of Yahoo News Comments | http://arxiv.org/pdf/1506.03775v1.pdf | author:Prakhar Biyani, Cornelia Caragea, Narayan Bhamidipati category:cs.CL cs.IR cs.SI published:2015-06-11 summary:Sentiment classification is widely used for product reviews and in onlinesocial media such as forums, Twitter, and blogs. However, the problem ofclassifying the sentiment of user comments on news sites has not been addressedyet. News sites cover a wide range of domains including politics, sports,technology, and entertainment, in contrast to other online social sites such asforums and review sites, which are specific to a particular domain. A userassociated with a news site is likely to post comments on diverse topics (e.g.,politics, smartphones, and sports) or diverse entities (e.g., Obama, iPhone, orGoogle). Classifying the sentiment of users tied to various entities may helpobtain a holistic view of their personality, which could be useful inapplications such as online advertising, content personalization, and politicalcampaign planning. In this paper, we formulate the problem of entity-specificsentiment classification of comments posted on news articles in Yahoo News andpropose novel features that are specific to news comments. Experimental resultsshow that our models outperform state-of-the-art baselines.
arxiv-11100-283 | Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic Gaussian Process | http://arxiv.org/pdf/1506.03768v1.pdf | author:Ye Wang, David B. Dunson category:stat.ML published:2015-06-11 summary:Learning of low dimensional structure in multidimensional data is a canonicalproblem in machine learning. One common approach is to suppose that theobserved data are close to a lower-dimensional smooth manifold. There are arich variety of manifold learning methods available, which allow mapping ofdata points to the manifold. However, there is a clear lack of probabilisticmethods that allow learning of the manifold along with the generativedistribution of the observed data. The best attempt is the Gaussian processlatent variable model (GP-LVM), but identifiability issues lead to poorperformance. We solve these issues by proposing a novel Coulomb repulsiveprocess (Corp) for locations of points on the manifold, inspired by physicalmodels of electrostatic interactions among particles. Combining this processwith a GP prior for the mapping function yields a novel electrostatic GP(electroGP) process. Focusing on the simple case of a one-dimensional manifold,we develop efficient inference algorithms, and illustrate substantiallyimproved performance in a variety of experiments including filling in missingframes in video.
arxiv-11100-284 | Spectral Representations for Convolutional Neural Networks | http://arxiv.org/pdf/1506.03767v1.pdf | author:Oren Rippel, Jasper Snoek, Ryan P. Adams category:stat.ML cs.LG published:2015-06-11 summary:Discrete Fourier transforms provide a significant speedup in the computationof convolutions in deep learning. In this work, we demonstrate that, beyond itsadvantages for efficient computation, the spectral domain also provides apowerful representation in which to model and train convolutional neuralnetworks (CNNs). We employ spectral representations to introduce a number of innovations toCNN design. First, we propose spectral pooling, which performs dimensionalityreduction by truncating the representation in the frequency domain. Thisapproach preserves considerably more information per parameter than otherpooling strategies and enables flexibility in the choice of pooling outputdimensionality. This representation also enables a new form of stochasticregularization by randomized modification of resolution. We show that thesemethods achieve competitive results on classification and approximation tasks,without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectralparameterization of convolutional filters. While this leaves the underlyingmodel unchanged, it results in a representation that greatly facilitatesoptimization. We observe on a variety of popular CNN configurations that thisleads to significantly faster convergence during training.
arxiv-11100-285 | Recovering communities in the general stochastic block model without knowing the parameters | http://arxiv.org/pdf/1506.03729v1.pdf | author:Emmanuel Abbe, Colin Sandon category:math.PR cs.IT cs.LG cs.SI math.IT published:2015-06-11 summary:Most recent developments on the stochastic block model (SBM) rely on theknowledge of the model parameters, or at least on the number of communities.This paper introduces efficient algorithms that do not require such knowledgeand yet achieve the optimal information-theoretic tradeoffs identified in[AS15] for linear size communities. The results are three-fold: (i) in theconstant degree regime, an algorithm is developed that requires only alower-bound on the relative sizes of the communities and detects communitieswith an optimal accuracy scaling for large degrees; (ii) in the regime wheredegrees are scaled by $\omega(1)$ (diverging degrees), this is enhanced into afully agnostic algorithm that only takes the graph in question andsimultaneously learns the model parameters (including the number ofcommunities) and detects communities with accuracy $1-o(1)$, with an overallquasi-linear complexity; (iii) in the logarithmic degree regime, an agnosticalgorithm is developed that learns the parameters and achieves the optimalCH-limit for exact recovery, in quasi-linear time. These provide the firstalgorithms affording efficiency, universality and information-theoreticoptimality for strong and weak consistency in the general SBM with linear sizecommunities.
arxiv-11100-286 | Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN) | http://arxiv.org/pdf/1412.6632v5.pdf | author:Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille category:cs.CV cs.CL cs.LG published:2014-12-20 summary:In this paper, we present a multimodal Recurrent Neural Network (m-RNN) modelfor generating novel image captions. It directly models the probabilitydistribution of generating a word given previous words and an image. Imagecaptions are generated by sampling from this distribution. The model consistsof two sub-networks: a deep recurrent neural network for sentences and a deepconvolutional network for images. These two sub-networks interact with eachother in a multimodal layer to form the whole m-RNN model. The effectiveness ofour model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K,Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. Inaddition, we apply the m-RNN model to retrieval tasks for retrieving images orsentences, and achieves significant performance improvement over thestate-of-the-art methods which directly optimize the ranking objective functionfor retrieval. The project page of this work is:www.stat.ucla.edu/~junhua.mao/m-RNN.html .
arxiv-11100-287 | On the Invariance of Dictionary Learning and Sparse Representation to Projecting Data to a Discriminative Space | http://arxiv.org/pdf/1503.02041v2.pdf | author:Mehrdad J. Gangeh, Ali Ghodsi category:cs.CV published:2015-03-06 summary:In this paper, it is proved that dictionary learning and sparserepresentation is invariant to a linear transformation. It subsumes the specialcase of transforming/projecting the data into a discriminative space. This isimportant because recently, supervised dictionary learning algorithms have beenproposed, which suggest to include the category information into the learningof dictionary to improve its discriminative power. Among them, there are someapproaches that propose to learn the dictionary in a discriminative projectedspace. To this end, two approaches have been proposed: first, assigning thediscriminative basis as the dictionary and second, perform dictionary learningin the projected space. Based on the invariance of dictionary learning to anytransformation in general, and to a discriminative space in particular, weadvocate the first approach.
arxiv-11100-288 | Margin-Based Feed-Forward Neural Network Classifiers | http://arxiv.org/pdf/1506.03626v1.pdf | author:Han Xiao, Xiaoyan Zhu category:cs.LG published:2015-06-11 summary:Margin-Based Principle has been proposed for a long time, it has been provedthat this principle could reduce the structural risk and improve theperformance in both theoretical and practical aspects. Meanwhile, feed-forwardneural network is a traditional classifier, which is very hot at present with adeeper architecture. However, the training algorithm of feed-forward neuralnetwork is developed and generated from Widrow-Hoff Principle that means tominimize the squared error. In this paper, we propose a new training algorithmfor feed-forward neural networks based on Margin-Based Principle, which couldeffectively promote the accuracy and generalization ability of neural networkclassifiers with less labelled samples and flexible network. We have conductedexperiments on four UCI open datasets and achieved good results as expected. Inconclusion, our model could handle more sparse labelled and more high-dimensiondataset in a high accuracy while modification from old ANN method to our methodis easy and almost free of work.
arxiv-11100-289 | Max-Entropy Feed-Forward Clustering Neural Network | http://arxiv.org/pdf/1506.03623v1.pdf | author:Han Xiao, Xiaoyan Zhu category:cs.LG published:2015-06-11 summary:The outputs of non-linear feed-forward neural network are positive, whichcould be treated as probability when they are normalized to one. If we takeEntropy-Based Principle into consideration, the outputs for each sample couldbe represented as the distribution of this sample for different clusters.Entropy-Based Principle is the principle with which we could estimate theunknown distribution under some limited conditions. As this paper defines twoprocesses in Feed-Forward Neural Network, our limited condition is theabstracted features of samples which are worked out in the abstraction process.And the final outputs are the probability distribution for different clustersin the clustering process. As Entropy-Based Principle is considered into thefeed-forward neural network, a clustering method is born. We have conductedsome experiments on six open UCI datasets, comparing with a few baselines andapplied purity as the measurement . The results illustrate that our methodoutperforms all the other baselines that are most popular clustering methods.
arxiv-11100-290 | Distributed Recurrent Neural Forward Models with Synaptic Adaptation for Complex Behaviors of Walking Robots | http://arxiv.org/pdf/1506.03599v1.pdf | author:Sakyasingha Dasgupta, Dennis Goldschmidt, Florentin Wörgötter, Poramate Manoonpong category:cs.NE cs.RO q-bio.NC published:2015-06-11 summary:Walking animals, like stick insects, cockroaches or ants, demonstrate afascinating range of locomotive abilities and complex behaviors. The locomotivebehaviors can consist of a variety of walking patterns along with adaptationthat allow the animals to deal with changes in environmental conditions, likeuneven terrains, gaps, obstacles etc. Biological study has revealed that suchcomplex behaviors are a result of a combination of biome- chanics and neuralmechanism thus representing the true nature of embodied interactions. While thebiomechanics helps maintain flexibility and sustain a variety of movements, theneural mechanisms generate movements while making appropriate predictionscrucial for achieving adaptation. Such predictions or planning ahead can beachieved by way of in- ternal models that are grounded in the overall behaviorof the animal. Inspired by these findings, we present here, an artificialbio-inspired walking system which effectively com- bines biomechanics (in termsof the body and leg structures) with the underlying neural mechanisms. Theneural mechanisms consist of 1) central pattern generator based control forgenerating basic rhythmic patterns and coordinated movements, 2) distributed(at each leg) recurrent neural network based adaptive forward models withefference copies as internal models for sensory predictions and instantaneousstate estimations, and 3) searching and elevation control for adapting themovement of an individual leg to deal with different environmental conditions.Using simulations we show that this bio-inspired approach with adaptiveinternal models allows the walking robot to perform complex loco- motivebehaviors as observed in insects, including walking on undulated terrains,crossing large gaps as well as climbing over high obstacles...
arxiv-11100-291 | Modeling Order in Neural Word Embeddings at Scale | http://arxiv.org/pdf/1506.02338v3.pdf | author:Andrew Trask, David Gilmore, Matthew Russell category:cs.CL published:2015-06-08 summary:Natural Language Processing (NLP) systems commonly leverage bag-of-wordsco-occurrence techniques to capture semantic and syntactic word relationships.The resulting word-level distributed representations often ignore morphologicalinformation, though character-level embeddings have proven valuable to NLPtasks. We propose a new neural language model incorporating both word order andcharacter order in its embedding. The model produces several vector spaces withmeaningful substructure, as evidenced by its performance of 85.8% on a recentword-analogy task, exceeding best published syntactic word-analogy scores by a58% error margin. Furthermore, the model includes several parallel trainingmethods, most notably allowing a skip-gram network with 160 billion parametersto be trained overnight on 3 multi-core CPUs, 14x larger than the previouslargest neural network.
arxiv-11100-292 | BoWFire: Detection of Fire in Still Images by Integrating Pixel Color and Texture Analysis | http://arxiv.org/pdf/1506.03495v1.pdf | author:Daniel Y. T. Chino, Letricia P. S. Avalhais, Jose F. Rodrigues Jr., Agma J. M. Traina category:cs.CV published:2015-06-10 summary:Emergency events involving fire are potentially harmful, demanding a fast andprecise decision making. The use of crowdsourcing image and videos on crisismanagement systems can aid in these situations by providing more informationthan verbal/textual descriptions. Due to the usual high volume of data,automatic solutions need to discard non-relevant content without losingrelevant information. There are several methods for fire detection on videousing color-based models. However, they are not adequate for still imageprocessing, because they can suffer on high false-positive results. Thesemethods also suffer from parameters with little physical meaning, which makesfine tuning a difficult task. In this context, we propose a novel firedetection method for still images that uses classification based on colorfeatures combined with texture classification on superpixel regions. Our methoduses a reduced number of parameters if compared to previous works, easing theprocess of fine tuning the method. Results show the effectiveness of our methodof reducing false-positives while its precision remains compatible with thestate-of-the-art methods.
arxiv-11100-293 | Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts | http://arxiv.org/pdf/1506.03493v1.pdf | author:Aaron Schein, John Paisley, David M. Blei, Hanna Wallach category:stat.ML cs.AI cs.LG cs.SI stat.AP published:2015-06-10 summary:We present a Bayesian tensor factorization model for inferring latent groupstructures from dynamic pairwise interaction patterns. For decades, politicalscientists have collected and analyzed records of the form "country $i$ tookaction $a$ toward country $j$ at time $t$"---known as dyadic events---in orderto form and test theories of international relations. We represent these eventdata as a tensor of counts and develop Bayesian Poisson tensor factorization toinfer a low-dimensional, interpretable representation of their salientpatterns. We demonstrate that our model's predictive performance is better thanthat of standard non-negative tensor factorization methods. We also provide acomparison of our variational updates to their maximum likelihood counterparts.In doing so, we identify a better way to form point estimates of the latentfactors than that typically used in Bayesian Poisson matrix factorization.Finally, we showcase our model as an exploratory analysis tool for politicalscientists. We show that the inferred latent factor matrices captureinterpretable multilateral relations that both conform to and inform ourknowledge of international affairs.
arxiv-11100-294 | Truthful Linear Regression | http://arxiv.org/pdf/1506.03489v1.pdf | author:Rachel Cummings, Stratis Ioannidis, Katrina Ligett category:cs.GT cs.DS stat.ML published:2015-06-10 summary:We consider the problem of fitting a linear model to data held by individualswho are concerned about their privacy. Incentivizing most players to truthfullyreport their data to the analyst constrains our design to mechanisms thatprovide a privacy guarantee to the participants; we use differential privacy tomodel individuals' privacy losses. This immediately poses a problem, asdifferentially private computation of a linear model necessarily produces abiased estimation, and existing approaches to design mechanisms to elicit datafrom privacy-sensitive individuals do not generalize well to biased estimators.We overcome this challenge through an appropriate design of the computation andpayment scheme.
arxiv-11100-295 | Spectral Detection in the Censored Block Model | http://arxiv.org/pdf/1502.00163v2.pdf | author:Alaa Saade, Florent Krzakala, Marc Lelarge, Lenka Zdeborová category:cs.SI cs.LG math.PR published:2015-01-31 summary:We consider the problem of partially recovering hidden binary variables fromthe observation of (few) censored edge weights, a problem with applications incommunity detection, correlation clustering and synchronization. We describetwo spectral algorithms for this task based on the non-backtracking and theBethe Hessian operators. These algorithms are shown to be asymptoticallyoptimal for the partial recovery problem, in that they detect the hiddenassignment as soon as it is information theoretically possible to do so.
arxiv-11100-296 | Image Tag Completion and Refinement by Subspace Clustering and Matrix Completion | http://arxiv.org/pdf/1506.03475v1.pdf | author:Yuqing Hou, Zhouchen Lin category:cs.CV published:2015-06-10 summary:Tag-based image retrieval (TBIR) has drawn much attention in recent years dueto the explosive amount of digital images and crowdsourcing tags. However, theTBIR applications still suffer from the deficient and inaccurate tags providedby users. Inspired by the subspace clustering methods, we formulate the tagcompletion problem in a subspace clustering model which assumes that images aresampled from subspaces, and complete the tags using the state-of-the-art LowRank Representation (LRR) method. And we propose a matrix completion algorithmto further refine the tags. Our empirical results on multiple benchmarkdatasets for image annotation show that the proposed algorithm outperformsstate-of-the-art approaches when handling missing and noisy tags.
arxiv-11100-297 | Fast Online Clustering with Randomized Skeleton Sets | http://arxiv.org/pdf/1506.03425v1.pdf | author:Krzysztof Choromanski, Sanjiv Kumar, Xiaofeng Liu category:cs.AI cs.LG published:2015-06-10 summary:We present a new fast online clustering algorithm that reliably recoversarbitrary-shaped data clusters in high throughout data streams. Unlike theexisting state-of-the-art online clustering methods based on k-means ork-medoid, it does not make any restrictive generative assumptions. In addition,in contrast to existing nonparametric clustering techniques such as DBScan orDenStream, it gives provable theoretical guarantees. To achieve fastclustering, we propose to represent each cluster by a skeleton set which isupdated continuously as new data is seen. A skeleton set consists of weightedsamples from the data where weights encode local densities. The size of eachskeleton set is adapted according to the cluster geometry. The proposedtechnique automatically detects the number of clusters and is robust tooutliers. The algorithm works for the infinite data stream where more than onepass over the data is not feasible. We provide theoretical guarantees on thequality of the clustering and also demonstrate its advantage over the existingstate-of-the-art on several datasets.
arxiv-11100-298 | piCholesky: Polynomial Interpolation of Multiple Cholesky Factors for Efficient Approximate Cross-Validation | http://arxiv.org/pdf/1404.0466v2.pdf | author:Da Kuang, Alex Gittens, Raffay Hamid category:cs.LG cs.NA published:2014-04-02 summary:The dominant cost in solving least-square problems using Newton's method isoften that of factorizing the Hessian matrix over multiple values of theregularization parameter ($\lambda$). We propose an efficient way tointerpolate the Cholesky factors of the Hessian matrix computed over a smallset of $\lambda$ values. This approximation enables us to optimally minimizethe hold-out error while incurring only a fraction of the cost compared toexact cross-validation. We provide a formal error bound for our approximationscheme and present solutions to a set of key implementation challenges thatallow our approach to maximally exploit the compute power of modernarchitectures. We present a thorough empirical analysis over multiple datasetsto show the effectiveness of our approach.
arxiv-11100-299 | Optimal Rates of Convergence for Noisy Sparse Phase Retrieval via Thresholded Wirtinger Flow | http://arxiv.org/pdf/1506.03382v1.pdf | author:T. Tony Cai, Xiaodong Li, Zongming Ma category:math.ST cs.IT math.IT math.NA stat.ML stat.TH published:2015-06-10 summary:This paper considers the noisy sparse phase retrieval problem: recovering asparse signal $x \in \mathbb{R}^p$ from noisy quadratic measurements $y_j =(a_j' x )^2 + \epsilon_j$, $j=1, \ldots, m$, with independent sub-exponentialnoise $\epsilon_j$. The goals are to understand the effect of the sparsity of$x$ on the estimation precision and to construct a computationally feasibleestimator to achieve the optimal rates. Inspired by the Wirtinger Flow [12]proposed for noiseless and non-sparse phase retrieval, a novel thresholdedgradient descent algorithm is proposed and it is shown to adaptively achievethe minimax optimal rates of convergence over a wide range of sparsity levelswhen the $a_j$'s are independent standard Gaussian random vectors, providedthat the sample size is sufficiently large compared to the sparsity of $x$.
arxiv-11100-300 | Accelerated Stochastic Gradient Descent for Minimizing Finite Sums | http://arxiv.org/pdf/1506.03016v2.pdf | author:Atsushi Nitanda category:stat.ML cs.LG published:2015-06-09 summary:We propose an optimization method for minimizing the finite sums of smoothconvex functions. Our method incorporates an accelerated gradient descent (AGD)and a stochastic variance reduction gradient (SVRG) in a mini-batch setting.Unlike SVRG, our method can be directly applied to non-strongly and stronglyconvex problems. We show that our method achieves a lower overall complexitythan the recently proposed methods that supports non-strongly convex problems.Moreover, this method has a fast rate of convergence for strongly convexproblems. Our experiments show the effectiveness of our method.
