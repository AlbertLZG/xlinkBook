arxiv-11100-1 | Improved repeatability measures for evaluating performance of feature detectors | http://arxiv.org/abs/1504.07967 | author:Shoaib Ehsan, Nadia Kanwal, Adrian F. Clark, Klaus D. McDonald-Maier category:cs.CV cs.PF published:2015-04-29 summary:The most frequently employed measure for performance characterisation oflocal feature detectors is repeatability, but it has been observed that thisdoes not necessarily mirror actual performance. Presented are improvedrepeatability formulations which correlate much better with the trueperformance of feature detectors. Comparative results for severalstate-of-the-art feature detectors are presented using these measures; it isfound that Hessian-based detectors are generally superior at identifyingfeatures when images are subject to various geometric and photometrictransformations.
arxiv-11100-2 | Learning Contextualized Music Semantics from Tags via a Siamese Network | http://arxiv.org/abs/1504.07968 | author:Ubai Sandouk, Ke Chen category:cs.LG I.2.6 published:2015-04-29 summary:Automatic annotation of music with tags is a promising methodology for theacquisition of semantics that facilitates music information retrieval andunderstanding. One of the biggest challenges for this methodology is modelingconcept semantics in context. Moreover, the out of vocabulary (OOV) problemexacerbates its difficulty and has yet to be addressed so far. In this paper,we propose a novel Siamese network to fight off the challenge. By means of tagfeatures and a probabilistic topic model, our Siamese network capturescontextualized music semantics from tags via unsupervised learning, which leadsto a contextualized music semantic space and a potential solution to the OOV.We have conducted simulations on two public tag collections, CAL500 andMagTag5K, and compared our approach to a number of the state-of-the-artmethods. Comparative results suggest that our approach outperforms thestate-of-the-art methods in terms of semantic priming measures.
arxiv-11100-3 | Can Machines Truly Think | http://arxiv.org/abs/1504.07571 | author:Murat Okandan category:cs.AI cs.NE published:2015-04-28 summary:Can machines truly think? This question and its answer have many implicationsthat depend, in large part, on any number of assumptions underlying how theissue has been addressed or considered previously. A crucial question, and onethat is almost taken for granted, is the starting point for this discussion:Can "thought" be achieved or emulated by algorithmic procedures?
arxiv-11100-4 | Differentially Private Release and Learning of Threshold Functions | http://arxiv.org/abs/1504.07553 | author:Mark Bun, Kobbi Nissim, Uri Stemmer, Salil Vadhan category:cs.CR cs.LG published:2015-04-28 summary:We prove new upper and lower bounds on the sample complexity of $(\epsilon,\delta)$ differentially private algorithms for releasing approximate answers tothreshold functions. A threshold function $c_x$ over a totally ordered domain$X$ evaluates to $c_x(y) = 1$ if $y \le x$, and evaluates to $0$ otherwise. Wegive the first nontrivial lower bound for releasing thresholds with$(\epsilon,\delta)$ differential privacy, showing that the task is impossibleover an infinite domain $X$, and moreover requires sample complexity $n \ge\Omega(\log^*X)$, which grows with the size of the domain. Inspired by thetechniques used to prove this lower bound, we give an algorithm for releasingthresholds with $n \le 2^{(1+ o(1))\log^*X}$ samples. This improves theprevious best upper bound of $8^{(1 + o(1))\log^*X}$ (Beimel et al., RANDOM'13). Our sample complexity upper and lower bounds also apply to the tasks oflearning distributions with respect to Kolmogorov distance and of properly PAClearning thresholds with differential privacy. The lower bound gives the firstseparation between the sample complexity of properly learning a concept classwith $(\epsilon,\delta)$ differential privacy and learning without privacy. Forproperly learning thresholds in $\ell$ dimensions, this lower bound extends to$n \ge \Omega(\ell \cdot \log^*X)$. To obtain our results, we give reductions in both directions from releasingand properly learning thresholds and the simpler interior point problem. Givena database $D$ of elements from $X$, the interior point problem asks for anelement between the smallest and largest elements in $D$. We introduce newrecursive constructions for bounding the sample complexity of the interiorpoint problem, as well as further reductions and techniques for provingimpossibility results for other basic problems in differential privacy.
arxiv-11100-5 | Becoming the Expert - Interactive Multi-Class Machine Teaching | http://arxiv.org/abs/1504.07575 | author:Edward Johns, Oisin Mac Aodha, Gabriel J. Brostow category:cs.CV cs.LG stat.ML published:2015-04-28 summary:Compared to machines, humans are extremely good at classifying images intocategories, especially when they possess prior knowledge of the categories athand. If this prior information is not available, supervision in the form ofteaching images is required. To learn categories more quickly, people shouldsee important and representative images first, followed by less importantimages later - or not at all. However, image-importance is individual-specific,i.e. a teaching image is important to a student if it changes their overallability to discriminate between classes. Further, students keep learning, sowhile image-importance depends on their current knowledge, it also varies withtime. In this work we propose an Interactive Machine Teaching algorithm thatenables a computer to teach challenging visual concepts to a human. Ouradaptive algorithm chooses, online, which labeled images from a teaching setshould be shown to the student as they learn. We show that a teaching strategythat probabilistically models the student's ability and progress, based ontheir correct and incorrect answers, produces better 'experts'. We presentresults using real human participants across several varied and challengingreal-world datasets.
arxiv-11100-6 | Non-Gaussian Discriminative Factor Models via the Max-Margin Rank-Likelihood | http://arxiv.org/abs/1504.07468 | author:Xin Yuan, Ricardo Henao, Ephraim L. Tsalik, Raymond J. Langley, Lawrence Carin category:stat.ML published:2015-04-28 summary:We consider the problem of discriminative factor analysis for data that arein general non-Gaussian. A Bayesian model based on the ranks of the data isproposed. We first introduce a new {\em max-margin} version of therank-likelihood. A discriminative factor model is then developed, integratingthe max-margin rank-likelihood and (linear) Bayesian support vector machines,which are also built on the max-margin principle. The discriminative factormodel is further extended to the {\em nonlinear} case through mixtures of locallinear classifiers, via Dirichlet processes. Fully local conjugacy of the modelyields efficient inference with both Markov Chain Monte Carlo and variationalBayes approaches. Extensive experiments on benchmark and real data demonstratesuperior performance of the proposed model and its potential for applicationsin computational biology.
arxiv-11100-7 | Speeding Up Neural Networks for Large Scale Classification using WTA Hashing | http://arxiv.org/abs/1504.07488 | author:Amir H. Bakhtiary, Agata Lapedriza, David Masip category:cs.CV published:2015-04-28 summary:In this paper we propose to use the Winner Takes All hashing technique tospeed up forward propagation and backward propagation in fully connected layersin convolutional neural networks. The proposed technique reduces significantlythe computational complexity, which in turn, allows us to train layers with alarge number of kernels with out the associated time penalty. As a consequence we are able to train convolutional neural network on a verylarge number of output classes with only a small increase in the computationalcost. To show the effectiveness of the technique we train a new output layer ona pretrained network using both the regular multiplicative approach and ourproposed hashing methodology. Our results showed no drop in performance anddemonstrate, with our implementation, a 7 fold speed up during the training.
arxiv-11100-8 | A Robust Lane Detection and Departure Warning System | http://arxiv.org/abs/1504.07590 | author:Mrinal Haloi, Dinesh Babu Jayagopi category:cs.CV 68T45 published:2015-04-28 summary:In this work, we have developed a robust lane detection and departure warningtechnique. Our system is based on single camera sensor. For lane detection amodified Inverse Perspective Mapping using only a few extrinsic cameraparameters and illuminant Invariant techniques is used. Lane markings arerepresented using a combination of 2nd and 4th order steerable filters, robustto shadowing. Effect of shadowing and extra sun light are removed using Labcolor space, and illuminant invariant representation. Lanes are assumed to becubic curves and fitted using robust RANSAC. This method can reliably detectlanes of the road and its boundary. This method has been experimented in Indianroad conditions under different challenging situations and the result obtainedwere very good. For lane departure angle an optical flow based method wereused.
arxiv-11100-9 | Identifying Reliable Annotations for Large Scale Image Segmentation | http://arxiv.org/abs/1504.07460 | author:Alexander Kolesnikov, Christoph H. Lampert category:cs.CV published:2015-04-28 summary:Challenging computer vision tasks, in particular semantic image segmentation,require large training sets of annotated images. While obtaining the actualimages is often unproblematic, creating the necessary annotation is a tediousand costly process. Therefore, one often has to work with unreliable annotationsources, such as Amazon Mechanical Turk or (semi-)automatic algorithmictechniques. In this work, we present a Gaussian process (GP) based techniquefor simultaneously identifying which images of a training set have unreliableannotation and learning a segmentation model in which the negative effect ofthese images is suppressed. Alternatively, the model can also just be used toidentify the most reliably annotated images from the training set, which canthen be used for training any other segmentation method. By relying on "deepfeatures" in combination with a linear covariance function, our GP can belearned and its hyperparameter determined efficiently using only matrixoperations and gradient-based optimization. This makes our method scalable evento large datasets with several million training instances.
arxiv-11100-10 | CommentWatcher: An Open Source Web-based platform for analyzing discussions on web forums | http://arxiv.org/abs/1504.07459 | author:Marian-Andrei Rizoiu, Adrien Guille, Julien Velcin category:cs.CL cs.SI published:2015-04-28 summary:We present CommentWatcher, an open source tool aimed at analyzing discussionson web forums. Constructed as a web platform, CommentWatcher features automaticmass fetching of user posts from forum on multiple sites, extracting topics,visualizing the topics as an expression cloud and exploring their temporalevolution. The underlying social network of users is simultaneously constructedusing the citation relations between users and visualized as a graph structure.Our platform addresses the issues of the diversity and dynamics of structuresof webpages hosting the forums by implementing a parser architecture that isindependent of the HTML structure of webpages. This allows easy on-the-flyadding of new websites. Two types of users are targeted: end users who seek tostudy the discussed topics and their temporal evolution, and researchers inneed of establishing a forum benchmark dataset and comparing the performancesof analysis tools.
arxiv-11100-11 | Embedded Platforms for Computer Vision-based Advanced Driver Assistance Systems: a Survey | http://arxiv.org/abs/1504.07442 | author:Gorka Velez, Oihana Otaegui category:cs.CV published:2015-04-28 summary:Computer Vision, either alone or combined with other technologies such asradar or Lidar, is one of the key technologies used in Advanced DriverAssistance Systems (ADAS). Its role understanding and analysing the drivingscene is of great importance as it can be noted by the number of ADASapplications that use this technology. However, porting a vision algorithm toan embedded automotive system is still very challenging, as there must be atrade-off between several design requisites. Furthermore, there is not astandard implementation platform, so different alternatives have been proposedby both the scientific community and the industry. This paper aims to reviewthe requisites and the different embedded implementation platforms that can beused for Computer Vision-based ADAS, with a critical analysis and an outlook tofuture trends.
arxiv-11100-12 | Compact CNN for Indexing Egocentric Videos | http://arxiv.org/abs/1504.07469 | author:Yair Poleg, Ariel Ephrat, Shmuel Peleg, Chetan Arora category:cs.CV published:2015-04-28 summary:While egocentric video is becoming increasingly popular, browsing it is verydifficult. In this paper we present a compact 3D Convolutional Neural Network(CNN) architecture for long-term activity recognition in egocentric videos.Recognizing long-term activities enables us to temporally segment (index) longand unstructured egocentric videos. Existing methods for this task are based onhand tuned features derived from visible objects, location of hands, as well asoptical flow. Given a sparse optical flow volume as input, our CNN classifies the camerawearer's activity. We obtain classification accuracy of 89%, which outperformsthe current state-of-the-art by 19%. Additional evaluation is performed on anextended egocentric video dataset, classifying twice the amount of categoriesthan current state-of-the-art. Furthermore, our CNN is able to recognizewhether a video is egocentric or not with 99.2% accuracy, up by 24% fromcurrent state-of-the-art. To better understand what the network actuallylearns, we propose a novel visualization of CNN kernels as flow fields.
arxiv-11100-13 | Lexical Translation Model Using a Deep Neural Network Architecture | http://arxiv.org/abs/1504.07395 | author:Thanh-Le Ha, Jan Niehues, Alex Waibel category:cs.CL cs.LG cs.NE published:2015-04-28 summary:In this paper we combine the advantages of a model using global sourcesentence contexts, the Discriminative Word Lexicon, and neural networks. Byusing deep neural networks instead of the linear maximum entropy model in theDiscriminative Word Lexicon models, we are able to leverage dependenciesbetween different source words due to the non-linearity. Furthermore, themodels for different target words can share parameters and therefore datasparsity problems are effectively reduced. By using this approach in a state-of-the-art translation system, we canimprove the performance by up to 0.5 BLEU points for three different languagepairs on the TED translation task.
arxiv-11100-14 | Building Classifiers to Predict the Start of Glucose-Lowering Pharmacotherapy Using Belgian Health Expenditure Data | http://arxiv.org/abs/1504.07389 | author:Marc Claesen, Frank De Smet, Pieter Gillard, Chantal Mathieu, Bart De Moor category:stat.ML cs.IR I.5.4; J.3 published:2015-04-28 summary:Early diagnosis is important for type 2 diabetes (T2D) to improve patientprognosis, prevent complications and reduce long-term treatment costs. Wepresent a novel risk profiling approach based exclusively on health expendituredata that is available to Belgian mutual health insurers. We used expendituredata related to drug purchases and medical provisions to construct models thatpredict whether a patient will start glucose-lowering pharmacotherapy in thecoming years, based on that patient's recent medical expenditure history. Thedesign and implementation of the modeling strategy are discussed in detail andseveral learning methods are benchmarked for our application. Our bestperforming model obtains between 74.9% and 76.8% area under the ROC curve,which is comparable to state-of-the-art risk prediction approaches for T2Dbased on questionnaires. In contrast to other methods, our approach can beimplemented on a population-wide scale at virtually no extra operational cost.Possibly, our approach can be further improved by additional information aboutsome risk factors of T2D that is unavailable in health expenditure data.
arxiv-11100-15 | Combined A*-Ants Algorithm: A New Multi-Parameter Vehicle Navigation Scheme | http://arxiv.org/abs/1504.07329 | author:Hojjat Salehinejad, Hossein Nezamabadi-pour, Saeid Saryazdi, Fereydoun Farrahi-Moghaddam category:cs.NE published:2015-04-28 summary:In this paper a multi-parameter A*(A- star)-ants based algorithm is proposedin order to find the best optimized multi-parameter path between two desiredpoints in regions. This algorithm recognizes paths, according to user desiredparameters using electronic maps. The proposed algorithm is a combination of A*and ants algorithm in which the proposed A* algorithm is the prologue to thesuggested ant based algorithm .In fact, this A* algorithm invigorates somepaths pheromones in ants algorithm. As one of implementations of this method,this algorithm was applied on a part of Kerman city, Iran as a multi-parametervehicle navigator. It finds the best optimized multi-parameter directionbetween two desired junctions based on city traveler parameters. Comparisonresults between the proposed method and ants algorithm demonstrates efficiencyand lower cost function results of the proposed method versus ants algorithm.
arxiv-11100-16 | Convolutional Channel Features | http://arxiv.org/abs/1504.07339 | author:Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li category:cs.CV published:2015-04-28 summary:Deep learning methods are powerful tools but often suffer from expensivecomputation and limited flexibility. An alternative is to combine light-weightmodels with deep representations. As successful cases exist in several visualproblems, a unified framework is absent. In this paper, we revisit two widelyused approaches in computer vision, namely filtered channel features andConvolutional Neural Networks (CNN), and absorb merits from both by proposingan integrated method called Convolutional Channel Features (CCF). CCF transferslow-level features from pre-trained CNN models to feed the boosting forestmodel. With the combination of CNN features and boosting forest, CCF benefitsfrom the richer capacity in feature representation compared with channelfeatures, as well as lower cost in computation and storage compared withend-to-end CNN methods. We show that CCF serves as a good way of tailoringpre-trained CNN models to diverse tasks without fine-tuning the whole networkto each task by achieving state-of-the-art performances in pedestriandetection, face detection, edge detection and object proposal generation.
arxiv-11100-17 | Toward Smart Power Grids: Communication Network Design for Power Grids Synchronization | http://arxiv.org/abs/1504.07327 | author:Hojjat Salehinejad, Farhad Pouladi, Siamak Talebi category:cs.NE published:2015-04-28 summary:In smart power grids, keeping the synchronicity of generators and thecorresponding controls is of great importance. To do so, a simple model isemployed in terms of swing equation to represent the interactions amongdynamics of generators and feedback control. In case of having a communicationnetwork available, the control can be done based on the transmittedmeasurements by the communication network. The stability of system is denotedby the largest eigenvalue of the weighted sum of the Laplacian matrices of thecommunication infrastructure and power network. In this work, we use graphtheory to model the communication network as a graph problem. Then, Ant ColonySystem (ACS) is employed for optimum design of above graph for synchronizationof power grids. Performance evaluation of the proposed method for the 39-busNew England power system versus methods such as exhaustive search and Rayleighquotient approximation indicates feasibility and effectiveness of our methodfor even large scale smart power grids.
arxiv-11100-18 | Reader-Aware Multi-Document Summarization via Sparse Coding | http://arxiv.org/abs/1504.07324 | author:Piji Li, Lidong Bing, Wai Lam, Hang Li, Yi Liao category:cs.CL cs.AI published:2015-04-28 summary:We propose a new MDS paradigm called reader-aware multi-documentsummarization (RA-MDS). Specifically, a set of reader comments associated withthe news reports are also collected. The generated summaries from the reportsfor the event should be salient according to not only the reports but also thereader comments. To tackle this RA-MDS problem, we propose asparse-coding-based method that is able to calculate the salience of the textunits by jointly considering news reports and reader comments. Anotherreader-aware characteristic of our framework is to improve linguistic qualityvia entity rewriting. The rewriting consideration is jointly assessed togetherwith other summarization requirements under a unified optimization model. Tosupport the generation of compressive summaries via optimization, we explore afiner syntactic unit, namely, noun/verb phrase. In this work, we also generatea data set for conducting RA-MDS. Extensive experiments on this data set andsome classical data sets demonstrate the effectiveness of our proposedapproach.
arxiv-11100-19 | Private Disclosure of Information in Health Tele-monitoring | http://arxiv.org/abs/1504.07313 | author:Daniel Aranki, Ruzena Bajcsy category:cs.CR cs.AI cs.IT cs.LG math.IT published:2015-04-28 summary:We present a novel framework, called Private Disclosure of Information (PDI),which is aimed to prevent an adversary from inferring certain sensitiveinformation about subjects using the data that they disclosed duringcommunication with an intended recipient. We show cases where it is possible toachieve perfect privacy regardless of the adversary's auxiliary knowledge whilepreserving full utility of the information to the intended recipient andprovide sufficient conditions for such cases. We also demonstrate theapplicability of PDI on a real-world data set that simulates a healthtele-monitoring scenario.
arxiv-11100-20 | Or's of And's for Interpretable Classification, with Application to Context-Aware Recommender Systems | http://arxiv.org/abs/1504.07614 | author:Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, Perry MacNeille category:cs.LG published:2015-04-28 summary:We present a machine learning algorithm for building classifiers that arecomprised of a small number of disjunctions of conjunctions (or's of and's). Anexample of a classifier of this form is as follows: If X satisfies (x1 = 'blue'AND x3 = 'middle') OR (x1 = 'blue' AND x2 = '<15') OR (x1 = 'yellow'), then wepredict that Y=1, ELSE predict Y=0. An attribute-value pair is called a literaland a conjunction of literals is called a pattern. Models of this form have theadvantage of being interpretable to human experts, since they produce a set ofconditions that concisely describe a specific class. We present twoprobabilistic models for forming a pattern set, one with a Beta-Binomial prior,and the other with Poisson priors. In both cases, there are prior parametersthat the user can set to encourage the model to have a desired size and shape,to conform with a domain-specific definition of interpretability. We providetwo scalable MAP inference approaches: a pattern level search, which involvesassociation rule mining, and a literal level search. We show stronger priorsreduce computation. We apply the Bayesian Or's of And's (BOA) model to predictuser behavior with respect to in-vehicle context-aware personalized recommendersystems.
arxiv-11100-21 | A novel variational model for image registration using Gaussian curvature | http://arxiv.org/abs/1504.07643 | author:Mazlinda Ibrahim, Ke Chen, Carlos Brito-Loeza category:math.NA cs.CV published:2015-04-28 summary:Image registration is one important task in many image processingapplications. It aims to align two or more images so that useful informationcan be extracted through comparison, combination or superposition. This isachieved by constructing an optimal trans- formation which ensures that thetemplate image becomes similar to a given reference image. Although many modelsexist, designing a model capable of modelling large and smooth deformationfield continues to pose a challenge. This paper proposes a novel variationalmodel for image registration using the Gaussian curvature as a regulariser. Themodel is motivated by the surface restoration work in geometric processing[Elsey and Esedoglu, Multiscale Model. Simul., (2009), pp. 1549-1573]. Aneffective numerical solver is provided for the model using an augmentedLagrangian method. Numerical experiments can show that the new modeloutperforms three competing models based on, respectively, a linear curvature[Fischer and Modersitzki, J. Math. Imaging Vis., (2003), pp. 81- 85], the meancurvature [Chumchob, Chen and Brito, Multiscale Model. Simul., (2011), pp.89-128] and the diffeomorphic demon model [Vercauteren at al., NeuroImage,(2009), pp. 61-72] in terms of robustness and accuracy.
arxiv-11100-22 | Nearly Optimal Deterministic Algorithm for Sparse Walsh-Hadamard Transform | http://arxiv.org/abs/1504.07648 | author:Mahdi Cheraghchi, Piotr Indyk category:cs.IT cs.CC cs.LG math.FA math.IT published:2015-04-28 summary:For every fixed constant $\alpha > 0$, we design an algorithm for computingthe $k$-sparse Walsh-Hadamard transform of an $N$-dimensional vector $x \in\mathbb{R}^N$ in time $k^{1+\alpha} (\log N)^{O(1)}$. Specifically, thealgorithm is given query access to $x$ and computes a $k$-sparse $\tilde{x} \in\mathbb{R}^N$ satisfying $\\tilde{x} - \hat{x}\_1 \leq c \\hat{x} -H_k(\hat{x})\_1$, for an absolute constant $c > 0$, where $\hat{x}$ is thetransform of $x$ and $H_k(\hat{x})$ is its best $k$-sparse approximation. Ouralgorithm is fully deterministic and only uses non-adaptive queries to $x$(i.e., all queries are determined and performed in parallel when the algorithmstarts). An important technical tool that we use is a construction of nearly optimaland linear lossless condensers which is a careful instantiation of the GUVcondenser (Guruswami, Umans, Vadhan, JACM 2009). Moreover, we design adeterministic and non-adaptive $\ell_1/\ell_1$ compressed sensing scheme basedon general lossless condensers that is equipped with a fast reconstructionalgorithm running in time $k^{1+\alpha} (\log N)^{O(1)}$ (for the GUV-basedcondenser) and is of independent interest. Our scheme significantly simplifiesand improves an earlier expander-based construction due to Berinde, Gilbert,Indyk, Karloff, Strauss (Allerton 2008). Our methods use linear lossless condensers in a black box fashion; therefore,any future improvement on explicit constructions of such condensers wouldimmediately translate to improved parameters in our framework (potentiallyleading to $k (\log N)^{O(1)}$ reconstruction time with a reduced exponent inthe poly-logarithmic factor, and eliminating the extra parameter $\alpha$). Finally, by allowing the algorithm to use randomness, while still usingnon-adaptive queries, the running time of the algorithm can be improved to$\tilde{O}(k \log^3 N)$.
arxiv-11100-23 | Evaluation of Explore-Exploit Policies in Multi-result Ranking Systems | http://arxiv.org/abs/1504.07662 | author:Dragomir Yankov, Pavel Berkhin, Lihong Li category:cs.LG published:2015-04-28 summary:We analyze the problem of using Explore-Exploit techniques to improveprecision in multi-result ranking systems such as web search, queryautocompletion and news recommendation. Adopting an exploration policy directlyonline, without understanding its impact on the production system, may haveunwanted consequences - the system may sustain large losses, create userdissatisfaction, or collect exploration data which does not help improveranking quality. An offline framework is thus necessary to let us decide whatpolicy and how we should apply in a production environment to ensure positiveoutcome. Here, we describe such an offline framework. Using the framework, we study a popular exploration policy - Thompsonsampling. We show that there are different ways of implementing it inmulti-result ranking systems, each having different semantic interpretation andleading to different results in terms of sustained click-through-rate (CTR)loss and expected model improvement. In particular, we demonstrate thatThompson sampling can act as an online learner optimizing CTR, which in somecases can lead to an interesting outcome: lift in CTR during exploration. Theobservation is important for production systems as it suggests that one can getboth valuable exploration data to improve ranking performance on the long run,and at the same time increase CTR while exploration lasts.
arxiv-11100-24 | Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers | http://arxiv.org/abs/1504.07676 | author:Abraham J. Wyner, Matthew Olson, Justin Bleich, David Mease category:stat.ML cs.LG stat.ME published:2015-04-28 summary:There is a large literature explaining why AdaBoost is a successfulclassifier. The literature on AdaBoost focuses on classifier margins andboosting's interpretation as the optimization of an exponential likelihoodfunction. These existing explanations, however, have been pointed out to beincomplete. A random forest is another popular ensemble method for which thereis substantially less explanation in the literature. We introduce a novelperspective on AdaBoost and random forests that proposes that the twoalgorithms work for similar reasons. While both classifiers achieve similarpredictive accuracy, random forests cannot be conceived as a directoptimization procedure. Rather, random forests is a self-averaging,interpolating algorithm which creates what we denote as a "spikey-smooth"classifier, and we view AdaBoost in the same light. We conjecture that bothAdaBoost and random forests succeed because of this mechanism. We provide anumber of examples and some theoretical justification to support thisexplanation. In the process, we question the conventional wisdom that suggeststhat boosting algorithms for classification require regularization or earlystopping and should be limited to low complexity classes of learners, such asdecision stumps. We conclude that boosting should be used like random forests:with large decision trees and without direct regularization or early stopping.
arxiv-11100-25 | Facial landmark detection using structured output deep neural networks | http://arxiv.org/abs/1504.07550 | author:Soufiane Belharbi, Clement Chatelain, Romain Herault, Sebastien Adam category:cs.LG stat.ML published:2015-04-28 summary:Facial landmark detection is an important step for many perception tasks suchas face recognition and facial analysis. Regression-based methods have shown alarge success. In particular, deep neural networks (DNN) has demonstrated astrong capability to model the high non-linearity between the face image andthe face shape. In this paper, we tackle this task as a structured outputproblem, where we exploit the strong dependencies that lie between the outputs.Beside learning a regression mapping function from the input to the output, welearn, in an unsupervised way, the inter-dependencies between the outputs. Forthis, we propose a generic regression framework for structured output problems.Our framework allows a successful incorporation of learning the outputstructure into DNN using the pre-training trick. We apply our method on afacial landmark detection task, where the output is strongly structured. Weevaluate our DNN, named Input/Output Deep Architecture (IODA), on two publicchallenging datasets: LFPW and HELEN. We show that IODA outperforms traditionaldeep architectures.
arxiv-11100-26 | Leveraging Deep Neural Networks and Knowledge Graphs for Entity Disambiguation | http://arxiv.org/abs/1504.07678 | author:Hongzhao Huang, Larry Heck, Heng Ji category:cs.CL published:2015-04-28 summary:Entity Disambiguation aims to link mentions of ambiguous entities to aknowledge base (e.g., Wikipedia). Modeling topical coherence is crucial forthis task based on the assumption that information from the same semanticcontext tends to belong to the same topic. This paper presents a novel deepsemantic relatedness model (DSRM) based on deep neural networks (DNN) andsemantic knowledge graphs (KGs) to measure entity semantic relatedness fortopical coherence modeling. The DSRM is directly trained on large-scale KGs andit maps heterogeneous types of knowledge of an entity from KGs to numericalfeature vectors in a latent space such that the distance between twosemantically-related entities is minimized. Compared with the state-of-the-artrelatedness approach proposed by (Milne and Witten, 2008a), the DSRM obtains19.4% and 24.5% reductions in entity disambiguation errors on two publiclyavailable datasets respectively.
arxiv-11100-27 | Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons | http://arxiv.org/abs/1504.07218 | author:Yuxin Chen, Changho Suh category:cs.LG cs.DS cs.IT math.IT math.ST stat.ML stat.TH published:2015-04-27 summary:This paper explores the preference-based top-$K$ rank aggregation problem.Suppose that a collection of items is repeatedly compared in pairs, and onewishes to recover a consistent ordering that emphasizes the top-$K$ rankeditems, based on partially revealed preferences. We focus on theBradley-Terry-Luce (BTL) model that postulates a set of latent preferencescores underlying all items, where the odds of paired comparisons depend onlyon the relative scores of the items involved. We characterize the minimax limits on identifiability of top-$K$ rankeditems, in the presence of random and non-adaptive sampling. Our resultshighlight a separation measure that quantifies the gap of preference scoresbetween the $K^{\text{th}}$ and $(K+1)^{\text{th}}$ ranked items. The minimumsample complexity required for reliable top-$K$ ranking scales inversely withthe separation measure irrespective of other preference distribution metrics.To approach this minimax limit, we propose a nearly linear-time ranking scheme,called \emph{Spectral MLE}, that returns the indices of the top-$K$ items inaccordance to a careful score estimate. In a nutshell, Spectral MLE starts withan initial score estimate with minimal squared loss (obtained via a spectralmethod), and then successively refines each component with the assistance ofcoordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ itemidentification under minimal sample complexity. The practical applicability ofSpectral MLE is further corroborated by numerical experiments.
arxiv-11100-28 | Meta learning of bounds on the Bayes classifier error | http://arxiv.org/abs/1504.07116 | author:Kevin R. Moon, Veronique Delouille, Alfred O. Hero III category:cs.LG astro-ph.SR cs.CV cs.IT math.IT published:2015-04-27 summary:Meta learning uses information from base learners (e.g. classifiers orestimators) as well as information about the learning problem to improve uponthe performance of a single base learner. For example, the Bayes error rate ofa given feature space, if known, can be used to aid in choosing a classifier,as well as in feature selection and model selection for the base classifiersand the meta classifier. Recent work in the field of f-divergence functionalestimation has led to the development of simple and rapidly convergingestimators that can be used to estimate various bounds on the Bayes error. Weestimate multiple bounds on the Bayes error using an estimator that appliesmeta learning to slowly converging plug-in estimators to obtain the parametricconvergence rate. We compare the estimated bounds empirically on simulated dataand then estimate the tighter bounds on features extracted from an image patchanalysis of sunspot continuum and magnetogram images.
arxiv-11100-29 | Random Forest for the Contextual Bandit Problem - extended version | http://arxiv.org/abs/1504.06952v19.pdf | author:Raphaël Féraud, Robin Allesiardo, Tanguy Urvoy, Fabrice Clérot category:cs.LG published:2015-04-27 summary:To address the contextual bandit problem, we propose an online random forestalgorithm. The analysis of the proposed algorithm is based on the samplecomplexity needed to find the optimal decision stump. Then, the decision stumpsare assembled in a random collection of decision trees, Bandit Forest. We showthat the proposed algorithm is optimal up to logarithmic factors. Thedependence of the sample complexity upon the number of contextual variables islogarithmic. The computational cost of the proposed algorithm with respect tothe time horizon is linear. These analytical results allow the proposedalgorithm to be efficient in real applications, where the number of events toprocess is huge, and where we expect that some contextual variables, chosenfrom a large set, have potentially non- linear dependencies with the rewards.In the experiments done to illustrate the theoretical analysis, Bandit Forestobtain promising results in comparison with state-of-the-art algorithms.
arxiv-11100-30 | Correlational Neural Networks | http://arxiv.org/abs/1504.07225 | author:Sarath Chandar, Mitesh M. Khapra, Hugo Larochelle, Balaraman Ravindran category:cs.CL cs.LG cs.NE stat.ML published:2015-04-27 summary:Common Representation Learning (CRL), wherein different descriptions (orviews) of the data are embedded in a common subspace, is receiving a lot ofattention recently. Two popular paradigms here are Canonical CorrelationAnalysis (CCA) based approaches and Autoencoder (AE) based approaches. CCAbased approaches learn a joint representation by maximizing correlation of theviews when projected to the common subspace. AE based methods learn a commonrepresentation by minimizing the error of reconstructing the two views. Each ofthese approaches has its own advantages and disadvantages. For example, whileCCA based approaches outperform AE based approaches for the task of transferlearning, they are not as scalable as the latter. In this work we propose an AEbased approach called Correlational Neural Network (CorrNet), that explicitlymaximizes correlation among the views when projected to the common subspace.Through a series of experiments, we demonstrate that the proposed CorrNet isbetter than the above mentioned approaches with respect to its ability to learncorrelated common representations. Further, we employ CorrNet for several crosslanguage tasks and show that the representations learned using CorrNet performbetter than the ones learned using other state of the art approaches.
arxiv-11100-31 | Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits | http://arxiv.org/abs/1504.06937 | author:Huasen Wu, R. Srikant, Xin Liu, Chong Jiang category:cs.LG stat.ML published:2015-04-27 summary:We study contextual bandits with budget and time constraints, referred to asconstrained contextual bandits.The time and budget constraints significantlycomplicate the exploration and exploitation tradeoff because they introducecomplex coupling among contexts over time.Such coupling effects make itdifficult to obtain oracle solutions that assume known statistics of bandits.To gain insight, we first study unit-cost systems with known contextdistribution. When the expected rewards are known, we develop an approximationof the oracle, referred to Adaptive-Linear-Programming (ALP), which achievesnear-optimality and only requires the ordering of expected rewards. With thesehighly desirable features, we then combine ALP with the upper-confidence-bound(UCB) method in the general case where the expected rewards are unknown {\it apriori}. We show that the proposed UCB-ALP algorithm achieves logarithmicregret except for certain boundary cases. Further, we design algorithms andobtain similar regret analysis results for more general systems with unknowncontext distribution and heterogeneous costs. To the best of our knowledge,this is the first work that shows how to achieve logarithmic regret inconstrained contextual bandits. Moreover, this work also sheds light on thestudy of computationally efficient algorithms for general constrainedcontextual bandits.
arxiv-11100-32 | Image Segmentation and Restoration Using Parametric Contours With Free Endpoints | http://arxiv.org/abs/1504.07259 | author:Heike Benninghoff, Harald Garcke category:cs.CV math.AP math.NA published:2015-04-27 summary:In this paper, we introduce a novel approach for active contours with freeendpoints. A scheme is presented for image segmentation and restoration basedon a discrete version of the Mumford-Shah functional where the contours can beboth closed and open curves. Additional to a flow of the curves in normaldirection, evolution laws for the tangential flow of the endpoints are derived.Using a parametric approach to describe the evolving contours together with anedge-preserving denoising, we obtain a fast method for image segmentation andrestoration. The analytical and numerical schemes are presented followed bynumerical experiments with artificial test images and with a real medicalimage.
arxiv-11100-33 | Mid-level Elements for Object Detection | http://arxiv.org/abs/1504.07284 | author:Aayush Bansal, Abhinav Shrivastava, Carl Doersch, Abhinav Gupta category:cs.CV published:2015-04-27 summary:Building on the success of recent discriminative mid-level elements, wepropose a surprisingly simple approach for object detection which performscomparable to the current state-of-the-art approaches on PASCAL VOC comp-3detection challenge (no external data). Through extensive experiments andablation analysis, we show how our approach effectively improves upon theHOG-based pipelines by adding an intermediate mid-level representation for thetask of object detection. This representation is easily interpretable andallows us to visualize what our object detector "sees". We also discuss theinsights our approach shares with CNN-based methods, such as sharingrepresentation between categories helps.
arxiv-11100-34 | Optimal Convergence Rate in Feed Forward Neural Networks using HJB Equation | http://arxiv.org/abs/1504.07278 | author:Vipul Arora, Laxmidhar Behera, Ajay Pratap Yadav category:cs.NE published:2015-04-27 summary:A control theoretic approach is presented in this paper for both batch andinstantaneous updates of weights in feed-forward neural networks. The popularHamilton-Jacobi-Bellman (HJB) equation has been used to generate an optimalweight update law. The remarkable contribution in this paper is that closedform solutions for both optimal cost and weight update can be achieved for anyfeed-forward network using HJB equation in a simple yet elegant manner. Theproposed approach has been compared with some of the existing best performinglearning algorithms. It is found as expected that the proposed approach isfaster in convergence in terms of computational time. Some of the benchmarktest data such as 8-bit parity, breast cancer and credit approval, as well as2D Gabor function have been used to validate our claims. The paper alsodiscusses issues related to global optimization. The limitations of populardeterministic weight update laws are critiqued and the possibility of globaloptimization using HJB formulation is discussed. It is hoped that the proposedalgorithm will bring in a lot of interest in researchers working in developingfast learning algorithms and global optimization.
arxiv-11100-35 | Sign Stable Random Projections for Large-Scale Learning | http://arxiv.org/abs/1504.07235 | author:Ping Li category:stat.ML cs.LG stat.CO published:2015-04-27 summary:We study the use of "sign $\alpha$-stable random projections" (where$0<\alpha\leq 2$) for building basic data processing tools in the context oflarge-scale machine learning applications (e.g., classification, regression,clustering, and near-neighbor search). After the processing by sign stablerandom projections, the inner products of the processed data approximatevarious types of nonlinear kernels depending on the value of $\alpha$. Thus,this approach provides an effective strategy for approximating nonlinearlearning algorithms essentially at the cost of linear learning. When $\alpha=2$, it is known that the corresponding nonlinear kernel is the arc-cosinekernel. When $\alpha=1$, the procedure approximates the arc-cos-$\chi^2$ kernel(under certain condition). When $\alpha\rightarrow0+$, it corresponds to theresemblance kernel. From practitioners' perspective, the method of sign $\alpha$-stable randomprojections is ready to be tested for large-scale learning applications, where$\alpha$ can be simply viewed as a tuning parameter. What is missing in theliterature is an extensive empirical study to show the effectiveness of signstable random projections, especially for $\alpha\neq 2$ or 1. The papersupplies such a study on a wide variety of classification datasets. Inparticular, we compare shoulder-by-shoulder sign stable random projections withthe recently proposed "0-bit consistent weighted sampling (CWS)" (Li 2015).
arxiv-11100-36 | Combining Local Appearance and Holistic View: Dual-Source Deep Neural Networks for Human Pose Estimation | http://arxiv.org/abs/1504.07159 | author:Xiaochuan Fan, Kang Zheng, Yuewei Lin, Song Wang category:cs.CV published:2015-04-27 summary:We propose a new learning-based method for estimating 2D human pose from asingle image, using Dual-Source Deep Convolutional Neural Networks (DS-CNN).Recently, many methods have been developed to estimate human pose by using posepriors that are estimated from physiologically inspired graphical models orlearned from a holistic perspective. In this paper, we propose to integrateboth the local (body) part appearance and the holistic view of each local partfor more accurate human pose estimation. Specifically, the proposed DS-CNNtakes a set of image patches (category-independent object proposals fortraining and multi-scale sliding windows for testing) as the input and thenlearns the appearance of each local part by considering their holistic views inthe full body. Using DS-CNN, we achieve both joint detection, which determineswhether an image patch contains a body joint, and joint localization, whichfinds the exact location of the joint in the image patch. Finally, we developan algorithm to combine these joint detection/localization results from all theimage patches for estimating the human pose. The experimental results show theeffectiveness of the proposed method by comparing to the state-of-the-arthuman-pose estimation methods based on pose priors that are estimated fromphysiologically inspired graphical models or learned from a holisticperspective.
arxiv-11100-37 | Fast Sampling for Bayesian Max-Margin Models | http://arxiv.org/abs/1504.07107 | author:Wenbo Hu, Jun Zhu, Minjie Xu, Bo Zhang category:stat.ML cs.AI cs.LG published:2015-04-27 summary:Bayesian max-margin models have shown great superiority in various machinelearning tasks with a likelihood regularization, while the probabilistic MonteCarlo sampling for these models still remains challenging, especially forlarge-scale settings. In analogy to the data augmentation technique to tacklewith non-smoothness of the hinge loss, we present a stochastic subgradient MCMCmethod which is easy to implement and computationally efficient. We investigatethe variants that use adaptive stepsizes and thermostats to improve mixingspeeds for Bayesian linear SVM. Furthermore, we design a stochastic subgradientHMC within Gibbs method and a doubly stochastic HMC algorithm for mixture ofSVMs, a popular extension of linear classifiers. Experimental results on a widerange of problems demonstrate the effectiveness of our approach.
arxiv-11100-38 | Document Classification by Inversion of Distributed Language Representations | http://arxiv.org/abs/1504.07295 | author:Matt Taddy category:cs.CL cs.IR stat.AP published:2015-04-27 summary:There have been many recent advances in the structure and measurement ofdistributed language models: those that map from words to a vector-space thatis rich in information about word choice and composition. This vector-space isthe distributed language representation. The goal of this note is to point outthat any distributed representation can be turned into a classifier throughinversion via Bayes rule. The approach is simple and modular, in that it willwork with any language representation whose training can be formulated asoptimizing a probability model. In our application to 2 million sentences fromYelp reviews, we also find that it performs as well as or better than complexpurpose-built algorithms.
arxiv-11100-39 | Shape Representation and Classification through Pattern Spectrum and Local Binary Pattern - A Decision Level Fusion Approach | http://arxiv.org/abs/1504.07082 | author:B. H. Shekar, Bharathi Pilar category:cs.CV published:2015-04-27 summary:In this paper, we present a decision level fused local Morphological PatternSpectrum(PS) and Local Binary Pattern (LBP) approach for an efficient shaperepresentation and classification. This method makes use of Earth MoversDistance(EMD) as the measure in feature matching and shape retrieval process.The proposed approach has three major phases : Feature Extraction, Constructionof hybrid spectrum knowledge base and Classification. In the first phase,feature extraction of the shape is done using pattern spectrum and local binarypattern method. In the second phase, the histograms of both pattern spectrumand local binary pattern are fused and stored in the knowledge base. In thethird phase, the comparison and matching of the features, which are representedin the form of histograms, is done using Earth Movers Distance(EMD) as metric.The top-n shapes are retrieved for each query shape. The accuracy is tested bymeans of standard Bulls eye score method. The experiments are conducted onpublicly available shape datasets like Kimia-99, Kimia-216 and MPEG-7. Thecomparative study is also provided with the well known approaches to exhibitthe retrieval accuracy of the proposed approach.
arxiv-11100-40 | Surrogate regret bounds for generalized classification performance metrics | http://arxiv.org/abs/1504.07272 | author:Wojciech Kotłowski, Krzysztof Dembczyński category:cs.LG published:2015-04-27 summary:We consider optimization of generalized performance metrics for binaryclassification by means of surrogate loss. We focus on a class of metrics,which are linear-fractional functions of the false positive and false negativerates (examples of which include $F_{\beta}$-measure, Jaccard similaritycoefficient, AM measure, and many others). Our analysis concerns the followingtwo-step procedure. First, a real-valued function $f$ is learned by minimizinga surrogate loss for binary classification on the training sample. It isassumed that the surrogate loss is a strongly proper composite loss function(examples of which include logistic loss, squared-error loss, exponential loss,etc.). Then, given $f$, a threshold $\hat{\theta}$ is tuned on a separatevalidation sample, by direct optimization of the target performance measure. Weshow that the regret of the resulting classifier (obtained from thresholding$f$ on $\hat{\theta}$) measured with respect to the target metric isupperbounded by the regret of $f$ measured with respect to the surrogate loss.Our finding is further analyzed in a computational study on both synthetic andreal data sets.
arxiv-11100-41 | Exploring semantically-related concepts from Wikipedia: the case of SeRE | http://arxiv.org/abs/1504.07071 | author:Daniel Hienert, Dennis Wegener, Siegfried Schomisch category:cs.CL cs.IR published:2015-04-27 summary:In this paper we present our web application SeRE designed to exploresemantically related concepts. Wikipedia and DBpedia are rich data sources toextract related entities for a given topic, like in- and out-links, broader andnarrower terms, categorisation information etc. We use the Wikipedia full textbody to compute the semantic relatedness for extracted terms, which results ina list of entities that are most relevant for a topic. For any given query, theuser interface of SeRE visualizes these related concepts, ordered by semanticrelatedness; with snippets from Wikipedia articles that explain the connectionbetween those two entities. In a user study we examine how SeRE can be used tofind important entities and their relationships for a given topic and to answerthe question of how the classification system can be used for filtering.
arxiv-11100-42 | On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes | http://arxiv.org/abs/1504.07027 | author:Alexander G. de G. Matthews, James Hensman, Richard E. Turner, Zoubin Ghahramani category:stat.ML published:2015-04-27 summary:The variational framework for learning inducing variables (Titsias, 2009a)has had a large impact on the Gaussian process literature. The framework may beinterpreted as minimizing a rigorously defined Kullback-Leibler divergencebetween the approximating and posterior processes. To our knowledge thisconnection has thus far gone unremarked in the literature. In this paper wegive a substantial generalization of the literature on this topic. We give anew proof of the result for infinite index sets which allows inducing pointsthat are not data points and likelihoods that depend on all function values. Wethen discuss augmented index sets and show that, contrary to previous works,marginal consistency of augmentation is not enough to guarantee consistency ofvariational inference with the original model. We then characterize an extracondition where such a guarantee is obtainable. Finally we show how ourframework sheds light on interdomain sparse approximations and sparseapproximations for Cox processes.
arxiv-11100-43 | SegSALSA-STR: A convex formulation to supervised hyperspectral image segmentation using hidden fields and structure tensor regularization | http://arxiv.org/abs/1504.07028 | author:Filipe Condessa, Jose Bioucas-Dias, Jelena Kovacevic category:cs.CV 68 published:2015-04-27 summary:We present a supervised hyperspectral image segmentation algorithm based on aconvex formulation of a marginal maximum a posteriori segmentation with hiddenfields and structure tensor regularization: Segmentation via the ConstraintSplit Augmented Lagrangian Shrinkage by Structure Tensor Regularization(SegSALSA-STR). This formulation avoids the generally discrete nature ofsegmentation problems and the inherent NP-hardness of the integer optimizationassociated. We extend the Segmentation via the Constraint Split Augmented LagrangianShrinkage (SegSALSA) algorithm by generalizing the vectorial total variationprior using a structure tensor prior constructed from a patch-based Jacobian.The resulting algorithm is convex, time-efficient and highly parallelizable.This shows the potential of combining hidden fields with convex optimizationthrough the inclusion of different regularizers. The SegSALSA-STR algorithm isvalidated in the segmentation of real hyperspectral images.
arxiv-11100-44 | On-Board Vision Processing For Small UAVs: Time to Rethink Strategy | http://arxiv.org/abs/1504.07021 | author:Shoaib Ehsan, Klaus D. McDonald-Maier category:cs.CV cs.RO published:2015-04-27 summary:The ultimate research goal for unmanned aerial vehicles (UAVs) is tofacilitate autonomy of operation. Research in the last decade has highlightedthe potential of vision sensing in this regard. Although vital foraccomplishment of missions assigned to any type of unmanned aerial vehicles,vision sensing is more critical for small aerial vehicles due to lack of highprecision inertial sensors. In addition, uncertainty of GPS signal in indoorand urban environments calls for more reliance on vision sensing for such smallvehicles. With off-line processing does not offer an attractive option in termsof autonomy, these vehicles have been challenging platforms to implement visionprocessing onboard due to their strict payload capacity and power budget. Thestrict constraints drive the need for new vision processing architectures forsmall unmanned aerial vehicles. Recent research has shown encouraging resultswith FPGA based hardware architectures. This paper reviews the bottle necksinvolved in implementing vision processing on-board, advocates the potential ofhardware based solutions to tackle strict constraints of small unmanned aerialvehicles and finally analyzes feasibility of ASICs, Structured ASICs and FPGAsfor use on future systems.
arxiv-11100-45 | Linear Spatial Pyramid Matching Using Non-convex and non-negative Sparse Coding for Image Classification | http://arxiv.org/abs/1504.06897 | author:Chengqiang Bao, Liangtian He, Yilun Wang category:cs.CV cs.LG 68T45 I.5.2 published:2015-04-27 summary:Recently sparse coding have been highly successful in image classificationmainly due to its capability of incorporating the sparsity of imagerepresentation. In this paper, we propose an improved sparse coding model basedon linear spatial pyramid matching(SPM) and Scale Invariant Feature Transform(SIFT ) descriptors. The novelty is the simultaneous non-convex andnon-negative characters added to the sparse coding model. Our numericalexperiments show that the improved approach using non-convex and non-negativesparse coding is superior than the original ScSPM[1] on several typicaldatabases.
arxiv-11100-46 | An Active Learning Based Approach For Effective Video Annotation And Retrieval | http://arxiv.org/abs/1504.07004 | author:Moitreya Chatterjee, Anton Leuski category:cs.MM cs.IR cs.LG H.3.3; H.5.1 published:2015-04-27 summary:Conventional multimedia annotation/retrieval systems such as NormalizedContinuous Relevance Model (NormCRM) [16] require a fully labeled training datafor a good performance. Active Learning, by determining an order for labelingthe training data, allows for a good performance even before the training datais fully annotated. In this work we propose an active learning algorithm, whichcombines a novel measure of sample uncertainty with a novel clustering-basedapproach for determining sample density and diversity and integrate it withNormCRM. The clusters are also iteratively refined to ensure both feature andlabel-level agreement among samples. We show that our approach outperformsmultiple baselines both on a recent, open character animation dataset and onthe popular TRECVID corpus at both the tasks of annotation and text-basedretrieval of videos.
arxiv-11100-47 | Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection | http://arxiv.org/abs/1504.07029 | author:David Novotny, Jiri Matas category:cs.CV published:2015-04-27 summary:A novel efficient method for extraction of object proposals is introduced.Its "objectness" function exploits deep spatial pyramid features, a novelfast-to-compute HoG-based edge statistic and the EdgeBoxes score. Theefficiency is achieved by the use of spatial bins in a novel combination withsparsity-inducing group normalized SVM. State-of-the-art recall performance isachieved on Pascal VOC07, significantly outperforming methods with comparablespeed. Interestingly, when only 100 proposals per image are considered themethod attains 78% recall on VOC07. The method improves mAP of the RCNNstate-of-the-art class-specific detector, increasing it by 10 points when only50 proposals are used in each image. The system trained on twenty classesperforms well on the two hundred class ILSVRC2013 set confirming generalizationcapability.
arxiv-11100-48 | Dynamic Body VSLAM with Semantic Constraints | http://arxiv.org/abs/1504.07269 | author:N. Dinesh Reddy, Prateek Singhal, Visesh Chari, K. Madhava Krishna category:cs.CV published:2015-04-27 summary:Image based reconstruction of urban environments is a challenging problemthat deals with optimization of large number of variables, and has severalsources of errors like the presence of dynamic objects. Since most large scaleapproaches make the assumption of observing static scenes, dynamic objects arerelegated to the noise modeling section of such systems. This is an approach ofconvenience since the RANSAC based framework used to compute most multiviewgeometric quantities for static scenes naturally confine dynamic objects to theclass of outlier measurements. However, reconstructing dynamic objects alongwith the static environment helps us get a complete picture of an urbanenvironment. Such understanding can then be used for important robotic taskslike path planning for autonomous navigation, obstacle tracking and avoidance,and other areas. In this paper, we propose a system for robust SLAM that worksin both static and dynamic environments. To overcome the challenge of dynamicobjects in the scene, we propose a new model to incorporate semanticconstraints into the reconstruction algorithm. While some of these constraintsare based on multi-layered dense CRFs trained over appearance as well as motioncues, other proposed constraints can be expressed as additional terms in thebundle adjustment optimization process that does iterative refinement of 3Dstructure and camera / object motion trajectories. We show results on thechallenging KITTI urban dataset for accuracy of motion segmentation andreconstruction of the trajectory and shape of moving objects relative to groundtruth. We are able to show average relative error reduction by a significantamount for moving object trajectory reconstruction relative to state-of-the-artmethods like VISO 2, as well as standard bundle adjustment algorithms.
arxiv-11100-49 | Detection and Recognition of Malaysian Special License Plate Based On SIFT Features | http://arxiv.org/abs/1504.06921 | author:Hooi Sin Ng, Yong Haur Tay, Kim Meng Liang, Hamam Mokayed, Hock Woon Hon category:cs.CV published:2015-04-27 summary:Automated car license plate recognition systems are developed and applied forpurpose of facilitating the surveillance, law enforcement, access control andintelligent transportation monitoring with least human intervention. In thispaper, an algorithm based on SIFT feature points clustering and matching isproposed to address the issue of recognizing Malaysian special plates. Thesespecial plates do not follow the format of standard car plates as they maycontain italic, cursive, connected and small letters. The algorithm is testedwith 150 Malaysian special plate images under different environment and thepromising experimental results demonstrate that the proposed algorithm isrelatively robust.
arxiv-11100-50 | Concept Extraction to Identify Adverse Drug Reactions in Medical Forums: A Comparison of Algorithms | http://arxiv.org/abs/1504.06936 | author:Alejandro Metke-Jimenez, Sarvnaz Karimi category:cs.AI cs.CL cs.IR published:2015-04-27 summary:Social media is becoming an increasingly important source of information tocomplement traditional pharmacovigilance methods. In order to identify signalsof potential adverse drug reactions, it is necessary to first identify medicalconcepts in the social media text. Most of the existing studies usedictionary-based methods which are not evaluated independently from the overallsignal detection task. We compare different approaches to automatically identify and normalisemedical concepts in consumer reviews in medical forums. Specifically, weimplement several dictionary-based methods popular in the relevant literature,as well as a method we suggest based on a state-of-the-art machine learningmethod for entity recognition. MetaMap, a popular biomedical concept extractiontool, is used as a baseline. Our evaluations were performed in a controlledsetting on a common corpus which is a collection of medical forum postsannotated with concepts and linked to controlled vocabularies such as MedDRAand SNOMED CT. To our knowledge, our study is the first to systematically examine the effectof popular concept extraction methods in the area of signal detection foradverse reactions. We show that the choice of algorithm or controlledvocabulary has a significant impact on concept extraction, which will impactthe overall signal detection process. We also show that our proposed machinelearning approach significantly outperforms all the other methods inidentification of both adverse reactions and drugs, even when trained with arelatively small set of annotated text.
arxiv-11100-51 | Modeling Recovery Curves With Application to Prostatectomy | http://arxiv.org/abs/1504.06964 | author:Fulton Wang, Tyler H. McCormick, Cynthia Rudin, John Gore category:stat.ME stat.AP stat.ML published:2015-04-27 summary:We propose a Bayesian model that predicts recovery curves based oninformation available before the disruptive event. A recovery curve of interestis the quantified sexual function of prostate cancer patients afterprostatectomy surgery. We illustrate the utility of our model as apre-treatment medical decision aid, producing personalized predictions that areboth interpretable and accurate. We uncover covariate relationships that agreewith and supplement that in existing medical literature.
arxiv-11100-52 | Accelerated nonlinear discriminant analysis | http://arxiv.org/abs/1504.07000 | author:Nikolaos Gkalelis, Vasileios Mezaris category:cs.LG published:2015-04-27 summary:In this paper, a novel nonlinear discriminant analysis is proposed.Experimental results show that the new method provides state of the artperformance when combined with LSVM in terms of training time and accuracy.
arxiv-11100-53 | Compression Artifacts Reduction by a Deep Convolutional Network | http://arxiv.org/abs/1504.06993 | author:Chao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang category:cs.CV I.4.5; I.2.6 published:2015-04-27 summary:Lossy compression introduces complex compression artifacts, particularly theblocking artifacts, ringing effects and blurring. Existing algorithms eitherfocus on removing blocking artifacts and produce blurred output, or restoressharpened images that are accompanied with ringing effects. Inspired by thedeep convolutional networks (DCN) on super-resolution, we formulate a compactand efficient network for seamless attenuation of different compressionartifacts. We also demonstrate that a deeper model can be effectively trainedwith the features learned in a shallow network. Following a similar "easy tohard" idea, we systematically investigate several practical transfer settingsand show the effectiveness of transfer learning in low-level vision problems.Our method shows superior performance than the state-of-the-arts both on thebenchmark datasets and the real-world use case (i.e. Twitter). In addition, weshow that our method can be applied as pre-processing to facilitate otherlow-level vision routines when they take compressed images as input.
arxiv-11100-54 | Computational Cost Reduction in Learned Transform Classifications | http://arxiv.org/abs/1504.06779 | author:Emerson Lopes Machado, Cristiano Jacques Miosso, Ricardo von Borries, Murilo Coutinho, Pedro de Azevedo Berger, Thiago Marques, Ricardo Pezzuol Jacobi category:cs.CV stat.ML published:2015-04-26 summary:We present a theoretical analysis and empirical evaluations of a novel set oftechniques for computational cost reduction of classifiers that are based onlearned transform and soft-threshold. By modifying optimization procedures fordictionary and classifier training, as well as the resulting dictionaryentries, our techniques allow to reduce the bit precision and to replace eachfloating-point multiplication by a single integer bit shift. We also show howthe optimization algorithms in some dictionary training methods can be modifiedto penalize higher-energy dictionaries. We applied our techniques with theclassifier Learning Algorithm for Soft-Thresholding, testing on the datasetsused in its original paper. Our results indicate it is feasible to use solelysums and bit shifts of integers to classify at test time with a limitedreduction of the classification accuracy. These low power operations are avaluable trade off in FPGA implementations as they increase the classificationthroughput while decrease both energy consumption and manufacturing cost.
arxiv-11100-55 | Overlapping Community Detection by Online Cluster Aggregation | http://arxiv.org/abs/1504.06798 | author:Mark Kozdoba, Shie Mannor category:cs.LG cs.SI physics.soc-ph published:2015-04-26 summary:We present a new online algorithm for detecting overlapping communities. Themain ingredients are a modification of an online k-means algorithm and a newapproach to modelling overlap in communities. An evaluation on large benchmarkgraphs shows that the quality of discovered communities compares favorably toseveral methods in the recent literature, while the running time issignificantly improved.
arxiv-11100-56 | Overlapping Communities Detection via Measure Space Embedding | http://arxiv.org/abs/1504.06796 | author:Mark Kozdoba, Shie Mannor category:cs.LG cs.SI stat.ML published:2015-04-26 summary:We present a new algorithm for community detection. The algorithm uses randomwalks to embed the graph in a space of measures, after which a modification of$k$-means in that space is applied. The algorithm is therefore fast and easilyparallelizable. We evaluate the algorithm on standard random graph benchmarks,including some overlapping community benchmarks, and find its performance to bebetter or at least as good as previously known algorithms. We also prove alinear time (in number of edges) guarantee for the algorithm on a$p,q$-stochastic block model with $p \geq c\cdot N^{-\frac{1}{2} + \epsilon}$and $p-q \geq c' \sqrt{p N^{-\frac{1}{2} + \epsilon} \log N}$.
arxiv-11100-57 | Bayesian kernel-based system identification with quantized output data | http://arxiv.org/abs/1504.06877 | author:Giulio Bottegal, Gianluigi Pillonetto, Håkan Hjalmarsson category:cs.SY stat.ML published:2015-04-26 summary:In this paper we introduce a novel method for linear system identificationwith quantized output data. We model the impulse response as a zero-meanGaussian process whose covariance (kernel) is given by the recently proposedstable spline kernel, which encodes information on regularity and exponentialstability. This serves as a starting point to cast our system identificationproblem into a Bayesian framework. We employ Markov Chain Monte Carlo (MCMC)methods to provide an estimate of the system. In particular, we show how todesign a Gibbs sampler which quickly converges to the target distribution.Numerical simulations show a substantial improvement in the accuracy of theestimates over state-of-the-art kernel-based methods when employed inidentification of systems with quantized data.
arxiv-11100-58 | Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review | http://arxiv.org/abs/1504.06868 | author:Gordon V. Cormack, Maura R. Grossman category:cs.IR cs.LG published:2015-04-26 summary:We enhance the autonomy of the continuous active learning method shown byCormack and Grossman (SIGIR 2014) to be effective for technology-assistedreview, in which documents from a collection are retrieved and reviewed, usingrelevance feedback, until substantially all of the relevant documents have beenreviewed. Autonomy is enhanced through the elimination of topic-specific anddataset-specific tuning parameters, so that the sole input required by the useris, at the outset, a short query, topic description, or single relevantdocument; and, throughout the review, ongoing relevance assessments of theretrieved documents. We show that our enhancements consistently yield superiorresults to Cormack and Grossman's version of continuous active learning, andother methods, not only on average, but on the vast majority of topics fromfour separate sets of tasks: the legal datasets examined by Cormack andGrossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, andthe construction of the TREC 2002 filtering test collection.
arxiv-11100-59 | Fast Dictionary Matching for Content-based Image Retrieval | http://arxiv.org/abs/1504.06864 | author:Patryk Najgebauer, Janusz Rygal, Tomasz Nowak, Jakub Romanowski, Leszek Rutkowski, Sviatoslav Voloshynovskiy, Rafal Scherer category:cs.CV published:2015-04-26 summary:This paper describes a method for searching for common sets of descriptorsbetween collections of images. The presented method operates on local interestkeypoints, which are generated using the SURF algorithm. The use of adictionary of descriptors allowed achieving good performance of thecontent-based image retrieval. The method can be used to initially determine aset of similar pairs of keypoints between images. For this purpose, we use acertain level of tolerance between values of descriptors, as values of featuredescriptors are almost never equal but similar between different images. Afterthat, the method compares the structure of rotation and location of interestpoints in one image with the point structure in other images. Thus, we wereable to find similar areas in images and determine the level of similaritybetween them, even when images contain different scenes.
arxiv-11100-60 | When Hillclimbers Beat Genetic Algorithms in Multimodal Optimization | http://arxiv.org/abs/1504.06859 | author:Fernando G. Lobo, Mosab Bazargani category:cs.NE published:2015-04-26 summary:It has been shown in the past that a multistart hillclimbing strategycompares favourably to a standard genetic algorithm with respect to solvinginstances of the multimodal problem generator. We extend that work and verifyif the utilization of diversity preservation techniques in the geneticalgorithm changes the outcome of the comparison. We do so under two scenarios:(1) when the goal is to find the global optimum, (2) when the goal is to findall optima. A mathematical analysis is performed for the multistart hillclimbingalgorithm and a through empirical study is conducted for solving instances ofthe multimodal problem generator with increasing number of optima, both withthe hillclimbing strategy as well as with genetic algorithms with niching.Although niching improves the performance of the genetic algorithm, it is stillinferior to the multistart hillclimbing strategy on this class of problems. An idealized niching strategy is also presented and it is argued that itsperformance should be close to a lower bound of what any evolutionary algorithmcan do on this class of problems.
arxiv-11100-61 | Complete Dictionary Recovery over the Sphere | http://arxiv.org/abs/1504.06785 | author:Ju Sun, Qing Qu, John Wright category:cs.IT cs.CV cs.LG math.IT math.OC stat.ML published:2015-04-26 summary:We consider the problem of recovering a complete (i.e., square andinvertible) matrix $\mathbf A_0$, from $\mathbf Y \in \mathbb R^{n \times p}$with $\mathbf Y = \mathbf A_0 \mathbf X_0$, provided $\mathbf X_0$ issufficiently sparse. This recovery problem is central to the theoreticalunderstanding of dictionary learning, which seeks a sparse representation for acollection of input signals, and finds numerous applications in modern signalprocessing and machine learning. We give the first efficient algorithm thatprovably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O(n)$ nonzeros percolumn, under suitable probability model for $\mathbf X_0$. In contrast, priorresults based on efficient algorithms provide recovery guarantees when $\mathbfX_0$ has only $O(n^{1-\delta})$ nonzeros per column for any constant $\delta\in (0, 1)$. Our algorithmic pipeline centers around solving a certain nonconvexoptimization problem with a spherical constraint, and hence is naturallyphrased in the language of manifold optimization. To show this apparently hardproblem is tractable, we first provide a geometric characterization of thehigh-dimensional objective landscape, which shows that with high probabilitythere are no "spurious" local minima. This particular geometric structureallows us to design a Riemannian trust region algorithm over the sphere thatprovably converges to one local minimizer with an arbitrary initialization,despite the presence of saddle points. The geometric approach we develop heremay also shed light on other problems arising from nonconvex recovery ofstructured signals.
arxiv-11100-62 | Deviation Based Pooling Strategies For Full Reference Image Quality Assessment | http://arxiv.org/abs/1504.06786 | author:Hossein Ziaei Nafchi, Rachid Hedjam, Atena Shahkolaei, Mohamed Cheriet category:cs.MM cs.CV published:2015-04-26 summary:The state-of-the-art pooling strategies for perceptual image qualityassessment (IQA) are based on the mean and the weighted mean. They are robustpooling strategies which usually provide a moderate to high performance fordifferent IQAs. Recently, standard deviation (SD) pooling was also proposed.Although, this deviation pooling provides a very high performance for a fewIQAs, its performance is lower than mean poolings for many other IQAs. In thispaper, we propose to use the mean absolute deviation (MAD) and show that it isa more robust and accurate pooling strategy for a wider range of IQAs. In fact,MAD pooling has the advantages of both mean pooling and SD pooling. The jointcomputation and use of the MAD and SD pooling strategies is also considered inthis paper. Experimental results provide useful information on the choice ofthe proper deviation pooling strategy for different IQA models.
arxiv-11100-63 | Maximum a Posteriori Estimation by Search in Probabilistic Programs | http://arxiv.org/abs/1504.06848 | author:David Tolpin, Frank Wood category:cs.AI stat.ML published:2015-04-26 summary:We introduce an approximate search algorithm for fast maximum a posterioriprobability estimation in probabilistic programs, which we call Bayesian ascentMonte Carlo (BaMC). Probabilistic programs represent probabilistic models withvarying number of mutually dependent finite, countable, and continuous randomvariables. BaMC is an anytime MAP search algorithm applicable to anycombination of random variables and dependencies. We compare BaMC to other MAPestimation algorithms and show that BaMC is faster and more robust on a rangeof probabilistic models.
arxiv-11100-64 | Comparison of Training Methods for Deep Neural Networks | http://arxiv.org/abs/1504.06825 | author:Patrick O. Glauner category:cs.LG cs.AI published:2015-04-26 summary:This report describes the difficulties of training neural networks and inparticular deep neural networks. It then provides a literature review oftraining methods for deep neural networks, with a focus on pre-training. Itfocuses on Deep Belief Networks composed of Restricted Boltzmann Machines andStacked Autoencoders and provides an outreach on further and alternativeapproaches. It also includes related practical recommendations from theliterature on training them. In the second part, initial experiments using someof the covered methods are performed on two databases. In particular,experiments are performed on the MNIST hand-written digit dataset and on facialemotion data from a Kaggle competition. The results are discussed in thecontext of results reported in other research papers. An error rate lower thanthe best contribution to the Kaggle competition is achieved using an optimizedStacked Autoencoder.
arxiv-11100-65 | Analysis of Nuclear Norm Regularization for Full-rank Matrix Completion | http://arxiv.org/abs/1504.06817 | author:Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou category:cs.LG stat.ML published:2015-04-26 summary:In this paper, we provide a theoretical analysis of the nuclear-normregularized least squares for full-rank matrix completion. Although similarformulations have been examined by previous studies, their results areunsatisfactory because only additive upper bounds are provided. Under theassumption that the top eigenspaces of the target matrix are incoherent, wederive a relative upper bound for recovering the best low-rank approximation ofthe unknown matrix. Our relative upper bound is tighter than previous additivebounds of other methods if the mass of the target matrix is concentrated on itstop eigenspaces, and also implies perfect recovery if it is low-rank. Theanalysis is built upon the optimality condition of the regularized formulationand existing guarantees for low-rank matrix completion. To the best of ourknowledge, this is first time such a relative bound is proved for theregularized formulation of matrix completion.
arxiv-11100-66 | FlowNet: Learning Optical Flow with Convolutional Networks | http://arxiv.org/abs/1504.06852 | author:Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Häusser, Caner Hazırbaş, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox category:cs.CV cs.LG I.2.6; I.4.8 published:2015-04-26 summary:Convolutional neural networks (CNNs) have recently been very successful in avariety of computer vision tasks, especially on those linked to recognition.Optical flow estimation has not been among the tasks where CNNs weresuccessful. In this paper we construct appropriate CNNs which are capable ofsolving the optical flow estimation problem as a supervised learning task. Wepropose and compare two architectures: a generic architecture and another oneincluding a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train aCNN, we generate a synthetic Flying Chairs dataset. We show that networkstrained on this unrealistic data still generalize very well to existingdatasets such as Sintel and KITTI, achieving competitive accuracy at framerates of 5 to 10 fps.
arxiv-11100-67 | Max-margin Deep Generative Models | http://arxiv.org/abs/1504.06787 | author:Chongxuan Li, Jun Zhu, Tianlin Shi, Bo Zhang category:cs.LG cs.CV published:2015-04-26 summary:Deep generative models (DGMs) are effective on learning multilayeredrepresentations of complex data and performing inference of input data byexploring the generative ability. However, little work has been done onexamining or empowering the discriminative ability of DGMs on making accuratepredictions. This paper presents max-margin deep generative models (mmDGMs),which explore the strongly discriminative principle of max-margin learning toimprove the discriminative power of DGMs, while retaining the generativecapability. We develop an efficient doubly stochastic subgradient algorithm forthe piecewise linear objective. Empirical results on MNIST and SVHN datasetsdemonstrate that (1) max-margin learning can significantly improve theprediction performance of DGMs and meanwhile retain the generative ability; and(2) mmDGMs are competitive to the state-of-the-art fully discriminativenetworks by employing deep convolutional neural networks (CNNs) as bothrecognition and generative models.
arxiv-11100-68 | Assessing binary classifiers using only positive and unlabeled data | http://arxiv.org/abs/1504.06837 | author:Marc Claesen, Jesse Davis, Frank De Smet, Bart De Moor category:stat.ML cs.IR cs.LG I.5.2 published:2015-04-26 summary:Assessing the performance of a learned model is a crucial part of machinelearning. However, in some domains only positive and unlabeled examples areavailable, which prohibits the use of most standard evaluation metrics. Wepropose an approach to estimate any metric based on contingency tables,including ROC and PR curves, using only positive and unlabeled data. Estimatingthese performance metrics is essentially reduced to estimating the fraction of(latent) positives in the unlabeled set, assuming known positives are a randomsample of all positives. We provide theoretical bounds on the quality of ourestimates, illustrate the importance of estimating the fraction of positives inthe unlabeled set and demonstrate empirically that we are able to reliablyestimate ROC and PR curves on real data.
arxiv-11100-69 | Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images | http://arxiv.org/abs/1504.06692 | author:Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille category:cs.CV cs.CL cs.LG published:2015-04-25 summary:In this paper, we address the task of learning novel visual concepts, andtheir interactions with other concepts, from a few images with sentencedescriptions. Using linguistic context and visual features, our method is ableto efficiently hypothesize the semantic meaning of new words and add them toits word dictionary so that they can be used to describe images which containthese novel concepts. Our method has an image captioning module based on m-RNNwith several improvements. In particular, we propose a transposed weightsharing scheme, which not only improves performance on image captioning, butalso makes the model more suitable for the novel concept learning task. Wepropose methods to prevent overfitting the new concepts. In addition, threenovel concept datasets are constructed for this new task. In the experiments,we show that our method effectively learns novel visual concepts from a fewexamples without disturbing the previously learned concepts. The project pageis http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html
arxiv-11100-70 | Differential Recurrent Neural Networks for Action Recognition | http://arxiv.org/abs/1504.06678 | author:Vivek Veeriah, Naifan Zhuang, Guo-Jun Qi category:cs.CV published:2015-04-25 summary:The long short-term memory (LSTM) neural network is capable of processingcomplex sequential information since it utilizes special gating schemes forlearning representations from long input sequences. It has the potential tomodel any sequential time-series data, where the current hidden state has to beconsidered in the context of the past hidden states. This property makes LSTMan ideal choice to learn the complex dynamics of various actions.Unfortunately, the conventional LSTMs do not consider the impact ofspatio-temporal dynamics corresponding to the given salient motion patterns,when they gate the information that ought to be memorized through time. Toaddress this problem, we propose a differential gating scheme for the LSTMneural network, which emphasizes on the change in information gain caused bythe salient motions between the successive frames. This change in informationgain is quantified by Derivative of States (DoS), and thus the proposed LSTMmodel is termed as differential Recurrent Neural Network (dRNN). We demonstratethe effectiveness of the proposed model by automatically recognizing actionsfrom the real-world 2D and 3D human action datasets. Our study is one of thefirst works towards demonstrating the potential of learning complex time-seriesrepresentations via high-order derivatives of states.
arxiv-11100-71 | TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking | http://arxiv.org/abs/1504.06755 | author:Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R. Kulkarni, Jianxiong Xiao category:cs.CV published:2015-04-25 summary:Traditional eye tracking requires specialized hardware, which meanscollecting gaze data from many observers is expensive, tedious and slow.Therefore, existing saliency prediction datasets are order-of-magnitudessmaller than typical datasets for other vision recognition tasks. The smallsize of these datasets limits the potential for training data intensivealgorithms, and causes overfitting in benchmark evaluation. To address thisdeficiency, this paper introduces a webcam-based gaze tracking system thatsupports large-scale, crowdsourced eye tracking deployed on Amazon MechanicalTurk (AMTurk). By a combination of careful algorithm and gaming protocoldesign, our system obtains eye tracking data for saliency prediction comparableto data gathered in a traditional lab setting, with relatively lower cost andless effort on the part of the researchers. Using this tool, we build asaliency dataset for a large number of natural images. We will open-source ourtool and provide a web server where researchers can upload their images to geteye tracking results from AMTurk.
arxiv-11100-72 | Online Convex Optimization Using Predictions | http://arxiv.org/abs/1504.06681 | author:Niangjun Chen, Anish Agarwal, Adam Wierman, Siddharth Barman, Lachlan L. H. Andrew category:cs.LG published:2015-04-25 summary:Making use of predictions is a crucial, but under-explored, area of onlinealgorithms. This paper studies a class of online optimization problems where wehave external noisy predictions available. We propose a stochastic predictionerror model that generalizes prior models in the learning and stochasticcontrol communities, incorporates correlation among prediction errors, andcaptures the fact that predictions improve as time passes. We prove thatachieving sublinear regret and constant competitive ratio for online algorithmsrequires the use of an unbounded prediction window in adversarial settings, butthat under more realistic stochastic prediction error models it is possible touse Averaging Fixed Horizon Control (AFHC) to simultaneously achieve sublinearregret and constant competitive ratio in expectation using only aconstant-sized prediction window. Furthermore, we show that the performance ofAFHC is tightly concentrated around its mean.
arxiv-11100-73 | A Prior Distribution over Directed Acyclic Graphs for Sparse Bayesian Networks | http://arxiv.org/abs/1504.06701 | author:Felix L. Rios, John M. Noble, Timo J. T. Koski category:stat.ML published:2015-04-25 summary:The main contribution of this article is a new prior distribution overdirected acyclic graphs, which gives larger weight to sparse graphs. Thisdistribution is intended for structured Bayesian networks, where the structureis given by an ordered block model. That is, the nodes of the graph are objectswhich fall into categories (or blocks); the blocks have a natural ordering. Thepresence of a relationship between two objects is denoted by an arrow, from theobject of lower category to the object of higher category. The modelsconsidered here were introduced in Kemp et al. (2004) for relational data andextended to multivariate data in Mansinghka et al. (2006). The prior over graphstructures presented here has an explicit formula. The number of nodes in eachlayer of the graph follow a Hoppe Ewens urn model. We consider the situation where the nodes of the graph represent randomvariables, whose joint probability distribution factorises along the DAG. Wedescribe Monte Carlo schemes for finding the optimal aposteriori structuregiven a data matrix and compare the performance with Mansinghka et al. (2006)and also with the uniform prior.
arxiv-11100-74 | Adaptive Locally Affine-Invariant Shape Matching | http://arxiv.org/abs/1504.06719 | author:Smit Marvaniya, Raj Gupta, Anurag Mittal category:cs.CV published:2015-04-25 summary:Matching deformable objects using their shapes is an important problem incomputer vision since shape is perhaps the most distinguishable characteristicof an object. The problem is difficult due to many factors such as intra-classvariations, local deformations, articulations, viewpoint changes and missed andextraneous contour portions due to errors in shape extraction. While smalllocal deformations has been handled in the literature by allowing some leewayin the matching of individual contour points via methods such as Chamferdistance and Hausdorff distance, handling more severe deformations andarticulations has been done by applying local geometric corrections such assimilarity or affine. However, determining which portions of the shape shouldbe used for the geometric corrections is very hard, although some methods havebeen tried. In this paper, we address this problem by an efficient search forthe group of contour segments to be clustered together for a geometriccorrection using Dynamic Programming by essentially searching for thesegmentations of two shapes that lead to the best matching between them. At thesame time, we allow portions of the contours to remain unmatched to handlemissing and extraneous contour portions. Experiments indicate that our methodoutperforms other algorithms, especially when the shapes to be matched are morecomplex.
arxiv-11100-75 | SIFT Vs SURF: Quantifying the Variation in Transformations | http://arxiv.org/abs/1504.06740 | author:Siddharth Srivastava category:cs.CV published:2015-04-25 summary:This paper studies the robustness of SIFT and SURF against different imagetransforms (rigid body, similarity, affine and projective) by quantitativelyanalyzing the variations in the extent of transformations. Previous studieshave been comparing the two techniques on absolute transformations rather thanthe specific amount of deformation caused by the transformation. The paperestablishes an exhaustive empirical analysis of such deformations and matchingcapability of SIFT and SURF with variations in matching parameters and theamount of tolerance. This is helpful in choosing the specific use case forapplying these techniques.
arxiv-11100-76 | Discriminative Switching Linear Dynamical Systems applied to Physiological Condition Monitoring | http://arxiv.org/abs/1504.06494 | author:Konstantinos Georgatzis, Christopher K. I. Williams category:cs.LG published:2015-04-24 summary:We present a Discriminative Switching Linear Dynamical System (DSLDS) appliedto patient monitoring in Intensive Care Units (ICUs). Our approach is based onidentifying the state-of-health of a patient given their observed vital signsusing a discriminative classifier, and then inferring their underlyingphysiological values conditioned on this status. The work builds on theFactorial Switching Linear Dynamical System (FSLDS) (Quinn et al., 2009) whichhas been previously used in a similar setting. The FSLDS is a generative model,whereas the DSLDS is a discriminative model. We demonstrate on two real-worlddatasets that the DSLDS is able to outperform the FSLDS in most cases ofinterest, and that an $\alpha$-mixture of the two models achieves higherperformance than either of the two models separately.
arxiv-11100-77 | Holistically-Nested Edge Detection | http://arxiv.org/abs/1504.06375 | author:Saining Xie, Zhuowen Tu category:cs.CV published:2015-04-24 summary:We develop a new edge detection algorithm that tackles two important issuesin this long-standing vision problem: (1) holistic image training andprediction; and (2) multi-scale and multi-level feature learning. Our proposedmethod, holistically-nested edge detection (HED), performs image-to-imageprediction by means of a deep learning model that leverages fully convolutionalneural networks and deeply-supervised nets. HED automatically learns richhierarchical representations (guided by deep supervision on side responses)that are important in order to approach the human ability resolve thechallenging ambiguity in edge and object boundary detection. We significantlyadvance the state-of-the-art on the BSD500 dataset (ODS F-score of .782) andthe NYU Depth dataset (ODS F-score of .746), and do so with an improved speed(0.4 second per image) that is orders of magnitude faster than some recentCNN-based edge detection algorithms.
arxiv-11100-78 | Social Trust Prediction via Max-norm Constrained 1-bit Matrix Completion | http://arxiv.org/abs/1504.06394 | author:Jing Wang, Jie Shen, Huan Xu category:cs.SI cs.LG stat.ML published:2015-04-24 summary:Social trust prediction addresses the significant problem of exploringinteractions among users in social networks. Naturally, this problem can beformulated in the matrix completion framework, with each entry indicating thetrustness or distrustness. However, there are two challenges for the socialtrust problem: 1) the observed data are with sign (1-bit) measurements; 2) theyare typically sampled non-uniformly. Most of the previous matrix completionmethods do not well handle the two issues. Motivated by the recent progress ofmax-norm, we propose to solve the problem with a 1-bit max-norm constrainedformulation. Since max-norm is not easy to optimize, we utilize a reformulationof max-norm which facilitates an efficient projected gradient decent algorithm.We demonstrate the superiority of our formulation on two benchmark datasets.
arxiv-11100-79 | On the Stability of Online Language Features: How Much Text do you Need to know a Person? | http://arxiv.org/abs/1504.06391 | author:Eben M. Haber category:cs.CL published:2015-04-24 summary:In recent years, numerous studies have inferred personality and other traitsfrom people's online writing. While these studies are encouraging, moreinformation is needed in order to use these techniques with confidence. How dolinguistic features vary across different online media, and how much text isrequired to have a representative sample for a person? In this paper, weexamine several large sets of online, user-generated text, drawn from Twitter,email, blogs, and online discussion forums. We examine and comparepopulation-wide results for the linguistic measure LIWC, and the inferredtraits of Big5 Personality and Basic Human Values. We also empirically measurethe stability of these traits across different sized samples for eachindividual. Our results highlight the importance of tuning models to eachonline medium, and include guidelines for the minimum amount of text requiredfor a representative result.
arxiv-11100-80 | Local Variation as a Statistical Hypothesis Test | http://arxiv.org/abs/1504.06507 | author:Michael Baltaxe, Peter Meer, Michael Lindenbaum category:cs.CV published:2015-04-24 summary:The goal of image oversegmentation is to divide an image into several pieces,each of which should ideally be part of an object. One of the simplest and yetmost effective oversegmentation algorithms is known as local variation (LV)(Felzenszwalb and Huttenlocher 2004). In this work, we study this algorithm andshow that algorithms similar to LV can be devised by applying differentstatistical models and decisions, thus providing further theoreticaljustification and a well-founded explanation for the unexpected highperformance of the LV approach. Some of these algorithms are based onstatistics of natural images and on a hypothesis testing decision; we denotethese algorithms probabilistic local variation (pLV). The best pLV algorithm,which relies on censored estimation, presents state-of-the-art results whilekeeping the same computational complexity of the LV algorithm.
arxiv-11100-81 | Semantic Motion Segmentation Using Dense CRF Formulation | http://arxiv.org/abs/1504.06587 | author:N. Dinesh Reddy, Prateek Singhal, K. Madhava Krishna category:cs.CV published:2015-04-24 summary:While the literature has been fairly dense in the areas of sceneunderstanding and semantic labeling there have been few works that make use ofmotion cues to embellish semantic performance and vice versa. In this paper, weaddress the problem of semantic motion segmentation, and show how semantic andmotion priors augments performance. We pro- pose an algorithm that jointlyinfers the semantic class and motion labels of an object. Integrating semantic,geometric and optical ow based constraints into a dense CRF-model we infer boththe object class as well as motion class, for each pixel. We found improvementin performance using a fully connected CRF as compared to a standardclique-based CRFs. For inference, we use a Mean Field approximation basedalgorithm. Our method outperforms recently pro- posed motion detectionalgorithms and also improves the semantic labeling compared to thestate-of-the-art Automatic Labeling Environment algorithm on the challengingKITTI dataset especially for object classes such as pedestrians and cars thatare critical to an outdoor robotic navigation scenario.
arxiv-11100-82 | Sampling Correctors | http://arxiv.org/abs/1504.06544 | author:Clément Canonne, Themis Gouleakis, Ronitt Rubinfeld category:cs.DS cs.LG math.PR published:2015-04-24 summary:In many situations, sample data is obtained from a noisy or imperfect source.In order to address such corruptions, this paper introduces the concept of asampling corrector. Such algorithms use structure that the distribution ispurported to have, in order to allow one to make "on-the-fly" corrections tosamples drawn from probability distributions. These algorithms then act asfilters between the noisy data and the end user. We show connections between sampling correctors, distribution learningalgorithms, and distribution property testing algorithms. We show that theseconnections can be utilized to expand the applicability of known distributionlearning and property testing algorithms as well as to achieve improvedalgorithms for those tasks. As a first step, we show how to design sampling correctors using properlearning algorithms. We then focus on the question of whether algorithms forsampling correctors can be more efficient in terms of sample complexity thanlearning algorithms for the analogous families of distributions. Whencorrecting monotonicity, we show that this is indeed the case when also grantedquery access to the cumulative distribution function. We also obtain samplingcorrectors for monotonicity without this stronger type of access, provided thatthe distribution be originally very close to monotone (namely, at a distance$O(1/\log^2 n)$). In addition to that, we consider a restricted error modelthat aims at capturing "missing data" corruptions. In this model, we show thatdistributions that are close to monotone have sampling correctors that aresignificantly more efficient than achievable by the learning approach. We also consider the question of whether an additional source of independentrandom bits is required by sampling correctors to implement the correctionprocess.
arxiv-11100-83 | Object Level Deep Feature Pooling for Compact Image Representation | http://arxiv.org/abs/1504.06591 | author:Konda Reddy Mopuri, R. Venkatesh Babu category:cs.CV published:2015-04-24 summary:Convolutional Neural Network (CNN) features have been successfully employedin recent works as an image descriptor for various vision tasks. But theinability of the deep CNN features to exhibit invariance to geometrictransformations and object compositions poses a great challenge for imagesearch. In this work, we demonstrate the effectiveness of the objectness priorover the deep CNN features of image regions for obtaining an invariant imagerepresentation. The proposed approach represents the image as a vector ofpooled CNN features describing the underlying objects. This representationprovides robustness to spatial layout of the objects in the scene and achievesinvariance to general geometric transformations, such as translation, rotationand scaling. The proposed approach also leads to a compact representation ofthe scene, making each image occupy a smaller memory footprint. Experimentsshow that the proposed representation achieves state of the art retrievalresults on a set of challenging benchmark image datasets, while maintaining acompact representation.
arxiv-11100-84 | Depth-based hand pose estimation: methods, data, and challenges | http://arxiv.org/abs/1504.06378 | author:James Steven Supancic III, Gregory Rogez, Yi Yang, Jamie Shotton, Deva Ramanan category:cs.CV published:2015-04-24 summary:Hand pose estimation has matured rapidly in recent years. The introduction ofcommodity depth sensors and a multitude of practical applications have spurrednew advances. We provide an extensive analysis of the state-of-the-art,focusing on hand pose estimation from a single depth frame. To do so, we haveimplemented a considerable number of systems, and will release all software andevaluation code. We summarize important conclusions here: (1) Pose estimationappears roughly solved for scenes with isolated hands. However, methods stillstruggle to analyze cluttered scenes where hands may be interacting with nearbyobjects and surfaces. To spur further progress we introduce a challenging newdataset with diverse, cluttered scenes. (2) Many methods evaluate themselveswith disparate criteria, making comparisons difficult. We define a consistentevaluation criteria, rigorously motivated by human experiments. (3) Weintroduce a simple nearest-neighbor baseline that outperforms most existingsystems. This implies that most systems do not generalize beyond their trainingsets. This also reinforces the under-appreciated point that training data is asimportant as the model itself. We conclude with directions for future progress.
arxiv-11100-85 | A Bayesian approach for structure learning in oscillating regulatory networks | http://arxiv.org/abs/1504.06553 | author:D Trejo, AJ Millar, G Sanguinetti category:stat.ML q-bio.QM published:2015-04-24 summary:Oscillations lie at the core of many biological processes, from the cellcycle, to circadian oscillations and developmental processes. Time-keepingmechanisms are essential to enable organisms to adapt to varying conditions inenvironmental cycles, from day/night to seasonal. Transcriptional regulatorynetworks are one of the mechanisms behind these biological oscillations.However, while identifying cyclically expressed genes from time seriesmeasurements is relatively easy, determining the structure of the interactionnetwork underpinning the oscillation is a far more challenging problem. Here,we explicitly leverage the oscillatory nature of the transcriptional signalsand present a method for reconstructing network interactions tailored to thisspecial but important class of genetic circuits. Our method is based onprojecting the signal onto a set of oscillatory basis functions using aDiscrete Fourier Transform. We build a Bayesian Hierarchical model within afrequency domain linear model in order to enforce sparsity and incorporateprior knowledge about the network structure. Experiments on real and simulateddata show that the method can lead to substantial improvements over competingapproaches if the oscillatory assumption is met, and remains competitive alsoin cases it is not.
arxiv-11100-86 | WxBS: Wide Baseline Stereo Generalizations | http://arxiv.org/abs/1504.06603 | author:Dmytro Mishkin, Jiri Matas, Michal Perdoch, Karel Lenc category:cs.CV published:2015-04-24 summary:We have presented a new problem -- the wide multiple baseline stereo (WxBS)-- which considers matching of images that simultaneously differ in more thanone image acquisition factor such as viewpoint, illumination, sensor type orwhere object appearance changes significantly, e.g. over time. A new datasetwith the ground truth for evaluation of matching algorithms has been introducedand will be made public. We have extensively tested a large set of popular and recent detectors anddescriptors and show than the combination of RootSIFT and HalfRootSIFT asdescriptors with MSER and Hessian-Affine detectors works best for manydifferent nuisance factors. We show that simple adaptive thresholding improvesHessian-Affine, DoG, MSER (and possibly other) detectors and allows to use themon infrared and low contrast images. A novel matching algorithm for addressing the WxBS problem has beenintroduced. We have shown experimentally that the WxBS-M matcher dominantes thestate-of-the-art methods both on both the new and existing datasets.
arxiv-11100-87 | Learning Dictionaries for Named Entity Recognition using Minimal Supervision | http://arxiv.org/abs/1504.06650 | author:Arvind Neelakantan, Michael Collins category:cs.CL stat.ML published:2015-04-24 summary:This paper describes an approach for automatic construction of dictionariesfor Named Entity Recognition (NER) using large amounts of unlabeled data and afew seed examples. We use Canonical Correlation Analysis (CCA) to obtain lowerdimensional embeddings (representations) for candidate phrases and classifythese phrases using a small number of labeled examples. Our method achieves16.5% and 11.3% F-1 score improvement over co-training on disease and virus NERrespectively. We also show that by adding candidate phrase embeddings asfeatures in a sequence tagger gives better performance compared to using wordembeddings.
arxiv-11100-88 | Classifying Relations by Ranking with Convolutional Neural Networks | http://arxiv.org/abs/1504.06580 | author:Cicero Nogueira dos Santos, Bing Xiang, Bowen Zhou category:cs.CL cs.LG cs.NE published:2015-04-24 summary:Relation classification is an important semantic processing task for whichstate-ofthe-art systems still rely on costly handcrafted features. In this workwe tackle the relation classification task using a convolutional neural networkthat performs classification by ranking (CR-CNN). We propose a new pairwiseranking loss function that makes it easy to reduce the impact of artificialclasses. We perform experiments using the the SemEval-2010 Task 8 dataset,which is designed for the task of classifying the relationship between twonominals marked in a sentence. Using CRCNN, we outperform the state-of-the-artfor this dataset and achieve a F1 of 84.1 without using any costly handcraftedfeatures. Additionally, our experimental results show that: (1) our approach ismore effective than CNN followed by a softmax classifier; (2) omitting therepresentation of the artificial class Other improves both precision andrecall; and (3) using only word embeddings as input features is enough toachieve state-of-the-art results if we consider only the text between the twotarget nominals.
arxiv-11100-89 | Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods | http://arxiv.org/abs/1504.06658 | author:Arvind Neelakantan, Ming-Wei Chang category:cs.CL stat.ML published:2015-04-24 summary:Most of previous work in knowledge base (KB) completion has focused on theproblem of relation extraction. In this work, we focus on the task of inferringmissing entity type instances in a KB, a fundamental task for KB competitionyet receives little attention. Due to the novelty of this task, we construct alarge-scale dataset and design an automatic evaluation methodology. Ourknowledge base completion method uses information within the existing KB andexternal information from Wikipedia. We show that individual methods trainedwith a global objective that considers unobserved cells from both the entityand the type side gives consistently higher quality predictions compared tobaseline methods. We also perform manual evaluation on a small subset of thedata to verify the effectiveness of our knowledge base completion methods andthe correctness of our proposed automatic evaluation method.
arxiv-11100-90 | Compositional Vector Space Models for Knowledge Base Completion | http://arxiv.org/abs/1504.06662 | author:Arvind Neelakantan, Benjamin Roth, Andrew McCallum category:cs.CL stat.ML published:2015-04-24 summary:Knowledge base (KB) completion adds new facts to a KB by making inferencesfrom existing facts, for example by inferring with high likelihoodnationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hoprelational synonyms like this, or use as evidence a multi-hop relational pathtreated as an atomic feature, like bornIn(X,Z) -> containedIn(Z,Y). This paperpresents an approach that reasons about conjunctions of multi-hop relationsnon-atomically, composing the implications of a path using a recursive neuralnetwork (RNN) that takes as inputs vector embeddings of the binary relation inthe path. Not only does this allow us to generalize to paths unseen at trainingtime, but also, with a single high-capacity RNN, to predict new relation typesnot seen when the compositional model was trained (zero-shot learning). Weassemble a new dataset of over 52M relational triples, and show that our methodimproves over a traditional classifier by 11%, and a method leveragingpre-trained embeddings by 7%.
arxiv-11100-91 | Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space | http://arxiv.org/abs/1504.06654 | author:Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, Andrew McCallum category:cs.CL stat.ML published:2015-04-24 summary:There is rising interest in vector-space word embeddings and their use inNLP, especially given recent methods for their fast estimation at very largescale. Nearly all this work, however, assumes a single vector per word typeignoring polysemy and thus jeopardizing their usefulness for downstream tasks.We present an extension to the Skip-gram model that efficiently learns multipleembeddings per word type. It differs from recent related work by jointlyperforming word sense discrimination and embedding learning, bynon-parametrically estimating the number of senses per word type, and by itsefficiency and scalability. We present new state-of-the-art results in the wordsimilarity in context task and demonstrate its scalability by training with onemachine on a corpus of nearly 1 billion tokens in less than 6 hours.
arxiv-11100-92 | Using Syntax-Based Machine Translation to Parse English into Abstract Meaning Representation | http://arxiv.org/abs/1504.06665 | author:Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan May category:cs.CL cs.AI I.2.7 published:2015-04-24 summary:We present a parser for Abstract Meaning Representation (AMR). We treatEnglish-to-AMR conversion within the framework of string-to-tree, syntax-basedmachine translation (SBMT). To make this work, we transform the AMR structureinto a form suitable for the mechanics of SBMT and useful for modeling. Weintroduce an AMR-specific language model and add data and features drawn fromsemantic resources. Our resulting AMR parser improves upon state-of-the-artresults by 7 Smatch points.
arxiv-11100-93 | Situational Object Boundary Detection | http://arxiv.org/abs/1504.06434 | author:Jasper Uijlings, Vittorio Ferrari category:cs.CV published:2015-04-24 summary:Intuitively, the appearance of true object boundaries varies from image toimage. Hence the usual monolithic approach of training a single boundarypredictor and applying it to all images regardless of their content is bound tobe suboptimal. In this paper we therefore propose situational object boundarydetection: We first define a variety of situations and train a specializedobject boundary detector for each of them using [Dollar and Zitnick 2013]. Thengiven a test image, we classify it into these situations using its context,which we model by global image appearance. We apply the correspondingsituational object boundary detectors, and fuse them based on theclassification probabilities. In experiments on ImageNet, Microsoft COCO, andPascal VOC 2012 segmentation we show that our situational object boundarydetection gives significant improvements over a monolithic approach.Additionally, our method substantially outperforms [Hariharan et al. 2011] onsemantic contour detection on their SBD dataset.
arxiv-11100-94 | Cultural Event Recognition with Visual ConvNets and Temporal Models | http://arxiv.org/abs/1504.06567 | author:Amaia Salvador, Matthias Zeppelzauer, Daniel Manchon-Vizuete, Andrea Calafell, Xavier Giro-i-Nieto category:cs.CV cs.CY published:2015-04-24 summary:This paper presents our contribution to the ChaLearn Challenge 2015 onCultural Event Classification. The challenge in this task is to automaticallyclassify images from 50 different cultural events. Our solution is based on thecombination of visual features extracted from convolutional neural networkswith temporal information using a hierarchical classifier scheme. We extractvisual features from the last three fully connected layers of both CaffeNet(pretrained with ImageNet) and our fine tuned version for the ChaLearnchallenge. We propose a late fusion strategy that trains a separate low-levelSVM on each of the extracted neural codes. The class predictions of thelow-level SVMs form the input to a higher level SVM, which gives the finalevent scores. We achieve our best result by adding a temporal refinement stepinto our classification scheme, which is applied directly to the output of eachlow-level SVM. Our approach penalizes high classification scores based onvisual features when their time stamp does not match well an event-specifictemporal distribution learned from the training and validation data. Our systemachieved the second best result in the ChaLearn Challenge 2015 on CulturalEvent Classification with a mean average precision of 0.767 on the test set.
arxiv-11100-95 | Handling oversampling in dynamic networks using link prediction | http://arxiv.org/abs/1504.06667 | author:Benjamin Fish, Rajmonda S. Caceres category:cs.SI cs.LG physics.soc-ph published:2015-04-24 summary:Oversampling is a common characteristic of data representing dynamicnetworks. It introduces noise into representations of dynamic networks, butthere has been little work so far to compensate for it. Oversampling can affectthe quality of many important algorithmic problems on dynamic networks,including link prediction. Link prediction seeks to predict edges that will beadded to the network given previous snapshots. We show that not only doesoversampling affect the quality of link prediction, but that we can use linkprediction to recover from the effects of oversampling. We also introduce anovel generative model of noise in dynamic networks that representsoversampling. We demonstrate the results of our approach on both synthetic andreal-world data.
arxiv-11100-96 | Use of Ensembles of Fourier Spectra in Capturing Recurrent Concepts in Data Streams | http://arxiv.org/abs/1504.06366 | author:Sripirakas Sakthithasan, Russel Pears, Albert Bifet, Bernhard Pfahringer category:cs.AI cs.LG published:2015-04-23 summary:In this research, we apply ensembles of Fourier encoded spectra to captureand mine recurring concepts in a data stream environment. Previous researchshowed that compact versions of Decision Trees can be obtained by applying theDiscrete Fourier Transform to accurately capture recurrent concepts in a datastream. However, in highly volatile environments where new concepts emergeoften, the approach of encoding each concept in a separate spectrum is nolonger viable due to memory overload and thus in this research we present anensemble approach that addresses this problem. Our empirical results on realworld data and synthetic data exhibiting varying degrees of recurrence revealthat the ensemble approach outperforms the single spectrum approach in terms ofclassification accuracy, memory and execution time.
arxiv-11100-97 | Graphical Fermat's Principle and Triangle-Free Graph Estimation | http://arxiv.org/abs/1504.06026 | author:Junwei Lu, Han Liu category:stat.ML published:2015-04-23 summary:We consider the problem of estimating undirected triangle-free graphs of highdimensional distributions. Triangle-free graphs form a rich graph family whichallows arbitrary loopy structures but 3-cliques. For inferential tractability,we propose a graphical Fermat's principle to regularize the distributionfamily. Such principle enforces the existence of a distribution-dependentpseudo-metric such that any two nodes have a smaller distance than that of twoother nodes who have a geodesic path include these two nodes. Guided by thisprinciple, we show that a greedy strategy is able to recover the true graph.The resulting algorithm only requires a pairwise distance matrix as input andis computationally even more efficient than calculating the minimum spanningtree. We consider graph estimation problems under different settings, includingdiscrete and nonparametric distribution families. Thorough numerical resultsare provided to illustrate the usefulness of the proposed method.
arxiv-11100-98 | On the Runtime of Randomized Local Search and Simple Evolutionary Algorithms for Dynamic Makespan Scheduling | http://arxiv.org/abs/1504.06363 | author:Frank Neumann, Carsten Witt category:cs.DS cs.NE published:2015-04-23 summary:Evolutionary algorithms have been frequently used for dynamic optimizationproblems. With this paper, we contribute to the theoretical understanding ofthis research area. We present the first computational complexity analysis ofevolutionary algorithms for a dynamic variant of a classical combinatorialoptimization problem, namely makespan scheduling. We study the model of astrong adversary which is allowed to change one job at regular intervals.Furthermore, we investigate the setting of random changes. Our results showthat randomized local search and a simple evolutionary algorithm are veryeffective in dynamically tracking changes made to the problem instance.
arxiv-11100-99 | Strategic Teaching and Learning in Games | http://arxiv.org/abs/1504.06341 | author:Burkhard C. Schipper category:cs.GT cs.AI cs.LG published:2015-04-23 summary:It is known that there are uncoupled learning heuristics leading to Nashequilibrium in all finite games. Why should players use such learningheuristics and where could they come from? We show that there is no uncoupledlearning heuristic leading to Nash equilibrium in all finite games that aplayer has an incentive to adopt, that would be evolutionary stable or thatcould "learn itself". Rather, a player has an incentive to strategically teachsuch a learning opponent in order secure at least the Stackelberg leaderpayoff. The impossibility result remains intact when restricted to the classesof generic games, two-player games, potential games, games with strategiccomplements or 2x2 games, in which learning is known to be "nice". Moregenerally, it also applies to uncoupled learning heuristics leading tocorrelated equilibria, rationalizable outcomes, iterated admissible outcomes,or minimal curb sets. A possibility result restricted to "strategicallytrivial" games fails if some generic games outside this class are considered aswell.
arxiv-11100-100 | Analysis of Stopping Active Learning based on Stabilizing Predictions | http://arxiv.org/abs/1504.06329 | author:Michael Bloodgood, John Grothendieck category:cs.LG cs.CL stat.ML published:2015-04-23 summary:Within the natural language processing (NLP) community, active learning hasbeen widely investigated and applied in order to alleviate the annotationbottleneck faced by developers of new NLP systems and technologies. This paperpresents the first theoretical analysis of stopping active learning based onstabilizing predictions (SP). The analysis has revealed three elements that arecentral to the success of the SP method: (1) bounds on Cohen's Kappa agreementbetween successively trained models impose bounds on differences in F-measureperformance of the models; (2) since the stop set does not have to be labeled,it can be made large in practice, helping to guarantee that the resultstransfer to previously unseen streams of examples at test/application time; and(3) good (low variance) sample estimates of Kappa between successive models canbe obtained. Proofs of relationships between the level of Kappa agreement andthe difference in performance between consecutive models are presented.Specifically, if the Kappa agreement between two models exceeds a threshold T(where $T>0$), then the difference in F-measure performance between thosemodels is bounded above by $\frac{4(1-T)}{T}$ in all cases. If precision of thepositive conjunction of the models is assumed to be $p$, then the bound can betightened to $\frac{4(1-T)}{(p+1)T}$.
arxiv-11100-101 | Regularization-free estimation in trace regression with symmetric positive semidefinite matrices | http://arxiv.org/abs/1504.06305 | author:Martin Slawski, Ping Li, Matthias Hein category:stat.ML cs.LG stat.ME published:2015-04-23 summary:Over the past few years, trace regression models have received considerableattention in the context of matrix completion, quantum state tomography, andcompressed sensing. Estimation of the underlying matrix fromregularization-based approaches promoting low-rankedness, notably nuclear normregularization, have enjoyed great popularity. In the present paper, we arguethat such regularization may no longer be necessary if the underlying matrix issymmetric positive semidefinite (\textsf{spd}) and the design satisfies certainconditions. In this situation, simple least squares estimation subject to an\textsf{spd} constraint may perform as well as regularization-based approacheswith a proper choice of the regularization parameter, which entails knowledgeof the noise level and/or tuning. By contrast, constrained least squaresestimation comes without any tuning parameter and may hence be preferred due toits simplicity.
arxiv-11100-102 | Edge Detection Based on Global and Local Parameters of the Image | http://arxiv.org/abs/1504.06036 | author:Andrew F. C. Brustolin category:cs.CV I.4.6 published:2015-04-23 summary:This paper presents an edge detection method based on global and localparameters of the image, which produces satisfactory results on the edgedetection of complex images and has a simple structure for execution. The localand global parameters of the image are arithmetic means and standarddeviations, the former acquired from a three sized window representing fivepixels, the latter acquired from the entire row or column. We obtain thedifferences of grayscale intensities between two adjacent pixels and the sum ofthe modulus of these differences from the horizontal and vertical scans of theimage. Using these obtained values, we calculate the local and globalparameters. After the gathering of the local and global parameters, we compareeach sum of the modulus of differences with its own local and global parameter.In the case of the comparison is true, the consecutive pixel to the modulus sumof differences index is marked as an edge. We present the results of the testswith grayscale images using different parameters and discuss the advantages anddisadvantages of each parameter value and algorithm structure chosen on theedge processing. There is a comparison of results between this papers detectorand Canny, where we evaluate the quality of the presented detector.
arxiv-11100-103 | Stability of Stochastic Approximations with `Controlled Markov' Noise and Temporal Difference Learning | http://arxiv.org/abs/1504.06043 | author:Arunselvan Ramaswamy, Shalabh Bhatnagar category:cs.SY stat.ML published:2015-04-23 summary:In this paper we present a `stability theorem' for stochastic approximation(SA) algorithms with `controlled Markov' noise. Such algorithms were firststudied by Borkar in 2006. Specifically, sufficient conditions are presentedwhich guarantee the stability of the iterates. Further, under these conditionsthe iterates are shown to track a solution to the differential inclusiondefined in terms of the ergodic occupation measures associated with the`controlled Markov' process. As an application to our main result we present animprovement to a general form of temporal difference learning algorithms.Specifically, we present sufficient conditions for their stability andconvergence using our framework. This paper builds on the works of Borkar aswell as Benveniste, Metivier and Priouret.
arxiv-11100-104 | Understanding and Diagnosing Visual Tracking Systems | http://arxiv.org/abs/1504.06055 | author:Naiyan Wang, Jianping Shi, Dit-Yan Yeung, Jiaya Jia category:cs.CV published:2015-04-23 summary:Several benchmark datasets for visual tracking research have been proposed inrecent years. Despite their usefulness, whether they are sufficient forunderstanding and diagnosing the strengths and weaknesses of different trackersremains questionable. To address this issue, we propose a framework by breakinga tracker down into five constituent parts, namely, motion model, featureextractor, observation model, model updater, and ensemble post-processor. Wethen conduct ablative experiments on each component to study how it affects theoverall result. Surprisingly, our findings are discrepant with some commonbeliefs in the visual tracking research community. We find that the featureextractor plays the most important role in a tracker. On the other hand,although the observation model is the focus of many studies, we find that itoften brings no significant improvement. Moreover, the motion model and modelupdater contain many details that could affect the result. Also, the ensemblepost-processor can improve the result substantially when the constituenttrackers have high diversity. Based on our findings, we put together some veryelementary building blocks to give a basic tracker which is competitive inperformance to the state-of-the-art trackers. We believe our framework canprovide a solid baseline when conducting controlled experiments for visualtracking research.
arxiv-11100-105 | A new approach for physiological time series | http://arxiv.org/abs/1504.06274 | author:Dong Mao, Yang Wang, Qiang Wu category:cs.LG stat.ML published:2015-04-23 summary:We developed a new approach for the analysis of physiological time series. Aniterative convolution filter is used to decompose the time series into variouscomponents. Statistics of these components are extracted as features tocharacterize the mechanisms underlying the time series. Motivated by thestudies that show many normal physiological systems involve irregularity whilethe decrease of irregularity usually implies the abnormality, the statisticsfor "outliers" in the components are used as features measuring irregularity.Support vector machines are used to select the most relevant features that areable to differentiate the time series from normal and abnormal systems. Thisnew approach is successfully used in the study of congestive heart failure byheart beat interval time series.
arxiv-11100-106 | Object Detection Networks on Convolutional Feature Maps | http://arxiv.org/abs/1504.06066 | author:Shaoqing Ren, Kaiming He, Ross Girshick, Xiangyu Zhang, Jian Sun category:cs.CV published:2015-04-23 summary:Most object detectors contain two important components: a feature extractorand an object classifier. The feature extractor has rapidly evolved withsignificant research efforts leading to better deep ConvNet architectures. Theobject classifier, however, has not received much attention and moststate-of-the-art systems (like R-CNN) use simple multi-layer perceptrons. Thispaper demonstrates that carefully designing deep networks for objectclassification is just as important. We take inspiration from traditionalobject classifiers, such as DPM, and experiment with deep networks that havepart-like filters and reason over latent variables. We discover that onpre-trained convolutional feature maps, even randomly initialized deepclassifiers produce excellent results, while the improvement due to fine-tuningis secondary; on HOG features, deep classifiers outperform DPMs and produce thebest HOG-only results without external data. We believe these findings providenew insight for developing object detection systems. Our framework, calledNetworks on Convolutional feature maps (NoC), achieves outstanding results onthe PASCAL VOC 2007 (73.3% mAP) and 2012 (68.8% mAP) benchmarks.
arxiv-11100-107 | Multimodal Convolutional Neural Networks for Matching Image and Sentence | http://arxiv.org/abs/1504.06063 | author:Lin Ma, Zhengdong Lu, Lifeng Shang, Hang Li category:cs.CV cs.CL cs.NE published:2015-04-23 summary:In this paper, we propose multimodal convolutional neural networks (m-CNNs)for matching image and sentence. Our m-CNN provides an end-to-end frameworkwith convolutional architectures to exploit image representation, wordcomposition, and the matching relations between the two modalities. Morespecifically, it consists of one image CNN encoding the image content, and onematching CNN learning the joint representation of image and sentence. Thematching CNN composes words to different semantic fragments and learns theinter-modal relations between image and the composed fragments at differentlevels, thus fully exploit the matching relations between image and sentence.Experimental results on benchmark databases of bidirectional image and sentenceretrieval demonstrate that the proposed m-CNNs can effectively capture theinformation necessary for image and sentence matching. Specifically, ourproposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K andMicrosoft COCO databases achieve the state-of-the-art performances.
arxiv-11100-108 | Open Data Platform for Knowledge Access in Plant Health Domain : VESPA Mining | http://arxiv.org/abs/1504.06077 | author:Nicolas Turenne, Mathieu Andro, Roselyne Corbière, Tien T. Phan category:cs.IR cs.CL published:2015-04-23 summary:Important data are locked in ancient literature. It would be uneconomic toproduce these data again and today or to extract them without the help of textmining technologies. Vespa is a text mining project whose aim is to extractdata on pest and crops interactions, to model and predict attacks on crops, andto reduce the use of pesticides. A few attempts proposed an agriculturalinformation access. Another originality of our work is to parse documents witha dependency of the document architecture.
arxiv-11100-109 | First Steps Towards a Runtime Comparison of Natural and Artificial Evolution | http://arxiv.org/abs/1504.06260 | author:Tiago Paixão, Jorge Pérez Heredia, Dirk Sudholt, Barbora Trubenová category:cs.NE F.2.2 published:2015-04-23 summary:Evolutionary algorithms (EAs) form a popular optimisation paradigm inspiredby natural evolution. In recent years the field of evolutionary computation hasdeveloped a rigorous analytical theory to analyse their runtime on manyillustrative problems. Here we apply this theory to a simple model of naturalevolution. In the Strong Selection Weak Mutation (SSWM) evolutionary regime thetime between occurrence of new mutations is much longer than the time it takesfor a new beneficial mutation to take over the population. In this situation,the population only contains copies of one genotype and evolution can bemodelled as a (1+1)-type process where the probability of accepting a newgenotype (improvements or worsenings) depends on the change in fitness. We present an initial runtime analysis of SSWM, quantifying its performancefor various parameters and investigating differences to the (1+1)EA. We showthat SSWM can have a moderate advantage over the (1+1)EA at crossing fitnessvalleys and study an example where SSWM outperforms the (1+1)EA by takingadvantage of information on the fitness gradient.
arxiv-11100-110 | Evolving Fuzzy Image Segmentation with Self-Configuration | http://arxiv.org/abs/1504.06266 | author:Ahmed Othman, Hamid R. Tizhoosh, Farzad Khalvati category:cs.CV published:2015-04-23 summary:Current image segmentation techniques usually require that the user tuneseveral parameters in order to obtain maximum segmentation accuracy, acomputationally inefficient approach, especially when a large number of imagesmust be processed sequentially in daily practice. The use of evolving fuzzysystems for designing a method that automatically adjusts parameters to segmentmedical images according to the quality expectation of expert users has beenproposed recently (Evolving fuzzy image segmentation EFIS). However, EFISsuffers from a few limitations when used in practice mainly due to some fixedparameters. For instance, EFIS depends on auto-detection of the object ofinterest for feature calculation, a task that is highly application-dependent.This shortcoming limits the applicability of EFIS, which was proposed with theultimate goal of offering a generic but adjustable segmentation scheme. In thispaper, a new version of EFIS is proposed to overcome these limitations. The newEFIS, called self-configuring EFIS (SC-EFIS), uses available training data toself-estimate the parameters that are fixed in EFIS. As well, the proposedSC-EFIS relies on a feature selection process that does not requireauto-detection of an ROI. The proposed SC-EFIS was evaluated using the samesegmentation algorithms and the same dataset as for EFIS. The results show thatSC-EFIS can provide the same results as EFIS but with a higher level ofautomation.
arxiv-11100-111 | Online Adaptive Hidden Markov Model for Multi-Tracker Fusion | http://arxiv.org/abs/1504.06103 | author:Tomas Vojir, Jiri Matas, Jana Noskova category:cs.CV published:2015-04-23 summary:In this paper, we propose a novel method for visual object tracking calledHMMTxD. The method fuses observations from complementary out-of-the boxtrackers and a detector by utilizing a hidden Markov model whose latent statescorrespond to a binary vector expressing the failure of individual trackers.The Markov model is trained in an unsupervised way, relying on an onlinelearned detector to provide a source of tracker-independent information for amodified Baum- Welch algorithm that updates the model w.r.t. the partiallyannotated data. We show the effectiveness of the proposed method on combination of two andthree tracking algorithms. The performance of HMMTxD is evaluated on twostandard benchmarks (CVPR2013 and VOT) and on a rich collection of 77 publiclyavailable sequences. The HMMTxD outperforms the state-of-the-art, oftensignificantly, on all datasets in almost all criteria.
arxiv-11100-112 | High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and its Applications to High-Level Vision | http://arxiv.org/abs/1504.06201 | author:Gedas Bertasius, Jianbo Shi, Lorenzo Torresani category:cs.CV published:2015-04-23 summary:Most of the current boundary detection systems rely exclusively on low-levelfeatures, such as color and texture. However, perception studies suggest thathumans employ object-level reasoning when judging if a particular pixel is aboundary. Inspired by this observation, in this work we show how to predictboundaries by exploiting object-level features from a pretrainedobject-classification network. Our method can be viewed as a "High-for-Low"approach where high-level object features inform the low-level boundarydetection process. Our model achieves state-of-the-art performance on anestablished boundary detection benchmark and it is efficient to run. Additionally, we show that due to the semantic nature of our boundaries wecan use them to aid a number of high-level vision tasks. We demonstrate thatusing our boundaries we improve the performance of state-of-the-art methods onthe problems of semantic boundary labeling, semantic segmentation and objectproposal generation. We can view this process as a "Low-for-High" scheme, wherelow-level boundaries aid high-level vision tasks. Thus, our contributions include a boundary detection system that is accurate,efficient, generalizes well to multiple datasets, and is also shown to improveexisting state-of-the-art high-level vision methods on three distinct tasks.
arxiv-11100-113 | x.ent: R Package for Entities and Relations Extraction based on Unsupervised Learning and Document Structure | http://arxiv.org/abs/1504.06078 | author:Nicolas Turenne, Tien Phan category:cs.CL cs.AI published:2015-04-23 summary:Relation extraction with accurate precision is still a challenge whenprocessing full text databases. We propose an approach based on cooccurrenceanalysis in each document for which we used document organization to improveaccuracy of relation extraction. This approach is implemented in a R packagecalled \emph{x.ent}. Another facet of extraction relies on use of extractedrelation into a querying system for expert end-users. Two datasets had beenused. One of them gets interest from specialists of epidemiology in planthealth. For this dataset usage is dedicated to plant-disease explorationthrough agricultural information news. An open-data platform exploits exportsfrom \emph{x.ent} and is publicly available.
arxiv-11100-114 | svcR: An R Package for Support Vector Clustering improved with Geometric Hashing applied to Lexical Pattern Discovery | http://arxiv.org/abs/1504.06080 | author:Nicolas Turenne category:cs.LG cs.CL published:2015-04-23 summary:We present a new R package which takes a numerical matrix format as datainput, and computes clusters using a support vector clustering method (SVC). Wehave implemented an original 2D-grid labeling approach to speed up clusterextraction. In this sense, SVC can be seen as an efficient cluster extractionif clusters are separable in a 2-D map. Secondly we showed that this SVCapproach using a Jaccard-Radial base kernel can help to classify well enough aset of terms into ontological classes and help to define regular expressionrules for information extraction in documents; our case study concerns a set ofterms and documents about developmental and molecular biology.
arxiv-11100-115 | Person Re-identification with Correspondence Structure Learning | http://arxiv.org/abs/1504.06243 | author:Yang Shen, Weiyao Lin, Junchi Yan, Mingliang Xu, Jianxin Wu, Jingdong Wang category:cs.CV published:2015-04-23 summary:This paper addresses the problem of handling spatial misalignments due tocamera-view changes or human-pose variations in person re-identification. Wefirst introduce a boosting-based approach to learn a correspondence structurewhich indicates the patch-wise matching probabilities between images from atarget camera pair. The learned correspondence structure can not only capturethe spatial correspondence pattern between cameras but also handle theviewpoint or human-pose variation in individual images. We further introduce aglobal-based matching process. It integrates a global matching constraint overthe learned correspondence structure to exclude cross-view misalignments duringthe image patch matching process, hence achieving a more reliable matchingscore between images. Experimental results on various datasets demonstrate theeffectiveness of our approach.
arxiv-11100-116 | Sparse Radial Sampling LBP for Writer Identification | http://arxiv.org/abs/1504.06133 | author:Anguelos Nicolaou, Andrew D. Bagdanov, Marcus Liwicki, Dimosthenis Karatzas category:cs.CV published:2015-04-23 summary:In this paper we present the use of Sparse Radial Sampling Local BinaryPatterns, a variant of Local Binary Patterns (LBP) for text-as-textureclassification. By adapting and extending the standard LBP operator to theparticularities of text we get a generic text-as-texture classification schemeand apply it to writer identification. In experiments on CVL and ICDAR 2013datasets, the proposed feature-set demonstrates State-Of-the-Art (SOA)performance. Among the SOA, the proposed method is the only one that is basedon dense extraction of a single local feature descriptor. This makes it fastand applicable at the earliest stages in a DIA pipeline without the need forsegmentation, binarization, or extraction of multiple features.
arxiv-11100-117 | Robust Principal Component Analysis on Graphs | http://arxiv.org/abs/1504.06151 | author:Nauman Shahid, Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre Vandergheynst category:cs.CV published:2015-04-23 summary:Principal Component Analysis (PCA) is the most widely used tool for lineardimensionality reduction and clustering. Still it is highly sensitive tooutliers and does not scale well with respect to the number of data samples.Robust PCA solves the first issue with a sparse penalty term. The second issuecan be handled with the matrix factorization model, which is howevernon-convex. Besides, PCA based clustering can also be enhanced by using a graphof data similarity. In this article, we introduce a new model called "RobustPCA on Graphs" which incorporates spectral graph regularization into the RobustPCA framework. Our proposed model benefits from 1) the robustness of principalcomponents to occlusions and missing values, 2) enhanced low-rank recovery, 3)improved clustering property due to the graph smoothness assumption on thelow-rank matrix, and 4) convexity of the resulting optimization problem.Extensive experiments on 8 benchmark, 3 video and 2 artificial datasets withcorruptions clearly reveal that our model outperforms 10 other state-of-the-artmodels in its clustering and low-rank recovery tasks.
arxiv-11100-118 | Collectively Embedding Multi-Relational Data for Predicting User Preferences | http://arxiv.org/abs/1504.06165 | author:Nitish Gupta, Sameer Singh category:cs.LG cs.IR published:2015-04-23 summary:Matrix factorization has found incredible success and widespread applicationas a collaborative filtering based approach to recommendations. Unfortunately,incorporating additional sources of evidence, especially ones that areincomplete and noisy, is quite difficult to achieve in such models, however, isoften crucial for obtaining further gains in accuracy. For example, additionalinformation about businesses from reviews, categories, and attributes should beleveraged for predicting user preferences, even though this information isoften inaccurate and partially-observed. Instead of creating customized methodsthat are specific to each type of evidences, in this paper we present a genericapproach to factorization of relational data that collectively models all therelations in the database. By learning a set of embeddings that are sharedacross all the relations, the model is able to incorporate observed informationfrom all the relations, while also predicting all the relations of interest.Our evaluation on multiple Amazon and Yelp datasets demonstrates effectiveutilization of additional information for held-out preference prediction, butfurther, we present accurate models even for the cold-starting businesses andproducts for which we do not observe any ratings or reviews. We also illustratethe capability of the model in imputing missing information and jointlyvisualizing words, categories, and attribute factors.
arxiv-11100-119 | An Elastic Image Registration Approach for Wireless Capsule Endoscope Localization | http://arxiv.org/abs/1504.06206 | author:Isabel N. Figueiredo, Carlos Leal, Luís Pinto, Pedro N. Figueiredo, Richard Tsai category:cs.CV published:2015-04-23 summary:Wireless Capsule Endoscope (WCE) is an innovative imaging device that permitsphysicians to examine all the areas of the Gastrointestinal (GI) tract. It isespecially important for the small intestine, where traditional invasiveendoscopies cannot reach. Although WCE represents an extremely importantadvance in medical imaging, a major drawback that remains unsolved is the WCEprecise location in the human body during its operating time. This is mainlydue to the complex physiological environment and the inherent capsule effectsduring its movement. When an abnormality is detected, in the WCE images,medical doctors do not know precisely where this abnormality is locatedrelative to the intestine and therefore they can not proceed efficiently withthe appropriate therapy. The primary objective of the present paper is to givea contribution to WCE localization, using image-based methods. The main focusof this work is on the description of a multiscale elastic image registrationapproach, its experimental application on WCE videos, and comparison with amultiscale affine registration. The proposed approach includes registrationsthat capture both rigid-like and non-rigid deformations, due respectively tothe rigid-like WCE movement and the elastic deformation of the small intestineoriginated by the GI peristaltic movement. Under this approach a qualitativeinformation about the WCE speed can be obtained, as well as the WCE locationand orientation via projective geometry. The results of the experimental testswith real WCE video frames show the good performance of the proposed approach,when elastic deformations of the small intestine are involved in successiveframes, and its superiority with respect to a multiscale affine imageregistration, which accounts for rigid-like deformations only and discardselastic deformations.
arxiv-11100-120 | Self-Tuned Deep Super Resolution | http://arxiv.org/abs/1504.05632 | author:Zhangyang Wang, Yingzhen Yang, Zhaowen Wang, Shiyu Chang, Wei Han, Jianchao Yang, Thomas S. Huang category:cs.LG cs.CV published:2015-04-22 summary:Deep learning has been successfully applied to image super resolution (SR).In this paper, we propose a deep joint super resolution (DJSR) model to exploitboth external and self similarities for SR. A Stacked Denoising ConvolutionalAuto Encoder (SDCAE) is first pre-trained on external examples with proper dataaugmentations. It is then fine-tuned with multi-scale self examples from eachinput, where the reliability of self examples is explicitly taken into account.We also enhance the model performance by sub-model training and selection. TheDJSR model is extensively evaluated and compared with state-of-the-arts, andshow noticeable performance improvements both quantitatively and perceptuallyon a wide range of images.
arxiv-11100-121 | Median and Mode Ellipse Parameterization for Robust Contour Fitting | http://arxiv.org/abs/1504.05623 | author:Michael A. Greminger category:cs.CV published:2015-04-22 summary:Problems that require the parameterization of closed contours arisefrequently in computer vision applications. This article introduces a new curveparameterization algorithm that is able to fit a closed curve to a set ofpoints while being robust to the presence of outliers and occlusions in thedata. This robustness property makes this algorithm applicable to computervision applications where misclassification of features may lead to outliers.The algorithm starts by fitting ellipses to numerous five point subsets fromthe source data. The closed curve is parameterized by determining the medianperimeter of the set of ellipses. The resulting curve is not an ellipse,allowing arbitrary closed contours to be parameterized. The use of the modalperimeter rather than the median perimeter is also explored. A detailedcomparison is made between the proposed curve fitting algorithm and existingrobust ellipse fitting algorithms. Finally, the utility of the algorithm forcomputer vision applications is demonstrated through the parameterization ofthe boundary of fuel droplets during combustion. The performance of theproposed algorithm and the performance of existing algorithms are compared to aground truth segmentation of the fuel droplet images, which demonstratesimproved performance for both area quantification and edge deviation.
arxiv-11100-122 | A Hierarchical Distance-dependent Bayesian Model for Event Coreference Resolution | http://arxiv.org/abs/1504.05929 | author:Bishan Yang, Claire Cardie, Peter Frazier category:cs.CL stat.ML published:2015-04-22 summary:We present a novel hierarchical distance-dependent Bayesian model for eventcoreference resolution. While existing generative models for event coreferenceresolution are completely unsupervised, our model allows for the incorporationof pairwise distances between event mentions -- information that is widely usedin supervised coreference models to guide the generative clustering processingfor better event clustering both within and across documents. We model thedistances between event mentions using a feature-rich learnable distancefunction and encode them as Bayesian priors for nonparametric clustering.Experiments on the ECB+ corpus show that our model outperforms state-of-the-artmethods for both within- and cross-document event coreference resolution.
arxiv-11100-123 | Rebuilding Factorized Information Criterion: Asymptotically Accurate Marginal Likelihood | http://arxiv.org/abs/1504.05665 | author:Kohei Hayashi, Shin-ichi Maeda, Ryohei Fujimaki category:cs.LG stat.ML published:2015-04-22 summary:Factorized information criterion (FIC) is a recently developed approximationtechnique for the marginal log-likelihood, which provides an automatic modelselection framework for a few latent variable models (LVMs) with tractableinference algorithms. This paper reconsiders FIC and fills theoretical gaps ofprevious FIC studies. First, we reveal the core idea of FIC that allowsgeneralization for a broader class of LVMs, including continuous LVMs, incontrast to previous FICs, which are applicable only to binary LVMs. Second, weinvestigate the model selection mechanism of the generalized FIC. Our analysisprovides a formal justification of FIC as a model selection criterion for LVMsand also a systematic procedure for pruning redundant latent variables thathave been removed heuristically in previous studies. Third, we provide aninterpretation of FIC as a variational free energy and uncover a fewpreviously-unknown their relationships. A demonstrative study on Bayesianprincipal component analysis is provided and numerical experiments support ourtheoretical results.
arxiv-11100-124 | Honeybees-inspired heuristic algorithms for numerical optimisation | http://arxiv.org/abs/1504.05766 | author:Muharrem Düğenci category:cs.NE published:2015-04-22 summary:Swarm intelligence is all about developing collective behaviours to solvecomplex, ill-structured and large-scale problems. Efficiency in collectivebehaviours depends on how to harmonise the individual contributions so that acomplementary collective effort can be achieved to offer a useful solution. Themain points in organising the harmony remains as managing the diversificationand intensification actions appropriately, where the efficiency of collectivebehaviours depends on blending these two actions appropriately. In this study,two swarm intelligence algorithms inspired of natural honeybee colonies havebeen overviewed with many respects and two new revisions and a hybrid versionhave been studied to improve the efficiencies in solving numerical optimisationproblems, which are well-known hard benchmarks. Consequently, the revisions andespecially the hybrid algorithm proposed have outperformed the two original beealgorithms in solving these very hard numerical optimisation benchmarks.
arxiv-11100-125 | On the Generalization Properties of Differential Privacy | http://arxiv.org/abs/1504.05800 | author:Kobbi Nissim, Uri Stemmer category:cs.LG cs.CR published:2015-04-22 summary:A new line of work, started with Dwork et al., studies the task of answeringstatistical queries using a sample and relates the problem to the concept ofdifferential privacy. By the Hoeffding bound, a sample of size $O(\logk/\alpha^2)$ suffices to answer $k$ non-adaptive queries within error $\alpha$,where the answers are computed by evaluating the statistical queries on thesample. This argument fails when the queries are chosen adaptively (and canhence depend on the sample). Dwork et al. showed that if the answers arecomputed with $(\epsilon,\delta)$-differential privacy then $O(\epsilon)$accuracy is guaranteed with probability $1-O(\delta^\epsilon)$. Using thePrivate Multiplicative Weights mechanism, they concluded that the sample sizecan still grow polylogarithmically with the $k$. Very recently, Bassily et al. presented an improved bound and showed that (avariant of) the private multiplicative weights algorithm can answer $k$adaptively chosen statistical queries using sample complexity that growslogarithmically in $k$. However, their results no longer hold for everydifferentially private algorithm, and require modifying the privatemultiplicative weights algorithm in order to obtain their high probabilitybounds. We greatly simplify the results of Dwork et al. and improve on the bound byshowing that differential privacy guarantees $O(\epsilon)$ accuracy withprobability $1-O(\delta\log(1/\epsilon)/\epsilon)$. It would be tempting toguess that an $(\epsilon,\delta)$-differentially private computation shouldguarantee $O(\epsilon)$ accuracy with probability $1-O(\delta)$. However, weshow that this is not the case, and that our bound is tight (up to logarithmicfactors).
arxiv-11100-126 | Rounding Methods for Neural Networks with Low Resolution Synaptic Weights | http://arxiv.org/abs/1504.05767 | author:Lorenz K. Muller, Giacomo Indiveri category:cs.NE published:2015-04-22 summary:Neural network algorithms simulated on standard computing platforms typicallymake use of high resolution weights, with floating-point notation. However, fordedicated hardware implementations of such algorithms, fixed-point synapticweights with low resolution are preferable. The basic approach of reducing theresolution of the weights in these algorithms by standard rounding methodsincurs drastic losses in performance. To reduce the resolution further, in theextreme case even to binary weights, more advanced techniques are necessary. Tothis end, we propose two methods for mapping neural network algorithms withhigh resolution weights to corresponding algorithms that work with lowresolution weights and demonstrate that their performance is substantiallybetter than standard rounding. We further use these methods to investigate theperformance of three common neural network algorithms under fixed memory sizeof the weight matrix with different weight resolutions. We show that dedicatedhardware systems, whose technology dictates very low weight resolutions (bethey electronic or biological) could in principle implement the algorithms westudy.
arxiv-11100-127 | On-the-fly Approximation of Multivariate Total Variation Minimization | http://arxiv.org/abs/1504.05854 | author:Jordan Frecon, Nelly Pustelnik, Patrice Abry, Laurent Condat category:cs.LG cs.NA math.OC published:2015-04-22 summary:In the context of change-point detection, addressed by Total Variationminimization strategies, an efficient on-the-fly algorithm has been designedleading to exact solutions for univariate data. In this contribution, anextension of such an on-the-fly strategy to multivariate data is investigated.The proposed algorithm relies on the local validation of the Karush-Kuhn-Tuckerconditions on the dual problem. Showing that the non-local nature of themultivariate setting precludes to obtain an exact on-the-fly solution, wedevise an on-the-fly algorithm delivering an approximate solution, whosequality is controlled by a practitioner-tunable parameter, acting as atrade-off between quality and computational cost. Performance assessment showsthat high quality solutions are obtained on-the-fly while benefiting ofcomputational costs several orders of magnitude lower than standard iterativeprocedures. The proposed algorithm thus provides practitioners with anefficient multivariate change-point detection on-the-fly procedure.
arxiv-11100-128 | LOAD: Local Orientation Adaptive Descriptor for Texture and Material Classification | http://arxiv.org/abs/1504.05809 | author:Xianbiao Qi, Guoying Zhao, Linlin Shen, Qingquan Li, Matti Pietikainen category:cs.CV published:2015-04-22 summary:In this paper, we propose a novel local feature, called Local OrientationAdaptive Descriptor (LOAD), to capture regional texture in an image. In LOAD,we proposed to define point description on an Adaptive Coordinate System (ACS),adopt a binary sequence descriptor to capture relationships between one pointand its neighbors and use multi-scale strategy to enhance the discriminativepower of the descriptor. The proposed LOAD enjoys not only discriminative powerto capture the texture information, but also has strong robustness toillumination variation and image rotation. Extensive experiments on benchmarkdata sets of texture classification and real-world material recognition showthat the proposed LOAD yields the state-of-the-art performance. It is worth tomention that we achieve a 65.4\% classification accuracy-- which is, to thebest of our knowledge, the highest record by far --on Flickr Material Databaseby using a single feature. Moreover, by combining LOAD with the featureextracted by Convolutional Neural Networks (CNN), we obtain significantlybetter performance than both the LOAD and CNN. This result confirms that theLOAD is complementary to the learning-based features.
arxiv-11100-129 | Learning of Behavior Trees for Autonomous Agents | http://arxiv.org/abs/1504.05811 | author:Michele Colledanchise, Ramviyas Parasuraman, Petter Ögren category:cs.RO cs.AI cs.LG published:2015-04-22 summary:Definition of an accurate system model for Automated Planner (AP) is oftenimpractical, especially for real-world problems. Conversely, off-the-shelfplanners fail to scale up and are domain dependent. These drawbacks areinherited from conventional transition systems such as Finite State Machines(FSMs) that describes the action-plan execution generated by the AP. On theother hand, Behavior Trees (BTs) represent a valid alternative to FSMspresenting many advantages in terms of modularity, reactiveness, scalabilityand domain-independence. In this paper, we propose a model-free AP frameworkusing Genetic Programming (GP) to derive an optimal BT for an autonomous agentto achieve a given goal in unknown (but fully observable) environments. Weillustrate the proposed framework using experiments conducted with an opensource benchmark Mario AI for automated generation of BTs that can play thegame character Mario to complete a certain level at various levels ofdifficulty to include enemies and obstacles.
arxiv-11100-130 | Can Partial Strong Labels Boost Multi-label Object Recognition? | http://arxiv.org/abs/1504.05843 | author:Hao Yang, Joey Tianyi Zhou, Yu Zhang, Bin-Bin Gao, Jianxin Wu, Jianfei Cai category:cs.CV cs.LG published:2015-04-22 summary:Convolutional neural networks (CNN) have shown great performance as a globalrepresentation for object recognition. However, for multi-label images thatcontain multiple objects from different categories, scales and locations,single CNN features might not be be optimal. To enhance the robustness anddiscriminative power of CNN features for multi-label object recognitionproblem, we propose a multi-view multi-instance framework. This frameworktransforms the multi-label classification problem into a multi-classmulti-instance learning problem by extracting object proposals from images. Amulti-view pipeline is then applied to generate a two-view representation ofeach proposal by exploiting two levels of labels in multi-label recognitionproblem. The proposed framework not only has the flexibility of utilizing bothweak and strong labels or just weak labels, but also holds the generalizationability to boost the performance of unseen categories by available stronglabels. Our framework is extensively compared with state-of-the-arthand-crafted feature based and CNN based methods on two multi-label benchmarkdatasets. The experimental results validate the discriminative power andgeneralization ability of the proposed framework. When combined with avery-deep network, we can achieve state-of-the-art results in both datasets.
arxiv-11100-131 | On the relation between Gaussian process quadratures and sigma-point methods | http://arxiv.org/abs/1504.05994 | author:Simo Särkkä, Jouni Hartikainen, Lennart Svensson, Fredrik Sandblom category:stat.ME math.DS stat.ML published:2015-04-22 summary:This article is concerned with Gaussian process quadratures, which arenumerical integration methods based on Gaussian process regression methods, andsigma-point methods, which are used in advanced non-linear Kalman filtering andsmoothing algorithms. We show that many sigma-point methods can be interpretedas Gaussian quadrature based methods with suitably selected covariancefunctions. We show that this interpretation also extends to more generalmultivariate Gauss--Hermite integration methods and related spherical cubaturerules. Additionally, we discuss different criteria for selecting thesigma-point locations: exactness for multivariate polynomials up to a givenorder, minimum average error, and quasi-random point sets. The performance ofthe different methods is tested in numerical experiments.
arxiv-11100-132 | Combining local regularity estimation and total variation optimization for scale-free texture segmentation | http://arxiv.org/abs/1504.05776 | author:Nelly Pustelnik, Herwig Wendt, Patrice Abry, Nicolas Dobigeon category:cs.CV published:2015-04-22 summary:Texture segmentation constitutes a standard image processing task, crucial tomany applications. The present contribution focuses on the particular subset ofscale-free textures and its originality resides in the combination of three keyingredients: First, texture characterization relies on the concept of localregularity ; Second, estimation of local regularity is based on new multiscalequantities referred to as wavelet leaders ; Third, segmentation from localregularity faces a fundamental bias variance trade-off: In nature, localregularity estimation shows high variability that impairs the detection ofchanges, while a posteriori smoothing of regularity estimates precludes fromlocating correctly changes. Instead, the present contribution proposes severalvariational problem formulations based on total variation and proximalresolutions that effectively circumvent this trade-off. Estimation andsegmentation performance for the proposed procedures are quantified andcompared on synthetic as well as on real-world textures.
arxiv-11100-133 | Spectral Norm of Random Kernel Matrices with Applications to Privacy | http://arxiv.org/abs/1504.05880 | author:Shiva Prasad Kasiviswanathan, Mark Rudelson category:stat.ML cs.CR cs.LG F.2.1 published:2015-04-22 summary:Kernel methods are an extremely popular set of techniques used for manyimportant machine learning and data analysis applications. In addition tohaving good practical performances, these methods are supported by awell-developed theory. Kernel methods use an implicit mapping of the input datainto a high dimensional feature space defined by a kernel function, i.e., afunction returning the inner product between the images of two data points inthe feature space. Central to any kernel method is the kernel matrix, which isbuilt by evaluating the kernel function on a given sample dataset. In this paper, we initiate the study of non-asymptotic spectral theory ofrandom kernel matrices. These are n x n random matrices whose (i,j)th entry isobtained by evaluating the kernel function on $x_i$ and $x_j$, where$x_1,...,x_n$ are a set of n independent random high-dimensional vectors. Ourmain contribution is to obtain tight upper bounds on the spectral norm (largesteigenvalue) of random kernel matrices constructed by commonly used kernelfunctions based on polynomials and Gaussian radial basis. As an application of these results, we provide lower bounds on the distortionneeded for releasing the coefficients of kernel ridge regression underattribute privacy, a general privacy notion which captures a large class ofprivacy definitions. Kernel ridge regression is standard method for performingnon-parametric regression that regularly outperforms traditional regressionapproaches in various domains. Our privacy distortion lower bounds are thefirst for any kernel technique, and our analysis assumes realistic scenariosfor the input, unlike all previous lower bounds for other release problemswhich only hold under very restrictive input settings.
arxiv-11100-134 | Normal Bandits of Unknown Means and Variances: Asymptotic Optimality, Finite Horizon Regret Bounds, and a Solution to an Open Problem | http://arxiv.org/abs/1504.05823 | author:Wesley Cowan, Junya Honda, Michael N. Katehakis category:stat.ML cs.LG published:2015-04-22 summary:Consider the problem of sampling sequentially from a finite number of $N \geq2$ populations, specified by random variables $X^i_k$, $ i = 1,\ldots , N,$ and$k = 1, 2, \ldots$; where $X^i_k$ denotes the outcome from population $i$ the$k^{th}$ time it is sampled. It is assumed that for each fixed $i$, $\{ X^i_k \}_{k \geq 1}$ is a sequence of i.i.d. normal random variables,with unknown mean $\mu_i$ and unknown variance $\sigma_i^2$. The objective is to have a policy $\pi$ for deciding from which of the $N$populations to sample form at any time $n=1,2,\ldots$ so as to maximize theexpected sum of outcomes of $n$ samples or equivalently to minimize the regretdue to lack on information of the parameters $\mu_i$ and $\sigma_i^2$. In thispaper, we present a simple inflated sample mean (ISM) index policy that isasymptotically optimal in the sense of Theorem 4 below. This resolves astanding open problem from Burnetas and Katehakis (1996). Additionally, finitehorizon regret bounds are given.
arxiv-11100-135 | Signal Recovery on Graphs: Random versus Experimentally Designed Sampling | http://arxiv.org/abs/1504.05427 | author:Siheng Chen, Rohan Varma, Aarti Singh, Jelena Kovačević category:cs.IT math.IT stat.ML published:2015-04-21 summary:We study signal recovery on graphs based on two sampling strategies: randomsampling and experimentally designed sampling. We propose a new class of smoothgraph signals, called approximately bandlimited, which generalizes thebandlimited class and is similar to the globally smooth class. We then proposetwo recovery strategies based on random sampling and experimentally designedsampling. The proposed recovery strategy based on experimentally designedsampling is similar to the leverage scores used in the matrix approximation. Weshow that while both strategies are unbiased estimators for the low-frequencycomponents, the convergence rate of experimentally designed sampling is muchfaster than that of random sampling when a graph is irregular. We validate theproposed recovery strategies on three specific graphs: a ring graph, anErd\H{o}s-R\'enyi graph, and a star graph. The simulation results support thetheoretical analysis.
arxiv-11100-136 | Automatic Face Recognition from Video | http://arxiv.org/abs/1504.05308 | author:Ognjen Arandjelovic category:cs.CV published:2015-04-21 summary:The objective of this work is to automatically recognize faces from videosequences in a realistic, unconstrained setup in which illumination conditionsare extreme and greatly changing, viewpoint and user motion pattern have a widevariability, and video input is of low quality. At the centre of focus are faceappearance manifolds: this thesis presents a significant advance of theirunderstanding and application in the sphere of face recognition. The two maincontributions are the Generic Shape-Illumination Manifold recognition algorithmand the Anisotropic Manifold Space clustering. The Generic Shape-IlluminationManifold is evaluated on a large data corpus acquired in real-world conditionsand its performance is shown to greatly exceed that of state-of-the-art methodsin the literature and the best performing commercial software. Empiricalevaluation of the Anisotropic Manifold Space clustering on a popular situationcomedy is also described with excellent preliminary results.
arxiv-11100-137 | Deep Spatial Pyramid: The Devil is Once Again in the Details | http://arxiv.org/abs/1504.05277 | author:Bin-Bin Gao, Xiu-Shen Wei, Jianxin Wu, Weiyao Lin category:cs.CV published:2015-04-21 summary:In this paper we show that by carefully making good choices for variousdetailed but important factors in a visual recognition framework using deeplearning features, one can achieve a simple, efficient, yet highly accurateimage classification system. We first list 5 important factors, based on bothexisting researches and ideas proposed in this paper. These important detailedfactors include: 1) $\ell_2$ matrix normalization is more effective thanunnormalized or $\ell_2$ vector normalization, 2) the proposed natural deepspatial pyramid is very effective, and 3) a very small $K$ in Fisher Vectorssurprisingly achieves higher accuracy than normally used large $K$ values.Along with other choices (convolutional activations and multiple scales), theproposed DSP framework is not only intuitive and efficient, but also achievesexcellent classification accuracy on many benchmark datasets. For example,DSP's accuracy on SUN397 is 59.78%, significantly higher than previousstate-of-the-art (53.86%).
arxiv-11100-138 | Effective Discriminative Feature Selection with Non-trivial Solutions | http://arxiv.org/abs/1504.05408 | author:Hong Tao, Chenping Hou, Feiping Nie, Yuanyuan Jiao, Dongyun Yi category:cs.LG published:2015-04-21 summary:Feature selection and feature transformation, the two main ways to reducedimensionality, are often presented separately. In this paper, a featureselection method is proposed by combining the popular transformation baseddimensionality reduction method Linear Discriminant Analysis (LDA) and sparsityregularization. We impose row sparsity on the transformation matrix of LDAthrough ${\ell}_{2,1}$-norm regularization to achieve feature selection, andthe resultant formulation optimizes for selecting the most discriminativefeatures and removing the redundant ones simultaneously. The formulation isextended to the ${\ell}_{2,p}$-norm regularized case: which is more likely tooffer better sparsity when $0<p<1$. Thus the formulation is a betterapproximation to the feature selection problem. An efficient algorithm isdeveloped to solve the ${\ell}_{2,p}$-norm based optimization problem and it isproved that the algorithm converges when $0<p\le 2$. Systematical experimentsare conducted to understand the work of the proposed method. Promisingexperimental results on various types of real-world data sets demonstrate theeffectiveness of our algorithm.
arxiv-11100-139 | Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word: The Impact of Word Representation on Sequence Labelling Tasks | http://arxiv.org/abs/1504.05319 | author:Lizhen Qu, Gabriela Ferraro, Liyuan Zhou, Weiwei Hou, Nathan Schneider, Timothy Baldwin category:cs.CL published:2015-04-21 summary:Word embeddings -- distributed word representations that can be learned fromunlabelled data -- have been shown to have high utility in many naturallanguage processing applications. In this paper, we perform an extrinsicevaluation of five popular word embedding methods in the context of foursequence labelling tasks: POS-tagging, syntactic chunking, NER and MWEidentification. A particular focus of the paper is analysing the effects oftask-based updating of word representations. We show that when using wordembeddings as features, as few as several hundred training instances aresufficient to achieve competitive results, and that word embeddings lead toimprovements over OOV words and out of domain. Perhaps more surprisingly, ourresults indicate there is little difference between the different wordembedding methods, and that simple Brown clusters are often competitive withword embeddings across all tasks we consider.
arxiv-11100-140 | Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition | http://arxiv.org/abs/1504.05477 | author:Cameron Musco, Christopher Musco category:cs.DS cs.LG cs.NA published:2015-04-21 summary:Since being analyzed by Rokhlin, Szlam, and Tygert and popularized by Halko,Martinsson, and Tropp, randomized Simultaneous Power Iteration has become themethod of choice for approximate singular value decomposition. It is moreaccurate than simpler sketching algorithms, yet still converges quickly for anymatrix, independently of singular value gaps. After $\tilde{O}(1/\epsilon)$iterations, it gives a low-rank approximation within $(1+\epsilon)$ of optimalfor spectral norm error. We give the first provable runtime improvement on Simultaneous Iteration: asimple randomized block Krylov method, closely related to the classic BlockLanczos algorithm, gives the same guarantees in just$\tilde{O}(1/\sqrt{\epsilon})$ iterations and performs substantially betterexperimentally. Despite their long history, our analysis is the first of aKrylov subspace method that does not depend on singular value gaps, which areunreliable in practice. Furthermore, while it is a simple accuracy benchmark, even $(1+\epsilon)$error for spectral norm low-rank approximation does not imply that an algorithmreturns high quality principal components, a major issue for data applications.We address this problem for the first time by showing that both Block KrylovIteration and a minor modification of Simultaneous Iteration give nearlyoptimal PCA for any matrix. This result further justifies their strength overnon-iterative sketching methods. Finally, we give insight beyond the worst case, justifying why bothalgorithms can run much faster in practice than predicted. We clarify howsimple techniques can take advantage of common matrix properties tosignificantly improve runtime.
arxiv-11100-141 | The adaptable buffer algorithm for high quantile estimation in non-stationary data streams | http://arxiv.org/abs/1504.05302 | author:Ognjen Arandjelovic, Duc-Son Pham, Svetha Venkatesh category:cs.CV published:2015-04-21 summary:The need to estimate a particular quantile of a distribution is an importantproblem which frequently arises in many computer vision and signal processingapplications. For example, our work was motivated by the requirements of manysemi-automatic surveillance analytics systems which detect abnormalities inclose-circuit television (CCTV) footage using statistical models of low-levelmotion features. In this paper we specifically address the problem ofestimating the running quantile of a data stream with non-stationarystochasticity when the memory for storing observations is limited. We makeseveral major contributions: (i) we derive an important theoretical resultwhich shows that the change in the quantile of a stream is constrainedregardless of the stochastic properties of data, (ii) we describe a set ofhigh-level design goals for an effective estimation algorithm that emerge as aconsequence of our theoretical findings, (iii) we introduce a novel algorithmwhich implements the aforementioned design goals by retaining a sample of datavalues in a manner adaptive to changes in the distribution of data andprogressively narrowing down its focus in the periods of quasi-stationarystochasticity, and (iv) we present a comprehensive evaluation of the proposedalgorithm and compare it with the existing methods in the literature on bothsynthetic data sets and three large `real-world' streams acquired in the courseof operation of an existing commercial surveillance system. Our findingsconvincingly demonstrate that the proposed method is highly successful andvastly outperforms the existing alternatives, especially when the targetquantile is high valued and the available buffer capacity severely limited.
arxiv-11100-142 | Adaptive Compressive Tracking via Online Vector Boosting Feature Selection | http://arxiv.org/abs/1504.05451 | author:Qingshan Liu, Jing Yang, Kaihua Zhang, Yi Wu category:cs.CV published:2015-04-21 summary:Recently, the compressive tracking (CT) method has attracted much attentiondue to its high efficiency, but it cannot well deal with the large scale targetappearance variations due to its data-independent random projection matrix thatresults in less discriminative features. To address this issue, in this paperwe propose an adaptive CT approach, which selects the most discriminativefeatures to design an effective appearance model. Our method significantlyimproves CT in three aspects: Firstly, the most discriminative features areselected via an online vector boosting method. Secondly, the objectrepresentation is updated in an effective online manner, which preserves thestable features while filtering out the noisy ones. Finally, a simple andeffective trajectory rectification approach is adopted that can make theestimated location more accurate. Extensive experiments on the CVPR2013tracking benchmark demonstrate the superior performance of our algorithmcompared over state-of-the-art tracking algorithms.
arxiv-11100-143 | Groupwise registration of aerial images | http://arxiv.org/abs/1504.05299 | author:Ognjen Arandjelovic, Duc-Son Pham, Svetha Venkatesh category:cs.CV published:2015-04-21 summary:This paper addresses the task of time separated aerial image registration.The ability to solve this problem accurately and reliably is important for avariety of subsequent image understanding applications. The principal challengelies in the extent and nature of transient appearance variation that a landarea can undergo, such as that caused by the change in illumination conditions,seasonal variations, or the occlusion by non-persistent objects (people, cars).Our work introduces several novelties: (i) unlike all previous work on aerialimage registration, we approach the problem using a set-based paradigm; (ii) weshow how local, pair-wise constraints can be used to enforce a globally goodregistration using a constraints graph structure; (iii) we show how a simpleholistic representation derived from raw aerial images can be used as a basicbuilding block of the constraints graph in a manner which achieves both highregistration accuracy and speed. We demonstrate: (i) that the proposed methodoutperforms the state-of-the-art for pair-wise registration already, achievinggreater accuracy and reliability, while at the same time reducing thecomputational cost of the task; and (ii) that the increase in the number ofavailable images in a set consistently reduces the average registration error.
arxiv-11100-144 | Learning Opposites with Evolving Rules | http://arxiv.org/abs/1504.05619 | author:Hamid R. Tizhoosh, Shahryar Rahnamayan category:cs.NE cs.LG published:2015-04-21 summary:The idea of opposition-based learning was introduced 10 years ago. Since thena noteworthy group of researchers has used some notions of oppositeness toimprove existing optimization and learning algorithms. Among others,evolutionary algorithms, reinforcement agents, and neural networks have beenreportedly extended into their opposition-based version to become faster and/ormore accurate. However, most works still use a simple notion of opposites,namely linear (or type- I) opposition, that for each $x\in[a,b]$ assigns itsopposite as $\breve{x}_I=a+b-x$. This, of course, is a very naive estimate ofthe actual or true (non-linear) opposite $\breve{x}_{II}$, which has beencalled type-II opposite in literature. In absence of any knowledge about afunction $y=f(\mathbf{x})$ that we need to approximate, there seems to be noalternative to the naivety of type-I opposition if one intents to utilizeoppositional concepts. But the question is if we can receive some level ofaccuracy increase and time savings by using the naive opposite estimate$\breve{x}_I$ according to all reports in literature, what would we be able togain, in terms of even higher accuracies and more reduction in computationalcomplexity, if we would generate and employ true opposites? This workintroduces an approach to approximate type-II opposites using evolving fuzzyrules when we first perform opposition mining. We show with multiple examplesthat learning true opposites is possible when we mine the opposites from thetraining data to subsequently approximate $\breve{x}_{II}=f(\mathbf{x},y)$.
arxiv-11100-145 | Viewpoint distortion compensation in practical surveillance systems | http://arxiv.org/abs/1504.05298 | author:Ognjen Arandjelovic, Duc-Son Pham, Svetha Venkatesh category:cs.CV published:2015-04-21 summary:Our aim is to estimate the perspective-effected geometric distortion of ascene from a video feed. In contrast to all previous work we wish to achievethis using from low-level, spatio-temporally local motion features used incommercial semi-automatic surveillance systems. We: (i) describe a densealgorithm which uses motion features to estimate the perspective distortion ateach image locus and then polls all such local estimates to arrive at theglobally best estimate, (ii) present an alternative coarse algorithm whichsubdivides the image frame into blocks, and uses motion features to deriveblock-specific motion characteristics and constrain the relationships betweenthese characteristics, with the perspective estimate emerging as a result of aglobal optimization scheme, and (iii) report the results of an evaluation usingnine large sets acquired using existing close-circuit television (CCTV)cameras. Our findings demonstrate that both of the proposed methods aresuccessful, their accuracy matching that of human labelling using completevisual data.
arxiv-11100-146 | Distance-based species tree estimation: information-theoretic trade-off between number of loci and sequence length under the coalescent | http://arxiv.org/abs/1504.05289 | author:Elchanan Mossel, Sebastien Roch category:math.PR cs.LG math.ST q-bio.PE stat.TH published:2015-04-21 summary:We consider the reconstruction of a phylogeny from multiple genes under themultispecies coalescent. We establish a connection with the sparse signaldetection problem, where one seeks to distinguish between a distribution and amixture of the distribution and a sparse signal. Using this connection, wederive an information-theoretic trade-off between the number of genes, $m$,needed for an accurate reconstruction and the sequence length, $k$, of thegenes. Specifically, we show that to detect a branch of length $f$, one needs$m = \Theta(1/[f^{2} \sqrt{k}])$.
arxiv-11100-147 | Decomposing Overcomplete 3rd Order Tensors using Sum-of-Squares Algorithms | http://arxiv.org/abs/1504.05287 | author:Rong Ge, Tengyu Ma category:cs.DS cs.LG stat.ML published:2015-04-21 summary:Tensor rank and low-rank tensor decompositions have many applications inlearning and complexity theory. Most known algorithms use unfoldings of tensorsand can only handle rank up to $n^{\lfloor p/2 \rfloor}$ for a $p$-th ordertensor in $\mathbb{R}^{n^p}$. Previously no efficient algorithm can decompose3rd order tensors when the rank is super-linear in the dimension. Using ideasfrom sum-of-squares hierarchy, we give the first quasi-polynomial timealgorithm that can decompose a random 3rd order tensor decomposition when therank is as large as $n^{3/2}/\textrm{polylog} n$. We also give a polynomial time algorithm for certifying the injective norm ofrandom low rank tensors. Our tensor decomposition algorithm exploits therelationship between injective norm and the tensor components. The proof relieson interesting tools for decoupling random variables to prove better matrixconcentration bounds, which can be useful in other settings.
arxiv-11100-148 | Temporal-Difference Networks | http://arxiv.org/abs/1504.05539 | author:Richard S. Sutton, Brian Tanner category:cs.LG published:2015-04-21 summary:We introduce a generalization of temporal-difference (TD) learning tonetworks of interrelated predictions. Rather than relating a single predictionto itself at a later time, as in conventional TD methods, a TD network relateseach prediction in a set of predictions to other predictions in the set at alater time. TD networks can represent and apply TD learning to a much widerclass of predictions than has previously been possible. Using a random-walkexample, we show that these networks can be used to learn to predict by a fixedinterval, which is not possible with conventional TD methods. Secondly, we showthat if the inter-predictive relationships are made conditional on action, thenthe usual learning-efficiency advantage of TD methods over Monte Carlo(supervised learning) methods becomes particularly pronounced. Thirdly, wedemonstrate that TD networks can learn predictive state representations thatenable exact solution of a non-Markov problem. A very broad range ofinter-predictive temporal relationships can be expressed in these networks.Overall we argue that TD networks represent a substantial extension of theabilities of TD methods and bring us closer to the goal of representing worldknowledge in entirely predictive, grounded terms.
arxiv-11100-149 | A robust and efficient video representation for action recognition | http://arxiv.org/abs/1504.05524 | author:Heng Wang, Dan Oneata, Jakob Verbeek, Cordelia Schmid category:cs.CV published:2015-04-21 summary:This paper introduces a state-of-the-art video representation and applies itto efficient action recognition and detection. We first propose to improve thepopular dense trajectory features by explicit camera motion estimation. Morespecifically, we extract feature point matches between frames using SURFdescriptors and dense optical flow. The matches are used to estimate ahomography with RANSAC. To improve the robustness of homography estimation, ahuman detector is employed to remove outlier matches from the human body ashuman motion is not constrained by the camera. Trajectories consistent with thehomography are considered as due to camera motion, and thus removed. We alsouse the homography to cancel out camera motion from the optical flow. Thisresults in significant improvement on motion-based HOF and MBH descriptors. Wefurther explore the recent Fisher vector as an alternative feature encodingapproach to the standard bag-of-words histogram, and consider different ways toinclude spatial layout information in these encodings. We present a large andvaried set of evaluations, considering (i) classification of short basicactions on six datasets, (ii) localization of such actions in feature-lengthmovies, and (iii) large-scale recognition of complex events. We find that ourimproved trajectory features significantly outperform previous densetrajectories, and that Fisher vectors are superior to bag-of-words encodingsfor video recognition tasks. In all three tasks, we show substantialimprovements over the state-of-the-art results.
arxiv-11100-150 | Online Learning Algorithm for Time Series Forecasting Suitable for Low Cost Wireless Sensor Networks Nodes | http://arxiv.org/abs/1504.05517 | author:Juan Pardo, Francisco Zamora-Martinez, Paloma Botella-Rocamora category:cs.NI cs.LG cs.SY published:2015-04-21 summary:Time series forecasting is an important predictive methodology which can beapplied to a wide range of problems. Particularly, forecasting the indoortemperature permits an improved utilization of the HVAC (Heating, Ventilatingand Air Conditioning) systems in a home and thus a better energy efficiency.With such purpose the paper describes how to implement an Artificial NeuralNetwork (ANN) algorithm in a low cost system-on-chip to develop an autonomousintelligent wireless sensor network. The present paper uses a Wireless SensorNetworks (WSN) to monitor and forecast the indoor temperature in a smart home,based on low resources and cost microcontroller technology as the 8051MCU. Anon-line learning approach, based on Back-Propagation (BP) algorithm for ANNs,has been developed for real-time time series learning. It performs the modeltraining with every new data that arrive to the system, without saving enormousquantities of data to create a historical database as usual, i.e., withoutprevious knowledge. Consequently to validate the approach a simulation studythrough a Bayesian baseline model have been tested in order to compare with adatabase of a real application aiming to see the performance and accuracy. Thecore of the paper is a new algorithm, based on the BP one, which has beendescribed in detail, and the challenge was how to implement a computationaldemanding algorithm in a simple architecture with very few hardware resources.
arxiv-11100-151 | Key-Pose Prediction in Cyclic Human Motion | http://arxiv.org/abs/1504.05369 | author:Dan Zecha, Rainer Lienhart category:cs.CV published:2015-04-21 summary:In this paper we study the problem of estimating innercyclic time intervalswithin repetitive motion sequences of top-class swimmers in a swimming channel.Interval limits are given by temporal occurrences of key-poses, i.e.distinctive postures of the body. A key-pose is defined by means of only one ortwo specific features of the complete posture. It is often difficult to detectsuch subtle features directly. We therefore propose the following method: Giventhat we observe the swimmer from the side, we build a pictorial structure ofposelets to robustly identify random support poses within the regular motion ofa swimmer. We formulate a maximum likelihood model which predicts a key-posegiven the occurrences of multiple support poses within one stroke. The maximumlikelihood can be extended with prior knowledge about the temporal location ofa key-pose in order to improve the prediction recall. We experimentally showthat our models reliably and robustly detect key-poses with a high precisionand that their performance can be improved by extending the framework withadditional camera views.
arxiv-11100-152 | Instance Optimal Learning | http://arxiv.org/abs/1504.05321 | author:Gregory Valiant, Paul Valiant category:cs.LG published:2015-04-21 summary:We consider the following basic learning task: given independent draws froman unknown distribution over a discrete support, output an approximation of thedistribution that is as accurate as possible in $\ell_1$ distance (i.e. totalvariation or statistical distance). Perhaps surprisingly, it is often possibleto "de-noise" the empirical distribution of the samples to return anapproximation of the true distribution that is significantly more accurate thanthe empirical distribution, without relying on any prior assumptions on thedistribution. We present an instance optimal learning algorithm which optimallyperforms this de-noising for every distribution for which such a de-noising ispossible. More formally, given $n$ independent draws from a distribution $p$,our algorithm returns a labelled vector whose expected distance from $p$ isequal to the minimum possible expected error that could be obtained by anyalgorithm that knows the true unlabeled vector of probabilities of distribution$p$ and simply needs to assign labels, up to an additive subconstant term thatis independent of $p$ and goes to zero as $n$ gets large. One conceptualimplication of this result is that for large samples, Bayesian assumptions onthe "shape" or bounds on the tail probabilities of a distribution over discretesupport are not helpful for the task of learning the distribution. As a consequence of our techniques, we also show that given a set of $n$samples from an arbitrary distribution, one can accurately estimate theexpected number of distinct elements that will be observed in a sample of anysize up to $n \log n$. This sort of extrapolation is practically relevant,particularly to domains such as genomics where it is important to understandhow much more might be discovered given larger sample sizes, and we areoptimistic that our approach is practically viable.
arxiv-11100-153 | Nonparametric Testing for Heterogeneous Correlation | http://arxiv.org/abs/1504.05392 | author:Stephen Bamattre, Rex Hu, Joseph S. Verducci category:stat.ML published:2015-04-21 summary:In the presence of weak overall correlation, it may be useful to investigateif the correlation is significantly and substantially more pronounced over asubpopulation. Two different testing procedures are compared. Both are based onthe rankings of the values of two variables from a data set with a large numbern of observations. The first maintains its level against Gaussian copulas; thesecond adapts to general alternatives in the sense that that the number ofparameters used in the test grows with n. An analysis of wine qualityillustrates how the methods detect heterogeneity of association betweenchemical properties of the wine, which are attributable to a mix of differentcultivars.
arxiv-11100-154 | Deep Convolutional Neural Networks Based on Semi-Discrete Frames | http://arxiv.org/abs/1504.05487 | author:Thomas Wiatowski, Helmut Bölcskei category:cs.LG cs.IT math.FA math.IT stat.ML published:2015-04-21 summary:Deep convolutional neural networks have led to breakthrough results inpractical feature extraction applications. The mathematical analysis of thesenetworks was pioneered by Mallat, 2012. Specifically, Mallat consideredso-called scattering networks based on identical semi-discrete wavelet framesin each network layer, and proved translation-invariance as well as deformationstability of the resulting feature extractor. The purpose of this paper is todevelop Mallat's theory further by allowing for different and, mostimportantly, general semi-discrete frames (such as, e.g., Gabor frames,wavelets, curvelets, shearlets, ridgelets) in distinct network layers. Thisallows to extract wider classes of features than point singularities resolvedby the wavelet transform. Our generalized feature extractor is proven to betranslation-invariant, and we develop deformation stability results for alarger class of deformations than those considered by Mallat. For Mallat'swavelet-based feature extractor, we get rid of a number of technicalconditions. The mathematical engine behind our results is continuous frametheory, which allows us to completely detach the invariance and deformationstability proofs from the particular algebraic structure of the underlyingframes.
arxiv-11100-155 | Can FCA-based Recommender System Suggest a Proper Classifier? | http://arxiv.org/abs/1504.05473 | author:Yury Kashnitsky, Dmitry I. Ignatov category:cs.IR cs.LG stat.ML 62-07 published:2015-04-21 summary:The paper briefly introduces multiple classifier systems and describes a newalgorithm, which improves classification accuracy by means of recommendation ofa proper algorithm to an object classification. This recommendation is doneassuming that a classifier is likely to predict the label of the objectcorrectly if it has correctly classified its neighbors. The process ofassigning a classifier to each object is based on Formal Concept Analysis. Weexplain the idea of the algorithm with a toy example and describe our firstexperiments with real-world datasets.
arxiv-11100-156 | A local approach to estimation in discrete loglinear models | http://arxiv.org/abs/1504.05434 | author:Helene Massam, Nanwei Wang category:stat.ML 62H17, 62M40 published:2015-04-21 summary:We consider two connected aspects of maximum likelihood estimation of theparameter for high-dimensional discrete graphical models: the existence of themaximum likelihood estimate (mle) and its computation. When the data is sparse, there are many zeros in the contingency table andthe maximum likelihood estimate of the parameter may not exist. Fienberg andRinaldo (2012) have shown that the mle does not exists iff the data vectorbelongs to a face of the so-called marginal cone spanned by the rows of thedesign matrix of the model. Identifying these faces in high-dimension ischallenging. In this paper, we take a local approach : we show that one suchface, albeit possibly not the smallest one, can be identified by looking at acollection of marginal graphical models generated by induced subgraphs$G_i,i=1,\ldots,k$ of $G$. This is our first contribution. Our second contribution concerns the composite maximum likelihood estimate.When the dimension of the problem is large, estimating the parameters of agiven graphical model through maximum likelihood is challenging, if notimpossible. The traditional approach to this problem has been local with theuse of composite likelihood based on local conditional likelihoods. A more recent development is to have the components of the compositelikelihood be marginal likelihoods centred around each $v$. We first show thatthe estimates obtained by consensus through local conditional and marginallikelihoods are identical. We then study the asymptotic properties of thecomposite maximum likelihood estimate when both the dimension of the model andthe sample size $N$ go to infinity.
arxiv-11100-157 | Self-Adaptive Hierarchical Sentence Model | http://arxiv.org/abs/1504.05070 | author:Han Zhao, Zhengdong Lu, Pascal Poupart category:cs.CL cs.LG cs.NE published:2015-04-20 summary:The ability to accurately model a sentence at varying stages (e.g.,word-phrase-sentence) plays a central role in natural language processing. Asan effort towards this goal we propose a self-adaptive hierarchical sentencemodel (AdaSent). AdaSent effectively forms a hierarchy of representations fromwords to phrases and then to sentences through recursive gated localcomposition of adjacent segments. We design a competitive mechanism (throughgating networks) to allow the representations of the same sentence to beengaged in a particular learning task (e.g., classification), thereforeeffectively mitigating the gradient vanishing problem persistent in otherrecursive models. Both qualitative and quantitative analysis shows that AdaSentcan automatically form and select the representations suitable for the task athand during training, yielding superior classification performance overcompetitor models on 5 benchmark data sets.
arxiv-11100-158 | Convolutional Neural Network-Based Image Representation for Visual Loop Closure Detection | http://arxiv.org/abs/1504.05241 | author:Yi Hou, Hong Zhang, Shilin Zhou category:cs.RO cs.CV published:2015-04-20 summary:Deep convolutional neural networks (CNN) have recently been shown in manycomputer vision and pattern recog- nition applications to outperform by asignificant margin state- of-the-art solutions that use traditionalhand-crafted features. However, this impressive performance is yet to be fullyexploited in robotics. In this paper, we focus one specific problem that canbenefit from the recent development of the CNN technology, i.e., we focus onusing a pre-trained CNN model as a method of generating an image representationappropriate for visual loop closure detection in SLAM (simultaneouslocalization and mapping). We perform a comprehensive evaluation of the outputsat the intermediate layers of a CNN as image descriptors, in comparison withstate-of-the-art image descriptors, in terms of their ability to match imagesfor detecting loop closures. The main conclusions of our study include: (a)CNN-based image representations perform comparably to state-of-the-art hand-crafted competitors in environments without significant lighting change, (b)they outperform state-of-the-art competitors when lighting changessignificantly, and (c) they are also significantly faster to extract than thestate-of-the-art hand-crafted features even on a conventional CPU and are twoorders of magnitude faster on an entry-level GPU.
arxiv-11100-159 | Exploiting Local Features from Deep Networks for Image Retrieval | http://arxiv.org/abs/1504.05133 | author:Joe Yue-Hei Ng, Fan Yang, Larry S. Davis category:cs.CV published:2015-04-20 summary:Deep convolutional neural networks have been successfully applied to imageclassification tasks. When these same networks have been applied to imageretrieval, the assumption has been made that the last layers would give thebest performance, as they do in classification. We show that for instance-levelimage retrieval, lower layers often perform better than the last layers inconvolutional neural networks. We present an approach for extractingconvolutional features from different layers of the networks, and adopt VLADencoding to encode features into a single vector for each image. We investigatethe effect of different layers and scales of input images on the performance ofconvolutional features using the recent deep networks OxfordNet and GoogLeNet.Experiments demonstrate that intermediate layers or higher layers with finerscales produce better results for image retrieval, compared to the last layer.When using compressed 128-D VLAD descriptors, our method obtainsstate-of-the-art results and outperforms other VLAD and CNN based approaches ontwo out of three test datasets. Our work provides guidance for transferringdeep networks trained on image classification to image retrieval tasks.
arxiv-11100-160 | Hybrid Genetic Algorithm and Lasso Test Approach for Inferring Well Supported Phylogenetic Trees based on Subsets of Chloroplastic Core Genes | http://arxiv.org/abs/1504.05095 | author:Bassam AlKindy, Christophe Guyeux, Jean-François Couchot, Michel Salomon, Christian Parisod, Jacques M. Bahi category:cs.AI cs.NE q-bio.PE q-bio.QM published:2015-04-20 summary:The amount of completely sequenced chloroplast genomes increases rapidlyevery day, leading to the possibility to build large scale phylogenetic treesof plant species. Considering a subset of close plant species defined accordingto their chloroplasts, the phylogenetic tree that can be inferred by their coregenes is not necessarily well supported, due to the possible occurrence of"problematic" genes (i.e., homoplasy, incomplete lineage sorting, horizontalgene transfers, etc.) which may blur phylogenetic signal. However, atrustworthy phylogenetic tree can still be obtained if the number ofproblematic genes is low, the problem being to determine the largest subset ofcore genes that produces the best supported tree. To discard problematic genesand due to the overwhelming number of possible combinations, we propose anhybrid approach that embeds both genetic algorithms and statistical tests.Given a set of organisms, the result is a pipeline of many stages for theproduction of well supported phylogenetic trees. The proposal has been appliedto different cases of plant families, leading to encouraging results for thesefamilies.
arxiv-11100-161 | Poisson Matrix Recovery and Completion | http://arxiv.org/abs/1504.05229 | author:Yang Cao, Yao Xie category:cs.LG math.ST stat.ML stat.TH published:2015-04-20 summary:We extend the theory of low-rank matrix recovery and completion to the casewhen Poisson observations for a linear combination or a subset of the entriesof a matrix are available, which arises in various applications with countdata. We consider the usual matrix recovery formulation through maximumlikelihood with proper constraints on the matrix $M$ of size $d_1$-by-$d_2$,and establish theoretical upper and lower bounds on the recovery error. Ourbounds for matrix completion are nearly optimal up to a factor on the order of$\mathcal{O}(\log(d_1 d_2))$. These bounds are obtained by combing techniquesfor compressed sensing for sparse vectors with Poisson noise and for analyzinglow-rank matrices, as well as adapting the arguments used for one-bit matrixcompletion \cite{davenport20121} (although these two problems are different innature) and the adaptation requires new techniques exploiting properties of thePoisson likelihood function and tackling the difficulties posed by the locallysub-Gaussian characteristic of the Poisson distribution. Our results highlighta few important distinctions of the Poisson case compared to the prior workincluding having to impose a minimum signal-to-noise requirement on eachobserved entry and a gap in the upper and lower bounds. We also develop a setof efficient iterative algorithms and demonstrate their good performance onsynthetic examples and real data.
arxiv-11100-162 | Multi-swarm PSO algorithm for the Quadratic Assignment Problem: a massive parallel implementation on the OpenCL platform | http://arxiv.org/abs/1504.05158 | author:Piotr Szwed, Wojciech Chmiel category:cs.NE published:2015-04-20 summary:This paper presents a multi-swarm PSO algorithm for the Quadratic AssignmentProblem (QAP) implemented on OpenCL platform. Our work was motivated by resultsof time efficiency tests performed for single-swarm algorithm implementationthat showed clearly that the benefits of a parallel execution platform can befully exploited, if the processed population is large. The described algorithmcan be executed in two modes: with independent swarms or with migration. Wediscuss the algorithm construction, as well as we report results of testsperformed on several problem instances from the QAPLIB library. During theexperiments the algorithm was configured to process large populations. Thisallowed us to collect statistical data related to values of goal functionreached by individual particles. We use them to demonstrate on two test casesthat although single particles seem to behave chaotically during theoptimization process, when the whole population is analyzed, the probabilitythat a particle will select a near-optimal solution grows.
arxiv-11100-163 | Optimal Nudging: Solving Average-Reward Semi-Markov Decision Processes as a Minimal Sequence of Cumulative Tasks | http://arxiv.org/abs/1504.05122 | author:Reinaldo Uribe Muriel, Fernando Lozando, Charles Anderson category:cs.LG cs.AI published:2015-04-20 summary:This paper describes a novel method to solve average-reward semi-Markovdecision processes, by reducing them to a minimal sequence of cumulative rewardproblems. The usual solution methods for this type of problems update the gain(optimal average reward) immediately after observing the result of taking anaction. The alternative introduced, optimal nudging, relies instead on settingthe gain to some fixed value, which transitorily makes the problem acumulative-reward task, solving it by any standard reinforcement learningmethod, and only then updating the gain in a way that minimizes uncertainty ina minmax sense. The rule for optimal gain update is derived by exploiting thegeometric features of the w-l space, a simple mapping of the space of policies.The total number of cumulative reward tasks that need to be solved is shown tobe small. Some experiments are presented to explore the features of thealgorithm and to compare its performance with other approaches.
arxiv-11100-164 | Partition MCMC for inference on acyclic digraphs | http://arxiv.org/abs/1504.05006 | author:Jack Kuipers, Giusi Moffa category:stat.ML published:2015-04-20 summary:Acyclic digraphs are the underlying representation of Bayesian networks, awidely used class of probabilistic graphical models. Learning the underlyinggraph from data is a way of gaining insights about the structural properties ofa domain. Structure learning forms one of the inference challenges ofstatistical graphical models. MCMC methods, notably structure MCMC, to sample graphs from the posteriordistribution given the data are probably the only viable option for Bayesianmodel averaging. Score modularity and restrictions on the number of parents ofeach node allow the graphs to be grouped into larger collections, which can bescored as a whole to improve the chain's convergence. Current examples ofalgorithms taking advantage of grouping are the biased order MCMC, which actson the alternative space of permuted triangular matrices, and non ergodic edgereversal moves. Here we propose a novel algorithm, which employs the underlying combinatorialstructure of DAGs to define a new grouping. As a result convergence is improvedcompared to structure MCMC, while still retaining the property of producing anunbiased sample. Finally the method can be combined with edge reversal moves toimprove the sampler further.
arxiv-11100-165 | Nonparametric Nearest Neighbor Random Process Clustering | http://arxiv.org/abs/1504.05059 | author:Michael Tschannen, Helmut Bölcskei category:stat.ML cs.IT cs.LG math.IT published:2015-04-20 summary:We consider the problem of clustering noisy finite-length observations ofstationary ergodic random processes according to their nonparametric generativemodels without prior knowledge of the model statistics and the number ofgenerative models. Two algorithms, both using the L1-distance between estimatedpower spectral densities (PSDs) as a measure of dissimilarity, are analyzed.The first algorithm, termed nearest neighbor process clustering (NNPC), to thebest of our knowledge, is new and relies on partitioning the nearest neighborgraph of the observations via spectral clustering. The second algorithm, simplyreferred to as k-means (KM), consists of a single k-means iteration withfarthest point initialization and was considered before in the literature,albeit with a different measure of dissimilarity and with asymptoticperformance results only. We show that both NNPC and KM succeed with highprobability under noise and even when the generative process PSDs overlapsignificantly, all provided that the observation length is sufficiently large.Our results quantify the tradeoff between the overlap of the generative processPSDs, the noise variance, and the observation length. Finally, we presentnumerical performance results for synthetic and real data.
arxiv-11100-166 | Illuminating search spaces by mapping elites | http://arxiv.org/abs/1504.04909 | author:Jean-Baptiste Mouret, Jeff Clune category:cs.AI cs.NE cs.RO q-bio.PE published:2015-04-20 summary:Many fields use search algorithms, which automatically explore a search spaceto find high-performing solutions: chemists search through the space ofmolecules to discover new drugs; engineers search for stronger, cheaper, saferdesigns, scientists search for models that best explain data, etc. The goal ofsearch algorithms has traditionally been to return the singlehighest-performing solution in a search space. Here we describe a new,fundamentally different type of algorithm that is more useful because itprovides a holistic view of how high-performing solutions are distributedthroughout a search space. It creates a map of high-performing solutions ateach point in a space defined by dimensions of variation that a user gets tochoose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)algorithm illuminates search spaces, allowing researchers to understand howinteresting attributes of solutions combine to affect performance, eitherpositively or, equally of interest, negatively. For example, a drug company maywish to understand how performance changes as the size of molecules and theircost-to-produce vary. MAP-Elites produces a large diversity of high-performing,yet qualitatively different solutions, which can be more helpful than a single,high-performing solution. Interestingly, because MAP-Elites explores more ofthe search space, it also tends to find a better overall solution thanstate-of-the-art search algorithms. We demonstrate the benefits of this newalgorithm in three different problem domains ranging from producing modularneural networks to designing simulated and real soft robots. Because MAP-Elites (1) illuminates the relationship between performance and dimensions ofinterest in solutions, (2) returns a set of high-performing, yet diversesolutions, and (3) improves finding a single, best solution, it will advancescience and engineering.
arxiv-11100-167 | Negatively Correlated Search | http://arxiv.org/abs/1504.04914 | author:Ke Tang, Peng Yang, Xin Yao category:cs.NE cs.AI published:2015-04-20 summary:Evolutionary Algorithms (EAs) have been shown to be powerful tools forcomplex optimization problems, which are ubiquitous in both communication andbig data analytics. This paper presents a new EA, namely Negatively CorrelatedSearch (NCS), which maintains multiple individual search processes in paralleland models the search behaviors of individual search processes as probabilitydistributions. NCS explicitly promotes negatively correlated search behaviorsby encouraging differences among the probability distributions (searchbehaviors). By this means, individual search processes share information andcooperate with each other to search diverse regions of a search space, whichmakes NCS a promising method for non-convex optimization. The cooperationscheme of NCS could also be regarded as a novel diversity preservation schemethat, different from other existing schemes, directly promotes diversity at thelevel of search behaviors rather than merely trying to maintain diversity amongcandidate solutions. Empirical studies showed that NCS is competitive towell-established search methods in the sense that NCS achieved the best overallperformance on 20 multimodal (non-convex) continuous optimization problems. Theadvantages of NCS over state-of-the-art approaches are also demonstrated with acase study on the synthesis of unequally spaced linear antenna arrays.
arxiv-11100-168 | Learning discriminative trajectorylet detector sets for accurate skeleton-based action recognition | http://arxiv.org/abs/1504.04923 | author:Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton von den Hengel category:cs.CV published:2015-04-20 summary:The introduction of low-cost RGB-D sensors has promoted the research inskeleton-based human action recognition. Devising a representation suitable forcharacterising actions on the basis of noisy skeleton sequences remains achallenge, however. We here provide two insights into this challenge. First, weshow that the discriminative information of a skeleton sequence usually residesin a short temporal interval and we propose a simple-but-effective localdescriptor called trajectorylet to capture the static and kinematic informationwithin this interval. Second, we further propose to encode each trajectoryletwith a discriminative trajectorylet detector set which is selected from a largenumber of candidate detectors trained through exemplar-SVMs. The action-levelrepresentation is obtained by pooling trajectorylet encodings. Evaluating onstandard datasets acquired from the Kinect sensor, it is demonstrated that ourmethod obtains superior results over existing approaches under variousexperimental setups.
arxiv-11100-169 | Weakly Supervised Fine-Grained Image Categorization | http://arxiv.org/abs/1504.04943 | author:Yu Zhang, Xiu-shen Wei, Jianxin Wu, Jianfei Cai, Jiangbo Lu, Viet-Anh Nguyen, Minh N. Do category:cs.CV published:2015-04-20 summary:In this paper, we categorize fine-grained images without using any object /part annotation neither in the training nor in the testing stage, a steptowards making it suitable for deployments. Fine-grained image categorizationaims to classify objects with subtle distinctions. Most existing works heavilyrely on object / part detectors to build the correspondence between objectparts by using object or object part annotations inside training images. Theneed for expensive object annotations prevents the wide usage of these methods.Instead, we propose to select useful parts from multi-scale part proposals inobjects, and use them to compute a global image representation forcategorization. This is specially designed for the annotation-free fine-grainedcategorization task, because useful parts have shown to play an important rolein existing annotation-dependent works but accurate part detectors can behardly acquired. With the proposed image representation, we can further detectand visualize the key (most discriminative) parts in objects of differentclasses. In the experiment, the proposed annotation-free method achieves betteraccuracy than that of state-of-the-art annotation-free and most existingannotation-dependent methods on two challenging datasets, which shows that itis not always necessary to use accurate object / part annotations infine-grained image categorization.
arxiv-11100-170 | F-SVM: Combination of Feature Transformation and SVM Learning via Convex Relaxation | http://arxiv.org/abs/1504.05035 | author:Xiaohe Wu, Wangmeng Zuo, Yuanyuan Zhu, Liang Lin category:cs.LG cs.CV published:2015-04-20 summary:The generalization error bound of support vector machine (SVM) depends on theratio of radius and margin, while standard SVM only considers the maximizationof the margin but ignores the minimization of the radius. Several approacheshave been proposed to integrate radius and margin for joint learning of featuretransformation and SVM classifier. However, most of them either require theform of the transformation matrix to be diagonal, or are non-convex andcomputationally expensive. In this paper, we suggest a novel approximation forthe radius of minimum enclosing ball (MEB) in feature space, and then propose aconvex radius-margin based SVM model for joint learning of featuretransformation and SVM classifier, i.e., F-SVM. An alternating minimizationmethod is adopted to solve the F-SVM model, where the feature transformation isupdatedvia gradient descent and the classifier is updated by employing theexisting SVM solver. By incorporating with kernel principal component analysis,F-SVM is further extended for joint learning of nonlinear transformation andclassifier. Experimental results on the UCI machine learning datasets and theLFW face datasets show that F-SVM outperforms the standard SVM and the existingradius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\mu}.
arxiv-11100-171 | Network Plasticity as Bayesian Inference | http://arxiv.org/abs/1504.05143 | author:David Kappel, Stefan Habenschuss, Robert Legenstein, Wolfgang Maass category:cs.NE q-bio.NC published:2015-04-20 summary:General results from statistical learning theory suggest to understand notonly brain computations, but also brain plasticity as probabilistic inference.But a model for that has been missing. We propose that inherently stochasticfeatures of synaptic plasticity and spine motility enable cortical networks ofneurons to carry out probabilistic inference by sampling from a posteriordistribution of network configurations. This model provides a viablealternative to existing models that propose convergence of parameters tomaximum likelihood values. It explains how priors on weight distributions andconnection probabilities can be merged optimally with learned experience, howcortical networks can generalize learned information so well to novelexperiences, and how they can compensate continuously for unforeseendisturbances of the network. The resulting new theory of network plasticityexplains from a functional perspective a number of experimental data onstochastic aspects of synaptic plasticity that previously appeared to be quitepuzzling.
arxiv-11100-172 | DEEP-CARVING: Discovering Visual Attributes by Carving Deep Neural Nets | http://arxiv.org/abs/1504.04871 | author:Sukrit Shankar, Vikas K. Garg, Roberto Cipolla category:cs.CV published:2015-04-19 summary:Most of the approaches for discovering visual attributes in images demandsignificant supervision, which is cumbersome to obtain. In this paper, we aimto discover visual attributes in a weakly supervised setting that is commonlyencountered with contemporary image search engines. Deep Convolutional NeuralNetworks (CNNs) have enjoyed remarkable success in vision applicationsrecently. However, in a weakly supervised scenario, widely used CNN trainingprocedures do not learn a robust model for predicting multiple attribute labelssimultaneously. The primary reason is that the attributes highly co-occurwithin the training data. To ameliorate this limitation, we proposeDeep-Carving, a novel training procedure with CNNs, that helps the netefficiently carve itself for the task of multiple attribute prediction. Duringtraining, the responses of the feature maps are exploited in an ingenious wayto provide the net with multiple pseudo-labels (for training images) forsubsequent iterations. The process is repeated periodically after a fixednumber of iterations, and enables the net carve itself iteratively forefficiently disentangling features. Additionally, we contribute anoun-adjective pairing inspired Natural Scenes Attributes Dataset to theresearch community, CAMIT - NSAD, containing a number of co-occurringattributes within a noun category. We describe, in detail, salient aspects ofthis dataset. Our experiments on CAMIT-NSAD and the SUN Attributes Dataset,with weak supervision, clearly demonstrate that the Deep-Carved CNNsconsistently achieve considerable improvement in the precision of attributeprediction over popular baseline methods.
arxiv-11100-173 | Compressing Neural Networks with the Hashing Trick | http://arxiv.org/abs/1504.04788 | author:Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, Yixin Chen category:cs.LG cs.NE published:2015-04-19 summary:As deep nets are increasingly used in applications suited for mobile devices,a fundamental dilemma becomes apparent: the trend in deep learning is to growmodels to absorb ever-increasing data set sizes; however mobile devices aredesigned with very little memory and cannot store such large models. We presenta novel network architecture, HashedNets, that exploits inherent redundancy inneural networks to achieve drastic reductions in model sizes. HashedNets uses alow-cost hash function to randomly group connection weights into hash buckets,and all connections within the same hash bucket share a single parameter value.These parameters are tuned to adjust to the HashedNets weight sharingarchitecture with standard backprop during training. Our hashing procedureintroduces no additional memory overhead, and we demonstrate on severalbenchmark data sets that HashedNets shrink the storage requirements of neuralnetworks substantially while mostly preserving generalization performance.
arxiv-11100-174 | Gradual Classical Logic for Attributed Objects - Extended in Re-Presentation | http://arxiv.org/abs/1504.04802 | author:Ryuta Arisaka category:cs.AI cs.CL cs.LO published:2015-04-19 summary:Our understanding about things is conceptual. By stating that we reason aboutobjects, it is in fact not the objects but concepts referring to them that wemanipulate. Now, so long just as we acknowledge infinitely extending notionssuch as space, time, size, colour, etc, - in short, any reasonable quality -into which an object is subjected, it becomes infeasible to affirm atomicity inthe concept referring to the object. However, formal/symbolic logics typicallypresume atomic entities upon which other expressions are built. Can we reflectour intuition about the concept onto formal/symbolic logics at all? I assurethat we can, but the usual perspective about the atomicity needs inspected. Inthis work, I present gradual logic which materialises the observation that wecannot tell apart whether a so-regarded atomic entity is atomic or is justatomic enough not to be considered non-atomic. The motivation is to capturecertain phenomena that naturally occur around concepts with attributes,including presupposition and contraries. I present logical particulars of thelogic, which is then mapped onto formal semantics. Two linguisticallyinteresting semantics will be considered. Decidability is shown.
arxiv-11100-175 | Exploring Bayesian Models for Multi-level Clustering of Hierarchically Grouped Sequential Data | http://arxiv.org/abs/1504.04850 | author:Adway Mitra category:cs.LG cs.AI published:2015-04-19 summary:A wide range of Bayesian models have been proposed for data that is dividedhierarchically into groups. These models aim to cluster the data at differentlevels of grouping, by assigning a mixture component to each datapoint, and amixture distribution to each group. Multi-level clustering is facilitated bythe sharing of these components and distributions by the groups. In this paper,we introduce the concept of Degree of Sharing (DoS) for the mixture componentsand distributions, with an aim to analyze and classify various existing models.Next we introduce a generalized hierarchical Bayesian model, of which theexisting models can be shown to be special cases. Unlike most of these models,our model takes into account the sequential nature of the data, and variousother temporal structures at different levels while assigning mixturecomponents and distributions. We show one specialization of this model aimed athierarchical segmentation of news transcripts, and present a Gibbs Samplingbased inference algorithm for it. We also show experimentally that the proposedmodel outperforms existing models for the same task.
arxiv-11100-176 | Visual Recognition Using Directional Distribution Distance | http://arxiv.org/abs/1504.04792 | author:Jianxin Wu, Bin-Bin Gao, Guoqing Liu category:cs.CV published:2015-04-19 summary:In computer vision, an entity such as an image or video is often representedas a set of instance vectors, which can be SIFT, motion, or deep learningfeature vectors extracted from different parts of that entity. Thus, it isessential to design efficient and effective methods to compare two sets ofinstance vectors. Existing methods such as FV, VLAD or Super Vectors haveachieved excellent results. However, this paper shows that these methods aredesigned based on a generative perspective, and a discriminative method can bemore effective in categorizing images or videos. The proposed D3(discriminative distribution distance) method effectively compares two sets astwo distributions, and proposes a directional total variation distance (DTVD)to measure how separated are they. Furthermore, a robust classifier-basedmethod is proposed to estimate DTVD robustly. The D3 method is evaluated inaction and image recognition tasks and has achieved excellent accuracy andspeed. D3 also has a synergy with FV. The combination of D3 and FV hasadvantages over D3, FV, and VLAD.
arxiv-11100-177 | Compression and the origins of Zipf's law of abbreviation | http://arxiv.org/abs/1504.04884 | author:R. Ferrer-i-Cancho, C. Bentz, C. Seguin category:cs.IT cs.CL cs.SI math.IT published:2015-04-19 summary:Languages across the world exhibit Zipf's law of abbreviation, namely morefrequent words tend to be shorter. The generalized version of the law - aninverse relationship between the frequency of a unit and its magnitude - holdsalso for the behaviours of other species and the genetic code. The apparentuniversality of this pattern in human language and its ubiquity in otherdomains calls for a theoretical understanding of its origins. To this end, wegeneralize the information theoretic concept of mean code length as a meanenergetic cost function over the probability and the magnitude of the types ofthe repertoire. We show that the minimization of that cost function and anegative correlation between probability and the magnitude of types areintimately related.
arxiv-11100-178 | Local, Private, Efficient Protocols for Succinct Histograms | http://arxiv.org/abs/1504.04686 | author:Raef Bassily, Adam Smith category:cs.CR cs.DS cs.LG F.2.0 published:2015-04-18 summary:We give efficient protocols and matching accuracy lower bounds for frequencyestimation in the local model for differential privacy. In this model,individual users randomize their data themselves, sending differentiallyprivate reports to an untrusted server that aggregates them. We study protocols that produce a succinct histogram representation of thedata. A succinct histogram is a list of the most frequent items in the data(often called "heavy hitters") along with estimates of their frequencies; thefrequency of all other items is implicitly estimated as 0. If there are $n$ users whose items come from a universe of size $d$, ourprotocols run in time polynomial in $n$ and $\log(d)$. With high probability,they estimate the accuracy of every item up to error$O\left(\sqrt{\log(d)/(\epsilon^2n)}\right)$ where $\epsilon$ is the privacyparameter. Moreover, we show that this much error is necessary, regardless ofcomputational efficiency, and even for the simple setting where only one itemappears with significant frequency in the data set. Previous protocols (Mishra and Sandler, 2006; Hsu, Khanna and Roth, 2012) forthis task either ran in time $\Omega(d)$ or had much worse error (about$\sqrt[6]{\log(d)/(\epsilon^2n)}$), and the only known lower bound on error was$\Omega(1/\sqrt{n})$. We also adapt a result of McGregor et al (2010) to the local setting. In amodel with public coins, we show that each user need only send 1 bit to theserver. For all known local protocols (including ours), the transformationpreserves computational efficiency.
arxiv-11100-179 | On the consistency of Multithreshold Entropy Linear Classifier | http://arxiv.org/abs/1504.04740 | author:Wojciech Marian Czarnecki category:cs.LG stat.ML published:2015-04-18 summary:Multithreshold Entropy Linear Classifier (MELC) is a recent classifier ideawhich employs information theoretic concept in order to create a multithresholdmaximum margin model. In this paper we analyze its consistency overmultithreshold linear models and show that its objective function upper boundsthe amount of misclassified points in a similar manner like hinge loss does insupport vector machines. For further confirmation we also conduct somenumerical experiments on five datasets.
arxiv-11100-180 | Fast optimization of Multithreshold Entropy Linear Classifier | http://arxiv.org/abs/1504.04739 | author:Rafal Jozefowicz, Wojciech Marian Czarnecki category:cs.LG stat.ML published:2015-04-18 summary:Multithreshold Entropy Linear Classifier (MELC) is a density based modelwhich searches for a linear projection maximizing the Cauchy-Schwarz Divergenceof dataset kernel density estimation. Despite its good empirical results, oneof its drawbacks is the optimization speed. In this paper we analyze how onecan speed it up through solving an approximate problem. We analyze two methods,both similar to the approximate solutions of the Kernel Density Estimationquerying and provide adaptive schemes for selecting a crucial parameters basedon user-specified acceptable error. Furthermore we show how one can exploitwell known conjugate gradients and L-BFGS optimizers despite the fact that theoriginal optimization problem should be solved on the sphere. All above methodsand modifications are tested on 10 real life datasets from UCI repository toconfirm their practical usability.
arxiv-11100-181 | A Knowledge-poor Pronoun Resolution System for Turkish | http://arxiv.org/abs/1504.04751 | author:Dilek Küçük, Meltem Turhan Yöndem category:cs.CL published:2015-04-18 summary:A pronoun resolution system which requires limited syntactic knowledge toidentify the antecedents of personal and reflexive pronouns in Turkish ispresented. As in its counterparts for languages like English, Spanish andFrench, the core of the system is the constraints and preferences determinedempirically. In the evaluation phase, it performed considerably better than thebaseline algorithm used for comparison. The system is significant for its beingthe first fully specified knowledge-poor computational framework for pronounresolution in Turkish where Turkish possesses different structural propertiesfrom the languages for which knowledge-poor systems had been developed.
arxiv-11100-182 | Time Resolution Dependence of Information Measures for Spiking Neurons: Atoms, Scaling, and Universality | http://arxiv.org/abs/1504.04756 | author:Sarah E. Marzen, Michael R. DeWeese, James P. Crutchfield category:q-bio.NC cs.NE math.PR nlin.CD published:2015-04-18 summary:The mutual information between stimulus and spike-train response is commonlyused to monitor neural coding efficiency, but neuronal computation broadlyconceived requires more refined and targeted information measures ofinput-output joint processes. A first step towards that larger goal is todevelop information measures for individual output processes, includinginformation generation (entropy rate), stored information (statisticalcomplexity), predictable information (excess entropy), and active informationaccumulation (bound information rate). We calculate these for spike trainsgenerated by a variety of noise-driven integrate-and-fire neurons as a functionof time resolution and for alternating renewal processes. We show that theirtime-resolution dependence reveals coarse-grained structural properties ofinterspike interval statistics; e.g., $\tau$-entropy rates that diverge lessquickly than the firing rate indicate interspike interval correlations. We alsofind evidence that the excess entropy and regularized statistical complexity ofdifferent types of integrate-and-fire neurons are universal in thecontinuous-time limit in the sense that they do not depend on mechanismdetails. This suggests a surprising simplicity in the spike trains generated bythese model neurons. Interestingly, neurons with gamma-distributed ISIs andneurons whose spike trains are alternating renewal processes do not fall intothe same universality class. These results lead to two conclusions. First, thedependence of information measures on time resolution reveals mechanisticdetails about spike train generation. Second, information measures can be usedas model selection tools for analyzing spike train processes.
arxiv-11100-183 | Understanding the Fisher Vector: a multimodal part model | http://arxiv.org/abs/1504.04763 | author:David Novotný, Diane Larlus, Florent Perronnin, Andrea Vedaldi category:cs.CV published:2015-04-18 summary:Fisher Vectors and related orderless visual statistics have demonstratedexcellent performance in object detection, sometimes superior to establishedapproaches such as the Deformable Part Models. However, it remains unclear howthese models can capture complex appearance variations using visual codebooksof limited sizes and coarse geometric information. In this work, we propose tointerpret Fisher-Vector-based object detectors as part-based models. Throughthe use of several visualizations and experiments, we show that this is auseful insight to explain the good performance of the model. Furthermore, wereveal for the first time several interesting properties of the FV, includingits ability to work well using only a small subset of input patches and visualwords. Finally, we discuss the relation of the FV and DPM detectors, pointingout differences and commonalities between them.
arxiv-11100-184 | Online Inference for Relation Extraction with a Reduced Feature Set | http://arxiv.org/abs/1504.04770 | author:Maxim Rabinovich, Cédric Archambeau category:cs.CL cs.LG published:2015-04-18 summary:Access to web-scale corpora is gradually bringing robust automatic knowledgebase creation and extension within reach. To exploit these largeunannotated---and extremely difficult to annotate---corpora, unsupervisedmachine learning methods are required. Probabilistic models of text haverecently found some success as such a tool, but scalability remains an obstaclein their application, with standard approaches relying on sampling schemes thatare known to be difficult to scale. In this report, we therefore present anempirical assessment of the sublinear time sparse stochastic variationalinference (SSVI) scheme applied to RelLDA. We demonstrate that online inferenceleads to relatively strong qualitative results but also identify some of itspathologies---and those of the model---which will need to be overcome if SSVIis to be used for large-scale relation extraction.
arxiv-11100-185 | Gap Analysis of Natural Language Processing Systems with respect to Linguistic Modality | http://arxiv.org/abs/1504.04716 | author:Vishal Shukla category:cs.CL cs.AI published:2015-04-18 summary:Modality is one of the important components of grammar in linguistics. Itlets speaker to express attitude towards, or give assessment or potentiality ofstate of affairs. It implies different senses and thus has differentperceptions as per the context. This paper presents an account showing the gapin the functionality of the current state of art Natural Language Processing(NLP) systems. The contextual nature of linguistic modality is studied. In thispaper, the works and logical approaches employed by Natural Language Processingsystems dealing with modality are reviewed. It sees human cognition andintelligence as multi-layered approach that can be implemented by intelligentsystems for learning. Lastly, current flow of research going on within thisfield is talked providing futurology.
arxiv-11100-186 | Unsupervised Dependency Parsing: Let's Use Supervised Parsers | http://arxiv.org/abs/1504.04666 | author:Phong Le, Willem Zuidema category:cs.CL cs.LG published:2015-04-18 summary:We present a self-training approach to unsupervised dependency parsing thatreuses existing supervised and unsupervised parsing algorithms. Our approach,called `iterated reranking' (IR), starts with dependency trees generated by anunsupervised parser, and iteratively improves these trees using the richerprobability models used in supervised parsing that are in turn trained on thesetrees. Our system achieves 1.8% accuracy higher than the state-of-the-partparser of Spitkovsky et al. (2013) on the WSJ corpus.
arxiv-11100-187 | Testing Closeness With Unequal Sized Samples | http://arxiv.org/abs/1504.04599 | author:Bhaswar B. Bhattacharya, Gregory Valiant category:cs.LG cs.IT math.IT math.ST stat.ML stat.TH published:2015-04-17 summary:We consider the problem of closeness testing for two discrete distributionsin the practically relevant setting of \emph{unequal} sized samples drawn fromeach of them. Specifically, given a target error parameter $\varepsilon > 0$,$m_1$ independent draws from an unknown distribution $p,$ and $m_2$ draws froman unknown distribution $q$, we describe a test for distinguishing the casethat $p=q$ from the case that $p-q_1 \geq \varepsilon$. If $p$ and $q$ aresupported on at most $n$ elements, then our test is successful with highprobability provided $m_1\geq n^{2/3}/\varepsilon^{4/3}$ and $m_2 =\Omega(\max\{\frac{n}{\sqrt m_1\varepsilon^2}, \frac{\sqrtn}{\varepsilon^2}\});$ we show that this tradeoff is optimal throughout thisrange, to constant factors. These results extend the recent work of Chan et al.who established the sample complexity when the two samples have equal sizes,and tightens the results of Acharya et al. by polynomials factors in both $n$and $\varepsilon$. As a consequence, we obtain an algorithm for estimating themixing time of a Markov chain on $n$ states up to a $\log n$ factor that uses$\tilde{O}(n^{3/2} \tau_{mix})$ queries to a "next node" oracle, improving uponthe $\tilde{O}(n^{5/3}\tau_{mix})$ query algorithm of Batu et al. Finally, wenote that the core of our testing algorithm is a relatively simple statisticthat seems to perform well in practice, both on synthetic data and on naturallanguage data.
arxiv-11100-188 | A spectral optical flow method for determining velocities from digital imagery | http://arxiv.org/abs/1504.04660 | author:Neal Hurlburt, Steve Jaffey category:cs.CV astro-ph.IM published:2015-04-17 summary:We present a method for determining surface flows from solar images basedupon optical flow techniques. We apply the method to sets of images obtained bya variety of solar imagers to assess its performance. The {\tt opflow3d}procedure is shown to extract accurate velocity estimates when provided perfecttest data and quickly generates results consistent with completely distinctmethods when applied on global scales. We also validate it in detail bycomparing it to an established method when applied to high-resolution datasetsand find that it provides comparable results without the need to tune, filteror otherwise preprocess the images before its application.
arxiv-11100-189 | Feasibility Preserving Constraint-Handling Strategies for Real Parameter Evolutionary Optimization | http://arxiv.org/abs/1504.04421 | author:Nikhil Padhye, Pulkit Mittal, Kalyanmoy Deb category:cs.NE published:2015-04-17 summary:Evolutionary Algorithms (EAs) are being routinely applied for a variety ofoptimization tasks, and real-parameter optimization in the presence ofconstraints is one such important area. During constrained optimization EAsoften create solutions that fall outside the feasible region; hence a viableconstraint- handling strategy is needed. This paper focuses on the class ofconstraint-handling strategies that repair infeasible solutions by bringingthem back into the search space and explicitly preserve feasibility of thesolutions. Several existing constraint-handling strategies are studied, and twonew single parameter constraint-handling methodologies based on parent-centricand inverse parabolic probability (IP) distribution are proposed. The existingand newly proposed constraint-handling methods are first studied with PSO, DE,GAs, and simulation results on four scalable test-problems under differentlocation settings of the optimum are presented. The newly proposedconstraint-handling methods exhibit robustness in terms of performance and alsosucceed on search spaces comprising up-to 500 variables while locating theoptimum within an error of 10$^{-10}$. The working principle of the IP basedmethods is also demonstrated on (i) some generic constrained optimizationproblems, and (ii) a classic `Weld' problem from structural design andmechanics. The successful performance of the proposed methods clearly exhibitstheir efficacy as a generic constrained-handling strategy for a wide range ofapplications.
arxiv-11100-190 | Deep Karaoke: Extracting Vocals from Musical Mixtures Using a Convolutional Deep Neural Network | http://arxiv.org/abs/1504.04658 | author:Andrew J. R. Simpson, Gerard Roma, Mark D. Plumbley category:cs.SD cs.LG cs.NE 68Txx published:2015-04-17 summary:Identification and extraction of singing voice from within musical mixturesis a key challenge in source separation and machine audition. Recently, deepneural networks (DNN) have been used to estimate 'ideal' binary masks forcarefully controlled cocktail party speech separation problems. However, it isnot yet known whether these methods are capable of generalizing to thediscrimination of voice and non-voice in the context of musical mixtures. Here,we trained a convolutional DNN (of around a billion parameters) to provideprobabilistic estimates of the ideal binary mask for separation of vocal soundsfrom real-world musical mixtures. We contrast our DNN results with moretraditional linear methods. Our approach may be useful for automatic removal ofvocal sounds from musical mixtures for 'karaoke' type applications.
arxiv-11100-191 | The Nataf-Beta Random Field Classifier: An Extension of the Beta Conjugate Prior to Classification Problems | http://arxiv.org/abs/1504.04588 | author:James-A. Goulet category:cs.LG I.5.2 published:2015-04-17 summary:This paper presents the Nataf-Beta Random Field Classifier, a discriminativeapproach that extends the applicability of the Beta conjugate prior toclassification problems. The approach's key feature is to model the probabilityof a class conditional on attribute values as a random field whose marginalsare Beta distributed, and where the parameters of marginals are themselvesdescribed by random fields. Although the classification accuracy of theapproach proposed does not statistically outperform the best accuraciesreported in the literature, it ranks among the top tier for the six benchmarkdatasets tested. The Nataf-Beta Random Field Classifier is suited as a generalpurpose classification approach for real-continuous and real-integer attributevalue problems.
arxiv-11100-192 | Biometrics for Child Vaccination and Welfare: Persistence of Fingerprint Recognition for Infants and Toddlers | http://arxiv.org/abs/1504.04651 | author:Anil K. Jain, Sunpreet S. Arora, Lacey Best-Rowden, Kai Cao, Prem Sewak Sudhish, Anjoo Bhatnagar category:cs.CV published:2015-04-17 summary:With a number of emerging applications requiring biometric recognition ofchildren (e.g., tracking child vaccination schedules, identifying missingchildren and preventing newborn baby swaps in hospitals), investigating thetemporal stability of biometric recognition accuracy for children is important.The persistence of recognition accuracy of three of the most commonly usedbiometric traits (fingerprints, face and iris) has been investigated foradults. However, persistence of biometric recognition accuracy has not beenstudied systematically for children in the age group of 0-4 years. Given thatvery young children are often uncooperative and do not comprehend or followinstructions, in our opinion, among all biometric modalities, fingerprints arethe most viable for recognizing children. This is primarily because it iseasier to capture fingerprints of young children compared to other biometrictraits, e.g., iris, where a child needs to stare directly towards the camera toinitiate iris capture. In this report, we detail our initiative to investigatethe persistence of fingerprint recognition for children in the age group of 0-4years. Based on preliminary results obtained for the data collected in thefirst phase of our study, use of fingerprints for recognition of 0-4 year-oldchildren appears promising.
arxiv-11100-193 | Color Constancy Using CNNs | http://arxiv.org/abs/1504.04548 | author:Simone Bianco, Claudio Cusano, Raimondo Schettini category:cs.CV published:2015-04-17 summary:In this work we describe a Convolutional Neural Network (CNN) to accuratelypredict the scene illumination. Taking image patches as input, the CNN works inthe spatial domain without using hand-crafted features that are employed bymost previous methods. The network consists of one convolutional layer with maxpooling, one fully connected layer and three output nodes. Within the networkstructure, feature learning and regression are integrated into one optimizationprocess, which leads to a more effective model for estimating sceneillumination. This approach achieves state-of-the-art performance on a standarddataset of RAW images. Preliminary experiments on images with spatially varyingillumination demonstrate the stability of the local illuminant estimationability of our CNN.
arxiv-11100-194 | Performance Evaluation of Machine Learning Algorithms in Post-operative Life Expectancy in the Lung Cancer Patients | http://arxiv.org/abs/1504.04646 | author:Kwetishe Joro Danjuma category:cs.LG published:2015-04-17 summary:The nature of clinical data makes it difficult to quickly select, tune andapply machine learning algorithms to clinical prognosis. As a result, a lot oftime is spent searching for the most appropriate machine learning algorithmsapplicable in clinical prognosis that contains either binary-valued ormulti-valued attributes. The study set out to identify and evaluate theperformance of machine learning classification schemes applied in clinicalprognosis of post-operative life expectancy in the lung cancer patients.Multilayer Perceptron, J48, and the Naive Bayes algorithms were used to trainand test models on Thoracic Surgery datasets obtained from the University ofCalifornia Irvine machine learning repository. Stratified 10-foldcross-validation was used to evaluate baseline performance accuracy of theclassifiers. The comparative analysis shows that multilayer perceptronperformed best with classification accuracy of 82.3%, J48 came out second withclassification accuracy of 81.8%, and Naive Bayes came out the worst withclassification accuracy of 74.4%. The quality and outcome of the chosen machinelearning algorithms depends on the ingenuity of the clinical miner.
arxiv-11100-195 | Hyperspectral pansharpening: a review | http://arxiv.org/abs/1504.04531 | author:Laetitia Loncan, Luis B. Almeida, José M. Bioucas-Dias, Xavier Briottet, Jocelyn Chanussot, Nicolas Dobigeon, Sophie Fabre, Wenzhi Liao, Giorgio A. Licciardi, Miguel Simões, Jean-Yves Tourneret, Miguel A. Veganzones, Gemine Vivone, Qi Wei, Naoto Yokoya category:cs.CV stat.AP published:2015-04-17 summary:Pansharpening aims at fusing a panchromatic image with a multispectral one,to generate an image with the high spatial resolution of the former and thehigh spectral resolution of the latter. In the last decade, many algorithmshave been presented in the literature for pansharpening using multispectraldata. With the increasing availability of hyperspectral systems, these methodsare now being adapted to hyperspectral images. In this work, we compare newpansharpening techniques designed for hyperspectral data with some of the stateof the art methods for multispectral pansharpening, which have been adapted forhyperspectral data. Eleven methods from different classes (componentsubstitution, multiresolution analysis, hybrid, Bayesian and matrixfactorization) are analyzed. These methods are applied to three datasets andtheir effectiveness and robustness are evaluated with widely used performanceindicators. In addition, all the pansharpening techniques considered in thispaper have been implemented in a MATLAB toolbox that is made available to thecommunity.
arxiv-11100-196 | Faster Algorithms for Testing under Conditional Sampling | http://arxiv.org/abs/1504.04103 | author:Moein Falahatgar, Ashkan Jafarpour, Alon Orlitsky, Venkatadheeraj Pichapathi, Ananda Theertha Suresh category:cs.DS cs.CC cs.LG math.ST stat.TH published:2015-04-16 summary:There has been considerable recent interest in distribution-tests whoserun-time and sample requirements are sublinear in the domain-size $k$. We studytwo of the most important tests under the conditional-sampling model where eachquery specifies a subset $S$ of the domain, and the response is a sample drawnfrom $S$ according to the underlying distribution. For identity testing, which asks whether the underlying distribution equals aspecific given distribution or $\epsilon$-differs from it, we reduce the knowntime and sample complexities from $\tilde{\mathcal{O}}(\epsilon^{-4})$ to$\tilde{\mathcal{O}}(\epsilon^{-2})$, thereby matching the informationtheoretic lower bound. For closeness testing, which asks whether twodistributions underlying observed data sets are equal or different, we reduceexisting complexity from $\tilde{\mathcal{O}}(\epsilon^{-4} \log^5 k)$ to aneven sub-logarithmic $\tilde{\mathcal{O}}(\epsilon^{-5} \log \log k)$ thusproviding a better bound to an open problem in Bertinoro Workshop on SublinearAlgorithms [Fisher, 2004].
arxiv-11100-197 | Segmentation of Subspaces in Sequential Data | http://arxiv.org/abs/1504.04090 | author:Stephen Tierney, Yi Guo, Junbin Gao category:cs.CV published:2015-04-16 summary:We propose Ordered Subspace Clustering (OSC) to segment data drawn from asequentially ordered union of subspaces. Similar to Sparse Subspace Clustering(SSC) we formulate the problem as one of finding a sparse representation butinclude an additional penalty term to take care of sequential data. We test ourmethod on data drawn from infrared hyper spectral, video and motion capturedata. Experiments show that our method, OSC, outperforms the state of the artmethods: Spatial Subspace Clustering (SpatSC), Low-Rank Representation (LRR)and SSC.
arxiv-11100-198 | FPA-CS: Focal Plane Array-based Compressive Imaging in Short-wave Infrared | http://arxiv.org/abs/1504.04085 | author:Huaijin Chen, M. Salman Asif, Aswin C. Sankaranarayanan, Ashok Veeraraghavan category:cs.CV published:2015-04-16 summary:Cameras for imaging in short and mid-wave infrared spectra are significantlymore expensive than their counterparts in visible imaging. As a result,high-resolution imaging in those spectrum remains beyond the reach of mostconsumers. Over the last decade, compressive sensing (CS) has emerged as apotential means to realize inexpensive short-wave infrared cameras. Oneapproach for doing this is the single-pixel camera (SPC) where a singledetector acquires coded measurements of a high-resolution image. Acomputational reconstruction algorithm is then used to recover the image fromthese coded measurements. Unfortunately, the measurement rate of a SPC isinsufficient to enable imaging at high spatial and temporal resolutions. We present a focal plane array-based compressive sensing (FPA-CS)architecture that achieves high spatial and temporal resolutions. The idea isto use an array of SPCs that sense in parallel to increase the measurementrate, and consequently, the achievable spatio-temporal resolution of thecamera. We develop a proof-of-concept prototype in the short-wave infraredusing a sensor with 64$\times$ 64 pixels; the prototype provides a 4096$\times$increase in the measurement rate compared to the SPC and achieves a megapixelresolution at video rate using CS techniques.
arxiv-11100-199 | Actively Learning to Attract Followers on Twitter | http://arxiv.org/abs/1504.04114 | author:Nir Levine, Timothy A. Mann, Shie Mannor category:stat.ML cs.LG cs.SI published:2015-04-16 summary:Twitter, a popular social network, presents great opportunities for on-linemachine learning research. However, previous research has focused almostentirely on learning from passively collected data. We study the problem oflearning to acquire followers through normative user behavior, as opposed tothe mass following policies applied by many bots. We formalize the problem as acontextual bandit problem, in which we consider retweeting content to be theaction chosen and each tweet (content) is accompanied by context. We designreward signals based on the change in followers. The result of our month longexperiment with 60 agents suggests that (1) aggregating experience acrossagents can adversely impact prediction accuracy and (2) the Twitter community'sresponse to different actions is non-stationary. Our findings suggest thatactively learning on-line can provide deeper insights about how to attractfollowers than machine learning over passively collected data alone.
arxiv-11100-200 | Multichannel sparse recovery of complex-valued signals using Huber's criterion | http://arxiv.org/abs/1504.04184 | author:Esa Ollila category:cs.IT math.IT stat.CO stat.ML published:2015-04-16 summary:In this paper, we generalize Huber's criterion to multichannel sparserecovery problem of complex-valued measurements where the objective is to findgood recovery of jointly sparse unknown signal vectors from the given multiplemeasurement vectors which are different linear combinations of the same knownelementary vectors. This requires careful characterization of robustcomplex-valued loss functions as well as Huber's criterion function for themultivariate sparse regression problem. We devise a greedy algorithm based onsimultaneous normalized iterative hard thresholding (SNIHT) algorithm. Unlikethe conventional SNIHT method, our algorithm, referred to as HUB-SNIHT, isrobust under heavy-tailed non-Gaussian noise conditions, yet has a negligibleperformance loss compared to SNIHT under Gaussian noise. Usefulness of themethod is illustrated in source localization application with sensor arrays.
arxiv-11100-201 | Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting | http://arxiv.org/abs/1504.04407 | author:Jakub Konečný, Jie Liu, Peter Richtárik, Martin Takáč category:cs.LG stat.ML published:2015-04-16 summary:We propose mS2GD: a method incorporating a mini-batching scheme for improvingthe theoretical complexity and practical performance of semi-stochasticgradient descent (S2GD). We consider the problem of minimizing a stronglyconvex function represented as the sum of an average of a large number ofsmooth convex functions, and a simple nonsmooth convex regularizer. Our methodfirst performs a deterministic step (computation of the gradient of theobjective function at the starting point), followed by a large number ofstochastic steps. The process is repeated a few times with the last iteratebecoming the new starting point. The novelty of our method is in introductionof mini-batching into the computation of stochastic steps. In each step,instead of choosing a single function, we sample $b$ functions, compute theirgradients, and compute the direction based on this. We analyze the complexityof the method and show that it benefits from two speedup effects. First, weprove that as long as $b$ is below a certain threshold, we can reach anypredefined accuracy with less overall work than without mini-batching. Second,our mini-batching scheme admits a simple parallel implementation, and hence issuitable for further acceleration by parallelization.
arxiv-11100-202 | Caffe con Troll: Shallow Ideas to Speed Up Deep Learning | http://arxiv.org/abs/1504.04343 | author:Stefan Hadjis, Firas Abuzaid, Ce Zhang, Christopher Ré category:cs.LG cs.CV stat.ML published:2015-04-16 summary:We present Caffe con Troll (CcT), a fully compatible end-to-end version ofthe popular framework Caffe with rebuilt internals. We built CcT to examine theperformance characteristics of training and deploying general-purposeconvolutional neural networks across different hardware architectures. We findthat, by employing standard batching optimizations for CPU training, we achievea 4.5x throughput improvement over Caffe on popular networks like CaffeNet.Moreover, with these improvements, the end-to-end training time for CNNs isdirectly proportional to the FLOPS delivered by the CPU, which enables us toefficiently train hybrid CPU-GPU systems for CNNs.
arxiv-11100-203 | Face Prediction Model for an Automatic Age-invariant Face Recognition System | http://arxiv.org/abs/1506.06046 | author:Poonam Yadav category:cs.CV cs.NE published:2015-04-16 summary:Automated face recognition and identification softwares are becoming part ofour daily life; it finds its abode not only with Facebook's auto photo tagging,Apple's iPhoto, Google's Picasa, Microsoft's Kinect, but also in HomelandSecurity Department's dedicated biometric face detection systems. Most of theseautomatic face identification systems fail where the effects of aging come intothe picture. Little work exists in the literature on the subject of faceprediction that accounts for aging, which is a vital part of the computer facerecognition systems. In recent years, individual face components' (e.g. eyes,nose, mouth) features based matching algorithms have emerged, but theseapproaches are still not efficient. Therefore, in this work we describe a FacePrediction Model (FPM), which predicts human face aging or growth related imagevariation using Principle Component Analysis (PCA) and Artificial NeuralNetwork (ANN) learning techniques. The FPM captures the facial changes, whichoccur with human aging and predicts the facial image with a few years of gapwith an acceptable accuracy of face matching from 76 to 86%.
arxiv-11100-204 | Genetic algorithm implementation for effective document subject search | http://arxiv.org/abs/1504.04216 | author:V. K. Ivanov, P. I. Meskin category:cs.IR cs.NE published:2015-04-16 summary:This paper describes the software implementation of genetic algorithm foridentifying and selecting most relevant results received during sequentiallyexecuted subject search operations. Simulated evolutionary process generatessustainable and effective population of search queries, forms search pattern ofdocuments or semantic core, creates relevant sets of required documents, allowsautomatic classification of search results. The paper discusses the features ofsubject search, justifies the use of a genetic algorithm, describes argumentsof the fitness function and describes basic steps and parameters of thealgorithm.
arxiv-11100-205 | Towards a relation extraction framework for cyber-security concepts | http://arxiv.org/abs/1504.04317 | author:Corinne L. Jones, Robert A. Bridges, Kelly Huffer, John Goodall category:cs.IR cs.CL cs.CR H.3.3 published:2015-04-16 summary:In order to assist security analysts in obtaining information pertaining totheir network, such as novel vulnerabilities, exploits, or patches, informationretrieval methods tailored to the security domain are needed. As labeled textdata is scarce and expensive, we follow developments in semi-supervised NaturalLanguage Processing and implement a bootstrapping algorithm for extractingsecurity entities and their relationships from text. The algorithm requireslittle input data, specifically, a few relations or patterns (heuristics foridentifying relations), and incorporates an active learning component whichqueries the user on the most important decisions to prevent drifting from thedesired relations. Preliminary testing on a small corpus shows promisingresults, obtaining precision of .82.
arxiv-11100-206 | Non-Uniform Stochastic Average Gradient Method for Training Conditional Random Fields | http://arxiv.org/abs/1504.04406 | author:Mark Schmidt, Reza Babanezhad, Mohamed Osama Ahmed, Aaron Defazio, Ann Clifton, Anoop Sarkar category:stat.ML cs.LG math.OC stat.CO published:2015-04-16 summary:We apply stochastic average gradient (SAG) algorithms for trainingconditional random fields (CRFs). We describe a practical implementation thatuses structure in the CRF gradient to reduce the memory requirement of thislinearly-convergent stochastic gradient method, propose a non-uniform samplingscheme that substantially improves practical performance, and analyze the rateof convergence of the SAGA variant under non-uniform sampling. Our experimentalresults reveal that our method often significantly outperforms existing methodsin terms of the training objective, and performs as well or better thanoptimally-tuned stochastic gradient methods in terms of test error.
arxiv-11100-207 | Theory of Dual-sparse Regularized Randomized Reduction | http://arxiv.org/abs/1504.03991 | author:Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu category:cs.LG stat.ML published:2015-04-15 summary:In this paper, we study randomized reduction methods, which reducehigh-dimensional features into low-dimensional space by randomized methods(e.g., random projection, random hashing), for large-scale high-dimensionalclassification. Previous theoretical results on randomized reduction methodshinge on strong assumptions about the data, e.g., low rank of the data matrixor a large separable margin of classification, which hinder their applicationsin broad domains. To address these limitations, we propose dual-sparseregularized randomized reduction methods that introduce a sparse regularizerinto the reduced dual problem. Under a mild condition that the original dualsolution is a (nearly) sparse vector, we show that the resulting dual solutionis close to the original dual solution and concentrates on its support set. Innumerical experiments, we present an empirical study to support the analysisand we also present a novel application of the dual-sparse regularizedrandomized reduction methods to reducing the communication cost of distributedlearning from large-scale high-dimensional data.
arxiv-11100-208 | Comparisons of wavelet functions in QRS signal to noise ratio enhancement and detection accuracy | http://arxiv.org/abs/1504.03834 | author:Pornchai Phukpattaranont category:cs.CV cs.CE published:2015-04-15 summary:We compare the capability of wavelet functions used for noise removal inpreprocessing step of a QRS detection algorithm in the electrocardiogram (ECG)signal. The QRS signal to noise ratio enhancement and the detection accuracy ofeach wavelet function are evaluated using three measures: (1) the ratio of themaximum beat amplitude to the minimum beat amplitude (RMM), (2) the mean ofabsolute of time error (MATE), and (3) the figure of merit (FOM). Three waveletfunctions from previous well-known publications are explored, i.e., Bior1.3,Db10, and Mexican hat wavelet functions. Results evaluated with the ECG signalfrom MIT-BIH arrhythmia database show that the Mexican hat wavelet function isbetter than the others. While the scale 8 of Mexican hat wavelet function canprovide the best enhancement in QRS signal to noise ratio, the scale 4 ofMexican hat wavelet function can provide the best detection accuracy. Theseresults may be combined and may enable the use of a single fixed threshold forall ECG records leading to the reduction in computational complexity of the QRSdetection algorithm.
arxiv-11100-209 | A Generative Model for Deep Convolutional Learning | http://arxiv.org/abs/1504.04054 | author:Yunchen Pu, Xin Yuan, Lawrence Carin category:stat.ML cs.LG cs.NE published:2015-04-15 summary:A generative model is developed for deep (multi-layered) convolutionaldictionary learning. A novel probabilistic pooling operation is integrated intothe deep model, yielding efficient bottom-up (pretraining) and top-down(refinement) probabilistic learning. Experimental results demonstrate powerfulcapabilities of the model to learn multi-layer features from images, andexcellent classification results are obtained on the MNIST and Caltech 101datasets.
arxiv-11100-210 | Bio-inspired Unsupervised Learning of Visual Features Leads to Robust Invariant Object Recognition | http://arxiv.org/abs/1504.03871 | author:Saeed Reza Kheradpisheh, Mohammad Ganjtabesh, Timothée Masquelier category:cs.CV q-bio.NC published:2015-04-15 summary:Retinal image of surrounding objects varies tremendously due to the changesin position, size, pose, illumination condition, background context, occlusion,noise, and nonrigid deformations. But despite these huge variations, our visualsystem is able to invariantly recognize any object in just a fraction of asecond. To date, various computational models have been proposed to mimic thehierarchical processing of the ventral visual pathway, with limited success.Here, we show that combining a biologically inspired network architecture witha biologically inspired learning rule significantly improves the models'performance when facing challenging object recognition problems. Our model isan asynchronous feedforward spiking neural network. When the network ispresented with natural images, the neurons in the entry layers detect edges,and the most activated ones fire first, while neurons in higher layers areequipped with spike timing-dependent plasticity. These neurons progressivelybecome selective to intermediate complexity visual features appropriate forobject categorization, as demonstrated using the 3D Object dataset provided bySavarese et al. at CVGLab, Stanford University. The model reached 96%categorization accuracy, which corresponds to two to three times fewer errorsthan the previous state-of-the-art, demonstrating that it is able to accuratelyrecognize different instances of multiple object classes in various appearanceconditions (different views, scales, tilts, and backgrounds). Severalstatistical analysis techniques are used to show that our model extracts classspecific and highly informative features.
arxiv-11100-211 | Anatomy-specific classification of medical images using deep convolutional nets | http://arxiv.org/abs/1504.04003 | author:Holger R. Roth, Christopher T. Lee, Hoo-Chang Shin, Ari Seff, Lauren Kim, Jianhua Yao, Le Lu, Ronald M. Summers category:cs.CV published:2015-04-15 summary:Automated classification of human anatomy is an important prerequisite formany computer-aided diagnosis systems. The spatial complexity and variabilityof anatomy throughout the human body makes classification difficult. "Deeplearning" methods such as convolutional networks (ConvNets) outperform otherstate-of-the-art methods in image classification tasks. In this work, wepresent a method for organ- or body-part-specific anatomical classification ofmedical images acquired using computed tomography (CT) with ConvNets. We traina ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomicalclasses. Key-images were mined from a hospital PACS archive, using a set of1,675 patients. We show that a data augmentation approach can help to enrichthe data set and improve classification performance. Using ConvNets and dataaugmentation, we achieve anatomy-specific classification error of 5.9 % andarea-under-the-curve (AUC) values of an average of 0.998 in testing. Wedemonstrate that deep learning can be used to train very reliable and accurateclassifiers that could initialize further computer-aided diagnosis.
arxiv-11100-212 | Deep convolutional networks for pancreas segmentation in CT imaging | http://arxiv.org/abs/1504.03967 | author:Holger R. Roth, Amal Farag, Le Lu, Evrim B. Turkbey, Ronald M. Summers category:cs.CV published:2015-04-15 summary:Automatic organ segmentation is an important prerequisite for manycomputer-aided diagnosis systems. The high anatomical variability of organs inthe abdomen, such as the pancreas, prevents many segmentation methods fromachieving high accuracies when compared to other segmentation of organs likethe liver, heart or kidneys. Recently, the availability of large annotatedtraining sets and the accessibility of affordable parallel computing resourcesvia GPUs have made it feasible for "deep learning" methods such asconvolutional networks (ConvNets) to succeed in image classification tasks.These methods have the advantage that used classification features are traineddirectly from the imaging data. We present a fully-automated bottom-up methodfor pancreas segmentation in computed tomography (CT) images of the abdomen.The method is based on hierarchical coarse-to-fine classification of localimage regions (superpixels). Superpixels are extracted from the abdominalregion using Simple Linear Iterative Clustering (SLIC). An initial probabilityresponse map is generated, using patch-level confidences and a two-levelcascade of random forest classifiers, from which superpixel regions withprobabilities larger 0.5 are retained. These retained superpixels serve as ahighly sensitive initial input of the pancreas and its surroundings to aConvNet that samples a bounding box around each superpixel at different scales(and random non-rigid deformations at training time) in order to assign a moredistinct probability of each superpixel region being pancreas or not. Weevaluate our method on CT images of 82 patients (60 for training, 2 forvalidation, and 20 for testing). Using ConvNets we achieve average Dice scoresof 68%+-10% (range, 43-80%) in testing. This shows promise for accuratepancreas segmentation, using a deep learning approach and compares favorably tostate-of-the-art methods.
arxiv-11100-213 | Text Localization in Video Using Multiscale Weber's Local Descriptor | http://arxiv.org/abs/1504.03810 | author:B. H. Shekar, Smitha M. L. category:cs.CV published:2015-04-15 summary:In this paper, we propose a novel approach for detecting the text present invideos and scene images based on the Multiscale Weber's Local Descriptor(MWLD). Given an input video, the shots are identified and the key frames areextracted based on their spatio-temporal relationship. From each key frame, wedetect the local region information using WLD with different radius andneighborhood relationship of pixel values and hence obtained intensity enhancedkey frames at multiple scales. These multiscale WLD key frames are mergedtogether and then the horizontal gradients are computed using morphologicaloperations. The obtained results are then binarized and the false positives areeliminated based on geometrical properties. Finally, we employ connectedcomponent analysis and morphological dilation operation to determine the textregions that aids in text localization. The experimental results obtained onpublicly available standard Hua, Horizontal-1 and Horizontal-2 video datasetillustrate that the proposed method can accurately detect and localize texts ofvarious sizes, fonts and colors in videos.
arxiv-11100-214 | Tracking Live Fish from Low-Contrast and Low-Frame-Rate Stereo Videos | http://arxiv.org/abs/1504.03811 | author:Meng-Che Chuang, Jenq-Neng Hwang, Kresimir Williams, Richard Towler category:cs.CV published:2015-04-15 summary:Non-extractive fish abundance estimation with the aid of visual analysis hasdrawn increasing attention. Unstable illumination, ubiquitous noise and lowframe rate video capturing in the underwater environment, however, makeconventional tracking methods unreliable. In this paper, we present a multiplefish tracking system for low-contrast and low-frame-rate stereo videos with theuse of a trawl-based underwater camera system. An automatic fish segmentationalgorithm overcomes the low-contrast issues by adopting a histogrambackprojection approach on double local-thresholded images to ensure anaccurate segmentation on the fish shape boundaries. Built upon a reliablefeature-based object matching method, a multiple-target tracking algorithm viaa modified Viterbi data association is proposed to overcome the poor motioncontinuity and frequent entrance/exit of fish targets under low-frame-ratescenarios. In addition, a computationally efficient block-matching approachperforms successful stereo matching, which enables an automatic fish-body tailcompensation to greatly reduce segmentation error and allows for an accuratefish length measurement. Experimental results show that an effective andreliable tracking performance for multiple live fish with underwater stereocameras is achieved.
arxiv-11100-215 | Bridging belief function theory to modern machine learning | http://arxiv.org/abs/1504.03874 | author:Thomas Burger category:cs.AI cs.LG published:2015-04-15 summary:Machine learning is a quickly evolving field which now looks really differentfrom what it was 15 years ago, when classification and clustering were majorissues. This document proposes several trends to explore the new questions ofmodern machine learning, with the strong afterthought that the belief functionframework has a major role to play.
arxiv-11100-216 | Linear Maximum Margin Classifier for Learning from Uncertain Data | http://arxiv.org/abs/1504.03892 | author:Christos Tzelepis, Vasileios Mezaris, Ioannis Patras category:cs.LG published:2015-04-15 summary:In this paper, we propose a maximum margin classifier that deals withuncertainty in data input. Specifically, we reformulate the SVM framework suchthat each input training entity is not solely a feature vector representation,but a multi-dimensional Gaussian distribution with given probability density,i.e., with a given mean and covariance matrix. The latter expresses theuncertainty. We arrive at a convex optimization problem, which is solved in theprimal form using a gradient descent approach. The resulting classifier, whichwe name SVM with Gaussian Sample Uncertainty (SVM-GSU), is tested on syntheticdata, as well as on the problem of event detection in video using thelarge-scale TRECVID MED 2014 dataset, and the problem of image classificationusing the MNIST dataset of handwritten digits. Experimental results verify theeffectiveness of the proposed classifier.
arxiv-11100-217 | Application of Enhanced-2D-CWT in Topographic Images for Mapping Landslide Risk Areas | http://arxiv.org/abs/1504.05137 | author:V. V. Vermehren Valenzuela, R. D. Lins, H. M. de Oliveira category:cs.CV physics.geo-ph published:2015-04-15 summary:There has been lately a number of catastrophic events of landslides andmudslides in the mountainous region of Rio de Janeiro, Brazil. Those werecaused by intense rain in localities where there was unplanned occupation ofslopes of hills and mountains. Thus, it became imperative creating an inventoryof landslide risk areas in densely populated cities. This work presents a wayof demarcating risk areas by using the bidimensional Continuous WaveletTransform (2D-CWT) applied to high resolution topographic images of themountainous region of Rio de Janeiro.
arxiv-11100-218 | HHCART: An Oblique Decision Tree | http://arxiv.org/abs/1504.03415 | author:D. C. Wickramarachchi, B. L. Robertson, M. Reale, C. J. Price, J. Brown category:stat.ML cs.LG published:2015-04-14 summary:Decision trees are a popular technique in statistical data classification.They recursively partition the feature space into disjoint sub-regions untileach sub-region becomes homogeneous with respect to a particular class. Thebasic Classification and Regression Tree (CART) algorithm partitions thefeature space using axis parallel splits. When the true decision boundaries arenot aligned with the feature axes, this approach can produce a complicatedboundary structure. Oblique decision trees use oblique decision boundaries topotentially simplify the boundary structure. The major limitation of thisapproach is that the tree induction algorithm is computationally expensive. Inthis article we present a new decision tree algorithm, called HHCART. Themethod utilizes a series of Householder matrices to reflect the training dataat each node during the tree construction. Each reflection is based on thedirections of the eigenvectors from each classes' covariance matrix.Considering axis parallel splits in the reflected training data provides anefficient way of finding oblique splits in the unreflected training data.Experimental results show that the accuracy and size of the HHCART trees arecomparable with some benchmark methods in the literature. The appealing featureof HHCART is that it can handle both qualitative and quantitative features inthe same oblique split.
arxiv-11100-219 | Learning to Compare Image Patches via Convolutional Neural Networks | http://arxiv.org/abs/1504.03641 | author:Sergey Zagoruyko, Nikos Komodakis category:cs.CV cs.LG cs.NE published:2015-04-14 summary:In this paper we show how to learn directly from image data (i.e., withoutresorting to manually-designed features) a general similarity function forcomparing image patches, which is a task of fundamental importance for manycomputer vision problems. To encode such a function, we opt for a CNN-basedmodel that is trained to account for a wide variety of changes in imageappearance. To that end, we explore and study multiple neural networkarchitectures, which are specifically adapted to this task. We show that suchan approach can significantly outperform the state-of-the-art on severalproblems and benchmark datasets.
arxiv-11100-220 | Clustering Assisted Fundamental Matrix Estimation | http://arxiv.org/abs/1504.03409 | author:Hao Wu, Yi Wan category:cs.CV published:2015-04-14 summary:In computer vision, the estimation of the fundamental matrix is a basicproblem that has been extensively studied. The accuracy of the estimationimposes a significant influence on subsequent tasks such as the cameratrajectory determination and 3D reconstruction. In this paper we propose a newmethod for fundamental matrix estimation that makes use of clustering a groupof 4D vectors. The key insight is the observation that among the 4D vectorsconstructed from matching pairs of points obtained from the SIFT algorithm,well-defined cluster points tend to be reliable inliers suitable forfundamental matrix estimation. Based on this, we utilizes a recently proposedefficient clustering method through density peaks seeking and propose a newclustering assisted method. Experimental results show that the proposedalgorithm is faster and more accurate than currently commonly used methods.
arxiv-11100-221 | Temporal ordering of clinical events | http://arxiv.org/abs/1504.03659 | author:Azad Dehghan category:cs.CL cs.AI published:2015-04-14 summary:This report describes a minimalistic set of methods engineered to anchorclinical events onto a temporal space. Specifically, we describe methods toextract clinical events (e.g., Problems, Treatments and Tests), temporalexpressions (i.e., time, date, duration, and frequency), and temporal links(e.g., Before, After, Overlap) between events and temporal entities. Thesemethods are developed and validated using high quality datasets.
arxiv-11100-222 | Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients | http://arxiv.org/abs/1504.03655 | author:Bo Xie, Yingyu Liang, Le Song category:cs.LG published:2015-04-14 summary:Nonlinear component analysis such as kernel Principle Component Analysis(KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used inmachine learning, statistics and data analysis, but they can not scale up tobig datasets. Recent attempts have employed random feature approximations toconvert the problem to the primal form for linear computational complexity.However, to obtain high quality solutions, the number of random features shouldbe the same order of magnitude as the number of data points, making suchapproach not directly applicable to the regime with millions of data points. We propose a simple, computationally efficient, and memory friendly algorithmbased on the "doubly stochastic gradients" to scale up a range of kernelnonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the\emph{non-convex} nature of these problems, our method enjoys theoreticalguarantees that it converges at the rate $\tilde{O}(1/t)$ to the globaloptimum, even for the top $k$ eigen subspace. Unlike many alternatives, ouralgorithm does not require explicit orthogonalization, which is infeasible onbig datasets. We demonstrate the effectiveness and scalability of our algorithmon large scale synthetic and real world datasets.
arxiv-11100-223 | A data-based classification of Slavic languages: Indices of qualitative variation applied to grapheme frequencies | http://arxiv.org/abs/1504.03608 | author:Michaela Koscová, Ján Macutek, Emmerich Kelih category:stat.AP cs.CL published:2015-04-14 summary:The Ord's graph is a simple graphical method for displaying frequencydistributions of data or theoretical distributions in the two-dimensionalplane. Its coordinates are proportions of the first three moments, eitherempirical or theoretical ones. A modification of the Ord's graph based onproportions of indices of qualitative variation is presented. Such amodification makes the graph applicable also to data of categorical character.In addition, the indices are normalized with values between 0 and 1, whichenables comparing data files divided into different numbers of categories. Boththe original and the new graph are used to display grapheme frequencies ineleven Slavic languages. As the original Ord's graph requires an assignment ofnumbers to the categories, graphemes were ordered decreasingly according totheir frequencies. Data were taken from parallel corpora, i.e., we work withgrapheme frequencies from a Russian novel and its translations to ten otherSlavic languages. Then, cluster analysis is applied to the graph coordinates.While the original graph yields results which are not linguisticallyinterpretable, the modification reveals meaningful relations among thelanguages.
arxiv-11100-224 | Image Denoising Using Low Rank Minimization With Modified Noise Estimation | http://arxiv.org/abs/1504.03439 | author:Zahid Hussain Shamsi, Hyun Sook Oh, Dai-Gyoung Kim category:cs.CV published:2015-04-14 summary:Recently, the application of low rank minimization to image denoising hasshown remarkable denoising results which are equivalent or better than those ofthe existing state-of-the-art algorithms. However, due to iterative nature oflow rank optimization, estimation of residual noise is an essential requirementafter each iteration. Currently, this noise is estimated by using the filterednoise in the previous iteration without considering the geometric structure ofthe given image. This estimate may be affected in the presence of moderate andsevere levels of noise. To obtain a more reliable estimate of residual noise,we propose a modified algorithm (GWNNM) which includes the contribution of thegeometric structure of an image to the existing noise estimation. Furthermore,the proposed algorithm exploits the difference of large and small singularvalues to enhance the edges and textures during the denoising process.Consequently, the proposed modifications achieve significant improvements inthe denoising results of the existing low rank optimization algorithms.
arxiv-11100-225 | Consensus based Detection in the Presence of Data Falsification Attacks | http://arxiv.org/abs/1504.03413 | author:Bhavya Kailkhura, Swastik Brahma, Pramod K. Varshney category:cs.SY cs.DC stat.AP stat.ML published:2015-04-14 summary:This paper considers the problem of detection in distributed networks in thepresence of data falsification (Byzantine) attacks. Detection approachesconsidered in the paper are based on fully distributed consensus algorithms,where all of the nodes exchange information only with their neighbors in theabsence of a fusion center. In such networks, we characterize the negativeeffect of Byzantines on the steady-state and transient detection performance ofthe conventional consensus based detection algorithms. To address this issue,we study the problem from the network designer's perspective. Morespecifically, we first propose a distributed weighted average consensusalgorithm that is robust to Byzantine attacks. We show that, under reasonableassumptions, the global test statistic for detection can be computed locally ateach node using our proposed consensus algorithm. We exploit the statisticaldistribution of the nodes' data to devise techniques for mitigating theinfluence of data falsifying Byzantines on the distributed detection system.Since some parameters of the statistical distribution of the nodes' data mightnot be known a priori, we propose learning based techniques to enable anadaptive design of the local fusion or update rules.
arxiv-11100-226 | Background Subtraction via Generalized Fused Lasso Foreground Modeling | http://arxiv.org/abs/1504.03707 | author:Bo Xin, Yuan Tian, Yizhou Wang, Wen Gao category:cs.CV published:2015-04-14 summary:Background Subtraction (BS) is one of the key steps in video analysis. Manybackground models have been proposed and achieved promising performance onpublic data sets. However, due to challenges such as illumination change,dynamic background etc. the resulted foreground segmentation often consists ofholes as well as background noise. In this regard, we consider generalizedfused lasso regularization to quest for intact structured foregrounds. Togetherwith certain assumptions about the background, such as the low-rank assumptionor the sparse-composition assumption (depending on whether pure backgroundframes are provided), we formulate BS as a matrix decomposition problem usingregularization terms for both the foreground and background matrices. Moreover,under the proposed formulation, the two generally distinctive backgroundassumptions can be solved in a unified manner. The optimization was carried outvia applying the augmented Lagrange multiplier (ALM) method in such a way thata fast parametric-flow algorithm is used for updating the foreground matrix.Experimental results on several popular BS data sets demonstrate the advantageof the proposed model compared to state-of-the-arts.
arxiv-11100-227 | Building Proteins in a Day: Efficient 3D Molecular Reconstruction | http://arxiv.org/abs/1504.03573 | author:Marcus A. Brubaker, Ali Punjani, David J. Fleet category:cs.CV q-bio.QM published:2015-04-14 summary:Discovering the 3D atomic structure of molecules such as proteins and virusesis a fundamental research problem in biology and medicine. ElectronCryomicroscopy (Cryo-EM) is a promising vision-based technique for structureestimation which attempts to reconstruct 3D structures from 2D images. Thispaper addresses the challenging problem of 3D reconstruction from 2D Cryo-EMimages. A new framework for estimation is introduced which relies on modernstochastic optimization techniques to scale to large datasets. We alsointroduce a novel technique which reduces the cost of evaluating the objectivefunction during optimization by over five orders or magnitude. The net resultis an approach capable of estimating 3D molecular structure from large scaledatasets in about a day on a single workstation.
arxiv-11100-228 | Efficient Scene Text Localization and Recognition with Local Character Refinement | http://arxiv.org/abs/1504.03522 | author:Lukáš Neumann, Jiří Matas category:cs.CV published:2015-04-14 summary:An unconstrained end-to-end text localization and recognition method ispresented. The method detects initial text hypothesis in a single pass by anefficient region-based method and subsequently refines the text hypothesisusing a more robust local text model, which deviates from the common assumptionof region-based methods that all characters are detected as connectedcomponents. Additionally, a novel feature based on character stroke area estimation isintroduced. The feature is efficiently computed from a region distance map, itis invariant to scaling and rotations and allows to efficiently detect textregions regardless of what portion of text they capture. The method runs in real time and achieves state-of-the-art text localizationand recognition results on the ICDAR 2013 Robust Reading dataset.
arxiv-11100-229 | Automated Analysis and Prediction of Job Interview Performance | http://arxiv.org/abs/1504.03425 | author:Iftekhar Naim, M. Iftekhar Tanveer, Daniel Gildea, Mohammed, Hoque category:cs.HC cs.AI cs.CL published:2015-04-14 summary:We present a computational framework for automatically quantifying verbal andnonverbal behaviors in the context of job interviews. The proposed framework istrained by analyzing the videos of 138 interview sessions with 69internship-seeking undergraduates at the Massachusetts Institute of Technology(MIT). Our automated analysis includes facial expressions (e.g., smiles, headgestures, facial tracking points), language (e.g., word counts, topicmodeling), and prosodic information (e.g., pitch, intonation, and pauses) ofthe interviewees. The ground truth labels are derived by taking a weightedaverage over the ratings of 9 independent judges. Our framework canautomatically predict the ratings for interview traits such as excitement,friendliness, and engagement with correlation coefficients of 0.75 or higher,and can quantify the relative importance of prosody, language, and facialexpressions. By analyzing the relative feature weights learned by theregression models, our framework recommends to speak more fluently, use lessfiller words, speak as "we" (vs. "I"), use more unique words, and smile more.We also find that the students who were rated highly while answering the firstinterview question were also rated highly overall (i.e., first impressionmatters). Finally, our MIT Interview dataset will be made available to otherresearchers to further validate and expand our findings.
arxiv-11100-230 | Regret vs. Communication: Distributed Stochastic Multi-Armed Bandits and Beyond | http://arxiv.org/abs/1504.03509 | author:Shuang Liu, Cheng Chen, Zhihua Zhang category:cs.LG published:2015-04-14 summary:In this paper, we consider the distributed stochastic multi-armed banditproblem, where a global arm set can be accessed by multiple playersindependently. The players are allowed to exchange their history ofobservations with each other at specific points in time. We study therelationship between regret and communication. When the time horizon is known,we propose the Over-Exploration strategy, which only requires one-roundcommunication and whose regret does not scale with the number of players. Whenthe time horizon is unknown, we measure the frequency of communication througha new notion called the density of the communication set, and give an exactcharacterization of the interplay between regret and communication.Specifically, a lower bound is established and stable strategies that match thelower bound are developed. The results and analyses in this paper are specificbut can be translated into more general settings.
arxiv-11100-231 | Probabilistic Clustering of Time-Evolving Distance Data | http://arxiv.org/abs/1504.03701 | author:Julia E. Vogt, Marius Kloft, Stefan Stark, Sudhir S. Raman, Sandhya Prabhakaran, Volker Roth, Gunnar Rätsch category:cs.LG stat.ML published:2015-04-14 summary:We present a novel probabilistic clustering model for objects that arerepresented via pairwise distances and observed at different time points. Theproposed method utilizes the information given by adjacent time points to findthe underlying cluster structure and obtain a smooth cluster evolution. Thisapproach allows the number of objects and clusters to differ at every timepoint, and no identification on the identities of the objects is needed.Further, the model does not require the number of clusters being specified inadvance -- they are instead determined automatically using a Dirichlet processprior. We validate our model on synthetic data showing that the proposed methodis more accurate than state-of-the-art clustering methods. Finally, we use ourdynamic clustering model to analyze and illustrate the evolution of braincancer patients over time.
arxiv-11100-232 | Sketch-based 3D Shape Retrieval using Convolutional Neural Networks | http://arxiv.org/abs/1504.03504 | author:Fang Wang, Le Kang, Yi Li category:cs.CV published:2015-04-14 summary:Retrieving 3D models from 2D human sketches has received considerableattention in the areas of graphics, image retrieval, and computer vision.Almost always in state of the art approaches a large amount of "best views" arecomputed for 3D models, with the hope that the query sketch matches one ofthese 2D projections of 3D models using predefined features. We argue that this two stage approach (view selection -- matching) ispragmatic but also problematic because the "best views" are subjective andambiguous, which makes the matching inputs obscure. This imprecise nature ofmatching further makes it challenging to choose features manually. Instead ofrelying on the elusive concept of "best views" and the hand-crafted features,we propose to define our views using a minimalism approach and learn featuresfor both sketches and views. Specifically, we drastically reduce the number ofviews to only two predefined directions for the whole dataset. Then, we learntwo Siamese Convolutional Neural Networks (CNNs), one for the views and one forthe sketches. The loss function is defined on the within-domain as well as thecross-domain similarities. Our experiments on three benchmark datasetsdemonstrate that our method is significantly better than state of the artapproaches, and outperforms them in all conventional metrics.
arxiv-11100-233 | Simultaneous Feature Learning and Hash Coding with Deep Neural Networks | http://arxiv.org/abs/1504.03410 | author:Hanjiang Lai, Yan Pan, Ye Liu, Shuicheng Yan category:cs.CV published:2015-04-14 summary:Similarity-preserving hashing is a widely-used method for nearest neighboursearch in large-scale image retrieval tasks. For most existing hashing methods,an image is first encoded as a vector of hand-engineering visual features,followed by another separate projection or quantization step that generatesbinary codes. However, such visual feature vectors may not be optimallycompatible with the coding process, thus producing sub-optimal hashing codes.In this paper, we propose a deep architecture for supervised hashing, in whichimages are mapped into binary codes via carefully designed deep neuralnetworks. The pipeline of the proposed deep architecture consists of threebuilding blocks: 1) a sub-network with a stack of convolution layers to producethe effective intermediate image features; 2) a divide-and-encode module todivide the intermediate image features into multiple branches, each encodedinto one hash bit; and 3) a triplet ranking loss designed to characterize thatone image is more similar to the second image than to the third one. Extensiveevaluations on several benchmark image datasets show that the proposedsimultaneous feature learning and hash coding pipeline brings substantialimprovements over other state-of-the-art supervised or unsupervised hashingmethods.
arxiv-11100-234 | Adaptive Randomized Dimension Reduction on Massive Data | http://arxiv.org/abs/1504.03183 | author:Gregory Darnell, Stoyan Georgiev, Sayan Mukherjee, Barbara E Engelhardt category:stat.ML q-bio.QM published:2015-04-13 summary:The scalability of statistical estimators is of increasing importance inmodern applications. One approach to implementing scalable algorithms is tocompress data into a low dimensional latent space using dimension reductionmethods. In this paper we develop an approach for dimension reduction thatexploits the assumption of low rank structure in high dimensional data to gainboth computational and statistical advantages. We adapt recent randomizedlow-rank approximation algorithms to provide an efficient solution to principalcomponent analysis (PCA), and we use this efficient solver to improve parameterestimation in large-scale linear mixed models (LMM) for association mapping instatistical and quantitative genomics. A key observation in this paper is thatrandomization serves a dual role, improving both computational and statisticalperformance by implicitly regularizing the covariance matrix estimate of therandom effect in a LMM. These statistical and computational advantages arehighlighted in our experiments on simulated data and large-scale genomicstudies.
arxiv-11100-235 | Robobarista: Object Part based Transfer of Manipulation Trajectories from Crowd-sourcing in 3D Pointclouds | http://arxiv.org/abs/1504.03071 | author:Jaeyong Sung, Seok Hyun Jin, Ashutosh Saxena category:cs.RO cs.AI cs.LG published:2015-04-13 summary:There is a large variety of objects and appliances in human environments,such as stoves, coffee dispensers, juice extractors, and so on. It ischallenging for a roboticist to program a robot for each of these object typesand for each of their instantiations. In this work, we present a novel approachto manipulation planning based on the idea that many household objects sharesimilarly-operated object parts. We formulate the manipulation planning as astructured prediction problem and design a deep learning model that can handlelarge noise in the manipulation demonstrations and learns features from threedifferent modalities: point-clouds, language and trajectory. In order tocollect a large number of manipulation demonstrations for different objects, wedeveloped a new crowd-sourcing platform called Robobarista. We test our modelon our dataset consisting of 116 objects with 249 parts along with 250 languageinstructions, for which there are 1225 crowd-sourced manipulationdemonstrations. We further show that our robot can even manipulate objects ithas never seen before.
arxiv-11100-236 | Streaming, Memory Limited Matrix Completion with Noise | http://arxiv.org/abs/1504.03156 | author:Se-Young Yun, Marc Lelarge, Alexandre Proutiere category:math.SP stat.ML published:2015-04-13 summary:In this paper, we consider the streaming memory-limited matrix completionproblem when the observed entries are noisy versions of a small random fractionof the original entries. We are interested in scenarios where the matrix sizeis very large so the matrix is very hard to store and manipulate. Here, columnsof the observed matrix are presented sequentially and the goal is to completethe missing entries after one pass on the data with limited memory space andlimited computational complexity. We propose a streaming algorithm whichproduces an estimate of the original matrix with a vanishing mean square error,uses memory space scaling linearly with the ambient dimension of the matrix,i.e. the memory required to store the output alone, and spends computations asmuch as the number of non-zero entries of the input matrix.
arxiv-11100-237 | Joint Learning of Distributed Representations for Images and Texts | http://arxiv.org/abs/1504.03083 | author:Xiaodong He, Rupesh Srivastava, Jianfeng Gao, Li Deng category:cs.CV published:2015-04-13 summary:This technical report provides extra details of the deep multimodalsimilarity model (DMSM) which was proposed in (Fang et al. 2015,arXiv:1411.4952). The model is trained via maximizing global semanticsimilarity between images and their captions in natural language using thepublic Microsoft COCO database, which consists of a large set of images andtheir corresponding captions. The learned representations attempt to capturethe combination of various visual concepts and cues.
arxiv-11100-238 | Learning Multiple Visual Tasks while Discovering their Structure | http://arxiv.org/abs/1504.03106 | author:Carlo Ciliberto, Lorenzo Rosasco, Silvia Villa category:cs.LG cs.CV published:2015-04-13 summary:Multi-task learning is a natural approach for computer vision applicationsthat require the simultaneous solution of several distinct but relatedproblems, e.g. object detection, classification, tracking of multiple agents,or denoising, to name a few. The key idea is that exploring task relatedness(structure) can lead to improved performances. In this paper, we propose and study a novel sparse, non-parametric approachexploiting the theory of Reproducing Kernel Hilbert Spaces for vector-valuedfunctions. We develop a suitable regularization framework which can beformulated as a convex optimization problem, and is provably solvable using analternating minimization approach. Empirical tests show that the proposedmethod compares favorably to state of the art techniques and further allows torecover interpretable structures, a problem of interest in its own right.
arxiv-11100-239 | Convex Learning of Multiple Tasks and their Structure | http://arxiv.org/abs/1504.03101 | author:Carlo Ciliberto, Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco category:cs.LG published:2015-04-13 summary:Reducing the amount of human supervision is a key problem in machine learningand a natural approach is that of exploiting the relations (structure) amongdifferent tasks. This is the idea at the core of multi-task learning. In thiscontext a fundamental question is how to incorporate the tasks structure in thelearning problem.We tackle this question by studying a general computationalframework that allows to encode a-priori knowledge of the tasks structure inthe form of a convex penalty; in this setting a variety of previously proposedmethods can be recovered as special cases, including linear and non-linearapproaches. Within this framework, we show that tasks and their structure canbe efficiently learned considering a convex optimization problem that can beapproached by means of block coordinate methods such as alternatingminimization and for which we prove convergence to the global minimum.
arxiv-11100-240 | Real-world Object Recognition with Off-the-shelf Deep Conv Nets: How Many Objects can iCub Learn? | http://arxiv.org/abs/1504.03154 | author:Giulia Pasquale, Carlo Ciliberto, Francesca Odone, Lorenzo Rosasco, Lorenzo Natale category:cs.RO cs.CV cs.LG published:2015-04-13 summary:The ability to visually recognize objects is a fundamental skill for roboticssystems. Indeed, a large variety of tasks involving manipulation, navigation orinteraction with other agents, deeply depends on the accurate understanding ofthe visual scene. Yet, at the time being, robots are lacking good visualperceptual systems, which often become the main bottleneck preventing the useof autonomous agents for real-world applications. Lately in computer vision, systems that learn suitable visual representationsand based on multi-layer deep convolutional networks are showing remarkableperformance in tasks such as large-scale visual recognition and imageretrieval. To this regard, it is natural to ask whether such remarkableperformance would generalize also to the robotic setting. In this paper we investigate such possibility, while taking further steps indeveloping a computational vision system to be embedded on a robotic platform,the iCub humanoid robot. In particular, we release a new dataset ({\sciCubWorld28}) that we use as a benchmark to address the question: {\it how manyobjects can iCub recognize?} Our study is developed in a learning frameworkwhich reflects the typical visual experience of a humanoid robot like the iCub.Experiments shed interesting insights on the strength and weaknesses of currentcomputer vision approaches applied in real robotic settings.
arxiv-11100-241 | Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction | http://arxiv.org/abs/1504.03293 | author:Yuting Zhang, Kihyuk Sohn, Ruben Villegas, Gang Pan, Honglak Lee category:cs.CV published:2015-04-13 summary:Object detection systems based on the deep convolutional neural network (CNN)have recently made ground- breaking advances on several object detectionbenchmarks. While the features learned by these high-capacity neural networksare discriminative for categorization, inaccurate localization is still a majorsource of error for detection. Building upon high-capacity CNN architectures,we address the localization problem by 1) using a search algorithm based onBayesian optimization that sequentially proposes candidate regions for anobject bounding box, and 2) training the CNN with a structured loss thatexplicitly penalizes the localization inaccuracy. In experiments, wedemonstrated that each of the proposed methods improves the detectionperformance over the baseline method on PASCAL VOC 2007 and 2012 datasets.Furthermore, two methods are complementary and significantly outperform theprevious state-of-the-art when combined.
arxiv-11100-242 | Review Mining for Feature Based Opinion Summarization and Visualization | http://arxiv.org/abs/1504.03068 | author:Ahmad Kamal category:cs.IR cs.CL published:2015-04-13 summary:The application and usage of opinion mining, especially for businessintelligence, product recommendation, targeted marketing etc. have fascinatedmany research attentions around the globe. Various research efforts attemptedto mine opinions from customer reviews at different levels of granularity,including word-, sentence-, and document-level. However, development of a fullyautomatic opinion mining and sentiment analysis system is still elusive. Thoughthe development of opinion mining and sentiment analysis systems are gettingmomentum, most of them attempt to perform document-level sentiment analysis,classifying a review document as positive, negative, or neutral. Suchdocument-level opinion mining approaches fail to provide insight about userssentiment on individual features of a product or service. Therefore, it seemsto be a great help for both customers and manufacturers, if the reviews couldbe processed at a finer-grained level and presented in a summarized formthrough some visual means, highlighting individual features of a product andusers sentiment expressed over them. In this paper, the design of a unifiedopinion mining and sentiment analysis framework is presented at theintersection of both machine learning and natural language processingapproaches. Also, design of a novel feature-level review summarization schemeis proposed to visualize mined features, opinions and their polarity values ina comprehendible way.
arxiv-11100-243 | Tight Bounds on Low-degree Spectral Concentration of Submodular and XOS functions | http://arxiv.org/abs/1504.03391 | author:Vitaly Feldman, Jan Vondrak category:cs.DS cs.LG published:2015-04-13 summary:Submodular and fractionally subadditive (or equivalently XOS) functions playa fundamental role in combinatorial optimization, algorithmic game theory andmachine learning. Motivated by learnability of these classes of functions fromrandom examples, we consider the question of how well such functions can beapproximated by low-degree polynomials in $\ell_2$ norm over the uniformdistribution. This question is equivalent to understanding of the concentrationof Fourier weight on low-degree coefficients, a central concept in Fourieranalysis. We show that 1. For any submodular function $f:\{0,1\}^n \rightarrow [0,1]$, there is apolynomial of degree $O(\log (1/\epsilon) / \epsilon^{4/5})$ approximating $f$within $\epsilon$ in $\ell_2$, and there is a submodular function that requiresdegree $\Omega(1/\epsilon^{4/5})$. 2. For any XOS function $f:\{0,1\}^n \rightarrow [0,1]$, there is apolynomial of degree $O(1/\epsilon)$ and there exists an XOS function thatrequires degree $\Omega(1/\epsilon)$. This improves on previous approaches that all showed an upper bound of$O(1/\epsilon^2)$ for submodular and XOS functions. The best previous lowerbound was $\Omega(1/\epsilon^{2/3})$ for monotone submodular functions. Ourtechniques reveal new structural properties of submodular and XOS functions andthe upper bounds lead to nearly optimal PAC learning algorithms for theseclasses of functions.
arxiv-11100-244 | Multiple Measurements and Joint Dimensionality Reduction for Large Scale Image Search with Short Vectors - Extended Version | http://arxiv.org/abs/1504.03285 | author:Filip Radenovic, Herve Jegou, Ondrej Chum category:cs.CV published:2015-04-13 summary:This paper addresses the construction of a short-vector (128D) imagerepresentation for large-scale image and particular object retrieval. Inparticular, the method of joint dimensionality reduction of multiplevocabularies is considered. We study a variety of vocabulary generationtechniques: different k-means initializations, different descriptortransformations, different measurement regions for descriptor extraction. Ourextensive evaluation shows that different combinations of vocabularies, eachpartitioning the descriptor space in a different yet complementary manner,results in a significant performance improvement, which exceeds thestate-of-the-art.
arxiv-11100-245 | Egyptian Dialect Stopword List Generation from Social Network Data | http://arxiv.org/abs/1508.02060 | author:Walaa Medhat, Ahmed H. Yousef, Hoda Korashy category:cs.CL published:2015-04-13 summary:This paper proposes a methodology for generating a stopword list from onlinesocial network (OSN) corpora in Egyptian Dialect(ED). The aim of the paper isto investigate the effect of removingED stopwords on the Sentiment Analysis(SA) task. The stopwords lists generated before were on Modern Standard Arabic(MSA) which is not the common language used in OSN. We have generated astopword list of Egyptian dialect to be used with the OSN corpora. We comparethe efficiency of text classification when using the generated list along withpreviously generated lists of MSA and combining the Egyptian dialect list withthe MSA list. The text classification was performed using Na\"ive Bayes andDecision Tree classifiers and two feature selection approaches, unigram andbigram. The experiments show that removing ED stopwords give better performancethan using lists of MSA stopwords only.
arxiv-11100-246 | Optimal Parameter Choices Through Self-Adjustment: Applying the 1/5-th Rule in Discrete Settings | http://arxiv.org/abs/1504.03212 | author:Benjamin Doerr, Carola Doerr category:cs.NE published:2015-04-13 summary:While evolutionary algorithms are known to be very successful for a broadrange of applications, the algorithm designer is often left with manyalgorithmic choices, for example, the size of the population, the mutationrates, and the crossover rates of the algorithm. These parameters are known tohave a crucial influence on the optimization time, and thus need to be chosencarefully, a task that often requires substantial efforts. Moreover, theoptimal parameters can change during the optimization process. It is thereforeof great interest to design mechanisms that dynamically choose best-possibleparameters. An example for such an update mechanism is the one-fifth successrule for step-size adaption in evolutionary strategies. While in continuousdomains this principle is well understood also from a mathematical point ofview, no comparable theory is available for problems in discrete domains. In this work we show that the one-fifth success rule can be effective also indiscrete settings. We regard the $(1+(\lambda,\lambda))$~GA proposed in[Doerr/Doerr/Ebel: From black-box complexity to designing new geneticalgorithms, TCS 2015]. We prove that if its population size is chosen accordingto the one-fifth success rule then the expected optimization time on\textsc{OneMax} is linear. This is better than what \emph{any} staticpopulation size $\lambda$ can achieve and is asymptotically optimal also amongall adaptive parameter choices.
arxiv-11100-247 | Generalized Correntropy for Robust Adaptive Filtering | http://arxiv.org/abs/1504.02931 | author:Badong Chen, Lei Xing, Haiquan Zhao, Nanning Zheng, José C. Príncipe category:stat.ML cs.IT math.IT published:2015-04-12 summary:As a robust nonlinear similarity measure in kernel space, correntropy hasreceived increasing attention in domains of machine learning and signalprocessing. In particular, the maximum correntropy criterion (MCC) has recentlybeen successfully applied in robust regression and filtering. The defaultkernel function in correntropy is the Gaussian kernel, which is, of course, notalways the best choice. In this work, we propose a generalized correntropy thatadopts the generalized Gaussian density (GGD) function as the kernel (notnecessarily a Mercer kernel), and present some important properties. We furtherpropose the generalized maximum correntropy criterion (GMCC), and apply it toadaptive filtering. An adaptive algorithm, called the GMCC algorithm, isderived, and the mean square convergence performance is studied. We show thatthe proposed algorithm is very stable and can achieve zero probability ofdivergence (POD). Simulation results confirm the theoretical expectations anddemonstrate the desirable performance of the new algorithm.
arxiv-11100-248 | Deep Transform: Cocktail Party Source Separation via Complex Convolution in a Deep Neural Network | http://arxiv.org/abs/1504.02945 | author:Andrew J. R. Simpson category:cs.SD cs.LG cs.NE 68Txx published:2015-04-12 summary:Convolutional deep neural networks (DNN) are state of the art in manyengineering problems but have not yet addressed the issue of how to deal withcomplex spectrograms. Here, we use circular statistics to provide a convenientprobabilistic estimate of spectrogram phase in a complex convolutional DNN. Ina typical cocktail party source separation scenario, we trained a convolutionalDNN to re-synthesize the complex spectrograms of two source speech signalsgiven a complex spectrogram of the monaural mixture - a discriminative deeptransform (DT). We then used this complex convolutional DT to obtainprobabilistic estimates of the magnitude and phase components of the sourcespectrograms. Our separation results are on a par with equivalent binary-maskbased non-complex separation approaches.
arxiv-11100-249 | Classification with Extreme Learning Machine and Ensemble Algorithms Over Randomly Partitioned Data | http://arxiv.org/abs/1504.02975 | author:Ferhat Özgür Çatak category:cs.LG published:2015-04-12 summary:In this age of Big Data, machine learning based data mining methods areextensively used to inspect large scale data sets. Deriving applicablepredictive modeling from these type of data sets is a challenging obstaclebecause of their high complexity. Opportunity with high data availabilitylevels, automated classification of data sets has become a critical andcomplicated function. In this paper, the power of applying MapReduce basedDistributed AdaBoosting of Extreme Learning Machine (ELM) are explored to buildreliable predictive bag of classification models. Thus, (i) dataset ensemblesare build; (ii) ELM algorithm is used to build weak classification models; and(iii) build a strong classification model from a set of weak classificationmodels. This training model is applied to the publicly available knowledgediscovery and data mining datasets.
arxiv-11100-250 | Computing trading strategies based on financial sentiment data using evolutionary optimization | http://arxiv.org/abs/1504.02972 | author:Ronald Hochreiter category:q-fin.PM cs.NE published:2015-04-12 summary:In this paper we apply evolutionary optimization techniques to computeoptimal rule-based trading strategies based on financial sentiment data. Thesentiment data was extracted from the social media service StockTwits toaccommodate the level of bullishness or bearishness of the online tradingcommunity towards certain stocks. Numerical results for all stocks from the DowJones Industrial Average (DJIA) index are presented and a comparison toclassical risk-return portfolio selection is provided.
arxiv-11100-251 | Gradual Training Method for Denoising Auto Encoders | http://arxiv.org/abs/1504.02902 | author:Alexander Kalmanovich, Gal Chechik category:cs.LG cs.NE published:2015-04-11 summary:Stacked denoising auto encoders (DAEs) are well known to learn useful deeprepresentations, which can be used to improve supervised training byinitializing a deep network. We investigate a training scheme of a deep DAE,where DAE layers are gradually added and keep adapting as additional layers areadded. We show that in the regime of mid-sized datasets, this gradual trainingprovides a small but consistent improvement over stacked training in bothreconstruction quality and classification error over stacked training on MNISTand CIFAR datasets.
arxiv-11100-252 | Quick sensitivity analysis for incremental data modification and its application to leave-one-out CV in linear classification problems | http://arxiv.org/abs/1504.02870 | author:Shota Okumura, Yoshiki Suzuki, Ichiro Takeuchi category:stat.ML cs.LG published:2015-04-11 summary:We introduce a novel sensitivity analysis framework for large scaleclassification problems that can be used when a small number of instances areincrementally added or removed. For quickly updating the classifier in such asituation, incremental learning algorithms have been intensively studied in theliterature. Although they are much more efficient than solving the optimizationproblem from scratch, their computational complexity yet depends on the entiretraining set size. It means that, if the original training set is large,completely solving an incremental learning problem might be still ratherexpensive. To circumvent this computational issue, we propose a novel frameworkthat allows us to make an inference about the updated classifier withoutactually re-optimizing it. Specifically, the proposed framework can quicklyprovide a lower and an upper bounds of a quantity on the unknown updatedclassifier. The main advantage of the proposed framework is that thecomputational cost of computing these bounds depends only on the number ofupdated instances. This property is quite advantageous in a typical sensitivityanalysis task where only a small number of instances are updated. In this paperwe demonstrate that the proposed framework is applicable to various practicalsensitivity analysis tasks, and the bounds provided by the framework are oftensufficiently tight for making desired inferences.
arxiv-11100-253 | Hierarchical Composition of Memristive Networks for Real-Time Computing | http://arxiv.org/abs/1504.02833 | author:Jens Bürger, Alireza Goudarzi, Darko Stefanovic, Christof Teuscher category:cs.ET cs.NE published:2015-04-11 summary:Advances in materials science have led to physical instantiations ofself-assembled networks of memristive devices and demonstrations of theircomputational capability through reservoir computing. Reservoir computing is anapproach that takes advantage of collective system dynamics for real-timecomputing. A dynamical system, called a reservoir, is excited with atime-varying signal and observations of its states are used to reconstruct adesired output signal. However, such a monolithic assembly limits thecomputational power due to signal interdependency and the resulting correlatedreadouts. Here, we introduce an approach that hierarchically composes a set ofinterconnected memristive networks into a larger reservoir. We use signalamplification and restoration to reduce reservoir state correlation, whichimproves the feature extraction from the input signals. Using the same numberof output signals, such a hierarchical composition of heterogeneous smallnetworks outperforms monolithic memristive networks by at least 20% on waveformgeneration tasks. On the NARMA-10 task, we reduce the error by up to a factorof 2 compared to homogeneous reservoirs with sigmoidal neurons, whereas singlememristive networks are unable to produce the correct result. Hierarchicalcomposition is key for solving more complex tasks with such novel nano-scalehardware.
arxiv-11100-254 | Appearance-Based Gaze Estimation in the Wild | http://arxiv.org/abs/1504.02863 | author:Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling category:cs.CV published:2015-04-11 summary:Appearance-based gaze estimation is believed to work well in real-worldsettings, but existing datasets have been collected under controlled laboratoryconditions and methods have been not evaluated across multiple datasets. Inthis work we study appearance-based gaze estimation in the wild. We present theMPIIGaze dataset that contains 213,659 images we collected from 15 participantsduring natural everyday laptop use over more than three months. Our dataset issignificantly more variable than existing ones with respect to appearance andillumination. We also present a method for in-the-wild appearance-based gazeestimation using multimodal convolutional neural networks that significantlyoutperforms state-of-the art methods in the most challenging cross-datasetevaluation. We present an extensive evaluation of several state-of-the-artimage-based gaze estimation algorithms on three current datasets, including ourown. This evaluation provides clear insights and allows us to identify keyresearch challenges of gaze estimation in the wild.
arxiv-11100-255 | High Density Noise Removal by Cascading Algorithms | http://arxiv.org/abs/1504.02856 | author:Arabinda Dash, Sujaya Kumar Sathua category:cs.CV published:2015-04-11 summary:An advanced non-linear cascading filter algorithm for the removal of highdensity salt and pepper noise from the digital images is proposed. The proposedmethod consists of two stages. The first stage Decision base Median Filter(DMF) acts as the preliminary noise removal algorithm. The second stage iseither Modified Decision Base Partial Trimmed Global Mean Filter (MDBPTGMF) orModified Decision Based Unsymmetric Trimmed Median Filter (MDBUTMF) which isused to remove the remaining noise and enhance the image quality. The DMFalgorithm performs well at low noise density but it fails to remove the noiseat medium and high level. The MDBPTGMF and MDUTMF have excellent performance atlow, medium and high noise density but these reduce the image quality and blurthe image at high noise level. So the basic idea behind this paper is tocombine the advantages of the filters used in both the stages to remove theSalt and Pepper noise and enhance the image quality at all the noise densitylevel. The proposed method is tested against different gray scale images and itgives better Mean Absolute Error (MAE), Peak Signal to Noise Ratio (PSNR) andImage Enhancement Factor (IEF) than the Adaptive Median Filter (AMF), DecisionBase Unsymmetric Trimmed Median Filter (DBUTMF), Modified Decision BaseUnsymmetric Trimmed Median Filter (MDBUTMF) and Decision Base Partial TrimmedGlobal Mean Filter (DBPTGMF).
arxiv-11100-256 | A Deep Embedding Model for Co-occurrence Learning | http://arxiv.org/abs/1504.02824 | author:Yelong Shen, Ruoming Jin, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li Deng category:cs.LG published:2015-04-11 summary:Co-occurrence Data is a common and important information source in manyareas, such as the word co-occurrence in the sentences, friends co-occurrencein social networks and products co-occurrence in commercial transaction data,etc, which contains rich correlation and clustering information about theitems. In this paper, we study co-occurrence data using a general energy-basedprobabilistic model, and we analyze three different categories of energy-basedmodel, namely, the $L_1$, $L_2$ and $L_k$ models, which are able to capturedifferent levels of dependency in the co-occurrence data. We also discuss howseveral typical existing models are related to these three types of energymodels, including the Fully Visible Boltzmann Machine (FVBM) ($L_2$), MatrixFactorization ($L_2$), Log-BiLinear (LBL) models ($L_2$), and the RestrictedBoltzmann Machine (RBM) model ($L_k$). Then, we propose a Deep Embedding Model(DEM) (an $L_k$ model) from the energy model in a \emph{principled} manner.Furthermore, motivated by the observation that the partition function in theenergy model is intractable and the fact that the major objective of modelingthe co-occurrence data is to predict using the conditional probability, weapply the \emph{maximum pseudo-likelihood} method to learn DEM. In consequence,the developed model and its learning method naturally avoid the abovedifficulties and can be easily used to compute the conditional probability inprediction. Interestingly, our method is equivalent to learning a specialstructured deep neural network using back-propagation and a special samplingstrategy, which makes it scalable on large-scale datasets. Finally, in theexperiments, we show that the DEM can achieve comparable or better results thanstate-of-the-art methods on datasets across several application domains.
arxiv-11100-257 | siftservice.com - Turning a Computer Vision algorithm into a World Wide Web Service | http://arxiv.org/abs/1504.02840 | author:Ahmad Pahlavan Tafti, Hamid Hassannia, Zeyun Yu category:cs.CV published:2015-04-11 summary:Image features detection and description is a longstanding topic in computervision and pattern recognition areas. The Scale Invariant Feature Transform(SIFT) is probably the most popular and widely demanded feature descriptorwhich facilitates a variety of computer vision applications such as imageregistration, object tracking, image forgery detection, and 3D surfacereconstruction. This work introduces a Software as a Service (SaaS) basedimplementation of the SIFT algorithm which is freely available athttp://siftservice.com for any academic, educational and research purposes. Theservice provides application-to-application interaction and aims RapidApplication Development (RAD) and also fast prototyping for computer visionstudents and researchers all around the world. An Internet connection is allthey need!
arxiv-11100-258 | Modelling Multi-level Power Usage with Latent States and Smooth Functions | http://arxiv.org/abs/1504.02813 | author:Camila P. E. de Souza, Nancy E. Heckman category:stat.ME stat.AP stat.ML published:2015-04-10 summary:We develop and apply a new approach for analyzing a building's business daypower usage. We treat each business day as a replicate and model power usage asarising from two smooth functions, one function giving power usage when thecooling system is off, the other function giving power usage when the coolingsystem is on. The condition "chiller on"/"chiller off" at any particular timecannot be observed directly, thus forming a latent process. In general, ourmethod can be applied to multi-curve data where each curve is driven by alatent state process. The state at any particular point determines a smoothfunction. Thus each curve follows what we call a switching nonparametricregression model. We develop an EM algorithm to estimate the parameters of thelatent process and the function corresponding to each state. We also obtainstandard errors for the parameter estimates of the state process. Simulationsstudies show the frequentist properties of our estimates.
arxiv-11100-259 | High-Dimensional Classification for Brain Decoding | http://arxiv.org/abs/1504.02800 | author:Nicole Croteau, Farouk S. Nathoo, Jiguo Cao, Ryan Budney category:stat.ML published:2015-04-10 summary:Brain decoding involves the determination of a subject's cognitive state oran associated stimulus from functional neuroimaging data measuring brainactivity. In this setting the cognitive state is typically characterized by anelement of a finite set, and the neuroimaging data comprise voluminous amountsof spatiotemporal data measuring some aspect of the neural signal. Theassociated statistical problem is one of classification from high-dimensionaldata. We explore the use of functional principal component analysis, mutualinformation networks, and persistent homology for examining the data throughexploratory analysis and for constructing features characterizing the neuralsignal for brain decoding. We review each approach from this perspective, andwe incorporate the features into a classifier based on symmetric multinomiallogistic regression with elastic net regularization. The approaches areillustrated in an application where the task is to infer, from brain activitymeasured with magnetoencephalography (MEG), the type of video stimulus shown toa subject.
arxiv-11100-260 | Diffusion Component Analysis: Unraveling Functional Topology in Biological Networks | http://arxiv.org/abs/1504.02719 | author:Hyunghoon Cho, Bonnie Berger, Jian Peng category:q-bio.MN cs.LG cs.SI stat.ML published:2015-04-10 summary:Complex biological systems have been successfully modeled by biochemical andgenetic interaction networks, typically gathered from high-throughput (HTP)data. These networks can be used to infer functional relationships betweengenes or proteins. Using the intuition that the topological role of a gene in anetwork relates to its biological function, local or diffusion based"guilt-by-association" and graph-theoretic methods have had success ininferring gene functions. Here we seek to improve function prediction byintegrating diffusion-based methods with a novel dimensionality reductiontechnique to overcome the incomplete and noisy nature of network data. In thispaper, we introduce diffusion component analysis (DCA), a framework that plugsin a diffusion model and learns a low-dimensional vector representation of eachnode to encode the topological properties of a network. As a proof of concept,we demonstrate DCA's substantial improvement over state-of-the-artdiffusion-based approaches in predicting protein function from molecularinteraction networks. Moreover, our DCA framework can integrate multiplenetworks from heterogeneous sources, consisting of genomic information,biochemical experiments and other resources, to even further improve functionprediction. Yet another layer of performance gain is achieved by integratingthe DCA framework with support vector machines that take our node vectorrepresentations as features. Overall, our DCA framework provides a novelrepresentation of nodes in a network that can be used as a plug-in architectureto other machine learning algorithms to decipher topological properties of andobtain novel insights into interactomes.
arxiv-11100-261 | Maximum Entropy Linear Manifold for Learning Discriminative Low-dimensional Representation | http://arxiv.org/abs/1504.02622 | author:Wojciech Marian Czarnecki, Rafał Józefowicz, Jacek Tabor category:cs.LG published:2015-04-10 summary:Representation learning is currently a very hot topic in modern machinelearning, mostly due to the great success of the deep learning methods. Inparticular low-dimensional representation which discriminates classes can notonly enhance the classification procedure, but also make it faster, whilecontrary to the high-dimensional embeddings can be efficiently used for visualbased exploratory data analysis. In this paper we propose Maximum Entropy Linear Manifold (MELM), amultidimensional generalization of Multithreshold Entropy Linear Classifiermodel which is able to find a low-dimensional linear data projection maximizingdiscriminativeness of projected classes. As a result we obtain a linearembedding which can be used for classification, class aware dimensionalityreduction and data visualization. MELM provides highly discriminative 2Dprojections of the data which can be used as a method for constructing robustclassifiers. We provide both empirical evaluation as well as some interesting theoreticalproperties of our objective function such us scale and affine transformationinvariance, connections with PCA and bounding of the expected balanced accuracyerror.
arxiv-11100-262 | Learning Arbitrary Statistical Mixtures of Discrete Distributions | http://arxiv.org/abs/1504.02526 | author:Jian Li, Yuval Rabani, Leonard J. Schulman, Chaitanya Swamy category:cs.LG cs.DS published:2015-04-10 summary:We study the problem of learning from unlabeled samples very generalstatistical mixture models on large finite sets. Specifically, the model to belearned, $\vartheta$, is a probability distribution over probabilitydistributions $p$, where each such $p$ is a probability distribution over $[n]= \{1,2,\dots,n\}$. When we sample from $\vartheta$, we do not observe $p$directly, but only indirectly and in very noisy fashion, by sampling from $[n]$repeatedly, independently $K$ times from the distribution $p$. The problem isto infer $\vartheta$ to high accuracy in transportation (earthmover) distance. We give the first efficient algorithms for learning this mixture modelwithout making any restricting assumptions on the structure of the distribution$\vartheta$. We bound the quality of the solution as a function of the size ofthe samples $K$ and the number of samples used. Our model and results haveapplications to a variety of unsupervised learning scenarios, includinglearning topic models and collaborative filtering.
arxiv-11100-263 | Study of Some Recent Crossovers Effects on Speed and Accuracy of Genetic Algorithm, Using Symmetric Travelling Salesman Problem | http://arxiv.org/abs/1504.02590 | author:Hassan Ismkhan, Kamran Zamanifar category:cs.NE published:2015-04-10 summary:The Travelling Salesman Problem (TSP) is one of the most famous optimizationproblems. The Genetic Algorithm (GA) is one of metaheuristics that have beenapplied to TSP. The Crossover and mutation operators are two important elementsof GA. There are many TSP solver crossover operators. In this paper, we stateimplementation of some recent TSP solver crossovers at first and then we useeach of them in GA to solve some Symmetric TSP (STSP) instances and finallycompare their effects on speed and accuracy of presented GA.
arxiv-11100-264 | A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category Recognition | http://arxiv.org/abs/1504.02764 | author:Roozbeh Mottaghi, Yu Xiang, Silvio Savarese category:cs.CV published:2015-04-10 summary:Despite the fact that object detection, 3D pose estimation, and sub-categoryrecognition are highly correlated tasks, they are usually addressedindependently from each other because of the huge space of parameters. Tojointly model all of these tasks, we propose a coarse-to-fine hierarchicalrepresentation, where each level of the hierarchy represents objects at adifferent level of granularity. The hierarchical representation preventsperformance loss, which is often caused by the increase in the number ofparameters (as we consider more tasks to model), and the joint modellingenables resolving ambiguities that exist in independent modelling of thesetasks. We augment PASCAL3D+ dataset with annotations for these tasks and showthat our hierarchical model is effective in joint modelling of objectdetection, 3D pose estimation, and sub-category recognition.
arxiv-11100-265 | Gradient of Probability Density Functions based Contrasts for Blind Source Separation (BSS) | http://arxiv.org/abs/1504.02712 | author:Dharmani Bhaveshkumar C category:cs.LG cs.IT math.IT stat.ML 94A17 published:2015-04-10 summary:The article derives some novel independence measures and contrast functionsfor Blind Source Separation (BSS) application. For the $k^{th}$ orderdifferentiable multivariate functions with equal hyper-volumes (region boundedby hyper-surfaces) and with a constraint of bounded support for $k>1$, itproves that equality of any $k^{th}$ order derivatives implies equality of thefunctions. The difference between product of marginal Probability DensityFunctions (PDFs) and joint PDF of a random vector is defined as FunctionDifference (FD) of a random vector. Assuming the PDFs are $k^{th}$ orderdifferentiable, the results on generalized functions are applied to theindependence condition. This brings new sets of independence measures and BSScontrasts based on the $L^p$-Norm, $ p \geq 1$ of - FD, gradient of FD (GFD)and Hessian of FD (HFD). Instead of a conventional two stage indirectestimation method for joint PDF based BSS contrast estimation, a single stagedirect estimation of the contrasts is desired. The article targets both theefficient estimation of the proposed contrasts and extension of the potentialtheory for an information field. The potential theory has a concept ofreference potential and it is used to derive closed form expression for therelative analysis of potential field. Analogous to it, there are introducedconcepts of Reference Information Potential (RIP) and Cross ReferenceInformation Potential (CRIP) based on the potential due to kernel functionsplaced at selected sample points as basis in kernel methods. The quantities areused to derive closed form expressions for information field analysis usingleast squares. The expressions are used to estimate $L^2$-Norm of FD and$L^2$-Norm of GFD based contrasts.
arxiv-11100-266 | Performance measures for classification systems with rejection | http://arxiv.org/abs/1504.02763 | author:Filipe Condessa, Jelena Kovacevic, Jose Bioucas-Dias category:cs.CV cs.LG 68-04 published:2015-04-10 summary:Classifiers with rejection are essential in real-world applications wheremisclassifications and their effects are critical. However, if no problemspecific cost function is defined, there are no established measures to assessthe performance of such classifiers. We introduce a set of desired propertiesfor performance measures for classifiers with rejection, based on which wepropose a set of three performance measures for the evaluation of theperformance of classifiers with rejection that satisfy the desired properties.The nonrejected accuracy measures the ability of the classifier to accuratelyclassify nonrejected samples; the classification quality measures the correctdecision making of the classifier with rejector; and the rejection qualitymeasures the ability to concentrate all misclassified samples onto the set ofrejected samples. From the measures, we derive the concept of relativeoptimality that allows us to connect the measures to a family of cost functionsthat take into account the trade-off between rejection and misclassification.We illustrate the use of the proposed performance measures on classifiers withrejection applied to synthetic and real-world data.
arxiv-11100-267 | HEp-2 Cell Image Classification with Deep Convolutional Neural Networks | http://arxiv.org/abs/1504.02531 | author:Zhimin Gao, Lei Wang, Luping Zhou, Jianjia Zhang category:cs.CV published:2015-04-10 summary:Efficient Human Epithelial-2 (HEp-2) cell image classification can facilitatethe diagnosis of many autoimmune diseases. This paper presents an automaticframework for this classification task, by utilizing the deep convolutionalneural networks (CNNs) which have recently attracted intensive attention invisual recognition. This paper elaborates the important components of thisframework, discusses multiple key factors that impact the efficiency oftraining a deep CNN, and systematically compares this framework with thewell-established image classification models in the literature. Experiments onbenchmark datasets show that i) the proposed framework can effectivelyoutperform existing models by properly applying data augmentation; ii) ourCNN-based framework demonstrates excellent adaptability across differentdatasets, which is highly desirable for classification under varying laboratorysettings. Our system is ranked high in the cell image classificationcompetition hosted by ICPR 2014.
arxiv-11100-268 | Image patch analysis of sunspots and active regions. II. Clustering via matrix factorization | http://arxiv.org/abs/1504.02762 | author:Kevin R. Moon, Veronique Delouille, Jimmy J. Li, Ruben De Visscher, Fraser Watson, Alfred O. Hero III category:astro-ph.SR cs.CV published:2015-04-10 summary:Separating active regions that are quiet from potentially eruptive ones is akey issue in Space Weather applications. Traditional classification schemessuch as Mount Wilson and McIntosh have been effective in relating an activeregion large scale magnetic configuration to its ability to produce eruptiveevents. However, their qualitative nature prevents systematic studies of anactive region's evolution for example. We introduce a new clustering of activeregions that is based on the local geometry observed in Line of Sightmagnetogram and continuum images. We use a reduced-dimension representation ofan active region that is obtained by factoring the corresponding data matrixcomprised of local image patches. Two factorizations can be compared via thedefinition of appropriate metrics on the resulting factors. The distancesobtained from these metrics are then used to cluster the active regions. Wefind that these metrics result in natural clusterings of active regions. Theclusterings are related to large scale descriptors of an active region such asits size, its local magnetic field distribution, and its complexity as measuredby the Mount Wilson classification scheme. We also find that including datafocused on the neutral line of an active region can result in an increasedcorrespondence between our clustering results and other active regiondescriptors such as the Mount Wilson classifications and the $R$ value. Weprovide some recommendations for which metrics, matrix factorizationtechniques, and regions of interest to use to study active regions.
arxiv-11100-269 | OneMax in Black-Box Models with Several Restrictions | http://arxiv.org/abs/1504.02644 | author:Carola Doerr, Johannes Lengler category:cs.NE cs.DS published:2015-04-10 summary:Black-box complexity studies lower bounds for the efficiency ofgeneral-purpose black-box optimization algorithms such as evolutionaryalgorithms and other search heuristics. Different models exist, each one beingdesigned to analyze a different aspect of typical heuristics such as the memorysize or the variation operators in use. While most of the previous works focuson one particular such aspect, we consider in this work how the combination ofseveral algorithmic restrictions influence the black-box complexity. Ourtestbed are so-called OneMax functions, a classical set of test functions thatis intimately related to classic coin-weighing problems and to the board gameMastermind. We analyze in particular the combined memory-restricted ranking-basedblack-box complexity of OneMax for different memory sizes. While its isolatedmemory-restricted as well as its ranking-based black-box complexity for bitstrings of length $n$ is only of order $n/\log n$, the combined model does notallow for algorithms being faster than linear in $n$, as can be seen bystandard information-theoretic considerations. We show that this linear boundis indeed asymptotically tight. Similar results are obtained for other memory-and offspring-sizes. Our results also apply to the (Monte Carlo) complexity ofOneMax in the recently introduced elitist model, in which only the best-so-farsolution can be kept in the memory. Finally, we also provide improved lowerbounds for the complexity of OneMax in the regarded models. Our result enlivens the quest for natural evolutionary algorithms optimizingOneMax in $o(n \log n)$ iterations.
arxiv-11100-270 | Car that Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models | http://arxiv.org/abs/1504.02789 | author:Ashesh Jain, Hema S. Koppula, Bharad Raghavan, Shane Soh, Ashutosh Saxena category:cs.CV published:2015-04-10 summary:Advanced Driver Assistance Systems (ADAS) have made driving safer over thelast decade. They prepare vehicles for unsafe road conditions and alert driversif they perform a dangerous maneuver. However, many accidents are unavoidablebecause by the time drivers are alerted, it is already too late. Anticipatingmaneuvers beforehand can alert drivers before they perform the maneuver andalso give ADAS more time to avoid or prepare for the danger. In this work we anticipate driving maneuvers a few seconds before they occur.For this purpose we equip a car with cameras and a computing device to capturethe driving context from both inside and outside of the car. We propose anAutoregressive Input-Output HMM to model the contextual information alongwiththe maneuvers. We evaluate our approach on a diverse data set with 1180 milesof natural freeway and city driving and show that we can anticipate maneuvers3.5 seconds before they occur with over 80\% F1-score in real-time.
arxiv-11100-271 | Time-causal and time-recursive spatio-temporal receptive fields | http://arxiv.org/abs/1504.02648 | author:Tony Lindeberg category:cs.CV q-bio.NC published:2015-04-10 summary:We present an improved model and theory for time-causal and time-recursivespatio-temporal receptive fields, based on a combination of Gaussian receptivefields over the spatial domain and first-order integrators or equivalentlytruncated exponential filters coupled in cascade over the temporal domain. Compared to previous spatio-temporal scale-space formulations in terms ofnon-enhancement of local extrema or scale invariance, these receptive fieldsare based on different scale-space axiomatics over time by ensuringnon-creation of new local extrema or zero-crossings with increasing temporalscale. Specifically, extensions are presented about (i) parameterizing theintermediate temporal scale levels, (ii) analysing the resulting temporaldynamics, (iii) transferring the theory to a discrete implementation, (iv)computing scale-normalized spatio-temporal derivative expressions forspatio-temporal feature detection and (v) computational modelling of receptivefields in the lateral geniculate nucleus (LGN) and the primary visual cortex(V1) in biological vision. We show that by distributing the intermediate temporal scale levels accordingto a logarithmic distribution, we obtain much faster temporal responseproperties (shorter temporal delays) compared to a uniform distribution.Specifically, these kernels converge very rapidly to a limit kernel possessingtrue self-similar scale-invariant properties over temporal scales, therebyallowing for true scale invariance over variations in the temporal scale,although the underlying temporal scale-space representation is based on adiscretized temporal scale parameter. We show how scale-normalized temporal derivatives can be defined for thesetime-causal scale-space kernels and how the composed theory can be used forcomputing basic types of scale-normalized spatio-temporal derivativeexpressions in a computationally efficient manner.
arxiv-11100-272 | Bayesian Inference of Graphical Model Structures Using Trees | http://arxiv.org/abs/1504.02723 | author:Loïc Schwaller, Stéphane Robin, Michael Stumpf category:stat.ML published:2015-04-10 summary:We propose to learn the structure of an undirected graphical model bycomputing exact posterior probabilities for local structures in a Bayesianframework. This task would be untractable without any restriction on theconsidered graphs. We limit our exploration to the spanning trees and definepriors on tree structures and parameters that allow fast and exact computationof the posterior probability for an edge to belong to the random tree thanks toan algebraic result called the Matrix-Tree theorem. We show that the assumptionwe have made does not prevent our approach to perform well on synthetic andflow cytometry data.
arxiv-11100-273 | Discrimination and characterization of Parkinsonian rest tremors by analyzing long-term correlations and multifractal signatures | http://arxiv.org/abs/1504.02756 | author:Lorenzo Livi, Alireza Sadeghian, Hamid Sadeghian category:physics.med-ph cs.CV published:2015-04-10 summary:In this paper, we analyze 48 signals of rest tremor velocity related to 12distinct subjects affected by Parkinson's disease. The subjects belong to twodifferent groups, formed by four and eight subjects with, respectively, high-and low-amplitude rest tremors. Each subject is tested in four settings, givenby combining the use of deep brain stimulation and L-DOPA medication. Wedevelop two main feature-based representations of such signals, which areobtained by considering (i) the long-term correlations and multifractalproperties, and (ii) the power spectra. The feature-based representations areinitially utilized for the purpose of characterizing the subjects underdifferent settings. In agreement with previous studies, we show that deep brainstimulation does not significantly characterize neither of the two groups,regardless of the adopted representation. On the other hand, the medicationeffect yields statistically significant differences in both high- andlow-amplitude tremor groups. We successively test several different instancesof the two feature-based representations of the signals in the setting ofsupervised classification and (nonlinear) feature transformation. We considerthree different classification problems, involving the recognition of (i) thepresence of medication, (ii) the use of deep brain stimulation, and (iii) themembership to the high- and low-amplitude tremor groups. Classification resultsshow that the use of medication can be discriminated with higher accuracy,considering many of the feature-based representations. Notably, we show thatthe best results are obtained with a parsimonious, two-dimensionalrepresentation encoding the long-term correlations and multifractal characterof the signals.
arxiv-11100-274 | Leveraging Twitter for Low-Resource Conversational Speech Language Modeling | http://arxiv.org/abs/1504.02490 | author:Aaron Jaech, Mari Ostendorf category:cs.CL published:2015-04-09 summary:In applications involving conversational speech, data sparsity is a limitingfactor in building a better language model. We propose a simple,language-independent method to quickly harvest large amounts of data fromTwitter to supplement a smaller training set that is more closely matched tothe domain. The techniques lead to a significant reduction in perplexity onfour low-resource languages even though the presence on Twitter of theselanguages is relatively small. We also find that the Twitter text is moreuseful for learning word classes than the in-domain text and that use of theseword classes leads to further reductions in perplexity. Additionally, weintroduce a method of using social and textual information to prioritize thedownload queue during the Twitter crawling. This maximizes the amount of usefuldata that can be collected, impacting both perplexity and vocabulary coverage.
arxiv-11100-275 | Connectivity Preserving Multivalued Functions in Digital Topology | http://arxiv.org/abs/1504.02174 | author:Laurence Boxer, P. Christopher Staecker category:cs.CV math.GN I.4.m published:2015-04-09 summary:We study connectivity preserving multivalued functions between digitalimages. This notion generalizes that of continuous multivalued functionsstudied mostly in the setting of the digital plane $Z^2$. We show thatconnectivity preserving multivalued functions, like continuous multivaluedfunctions, are appropriate models for digital morpholological operations.Connectivity preservation, unlike continuity, is preserved by compositions, andgeneralizes easily to higher dimensions and arbitrary adjacency relations.
arxiv-11100-276 | What Do Deep CNNs Learn About Objects? | http://arxiv.org/abs/1504.02485 | author:Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko category:cs.CV published:2015-04-09 summary:Deep convolutional neural networks learn extremely powerful imagerepresentations, yet most of that power is hidden in the millions of deep-layerparameters. What exactly do these parameters represent? Recent work has startedto analyse CNN representations, finding that, e.g., they are invariant to some2D transformations Fischer et al. (2014), but are confused by particular typesof image noise Nguyen et al. (2014). In this work, we delve deeper and ask: howinvariant are CNNs to object-class variations caused by 3D shape, pose, andphotorealism?
arxiv-11100-277 | Unsupervised Feature Learning from Temporal Data | http://arxiv.org/abs/1504.02518 | author:Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun category:cs.CV cs.LG published:2015-04-09 summary:Current state-of-the-art classification and detection algorithms rely onsupervised training. In this work we study unsupervised feature learning in thecontext of temporally coherent video data. We focus on feature learning fromunlabeled video data, using the assumption that adjacent video frames containsemantically similar information. This assumption is exploited to train aconvolutional pooling auto-encoder regularized by slowness and sparsity. Weestablish a connection between slow feature learning to metric learning andshow that the trained encoder can be used to define a more temporally andsemantically coherent metric.
arxiv-11100-278 | Predicting Complete 3D Models of Indoor Scenes | http://arxiv.org/abs/1504.02437 | author:Ruiqi Guo, Chuhang Zou, Derek Hoiem category:cs.CV published:2015-04-09 summary:One major goal of vision is to infer physical models of objects, surfaces,and their layout from sensors. In this paper, we aim to interpret indoor scenesfrom one RGBD image. Our representation encodes the layout of walls, which mustconform to a Manhattan structure but is otherwise flexible, and the layout andextent of objects, modeled with CAD-like 3D shapes. We represent both thevisible and occluded portions of the scene, producing a $complete$ 3D parse.Such a scene interpretation is useful for robotics and visual reasoning, butdifficult to produce due to the well-known challenge of segmentation, the highdegree of occlusion, and the diversity of objects in indoor scene. We take adata-driven approach, generating sets of potential object regions, matching toregions in training images, and transferring and aligning associated 3D modelswhile encouraging fit to observations and overall consistency. We demonstrateencouraging results on the NYU v2 dataset and highlight a variety ofinteresting directions for future work.
arxiv-11100-279 | Deciding when to stop: Efficient stopping of active learning guided drug-target prediction | http://arxiv.org/abs/1504.02406 | author:Maja Temerinac-Ott, Armaghan W. Naik, Robert F. Murphy category:q-bio.QM cs.LG stat.ML published:2015-04-09 summary:Active learning has shown to reduce the number of experiments needed toobtain high-confidence drug-target predictions. However, in order to actuallysave experiments using active learning, it is crucial to have a method toevaluate the quality of the current prediction and decide when to stop theexperimentation process. Only by applying reliable stoping criteria to activelearning, time and costs in the experimental process can be actually saved. Wecompute active learning traces on simulated drug-target matrices in order tolearn a regression model for the accuracy of the active learner. By analyzingthe performance of the regression model on simulated data, we design stoppingcriteria for previously unseen experimental matrices. We demonstrate on fourpreviously characterized drug effect data sets that applying the stoppingcriteria can result in upto 40% savings of the total experiments for highlyaccurate predictions.
arxiv-11100-280 | Real-time Monocular Object SLAM | http://arxiv.org/abs/1504.02398 | author:Dorian Gálvez-López, Marta Salas, Juan D. Tardós, J. M. M. Montiel category:cs.RO cs.CV published:2015-04-09 summary:We present a real-time object-based SLAM system that leverages the largestobject database to date. Our approach comprises two main components: 1) amonocular SLAM algorithm that exploits object rigidity constraints to improvethe map and find its real scale, and 2) a novel object recognition algorithmbased on bags of binary words, which provides live detections with a databaseof 500 3D objects. The two components work together and benefit each other: theSLAM algorithm accumulates information from the observations of the objects,anchors object features to especial map landmarks and sets constrains on theoptimization. At the same time, objects partially or fully located within themap are used as a prior to guide the recognition algorithm, achieving higherrecall. We evaluate our proposal on five real environments showing improvementson the accuracy of the map and efficiency with respect to otherstate-of-the-art techniques.
arxiv-11100-281 | A Multiphase Image Segmentation Based on Fuzzy Membership Functions and L1-norm Fidelity | http://arxiv.org/abs/1504.02206 | author:Fang Li, Stanley Osher, Jing Qin, Ming Yan category:math.OC cs.CV published:2015-04-09 summary:In this paper, we propose a variational multiphase image segmentation modelbased on fuzzy membership functions and L1-norm fidelity. Then we apply thealternating direction method of multipliers to solve an equivalent problem. Allthe subproblems can be solved efficiently. Specifically, we propose a fastmethod to calculate the fuzzy median. Experimental results and comparisons showthat the L1-norm based method is more robust to outliers such as impulse noiseand keeps better contrast than its L2-norm counterpart. Theoretically, we provethe existence of the minimizer and analyze the convergence of the algorithm.
arxiv-11100-282 | Concentric network symmetry grasps authors' styles in word adjacency networks | http://arxiv.org/abs/1504.02162 | author:Diego R. Amancio, Filipi N. Silva, Luciano da F. Costa category:cs.CL published:2015-04-09 summary:Several characteristics of written texts have been inferred from statisticalanalysis derived from networked models. Even though many network measurementshave been adapted to study textual properties at several levels of complexity,some textual aspects have been disregarded. In this paper, we study thesymmetry of word adjacency networks, a well-known representation of text as agraph. A statistical analysis of the symmetry distribution performed in severalnovels showed that most of the words do not display symmetric patterns ofconnectivity. More specifically, the merged symmetry displayed a distributionsimilar to the ubiquitous power-law distribution. Our experiments also revealedthat the studied metrics do not correlate with other traditional networkmeasurements, such as the degree or betweenness centrality. The effectivenessof the symmetry measurements was verified in the authorship attribution task.Interestingly, we found that specific authors prefer particular types ofsymmetric motifs. As a consequence, the authorship of books could be accuratelyidentified in 82.5% of the cases, in a dataset comprising books written by 8authors. Because the proposed measurements for text analysis are complementaryto the traditional approach, they can be used to improve the characterizationof text networks, which might be useful for related applications, such as thoserelying on the identification of topical words and information retrieval.
arxiv-11100-283 | Linearly Supporting Feature Extraction For Automated Estimation Of Stellar Atmospheric Parameters | http://arxiv.org/abs/1504.02164 | author:Xiangru Li, Yu Lu, Georges Comte, Ali Luo, Yongheng Zhao, Yongjun Wang category:astro-ph.SR astro-ph.IM cs.CV published:2015-04-09 summary:We describe a scheme to extract linearly supporting (LSU) features fromstellar spectra to automatically estimate the atmospheric parameters $T_{eff}$,log$~g$, and [Fe/H]. "Linearly supporting" means that the atmosphericparameters can be accurately estimated from the extracted features through alinear model. The successive steps of the process are as follow: first,decompose the spectrum using a wavelet packet (WP) and represent it by thederived decomposition coefficients; second, detect representative spectralfeatures from the decomposition coefficients using the proposed method LeastAbsolute Shrinkage and Selection Operator (LARS)$_{bs}$; third, estimate theatmospheric parameters $T_{eff}$, log$~g$, and [Fe/H] from the detectedfeatures using a linear regression method. One prominent characteristic of thisscheme is its ability to evaluate quantitatively the contribution of eachdetected feature to the atmospheric parameter estimate and also to trace backthe physical significance of that feature. This work also shows that theusefulness of a component depends on both wavelength and frequency. Theproposed scheme has been evaluated on both real spectra from the Sloan DigitalSky Survey (SDSS)/SEGUE and synthetic spectra calculated from Kurucz's NEWODFmodels. On real spectra, we extracted 23 features to estimate $T_{eff}$, 62features for log$~g$, and 68 features for [Fe/H]. Test consistencies betweenour estimates and those provided by the Spectroscopic Sarameter Pipeline ofSDSS show that the mean absolute errors (MAEs) are 0.0062 dex for log$~T_{eff}$(83 K for $T_{eff}$), 0.2345 dex for log$~g$, and 0.1564 dex for [Fe/H]. Forthe synthetic spectra, the MAE test accuracies are 0.0022 dex for log$~T_{eff}$(32 K for $T_{eff}$), 0.0337 dex for log$~g$, and 0.0268 dex for [Fe/H].
arxiv-11100-284 | Phase Transitions in Spectral Community Detection of Large Noisy Networks | http://arxiv.org/abs/1504.02412 | author:Pin-Yu Chen, Alfred O. Hero III category:cs.SI physics.soc-ph stat.ML published:2015-04-09 summary:In this paper, we study the sensitivity of the spectral clustering basedcommunity detection algorithm subject to a Erdos-Renyi type random noise model.We prove phase transitions in community detectability as a function of theexternal edge connection probability and the noisy edge presence probabilityunder a general network model where two arbitrarily connected communities areinterconnected by random external edges. Specifically, the community detectionperformance transitions from almost perfect detectability to low detectabilityas the inter-community edge connection probability exceeds some critical value.We derive upper and lower bounds on the critical value and show that the boundsare identical when the two communities have the same size. The phase transitionresults are validated using network simulations. Using the derived expressionsfor the phase transition threshold we propose a method for estimating thisthreshold from observed data.
arxiv-11100-285 | A Collection of Challenging Optimization Problems in Science, Engineering and Economics | http://arxiv.org/abs/1504.02366 | author:Dhagash Mehta, Crina Grosan category:cs.NA cs.MS cs.NE math.AG math.NA math.OC published:2015-04-09 summary:Function optimization and finding simultaneous solutions of a system ofnonlinear equations (SNE) are two closely related and important optimizationproblems. However, unlike in the case of function optimization in which one isrequired to find the global minimum and sometimes local minima, a database ofchallenging SNEs where one is required to find stationary points (extrama andsaddle points) is not readily available. In this article, we initiate buildingsuch a database of important SNE (which also includes related functionoptimization problems), arising from Science, Engineering and Economics. Afterproviding a short review of the most commonly used mathematical andcomputational approaches to find solutions of such systems, we provide apreliminary list of challenging problems by writing the Mathematicalformulation down, briefly explaning the origin and importance of the problemand giving a short account on the currently known results, for each of theproblems. We anticipate that this database will not only help benchmarkingnovel numerical methods for solving SNEs and function optimization problems butalso will help advancing the corresponding research areas.
arxiv-11100-286 | Robust, scalable and fast bootstrap method for analyzing large scale data | http://arxiv.org/abs/1504.02382 | author:Shahab Basiri, Esa Ollila, Visa Koivunen category:stat.ME cs.IR cs.IT math.IT stat.CO stat.ML published:2015-04-09 summary:In this paper we address the problem of performing statistical inference forlarge scale data sets i.e., Big Data. The volume and dimensionality of the datamay be so high that it cannot be processed or stored in a single computingnode. We propose a scalable, statistically robust and computationally efficientbootstrap method, compatible with distributed processing and storage systems.Bootstrap resamples are constructed with smaller number of distinct data pointson multiple disjoint subsets of data, similarly to the bag of little bootstrapmethod (BLB) [1]. Then significant savings in computation is achieved byavoiding the re-computation of the estimator for each bootstrap sample.Instead, a computationally efficient fixed-point estimation equation isanalytically solved via a smart approximation following the Fast and RobustBootstrap method (FRB) [2]. Our proposed bootstrap method facilitates the useof highly robust statistical methods in analyzing large scale data sets. Thefavorable statistical properties of the method are established analytically.Numerical examples demonstrate scalability, low complexity and robuststatistical performance of the method in analyzing large data sets.
arxiv-11100-287 | Kernel Manifold Alignment | http://arxiv.org/abs/1504.02338 | author:Devis Tuia, Gustau Camps-Valls category:stat.ML cs.LG published:2015-04-09 summary:We introduce a kernel method for manifold alignment (KEMA) and domainadaptation that can match an arbitrary number of data sources without needingcorresponding pairs, just few labeled examples in all domains. KEMA hasinteresting properties: 1) it generalizes other manifold alignment methods, 2)it can align manifolds of very different complexities, performing a sort ofmanifold unfolding plus alignment, 3) it can define a domain-specific metric tocope with multimodal specificities, 4) it can align data spaces of differentdimensionality, 5) it is robust to strong nonlinear feature deformations, and6) it is closed-form invertible which allows transfer across-domains and datasynthesis. We also present a reduced-rank version for computational efficiencyand discuss the generalization performance of KEMA under Rademacher principlesof stability. KEMA exhibits very good performance over competing methods insynthetic examples, visual object recognition and recognition of facialexpressions tasks.
arxiv-11100-288 | Exploring EEG for Object Detection and Retrieval | http://arxiv.org/abs/1504.02356 | author:Eva Mohedano, Amaia Salvador, Sergi Porta, Xavier Giró-i-Nieto, Graham Healy, Kevin McGuinness, Noel O'Connor, Alan F. Smeaton category:cs.HC cs.CV cs.IR H.1.2; H.3.3 published:2015-04-09 summary:This paper explores the potential for using Brain Computer Interfaces (BCI)as a relevance feedback mechanism in content-based image retrieval. Weinvestigate if it is possible to capture useful EEG signals to detect ifrelevant objects are present in a dataset of realistic and complex images. Weperform several experiments using a rapid serial visual presentation (RSVP) ofimages at different rates (5Hz and 10Hz) on 8 users with different degrees offamiliarization with BCI and the dataset. We then use the feedback from the BCIand mouse-based interfaces to retrieve localized objects in a subset of TRECVidimages. We show that it is indeed possible to detect such objects in compleximages and, also, that users with previous knowledge on the dataset orexperience with the RSVP outperform others. When the users have limited time toannotate the images (100 seconds in our experiments) both interfaces arecomparable in performance. Comparing our best users in a retrieval task, wefound that EEG-based relevance feedback outperforms mouse-based feedback. Therealistic and complex image dataset differentiates our work from previousstudies on EEG for image retrieval.
arxiv-11100-289 | When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition | http://arxiv.org/abs/1504.02351 | author:Guosheng Hu, Yongxin Yang, Dong Yi, Josef Kittler, William Christmas, Stan Z. Li, Timothy Hospedales category:cs.CV cs.LG cs.NE published:2015-04-09 summary:Deep learning, in particular Convolutional Neural Network (CNN), has achievedpromising results in face recognition recently. However, it remains an openquestion: why CNNs work well and how to design a 'good' architecture. Theexisting works tend to focus on reporting CNN architectures that work well forface recognition rather than investigate the reason. In this work, we conductan extensive evaluation of CNN-based face recognition systems (CNN-FRS) on acommon ground to make our work easily reproducible. Specifically, we use publicdatabase LFW (Labeled Faces in the Wild) to train CNNs, unlike most existingCNNs trained on private databases. We propose three CNN architectures which arethe first reported architectures trained using LFW data. This paperquantitatively compares the architectures of CNNs and evaluate the effect ofdifferent implementation choices. We identify several useful properties ofCNN-FRS. For instance, the dimensionality of the learned features can besignificantly reduced without adverse effect on face recognition accuracy. Inaddition, traditional metric learning method exploiting CNN-learned features isevaluated. Experiments show two crucial factors to good CNN-FRS performance arethe fusion of multiple CNNs and metric learning. To make our work reproducible,source code and models will be made publicly available.
arxiv-11100-290 | `local' vs. `global' parameters -- breaking the gaussian complexity barrier | http://arxiv.org/abs/1504.02191 | author:Shahar Mendelson category:stat.ML math.ST stat.TH published:2015-04-09 summary:We show that if $F$ is a convex class of functions that is $L$-subgaussian,the error rate of learning problems generated by independent noise isequivalent to a fixed point determined by `local' covering estimates of theclass, rather than by the gaussian averages. To that end, we establish newsharp upper and lower estimates on the error rate for such problems.
arxiv-11100-291 | Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor | http://arxiv.org/abs/1504.02340 | author:Wongun Choi category:cs.CV published:2015-04-09 summary:In this paper, we focus on the two key aspects of multiple target trackingproblem: 1) designing an accurate affinity measure to associate detections and2) implementing an efficient and accurate (near) online multiple targettracking algorithm. As the first contribution, we introduce a novel AggregatedLocal Flow Descriptor (ALFD) that encodes the relative motion pattern between apair of temporally distant detections using long term interest pointtrajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robustaffinity measure for estimating the likelihood of matching detectionsregardless of the application scenarios. As another contribution, we present aNear-Online Multi-target Tracking (NOMT) algorithm. The tracking problem isformulated as a data-association between targets and detections in a temporalwindow, that is performed repeatedly at every frame. While being efficient,NOMT achieves robustness via integrating multiple cues including ALFD metric,target dynamics, appearance similarity, and long term trajectory regularizationinto the model. Our ablative analysis verifies the superiority of the ALFDmetric over the other conventional affinity metrics. We run a comprehensiveexperimental evaluation on two challenging tracking datasets, KITTI and MOTdatasets. The NOMT method combined with ALFD metric achieves the best accuracyin both datasets with significant margins (about 10% higher MOTA) over thestate-of-the-arts.
arxiv-11100-292 | Extraction of Protein Sequence Motif Information using PSO K-Means | http://arxiv.org/abs/1504.02235 | author:R. Gowri, R. Rathipriya category:cs.CV published:2015-04-09 summary:The main objective of the paper is to find the motif information.Thefunctionalities of the proteins are ideally found from their motif informationwhich is extracted using various techniques like clustering with k-means,hybrid k-means, self-organising maps, etc., in the literature. In this workprotein sequence information is extracted using optimised k-means algorithm.The particle swarm optimisation technique is one of the frequently usedoptimisation method. In the current work the PSO k-means is used for motifinformation extraction. This paper also deals with the comparison between themotif information obtained from clusters and biclustersusing PSO k-meansalgorithm. The motif information acquired is based on the structure homogeneityof the protein sequence.
arxiv-11100-293 | Projective simulation with generalization | http://arxiv.org/abs/1504.02247 | author:Alexey A. Melnikov, Adi Makmal, Vedran Dunjko, Hans J. Briegel category:cs.AI cs.LG stat.ML published:2015-04-09 summary:The ability to generalize is an important feature of any intelligent agent.Not only because it may allow the agent to cope with large amounts of data, butalso because in some environments, an agent with no generalization ability issimply doomed to fail. In this work we outline several criteria forgeneralization, and present a dynamic and autonomous machinery that enablesprojective simulation agents to meaningfully generalize. Projective simulation,a novel, physical, approach to artificial intelligence, was recently shown toperform well, in comparison with standard models, on both simple reinforcementlearning problems, as well as on more complicated canonical tasks, such as the"grid world" and the "mountain car problem". Both the basic projectivesimulation model and the presented generalization machinery are based on verysimple principles. This simplicity allows us to provide a full analyticalanalysis of the agent's performance and to illustrate the benefit the agentgains by generalizing. Specifically, we show how such an ability allows theagent to learn in rather extreme environments, in which learning is otherwiseimpossible.
arxiv-11100-294 | A Group Theoretic Perspective on Unsupervised Deep Learning | http://arxiv.org/abs/1504.02462 | author:Arnab Paul, Suresh Venkatasubramanian category:cs.LG cs.NE stat.ML published:2015-04-08 summary:Why does Deep Learning work? What representations does it capture? How dohigher-order representations emerge? We study these questions from theperspective of group theory, thereby opening a new approach towards a theory ofDeep learning. One factor behind the recent resurgence of the subject is a key algorithmicstep called {\em pretraining}: first search for a good generative model for theinput samples, and repeat the process one layer at a time. We show deeperimplications of this simple principle, by establishing a connection with theinterplay of orbits and stabilizers of group actions. Although the neuralnetworks themselves may not form groups, we show the existence of {\em shadow}groups whose elements serve as close approximations. Over the shadow groups, the pre-training step, originally introduced as amechanism to better initialize a network, becomes equivalent to a search forfeatures with minimal orbits. Intuitively, these features are in a way the {\emsimplest}. Which explains why a deep learning network learns simple featuresfirst. Next, we show how the same principle, when repeated in the deeperlayers, can capture higher order representations, and why representationcomplexity increases as the layers get deeper.
arxiv-11100-295 | Data Mining for Prediction of Human Performance Capability in the Software-Industry | http://arxiv.org/abs/1504.01934 | author:Gaurav Singh Thakur, Anubhav Gupta, Sangita Gupta category:cs.LG published:2015-04-08 summary:The recruitment of new personnel is one of the most essential businessprocesses which affect the quality of human capital within any company. It ishighly essential for the companies to ensure the recruitment of right talent tomaintain a competitive edge over the others in the market. However IT companiesoften face a problem while recruiting new people for their ongoing projects dueto lack of a proper framework that defines a criteria for the selectionprocess. In this paper we aim to develop a framework that would allow anyproject manager to take the right decision for selecting new talent bycorrelating performance parameters with the other domain-specific attributes ofthe candidates. Also, another important motivation behind this project is tocheck the validity of the selection procedure often followed by various bigcompanies in both public and private sectors which focus only on academicscores, GPA/grades of students from colleges and other academic backgrounds. Wetest if such a decision will produce optimal results in the industry or isthere a need for change that offers a more holistic approach to recruitment ofnew talent in the software companies. The scope of this work extends beyond theIT domain and a similar procedure can be adopted to develop a recruitmentframework in other fields as well. Data-mining techniques provide usefulinformation from the historical projects depending on which the hiring-managercan make decisions for recruiting high-quality workforce. This study aims tobridge this hiatus by developing a data-mining framework based on anensemble-learning technique to refocus on the criteria for personnel selection.The results from this research clearly demonstrated that there is a need torefocus on the selection-criteria for quality objectives.
arxiv-11100-296 | MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking | http://arxiv.org/abs/1504.01942 | author:Laura Leal-Taixé, Anton Milan, Ian Reid, Stefan Roth, Konrad Schindler category:cs.CV published:2015-04-08 summary:In the recent past, the computer vision community has developed centralizedbenchmarks for the performance evaluation of a variety of tasks, includinggeneric object and pedestrian detection, 3D reconstruction, optical flow,single-object short-term tracking, and stereo estimation. Despite potentialpitfalls of such benchmarks, they have proved to be extremely helpful toadvance the state of the art in the respective area. Interestingly, there hasbeen rather limited work on the standardization of quantitative benchmarks formultiple target tracking. One of the few exceptions is the well-known PETSdataset, targeted primarily at surveillance applications. Despite being widelyused, it is often applied inconsistently, for example involving using differentsubsets of the available data, different ways of training the models, ordiffering evaluation scripts. This paper describes our work toward a novelmultiple object tracking benchmark aimed to address such issues. We discuss thechallenges of creating such a framework, collecting existing and new data,gathering state-of-the-art methods to be tested on the datasets, and finallycreating a unified evaluation system. With MOTChallenge we aim to pave the waytoward a unified evaluation framework for a more meaningful quantification ofmulti-target tracking.
arxiv-11100-297 | Evaluating Two-Stream CNN for Video Classification | http://arxiv.org/abs/1504.01920 | author:Hao Ye, Zuxuan Wu, Rui-Wei Zhao, Xi Wang, Yu-Gang Jiang, Xiangyang Xue category:cs.CV published:2015-04-08 summary:Videos contain very rich semantic information. Traditional hand-craftedfeatures are known to be inadequate in analyzing complex video semantics.Inspired by the huge success of the deep learning methods in analyzing image,audio and text data, significant efforts are recently being devoted to thedesign of deep nets for video analytics. Among the many practical needs,classifying videos (or video clips) based on their major semantic categories(e.g., "skiing") is useful in many applications. In this paper, we conduct anin-depth study to investigate important implementation options that may affectthe performance of deep nets on video classification. Our evaluations areconducted on top of a recent two-stream convolutional neural network (CNN)pipeline, which uses both static frames and motion optical flows, and hasdemonstrated competitive performance against the state-of-the-art methods. Inorder to gain insights and to arrive at a practical guideline, many importantoptions are studied, including network architectures, model fusion, learningparameters and the final prediction methods. Based on the evaluations, verycompetitive results are attained on two popular video classificationbenchmarks. We hope that the discussions and conclusions from this work canhelp researchers in related fields to quickly set up a good basis for furtherinvestigations along this very promising direction.
arxiv-11100-298 | The Computational Power of Optimization in Online Learning | http://arxiv.org/abs/1504.02089 | author:Elad Hazan, Tomer Koren category:cs.LG cs.GT published:2015-04-08 summary:We consider the fundamental problem of prediction with expert advice wherethe experts are "optimizable": there is a black-box optimization oracle thatcan be used to compute, in constant time, the leading expert in retrospect atany point in time. In this setting, we give a novel online algorithm thatattains vanishing regret with respect to $N$ experts in total$\widetilde{O}(\sqrt{N})$ computation time. We also give a lower bound showingthat this running time cannot be improved (up to log factors) in the oraclemodel, thereby exhibiting a quadratic speedup as compared to the standard,oracle-free setting where the required time for vanishing regret is$\widetilde{\Theta}(N)$. These results demonstrate an exponential gap betweenthe power of optimization in online learning and its power in statisticallearning: in the latter, an optimization oracle---i.e., an efficient empiricalrisk minimizer---allows to learn a finite hypothesis class of size $N$ in time$O(\log{N})$. We also study the implications of our results to learning inrepeated zero-sum games, in a setting where the players have access to oraclesthat compute, in constant time, their best-response to any mixed strategy oftheir opponent. We show that the runtime required for approximating the minimaxvalue of the game in this setting is $\widetilde{\Theta}(\sqrt{N})$, yieldingagain a quadratic improvement upon the oracle-free setting, where$\widetilde{\Theta}(N)$ is known to be tight.
arxiv-11100-299 | Kernelized Low Rank Representation on Grassmann Manifolds | http://arxiv.org/abs/1504.01806 | author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV published:2015-04-08 summary:Low rank representation (LRR) has recently attracted great interest due toits pleasing efficacy in exploring low-dimensional subspace structures embeddedin data. One of its successful applications is subspace clustering which meansdata are clustered according to the subspaces they belong to. In this paper, ata higher level, we intend to cluster subspaces into classes of subspaces. Thisis naturally described as a clustering problem on Grassmann manifold. Thenovelty of this paper is to generalize LRR on Euclidean space onto an LRR modelon Grassmann manifold in a uniform kernelized framework. The new methods havemany applications in computer vision tasks. Several clustering experiments areconducted on handwritten digit images, dynamic textures, human face clips andtraffic scene sequences. The experimental results show that the proposedmethods outperform a number of state-of-the-art subspace clustering methods.
arxiv-11100-300 | Low Rank Representation on Grassmann Manifolds: An Extrinsic Perspective | http://arxiv.org/abs/1504.01807 | author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV published:2015-04-08 summary:Many computer vision algorithms employ subspace models to represent data. TheLow-rank representation (LRR) has been successfully applied in subspaceclustering for which data are clustered according to their subspace structures.The possibility of extending LRR on Grassmann manifold is explored in thispaper. Rather than directly embedding Grassmann manifold into a symmetricmatrix space, an extrinsic view is taken by building the self-representation ofLRR over the tangent space of each Grassmannian point. A new algorithm forsolving the proposed Grassmannian LRR model is designed and implemented.Several clustering experiments are conducted on handwritten digits dataset,dynamic texture video clips and YouTube celebrity face video data. Theexperimental results show our method outperforms a number of existing methods.
