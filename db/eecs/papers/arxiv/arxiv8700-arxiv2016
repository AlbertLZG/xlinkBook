arxiv-1411-3128 | Deep Multi-Instance Transfer Learning |  http://arxiv.org/abs/1411.3128  | author:Dimitrios Kotzias, Misha Denil, Phil Blunsom, Nando de Freitas category:cs.LG stat.ML published:2014-11-12 summary:We present a new approach for transferring knowledge from groups toindividuals that comprise them. We evaluate our method in text, by inferringthe ratings of individual sentences using full-review ratings. This approach,which combines ideas from transfer learning, deep learning and multi-instancelearning, reduces the need for laborious human labelling of fine-grained datawhen abundant labels are available at the group level.
arxiv-1411-3229 | Multi-modal Image Registration for Correlative Microscopy |  http://arxiv.org/abs/1411.3229  | author:Tian Cao, Christopher Zach, Shannon Modla, Debbie Powell, Kirk Czymmek, Marc Niethammer category:cs.CV published:2014-11-12 summary:Correlative microscopy is a methodology combining the functionality of lightmicroscopy with the high resolution of electron microscopy and other microscopytechnologies. Image registration for correlative microscopy is quitechallenging because it is a multi-modal, multi-scale and multi-dimensionalregistration problem. In this report, I introduce two methods of imageregistration for correlative microscopy. The first method is based on fiducials(beads). I generate landmarks from the fiducials and compute the similaritytransformation matrix based on three pairs of nearest corresponding landmarks.A least-squares matching process is applied afterwards to further refine theregistration. The second method is inspired by the image analogies approach. Iintroduce the sparse representation model into image analogies. I first trainrepresentative image patches (dictionaries) for pre-registered datasets fromtwo different modalities, and then I use the sparse coding technique totransfer a given image to a predicted image from one modality to another basedon the learned dictionaries. The final image registration is between thepredicted image and the original image corresponding to the given image in thedifferent modality. The method transforms a multi-modal registration problem toa mono-modal one. I test my approaches on Transmission Electron Microscopy(TEM) and confocal microscopy images. Experimental results of the methods arealso shown in this report.
arxiv-1411-3230 | Sparse Modeling for Image and Vision Processing |  http://arxiv.org/abs/1411.3230  | author:Julien Mairal, Francis Bach, Jean Ponce category:cs.CV published:2014-11-12 summary:In recent years, a large amount of multi-disciplinary research has beenconducted on sparse models and their applications. In statistics and machinelearning, the sparsity principle is used to perform model selection---that is,automatically selecting a simple model among a large collection of them. Insignal processing, sparse coding consists of representing data with linearcombinations of a few dictionary elements. Subsequently, the correspondingtools have been widely adopted by several scientific communities such asneuroscience, bioinformatics, or computer vision. The goal of this monograph isto offer a self-contained view of sparse modeling for visual recognition andimage processing. More specifically, we focus on applications where thedictionary is learned and adapted to data, yielding a compact representationthat has been successful in various contexts.
arxiv-1411-2942 | 3D Shape Estimation from 2D Landmarks: A Convex Relaxation Approach |  http://arxiv.org/abs/1411.2942  | author:Xiaowei Zhou, Spyridon Leonardos, Xiaoyan Hu, Kostas Daniilidis category:cs.CV published:2014-11-11 summary:We investigate the problem of estimating the 3D shape of an object, given aset of 2D landmarks in a single image. To alleviate the reconstructionambiguity, a widely-used approach is to confine the unknown 3D shape within ashape space built upon existing shapes. While this approach has proven to besuccessful in various applications, a challenging issue remains, i.e., thejoint estimation of shape parameters and camera-pose parameters requires tosolve a nonconvex optimization problem. The existing methods often adopt analternating minimization scheme to locally update the parameters, andconsequently the solution is sensitive to initialization. In this paper, wepropose a convex formulation to address this problem and develop an efficientalgorithm to solve the proposed convex program. We demonstrate the exactrecovery property of the proposed method, its merits compared to alternativemethods, and the applicability in human pose and car shape estimation.
arxiv-1411-2698 | Bayesian group latent factor analysis with structured sparsity |  http://arxiv.org/abs/1411.2698  | author:Shiwen Zhao, Chuan Gao, Sayan Mukherjee, Barbara E Engelhardt category:stat.ME q-bio.QM stat.ML published:2014-11-11 summary:Latent factor models are the canonical statistical tool for exploratoryanalyses of low-dimensional linear structure for an observation matrix with pfeatures across n samples. We develop a structured Bayesian group factoranalysis model that extends the factor model to multiple coupled observationmatrices; in the case of two observations, this reduces to a Bayesian model ofcanonical correlation analysis. The main contribution of this work is tocarefully define a structured Bayesian prior that encourages both element-wiseand column-wise shrinkage and leads to desirable behavior on high-dimensionaldata. In particular, our model puts a structured prior on the joint factorloading matrix, regularizing at three levels, which enables element-wisesparsity and unsupervised recovery of latent factors corresponding tostructured variance across arbitrary subsets of the observations. In addition,our structured prior allows for both dense and sparse latent factors so thatcovariation among either all features or only a subset of features can both berecovered. We use fast parameter-expanded expectation-maximization forparameter estimation in this model. We validate our method on both simulateddata with substantial structure and real data, comparing against a number ofstate-of-the-art approaches. These results illustrate useful properties of ourmodel, including i) recovering sparse signal in the presence of dense effects;ii) the ability to scale naturally to large numbers of observations; iii)flexible observation- and factor-specific regularization to recover factorswith a wide variety of sparsity levels and percentage of variance explained;and iv) tractable inference that scales to modern genomic and document datasizes.
arxiv-1411-3013 | Bayesian Evidence and Model Selection |  http://arxiv.org/abs/1411.3013  | author:Kevin H. Knuth, Michael Habeck, Nabin K. Malakar, Asim M. Mubeen, Ben Placek category:stat.ME astro-ph.IM stat.AP stat.CO stat.ML published:2014-11-11 summary:In this paper we review the concepts of Bayesian evidence and Bayes factors,also known as log odds ratios, and their application to model selection. Thetheory is presented along with a discussion of analytic, approximate andnumerical techniques. Specific attention is paid to the Laplace approximation,variational Bayes, importance sampling, thermodynamic integration, and nestedsampling and its recent variants. Analogies to statistical physics, from whichmany of these techniques originate, are discussed in order to provide readerswith deeper insights that may lead to new techniques. The utility of Bayesianmodel testing in the domain sciences is demonstrated by presenting fourspecific practical examples considered within the context of signal processingin the areas of signal detection, sensor characterization, scientific modelselection and molecular force characterization.
arxiv-1411-2861 | Computational Baby Learning |  http://arxiv.org/abs/1411.2861  | author:Xiaodan Liang, Si Liu, Yunchao Wei, Luoqi Liu, Liang Lin, Shuicheng Yan category:cs.CV published:2014-11-11 summary:Intuitive observations show that a baby may inherently possess the capabilityof recognizing a new visual concept (e.g., chair, dog) by learning from onlyvery few positive instances taught by parent(s) or others, and this recognitioncapability can be gradually further improved by exploring and/or interactingwith the real instances in the physical world. Inspired by these observations,we propose a computational model for slightly-supervised object detection,based on prior knowledge modelling, exemplar learning and learning with videocontexts. The prior knowledge is modeled with a pre-trained ConvolutionalNeural Network (CNN). When very few instances of a new concept are given, aninitial concept detector is built by exemplar learning over the deep featuresfrom the pre-trained CNN. Simulating the baby's interaction with physicalworld, the well-designed tracking solution is then used to discover morediverse instances from the massive online unlabeled videos. Once a positiveinstance is detected/identified with high score in each video, more variableinstances possibly from different view-angles and/or different distances aretracked and accumulated. Then the concept detector can be fine-tuned based onthese new instances. This process can be repeated again and again till weobtain a very mature concept detector. Extensive experiments on PascalVOC-07/10/12 object detection datasets well demonstrate the effectiveness ofour framework. It can beat the state-of-the-art full-training basedperformances by learning from very few samples for each object category, alongwith about 20,000 unlabeled videos.
arxiv-1411-2897 | Accelerating the ANT Colony Optimization By Smart ANTs, Using Genetic Operator |  http://arxiv.org/abs/1411.2897  | author:Hassan Ismkhan category:cs.NE published:2014-11-11 summary:This paper research review Ant colony optimization (ACO) and GeneticAlgorithm (GA), both are two powerful meta-heuristics. This paper explains somemajor defects of these two algorithm at first then proposes a new model for ACOin which, artificial ants use a quick genetic operator and accelerate theiractions in selecting next state. Experimental results show that proposed hybridalgorithm is effective and its performance including speed and accuracy beatsother version.
arxiv-1411-2821 | Turn Down that Noise: Synaptic Encoding of Afferent SNR in a Single Spiking Neuron |  http://arxiv.org/abs/1411.2821  | author:Saeed Afshar, Libin George, Jonathan Tapson, Andre van Schaik, Philip de Chazal, Tara Julia Hamilton category:cs.NE q-bio.NC published:2014-11-11 summary:We have added a simplified neuromorphic model of Spike Time DependentPlasticity (STDP) to the Synapto-dendritic Kernel Adapting Neuron (SKAN). Theresulting neuron model is the first to show synaptic encoding of afferentsignal to noise ratio in addition to the unsupervised learning of spatiotemporal spike patterns. The neuron model is particularly suitable forimplementation in digital neuromorphic hardware as it does not use any complexmathematical operations and uses a novel approach to achieve synaptichomeostasis. The neurons noise compensation properties are characterized andtested on noise corrupted zeros digits of the MNIST handwritten dataset.Results show the simultaneously learning common patterns in its input datawhile dynamically weighing individual afferent channels based on their signalto noise ratio. Despite its simplicity the interesting behaviors of the neuronmodel and the resulting computational power may offer insights into biologicalsystems.
arxiv-1411-2820 | Supervised Classification of Flow Cytometric Samples via the Joint Clustering and Matching (JCM) Procedure |  http://arxiv.org/abs/1411.2820  | author:Sharon X. Lee, Geoffrey J. McLachlan, Saumyadipta Pyne category:q-bio.QM stat.ME stat.ML published:2014-11-11 summary:We consider the use of the Joint Clustering and Matching (JCM) procedure forthe supervised classification of a flow cytometric sample with respect to anumber of predefined classes of such samples. The JCM procedure has beenproposed as a method for the unsupervised classification of cells within asample into a number of clusters and in the case of multiple samples, thematching of these clusters across the samples. The two tasks of clustering andmatching of the clusters are performed simultaneously within the JCM framework.In this paper, we consider the case where there is a number of distinct classesof samples whose class of origin is known, and the problem is to classify a newsample of unknown class of origin to one of these predefined classes. Forexample, the different classes might correspond to the types of a particulardisease or to the various health outcomes of a patient subsequent to a courseof treatment. We show and demonstrate on some real datasets how the JCMprocedure can be used to carry out this supervised classification task. Amixture distribution is used to model the distribution of the expressions of afixed set of markers for each cell in a sample with the components in themixture model corresponding to the various populations of cells in thecomposition of the sample. For each class of samples, a class template isformed by the adoption of random-effects terms to model the inter-samplevariation within a class. The classification of a new unclassified sample isundertaken by assigning the unclassified sample to the class that minimizes theKullback-Leibler distance between its fitted mixture density and each classdensity provided by the class templates.
arxiv-1411-2795 | Speaker Identification From Youtube Obtained Data |  http://arxiv.org/abs/1411.2795  | author:Nitesh Kumar Chaudhary category:cs.SD cs.LG published:2014-11-11 summary:An efficient, and intuitive algorithm is presented for the identification ofspeakers from a long dataset (like YouTube long discussion, Cocktail partyrecorded audio or video).The goal of automatic speaker identification is toidentify the number of different speakers and prepare a model for that speakerby extraction, characterization and speaker-specific information contained inthe speech signal. It has many diverse application specially in the field ofSurveillance, Immigrations at Airport, cyber security, transcription inmulti-source of similar sound source, where it is difficult to assigntranscription arbitrary. The most commonly speech parametrization used inspeaker verification, K-mean, cepstral analysis, is detailed. Gaussian mixturemodeling, which is the speaker modeling technique is then explained. Gaussianmixture models (GMM), perhaps the most robust machine learning algorithm hasbeen introduced examine and judge carefully speaker identification in textindependent. The application or employment of Gaussian mixture models formonitoring & Analysing speaker identity is encouraged by the familiarity,awareness, or understanding gained through experience that Gaussian spectrumdepict the characteristics of speaker's spectral conformational pattern andremarkable ability of GMM to construct capricious densities after that weillustrate 'Expectation maximization' an iterative algorithm which takes somearbitrary value in initial estimation and carry on the iterative process untilthe convergence of value is observed,so by doing various number of experimentswe are able to obtain 79 ~ 82% of identification rate using Vector quantizationand 85 ~ 92.6% of identification rate using GMM modeling by Expectationmaximization parameter estimation depending on variation of parameter.
arxiv-1411-2919 | Bounded Regret for Finite-Armed Structured Bandits |  http://arxiv.org/abs/1411.2919  | author:Tor Lattimore, Remi Munos category:cs.LG published:2014-11-11 summary:We study a new type of K-armed bandit problem where the expected return ofone arm may depend on the returns of other arms. We present a new algorithm forthis general class of problems and show that under certain circumstances it ispossible to achieve finite expected cumulative regret. We also giveproblem-dependent lower bounds on the cumulative regret showing that at leastin special cases the new algorithm is nearly optimal.
arxiv-1411-3197 | Warranty Cost Estimation Using Bayesian Network |  http://arxiv.org/abs/1411.3197  | author:Karamjit Singh, Puneet Agarwal, Gautam Shroff category:cs.AI cs.LG published:2014-11-11 summary:All multi-component product manufacturing companies face the problem ofwarranty cost estimation. Failure rate analysis of components plays a key rolein this problem. Data source used for failure rate analysis has traditionallybeen past failure data of components. However, failure rate analysis can beimproved by means of fusion of additional information, such as symptomsobserved during after-sale service of the product, geographical information(hilly or plains areas), and information from tele-diagnostic analytics. Inthis paper, we propose an approach, which learns dependency betweenpart-failures and symptoms gleaned from such diverse sources of information, topredict expected number of failures with better accuracy. We also indicate howthe optimum warranty period can be computed. We demonstrate, through empiricalresults, that our method can improve the warranty cost estimates significantly.
arxiv-1411-2679 | Inferring User Preferences by Probabilistic Logical Reasoning over Social Networks |  http://arxiv.org/abs/1411.2679  | author:Jiwei Li, Alan Ritter, Dan Jurafsky category:cs.SI cs.AI cs.CL cs.LG published:2014-11-11 summary:We propose a framework for inferring the latent attitudes or preferences ofusers by performing probabilistic first-order logical reasoning over the socialnetwork graph. Our method answers questions about Twitter users like {\em Doesthis user like sushi?} or {\em Is this user a New York Knicks fan?} by buildinga probabilistic model that reasons over user attributes (the user's location orgender) and the social network (the user's friends and spouse), via inferenceslike homophily (I am more likely to like sushi if spouse or friends like sushi,I am more likely to like the Knicks if I live in New York). The algorithm usesdistant supervision, semi-supervised data harvesting and vector space models toextract user attributes (e.g. spouse, education, location) and preferences(likes and dislikes) from text. The extracted propositions are then fed into aprobabilistic reasoner (we investigate both Markov Logic and Probabilistic SoftLogic). Our experiments show that probabilistic logical reasoning significantlyimproves the performance on attribute and relation extraction, and alsoachieves an F-score of 0.791 at predicting a users likes or dislikes,significantly better than two strong baselines.
arxiv-1411-2674 | The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation |  http://arxiv.org/abs/1411.2674  | author:Fangjian Guo, Charles Blundell, Hanna Wallach, Katherine Heller category:stat.ML cs.CL cs.LG cs.SI published:2014-11-11 summary:We present the Bayesian Echo Chamber, a new Bayesian generative model forsocial interaction data. By modeling the evolution of people's language usageover time, this model discovers latent influence relationships between them.Unlike previous work on inferring influence, which has primarily focused onsimple temporal dynamics evidenced via turn-taking behavior, our model capturesmore nuanced influence relationships, evidenced via linguistic accommodationpatterns in interaction content. The model, which is based on a discrete analogof the multivariate Hawkes process, permits a fully Bayesian inferencealgorithm. We validate our model's ability to discover latent influencepatterns using transcripts of arguments heard by the US Supreme Court and themovie "12 Angry Men." We showcase our model's capabilities by using it to inferlatent influence patterns from Federal Open Market Committee meetingtranscripts, demonstrating state-of-the-art performance at uncovering socialdynamics in group discussions.
arxiv-1411-2738 | word2vec Parameter Learning Explained |  http://arxiv.org/abs/1411.2738  | author:Xin Rong category:cs.CL published:2014-11-11 summary:The word2vec model and application by Mikolov et al. have attracted a greatamount of attention in recent two years. The vector representations of wordslearned by word2vec models have been shown to carry semantic meanings and areuseful in various NLP tasks. As an increasing number of researchers would liketo experiment with word2vec or similar techniques, I notice that there lacks amaterial that comprehensively explains the parameter learning process of wordembedding models in details, thus preventing researchers that are non-expertsin neural networks from understanding the working mechanism of such models. This note provides detailed derivations and explanations of the parameterupdate equations of the word2vec models, including the original continuousbag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimizationtechniques, including hierarchical softmax and negative sampling. Intuitiveinterpretations of the gradient equations are also provided alongsidemathematical derivations. In the appendix, a review on the basics of neuron networks andbackpropagation is provided. I also created an interactive demo, wevi, tofacilitate the intuitive understanding of the model.
arxiv-1411-2540 | Parameter estimation in spherical symmetry groups |  http://arxiv.org/abs/1411.2540  | author:Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Marc DeGraef, Jeffrey Simmons, Alfred Hero category:stat.ML published:2014-11-10 summary:This paper considers statistical estimation problems where the probabilitydistribution of the observed random variable is invariant with respect toactions of a finite topological group. It is shown that any such distributionmust satisfy a restricted finite mixture representation. When specialized tothe case of distributions over the sphere that are invariant to the actions ofa finite spherical symmetry group $\mathcal G$, a group-invariant extension ofthe Von Mises Fisher (VMF) distribution is obtained. The $\mathcal G$-invariantVMF is parameterized by location and scale parameters that specify thedistribution's mean orientation and its concentration about the mean,respectively. Using the restricted finite mixture representation theseparameters can be estimated using an Expectation Maximization (EM) maximumlikelihood (ML) estimation algorithm. This is illustrated for the problem ofmean crystal orientation estimation under the spherically symmetric groupassociated with the crystal form, e.g., cubic or octahedral or hexahedral.Simulations and experiments establish the advantages of the extended VMF EM-MLestimator for data acquired by Electron Backscatter Diffraction (EBSD)microscopy of a polycrystalline Nickel alloy sample.
arxiv-1411-2374 | Similarity Learning for High-Dimensional Sparse Data |  http://arxiv.org/abs/1411.2374  | author:Kuan Liu, Aurélien Bellet, Fei Sha category:cs.LG stat.ML published:2014-11-10 summary:A good measure of similarity between data points is crucial to many tasks inmachine learning. Similarity and metric learning methods learn such measuresautomatically from data, but they do not scale well respect to thedimensionality of the data. In this paper, we propose a method that can learnefficiently similarity measure from high-dimensional sparse data. The core ideais to parameterize the similarity measure as a convex combination of rank-onematrices with specific sparsity structures. The parameters are then optimizedwith an approximate Frank-Wolfe procedure to maximally satisfy relativesimilarity constraints on the training data. Our algorithm greedilyincorporates one pair of features at a time into the similarity measure,providing an efficient way to control the number of active features and thusreduce overfitting. It enjoys very appealing convergence guarantees and itstime and memory complexity depends on the sparsity of the data instead of thedimension of the feature space. Our experiments on real-world high-dimensionaldatasets demonstrate its potential for classification, dimensionality reductionand data exploration.
arxiv-1411-2328 | Modeling Word Relatedness in Latent Dirichlet Allocation |  http://arxiv.org/abs/1411.2328  | author:Xun Wang category:cs.CL cs.AI published:2014-11-10 summary:Standard LDA model suffers the problem that the topic assignment of each wordis independent and word correlation hence is neglected. To address thisproblem, in this paper, we propose a model called Word Related Latent DirichletAllocation (WR-LDA) by incorporating word correlation into LDA topic models.This leads to new capabilities that standard LDA model does not have such asestimating infrequently occurring words or multi-language topic modeling.Experimental results demonstrate the effectiveness of our model compared withstandard LDA.
arxiv-1411-2305 | Model-Parallel Inference for Big Topic Models |  http://arxiv.org/abs/1411.2305  | author:Xun Zheng, Jin Kyu Kim, Qirong Ho, Eric P. Xing category:cs.DC cs.LG stat.ML published:2014-11-10 summary:In real world industrial applications of topic modeling, the ability tocapture gigantic conceptual space by learning an ultra-high dimensional topicalrepresentation, i.e., the so-called "big model", is becoming the nextdesideratum after enthusiasms on "big data", especially for fine-graineddownstream tasks such as online advertising, where good performances areusually achieved by regression-based predictors built on millions if notbillions of input features. The conventional data-parallel approach fortraining gigantic topic models turns out to be rather inefficient in utilizingthe power of parallelism, due to the heavy dependency on a centralized image of"model". Big model size also poses another challenge on the storage, whereavailable model size is bounded by the smallest RAM of nodes. To address theseissues, we explore another type of parallelism, namely model-parallelism, whichenables training of disjoint blocks of a big topic model in parallel. Byintegrating data-parallelism with model-parallelism, we show that dependenciesbetween distributed elements can be handled seamlessly, achieving not onlyfaster convergence but also an ability to tackle significantly bigger modelsize. We describe an architecture for model-parallel inference of LDA, andpresent a variant of collapsed Gibbs sampling algorithm tailored for it.Experimental results demonstrate the ability of this system to handle topicmodeling with unprecedented amount of 200 billion model variables only on alow-end cluster with very limited computational resources and bandwidth.
arxiv-1411-2405 | Sparse Estimation with Generalized Beta Mixture and the Horseshoe Prior |  http://arxiv.org/abs/1411.2405  | author:Zahra Sabetsarvestani, Hamidreza Amindavar category:cs.IT math.IT stat.ML published:2014-11-10 summary:In this paper, the use of the Generalized Beta Mixture (GBM) and Horseshoedistributions as priors in the Bayesian Compressive Sensing framework isproposed. The distributions are considered in a two-layer hierarchical model,making the corresponding inference problem amenable to Expectation Maximization(EM). We present an explicit, algebraic EM-update rule for the models, yieldingtwo fast and experimentally validated algorithms for signal recovery.Experimental results show that our algorithms outperform state-of-the-artmethods on a wide range of sparsity levels and amplitudes in terms ofreconstruction accuracy, convergence rate and sparsity. The largest improvementcan be observed for sparse signals with high amplitudes.
arxiv-1411-2664 | Preserving Statistical Validity in Adaptive Data Analysis |  http://arxiv.org/abs/1411.2664  | author:Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, Aaron Roth category:cs.LG cs.DS published:2014-11-10 summary:A great deal of effort has been devoted to reducing the risk of spuriousscientific discoveries, from the use of sophisticated validation techniques, todeep statistical methods for controlling the false discovery rate in multiplehypothesis testing. However, there is a fundamental disconnect between thetheoretical results and the practice of data analysis: the theory ofstatistical inference assumes a fixed collection of hypotheses to be tested, orlearning algorithms to be applied, selected non-adaptively before the data aregathered, whereas in practice data is shared and reused with hypotheses and newanalyses being generated on the basis of data exploration and the outcomes ofprevious analyses. In this work we initiate a principled study of how to guarantee the validityof statistical inference in adaptive data analysis. As an instance of thisproblem, we propose and investigate the question of estimating the expectationsof $m$ adaptively chosen functions on an unknown distribution given $n$ randomsamples. We show that, surprisingly, there is a way to estimate an exponential in $n$number of expectations accurately even if the functions are chosen adaptively.This gives an exponential improvement over standard empirical estimators thatare limited to a linear number of estimates. Our result follows from a generaltechnique that counter-intuitively involves actively perturbing andcoordinating the estimates, using techniques developed for privacypreservation. We give additional applications of this technique to ourquestion.
arxiv-1411-2337 | Multi-Task Metric Learning on Network Data |  http://arxiv.org/abs/1411.2337  | author:Chen Fang, Daniel N. Rockmore category:stat.ML cs.LG published:2014-11-10 summary:Multi-task learning (MTL) improves prediction performance in differentcontexts by learning models jointly on multiple different, but related tasks.Network data, which are a priori data with a rich relational structure, providean important context for applying MTL. In particular, the explicit relationalstructure implies that network data is not i.i.d. data. Network data also oftencomes with significant metadata (i.e., attributes) associated with each entity(node). Moreover, due to the diversity and variation in network data (e.g.,multi-relational links or multi-category entities), various tasks can beperformed and often a rich correlation exists between them. Learning algorithmsshould exploit all of these additional sources of information for betterperformance. In this work we take a metric-learning point of view for the MTLproblem in the network context. Our approach builds on structure preservingmetric learning (SPML). In particular SPML learns a Mahalanobis distance metricfor node attributes using network structure as supervision, so that the learneddistance function encodes the structure and can be used to predict linkpatterns from attributes. SPML is described for single-task learning on singlenetwork. Herein, we propose a multi-task version of SPML, abbreviated asMT-SPML, which is able to learn across multiple related tasks on multiplenetworks via shared intermediate parametrization. MT-SPML learns a specificmetric for each task and a common metric for all tasks. The task correlation iscarried through the common metric and the individual metrics encode taskspecific information. When combined together, they are structure-preservingwith respect to individual tasks. MT-SPML works on general networks, thus issuitable for a wide variety of problems. In experiments, we challenge MT-SPMLon two real-word problems, where MT-SPML achieves significant improvement.
arxiv-1411-2335 | An Improved Tracking using IMU and Vision Fusion for Mobile Augmented Reality Applications |  http://arxiv.org/abs/1411.2335  | author:Kriti Kumar, Ashley Varghese, Pavan K Reddy, N Narendra, Prashanth Swamy, M Girish Chandra, P Balamuralidhar category:cs.CV published:2014-11-10 summary:Mobile Augmented Reality (MAR) is becoming an important cyber-physical systemapplication given the ubiquitous availability of mobile phones. With the needto operate in unprepared environments, accurate and robust registration andtracking has become an important research problem to solve. In fact, when MARis used for tele-interactive applications involving large distances, say froman accident site to insurance office, tracking at both the ends is desirableand further it is essential to appropriately fuse inertial and vision sensorsdata. In this paper, we present results and discuss some insights gained inmarker-less tracking during the development of a prototype pertaining to anexample use case related to breakdown or damage assessment of a vehicle. Thenovelty of this paper is in bringing together different components and moduleswith appropriate enhancements towards a complete working system.
arxiv-1411-2539 | Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models |  http://arxiv.org/abs/1411.2539  | author:Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel category:cs.LG cs.CL cs.CV published:2014-11-10 summary:Inspired by recent advances in multimodal learning and machine translation,we introduce an encoder-decoder pipeline that learns (a): a multimodal jointembedding space with images and text and (b): a novel language model fordecoding distributed representations from our space. Our pipeline effectivelyunifies joint image-text embedding models with multimodal neural languagemodels. We introduce the structure-content neural language model thatdisentangles the structure of a sentence to its content, conditioned onrepresentations produced by the encoder. The encoder allows one to rank imagesand sentences while the decoder can generate novel descriptions from scratch.Using LSTM to encode sentences, we match the state-of-the-art performance onFlickr8K and Flickr30K without using object detections. We also set new bestresults when using the 19-layer Oxford convolutional network. Furthermore weshow that with linear encoders, the learned embedding space captures multimodalregularities in terms of vector space arithmetic e.g. *image of a blue car* -"blue" + "red" is near images of red cars. Sample captions generated for 800images are made available for comparison.
arxiv-1411-2581 | Deep Exponential Families |  http://arxiv.org/abs/1411.2581  | author:Rajesh Ranganath, Linpeng Tang, Laurent Charlin, David M. Blei category:stat.ML cs.LG published:2014-11-10 summary:We describe \textit{deep exponential families} (DEFs), a class of latentvariable models that are inspired by the hidden structures used in deep neuralnetworks. DEFs capture a hierarchy of dependencies between latent variables,and are easily generalized to many settings through exponential families. Weperform inference using recent "black box" variational inference techniques. Wethen evaluate various DEFs on text and combine multiple DEFs into a model forpairwise recommendation data. In an extensive study, we show that going beyondone layer improves predictions for DEFs. We demonstrate that DEFs findinteresting exploratory structure in large data sets, and give betterpredictive performance than state-of-the-art models.
arxiv-1411-2635 | A chain rule for the expected suprema of Gaussian processes |  http://arxiv.org/abs/1411.2635  | author:Andreas Maurer category:cs.LG published:2014-11-10 summary:The expected supremum of a Gaussian process indexed by the image of an indexset under a function class is bounded in terms of separate properties of theindex set and the function class. The bound is relevant to the estimation ofnonlinear transformations or the analysis of learning algorithms wheneverhypotheses are chosen from composite classes, as is the case for multi-layermodels.
arxiv-1411-2645 | Non-crossing dependencies: least effort, not grammar |  http://arxiv.org/abs/1411.2645  | author:Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph published:2014-11-10 summary:The use of null hypotheses (in a statistical sense) is common in hardsciences but not in theoretical linguistics. Here the null hypothesis that thelow frequency of syntactic dependency crossings is expected by an arbitraryordering of words is rejected. It is shown that this would require stardependency structures, which are both unrealistic and too restrictive. Thehypothesis of the limited resources of the human brain is revisited. Strongernull hypotheses taking into account actual dependency lengths for thelikelihood of crossings are presented. Those hypotheses suggests that crossingsare likely to reduce when dependencies are shortened. A hypothesis based onpressure to reduce dependency lengths is more parsimonious than a principle ofminimization of crossings or a grammatical ban that is totally dissociated fromthe general and non-linguistic principle of economy.
arxiv-1411-2331 | N$^3$LARS: Minimum Redundancy Maximum Relevance Feature Selection for Large and High-dimensional Data |  http://arxiv.org/abs/1411.2331  | author:Makoto Yamada, Avishek Saha, Hua Ouyang, Dawei Yin, Yi Chang category:stat.ML cs.LG published:2014-11-10 summary:We propose a feature selection method that finds non-redundant features froma large and high-dimensional data in nonlinear way. Specifically, we propose anonlinear extension of the non-negative least-angle regression (LARS) calledN${}^3$LARS, where the similarity between input and output is measured throughthe normalized version of the Hilbert-Schmidt Independence Criterion (HSIC). Anadvantage of N${}^3$LARS is that it can easily incorporate with map-reduceframeworks such as Hadoop and Spark. Thus, with the help of distributedcomputing, a set of features can be efficiently selected from a large andhigh-dimensional data. Moreover, N${}^3$LARS is a convex method and can find aglobal optimum solution. The effectiveness of the proposed method is firstdemonstrated through feature selection experiments for classification andregression with small and high-dimensional datasets. Finally, we evaluate ourproposed method over a large and high-dimensional biology dataset.
arxiv-1411-2316 | Zero-Aliasing Correlation Filters for Object Recognition |  http://arxiv.org/abs/1411.2316  | author:Joseph A. Fernandez, Vishnu Naresh Boddeti, Andres Rodriguez, B. V. K. Vijaya Kumar category:cs.CV stat.ML published:2014-11-10 summary:Correlation filters (CFs) are a class of classifiers that are attractive forobject localization and tracking applications. Traditionally, CFs have beendesigned in the frequency domain using the discrete Fourier transform (DFT),where correlation is efficiently implemented. However, existing CF designs donot account for the fact that the multiplication of two DFTs in the frequencydomain corresponds to a circular correlation in the time/spatial domain.Because this was previously unaccounted for, prior CF designs are not trulyoptimal, as their optimization criteria do not accurately quantify theiroptimization intention. In this paper, we introduce new zero-aliasingconstraints that completely eliminate this aliasing problem by ensuring thatthe optimization criterion for a given CF corresponds to a linear correlationrather than a circular correlation. This means that previous CF designs can besignificantly improved by this reformulation. We demonstrate the benefits ofthis new CF design approach with several important CFs. We present experimentalresults on diverse data sets and present solutions to the computationalchallenges associated with computing these CFs. Code for the CFs described inthis paper and their respective zero-aliasing versions is available athttp://vishnu.boddeti.net/projects/correlation-filters.html
arxiv-1411-2276 | Trade-Offs in Exploiting Body Morphology for Control: from Simple Bodies and Model-Based Control to Complex Bodies with Model-Free Distributed Control Schemes |  http://arxiv.org/abs/1411.2276  | author:Matej Hoffmann, Vincent C. Müller category:cs.RO cs.NE cs.SY published:2014-11-09 summary:Tailoring the design of robot bodies for control purposes is implicitlyperformed by engineers, however, a methodology or set of tools is largelyabsent and optimization of morphology (shape, material properties of robotbodies, etc.) is lagging behind the development of controllers. This has becomeeven more prominent with the advent of compliant, deformable or "soft" bodies.These carry substantial potential regarding their exploitation forcontrol---sometimes referred to as "morphological computation" in the sense ofoffloading computation needed for control to the body. Here, we will argue infavor of a dynamical systems rather than computational perspective on theproblem. Then, we will look at the pros and cons of simple vs. complex bodies,critically reviewing the attractive notion of "soft" bodies automaticallytaking over control tasks. We will address another key dimension of the designspace---whether model-based control should be used and to what extent it isfeasible to develop faithful models for different morphologies.
arxiv-1411-2584 | Applications of sampling Kantorovich operators to thermographic images for seismic engineering |  http://arxiv.org/abs/1411.2584  | author:Danilo Costarelli, Federico Cluni, Anna Maria Minotti, Gianluca Vinti category:cs.CV math.NA published:2014-11-09 summary:In this paper, we present some applications of the multivariate samplingKantorovich operators $S_w$ to seismic engineering. The mathematical theory ofthese operators, both in the space of continuous functions and in Orliczspaces, show how it is possible to approximate/reconstruct multivariatesignals, such as images. In particular, to obtain applications forthermographic images a mathematical algorithm is developed using MATLAB andmatrix calculus. The setting of Orlicz spaces is important since allow us toreconstruct not necessarily continuous signals by means of $S_w$. Thereconstruction of thermographic images of buildings by our sampling Kantorovichalgorithm allow us to obtain models for the simulation of the behavior ofstructures under seismic action. We analyze a real world case study in term ofstructural analysis and we compare the behavior of the building under seismicaction using various models.
arxiv-1411-2214 | Abnormal Object Recognition: A Comprehensive Study |  http://arxiv.org/abs/1411.2214  | author:Babak Saleh, Ali Farhadi, Ahmed Elgammal category:cs.CV published:2014-11-09 summary:When describing images, humans tend not to talk about the obvious, but rathermention what they find interesting. We argue that abnormalities and deviationsfrom typicalities are among the most important components that form what isworth mentioning. In this paper we introduce the abnormality detection as arecognition problem and show how to model typicalities and, consequently,meaningful deviations from prototypical properties of categories. Our model canrecognize abnormalities and report the main reasons of any recognizedabnormality. We introduce the abnormality detection dataset and showinteresting results on how to reason about abnormalities.
arxiv-1411-2173 | Stacked Quantizers for Compositional Vector Compression |  http://arxiv.org/abs/1411.2173  | author:Julieta Martinez, Holger H. Hoos, James J. Little category:cs.CV published:2014-11-08 summary:Recently, Babenko and Lempitsky introduced Additive Quantization (AQ), ageneralization of Product Quantization (PQ) where a non-independent set ofcodebooks is used to compress vectors into small binary codes. Unfortunately,under this scheme encoding cannot be done independently in each codebook, andoptimal encoding is an NP-hard problem. In this paper, we observe that PQ andAQ are both compositional quantizers that lie on the extremes of the codebookdependence-independence assumption, and explore an intermediate approach thatexploits a hierarchical structure in the codebooks. This results in a methodthat achieves quantization error on par with or lower than AQ, while beingseveral orders of magnitude faster. We perform a complexity analysis of PQ, AQand our method, and evaluate our approach on standard benchmarks of SIFT andGIST descriptors, as well as on new datasets of features obtained fromstate-of-the-art convolutional neural networks.
arxiv-1411-2158 | Covariate-assisted spectral clustering |  http://arxiv.org/abs/1411.2158  | author:Norbert Binkiewicz, Joshua T. Vogelstein, Karl Rohe category:stat.ML cs.LG math.ST stat.ME stat.TH published:2014-11-08 summary:Biological and social systems consist of myriad interacting units. Theinteractions can be represented in the form of a graph or network. Measurementsof these graphs can reveal the underlying structure of these interactions,which provides insight into the systems that generated the graphs. Moreover, inapplications such as connectomics, social networks, and genomics, graph dataare accompanied by contextualizing measures on each node. We utilize these nodecovariates to help uncover latent communities in a graph, using a modificationof spectral clustering. Statistical guarantees are provided under a jointmixture model that we call the node-contextualized stochastic blockmodel,including a bound on the mis-clustering rate. For most simulated conditions,covariate-assisted spectral clustering yields results superior to regularizedspectral clustering without node covariates and to an adaptation of canonicalcorrelation analysis. We apply our clustering method to large brain graphsderived from diffusion MRI data, using the node locations or neurologicalregion membership as covariates. In both cases, covariate-assisted spectralclustering yields clusters that are easier to interpret neurologically.
arxiv-1411-2066 | Learning Theory for Distribution Regression |  http://arxiv.org/abs/1411.2066  | author:Zoltan Szabo, Bharath Sriperumbudur, Barnabas Poczos, Arthur Gretton category:math.ST cs.LG math.FA stat.ML stat.TH G.3; I.2.6 published:2014-11-08 summary:We focus on the distribution regression problem: regressing to vector-valuedoutputs from probability measures. Many important machine learning andstatistical tasks fit into this framework, including multi-instance learning,and point estimation problems without analytical solution (such ashyperparameter or entropy estimation). Despite the large number of availableheuristics in the literature, the inherent two-stage sampled nature of theproblem makes the theoretical analysis quite challenging, since in practiceonly samples from sampled distributions are observable, and the estimates haveto rely on similarities computed between sets of points. To the best of ourknowledge, the only existing technique with consistency guarantees fordistribution regression requires kernel density estimation as an intermediatestep (which often performs poorly in practice), and the domain of thedistributions to be compact Euclidean. In this paper, we study a simple,analytically computable, ridge regression-based alternative to distributionregression, where we embed the distributions to a reproducing kernel Hilbertspace, and learn the regressor from the embeddings to the outputs. Our maincontribution is to prove that this scheme is consistent in the two-stagesampled setup under mild conditions (on separable topological domains enrichedwith kernels): we present an exact computational-statistical efficiencytradeoff analysis showing that the studied estimator is able to match theone-stage sampled minimax optimal rate. This result answers a 16-year-old openquestion, establishing the consistency of the classical set kernel [Haussler,1999; Gaertner et. al, 2002] in regression. We also cover consistency for morerecent kernels on distributions, including those due to [Christmann andSteinwart, 2010].
arxiv-1411-2153 | Evolving intraday foreign exchange trading strategies utilizing multiple instruments price series |  http://arxiv.org/abs/1411.2153  | author:Simone Cirillo, Stefan Lloyd, Peter Nordin category:cs.NE q-fin.TR I.2.2 published:2014-11-08 summary:We propose a Genetic Programming architecture for the generation of foreignexchange trading strategies. The system's principal features are the evolutionof free-form strategies which do not rely on any prior models and theutilization of price series from multiple instruments as input data. Thislatter feature constitutes an innovation with respect to previous worksdocumented in literature. In this article we utilize Open, High, Low, Close bardata at a 5 minutes frequency for the AUD.USD, EUR.USD, GBP.USD and USD.JPYcurrency pairs. We will test the implementation analyzing the in-sample andout-of-sample performance of strategies for trading the USD.JPY obtained acrossmultiple algorithm runs. We will also evaluate the differences betweenstrategies selected according to two different criteria: one relies on thefitness obtained on the training set only, the second one makes use of anadditional validation dataset. Strategy activity and trade accuracy areremarkably stable between in and out of sample results. From a profitabilityaspect, the two criteria both result in strategies successful on out-of-sampledata but exhibiting different characteristics. The overall best performingout-of-sample strategy achieves a yearly return of 19%.
arxiv-1411-2141 | Fast Mesh-Based Medical Image Registration |  http://arxiv.org/abs/1411.2141  | author:Ahmadreza Baghaie, Zeyun Yu, Roshan M. D'souza category:cs.CV published:2014-11-08 summary:In this paper a fast triangular mesh based registration method is proposed.Having Template and Reference images as inputs, the template image istriangulated using a content adaptive mesh generation algorithm. Consideringthe pixel values at mesh nodes, interpolated using spline interpolation methodfor both of the images, the energy functional needed for image registration isminimized. The minimization process was achieved using a mesh baseddiscretization of the distance measure and regularization term which resultedin a sparse system of linear equations, which due to the smaller size incomparison to the pixel-wise registration method, can be solved directly. MeanSquared Di?erence (MSD) is used as a metric for evaluating the results. Usingthe mesh based technique, higher speed was achieved compared to pixel-basedcurvature registration technique with fast DCT solver. The implementation wasdone in MATLAB without any speci?c optimization. Higher speeds can be achievedusing C/C++ implementations.
arxiv-1411-2090 | Parallax Effect Free Mosaicing of Underwater Video Sequence Based on Texture Features |  http://arxiv.org/abs/1411.2090  | author:Nagaraja S., Prabhakar C. J., Praveen Kumar P. U category:cs.CV published:2014-11-08 summary:In this paper, we present feature-based technique for construction of mosaicimage from underwater video sequence, which suffers from parallax distortiondue to propagation properties of light in the underwater environment. The mostof the available mosaic tools and underwater image mosaicing techniques yieldsfinal result with some artifacts such as blurring, ghosting and seam due topresence of parallax in the input images. The removal of parallax from inputimages may not reduce its effects instead it must be corrected in successivesteps of mosaicing. Thus, our approach minimizes the parallax effects byadopting an efficient local alignment technique after global registration. Weextract texture features using Centre Symmetric Local Binary Pattern (CS-LBP)descriptor in order to find feature correspondences, which are used further forestimation of homography through RANSAC. In order to increase the accuracy ofglobal registration, we perform preprocessing such as colour alignment betweentwo selected frames based on colour distribution adjustment. Because ofexistence of 100% overlap in consecutive frames of underwater video, we selectframes with minimum overlap based on mutual offset in order to reduce thecomputation cost during mosaicing. Our approach minimizes the parallax effectsconsiderably in final mosaic constructed using our own underwater videosequences.
arxiv-1504-03315 | A Novel Approach to Develop a New Hybrid Technique for Trademark Image Retrieval |  http://arxiv.org/abs/1504.03315  | author:Saurabh Agarwal, Punit Kumar Johari category:cs.CV published:2014-11-08 summary:Trademark Image Retrieval is playing a vital role as a part of CBIR System.Trademark is of great significance because it carries the status value of anycompany. To retrieve such a fake or copied trademark we design a retrievalsystem which is based on hybrid techniques. It contains a mixture of twodifferent feature vector which combined together to give a suitable retrievalsystem. In the proposed system we extract the corner feature which is appliedon an edge pixel image. This feature is used to extract the relevant image andto more purify the result we apply other feature which is the invariant momentfeature. From the experimental result we conclude that the system is 85 percentefficient.
arxiv-1411-1990 | A totally unimodular view of structured sparsity |  http://arxiv.org/abs/1411.1990  | author:Marwa El Halabi, Volkan Cevher category:cs.LG stat.ML published:2014-11-07 summary:This paper describes a simple framework for structured sparse recovery basedon convex optimization. We show that many structured sparsity models can benaturally represented by linear matrix inequalities on the support of theunknown parameters, where the constraint matrix has a totally unimodular (TU)structure. For such structured models, tight convex relaxations can be obtainedin polynomial time via linear programming. Our modeling framework unifies theprevalent structured sparsity norms in the literature, introduces newinteresting ones, and renders their tightness and tractability argumentstransparent.
arxiv-1411-1810 | Multicanonical Stochastic Variational Inference |  http://arxiv.org/abs/1411.1810  | author:Stephan Mandt, James McInerney, Farhan Abrol, Rajesh Ranganath, David Blei category:stat.ML cs.LG published:2014-11-07 summary:Stochastic variational inference (SVI) enables approximate posteriorinference with large data sets for otherwise intractable models, but like allvariational inference algorithms it suffers from local optima. Deterministicannealing, which we formulate here for the generic class of conditionallyconjugate exponential family models, uses a temperature parameter thatdeterministically deforms the objective, and reduce this parameter over thecourse of the optimization to recover the original variational set-up. Awell-known drawback in annealing approaches is the choice of the annealingschedule. We therefore introduce multicanonical variational inference (MVI), avariational algorithm that operates at several annealing temperaturessimultaneously. This algorithm gives us adaptive annealing schedules. Comparedto the traditional SVI algorithm, both approaches find improved predictivelikelihoods on held-out data, with MVI being close to the best-tuned annealingschedule.
arxiv-1411-1805 | Faithful Variable Screening for High-Dimensional Convex Regression |  http://arxiv.org/abs/1411.1805  | author:Min Xu, Minhua Chen, John Lafferty category:math.ST stat.ML stat.TH published:2014-11-07 summary:We study the problem of variable selection in convex nonparametricregression. Under the assumption that the true regression function is convexand sparse, we develop a screening procedure to select a subset of variablesthat contains the relevant variables. Our approach is a two-stage quadraticprogramming method that estimates a sum of one-dimensional convex functions,followed by one-dimensional concave regression fits on the residuals. Incontrast to previous methods for sparse additive models, the optimization isfinite dimensional and requires no tuning parameters for smoothness. Underappropriate assumptions, we prove that the procedure is faithful in thepopulation setting, yielding no false negatives. We give a finite samplestatistical analysis, and introduce algorithms for efficiently carrying out therequired quadratic programs. The approach leads to computational andstatistical advantages over fitting a full model, and provides an effective,practical approach to variable screening in convex regression.
arxiv-1411-2021 | Partitioning Well-Clustered Graphs: Spectral Clustering Works! |  http://arxiv.org/abs/1411.2021  | author:Richard Peng, He Sun, Luca Zanetti category:cs.DS cs.LG published:2014-11-07 summary:In this paper we study variants of the widely used spectral clustering thatpartitions a graph into k clusters by (1) embedding the vertices of a graphinto a low-dimensional space using the bottom eigenvectors of the Laplacianmatrix, and (2) grouping the embedded points into k clusters via k-meansalgorithms. We show that, for a wide class of graphs, spectral clustering givesa good approximation of the optimal clustering. While this approach wasproposed in the early 1990s and has comprehensive applications, prior to ourwork similar results were known only for graphs generated from stochasticmodels. We also give a nearly-linear time algorithm for partitioning well-clusteredgraphs based on heat kernel embeddings and approximate nearest neighbor datastructures.
arxiv-1411-1997 | Differential gene co-expression networks via Bayesian biclustering models |  http://arxiv.org/abs/1411.1997  | author:Chuan Gao, Shiwen Zhao, Ian C. McDowell, Christopher D. Brown, Barbara E. Engelhardt category:stat.ME q-bio.GN q-bio.MN stat.ML published:2014-11-07 summary:Identifying latent structure in large data matrices is essential forexploring biological processes. Here, we consider recovering gene co-expressionnetworks from gene expression data, where each network encodes relationshipsbetween genes that are locally co-regulated by shared biological mechanisms. Todo this, we develop a Bayesian statistical model for biclustering to infersubsets of co-regulated genes whose covariation may be observed in only asubset of the samples. Our biclustering method, BicMix, has desirableproperties, including allowing overcomplete representations of the data,computational tractability, and jointly modeling unknown confounders andbiological signals. Compared with related biclustering methods, BicMix recoverslatent structure with higher precision across diverse simulation scenarios.Further, we develop a method to recover gene co-expression networks from theestimated sparse biclustering matrices. We apply BicMix to breast cancer geneexpression data and recover a gene co-expression network that is differentialacross ER+ and ER- samples.
arxiv-1411-2057 | Online Collaborative-Filtering on Graphs |  http://arxiv.org/abs/1411.2057  | author:Siddhartha Banerjee, Sujay Sanghavi, Sanjay Shakkottai category:cs.LG published:2014-11-07 summary:A common phenomena in modern recommendation systems is the use of feedbackfrom one user to infer the `value' of an item to other users. This results inan exploration vs. exploitation trade-off, in which items of possibly low valuehave to be presented to users in order to ascertain their value. Existingapproaches to solving this problem focus on the case where the number of itemsare small, or admit some underlying structure -- it is unclear, however, ifgood recommendation is possible when dealing with content-rich settings withunstructured content. We consider this problem under a simple natural model, wherein the number ofitems and the number of item-views are of the same order, and an `access-graph'constrains which user is allowed to see which item. Our main insight is thatthe presence of the access-graph in fact makes good recommendation possible --however this requires the exploration policy to be designed to take advantageof the access-graph. Our results demonstrate the importance of `serendipity' inexploration, and how higher graph-expansion translates to a higher quality ofrecommendations; it also suggests a reason why in some settings, simplepolicies like Twitter's `Latest-First' policy achieve a good performance. From a technical perspective, our model presents a way to studyexploration-exploitation tradeoffs in settings where the number of `trials' and`strategies' are large (potentially infinite), and more importantly, of thesame order. Our algorithms admit competitive-ratio guarantees which hold forthe worst-case user, under both finite-population and infinite-horizonsettings, and are parametrized in terms of properties of the underlying graph.Conversely, we also demonstrate that improperly-designed policies can be highlysub-optimal, and that in many settings, our results are order-wise optimal.
arxiv-1411-2045 | Multivariate f-Divergence Estimation With Confidence |  http://arxiv.org/abs/1411.2045  | author:Kevin R. Moon, Alfred O. Hero III category:cs.IT math.IT stat.ML published:2014-11-07 summary:The problem of f-divergence estimation is important in the fields of machinelearning, information theory, and statistics. While several nonparametricdivergence estimators exist, relatively few have known convergence properties.In particular, even for those estimators whose MSE convergence rates are known,the asymptotic distributions are unknown. We establish the asymptotic normalityof a recently proposed ensemble estimator of f-divergence between twodistributions from a finite number of samples. This estimator has MSEconvergence rate of O(1/T), is simple to implement, and performs well in highdimensions. This theory enables us to perform divergence-based inference taskssuch as testing equality of pairs of distributions based on empirical samples.We experimentally validate our theoretical results and, as an illustration, usethem to empirically bound the best achievable classification error.
arxiv-1411-1804 | Beta Process Non-negative Matrix Factorization with Stochastic Structured Mean-Field Variational Inference |  http://arxiv.org/abs/1411.1804  | author:Dawen Liang, Matthew D. Hoffman category:stat.ML cs.LG published:2014-11-07 summary:Beta process is the standard nonparametric Bayesian prior for latent factormodel. In this paper, we derive a structured mean-field variational inferencealgorithm for a beta process non-negative matrix factorization (NMF) model withPoisson likelihood. Unlike the linear Gaussian model, which is well-studied inthe nonparametric Bayesian literature, NMF model with beta process prior doesnot enjoy the conjugacy. We leverage the recently developed stochasticstructured mean-field variational inference to relax the conjugacy constraintand restore the dependencies among the latent variables in the approximatingvariational distribution. Preliminary results on both synthetic and realexamples demonstrate that the proposed inference algorithm can reasonablyrecover the hidden structure of the data.
arxiv-1411-2003 | Efficient Estimation of Mutual Information for Strongly Dependent Variables |  http://arxiv.org/abs/1411.2003  | author:Shuyang Gao, Greg Ver Steeg, Aram Galstyan category:cs.IT math.IT stat.ML published:2014-11-07 summary:We demonstrate that a popular class of nonparametric mutual information (MI)estimators based on k-nearest-neighbor graphs requires number of samples thatscales exponentially with the true MI. Consequently, accurate estimation of MIbetween two strongly dependent variables is possible only for prohibitivelylarge sample size. This important yet overlooked shortcoming of the existingestimators is due to their implicit reliance on local uniformity of theunderlying joint distribution. We introduce a new estimator that is robust tolocal non-uniformity, works well with limited data, and is able to capturerelationship strengths over many orders of magnitude. We demonstrate thesuperior performance of the proposed estimator on both synthetic and real-worlddata.
arxiv-1411-1999 | Azhary: An Arabic Lexical Ontology |  http://arxiv.org/abs/1411.1999  | author:Hossam Ishkewy, Hany Harb, Hassan Farahat category:cs.AI cs.CL published:2014-11-07 summary:Arabic language is the most spoken languages in the Semitic languages group,and one of the most common languages in the world spoken by more than 422million. It is also of paramount importance to Muslims, it is a sacred languageof the Islamic Holly Book (Quran) and prayer (and other acts of worship) inIslam is performed only by mastering some of Arabic words. Arabic is also amajor ritual language of a number of Christian churches in the Arab world andit is also used in writing several intellectual and religious Jewish books inthe Middle Ages. Despite this, there is no semantic Arabic lexicon whichresearchers can depend on. In this paper we introduce Azhary as a lexicalontology for the Arabic language. It groups Arabic words into sets of synonymscalled synsets, and records a number of relationships between words such assynonym, antonym, hypernym, hyponym, meronym, holonym and associationrelations. The ontology contains 26,195 words organized in 13,328 synsets. Ithas been developed and contrasted against AWN which is the most commonavailable Arabic lexical ontology.
arxiv-1411-2005 | Scalable Variational Gaussian Process Classification |  http://arxiv.org/abs/1411.2005  | author:James Hensman, Alex Matthews, Zoubin Ghahramani category:stat.ML published:2014-11-07 summary:Gaussian process classification is a popular method with a number ofappealing properties. We show how to scale the model within a variationalinducing point framework, outperforming the state of the art on benchmarkdatasets. Importantly, the variational formulation can be exploited to allowclassification in problems with millions of data points, as we demonstrate inexperiments.
arxiv-1411-1490 | Efficient Representations for Life-Long Learning and Autoencoding |  http://arxiv.org/abs/1411.1490  | author:Maria-Florina Balcan, Avrim Blum, Santosh Vempala category:cs.LG published:2014-11-06 summary:It has been a long-standing goal in machine learning, as well as in AI moregenerally, to develop life-long learning systems that learn many differenttasks over time, and reuse insights from tasks learned, "learning to learn" asthey do so. In this work we pose and provide efficient algorithms for severalnatural theoretical formulations of this goal. Specifically, we consider theproblem of learning many different target functions over time, that sharecertain commonalities that are initially unknown to the learning algorithm. Ouraim is to learn new internal representations as the algorithm learns new targetfunctions, that capture this commonality and allow subsequent learning tasks tobe solved more efficiently and from less data. We develop efficient algorithmsfor two very different kinds of commonalities that target functions mightshare: one based on learning common low-dimensional and unions oflow-dimensional subspaces and one based on learning nonlinear Booleancombinations of features. Our algorithms for learning Boolean featurecombinations additionally have a dual interpretation, and can be viewed asgiving an efficient procedure for constructing near-optimal sparse Booleanautoencoders under a natural "anchor-set" assumption.
arxiv-1411-1690 | Sublinear-Time Approximate MCMC Transitions for Probabilistic Programs |  http://arxiv.org/abs/1411.1690  | author:Yutian Chen, Vikash Mansinghka, Zoubin Ghahramani category:stat.ML published:2014-11-06 summary:Probabilistic programming languages can simplify the development of machinelearning techniques, but only if inference is sufficiently scalable.Unfortunately, Bayesian parameter estimation for highly coupled models such asregressions and state-space models still scales poorly; each MCMC transitiontakes linear time in the number of observations. This paper describes asublinear-time algorithm for making Metropolis-Hastings (MH) updates to latentvariables in probabilistic programs. The approach generalizes recentlyintroduced approximate MH techniques: instead of subsampling data items assumedto be independent, it subsamples edges in a dynamically constructed graphicalmodel. It thus applies to a broader class of problems and interoperates withother general-purpose inference techniques. Empirical results, includingconfirmation of sublinear per-transition scaling, are presented for Bayesianlogistic regression, nonlinear classification via joint Dirichlet processmixtures, and parameter estimation for stochastic volatility models (with stateestimation via particle MCMC). All three applications use the sameimplementation, and each requires under 20 lines of probabilistic code.
arxiv-1411-1488 | Analyzing Tensor Power Method Dynamics in Overcomplete Regime |  http://arxiv.org/abs/1411.1488  | author:Anima Anandkumar, Rong Ge, Majid Janzamin category:cs.LG stat.ML published:2014-11-06 summary:We present a novel analysis of the dynamics of tensor power iterations in theovercomplete regime where the tensor CP rank is larger than the inputdimension. Finding the CP decomposition of an overcomplete tensor is NP-hard ingeneral. We consider the case where the tensor components are randomly drawn,and show that the simple power iteration recovers the components with boundederror under mild initialization conditions. We apply our analysis tounsupervised learning of latent variable models, such as multi-view mixturemodels and spherical Gaussian mixtures. Given the third order moment tensor, welearn the parameters using tensor power iterations. We prove it can correctlylearn the model parameters when the number of hidden components $k$ is muchlarger than the data dimension $d$, up to $k = o(d^{1.5})$. We initialize thepower iterations with data samples and prove its success under mild conditionson the signal-to-noise ratio of the samples. Our analysis significantly expandsthe class of latent variable models where spectral methods are applicable. Ouranalysis also deals with noise in the input tensor leading to sample complexityresult in the application to learning latent variable models.
arxiv-1411-1752 | Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets |  http://arxiv.org/abs/1411.1752  | author:Adarsh Prasad, Stefanie Jegelka, Dhruv Batra category:cs.LG cs.AI cs.CV cs.IR stat.ML published:2014-11-06 summary:To cope with the high level of ambiguity faced in domains such as ComputerVision or Natural Language processing, robust prediction methods often searchfor a diverse set of high-quality candidate solutions or proposals. Instructured prediction problems, this becomes a daunting task, as the solutionspace (image labelings, sentence parses, etc.) is exponentially large. We studygreedy algorithms for finding a diverse subset of solutions instructured-output spaces by drawing new connections between submodularfunctions over combinatorial item sets and High-Order Potentials (HOPs) studiedfor graphical models. Specifically, we show via examples that when marginalgains of submodular diversity functions allow structured representations, thisenables efficient (sub-linear time) approximate maximization by reducing thegreedy augmentation step to inference in a factor graph with appropriatelyconstructed HOPs. We discuss benefits, tradeoffs, and show that ourconstructions lead to significantly better proposals.
arxiv-1411-1623 | A Hybrid Recurrent Neural Network For Music Transcription |  http://arxiv.org/abs/1411.1623  | author:Siddharth Sigtia, Emmanouil Benetos, Nicolas Boulanger-Lewandowski, Tillman Weyde, Artur S. d'Avila Garcez, Simon Dixon category:cs.LG published:2014-11-06 summary:We investigate the problem of incorporating higher-level symbolic score-likeinformation into Automatic Music Transcription (AMT) systems to improve theirperformance. We use recurrent neural networks (RNNs) and their variants asmusic language models (MLMs) and present a generative architecture forcombining these models with predictions from a frame level acoustic classifier.We also compare different neural network architectures for acoustic modeling.The proposed model computes a distribution over possible output sequences giventhe acoustic input signal and we present an algorithm for performing a globalsearch for good candidate transcriptions. The performance of the proposed modelis evaluated on piano music from the MAPS dataset and we observe that theproposed model consistently outperforms existing transcription methods.
arxiv-1411-1792 | How transferable are features in deep neural networks? |  http://arxiv.org/abs/1411.1792  | author:Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson category:cs.LG cs.NE published:2014-11-06 summary:Many deep neural networks trained on natural images exhibit a curiousphenomenon in common: on the first layer they learn features similar to Gaborfilters and color blobs. Such first-layer features appear not to be specific toa particular dataset or task, but general in that they are applicable to manydatasets and tasks. Features must eventually transition from general tospecific by the last layer of the network, but this transition has not beenstudied extensively. In this paper we experimentally quantify the generalityversus specificity of neurons in each layer of a deep convolutional neuralnetwork and report a few surprising results. Transferability is negativelyaffected by two distinct issues: (1) the specialization of higher layer neuronsto their original task at the expense of performance on the target task, whichwas expected, and (2) optimization difficulties related to splitting networksbetween co-adapted neurons, which was not expected. In an example networktrained on ImageNet, we demonstrate that either of these two issues maydominate, depending on whether features are transferred from the bottom,middle, or top of the network. We also document that the transferability offeatures decreases as the distance between the base task and target taskincreases, but that transferring features even from distant tasks can be betterthan using random features. A final surprising result is that initializing anetwork with transferred features from almost any number of layers can producea boost to generalization that lingers even after fine-tuning to the targetdataset.
arxiv-1411-1670 | Stochastic Variational Inference for Hidden Markov Models |  http://arxiv.org/abs/1411.1670  | author:Nicholas J. Foti, Jason Xu, Dillon Laird, Emily B. Fox category:stat.ML published:2014-11-06 summary:Variational inference algorithms have proven successful for Bayesian analysisin large data settings, with recent advances using stochastic variationalinference (SVI). However, such methods have largely been studied in independentor exchangeable data settings. We develop an SVI algorithm to learn theparameters of hidden Markov models (HMMs) in a time-dependent data setting. Thechallenge in applying stochastic optimization in this setting arises fromdependencies in the chain, which must be broken to consider minibatches ofobservations. We propose an algorithm that harnesses the memory decay of thechain to adaptively bound errors arising from edge effects. We demonstrate theeffectiveness of our algorithm on synthetic experiments and a large genomicsdataset where a batch algorithm is computationally infeasible.
arxiv-1411-1557 | Proof Supplement - Learning Sparse Causal Models is not NP-hard (UAI2013) |  http://arxiv.org/abs/1411.1557  | author:Tom Claassen, Joris M. Mooij, Tom Heskes category:stat.ML published:2014-11-06 summary:This article contains detailed proofs and additional examples related to theUAI-2013 submission `Learning Sparse Causal Models is not NP-hard'. Itdescribes the FCI+ algorithm: a method for sound and complete causal modeldiscovery in the presence of latent confounders and/or selection bias, that hasworst case polynomial complexity of order $N^{2(k+1)}$ in the number ofindependence tests, for sparse graphs over $N$ nodes, bounded by node degree$k$. The algorithm is an adaptation of the well-known FCI algorithm by (Spirteset al., 2000) that is also sound and complete, but has worst case complexityexponential in $N$.
arxiv-1411-1509 | Convolutional Neural Network-based Place Recognition |  http://arxiv.org/abs/1411.1509  | author:Zetao Chen, Obadiah Lam, Adam Jacobson, Michael Milford category:cs.CV cs.LG cs.NE published:2014-11-06 summary:Recently Convolutional Neural Networks (CNNs) have been shown to achievestate-of-the-art performance on various classification tasks. In this paper, wepresent for the first time a place recognition technique based on CNN models,by combining the powerful features learnt by CNNs with a spatial and sequentialfilter. Applying the system to a 70 km benchmark place recognition dataset weachieve a 75% increase in recall at 100% precision, significantly outperformingall previous state of the art techniques. We also conduct a comprehensiveperformance comparison of the utility of features from all 21 layers for placerecognition, both for the benchmark dataset and for a second dataset with moresignificant viewpoint changes.
arxiv-1411-1469 | A Generic Sample Splitting Approach for Refined Community Recovery in Stochastic Block Models |  http://arxiv.org/abs/1411.1469  | author:Jing Lei, Lingxue Zhu category:stat.ML math.ST stat.TH published:2014-11-06 summary:We propose and analyze a generic method for community recovery in stochasticblock models and degree corrected block models. This approach can exactlyrecover the hidden communities with high probability when the expected nodedegrees are of order $\log n$ or higher. Starting from a roughly correctcommunity partition given by some conventional community recovery algorithm,this method refines the partition in a cross clustering step. Our resultssimplify and extend some of the previous work on exact community recovery,discovering the key role played by sample splitting. The proposed method issimple and can be implemented with many practical community recoveryalgorithms.
arxiv-1411-1784 | Conditional Generative Adversarial Nets |  http://arxiv.org/abs/1411.1784  | author:Mehdi Mirza, Simon Osindero category:cs.LG cs.AI cs.CV stat.ML published:2014-11-06 summary:Generative Adversarial Nets [8] were recently introduced as a novel way totrain generative models. In this work we introduce the conditional version ofgenerative adversarial nets, which can be constructed by simply feeding thedata, y, we wish to condition on to both the generator and discriminator. Weshow that this model can generate MNIST digits conditioned on class labels. Wealso illustrate how this model could be used to learn a multi-modal model, andprovide preliminary examples of an application to image tagging in which wedemonstrate how this approach can generate descriptive tags which are not partof training labels.
arxiv-1411-1537 | Large-Margin Determinantal Point Processes |  http://arxiv.org/abs/1411.1537  | author:Boqing Gong, Wei-lun Chao, Kristen Grauman, Fei Sha category:stat.ML cs.CV cs.LG published:2014-11-06 summary:Determinantal point processes (DPPs) offer a powerful approach to modelingdiversity in many applications where the goal is to select a diverse subset. Westudy the problem of learning the parameters (the kernel matrix) of a DPP fromlabeled training data. We make two contributions. First, we show how toreparameterize a DPP's kernel matrix with multiple kernel functions, thusenhancing modeling flexibility. Second, we propose a novel parameter estimationtechnique based on the principle of large margin separation. In contrast to thestate-of-the-art method of maximum likelihood estimation, our large-margin lossfunction explicitly models errors in selecting the target subsets, and it canbe customized to trade off different types of errors (precision vs. recall).Extensive empirical studies validate our contributions, including applicationson challenging document and video summarization, where flexibility in modelingthe kernel matrix and balancing different errors is indispensable.
arxiv-1411-1125 | Distributed Low-Rank Estimation Based on Joint Iterative Optimization in Wireless Sensor Networks |  http://arxiv.org/abs/1411.1125  | author:S. Xu, R. C. de Lamare, H. V. Poor category:cs.IT cs.LG math.IT published:2014-11-05 summary:This paper proposes a novel distributed reduced--rank scheme and an adaptivealgorithm for distributed estimation in wireless sensor networks. The proposeddistributed scheme is based on a transformation that performs dimensionalityreduction at each agent of the network followed by a reduced-dimensionparameter vector. A distributed reduced-rank joint iterative estimationalgorithm is developed, which has the ability to achieve significantly reducedcommunication overhead and improved performance when compared with existingtechniques. Simulation results illustrate the advantages of the proposedstrategy in terms of convergence rate and mean square error performance.
arxiv-1411-1158 | On the Complexity of Learning with Kernels |  http://arxiv.org/abs/1411.1158  | author:Nicolò Cesa-Bianchi, Yishay Mansour, Ohad Shamir category:cs.LG stat.ML published:2014-11-05 summary:A well-recognized limitation of kernel learning is the requirement to handlea kernel matrix, whose size is quadratic in the number of training examples.Many methods have been proposed to reduce this computational cost, mostly byusing a subset of the kernel matrix entries, or some form of low-rank matrixapproximation, or a random projection method. In this paper, we study lowerbounds on the error attainable by such methods as a function of the number ofentries observed in the kernel matrix or the rank of an approximate kernelmatrix. We show that there are kernel learning problems where no such methodwill lead to non-trivial computational savings. Our results also quantify howthe problem difficulty depends on parameters such as the nature of the lossfunction, the regularization parameter, the norm of the desired predictor, andthe kernel matrix rank. Our results also suggest cases where more efficientkernel learning might be possible.
arxiv-1411-1420 | Basis Learning as an Algorithmic Primitive |  http://arxiv.org/abs/1411.1420  | author:Mikhail Belkin, Luis Rademacher, James Voss category:cs.LG published:2014-11-05 summary:A number of important problems in theoretical computer science and machinelearning can be interpreted as recovering a certain basis. These includecertain tensor decompositions, Independent Component Analysis (ICA), spectralclustering and Gaussian mixture learning. Each of these problems reduces to aninstance of our general model, which we call a "Basis Encoding Function" (BEF).We show that learning a basis within this model can then be provably andefficiently achieved using a first order iteration algorithm (gradientiteration). Our algorithm goes beyond tensor methods, providing afunction-based generalization for a number of existing methods including theclassical matrix power method, the tensor power iteration as well ascumulant-based FastICA. Our framework also unifies the unusual phenomenonobserved in these domains that they can be solved using efficient non-convexoptimization. Specifically, we describe a class of BEFs such that their localmaxima on the unit sphere are in one-to-one correspondence with the basiselements. This description relies on a certain "hidden convexity" property ofthese functions. We provide a complete theoretical analysis of gradient iteration even whenthe BEF is perturbed. We show convergence and complexity bounds polynomial indimension and other relevant parameters, such as perturbation size. Ourperturbation results can be considered as a non-linear version of the classicalDavis-Kahan theorem for perturbations of eigenvectors of symmetric matrices. Inaddition we show that our algorithm exhibits fast (superlinear) convergence andrelate the speed of convergence to the properties of the BEF. Moreover, thegradient iteration algorithm can be easily and efficiently implemented inpractice. Finally we apply our framework by providing the first provablealgorithm for recovery in a general perturbed ICA model.
arxiv-1411-4297 | Application of Multi-core Parallel Programming to a Combination of Ant Colony Optimization and Genetic Algorithm |  http://arxiv.org/abs/1411.4297  | author:Rishita Kalyani category:cs.NE published:2014-11-05 summary:This Paper will deal with a combination of Ant Colony and Genetic ProgrammingAlgorithm to optimize Travelling Salesmen problem (NP-Hard). However, thecomplexity of the algorithm requires considerable computational time andresources. Parallel implementation can reduce the computational time. In thispaper, emphasis in the parallelizing section is given to Multi-corearchitecture and Multi-Processor Systems which is developed and used almosteverywhere today and hence, multi-core parallelization to the combination ofalgorithm is achieved by OpenMP library by Intel Corporation.
arxiv-1411-1147 | Conditional Random Field Autoencoders for Unsupervised Structured Prediction |  http://arxiv.org/abs/1411.1147  | author:Waleed Ammar, Chris Dyer, Noah A. Smith category:cs.LG cs.CL published:2014-11-05 summary:We introduce a framework for unsupervised learning of structured predictorswith overlapping, global features. Each input's latent representation ispredicted conditional on the observable data using a feature-rich conditionalrandom field. Then a reconstruction of the input is (re)generated, conditionalon the latent structure, using models for which maximum likelihood estimationhas a closed-form. Our autoencoder formulation enables efficient learningwithout making unrealistic independence assumptions or restricting the kinds offeatures that can be used. We illustrate insightful connections to traditionalautoencoders, posterior regularization and multi-view learning. We showcompetitive results with instantiations of the model for two canonical NLPtasks: part-of-speech induction and bitext word alignment, and show thattraining our model can be substantially more efficient than comparablefeature-rich baselines.
arxiv-1411-1434 | On the Information Theoretic Limits of Learning Ising Models |  http://arxiv.org/abs/1411.1434  | author:Karthikeyan Shanmugam, Rashish Tandon, Alexandros G. Dimakis, Pradeep Ravikumar category:cs.LG published:2014-11-05 summary:We provide a general framework for computing lower-bounds on the samplecomplexity of recovering the underlying graphs of Ising models, given i.i.dsamples. While there have been recent results for specific graph classes, theseinvolve fairly extensive technical arguments that are specialized to eachspecific graph class. In contrast, we isolate two key graph-structuralingredients that can then be used to specify sample complexity lower-bounds.Presence of these structural properties makes the graph class hard to learn. Wederive corollaries of our main result that not only recover existing recentresults, but also provide lower bounds for novel graph classes not consideredpreviously. We also extend our framework to the random graph setting and derivecorollaries for Erd\H{o}s-R\'{e}nyi graphs in a certain dense setting.
arxiv-1411-1134 | Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems |  http://arxiv.org/abs/1411.1134  | author:Christopher De Sa, Kunle Olukotun, Christopher Ré category:cs.LG math.OC stat.ML published:2014-11-05 summary:Stochastic gradient descent (SGD) on a low-rank factorization is commonlyemployed to speed up matrix problems including matrix completion, subspacetracking, and SDP relaxation. In this paper, we exhibit a step size scheme forSGD on a low-rank least-squares problem, and we prove that, under broadsampling conditions, our method converges globally from a random starting pointwithin $O(\epsilon^{-1} n \log n)$ steps with constant probability forconstant-rank problems. Our modification of SGD relates it to stochastic poweriteration. We also show experiments to illustrate the runtime and convergenceof the algorithm.
arxiv-1411-1316 | Rapid Skill Capture in a First-Person Shooter |  http://arxiv.org/abs/1411.1316  | author:David Buckley, Ke Chen, Joshua Knowles category:cs.HC cs.LG published:2014-11-05 summary:Various aspects of computer game design, including adaptive elements of gamelevels, characteristics of 'bot' behavior, and player matching in multiplayergames, would ideally be sensitive to a player's skill level. Yet, whiledifficulty and player learning have been explored in the context of games,there has been little work analyzing skill per se, and how it pertains to aplayer's input. To this end, we present a data set of 476 game logs from over40 players of a first-person shooter game (Red Eclipse) as a basis of a casestudy. We then analyze different metrics of skill and show that some of thesecan be predicted using only a few seconds of keyboard and mouse input. We arguethat the techniques used here are useful for adapting games to match players'skill levels rapidly, perhaps more rapidly than solutions based on performanceaveraging such as TrueSkill.
arxiv-1411-1119 | Projecting Markov Random Field Parameters for Fast Mixing |  http://arxiv.org/abs/1411.1119  | author:Xianghang Liu, Justin Domke category:cs.LG stat.ML published:2014-11-05 summary:Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerfultechniques to sample from almost arbitrary distributions. The flaw in practiceis that it can take a large and/or unknown amount of time to converge to thestationary distribution. This paper gives sufficient conditions to guaranteethat univariate Gibbs sampling on Markov Random Fields (MRFs) will be fastmixing, in a precise sense. Further, an algorithm is given to project onto thisset of fast-mixing parameters in the Euclidean norm. Following recent work, wegive an example use of this to project in various divergence measures,comparing univariate marginals obtained by sampling after projection to commonvariational methods and Gibbs sampling on the original parameters.
arxiv-1411-1442 | Optical Character Recognition, Using K-Nearest Neighbors |  http://arxiv.org/abs/1411.1442  | author:Wei Wang category:cs.CV published:2014-11-05 summary:The problem of optical character recognition, OCR, has been widely discussedin the literature. Having a hand-written text, the program aims at recognizingthe text. Even though there are several approaches to this issue, it is stillan open problem. In this paper we would like to propose an approach that usesK-nearest neighbors algorithm, and has the accuracy of more than 90%. Thetraining and run time is also very short.
arxiv-1411-1446 | Electrocardiography Separation of Mother and Baby |  http://arxiv.org/abs/1411.1446  | author:Wei Wang category:cs.CV cs.LG published:2014-11-05 summary:Extraction of Electrocardiography (ECG or EKG) signals of mother and baby isa challenging task, because one single device is used and it receives a mixtureof multiple heart beats. In this paper, we would like to design a filter toseparate the signals from each other.
arxiv-1411-1285 | Controlling false discoveries in high-dimensional situations: Boosting with stability selection |  http://arxiv.org/abs/1411.1285  | author:Benjamin Hofner, Luigi Boccuto, Markus Göker category:stat.ML stat.AP stat.CO published:2014-11-05 summary:Modern biotechnologies often result in high-dimensional data sets with muchmore variables than observations (n $\ll$ p). These data sets pose newchallenges to statistical analysis: Variable selection becomes one of the mostimportant tasks in this setting. We assess the recently proposed flexibleframework for variable selection called stability selection. By the use ofresampling procedures, stability selection adds a finite sample error controlto high-dimensional variable selection procedures such as Lasso or boosting. Weconsider the combination of boosting and stability selection and presentresults from a detailed simulation study that provides insights into theusefulness of this combination. Limitations are discussed and guidance on thespecification and tuning of stability selection is given. The interpretation ofthe used error bounds is elaborated and insights for practical data analysisare given. The results will be used to detect differentially expressedphenotype measurements in patients with autism spectrum disorders. All methodsare implemented in the freely available R package stabs.
arxiv-1411-1243 | Using Twitter to predict football outcomes |  http://arxiv.org/abs/1411.1243  | author:Stylianos Kampakis, Andreas Adamides category:stat.ML cs.CL cs.SI I.2.m published:2014-11-05 summary:Twitter has been proven to be a notable source for predictive modelling onvarious domains such as the stock market, the dissemination of diseases orsports outcomes. However, such a study has not been conducted in football(soccer) so far. The purpose of this research was to study whether data minedfrom Twitter can be used for this purpose. We built a set of predictive modelsfor the outcome of football games of the English Premier League for a 3 monthperiod based on tweets and we studied whether these models can overcomepredictive models which use only historical data and simple footballstatistics. Moreover, combined models are constructed using both Twitter andhistorical data. The final results indicate that data mined from Twitter canindeed be a useful source for predicting games in the Premier League. The finalTwitter-based model performs significantly better than chance when measured byCohen's kappa and is comparable to the model that uses simple statistics andhistorical data. Combining both models raises the performance higher than itwas achieved by each individual model. Thereby, this study provides evidencethat Twitter derived features can indeed provide useful information for theprediction of football (soccer) outcomes.
arxiv-1411-1172 | Tensor object classification via multilinear discriminant analysis network |  http://arxiv.org/abs/1411.1172  | author:Rui Zeng, Jiasong Wu, Lotfi Senhadji, Huazhong Shu category:cs.CV published:2014-11-05 summary:This paper proposes a multilinear discriminant analysis network (MLDANet) forthe recognition of multidimensional objects, known as tensor objects. TheMLDANet is a variation of linear discriminant analysis network (LDANet) andprincipal component analysis network (PCANet), both of which are the recentlyproposed deep learning algorithms. The MLDANet consists of three parts: 1) Theencoder learned by MLDA from tensor data. 2) Features maps ob-tained fromdecoder. 3) The use of binary hashing and histogram for feature pooling. Alearning algorithm for MLDANet is described. Evaluations on UCF11 databaseindicate that the proposed MLDANet outperforms the PCANet, LDANet, MPCA + LDA,and MLDA in terms of classification for tensor objects.
arxiv-1411-1171 | Multilinear Principal Component Analysis Network for Tensor Object Classification |  http://arxiv.org/abs/1411.1171  | author:Rui Zeng, Jiasong Wu, Zhuhong Shao, Lotfi Senhadji, Huazhong Shu category:cs.CV published:2014-11-05 summary:The recently proposed principal component analysis network (PCANet) has beenproved high performance for visual content classification. In this letter, wedevelop a tensorial extension of PCANet, namely, multilinear principal analysiscomponent network (MPCANet), for tensor object classification. Compared toPCANet, the proposed MPCANet uses the spatial structure and the relationshipbetween each dimension of tensor objects much more efficiently. Experimentswere conducted on different visual content datasets including UCF sports actionvideo sequences database and UCF11 database. The experimental results haverevealed that the proposed MPCANet achieves higher classification accuracy thanPCANet for tensor object classification.
arxiv-1411-1372 | Online SLAM with Any-time Self-calibration and Automatic Change Detection |  http://arxiv.org/abs/1411.1372  | author:Nima Keivan, Gabe Sibley category:cs.CV cs.RO published:2014-11-05 summary:A framework for online simultaneous localization, mapping andself-calibration is presented which can detect and handle significant change inthe calibration parameters. Estimates are computed in constant-time byfactoring the problem and focusing on segments of the trajectory that are mostinformative for the purposes of calibration. A novel technique is presented todetect the probability that a significant change is present in the calibrationparameters. The system is then able to re-calibrate. Maximum likelihoodtrajectory and map estimates are computed using an asynchronous and adaptiveoptimization. The system requires no prior information and is able toinitialize without any special motions or routines, or in the case whereobservability over calibration parameters is delayed. The system isexperimentally validated to calibrate camera intrinsic parameters for anonlinear camera model on a monocular dataset featuring a significant zoomevent partway through, and achieves high accuracy despite unknown initialcalibration parameters. Self-calibration and re-calibration parameters areshown to closely match estimates computed using a calibration target. Theaccuracy of the system is demonstrated with SLAM results that achieve sub-1%distance-travel error even in the presence of significant re-calibrationevents.
arxiv-1411-1297 | Edge Detection based on Kernel Density Estimation |  http://arxiv.org/abs/1411.1297  | author:Osvaldo Pereira, Esley Torre, Yasel Garcés, Roberto Rodríguez category:cs.CV published:2014-11-05 summary:Edges of an image are considered a crucial type of information. These can beextracted by applying edge detectors with different methodology. Edge detectionis a vital step in computer vision tasks, because it is an essential issue forpattern recognition and visual interpretation. In this paper, we propose a newmethod for edge detection in images, based on the estimation by kernel of theprobability density function. In our algorithm, pixels in the image withminimum value of density function are labeled as edges. The boundary betweentwo homogeneous regions is defined in two domains: the spatial/lattice domainand the range/color domain. Extensive experimental evaluations proved that ouredge detection method is significantly a competitive algorithm.
arxiv-1411-0740 | State-of-the-Art in Retinal Optical Coherence Tomography Image Analysis |  http://arxiv.org/abs/1411.0740  | author:Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu category:cs.CV published:2014-11-04 summary:Optical Coherence Tomography (OCT) is one of the most emerging imagingmodalities that has been used widely in the field of biomedical imaging. Fromits emergence in 1990's, plenty of hardware and software improvements have beenmade. Its applications range from ophthalmology to dermatology to coronaryimaging etc. Here, the focus is on applications of OCT in ophthalmology andretinal imaging. OCT is able to non-invasively produce cross-sectional volumeimages of the tissues which are further used for analysis of the tissuestructure and its properties. Due to the underlying physics, OCT images usuallysuffer from a granular pattern, called speckle noise, which restricts theprocess of interpretation, hence requiring specialized noise reductiontechniques to remove the noise while preserving image details. Also, given thefact that OCT images are in the $\mu m$ -level, further analysis in needed todistinguish between the different structures in the imaged volume. Thereforethe use of different segmentation techniques are of high importance. Themovement of the tissue under imaging or the progression of disease in thetissue also imposes further implications both on the quality and the properinterpretation of the acquired images. Thus, use of image registrationtechniques can be very helpful. In this work, an overview of such imageanalysis techniques will be given.
arxiv-1411-1045 | Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet |  http://arxiv.org/abs/1411.1045  | author:Matthias Kümmerer, Lucas Theis, Matthias Bethge category:cs.CV q-bio.NC stat.AP published:2014-11-04 summary:Recent results suggest that state-of-the-art saliency models perform far fromoptimal in predicting fixations. This lack in performance has been attributedto an inability to model the influence of high-level image features such asobjects. Recent seminal advances in applying deep neural networks to tasks likeobject recognition suggests that they are able to capture this kind ofstructure. However, the enormous amount of training data necessary to trainthese networks makes them difficult to apply directly to saliency prediction.We present a novel way of reusing existing neural networks that have beenpretrained on the task of object recognition in models of fixation prediction.Using the well-known network of Krizhevsky et al. (2012), we come up with a newsaliency model that significantly outperforms all state-of-the-art models onthe MIT Saliency Benchmark. We show that the structure of this network allowsnew insights in the psychophysics of fixation selection and potentially theirneural implementation. To train our network, we build on recent work on themodeling of saliency as point processes.
arxiv-1411-0895 | Tied Probabilistic Linear Discriminant Analysis for Speech Recognition |  http://arxiv.org/abs/1411.0895  | author:Liang Lu, Steve Renals category:cs.CL cs.AI published:2014-11-04 summary:Acoustic models using probabilistic linear discriminant analysis (PLDA)capture the correlations within feature vectors using subspaces which do notvastly expand the model. This allows high dimensional and correlated featurespaces to be used, without requiring the estimation of multiple high dimensioncovariance matrices. In this letter we extend the recently presented PLDAmixture model for speech recognition through a tied PLDA approach, which isbetter able to control the model size to avoid overfitting. We carried outexperiments using the Switchboard corpus, with both mel frequency cepstralcoefficient features and bottleneck feature derived from a deep neural network.Reductions in word error rate were obtained by using tied PLDA, compared withthe PLDA mixture model, subspace Gaussian mixture models, and deep neuralnetworks.
arxiv-1411-0900 | Kernel Mean Estimation via Spectral Filtering |  http://arxiv.org/abs/1411.0900  | author:Krikamol Muandet, Bharath Sriperumbudur, Bernhard Schölkopf category:stat.ML math.ST stat.TH published:2014-11-04 summary:The problem of estimating the kernel mean in a reproducing kernel Hilbertspace (RKHS) is central to kernel methods in that it is used by classicalapproaches (e.g., when centering a kernel PCA matrix), and it also forms thecore inference step of modern kernel methods (e.g., kernel-based non-parametrictests) that rely on embedding probability distributions in RKHSs. Muandet etal. (2014) has shown that shrinkage can help in constructing "better"estimators of the kernel mean than the empirical estimator. The present paperstudies the consistency and admissibility of the estimators in Muandet et al.(2014), and proposes a wider class of shrinkage estimators that improve uponthe empirical estimator by considering appropriate basis functions. Using thekernel PCA basis, we show that some of these estimators can be constructedusing spectral filtering algorithms which are shown to be consistent under sometechnical assumptions. Our theoretical analysis also reveals a fundamentalconnection to the kernel-based supervised learning framework. The proposedestimators are simple to implement and perform well in practice.
arxiv-1411-0860 | CUR Algorithm for Partially Observed Matrices |  http://arxiv.org/abs/1411.0860  | author:Miao Xu, Rong Jin, Zhi-Hua Zhou category:cs.LG published:2014-11-04 summary:CUR matrix decomposition computes the low rank approximation of a givenmatrix by using the actual rows and columns of the matrix. It has been a veryuseful tool for handling large matrices. One limitation with the existingalgorithms for CUR matrix decomposition is that they need an access to the {\itfull} matrix, a requirement that can be difficult to fulfill in many real worldapplications. In this work, we alleviate this limitation by developing a CURdecomposition algorithm for partially observed matrices. In particular, theproposed algorithm computes the low rank approximation of the target matrixbased on (i) the randomly sampled rows and columns, and (ii) a subset ofobserved entries that are randomly sampled from the matrix. Our analysis showsthe relative error bound, measured by spectral norm, for the proposed algorithmwhen the target matrix is of full rank. We also show that only $O(n r\ln r)$observed entries are needed by the proposed algorithm to perfectly recover arank $r$ matrix of size $n\times n$, which improves the sample complexity ofthe existing algorithms for matrix completion. Empirical studies on bothsynthetic and real-world datasets verify our theoretical claims and demonstratethe effectiveness of the proposed algorithm.
arxiv-1411-1088 | Expectation-Maximization for Learning Determinantal Point Processes |  http://arxiv.org/abs/1411.1088  | author:Jennifer Gillenwater, Alex Kulesza, Emily Fox, Ben Taskar category:stat.ML cs.LG published:2014-11-04 summary:A determinantal point process (DPP) is a probabilistic model of set diversitycompactly parameterized by a positive semi-definite kernel matrix. To fit a DPPto a given task, we would like to learn the entries of its kernel matrix bymaximizing the log-likelihood of the available data. However, log-likelihood isnon-convex in the entries of the kernel matrix, and this learning problem isconjectured to be NP-hard. Thus, previous work has instead focused on morerestricted convex learning settings: learning only a single weight for each rowof the kernel matrix, or learning weights for a linear combination of DPPs withfixed kernel matrices. In this work we propose a novel algorithm for learningthe full kernel matrix. By changing the kernel parameterization from matrixentries to eigenvalues and eigenvectors, and then lower-bounding the likelihoodin the manner of expectation-maximization algorithms, we obtain an effectiveoptimization procedure. We test our method on a real-world productrecommendation task, and achieve relative gains of up to 16.5% in testlog-likelihood compared to the naive approach of maximizing likelihood byprojected gradient ascent on the entries of the kernel matrix.
arxiv-1411-1091 | Do Convnets Learn Correspondence? |  http://arxiv.org/abs/1411.1091  | author:Jonathan Long, Ning Zhang, Trevor Darrell category:cs.CV cs.LG cs.NE published:2014-11-04 summary:Convolutional neural nets (convnets) trained from massive labeled datasetshave substantially improved the state-of-the-art in image classification andobject detection. However, visual understanding requires establishingcorrespondence on a finer level than object category. Given their large poolingregions and training from whole-image labels, it is not clear that convnetsderive their success from an accurate correspondence model which could be usedfor precise localization. In this paper, we study the effectiveness of convnetactivation features for tasks requiring correspondence. We present evidencethat convnet features localize at a much finer scale than their receptive fieldsizes, that they can be used to perform intraclass alignment as well asconventional hand-engineered features, and that they outperform conventionalfeatures in keypoint prediction on objects from PASCAL VOC 2011.
arxiv-1411-1087 | Fast Exact Matrix Completion with Finite Samples |  http://arxiv.org/abs/1411.1087  | author:Prateek Jain, Praneeth Netrapalli category:cs.NA cs.DS cs.IT cs.LG math.IT stat.ML published:2014-11-04 summary:Matrix completion is the problem of recovering a low rank matrix by observinga small fraction of its entries. A series of recent works [KOM12,JNS13,HW14]have proposed fast non-convex optimization based iterative algorithms to solvethis problem. However, the sample complexity in all these results issub-optimal in its dependence on the rank, condition number and the desiredaccuracy. In this paper, we present a fast iterative algorithm that solves the matrixcompletion problem by observing $O(nr^5 \log^3 n)$ entries, which isindependent of the condition number and the desired accuracy. The run time ofour algorithm is $O(nr^7\log^3 n\log 1/\epsilon)$ which is near linear in thedimension of the matrix. To the best of our knowledge, this is the first nearlinear time algorithm for exact matrix completion with finite sample complexity(i.e. independent of $\epsilon$). Our algorithm is based on a well known projected gradient descent method,where the projection is onto the (non-convex) set of low rank matrices. Thereare two key ideas in our result: 1) our argument is based on a $\ell_{\infty}$norm potential function (as opposed to the spectral norm) and provides a novelway to obtain perturbation bounds for it. 2) we prove and use a naturalextension of the Davis-Kahan theorem to obtain perturbation bounds on the bestlow rank approximation of matrices with good eigen-gap. Both of these ideas maybe of independent interest.
arxiv-1411-0894 | Classification with the nearest neighbor rule in general finite dimensional spaces: necessary and sufficient conditions |  http://arxiv.org/abs/1411.0894  | author:Sébastien Gadat, Thierry Klein, Clément Marteau category:math.ST stat.ML stat.TH published:2014-11-04 summary:Given an $n$-sample of random vectors $(X_i,Y_i)_{1 \leq i \leq n}$ whosejoint law is unknown, the long-standing problem of supervised classificationaims to \textit{optimally} predict the label $Y$ of a given a new observation$X$. In this context, the nearest neighbor rule is a popular flexible andintuitive method in non-parametric situations. Even if this algorithm is commonly used in the machine learning andstatistics communities, less is known about its prediction ability in generalfinite dimensional spaces, especially when the support of the density of theobservations is $\mathbb{R}^d$. This paper is devoted to the study of thestatistical properties of the nearest neighbor rule in various situations. Inparticular, attention is paid to the marginal law of $X$, as well as thesmoothness and margin properties of the \textit{regression function} $\eta(X) =\mathbb{E}[Y X]$. We identify two necessary and sufficient conditions toobtain uniform consistency rates of classification and to derive sharpestimates in the case of the nearest neighbor rule. Some numerical experimentsare proposed at the end of the paper to help illustrate the discussion.
arxiv-1411-0814 | A random algorithm for low-rank decomposition of large-scale matrices with missing entries |  http://arxiv.org/abs/1411.0814  | author:Yiguang Liu category:cs.NA cs.CV published:2014-11-04 summary:A Random SubMatrix method (RSM) is proposed to calculate the low-rankdecomposition of large-scale matrices with known entry percentage \rho. RSM isvery fast as the floating-point operations (flops) required are comparedfavorably with the state-of-the-art algorithms. Meanwhile RSM is verymemory-saving. With known entries homogeneously distributed in the givenmatrix, sub-matrices formed by known entries are randomly selected. Accordingto the just proved theorem that subspace related to smaller singular values isless perturbed by noise, the null vectors or the right singular vectorsassociated with the minor singular values are calculated for each submatrix.The vectors are the null vectors of the corresponding submatrix in the groundtruth of the given large-scale matrix. If enough sub-matrices are randomlychosen, the low-rank decomposition is estimated. The experimental results onrandom synthetical matrices with sizes such as 131072X1024 and on real datasets indicate that RSM is much faster and memory-saving, and, meanwhile, hasconsiderable high precision achieving or approximating to the best.
arxiv-1411-1076 | A statistical model for tensor PCA |  http://arxiv.org/abs/1411.1076  | author:Andrea Montanari, Emile Richard category:cs.LG cs.IT math.IT stat.ML published:2014-11-04 summary:We consider the Principal Component Analysis problem for large tensors ofarbitrary order $k$ under a single-spike (or rank-one plus noise) model. On theone hand, we use information theory, and recent results in probability theory,to establish necessary and sufficient conditions under which the principalcomponent can be estimated using unbounded computational resources. It turnsout that this is possible as soon as the signal-to-noise ratio $\beta$ becomeslarger than $C\sqrt{k\log k}$ (and in particular $\beta$ can remain bounded asthe problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms,based on tensor unfolding, power iteration and message passing ideas fromgraphical models. We show that, unless the signal-to-noise ratio diverges inthe system dimensions, none of these approaches succeeds. This is possiblyrelated to a fundamental limitation of computationally tractable estimators forthis problem. We discuss various initializations for tensor power iteration, and show thata tractable initialization based on the spectrum of the matricized tensoroutperforms significantly baseline methods, statistically and computationally.Finally, we consider the case in which additional side information is availableabout the unknown signal. We characterize the amount of side information thatallows the iterative algorithms to converge to a good estimate.
arxiv-1411-0861 | Using Linguistic Features to Estimate Suicide Probability of Chinese Microblog Users |  http://arxiv.org/abs/1411.0861  | author:Lei Zhang, Xiaolei Huang, Tianli Liu, Zhenxiang Chen, Tingshao Zhu category:cs.SI cs.CL published:2014-11-04 summary:If people with high risk of suicide can be identified through social medialike microblog, it is possible to implement an active intervention system tosave their lives. Based on this motivation, the current study administered theSuicide Probability Scale(SPS) to 1041 weibo users at Sina Weibo, which is aleading microblog service provider in China. Two NLP (Natural LanguageProcessing) methods, the Chinese edition of Linguistic Inquiry and Word Count(LIWC) lexicon and Latent Dirichlet Allocation (LDA), are used to extractlinguistic features from the Sina Weibo data. We trained predicting models bymachine learning algorithm based on these two types of features, to estimatesuicide probability based on linguistic features. The experiment resultsindicate that LDA can find topics that relate to suicide probability, andimprove the performance of prediction. Our study adds value in prediction ofsuicidal probability of social network users with their behaviors.
arxiv-1411-0802 | Simultaneous Localization, Mapping, and Manipulation for Unsupervised Object Discovery |  http://arxiv.org/abs/1411.0802  | author:Lu Ma, Mahsa Ghafarianzadeh, Dave Coleman, Nikolaus Correll, Gabe Sibley category:cs.RO cs.CV published:2014-11-04 summary:We present an unsupervised framework for simultaneous appearance-based objectdiscovery, detection, tracking and reconstruction using RGBD cameras and arobot manipulator. The system performs dense 3D simultaneous localization andmapping concurrently with unsupervised object discovery. Putative objects thatare spatially and visually coherent are manipulated by the robot to gainadditional motion-cues. The robot uses appearance alone, followed by structureand motion cues, to jointly discover, verify, learn and improve models ofobjects. Induced motion segmentation reinforces learned models which arerepresented implicitly as 2D and 3D level sets to capture both shape andappearance. We compare three different approaches for appearance-based objectdiscovery and find that a novel form of spatio-temporal super-pixels gives thehighest quality candidate object models in terms of precision and recall. Liveexperiments with a Baxter robot demonstrate a holistic pipeline capable ofautomatic discovery, verification, detection, tracking and reconstruction ofunknown objects.
arxiv-1411-0997 | Iterated geometric harmonics for data imputation and reconstruction of missing data |  http://arxiv.org/abs/1411.0997  | author:Chad Eckman, Jonathan A. Lindgren, Erin P. J. Pearse, David J. Sacco, Zachariah Zhang category:cs.LG stat.ML published:2014-11-04 summary:The method of geometric harmonics is adapted to the situation of incompletedata by means of the iterated geometric harmonics (IGH) scheme. The method istested on natural and synthetic data sets with 50--500 data points anddimensionality of 400--10,000. Experiments suggest that the algorithm convergesto a near optimal solution within 4--6 iterations, at runtimes of less than 30minutes on a medium-grade desktop computer. The imputation of missing datavalues is applied to collections of damaged images (suffering from dataannihilation rates of up to 70\%) which are reconstructed with a surprisingdegree of accuracy.
arxiv-1411-0791 | A Robust Point Sets Matching Method |  http://arxiv.org/abs/1411.0791  | author:Xiao Liu, Congying Han, Tiande Guo category:cs.CV published:2014-11-04 summary:Point sets matching method is very important in computer vision, featureextraction, fingerprint matching, motion estimation and so on. This paperproposes a robust point sets matching method. We present an iterative algorithmthat is robust to noise case. Firstly, we calculate all transformations betweentwo points. Then similarity matrix are computed to measure the possibility thattwo transformation are both true. We iteratively update the matching scorematrix by using the similarity matrix. By using matching algorithm on graph, weobtain the matching result. Experimental results obtained by our approach showrobustness to outlier and jitter.
arxiv-1411-1006 | A Probabilistic Translation Method for Dictionary-based Cross-lingual Information Retrieval in Agglutinative Languages |  http://arxiv.org/abs/1411.1006  | author:Javid Dadashkarimi, Azadeh Shakery, Heshaam Faili category:cs.IR cs.CL published:2014-11-04 summary:Translation ambiguity, out of vocabulary words and missing some translationsin bilingual dictionaries make dictionary-based Cross-language InformationRetrieval (CLIR) a challenging task. Moreover, in agglutinative languages whichdo not have reliable stemmers, missing various lexical formations in bilingualdictionaries degrades CLIR performance. This paper aims to introduce aprobabilistic translation model to solve the ambiguity problem, and also toprovide most likely formations of a dictionary candidate. We propose MinimumEdit Support Candidates (MESC) method that exploits a monolingual corpus and abilingual dictionary to translate users' native language queries to documents'language. Our experiments show that the proposed method outperformsstate-of-the-art dictionary-based English-Persian CLIR.
arxiv-1411-0972 | Convex Optimization for Big Data |  http://arxiv.org/abs/1411.0972  | author:Volkan Cevher, Stephen Becker, Mark Schmidt category:math.OC cs.LG stat.ML published:2014-11-04 summary:This article reviews recent advances in convex optimization algorithms forBig Data, which aim to reduce the computational, storage, and communicationsbottlenecks. We provide an overview of this emerging field, describecontemporary approximation techniques like first-order methods andrandomization for scalability, and survey the important role of parallel anddistributed computation. The new Big Data algorithms are based on surprisinglysimple principles and attain staggering accelerations even on classicalproblems.
arxiv-1411-0778 | Detecting Suicidal Ideation in Chinese Microblogs with Psychological Lexicons |  http://arxiv.org/abs/1411.0778  | author:Xiaolei Huang, Lei Zhang, Tianli Liu, David Chiu, Tingshao Zhu, Xin Li category:cs.CL published:2014-11-04 summary:Suicide is among the leading causes of death in China. However, technicalapproaches toward preventing suicide are challenging and remaining underdevelopment. Recently, several actual suicidal cases were preceded by users whoposted microblogs with suicidal ideation to Sina Weibo, a Chinese social medianetwork akin to Twitter. It would therefore be desirable to detect suicidalideations from microblogs in real-time, and immediately alert appropriatesupport groups, which may lead to successful prevention. In this paper, wepropose a real-time suicidal ideation detection system deployed over Weibo,using machine learning and known psychological techniques. Currently, we haveidentified 53 known suicidal cases who posted suicide notes on Weibo prior totheir deaths.We explore linguistic features of these known cases using apsychological lexicon dictionary, and train an effective suicidal Weibo postdetection model. 6714 tagged posts and several classifiers are used to verifythe model. By combining both machine learning and psychological knowledge, SVMclassifier has the best performance of different classifiers, yielding anF-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.
arxiv-1411-0939 | Simple approximate MAP Inference for Dirichlet processes |  http://arxiv.org/abs/1411.0939  | author:Yordan P. Raykov, Alexis Boukouvalas, Max A. Little category:stat.ML published:2014-11-04 summary:The Dirichlet process mixture (DPM) is a ubiquitous, flexible Bayesiannonparametric statistical model. However, full probabilistic inference in thismodel is analytically intractable, so that computationally intensive techniquessuch as Gibb's sampling are required. As a result, DPM-based methods, whichhave considerable potential, are restricted to applications in whichcomputational resources and time for inference is plentiful. For example, theywould not be practical for digital signal processing on embedded hardware,where computational resources are at a serious premium. Here, we developsimplified yet statistically rigorous approximate maximum a-posteriori (MAP)inference algorithms for DPMs. This algorithm is as simple as K-meansclustering, performs in experiments as well as Gibb's sampling, while requiringonly a fraction of the computational effort. Unlike related small varianceasymptotics, our algorithm is non-degenerate and so inherits the "rich getricher" property of the Dirichlet process. It also retains a non-degenerateclosed-form likelihood which enables standard tools such as cross-validation tobe used. This is a well-posed approximation to the MAP solution of theprobabilistic DPM model.
arxiv-1411-0763 | A Weighted Common Subgraph Matching Algorithm |  http://arxiv.org/abs/1411.0763  | author:Xu Yang, Hong Qiao, Zhi-Yong Liu category:cs.DS cs.CV published:2014-11-04 summary:We propose a weighted common subgraph (WCS) matching algorithm to find themost similar subgraphs in two labeled weighted graphs. WCS matching, as anatural generalization of the equal-sized graph matching or subgraph matching,finds wide applications in many computer vision and machine learning tasks. Inthis paper, the WCS matching is first formulated as a combinatorialoptimization problem over the set of partial permutation matrices. Then it isapproximately solved by a recently proposed combinatorial optimizationframework - Graduated NonConvexity and Concavity Procedure (GNCCP).Experimental comparisons on both synthetic graphs and real world imagesvalidate its robustness against noise level, problem size, outlier number, andedge density.
arxiv-1411-0877 | Vector Autoregressions with Parsimoniously Time Varying Parameters and an Application to Monetary Policy |  http://arxiv.org/abs/1411.0877  | author:Laurent Callot, Johannes Tang Kristensen category:math.ST stat.ML stat.TH 91G70 published:2014-11-04 summary:This paper proposes a parsimoniously time varying parameter vectorautoregressive model (with exogenous variables, VARX) and studies theproperties of the Lasso and adaptive Lasso as estimators of this model. Theparameters of the model are assumed to follow parsimonious random walks, whereparsimony stems from the assumption that increments to the parameters have anon-zero probability of being exactly equal to zero. By varying the degree ofparsimony our model can accommodate constant parameters, an unknown number ofstructural breaks, or parameters with a high degree of variation. We characterize the finite sample properties of the Lasso by deriving upperbounds on the estimation and prediction errors that are valid with highprobability; and asymptotically we show that these bounds tend to zero withprobability tending to one if the number of non zero increments grows slowerthan $\sqrt{T}$. By simulation experiments we investigate the properties of the Lasso and theadaptive Lasso in settings where the parameters are stable, experiencestructural breaks, or follow a parsimonious random walk. We use our model toinvestigate the monetary policy response to inflation and business cyclefluctuations in the US by estimating a parsimoniously time varying parameterTaylor rule. We document substantial changes in the policy response of the Fedin the 1980s and since 2008.
arxiv-1411-0582 | Affective Facial Expression Processing via Simulation: A Probabilistic Model |  http://arxiv.org/abs/1411.0582  | author:Jonathan Vitale, Mary-Anne Williams, Benjamin Johnston, Giuseppe Boccignone category:cs.CV published:2014-11-03 summary:Understanding the mental state of other people is an important skill forintelligent agents and robots to operate within social environments. However,the mental processes involved in `mind-reading' are complex. One explanation ofsuch processes is Simulation Theory - it is supported by a large body ofneuropsychological research. Yet, determining the best computational model ortheory to use in simulation-style emotion detection, is far from beingunderstood. In this work, we use Simulation Theory and neuroscience findings onMirror-Neuron Systems as the basis for a novel computational model, as a way tohandle affective facial expressions. The model is based on a probabilisticmapping of observations from multiple identities onto a single fixed identity(`internal transcoding of external stimuli'), and then onto a latent space(`phenomenological response'). Together with the proposed architecture wepresent some promising preliminary results
arxiv-1411-0588 | On Detecting Noun-Adjective Agreement Errors in Bulgarian Language Using GATE |  http://arxiv.org/abs/1411.0588  | author:Nadezhda Borisova, Grigor Iliev, Elena Karashtranova category:cs.CL published:2014-11-03 summary:In this article, we describe an approach for automatic detection ofnoun-adjective agreement errors in Bulgarian texts by explaining the necessarysteps required to develop a simple Java-based language processing application.For this purpose, we use the GATE language processing framework, which iscapable of analyzing texts in Bulgarian language and can be embedded insoftware applications, accessed through a set of Java APIs. In our exampleapplication we also demonstrate how to use the functionality of GATE to performregular expressions over annotations for detecting agreement errors in simplenoun phrases formed by two words - attributive adjective and a noun, where theattributive adjective precedes the noun. The provided code samples can also beused as a starting point for implementing natural language processingfunctionalities in software applications related to language processing taskslike detection, annotation and retrieval of word groups meeting a specific setof criteria.
arxiv-1411-0541 | Distributed Submodular Maximization |  http://arxiv.org/abs/1411.0541  | author:Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, Andreas Krause category:cs.LG cs.AI cs.DC cs.IR published:2014-11-03 summary:Many large-scale machine learning problems -- clustering, non-parametriclearning, kernel machines, etc. -- require selecting a small yet representativesubset from a large dataset. Such problems can often be reduced to maximizing asubmodular set function subject to various constraints. Classical approaches tosubmodular optimization require centralized access to the full dataset, whichis impractical for truly large-scale problems. In this paper, we consider theproblem of submodular function maximization in a distributed fashion. Wedevelop a simple, two-stage protocol GreeDi, that is easily implemented usingMapReduce style computations. We theoretically analyze our approach, and showthat under certain natural conditions, performance close to the centralizedapproach can be achieved. We begin with monotone submodular maximizationsubject to a cardinality constraint, and then extend this approach to obtainapproximation guarantees for (not necessarily monotone) submodular maximizationsubject to more general constraints including matroid or knapsackconstraints.In our extensive experiments, we demonstrate the effectiveness ofour approach on several applications, including sparse Gaussian processinference and exemplar based clustering on tens of millions of examples usingHadoop.
arxiv-1411-0591 | Bayesian feature selection with strongly-regularizing priors maps to the Ising Model |  http://arxiv.org/abs/1411.0591  | author:Charles K. Fisher, Pankaj Mehta category:cs.LG stat.ML published:2014-11-03 summary:Identifying small subsets of features that are relevant for prediction and/orclassification tasks is a central problem in machine learning and statistics.The feature selection task is especially important, and computationallydifficult, for modern datasets where the number of features can be comparableto, or even exceed, the number of samples. Here, we show that feature selectionwith Bayesian inference takes a universal form and reduces to calculating themagnetizations of an Ising model, under some mild conditions. Our resultsexploit the observation that the evidence takes a universal form forstrongly-regularizing priors --- priors that have a large effect on theposterior probability even in the infinite data limit. We derive explicitexpressions for feature selection for generalized linear models, a large classof statistical techniques that include linear and logistic regression. Weillustrate the power of our approach by analyzing feature selection in alogistic regression-based classifier trained to distinguish between the lettersB and D in the notMNIST dataset.
arxiv-1411-0589 | Modular proximal optimization for multidimensional total-variation regularization |  http://arxiv.org/abs/1411.0589  | author:Álvaro Barbero, Suvrit Sra category:stat.ML math.OC published:2014-11-03 summary:One of the most frequently used notions of "structured sparsity" is that ofsparse (discrete) gradients, a structure typically elicited through\emph{Total-Variation (TV)} regularizers. This paper focuses on anisotropicTV-regularizers, in particular on $\ell_p$-norm \emph{weighted TV regularizers}for which it develops efficient algorithms to compute the correspondingproximity operators. Our algorithms enable one to scalably incorporate TVregularization of vector, matrix, or tensor data into a proximal convexoptimization solvers. For the special case of vectors, we derive and implementa highly efficient weighted 1D-TV solver. This solver provides a backbone forsubsequently handling the more complex task of higher-dimensional (two or more)TV by means of a modular proximal optimization approach. We present numericalexperiments that demonstrate how our 1D-TV solver matches or exceeds the bestknown 1D-TV solvers. Thereafter, we illustrate the benefits of our modulardesign through extensive experiments on: (i) image denoising; (ii) imagedeconvolution; and (iii) four variants of fused-lasso. Our results show theflexibility and speed our TV solvers offer over competing approaches. Tounderscore our claims, we provide our TV solvers in an easy to usemulti-threaded C++ library (which also aids reproducibility of our results).
arxiv-1411-0439 | Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature |  http://arxiv.org/abs/1411.0439  | author:Tom Gunter, Michael A. Osborne, Roman Garnett, Philipp Hennig, Stephen J. Roberts category:stat.ML published:2014-11-03 summary:We propose a novel sampling framework for inference in probabilistic models:an active learning approach that converges more quickly (in wall-clock time)than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge inprobabilistic inference is numerical integration, to average over ensembles ofmodels or unknown (hyper-)parameters (for example to compute the marginallikelihood or a partition function). MCMC has provided approaches to numericalintegration that deliver state-of-the-art inference, but can suffer from sampleinefficiency and poor convergence diagnostics. Bayesian quadrature techniquesoffer a model-based solution to such problems, but their uptake has beenhindered by prohibitive computation costs. We introduce a warped model forprobabilistic integrands (likelihoods) that are known to be non-negative,permitting a cheap active learning scheme to optimally select sample locations.Our algorithm is demonstrated to offer faster convergence (in seconds) relativeto simple Monte Carlo and annealed importance sampling on both synthetic andreal-world examples.
arxiv-1411-0560 | Multivariate response and parsimony for Gaussian cluster-weighted models |  http://arxiv.org/abs/1411.0560  | author:Utkarsh J. Dang, Antonio Punzo, Paul D. McNicholas, Salvatore Ingrassia, Ryan P. Browne category:stat.CO stat.ME stat.ML published:2014-11-03 summary:A family of parsimonious Gaussian cluster-weighted models is presented. Thisfamily concerns a multivariate extension to cluster-weighted modelling that canaccount for correlations between multivariate responses. Parsimony is attainedby constraining parts of an eigen-decomposition imposed on the componentcovariance matrices. A sufficient condition for identifiability is provided andan expectation-maximization algorithm is presented for parameter estimation.Model performance is investigated on both synthetic and classical real datasets and compared with some popular approaches. Finally, accounting for lineardependencies in the presence of a linear regression structure is shown to offerbetter performance, vis-\`{a}-vis clustering, over existing methodologies.
arxiv-1411-0347 | Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares |  http://arxiv.org/abs/1411.0347  | author:Mert Pilanci, Martin J. Wainwright category:math.OC cs.IT cs.LG math.IT stat.ML published:2014-11-03 summary:We study randomized sketching methods for approximately solving least-squaresproblem with a general convex constraint. The quality of a least-squaresapproximation can be assessed in different ways: either in terms of the valueof the quadratic objective function (cost approximation), or in terms of somedistance measure between the approximate minimizer and the true minimizer(solution approximation). Focusing on the latter criterion, our first mainresult provides a general lower bound on any randomized method that sketchesboth the data matrix and vector in a least-squares problem; as a surprisingconsequence, the most widely used least-squares sketch is sub-optimal forsolution approximation. We then present a new method known as the iterativeHessian sketch, and show that it can be used to obtain approximations to theoriginal least-squares problem using a projection dimension proportional to thestatistical complexity of the least-squares minimizer, and a logarithmic numberof iterations. We illustrate our general theory with simulations for bothunconstrained and constrained versions of least-squares, including$\ell_1$-regularization and nuclear norm constraints. We also numericallydemonstrate the practicality of our approach in a real face expressionclassification experiment.
arxiv-1411-0547 | Correlation Clustering with Constrained Cluster Sizes and Extended Weights Bounds |  http://arxiv.org/abs/1411.0547  | author:Gregory J. Puleo, Olgica Milenkovic category:cs.LG cs.DS published:2014-11-03 summary:We consider the problem of correlation clustering on graphs with constraintson both the cluster sizes and the positive and negative weights of edges. Ourcontributions are twofold: First, we introduce the problem of correlationclustering with bounded cluster sizes. Second, we extend the regime of weightvalues for which the clustering may be performed with constant approximationguarantees in polynomial time and apply the results to the bounded cluster sizeproblem.
arxiv-1411-0392 | Sparsity Constrained Graph Regularized NMF for Spectral Unmixing of Hyperspectral Data |  http://arxiv.org/abs/1411.0392  | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2014-11-03 summary:Hyperspectral images contain mixed pixels due to low spatial resolution ofhyperspectral sensors. Mixed pixels are pixels containing more than onedistinct material called endmembers. The presence percentages of endmembers inmixed pixels are called abundance fractions. Spectral unmixing problem refersto decomposing these pixels into a set of endmembers and abundance fractions.Due to nonnegativity constraint on abundance fractions, nonnegative matrixfactorization methods (NMF) have been widely used for solving spectral unmixingproblem. In this paper we have used graph regularized NMF (GNMF) methodcombined with sparseness constraint to decompose mixed pixels in hyperspectralimagery. This method preserves the geometrical structure of data whilerepresenting it in low dimensional space. Adaptive regularization parameterbased on temperature schedule in simulated annealing method also has been usedin this paper for the sparseness term. Proposed algorithm is applied onsynthetic and real datasets. Synthetic data is generated based on endmembersfrom USGS spectral library. AVIRIS Cuprite dataset is used as real dataset forevaluation of proposed method. Results are quantified based on spectral angledistance (SAD) and abundance angle distance (AAD) measures. Results incomparison with other methods show that the proposed method can unmix data moreeffectively. Specifically for the Cuprite dataset, performance of the proposedmethod is approximately 10% better than the VCA and Sparse NMF in terms of rootmean square of SAD.
arxiv-1411-0442 | Non Binary Local Gradient Contours for Face Recognition |  http://arxiv.org/abs/1411.0442  | author:Abdullah Gubbi, Mohammad Fazle Azeem, M Sharmila Kumari category:cs.CV published:2014-11-03 summary:As the features from the traditional Local Binary Patterns (LBP) and LocalDirectional Patterns (LDP) are found to be ineffective for face recognition, wehave proposed a new approach derived on the basis of Information sets wherebythe loss of information that occurs during the binarization is eliminated. Theinformation sets expand the scope of fuzzy sets by connecting the attribute andthe corresponding membership function value as a product. Since face is havingsmooth texture in a limited area, the extracted features must be highlydiscernible. To limit the number of features, we consider only the nonoverlapping windows. By the application of the information set theory we canreduce the number of feature of an image. The derived features are shown towork fairly well over eigenface, fisherface and LBP methods.
arxiv-1411-0728 | A Learning Scheme for Approachability in MDPs and Stackelberg Stochastic Games |  http://arxiv.org/abs/1411.0728  | author:Dileep Kalathil, Vivek Borkar, Rahul Jain category:cs.LG cs.GT cs.SY math.OC published:2014-11-03 summary:The notion of approachability was introduced by Blackwell in the context ofvector-valued repeated games. The famous approachability theorem prescribes astrategy for approachability, i.e., for `steering' the average vector-cost of agiven player towards a given target set, irrespective of the strategies of theother players. In this paper, motivated from the multi-objectiveoptimization/decision making problems in dynamically changing environments, weaddress the approachability problem in Markov Decision Processes (MDPs) andStackelberg stochastic games with vector-valued cost functions. We make twomain contributions. Firstly, we give simple and computationally tractablestrategy for approachability for MDPs and Stackelberg stochastic games.Secondly, we give reinforcement learning algorithms to learn the approachablestrategy when the transition kernel is unknown. We also show that theconditions that we give for approachability are both necessary and sufficientfor convex sets and thus a complete characterization. We also give sufficientconditions for non-convex sets.
arxiv-1411-0707 | A Nonparametric Adaptive Nonlinear Statistical Filter |  http://arxiv.org/abs/1411.0707  | author:Michael Busch, Jeff Moehlis category:stat.ML published:2014-11-03 summary:We use statistical learning methods to construct an adaptive state estimatorfor nonlinear stochastic systems. Optimal state estimation, in the form of aKalman filter, requires knowledge of the system's process and measurementuncertainty. We propose that these uncertainties can be estimated from(conditioned on) past observed data, and without making any assumptions of thesystem's prior distribution. The system's prior distribution at each time stepis constructed from an ensemble of least-squares estimates on sub-sampled setsof the data via jackknife sampling. As new data is acquired, the stateestimates, process uncertainty, and measurement uncertainty are updatedaccordingly, as described in this manuscript.
arxiv-1411-0652 | Clustering memes in social media streams |  http://arxiv.org/abs/1411.0652  | author:Mohsen JafariAsbagh, Emilio Ferrara, Onur Varol, Filippo Menczer, Alessandro Flammini category:cs.SI cs.CY cs.LG physics.soc-ph published:2014-11-03 summary:The problem of clustering content in social media has pervasive applications,including the identification of discussion topics, event detection, and contentrecommendation. Here we describe a streaming framework for online detection andclustering of memes in social media, specifically Twitter. A pre-clusteringprocedure, namely protomeme detection, first isolates atomic tokens ofinformation carried by the tweets. Protomemes are thereafter aggregated, basedon multiple similarity measures, to obtain memes as cohesive groups of tweetsreflecting actual concepts or topics of discussion. The clustering algorithmtakes into account various dimensions of the data and metadata, includingnatural language, the social network, and the patterns of informationdiffusion. As a result, our system can build clusters of semantically,structurally, and topically related tweets. The clustering process is based ona variant of Online K-means that incorporates a memory mechanism, used to"forget" old memes and replace them over time with the new ones. The evaluationof our framework is carried out by using a dataset of Twitter trending topics.Over a one-week period, we systematically determined whether our algorithm wasable to recover the trending hashtags. We show that the proposed methodoutperforms baseline algorithms that only use content features, as well as astate-of-the-art event detection method that assumes full knowledge of theunderlying follower network. We finally show that our online learning frameworkis flexible, due to its independence of the adopted clustering algorithm, andbest suited to work in a streaming scenario.
arxiv-1411-0630 | Active Inference for Binary Symmetric Hidden Markov Models |  http://arxiv.org/abs/1411.0630  | author:Armen E. Allahverdyan, Aram Galstyan category:stat.ML cs.IT cs.LG math.IT published:2014-11-03 summary:We consider active maximum a posteriori (MAP) inference problem for HiddenMarkov Models (HMM), where, given an initial MAP estimate of the hiddensequence, we select to label certain states in the sequence to improve theestimation accuracy of the remaining states. We develop an analytical approachto this problem for the case of binary symmetric HMMs, and obtain a closed formsolution that relates the expected error reduction to model parameters underthe specified active inference scheme. We then use this solution to determinemost optimal active inference scheme in terms of error reduction, and examinethe relation of those schemes to heuristic principles of uncertainty reductionand solution unicity.
arxiv-1411-0602 | Factorbird - a Parameter Server Approach to Distributed Matrix Factorization |  http://arxiv.org/abs/1411.0602  | author:Sebastian Schelter, Venu Satuluri, Reza Zadeh category:cs.LG published:2014-11-03 summary:We present Factorbird, a prototype of a parameter server approach forfactorizing large matrices with Stochastic Gradient Descent-based algorithms.We designed Factorbird to meet the following desiderata: (a) scalability totall and wide matrices with dozens of billions of non-zeros, (b) extensibilityto different kinds of models and loss functions as long as they can beoptimized using Stochastic Gradient Descent (SGD), and (c) adaptability to bothbatch and streaming scenarios. Factorbird uses a parameter server in order toscale to models that exceed the memory of an individual machine, and employslock-free Hogwild!-style learning with a special partitioning scheme todrastically reduce conflicting updates. We also discuss other aspects of thedesign of our system such as how to efficiently grid search for hyperparametersat scale. We present experiments of Factorbird on a matrix built from a subsetof Twitter's interaction graph, consisting of more than 38 billion non-zerosand about 200 million rows and columns, which is to the best of our knowledgethe largest matrix on which factorization results have been reported in theliterature.
arxiv-1411-0217 | Cuckoo Search Inspired Hybridization of the Nelder-Mead Simplex Algorithm Applied to Optimization of Photovoltaic Cells |  http://arxiv.org/abs/1411.0217  | author:Raka Jovanovic, Sabre Kais, Fahhad H. Alharbi category:cs.NE published:2014-11-02 summary:A new hybridization of the Cuckoo Search (CS) is developed and applied tooptimize multi-cell solar systems; namely multi-junction and split spectrumcells. The new approach consists of combining the CS with the Nelder-Meadmethod. More precisely, instead of using single solutions as nests for the CS,we use the concept of a simplex which is used in the Nelder-Mead algorithm.This makes it possible to use the flip operation introduces in the Nelder-Meadalgorithm instead of the Levy flight which is a standard part of the CS. Inthis way, the hybridized algorithm becomes more robust and less sensitive toparameter tuning which exists in CS. The goal of our work was to optimize theperformance of multi-cell solar systems. Although the underlying problemconsists of the minimization of a function of a relatively small number ofparameters, the difficulty comes from the fact that the evaluation of thefunction is complex and only a small number of evaluations is possible. In ourtest, we show that the new method has a better performance when compared tosimilar but more compex hybridizations of Nelder-Mead algorithm using geneticalgorithms or particle swarm optimization on standard benchmark functions.Finally, we show that the new method outperforms some standard meta-heuristicsfor the problem of interest.
arxiv-1411-0247 | Random feedback weights support learning in deep neural networks |  http://arxiv.org/abs/1411.0247  | author:Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, Colin J. Akerman category:q-bio.NC cs.NE published:2014-11-02 summary:The brain processes information through many layers of neurons. This deeparchitecture is representationally powerful, but it complicates learning bymaking it hard to identify the responsible neurons when a mistake is made. Inmachine learning, the backpropagation algorithm assigns blame to a neuron bycomputing exactly how it contributed to an error. To do this, it multiplieserror signals by matrices consisting of all the synaptic weights on theneuron's axon and farther downstream. This operation requires a preciselychoreographed transport of synaptic weight information, which is thought to beimpossible in the brain. Here we present a surprisingly simple algorithm fordeep learning, which assigns blame by multiplying error signals by randomsynaptic weights. We show that a network can learn to extract usefulinformation from signals sent through these random feedback connections. Inessence, the network learns to learn. We demonstrate that this new mechanismperforms as quickly and accurately as backpropagation on a variety of problemsand describe the principles which underlie its function. Our demonstrationprovides a plausible basis for how a neuron can be adapted using error signalsgenerated at distal locations in the brain, and thus dispels long-heldassumptions about the algorithmic constraints on learning in neural circuits.
arxiv-1411-0282 | Noisy Matrix Completion under Sparse Factor Models |  http://arxiv.org/abs/1411.0282  | author:Akshay Soni, Swayambhoo Jain, Jarvis Haupt, Stefano Gonella category:stat.ML cs.IT math.IT stat.AP published:2014-11-02 summary:This paper examines a general class of noisy matrix completion tasks wherethe goal is to estimate a matrix from observations obtained at a subset of itsentries, each of which is subject to random noise or corruption. Our specificfocus is on settings where the matrix to be estimated is well-approximated by aproduct of two (a priori unknown) matrices, one of which is sparse. Suchstructural models - referred to here as "sparse factor models" - have beenwidely used, for example, in subspace clustering applications, as well as incontemporary sparse modeling and dictionary learning tasks. Our maintheoretical contributions are estimation error bounds for sparsity-regularizedmaximum likelihood estimators for problems of this form, which are applicableto a number of different observation noise or corruption models. Severalspecific implications are examined, including scenarios where observations arecorrupted by additive Gaussian noise or additive heavier-tailed (Laplace)noise, Poisson-distributed observations, and highly-quantized (e.g., one-bit)observations. We also propose a simple algorithmic approach based on thealternating direction method of multipliers for these tasks, and provideexperimental evidence to support our error analyses.
arxiv-1411-0288 | A General Framework for Mixed Graphical Models |  http://arxiv.org/abs/1411.0288  | author:Eunho Yang, Pradeep Ravikumar, Genevera I. Allen, Yulia Baker, Ying-Wooi Wan, Zhandong Liu category:math.ST stat.ML stat.TH published:2014-11-02 summary:"Mixed Data" comprising a large number of heterogeneous variables (e.g.count, binary, continuous, skewed continuous, among other data types) areprevalent in varied areas such as genomics and proteomics, imaging genetics,national security, social networking, and Internet advertising. There have beenlimited efforts at statistically modeling such mixed data jointly, in partbecause of the lack of computationally amenable multivariate distributions thatcan capture direct dependencies between such mixed variables of differenttypes. In this paper, we address this by introducing a novel class of BlockDirected Markov Random Fields (BDMRFs). Using the basic building block ofnode-conditional univariate exponential families from Yang et al. (2012), weintroduce a class of mixed conditional random field distributions, that arethen chained according to a block-directed acyclic graph to form our class ofBlock Directed Markov Random Fields (BDMRFs). The Markov independence graphstructure underlying a BDMRF thus has both directed and undirected edges. Weintroduce conditions under which these distributions exist and arenormalizable, study several instances of our models, and propose scalablepenalized conditional likelihood estimators with statistical guarantees forrecovering the underlying network structure. Simulations as well as anapplication to learning mixed genomic networks from next generation sequencingexpression data and mutation data demonstrate the versatility of our methods.
arxiv-1411-0326 | High Dynamic Range Imaging by Perceptual Logarithmic Exposure Merging |  http://arxiv.org/abs/1411.0326  | author:Corneliu Florea, Constantin Vertan, Laura Florea category:cs.CV published:2014-11-02 summary:In this paper we emphasize a similarity between the Logarithmic-Type ImageProcessing (LTIP) model and the Naka-Rushton model of the Human Visual System(HVS). LTIP is a derivation of the Logarithmic Image Processing (LIP), whichfurther replaces the logarithmic function with a ratio of polynomial functions.Based on this similarity, we show that it is possible to present an unifyingframework for the High Dynamic Range (HDR) imaging problem, namely thatperforming exposure merging under the LTIP model is equivalent to standardirradiance map fusion. The resulting HDR algorithm is shown to provide highquality in both subjective and objective evaluations.
arxiv-1411-0254 | Variational Inference for Gaussian Process Modulated Poisson Processes |  http://arxiv.org/abs/1411.0254  | author:Chris Lloyd, Tom Gunter, Michael A. Osborne, Stephen J. Roberts category:stat.ML published:2014-11-02 summary:We present the first fully variational Bayesian inference scheme forcontinuous Gaussian-process-modulated Poisson processes. Such point processesare used in a variety of domains, including neuroscience, geo-statistics andastronomy, but their use is hindered by the computational cost of existinginference schemes. Our scheme: requires no discretisation of the domain; scaleslinearly in the number of observed events; and is many orders of magnitudefaster than previous sampling based approaches. The resulting algorithm isshown to outperform standard methods on synthetic examples, coal miningdisaster data and in the prediction of Malaria incidences in Kenya.
arxiv-1411-0292 | Population Empirical Bayes |  http://arxiv.org/abs/1411.0292  | author:Alp Kucukelbir, David M. Blei category:stat.ML cs.LG published:2014-11-02 summary:Bayesian predictive inference analyzes a dataset to make predictions aboutnew observations. When a model does not match the data, predictive accuracysuffers. We develop population empirical Bayes (POP-EB), a hierarchicalframework that explicitly models the empirical population distribution as partof Bayesian analysis. We introduce a new concept, the latent dataset, as ahierarchical variable and set the empirical population as its prior. This leadsto a new predictive density that mitigates model mismatch. We efficiently applythis method to complex models by proposing a stochastic variational inferencealgorithm, called bumping variational inference (BUMP-VI). We demonstrateimproved predictive accuracy over classical Bayesian inference in three models:a linear regression model of health data, a Bayesian mixture model of naturalimages, and a latent Dirichlet allocation topic model of scientific documents.
arxiv-1411-0296 | Geodesic Exponential Kernels: When Curvature and Linearity Conflict |  http://arxiv.org/abs/1411.0296  | author:Aasa Feragen, Francois Lauze, Søren Hauberg category:cs.LG cs.CV published:2014-11-02 summary:We consider kernel methods on general geodesic metric spaces and provide bothnegative and positive results. First we show that the common Gaussian kernelcan only be generalized to a positive definite kernel on a geodesic metricspace if the space is flat. As a result, for data on a Riemannian manifold, thegeodesic Gaussian kernel is only positive definite if the Riemannian manifoldis Euclidean. This implies that any attempt to design geodesic Gaussian kernelson curved Riemannian manifolds is futile. However, we show that for spaces withconditionally negative definite distances the geodesic Laplacian kernel can begeneralized while retaining positive definiteness. This implies that geodesicLaplacian kernels can be generalized to some curved spaces, including spheresand hyperbolic spaces. Our theoretical results are verified empirically.
arxiv-1411-0306 | Fast Randomized Kernel Methods With Statistical Guarantees |  http://arxiv.org/abs/1411.0306  | author:Ahmed El Alaoui, Michael W. Mahoney category:stat.ML cs.LG stat.CO published:2014-11-02 summary:One approach to improving the running time of kernel-based machine learningmethods is to build a small sketch of the input and use it in lieu of the fullkernel matrix in the machine learning task of interest. Here, we describe aversion of this approach that comes with running time guarantees as well asimproved guarantees on its statistical performance. By extending the notion of\emph{statistical leverage scores} to the setting of kernel ridge regression,our main statistical result is to identify an importance sampling distributionthat reduces the size of the sketch (i.e., the required number of columns to besampled) to the \emph{effective dimensionality} of the problem. This quantityis often much smaller than previous bounds that depend on the \emph{maximaldegrees of freedom}. Our main algorithmic result is to present a fast algorithmto compute approximations to these scores. This algorithm runs in time that islinear in the number of samples---more precisely, the running time is$O(np^2)$, where the parameter $p$ depends only on the trace of the kernelmatrix and the regularization parameter---and it can be applied to the matrixof feature vectors, without having to form the full kernel matrix. This isobtained via a variant of length-squared sampling that we adapt to the kernelsetting in a way that is of independent interest. Lastly, we provide empiricalresults illustrating our theory, and we discuss how this new notion of thestatistical leverage of a data point captures in a fine way the difficulty ofthe original statistical learning problem.
arxiv-1411-0189 | Synchronization Clustering based on a Linearized Version of Vicsek model |  http://arxiv.org/abs/1411.0189  | author:Xinquan Chen category:cs.LG cs.DB published:2014-11-02 summary:This paper presents a kind of effective synchronization clustering methodbased on a linearized version of Vicsek model. This method can be representedby an Effective Synchronization Clustering algorithm (ESynC), an Improvedversion of ESynC algorithm (IESynC), a Shrinking Synchronization Clusteringalgorithm based on another linear Vicsek model (SSynC), and an effectiveMulti-level Synchronization Clustering algorithm (MSynC). After some analysisand comparisions, we find that ESynC algorithm based on the Linearized versionof the Vicsek model has better synchronization effect than SynC algorithm basedon an extensive Kuramoto model and a similar synchronization clusteringalgorithm based on the original Vicsek model. By simulated experiments of someartificial data sets, we observe that ESynC algorithm, IESynC algorithm, andSSynC algorithm can get better synchronization effect although it needs lessiterative times and less time than SynC algorithm. In some simulations, we alsoobserve that IESynC algorithm and SSynC algorithm can get some improvements intime cost than ESynC algorithm. At last, it gives some research expectations topopularize this algorithm.
arxiv-1411-0085 | Complex Events Recognition under Uncertainty in a Sensor Network |  http://arxiv.org/abs/1411.0085  | author:Atul Kanaujia, Tae Eun Choe, Hongli Deng category:cs.CV published:2014-11-01 summary:Automated extraction of semantic information from a network of sensors forcognitive analysis and human-like reasoning is a desired capability in futureground surveillance systems. We tackle the problem of complex decision makingunder uncertainty in network information environment, where lack of effectivevisual processing tools, incomplete domain knowledge frequently causeuncertainty in the visual primitives, leading to sub-optimal decisions. Whilestate-of-the-art vision techniques exist in detecting visual entities (humans,vehicles and scene elements) in an image, a missing functionality is theability to merge the information to reveal meaningful information for highlevel inference. In this work, we develop a probabilistic first order predicatelogic(FOPL) based reasoning system for recognizing complex events insynchronized stream of videos, acquired from sensors with non-overlappingfields of view. We adopt Markov Logic Network(MLN) as a tool to modeluncertainty in observations, and fuse information extracted from heterogeneousdata in a probabilistically consistent way. MLN overcomes strong dependence onpure empirical learning by incorporating domain knowledge, in the form ofuser-defined rules and confidences associated with them. This work demonstratesthat the MLN based decision control system can be made scalable to modelstatistical relations between a variety of entities and over long videosequences. Experiments with real-world data, under a variety of settings,illustrate the mathematical soundness and wide-ranging applicability of ourapproach.
arxiv-1411-0126 | Detection of texts in natural images |  http://arxiv.org/abs/1411.0126  | author:Gowtham Rangarajan Raman category:cs.CV published:2014-11-01 summary:A framework that makes use of Connected components and supervised Supportmachine to recognise texts is proposed. The image is preprocessed and and edgegraph is calculated using a probabilistic framework to compensate forphotometric noise. Connected components over the resultant image is calculated,which is bounded and then pruned using geometric constraints. Finally a GaborFeature based SVM is used to classify the presence of text in the candidates.The proposed method was tested with ICDAR 10 dataset and few other imagesavailable on the internet. It resulted in a recall and precision metric of 0.72and 0.88 comfortably better than the benchmark Eiphstein's algorithm. Theproposed method recorded a 0.70 and 0.74 in natural images which issignificantly better than current methods on natural images. The proposedmethod also scales almost linearly for high resolution, cluttered images.
arxiv-1411-0130 | A Two-phase Decision Support Framework for the Automatic Screening of Digital Fundus Images |  http://arxiv.org/abs/1411.0130  | author:Balint Antal, Andras Hajdu, Zsuzsanna Maros-Szabo, Zsolt Torok, Adrienne Csutak, Tunde Peto category:cs.CV published:2014-11-01 summary:In this paper we give a brief review on the present status of automateddetection systems describe for the screening of diabetic retinopathy. Wefurther detail an enhanced detection procedure that consists of two steps.First, a pre-screening algorithm is considered to classify the input digitalfundus images based on the severity of abnormalities. If an image is found tobe seriously abnormal, it will not be analysed further with robust lesiondetector algorithms. As a further improvement, we introduce a novel featureextraction approach based on clinical observations. The second step of theproposed method detects regions of interest with possible lesions on the imagesthat previously passed the pre-screening step. These regions will serve asinput to the specific lesion detectors for detailed analysis. This procedurecan increase the computational performance of a screening system. Experimentalresults show that both two steps of the proposed approach are capable toefficiently exclude a large amount of data from further processing, thus, todecrease the computational burden of the automatic screening system.
arxiv-1411-0161 | Entropy of Overcomplete Kernel Dictionaries |  http://arxiv.org/abs/1411.0161  | author:Paul Honeine category:cs.IT cs.CV cs.LG cs.NE math.IT stat.ML published:2014-11-01 summary:In signal analysis and synthesis, linear approximation theory considers alinear decomposition of any given signal in a set of atoms, collected into aso-called dictionary. Relevant sparse representations are obtained by relaxingthe orthogonality condition of the atoms, yielding overcomplete dictionarieswith an extended number of atoms. More generally than the linear decomposition,overcomplete kernel dictionaries provide an elegant nonlinear extension bydefining the atoms through a mapping kernel function (e.g., the gaussiankernel). Models based on such kernel dictionaries are used in neural networks,gaussian processes and online learning with kernels. The quality of an overcomplete dictionary is evaluated with a diversitymeasure the distance, the approximation, the coherence and the Babel measures.In this paper, we develop a framework to examine overcomplete kerneldictionaries with the entropy from information theory. Indeed, a higher valueof the entropy is associated to a further uniform spread of the atoms over thespace. For each of the aforementioned diversity measures, we derive lowerbounds on the entropy. Several definitions of the entropy are examined, with anextensive analysis in both the input space and the mapped feature space.
arxiv-1411-0129 | The Latent Structure of Dictionaries |  http://arxiv.org/abs/1411.0129  | author:Philippe Vincent-Lamarre, Alexandre Blondin Massé, Marcos Lopes, Mélanie Lord, Odile Marcotte, Stevan Harnad category:cs.CL cs.IR published:2014-11-01 summary:How many words (and which ones) are sufficient to define all other words?When dictionaries are analyzed as directed graphs with links from definingwords to defined words, they reveal a latent structure. Recursively removingall words that are reachable by definition but that do not define any furtherwords reduces the dictionary to a Kernel of about 10%. This is still not thesmallest number of words that can define all the rest. About 75% of the Kernelturns out to be its Core, a Strongly Connected Subset of words with adefinitional path to and from any pair of its words and no word's definitiondepending on a word outside the set. But the Core cannot define all the rest ofthe dictionary. The 25% of the Kernel surrounding the Core consists of smallstrongly connected subsets of words: the Satellites. The size of the smallestset of words that can define all the rest (the graph's Minimum Feedback VertexSet or MinSet) is about 1% of the dictionary, 15% of the Kernel, and half-Core,half-Satellite. But every dictionary has a huge number of MinSets. The Corewords are learned earlier, more frequent, and less concrete than theSatellites, which in turn are learned earlier and more frequent but moreconcrete than the rest of the Dictionary. In principle, only one MinSet's wordswould need to be grounded through the sensorimotor capacity to recognize andcategorize their referents. In a dual-code sensorimotor-symbolic model of themental lexicon, the symbolic code could do all the rest via re-combinatorydefinition.
arxiv-1411-0073 | Learning Mixed Multinomial Logit Model from Ordinal Data |  http://arxiv.org/abs/1411.0073  | author:Sewoong Oh, Devavrat Shah category:stat.ML published:2014-11-01 summary:Motivated by generating personalized recommendations using ordinal (orpreference) data, we study the question of learning a mixture of MultiNomialLogit (MNL) model, a parameterized class of distributions over permutations,from partial ordinal or preference data (e.g. pair-wise comparisons). Despiteits long standing importance across disciplines including social choice,operations research and revenue management, little is known about thisquestion. In case of single MNL models (no mixture), computationally andstatistically tractable learning from pair-wise comparisons is feasible.However, even learning mixture with two MNL components is infeasible ingeneral. Given this state of affairs, we seek conditions under which it is feasible tolearn the mixture model in both computationally and statistically efficientmanner. We present a sufficient condition as well as an efficient algorithm forlearning mixed MNL models from partial preferences/comparisons data. Inparticular, a mixture of $r$ MNL components over $n$ objects can be learntusing samples whose size scales polynomially in $n$ and $r$ (concretely,$r^{3.5}n^3(log n)^4$, with $r\ll n^{2/7}$ when the model parameters aresufficiently incoherent). The algorithm has two phases: first, learn thepair-wise marginals for each component using tensor decomposition; second,learn the model parameters for each component using Rank Centrality introducedby Negahban et al. In the process of proving these results, we obtain ageneralization of existing analysis for tensor decomposition to a morerealistic regime where only partial information about each sample is available.
arxiv-1411-0169 | Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms |  http://arxiv.org/abs/1411.0169  | author:Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun category:cs.LG cs.DS math.ST stat.TH published:2014-11-01 summary:Let $p$ be an unknown and arbitrary probability distribution over $[0,1)$. Weconsider the problem of {\em density estimation}, in which a learning algorithmis given i.i.d. draws from $p$ and must (with high probability) output ahypothesis distribution that is close to $p$. The main contribution of thispaper is a highly efficient density estimation algorithm for learning using avariable-width histogram, i.e., a hypothesis distribution with a piecewiseconstant probability density function. In more detail, for any $k$ and $\epsilon$, we give an algorithm that makes$\tilde{O}(k/\epsilon^2)$ draws from $p$, runs in $\tilde{O}(k/\epsilon^2)$time, and outputs a hypothesis distribution $h$ that is piecewise constant with$O(k \log^2(1/\epsilon))$ pieces. With high probability the hypothesis $h$satisfies $d_{\mathrm{TV}}(p,h) \leq C \cdot \mathrm{opt}_k(p) + \epsilon$,where $d_{\mathrm{TV}}$ denotes the total variation distance (statisticaldistance), $C$ is a universal constant, and $\mathrm{opt}_k(p)$ is the smallesttotal variation distance between $p$ and any $k$-piecewise constantdistribution. The sample size and running time of our algorithm are optimal upto logarithmic factors. The "approximation factor" $C$ in our result isinherent in the problem, as we prove that no algorithm with sample size boundedin terms of $k$ and $\epsilon$ can achieve $C<2$ regardless of what kind ofhypothesis distribution it uses.
arxiv-1410-8620 | A Comparison of learning algorithms on the Arcade Learning Environment |  http://arxiv.org/abs/1410.8620  | author:Aaron Defazio, Thore Graepel category:cs.LG cs.AI published:2014-10-31 summary:Reinforcement learning agents have traditionally been evaluated on small toyproblems. With advances in computing power and the advent of the ArcadeLearning Environment, it is now possible to evaluate algorithms on diverse anddifficult problems within a consistent framework. We discuss some challengesposed by the arcade learning environment which do not manifest in simplerenvironments. We then provide a comparison of model-free, linear learningalgorithms on this challenging problem set.
arxiv-1410-8623 | Addressing the non-functional requirements of computer vision systems: A case study |  http://arxiv.org/abs/1410.8623  | author:Shannon Fenn, Alexandre Mendes, David Budden category:cs.CV cs.RO cs.SE published:2014-10-31 summary:Computer vision plays a major role in the robotics industry, where visiondata is frequently used for navigation and high-level decision making. Althoughthere is significant research in algorithms and functional requirements, thereis a comparative lack of emphasis on how best to map these abstract conceptsonto an appropriate software architecture. In this study, we distinguish between the functional and non-functionalrequirements of a computer vision system. Using a RoboCup humanoid robot systemas a case study, we propose and develop a software architecture that fulfillsthe latter criteria. The modifiability of the proposed architecture is demonstrated by detailing anumber of feature detection algorithms and emphasizing which aspects of theunderlying framework were modified to support their integration. To demonstrateportability, we port our vision system (designed for an application-specificDARwIn-OP humanoid robot) to a general-purpose, Raspberry Pi computer. Weevaluate performance on both platforms and compare them to a vision systemoptimised for functional requirements only. The architecture and implementation presented in this study provide a highlygeneralisable framework for computer vision system design that is of particularbenefit in research and development, competition and other environments inwhich rapid system evolution is necessary.
arxiv-1410-8668 | Experiments to Improve Named Entity Recognition on Turkish Tweets |  http://arxiv.org/abs/1410.8668  | author:Dilek Küçük, Ralf Steinberger category:cs.CL published:2014-10-31 summary:Social media texts are significant information sources for severalapplication areas including trend analysis, event monitoring, and opinionmining. Unfortunately, existing solutions for tasks such as named entityrecognition that perform well on formal texts usually perform poorly whenapplied to social media texts. In this paper, we report on experiments thathave the purpose of improving named entity recognition on Turkish tweets, usingtwo different annotated data sets. In these experiments, starting with abaseline named entity recognition system, we adapt its recognition rules andresources to better fit Twitter language by relaxing its capitalizationconstraint and by diacritics-based expansion of its lexical resources, and weemploy a simplistic normalization scheme on tweets to observe the effects ofthese on the overall named entity recognition performance on Turkish tweets.The evaluation results of the system with these different settings are providedwith discussions of these results.
arxiv-1410-8675 | Partition-wise Linear Models |  http://arxiv.org/abs/1410.8675  | author:Hidekazu Oiwa, Ryohei Fujimaki category:stat.ML cs.LG published:2014-10-31 summary:Region-specific linear models are widely used in practical applicationsbecause of their non-linear but highly interpretable model representations. Oneof the key challenges in their use is non-convexity in simultaneousoptimization of regions and region-specific models. This paper proposes novelconvex region-specific linear models, which we refer to as partition-wiselinear models. Our key ideas are 1) assigning linear models not to regions butto partitions (region-specifiers) and representing region-specific linearmodels by linear combinations of partition-specific models, and 2) optimizingregions via partition selection from a large number of given partitioncandidates by means of convex structured regularizations. In addition toproviding initialization-free globally-optimal solutions, our convexformulation makes it possible to derive a generalization bound and to use suchadvanced optimization techniques as proximal methods and decomposition of theproximal maps for sparsity-inducing regularizations. Experimental resultsdemonstrate that our partition-wise linear models perform better than or are atleast competitive with state-of-the-art region-specific or locally linearmodels.
arxiv-1410-8618 | Symmetric low-rank representation for subspace clustering |  http://arxiv.org/abs/1410.8618  | author:Jie Chen, Haixian Zhang, Hua Mao, Yongsheng Sang, Zhang Yi category:cs.CV published:2014-10-31 summary:We propose a symmetric low-rank representation (SLRR) method for subspaceclustering, which assumes that a data set is approximately drawn from the unionof multiple subspaces. The proposed technique can reveal the membership ofmultiple subspaces through the self-expressiveness property of the data. Inparticular, the SLRR method considers a collaborative representation combinedwith low-rank matrix recovery techniques as a low-rank representation to learna symmetric low-rank representation, which preserves the subspace structures ofhigh-dimensional data. In contrast to performing iterative singular valuedecomposition in some existing low-rank representation based algorithms, thesymmetric low-rank representation in the SLRR method can be calculated as aclosed form solution by solving the symmetric low-rank optimization problem. Bymaking use of the angular information of the principal directions of thesymmetric low-rank representation, an affinity graph matrix is constructed forspectral clustering. Extensive experimental results show that it outperformsstate-of-the-art subspace clustering algorithms.
arxiv-1411-0022 | Generalized Adaptive Dictionary Learning via Domain Shift Minimization |  http://arxiv.org/abs/1411.0022  | author:Varun Panaganti category:cs.CV published:2014-10-31 summary:Visual data driven dictionaries have been successfully employed for variousobject recognition and classification tasks. However, the task becomes morechallenging if the training and test data are from contrasting domains. In thispaper, we propose a novel and generalized approach towards learning an adaptiveand common dictionary for multiple domains. Precisely, we project the data fromdifferent domains onto a low dimensional space while preserving the intrinsicstructure of data from each domain. We also minimize the domain-shift among thedata from each pair of domains. Simultaneously, we learn a common adaptivedictionary. Our algorithm can also be modified to learn class-specificdictionaries which can be used for classification. We additionally propose adiscriminative manifold regularization which imposes the intrinsic structure ofclass specific features onto the sparse coefficients. Experiments on imageclassification show that our approach fares better compared to the existingstate-of-the-art methods.
arxiv-1411-0030 | A* Sampling |  http://arxiv.org/abs/1411.0030  | author:Chris J. Maddison, Daniel Tarlow, Tom Minka category:stat.CO stat.ML published:2014-10-31 summary:The problem of drawing samples from a discrete distribution can be convertedinto a discrete optimization problem. In this work, we show how sampling from acontinuous distribution can be converted into an optimization problem overcontinuous space. Central to the method is a stochastic process recentlydescribed in mathematical statistics that we call the Gumbel process. Wepresent a new construction of the Gumbel process and A* sampling, a practicalgeneric sampling algorithm that searches for the maximum of a Gumbel processusing A* search. We analyze the correctness and convergence time of A* samplingand demonstrate empirically that it makes more efficient use of bound andlikelihood evaluations than the most closely related adaptive rejectionsampling-based algorithms.
arxiv-1411-0007 | Rapid Adaptation of POS Tagging for Domain Specific Uses |  http://arxiv.org/abs/1411.0007  | author:John E. Miller, Michael Bloodgood, Manabu Torii, K. Vijay-Shanker category:cs.CL cs.LG stat.ML published:2014-10-31 summary:Part-of-speech (POS) tagging is a fundamental component for performingnatural language tasks such as parsing, information extraction, and questionanswering. When POS taggers are trained in one domain and applied insignificantly different domains, their performance can degrade dramatically. Wepresent a methodology for rapid adaptation of POS taggers to new domains. Ourtechnique is unsupervised in that a manually annotated corpus for the newdomain is not necessary. We use suffix information gathered from large amountsof raw text as well as orthographic information to increase the lexicalcoverage. We present an experiment in the Biological domain where our POStagger achieves results comparable to POS taggers specifically trained to thisdomain.
arxiv-1411-6591 | A Latent Source Model for Online Collaborative Filtering |  http://arxiv.org/abs/1411.6591  | author:Guy Bresler, George H. Chen, Devavrat Shah category:cs.LG cs.IR stat.ML published:2014-10-31 summary:Despite the prevalence of collaborative filtering in recommendation systems,there has been little theoretical development on why and how well it works,especially in the "online" setting, where items are recommended to users overtime. We address this theoretical gap by introducing a model for onlinerecommendation systems, cast item recommendation under the model as a learningproblem, and analyze the performance of a cosine-similarity collaborativefiltering method. In our model, each of $n$ users either likes or dislikes eachof $m$ items. We assume there to be $k$ types of users, and all the users of agiven type share a common string of probabilities determining the chance ofliking each item. At each time step, we recommend an item to each user, where akey distinction from related bandit literature is that once a user consumes anitem (e.g., watches a movie), then that item cannot be recommended to the sameuser again. The goal is to maximize the number of likable items recommended tousers over time. Our main result establishes that after nearly $\log(km)$initial learning time steps, a simple collaborative filtering algorithmachieves essentially optimal performance without knowing $k$. The algorithm hasan exploitation step that uses cosine similarity and two types of explorationsteps, one to explore the space of items (standard in the literature) and theother to explore similarity between users (novel to this work).
arxiv-1410-8864 | Greedy Subspace Clustering |  http://arxiv.org/abs/1410.8864  | author:Dohyung Park, Constantine Caramanis, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT published:2014-10-31 summary:We consider the problem of subspace clustering: given points that lie on ornear the union of many low-dimensional linear subspaces, recover the subspaces.To this end, one first identifies sets of points close to the same subspace anduses the sets to estimate the subspaces. As the geometric structure of theclusters (linear subspaces) forbids proper performance of general distancebased approaches such as K-means, many model-specific methods have beenproposed. In this paper, we provide new simple and efficient algorithms forthis problem. Our statistical analysis shows that the algorithms are guaranteedexact (perfect) clustering performance under certain conditions on the numberof points and the affinity between subspaces. These conditions are weaker thanthose considered in the standard statistical literature. Experimental resultson synthetic data generated from the standard unions of subspaces modeldemonstrate our theory. We also show that our algorithm performs competitivelyagainst state-of-the-art algorithms on real-world applications such as motionsegmentation and face clustering, with much simpler implementation and lowercomputational cost.
arxiv-1411-0023 | Validation of Matching |  http://arxiv.org/abs/1411.0023  | author:Ya Le, Eric Bax, Nicola Barbieri, David Garcia Soriano, Jitesh Mehta, James Li category:cs.LG stat.ML published:2014-10-31 summary:We introduce a technique to compute probably approximately correct (PAC)bounds on precision and recall for matching algorithms. The bounds require someverified matches, but those matches may be used to develop the algorithms. Thebounds can be applied to network reconciliation or entity resolutionalgorithms, which identify nodes in different networks or values in a data setthat correspond to the same entity. For network reconciliation, the bounds donot require knowledge of the network generation process.
arxiv-1410-8783 | Supervised learning model for parsing Arabic language |  http://arxiv.org/abs/1410.8783  | author:Nabil Khoufi, Chafik Aloulou, Lamia Hadrich Belguith category:cs.CL cs.LG I.2.7 published:2014-10-31 summary:Parsing the Arabic language is a difficult task given the specificities ofthis language and given the scarcity of digital resources (grammars andannotated corpora). In this paper, we suggest a method for Arabic parsing basedon supervised machine learning. We used the SVMs algorithm to select thesyntactic labels of the sentence. Furthermore, we evaluated our parserfollowing the cross validation method by using the Penn Arabic Treebank. Theobtained results are very encouraging.
arxiv-1410-8750 | Learning Mixtures of Ranking Models |  http://arxiv.org/abs/1410.8750  | author:Pranjal Awasthi, Avrim Blum, Or Sheffet, Aravindan Vijayaraghavan category:cs.LG published:2014-10-31 summary:This work concerns learning probabilistic models for ranking data in aheterogeneous population. The specific problem we study is learning theparameters of a Mallows Mixture Model. Despite being widely studied, currentheuristics for this problem do not have theoretical guarantees and can getstuck in bad local optima. We present the first polynomial time algorithm whichprovably learns the parameters of a mixture of two Mallows models. A keycomponent of our algorithm is a novel use of tensor decomposition techniques tolearn the top-k prefix in both the rankings. Before this work, even thequestion of identifiability in the case of a mixture of two Mallows models wasunresolved.
arxiv-1410-8546 | A Solution for Multi-Alignment by Transformation Synchronisation |  http://arxiv.org/abs/1410.8546  | author:Florian Bernard, Johan Thunberg, Peter Gemmar, Frank Hertel, Andreas Husch, Jorge Goncalves category:cs.CV stat.ML published:2014-10-30 summary:The alignment of a set of objects by means of transformations plays animportant role in computer vision. Whilst the case for only two objects can besolved globally, when multiple objects are considered usually iterative methodsare used. In practice the iterative methods perform well if the relativetransformations between any pair of objects are free of noise. However, if onlynoisy relative transformations are available (e.g. due to missing data or wrongcorrespondences) the iterative methods may fail. Based on the observation that the underlying noise-free transformations canbe retrieved from the null space of a matrix that can directly be obtained frompairwise alignments, this paper presents a novel method for the synchronisationof pairwise transformations such that they are transitively consistent. Simulations demonstrate that for noisy transformations, a large proportion ofmissing data and even for wrong correspondence assignments the method deliversencouraging results.
arxiv-1410-8275 | Stable Autoencoding: A Flexible Framework for Regularized Low-Rank Matrix Estimation |  http://arxiv.org/abs/1410.8275  | author:Julie Josse, Stefan Wager category:stat.ME cs.LG stat.ML published:2014-10-30 summary:We develop a framework for low-rank matrix estimation that allows us totransform noise models into regularization schemes via a simple parametricbootstrap. Effectively, our procedure seeks an autoencoding basis for theobserved matrix that is robust with respect to the specified noise model. Inthe simplest case, with an isotropic noise model, our procedure is equivalentto a classical singular value shrinkage estimator. For non-isotropic noisemodels, however, our method does not reduce to singular value shrinkage, andinstead yields new estimators that perform well in experiments. Moreover, byiterating our stable autoencoding scheme, we can automatically generatelow-rank estimates without specifying the target rank as a tuning parameter.
arxiv-1410-8516 | NICE: Non-linear Independent Components Estimation |  http://arxiv.org/abs/1410.8516  | author:Laurent Dinh, David Krueger, Yoshua Bengio category:cs.LG published:2014-10-30 summary:We propose a deep learning framework for modeling complex high-dimensionaldensities called Non-linear Independent Component Estimation (NICE). It isbased on the idea that a good representation is one in which the data has adistribution that is easy to model. For this purpose, a non-lineardeterministic transformation of the data is learned that maps it to a latentspace so as to make the transformed data conform to a factorized distribution,i.e., resulting in independent latent variables. We parametrize thistransformation so that computing the Jacobian determinant and inverse transformis trivial, yet we maintain the ability to learn complex non-lineartransformations, via a composition of simple building blocks, each based on adeep neural network. The training criterion is simply the exact log-likelihood,which is tractable. Unbiased ancestral sampling is also easy. We show that thisapproach yields good generative models on four image datasets and can be usedfor inpainting.
arxiv-1410-8498 | Training for Fast Sequential Prediction Using Dynamic Feature Selection |  http://arxiv.org/abs/1410.8498  | author:Emma Strubell, Luke Vilnis, Andrew McCallum category:cs.CL cs.AI published:2014-10-30 summary:We present paired learning and inference algorithms for significantlyreducing computation and increasing speed of the vector dot products in theclassifiers that are at the heart of many NLP components. This is accomplishedby partitioning the features into a sequence of templates which are orderedsuch that high confidence can often be reached using only a small fraction ofall features. Parameter estimation is arranged to maximize accuracy and earlyconfidence in this sequence. We present experiments in left-to-rightpart-of-speech tagging on WSJ, demonstrating that we can preserve accuracyabove 97% with over a five-fold reduction in run-time.
arxiv-1410-8586 | DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks |  http://arxiv.org/abs/1410.8586  | author:Tao Chen, Damian Borth, Trevor Darrell, Shih-Fu Chang category:cs.CV cs.LG cs.MM cs.NE H.3.3 published:2014-10-30 summary:This paper introduces a visual sentiment concept classification method basedon deep convolutional neural networks (CNNs). The visual sentiment concepts areadjective noun pairs (ANPs) automatically discovered from the tags of webphotos, and can be utilized as effective statistical cues for detectingemotions depicted in the images. Nearly one million Flickr images tagged withthese ANPs are downloaded to train the classifiers of the concepts. We adoptthe popular model of deep convolutional neural networks which recently showsgreat performance improvement on classifying large-scale web-based imagedataset such as ImageNet. Our deep CNNs model is trained based on Caffe, anewly developed deep learning framework. To deal with the biased training datawhich only contains images with strong sentiment and to prevent overfitting, weinitialize the model with the model weights trained from ImageNet. Performanceevaluation shows the newly trained deep CNNs model SentiBank 2.0 (or calledDeepSentiBank) is significantly improved in both annotation accuracy andretrieval performance, compared to its predecessors which mainly use binary SVMclassification models.
arxiv-1410-8581 | Semi-Automatic Construction of a Domain Ontology for Wind Energy Using Wikipedia Articles |  http://arxiv.org/abs/1410.8581  | author:Dilek Küçük, Yusuf Arslan category:cs.CL cs.CE published:2014-10-30 summary:Domain ontologies are important information sources for knowledge-basedsystems. Yet, building domain ontologies from scratch is known to be a verylabor-intensive process. In this study, we present our semi-automatic approachto building an ontology for the domain of wind energy which is an importanttype of renewable energy with a growing share in electricity generation allover the world. Related Wikipedia articles are first processed in an automatedmanner to determine the basic concepts of the domain together with theirproperties and next the concepts, properties, and relationships are organizedto arrive at the ultimate ontology. We also provide pointers to otherengineering ontologies which could be utilized together with the proposed windenergy ontology in addition to its prospective application areas. The currentstudy is significant as, to the best of our knowledge, it proposes the firstconsiderably wide-coverage ontology for the wind energy domain and the ontologyis built through a semi-automatic process which makes use of the related Webresources, thereby reducing the overall cost of the ontology building process.
arxiv-1410-8580 | An Online Algorithm for Learning Selectivity to Mixture Means |  http://arxiv.org/abs/1410.8580  | author:Matthew Lawlor, Steven Zucker category:q-bio.NC cs.LG published:2014-10-30 summary:We develop a biologically-plausible learning rule called Triplet BCM thatprovably converges to the class means of general mixture models. This rulegeneralizes the classical BCM neural rule, and provides a novel interpretationof classical BCM as performing a kind of tensor decomposition. It achieves asubstantial generalization over classical BCM by incorporating triplets ofsamples from the mixtures, which provides a novel information processinginterpretation to spike-timing-dependent plasticity. We provide complete proofsof convergence of this learning rule, and an extended discussion of theconnection between BCM and tensor learning.
arxiv-1410-8577 | An Ensemble-based System for Microaneurysm Detection and Diabetic Retinopathy Grading |  http://arxiv.org/abs/1410.8577  | author:Balint Antal, Andras Hajdu category:cs.CV cs.AI stat.AP stat.ML published:2014-10-30 summary:Reliable microaneurysm detection in digital fundus images is still an openissue in medical image processing. We propose an ensemble-based framework toimprove microaneurysm detection. Unlike the well-known approach of consideringthe output of multiple classifiers, we propose a combination of internalcomponents of microaneurysm detectors, namely preprocessing methods andcandidate extractors. We have evaluated our approach for microaneurysmdetection in an online competition, where this algorithm is currently ranked asfirst and also on two other databases. Since microaneurysm detection isdecisive in diabetic retinopathy grading, we also tested the proposed methodfor this task on the publicly available Messidor database, where a promisingAUC 0.90 with 0.01 uncertainty is achieved in a 'DR/non-DR'-type classificationbased on the presence or absence of the microaneurysms.
arxiv-1410-8576 | An ensemble-based system for automatic screening of diabetic retinopathy |  http://arxiv.org/abs/1410.8576  | author:Balint Antal, Andras Hajdu category:cs.CV cs.LG stat.AP stat.ML published:2014-10-30 summary:In this paper, an ensemble-based method for the screening of diabeticretinopathy (DR) is proposed. This approach is based on features extracted fromthe output of several retinal image processing algorithms, such as image-level(quality assessment, pre-screening, AM/FM), lesion-specific (microaneurysms,exudates) and anatomical (macula, optic disc) components. The actual decisionabout the presence of the disease is then made by an ensemble of machinelearning classifiers. We have tested our approach on the publicly availableMessidor database, where 90% sensitivity, 91% specificity and 90% accuracy and0.989 AUC are achieved in a disease/no-disease setting. These results arehighly competitive in this field and suggest that retinal image processing is avalid approach for automatic DR screening.
arxiv-1410-8553 | A random forest system combination approach for error detection in digital dictionaries |  http://arxiv.org/abs/1410.8553  | author:Michael Bloodgood, Peng Ye, Paul Rodrigues, David Zajic, David Doermann category:cs.CL cs.LG stat.ML published:2014-10-30 summary:When digitizing a print bilingual dictionary, whether via optical characterrecognition or manual entry, it is inevitable that errors are introduced intothe electronic version that is created. We investigate automating the processof detecting errors in an XML representation of a digitized print dictionaryusing a hybrid approach that combines rule-based, feature-based, and languagemodel-based methods. We investigate combining methods and show that usingrandom forests is a promising approach. We find that in isolation, unsupervisedmethods rival the performance of supervised methods. Random forests typicallyrequire training data so we investigate how we can apply random forests tocombine individual base methods that are themselves unsupervised withoutrequiring large amounts of training data. Experiments reveal empirically that arelatively small amount of data is sufficient and can potentially be furtherreduced through specific selection criteria.
arxiv-1410-8749 | What a Nasty day: Exploring Mood-Weather Relationship from Twitter |  http://arxiv.org/abs/1410.8749  | author:Jiwei Li, Xun Wang, Eduard Hovy category:cs.SI cs.CL published:2014-10-30 summary:While it has long been believed in psychology that weather somehow influenceshuman's mood, the debates have been going on for decades about how they arecorrelated. In this paper, we try to study this long-lasting topic byharnessing a new source of data compared from traditional psychologicalresearches: Twitter. We analyze 2 years' twitter data collected by twitter APIwhich amounts to $10\%$ of all postings and try to reveal the correlationsbetween multiple dimensional structure of human mood with meteorologicaleffects. Some of our findings confirm existing hypotheses, while otherscontradict them. We are hopeful that our approach, along with the new datasource, can shed on the long-going debates on weather-mood correlation.
arxiv-1410-8420 | Learning circuits with few negations |  http://arxiv.org/abs/1410.8420  | author:Eric Blais, Clément L. Canonne, Igor C. Oliveira, Rocco A. Servedio, Li-Yang Tan category:cs.CC cs.DM cs.LG published:2014-10-30 summary:Monotone Boolean functions, and the monotone Boolean circuits that computethem, have been intensively studied in complexity theory. In this paper westudy the structure of Boolean functions in terms of the minimum number ofnegations in any circuit computing them, a complexity measure that interpolatesbetween monotone functions and the class of all functions. We study thisgeneralization of monotonicity from the vantage point of learning theory,giving near-matching upper and lower bounds on the uniform-distributionlearnability of circuits in terms of the number of negations they contain. Ourupper bounds are based on a new structural characterization of negation-limitedcircuits that extends a classical result of A. A. Markov. Our lower bounds,which employ Fourier-analytic tools from hardness amplification, give newresults even for circuits with no negations (i.e. monotone functions).
arxiv-1410-8372 | On Estimating $L_2^2$ Divergence |  http://arxiv.org/abs/1410.8372  | author:Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, Larry Wasserman category:stat.ML published:2014-10-30 summary:We give a comprehensive theoretical characterization of a nonparametricestimator for the $L_2^2$ divergence between two continuous distributions. Wefirst bound the rate of convergence of our estimator, showing that it is$\sqrt{n}$-consistent provided the densities are sufficiently smooth. In thissmooth regime, we then show that our estimator is asymptotically normal,construct asymptotic confidence intervals, and establish a Berry-Ess\'{e}enstyle inequality characterizing the rate of convergence to normality. We alsoshow that this estimator is minimax optimal.
arxiv-1410-8206 | Addressing the Rare Word Problem in Neural Machine Translation |  http://arxiv.org/abs/1410.8206  | author:Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, Wojciech Zaremba category:cs.CL cs.LG cs.NE published:2014-10-30 summary:Neural Machine Translation (NMT) is a new approach to machine translationthat has shown promising results that are comparable to traditional approaches.A significant weakness in conventional NMT systems is their inability tocorrectly translate very rare words: end-to-end NMTs tend to have relativelysmall vocabularies with a single unk symbol that represents every possibleout-of-vocabulary (OOV) word. In this paper, we propose and implement aneffective technique to address this problem. We train an NMT system on datathat is augmented by the output of a word alignment algorithm, allowing the NMTsystem to emit, for each OOV word in the target sentence, the position of itscorresponding word in the source sentence. This information is later utilizedin a post-processing step that translates every OOV word using a dictionary.Our experiments on the WMT14 English to French translation task show that thismethod provides a substantial improvement of up to 2.8 BLEU points over anequivalent NMT system that does not use this technique. With 37.5 BLEU points,our NMT system is the first to surpass the best result achieved on a WMT14contest task.
arxiv-1410-8326 | Towards Learning Object Affordance Priors from Technical Texts |  http://arxiv.org/abs/1410.8326  | author:Nicholas H. Kirk category:cs.LG cs.AI cs.CL cs.RO 68T05 published:2014-10-30 summary:Everyday activities performed by artificial assistants can potentially beexecuted naively and dangerously given their lack of common sense knowledge.This paper presents conceptual work towards obtaining prior knowledge on theusual modality (passive or active) of any given entity, and their affordanceestimates, by extracting high-confidence ability modality semantic relations (Xcan Y relationship) from non-figurative texts, by analyzing co-occurrence ofgrammatical instances of subjects and verbs, and verbs and objects. Thediscussion includes an outline of the concept, potential and limitations, andpossible feature and learning framework adoption.
arxiv-1412-6141 | Efficient Decision-Making by Volume-Conserving Physical Object |  http://arxiv.org/abs/1412.6141  | author:Song-Ju Kim, Masashi Aono, Etsushi Nameda category:cs.AI cs.LG nlin.AO published:2014-10-30 summary:We demonstrate that any physical object, as long as its volume is conservedwhen coupled with suitable operations, provides a sophisticated decision-makingcapability. We consider the problem of finding, as accurately and quickly aspossible, the most profitable option from a set of options that givesstochastic rewards. These decisions are made as dictated by a physical object,which is moved in a manner similar to the fluctuations of a rigid body in atug-of-war game. Our analytical calculations validate statistical reasons whyour method exhibits higher efficiency than conventional algorithms.
arxiv-1411-0024 | Robust sketching for multiple square-root LASSO problems |  http://arxiv.org/abs/1411.0024  | author:Vu Pham, Laurent El Ghaoui, Arturo Fernandez category:math.OC cs.LG cs.SY stat.ML published:2014-10-30 summary:Many learning tasks, such as cross-validation, parameter search, orleave-one-out analysis, involve multiple instances of similar problems, eachinstance sharing a large part of learning data with the others. We introduce arobust framework for solving multiple square-root LASSO problems, based on asketch of the learning data that uses low-rank approximations. Our approachallows a dramatic reduction in computational effort, in effect reducing thenumber of observations from $m$ (the number of observations to start with) to$k$ (the number of singular values retained in the low-rank model), while notsacrificing---sometimes even improving---the statistical performance.Theoretical analysis, as well as numerical experiments on both synthetic andreal data, illustrate the efficiency of the method in large scale applications.
arxiv-1410-8251 | Notes on Noise Contrastive Estimation and Negative Sampling |  http://arxiv.org/abs/1410.8251  | author:Chris Dyer category:cs.LG published:2014-10-30 summary:Estimating the parameters of probabilistic models of language such as maxentmodels and probabilistic neural models is computationally difficult since itinvolves evaluating partition functions by summing over an entire vocabulary,which may be millions of word types in size. Two closely relatedstrategies---noise contrastive estimation (Mnih and Teh, 2012; Mnih andKavukcuoglu, 2013; Vaswani et al., 2013) and negative sampling (Mikolov et al.,2012; Goldberg and Levy, 2014)---have emerged as popular solutions to thiscomputational problem, but some confusion remains as to which is moreappropriate and when. This document explicates their relationships to eachother and to other estimation techniques. The analysis shows that, althoughthey are superficially similar, NCE is a general parameter estimation techniquethat is asymptotically unbiased, while negative sampling is best understood asa family of binary classification models that are useful for learning wordrepresentations but not as a general-purpose estimator.
arxiv-1410-8229 | Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse Recovery and the Grouping Effect |  http://arxiv.org/abs/1410.8229  | author:Mehmet Eren Ahsen, Mathukumalli Vidyasagar category:stat.ML 90C25 published:2014-10-30 summary:In this paper we introduce a new optimization formulation for sparseregression and compressed sensing, called CLOT (Combined L-One and Two),wherein the regularizer is a convex combination of the $\ell_1$- and$\ell_2$-norms. This formulation differs from the Elastic Net (EN) formulation,in which the regularizer is a convex combination of the $\ell_1$- and$\ell_2$-norm squared. This seemingly simple modification has fairlysignificant consequences. In particular, it is shown in this paper that the ENformulation \textit{does not achieve} robust recovery of sparse vectors in thecontext of compressed sensing, whereas the new CLOT formulation does so. Also,like EN but unlike LASSO, the CLOT formulation achieves the grouping effect,wherein coefficients of highly correlated columns of the measurement (ordesign) matrix are assigned roughly comparable values. It is noteworthy thatLASSO does not have the grouping effect and EN (as shown here) does not achieverobust sparse recovery. Therefore the CLOT formulation combines the bestfeatures of both LASSO (robust sparse recovery) and EN (grouping effect). The CLOT formulation is a special case of another one called SGL (SparseGroup LASSO) which was introduced into the literature previously, but withoutany analysis of either the grouping effect or robust sparse recovery. It isshown here that SGL achieves robust sparse recovery, and also achieves aversion of the grouping effect in that coefficients of highly correlatedcolumns of the measurement (or design) matrix are assigned roughly comparablevalues, \textit{if the columns belong to the same group}.
arxiv-1411-1971 | Power-Law Graph Cuts |  http://arxiv.org/abs/1411.1971  | author:Xiangyang Zhou, Jiaxin Zhang, Brian Kulis category:cs.CV cs.LG stat.ML published:2014-10-29 summary:Algorithms based on spectral graph cut objectives such as normalized cuts,ratio cuts and ratio association have become popular in recent years becausethey are widely applicable and simple to implement via standard eigenvectorcomputations. Despite strong performance for a number of clustering tasks,spectral graph cut algorithms still suffer from several limitations: first,they require the number of clusters to be known in advance, but thisinformation is often unknown a priori; second, they tend to produce clusterswith uniform sizes. In some cases, the true clusters exhibit a known sizedistribution; in image segmentation, for instance, human-segmented images tendto yield segment sizes that follow a power-law distribution. In this paper, wepropose a general framework of power-law graph cut algorithms that produceclusters whose sizes are power-law distributed, and also does not fix thenumber of clusters upfront. To achieve our goals, we treat the Pitman-Yorexchangeable partition probability function (EPPF) as a regularizer to graphcut objectives. Because the resulting objectives cannot be solved by relaxingvia eigenvectors, we derive a simple iterative algorithm to locally optimizethe objectives. Moreover, we show that our proposed algorithm can be viewed asperforming MAP inference on a particular Pitman-Yor mixture model. Ourexperiments on various data sets show the effectiveness of our algorithms.
arxiv-1410-7852 | A Markov Decision Process Analysis of the Cold Start Problem in Bayesian Information Filtering |  http://arxiv.org/abs/1410.7852  | author:Xiaoting Zhao, Peter I. Frazier category:cs.LG cs.IR math.OC published:2014-10-29 summary:We consider the information filtering problem, in which we face a stream ofitems, and must decide which ones to forward to a user to maximize the numberof relevant items shown, minus a penalty for each irrelevant item shown.Forwarding decisions are made separately in a personalized way for each user.We focus on the cold-start setting for this problem, in which we have limitedhistorical data on the user's preferences, and must rely on feedback fromforwarded articles to learn which the fraction of items relevant to the user ineach of several item categories. Performing well in this setting requirestrading exploration vs. exploitation, forwarding items that are likely to beirrelevant, to allow learning that will improve later performance. In aBayesian setting, and using Markov decision processes, we show how theBayes-optimal forwarding algorithm can be computed efficiently when the userwill examine each forwarded article, and how an upper bound on theBayes-optimal procedure and a heuristic index policy can be obtained for thesetting when the user will examine only a limited number of forwarded items. Wepresent results from simulation experiments using parameters estimated usinghistorical data from arXiv.org.
arxiv-1410-8149 | Detecting Structural Irregularity in Electronic Dictionaries Using Language Modeling |  http://arxiv.org/abs/1410.8149  | author:Paul Rodrigues, David Zajic, David Doermann, Michael Bloodgood, Peng Ye category:cs.CL cs.LG published:2014-10-29 summary:Dictionaries are often developed using tools that save to Extensible MarkupLanguage (XML)-based standards. These standards often allow high-levelrepeating elements to represent lexical entries, and utilize descendants ofthese repeating elements to represent the structure within each lexical entry,in the form of an XML tree. In many cases, dictionaries are published that haveerrors and inconsistencies that are expensive to find manually. This paperdiscusses a method for dictionary writers to quickly audit structuralregularity across entries in a dictionary by using statistical languagemodeling. The approach learns the patterns of XML nodes that could occur withinan XML tree, and then calculates the probability of each XML tree in thedictionary against these patterns to look for entries that diverge from thenorm.
arxiv-1410-8043 | High-Performance Distributed ML at Scale through Parameter Server Consistency Models |  http://arxiv.org/abs/1410.8043  | author:Wei Dai, Abhimanu Kumar, Jinliang Wei, Qirong Ho, Garth Gibson, Eric P. Xing category:cs.LG stat.ML published:2014-10-29 summary:As Machine Learning (ML) applications increase in data size and modelcomplexity, practitioners turn to distributed clusters to satisfy the increasedcomputational and memory demands. Unfortunately, effective use of clusters forML requires considerable expertise in writing distributed code, whilehighly-abstracted frameworks like Hadoop have not, in practice, approached theperformance seen in specialized ML implementations. The recent Parameter Server(PS) paradigm is a middle ground between these extremes, allowing easyconversion of single-machine parallel ML applications into distributed ones,while maintaining high throughput through relaxed "consistency models" thatallow inconsistent parameter reads. However, due to insufficient theoreticalstudy, it is not clear which of these consistency models can really ensurecorrect ML algorithm output; at the same time, there remain manytheoretically-motivated but undiscovered opportunities to maximizecomputational throughput. Motivated by this challenge, we study both thetheoretical guarantees and empirical behavior of iterative-convergent MLalgorithms in existing PS consistency models. We then use the gleaned insightsto improve a consistency model using an "eager" PS communication mechanism, andimplement it as a new PS system that enables ML algorithms to reach theirsolution more quickly.
arxiv-1410-8151 | A comparison of dense region detectors for image search and fine-grained classification |  http://arxiv.org/abs/1410.8151  | author:Ahmet Iscen, Giorgos Tolias, Philippe-Henri Gosselin, Hervé Jégou category:cs.CV published:2014-10-29 summary:We consider a pipeline for image classification or search based on codingapproaches like Bag of Words or Fisher vectors. In this context, the mostcommon approach is to extract the image patches regularly in a dense manner onseveral scales. This paper proposes and evaluates alternative choices toextract patches densely. Beyond simple strategies derived from regular interestregion detectors, we propose approaches based on super-pixels, edges, and abank of Zernike filters used as detectors. The different approaches areevaluated on recent image retrieval and fine-grain classification benchmarks.Our results show that the regular dense detector is outperformed by othermethods in most situations, leading us to improve the state of the art incomparable setups on standard retrieval and fined-grain benchmarks. As abyproduct of our study, we show that existing methods for blob and super-pixelextraction achieve high accuracy if the patches are extracted along the edgesand not around the detected regions.
arxiv-1410-8808 | A Semantic Web of Know-How: Linked Data for Community-Centric Tasks |  http://arxiv.org/abs/1410.8808  | author:Paolo Pareti, Ewan Klein, Adam Barker category:cs.AI cs.CL published:2014-10-29 summary:This paper proposes a novel framework for representing community know-how onthe Semantic Web. Procedural knowledge generated by web communities typicallytakes the form of natural language instructions or videos and is largelyunstructured. The absence of semantic structure impedes the deployment of manyuseful applications, in particular the ability to discover and integrateknow-how automatically. We discuss the characteristics of community know-howand argue that existing knowledge representation frameworks fail to representit adequately. We present a novel framework for representing the semanticstructure of community know-how and demonstrate the feasibility of our approachby providing a concrete implementation which includes a method forautomatically acquiring procedural knowledge for real-world tasks.
arxiv-1410-7922 | Extended Dynamic Programming and Fast Multidimensional Search Algorithm for Energy Minization in Stereo and Motion |  http://arxiv.org/abs/1410.7922  | author:Mikhail G. Mozerov category:cs.CV published:2014-10-29 summary:This paper presents a novel extended dynamic programming approach for energyminimization (EDP) to solve the correspondence problem for stereo and motion. Asignificant speedup is achieved using a recursive minimum search strategy(RMS). The mentioned speedup is particularly important if the disparity spaceis 2D as well as 3D. The proposed RMS can also be applied in the well-knowndynamic programming (DP) approach for stereo and motion. In this case, thegeneral 2D problem of the global discrete energy minimization is reduced toseveral mutually independent sub-problems of the one-dimensional minimization.The EDP method is used when the approximation of the general 2D discrete energyminimization problem is considered. Then the RMS algorithm is an essential partof the EDP method. Using the EDP algorithm we obtain a lower energy bound thanthe graph cuts (GC) expansion technique on stereo and motion problems. Theproposed calculation scheme possesses natural parallelism and can be realizedon graphics processing unit (GPU) platforms, and can be potentially restrictedfurther by the number of scanlines in the image plane. Furthermore, the RMS andEDP methods can be used in any optimization problem where the objectivefunction meets specific conditions in the smoothness term.
arxiv-1410-8034 | Latent Feature Based FM Model For Rating Prediction |  http://arxiv.org/abs/1410.8034  | author:Xudong Liu, Bin Zhang, Ting Zhang, Chang Liu category:cs.LG cs.IR stat.ML 68-XX H.2.8 published:2014-10-29 summary:Rating Prediction is a basic problem in Recommender System, and one of themost widely used method is Factorization Machines(FM). However, traditionalmatrix factorization methods fail to utilize the benefit of implicit feedback,which has been proved to be important in Rating Prediction problem. In thiswork, we consider a specific situation, movie rating prediction, where weassume that watching history has a big influence on his/her rating behavior onan item. We introduce two models, Latent Dirichlet Allocation(LDA) andword2vec, both of which perform state-of-the-art results in training latentfeatures. Based on that, we propose two feature based models. One is theTopic-based FM Model which provides the implicit feedback to the matrixfactorization. The other is the Vector-based FM Model which expresses the orderinfo of watching history. Empirical results on three datasets demonstrate thatour method performs better than the baseline model and confirm thatVector-based FM Model usually works better as it contains the order info.
arxiv-1410-8027 | Towards a Visual Turing Challenge |  http://arxiv.org/abs/1410.8027  | author:Mateusz Malinowski, Mario Fritz category:cs.AI cs.CL cs.CV cs.GL cs.LG published:2014-10-29 summary:As language and visual understanding by machines progresses rapidly, we areobserving an increasing interest in holistic architectures that tightlyinterlink both modalities in a joint learning and inference process. This trendhas allowed the community to progress towards more challenging and open tasksand refueled the hope at achieving the old AI dream of building machines thatcould pass a turing test in open domains. In order to steadily make progresstowards this goal, we realize that quantifying performance becomes increasinglydifficult. Therefore we ask how we can precisely define such challenges and howwe can evaluate different algorithms on this open tasks? In this paper, wesummarize and discuss such challenges as well as try to give answers whereappropriate options are available in the literature. We exemplify some of thesolutions on a recently presented dataset of question-answering task based onreal-world indoor images that establishes a visual turing challenge. Finally,we argue despite the success of unique ground-truth annotation, we likely haveto step away from carefully curated dataset and rather rely on 'socialconsensus' as the main driving force to create suitable benchmarks. Providingcoverage in this inherently ambiguous output space is an emerging challengethat we face in order to make quantifiable progress in this area.
arxiv-1410-7890 | Global Bandits with Holder Continuity |  http://arxiv.org/abs/1410.7890  | author:Onur Atan, Cem Tekin, Mihaela van der Schaar category:cs.LG published:2014-10-29 summary:Standard Multi-Armed Bandit (MAB) problems assume that the arms areindependent. However, in many application scenarios, the information obtainedby playing an arm provides information about the remainder of the arms. Hence,in such applications, this informativeness can and should be exploited toenable faster convergence to the optimal solution. In this paper, we introduceand formalize the Global MAB (GMAB), in which arms are globally informativethrough a global parameter, i.e., choosing an arm reveals information about allthe arms. We propose a greedy policy for the GMAB which always selects the armwith the highest estimated expected reward, and prove that it achieves boundedparameter-dependent regret. Hence, this policy selects suboptimal arms onlyfinitely many times, and after a finite number of initial time steps, theoptimal arm is selected in all of the remaining time steps with probabilityone. In addition, we also study how the informativeness of the arms about eachother's rewards affects the speed of learning. Specifically, we prove that theparameter-free (worst-case) regret is sublinear in time, and decreases with theinformativeness of the arms. We also prove a sublinear in time Bayesian riskbound for the GMAB which reduces to the well-known Bayesian risk bound forlinearly parameterized bandits when the arms are fully informative. GMABs haveapplications ranging from drug and treatment discovery to dynamic pricing.
arxiv-1410-7883 | Sub-threshold CMOS Spiking Neuron Circuit Design for Navigation Inspired by C. elegans Chemotaxis |  http://arxiv.org/abs/1410.7883  | author:Shibani Santurkar, Bipin Rajendran category:cs.NE q-bio.NC published:2014-10-29 summary:We demonstrate a spiking neural network for navigation motivated by thechemotaxis network of Caenorhabditis elegans. Our network uses informationregarding temporal gradients in the tracking variable's concentration to makenavigational decisions. The gradient information is determined by mimicking theunderlying mechanisms of the ASE neurons of C. elegans. Simulations show thatour model is able to forage and track a target set-point in extremely noisyenvironments. We develop a VLSI implementation for the main gradient detectorneurons, which could be integrated with standard comparator circuitry todevelop a robust circuit for navigation and contour tracking.
arxiv-1410-7881 | A neural circuit for navigation inspired by C. elegans Chemotaxis |  http://arxiv.org/abs/1410.7881  | author:Shibani Santurkar, Bipin Rajendran category:cs.NE q-bio.NC published:2014-10-29 summary:We develop an artificial neural circuit for contour tracking and navigationinspired by the chemotaxis of the nematode Caenorhabditis elegans. In order toharness the computational advantages spiking neural networks promise over theirnon-spiking counterparts, we develop a network comprising 7-spiking neuronswith non-plastic synapses which we show is extremely robust in tracking a rangeof concentrations. Our worm uses information regarding local temporal gradientsin sodium chloride concentration to decide the instantaneous path for foraging,exploration and tracking. A key neuron pair in the C. elegans chemotaxisnetwork is the ASEL & ASER neuron pair, which capture the gradient ofconcentration sensed by the worm in their graded membrane potentials. Theprimary sensory neurons for our network are a pair of artificial spikingneurons that function as gradient detectors whose design is adapted from acomputational model of the ASE neuron pair in C. elegans. Simulations show thatour worm is able to detect the set-point with approximately four times higherprobability than the optimal memoryless Levy foraging model. We also show thatour spiking neural network is much more efficient and noise-resilient whilenavigating and tracking a contour, as compared to an equivalent non-spikingnetwork. We demonstrate that our model is extremely robust to noise and withslight modifications can be used for other practical applications such asobstacle avoidance. Our network model could also be extended for use inthree-dimensional contour tracking or obstacle avoidance.
arxiv-1410-7876 | Collaborative Multi-sensor Classification via Sparsity-based Representation |  http://arxiv.org/abs/1410.7876  | author:Minh Dao, Nam H. Nguyen, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV cs.LG stat.ML published:2014-10-29 summary:In this paper, we propose a general collaborative sparse representationframework for multi-sensor classification, which takes into account thecorrelations as well as complementary information between heterogeneous sensorssimultaneously while considering joint sparsity within each sensor'sobservations. We also robustify our models to deal with the presence of sparsenoise and low-rank interference signals. Specifically, we demonstrate thatincorporating the noise or interference signal as a low-rank component in ourmodels is essential in a multi-sensor classification problem when multipleco-located sources/sensors simultaneously record the same physical event. Wefurther extend our frameworks to kernelized models which rely on sparselyrepresenting a test sample in terms of all the training samples in a featurespace induced by a kernel function. A fast and efficient algorithm based onalternative direction method is proposed where its convergence to optimalsolution is guaranteed. Extensive experiments are conducted on real data setscollected by researchers at the U.S. Army Research Laboratory and the resultsare compared with the conventional classifiers to verify the effectiveness ofthe proposed methods in the application of automatic multi-sensor border patrolcontrol, where we often have to discriminate between human and animalfootsteps.
arxiv-1410-7875 | Faster graphical model identification of tandem mass spectra using peptide word lattices |  http://arxiv.org/abs/1410.7875  | author:Shengjie Wang, John T. Halloran, Jeff A. Bilmes, William S. Noble category:q-bio.MN stat.ML published:2014-10-29 summary:Liquid chromatography coupled with tandem mass spectrometry, also known asshotgun proteomics, is a widely-used high-throughput technology for identifyingproteins in complex biological samples. Analysis of the tens of thousands offragmentation spectra produced by a typical shotgun proteomics experimentbegins by assigning to each observed spectrum the peptide hypothesized to beresponsible for generating the spectrum, typically done by searching eachspectrum against a database of peptides. We have recently described a machinelearning method---Dynamic Bayesian Network for Rapid Identification of Peptides(DRIP)---that not only achieves state-of-the-art spectrum identificationperformance on a variety of datasets but also provides a trainable modelcapable of returning valuable auxiliary information regarding specificpeptide-spectrum matches. In this work, we present two significant improvementsto DRIP. First, we describe how to use word lattices, which are widely used innatural language processing, to significantly speed up DRIP's computations. Toour knowledge, all existing shotgun proteomics search engines computeindependent scores between a given observed spectrum and each possiblecandidate peptide from the database. The key idea of the word lattice is torepresent the set of candidate peptides in a single data structure, therebyallowing sharing of redundant computations among the different candidates. Wedemonstrate that using lattices in conjunction with DRIP leads to speedups onthe order of tens across yeast and worm data sets. Second, we introduce avariant of DRIP that uses a discriminative training framework, performingmaximum mutual entropy estimation rather than maximum likelihood estimation.This modification improves DRIP's statistical power, enabling us to increasethe number of identified spectrum at a 1% false discovery rate on yeast andworm data sets.
arxiv-1410-7484 | Abrupt Motion Tracking via Nearest Neighbor Field Driven Stochastic Sampling |  http://arxiv.org/abs/1410.7484  | author:Tianfei Zhou, Yao Lu, Feng Lv, Huijun Di, Qingjie Zhao, Jian Zhang category:cs.CV published:2014-10-28 summary:Stochastic sampling based trackers have shown good performance for abruptmotion tracking so that they have gained popularity in recent years. However,conventional methods tend to use a two-stage sampling paradigm, in which thesearch space needs to be uniformly explored with an inefficient preliminarysampling phase. In this paper, we propose a novel sampling-based method in theBayesian filtering framework to address the problem. Within the framework,nearest neighbor field estimation is utilized to compute the importanceproposal probabilities, which guide the Markov chain search towards promisingregions and thus enhance the sampling efficiency; given the motion priors, asmoothing stochastic sampling Monte Carlo algorithm is proposed to approximatethe posterior distribution through a smoothing weight-updating scheme.Moreover, to track the abrupt and the smooth motions simultaneously, we developan abrupt-motion detection scheme which can discover the presence of abruptmotions during online tracking. Extensive experiments on challenging imagesequences demonstrate the effectiveness and the robustness of our algorithm inhandling the abrupt motions.
arxiv-1411-2883 | A new estimate of mutual information based measure of dependence between two variables: properties and fast implementation |  http://arxiv.org/abs/1411.2883  | author:Namita Jain, C. A. Murthy category:cs.IT cs.LG math.IT published:2014-10-28 summary:This article proposes a new method to estimate an existing mutual informationbased dependence measure using histogram density estimates. Finding a suitablebin length for histogram is an open problem. We propose a new way of computingthe bin length for histogram using a function of maximum separation betweenpoints. The chosen bin length leads to consistent density estimates forhistogram method. The values of density thus obtained are used to calculate anestimate of an existing dependence measure. The proposed estimate is named asMutual Information Based Dependence Index (MIDI). Some important properties ofMIDI have also been stated. The performance of the proposed method has beencompared to generally accepted measures like Distance Correlation (dcor),Maximal Information Coefficient (MINE) in terms of accuracy and computationalcomplexity with the help of several artificial data sets with different amountsof noise. The proposed method is able to detect many types of relationshipsbetween variables, without making any assumption about the functional form ofthe relationship. The power statistics of proposed method illustrate theireffectiveness in detecting non linear relationship. Thus, it is able to achievegenerality without a high rate of false positive cases. MIDI is found to workbetter on a real life data set than competing methods. The proposed method isfound to overcome some of the limitations which occur with dcor and MINE.Computationally, MIDI is found to be better than dcor and MINE, in terms oftime and memory, making it suitable for large data sets.
arxiv-1410-7835 | Fast Learning of Relational Dependency Networks |  http://arxiv.org/abs/1410.7835  | author:Oliver Schulte, Zhensong Qian, Arthur E. Kirkpatrick, Xiaoqian Yin, Yan Sun category:cs.LG published:2014-10-28 summary:A Relational Dependency Network (RDN) is a directed graphical model widelyused for multi-relational data. These networks allow cyclic dependencies,necessary to represent relational autocorrelations. We describe an approach forlearning both the RDN's structure and its parameters, given an input relationaldatabase: First learn a Bayesian network (BN), then transform the Bayesiannetwork to an RDN. Thus fast Bayes net learning can provide fast RDN learning.The BN-to-RDN transform comprises a simple, local adjustment of the Bayes netstructure and a closed-form transform of the Bayes net parameters. This methodcan learn an RDN for a dataset with a million tuples in minutes. We empiricallycompare our approach to state-of-the art RDN learning methods that usefunctional gradient boosting, on five benchmark datasets. Learning RDNs via BNsscales much better to large datasets than learning RDNs with boosting, andprovides competitive accuracy in predictions.
arxiv-1410-7550 | Learning deep dynamical models from image pixels |  http://arxiv.org/abs/1410.7550  | author:Niklas Wahlström, Thomas B. Schön, Marc Peter Deisenroth category:stat.ML cs.LG cs.NE cs.SY published:2014-10-28 summary:Modeling dynamical systems is important in many disciplines, e.g., control,robotics, or neurotechnology. Commonly the state of these systems is notdirectly observed, but only available through noisy and potentiallyhigh-dimensional observations. In these cases, system identification, i.e.,finding the measurement mapping and the transition mapping (system dynamics) inlatent space can be challenging. For linear system dynamics and measurementmappings efficient solutions for system identification are available. However,in practical applications, the linearity assumptions does not hold, requiringnon-linear system identification techniques. If additionally the observationsare high-dimensional (e.g., images), non-linear system identification isinherently hard. To address the problem of non-linear system identificationfrom high-dimensional observations, we combine recent advances in deep learningand system identification. In particular, we jointly learn a low-dimensionalembedding of the observation by means of deep auto-encoders and a predictivetransition model in this low-dimensional space. We demonstrate that our modelenables learning good predictive models of dynamical systems from pixelinformation only.
arxiv-1410-7659 | Learning graphical models from the Glauber dynamics |  http://arxiv.org/abs/1410.7659  | author:Guy Bresler, David Gamarnik, Devavrat Shah category:cs.LG cs.IT math.IT stat.CO stat.ML published:2014-10-28 summary:In this paper we consider the problem of learning undirected graphical modelsfrom data generated according to the Glauber dynamics. The Glauber dynamics isa Markov chain that sequentially updates individual nodes (variables) in agraphical model and it is frequently used to sample from the stationarydistribution (to which it converges given sufficient time). Additionally, theGlauber dynamics is a natural dynamical model in a variety of settings. Thiswork deviates from the standard formulation of graphical model learning in theliterature, where one assumes access to i.i.d. samples from the distribution. Much of the research on graphical model learning has been directed towardsfinding algorithms with low computational cost. As the main result of thiswork, we establish that the problem of reconstructing binary pairwise graphicalmodels is computationally tractable when we observe the Glauber dynamics.Specifically, we show that a binary pairwise graphical model on $p$ nodes withmaximum degree $d$ can be learned in time $f(d)p^2\log p$, for a function$f(d)$, using nearly the information-theoretic minimum number of samples.
arxiv-1410-7812 | Beta-Negative Binomial Process and Exchangeable Random Partitions for Mixed-Membership Modeling |  http://arxiv.org/abs/1410.7812  | author:Mingyuan Zhou category:stat.ME stat.ML published:2014-10-28 summary:The beta-negative binomial process (BNBP), an integer-valued stochasticprocess, is employed to partition a count vector into a latent random countmatrix. As the marginal probability distribution of the BNBP that governs theexchangeable random partitions of grouped data has not yet been developed,current inference for the BNBP has to truncate the number of atoms of the betaprocess. This paper introduces an exchangeable partition probability functionto explicitly describe how the BNBP clusters the data points of each group intoa random number of exchangeable partitions, which are shared across all thegroups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to anovel nonparametric Bayesian topic model that is distinct from existing ones,with simple implementation, fast convergence, good mixing, and state-of-the-artpredictive performance.
arxiv-1410-7827 | Generalized Product of Experts for Automatic and Principled Fusion of Gaussian Process Predictions |  http://arxiv.org/abs/1410.7827  | author:Yanshuai Cao, David J. Fleet category:cs.LG cs.AI stat.ML published:2014-10-28 summary:In this work, we propose a generalized product of experts (gPoE) frameworkfor combining the predictions of multiple probabilistic models. We identifyfour desirable properties that are important for scalability, expressivenessand robustness, when learning and inferring with a combination of multiplemodels. Through analysis and experiments, we show that gPoE of Gaussianprocesses (GP) have these qualities, while no other existing combinationschemes satisfy all of them at the same time. The resulting GP-gPoE is highlyscalable as individual GP experts can be independently learned in parallel;very expressive as the way experts are combined depends on the input ratherthan fixed; the combined prediction is still a valid probabilistic model withnatural interpretation; and finally robust to unreliable predictions fromindividual experts.
arxiv-1410-7787 | Correcting Errors in Digital Lexicographic Resources Using a Dictionary Manipulation Language |  http://arxiv.org/abs/1410.7787  | author:David Zajic, Michael Maxwell, David Doermann, Paul Rodrigues, Michael Bloodgood category:cs.CL published:2014-10-28 summary:We describe a paradigm for combining manual and automatic error correction ofnoisy structured lexicographic data. Modifications to the structure andunderlying text of the lexicographic data are expressed in a simple,interpreted programming language. Dictionary Manipulation Language (DML)commands identify nodes by unique identifiers, and manipulations are performedusing simple commands such as create, move, set text, etc. Corrected lexiconsare produced by applying sequences of DML commands to the source version of thelexicon. DML commands can be written manually to repair one-off errors orgenerated automatically to correct recurring problems. We discuss advantages ofthe paradigm for the task of editing digital bilingual dictionaries.
arxiv-1410-7762 | A hierarchical framework for object recognition |  http://arxiv.org/abs/1410.7762  | author:Reza Moazzezi category:cs.CV published:2014-10-28 summary:Object recognition in the presence of background clutter and distractors is acentral problem both in neuroscience and in machine learning. However, theperformance level of the models that are inspired by cortical mechanisms,including deep networks such as convolutional neural networks and deep beliefnetworks, is shown to significantly decrease in the presence of noise andbackground objects [19, 24]. Here we develop a computational framework that ishierarchical, relies heavily on key properties of the visual cortex includingmid-level feature selectivity in visual area V4 and Inferotemporal cortex (IT)[4, 9, 12, 18], high degrees of selectivity and invariance in IT [13, 17, 18]and the prior knowledge that is built into cortical circuits (such as theemergence of edge detector neurons in primary visual cortex before the onset ofthe visual experience) [1, 21], and addresses the problem of object recognitionin the presence of background noise and distractors. Our approach isspecifically designed to address large deformations, allows flexiblecommunication between different layers of representation and learns highlyselective filters from a small number of training examples.
arxiv-1410-7690 | Trend Filtering on Graphs |  http://arxiv.org/abs/1410.7690  | author:Yu-Xiang Wang, James Sharpnack, Alex Smola, Ryan J. Tibshirani category:stat.ML cs.AI cs.LG stat.ME 62G05 published:2014-10-28 summary:We introduce a family of adaptive estimators on graphs, based on penalizingthe $\ell_1$ norm of discrete graph differences. This generalizes the idea oftrend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariatenonparametric regression, to graphs. Analogous to the univariate case, graphtrend filtering exhibits a level of local adaptivity unmatched by the usual$\ell_2$-based graph smoothers. It is also defined by a convex minimizationproblem that is readily solved (e.g., by fast ADMM or Newton algorithms). Wedemonstrate the merits of graph trend filtering through examples and theory.
arxiv-1410-7730 | New similarity index based on entropy and group theory |  http://arxiv.org/abs/1410.7730  | author:Yasel Garcés, Esley Torres, Osvaldo Pereira, Roberto Rodríguez category:cs.CV published:2014-10-28 summary:In this work, we propose a new similarity index for images considering theentropy function and group theory. This index considers an algebraic group ofimages, it is defined by an inner law that provides a novel approach for thesubtraction of images. Through an equivalence relationship in the field ofimages, we prove the existence of the quotient group, on which the newsimilarity index is defined. We also present the main properties of the newindex, and the immediate application thereof as a stopping criterion of the"Mean Shift Iterative Algorithm".
arxiv-1410-7709 | Anomaly Detection Framework Using Rule Extraction for Efficient Intrusion Detection |  http://arxiv.org/abs/1410.7709  | author:Antti Juvonen, Tuomo Sipola category:cs.LG cs.CR published:2014-10-28 summary:Huge datasets in cyber security, such as network traffic logs, can beanalyzed using machine learning and data mining methods. However, the amount ofcollected data is increasing, which makes analysis more difficult. Many machinelearning methods have not been designed for big datasets, and consequently areslow and difficult to understand. We address the issue of efficient networktraffic classification by creating an intrusion detection framework thatapplies dimensionality reduction and conjunctive rule extraction. The systemcan perform unsupervised anomaly detection and use this information to createconjunctive rules that classify huge amounts of traffic in real time. We testthe implemented system with the widely used KDD Cup 99 dataset and real-worldnetwork logs to confirm that the performance is satisfactory. This system istransparent and does not work like a black box, making it intuitive for domainexperts, such as network administrators.
arxiv-1410-7660 | Non-convex Robust PCA |  http://arxiv.org/abs/1410.7660  | author:Praneeth Netrapalli, U N Niranjan, Sujay Sanghavi, Animashree Anandkumar, Prateek Jain category:cs.IT cs.LG math.IT stat.ML published:2014-10-28 summary:We propose a new method for robust PCA -- the task of recovering a low-rankmatrix from sparse corruptions that are of unknown value and support. Ourmethod involves alternating between projecting appropriate residuals onto theset of low-rank matrices, and the set of sparse matrices; each projection is{\em non-convex} but easy to compute. In spite of this non-convexity, weestablish exact recovery of the low-rank matrix, under the same conditions thatare required by existing methods (which are based on convex optimization). Foran $m \times n$ input matrix ($m \leq n)$, our method has a running time of$O(r^2mn)$ per iteration, and needs $O(\log(1/\epsilon))$ iterations to reachan accuracy of $\epsilon$. This is close to the running time of simple PCA viathe power method, which requires $O(rmn)$ per iteration, and$O(\log(1/\epsilon))$ iterations. In contrast, existing methods for robust PCA,which are based on convex optimization, have $O(m^2n)$ complexity periteration, and take $O(1/\epsilon)$ iterations, i.e., exponentially moreiterations for the same accuracy. Experiments on both synthetic and real data establishes the improved speedand accuracy of our method over existing convex implementations.
arxiv-1410-7613 | A Short Image Series Based Scheme for Time Series Digital Image Correlation |  http://arxiv.org/abs/1410.7613  | author:Xian Wang, Shaopeng Ma category:physics.optics cs.CV 78Mxx published:2014-10-28 summary:A new scheme for digital image correlation, i.e., short time series DIC(STS-DIC) is proposed. Instead of processing the original deformed speckleimages individually, STS-DIC combines several adjacent deformed speckle imagesfrom a short time series and then processes the averaged image, for whichdeformation continuity over time is introduced. The deformation of severaladjacent images is assumed to be linear in time and a new spatial-temporaldisplacement representation method with eight unknowns is presented based onthe subset-based representation method. Then, the model of STS-DIC is createdand a solving scheme is developed based on the Newton-Raphson iteration. Theproposed method is verified for numerical and experimental cases. The resultsshow that the proposed STS-DIC greatly improves the accuracy of traditionalDIC, both under simple and complicated deformation conditions, while retainingacceptable actual computational cost.
arxiv-1410-7596 | Fast Algorithms for Online Stochastic Convex Programming |  http://arxiv.org/abs/1410.7596  | author:Shipra Agrawal, Nikhil R. Devanur category:cs.LG cs.DS math.OC F.1.2; G.1.6 published:2014-10-28 summary:We introduce the online stochastic Convex Programming (CP) problem, a verygeneral version of stochastic online problems which allows arbitrary concaveobjectives and convex feasibility constraints. Many well-studied problems likeonline stochastic packing and covering, online stochastic matching with concavereturns, etc. form a special case of online stochastic CP. We present fastalgorithms for these problems, which achieve near-optimal regret guarantees forboth the i.i.d. and the random permutation models of stochastic inputs. Whenapplied to the special case online packing, our ideas yield a simpler andfaster primal-dual algorithm for this well studied problem, which achieves theoptimal competitive ratio. Our techniques make explicit the connection ofprimal-dual paradigm and online learning to online stochastic CP.
arxiv-1410-7580 | Robust Piecewise-Constant Smoothing: M-Smoother Revisited |  http://arxiv.org/abs/1410.7580  | author:Linchao Bao, Qingxiong Yang category:cs.CV published:2014-10-28 summary:A robust estimator, namely M-smoother, for piecewise-constant smoothing isrevisited in this paper. Starting from its generalized formulation, we proposea numerical scheme/framework for solving it via a series of weighted-averagefiltering (e.g., box filtering, Gaussian filtering, bilateral filtering, andguided filtering). Because of the equivalence between M-smoother andlocal-histogram-based filters (such as median filter and mode filter), theproposed framework enables fast approximation of histogram filters via a numberof box filtering or Gaussian filtering. In addition, high-qualitypiecewise-constant smoothing can be achieved via a number of bilateralfiltering or guided filtering integrated in the proposed framework. Experimentson depth map denoising show the effectiveness of our framework.
arxiv-1410-7326 | Neuroevolution in Games: State of the Art and Open Challenges |  http://arxiv.org/abs/1410.7326  | author:Sebastian Risi, Julian Togelius category:cs.NE published:2014-10-27 summary:This paper surveys research on applying neuroevolution (NE) to games. Inneuroevolution, artificial neural networks are trained through evolutionaryalgorithms, taking inspiration from the way biological brains evolved. Weanalyse the application of NE in games along five different axes, which are therole NE is chosen to play in a game, the different types of neural networksused, the way these networks are evolved, how the fitness is determined andwhat type of input the network receives. The article also highlights importantopen research challenges in the field.
arxiv-1410-7171 | Exponentiated Subgradient Algorithm for Online Optimization under the Random Permutation Model |  http://arxiv.org/abs/1410.7171  | author:Reza Eghbali, Jon Swenson, Maryam Fazel category:math.OC cs.DS cs.LG published:2014-10-27 summary:Online optimization problems arise in many resource allocation tasks, wherethe future demands for each resource and the associated utility functionschange over time and are not known apriori, yet resources need to be allocatedat every point in time despite the future uncertainty. In this paper, weconsider online optimization problems with general concave utilities. We modifyand extend an online optimization algorithm proposed by Devanur et al. forlinear programming to this general setting. The model we use for the arrival ofthe utilities and demands is known as the random permutation model, where afixed collection of utilities and demands are presented to the algorithm inrandom order. We prove that under this model the algorithm achieves acompetitive ratio of $1-O(\epsilon)$ under a near-optimal assumption that thebid to budget ratio is $O (\frac{\epsilon^2}{\log({m}/{\epsilon})})$, where $m$is the number of resources, while enjoying a significantly lower computationalcost than the optimal algorithm proposed by Kesselheim et al. We draw aconnection between the proposed algorithm and subgradient methods used inconvex optimization. In addition, we present numerical experiments thatdemonstrate the performance and speed of this algorithm in comparison toexisting algorithms.
arxiv-1410-7414 | Fast Function to Function Regression |  http://arxiv.org/abs/1410.7414  | author:Junier Oliva, Willie Neiswanger, Barnabas Poczos, Eric Xing, Jeff Schneider category:stat.ML cs.LG published:2014-10-27 summary:We analyze the problem of regression when both input covariates and outputresponses are functions from a nonparametric function class. Function tofunction regression (FFR) covers a large range of interesting applicationsincluding time-series prediction problems, and also more general tasks likestudying a mapping between two separate types of distributions. However,previous nonparametric estimators for FFR type problems scale badlycomputationally with the number of input/output pairs in a data-set. Given thecomplexity of a mapping between general functions it may be necessary toconsider large data-sets in order to achieve a low estimation risk. To addressthis issue, we develop a novel scalable nonparametric estimator, theTriple-Basis Estimator (3BE), which is capable of operating over datasets withmany instances. To the best of our knowledge, the 3BE is the firstnonparametric FFR estimator that can scale to massive datasets. We analyze the3BE's risk and derive an upperbound rate. Furthermore, we show an improvementof several orders of magnitude in terms of prediction speed and a reduction inerror over previous estimators in various real-world data-sets.
arxiv-1410-7376 | Visual Chunking: A List Prediction Framework for Region-Based Object Detection |  http://arxiv.org/abs/1410.7376  | author:Nicholas Rhinehart, Jiaji Zhou, Martial Hebert, J. Andrew Bagnell category:cs.CV published:2014-10-27 summary:We consider detecting objects in an image by iteratively selecting from a setof arbitrarily shaped candidate regions. Our generic approach, which we termvisual chunking, reasons about the locations of multiple object instances in animage while expressively describing object boundaries. We design anoptimization criterion for measuring the performance of a list of suchdetections as a natural extension to a common per-instance metric. We presentan efficient algorithm with provable performance for building a high-qualitylist of detections from any candidate set of region-based proposals. We alsodevelop a simple class-specific algorithm to generate a candidate regioninstance in near-linear time in the number of low-level superpixels thatoutperforms other region generating methods. In order to make predictions onnovel images at testing time without access to ground truth, we developlearning approaches to emulate these algorithms' behaviors. We demonstrate thatour new approach outperforms sophisticated baselines on benchmark datasets.
arxiv-1410-7172 | Heteroscedastic Treed Bayesian Optimisation |  http://arxiv.org/abs/1410.7172  | author:John-Alexander M. Assael, Ziyu Wang, Bobak Shahriari, Nando de Freitas category:cs.LG math.OC stat.ML published:2014-10-27 summary:Optimising black-box functions is important in many disciplines, such astuning machine learning models, robotics, finance and mining exploration.Bayesian optimisation is a state-of-the-art technique for the globaloptimisation of black-box functions which are expensive to evaluate. At thecore of this approach is a Gaussian process prior that captures our beliefabout the distribution over functions. However, in many cases a single Gaussianprocess is not flexible enough to capture non-stationarity in the objectivefunction. Consequently, heteroscedasticity negatively affects performance oftraditional Bayesian methods. In this paper, we propose a novel prior modelwith hierarchical parameter learning that tackles the problem ofnon-stationarity in Bayesian optimisation. Our results demonstrate substantialimprovements in a wide range of applications, including automatic machinelearning and mining exploration.
arxiv-1410-7429 | Higher-order MRFs based image super resolution: why not MAP? |  http://arxiv.org/abs/1410.7429  | author:Yunjin Chen category:cs.CV published:2014-10-27 summary:A trainable filter-based higher-order Markov Random Fields (MRFs) model - theso called Fields of Experts (FoE), has proved a highly effective image priormodel for many classic image restoration problems. Generally, two options areavailable to incorporate the learned FoE prior in the inference procedure: (1)sampling-based minimum mean square error (MMSE) estimate, and (2) energyminimization-based maximum a posteriori (MAP) estimate. This letter is devotedto the FoE prior based single image super resolution (SR) problem, and wesuggest to make use of the MAP estimate for inference based on two facts: (I)It is well-known that the MAP inference has a remarkable advantage of highcomputational efficiency, while the sampling-based MMSE estimate is very timeconsuming. (II) Practical SR experiment results demonstrate that the MAPestimate works equally well compared to the MMSE estimate with exactly the sameFoE prior model. Moreover, it can lead to even further improvements byincorporating our discriminatively trained FoE prior model. In summary, we holdthat for higher-order natural image prior based SR problem, it is better toemploy the MAP estimate for inference.
arxiv-1410-7452 | Consensus Message Passing for Layered Graphical Models |  http://arxiv.org/abs/1410.7452  | author:Varun Jampani, S. M. Ali Eslami, Daniel Tarlow, Pushmeet Kohli, John Winn category:cs.CV cs.AI cs.LG published:2014-10-27 summary:Generative models provide a powerful framework for probabilistic reasoning.However, in many domains their use has been hampered by the practicaldifficulties of inference. This is particularly the case in computer vision,where models of the imaging process tend to be large, loopy and layered. Forthis reason bottom-up conditional models have traditionally dominated in suchdomains. We find that widely-used, general-purpose message passing inferencealgorithms such as Expectation Propagation (EP) and Variational Message Passing(VMP) fail on the simplest of vision models. With these models in mind, weintroduce a modification to message passing that learns to exploit theirlayered structure by passing 'consensus' messages that guide inference towardsgood solutions. Experiments on a variety of problems show that the proposedtechnique leads to significantly more accurate inference results, not only whencompared to standard EP and VMP, but also when compared to competitivebottom-up conditional models.
arxiv-1410-7454 | Deep Structured learning for mass segmentation from Mammograms |  http://arxiv.org/abs/1410.7454  | author:Neeraj Dhungel, Gustavo Carneiro, Andrew P. Bradley category:cs.CV published:2014-10-27 summary:In this paper, we present a novel method for the segmentation of breastmasses from mammograms exploring structured and deep learning. Specifically,using structured support vector machine (SSVM), we formulate a model thatcombines different types of potential functions, including one that classifiesimage regions using deep learning. Our main goal with this work is to show theaccuracy and efficiency improvements that these relatively new techniques canprovide for the segmentation of breast masses from mammograms. We also proposean easily reproducible quantitative analysis to as- sess the performance ofbreast mass segmentation methodologies based on widely accepted accuracy andrunning time measurements on public datasets, which will facilitate furthercomparisons for this segmentation problem. In particular, we use two publiclyavailable datasets (DDSM-BCRP and INbreast) and propose the computa- tion ofthe running time taken for the methodology to produce a mass segmentation givenan input image and the use of the Dice index to quantitatively measure thesegmentation accuracy. For both databases, we show that our proposedmethodology produces competitive results in terms of accuracy and running time.
arxiv-1410-7328 | Exact Expression For Information Distance |  http://arxiv.org/abs/1410.7328  | author:P. M. B. Vitanyi category:cs.IT cs.CC cs.CV cs.DM math.IT published:2014-10-27 summary:Information distance can be defined not only between two strings but also ina finite multiset of strings of cardinality greater than two. We give anelementary proof for expressing the information distance. It is exact since foreach cardinality of the multiset the lower bound for some multiset equals theupper bound for all multisets up to a constant additive term. We discussoverlap.
arxiv-1410-7404 | Maximally Informative Hierarchical Representations of High-Dimensional Data |  http://arxiv.org/abs/1410.7404  | author:Greg Ver Steeg, Aram Galstyan category:stat.ML cs.LG published:2014-10-27 summary:We consider a set of probabilistic functions of some input variables as arepresentation of the inputs. We present bounds on how informative arepresentation is about input data. We extend these bounds to hierarchicalrepresentations so that we can quantify the contribution of each layer towardscapturing the information in the original data. The special form of thesebounds leads to a simple, bottom-up optimization procedure to constructhierarchical representations that are also maximally informative about thedata. This optimization has linear computational complexity and constant samplecomplexity in the number of variables. These results establish a new approachto unsupervised learning of deep representations that is both principled andpractical. We demonstrate the usefulness of the approach on both synthetic andreal-world data.
arxiv-1410-7100 | Estimating the intrinsic dimension in fMRI space via dataset fractal analysis - Counting the `cpu cores' of the human brain |  http://arxiv.org/abs/1410.7100  | author:Harris V. Georgiou category:cs.AI cs.CV q-bio.NC stat.ML published:2014-10-27 summary:Functional Magnetic Resonance Imaging (fMRI) is a powerful non-invasive toolfor localizing and analyzing brain activity. This study focuses on one veryimportant aspect of the functional properties of human brain, specifically theestimation of the level of parallelism when performing complex cognitive tasks.Using fMRI as the main modality, the human brain activity is investigatedthrough a purely data-driven signal processing and dimensionality analysisapproach. Specifically, the fMRI signal is treated as a multi-dimensional dataspace and its intrinsic `complexity' is studied via dataset fractal analysisand blind-source separation (BSS) methods. One simulated and two real fMRIdatasets are used in combination with Independent Component Analysis (ICA) andfractal analysis for estimating the intrinsic (true) dimensionality, in orderto provide data-driven experimental evidence on the number of independent brainprocesses that run in parallel when visual or visuo-motor tasks are performed.Although this number is can not be defined as a strict threshold but rather asa continuous range, when a specific activation level is defined, acorresponding number of parallel processes or the casual equivalent of `cpucores' can be detected in normal human brain activity.
arxiv-1410-7455 | Parallel training of DNNs with Natural Gradient and Parameter Averaging |  http://arxiv.org/abs/1410.7455  | author:Daniel Povey, Xiaohui Zhang, Sanjeev Khudanpur category:cs.NE cs.LG stat.ML published:2014-10-27 summary:We describe the neural-network training framework used in the Kaldi speechrecognition toolkit, which is geared towards training DNNs with large amountsof training data using multiple GPU-equipped or multi-core machines. In orderto be as hardware-agnostic as possible, we needed a way to use multiplemachines without generating excessive network traffic. Our method is to averagethe neural network parameters periodically (typically every minute or two), andredistribute the averaged parameters to the machines for further training. Eachmachine sees different data. By itself, this method does not work very well.However, we have another method, an approximate and efficient implementation ofNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allowour periodic-averaging method to work well, as well as substantially improvingthe convergence of SGD on a single machine.
arxiv-1410-7164 | Directional Bilateral Filters |  http://arxiv.org/abs/1410.7164  | author:Manasij Venkatesh, Chandra Sekhar Seelamantula category:cs.CV published:2014-10-27 summary:We propose a bilateral filter with a locally controlled domain kernel fordirectional edge-preserving smoothing. Traditional bilateral filters use arange kernel, which is responsible for edge preservation, and a fixed domainkernel that performs smoothing. Our intuition is that orientation andanisotropy of image structures should be incorporated into the domain kernelwhile smoothing. For this purpose, we employ an oriented Gaussian domain kernellocally controlled by a structure tensor. The oriented domain kernel combinedwith a range kernel forms the directional bilateral filter. The two kernelsassist each other in effectively suppressing the influence of the outlierswhile smoothing. To find the optimal parameters of the directional bilateralfilter, we propose the use of Stein's unbiased risk estimate (SURE). We testthe capabilities of the kernels separately as well as together, first onsynthetic images, and then on real endoscopic images. The directional bilateralfilter has better denoising performance than the Gaussian bilateral filter atvarious noise levels in terms of peak signal-to-noise ratio (PSNR).
arxiv-1410-7365 | Multiple Output Regression with Latent Noise |  http://arxiv.org/abs/1410.7365  | author:Jussi Gillberg, Pekka Marttinen, Matti Pirinen, Antti J. Kangas, Pasi Soininen, Mehreen Ali, Aki S. Havulinna, Marjo-Riitta Marjo-Riitta Järvelin, Mika Ala-Korpela, Samuel Kaski category:stat.ML published:2014-10-27 summary:In high-dimensional data, structured noise caused by observed and unobservedfactors affecting multiple target variables simultaneously, imposes a seriouschallenge for modeling, by masking the often weak signal. Therefore, (1)explaining away the structured noise in multiple-output regression is ofparamount importance. Additionally, (2) assumptions about the correlationstructure of the regression weights are needed. We note that both can beformulated in a natural way in a latent variable model, in which both theinteresting signal and the noise are mediated through the same latent factors.Under this assumption, the signal model then borrows strength from the noisemodel by encouraging similar effects on correlated targets. We introduce ahyperparameter for the \emph{latent signal-to-noise ratio} which turns out tobe important for modelling weak signals, and an ordered infinite-dimensionalshrinkage prior that resolves the rotational unidentifiability in reduced-rankregression models. Simulations and prediction experiments with metabolite, geneexpression, FMRI measurement, and macroeconomic time series data show that ourmodel equals or exceeds the state-of-the-art performance and, in particular,outperforms the standard approach of assuming independent noise and signalmodels.
arxiv-1410-7182 | Analysis of Named Entity Recognition and Linking for Tweets |  http://arxiv.org/abs/1410.7182  | author:Leon Derczynski, Diana Maynard, Giuseppe Rizzo, Marieke van Erp, Genevieve Gorrell, Raphaël Troncy, Johann Petrak, Kalina Bontcheva category:cs.CL published:2014-10-27 summary:Applying natural language processing for mining and intelligent informationaccess to tweets (a form of microblog) is a challenging, emerging researcharea. Unlike carefully authored news text and other longer content, tweets posea number of new challenges, due to their short, noisy, context-dependent, anddynamic nature. Information extraction from tweets is typically performed in apipeline, comprising consecutive stages of language identification,tokenisation, part-of-speech tagging, named entity recognition and entitydisambiguation (e.g. with respect to DBpedia). In this work, we describe a newTwitter entity disambiguation dataset, and conduct an empirical analysis ofnamed entity recognition and disambiguation, investigating how robust a numberof state-of-the-art systems are on such noisy texts, what the main sources oferror are, and which problems should be further investigated to improve thestate of the art.
arxiv-1410-7211 | A method for context-based adaptive QRS clustering in real-time |  http://arxiv.org/abs/1410.7211  | author:Daniel Castro, Paulo Félix, Jesús Presedo category:cs.CV physics.med-ph published:2014-10-27 summary:Continuous follow-up of heart condition through long-term electrocardiogrammonitoring is an invaluable tool for diagnosing some cardiac arrhythmias. Insuch context, providing tools for fast locating alterations of normalconduction patterns is mandatory and still remains an open issue. This workpresents a real-time method for adaptive clustering QRS complexes frommultilead ECG signals that provides the set of QRS morphologies that appearduring an ECG recording. The method processes the QRS complexes sequentially,grouping them into a dynamic set of clusters based on the information contentof the temporal context. The clusters are represented by templates which evolveover time and adapt to the QRS morphology changes. Rules to create, merge andremove clusters are defined along with techniques for noise detection in orderto avoid their proliferation. To cope with beat misalignment, DerivativeDynamic Time Warping is used. The proposed method has been validated againstthe MIT-BIH Arrhythmia Database and the AHA ECG Database showing a globalpurity of 98.56% and 99.56%, respectively. Results show that our proposal notonly provides better results than previous offline solutions but also fulfillsreal-time requirements.
arxiv-1410-7220 | Exact and Heuristic Algorithms for Semi-Nonnegative Matrix Factorization |  http://arxiv.org/abs/1410.7220  | author:Nicolas Gillis, Abhishek Kumar category:math.NA cs.LG cs.NA math.OC stat.ML published:2014-10-27 summary:Given a matrix $M$ (not necessarily nonnegative) and a factorization rank$r$, semi-nonnegative matrix factorization (semi-NMF) looks for a matrix $U$with $r$ columns and a nonnegative matrix $V$ with $r$ rows such that $UV$ isthe best possible approximation of $M$ according to some metric. In this paper,we study the properties of semi-NMF from which we develop exact and heuristicalgorithms. Our contribution is threefold. First, we prove that the error of asemi-NMF of rank $r$ has to be smaller than the best unconstrainedapproximation of rank $r-1$. This leads us to a new initialization procedurebased on the singular value decomposition (SVD) with a guarantee on the qualityof the approximation. Second, we propose an exact algorithm (that is, analgorithm that finds an optimal solution), also based on the SVD, for a certainclass of matrices (including nonnegative irreducible matrices) from which wederive an initialization for matrices not belonging to that class. Numericalexperiments illustrate that this second approach performs extremely well, andallows us to compute optimal semi-NMF decompositions in many situations.Finally, we analyze the computational complexity of semi-NMF proving itsNP-hardness, already in the rank-one case (that is, for $r = 1$), and we showthat semi-NMF is sometimes ill-posed (that is, an optimal solution does notexist).
arxiv-1410-7241 | A Greedy Homotopy Method for Regression with Nonconvex Constraints |  http://arxiv.org/abs/1410.7241  | author:Fabian L. Wauthier, Peter Donnelly category:stat.ML stat.ME published:2014-10-27 summary:Constrained least squares regression is an essential tool forhigh-dimensional data analysis. Given a partition $\mathcal{G}$ of inputvariables, this paper considers a particular class of nonconvex constraintfunctions that encourage the linear model to select a small number of variablesfrom a small number of groups in $\mathcal{G}$. Such constraints are relevantin many practical applications, such as Genome-Wide Association Studies (GWAS).Motivated by the efficiency of the Lasso homotopy method, we present RepLasso,a greedy homotopy algorithm that tries to solve the induced sequence ofnonconvex problems by solving a sequence of suitably adapted convex surrogateproblems. We prove that in some situations RepLasso recovers the global minimaof the nonconvex problem. Moreover, even if it does not recover global minima,we prove that in relevant cases it will still do no worse than the Lasso interms of support and signed support recovery, while in practice outperformingit. We show empirically that the strategy can also be used to improve overother Lasso-style algorithms. Finally, a GWAS of ankylosing spondylitishighlights our method's practical utility.
arxiv-1410-7252 | Iris Biometric System using a hybrid approach |  http://arxiv.org/abs/1410.7252  | author:Abhimanyu Sarin, Dr. Jagadish Nayak category:cs.CV 47G20 published:2014-10-27 summary:Iris Recognition Systems are ocular- based biometric devices used primarilyfor security reasons. The complexity and the randomness of the Iris, amongstvarious other factors, ensure that this biometric system is inarguably an exactand reliable method of identification. The algorithm is responsible forautomatic localization and segmentation of boundaries using circular HoughTransform, noise reductions, image enhancement and feature extraction acrossnumerous distinct images present in the database. This paper delves into thevarious kinds of techniques required to approximate the pupillary and limbicboundaries of the enrolled iris image, captured using a suitable imageacquisition device and perform feature extraction on the normalized iris imagewith the help of Haar Wavelets to encode the input data into a binary stringformat. These techniques were validated using images from the CASIA database,and various other procedures were also tried and tested.
arxiv-1410-7140 | A data-driven method for syndrome type identification and classification in traditional Chinese medicine |  http://arxiv.org/abs/1410.7140  | author:Nevin L. Zhang, Chen Fu, Teng Fei Liu, Bao Xin Chen, Kin Man Poon, Pei Xian Chen, Yun Ling Zhang category:cs.LG stat.AP published:2014-10-27 summary:Objective: The efficacy of traditional Chinese medicine (TCM) treatments forWestern medicine (WM) diseases relies heavily on the proper classification ofpatients into TCM syndrome types. We develop a data-driven method for solvingthe classification problem, where syndrome types are identified and quantifiedbased on patterns detected in unlabeled symptom survey data. Method: Latent class analysis (LCA) has been applied in WM research to solvea similar problem, i.e., to identify subtypes of a patient population in theabsence of a gold standard. A widely known weakness of LCA is that it makes anunrealistically strong independence assumption. We relax the assumption byfirst detecting symptom co-occurrence patterns from survey data and use thosepatterns instead of the symptoms as features for LCA. Results: The result ofthe investigation is a six-step method: Data collection, symptom co-occurrencepattern discovery, pattern interpretation, syndrome identification, syndrometype identification, and syndrome type classification. A software packagecalled Lantern is developed to support the application of the method. Themethod is illustrated using a data set on Vascular Mild Cognitive Impairment(VMCI). Conclusions: A data-driven method for TCM syndrome identification andclassification is presented. The method can be used to answer the followingquestions about a Western medicine disease: What TCM syndrome types are thereamong the patients with the disease? What is the prevalence of each syndrometype? What are the statistical characteristics of each syndrome type in termsof occurrence of symptoms? How can we determine the syndrome type(s) of apatient?
arxiv-1410-7291 | Sensitivity Analysis for Computationally Expensive Models using Optimization and Objective-oriented Surrogate Approximations |  http://arxiv.org/abs/1410.7291  | author:Yilun Wang, Christine A. Shoemaker category:stat.ML 62K05 G.3 published:2014-10-27 summary:In this paper, we focus on developing efficient sensitivity analysis methodsfor a computationally expensive objective function $f(x)$ in the case that theminimization of it has just been performed. Here "computationally expensive"means that each of its evaluation takes significant amount of time, andtherefore our main goal to use a small number of function evaluations of $f(x)$to further infer the sensitivity information of these different parameters.Correspondingly, we consider the optimization procedure as an adaptiveexperimental design and re-use its available function evaluations as theinitial design points to establish a surrogate model $s(x)$ (or called responsesurface). The sensitivity analysis is performed on $s(x)$, which is an lieu of$f(x)$. Furthermore, we propose a new local multivariate sensitivity measure,for example, around the optimal solution, for high dimensional problems. Then acorresponding "objective-oriented experimental design" is proposed in order tomake the generated surrogate $s(x)$ better suitable for the accuratecalculation of the proposed specific local sensitivity quantities. In addition,we demonstrate the better performance of the Gaussian radial basis functioninterpolator over Kriging in our cases, which are of relatively highdimensionality and few experimental design points. Numerical experimentsdemonstrate that the optimization procedure and the "objective-orientedexperimental design" behavior much better than the classical Latin HypercubeDesign. In addition, the performance of Kriging is not as good as Gaussian RBF,especially in the case of high dimensional problems.
arxiv-1410-7265 | An Unsupervised Ensemble-based Markov Random Field Approach to Microscope Cell Image Segmentation |  http://arxiv.org/abs/1410.7265  | author:Balint Antal, Bence Remenyik, Andras Hajdu category:cs.CV cs.AI q-bio.QM published:2014-10-27 summary:In this paper, we propose an approach to the unsupervised segmentation ofimages using Markov Random Field. The proposed approach is based on the idea ofBit Plane Slicing. We use the planes as initial labellings for an ensemble ofsegmentations. With pixelwise voting, a robust segmentation approach can beachieved, which we demonstrate on microscope cell images. We tested ourapproach on a publicly available database, where it proven to be competitivewith other methods and manual segmentation.
arxiv-1410-7279 | Topology Adaptive Graph Estimation in High Dimensions |  http://arxiv.org/abs/1410.7279  | author:Johannes Lederer, Christian Müller category:stat.ML stat.ME published:2014-10-27 summary:We introduce Graphical TREX (GTREX), a novel method for graph estimation inhigh-dimensional Gaussian graphical models. By conducting neighborhoodselection with TREX, GTREX avoids tuning parameters and is adaptive to thegraph topology. We compare GTREX with standard methods on a new simulationset-up that is designed to assess accurately the strengths and shortcomings ofdifferent methods. These simulations show that a neighborhood selection schemebased on Lasso and an optimal (in practice unknown) tuning parameteroutperforms other standard methods over a large spectrum of scenarios.Moreover, we show that GTREX can rival this scheme and, therefore, can providecompetitive graph estimation without the need for tuning parameter calibration.
arxiv-1410-7371 | A General Statistic Framework for Genome-based Disease Risk Prediction |  http://arxiv.org/abs/1410.7371  | author:L. Ma, N. Lin, C. I. Amos, M. M. Xiong category:stat.ML published:2014-10-27 summary:Advances of modern sensing and sequencing technologies generate a deluge ofhigh dimensional space-temporal physiological and next-generation sequencing(NGS) data. Physiological traits are observed either as continuous randomfunctions, or on a dense grid and referred to as function-valued traits. Bothphysiological and NGS data are highly correlated data with their inherentorder, spacing, and functional nature which are ignored by traditionalsummary-based univariate and multivariate regression methods designed forquantitative genetic analysis of scalar trait and common variants. To capturemorphological and dynamic features of the data and utilize their dependentstructure, we propose a functional linear model (FLM) in which a trait curve ismodeled as a response function, the genetic variation in a genomic region orgene is modeled as a functional predictor, and the genetic effects are modeledas a function of both time and genomic position (FLMF) for genetic analysis offunction-valued trait with both GWAS and NGS data. By extensive simulations, wedemonstrate that the FLMF has the correct type 1 error rates and much higherpower to detect association than the existing methods. The FLMF is applied tosleep data from Starr County health studies where oxygen saturation weremeasured in 22,670 seconds on average for 833 individuals. We found 65 genesthat were significantly associated with oxygen saturation functional trait withP-values ranging from 2.40E-06 to 2.53E-21. The results clearly demonstratethat the FLMF substantially outperforms the traditional genetic models withscalar trait.
arxiv-1410-7372 | Feature Selection through Minimization of the VC dimension |  http://arxiv.org/abs/1410.7372  | author:Jayadeva, Sanjit S. Batra, Siddharth Sabharwal category:cs.LG I.5.1; I.5.2 published:2014-10-27 summary:Feature selection involes identifying the most relevant subset of inputfeatures, with a view to improving generalization of predictive models byreducing overfitting. Directly searching for the most relevant combination ofattributes is NP-hard. Variable selection is of critical importance in manyapplications, such as micro-array data analysis, where selecting a small numberof discriminative features is crucial to developing useful models of diseasemechanisms, as well as for prioritizing targets for drug discovery. Therecently proposed Minimal Complexity Machine (MCM) provides a way to learn ahyperplane classifier by minimizing an exact (\boldmath{$\Theta$}) bound on itsVC dimension. It is well known that a lower VC dimension contributes to goodgeneralization. For a linear hyperplane classifier in the input space, the VCdimension is upper bounded by the number of features; hence, a linearclassifier with a small VC dimension is parsimonious in the set of features itemploys. In this paper, we use the linear MCM to learn a classifier in which alarge number of weights are zero; features with non-zero weights are the onesthat are chosen. Selected features are used to learn a kernel SVM classifier.On a number of benchmark datasets, the features chosen by the linear MCM yieldcomparable or better test set accuracy than when methods such as ReliefF andFCBF are used for the task. The linear MCM typically chooses one-tenth thenumber of attributes chosen by the other methods; on some very high dimensionaldatasets, the MCM chooses about $0.6\%$ of the features; in comparison, ReliefFand FCBF choose 70 to 140 times more features, thus demonstrating thatminimizing the VC dimension may provide a new, and very effective route forfeature selection and for learning sparse representations.
arxiv-1410-6973 | Differentially- and non-differentially-private random decision trees |  http://arxiv.org/abs/1410.6973  | author:Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Yann LeCun category:cs.LG published:2014-10-26 summary:We consider supervised learning with random decision trees, where the treeconstruction is completely random. The method is popularly used and works wellin practice despite the simplicity of the setting, but its statisticalmechanism is not yet well-understood. In this paper we provide strongtheoretical guarantees regarding learning with random decision trees. Weanalyze and compare three different variants of the algorithm that have minimalmemory requirements: majority voting, threshold averaging and probabilisticaveraging. The random structure of the tree enables us to adapt these methodsto a differentially-private setting thus we also propose differentially-privateversions of all three schemes. We give upper-bounds on the generalization errorand mathematically explain how the accuracy depends on the number of randomdecision trees. Furthermore, we prove that only logarithmic (in the size of thedataset) number of independently selected random decision trees suffice tocorrectly classify most of the data, even when differential-privacy guaranteesmust be maintained. We empirically show that majority voting and thresholdaveraging give the best accuracy, also for conservative users requiring highprivacy guarantees. Furthermore, we demonstrate that a simple majority votingrule is an especially good candidate for the differentially-private classifiersince it is much less sensitive to the choice of forest parameters than othermethods.
arxiv-1410-6984 | Fully Automated Myocardial Infarction Classification using Ordinary Differential Equations |  http://arxiv.org/abs/1410.6984  | author:Getie Zewdie, Momiao Xiong category:stat.ML published:2014-10-26 summary:Portable, Wearable and Wireless electrocardiogram (ECG) Systems have thepotential to be used as point-of-care for cardiovascular disease diagnosticsystems. Such wearable and wireless ECG systems require automatic detection ofcardiovascular disease. Even in the primary care, automation of ECG diagnosticsystems will improve efficiency of ECG diagnosis and reduce the minimaltraining requirement of local healthcare workers. However, few fully automaticmyocardial infarction (MI) disease detection algorithms have well beendeveloped. This paper presents a novel automatic MI classification algorithmusing second order ordinary differential equation (ODE) with time varyingcoefficients, which simultaneously captures morphological and dynamic featureof highly correlated ECG signals. By effectively estimating the unobservedstate variables and the parameters of the second order ODE, the accuracy of theclassification was significantly improved. The estimated time varyingcoefficients of the second order ODE were used as an input to the supportvector machine (SVM) for the MI classification. The proposed method was appliedto the PTB diagnostic ECG database within Physionet. The overall sensitivity,specificity, and classification accuracy of 12 lead ECGs for MI binaryclassifications were 98.7%, 96.4% and 98.3%, respectively. We also found thateven using one lead ECG signals, we can reach accuracy as high as 97%.Multiclass MI classification is a challenging task but the developed ODEapproach for 12 lead ECGs coupled with multiclass SVM reached 96.4% accuracyfor classifying 5 subgroups of MI and healthy controls.
arxiv-1410-6990 | Local Rademacher Complexity for Multi-label Learning |  http://arxiv.org/abs/1410.6990  | author:Chang Xu, Tongliang Liu, Dacheng Tao, Chao Xu category:stat.ML cs.LG published:2014-10-26 summary:We analyze the local Rademacher complexity of empirical risk minimization(ERM)-based multi-label learning algorithms, and in doing so propose a newalgorithm for multi-label learning. Rather than using the trace norm toregularize the multi-label predictor, we instead minimize the tail sum of thesingular values of the predictor in multi-label learning. Benefiting from theuse of the local Rademacher complexity, our algorithm, therefore, has a sharpergeneralization error bound and a faster convergence rate. Compared to methodsthat minimize over all singular values, concentrating on the tail singularvalues results in better recovery of the low-rank structure of the multi-labelpredictor, which plays an import role in exploiting label correlations. Wepropose a new conditional singular value thresholding algorithm to solve theresulting objective function. Empirical studies on real-world datasets validateour theoretical results and demonstrate the effectiveness of the proposedalgorithm.
arxiv-1410-6975 | Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering |  http://arxiv.org/abs/1410.6975  | author:Apoorv Agarwal, Anna Choromanska, Krzysztof Choromanski category:cs.LG published:2014-10-26 summary:In this paper, we compare three initialization schemes for the KMEANSclustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of kneeds to be set by the user of the algorithms. (Kang 2013) recently proposed anovel use of determinantal point processes for sampling the initial centroidsfor the KMEANS algorithm (we call it KMEANSD++). They, however, do not provideany evaluation establishing that KMEANSD++ is better than other algorithms. Inthis paper, we show that the performance of KMEANSD++ is comparable to KMEANS++(both of which are better than KMEANSRAND) with KMEANSD++ having an additionalthat it can automatically approximate the value of k.
arxiv-1410-7050 | A PTAS for Agnostically Learning Halfspaces |  http://arxiv.org/abs/1410.7050  | author:Amit Daniely category:cs.DS cs.LG published:2014-10-26 summary:We present a PTAS for agnostically learning halfspaces w.r.t. the uniformdistribution on the $d$ dimensional sphere. Namely, we show that for every$\mu>0$ there is an algorithm that runs in time$\mathrm{poly}(d,\frac{1}{\epsilon})$, and is guaranteed to return a classifierwith error at most $(1+\mu)\mathrm{opt}+\epsilon$, where $\mathrm{opt}$ is theerror of the best halfspace classifier. This improves on Awasthi, Balcan andLong [ABL14] who showed an algorithm with an (unspecified) constantapproximation ratio. Our algorithm combines the classical technique ofpolynomial regression (e.g. [LMN89, KKMS05]), together with the newlocalization technique of [ABL14].
arxiv-1410-7383 | A Ternary Non-Commutative Latent Factor Model for Scalable Three-Way Real Tensor Completion |  http://arxiv.org/abs/1410.7383  | author:Guy Baruch category:stat.ML published:2014-10-26 summary:Motivated by large-scale Collaborative-Filtering applications, we present aNon-Commuting Latent Factor (NCLF) tensor-completion approach for modelingthree-way arrays, which is diagonal like the standard PARAFAC, but whereindifferent terms distinguish different kinds of three-way relations ofco-clusters, as determined by permutations of latent factors. The first keycomponent of the algebraic representation is the usage of two non-commutativereal trilinear operations as the building blocks of the approximation. Theseoperations are the standard three dimensional triple-product and a trilinearproduct on a two-dimensional real vector space, which is a representation ofthe real Clifford Algebra Cl(1,1) (a certain Majorana spinor). Both operationsare purely ternary in that they cannot be decomposed into two group-operationson the relevant spaces. The second key component of the method is combiningthese operations using permutation-symmetry preserving linear combinations. Weapply the model to the MovieLens and Fannie Mae datasets, and find that itoutperforms the PARAFAC model. We propose some future directions, such asunsupervised-learning.
arxiv-1410-7074 | Random Sampling in an Age of Automation: Minimizing Expenditures through Balanced Collection and Annotation |  http://arxiv.org/abs/1410.7074  | author:Oscar Beijbom category:cs.CY cs.LG stat.ME published:2014-10-26 summary:Methods for automated collection and annotation are changing thecost-structures of sampling surveys for a wide range of applications. Digitalsamples in the form of images or audio recordings can be collected rapidly, andannotated by computer programs or crowd workers. We consider the problem ofestimating a population mean under these new cost-structures, and propose aHybrid-Offset sampling design. This design utilizes two annotators: a primary,which is accurate but costly (e.g. a human expert) and an auxiliary which isnoisy but cheap (e.g. a computer program), in order to minimize total samplingexpenditures. Our analysis gives necessary conditions for the Hybrid-Offsetdesign and specifies optimal sample sizes for both annotators. Simulations ondata from a coral reef survey program indicate that the Hybrid-Offset designoutperforms several alternative sampling designs. In particular, samplingexpenditures are reduced 50% compared to the Conventional design currentlydeployed by the coral ecologists.
arxiv-1410-7029 | A Novel Statistical Method Based on Dynamic Models for Classification |  http://arxiv.org/abs/1410.7029  | author:Lerong Li, Momiao Xiong category:stat.ML published:2014-10-26 summary:Realizations of stochastic process are often observed temporal data orfunctional data. There are growing interests in classification of dynamic orfunctional data. The basic feature of functional data is that the functionaldata have infinite dimensions and are highly correlated. An essential issue forclassifying dynamic and functional data is how to effectively reduce theirdimension and explore dynamic feature. However, few statistical methods fordynamic data classification have directly used rich dynamic features of thedata. We propose to use second order ordinary differential equation (ODE) tomodel dynamic process and principal differential analysis to estimate constantor time-varying parameters in the ODE. We examine differential dynamicproperties of the dynamic system across different conditions includingstability and transient-response, which determine how the dynamic systemsmaintain their functions and performance under a broad range of random internaland external perturbations. We use the parameters in the ODE as features forclassifiers. As a proof of principle, the proposed methods are applied toclassifying normal and abnormal QRS complexes in the electrocardiogram (ECG)data analysis, which is of great clinical values in diagnosis of cardiovasculardiseases. We show that the ODE-based classification methods in QRS complexclassification outperform the currently widely used neural networks withFourier expansion coefficients of the functional data as their features. Weexpect that the dynamic model-based classification methods may open a newavenue for functional data classification.
arxiv-1410-7057 | Sparse Distributed Learning via Heterogeneous Diffusion Adaptive Networks |  http://arxiv.org/abs/1410.7057  | author:Bijit Kumar Das, Mrityunjoy Chakraborty, Jerónimo Arenas-García category:cs.LG cs.DC cs.SY stat.ML published:2014-10-26 summary:In-network distributed estimation of sparse parameter vectors via diffusionLMS strategies has been studied and investigated in recent years. In all theexisting works, some convex regularization approach has been used at each nodeof the network in order to achieve an overall network performance superior tothat of the simple diffusion LMS, albeit at the cost of increased computationaloverhead. In this paper, we provide analytical as well as experimental resultswhich show that the convex regularization can be selectively applied only tosome chosen nodes keeping rest of the nodes sparsity agnostic, while stillenjoying the same optimum behavior as can be realized by deploying the convexregularization at all the nodes. Due to the incorporation of unregularizedlearning at a subset of nodes, less computational cost is needed in theproposed approach. We also provide a guideline for selection of the sparsityaware nodes and a closed form expression for the optimum regularizationparameter.
arxiv-1411-1668 | On Chord and Sagitta in ${\mathbb Z}^2$: An Analysis towards Fast and Robust Circular Arc Detection |  http://arxiv.org/abs/1411.1668  | author:Sahadev Bera, Shyamosree Pal, Partha Bhowmick, Bhargab B. Bhattacharya category:cs.CG cs.CV published:2014-10-26 summary:Although chord and sagitta, when considered in tandem, may reflect manyunderlying geometric properties of circles on the Euclidean plane, theirimplications on the digital plane are not yet well-understood. In this paper,we explore some of their fundamental properties on the digital plane that havea strong bearing on the unsupervised detection of circles and circular arcs ina digital image. We show that although the chord-and-sagitta properties of areal circle do not readily migrate to the digital plane, they can indeed beused for the analysis in the discrete domain based on certain bounds on theirdeviations, which are derived from the real domain. In particular, we derive anupper bound on the circumferential angular deviation of a point in the contextof chord property, and an upper bound on the relative error in radiusestimation with regard to the sagitta property. Using these two bounds, wedesign a novel algorithm for the detection and parameterization of circles andcircular arcs, which does not require any heuristic initialization or manualtuning. The chord property is deployed for the detection of circular arcs,whereas the sagitta property is used to estimate their centers and radii.Finally, to improve the accuracy of estimation, the notion of restricted Houghtransform is used. Experimental results demonstrate superior efficiency androbustness of the proposed methodology compared to existing techniques.
arxiv-1410-7098 | Concavity of reweighted Kikuchi approximation |  http://arxiv.org/abs/1410.7098  | author:Po-Ling Loh, Andre Wibisono category:stat.ML math.ST stat.TH published:2014-10-26 summary:We analyze a reweighted version of the Kikuchi approximation for estimatingthe log partition function of a product distribution defined over a regiongraph. We establish sufficient conditions for the concavity of our reweightedobjective function in terms of weight assignments in the Kikuchi expansion, andshow that a reweighted version of the sum product algorithm applied to theKikuchi region graph will produce global optima of the Kikuchi approximationwhenever the algorithm converges. When the region graph has two layers,corresponding to a Bethe approximation, we show that our sufficient conditionsfor concavity are also necessary. Finally, we provide an explicitcharacterization of the polytope of concavity in terms of the cycle structureof the region graph. We conclude with simulations that demonstrate theadvantages of the reweighted Kikuchi approach.
arxiv-1410-6996 | Improved depth imaging by constrained full-waveform inversion |  http://arxiv.org/abs/1410.6996  | author:Musa Maharramov, Biondo Biondi category:physics.geo-ph cs.CV published:2014-10-26 summary:We propose a formulation of full-wavefield inversion (FWI) as a constrainedoptimization problem, and describe a computationally efficient technique forsolving constrained full-wavefield inversion (CFWI). The technique is based onusing a total-variation regularization method, with the regularization weightedin favor of constraining deeper subsurface model sections. The method helps topromote "edge-preserving" blocky model inversion where fitting the seismic dataalone fails to adequately constrain the model. The method is demonstrated onsynthetic datasets with added noise, and is shown to enhance the sharpness ofthe inverted model and correctly reposition mispositioned reflectors by betterconstraining the velocity model at depth.
arxiv-1410-6991 | A provable SVD-based algorithm for learning topics in dominant admixture corpus |  http://arxiv.org/abs/1410.6991  | author:Trapit Bansal, Chiranjib Bhattacharyya, Ravindran Kannan category:stat.ML cs.LG published:2014-10-26 summary:Topic models, such as Latent Dirichlet Allocation (LDA), posit that documentsare drawn from admixtures of distributions over words, known as topics. Theinference problem of recovering topics from admixtures, is NP-hard. Assumingseparability, a strong assumption, [4] gave the first provable algorithm forinference. For LDA model, [6] gave a provable algorithm using tensor-methods.But [4,6] do not learn topic vectors with bounded $l_1$ error (a naturalmeasure for probability vectors). Our aim is to develop a model which makesintuitive and empirically supported assumptions and to design an algorithm withnatural, simple components such as SVD, which provably solves the inferenceproblem for the model with bounded $l_1$ error. A topic in LDA and other modelsis essentially characterized by a group of co-occurring words. Motivated bythis, we introduce topic specific Catchwords, group of words which occur withstrictly greater frequency in a topic than any other topic individually and arerequired to have high frequency together rather than individually. A majorcontribution of the paper is to show that under this more realistic assumption,which is empirically verified on real corpora, a singular value decomposition(SVD) based algorithm with a crucial pre-processing step of thresholding, canprovably recover the topics from a collection of documents drawn from Dominantadmixtures. Dominant admixtures are convex combination of distributions inwhich one distribution has a significantly higher contribution than others.Apart from the simplicity of the algorithm, the sample complexity has nearoptimal dependence on $w_0$, the lowest probability that a topic is dominant,and is better than [4]. Empirical evidence shows that on several real worldcorpora, both Catchwords and Dominant admixture assumptions hold and theproposed algorithm substantially outperforms the state of the art [5].
arxiv-1410-6959 | An Aggregation Method for Sparse Logistic Regression |  http://arxiv.org/abs/1410.6959  | author:Zhe Liu category:stat.ML published:2014-10-25 summary:$L_1$ regularized logistic regression has now become a workhorse of datamining and bioinformatics: it is widely used for many classification problems,particularly ones with many features. However, $L_1$ regularization typicallyselects too many features and that so-called false positives are unavoidable.In this paper, we demonstrate and analyze an aggregation method for sparselogistic regression in high dimensions. This approach linearly combines theestimators from a suitable set of logistic models with different underlyingsparsity patterns and can balance the predictive ability and modelinterpretability. Numerical performance of our proposed aggregation method isthen investigated using simulation studies. We also analyze a publishedgenome-wide case-control dataset to further evaluate the usefulness of theaggregation method in multilocus association mapping.
arxiv-1410-6880 | Screening Rules for Overlapping Group Lasso |  http://arxiv.org/abs/1410.6880  | author:Seunghak Lee, Eric P. Xing category:stat.ML cs.LG published:2014-10-25 summary:Recently, to solve large-scale lasso and group lasso problems, screeningrules have been developed, the goal of which is to reduce the problem size byefficiently discarding zero coefficients using simple rules independently ofthe others. However, screening for overlapping group lasso remains an openchallenge because the overlaps between groups make it infeasible to test eachgroup independently. In this paper, we develop screening rules for overlappinggroup lasso. To address the challenge arising from groups with overlaps, wetake into account overlapping groups only if they are inclusive of the groupbeing tested, and then we derive screening rules, adopting the dual polytopeprojection approach. This strategy allows us to screen each group independentlyof each other. In our experiments, we demonstrate the efficiency of ourscreening rules on various datasets.
arxiv-1410-6909 | A Framework for On-Line Devanagari Handwritten Character Recognition |  http://arxiv.org/abs/1410.6909  | author:Sunil Kumar Kopparapu, Lajish V. L category:cs.CV published:2014-10-25 summary:The main challenge in on-line handwritten character recognition in Indianlan- guage is the large size of the character set, larger similarity betweendifferent characters in the script and the huge variation in writing style. Inthis paper we propose a framework for on-line handwitten script recognitiontaking cues from speech signal processing literature. The framework is based onidentify- ing strokes, which in turn lead to recognition of handwritten on-linecharacters rather that the conventional character identification. Though theframework is described for Devanagari script, the framework is general and canbe applied to any language. The proposed platform consists of pre-processing, feature extraction, recog-nition and post processing like the conventional character recognition but ap-plied to strokes. The on-line Devanagari character recognition reduces to oneof recognizing one of 69 primitives and recognition of a character is performedby recognizing a sequence of such primitives. We further show the impact ofnoise removal on on-line raw data which is usually noisy. The use of FuzzyDirec- tional Features to enhance the accuracy of stroke recognition is alsodescribed. The recognition results are compared with commonly used directionalfeatures in literature using several classifiers.
arxiv-1410-7382 | Modified Mel Filter Bank to Compute MFCC of Subsampled Speech |  http://arxiv.org/abs/1410.7382  | author:Kiran Kumar Bhuvanagiri, Sunil Kumar Kopparapu category:cs.CL cs.SD published:2014-10-25 summary:Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly usedspeech features in most speech and speaker recognition applications. In thiswork, we propose a modified Mel filter bank to extract MFCCs from subsampledspeech. We also propose a stronger metric which effectively captures thecorrelation between MFCCs of original speech and MFCC of resampled speech. Itis found that the proposed method of filter bank construction performsdistinguishably well and gives recognition performance on resampled speechclose to recognition accuracies on original speech.
arxiv-1410-6903 | Choice of Mel Filter Bank in Computing MFCC of a Resampled Speech |  http://arxiv.org/abs/1410.6903  | author:Laxmi Narayana M., Sunil Kumar Kopparapu category:cs.SD cs.CL published:2014-10-25 summary:Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly usedspeech features in most speech and speaker recognition applications. In thispaper, we study the effect of resampling a speech signal on these speechfeatures. We first derive a relationship between the MFCC param- eters of theresampled speech and the MFCC parameters of the original speech. We propose sixmethods of calculating the MFCC parameters of downsampled speech bytransforming the Mel filter bank used to com- pute MFCC of the original speech.We then experimentally compute the MFCC parameters of the down sampled speechusing the proposed meth- ods and compute the Pearson coefficient between theMFCC parameters of the downsampled speech and that of the original speech toidentify the most effective choice of Mel-filter band that enables the computedMFCC of the resampled speech to be as close as possible to the original speechsample MFCC.
arxiv-1410-6830 | Clustering Words by Projection Entropy |  http://arxiv.org/abs/1410.6830  | author:Işık Barış Fidaner, Ali Taylan Cemgil category:cs.CL cs.LG published:2014-10-24 summary:We apply entropy agglomeration (EA), a recently introduced algorithm, tocluster the words of a literary text. EA is a greedy agglomerative procedurethat minimizes projection entropy (PE), a function that can quantify thesegmentedness of an element set. To apply it, the text is reduced to a featureallocation, a combinatorial object to represent the word occurences in thetext's paragraphs. The experiment results demonstrate that EA, despite itsreduction and simplicity, is useful in capturing significant relationshipsamong the words in the text. This procedure was implemented in Python andpublished as a free software: REBUS.
arxiv-1410-6776 | Online and Stochastic Gradient Methods for Non-decomposable Loss Functions |  http://arxiv.org/abs/1410.6776  | author:Purushottam Kar, Harikrishna Narasimhan, Prateek Jain category:cs.LG stat.ML published:2014-10-24 summary:Modern applications in sensitive domains such as biometrics and medicinefrequently require the use of non-decomposable loss functions such asprecision@k, F-measure etc. Compared to point loss functions such ashinge-loss, these offer much more fine grained control over prediction, but atthe same time present novel challenges in terms of algorithm design andanalysis. In this work we initiate a study of online learning techniques forsuch non-decomposable loss functions with an aim to enable incremental learningas well as design scalable solvers for batch problems. To this end, we proposean online learning framework for such loss functions. Our model enjoys severalnice properties, chief amongst them being the existence of efficient onlinelearning algorithms with sublinear regret and online to batch conversionbounds. Our model is a provable extension of existing online learning modelsfor point loss functions. We instantiate two popular losses, prec@k and pAUC,in our model and prove sublinear regret bounds for both of them. Our proofsrequire a novel structural lemma over ranked lists which may be of independentinterest. We then develop scalable stochastic gradient descent solvers fornon-decomposable loss functions. We show that for a large family of lossfunctions satisfying a certain uniform convergence property (that includesprec@k, pAUC, and F-measure), our methods provably converge to the empiricalrisk minimizer. Such uniform convergence results were not known for theselosses and we establish these using novel proof techniques. We then useextensive experimentation on real life and benchmark datasets to establish thatour method can be orders of magnitude faster than a recently proposed cuttingplane method.
arxiv-1410-6736 | On The Effect of Hyperedge Weights On Hypergraph Learning |  http://arxiv.org/abs/1410.6736  | author:Sheng Huang, Ahmed Elgammal, Dan Yang category:cs.CV published:2014-10-24 summary:Hypergraph is a powerful representation in several computer vision, machinelearning and pattern recognition problems. In the last decade, many researchershave been keen to develop different hypergraph models. In contrast, no muchattention has been paid to the design of hyperedge weights. However, manystudies on pairwise graphs show that the choice of edge weight cansignificantly influence the performances of such graph algorithms. We arguethat this also applies to hypegraphs. In this paper, we empirically discuss theinfluence of hyperedge weight on hypegraph learning via proposing three novelhyperedge weights from the perspectives of geometry, multivariate statisticalanalysis and linear regression. Extensive experiments on ORL, COIL20, JAFFE,Sheffield, Scene15 and Caltech256 databases verify our hypothesis. Similar tograph learning, several representative hyperedge weighting schemes can beconcluded by our experimental studies. Moreover, the experiments alsodemonstrate that the combinations of such weighting schemes and conventionalhypergraph models can get very promising classification and clusteringperformances in comparison with some recent state-of-the-art algorithms.
arxiv-1410-6801 | Dimensionality Reduction for k-Means Clustering and Low Rank Approximation |  http://arxiv.org/abs/1410.6801  | author:Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco, Madalina Persu category:cs.DS cs.LG published:2014-10-24 summary:We show how to approximate a data matrix $\mathbf{A}$ with a much smallersketch $\mathbf{\tilde A}$ that can be used to solve a general class ofconstrained k-rank approximation problems to within $(1+\epsilon)$ error.Importantly, this class of problems includes $k$-means clustering andunconstrained low rank approximation (i.e. principal component analysis). Byreducing data points to just $O(k)$ dimensions, our methods genericallyaccelerate any exact, approximate, or heuristic algorithm for these ubiquitousproblems. For $k$-means dimensionality reduction, we provide $(1+\epsilon)$ relativeerror results for many common sketching techniques, including random rowprojection, column selection, and approximate SVD. For approximate principalcomponent analysis, we give a simple alternative to known algorithms that hasapplications in the streaming setting. Additionally, we extend recent work oncolumn-based matrix reconstruction, giving column subsets that not only `cover'a good subspace for $\bv{A}$, but can be used directly to compute thissubspace. Finally, for $k$-means clustering, we show how to achieve a $(9+\epsilon)$approximation by Johnson-Lindenstrauss projecting data points to just $O(\logk/\epsilon^2)$ dimensions. This gives the first result that leverages thespecific structure of $k$-means to achieve dimension independent of input sizeand sublinear in $k$.
arxiv-1410-6791 | Bayesian Manifold Learning: The Locally Linear Latent Variable Model (LL-LVM) |  http://arxiv.org/abs/1410.6791  | author:Mijung Park, Wittawat Jitkrittum, Ahmad Qamar, Zoltan Szabo, Lars Buesing, Maneesh Sahani category:stat.ML 62F15 G.3; I.2.6 published:2014-10-24 summary:We introduce the Locally Linear Latent Variable Model (LL-LVM), aprobabilistic model for non-linear manifold discovery that describes a jointdistribution over observations, their manifold coordinates and locally linearmaps conditioned on a set of neighbourhood relationships. The model allowsstraightforward variational optimisation of the posterior distribution oncoordinates and locally linear maps from the latent space to the observationspace given the data. Thus, the LL-LVM encapsulates the local-geometrypreserving intuitions that underlie non-probabilistic methods such as locallylinear embedding (LLE). Its probabilistic semantics make it easy to evaluatethe quality of hypothesised neighbourhood relationships, select the intrinsicdimensionality of the manifold, construct out-of-sample extensions and tocombine the manifold model with additional probabilistic models that capturethe structure of coordinates within the manifold.
arxiv-1410-6853 | Covariance Matrices for Mean Field Variational Bayes |  http://arxiv.org/abs/1410.6853  | author:Ryan Giordano, Tamara Broderick category:stat.ML cs.LG stat.ME published:2014-10-24 summary:Mean Field Variational Bayes (MFVB) is a popular posterior approximationmethod due to its fast runtime on large-scale data sets. However, it is wellknown that a major failing of MFVB is its (sometimes severe) underestimates ofthe uncertainty of model variables and lack of information about model variablecovariance. We develop a fast, general methodology for exponential familiesthat augments MFVB to deliver accurate uncertainty estimates for modelvariables -- both for individual variables and coherently across variables.MFVB for exponential families defines a fixed-point equation in the means ofthe approximating posterior, and our approach yields a covariance estimate byperturbing this fixed point. Inspired by linear response theory, we call ourmethod linear response variational Bayes (LRVB). We demonstrate the accuracy ofour method on simulated data sets.
arxiv-1410-6751 | Detecting Figures and Part Labels in Patents: Competition-Based Development of Image Processing Algorithms |  http://arxiv.org/abs/1410.6751  | author:Christoph Riedl, Richard Zanibbi, Marti A. Hearst, Siyu Zhu, Michael Menietti, Jason Crusan, Ivan Metelsky, Karim R. Lakhani category:cs.CV cs.IR published:2014-10-24 summary:We report the findings of a month-long online competition in whichparticipants developed algorithms for augmenting the digital version of patentdocuments published by the United States Patent and Trademark Office (USPTO).The goal was to detect figures and part labels in U.S. patent drawing pages.The challenge drew 232 teams of two, of which 70 teams (30%) submittedsolutions. Collectively, teams submitted 1,797 solutions that were compiled onthe competition servers. Participants reported spending an average of 63 hoursdeveloping their solutions, resulting in a total of 5,591 hours of developmenttime. A manually labeled dataset of 306 patents was used for training, onlinesystem tests, and evaluation. The design and performance of the top-5 systemsare presented, along with a system developed after the competition whichillustrates that winning teams produced near state-of-the-art results understrict time and computation constraints. For the 1st place system, the harmonicmean of recall and precision (f-measure) was 88.57% for figure regiondetection, 78.81% for figure regions with correctly recognized figure titles,and 70.98% for part label detection and character recognition. Data andsoftware from the competition are available through the online UCI MachineLearning repository to inspire follow-on work by the image processingcommunity.
arxiv-1410-6714 | Stochastic Blockmodeling for Online Advertising |  http://arxiv.org/abs/1410.6714  | author:Li Chen, Matthew Patton category:stat.ML stat.AP published:2014-10-24 summary:Online advertising is an important and huge industry. Having knowledge of thewebsite attributes can contribute greatly to business strategies forad-targeting, content display, inventory purchase or revenue prediction.Classical inferences on users and sites impose challenge, because the data isvoluminous, sparse, high-dimensional and noisy. In this paper, we introduce astochastic blockmodeling for the website relations induced by the event ofonline user visitation. We propose two clustering algorithms to discover theinstrinsic structures of websites, and compare the performance with agoodness-of-fit method and a deterministic graph partitioning method. Wedemonstrate the effectiveness of our algorithms on both simulation and AOLwebsite dataset.
arxiv-1410-6834 | Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes |  http://arxiv.org/abs/1410.6834  | author:Yves-Laurent Kom Samo, Stephen Roberts category:stat.ML published:2014-10-24 summary:In this paper we propose the first non-parametric Bayesian model usingGaussian Processes to make inference on Poisson Point Processes withoutresorting to gridding the domain or to introducing latent thinning points.Unlike competing models that scale cubically and have a squared memoryrequirement in the number of data points, our model has a linear complexity andmemory requirement. We propose an MCMC sampler and show that our model isfaster, more accurate and generates less correlated samples than competingmodels on both synthetic and real-life data. Finally, we show that our modeleasily handles data sizes not considered thus far by alternate approaches.
arxiv-1410-6604 | Median Selection Subset Aggregation for Parallel Inference |  http://arxiv.org/abs/1410.6604  | author:Xiangyu Wang, Peichao Peng, David Dunson category:stat.ML cs.DC stat.CO stat.ME published:2014-10-24 summary:For massive data sets, efficient computation commonly relies on distributedalgorithms that store and process subsets of the data on different machines,minimizing communication costs. Our focus is on regression and classificationproblems involving many features. A variety of distributed algorithms have beenproposed in this context, but challenges arise in defining an algorithm withlow communication, theoretical guarantees and excellent practical performancein general settings. We propose a MEdian Selection Subset AGgregation Estimator(message) algorithm, which attempts to solve these problems. The algorithmapplies feature selection in parallel for each subset using Lasso or anothermethod, calculates the `median' feature inclusion index, estimates coefficientsfor the selected features in parallel for each subset, and then averages theseestimates. The algorithm is simple, involves very minimal communication, scalesefficiently in both sample and feature size, and has theoretical guarantees. Inparticular, we show model selection consistency and coefficient estimationefficiency. Extensive experiments show excellent performance in variableselection, estimation, prediction, and computation time relative to usualcompetitors.
arxiv-1410-6532 | A Novel Visual Word Co-occurrence Model for Person Re-identification |  http://arxiv.org/abs/1410.6532  | author:Ziming Zhang, Yuting Chen, Venkatesh Saligrama category:cs.CV published:2014-10-24 summary:Person re-identification aims to maintain the identity of an individual indiverse locations through different non-overlapping camera views. The problemis fundamentally challenging due to appearance variations resulting fromdiffering poses, illumination and configurations of camera views. To deal withthese difficulties, we propose a novel visual word co-occurrence model. Wefirst map each pixel of an image to a visual word using a codebook, which islearned in an unsupervised manner. The appearance transformation between cameraviews is encoded by a co-occurrence matrix of visual word joint distributionsin probe and gallery images. Our appearance model naturally accounts forspatial similarities and variations caused by pose, illumination &configuration change across camera views. Linear SVMs are then trained asclassifiers using these co-occurrence descriptors. On the VIPeR and CUHK Campusbenchmark datasets, our method achieves 83.86% and 85.49% at rank-15 on theCumulative Match Characteristic (CMC) curves, and beats the state-of-the-artresults by 10.44% and 22.27%.
arxiv-1410-6289 | Signal inference with unknown response: Calibration-uncertainty renormalized estimator |  http://arxiv.org/abs/1410.6289  | author:Sebastian Dorn, Torsten A. Enßlin, Maksim Greiner, Marco Selig, Vanessa Boehm category:astro-ph.IM cs.IT math.IT stat.ML published:2014-10-23 summary:The calibration of a measurement device is crucial for every scientificexperiment, where a signal has to be inferred from data. We present CURE, thecalibration uncertainty renormalized estimator, to reconstruct a signal andsimultaneously the instrument's calibration from the same data without knowingthe exact calibration, but its covariance structure. The idea of CURE,developed in the framework of information field theory, is starting with anassumed calibration to successively include more and more portions ofcalibration uncertainty into the signal inference equations and to absorb theresulting corrections into renormalized signal (and calibration) solutions.Thereby, the signal inference and calibration problem turns into solving asingle system of ordinary differential equations and can be identified withcommon resummation techniques used in field theories. We verify CURE byapplying it to a simplistic toy example and compare it against existentself-calibration schemes, Wiener filter solutions, and Markov Chain Monte Carlosampling. We conclude that the method is able to keep up in accuracy with thebest self-calibration methods and serves as a non-iterative alternative to it.
arxiv-1410-6466 | Model Selection for Topic Models via Spectral Decomposition |  http://arxiv.org/abs/1410.6466  | author:Dehua Cheng, Xinran He, Yan Liu category:stat.ML cs.IR cs.LG stat.CO 62H30 H.3.3 published:2014-10-23 summary:Topic models have achieved significant successes in analyzing large-scaletext corpus. In practical applications, we are always confronted with thechallenge of model selection, i.e., how to appropriately set the number oftopics. Following recent advances in topic model inference via tensordecomposition, we make a first attempt to provide theoretical analysis on modelselection in latent Dirichlet allocation. Under mild conditions, we derive theupper bound and lower bound on the number of topics given a text collection offinite size. Experimental results demonstrate that our bounds are accurate andtight. Furthermore, using Gaussian mixture model as an example, we show thatour methodology can be easily generalized to model selection analysis for otherlatent models.
arxiv-1410-6264 | Capturing spatial interdependence in image features: the counting grid, an epitomic representation for bags of features |  http://arxiv.org/abs/1410.6264  | author:Alessandro Perina, Nebojsa Jojic category:cs.CV stat.ML published:2014-10-23 summary:In recent scene recognition research images or large image regions are oftenrepresented as disorganized "bags" of features which can then be analyzed usingmodels originally developed to capture co-variation of word counts in text.However, image feature counts are likely to be constrained in different waysthan word counts in text. For example, as a camera pans upwards from a buildingentrance over its first few floors and then further up into the sky Fig. 1,some feature counts in the image drop while others rise -- only to drop againgiving way to features found more often at higher elevations. The space of allpossible feature count combinations is constrained both by the properties ofthe larger scene and the size and the location of the window into it. Tocapture such variation, in this paper we propose the use of the counting gridmodel. This generative model is based on a grid of feature counts, considerablylarger than any of the modeled images, and considerably smaller than the realestate needed to tile the images next to each other tightly. Each modeled imageis assumed to have a representative window in the grid in which the featurecounts mimic the feature distribution in the image. We provide a learningprocedure that jointly maps all images in the training set to the counting gridand estimates the appropriate local counts in it. Experimentally, wedemonstrate that the resulting representation captures the space of featurecount combinations more accurately than the traditional models, not only whenthe input images come from a panning camera, but even when modeling images ofdifferent scenes from the same category.
arxiv-1410-6387 | On Lower and Upper Bounds in Smooth Strongly Convex Optimization - A Unified Approach via Linear Iterative Methods |  http://arxiv.org/abs/1410.6387  | author:Yossi Arjevani category:math.OC cs.LG published:2014-10-23 summary:In this thesis we develop a novel framework to study smooth and stronglyconvex optimization algorithms, both deterministic and stochastic. Focusing onquadratic functions we are able to examine optimization algorithms as arecursive application of linear operators. This, in turn, reveals a powerfulconnection between a class of optimization algorithms and the analytic theoryof polynomials whereby new lower and upper bounds are derived. In particular,we present a new and natural derivation of Nesterov's well-known AcceleratedGradient Descent method by employing simple 'economic' polynomials. This rathernatural interpretation of AGD contrasts with earlier ones which lacked asimple, yet solid, motivation. Lastly, whereas existing lower bounds are onlyvalid when the dimensionality scales with the number of iterations, our lowerbound holds in the natural regime where the dimensionality is fixed.
arxiv-1410-6271 | A General Stochastic Algorithmic Framework for Minimizing Expensive Black Box Objective Functions Based on Surrogate Models and Sensitivity Analysis |  http://arxiv.org/abs/1410.6271  | author:Yilun Wang, Christine A. Shoemaker category:stat.ML 90C26, 90C56 published:2014-10-23 summary:We are focusing on bound constrained global optimization problems, whoseobjective functions are computationally expensive black-box functions and havemultiple local minima. The recently popular Metric Stochastic Response Surface(MSRS) algorithm proposed by \cite{Regis2007SRBF} based on adaptive orsequential learning based on response surfaces is revisited and furtherextended for better performance in case of higher dimensional problems.Specifically, we propose a new way to generate the candidate points which thenext function evaluation point is picked from according to the metric criteria,based on a new definition of distance, and prove the global convergence of thecorresponding. Correspondingly, a more adaptive implementation of MSRS, named"SO-SA", is presented. "SO-SA" is is more likely to perturb those mostsensitive coordinates when generating the candidate points, instead ofperturbing all coordinates simultaneously. Numerical experiments on both synthetic problems and real problemsdemonstrate the advantages of our new algorithm, compared with many state ofthe art alternatives.}
arxiv-1410-6460 | Markov Chain Monte Carlo and Variational Inference: Bridging the Gap |  http://arxiv.org/abs/1410.6460  | author:Tim Salimans, Diederik P. Kingma, Max Welling category:stat.CO stat.ML published:2014-10-23 summary:Recent advances in stochastic gradient variational inference have made itpossible to perform variational Bayesian inference with posteriorapproximations containing auxiliary random variables. This enables us toexplore a new synthesis of variational inference and Monte Carlo methods wherewe incorporate one or more steps of MCMC into our variational approximation. Bydoing so we obtain a rich class of inference algorithms bridging the gapbetween variational methods and MCMC, and offering the best of both worlds:fast posterior approximation through the maximization of an explicit objective,with the option of trading off additional computation for additional accuracy.We describe the theoretical foundations that make this possible and show somepromising first results.
arxiv-1410-6413 | Initialization of multilayer forecasting artifical neural networks |  http://arxiv.org/abs/1410.6413  | author:Vladimir V. Bochkarev, Yulia S. Maslennikova category:cs.NE stat.ME I.5.1 published:2014-10-23 summary:In this paper, a new method was developed for initialising artificial neuralnetworks predicting dynamics of time series. Initial weighting coefficientswere determined for neurons analogously to the case of a linear predictionfilter. Moreover, to improve the accuracy of the initialization method for amultilayer neural network, some variants of decomposition of the transformationmatrix corresponding to the linear prediction filter were suggested. Theefficiency of the proposed neural network prediction method by forecastingsolutions of the Lorentz chaotic system is shown in this paper.
arxiv-1410-6313 | Canonical Polyadic Decomposition with Auxiliary Information for Brain Computer Interface |  http://arxiv.org/abs/1410.6313  | author:Junhua Li, Chao Li, Andrzej Cichocki category:cs.CV published:2014-10-23 summary:Physiological signals are often organized in the form of multiple dimensions(e.g., channel, time, task, and 3D voxel), so it is better to preserve originalorganization structure when processing. Unlike vector-based methods thatdestroy data structure, Canonical Polyadic Decomposition (CPD) aims to processphysiological signals in the form of multi-way array, which considersrelationships between dimensions and preserves structure information containedby the physiological signal. Nowadays, CPD is utilized as an unsupervisedmethod for feature extraction in a classification problem. After that, aclassifier, such as support vector machine, is required to classify thosefeatures. In this manner, classification task is achieved in two isolatedsteps. We proposed supervised Canonical Polyadic Decomposition by directlyincorporating auxiliary label information during decomposition, by which aclassification task can be achieved without an extra step of classifiertraining. The proposed method merges the decomposition and classifier learningtogether, so it reduces procedure of classification task compared with that ofrespective decomposition and classification. In order to evaluate theperformance of the proposed method, three different kinds of signals, syntheticsignal, EEG signal, and MEG signal, were used. The results based on evaluationsof synthetic and real signals demonstrated that the proposed method iseffective and efficient.
arxiv-1410-6333 | A Regularization Approach to Blind Deblurring and Denoising of QR Barcodes |  http://arxiv.org/abs/1410.6333  | author:Yves van Gennip, Prashant Athavale, Jérôme Gilles, Rustum Choksi category:cs.CV math.NA 68U10, 65K10 published:2014-10-23 summary:QR bar codes are prototypical images for which part of the image is a prioriknown (required patterns). Open source bar code readers, such as ZBar, arereadily available. We exploit both these facts to provide and assess purelyregularization-based methods for blind deblurring of QR bar codes in thepresence of noise.
arxiv-1410-6447 | Density-Based Region Search with Arbitrary Shape for Object Localization |  http://arxiv.org/abs/1410.6447  | author:Ji Zhao, Deyu Meng, Jiayi Ma category:cs.CV published:2014-10-23 summary:Region search is widely used for object localization. Typically, the regionsearch methods project the score of a classifier into an image plane, and thensearch the region with the maximal score. The recently proposed region searchmethods, such as efficient subwindow search and efficient region search, %whichlocalize objects from the score distribution on an image are much moreefficient than sliding window search. However, for some classifiers and tasks,the projected scores are nearly all positive, and hence maximizing the score ofa region results in localizing nearly the entire images as objects, which ismeaningless. In this paper, we observe that the large scores are mainly concentrated on oraround objects. Based on this observation, we propose a method, named level setmaximum-weight connected subgraph (LS-MWCS), which localizes objects witharbitrary shapes by searching regions with the densest score rather than themaximal score. The region density can be controlled by a parameter flexibly.And we prove an important property of the proposed LS-MWCS, which guaranteesthat the region with the densest score can be searched. Moreover, the LS-MWCScan be efficiently optimized by belief propagation. The method is evaluated onthe problem of weakly-supervised object localization, and the quantitativeresults demonstrate the superiorities of our LS-MWCS compared to otherstate-of-the-art methods.
arxiv-1410-6472 | Foreground-Background Segmentation Based on Codebook and Edge Detector |  http://arxiv.org/abs/1410.6472  | author:Mikaël A. Mousse, Eugène C. Ezin, Cina Motamed category:cs.CV published:2014-10-23 summary:Background modeling techniques are used for moving object detection in video.Many algorithms exist in the field of object detection with different purposes.In this paper, we propose an improvement of moving object detection based oncodebook segmentation. We associate the original codebook algorithm with anedge detection algorithm. Our goal is to prove the efficiency of using an edgedetection algorithm with a background modeling algorithm. Throughout our study,we compared the quality of the moving object detection when codebooksegmentation algorithm is associated with some standard edge detectors. In eachcase, we use frame-based metrics for the evaluation of the detection. Thedifferent results are presented and analyzed.
arxiv-1410-6382 | Attribute Efficient Linear Regression with Data-Dependent Sampling |  http://arxiv.org/abs/1410.6382  | author:Doron Kukliansky, Ohad Shamir category:cs.LG stat.ML published:2014-10-23 summary:In this paper we analyze a budgeted learning setting, in which the learnercan only choose and observe a small subset of the attributes of each trainingexample. We develop efficient algorithms for ridge and lasso linear regression,which utilize the geometry of the data by a novel data-dependent samplingscheme. When the learner has prior knowledge on the second moments of theattributes, the optimal sampling probabilities can be calculated precisely, andresult in data-dependent improvements factors for the excess risk over thestate-of-the-art that may be as large as $O(\sqrt{d})$, where $d$ is theproblem's dimension. Moreover, under reasonable assumptions our algorithms canuse less attributes than full-information algorithms, which is the main concernin budgeted learning settings. To the best of our knowledge, these are thefirst algorithms able to do so in our setting. Where no such prior knowledge isavailable, we develop a simple estimation technique that given a sufficientamount of training examples, achieves similar improvements. We complement ourtheoretical analysis with experiments on several data sets which support ourclaims.
arxiv-1410-6095 | Online Energy Price Matrix Factorization for Power Grid Topology Tracking |  http://arxiv.org/abs/1410.6095  | author:Vassilis Kekatos, Georgios B. Giannakis, Ross Baldick category:stat.ML cs.LG math.OC stat.AP published:2014-10-22 summary:Grid security and open markets are two major smart grid goals. Transparencyof market data facilitates a competitive and efficient energy environment, yetit may also reveal critical physical system information. Recovering the gridtopology based solely on publicly available market data is explored here.Real-time energy prices are calculated as the Lagrange multipliers ofnetwork-constrained economic dispatch; that is, via a linear program (LP)typically solved every 5 minutes. Granted the grid Laplacian is a parameter ofthis LP, one could infer such a topology-revealing matrix upon observingsuccessive LP dual outcomes. The matrix of spatio-temporal prices is firstshown to factor as the product of the inverse Laplacian times a sparse matrix.Leveraging results from sparse matrix decompositions, topology recovery schemeswith complementary strengths are subsequently formulated. Solvers scalable tohigh-dimensional and streaming market data are devised. Numerical validationusing real load data on the IEEE 30-bus grid provide useful input for currentand future market designs.
arxiv-1410-6131 | Penalized versus constrained generalized eigenvalue problems |  http://arxiv.org/abs/1410.6131  | author:Irina Gaynanova, James Booth, Martin T. Wells category:stat.CO stat.ML published:2014-10-22 summary:We investigate the difference between using an $\ell_1$ penalty versus an$\ell_1$ constraint in generalized eigenvalue problems, such as principalcomponent analysis and discriminant analysis. Our main finding is that an$\ell_1$ penalty may fail to provide very sparse solutions; a severedisadvantage for variable selection that can be remedied by using an $\ell_1$constraint. Our claims are supported both by empirical evidence and theoreticalanalysis. Finally, we illustrate the advantages of an $\ell_1$ constraint inthe context of discriminant analysis and principal component analysis.
arxiv-1410-6126 | Motion Estimation via Robust Decomposition with Constrained Rank |  http://arxiv.org/abs/1410.6126  | author:German Ros, Jose Alvarez, Julio Guerrero category:cs.CV published:2014-10-22 summary:In this work, we address the problem of outlier detection for robust motionestimation by using modern sparse-low-rank decompositions, i.e., RobustPCA-like methods, to impose global rank constraints. Robust decompositions haveshown to be good at splitting a corrupted matrix into an uncorrupted low-rankmatrix and a sparse matrix, containing outliers. However, this process onlyworks when matrices have relatively low rank with respect to their ambientspace, a property not met in motion estimation problems. As a solution, wepropose to exploit the partial information present in the decomposition todecide which matches are outliers. We provide evidences showing that even whenit is not possible to recover an uncorrupted low-rank matrix, the resultinginformation can be exploited for outlier detection. To this end we propose theRobust Decomposition with Constrained Rank (RD-CR), a proximal gradient basedmethod that enforces the rank constraints inherent to motion estimation. Wealso present a general framework to perform robust estimation for stereo VisualOdometry, based on our RD-CR and a simple but effective compressed optimizationmethod that achieves high performance. Our evaluation on synthetic data and onthe KITTI dataset demonstrates the applicability of our approach in complexscenarios and it yields state-of-the-art performance.
arxiv-1410-6414 | A Parallel and Efficient Algorithm for Learning to Match |  http://arxiv.org/abs/1410.6414  | author:Jingbo Shang, Tianqi Chen, Hang Li, Zhengdong Lu, Yong Yu category:cs.LG cs.AI published:2014-10-22 summary:Many tasks in data mining and related fields can be formalized as matchingbetween objects in two heterogeneous domains, including collaborativefiltering, link prediction, image tagging, and web search. Machine learningtechniques, referred to as learning-to-match in this paper, have beensuccessfully applied to the problems. Among them, a class of state-of-the-artmethods, named feature-based matrix factorization, formalize the task as anextension to matrix factorization by incorporating auxiliary features into themodel. Unfortunately, making those algorithms scale to real world problems ischallenging, and simple parallelization strategies fail due to the complexcross talking patterns between sub-tasks. In this paper, we tackle thischallenge with a novel parallel and efficient algorithm for feature-basedmatrix factorization. Our algorithm, based on coordinate descent, can easilyhandle hundreds of millions of instances and features on a single machine. Thekey recipe of this algorithm is an iterative relaxation of the objective tofacilitate parallel updates of parameters, with guaranteed convergence onminimizing the original objective function. Experimental results demonstratethat the proposed method is effective on a wide range of matching problems,with efficiency significantly improved upon the baselines while accuracyretained unchanged.
arxiv-1410-6093 | Cosine Similarity Measure According to a Convex Cost Function |  http://arxiv.org/abs/1410.6093  | author:Osman Gunay, Cem Emre Akbas, A. Enis Cetin category:cs.LG published:2014-10-22 summary:In this paper, we describe a new vector similarity measure associated with aconvex cost function. Given two vectors, we determine the surface normals ofthe convex function at the vectors. The angle between the two surface normalsis the similarity measure. Convex cost function can be the negative entropyfunction, total variation (TV) function and filtered variation function. Theconvex cost function need not be differentiable everywhere. In general, we needto compute the gradient of the cost function to compute the surface normals. Ifthe gradient does not exist at a given vector, it is possible to use thesubgradients and the normal producing the smallest angle between the twovectors is used to compute the similarity measure.
arxiv-1410-6031 | Demixed principal component analysis of population activity in higher cortical areas reveals independent representation of task parameters |  http://arxiv.org/abs/1410.6031  | author:Dmitry Kobak, Wieland Brendel, Christos Constantinidis, Claudia E. Feierstein, Adam Kepecs, Zachary F. Mainen, Ranulfo Romo, Xue-Lian Qi, Naoshige Uchida, Christian K. Machens category:q-bio.NC stat.ML published:2014-10-22 summary:Neurons in higher cortical areas, such as the prefrontal cortex, are known tobe tuned to a variety of sensory and motor variables. The resulting diversityof neural tuning often obscures the represented information. Here we introducea novel dimensionality reduction technique, demixed principal componentanalysis (dPCA), which automatically discovers and highlights the essentialfeatures in complex population activities. We reanalyze population data fromthe prefrontal areas of rats and monkeys performing a variety of working memoryand decision-making tasks. In each case, dPCA summarizes the relevant featuresof the population response in a single figure. The population activity isdecomposed into a few demixed components that capture most of the variance inthe data and that highlight dynamic tuning of the population to various taskparameters, such as stimuli, decisions, rewards, etc. Moreover, dPCA revealsstrong, condition-independent components of the population activity that remainunnoticed with conventional approaches.
arxiv-1410-5926 | Salient Object Detection: A Discriminative Regional Feature Integration Approach |  http://arxiv.org/abs/1410.5926  | author:Huaizu Jiang, Zejian Yuan, Ming-Ming Cheng, Yihong Gong, Nanning Zheng, Jingdong Wang category:cs.CV published:2014-10-22 summary:Salient object detection has been attracting a lot of interest, and recentlyvarious heuristic computational models have been designed. In this paper, weformulate saliency map computation as a regression problem. Our method, whichis based on multi-level image segmentation, utilizes the supervised learningapproach to map the regional feature vector to a saliency score. Saliencyscores across multiple levels are finally fused to produce the saliency map.The contributions lie in two-fold. One is that we propose a discriminateregional feature integration approach for salient object detection. Comparedwith existing heuristic models, our proposed method is able to automaticallyintegrate high-dimensional regional saliency features and choose discriminativeones. The other is that by investigating standard generic region properties aswell as two widely studied concepts for salient object detection, i.e.,regional contrast and backgroundness, our approach significantly outperformsstate-of-the-art methods on six benchmark datasets. Meanwhile, we demonstratethat our method runs as fast as most existing algorithms.
arxiv-1410-5920 | Active Regression by Stratification |  http://arxiv.org/abs/1410.5920  | author:Sivan Sabato, Remi Munos category:stat.ML cs.LG published:2014-10-22 summary:We propose a new active learning algorithm for parametric linear regressionwith random design. We provide finite sample convergence guarantees for generaldistributions in the misspecified model. This is the first active learner forthis setting that provably can improve over passive learning. Unlike otherlearning settings (such as classification), in regression the passive learningrate of $O(1/\epsilon)$ cannot in general be improved upon. Nonetheless, theso-called `constant' in the rate of convergence, which is characterized by adistribution-dependent risk, can be improved in many cases. For a givendistribution, achieving the optimal risk requires prior knowledge of thedistribution. Following the stratification technique advocated in Monte-Carlofunction integration, our active learner approaches the optimal risk usingpiecewise constant approximations.
arxiv-1410-5894 | Vehicle Detection and Tracking Techniques: A Concise Review |  http://arxiv.org/abs/1410.5894  | author:Raad Ahmed Hadi, Ghazali Sulong, Loay Edwar George category:cs.CV published:2014-10-22 summary:Vehicle detection and tracking applications play an important role forcivilian and military applications such as in highway traffic surveillancecontrol, management and urban traffic planning. Vehicle detection process onroad are used for vehicle tracking, counts, average speed of each individualvehicle, traffic analysis and vehicle categorizing objectives and may beimplemented under different environments changes. In this review, we present aconcise overview of image processing methods and analysis tools which used inbuilding these previous mentioned applications that involved developing trafficsurveillance systems. More precisely and in contrast with other reviews, weclassified the processing methods under three categories for more clarificationto explain the traffic systems.
arxiv-1410-5600 | Mobility Enhancement for Elderly |  http://arxiv.org/abs/1410.5600  | author:Ramviyas Parasuraman category:cs.CV cs.RO published:2014-10-21 summary:Loss of Mobility is a common handicap to senior citizens. It denies them theease of movement they would like to have like outdoor visits, movement inhospitals, social outgoings, but more seriously in the day to day in-houseroutine functions necessary for living etc. Trying to overcome this handicap bymeans of servant or domestic help and simple wheel chairs is not only costly inthe long run, but forces the senior citizen to be at the mercy of sincerity ofdomestic helps and also the consequent loss of dignity. In order to give adignified life, the mobility obtained must be at the complete discretion, willand control of the senior citizen. This can be provided only by a reasonablysophisticated and versatile wheel chair, giving enhanced ability of vision,hearing through man-machine interface, and sensor aided navigation and control.More often than not senior people have poor vision which makes it difficult forthem to maker visual judgement and so calls for the use of ArtificialIntelligence in visual image analysis and guided navigation systems. In this project, we deal with two important enhancement features for mobilityenhancement, Audio command and Vision aided obstacle detection and navigation.We have implemented speech recognition algorithm using template of stored wordsfor identifying the voice command given by the user. This frees the user of anagile hand to operate joystick or mouse control. Also, we have developed a newappearance based obstacle detection system using stereo-vision cameras whichestimates the distance of nearest obstacle to the wheel chair and takesnecessary action. This helps user in making better judgement of route andnavigate obstacles. The main challenge in this project is how to navigate in anunknown/unfamiliar environment by avoiding obstacles.
arxiv-1410-5524 | Learning to Rank Binary Codes |  http://arxiv.org/abs/1410.5524  | author:Jie Feng, Wei Liu, Yan Wang category:cs.CV published:2014-10-21 summary:Binary codes have been widely used in vision problems as a compact featurerepresentation to achieve both space and time advantages. Various methods havebeen proposed to learn data-dependent hash functions which map a feature vectorto a binary code. However, considerable data information is inevitably lostduring the binarization step which also causes ambiguity in measuring samplesimilarity using Hamming distance. Besides, the learned hash functions cannotbe changed after training, which makes them incapable of adapting to new dataoutside the training data set. To address both issues, in this paper we proposea flexible bitwise weight learning framework based on the binary codes obtainedby state-of-the-art hashing methods, and incorporate the learned weights intothe weighted Hamming distance computation. We then formulate the proposedframework as a ranking problem and leverage the Ranking SVM model to offlinetackle the weight learning. The framework is further extended to an online modewhich updates the weights at each time new data comes, thereby making itscalable to large and dynamic data sets. Extensive experimental resultsdemonstrate significant performance gains of using binary codes with bitwiseweighting in image retrieval tasks. It is appealing that the online weightlearning leads to comparable accuracy with its offline counterpart, which thusmakes our approach practical for realistic applications.
arxiv-1410-5557 | Where do goals come from? A Generic Approach to Autonomous Goal-System Development |  http://arxiv.org/abs/1410.5557  | author:Matthias Rolf, Minoru Asada category:cs.LG cs.AI published:2014-10-21 summary:Goals express agents' intentions and allow them to organize their behaviorbased on low-dimensional abstractions of high-dimensional world states. How canagents develop such goals autonomously? This paper proposes a detailedconceptual and computational account to this longstanding problem. We argue toconsider goals as high-level abstractions of lower-level intention mechanismssuch as rewards and values, and point out that goals need to be consideredalongside with a detection of the own actions' effects. We propose Latent GoalAnalysis as a computational learning formulation thereof, and showconstructively that any reward or value function can by explained by goals andsuch self-detection as latent mechanisms. We first show that learned goalsprovide a highly effective dimensionality reduction in a practicalreinforcement learning problem. Then, we investigate a developmental scenarioin which entirely task-unspecific rewards induced by visual saliency lead toself and goal representations that constitute goal-directed reaching.
arxiv-1410-5610 | Universality of Power Law Coding for Principal Neurons |  http://arxiv.org/abs/1410.5610  | author:Gabriele Scheler category:q-bio.NC cs.NE published:2014-10-21 summary:In this paper we document distributions for spike rates, synaptic weights andneural gains for principal neurons in various tissues and under differentbehavioral conditions. We find a remarkable consistency of a power-law,specifically lognormal, distribution across observations from auditory orvisual cortex as well as midbrain nuclei, cerebellar Purkinje cells andstriatal medium spiny neurons. An exception is documented for fast-spikinginterneurons, as non-coding neurons, which seem to follow a normaldistribution. The difference between strongly recurrent and transferconnectivity (cortex vs. striatum and cerebellum), or the level of activation(low in cortex, high in Purkinje cells and midbrain nuclei) seems to beirrelevant for these distributions. This has certain implications on neuralcoding. In particular, logarithmic scale distribution of neuronal outputappears as a structural phenomenon that is always present in coding neurons. Wealso report data for a lognormal distribution of synaptic strengths in cortex,cerebellum and hippocampus and for intrinsic excitability in striatum, cortexand cerebellum. We present a neural model for gain, weights and spike rates,specifically matching the width of distributions. We discuss the data from theperspective of a hierarchical coding scheme with few sparse or top-levelfeatures and many additional distributed low-level features. Logarithmic-scalecoding may solve an access problem by combining a local modular structure withfew high frequency contact points. Computational models may need to incorporatethese observations as primary constraints. More data are needed to consolidatethe observations.
arxiv-1410-5816 | Daily Stress Recognition from Mobile Phone Data, Weather Conditions and Individual Traits |  http://arxiv.org/abs/1410.5816  | author:Andrey Bogomolov, Bruno Lepri, Michela Ferron, Fabio Pianesi, Alex, Pentland category:cs.CY cs.LG stat.AP stat.ML published:2014-10-21 summary:Research has proven that stress reduces quality of life and causes manydiseases. For this reason, several researchers devised stress detection systemsbased on physiological parameters. However, these systems require thatobtrusive sensors are continuously carried by the user. In our paper, wepropose an alternative approach providing evidence that daily stress can bereliably recognized based on behavioral metrics, derived from the user's mobilephone activity and from additional indicators, such as the weather conditions(data pertaining to transitory properties of the environment) and thepersonality traits (data concerning permanent dispositions of individuals). Ourmultifactorial statistical model, which is person-independent, obtains theaccuracy score of 72.28% for a 2-class daily stress recognition problem. Themodel is efficient to implement for most of multimedia applications due tohighly reduced low-dimensional feature space (32d). Moreover, we identify anddiscuss the indicators which have strong predictive power.
arxiv-1410-5518 | On Symmetric and Asymmetric LSHs for Inner Product Search |  http://arxiv.org/abs/1410.5518  | author:Behnam Neyshabur, Nathan Srebro category:stat.ML cs.DS cs.IR cs.LG published:2014-10-21 summary:We consider the problem of designing locality sensitive hashes (LSH) forinner product similarity, and of the power of asymmetric hashes in thiscontext. Shrivastava and Li argue that there is no symmetric LSH for theproblem and propose an asymmetric LSH based on different mappings for query anddatabase points. However, we show there does exist a simple symmetric LSH thatenjoys stronger guarantees and better empirical performance than the asymmetricLSH they suggest. We also show a variant of the settings where asymmetry isin-fact needed, but there a different asymmetric LSH is required.
arxiv-1410-5792 | Generalized Compression Dictionary Distance as Universal Similarity Measure |  http://arxiv.org/abs/1410.5792  | author:Andrey Bogomolov, Bruno Lepri, Fabio Pianesi category:stat.ML cs.AI cs.CC cs.IT math.IT published:2014-10-21 summary:We present a new similarity measure based on information theoretic measureswhich is superior than Normalized Compression Distance for clustering problemsand inherits the useful properties of conditional Kolmogorov complexity. Weshow that Normalized Compression Dictionary Size and Normalized CompressionDictionary Entropy are computationally more efficient, as the need to performthe compression itself is eliminated. Also they scale linearly with exponentialvector size growth and are content independent. We show that normalizedcompression dictionary distance is compressor independent, if limited tolossless compressors, which gives space for optimizations and implementationspeed improvement for real-time and big data applications. The introducedmeasure is applicable for machine learning tasks of parameter-free unsupervisedclustering, supervised learning such as classification and regression, featureselection, and is applicable for big data problems with order of magnitudespeed increase.
arxiv-1410-5850 | A Fast Hybrid Primal Heuristic for Multiband Robust Capacitated Network Design with Multiple Time Periods |  http://arxiv.org/abs/1410.5850  | author:Fabio D'Andreagiovanni, Jonatan Krolikowski, Jonad Pulaj category:math.OC cs.DS cs.NE published:2014-10-21 summary:We investigate the Robust Multiperiod Network Design Problem, ageneralization of the Capacitated Network Design Problem (CNDP) that, besidesestablishing flow routing and network capacity installation as in a canonicalCNDP, also considers a planning horizon made up of multiple time periods andprotection against fluctuations in traffic volumes. As a remedy against trafficvolume uncertainty, we propose a Robust Optimization model based on MultibandRobustness (B\"using and D'Andreagiovanni, 2012), a refinement of classicalGamma-Robustness by Bertsimas and Sim that uses a system of multiple deviationbands. Since the resulting optimization problem may prove very challenging evenfor instances of moderate size solved by a state-of-the-art optimizationsolver, we propose a hybrid primal heuristic that combines a randomized fixingstrategy inspired by ant colony optimization, which exploits information comingfrom linear relaxations of the problem, and an exact large neighbourhoodsearch. Computational experiments on a set of realistic instances from theSNDlib show that our original heuristic can run fast and produce solutions ofextremely high quality associated with low optimality gaps.
arxiv-1410-5861 | Compositional Structure Learning for Action Understanding |  http://arxiv.org/abs/1410.5861  | author:Ran Xu, Gang Chen, Caiming Xiong, Wei Chen, Jason J. Corso category:cs.CV published:2014-10-21 summary:The focus of the action understanding literature has predominately beenclassification, how- ever, there are many applications demanding richer actionunderstanding such as mobile robotics and video search, with solutions toclassification, localization and detection. In this paper, we propose acompositional model that leverages a new mid-level representation calledcompositional trajectories and a locally articulated spatiotemporal deformableparts model (LALSDPM) for fully action understanding. Our methods isadvantageous in capturing the variable structure of dynamic human activity overa long range. First, the compositional trajectories capture long-ranging,frequently co-occurring groups of trajectories in space time and represent themin discriminative hierarchies, where human motion is largely separated fromcamera motion; second, LASTDPM learns a structured model with multi-layerdeformable parts to capture multiple levels of articulated motion. We implementour methods and demonstrate state of the art performance on all three problems:action detection, localization, and recognition.
arxiv-1410-5877 | Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation |  http://arxiv.org/abs/1410.5877  | author:Michael Bloodgood, Chris Callison-Burch category:cs.CL cs.LG stat.ML published:2014-10-21 summary:We explore how to improve machine translation systems by adding moretranslation data in situations where we already have substantial resources. Themain challenge is how to buck the trend of diminishing returns that is commonlyencountered. We present an active learning-style data solicitation algorithm tomeet this challenge. We test it, gathering annotations via Amazon MechanicalTurk, and find that we get an order of magnitude increase in performance ratesof improvement.
arxiv-1410-5884 | Mean-Field Networks |  http://arxiv.org/abs/1410.5884  | author:Yujia Li, Richard Zemel category:cs.LG stat.ML published:2014-10-21 summary:The mean field algorithm is a widely used approximate inference algorithm forgraphical models whose exact inference is intractable. In each iteration ofmean field, the approximate marginals for each variable are updated by gettinginformation from the neighbors. This process can be equivalently converted intoa feedforward network, with each layer representing one iteration of mean fieldand with tied weights on all layers. This conversion enables a few naturalextensions, e.g. untying the weights in the network. In this paper, we studythese mean field networks (MFNs), and use them as inference tools as well asdiscriminative models. Preliminary experiment results show that MFNs can learnto do inference very efficiently and perform significantly better than meanfield as discriminative models.
arxiv-1410-5605 | Attentive monitoring of multiple video streams driven by a Bayesian foraging strategy |  http://arxiv.org/abs/1410.5605  | author:Paolo Napoletano, Giuseppe Boccignone, Francesco Tisato category:cs.CV published:2014-10-21 summary:In this paper we shall consider the problem of deploying attention to subsetsof the video streams for collating the most relevant data and information ofinterest related to a given task. We formalize this monitoring problem as aforaging problem. We propose a probabilistic framework to model observer'sattentive behavior as the behavior of a forager. The forager, moment to moment,focuses its attention on the most informative stream/camera, detectsinteresting objects or activities, or switches to a more profitable stream. Theapproach proposed here is suitable to be exploited for multi-stream videosummarization. Meanwhile, it can serve as a preliminary step for moresophisticated video surveillance, e.g. activity and behavior analysis.Experimental results achieved on the UCR Videoweb Activities Dataset, apublicly available dataset, are presented to illustrate the utility of theproposed technique.
arxiv-1410-5684 | Regularizing Recurrent Networks - On Injected Noise and Norm-based Methods |  http://arxiv.org/abs/1410.5684  | author:Saahil Ognawala, Justin Bayer category:stat.ML cs.LG published:2014-10-21 summary:Advancements in parallel processing have lead to a surge in multilayerperceptrons' (MLP) applications and deep learning in the past decades.Recurrent Neural Networks (RNNs) give additional representational power tofeedforward MLPs by providing a way to treat sequential data. However, RNNs arehard to train using conventional error backpropagation methods because of thedifficulty in relating inputs over many time-steps. Regularization approachesfrom MLP sphere, like dropout and noisy weight training, have beeninsufficiently applied and tested on simple RNNs. Moreover, solutions have beenproposed to improve convergence in RNNs but not enough to improve the long termdependency remembering capabilities thereof. In this study, we aim to empirically evaluate the remembering andgeneralization ability of RNNs on polyphonic musical datasets. The models aretrained with injected noise, random dropout, norm-based regularizers and theirrespective performances compared to well-initialized plain RNNs and advancedregularization methods like fast-dropout. We conclude with evidence thattraining with noise does not improve performance as conjectured by a few worksin RNN optimization before ours.
arxiv-1410-5522 | Variational Reformulation of Bayesian Inverse Problems |  http://arxiv.org/abs/1410.5522  | author:Panagiotis Tsilifis, Ilias Bilionis, Ioannis Katsounaros, Nicholas Zabaras category:stat.ML published:2014-10-21 summary:The classical approach to inverse problems is based on the optimization of amisfit function. Despite its computational appeal, such an approach suffersfrom many shortcomings, e.g., non-uniqueness of solutions, modeling priorknowledge, etc. The Bayesian formalism to inverse problems avoids most of thedifficulties encountered by the optimization approach, albeit at an increasedcomputational cost. In this work, we use information theoretic arguments tocast the Bayesian inference problem in terms of an optimization problem. Theresulting scheme combines the theoretical soundness of fully Bayesian inferencewith the computational efficiency of a simple optimization.
arxiv-1410-5652 | Improvement of PSO algorithm by memory based gradient search - application in inventory management |  http://arxiv.org/abs/1410.5652  | author:Tamás Varga, András Király, János Abonyi category:cs.NE published:2014-10-21 summary:Advanced inventory management in complex supply chains requires effective androbust nonlinear optimization due to the stochastic nature of supply and demandvariations. Application of estimated gradients can boost up the convergence ofParticle Swarm Optimization (PSO) algorithm but classical gradient calculationcannot be applied to stochastic and uncertain systems. In these situationsMonte-Carlo (MC) simulation can be applied to determine the gradient. Wedeveloped a memory based algorithm where instead of generating and evaluatingnew simulated samples the stored and shared former function evaluations of theparticles are sampled to estimate the gradients by local weighted least squaresregression. The performance of the resulted regional gradient-based PSO isverified by several benchmark problems and in a complex application examplewhere optimal reorder points of a supply chain are determined.
arxiv-1410-5485 | A stronger null hypothesis for crossing dependencies |  http://arxiv.org/abs/1410.5485  | author:Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph published:2014-10-20 summary:The syntactic structure of a sentence can be modeled as a tree where verticesare words and edges indicate syntactic dependencies between words. It iswell-known that those edges normally do not cross when drawn over the sentence.Here a new null hypothesis for the number of edge crossings of a sentence ispresented. That null hypothesis takes into account the length of the pair ofedges that may cross and predicts the relative number of crossings in randomtrees with a small error, suggesting that a ban of crossings or a principle ofminimization of crossings are not needed in general to explain the origins ofnon-crossing dependencies. Our work paves the way for more powerful nullhypotheses to investigate the origins of non-crossing dependencies in nature.
arxiv-1410-5801 | Artifact reduction in multichannel pervasive EEG using hybrid WPT-ICA and WPT-EMD signal decomposition techniques |  http://arxiv.org/abs/1410.5801  | author:Valentina Bono, Wasifa Jamal, Saptarshi Das, Koushik Maharatna category:physics.med-ph cs.LG stat.AP stat.ME published:2014-10-20 summary:In order to reduce the muscle artifacts in multi-channel pervasiveElectroencephalogram (EEG) signals, we here propose and compare two hybridalgorithms by combining the concept of wavelet packet transform (WPT),empirical mode decomposition (EMD) and Independent Component Analysis (ICA).The signal cleaning performances of WPT-EMD and WPT-ICA algorithms have beencompared using a signal-to-noise ratio (SNR)-like criterion for artifacts. Thealgorithms have been tested on multiple trials of four different artifact casesviz. eye-blinking and muscle artifacts including left and right hand movementand head-shaking.
arxiv-1410-5410 | Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner Product Search (MIPS) |  http://arxiv.org/abs/1410.5410  | author:Anshumali Shrivastava, Ping Li category:stat.ML cs.DS cs.IR cs.LG published:2014-10-20 summary:Recently it was shown that the problem of Maximum Inner Product Search (MIPS)is efficient and it admits provably sub-linear hashing algorithms. Asymmetrictransformations before hashing were the key in solving MIPS which was otherwisehard. In the prior work, the authors use asymmetric transformations whichconvert the problem of approximate MIPS into the problem of approximate nearneighbor search which can be efficiently solved using hashing. In this work, weprovide a different transformation which converts the problem of approximateMIPS into the problem of approximate cosine similarity search which can beefficiently solved using signed random projections. Theoretical analysis showthat the new scheme is significantly better than the original scheme for MIPS.Experimental evaluations strongly support the theoretical findings.
arxiv-1410-5401 | Neural Turing Machines |  http://arxiv.org/abs/1410.5401  | author:Alex Graves, Greg Wayne, Ivo Danihelka category:cs.NE published:2014-10-20 summary:We extend the capabilities of neural networks by coupling them to externalmemory resources, which they can interact with by attentional processes. Thecombined system is analogous to a Turing Machine or Von Neumann architecturebut is differentiable end-to-end, allowing it to be efficiently trained withgradient descent. Preliminary results demonstrate that Neural Turing Machinescan infer simple algorithms such as copying, sorting, and associative recallfrom input and output examples.
arxiv-1410-5263 | Building pattern recognition applications with the SPARE library |  http://arxiv.org/abs/1410.5263  | author:Lorenzo Livi, Guido Del Vescovo, Antonello Rizzi, Fabio Massimo Frattale Mascioli category:cs.CV cs.MS D.2.2 published:2014-10-20 summary:This paper presents the SPARE C++ library, an open source software toolconceived to build pattern recognition and soft computing systems. The libraryfollows the requirement of the generality: most of the implemented algorithmsare able to process user-defined input data types transparently, such aslabeled graphs and sequences of objects, as well as standard numeric vectors.Here we present a high-level picture of the SPARE library characteristics,focusing instead on the specific practical possibility of constructing patternrecognition systems for different input data types. In particular, as a proofof concept, we discuss two application instances involving clustering ofreal-valued multidimensional sequences and classification of labeled graphs.
arxiv-1410-5358 | Remote sensing image classification exploiting multiple kernel learning |  http://arxiv.org/abs/1410.5358  | author:Claudio Cusano, Paolo Napoletano, Raimondo Schettini category:cs.CV published:2014-10-20 summary:We propose a strategy for land use classification which exploits MultipleKernel Learning (MKL) to automatically determine a suitable combination of aset of features without requiring any heuristic knowledge about theclassification task. We present a novel procedure that allows MKL to achievegood performance in the case of small training sets. Experimental results onpublicly available datasets demonstrate the feasibility of the proposedapproach.
arxiv-1410-5467 | Machine Learning of Coq Proof Guidance: First Experiments |  http://arxiv.org/abs/1410.5467  | author:Cezary Kaliszyk, Lionel Mamane, Josef Urban category:cs.LO cs.LG published:2014-10-20 summary:We report the results of the first experiments with learning proofdependencies from the formalizations done with the Coq system. We explain theprocess of obtaining the dependencies from the Coq proofs, the characterizationof formulas that is used for the learning, and the evaluation method. Variousmachine learning methods are compared on a dataset of 5021 toplevel Coq proofscoming from the CoRN repository. The best resulting method covers on average75% of the needed proof dependencies among the first 100 predictions, which isa comparable performance of such initial experiments on other large-theorycorpora.
arxiv-1410-7795 | Classification of Autism Spectrum Disorder Using Supervised Learning of Brain Connectivity Measures Extracted from Synchrostates |  http://arxiv.org/abs/1410.7795  | author:Wasifa Jamal, Saptarshi Das, Ioana-Anastasia Oprescu, Koushik Maharatna, Fabio Apicella, Federico Sicca category:physics.med-ph cs.CV stat.AP stat.ML published:2014-10-20 summary:Objective. The paper investigates the presence of autism using the functionalbrain connectivity measures derived from electro-encephalogram (EEG) ofchildren during face perception tasks. Approach. Phase synchronized patternsfrom 128-channel EEG signals are obtained for typical children and childrenwith autism spectrum disorder (ASD). The phase synchronized states orsynchrostates temporally switch amongst themselves as an underlying process forthe completion of a particular cognitive task. We used 12 subjects in eachgroup (ASD and typical) for analyzing their EEG while processing fearful, happyand neutral faces. The minimal and maximally occurring synchrostates for eachsubject are chosen for extraction of brain connectivity features, which areused for classification between these two groups of subjects. Among differentsupervised learning techniques, we here explored the discriminant analysis andsupport vector machine both with polynomial kernels for the classificationtask. Main results. The leave one out cross-validation of the classificationalgorithm gives 94.7% accuracy as the best performance with correspondingsensitivity and specificity values as 85.7% and 100% respectively.Significance. The proposed method gives high classification accuracies andoutperforms other contemporary research results. The effectiveness of theproposed method for classification of autistic and typical children suggeststhe possibility of using it on a larger population to validate it for clinicalpractice.
arxiv-1410-5362 | Prediction of Synchrostate Transitions in EEG Signals Using Markov Chain Models |  http://arxiv.org/abs/1410.5362  | author:Wasifa Jamal, Saptarshi Das, Ioana-Anastasia Oprescu, Koushik Maharatna category:q-bio.NC physics.med-ph stat.AP stat.ML published:2014-10-20 summary:This paper proposes a stochastic model using the concept of Markov chains forthe inter-state transitions of the millisecond order quasi-stable phasesynchronized patterns or synchrostates, found in multi-channelElectroencephalogram (EEG) signals. First and second order transitionprobability matrices are estimated for Markov chain modelling from 100 trialsof 128-channel EEG signals during two different face perception tasks.Prediction accuracies with such finite Markov chain models for synchrostatetransition are also compared, under a data-partitioning based cross-validationscheme.
arxiv-1410-5491 | Using Mechanical Turk to Build Machine Translation Evaluation Sets |  http://arxiv.org/abs/1410.5491  | author:Michael Bloodgood, Chris Callison-Burch category:cs.CL cs.LG stat.ML published:2014-10-20 summary:Building machine translation (MT) test sets is a relatively expensive task.As MT becomes increasingly desired for more and more language pairs and moreand more domains, it becomes necessary to build test sets for each case. Inthis paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MTtest sets cheaply. We find that MTurk can be used to make test sets muchcheaper than professionally-produced test sets. More importantly, inexperiments with multiple MT systems, we find that the MTurk-produced test setsyield essentially the same conclusions regarding system performance as theprofessionally-produced test sets yield.
arxiv-1410-5392 | Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling for Gaussian Graphical Models |  http://arxiv.org/abs/1410.5392  | author:Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, Shang-Hua Teng category:cs.DS cs.LG cs.NA math.NA stat.CO stat.ML published:2014-10-20 summary:Motivated by a sampling problem basic to computational statistical inference,we develop a nearly optimal algorithm for a fundamental problem in spectralgraph theory and numerical analysis. Given an $n\times n$ SDDM matrix ${\bf\mathbf{M}}$, and a constant $-1 \leq p \leq 1$, our algorithm gives efficientaccess to a sparse $n\times n$ linear operator $\tilde{\mathbf{C}}$ such that$${\mathbf{M}}^{p} \approx \tilde{\mathbf{C}} \tilde{\mathbf{C}}^\top.$$ Thesolution is based on factoring ${\bf \mathbf{M}}$ into a product of simple andsparse matrices using squaring and spectral sparsification. For ${\mathbf{M}}$with $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, andpolylogarithmic depth on a parallel machine with $m$ processors. This gives thefirst sampling algorithm that only requires nearly linear work and $n$ i.i.d.random univariate Gaussian samples to generate i.i.d. random samples for$n$-dimensional Gaussian random fields with SDDM precision matrices. Forsampling this natural subclass of Gaussian random fields, it is optimal in therandomness and nearly optimal in the work and parallel complexity. In addition,our sampling algorithm can be directly extended to Gaussian random fields withSDD precision matrices.
arxiv-1410-5137 | On Iterative Hard Thresholding Methods for High-dimensional M-Estimation |  http://arxiv.org/abs/1410.5137  | author:Prateek Jain, Ambuj Tewari, Purushottam Kar category:cs.LG stat.ML published:2014-10-20 summary:The use of M-estimators in generalized linear regression models in highdimensional settings requires risk minimization with hard $L_0$ constraints. Ofthe known methods, the class of projected gradient descent (also known asiterative hard thresholding (IHT)) methods is known to offer the fastest andmost scalable solutions. However, the current state-of-the-art is only able toanalyze these methods in extremely restrictive settings which do not hold inhigh dimensional statistical models. In this work we bridge this gap byproviding the first analysis for IHT-style methods in the high dimensionalstatistical setting. Our bounds are tight and match known minimax lower bounds.Our results rely on a general analysis framework that enables us to analyzeseveral popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) inthe high dimensional regression setting. We also extend our analysis to a largefamily of "fully corrective methods" that includes two-stage and partialhard-thresholding algorithms. We show that our results hold for the problem ofsparse regression, as well as low-rank matrix recovery.
arxiv-1410-5473 | Feature Selection Based on Confidence Machine |  http://arxiv.org/abs/1410.5473  | author:Chang Liu, Yi Xu category:cs.LG published:2014-10-20 summary:In machine learning and pattern recognition, feature selection has been a hottopic in the literature. Unsupervised feature selection is challenging due tothe loss of labels which would supply the related information.How to define anappropriate metric is the key for feature selection. We propose a filter methodfor unsupervised feature selection which is based on the Confidence Machine.Confidence Machine offers an estimation of confidence on a feature'reliability.In this paper, we provide the math model of Confidence Machine in the contextof feature selection, which maximizes the relevance and minimizes theredundancy of the selected feature. We compare our method against classicfeature selection methods Laplacian Score, Pearson Correlation and PrincipalComponent Analysis on benchmark data sets. The experimental results demonstratethe efficiency and effectiveness of our method.
arxiv-1410-5224 | Supervised mid-level features for word image representation |  http://arxiv.org/abs/1410.5224  | author:Albert Gordo category:cs.CV published:2014-10-20 summary:This paper addresses the problem of learning word image representations:given the cropped image of a word, we are interested in finding a descriptive,robust, and compact fixed-length representation. Machine learning techniquescan then be supplied with these representations to produce models useful forword retrieval or recognition tasks. Although many works have focused on themachine learning aspect once a global representation has been produced, littlework has been devoted to the construction of those base image representations:most works use standard coding and aggregation techniques directly on top ofstandard computer vision features such as SIFT or HOG. We propose to learn local mid-level features suitable for building word imagerepresentations. These features are learnt by leveraging character bounding boxannotations on a small set of training images. However, contrary to otherapproaches that use character bounding box information, our approach does notrely on detecting the individual characters explicitly at testing time. Ourlocal mid-level features can then be aggregated to produce a global word imagesignature. When pairing these features with the recent word attributesframework of Almaz\'an et al., we obtain results comparable with or better thanthe state-of-the-art on matching and recognition tasks using global descriptorsof only 96 dimensions.
