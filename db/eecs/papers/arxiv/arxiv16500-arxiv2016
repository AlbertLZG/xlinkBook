arxiv-16500-1 | Classical Statistics and Statistical Learning in Imaging Neuroscience | http://arxiv.org/abs/1603.01857 | author:Danilo Bzdok category:stat.ML q-bio.NC published:2016-03-06 summary:Neuroimaging research has predominantly drawn conclusions based on classicalstatistics, including null-hypothesis testing, t-tests, and ANOVA. Throughoutrecent years, statistical learning methods enjoy increasing popularity,including cross-validation, pattern classification, and sparsity-inducingregression. These two methodological families used for neuroimaging dataanalysis can be viewed as two extremes of a continuum. Yet, they originatedfrom different historical contexts, build on different theories, rest ondifferent assumptions, evaluate different outcome metrics, and permit differentconclusions. This paper portrays commonalities and differences betweenclassical statistics and statistical learning with their relation toneuroimaging research. The conceptual implications are illustrated in threecommon analysis scenarios. It is thus tried to resolve possible confusionbetween classical hypothesis testing and data-guided model estimation bydiscussing their ramifications for the neuroimaging access to neurobiology.
arxiv-16500-2 | Online Learning to Rank with Feedback at the Top | http://arxiv.org/abs/1603.01855 | author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG published:2016-03-06 summary:We consider an online learning to rank setting in which, at each round, anoblivious adversary generates a list of $m$ documents, pertaining to a query,and the learner produces scores to rank the documents. The adversary thengenerates a relevance vector and the learner updates its ranker according tothe feedback received. We consider the setting where the feedback is restrictedto be the relevance levels of only the top $k$ documents in the ranked list for$k \ll m$. However, the performance of learner is judged based on theunrevealed full relevance vectors, using an appropriate learning to rank lossfunction. We develop efficient algorithms for well known losses in thepointwise, pairwise and listwise families. We also prove that no onlinealgorithm can have sublinear regret, with top-1 feedback, for any loss that iscalibrated with respect to NDCG. We apply our algorithms on benchmark datasetsdemonstrating efficient online learning of a ranking function from highlyrestricted feedback.
arxiv-16500-3 | Proximal groupoid patterns In digital images | http://arxiv.org/abs/1603.01842 | author:Enoch A-iyeh, James F. Peters category:cs.CV 54E40 published:2016-03-06 summary:The focus of this article is on the detection and classification of patternsbased on groupoids. The approach hinges on descriptive proximity of points in aset based on the neighborliness property. This approach lends support to imageanalysis and understanding and in studying nearness of image segments. Apractical application of the approach is in terms of the analysis of naturalimages for pattern identification and classification.
arxiv-16500-4 | Personalized Advertisement Recommendation: A Ranking Approach to Address the Ubiquitous Click Sparsity Problem | http://arxiv.org/abs/1603.01870 | author:Sougata Chaudhuri, Georgios Theocharous, Mohammad Ghavamzadeh category:cs.LG cs.IR published:2016-03-06 summary:We study the problem of personalized advertisement recommendation (PAR),which consist of a user visiting a system (website) and the system displayingone of $K$ ads to the user. The system uses an internal ad recommendationpolicy to map the user's profile (context) to one of the ads. The user eitherclicks or ignores the ad and correspondingly, the system updates itsrecommendation policy. PAR problem is usually tackled by scalable\emph{contextual bandit} algorithms, where the policies are generally based onclassifiers. A practical problem in PAR is extreme click sparsity, due to veryfew users actually clicking on ads. We systematically study the drawback ofusing contextual bandit algorithms based on classifier-based policies, in faceof extreme click sparsity. We then suggest an alternate policy, based onrankers, learnt by optimizing the Area Under the Curve (AUC) ranking loss,which can significantly alleviate the problem of click sparsity. We conductextensive experiments on public datasets, as well as three industry proprietarydatasets, to illustrate the improvement in click-through-rate (CTR) obtained byusing the ranker-based policy over classifier-based policies.
arxiv-16500-5 | General Participative Media Single Image Restoration | http://arxiv.org/abs/1603.01864 | author:Felipe Codevilla, Joel D. O. Gaya, Amanda C. Duarte, Silvia Botelho category:cs.CV published:2016-03-06 summary:This paper describes a method to restore degraded images captured in generalparticipative media --- fog, turbid water, sand storm, etc. To obtaingenerality, we, first, propose a novel interpretation of the participativemedia image formation by considering the color variation of the media. Second,we introduce that joining different image priors is an effective alternativefor image restoration. The proposed method contains a Composite Prior supportedby statistics collected on both haze-free and degraded participativeenvironment images. The key of the method is joining two complementary measures--- local contrast and color. The results presented for a variety of underwaterand haze images demonstrate the power of the method. Moreover, we showed thepotential of our method using a special dataset for which a reference haze-freeimage is available for comparison.
arxiv-16500-6 | Variational methods for Conditional Multimodal Learning: Generating Human Faces from Attributes | http://arxiv.org/abs/1603.01801 | author:Gaurav Pandey, Ambedkar Dukkipati category:cs.CV cs.LG stat.ML published:2016-03-06 summary:Prior to this decade, the field of computer vision was primarily focusedaround hand-crafted feature extraction methods used in conjunction withdiscriminative models for specific tasks such as object recognition,detection/localization, tracking etc. A generative image understanding wasneither within reach nor the prime concern of the period. In this paper, weaddress the following problem: Given a description of a human face, can wegenerate the image corresponding to it? We frame this problem as a conditionalmodality learning problem and use variational methods for maximizing thecorresponding conditional log-likelihood. The resultant deep model, which werefer to as conditional multimodal autoencoder (CMMA), forces the latentrepresentation obtained from the attributes alone to be 'close' to the jointrepresentation obtained from both face and attributes. We show that the facesgenerated from attributes using the proposed model, are qualitatively andquantitatively more representative of the attributes from which they weregenerated, than those obtained by other deep generative models. We also proposea secondary task, whereby the existing faces are modified by modifying thecorresponding attributes. We observe that the modifications in face introducedby the proposed model are representative of the corresponding modifications inattributes. Hence, our proposed method solves the above mentioned problem.
arxiv-16500-7 | Hierarchical Decision Making In Electricity Grid Management | http://arxiv.org/abs/1603.01840 | author:Gal Dalal, Elad Gilboa, Shie Mannor category:cs.AI cs.LG stat.AP published:2016-03-06 summary:The power grid is a complex and vital system that necessitates carefulreliability management. Managing the grid is a difficult problem with multipletime scales of decision making and stochastic behavior due to renewable energygenerations, variable demand and unplanned outages. Solving this problem in theface of uncertainty requires a new methodology with tractable algorithms. Inthis work, we introduce a new model for hierarchical decision making in complexsystems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., alevel of abstraction, for real-time power grid reliability. We devise analgorithm that alternates between slow time-scale policy improvement, and fasttime-scale value function approximation. We compare our results to prevailingheuristics, and show the strength of our method.
arxiv-16500-8 | Generalization error bounds for learning to rank: Does the length of document lists matter? | http://arxiv.org/abs/1603.01860 | author:Ambuj Tewari, Sougata Chaudhuri category:cs.LG published:2016-03-06 summary:We consider the generalization ability of algorithms for learning to rank ata query level, a problem also called subset ranking. Existing generalizationerror bounds necessarily degrade as the size of the document list associatedwith a query increases. We show that such a degradation is not intrinsic to theproblem. For several loss functions, including the cross-entropy loss used inthe well known ListNet method, there is \emph{no} degradation in generalizationability as document lists become longer. We also provide novel generalizationerror bounds under $\ell_1$ regularization and faster convergence rates if theloss function is smooth.
arxiv-16500-9 | Underwater Fish Tracking for Moving Cameras based on Deformable Multiple Kernels | http://arxiv.org/abs/1603.01695 | author:Meng-Che Chuang, Jenq-Neng Hwang, Jian-Hui Ye, Shih-Chia Huang, Kresimir Williams category:cs.CV published:2016-03-05 summary:Fishery surveys that call for the use of single or multiple underwatercameras have been an emerging technology as a non-extractive mean to estimatethe abundance of fish stocks. Tracking live fish in an open aquatic environmentposts challenges that are different from general pedestrian or vehicle trackingin surveillance applications. In many rough habitats fish are monitored bycameras installed on moving platforms, where tracking is even more challengingdue to inapplicability of background models. In this paper, a novel trackingalgorithm based on the deformable multiple kernels (DMK) is proposed to addressthese challenges. Inspired by the deformable part model (DPM) technique, a setof kernels is defined to represent the holistic object and several parts thatare arranged in a deformable configuration. Color histogram, texture histogramand the histogram of oriented gradients (HOG) are extracted and serve as objectfeatures. Kernel motion is efficiently estimated by the mean-shift algorithm oncolor and texture features to realize tracking. Furthermore, the HOG-featuredeformation costs are adopted as soft constraints on kernel positions tomaintain the part configuration. Experimental results on practical video setfrom underwater moving cameras show the reliable performance of the proposedmethod with much less computational cost comparing with state-of-the-arttechniques.
arxiv-16500-10 | Network Morphism | http://arxiv.org/abs/1603.01670 | author:Tao Wei, Changhu Wang, Yong Rui, Chang Wen Chen category:cs.LG cs.CV cs.NE published:2016-03-05 summary:We present in this paper a systematic study on how to morph a well-trainedneural network to a new one so that its network function can be completelypreserved. We define this as \emph{network morphism} in this research. Aftermorphing a parent network, the child network is expected to inherit theknowledge from its parent network and also has the potential to continuegrowing into a more powerful one with much shortened training time. The firstrequirement for this network morphism is its ability to handle diverse morphingtypes of networks, including changes of depth, width, kernel size, and evensubnet. To meet this requirement, we first introduce the network morphismequations, and then develop novel morphing algorithms for all these morphingtypes for both classic and convolutional neural networks. The secondrequirement for this network morphism is its ability to deal with non-linearityin a network. We propose a family of parametric-activation functions tofacilitate the morphing of any continuous non-linear activation neurons.Experimental results on benchmark datasets and typical neural networksdemonstrate the effectiveness of the proposed network morphism scheme.
arxiv-16500-11 | UTA-poly and UTA-splines: additive value functions with polynomial marginals | http://arxiv.org/abs/1603.02626 | author:Olivier Sobrie, Nicolas Gillis, Vincent Mousseau, Marc Pirlot category:math.OC cs.AI cs.LG published:2016-03-05 summary:Additive utility function models are widely used in multiple criteriadecision analysis. In such models, a numerical value is associated to eachalternative involved in the decision problem. It is computed by aggregating thescores of the alternative on the different criteria of the decision problem.The score of an alternative is determined by a marginal value function thatevolves monotonically as a function of the performance of the alternative onthis criterion. Determining the shape of the marginals is not easy for adecision maker. It is easier for him/her to make statements such as"alternative $a$ is preferred to $b$". In order to help the decision maker, UTAdisaggregation procedures use linear programming to approximate the marginalsby piecewise linear functions based only on such statements. In this paper, wepropose to infer polynomials and splines instead of piecewise linear functionsfor the marginals. In this aim, we use semidefinite programming instead oflinear programming. We illustrate this new elicitation method and present someexperimental results.
arxiv-16500-12 | A Feature Learning and Object Recognition Framework for Underwater Fish Images | http://arxiv.org/abs/1603.01696 | author:Meng-Che Chuang, Jenq-Neng Hwang, Kresimir Williams category:cs.CV published:2016-03-05 summary:Live fish recognition is one of the most crucial elements of fisheries surveyapplications where vast amount of data are rapidly acquired. Different fromgeneral scenarios, challenges to underwater image recognition are posted bypoor image quality, uncontrolled objects and environment, as well as difficultyin acquiring representative samples. Also, most existing feature extractiontechniques are hindered from automation due to involving human supervision.Toward this end, we propose an underwater fish recognition framework thatconsists of a fully unsupervised feature learning technique and anerror-resilient classifier. Object parts are initialized based on saliency andrelaxation labeling to match object parts correctly. A non-rigid part model isthen learned based on fitness, separation and discrimination criteria. For theclassifier, an unsupervised clustering approach generates a binary classhierarchy, where each node is a classifier. To exploit information fromambiguous images, the notion of partial classification is introduced to assigncoarse labels by optimizing the "benefit" of indecision made by the classifier.Experiments show that the proposed framework achieves high accuracy on bothpublic and self-collected underwater fish images with high uncertainty andclass imbalance.
arxiv-16500-13 | Grading of Mammalian Cumulus Oocyte Complexes using Machine Learning for in Vitro Embryo Culture | http://arxiv.org/abs/1603.01739 | author:Viswanath P Sudarshan, Tobias Weiser, Phalgun Chintala, Subhamoy Mandal, Rahul Dutta category:cs.CV physics.med-ph published:2016-03-05 summary:Visual observation of Cumulus Oocyte Complexes provides only limitedinformation about its functional competence, whereas the molecular evaluationsmethods are cumbersome or costly. Image analysis of mammalian oocytes canprovide attractive alternative to address this challenge. However, it iscomplex, given the huge number of oocytes under inspection and the subjectivenature of the features inspected for identification. Supervised machinelearning methods like random forest with annotations from expert biologists canmake the analysis task standardized and reduces inter-subject variability. Wepresent a semi-automatic framework for predicting the class an oocyte belongsto, based on multi-object parametric segmentation on the acquired microscopicimage followed by a feature based classification using random forests.
arxiv-16500-14 | Classifier ensemble creation via false labelling | http://arxiv.org/abs/1603.01716 | author:Bálint Antal category:cs.LG I.2, I.5.2 published:2016-03-05 summary:In this paper, a novel approach to classifier ensemble creation is presented.While other ensemble creation techniques are based on careful selection ofexisting classifiers or preprocessing of the data, the presented approachautomatically creates an optimal labelling for a number of classifiers, whichare then assigned to the original data instances and fed to classifiers. Theapproach has been evaluated on high-dimensional biomedical datasets. Theresults show that the approach outperformed individual approaches in all cases.
arxiv-16500-15 | Saliency Detection combining Multi-layer Integration algorithm with background prior and energy function | http://arxiv.org/abs/1603.01684 | author:Hanling Zhang, Chenxing Xia category:cs.CV published:2016-03-05 summary:In this paper, we propose an improved mechanism for saliency detection.Firstly,based on a neoteric background prior selecting four corners of an imageas background,we use color and spatial contrast with each superpixel to obtaina salinecy map(CBP). Inspired by reverse-measurement methods to improve theaccuracy of measurement in Engineering,we employ the Objectness labels asforeground prior based on part of information of CBP to construct amap(OFP).Further,an original energy function is applied to optimize both ofthem respectively and a single-layer saliency map(SLP)is formed by merging theabove twos.Finally,to deal with the scale problem,we obtain our multi-layermap(MLP) by presenting an integration algorithm to take advantage of multiplesaliency maps. Quantitative and qualitative experiments on three datasetsdemonstrate that our method performs favorably against the state-of-the-artalgorithm.
arxiv-16500-16 | Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks | http://arxiv.org/abs/1603.01768 | author:Alex J. Champandard category:cs.CV published:2016-03-05 summary:Convolutional neural networks (CNNs) have proven highly effective at imagesynthesis and style transfer. For most users, however, using them as tools canbe a challenging task due to their unpredictable behavior that goes againstcommon intuitions. This paper introduces a novel concept to augment suchgenerative architectures with semantic annotations, either by manuallyauthoring pixel labels or using existing solutions for semantic segmentation.The result is a content-aware generative algorithm that offers meaningfulcontrol over the outcome. Thus, we increase the quality of images generated byavoiding common glitches, make the results look significantly more plausible,and extend the functional range of these algorithms---whether for portraits orlandscapes, etc. Applications include semantic style transfer and turningdoodles with few colors into masterful paintings!
arxiv-16500-17 | High-Dimensional Metrics in R | http://arxiv.org/abs/1603.01700 | author:Victor Chernozhukov, Chris Hansen, Martin Spindler category:stat.ML stat.ME published:2016-03-05 summary:The package High-dimensional Metrics (\Rpackage{hdm}) is an evolvingcollection of statistical methods for estimation and quantification ofuncertainty in high-dimensional approximately sparse models. It focuses onproviding semi-parametrically efficient estimators, confidence intervals, andsignificance testing for low-dimensional subcomponents of the high-dimensionalparameter vector. This vignette offers a brief introduction and a tutorial tothe implemented methods. \R and the package \Rpackage{hdm} are open-sourcesoftware projects and can be freely downloaded from CRAN:\texttt{http://cran.r-project.org}.
arxiv-16500-18 | A single-phase, proximal path-following framework | http://arxiv.org/abs/1603.01681 | author:Quoc Tran-Dinh, Anastasios Kyrillidis, Volkan Cevher category:math.OC cs.IT math.IT stat.ML published:2016-03-05 summary:We propose a new proximal, path-following framework for a class of---possiblynon-smooth---constrained convex problems. We consider settings where thenon-smooth part is endowed with a proximity operator, and the constraint set isequipped with a self-concordant barrier. Our main contribution is a newre-parametrization of the optimality condition of the barrier problem, thatallows us to process the objective function with its proximal operator within anew path following scheme. In particular, our approach relies on the followingtwo main ideas. First, we re-parameterize the optimality condition as anauxiliary problem, such that a "good" initial point is available. Second, wecombine the proximal operator of the objective and path-following ideas todesign a single phase, proximal, path-following algorithm. Our method has several advantages. First, it allows handling non-smoothobjectives via proximal operators, this avoids lifting the problem dimensionvia slack variables and additional constraints. Second, it consists of only a\emph{single phase} as compared to a two-phase algorithm in [43] In this work,we show how to overcome this difficulty in the proximal setting and prove thatour scheme has the same $\mathcal{O}(\sqrt{\nu}\log(1/\varepsilon))$ worst-caseiteration-complexity with standard approaches [30, 33], but our method canhandle nonsmooth objectives, where $\nu$ is the barrier parameter and$\varepsilon$ is a desired accuracy. Finally, our framework allows errors inthe calculation of proximal-Newton search directions, without sacrificing theworst-case iteration complexity. We demonstrate the merits of our algorithm viathree numerical examples, where proximal operators play a key role to improvethe performance over off-the-shelf interior-point solvers.
arxiv-16500-19 | Parallel Texts in the Hebrew Bible, New Methods and Visualizations | http://arxiv.org/abs/1603.01541 | author:Martijn Naaijer, Dirk Roorda category:cs.CL published:2016-03-04 summary:In this article we develop an algorithm to detect parallel texts in theMasoretic Text of the Hebrew Bible. The results are presented online andchapters in the Hebrew Bible containing parallel passages can be inspectedsynoptically. Differences between parallel passages are highlighted. In asimilar way the MT of Isaiah is presented synoptically with 1QIsaa. We alsoinvestigate how one can investigate the degree of similarity between parallelpassages with the help of a case study of 2 Kings 19-25 and its parallels inIsaiah, Jeremiah and 2 Chronicles.
arxiv-16500-20 | Text Understanding with the Attention Sum Reader Network | http://arxiv.org/abs/1603.01547 | author:Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, Jan Kleindienst category:cs.CL published:2016-03-04 summary:Several large cloze-style context-question-answer datasets have beenintroduced recently: the CNN and Daily Mail news data and the Children's BookTest. Thanks to the size of these datasets, the associated text comprehensiontask is well suited for deep-learning techniques that currently seem tooutperform all alternative approaches. We present a new, simple model that usesattention to directly pick the answer from the context as opposed to computingthe answer using a blended representation of words in the document as is usualin similar models. This makes the model particularly suitable forquestion-answering problems where the answer is a single word from thedocument. Our model outperforms models previously proposed for these tasks by alarge margin.
arxiv-16500-21 | Getting More Out Of Syntax with PropS | http://arxiv.org/abs/1603.01648 | author:Gabriel Stanovsky, Jessica Ficler, Ido Dagan, Yoav Goldberg category:cs.CL published:2016-03-04 summary:Semantic NLP applications often rely on dependency trees to recognize majorelements of the proposition structure of sentences. Yet, while much semanticstructure is indeed expressed by syntax, many phenomena are not easily read outof dependency trees, often leading to further ad-hoc heuristic post-processingor to information loss. To directly address the needs of semantic applications,we present PropS -- an output representation designed to explicitly anduniformly express much of the proposition structure which is implied fromsyntax, and an associated tool for extracting it from dependency trees.
arxiv-16500-22 | A Bayesian Model of Multilingual Unsupervised Semantic Role Induction | http://arxiv.org/abs/1603.01514 | author:Nikhil Garg, James Henderson category:cs.CL published:2016-03-04 summary:We propose a Bayesian model of unsupervised semantic role induction inmultiple languages, and use it to explore the usefulness of parallel corporafor this task. Our joint Bayesian model consists of individual models for eachlanguage plus additional latent variables that capture alignments between rolesacross languages. Because it is a generative Bayesian model, we can doevaluations in a variety of scenarios just by varying the inference procedure,without changing the model, thereby comparing the scenarios directly. Wecompare using only monolingual data, using a parallel corpus, using a parallelcorpus with annotations in the other language, and using small amounts ofannotation in the target language. We find that the biggest impact of adding aparallel corpus to training is actually the increase in mono-lingual data, withthe alignments to another language resulting in small improvements, even withlabeled data for the other language.
arxiv-16500-23 | Joint Learning Templates and Slots for Event Schema Induction | http://arxiv.org/abs/1603.01333 | author:Lei Sha, Sujian Li, Baobao Chang, Zhifang Sui category:cs.CL published:2016-03-04 summary:Automatic event schema induction (AESI) means to extract meta-event from rawtext, in other words, to find out what types (templates) of event may exist inthe raw text and what roles (slots) may exist in each event type. In thispaper, we propose a joint entity-driven model to learn templates and slotssimultaneously based on the constraints of templates and slots in the samesentence. In addition, the entities' semantic information is also consideredfor the inner connectivity of the entities. We borrow the normalized cutcriteria in image segmentation to divide the entities into more accuratetemplate clusters and slot clusters. The experiment shows that our model gainsa relatively higher result than previous work.
arxiv-16500-24 | Optimized Polynomial Evaluation with Semantic Annotations | http://arxiv.org/abs/1603.01520 | author:Daniel Rubio Bonilla, Colin W. Glass, Jan Kuper category:cs.PL cs.CL B.1.4 published:2016-03-04 summary:In this paper we discuss how semantic annotations can be used to introducemathematical algorithmic information of the underlying imperative code toenable compilers to produce code transformations that will enable betterperformance. By using this approaches not only good performance is achieved,but also better programmability, maintainability and portability acrossdifferent hardware architectures. To exemplify this we will use polynomialequations of different degrees.
arxiv-16500-25 | Sequential ranking under random semi-bandit feedback | http://arxiv.org/abs/1603.01450 | author:Hossein Vahabi, Paul Lagrée, Claire Vernade, Olivier Cappé category:cs.DS cs.LG published:2016-03-04 summary:In many web applications, a recommendation is not a single item suggested toa user but a list of possibly interesting contents that may be ranked in somecontexts. The combinatorial bandit problem has been studied quite extensivelythese last two years and many theoretical results now exist : lower bounds onthe regret or asymptotically optimal algorithms. However, because of thevariety of situations that can be considered, results are designed to solve theproblem for a specific reward structure such as the Cascade Model. The presentwork focuses on the problem of ranking items when the user is allowed to clickon several items while scanning the list from top to bottom.
arxiv-16500-26 | Performance Localisation | http://arxiv.org/abs/1603.01489 | author:Brendan Cody-Kenny, Stephen Barrett category:cs.SE cs.NE cs.PF published:2016-03-04 summary:Profiling is a prominent technique for finding the location of performance"bottlenecks" in code. Profiling can be performed by adding code to a programwhich increments a counter for each line of code each time it is executed. Anylines of code which have a large execution count relative to other lines in theprogram can be considered a bottleneck. Though code profiling can determine thelocation of a performance issue or bottleneck, we posit that the code changerequired to improve performance may not always be found at the same location.Developers must frequently trace back through a program to understand what codeis contributing to a bottleneck. We seek to highlight code which is likelycausing or has the most effect on the overall execution cost of a program. Inthis document we compare different methods for localising potential performanceimprovements.
arxiv-16500-27 | Dynamic Memory Networks for Visual and Textual Question Answering | http://arxiv.org/abs/1603.01417 | author:Caiming Xiong, Stephen Merity, Richard Socher category:cs.NE cs.CL cs.CV published:2016-03-04 summary:Neural network architectures with memory and attention mechanisms exhibitcertain reasoning capabilities required for question answering. One sucharchitecture, the dynamic memory network (DMN), obtained high accuracy on avariety of language tasks. However, it was not shown whether the architectureachieves strong results for question answering when supporting facts are notmarked during training or whether it could be applied to other modalities suchas images. Based on an analysis of the DMN, we propose several improvements toits memory and input modules. Together with these changes we introduce a novelinput module for images in order to be able to answer visual questions. Our newDMN+ model improves the state of the art on both the Visual Question Answeringdataset and the \babi-10k text question-answering dataset without supportingfact supervision.
arxiv-16500-28 | X-rank and identifiability for a polynomial decomposition model | http://arxiv.org/abs/1603.01566 | author:Pierre Comon, Yang Qi, Konstantin Usevich category:cs.IT math.IT math.NA stat.ML published:2016-03-04 summary:In this paper, we study a polynomial decomposition model that arises inproblems of system identification, signal processing and machine learning. Weshow that this decomposition is a special case of the X-rank decomposition ---a powerful novel concept in algebraic geometry that generalizes the tensor CPdecomposition. We prove new results on generic/maximal rank and onidentifiability of the polynomial decomposition model. In the paper, we try tomake results and basic tools accessible for a general audience (assuming noknowledge of algebraic geometry or its prerequisites).
arxiv-16500-29 | Sentiment Analysis in Scholarly Book Reviews | http://arxiv.org/abs/1603.01595 | author:Hussam Hamdan, Patrice Bellot, Frederic Bechet category:cs.CL cs.AI published:2016-03-04 summary:So far different studies have tackled the sentiment analysis in severaldomains such as restaurant and movie reviews. But, this problem has not beenstudied in scholarly book reviews which is different in terms of review styleand size. In this paper, we propose to combine different features in order tobe presented to a supervised classifiers which extract the opinion targetexpressions and detect their polarities in scholarly book reviews. We constructa labeled corpus for training and evaluating our methods in French bookreviews. We also evaluate them on English restaurant reviews in order tomeasure their robustness across the domains and languages. The evaluation showsthat our methods are enough robust for English restaurant reviews and Frenchbook reviews.
arxiv-16500-30 | Lasso estimation for GEFCom2014 probabilistic electric load forecasting | http://arxiv.org/abs/1603.01376 | author:Florian Ziel, Bidong Liu category:stat.AP stat.ML G.3; I.5 published:2016-03-04 summary:We present a methodology for probabilistic load forecasting that is based onlasso (least absolute shrinkage and selection operator) estimation. The modelconsidered can be regarded as a bivariate time-varying thresholdautoregressive(AR) process for the hourly electric load and temperature. Thejoint modeling approach incorporates the temperature effects directly, andreflects daily, weekly, and annual seasonal patterns and public holidayeffects. We provide two empirical studies, one based on the probabilistic loadforecasting track of the Global Energy Forecasting Competition 2014(GEFCom2014-L), and the other based on another recent probabilistic loadforecasting competition that follows a setup similar to that of GEFCom2014-L.In both empirical case studies, the proposed methodology outperforms twomultiple linear regression based benchmarks from among the top eight entries toGEFCom2014-L.
arxiv-16500-31 | Learning deep representation of multityped objects and tasks | http://arxiv.org/abs/1603.01359 | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.CV cs.LG published:2016-03-04 summary:We introduce a deep multitask architecture to integrate multitypedrepresentations of multimodal objects. This multitype exposition is lessabstract than the multimodal characterization, but more machine-friendly, andthus is more precise to model. For example, an image can be described bymultiple visual views, which can be in the forms of bag-of-words (counts) orcolor/texture histograms (real-valued). At the same time, the image may haveseveral social tags, which are best described using a sparse binary vector. Ourdeep model takes as input multiple type-specific features, narrows thecross-modality semantic gaps, learns cross-type correlation, and produces ahigh-level homogeneous representation. At the same time, the model supportsheterogeneously typed tasks. We demonstrate the capacity of the model on twoapplications: social image retrieval and multiple concept prediction. The deeparchitecture produces more compact representation, naturally integratesmultiviews and multimodalities, exploits better side information, and mostimportantly, performs competitively against baselines.
arxiv-16500-32 | Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning | http://arxiv.org/abs/1603.01597 | author:Mike Kestemont, Jeroen De Gussem category:cs.CL cs.LG stat.ML published:2016-03-04 summary:In this paper we consider two sequence tagging tasks for medieval Latin:part-of-speech tagging and lemmatization. These are both basic, yetfoundational preprocessing steps in applications such as text re-use detection.Nevertheless, they are generally complicated by the considerable orthographicvariation which is typical of medieval Latin. In Digital Classics, these tasksare traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion.For example, a lexicon is used to generate all the potential lemma-tag pairsfor a token, and next, a context-aware PoS-tagger is used to select the mostappropriate tag-lemma pair. Apart from the problems with out-of-lexicon items,error percolation is a major downside of such approaches. In this paper weexplore the possibility to elegantly solve these tasks using a single,integrated approach. For this, we make use of a layered neural networkarchitecture from the field of deep representation learning.
arxiv-16500-33 | A Unified View of Localized Kernel Learning | http://arxiv.org/abs/1603.01374 | author:John Moeller, Sarathkrishna Swaminathan, Suresh Venkatasubramanian category:cs.LG stat.ML published:2016-03-04 summary:Multiple Kernel Learning, or MKL, extends (kernelized) SVM by attempting tolearn not only a classifier/regressor but also the best kernel for the trainingtask, usually from a combination of existing kernel functions. Most MKL methodsseek the combined kernel that performs best over every training example,sacrificing performance in some areas to seek a global optimum. Localizedkernel learning (LKL) overcomes this limitation by allowing the trainingalgorithm to match a component kernel to the examples that can exploit it best.Several approaches to the localized kernel learning problem have been exploredin the last several years. We unify many of these approaches under one simplesystem and design a new algorithm with improved performance. We also developenhanced versions of existing algorithms, with an eye on scalability andperformance.
arxiv-16500-34 | Neural Architectures for Named Entity Recognition | http://arxiv.org/abs/1603.01360 | author:Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer category:cs.CL published:2016-03-04 summary:State-of-the-art named entity recognition systems rely heavily onhand-crafted features and domain-specific knowledge in order to learneffectively from the small, supervised training corpora that are available. Inthis paper, we introduce two new neural architectures---one based onbidirectional LSTMs and conditional random fields, and the other thatconstructs and labels segments using a transition-based approach inspired byshift-reduce parsers. Our models rely on two sources of information aboutwords: character-based word representations learned from the supervised corpusand unsupervised word representations learned from unannotated corpora. Ourmodels obtain state-of-the-art performance in NER in four languages withoutresorting to any language-specific knowledge or resources such as gazetteers.
arxiv-16500-35 | End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF | http://arxiv.org/abs/1603.01354 | author:Xuezhe Ma, Eduard Hovy category:cs.LG cs.CL stat.ML published:2016-03-04 summary:State-of-the-art sequence labeling systems traditionally require largeamounts of task-specific knowledge in the form of hand-crafted features anddata pre-processing. In this paper, we introduce a novel neutral networkarchitecture that benefits from both word- and character-level representationsautomatically, by using combination of bidirectional LSTM, CNN and CRF. Oursystem is truly end-to-end, requiring no feature engineering or datapre-processing, thus making it applicable to a wide range of sequence labelingtasks on different languages. We evaluate our system on two data sets for twosequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS)tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtainstate-of-the-art performance on both the two data --- 97.55\% accuracy for POStagging and 91.21\% F1 for NER.
arxiv-16500-36 | Depth Superresolution using Motion Adaptive Regularization | http://arxiv.org/abs/1603.01633 | author:Ulugbek S. Kamilov, Petros T. Boufounos category:cs.CV published:2016-03-04 summary:Spatial resolution of depth sensors is often significantly lower compared tothat of conventional optical cameras. Recent work has explored the idea ofimproving the resolution of depth using higher resolution intensity as a sideinformation. In this paper, we demonstrate that further incorporating temporalinformation in videos can significantly improve the results. In particular, wepropose a novel approach that improves depth resolution, exploiting thespace-time redundancy in the depth and intensity using motion-adaptive low-rankregularization. Experiments confirm that the proposed approach substantiallyimproves the quality of the estimated high-resolution depth. Our approach canbe a first component in systems using vision techniques that rely on highresolution depth information.
arxiv-16500-37 | Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks | http://arxiv.org/abs/1603.01431 | author:Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, Venu Govindaraju category:stat.ML cs.LG published:2016-03-04 summary:While the authors of Batch Normalization (BN) identify and address animportant problem involved in training deep networks-- \textit{InternalCovariate Shift}-- the current solution has multiple drawbacks. For instance,BN depends on batch statistics for layerwise input normalization duringtraining which makes the estimates of mean and standard deviation of input(distribution) to hidden layers inaccurate due to shifting parameter values(specially during initial training epochs). Another fundamental problem with BNis that it cannot be used with batch-size $ 1 $ during training. We addressthese (and other) drawbacks of BN by proposing a non-adaptive normalizationtechnique for removing covariate shift, that we call \textit{NormalizationPropagation}. Our approach does not depend on batch statistics, but rather usesa data-independent parametric estimate of mean and standard-deviation in everylayer thus being faster compared with BN. We exploit the observation that thepre-activation before Rectified Linear Units follow a Gaussian distribution indeep networks, and that once the first and second order statistics of any givendataset are normalized, we can forward propagate this normalization without theneed for recalculating the approximate statistics (using data) for any of thehidden layers.
arxiv-16500-38 | Training Input-Output Recurrent Neural Networks through Spectral Methods | http://arxiv.org/abs/1603.00954 | author:Hanie Sedghi, Anima Anandkumar category:cs.LG cs.NE stat.ML published:2016-03-03 summary:We consider the problem of training input-output recurrent neural networks(RNN) for sequence labeling tasks. We propose a novel spectral approach forlearning the network parameters. It is based on decomposition of thecross-moment tensor between the output and a non-linear transformation of theinput, based on score functions. We guarantee consistent learning withpolynomial sample and computational complexity under transparent conditionssuch as non-degeneracy of model parameters, polynomial activations for theneurons, and a Markovian evolution of the input sequence. We also extend ourresults to Bidirectional RNN which uses both previous and future information tooutput the label at each time point, and is employed in many NLP tasks such asPOS tagging.
arxiv-16500-39 | Decision Forests, Convolutional Networks and the Models in-Between | http://arxiv.org/abs/1603.01250 | author:Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie Shotton, Matthew Brown, Antonio Criminisi category:cs.CV cs.AI published:2016-03-03 summary:This paper investigates the connections between two state of the artclassifiers: decision forests (DFs, including decision jungles) andconvolutional neural networks (CNNs). Decision forests are computationallyefficient thanks to their conditional computation property (computation isconfined to only a small region of the tree, the nodes along a single branch).CNNs achieve state of the art accuracy, thanks to their representation learningcapabilities. We present a systematic analysis of how to fuse conditionalcomputation with representation learning and achieve a continuum of hybridmodels with different ratios of accuracy vs. efficiency. We call this newfamily of hybrid models conditional networks. Conditional networks can bethought of as: i) decision trees augmented with data transformation operators,or ii) CNNs, with block-diagonal sparse weight matrices, and explicit datarouting functions. Experimental validation is performed on the common task ofimage classification on both the CIFAR and Imagenet datasets. Compared to stateof the art CNNs, our hybrid models yield the same accuracy with a fraction ofthe compute cost and much smaller number of parameters.
arxiv-16500-40 | Learning Real and Boolean Functions: When Is Deep Better Than Shallow | http://arxiv.org/abs/1603.00988 | author:Hrushikesh Mhaskar, Qianli Liao, Tomaso Poggio category:cs.LG published:2016-03-03 summary:We describe computational tasks - especially in vision - that correspond tocompositional/hierarchical functions. While the universal approximationproperty holds both for hierarchical and shallow networks, we prove that deep(hierarchical) networks can approximate the class of compositional functionswith the same accuracy as shallow networks but with exponentially lowerVC-dimension as well as the number of training parameters. This leads to thequestion of approximation by sparse polynomials (in the number of independentparameters) and, as a consequence, by deep networks. We also discussconnections between our results and learnability of sparse Boolean functions,settling an old conjecture by Bengio.
arxiv-16500-41 | Multi-domain Neural Network Language Generation for Spoken Dialogue Systems | http://arxiv.org/abs/1603.01232 | author:Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, David Vandyke, Steve Young category:cs.CL published:2016-03-03 summary:Moving from limited-domain natural language generation (NLG) to open domainis difficult because the number of semantic input combinations growsexponentially with the number of domains. Therefore, it is important toleverage existing resources and exploit similarities between domains tofacilitate domain adaptation. In this paper, we propose a procedure to trainmulti-domain, Recurrent Neural Network-based (RNN) language generators viamultiple adaptation steps. In this procedure, a model is first trained oncounterfeited data synthesised from an out-of-domain dataset, and then finetuned on a small set of in-domain utterances with a discriminative objectivefunction. Corpus-based evaluation results show that the proposed procedure canachieve competitive performance in terms of BLEU score and slot error ratewhile significantly reducing the data needed to train generators in new, unseendomains. In subjective testing, human judges confirm that the procedure greatlyimproves generator performance when only a small amount of data is available inthe domain.
arxiv-16500-42 | HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition | http://arxiv.org/abs/1603.01249 | author:Rajeev Ranjan, Vishal M. Patel, Rama Chellappa category:cs.CV published:2016-03-03 summary:We present an algorithm for simultaneous face detection, landmarkslocalization, pose estimation and gender recognition using deep convolutionalneural networks (CNN). The proposed method called, Hyperface, fuses theintermediate layers of a deep CNN using a separate CNN and trains multi-taskloss on the fused features. It exploits the synergy among the tasks whichboosts up their individual performances. Extensive experiments show that theproposed method is able to capture both global and local information of facesand performs significantly better than many competitive algorithms for each ofthese four tasks.
arxiv-16500-43 | Convolutional Neural Networks using Logarithmic Data Representation | http://arxiv.org/abs/1603.01025 | author:Daisuke Miyashita, Edward H. Lee, Boris Murmann category:cs.NE cs.LG published:2016-03-03 summary:Recent advances in convolutional neural networks have considered modelcomplexity and hardware efficiency to enable deployment onto embedded systemsand mobile devices. For example, it is now well-known that the arithmeticoperations of deep networks can be encoded down to 8-bit fixed-point withoutsignificant deterioration in performance. However, further reduction inprecision down to as low as 3-bit fixed-point results in significant losses inperformance. In this paper we propose a new data representation that enablesstate-of-the-art networks to be encoded to 3 bits with negligible loss inclassification performance. To perform this, we take advantage of the fact thatthe weights and activations in a trained network naturally have non-uniformdistributions. Using non-uniform, base-2 logarithmic representation to encodeweights, communicate activations, and perform dot-products enables networks to1) achieve higher classification accuracies than fixed-point at the sameresolution and 2) eliminate bulky digital multipliers. Finally, we propose anend-to-end training procedure that uses log representation at 5-bits, whichachieves higher final test accuracy than linear at 5-bits.
arxiv-16500-44 | Rank Aggregation for Course Sequence Discovery | http://arxiv.org/abs/1603.02695 | author:Mihai Cucuringu, Charlie Marshak, Dillon Montag, Puck Rombach category:cs.LG published:2016-03-03 summary:In this work, we adapt the rank aggregation framework for the discovery ofoptimal course sequences at the university level. Each student provides apartial ranking of the courses taken throughout his or her undergraduatecareer. We compute pairwise rank comparisons between courses based on the orderstudents typically take them, aggregate the results over the entire studentpopulation, and then obtain a proxy for the rank offset between pairs ofcourses. We extract a global ranking of the courses via several state-of-theart algorithms for ranking with pairwise noisy information, includingSerialRank, Rank Centrality, and the recent SyncRank based on the groupsynchronization problem. We test this application of rank aggregation on 15years of student data from the Department of Mathematics at the University ofCalifornia, Los Angeles (UCLA). Furthermore, we experiment with the aboveapproach on different subsets of the student population conditioned on finalGPA, and highlight several differences in the obtained rankings that uncoverhidden pre-requisites in the Mathematics curriculum.
arxiv-16500-45 | Whitening-Free Least-Squares Non-Gaussian Component Analysis | http://arxiv.org/abs/1603.01029 | author:Hiroaki Shiino, Hiroaki Sasaki, Gang Niu, Masashi Sugiyama category:stat.ML published:2016-03-03 summary:Non-Gaussian component analysis (NGCA) is an unsupervised linear dimensionreduction method that extracts low-dimensional non-Gaussian "signals" fromhigh-dimensional data contaminated with Gaussian noise. NGCA can be regarded asa generalization of projection pursuit (PP) and independent component analysis(ICA) to multi-dimensional and dependent non-Gaussian components. Indeed,seminal approaches to NGCA are based on PP and ICA. Recently, a novel NGCAapproach called least-squares NGCA (LSNGCA) has been developed, which gives asolution analytically through least-squares estimation of log-density gradientsand eigendecomposition. However, since pre-whitening of data is involved inLSNGCA, it performs unreliably when the data covariance matrix isill-conditioned, which is often the case in high-dimensional data analysis. Inthis paper, we propose a whitening-free LSNGCA method and experimentallydemonstrate its superiority.
arxiv-16500-46 | Photographic dataset: random peppercorns | http://arxiv.org/abs/1603.01046 | author:Teemu Helenius, Samuli Siltanen category:cs.CV published:2016-03-03 summary:This is a photographic dataset collected for testing image processingalgorithms. The idea is to have sets of different but statistically similarimages. In this work the images show randomly distributed peppercorns. Thedataset is made available at www.fips.fi/photographic_dataset.php .
arxiv-16500-47 | Automatic learning of gait signatures for people identification | http://arxiv.org/abs/1603.01006 | author:F. M. Castro, M. J. Marin-Jimenez, N. Guil, N. Perez de la Blanca category:cs.CV cs.AI published:2016-03-03 summary:This work targets people identification in video based on the way they walk(i.e. gait). While classical methods typically derive gait signatures fromsequences of binary silhouettes, in this work we explore the use ofconvolutional neural networks (CNN) for learning high-level descriptors fromlow-level motion features (i.e. optical flow components). We carry out athorough experimental evaluation of the proposed CNN architecture on thechallenging TUM-GAID dataset. The experimental results indicate that usingspatio-temporal cuboids of optical flow as input data for CNN allows to obtainstate-of-the-art results on the gait task with an image resolution eight timeslower than the previously reported results (i.e. 80x60 pixels).
arxiv-16500-48 | Camera identification with deep convolutional networks | http://arxiv.org/abs/1603.01068 | author:Luca Baroffio, Luca Bondi, Paolo Bestagini, Stefano Tubaro category:cs.CV cs.MM published:2016-03-03 summary:The possibility of detecting which camera has been used to shoot a specificpicture is of paramount importance for many forensics tasks. This is extremelyuseful for copyright infringement cases, ownership attribution, as well as fordetecting the authors of distributed illicit material (e.g., pedo-pornographicshots). Due to its importance, the forensics community has developed a seriesof robust detectors that exploit characteristic traces left by each camera onthe acquired images during the acquisition pipeline. These traces arereverse-engineered in order to attribute a picture to a camera. In this paper,we investigate an alternative approach to solve camera identification problem.Indeed, we propose a data-driven algorithm based on convolutional neuralnetworks, which learns features characterizing each camera directly from theacquired pictures. The proposed approach is tested on both instance-attributionand model-attribution, providing an accuracy greater than 94% in discriminating27 camera models.
arxiv-16500-49 | A novel and automatic pectoral muscle identification algorithm for mediolateral oblique (MLO) view mammograms using ImageJ | http://arxiv.org/abs/1603.01056 | author:Chao Wang category:cs.CV published:2016-03-03 summary:Pectoral muscle identification is often required for breast cancer riskanalysis, such as estimating breast density. Traditional methods areoverwhelmingly based on manual visual assessment or straight line fitting forthe pectoral muscle boundary, which are inefficient and inaccurate sincepectoral muscle in mammograms can have curved boundaries. This paper proposes a novel and automatic pectoral muscle identificationalgorithm for MLO view mammograms. It is suitable for both scanned film andfull field digital mammograms. This algorithm is demonstrated using a publicdomain software ImageJ. A validation of this algorithm has been performed usingreal-world data and it shows promising result.
arxiv-16500-50 | What is the right way to represent document images? | http://arxiv.org/abs/1603.01076 | author:Gabriela Csurka, Diane Larlus, Albert Gordo, Jon Almazan category:cs.CV published:2016-03-03 summary:In this article we study the problem of document image representation basedon visual features. We propose a comprehensive experimental study that comparesthree types of visual document image representations: (1) traditional so-calledshallow features, such as the RunLength and the Fisher-Vector descriptors, (2)deep features based on Convolutional Neural Networks, and (3) featuresextracted from hybrid architectures that take inspiration from the two previousones. We evaluate these features in several tasks (i.e. classification, clustering,and retrieval) and in different setups (e.g. domain transfer) using severalpublic and in-house datasets. Our results show that deep features generallyoutperform other types of features when there is no domain shift and the newtask is closely related to the one used to train the model. However, when alarge domain or task shift is present, the Fisher-Vector shallow featuresgeneralize better and often obtain the best results.
arxiv-16500-51 | Self-localization from Images with Small Overlap | http://arxiv.org/abs/1603.00993 | author:Tanaka Kanji category:cs.CV published:2016-03-03 summary:With the recent success of visual features from deep convolutional neuralnetworks (DCNN) in visual robot self-localization, it has become important andpractical to address more general self-localization scenarios. In this paper,we address the scenario of self-localization from images with small overlap. Weexplicitly introduce a localization difficulty index as a decreasing functionof view overlap between query and relevant database images and investigateperformance versus difficulty for challenging cross-view self-localizationtasks. We then reformulate the self-localization as a scalablebag-of-visual-features (BoVF) scene retrieval and present an efficient solutioncalled PCA-NBNN, aiming to facilitate fast and yet discriminativecorrespondence between partially overlapping images. The proposed approachadopts recent findings in discriminativity preserving encoding of DCNN featuresusing principal component analysis (PCA) and cross-domain scene matching usingnaive Bayes nearest neighbor distance metric (NBNN). We experimentallydemonstrate that the proposed PCA-NBNN framework frequently achieves comparableresults to previous DCNN features and that the BoVF model is significantly moreefficient. We further address an important alternative scenario of"self-localization from images with NO overlap" and report the result.
arxiv-16500-52 | Right Ideals of a Ring and Sublanguages of Science | http://arxiv.org/abs/1603.01032 | author:Javier Arias Navarro category:cs.CL published:2016-03-03 summary:Among Zellig Harris's numerous contributions to linguistics his theory of thesublanguages of science probably ranks among the most underrated. However, notonly has this theory led to some exhaustive and meaningful applications in thestudy of the grammar of immunology language and its changes over time, but italso illustrates the nature of mathematical relations between chunks or subsetsof a grammar and the language as a whole. This becomes most clear when dealingwith the connection between metalanguage and language, as well as whenreflecting on operators. This paper tries to justify the claim that the sublanguages of science standin a particular algebraic relation to the rest of the language they areembedded in, namely, that of right ideals in a ring.
arxiv-16500-53 | Interactive and Scale Invariant Segmentation of the Rectum/Sigmoid via User-Defined Templates | http://arxiv.org/abs/1603.00961 | author:Tobias Lüddemann, Jan Egger category:cs.CV cs.GR published:2016-03-03 summary:Among all types of cancer, gynecological malignancies belong to the 4th mostfrequent type of cancer among women. Besides chemotherapy and external beamradiation, brachytherapy is the standard procedure for the treatment of thesemalignancies. In the progress of treatment planning, localization of the tumoras the target volume and adjacent organs of risks by segmentation is crucial toaccomplish an optimal radiation distribution to the tumor while simultaneouslypreserving healthy tissue. Segmentation is performed manually and represents atime-consuming task in clinical daily routine. This study focuses on thesegmentation of the rectum/sigmoid colon as an Organ-At-Risk in gynecologicalbrachytherapy. The proposed segmentation method uses an interactive,graph-based segmentation scheme with a user-defined template. The schemecreates a directed two dimensional graph, followed by the minimal cost closedset computation on the graph, resulting in an outlining of the rectum. Thegraphs outline is dynamically adapted to the last calculated cut. Evaluationwas performed by comparing manual segmentations of the rectum/sigmoid colon toresults achieved with the proposed method. The comparison of the algorithmic tomanual results yielded to a Dice Similarity Coefficient value of 83.85+/-4.08%,in comparison to 83.97+/-8.08% for the comparison of two manual segmentationsof the same physician. Utilizing the proposed methodology resulted in a mediantime of 128 seconds per dataset, compared to 300 seconds needed for pure manualsegmentation.
arxiv-16500-54 | Overdispersed Black-Box Variational Inference | http://arxiv.org/abs/1603.01140 | author:Francisco J. R. Ruiz, Michalis K. Titsias, David M. Blei category:stat.ML published:2016-03-03 summary:We introduce overdispersed black-box variational inference, a method toreduce the variance of the Monte Carlo estimator of the gradient in black-boxvariational inference. Instead of taking samples from the variationaldistribution, we use importance sampling to take samples from an overdisperseddistribution in the same exponential family as the variational approximation.Our approach is general since it can be readily applied to any exponentialfamily distribution, which is the typical choice for the variationalapproximation. We run experiments on two non-conjugate probabilistic models toshow that our method effectively reduces the variance, and the overheadintroduced by the computation of the proposal parameters and the importanceweights is negligible. We find that our overdispersed importance samplingscheme provides lower variance than black-box variational inference, even whenthe latter uses twice the number of samples. This results in faster convergenceof the black-box inference procedure.
arxiv-16500-55 | Cellular Automata Segmentation of the Boundary between the Compacta of Vertebral Bodies and Surrounding Structures | http://arxiv.org/abs/1603.00960 | author:Jan Egger, Christopher Nimsky category:cs.CV cs.CG cs.GR published:2016-03-03 summary:Due to the aging population, spinal diseases get more and more commonnowadays; e.g., lifetime risk of osteoporotic fracture is 40% for white womenand 13% for white men in the United States. Thus the numbers of surgical spinalprocedures are also increasing with the aging population and precise diagnosisplays a vital role in reducing complication and recurrence of symptoms. Spinalimaging of vertebral column is a tedious process subjected to interpretationerrors. In this contribution, we aim to reduce time and error for vertebralinterpretation by applying and studying the GrowCut-algorithm for boundarysegmentation between vertebral body compacta and surrounding structures.GrowCut is a competitive region growing algorithm using cellular automata. Forour study, vertebral T2-weighted Magnetic Resonance Imaging (MRI) scans werefirst manually outlined by neurosurgeons. Then, the vertebral bodies weresegmented in the medical images by a GrowCut-trained physician using thesemi-automated GrowCut-algorithm. Afterwards, results of both segmentationprocesses were compared using the Dice Similarity Coefficient (DSC) and theHausdorff Distance (HD) which yielded to a DSC of 82.99+/-5.03% and a HD of18.91+/-7.2 voxel, respectively. In addition, the times have been measuredduring the manual and the GrowCut segmentations, showing that aGrowCut-segmentation - with an average time of less than six minutes(5.77+/-0.73) - is significantly shorter than a pure manual outlining.
arxiv-16500-56 | PCANet: An energy perspective | http://arxiv.org/abs/1603.00944 | author:Jiasong Wu, Shijie Qiu, Youyong Kong, Longyu Jiang, Lotfi Senhadji, Huazhong Shu category:cs.CV published:2016-03-03 summary:The principal component analysis network (PCANet), which is one of therecently proposed deep learning architectures, achieves the state-of-the-artclassification accuracy in various databases. However, the explanation of thePCANet is lacked. In this paper, we try to explain why PCANet works well fromenergy perspective point of view based on a set of experiments. The impact ofvarious parameters on the error rate of PCANet is analyzed in depth. It wasfound that this error rate is correlated with the logarithm of energy of image.The proposed energy explanation approach can be used as a testing method forchecking if every step of the constructed networks is necessary.
arxiv-16500-57 | Enhancing Freebase Question Answering Using Textual Evidence | http://arxiv.org/abs/1603.00957 | author:Kun Xu, Yansong Feng, Siva Reddy, Songfang Huang, Dongyan Zhao category:cs.CL published:2016-03-03 summary:Existing knowledge-based question answering systems often rely on smallannotated training data. While shallow methods like information extractiontechniques are robust to data scarcity, they are less expressive than deepunderstanding methods, thereby failing at answering questions involvingmultiple constraints. Here we alleviate this problem by empowering a relationextraction method with additional evidence from Wikipedia. We first present anovel neural network based relation extractor to retrieve the candidate answersfrom Freebase, and then develop a refinement model to validate answers usingWikipedia. We achieve 53.3 F1 on WebQuestions, a substantial improvement overthe state-of-the-art.
arxiv-16500-58 | Deep Reinforcement Learning from Self-Play in Imperfect-Information Games | http://arxiv.org/abs/1603.01121 | author:Johannes Heinrich, David Silver category:cs.LG cs.AI cs.GT published:2016-03-03 summary:Many real-world applications can be described as large-scale games ofimperfect information. To deal with these challenging domains, prior work hasfocused on computing Nash equilibria in a handcrafted abstraction of thedomain. In this paper we introduce the first scalable end-to-end approach tolearning approximate Nash equilibria without any prior knowledge. Our methodcombines fictitious self-play with deep reinforcement learning. When applied toLeduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium,whereas common reinforcement learning methods diverged. In Limit Texas Holdem,a poker game of real-world scale, NFSP learnt a competitive strategy thatapproached the performance of human experts and state-of-the-art methods.
arxiv-16500-59 | Elastic Net Hypergraph Learning for Image Clustering and Semi-supervised Classification | http://arxiv.org/abs/1603.01096 | author:Qingshan Liu, Yubao Sun, Cantian Wang, Tongliang Liu, Dacheng Tao category:cs.CV published:2016-03-03 summary:Graph model is emerging as a very effective tool for learning the complexstructures and relationships hidden in data. Generally, the critical purpose ofgraph-oriented learning algorithms is to construct an informative graph forimage clustering and classification tasks. In addition to the classical$K$-nearest-neighbor and $r$-neighborhood methods for graph construction,$l_1$-graph and its variants are emerging methods for finding the neighboringsamples of a center datum, where the corresponding ingoing edge weights aresimultaneously derived by the sparse reconstruction coefficients of theremaining samples. However, the pair-wise links of $l_1$-graph are not capableof capturing the high order relationships between the center datum and itsprominent data in sparse reconstruction. Meanwhile, from the perspective ofvariable selection, the $l_1$ norm sparse constraint, regarded as a LASSOmodel, tends to select only one datum from a group of data that are highlycorrelated and ignore the others. To simultaneously cope with these drawbacks,we propose a new elastic net hypergraph learning model, which consists of twosteps. In the first step, the Robust Matrix Elastic Net model is constructed tofind the canonically related samples in a somewhat greedy way, achieving thegrouping effect by adding the $l_2$ penalty to the $l_1$ constraint. In thesecond step, hypergraph is used to represent the high order relationshipsbetween each datum and its prominent samples by regarding them as a hyperedge.Subsequently, hypergraph Laplacian matrix is constructed for further analysis.New hypergraph learning algorithms, including unsupervised clustering andmulti-class semi-supervised classification, are then derived. Extensiveexperiments on face and handwriting databases demonstrate the effectiveness ofthe proposed method.
arxiv-16500-60 | Audio Word2Vec: Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Autoencoder | http://arxiv.org/abs/1603.00982 | author:Yu-An Chung, Chao-Chung Wu, Chia-Hao Shen, Hung-Yi Lee, Lin-Shan Lee category:cs.SD cs.LG published:2016-03-03 summary:The vector representations of fixed dimensionality for words (in text)offered by Word2Vec have been shown to be very useful in many applicationscenarios, in particular due to the semantic information they carry. This paperproposes a parallel version, the Audio Word2Vec. It offers the vectorrepresentations of fixed dimensionality for variable-length audio segments.These vector representations are shown to describe the sequential phoneticstructures of the audio segments to a good degree, with very attractive realworld applications such as query-by-example Spoken Term Detection (STD). Inthis STD application, the proposed approach significantly outperformed theconventional Dynamic Time Warping (DTW) based approaches at significantly lowercomputation requirements. We propose unsupervised learning of Audio Word2Vecfrom audio data without human annotation using Sequence-to-sequence Audoencoder(SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM)units: the first RNN (encoder) maps the input audio sequence into a vectorrepresentation of fixed dimensionality, and the second RNN (decoder) maps therepresentation back to the input audio sequence. The two RNNs are jointlytrained by minimizing the reconstruction error. Denoising Sequence-to-sequenceAutoencoder (DSA) is furthered proposed offering more robust learning.
arxiv-16500-61 | Modeling the Sequence of Brain Volumes by Local Mesh Models for Brain Decoding | http://arxiv.org/abs/1603.01067 | author:Itir Onal, Mete Ozay, Eda Mizrak, Ilke Oztekin, Fatos T. Yarman Vural category:cs.LG cs.AI cs.CV published:2016-03-03 summary:We represent the sequence of fMRI (Functional Magnetic Resonance Imaging)brain volumes recorded during a cognitive stimulus by a graph which consists ofa set of local meshes. The corresponding cognitive process, encoded in thebrain, is then represented by these meshes each of which is estimated assuminga linear relationship among the voxel time series in a predefined locality.First, we define the concept of locality in two neighborhood systems, namely,the spatial and functional neighborhoods. Then, we construct spatially andfunctionally local meshes around each voxel, called seed voxel, by connectingit either to its spatial or functional p-nearest neighbors. The mesh formedaround a voxel is a directed sub-graph with a star topology, where thedirection of the edges is taken towards the seed voxel at the center of themesh. We represent the time series recorded at each seed voxel in terms oflinear combination of the time series of its p-nearest neighbors in the mesh.The relationships between a seed voxel and its neighbors are represented by theedge weights of each mesh, and are estimated by solving a linear regressionequation. The estimated mesh edge weights lead to a better representation ofinformation in the brain for encoding and decoding of the cognitive tasks. Wetest our model on a visual object recognition and emotional memory retrievalexperiments using Support Vector Machines that are trained using the mesh edgeweights as features. In the experimental analysis, we observe that the edgeweights of the spatial and functional meshes perform better than thestate-of-the-art brain decoding models.
arxiv-16500-62 | Modular Decomposition and Analysis of Registration based Trackers | http://arxiv.org/abs/1603.01292 | author:Abhineet Singh, Ankush Roy, Xi Zhang, Martin Jagersand category:cs.CV published:2016-03-03 summary:This paper presents a new way to study registration based trackers bydecomposing them into three constituent sub modules: appearance model, statespace model and search method. It is often the case that when a new tracker isintroduced in literature, it only contributes to one or two of these submodules while using existing methods for the rest. Since these are oftenselected arbitrarily by the authors, they may not be optimal for the newmethod. In such cases, our breakdown can help to experimentally find the bestcombination of methods for these sub modules while also providing a frameworkwithin which the contributions of the new tracker can be clearly demarcated andthus studied better. We show how existing trackers can be broken down using thesuggested methodology and compare the performance of the default configurationchosen by the authors against other possible combinations to demonstrate thenew insights that can be gained by such an approach. We also present an opensource system that provides a convenient interface to plug in a new method forany sub module and test it against all possible combinations of methods for theother two sub modules while also serving as a fast and efficient solution forpractical tracking requirements.
arxiv-16500-63 | Sparse model selection in the highly under-sampled regime | http://arxiv.org/abs/1603.00952 | author:Nicola Bulso, Matteo Marsili, Yasser Roudi category:stat.ML published:2016-03-03 summary:We propose a method for recovering the structure of a sparse undirectedgraphical model when very few samples are available. The method decides aboutthe presence or absence of bonds between pairs of variable by considering onepair at a time and using a closed form formula, analytically derived bycalculating the posterior probability for every possible model explaining a twobody system using Jeffreys prior. The approach does not rely on theoptimisation of any cost functions and consequently is much faster thanexisting algorithms. Despite this time and computational advantage, numericalresults show that for several sparse topologies the algorithm is comparable tothe best existing algorithms, and is more accurate in the presence of hiddenvariables. We apply this approach to the analysis of US stock market data andto neural data, in order to show its efficiency in recovering robuststatistical dependencies in real data with non stationary correlations in timeand space.
arxiv-16500-64 | MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification | http://arxiv.org/abs/1603.00968 | author:Ye Zhang, Stephen Roller, Byron Wallace category:cs.CL published:2016-03-03 summary:We introduce a novel, simple convolution neural network (CNN) architecture -multi-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets ofword embeddings for sentence classification. MGNC-CNN extracts features frominput embedding sets independently and then joins these at the penultimatelayer in the network to form a final feature vector. We then adopt a groupregularization strategy that differentially penalizes weights associated withthe subcomponents generated from the respective embedding sets. This model ismuch simpler than comparable alternative architectures and requiressubstantially less training time. Furthermore, it is flexible in that it doesnot require input word embeddings to be of the same dimensionality. We showthat MGNC-CNN consistently outperforms baseline models.
arxiv-16500-65 | Hybrid Collaborative Filtering with Neural Networks | http://arxiv.org/abs/1603.00806 | author:Florian Strub, Jeremie Mary, Romaric Gaudel category:cs.IR cs.AI cs.NE published:2016-03-02 summary:Collaborative Filtering aims at exploiting the feedback of users to providepersonalised recommendations. Such algorithms look for latent variables in alarge sparse matrix of ratings. They can be enhanced by adding side informationto tackle the well-known cold start problem. While Neu-ral Networks havetremendous success in image and speech recognition, they have received lessattention in Collaborative Filtering. This is all the more surprising thatNeural Networks are able to discover latent variables in large andheterogeneous datasets. In this paper, we introduce a Collaborative FilteringNeural network architecture aka CFN which computes a non-linear MatrixFactorization from sparse rating inputs and side information. We showexperimentally on the MovieLens and Douban dataset that CFN outper-forms thestate of the art and benefits from side information. We provide animplementation of the algorithm as a reusable plugin for Torch, a popularNeural Network framework.
arxiv-16500-66 | Super Mario as a String: Platformer Level Generation Via LSTMs | http://arxiv.org/abs/1603.00930 | author:Adam Summerville, Michael Mateas category:cs.NE cs.LG published:2016-03-02 summary:The procedural generation of video game levels has existed for at least 30years, but only recently have machine learning approaches been used to generatelevels without specifying the rules for generation. A number of these havelooked at platformer levels as a sequence of characters and performedgeneration using Markov chains. In this paper we examine the use of LongShort-Term Memory recurrent neural networks (LSTMs) for the purpose ofgenerating levels trained from a corpus of Super Mario Brothers levels. Weanalyze a number of different data representations and how the generated levelsfit into the space of human authored Super Mario Brothers levels.
arxiv-16500-67 | Evolving Boolean Regulatory Networks with Variable Gene Expression Times | http://arxiv.org/abs/1603.01185 | author:Larry Bull category:q-bio.BM cs.NE q-bio.MN published:2016-03-02 summary:The time taken for gene expression varies not least because proteins vary inlength considerably. This paper uses an abstract, tuneable Boolean regulatorynetwork model to explore gene expression time variation. In particular, it isshown how non-uniform expression times can emerge under certain conditionsthrough simulated evolution. That is, gene expression time variance appearsbeneficial in the shaping of the dynamical behaviour of the regulatory networkwithout explicit consideration of protein function.
arxiv-16500-68 | Automatic segmentation of lizard spots using an active contour model | http://arxiv.org/abs/1603.00841 | author:Jhony Giraldo, Augusto Salazar category:cs.CV published:2016-03-02 summary:Animal biometrics is a challenging task. In the literature, many algorithmshave been used, e.g. penguin chest recognition, elephant ears recognition andleopard stripes pattern recognition, but to use technology to a large extent inthis area of research, still a lot of work has to be done. One important targetin animal biometrics is to automate the segmentation process, so in this paperwe propose a segmentation algorithm for extracting the spots of Diploglossusmillepunctatus, an endangered lizard species. The automatic segmentation isachieved with a combination of preprocessing, active contours and morphology.The parameters of each stage of the segmentation algorithm are found using anoptimization procedure, which is guided by the ground truth. The results showthat automatic segmentation of spots is possible. A 78.37 % of correctsegmentation in average is reached.
arxiv-16500-69 | A Kernel Test for Three-Variable Interactions with Random Processes | http://arxiv.org/abs/1603.00929 | author:Paul K. Rubenstein, Kacper P. Chwialkowski, Arthur Gretton category:stat.ML 62G10 published:2016-03-02 summary:We apply a wild bootstrap method to the Lancaster three-variable interactionmeasure in order to detect factorisation of the joint distribution on threevariables forming a stationary random process, for which the existingpermutation bootstrap method fails. As in the i.i.d. case, the Lancaster testis found to outperform existing tests in cases for which two independentvariables individually have a weak influence on a third, but that whenconsidered jointly the influence is strong. The main contributions of thispaper are twofold: first, we prove that the Lancaster statistic satisfies theconditions required to estimate the quantiles of the null distribution usingthe wild bootstrap; second, the manner in which this is proved is novel,simpler than existing methods, and can further be applied to other statistics.
arxiv-16500-70 | Distributed Estimation of Dynamic Parameters : Regret Analysis | http://arxiv.org/abs/1603.00576 | author:Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie category:math.OC cs.LG cs.SI published:2016-03-02 summary:This paper addresses the estimation of a time- varying parameter in anetwork. A group of agents sequentially receive noisy signals about theparameter (or moving target), which does not follow any particular dynamics.The parameter is not observable to an individual agent, but it is globallyidentifiable for the whole network. Viewing the problem with an onlineoptimization lens, we aim to provide the finite-time or non-asymptotic analysisof the problem. To this end, we use a notion of dynamic regret which suits theonline, non-stationary nature of the problem. In our setting, dynamic regretcan be recognized as a finite-time counterpart of stability in the mean- squaresense. We develop a distributed, online algorithm for tracking the movingtarget. Defining the path-length as the consecutive differences between targetlocations, we express an upper bound on regret in terms of the path-length ofthe target and network errors. We further show the consistency of the resultwith static setting and noiseless observations.
arxiv-16500-71 | Learnt quasi-transitive similarity for retrieval from large collections of faces | http://arxiv.org/abs/1603.00560 | author:Ognjen Arandjelovic category:cs.CV published:2016-03-02 summary:We are interested in identity-based retrieval of face sets from largeunlabelled collections acquired in uncontrolled environments. Given a baselinealgorithm for measuring the similarity of two face sets, the meta-algorithmintroduced in this paper seeks to leverage the structure of the data corpus tomake the best use of the available baseline. In particular, we show how partialtransitivity of inter-personal similarity can be exploited to improve theretrieval of particularly challenging sets which poorly match the query underthe baseline measure. We: (i) describe the use of proxy sets as a means ofcomputing the similarity between two sets, (ii) introduce transitivitymeta-features based on the similarity of salient modes of appearance variationbetween sets, (iii) show how quasi-transitivity can be learnt from suchfeatures without any labelling or manual intervention, and (iv) demonstrate theeffectiveness of the proposed methodology through experiments on thenotoriously challenging YouTube database and two successful baselines from theliterature.
arxiv-16500-72 | PLATO: Policy Learning using Adaptive Trajectory Optimization | http://arxiv.org/abs/1603.00622 | author:Gregory Kahn, Tianhao Zhang, Sergey Levine, Pieter Abbeel category:cs.LG published:2016-03-02 summary:Policy search can in principle acquire complex strategies for control ofrobots, self-driving vehicles, and other autonomous systems. When the policy istrained to process raw sensory inputs, such as images and depth maps, it canacquire a strategy that combines perception and control. However, effectivelyprocessing such complex inputs requires an expressive policy class, such as alarge neural network. These high-dimensional policies are difficult to train,especially when training must be done for safety-critical systems. We proposePLATO, an algorithm that trains complex control policies with supervisedlearning, using model-predictive control (MPC) to generate the supervision.PLATO uses an adaptive training method to modify the behavior of MPC togradually match the learned policy, in order to generate training samples atstates that are likely to be visited by the policy while avoiding highlyundesirable on-policy actions. We prove that this type of adaptive MPC expertproduces supervision that leads to good long-horizon performance of theresulting policy, and empirically demonstrate that MPC can still avoiddangerous on-policy actions in unexpected situations during training. Comparedto prior methods, our empirical results demonstrate that PLATO learns fasterand often converges to a better solution on a set of challenging simulatedexperiments involving autonomous aerial vehicles.
arxiv-16500-73 | Probabilistic Relational Model Benchmark Generation | http://arxiv.org/abs/1603.00709 | author:Mouna Ben Ishak, Rajani Chulyadyo, Philippe Leray category:cs.LG cs.AI published:2016-03-02 summary:The validation of any database mining methodology goes through an evaluationprocess where benchmarks availability is essential. In this paper, we aim torandomly generate relational database benchmarks that allow to checkprobabilistic dependencies among the attributes. We are particularly interestedin Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs)to a relational data mining context and enable effective and robust reasoningover relational data. Even though a panoply of works have focused, separately ,on the generation of random Bayesian networks and relational databases, no workhas been identified for PRMs on that track. This paper provides an algorithmicapproach for generating random PRMs from scratch to fill this gap. The proposedmethod allows to generate PRMs as well as synthetic relational data from arandomly generated relational schema and a random set of probabilisticdependencies. This can be of interest not only for machine learning researchersto evaluate their proposals in a common framework, but also for databasesdesigners to evaluate the effectiveness of the components of a databasemanagement system.
arxiv-16500-74 | Continuous Deep Q-Learning with Model-based Acceleration | http://arxiv.org/abs/1603.00748 | author:Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, Sergey Levine category:cs.LG cs.AI cs.RO cs.SY published:2016-03-02 summary:Model-free reinforcement learning has been successfully applied to a range ofchallenging problems, and has recently been extended to handle large neuralnetwork policies and value functions. However, the sample complexity ofmodel-free algorithms, particularly when using high-dimensional functionapproximators, tends to limit their applicability to physical systems. In thispaper, we explore algorithms and representations to reduce the samplecomplexity of deep reinforcement learning for continuous control tasks. Wepropose two complementary techniques for improving the efficiency of suchalgorithms. First, we derive a continuous variant of the Q-learning algorithm,which we call normalized adantage functions (NAF), as an alternative to themore commonly used policy gradient and actor-critic methods. NAF representationallows us to apply Q-learning with experience replay to continuous tasks, andsubstantially improves performance on a set of simulated robotic control tasks.To further improve the efficiency of our approach, we explore the use oflearned models for accelerating model-free reinforcement learning. We show thatiteratively refitted local linear models are especially effective for this, anddemonstrate substantially faster learning on domains where such models areapplicable.
arxiv-16500-75 | Asymptotic behavior of $\ell_p$-based Laplacian regularization in semi-supervised learning | http://arxiv.org/abs/1603.00564 | author:Ahmed El Alaoui, Xiang Cheng, Aaditya Ramdas, Martin J. Wainwright, Michael I. Jordan category:cs.LG stat.ML published:2016-03-02 summary:Given a weighted graph with $N$ vertices, consider a real-valued regressionproblem in a semi-supervised setting, where one observes $n$ labeled vertices,and the task is to label the remaining ones. We present a theoretical study of$\ell_p$-based Laplacian regularization under a $d$-dimensional geometricrandom graph model. We provide a variational characterization of theperformance of this regularized learner as $N$ grows to infinity while $n$stays constant, the associated optimality conditions lead to a partialdifferential equation that must be satisfied by the associated functionestimate $\hat{f}$. From this formulation we derive several predictions on thelimiting behavior the $d$-dimensional function $\hat{f}$, including (a) a phasetransition in its smoothness at the threshold $p = d + 1$, and (b) a tradeoffbetween smoothness and sensitivity to the underlying unlabeled datadistribution $P$. Thus, over the range $p \leq d$, the function estimate$\hat{f}$ is degenerate and "spiky," whereas for $p\geq d+1$, the functionestimate $\hat{f}$ is smooth. We show that the effect of the underlying densityvanishes monotonically with $p$, such that in the limit $p = \infty$,corresponding to the so-called Absolutely Minimal Lipschitz Extension, theestimate $\hat{f}$ is independent of the distribution $P$. Under the assumptionof semi-supervised smoothness, ignoring $P$ can lead to poor statisticalperformance, in particular, we construct a specific example for $d=1$ todemonstrate that $p=2$ has lower risk than $p=\infty$ due to the former penaltyadapting to $P$ and the latter ignoring it. We also provide simulations thatverify the accuracy of our predictions for finite sample sizes. Together, theseproperties show that $p = d+1$ is an optimal choice, yielding a functionestimate $\hat{f}$ that is both smooth and non-degenerate, while remainingmaximally sensitive to $P$.
arxiv-16500-76 | Character-based Neural Machine Translation | http://arxiv.org/abs/1603.00810 | author:Marta R. Costa-Jussà, José A. R. Fonollosa category:cs.CL cs.LG cs.NE stat.ML published:2016-03-02 summary:Neural Machine Translation (MT) has reached state-of-the-art results.However, one of the main challenges that neural MT still faces is dealing withvery large vocabularies and morphologically rich languages. In this paper, wepropose a neural MT system using character-based embeddings in combination withconvolutional and highway layers to replace the standard lookup-based wordrepresentations. The resulting unlimited-vocabulary and affix-aware source wordembeddings are tested in a state-of-the-art neural MT based on anattention-based bidirectional recurrent neural network. The proposed MT schemeprovides improved results even when the source language is not morphologicallyrich. Improvements up to 3 BLEU points are obtained in the German-English WMTtask.
arxiv-16500-77 | Shallow and Deep Convolutional Networks for Saliency Prediction | http://arxiv.org/abs/1603.00845 | author:Junting Pan, Kevin McGuinness, Elisa Sayrol, Noel O'Connor, Xavier Giro-i-Nieto category:cs.CV cs.LG published:2016-03-02 summary:The prediction of salient areas in images has been traditionally addressedwith hand-crafted features based on neuroscience principles. This paper,however, addresses the problem with a completely data-driven approach bytraining a convolutional neural network (convnet). The learning process isformulated as a minimization of a loss function that measures the Euclideandistance of the predicted saliency map with the provided ground truth. Therecent publication of large datasets of saliency prediction has provided enoughdata to train end-to-end architectures that are both fast and accurate. Twodesigns are proposed: a shallow convnet trained from scratch, and a anotherdeeper solution whose first three layers are adapted from another networktrained for classification. To the authors knowledge, these are the firstend-to-end CNNs trained and tested for the purpose of saliency prediction.
arxiv-16500-78 | Molecular Graph Convolutions: Moving Beyond Fingerprints | http://arxiv.org/abs/1603.00856 | author:Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, Patrick Riley category:stat.ML cs.LG published:2016-03-02 summary:Molecular "fingerprints" encoding structural information are the workhorse ofcheminformatics and machine learning in drug discovery applications. However,fingerprint representations necessarily emphasize particular aspects of themolecular structure while ignoring others, rather than allowing the model tomake data-driven decisions. We describe molecular graph convolutions, a novelmachine learning architecture for learning from undirected graphs, specificallysmall molecules. Graph convolutions use a simple encoding of the moleculargraph (atoms, bonds, distances, etc.), allowing the model to take greateradvantage of information in the graph structure.
arxiv-16500-79 | Equity forecast: Predicting long term stock price movement using machine learning | http://arxiv.org/abs/1603.00751 | author:Nikola Milosevic category:cs.LG q-fin.GN published:2016-03-02 summary:Long term investment is one of the major investment strategies. However,calculating intrinsic value of some company and evaluating shares for long terminvestment is not easy, since analyst have to care about a large number offinancial indicators and evaluate them in a right manner. So far, little helpin predicting the direction of the company value over the longer period of timehas been provided from the machines. In this paper we present a machinelearning aided approach to evaluate the equity's future price over the longtime. Our method is able to correctly predict whether some company's value willbe 10% higher or not over the period of one year in 76.5% of cases.
arxiv-16500-80 | Counter-fitting Word Vectors to Linguistic Constraints | http://arxiv.org/abs/1603.00892 | author:Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gašić, Lina Rojas-Barahona, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, Steve Young category:cs.CL cs.LG published:2016-03-02 summary:In this work, we present a novel counter-fitting method which injectsantonymy and synonymy constraints into vector space representations in order toimprove the vectors' capability for judging semantic similarity. Applying thismethod to publicly available pre-trained word vectors leads to a new state ofthe art performance on the SimLex-999 dataset. We also show how the method canbe used to tailor the word vector space for the downstream task of dialoguestate tracking, resulting in robust improvements across different dialoguedomains.
arxiv-16500-81 | Without-Replacement Sampling for Stochastic Gradient Methods: Convergence Results and Application to Distributed Optimization | http://arxiv.org/abs/1603.00570 | author:Ohad Shamir category:cs.LG math.OC stat.ML published:2016-03-02 summary:Stochastic gradient methods for machine learning and optimization problemsare usually analyzed assuming data points are sampled with replacement. Inpractice, however, sampling without replacement is very common, easier toimplement in many cases, and often performs better. In this paper, we providecompetitive convergence guarantees for without-replacement sampling, undervarious scenarios, for three types of algorithms: Any algorithm with onlineregret guarantees, stochastic gradient descent, and SVRG. A useful applicationof our SVRG analysis is a nearly-optimal algorithm for regularized leastsquares in a distributed setting, in terms of both communication complexity andruntime complexity, when the data is randomly partitioned and the conditionnumber can be as large as the data size (up to logarithmic factors). Our prooftechniques combine ideas from stochastic optimization, adversarial onlinelearning, and transductive learning theory, and can potentially be applied toother stochastic optimization and learning problems.
arxiv-16500-82 | LiDAR Ground Filtering Algorithm for Urban Areas Using Scan Line Based Segmentation | http://arxiv.org/abs/1603.00912 | author:Lei Wang, Yongun Zhang category:cs.CV published:2016-03-02 summary:This paper addresses the task of separating ground points from airborne LiDARpoint cloud data in urban areas. A novel ground filtering method using scanline segmentation is proposed here, which we call SLSGF. It utilizes the scanline information in LiDAR data to segment the LiDAR data. The similaritymeasurements are designed to make it possible to segment complex roofstructures into a single segment as much as possible so the topologicalrelationships between the roof and the ground are simpler, which will benefitthe labeling process. In the labeling process, the initial ground segments aredetected and a coarse to fine labeling scheme is applied. Data from ISPRS 2011are used to test the accuracy of SLSGF; and our analytical and experimentalresults show that this method is computationally-efficient andnoise-insensitive, thereby making a denoising process unnecessary beforefiltering.
arxiv-16500-83 | Synthesized Classifiers for Zero-Shot Learning | http://arxiv.org/abs/1603.00550 | author:Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, Fei Sha category:cs.CV published:2016-03-02 summary:Given semantic descriptions of object classes, zero-shot learning aims toaccurately recognize objects of the unseen classes, from which no examples areavailable at the training stage, by associating them to the seen classes, fromwhich labeled examples are provided. We propose to tackle this problem from theperspective of manifold learning. Our main idea is to align the semantic spacethat is derived from external information to the model space that concernsitself with recognizing visual features. To this end, we introduce a set of"phantom" object classes whose coordinates live in both the semantic space andthe model space. Serving as bases in a dictionary, they can be optimized fromlabeled data such that the synthesized real object classifiers achieve optimaldiscriminative performance. We demonstrate superior accuracy of our approachover the state of the art on four benchmark datasets for zero-shot learning,including the full ImageNet Fall 2011 dataset with more than 20,000 unseenclasses.
arxiv-16500-84 | Learning Word Segmentation Representations to Improve Named Entity Recognition for Chinese Social Media | http://arxiv.org/abs/1603.00786 | author:Nanyun Peng, Mark Dredze category:cs.CL published:2016-03-02 summary:Named entity recognition, and other information extraction tasks, frequentlyuse linguistic features such as part of speech tags or chunkings. For languageswhere word boundaries are not readily identified in text, word segmentation isa key first step to generating features for an NER system. While using wordboundary tags as features are helpful, the signals that aid in identifyingthese boundaries may provide richer information for an NER system. Newstate-of-the-art word segmentation systems use neural models to learnrepresentations for predicting word boundaries. We show that these samerepresentations, jointly trained with an NER system, yield significantimprovements in NER for Chinese social media. In our experiments, jointlytraining NER and word segmentation with an LSTM-CRF model yields nearly 5%absolute improvement over previously published results.
arxiv-16500-85 | US-Cut: Interactive Algorithm for rapid Detection and Segmentation of Liver Tumors in Ultrasound Acquisitions | http://arxiv.org/abs/1603.00546 | author:Jan Egger, Philip Voglreiter, Mark Dokter, Michael Hofmann, Xiaojun Chen, Wolfram G. Zoller, Dieter Schmalstieg, Alexander Hann category:cs.CV cs.CE cs.CG cs.GR published:2016-03-02 summary:Ultrasound (US) is the most commonly used liver imaging modality worldwide.It plays an important role in follow-up of cancer patients with livermetastases. We present an interactive segmentation approach for liver tumors inUS acquisitions. Due to the low image quality and the low contrast between thetumors and the surrounding tissue in US images, the segmentation is verychallenging. Thus, the clinical practice still relies on manual measurement andoutlining of the tumors in the US images. We target this problem by applying aninteractive segmentation algorithm to the US data, allowing the user to getreal-time feedback of the segmentation results. The algorithm has beendeveloped and tested hand-in-hand by physicians and computer scientists to makesure a future practical usage in a clinical setting is feasible. To covertypical acquisitions from the clinical routine, the approach has been evaluatedwith dozens of datasets where the tumors are hyperechoic (brighter), hypoechoic(darker) or isoechoic (similar) in comparison to the surrounding liver tissue.Due to the interactive real-time behavior of the approach, it was possible evenin difficult cases to find satisfying segmentations of the tumors withinseconds and without parameter settings, and the average tumor deviation wasonly 1.4mm compared with manual measurements. However, the long term goal is toease the volumetric acquisition of liver tumors in order to evaluate fortreatment response. Additional aim is the registration of intraoperative USimages via the interactive segmentations to the patient's pre-interventional CTacquisitions.
arxiv-16500-86 | Automatic Differentiation Variational Inference | http://arxiv.org/abs/1603.00788 | author:Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M. Blei category:stat.ML cs.AI cs.LG stat.CO published:2016-03-02 summary:Probabilistic modeling is iterative. A scientist posits a simple model, fitsit to her data, refines it according to her analysis, and repeats. However,fitting complex models to large data is a bottleneck in this process. Derivingalgorithms for new models can be both mathematically and computationallychallenging, which makes it difficult to efficiently cycle through the steps.To this end, we develop automatic differentiation variational inference (ADVI).Using our method, the scientist only provides a probabilistic model and adataset, nothing else. ADVI automatically derives an efficient variationalinference algorithm, freeing the scientist to refine and explore many models.ADVI supports a broad class of models-no conjugacy assumptions are required. Westudy ADVI across ten different models and apply it to a dataset with millionsof observations. ADVI is integrated into Stan, a probabilistic programmingsystem; it is available for immediate use.
arxiv-16500-87 | A Nonlinear Weighted Total Variation Image Reconstruction Algorithm for Electrical Capacitance Tomography | http://arxiv.org/abs/1603.00816 | author:Kezhi Li, Daniel Holland category:cs.CV published:2016-03-02 summary:Based on the techniques of iterative soft thresholding on total variationpenalty and adaptive reweighted compressive sensing, a new iterativereconstruction algorithm for electrical capacitance tomography (ECT) isproposed. This algorithm encourages sharp changes in the ECT image andovercomes the disadvantage of the $l_1$ minimization by equipping the totalvariation an adaptive weighted depending on the reconstructed image. Moreover,the nonlinear effect is also partially reduced due to the adoption of theupdated accurate sensitivity matrix. Simulation results show that it recoversECT images more precisely and therefore suitable for the imaging of multiphasesystems in industrial or medical applications.
arxiv-16500-88 | Flies as Ship Captains? Digital Evolution Unravels Selective Pressures to Avoid Collision in Drosophila | http://arxiv.org/abs/1603.00802 | author:Ali Tehrani-Saleh, Christoph Adami category:q-bio.PE cs.CV nlin.AO q-bio.NC published:2016-03-02 summary:Flies that walk in a covered planar arena on straight paths avoid collidingwith each other, but which of the two flies stops is not random.High-throughput video observations, coupled with dedicated experiments withcontrolled robot flies have revealed that flies utilize the type of optic flowon their retina as a determinant of who should stop, a strategy also used byship captains to determine which of two ships on a collision course shouldthrow engines in reverse. We use digital evolution to test whether thisstrategy evolves when collision avoidance is the sole penalty. We find that thestrategy does indeed evolve in a narrow range of cost/benefit ratios, forexperiments in which the "regressive motion" cue is error free. We speculatethat these stringent conditions may not be sufficient to evolve the strategy inreal flies, pointing perhaps to auxiliary costs and benefits not modeled in ourstudy
arxiv-16500-89 | MOT16: A Benchmark for Multi-Object Tracking | http://arxiv.org/abs/1603.00831 | author:Anton Milan, Laura Leal-Taixe, Ian Reid, Stefan Roth, Konrad Schindler category:cs.CV published:2016-03-02 summary:Standardized benchmarks are crucial for the majority of computer visionapplications. Although leaderboards and ranking tables should not beover-claimed, benchmarks often provide the most objective measure ofperformance and are therefore important guides for reseach. Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, waslaunched with the goal of collecting existing and new data and creating aframework for the standardized evaluation of multiple object tracking methods.The first release of the benchmark focuses on multiple people tracking, sincepedestrians are by far the most studied object in the tracking community. Thispaper accompanies a new release of the MOTChallenge benchmark. Unlike theinitial release, all videos of MOT16 have been carefully annotated following aconsistent protocol. Moreover, it not only offers a significant increase in thenumber of labeled boxes, but also provides multiple object classes besidepedestrians and the level of visibility for every single object of interest.
arxiv-16500-90 | LOFS: Library of Online Streaming Feature Selection | http://arxiv.org/abs/1603.00531 | author:Kui Yu, Wei Ding, Xindong Wu category:cs.LG stat.ML published:2016-03-02 summary:As an emerging research direction, online streaming feature selection dealswith sequentially added dimensions in a feature space while the number of datainstances is fixed. Online streaming feature selection provides a new,complementary algorithmic methodology to enrich online feature selection,especially targets to high dimensionality in big data analytics. This paperintroduces the first comprehensive open-source library for use in MATLAB thatimplements the state-of-the-art algorithms of online streaming featureselection. The library is designed to facilitate the development of newalgorithms in this exciting research direction and make comparisons between thenew methods and existing ones available.
arxiv-16500-91 | GOGMA: Globally-Optimal Gaussian Mixture Alignment | http://arxiv.org/abs/1603.00150 | author:Dylan Campbell, Lars Petersson category:cs.CV cs.RO published:2016-03-01 summary:Gaussian mixture alignment is a family of approaches that are frequently usedfor robustly solving the point-set registration problem. However, since theyuse local optimisation, they are susceptible to local minima and can onlyguarantee local optimality. Consequently, their accuracy is strongly dependenton the quality of the initialisation. This paper presents the firstglobally-optimal solution to the 3D rigid Gaussian mixture alignment problemunder the L2 distance between mixtures. The algorithm, named GOGMA, employs abranch-and-bound approach to search the space of 3D rigid motions SE(3),guaranteeing global optimality regardless of the initialisation. The geometryof SE(3) was used to find novel upper and lower bounds for the objectivefunction and local optimisation was integrated into the scheme to accelerateconvergence without voiding the optimality guarantee. The evaluationempirically supported the optimality proof and showed that the method performedmuch more robustly on two challenging datasets than an existingglobally-optimal registration solution.
arxiv-16500-92 | Convolutional Rectifier Networks as Generalized Tensor Decompositions | http://arxiv.org/abs/1603.00162 | author:Nadav Cohen, Amnon Shashua category:cs.NE cs.LG published:2016-03-01 summary:Convolutional rectifier networks, i.e. convolutional neural networks withrectified linear activation and max or average pooling, are the cornerstone ofmodern deep learning. However, despite their wide use and success, ourtheoretical understanding of the expressive properties that drive thesenetworks is partial at best. On other hand, we have a much firmer grasp ofthese issues in the world of arithmetic circuits. Specifically, it is knownthat convolutional arithmetic circuits posses the property of "complete depthefficiency", meaning that besides a negligible set, all functions that can beimplemented by a deep network of polynomial size, require exponential size inorder to be implemented (or even approximated) by a shallow network. In thispaper we describe a construction based on generalized tensor decompositions,that transforms convolutional arithmetic circuits into convolutional rectifiernetworks. We then use mathematical tools available from the world of arithmeticcircuits to prove new results. First, we show that convolutional rectifiernetworks are universal with max pooling but not with average pooling. Second,and more importantly, we show that depth efficiency is weaker withconvolutional rectifier networks than it is with convolutional arithmeticcircuits. This leads us to believe that developing effective methods fortraining convolutional arithmetic circuits, thereby fulfilling their expressivepotential, may give rise to a deep learning architecture that is provablysuperior to convolutional rectifier networks but has so far been overlooked bypractitioners.
arxiv-16500-93 | A Nonlinear Adaptive Filter Based on the Model of Simple Multilinear Functionals | http://arxiv.org/abs/1603.00427 | author:Felipe C. Pinheiro, Cássio G. Lopes category:cs.SY cs.LG published:2016-03-01 summary:Nonlinear adaptive filtering allows for modeling of some additional aspectsof a general system and usually relies on highly complex algorithms, such asthose based on the Volterra series. Through the use of the Kronecker productand some basic facts of tensor algebra, we propose a simple model ofnonlinearity, one that can be interpreted as a product of the outputs of K FIRlinear filters, and compute its cost function together with its gradient, whichallows for some analysis of the optimization problem. We use these results itin a stochastic gradient framework, from which we derive an LMS-like algorithmand investigate the problems of multi-modality in the mean-square error surfaceand the choice of adequate initial conditions. Its computational complexity iscalculated. The new algorithm is tested in a system identification setup and iscompared with other polynomial algorithms from the literature, presentingfavorable convergence and/or computational complexity.
arxiv-16500-94 | Noisy Activation Functions | http://arxiv.org/abs/1603.00391 | author:Caglar Gulcehre, Marcin Moczulski, Misha Denil, Yoshua Bengio category:cs.LG cs.NE stat.ML published:2016-03-01 summary:Common nonlinear activation functions used in neural networks can causetraining difficulties due to the saturation behavior of the activationfunction, which may hide dependencies that are not visible to vanilla-SGD(using first order gradients only). Gating mechanisms that use softlysaturating activation functions to emulate the discrete switching of digitallogic circuits are good examples of this. We propose to exploit the injectionof appropriate noise so that the gradients may flow easily, even if thenoiseless application of the activation function would yield zero gradient.Large noise will dominate the noise-free gradient and allow stochastic gradientdescent toexplore more. By adding noise only to the problematic parts of theactivation function, we allow the optimization procedure to explore theboundary between the degenerate (saturating) and the well-behaved parts of theactivation function. We also establish connections to simulated annealing, whenthe amount of noise is annealed down, making it easier to optimize hardobjective functions. We find experimentally that replacing such saturatingactivation functions by noisy variants helps training in many contexts,yielding state-of-the-art or competitive results on different datasets andtask, especially when training seems to be the most difficult, e.g., whencurriculum learning is necessary to obtain good results.
arxiv-16500-95 | Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs | http://arxiv.org/abs/1603.00423 | author:Phong Le, Willem Zuidema category:cs.AI cs.CL cs.NE published:2016-03-01 summary:Recursive neural networks (RNN) and their recently proposed extensionrecursive long short term memory networks (RLSTM) are models that computerepresentations for sentences, by recursively combining word embeddingsaccording to an externally provided parse tree. Both models thus, unlikerecurrent networks, explicitly make use of the hierarchical structure of asentence. In this paper, we demonstrate that RNNs nevertheless suffer from thevanishing gradient and long distance dependency problem, and that RLSTMsgreatly improve over RNN's on these problems. We present an artificial learningtask that allows us to quantify the severity of these problems for both models.We further show that a ratio of gradients (at the root node and a focal leafnode) is highly indicative of the success of backpropagation at optimizing therelevant weights low in the tree. This paper thus provides an explanation forexisting, superior results of RLSTMs on tasks such as sentiment analysis, andsuggests that the benefits of including hierarchical structure and of includingLSTM-style gating are complementary.
arxiv-16500-96 | Pattern recognition on the quantum Bloch sphere | http://arxiv.org/abs/1603.00173 | author:Giuseppe Sergioli, Enrica Santucci, Luca Didaci, Jaroslaw A. Miszczak, Roberto Giuntini category:quant-ph cs.CV published:2016-03-01 summary:We introduce a framework suitable for describing pattern recognition taskusing the mathematical language of density matrices. In particular, we providea one-to-one correspondence between patterns and density operators, representedby mixed states when the uncertainty comes into play. The classificationprocess in the quantum framework is performed by the introduction of anormalized trace distance between density operators in place of the Euclideandistance between patterns. We provide a comparison of the introduced method inthe case of 2D data classification.
arxiv-16500-97 | Storm Detection by Visual Learning Using Satellite Images | http://arxiv.org/abs/1603.00146 | author:Yu Zhang, Stephen Wistar, Jia Li, Michael Steinberg, James Z. Wang category:cs.CV published:2016-03-01 summary:Computers are widely utilized in today's weather forecasting as a powerfultool to leverage an enormous amount of data. Yet, despite the availability ofsuch data, current techniques often fall short of producing reliable detailedstorm forecasts. Each year severe thunderstorms cause significant damage andloss of life, some of which could be avoided if better forecasts wereavailable. We propose a computer algorithm that analyzes satellite images fromhistorical archives to locate visual signatures of severe thunderstorms forshort-term predictions. While computers are involved in weather forecasts tosolve numerical models based on sensory data, they are less competent inforecasting based on visual patterns from satellite images. In our system, weextract and summarize important visual storm evidence from satellite imagesequences in the way that meteorologists interpret the images. In particular,the algorithm extracts and fits local cloud motion from image sequences tomodel the storm-related cloud patches. Image data from the year 2008 have beenadopted to train the model, and historical thunderstorm reports in continentalUS from 2000 through 2013 have been used as the ground-truth and priors in themodeling process. Experiments demonstrate the usefulness and potential of thealgorithm for producing more accurate thunderstorm forecasts.
arxiv-16500-98 | Solving Combinatorial Games using Products, Projections and Lexicographically Optimal Bases | http://arxiv.org/abs/1603.00522 | author:Swati Gupta, Michel Goemans, Patrick Jaillet category:cs.LG published:2016-03-01 summary:In order to find Nash-equilibria for two-player zero-sum games where eachplayer plays combinatorial objects like spanning trees, matchings etc, weconsider two online learning algorithms: the online mirror descent (OMD)algorithm and the multiplicative weights update (MWU) algorithm. The OMDalgorithm requires the computation of a certain Bregman projection, that hasclosed form solutions for simple convex sets like the Euclidean ball or thesimplex. However, for general polyhedra one often needs to exploit the generalmachinery of convex optimization. We give a novel primal-style algorithm forcomputing Bregman projections on the base polytopes of polymatroids. Next, inthe case of the MWU algorithm, although it scales logarithmically in the numberof pure strategies or experts $N$ in terms of regret, the algorithm takes timepolynomial in $N$; this especially becomes a problem when learningcombinatorial objects. We give a general recipe to simulate the multiplicativeweights update algorithm in time polynomial in their natural dimension. This isuseful whenever there exists a polynomial time generalized counting oracle(even if approximate) over these objects. Finally, using the combinatorialstructure of symmetric Nash-equilibria (SNE) when both players play bases ofmatroids, we show that these can be found with a single projection or convexminimization (without using online learning).
arxiv-16500-99 | Segmental Recurrent Neural Networks for End-to-end Speech Recognition | http://arxiv.org/abs/1603.00223 | author:Liang Lu, Lingpeng Kong, Chris Dyer, Noah A. Smith, Steve Renals category:cs.CL cs.LG cs.NE published:2016-03-01 summary:We study the segmental recurrent neural network for end-to-end acousticmodelling. This model connects the segmental conditional random field (CRF)with a recurrent neural network (RNN) used for feature extraction. Compared tomost previous CRF-based acoustic models, it does not rely on an external systemto provide features or segmentation boundaries. Instead, this modelmarginalises out all the possible segmentations, and features are extractedfrom the RNN trained together with the segmental CRF. In essence, this model isself-contained and can be trained end-to-end. In this paper, we discusspractical training and decoding issues as well as the method to speed up thetraining in the context of speech recognition. We performed experiments on theTIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-passdecoding --- the best reported result using CRFs, despite the fact that we onlyused a zeroth-order CRF and without using any language model.
arxiv-16500-100 | Weakly Supervised Localization using Deep Feature Maps | http://arxiv.org/abs/1603.00489 | author:Archith J. Bency, Heesung Kwon, Hyungtae Lee, S. Karthikeyan, B. S. Manjunath category:cs.CV published:2016-03-01 summary:Object localization is an important computer vision problem with a variety ofapplications. The lack of large scale object-level annotations and the relativeabundance of image-level labels makes a compelling case for weak supervision inthe object localization task. Deep Convolutional Neural Networks are a class ofstate-of-the-art methods for the related problem of object recognition. In thispaper, we describe a novel object localization algorithm which usesclassification networks trained on only image labels. This weakly supervisedmethod leverages local spatial and semantic patterns captured in theconvolutional layers of classification networks. We propose an efficient beamsearch based approach to detect and localize multiple objects in images. Theproposed method significantly outperforms the state-of-the-art in standardobject localization data-sets with a 8 point increase in mAP scores.
arxiv-16500-101 | On Tie Strength Augmented Social Correlation for Inferring Preference of Mobile Telco Users | http://arxiv.org/abs/1603.00145 | author:Shifeng Liu, Zheng Hu, Sujit Dey, Xin Ke category:cs.SI cs.IR cs.LG published:2016-03-01 summary:For mobile telecom operators, it is critical to build preference profiles oftheir customers and connected users, which can help operators make bettermarketing strategies, and provide more personalized services. With thedeployment of deep packet inspection (DPI) in telecom networks, it is possiblefor the telco operators to obtain user online preference. However, DPI has itslimitations and user preference derived only from DPI faces sparsity and coldstart problems. To better infer the user preference, social correlation intelco users network derived from Call Detailed Records (CDRs) with regard toonline preference is investigated. Though widely verified in several onlinesocial networks, social correlation between online preference of users inmobile telco networks, where the CDRs derived relationship are of less socialproperties and user mobile internet surfing activities are not visible toneighbourhood, has not been explored at a large scale. Based on a real worldtelecom dataset including CDRs and preference of more than $550K$ users forseveral months, we verified that correlation does exist between onlinepreference in such \textit{ambiguous} social network. Furthermore, we foundthat the stronger ties that users build, the more similarity between theirpreference may have. After defining the preference inferring task as a Top-$K$recommendation problem, we incorporated Matrix Factorization CollaborativeFiltering model with social correlation and tie strength based on call patternsto generate Top-$K$ preferred categories for users. The proposed Tie StrengthAugmented Social Recommendation (TSASoRec) model takes data sparsity and coldstart user problems into account, considering both the recorded and missingrecorded category entries. The experiment on real dataset shows the proposedmodel can better infer user preference, especially for cold start users.
arxiv-16500-102 | Keypoint Density-based Region Proposal for Fine-Grained Object Detection and Classification using Regions with Convolutional Neural Network Features | http://arxiv.org/abs/1603.00502 | author:JT Turner, Kalyan Gupta, Brendan Morris, David W. Aha category:cs.CV published:2016-03-01 summary:Although recent advances in regional Convolutional Neural Networks (CNNs)enable them to outperform conventional techniques on standard object detectionand classification tasks, their response time is still slow for real-timeperformance. To address this issue, we propose a method for region proposal asan alternative to selective search, which is used in current state-of-the artobject detection algorithms. We evaluate our Keypoint Density-based RegionProposal (KDRP) approach and show that it speeds up detection andclassification on fine-grained tasks by 100% versus the existing selectivesearch region proposal technique without compromising classification accuracy.KDRP makes the application of CNNs to real-time detection and classificationfeasible.
arxiv-16500-103 | Scalable Metric Learning via Weighted Approximate Rank Component Analysis | http://arxiv.org/abs/1603.00370 | author:Cijo Jose, Francois Fleuret category:cs.CV published:2016-03-01 summary:We are interested in the large-scale learning of Mahalanobis distances, witha particular focus on person re-identification. We propose a metric learning formulation called Weighted Approximate RankComponent Analysis (WARCA). WARCA optimizes the precision at top ranks bycombining the WARP loss with a regularizer that favors orthonormal linearmappings, and avoids rank-deficient embeddings. Using this new regularizerallows us to adapt the large-scale WSABIE procedure and to leverage the Adamstochastic optimization algorithm, which results in an algorithm that scalesgracefully to very large data-sets. Also, we derive a kernelized version whichallows to take advantage of state-of-the-art features for re-identificationwhen data-set size permits kernel computation. Benchmarks on recent and standard re-identification data-sets show that ourmethod beats existing state-of-the-art techniques both in term of accuracy andspeed. We also provide experimental analysis to shade lights on the propertiesof the regularizer we use, and how it improves performance.
arxiv-16500-104 | Technical Report: Band selection for nonlinear unmixing of hyperspectral images as a maximal click problem | http://arxiv.org/abs/1603.00437 | author:Tales Imbiriba, José Carlos Moreira Bermudez, Cédric Richard category:cs.CV published:2016-03-01 summary:Kernel-based nonlinear mixing models have been applied to unmix spectralinformation of hyperspectral images when the type of mixing occurring in thescene is too complex or unknown. Such methods, however, usually require theinversion of matrices of sizes equal to the number of spectral bands. Reducingthe computational load of these methods remains a challenge in large scaleapplications. This paper proposes a centralized method for band selection (BS)in the reproducing kernel Hilbert space (RKHS). It is based upon the coherencecriterion, which sets the largest value allowed for correlations between thebasis kernel functions characterizing the unmixing model. We show that theproposed BS approach is equivalent to solving a maximum clique problem (MCP),that is, searching for the biggest complete subgraph in a graph. Furthermore,we devise a strategy for selecting the coherence threshold and the Gaussiankernel bandwidth using coherence bounds for linearly independent bases.Simulation results illustrate the efficiency of the proposed method.
arxiv-16500-105 | Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization | http://arxiv.org/abs/1603.00448 | author:Chelsea Finn, Sergey Levine, Pieter Abbeel category:cs.LG cs.AI cs.RO published:2016-03-01 summary:Reinforcement learning can acquire complex behaviors from high-levelspecifications. However, defining a cost function that can be optimizedeffectively and encodes the correct task is challenging in practice. We explorehow inverse optimal control (IOC) can be used to learn behaviors fromdemonstrations, with applications to torque control of high-dimensional roboticsystems. Our method addresses two key challenges in inverse optimal control:first, the need for informative features and effective regularization to imposestructure on the cost, and second, the difficulty of learning the cost functionunder unknown dynamics for high-dimensional continuous systems. To address theformer challenge, we present an algorithm capable of learning arbitrarynonlinear cost functions, such as neural networks, without meticulous featureengineering. To address the latter challenge, we formulate an efficientsample-based approximation for MaxEnt IOC. We evaluate our method on a seriesof simulated tasks and real-world robotic manipulation problems, demonstratingsubstantial improvement over prior methods both in terms of task complexity andsample efficiency.
arxiv-16500-106 | Robust Multi-body Feature Tracker: A Segmentation-free Approach | http://arxiv.org/abs/1603.00110 | author:Pan Ji, Hongdong Li, Mathieu Salzmann, Yiran Zhong category:cs.CV published:2016-03-01 summary:Feature tracking is a fundamental problem in computer vision, withapplications in many computer vision tasks, such as visual SLAM and actionrecognition. This paper introduces a novel multi-body feature tracker thatexploits a multi-body rigidity assumption to improve tracking robustness undera general perspective camera model. A conventional approach to addressing thisproblem would consist of alternating between solving two subtasks: motionsegmentation and feature tracking under rigidity constraints for each segment.This approach, however, requires knowing the number of motions, as well asassigning points to motion groups, which is typically sensitive to the motionestimates. By contrast, here, we introduce a segmentation-free solution tomulti-body feature tracking that bypasses the motion assignment step andreduces to solving a series of subproblems with closed-form solutions. Ourexperiments demonstrate the benefits of our approach in terms of trackingaccuracy and robustness to noise.
arxiv-16500-107 | A Universal Update-pacing Framework For Visual Tracking | http://arxiv.org/abs/1603.00132 | author:Zexi Hu, Yuefang Gao, Dong Wang, Xuhong Tian category:cs.CV published:2016-03-01 summary:This paper proposes a novel framework to alleviate the model drift problem invisual tracking, which is based on paced updates and trajectory selection.Given a base tracker, an ensemble of trackers is generated, in which eachtracker's update behavior will be paced and then traces the target objectforward and backward to generate a pair of trajectories in an interval. Then,we implicitly perform self-examination based on trajectory pair of each trackerand select the most robust tracker. The proposed framework can effectivelyleverage temporal context of sequential frames and avoid to learn corruptedinformation. Extensive experiments on the standard benchmark suggest that theproposed framework achieves superior performance against state-of-the-arttrackers.
arxiv-16500-108 | Fourier ptychographic reconstruction using Poisson maximum likelihood and truncated Wirtinger gradient | http://arxiv.org/abs/1603.04746 | author:Liheng Bian, Jinli Suo, Jaebum Chung, Xiaoze Ou, Changhuei Yang, Feng Chen, Qionghai Dai category:cs.CV physics.optics published:2016-03-01 summary:Fourier ptychographic microscopy (FPM) is a novel computational coherentimaging technique for high space-bandwidth product imaging. Mathematically,Fourier ptychographic (FP) reconstruction can be implemented as a phaseretrieval optimization process, in which we only obtain low resolutionintensity images corresponding to the sub-bands of the sample's high resolution(HR) spatial spectrum, and aim to retrieve the complex HR spectrum. In realsetups, the measurements always suffer from various degenerations such asGaussian noise, Poisson noise, speckle noise and pupil location error, whichwould largely degrade the reconstruction. To efficiently address thesedegenerations, we propose a novel FP reconstruction method under a gradientdescent optimization framework in this paper. The technique utilizes Poissonmaximum likelihood for better signal modeling, and truncated Wirtinger gradientfor error removal. Results on both simulated data and real data captured usingour laser FPM setup show that the proposed method outperforms otherstate-of-the-art algorithms. Also, we have released our source code fornon-commercial use.
arxiv-16500-109 | Crowdsourcing On-street Parking Space Detection | http://arxiv.org/abs/1603.00441 | author:Ruizhi Liao, Cristian Roman, Peter Ball, Shumao Ou, Liping Chen category:cs.HC cs.LG published:2016-03-01 summary:As the number of vehicles continues to grow, parking spaces are at a premiumin city streets. Additionally, due to the lack of knowledge about streetparking spaces, heuristic circling the blocks not only costs drivers' time andfuel, but also increases city congestion. In the wake of recent trend to buildconvenient, green and energy-efficient smart cities, we rethink commontechniques adopted by high-profile smart parking systems, and present auser-engaged (crowdsourcing) and sonar-based prototype to identify urbanon-street parking spaces. The prototype includes an ultrasonic sensor, a GPSreceiver and associated Arduino micro-controllers. It is mounted on thepassenger side of a car to measure the distance from the vehicle to the nearestroadside obstacle. Multiple road tests are conducted around Wheatley, Oxford togather results and emulate the crowdsourcing approach. By extracting parkedvehicles' features from the collected trace, a supervised learning algorithm isdeveloped to estimate roadside parking occupancy and spot illegal parkingvehicles. A quantity estimation model is derived to calculate the requirednumber of sensing units to cover urban streets. The estimation isquantitatively compared to a fixed sensing solution. The results show that thecrowdsourcing way would need substantially fewer sensors compared to the fixedsensing system.
arxiv-16500-110 | Multi-Information Source Optimization with General Model Discrepancies | http://arxiv.org/abs/1603.00389 | author:Matthias Poloczek, Jialei Wang, Peter I. Frazier category:stat.ML published:2016-03-01 summary:In the multi-information source optimization problem our goal is to optimizea complex design. However, we only have indirect access to the objective valueof any design via information sources that are subject to model discrepancy,i.e. whose internal model inherently deviates from reality. We present a novelalgorithm that is based on a rigorous mathematical treatment of theuncertainties arising from the model discrepancies. Its optimization decisionsrely on a stringent value of information analysis that trades off the predictedbenefit and its cost. We conduct an experimental evaluation that demonstratesthat the method consistently outperforms other state-of-the-art techniques: itfinds designs of considerably higher objective value and additionally inflictsless cost in the exploration process.
arxiv-16500-111 | Event Search and Analytics: Detecting Events in Semantically Annotated Corpora for Search and Analytics | http://arxiv.org/abs/1603.00260 | author:Dhruv Gupta category:cs.IR cs.CL published:2016-03-01 summary:In this article, I present the questions that I seek to answer in my PhDresearch. I posit to analyze natural language text with the help of semanticannotations and mine important events for navigating large text corpora.Semantic annotations such as named entities, geographic locations, and temporalexpressions can help us mine events from the given corpora. These events thusprovide us with useful means to discover the locked knowledge in them. I posethree problems that can help unlock this knowledge vault in semanticallyannotated text corpora: i. identifying important events; ii. semantic search;and iii. event analytics.
arxiv-16500-112 | Designing Domain Specific Word Embeddings: Applications to Disease Surveillance | http://arxiv.org/abs/1603.00106 | author:Saurav Ghosh, Prithwish Chakraborty, Emily Cohn, John S. Brownstein, Naren Ramakrishnan category:cs.LG cs.CL stat.ML published:2016-03-01 summary:Traditional disease surveillance can be augmented with a wide variety ofrealtime sources such as news and social media. However, these sources are ingeneral unstructured and construction of surveillance tools such as taxonomicalcorrelations and trace mapping involves considerable human supervision. In thispaper, we motivate a disease vocabulary driven word2vec model (Dis2Vec) whichwe use to model diseases and constituent attributes as word embeddings from theHealthMap news corpus. We use these word embeddings to create diseasetaxonomies and evaluate our model accuracy against human annotated taxonomies.We compare our accuracies against several state-of-the art word2vec methods.Our results demonstrate that Dis2Vec outperforms traditional distributed vectorrepresentations in its ability to faithfully capture disease attributes andaccurately forecast outbreaks.
arxiv-16500-113 | Gland Segmentation in Colon Histology Images: The GlaS Challenge Contest | http://arxiv.org/abs/1603.00275 | author:Korsuk Sirinukunwattana, Josien P. W. Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang, Bogdan J. Matuszewski, Elia Bruni, Urko Sanchez, Anton Böhm, Olaf Ronneberger, Bassem Ben Cheikh, Daniel Racoceanu, Philipp Kainz, Michael Pfeiffer, Martin Urschler, David R. J. Snead, Nasir M. Rajpoot category:cs.CV published:2016-03-01 summary:Colorectal adenocarcinoma originating in intestinal glandular structures isthe most common form of colon cancer. In clinical practice, the morphology ofintestinal glands, including architectural appearance and glandular formation,is used by pathologists to inform prognosis and plan the treatment ofindividual patients. However, achieving good inter-observer as well asintra-observer reproducibility of cancer grading is still a major challenge inmodern pathology. An automated approach which quantifies the morphology ofglands is a solution to the problem. This paper provides an overview to theGland Segmentation in Colon Histology Images Challenge Contest (GlaS) held atMICCAI'2015. Details of the challenge, including organization, dataset andevaluation criteria, are presented, along with the method descriptions andevaluation results from the top performing methods.
arxiv-16500-114 | Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach | http://arxiv.org/abs/1603.00438 | author:Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid category:cs.CV published:2016-03-01 summary:Convolutional neural networks (CNNs) have recently received a lot ofattention due to their ability to model local stationary structures in naturalimages in a multi-scale fashion, when learning all model parameters withsupervision. While excellent performance was achieved for image classificationwhen large amounts of labeled visual data are available, their success forun-supervised tasks such as image retrieval has been moderate so far. Our paperfocuses on this latter setting and explores several methods for learning patchdescriptors without supervision with application to matching and instance-levelretrieval. To that effect, we propose a new family of convolutional descriptorsfor patch representation , based on the recently introduced convolutionalkernel networks. We show that our descriptor, named Patch-CKN, performs betterthan SIFT as well as other convolutional networks learned by artificiallyintroducing supervision and is significantly faster to train. To demonstrateits effectiveness, we perform an extensive evaluation on standard benchmarksfor patch and image retrieval where we obtain state-of-the-art results. We alsointroduce a new dataset called RomePatches, which allows to simultaneouslystudy descriptor performance for patch and image retrieval.
arxiv-16500-115 | Kernel-based Tests for Joint Independence | http://arxiv.org/abs/1603.00285 | author:Niklas Pfister, Peter Bühlmann, Bernhard Schölkopf, Jonas Peters category:math.ST stat.ML stat.TH published:2016-03-01 summary:We investigate the problem of testing whether $d$ random variables, which mayor may not be continuous, are jointly (or mutually) independent. Our methodbuilds on ideas of the two variable Hilbert-Schmidt independence criterion(HSIC) but allows for an arbitrary number of variables. We embed the$d$-dimensional joint distribution and the product of the marginals into areproducing kernel Hilbert space and define the $d$-variable Hilbert-Schmidtindependence criterion (dHSIC) as the squared distance between the embeddings.In the population case, the value of dHSIC is zero if and only if the $d$variables are jointly independent, as long as the kernel is characteristic.Based on an empirical estimate of dHSIC, we define three differentnon-parametric hypothesis tests: a permutation test, a bootstrap test and atest based on a Gamma approximation. We prove that the permutation testachieves the significance level and that the bootstrap test achieves pointwiseasymptotic significance level as well as pointwise asymptotic consistency(i.e., it is able to detect any type of fixed dependence in the large samplelimit). The Gamma approximation does not come with these guarantees; however,it is computationally very fast and for small $d$, it performs well inpractice. Finally, we apply the test to a problem in causal discovery.
arxiv-16500-116 | Learning Multilayer Channel Features for Pedestrian Detection | http://arxiv.org/abs/1603.00124 | author:Jiale Cao, Yanwei Pang, Xuelong Li category:cs.CV published:2016-03-01 summary:Pedestrian detection based on the combination of Convolutional Neural Network(i.e., CNN) and traditional handcrafted features (i.e., HOG+LUV) has achievedgreat success. Generally, HOG+LUV are used to generate the candidate proposalsand then CNN classifies these proposals. Despite its success, there is stillroom for improvement. For example, CNN classifies these proposals by thefull-connected layer features while proposal scores and the features in theinner-layers of CNN are ignored. In this paper, we propose a unifying frameworkcalled Multilayer Channel Features (MCF) to overcome the drawback. It firstlyintegrates HOG+LUV with each layer of CNN into a multi-layer image channels.Based on the multi-layer image channels, a multi-stage cascade AdaBoost is thenlearned. The weak classifiers in each stage of the multi-stage cascade islearned from the image channels of corresponding layer. With more abundantfeatures, MCF achieves the state-of-the-art on Caltech pedestrian dataset(i.e., 10.40% miss rate). Using new and accurate annotations, MCF achieves7.98% miss rate. As many non-pedestrian detection windows can be quicklyrejected by the first few stages, it accelerates detection speed by 1.43 times.By eliminating the highly overlapped detection windows with lower scores afterthe first stage, it's 4.07 times faster with negligible performance loss.
arxiv-16500-117 | Easy-First Dependency Parsing with Hierarchical Tree LSTMs | http://arxiv.org/abs/1603.00375 | author:Eliyahu Kiperwasser, Yoav Goldberg category:cs.CL published:2016-03-01 summary:We suggest a compositional vector representation of parse trees that relieson a recursive combination of recurrent-neural network encoders. To demonstrateits effectiveness, we use the representation as the backbone of a greedy,bottom-up dependency parser, achieving state-of-the-art accuracies for Englishand Chinese, without relying on external word embeddings. The parser'simplementation is available for download at the first author's webpage.
arxiv-16500-118 | Cascaded Subpatch Networks for Effective CNNs | http://arxiv.org/abs/1603.00128 | author:Xiaoheng Jiang, Yanwei Pang, Manli Sun, Xuelong Li category:cs.CV published:2016-03-01 summary:Conventional Convolutional Neural Networks (CNNs) use either a linear ornon-linear filter to extract features from an image patch (region) of spatialsize $ H\times W $ (Typically, $ H $ is small and is equal to $ W$, e.g., $ H $is 5 or 7). Generally, the size of the filter is equal to the size $ H\times W$ of the input patch. We argue that the representation ability of equal-sizestrategy is not strong enough. To overcome the drawback, we propose to usesubpatch filter whose spatial size $ h\times w $ is smaller than $ H\times W $.The proposed subpatch filter consists of two subsequent filters. The first oneis a linear filter of spatial size $ h\times w $ and is aimed at extractingfeatures from spatial domain. The second one is of spatial size $ 1\times 1 $and is used for strengthening the connection between different input featurechannels and for reducing the number of parameters. The subpatch filterconvolves with the input patch and the resulting network is called a subpatchnetwork. Taking the output of one subpatch network as input, we further repeatconstructing subpatch networks until the output contains only one neuron inspatial domain. These subpatch networks form a new network called CascadedSubpatch Network (CSNet). The feature layer generated by CSNet is called csconvlayer. For the whole input image, we construct a deep neural network bystacking a sequence of csconv layers. Experimental results on four benchmarkdatasets demonstrate the effectiveness and compactness of the proposed CSNet.For example, our CSNet reaches a test error of $ 5.68\% $ on the CIFAR10dataset without model averaging. To the best of our knowledge, this is the bestresult ever obtained on the CIFAR10 dataset.
arxiv-16500-119 | Dual Smoothing and Level Set Techniques for Variational Matrix Decomposition | http://arxiv.org/abs/1603.00284 | author:Aleksandr Y. Aravkin, Stephen Becker category:stat.ML cs.CV math.OC published:2016-03-01 summary:We focus on the robust principal component analysis (RPCA) problem, andreview a range of old and new convex formulations for the problem and itsvariants. We then review dual smoothing and level set techniques in convexoptimization, present several novel theoretical results, and apply thetechniques on the RPCA problem. In the final sections, we show a range ofnumerical experiments for simulated and real-world problems.
arxiv-16500-120 | Modular Tracking Framework: A Unified Approach to Registration based Tracking | http://arxiv.org/abs/1602.09130 | author:Abhineet Singh, Martin Jagersand category:cs.CV cs.RO published:2016-02-29 summary:This paper presents a modular, extensible and highly efficient open sourceframework for registration based tracking. It is implemented entirely in C++and is designed from the ground up to easily integrate with systems thatsupport any of several major vision and robotics libraries including OpenCV,ROS and Eigen. To establish the theoretical basis for the design of thissystem, we introduce a new way to study registration based trackers bydecomposing them into three constituent sub modules while also extending theunifying formulation described in \cite{Baker04lucasKanade_paper} to accountfor several important advances in the field since its publication. In addition to being a practical solution for fast and high precisiontracking, this system can also serve as a useful research tool by allowingexisting and new methods for any of the aforementioned sub modules to bestudied better. When a new method for one of these sub modules is introduced inliterature, this breakdown can help to experimentally find the combination ofmethods for the other sub modules that is optimum for it while also allowingmore comprehensive comparisons with existing methods to understand itscontributions better. By extensive use of generic programming, the system makes it easy to plug ina new method for any of the sub modules so that it can not only be tested withexisting methods for other sub modules but also become immediately availablefor deployment in any system that uses the framework.
arxiv-16500-121 | Evaluation of Deep Learning based Pose Estimation for Sign Language Recognition | http://arxiv.org/abs/1602.09065 | author:Srujana Gattupalli, Amir Ghaderi, Vassilis Athitsos category:cs.CV published:2016-02-29 summary:Human body pose estimation and hand detection are two important tasks forsystems that perform computer vision-based sign language recognition(SLR).However, both tasks are challenging, especially when the input is color videos,with no depth information. Many algorithms have been proposed in the literaturefor these tasks, and some of the most successful recent algorithms are based ondeep learning. In this paper, we introduce a dataset for human pose estimationfor SLR domain. We evaluate the performance of two deep learning based poseestimation methods, by performing user-independent experiments on our dataset.We also perform transfer learning, and we obtain results that demonstrate thattransfer learning can improve pose estimation accuracy. The dataset and resultsfrom these methods can create a useful baseline for future works.
arxiv-16500-122 | Clustering Based Feature Learning on Variable Stars | http://arxiv.org/abs/1602.08977 | author:Cristóbal Mackenzie, Karim Pichara, Pavlos Protopapas category:astro-ph.SR cs.CV published:2016-02-29 summary:The success of automatic classification of variable stars strongly depends onthe lightcurve representation. Usually, lightcurves are represented as a vectorof many statistical descriptors designed by astronomers called features. Thesedescriptors commonly demand significant computational power to calculate,require substantial research effort to develop and do not guarantee goodperformance on the final classification task. Today, lightcurve representationis not entirely automatic; algorithms that extract lightcurve features aredesigned by humans and must be manually tuned up for every survey. The vastamounts of data that will be generated in future surveys like LSST meanastronomers must develop analysis pipelines that are both scalable andautomated. Recently, substantial efforts have been made in the machine learningcommunity to develop methods that prescind from expert-designed and manuallytuned features for features that are automatically learned from data. In thiswork we present what is, to our knowledge, the first unsupervised featurelearning algorithm designed for variable stars. Our method first extracts alarge number of lightcurve subsequences from a given set of photometric data,which are then clustered to find common local patterns in the time series.Representatives of these patterns, called exemplars, are then used to transformlightcurves of a labeled set into a new representation that can then be used totrain an automatic classifier. The proposed algorithm learns the features fromboth labeled and unlabeled lightcurves, overcoming the bias generated when thelearning process is done only with labeled data. We test our method on MACHOand OGLE datasets; the results show that the classification performance weachieve is as good and in some cases better than the performance achieved usingtraditional features, while the computational cost is significantly lower.
arxiv-16500-123 | FALDOI: Large Displacement Optical Flow by Astute Initialization | http://arxiv.org/abs/1602.08960 | author:Roberto P. Palomares, Gloria Haro, Coloma Ballester, Enric Meinhardt-Llopis category:cs.CV published:2016-02-29 summary:We propose a large displacement optical flow method that introduces a newstrategy to compute a good local minimum of any optical flow energy functional.The method requires a given set of discrete matches, which can be extremelysparse, and an energy functional. The matches are used to guide a structuredcoordinate-descent of the energy functional around these keypoints. It resultsin a two-step minimization method at the finest scale which is very robust tothe inevitable outliers of the sparse matcher, and it is better than the multi-scale methods, especially when there are small objects with very largedisplacements, that the multi-scale methods are incapable to find. Indeed, theproposed method recovers the correct motion field of any object which has atleast one correct match, regardless of the magnitude of the displacement. Wevalidate our proposal using several optical flow variational models. Theresults consistently outperform the coarse-to-fine approaches and achieve goodqualitative and quantitative performance on the standard optical flowbenchmarks.
arxiv-16500-124 | Representation of linguistic form and function in recurrent neural networks | http://arxiv.org/abs/1602.08952 | author:Ákos Kádár, Grzegorz Chrupała, Afra Alishahi category:cs.CL cs.LG published:2016-02-29 summary:We present novel methods for analysing the activation patterns of RNNs andidentifying the types of linguistic structure they learn. As a case study, weuse a multi-task gated recurrent network model consisting of two parallelpathways with shared word embeddings trained on predicting the representationsof the visual scene corresponding to an input sentence, and predicting the nextword in the same sentence. We show that the image prediction pathway issensitive to the information structure of the sentence, and pays selectiveattention to lexical categories and grammatical functions that carry semanticinformation. It also learns to treat the same input token differently dependingon its grammatical functions in the sentence. The language model iscomparatively more sensitive to words with a syntactic function. Our analysisof the function of individual hidden units shows that each pathway containsspecialized units tuned to patterns informative for the task, some of which cancarry activations to later time steps to encode long-term dependencies.
arxiv-16500-125 | Even Trolls Are Useful: Efficient Link Classification in Signed Networks | http://arxiv.org/abs/1602.08986 | author:Géraud Le Falher, Fabio Vitale category:cs.LG cs.SI physics.soc-ph published:2016-02-29 summary:We address the problem of classifying the links of signed social networksgiven their full structural topology. Motivated by a binary user behaviourassumption, which is supported by decades of research in psychology, we developan efficient and surprisingly simple approach to solve this classificationproblem. Our methods operate both within the active and batch settings. Wedemonstrate that the algorithms we developed are extremely fast in boththeoretical and practical terms. Within the active setting, we provide a newcomplexity measure and a rigorous analysis of our methods that hold forarbitrary signed networks. We validate our theoretical claims carrying out aset of experiments on three well known real-world datasets, showing that ourmethods outperform the competitors while being much faster.
arxiv-16500-126 | Easy Monotonic Policy Iteration | http://arxiv.org/abs/1602.09118 | author:Joshua Achiam category:cs.LG cs.AI stat.ML published:2016-02-29 summary:A key problem in reinforcement learning for control with general functionapproximators (such as deep neural networks and other nonlinear functions) isthat, for many algorithms employed in practice, updates to the policy or$Q$-function may fail to improve performance---or worse, actually cause thepolicy performance to degrade. Prior work has addressed this for policyiteration by deriving tight policy improvement bounds; by optimizing the lowerbound on policy improvement, a better policy is guaranteed. However, existingapproaches suffer from bounds that are hard to optimize in practice becausethey include sup norm terms which cannot be efficiently estimated ordifferentiated. In this work, we derive a better policy improvement bound wherethe sup norm of the policy divergence has been replaced with an averagedivergence; this leads to an algorithm, Easy Monotonic Policy Iteration, thatgenerates sequences of policies with guaranteed non-decreasing returns and iseasy to implement in a sample-based framework.
arxiv-16500-127 | $L_2$Boosting in High-Dimensions: Rate of Convergence | http://arxiv.org/abs/1602.08927 | author:Ye Luo, Martin Spindler category:stat.ML cs.LG math.ST stat.ME stat.TH published:2016-02-29 summary:Boosting is one of the most significant developments in machine learning.This paper studies the rate of convergence of $L_2$Boosting, which is tailoredfor regression, in a high-dimensional setting. Moreover, we introduce so-called\textquotedblleft post-Boosting\textquotedblright. This is a post-selectionestimator which applies ordinary least squares to the variables selected in thefirst stage by $L_2$Boosting. Another variant is orthogonal boosting whereafter each step an orthogonal projection is conducted. We show that bothpost-$L_2$Boosting and the orthogonal boosting achieve the same rate ofconvergence as Lasso in a sparse, high-dimensional setting. The\textquotedblleft classical\textquotedblright $L_2$Boosting achieves a slowerconvergence rate for prediction, but no assumptions on the design matrix areimposed for this result in contrast to rates e.g.~established with LASSO. Wealso introduce rules for early stopping which can easily be implemented andwill be used in applied work. Moreover, our results also allow a directcomparison between LASSO and boosting that has been missing in the literature.Finally, we present simulation studies to illustrate the relevance of ourtheoretical results and to provide insights into the practical aspects ofboosting. In the simulation studies post-$L_2$Boosting clearly outperformsLASSO.
arxiv-16500-128 | Beyond CCA: Moment Matching for Multi-View Models | http://arxiv.org/abs/1602.09013 | author:Anastasia Podosinnikova, Francis Bach, Simon Lacoste-Julien category:stat.ML cs.LG published:2016-02-29 summary:We introduce three novel semi-parametric extensions of probabilisticcanonical correlation analysis with identifiability guarantees. We considermoment matching techniques for estimation in these models. For that, by drawingexplicit links between the new models and a discrete version of independentcomponent analysis (DICA), we first extend the DICA cumulant tensors to the newdiscrete version of CCA. By further using a close connection with independentcomponent analysis, we introduce generalized covariance matrices, which canreplace the cumulant tensors in the moment matching framework, and, therefore,improve sample complexity and simplify derivations and algorithmssignificantly. As the tensor power method or orthogonal joint diagonalizationare not applicable in the new setting, we use non-orthogonal jointdiagonalization techniques for matching the cumulants. We demonstrateperformance of the proposed models and estimation techniques on experimentswith both synthetic and real datasets.
arxiv-16500-129 | Stochastic bandits on a social network: Collaborative learning with local information sharing | http://arxiv.org/abs/1602.08886 | author:Ravi Kumar Kolla, Krishna Jagannathan, Aditya Gopalan category:cs.LG stat.ML published:2016-02-29 summary:We consider a collaborative online learning paradigm, wherein a group ofagents connected through a social network are engaged in learning a Multi-ArmedBandit problem. Each time an agent takes an action, the corresponding reward isinstantaneously observed by the agent, as well as its neighbours in the socialnetwork. We perform a regret analysis of various policies in this collaborativelearning setting. A key finding of this paper is that appropriate networkextensions of widely-studied single agent learning policies do not perform wellin terms of regret. In particular, we identify a class of non-altruistic andindividually consistent policies, which could suffer a large regret. We alsoshow that the regret performance can be substantially improved by exploitingthe network structure. Specifically, we consider a star network, which is acommon motif in hierarchical social networks, and show that the hub agent canbe used as an information sink, to aid the learning rates of the entirenetwork. We also present numerical experiments to corroborate our analyticalresults.
arxiv-16500-130 | Pandora: Description of a Painting Database for Art Movement Recognition with Baselines and Perspectives | http://arxiv.org/abs/1602.08855 | author:Corneliu Florea, Razvan Condorovici, Constantin Vertan, Raluca Boia, Laura Florea, Ruxandra Vranceanu category:cs.CV published:2016-02-29 summary:To facilitate computer analysis of visual art, in the form of paintings, weintroduce Pandora (Paintings Dataset for Recognizing the Art movement)database, a collection of digitized paintings labelled with respect to theartistic movement. Noting that the set of databases available as benchmarks forevaluation is highly reduced and most existing ones are limited in variabilityand number of images, we propose a novel large scale dataset of digitalpaintings. The database consists of more than 7700 images from 12 artmovements. Each genre is illustrated by a number of images varying from 250 tonearly 1000. We investigate how local and global features and classificationsystems are able to recognize the art movement. Our experimental resultssuggest that accurate recognition is achievable by a combination of variouscategories.To facilitate computer analysis of visual art, in the form ofpaintings, we introduce Pandora (Paintings Dataset for Recognizing the Artmovement) database, a collection of digitized paintings labelled with respectto the artistic movement. Noting that the set of databases available asbenchmarks for evaluation is highly reduced and most existing ones are limitedin variability and number of images, we propose a novel large scale dataset ofdigital paintings. The database consists of more than 7700 images from 12 artmovements. Each genre is illustrated by a number of images varying from 250 tonearly 1000. We investigate how local and global features and classificationsystems are able to recognize the art movement. Our experimental resultssuggest that accurate recognition is achievable by a combination of variouscategories.
arxiv-16500-131 | On Complex Valued Convolutional Neural Networks | http://arxiv.org/abs/1602.09046 | author:Nitzan Guberman category:cs.NE published:2016-02-29 summary:Convolutional neural networks (CNNs) are the cutting edge model forsupervised machine learning in computer vision. In recent years CNNs haveoutperformed traditional approaches in many computer vision tasks such asobject detection, image classification and face recognition. CNNs arevulnerable to overfitting, and a lot of research focuses on findingregularization methods to overcome it. One approach is designing task specificmodels based on prior knowledge. Several works have shown that properties of natural images can be easilycaptured using complex numbers. Motivated by these works, we present avariation of the CNN model with complex valued input and weights. We constructthe complex model as a generalization of the real model. Lack of order over thecomplex field raises several difficulties both in the definition and in thetraining of the network. We address these issues and suggest possiblesolutions. The resulting model is shown to be a restricted form of a real valued CNNwith twice the parameters. It is sensitive to phase structure, and we suggestit serves as a regularized model for problems where such structure isimportant. This suggestion is verified empirically by comparing the performanceof a complex and a real network in the problem of cell detection. The twonetworks achieve comparable results, and although the complex model is hard totrain, it is significantly less vulnerable to overfitting. We also demonstratethat the complex network detects meaningful phase structure in the data.
arxiv-16500-132 | Iterative Aggregation Method for Solving Principal Component Analysis Problems | http://arxiv.org/abs/1602.08800 | author:Vitaly Bulgakov category:cs.NA cs.IR cs.LG published:2016-02-29 summary:Motivated by the previously developed multilevel aggregation method forsolving structural analysis problems a novel two-level aggregation approach forefficient iterative solution of Principal Component Analysis (PCA) problems isproposed. The course aggregation model of the original covariance matrix isused in the iterative solution of the eigenvalue problem by a power iterationsmethod. The method is tested on several data sets consisting of large number oftext documents.
arxiv-16500-133 | Bioinformatics and Classical Literary Study | http://arxiv.org/abs/1602.08844 | author:Pramit Chaudhuri, Joseph P. Dexter category:cs.CL published:2016-02-29 summary:This paper describes a collaborative project between classicists,quantitative biologists, and computer scientists to apply ideas and methodsdrawn from the sciences to the study of literature. A core goal of the projectis the use of computational biology, natural language processing, and machinelearning techniques to investigate intertextuality, reception, and relatedphenomena of literary significance. As a case study in our approach, here wedescribe the use of sequence alignment, a common technique in genomics, todetect intertextuality in Latin literature. Sequence alignment is distinguishedby its ability to find inexact verbal parallels, which makes it ideal foridentifying phonetic resemblances in large corpora of Latin texts. Althoughespecially suited to Latin, sequence alignment in principle can be extended tomany other languages.
arxiv-16500-134 | Exploring the coevolution of predator and prey morphology and behavior | http://arxiv.org/abs/1602.08802 | author:Randal S. Olson, Arend Hintze, Fred C. Dyer, Jason H. Moore, Christoph Adami category:q-bio.PE cs.NE published:2016-02-29 summary:A common idiom in biology education states, "Eyes in the front, the animalhunts. Eyes on the side, the animal hides." In this paper, we explore onepossible explanation for why predators tend to have forward-facing, high-acuityvisual systems. We do so using an agent-based computational model of evolution,where predators and prey interact and adapt their behavior and morphology toone another over successive generations of evolution. In this model, we observea coevolutionary cycle between prey swarming behavior and the predator's visualsystem, where the predator and prey continually adapt their visual system andbehavior, respectively, over evolutionary time in reaction to one another dueto the well-known "predator confusion effect." Furthermore, we provide evidencethat the predator visual system is what drives this coevolutionary cycle, andsuggest that the cycle could be closed if the predator evolves a hybrid visualsystem capable of narrow, high-acuity vision for tracking prey as well asbroad, coarse vision for prey discovery. Thus, the conflicting demands imposedon a predator's visual system by the predator confusion effect could have ledto the evolution of complex eyes in many predators.
arxiv-16500-135 | On the entropy numbers of the mixed smoothness function classes | http://arxiv.org/abs/1602.08712 | author:V. Temlyakov category:math.NA math.FA stat.ML published:2016-02-28 summary:Behavior of the entropy numbers of classes of multivariate functions withmixed smoothness is studied here. This problem has a long history and somefundamental problems in the area are still open. The main goal of this paper isto develop a new method of proving the upper bounds for the entropy numbers.This method is based on recent developments of nonlinear approximation, inparticular, on greedy approximation. This method consists of the following twosteps strategy. At the first step we obtain bounds of the best m-termapproximations with respect to a dictionary. At the second step we use generalinequalities relating the entropy numbers to the best m-term approximations.For the lower bounds we use the volume estimates method, which is a well knownpowerful method for proving the lower bounds for the entropy numbers. It wasused in a number of previous papers.
arxiv-16500-136 | Lie Access Neural Turing Machine | http://arxiv.org/abs/1602.08671 | author:Greg Yang category:cs.NE cs.AI cs.LG published:2016-02-28 summary:Recently, Neural Turing Machine and Memory Networks have shown that adding anexternal memory can greatly ameliorate a traditional recurrent neural network'stendency to forget after a long period of time. Here we present a new design ofan external memory, wherein memories are stored in an Euclidean key space$\mathbb R^n$. An LSTM controller performs read and write via specializedstructures called read and write heads, following the design of Neural TuringMachine. It can move a head by either providing a new address in the key space(aka random access) or moving from its previous position via a Lie group action(aka Lie access). In this way, the "L" and "R" instructions of a traditionalTuring Machine is generalized to arbitrary elements of a fixed Lie groupaction. For this reason, we name this new model the Lie Access Neural TuringMachine, or LANTM. We tested two different configurations of LANTM against an LSTM baseline inseveral basic experiments. As LANTM is differentiable end-to-end, training wasdone with RMSProp. We found the right configuration of LANTM to be capable oflearning different permutation and arithmetic tasks and extrapolating to atleast twice the input size, all with the number of parameters 2 orders ofmagnitude below that for the LSTM baseline. In particular, we trained LANTM onaddition of $k$-digit numbers for $2 \le k \le 16$, but it was able togeneralize almost perfectly to $17 \le k \le 32$.
arxiv-16500-137 | Investigating practical linear temporal difference learning | http://arxiv.org/abs/1602.08771 | author:Adam White, Martha White category:cs.LG cs.AI stat.ML published:2016-02-28 summary:Off-policy reinforcement learning has many applications including: learningfrom demonstration, learning multiple goal seeking policies in parallel, andrepresenting predictive knowledge. Recently there has been an proliferation ofnew policy-evaluation algorithms that fill a longstanding algorithmic void inreinforcement learning: combining robustness to off-policy sampling, functionapproximation, linear complexity, and temporal difference (TD) updates. Thispaper contains two main contributions. First, we derive two new hybrid TDpolicy-evaluation algorithms, which fill a gap in this collection ofalgorithms. Second, we perform an empirical comparison to elicit which of thesenew linear TD methods should be preferred in different situations, and makeconcrete suggestions about practical use.
arxiv-16500-138 | QuotationFinder - Searching for Quotations and Allusions in Greek and Latin Texts and Establishing the Degree to Which a Quotation or Allusion Matches Its Source | http://arxiv.org/abs/1602.08657 | author:Luc Herren category:cs.CL published:2016-02-28 summary:The software programs generally used with the TLG (Thesaurus Linguae Graecae)and the CLCLT (CETEDOC Library of Christian Latin Texts) CD-ROMs are not wellsuited for finding quotations and allusions. QuotationFinder uses moresophisticated criteria as it ranks search results based on how closely theymatch the source text, listing search results with literal quotations first andloose verbal parallels last.
arxiv-16500-139 | Does quantification without adjustments work? | http://arxiv.org/abs/1602.08780 | author:Dirk Tasche category:stat.ML cs.LG math.ST stat.TH 62C10 published:2016-02-28 summary:Classification is the task of predicting the class labels of objects based onthe observation of their features. In contrast, quantification has been definedas the task of determining the prevalence of the positive class labels in atarget dataset. The simplest approach to quantification is Classify & Countwhere a classifier is optimised for classification on a training set andapplied to the target dataset for the prediction of positive class labels. Thenumber of predicted positive labels is then used as an estimate of the positiveclass prevalence in the target dataset. Since the performance of Classify &Count for quantification is known to be inferior its results typically aresubject to adjustments. However, some researchers recently have suggested thatClassify & Count might actually work without adjustments if it is based on aclassifier that was specifically trained for quantification. We discuss thetheoretical foundation for this claim and explore its potential and limitationswith a numerical example based on the binormal model with equal variances.
arxiv-16500-140 | Optimizing the Learning Order of Chinese Characters Using a Novel Topological Sort Algorithm | http://arxiv.org/abs/1602.08742 | author:James C. Loach, Jinzhao Wang category:cs.CL physics.soc-ph published:2016-02-28 summary:We develop a novel algorithm for optimizing the order in which Chinesecharacters are learned, one that incorporates the benefits of learning them inorder of usage frequency and in order of their hierarchal structuralrelationships. We show that our work outperforms previously published orderingsand algorithms. Our algorithm is applicable to any scheduling task where nodeshave intrinsic differences in importance and must be visited in topologicalorder.
arxiv-16500-141 | Structured Prediction with Test-time Budget Constraints | http://arxiv.org/abs/1602.08761 | author:Tolga Bolukbasi, Kai-Wei Chang, Joseph Wang, Venkatesh Saligrama category:stat.ML cs.CL cs.CV cs.LG published:2016-02-28 summary:We study the problem of structured prediction under test-time budgetconstraints. We propose a novel approach applicable to a wide range ofstructured prediction problems in computer vision and natural languageprocessing. Our approach seeks to adaptively generate computationally costlyfeatures during test-time in order to reduce the computational cost ofprediction while maintaining prediction performance. We show that training theadaptive feature generation system can be reduced to a series of structuredlearning problems, resulting in efficient training using existing structuredlearning algorithms. This framework provides theoretical justification forseveral existing heuristic approaches found in literature. We evaluate ourproposed adaptive system on two real-world structured prediction tasks, opticalcharacter recognition (OCR) and dependency parsing. For OCR our method cuts thefeature acquisition time by half coming within a 1% margin of top accuracy. Fordependency parsing we realize an overall runtime gain of 20% withoutsignificant loss in performance.
arxiv-16500-142 | Gibberish Semantics: How Good is Russian Twitter in Word Semantic Similarity Task? | http://arxiv.org/abs/1602.08741 | author:Nikolay N. Vasiliev category:cs.CL published:2016-02-28 summary:The most studied and most successful language models were developed andevaluated mainly for English and other close European languages, such asFrench, German, etc. It is important to study applicability of these models toother languages. The use of vector space models for Russian was recentlystudied for multiple corpora, such as Wikipedia, RuWac, lib.ru. These modelswere evaluated against word semantic similarity task. For our knowledge Twitterwas not considered as a corpus for this task, with this work we fill the gap.Results for vectors trained on Twitter corpus are comparable in accuracy withother single-corpus trained models, although the best performance is currentlyachieved by combination of multiple corpora.
arxiv-16500-143 | Measuring and Predicting Tag Importance for Image Retrieval | http://arxiv.org/abs/1602.08680 | author:Shangwen Li, Sanjay Purushotham, Chen Chen, Yuzhuo Ren, C. -C. Jay Kuo category:cs.CV published:2016-02-28 summary:Textual data such as tags, sentence descriptions are combined with visualcues to reduce the semantic gap for image retrieval applications in today'sMultimodal Image Retrieval (MIR) systems. However, all tags are treated asequally important in these systems, which may result in misalignment betweenvisual and textual modalities during MIR training. This will further lead todegenerated retrieval performance at query time. To address this issue, weinvestigate the problem of tag importance prediction, where the goal is toautomatically predict the tag importance and use it in image retrieval. Toachieve this, we first propose a method to measure the relative importance ofobject and scene tags from image sentence descriptions. Using this as theground truth, we present a tag importance prediction model by exploiting jointvisual, semantic and context cues. The Structural Support Vector Machine (SSVM)formulation is adopted to ensure efficient training of the prediction model.Then, the Canonical Correlation Analysis (CCA) is employed to learn therelation between the image visual feature and tag importance to obtain robustretrieval performance. Experimental results on three real-world datasets show asignificant performance improvement of the proposed MIR with Tag ImportancePrediction (MIR/TIP) system over other MIR systems.
arxiv-16500-144 | Stability and Structural Properties of Gene Regulation Networks with Coregulation Rules | http://arxiv.org/abs/1602.08753 | author:Jonathan H. Warrell, Musa M. Mhlanga category:stat.ML q-bio.MN q-bio.QM published:2016-02-28 summary:Coregulation of the expression of groups of genes has been extensivelydemonstrated empirically in bacterial and eukaryotic systems. Such coregulationcan arise through the use of shared regulatory motifs, which allow thecoordinated expression of modules (and module groups) of functionally relatedgenes across the genome. Coregulation can also arise through the physicalassociation of multi-gene complexes through chromosomal looping, which are thentranscribed together. We present a general formalism for modeling coregulationrules in the framework of Random Boolean Networks (RBN), and develop specificmodels for transcription factor networks with modular structure (includingmodule groups, and multi-input modules (MIM) with autoregulation) andmulti-gene complexes (including hierarchical differentiation between multi-genecomplex members). We develop a mean-field approach to analyse the stability oflarge networks incorporating coregulation, and show that autoregulated MIM andhierarchical gene-complex models can achieve greater stability than networkswithout coregulation whose rules have matching activation frequency. We providefurther analysis of the stability of small networks of both kinds throughsimulations. We also characterize several general properties of the transientsand attractors in the hierarchical coregulation model, and show usingsimulations that the steady-state distribution factorizes hierarchically as aBayesian network in a Markov Jump Process analogue of the RBN model.
arxiv-16500-145 | Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus | http://arxiv.org/abs/1602.08715 | author:Avi Shmidman, Moshe Koppel, Ely Porat category:cs.CL published:2016-02-28 summary:We propose a method for efficiently finding all parallel passages in a largecorpus, even if the passages are not quite identical due to rephrasing andorthographic variation. The key ideas are the representation of each word inthe corpus by its two most infrequent letters, finding matched pairs of stringsof four or five words that differ by at most one word and then identifyingclusters of such matched pairs. Using this method, over 4600 parallel pairs ofpassages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus ofover 1.8 million words, in just over 30 seconds. Empirical comparisons onsample data indicate that the coverage obtained by our method is essentiallythe same as that obtained using slow exhaustive methods.
arxiv-16500-146 | A Structured Variational Auto-encoder for Learning Deep Hierarchies of Sparse Features | http://arxiv.org/abs/1602.08734 | author:Tim Salimans category:stat.ML cs.LG stat.CO published:2016-02-28 summary:In this note we present a generative model of natural images consisting of adeep hierarchy of layers of latent random variables, each of which follows anew type of distribution that we call rectified Gaussian. These rectifiedGaussian units allow spike-and-slab type sparsity, while retaining thedifferentiability necessary for efficient stochastic gradient variationalinference. To learn the parameters of the new model, we approximate theposterior of the latent variables with a variational auto-encoder. Rather thanmaking the usual mean-field assumption however, the encoder parameterizes a newtype of structured variational approximation that retains the priordependencies of the generative model. Using this structured posteriorapproximation, we are able to perform joint training of deep models with manylayers of latent random variables, without having to resort to stacking orother layerwise training procedures.
arxiv-16500-147 | Significance Driven Hybrid 8T-6T SRAM for Energy-Efficient Synaptic Storage in Artificial Neural Networks | http://arxiv.org/abs/1602.08556 | author:Gopalakrishnan Srinivasan, Parami Wijesinghe, Syed Shakib Sarwar, Akhilesh Jaiswal, Kaushik Roy category:cs.NE published:2016-02-27 summary:Multilayered artificial neural networks (ANN) have found widespread utilityin classification and recognition applications. The scale and complexity ofsuch networks together with the inadequacies of general purpose computingplatforms have led to a significant interest in the development of efficienthardware implementations. In this work, we focus on designing energy efficienton-chip storage for the synaptic weights. In order to minimize the powerconsumption of typical digital CMOS implementations of such large-scalenetworks, the digital neurons could be operated reliably at scaled voltages byreducing the clock frequency. On the contrary, the on-chip synaptic storagedesigned using a conventional 6T SRAM is susceptible to bitcell failures atreduced voltages. However, the intrinsic error resiliency of NNs to smallsynaptic weight perturbations enables us to scale the operating voltage of the6TSRAM. Our analysis on a widely used digit recognition dataset indicates thatthe voltage can be scaled by 200mV from the nominal operating voltage (950mV)for practically no loss (less than 0.5%) in accuracy (22nm predictivetechnology). Scaling beyond that causes substantial performance degradationowing to increased probability of failures in the MSBs of the synaptic weights.We, therefore propose a significance driven hybrid 8T-6T SRAM, wherein thesensitive MSBs are stored in 8T bitcells that are robust at scaled voltages dueto decoupled read and write paths. In an effort to further minimize the areapenalty, we present a synaptic-sensitivity driven hybrid memory architectureconsisting of multiple 8T-6T SRAM banks. Our circuit to system-level simulationframework shows that the proposed synaptic-sensitivity driven architectureprovides a 30.91% reduction in the memory access power with a 10.41% areaoverhead, for less than 1% loss in the classification accuracy.
arxiv-16500-148 | Multiplier-less Artificial Neurons Exploiting Error Resiliency for Energy-Efficient Neural Computing | http://arxiv.org/abs/1602.08557 | author:Syed Shakib Sarwar, Swagath Venkataramani, Anand Raghunathan, Kaushik Roy category:cs.NE published:2016-02-27 summary:Large-scale artificial neural networks have shown significant promise inaddressing a wide range of classification and recognition applications.However, their large computational requirements stretch the capabilities ofcomputing platforms. The fundamental components of these neural networks arethe neurons and its synapses. The core of a digital hardware neuron consists ofmultiplier, accumulator and activation function. Multipliers consume most ofthe processing energy in the digital neurons, and thereby in the hardwareimplementations of artificial neural networks. We propose an approximatemultiplier that utilizes the notion of computation sharing and exploits errorresilience of neural network applications to achieve improved energyconsumption. We also propose Multiplier-less Artificial Neuron (MAN) for evenlarger improvement in energy consumption and adapt the training process toensure minimal degradation in accuracy. We evaluated the proposed design on 5recognition applications. The results show, 35% and 60% reduction in energyconsumption, for neuron sizes of 8 bits and 12 bits, respectively, with amaximum of ~2.83% loss in network accuracy, compared to a conventional neuronimplementation. We also achieve 37% and 62% reduction in area for a neuron sizeof 8 bits and 12 bits, respectively, under iso-speed conditions.
arxiv-16500-149 | Towards Neural Knowledge DNA | http://arxiv.org/abs/1602.08571 | author:Haoxi Zhang, Cesar Sanin, Edward Szczerbicki category:cs.AI cs.NE published:2016-02-27 summary:In this paper, we propose the Neural Knowledge DNA, a framework that tailorsthe ideas underlying the success of neural networks to the scope of knowledgerepresentation. Knowledge representation is a fundamental field that dedicateto representing information about the world in a form that computer systems canutilize to solve complex tasks. The proposed Neural Knowledge DNA is designedto support discovering, storing, reusing, improving, and sharing knowledgeamong machines and organisation. It is constructed in a similar fashion of howDNA formed: built up by four essential elements. As the DNA producesphenotypes, the Neural Knowledge DNA carries information and knowledge via itsfour essential elements, namely, Networks, Experiences, States, and Actions.
arxiv-16500-150 | Graph clustering, variational image segmentation methods and Hough transform scale detection for object measurement in images | http://arxiv.org/abs/1602.08574 | author:Luca Calatroni, Yves van Gennip, Carola-Bibiane Schönlieb, Hannah Rowland, Arjuna Flenner category:math.AP cs.CV published:2016-02-27 summary:We consider the problem of scale detection in images where a region ofinterest is present together with a measurement tool (e.g. a ruler). For thesegmentation part, we focus on the graph based method by Flenner and Bertozziwhich reinterprets classical continuous Ginzburg-Landau minimisation models ina totally discrete framework. To overcome the numerical difficulties due to thelarge size of the images considered we use matrix completion and splittingtechniques. The scale on the measurement tool is detected via a Hough transformbased algorithm. The method is then applied to some measurement tasks arisingin real-world applications such as zoology, medicine and archaeology.
arxiv-16500-151 | Single-Image Superresolution Through Directional Representations | http://arxiv.org/abs/1602.08575 | author:Wojciech Czaja, James M. Murphy, Daniel Weinberg category:cs.CV published:2016-02-27 summary:We develop a mathematically-motivated algorithm for image superresolution,based on the discrete shearlet transform. The shearlet transform is stronglydirectional, and is known to provide near-optimally sparse representations fora broad class of images. This often leads to superior performance in edgedetection and image representation, when compared to other isotropic frames. Wejustify the use of shearlet frames for superresolution mathematically beforepresenting a superresolution algorithm that combines the shearlet transformwith the sparse mixing estimators (SME) approach pioneered by Mallat and Yu.Our algorithm is compared with an isotropic superresolution method, a previousprototype of a shearlet superresolution algorithm, and SME superresolution witha discrete wavelet frame. Our numerical results on a variety of image typesshow strong performance in terms of PSNR.
arxiv-16500-152 | Content-based Video Indexing and Retrieval Using Corr-LDA | http://arxiv.org/abs/1602.08581 | author:Rahul Radhakrishnan Iyer, Sanjeel Parekh, Vikas Mohandoss, Anush Ramsurat, Bhiksha Raj, Rita Singh category:cs.IR cs.CV published:2016-02-27 summary:Existing video indexing and retrieval methods on popular web-based multimediasharing websites are based on user-provided sparse tagging. This paper proposesa very specific way of searching for video clips, based on the content of thevideo. We present our work on Content-based Video Indexing and Retrieval usingthe Correspondence-Latent Dirichlet Allocation (corr-LDA) probabilisticframework. This is a model that provides for auto-annotation of videos in adatabase with textual descriptors, and brings the added benefit of utilizingthe semantic relations between the content of the video and text. We use theconcept-level matching provided by corr-LDA to build correspondences betweentext and multimedia, with the objective of retrieving content with increasedaccuracy. In our experiments, we employ only the audio components of theindividual recordings and compare our results with an SVM-based approach.
arxiv-16500-153 | DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and Caffe Compatibility | http://arxiv.org/abs/1602.08191 | author:Hanjoo Kim, Jaehong Park, Jaehee Jang, Sungroh Yoon category:cs.LG published:2016-02-26 summary:The increasing complexity of deep neural networks (DNNs) has made itchallenging to exploit existing large-scale data processing pipelines forhandling massive data and parameters involved in DNN training. Distributedcomputing platforms and GPGPU-based acceleration provide a mainstream solutionto this computational challenge. In this paper, we propose DeepSpark, adistributed and parallel deep learning framework that simultaneously exploitsApache Spark for large-scale distributed data management and Caffe forGPU-based acceleration. DeepSpark directly accepts Caffe input specifications,providing seamless compatibility with existing designs and network structures.To support parallel operations, DeepSpark automatically distributes workloadsand parameters to Caffe-running nodes using Spark and iteratively aggregatestraining results by a novel lock-free asynchronous variant of the popularelastic averaging stochastic gradient descent (SGD) update scheme, effectivelycomplementing the synchronized processing capabilities of Spark. DeepSpark isan on-going project, and the current release is available athttp://deepspark.snu.ac.kr.
arxiv-16500-154 | A Single Model Explains both Visual and Auditory Precortical Coding | http://arxiv.org/abs/1602.08486 | author:Honghao Shan, Matthew H. Tong, Garrison W. Cottrell category:q-bio.NC cs.CV cs.LG cs.NE published:2016-02-26 summary:Precortical neural systems encode information collected by the senses, butthe driving principles of the encoding used have remained a subject of debate.We present a model of retinal coding that is based on three constraints:information preservation, minimization of the neural wiring, and responseequalization. The resulting novel version of sparse principal componentsanalysis successfully captures a number of known characteristics of the retinalcoding system, such as center-surround receptive fields, color opponencychannels, and spatiotemporal responses that correspond to magnocellular andparvocellular pathways. Furthermore, when trained on auditory data, the samemodel learns receptive fields well fit by gammatone filters, commonly used tomodel precortical auditory coding. This suggests that efficient coding may be aunifying principle of precortical encoding across modalities.
arxiv-16500-155 | Architectural Complexity Measures of Recurrent Neural Networks | http://arxiv.org/abs/1602.08210 | author:Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov, Yoshua Bengio category:cs.LG cs.NE published:2016-02-26 summary:In this paper, we systematically analyse the connecting architectures ofrecurrent neural networks (RNNs). Our main contribution is twofold: first, wepresent a rigorous graph-theoretic framework describing the connectingarchitectures of RNNs in general. Second, we propose three architecturecomplexity measures of RNNs: (a) the recurrent depth, which captures the RNN'sover-time nonlinear complexity, (b) the feedforward depth, which captures thelocal input-output nonlinearity (similar to the "depth" in feedforward neuralnetworks (FNNs)), and (c) the recurrent skip coefficient which captures howrapidly the information propagates over time. Our experimental results showthat RNNs might benefit from larger recurrent depth and feedforward depth. Wefurther demonstrate that increasing recurrent skip coefficient offersperformance boosts on long term dependency problems, as we improve thestate-of-the-art for sequential MNIST dataset.
arxiv-16500-156 | Simple Bayesian Algorithms for Best Arm Identification | http://arxiv.org/abs/1602.08448 | author:Daniel Russo category:cs.LG published:2016-02-26 summary:This paper considers the optimal adaptive allocation of measurement effortfor identifying the best among a finite set of options or designs. Anexperimenter sequentially chooses designs to measure and observes noisy signalsof their quality with the goal of confidently identifying the best design aftera small number of measurements. I propose three simple Bayesian algorithms foradaptively allocating measurement effort. One is Top-Two Probability sampling,which computes the two designs with the highest posterior probability of beingoptimal, and then randomizes to select among these two. One is a variant atop-two sampling which considers not only the probability a design is optimal,but the expected amount by which it exceeds other designs. The final algorithmis a modified version of Thompson sampling that is tailored for identifying thebest design. I prove that these simple algorithms satisfy a strong optimalityproperty. In a frequestist setting where the true quality of the designs isfixed, the posterior is said to be consistent if it correctly identifies theoptimal design, in the sense that that the posterior probability assigned tothe event that some other design is optimal converges to zero as measurementsare collected. I show that under the proposed algorithms this convergenceoccurs at an exponential rate, and the corresponding exponent is the bestpossible among all allocation
arxiv-16500-157 | Seq-NMS for Video Object Detection | http://arxiv.org/abs/1602.08465 | author:Wei Han, Pooya Khorrami, Tom Le Paine, Prajit Ramachandran, Mohammad Babaeizadeh, Honghui Shi, Jianan Li, Shuicheng Yan, Thomas S. Huang category:cs.CV published:2016-02-26 summary:Video object detection is challenging because objects that are easilydetected in one frame may be difficult to detect in another frame within thesame clip. Recently, there have been major advances for doing object detectionin a single image. These methods typically contain three phases: (i) objectproposal generation (ii) object classification and (iii) post-processing. Wepropose a modification of the post-processing phase that uses high-scoringobject detections from nearby frames to boost scores of weaker detectionswithin the same clip. We show that our method obtains superior results tostate-of-the-art single image object detection techniques. Our method placed3rd in the video object detection (VID) task of the ImageNet Large Scale VisualRecognition Challenge 2015 (ILSVRC2015).
arxiv-16500-158 | Cortical Computation via Iterative Constructions | http://arxiv.org/abs/1602.08357 | author:Christos Papadimitrou, Samantha Petti, Santosh Vempala category:cs.NE cs.DS published:2016-02-26 summary:We study Boolean functions of an arbitrary number of input variables that canbe realized by simple iterative constructions based on constant-sizeprimitives. This restricted type of construction needs little globalcoordination or control and thus is a candidate for neurally feasiblecomputation. Valiant's construction of a majority function can be realized inthis manner and, as we show, can be generalized to any uniform thresholdfunction. We study the rate of convergence, finding that while linearconvergence to the correct function can be achieved for any threshold using afixed set of primitives, for quadratic convergence, the size of the primitivesmust grow as the threshold approaches 0 or 1. We also study finite realizationsof this process and the learnability of the functions realized. We show thatthe constructions realized are accurate outside a small interval near thetarget threshold, where the size of the construction grows as the inversesquare of the interval width. This phenomenon, that errors are higher closer tothresholds (and thresholds closer to the boundary are harder to represent), isa well-known cognitive finding.
arxiv-16500-159 | Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets | http://arxiv.org/abs/1602.08350 | author:Patrick O. Glauner, Andre Boechat, Lautaro Dolberg, Radu State, Franck Bettinger, Yves Rangoni, Diogo Duarte category:cs.LG cs.AI published:2016-02-26 summary:Non-technical losses (NTL) such as electricity theft cause significant harmto our economies, as in some countries they may range up to 40% of the totalelectricity distributed. Detecting NTLs requires costly on-site inspections.Accurate prediction of NTLs for customers using machine learning is thereforecrucial. To date, related research largely ignore that the two classes ofregular and non-regular customers are highly imbalanced, that NTL proportionsmay change and mostly consider small data sets, often not allowing to deploythe results in production. In this paper, we present a comprehensive approachto assess three NTL detection models for different NTL proportions in largereal world data sets of 100Ks of customers: Boolean rules, fuzzy logic andSupport Vector Machine. This work has resulted in appreciable results that areabout to be deployed in a leading industry solution. We believe that theconsiderations and observations made in this contribution are necessary forfuture smart meter research in order to report their effectiveness onimbalanced and large real world data sets.
arxiv-16500-160 | Bounded Rational Decision-Making in Feedforward Neural Networks | http://arxiv.org/abs/1602.08332 | author:Felix Leibfried, Daniel Alexander Braun category:cs.AI cs.LG cs.NE published:2016-02-26 summary:Bounded rational decision-makers transform sensory input into motor outputunder limited computational resources. Mathematically, such decision-makers canbe modeled as information-theoretic channels with limited transmission rate.Here, we apply this formalism for the first time to multilayer feedforwardneural networks. We derive synaptic weight update rules for two scenarios,where either each neuron is considered as a bounded rational decision-maker orthe network as a whole. In the update rules, bounded rationality translatesinto information-theoretically motivated types of regularization in weightspace. In experiments on the MNIST benchmark classification task forhandwritten digits, we show that such information-theoretic regularizationsuccessfully prevents overfitting across different architectures and attainsstate-of-the-art results for both ordinary and convolutional neural networks.
arxiv-16500-161 | We don't need no bounding-boxes: Training object class detectors using only human verification | http://arxiv.org/abs/1602.08405 | author:Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, Vittorio Ferrari category:cs.CV published:2016-02-26 summary:Training object class detectors typically requires a large set of images inwhich objects are annotated by bounding-boxes. However, manually drawingbounding-boxes is very time consuming. We propose a new scheme for trainingobject detectors which only requires annotators to verify bounding-boxesproduced automatically by the learning algorithm. Our scheme iterates betweenre-training the detector, re-localizing objects in the training images, andhuman verification. We use the verification signal both to improve re-trainingand to reduce the search space for re-localisation, which makes these stepsdifferent to what is normally done in a weakly supervised setting. Extensiveexperiments on PASCAL VOC 2007 show that (1) using human verification to updatedetectors and reduce the search space leads to the rapid production ofhigh-quality bounding-box annotations; (2) our scheme delivers detectorsperforming almost as good as those trained in a fully supervised setting,without ever drawing any bounding-box; (3) as the verification task is veryquick, our scheme substantially reduces total annotation time by a factor6x-9x.
arxiv-16500-162 | Victory Sign Biometric for Terrorists Identification | http://arxiv.org/abs/1602.08325 | author:Ahmad B. A. Hassanat, Mahmoud B. Alhasanat, Mohammad Ali Abbadi, Eman Btoush, Mouhammd Al-Awadi, Ahmad S. Tarawneh category:cs.CV published:2016-02-26 summary:Covering the face and all body parts, sometimes the only evidence to identifya person is their hand geometry, and not the whole hand- only two fingers (theindex and the middle fingers) while showing the victory sign, as seen in manyterrorists videos. This paper investigates for the first time a new way toidentify persons, particularly (terrorists) from their victory sign. We havecreated a new database in this regard using a mobile phone camera, imaging thevictory signs of 50 different persons over two sessions. Simple measurementsfor the fingers, in addition to the Hu Moments for the areas of the fingerswere used to extract the geometric features of the shown part of the hand shownafter segmentation. The experimental results using the KNN classifier wereencouraging for most of the recorded persons; with about 40% to 93% totalidentification accuracy, depending on the features, distance metric and K used.
arxiv-16500-163 | Deep Spiking Networks | http://arxiv.org/abs/1602.08323 | author:Peter O'Connor, Max Welling category:cs.NE published:2016-02-26 summary:We introduce the Spiking Multi-Layer Perceptron (SMLP). The SMLP is a spikingversion of a conventional Multi-Layer Perceptron with rectified-linear units.Our architecture is event-based, meaning that neurons in the networkcommunicate by sending "events" to downstream neurons, and that the state ofeach neuron is only updated when it receives an event. We show that the SMLPbehaves identically, during both prediction and training, to a conventionaldeep network of rectified-linear units in the limiting case where we run thespiking network for a long time. We apply this architecture to a conventionalclassification problem (MNIST) and achieve performance very close to that of aconventional MLP with the same architecture. Our network is a naturalarchitecture for learning based on streaming event-based data, and haspotential applications in robotic systems systems, which require low power andlow response latency.
arxiv-16500-164 | Harnessing disordered quantum dynamics for machine learning | http://arxiv.org/abs/1602.08159 | author:Keisuke Fujii, Kohei Nakajima category:quant-ph cs.AI cs.LG cs.NE nlin.CD published:2016-02-26 summary:Quantum computer has an amazing potential of fast information processing.However, realisation of a digital quantum computer is still a challengingproblem requiring highly accurate controls and key application strategies. Herewe propose a novel platform, quantum reservoir computing, to solve these issuessuccessfully by exploiting natural quantum dynamics, which is ubiquitous inlaboratories nowadays, for machine learning. In this framework, nonlineardynamics including classical chaos can be universally emulated in quantumsystems. A number of numerical experiments show that quantum systems consistingof at most seven qubits possess computational capabilities comparable toconventional recurrent neural networks of 500 nodes. This discovery opens up anew paradigm for information processing with artificial intelligence powered byquantum physics.
arxiv-16500-165 | Enhancing Genetic Algorithms using Multi Mutations | http://arxiv.org/abs/1602.08313 | author:Ahmad B. A. Hassanat, Esra'a Alkafaween, Nedal A. Al-Nawaiseh, Mohammad A. Abbadi, Mouhammd Alkasassbeh, Mahmoud B. Alhasanat category:cs.AI cs.NE published:2016-02-26 summary:Mutation is one of the most important stages of the genetic algorithm becauseof its impact on the exploration of global optima, and to overcome prematureconvergence. There are many types of mutation, and the problem lies inselection of the appropriate type, where the decision becomes more difficultand needs more trial and error. This paper investigates the use of more thanone mutation operator to enhance the performance of genetic algorithms. Novelmutation operators are proposed, in addition to two selection strategies forthe mutation operators, one of which is based on selecting the best mutationoperator and the other randomly selects any operator. Several experiments onsome Travelling Salesman Problems (TSP) were conducted to evaluate the proposedmethods, and these were compared to the well-known exchange mutation andrearrangement mutation. The results show the importance of some of the proposedmethods, in addition to the significant enhancement of the genetic algorithm'sperformance, particularly when using more than one mutation operator.
arxiv-16500-166 | Search by Ideal Candidates: Next Generation of Talent Search at LinkedIn | http://arxiv.org/abs/1602.08186 | author:Viet Ha-Thuc, Ye Xu, Satya Pradeep Kanduri, Xianren Wu, Vijay Dialani, Yan Yan, Abhishek Gupta, Shakti Sinha category:cs.IR cs.LG published:2016-02-26 summary:One key challenge in talent search is how to translate complex criteria of ahiring position into a search query. This typically requires deep knowledge onwhich skills are typically needed for the position, what are theiralternatives, which companies are likely to have such candidates, etc. However,listing examples of suitable candidates for a given position is a relativelyeasy job. Therefore, in order to help searchers overcome this challenge, wedesign a next generation of talent search paradigm at LinkedIn: Search by IdealCandidates. This new system only needs the searcher to input one or severalexamples of suitable candidates for the position. The system will generate aquery based on the input candidates and then retrieve and rank results based onthe query as well as the input candidates. The query is also shown to thesearcher to make the system transparent and to allow the searcher to interactwith it. As the searcher modifies the initial query and makes it deviate fromthe ideal candidates, the search ranking function dynamically adjusts anrefreshes the ranking results balancing between the roles of query and idealcandidates. As of writing this paper, the new system is being launched to ourcustomers.
arxiv-16500-167 | Scalable and Sustainable Deep Learning via Randomized Hashing | http://arxiv.org/abs/1602.08194 | author:Ryan Spring, Anshumali Shrivastava category:stat.ML cs.LG cs.NE published:2016-02-26 summary:Current deep learning architectures are growing larger in order to learn fromenormous datasets.These architectures require giant matrix multiplicationoperations to train millions or billions of parameters during forward and backpropagation steps. These operations are very expensive from a computational andenergy standpoint. We present a novel technique to reduce the amount ofcomputation needed to train and test deep net-works drastically. Our approachcombines recent ideas from adaptive dropouts and randomized hashing for maximuminner product search to select only the nodes with the highest activationefficiently. Our new algorithm for training deep networks reduces the overallcomputational cost,of both feed-forward pass and backpropagation,by operatingon significantly fewer nodes. As a consequence, our algorithm only requires 5%of computations (multiplications) compared to traditional algorithms, withoutany loss in the accuracy. Furthermore, due to very sparse gradient updates, ouralgorithm is ideally suited for asynchronous training leading to near linearspeedup with increasing parallelism. We demonstrate the scalability andsustainability (energy efficiency) of our proposed algorithm via rigorousexperimental evaluations.
arxiv-16500-168 | Patch-Ordering as a Regularization for Inverse Problems in Image Processing | http://arxiv.org/abs/1602.08510 | author:Gregory Vaksman, Michael Zibulevsky, Michael Elad category:cs.CV published:2016-02-26 summary:Recent work in image processing suggests that operating on (overlapping)patches in an image may lead to state-of-the-art results. This has beendemonstrated for a variety of problems including denoising, inpainting,deblurring, and super-resolution. The work reported in [1,2] takes an extrastep forward by showing that ordering these patches to form an approximateshortest path can be leveraged for better processing. The core idea is to applya simple filter on the resulting 1D smoothed signal obtained after thepatch-permutation. This idea has been also explored in combination with awavelet pyramid, leading eventually to a sophisticated and highly effectiveregularizer for inverse problems in imaging. In this work we further study thepatch-permutation concept, and harness it to propose a new simple yet effectiveregularization for image restoration problems. Our approach builds on theclassic Maximum A'posteriori probability (MAP), with a penalty functionconsisting of a regular log-likelihood term and a novel permutation-basedregularization term. Using a plain 1D Laplacian, the proposed regularizationforces robust smoothness (L1) on the permuted pixels. Since the permutationoriginates from patch-ordering, we propose to accumulate the smoothness termsover all the patches' pixels. Furthermore, we take into account the founddistances between adjacent patches in the ordering, by weighting the Laplacianoutcome. We demonstrate the proposed scheme on a diverse set of problems: (i)severe Poisson image denoising, (ii) Gaussian image denoising, (iii) imagedeblurring, and (iv) single image super-resolution. In all these cases, we userecent methods that handle these problems as initialization to our scheme. Thisis followed by an L-BFGS optimization of the above-described penalty function,leading to state-of-the-art results, and especially so for highly ill-posedcases.
arxiv-16500-169 | Learning and Free Energy in Expectation Consistent Approximate Inference | http://arxiv.org/abs/1602.08207 | author:Alyson K. Fletcher category:cs.IT math.IT stat.ML published:2016-02-26 summary:Approximations of loopy belief propagation are commonly combined withexpectation-maximization (EM) for probabilistic inference problems when thedensities have unknown parameters. This work considers an approximate EMlearning method combined with Opper and Winther's Expectation ConsistentApproximate Inference method. The combined algorithm is called EM-EC and isshown to have a simple variational free energy interpretation. In addition, thealgorithm can provide a computationally efficient and general approach to anumber of learning problems with hidden states including empirical Bayesianforms of regression, classification, compressed sensing, and sparse Bayesianlearning. Systems with linear dynamics interconnected with non-Gaussian ornonlinear components can also be easily considered.
arxiv-16500-170 | Multimodal Emotion Recognition Using Multimodal Deep Learning | http://arxiv.org/abs/1602.08225 | author:Wei Liu, Wei-Long Zheng, Bao-Liang Lu category:cs.HC cs.CV cs.LG published:2016-02-26 summary:To enhance the performance of affective models and reduce the cost ofacquiring physiological signals for real-world applications, we adoptmultimodal deep learning approach to construct affective models from multiplephysiological signals. For unimodal enhancement task, we indicate that the bestrecognition accuracy of 82.11% on SEED dataset is achieved with sharedrepresentations generated by Deep AutoEncoder (DAE) model. For multimodalfacilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE)achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets,respectively, which are much superior to the state-of-the-art approaches. Forcross-modal learning task, our experimental results demonstrate that the meanaccuracy of 66.34% is achieved on SEED dataset through shared representationsgenerated by EEG-based DAE as training samples and shared representationsgenerated by eye-based DAE as testing sample, and vice versa.
arxiv-16500-171 | Theoretical Analysis of the $k$-Means Algorithm - A Survey | http://arxiv.org/abs/1602.08254 | author:Johannes Blömer, Christiane Lammersen, Melanie Schmidt, Christian Sohler category:cs.DS cs.LG published:2016-02-26 summary:The $k$-means algorithm is one of the most widely used clustering heuristics.Despite its simplicity, analyzing its running time and quality of approximationis surprisingly difficult and can lead to deep insights that can be used toimprove the algorithm. In this paper we survey the recent results in thisdirection as well as several extension of the basic $k$-means method.
arxiv-16500-172 | Shape-aware Surface Reconstruction from Sparse Data | http://arxiv.org/abs/1602.08425 | author:Florian Bernard, Luis Salamanca, Johan Thunberg, Alexander Tack, Dennis Jentsch, Hans Lamecker, Stefan Zachow, Frank Hertel, Jorge Goncalves, Peter Gemmar category:cs.CV stat.ML published:2016-02-26 summary:The reconstruction of an object's shape or surface from a set of 3D points isa common topic in materials and life sciences, computationally handled incomputer graphics. Such points usually stem from optical or tactile 3Dcoordinate measuring equipment. Surface reconstruction also appears in medicalimage analysis, e.g. in anatomy reconstruction from tomographic measurements orthe alignment of intra-operative navigation and preoperative planning data. Incontrast to mere 3D point clouds, medical imaging yields contextual informationon the 3D point data that can be used to adopt prior information on the shapethat is to be reconstructed from the measurements. In this work we propose touse a statistical shape model (SSM) as a prior for surface reconstruction. Theprior knowledge is represented by a point distribution model (PDM) that isassociated with a surface mesh. Using the shape distribution that is modelledby the PDM, we reformulate the problem of surface reconstruction from aprobabilistic perspective based on a Gaussian Mixture Model (GMM). In order todo so, the given measurements are interpreted as samples of the GMM. By usingmixture components with anisotropic covariances that are oriented according tothe surface normals at the PDM points, a surface-based fitting is accomplished.By estimating the parameters of the GMM in a maximum a posteriori manner, thereconstruction of the surface from the given measurements is achieved.Extensive experiments suggest that our proposed approach leads to superiorsurface reconstructions compared to Iterative Closest Point (ICP) methods.
arxiv-16500-173 | Multivariate Hawkes Processes for Large-scale Inference | http://arxiv.org/abs/1602.08418 | author:Rémi Lemonnier, Kevin Scaman, Argyris Kalogeratos category:stat.ML published:2016-02-26 summary:In this paper, we present a framework for fitting multivariate Hawkesprocesses for large-scale problems both in the number of events in the observedhistory $n$ and the number of event types $d$ (i.e. dimensions). The proposedLow-Rank Hawkes Process (LRHP) framework introduces a low-rank approximation ofthe kernel matrix that allows to perform the nonparametric learning of the$d^2$ triggering kernels using at most $O(ndr^2)$ operations, where $r$ is therank of the approximation ($r \ll d,n$). This comes as a major improvement tothe existing state-of-the-art inference algorithms that are in $O(nd^2)$.Furthermore, the low-rank approximation allows LRHP to learn representativepatterns of interaction between event types, which may be valuable for theanalysis of such complex processes in real world datasets. The efficiency andscalability of our approach is illustrated with numerical experiments onsimulated as well as real datasets.
arxiv-16500-174 | Practical Riemannian Neural Networks | http://arxiv.org/abs/1602.08007 | author:Gaétan Marceau-Caron, Yann Ollivier category:cs.NE cs.LG stat.ML published:2016-02-25 summary:We provide the first experimental results on non-synthetic datasets for thequasi-diagonal Riemannian gradient descents for neural networks introduced in[Ollivier, 2015]. These include the MNIST, SVHN, and FACE datasets as well as apreviously unpublished electroencephalogram dataset. The quasi-diagonalRiemannian algorithms consistently beat simple stochastic gradient gradientdescents by a varying margin. The computational overhead with respect to simplebackpropagation is around a factor $2$. Perhaps more interestingly, thesemethods also reach their final performance quickly, thus requiring fewertraining epochs and a smaller total computation time. We also present an implementation guide to these Riemannian gradient descentsfor neural networks, showing how the quasi-diagonal versions can be implementedwith minimal effort on top of existing routines which compute gradients.
arxiv-16500-175 | Modeling cumulative biological phenomena with Suppes-Bayes causal networks | http://arxiv.org/abs/1602.07857 | author:Daniele Ramazzotti, Alex Graudenzi, Marco Antoniotti category:cs.AI cs.LG published:2016-02-25 summary:Several statistical techniques have been recently developed for the inferenceof cancer progression models from the increasingly available NGScross-sectional mutational profiles. A particular algorithm, CAPRI, was provento be the most efficient with respect to sample size and level of noise in thedata. The algorithm combines structural constraints based on Suppes' theory ofprobabilistic causation and maximum likelihood fit with regularization, anddefines constrained Bayesian networks, named Suppes-Bayes Causal Networks(SBCNs), which account for the selective advantage relations among genomicevents. In general, SBCNs are effective in modeling any phenomenon driven bycumulative dynamics, as long as the modeled events are persistent. Here wediscuss on the effectiveness of the SBCN theoretical framework and weinvestigate the influence of: (i) the priors based on Suppes' theory and (ii)different maximum likelihood regularization parameters on the inferenceperformance estimated on large synthetically generated datasets.
arxiv-16500-176 | Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks | http://arxiv.org/abs/1602.07868 | author:Tim Salimans, Diederik P. Kingma category:cs.LG cs.AI cs.NE published:2016-02-25 summary:We present weight normalization: a reparameterization of the weight vectorsin a neural network that decouples the length of those weight vectors fromtheir direction. By reparameterizing the weights in this way we improve theconditioning of the optimization problem and we speed up convergence ofstochastic gradient descent. Our reparameterization is inspired by batchnormalization but does not introduce any dependencies between the examples in aminibatch. This means that our method can also be applied successfully torecurrent models such as LSTMs and to noise-sensitive applications such as deepreinforcement learning or generative models, for which batch normalization isless well suited. Although our method is much simpler, it still provides muchof the speed-up of full batch normalization. In addition, the computationaloverhead of our method is lower, permitting more optimization steps to be takenin the same amount of time. We demonstrate the usefulness of our method onapplications in supervised image recognition, generative modelling, and deepreinforcement learning.
arxiv-16500-177 | How effective can simple ordinal peer grading be? | http://arxiv.org/abs/1602.07985 | author:Ioannis Caragiannis, George A. Krimpas, Alexandros A. Voudouris category:cs.AI cs.DS cs.LG published:2016-02-25 summary:Ordinal peer grading has been proposed as a simple and scalable solution forcomputing reliable information about student performance in massive open onlinecourses. The idea is to outsource the grading task to the students themselvesas follows. After the end of an exam, each student is asked to rank ---in termsof quality--- a bundle of exam papers by fellow students. An aggregation rulewill then combine the individual rankings into a global one that contains allstudents. We define a broad class of simple aggregation rules and present atheoretical framework for assessing their effectiveness. When statisticalinformation about the grading behaviour of students is available, the frameworkcan be used to compute the optimal rule from this class with respect to aseries of performance objectives. For example, a natural rule known as Borda isproved to be optimal when students grade correctly. In addition, we presentextensive simulations and a field experiment that validate our theory and proveit to be extremely accurate in predicting the performance of aggregation ruleseven when only rough information about grading behaviour is available.
arxiv-16500-178 | Auto-JacoBin: Auto-encoder Jacobian Binary Hashing | http://arxiv.org/abs/1602.08127 | author:Xiping Fu, Brendan McCane, Steven Mills, Michael Albert, Lech Szymanski category:cs.CV cs.LG published:2016-02-25 summary:Binary codes can be used to speed up nearest neighbor search tasks in largescale data sets as they are efficient for both storage and retrieval. In thispaper, we propose a robust auto-encoder model that preserves the geometricrelationships of high-dimensional data sets in Hamming space. This is done byconsidering a noise-removing function in a region surrounding the manifoldwhere the training data points lie. This function is defined with the propertythat it projects the data points near the manifold into the manifold wisely,and we approximate this function by its first order approximation. Experimentalresults show that the proposed method achieves better than state-of-the-artresults on three large scale high dimensional data sets.
arxiv-16500-179 | Meta-learning within Projective Simulation | http://arxiv.org/abs/1602.08017 | author:Adi Makmal, Alexey A. Melnikov, Vedran Dunjko, Hans J. Briegel category:cs.AI cs.LG stat.ML published:2016-02-25 summary:Learning models of artificial intelligence can nowadays perform very well ona large variety of tasks. However, in practice different task environments arebest handled by different learning models, rather than a single, universal,approach. Most non-trivial models thus require the adjustment of several tomany learning parameters, which is often done on a case-by-case basis by anexternal party. Meta-learning refers to the ability of an agent to autonomouslyand dynamically adjust its own learning parameters, or meta-parameters. In thiswork we show how projective simulation, a recently developed model ofartificial intelligence, can naturally be extended to account for meta-learningin reinforcement learning settings. The projective simulation approach is basedon a random walk process over a network of clips. The suggested meta-learningscheme builds upon the same design and employs clip networks to monitor theagent's performance and to adjust its meta-parameters "on the fly". Wedistinguish between "reflexive adaptation" and "adaptation through learning",and show the utility of both approaches. In addition, a trade-off betweenflexibility and learning-time is addressed. The extended model is examined onthree different kinds of reinforcement learning tasks, in which the agent hasdifferent optimal values of the meta-parameters, and is shown to perform well,reaching near-optimal to optimal success rates in all of them, without everneeding to manually adjust any meta-parameter.
arxiv-16500-180 | PCA/LDA Approach for Text-Independent Speaker Recognition | http://arxiv.org/abs/1602.08045 | author:Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith category:cs.SD cs.LG published:2016-02-25 summary:Various algorithms for text-independent speaker recognition have beendeveloped through the decades, aiming to improve both accuracy and e?ciency.This paper presents a novel PCA/LDA-based approach that is faster thantraditional statistical model-based methods and achieves competitive results.First, the performance based on only PCA and only LDA is measured; then a mixedmodel, taking advantages of both methods, is introduced. A subset of the TIMITcorpus composed of 200 male speakers, is used for enrollment, validation andtesting. The best results achieve 100%; 96% and 95% classi?cation rate atpopulation level 50; 100 and 200, using 39-dimensional MFCC features with deltaand double delta. These results are based on 12-second text-independent speechfor training and 4-second data for test. These are comparable to theconventional MFCC-GMM methods, but require signi?cantly less time to train andoperate.
arxiv-16500-181 | Recurrent Neural Network Grammars | http://arxiv.org/abs/1602.07776 | author:Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A. Smith category:cs.CL cs.NE published:2016-02-25 summary:We introduce recurrent neural network grammars, probabilistic models ofsentences with explicit phrase structure. We explain efficient inferenceprocedures that allow application to both parsing and language modeling.Experiments show that they provide better parsing in English than any singlepreviously published supervised generative model and better language modelingthan state-of-the-art sequential RNNs in English and Chinese.
arxiv-16500-182 | Reinforcement Learning of POMDP's using Spectral Methods | http://arxiv.org/abs/1602.07764 | author:Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar category:cs.AI cs.LG cs.NA math.OC stat.ML published:2016-02-25 summary:We propose a new reinforcement learning algorithm for partially observableMarkov decision processes (POMDP) based on spectral decomposition methods.While spectral methods have been previously employed for consistent learning of(passive) latent variable models such as hidden Markov models, POMDPs are morechallenging since the learner interacts with the environment and possiblychanges the future observations in the process. We devise a learning algorithmrunning through episodes, in each episode we employ spectral techniques tolearn the POMDP parameters from a trajectory generated by a fixed policy. Atthe end of the episode, an optimization oracle returns the optimal memorylessplanning policy which maximizes the expected reward based on the estimatedPOMDP model. We prove an order-optimal regret bound w.r.t. the optimalmemoryless policy and efficient scaling with respect to the dimensionality ofobservation and action spaces.
arxiv-16500-183 | Thompson Sampling is Asymptotically Optimal in General Environments | http://arxiv.org/abs/1602.07905 | author:Jan Leike, Tor Lattimore, Laurent Orseau, Marcus Hutter category:cs.LG cs.AI stat.ML published:2016-02-25 summary:We discuss a variant of Thompson sampling for nonparametric reinforcementlearning in a countable classes of general stochastic environments. Theseenvironments can be non-Markov, non-ergodic, and partially observable. We showthat Thompson sampling learns the environment class in the sense that (1)asymptotically its value converges to the optimal value in mean and (2) given arecoverability assumption regret is sublinear.
arxiv-16500-184 | Automated Word Prediction in Bangla Language Using Stochastic Language Models | http://arxiv.org/abs/1602.07803 | author:Md. Masudul Haque, Md. Tarek Habib, Md. Mokhlesur Rahman category:cs.CL published:2016-02-25 summary:Word completion and word prediction are two important phenomena in typingthat benefit users who type using keyboard or other similar devices. They canhave profound impact on the typing of disable people. Our work is based on wordprediction on Bangla sentence by using stochastic, i.e. N-gram language modelsuch as unigram, bigram, trigram, deleted Interpolation and backoff models forauto completing a sentence by predicting a correct word in a sentence whichsaves time and keystrokes of typing and also reduces misspelling. We use largedata corpus of Bangla language of different word types to predict correct wordwith the accuracy as much as possible. We have found promising results. We hopethat our work will impact on the baseline for automated Bangla typing.
arxiv-16500-185 | Learning to Abstain from Binary Prediction | http://arxiv.org/abs/1602.08151 | author:Akshay Balsubramani category:cs.LG stat.ML published:2016-02-25 summary:We address how to learn a binary classifier capable of abstaining from makinga label prediction. Such a classifier hopes to abstain where it would be mostinaccurate if forced to predict, so it has two goals in tension with eachother: minimizing errors, and avoiding abstaining unnecessarily often. In this work, we exactly characterize the best achievable tradeoff betweenthese two goals in a general semi-supervised setting, given an ensemble ofclassifiers of varying competence as well as unlabeled data on which we wish topredict or abstain. We give an algorithm for learning a classifier which tradesoff its errors with abstentions in a minimax optimal manner. This algorithm isas efficient as linear learning and prediction, and comes with strong androbust theoretical guarantees. Our analysis extends to a large class of lossfunctions and other scenarios, including ensembles comprised of "specialist"classifiers that can themselves abstain.
arxiv-16500-186 | Firefly Algorithm for optimization problems with non-continuous variables: A Review and Analysis | http://arxiv.org/abs/1602.07884 | author:Surafel Luleseged Tilahun, Jean Medard T Ngnotchouye category:cs.NE published:2016-02-25 summary:Firefly algorithm is a swarm based metaheuristic algorithm inspired by theflashing behavior of fireflies. It is an effective and an easy to implementalgorithm. It has been tested on different problems from different disciplinesand found to be effective. Even though the algorithm is proposed foroptimization problems with continuous variables, it has been modified and usedfor problems with non-continuous variables, including binary and integer valuedproblems. In this paper a detailed review of this modifications of fireflyalgorithm for problems with non-continuous variables will be discussed. Thestrength and weakness of the modifications along with possible future workswill be presented.
arxiv-16500-187 | CNN for License Plate Motion Deblurring | http://arxiv.org/abs/1602.07873 | author:Pavel Svoboda, Michal Hradis, Lukas Marsik, Pavel Zemcik category:cs.CV published:2016-02-25 summary:In this work we explore the previously proposed approach of direct blinddeconvolution and denoising with convolutional neural networks in a situationwhere the blur kernels are partially constrained. We focus on blurred imagesfrom a real-life traffic surveillance system, on which we, for the first time,demonstrate that neural networks trained on artificial data provide superiorreconstruction quality on real images compared to traditional blinddeconvolution methods. The training data is easy to obtain by blurring sharpphotos from a target system with a very rough approximation of the expectedblur kernels, thereby allowing custom CNNs to be trained for a specificapplication (image content and blur range). Additionally, we evaluate thebehavior and limits of the CNNs with respect to blur direction range andlength.
arxiv-16500-188 | Projected Estimators for Robust Semi-supervised Classification | http://arxiv.org/abs/1602.07865 | author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG published:2016-02-25 summary:For semi-supervised techniques to be applied safely in practice we at leastwant methods to outperform their supervised counterparts. We study thisquestion for classification using the well-known quadratic surrogate lossfunction. Using a projection of the supervised estimate onto a set ofconstraints imposed by the unlabeled data, we find we can safely improve overthe supervised solution in terms of this quadratic loss. Unlike otherapproaches to semi-supervised learning, the procedure does not rely onassumptions that are not intrinsic to the classifier at hand. It istheoretically demonstrated that, measured on the labeled and unlabeled trainingdata, this semi-supervised procedure never gives a lower quadratic loss thanthe supervised alternative. To our knowledge this is the first approach thatoffers such strong, albeit conservative, guarantees for improvement over thesupervised solution. The characteristics of our approach are explicated usingbenchmark datasets to further understand the similarities and differencesbetween the quadratic loss criterion used in the theoretical results and theclassification accuracy often considered in practice.
arxiv-16500-189 | Learning Gaussian Graphical Models With Fractional Marginal Pseudo-likelihood | http://arxiv.org/abs/1602.07863 | author:Janne Leppä-aho, Johan Pensar, Teemu Roos, Jukka Corander category:stat.ML cs.LG published:2016-02-25 summary:We propose a Bayesian approximate inference method for learning thedependence structure of a Gaussian graphical model. Using pseudo-likelihood, wederive an analytical expression to approximate the marginal likelihood for anarbitrary graph structure without invoking any assumptions aboutdecomposability. The majority of the existing methods for learning Gaussiangraphical models are either restricted to decomposable graphs or requirespecification of a tuning parameter that may have a substantial impact onlearned structures. By combining a simple sparsity inducing prior for the graphstructures with a default reference prior for the model parameters, we obtain afast and easily applicable scoring function that works well for evenhigh-dimensional data. We demonstrate the favourable performance of ourapproach by large-scale comparisons against the leading methods for learningnon-decomposable Gaussian graphical models. A theoretical justification for ourmethod is provided by showing that it yields a consistent estimator of thegraph structure.
arxiv-16500-190 | Virtualizing Deep Neural Networks for Memory-Efficient Neural Network Design | http://arxiv.org/abs/1602.08124 | author:Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, Stephen W. Keckler category:cs.DC cs.LG cs.NE published:2016-02-25 summary:The most widely used machine learning frameworks require users to carefullytune their memory usage so that the deep neural network (DNN) fits into theDRAM capacity of a GPU. This restriction hampers a researcher's flexibility tostudy different machine learning algorithms, forcing them to either use a lessdesirable network architecture or parallelize the processing across multipleGPUs. We propose a runtime memory manager that virtualizes the memory usage ofDNNs such that both GPU and CPU memory can simultaneously be utilized fortraining larger DNNs. Our virtualized DNN (vDNN) reduces the average memoryusage of AlexNet by 61% and OverFeat by 83%, a significant reduction in memoryrequirements of DNNs. Similar experiments on VGG-16, one of the deepest andmemory hungry DNNs to date, demonstrate the memory-efficiency of our proposal.vDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to betrained on a single NVIDIA K40 GPU card containing 12 GB of memory, with 22%performance loss compared to a hypothetical GPU with enough memory to hold theentire DNN.
arxiv-16500-191 | Autonomous navigation for low-altitude UAVs in urban areas | http://arxiv.org/abs/1602.08141 | author:Thomas Castelli, Aidean Sharghi, Don Harper, Alain Tremeau, Mubarak Shah category:cs.RO cs.CV published:2016-02-25 summary:In recent years, consumer Unmanned Aerial Vehicles have become very popular,everyone can buy and fly a drone without previous experience, which raisesconcern in regards to regulations and public safety. In this paper, we presenta novel approach towards enabling safe operation of such vehicles in urbanareas. Our method uses geodetically accurate dataset images with GeographicalInformation System (GIS) data of road networks and buildings provided by GoogleMaps, to compute a weighted A* shortest path from start to end locations of amission. Weights represent the potential risk of injuries for individuals inall categories of land-use, i.e. flying over buildings is considered safer thanabove roads. We enable safe UAV operation in regards to 1- land-use bycomputing a static global path dependent on environmental structures, and 2-avoiding flying over moving objects such as cars and pedestrians by dynamicallyoptimizing the path locally during the flight. As all input sources are firstgeo-registered, pixels and GPS coordinates are equivalent, it therefore allowsus to generate an automated and user-friendly mission with GPS waypointsreadable by consumer drones' autopilots. We simulated 54 missions and showsignificant improvement in maximizing UAV's standoff distance to moving objectswith a quantified safety parameter over 40 times better than the naive straightline navigation.
arxiv-16500-192 | Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep Neural Network | http://arxiv.org/abs/1602.08118 | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2016-02-25 summary:Recurrent neural networks (RNN) are capable of learning to encode and exploitactivation history over an arbitrary timescale. However, in practice, state ofthe art gradient descent based training methods are known to suffer fromdifficulties in learning long term dependencies. Here, we describe a noveltraining method that involves concurrent parallel cloned networks, each sharingthe same weights, each trained at different stimulus phase and each maintainingindependent activation histories. Training proceeds by recursively performingbatch-updates over the parallel clones as activation history is progressivelyincreased. This allows conflicts to propagate hierarchically from short-termcontexts towards longer-term contexts until they are resolved. We illustratethe parallel clones method and hierarchical conflict propagation with acharacter-level deep RNN tasked with memorizing a paragraph of Moby Dick (byHerman Melville).
arxiv-16500-193 | A Bayesian baseline for belief in uncommon events | http://arxiv.org/abs/1602.07836 | author:V. Palonen category:stat.AP stat.ML stat.OT published:2016-02-25 summary:The plausibility of uncommon events and miracles based on testimony of suchan event has been much discussed. When analyzing the probabilities involved, ithas mostly been assumed that the common events can be taken as data in thecalculations. However, we usually have only testimonies for the common events.While this difference does not have a significant effect on the inductive partof the inference, it has a large influence on how one should view thereliability of testimonies. In this work, a full Bayesian solution is given forthe more realistic case, where one has a large number of testimonies for acommon event and one testimony for an uncommon event. It is seen that, in orderfor there to be a large amount of testimonies for a common event, thetestimonies will probably be quite reliable. For this reason, because thetestimonies are quite reliable based on the testimonies for the common events,the probability for the uncommon event, given a testimony for it, is alsohigher. Hence, one should be more open-minded when considering the plausibilityof uncommon events.
arxiv-16500-194 | PCA Method for Automated Detection of Mispronounced Words | http://arxiv.org/abs/1602.08128 | author:Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith category:cs.SD cs.CL cs.LG published:2016-02-25 summary:This paper presents a method for detecting mispronunciations with the aim ofimproving Computer Assisted Language Learning (CALL) tools used by foreignlanguage learners. The algorithm is based on Principle Component Analysis(PCA). It is hierarchical with each successive step refining the estimate toclassify the test word as being either mispronounced or correct. Preprocessingbefore detection, like normalization and time-scale modification, isimplemented to guarantee uniformity of the feature vectors input to thedetection system. The performance using various features including spectrogramsand Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated.Best results were obtained using MFCCs, achieving up to 99% accuracy in wordverification and 93% in native/non-native classification. Compared with HiddenMarkov Models (HMMs) which are used pervasively in recognition application,this particular approach is computational efficient and effective when trainingdata is limited.
arxiv-16500-195 | Probably Approximately Correct Greedy Maximization | http://arxiv.org/abs/1602.07860 | author:Yash Satsangi, Shimon Whiteson, Frans A. Oliehoek category:cs.AI cs.LG stat.ML published:2016-02-25 summary:Submodular function maximization finds application in a variety of real-worlddecision-making problems. However, most existing methods, based on greedymaximization, assume it is computationally feasible to evaluate F, the functionbeing maximized. Unfortunately, in many realistic settings F is too expensiveto evaluate exactly even once. We present probably approximately correct greedymaximization, which requires access only to cheap anytime confidence bounds onF and uses them to prune elements. We show that, with high probability, ourmethod returns an approximately optimal set. We propose novel, cheap confidencebounds for conditional entropy, which appears in many common choices of F andfor which it is difficult to find unbiased or bounded estimates. Finally,results on a real-world dataset from a multi-camera tracking system in ashopping mall demonstrate that our approach performs comparably to existingmethods, but at a fraction of the computational cost.
arxiv-16500-196 | Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly Detection | http://arxiv.org/abs/1602.07807 | author:Michael Bloodgood, Benjamin Strauss category:cs.DB cs.CL stat.ML published:2016-02-25 summary:Many important forms of data are stored digitally in XML format. Errors canoccur in the textual content of the data in the fields of the XML. Fixing theseerrors manually is time-consuming and expensive, especially for large amountsof data. There is increasing interest in the research, development, and use ofautomated techniques for assisting with data cleaning. Electronic dictionariesare an important form of data frequently stored in XML format that frequentlyhave errors introduced through a mixture of manual typographical entry errorsand optical character recognition errors. In this paper we describe methods forflagging statistical anomalies as likely errors in electronic dictionariesstored in XML format. We describe six systems based on different sources ofinformation. The systems detect errors using various signals in the dataincluding uncommon characters, text length, character-based language models,word-based language models, tied-field length ratios, and tied-fieldtransliteration models. Four of the systems detect errors based on expectationsautomatically inferred from content within elements of a single field type. Wecall these single-field systems. Two of the systems detect errors based oncorrespondence expectations automatically inferred from content within elementsof multiple related field types. We call these tied-field systems. For eachsystem, we provide an intuitive analysis of the type of error that it issuccessful at detecting. Finally, we describe two larger-scale evaluationsusing crowdsourcing with Amazon's Mechanical Turk platform and using theannotations of a domain expert. The evaluations consistently show that thesystems are useful for improving the efficiency with which errors in XMLelectronic dictionaries can be detected.
arxiv-16500-197 | Adaptive Frequency Cepstral Coefficients for Word Mispronunciation Detection | http://arxiv.org/abs/1602.08132 | author:Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith category:cs.SD cs.CV published:2016-02-25 summary:Systems based on automatic speech recognition (ASR) technology can provideimportant functionality in computer assisted language learning applications.This is a young but growing area of research motivated by the large number ofstudents studying foreign languages. Here we propose a Hidden Markov Model(HMM)-based method to detect mispronunciations. Exploiting the specific dialogscripting employed in language learning software, HMMs are trained fordifferent pronunciations. New adaptive features have been developed andobtained through an adaptive warping of the frequency scale prior to computingthe cepstral coefficients. The optimization criterion used for the warpingfunction is to maximize separation of two major groups of pronunciations(native and non-native) in terms of classification rate. Experimental resultsshow that the adaptive frequency scale yields a better coefficientrepresentation leading to higher classification rates in comparison withconventional HMMs using Mel-frequency cepstral coefficients.
arxiv-16500-198 | Fast Nonsmooth Regularized Risk Minimization with Continuation | http://arxiv.org/abs/1602.07844 | author:Shuai Zheng, Ruiliang Zhang, James T. Kwok category:cs.LG published:2016-02-25 summary:In regularized risk minimization, the associated optimization problem becomesparticularly difficult when both the loss and regularizer are nonsmooth.Existing approaches either have slow or unclear convergence properties, arerestricted to limited problem subclasses, or require careful setting of asmoothing parameter. In this paper, we propose a continuation algorithm that isapplicable to a large class of nonsmooth regularized risk minimizationproblems, can be flexibly used with a number of existing solvers for theunderlying smoothed subproblem, and with convergence results on the wholealgorithm rather than just one of its subproblems. In particular, whenaccelerated solvers are used, the proposed algorithm achieves the fastest knownrates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convexproblems. Experiments on nonsmooth classification and regression tasksdemonstrate that the proposed algorithm outperforms the state-of-the-art.
arxiv-16500-199 | Expectation Consistent Approximate Inference: Generalizations and Convergence | http://arxiv.org/abs/1602.07795 | author:Alyson Fletcher, Mojtaba Sahraee-Ardakan, Sundeep Rangan, Philip Schniter category:cs.IT math.IT stat.ML published:2016-02-25 summary:Approximations of loopy belief propagation, including expectation propagationand approximate message passing, have attracted considerable attention forprobabilistic inference problems. This paper proposes and analyzes ageneralization of Opper and Winther's expectation consistent (EC) approximateinference method. The proposed method, called Generalized ExpectationConsistency (GEC), can be applied to both maximum a posteriori (MAP) andminimum mean squared error (MMSE) estimation. Here we characterize its fixedpoints, convergence, and performance relative to the replica prediction ofoptimality.
arxiv-16500-200 | Monomial Gamma Monte Carlo Sampling | http://arxiv.org/abs/1602.07800 | author:Yizhe Zhang, Xiangyu Wang, Changyou Chen, Kai Fan, Lawrence Carin category:stat.ML stat.ME published:2016-02-25 summary:We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling bydemonstrating their connection under the canonical transformation fromHamiltonian mechanics. This insight enables us to extend HMC and slice samplingto a broader family of samplers, called monomial Gamma samplers (MGS). Weanalyze theoretically the mixing performance of such samplers by proving thatthe MGS draws samples from a target distribution with zero-autocorrelation, inthe limit of a single parameter. This property potentially allows us togenerating decorrelated samples, which is not achievable by existing MCMCalgorithms. We further show that this performance gain is obtained at a cost ofincreasing the complexity of numerical integrators. Our theoretical results arevalidated with synthetic data and real-world applications.
arxiv-16500-201 | Top-N Recommendation with Novel Rank Approximation | http://arxiv.org/abs/1602.07783 | author:Zhao Kang, Qiang Cheng category:cs.IR cs.AI stat.ML published:2016-02-25 summary:The importance of accurate recommender systems has been widely recognized byacademia and industry. However, the recommendation quality is still rather low.Recently, a linear sparse and low-rank representation of the user-item matrixhas been applied to produce Top-N recommendations. This approach uses thenuclear norm as a convex relaxation for the rank function and has achievedbetter recommendation accuracy than the state-of-the-art methods. In the pastseveral years, solving rank minimization problems by leveraging nonconvexrelaxations has received increasing attention. Some empirical resultsdemonstrate that it can provide a better approximation to original problemsthan convex relaxation. In this paper, we propose a novel rank approximation toenhance the performance of Top-N recommendation systems, where theapproximation error is controllable. Experimental results on real data showthat the proposed rank approximation improves the Top-$N$ recommendationaccuracy substantially.
arxiv-16500-202 | Learning functions across many orders of magnitudes | http://arxiv.org/abs/1602.07714 | author:Hado van Hasselt, Arthur Guez, Matteo Hessel, David Silver category:cs.LG cs.AI cs.NE stat.ML published:2016-02-24 summary:Learning non-linear functions can be hard when the magnitude of the targetfunction is unknown beforehand, as most learning algorithms are not scaleinvariant. We propose an algorithm to adaptively normalize these targets. Thisis complementary to recent advances in input normalization. Importantly, theproposed method preserves the unnormalized outputs whenever the normalizationis updated to avoid instability caused by non-stationarity. It can be combinedwith any learning algorithm and any non-linear function approximation,including the important special case of deep learning. We empirically validatethe method in supervised learning and reinforcement learning and apply it tolearning how to play Atari 2600 games. Previous work on applying deep learningto this domain relied on clipping the rewards to make learning in differentgames more homogeneous, but this uses the domain-specific knowledge that inthese games counting rewards is often almost as informative as summing these.Using our adaptive normalization we can remove this heuristic withoutdiminishing overall performance, and even improve performance on some games,such as Ms. Pac-Man and Centipede, on which previous methods did not performwell.
arxiv-16500-203 | Online Dual Coordinate Ascent Learning | http://arxiv.org/abs/1602.07630 | author:Bicheng Ying, Kun Yuan, Ali H. Sayed category:math.OC cs.LG stat.ML published:2016-02-24 summary:The stochastic dual coordinate-ascent (S-DCA) technique is a usefulalternative to the traditional stochastic gradient-descent algorithm forsolving large-scale optimization problems due to its scalability to large datasets and strong theoretical guarantees. However, the available S-DCAformulation is limited to finite sample sizes and relies on performing multiplepasses over the same data. This formulation is not well-suited for onlineimplementations where data keep streaming in. In this work, we develop an {\emonline} dual coordinate-ascent (O-DCA) algorithm that is able to respond tostreaming data and does not need to revisit the past data. This feature embedsthe resulting construction with continuous adaptation, learning, and trackingabilities, which are particularly attractive for online learning scenarios.
arxiv-16500-204 | Noisy population recovery in polynomial time | http://arxiv.org/abs/1602.07616 | author:Anindya De, Michael Saks, Sijian Tang category:cs.CC cs.DS cs.LG published:2016-02-24 summary:In the noisy population recovery problem of Dvir et al., the goal is to learnan unknown distribution $f$ on binary strings of length $n$ from noisy samples.For some parameter $\mu \in [0,1]$, a noisy sample is generated by flippingeach coordinate of a sample from $f$ independently with probability$(1-\mu)/2$. We assume an upper bound $k$ on the size of the support of thedistribution, and the goal is to estimate the probability of any string towithin some given error $\varepsilon$. It is known that the algorithmiccomplexity and sample complexity of this problem are polynomially related toeach other. We show that for $\mu > 0$, the sample complexity (and hence the algorithmiccomplexity) is bounded by a polynomial in $k$, $n$ and $1/\varepsilon$improving upon the previous best result of $\mathsf{poly}(k^{\log\logk},n,1/\varepsilon)$ due to Lovett and Zhang. Our proof combines ideas from Lovett and Zhang with a \emph{noise attenuated}version of M\"{o}bius inversion. In turn, the latter crucially uses theconstruction of \emph{robust local inverse} due to Moitra and Saks.
arxiv-16500-205 | Bayesian Exploration: Incentivizing Exploration in Bayesian Games | http://arxiv.org/abs/1602.07570 | author:Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, Zhiwei Steven Wu category:cs.GT cs.DS cs.LG published:2016-02-24 summary:We consider a ubiquitous scenario in the Internet economy when individualdecision-makers (henceforth, agents) both produce and consume information asthey make strategic choices in an uncertain environment. This creates athree-way tradeoff between exploration (trying out insufficiently exploredalternatives to help others in the future), exploitation (making optimaldecisions given the information discovered by other agents), and incentives ofthe agents (who are myopically interested in exploitation, while preferring theothers to explore). We posit a principal who controls the flow of informationfrom agents that came before, and strives to coordinate the agents towards asocially optimal balance between exploration and exploitation, not using anymonetary transfers. The goal is to design a recommendation policy for theprincipal which respects agents' incentives and minimizes a suitable notion ofregret. We extend prior work in this direction to allow the agents to interact withone another in a shared environment: at each time step, multiple agents arriveto play a Bayesian game, receive recommendations, choose their actions, receivetheir payoffs, and then leave the game forever. The agents now face two sourcesof uncertainty: the actions of the other agents and the parameters of theuncertain game environment. Our main contribution is to show that the principal can achieve constantregret when the utilities are deterministic (where the constant depends on theprior distribution, but not on the time horizon), and logarithmic regret whenthe utilities are stochastic. As a key technical tool, we introduce the conceptof explorable actions, the actions which some incentive-compatible policy canrecommend with non-zero probability. We show how the principal can identify(and explore) all explorable actions, and use the revealed information toperform optimally.
arxiv-16500-206 | Adaptive Learning with Robust Generalization Guarantees | http://arxiv.org/abs/1602.07726 | author:Rachel Cummings, Katrina Ligett, Kobbi Nissim, Aaron Roth, Zhiwei Steven Wu category:cs.DS cs.LG published:2016-02-24 summary:The traditional notion of generalization --- i.e., learning a hypothesiswhose empirical error is close to its true error --- is surprisingly brittle.As has recently been noted [DFH+15b], even if several algorithms have thisguarantee in isolation, the guarantee need not hold if the algorithms arecomposed adaptively. In this paper, we study three notions of generalization---increasing in strength--- that are robust to post-processing and amenable toadaptive composition, and examine the relationships between them. We call the weakest such notion Robust Generalization. A second,intermediate, notion is the stability guarantee known as differential privacy.The strongest guarantee we consider we call Perfect Generalization. We provethat every hypothesis class that is PAC learnable is also PAC learnable in arobustly generalizing fashion, albeit with an exponential blowup in samplecomplexity. We conjecture that a stronger version of this theorem also holdsthat avoids any blowup in sample complexity (and, in fact, it would, subject toa longstanding conjecture [LW86, War03]). It was previously known thatdifferentially private algorithms satisfy robust generalization. In this paper,we show that robust generalization is a strictly weaker concept, and that thereis a learning task that can be carried out subject to robust generalizationguarantees, yet cannot be carried out subject to differential privacy,answering an open question of [DFH+15a]. We also show that perfectgeneralization is a strictly stronger guarantee than differential privacy, butthat, nevertheless, many learning tasks can be carried out subject to theguarantees of perfect generalization.
arxiv-16500-207 | On the Accuracy of Point Localisation in a Circular Camera-Array | http://arxiv.org/abs/1602.07542 | author:Alireza Ghasemi, Adam Scholefield, Martin Vetterli category:cs.CV published:2016-02-24 summary:Although many advances have been made in light-field and camera-array imageprocessing, there is still a lack of thorough analysis of the localisationaccuracy of different multi-camera systems. By considering the problem from aframe-quantisation perspective, we are able to quantify the point localisationerror of circular camera configurations. Specifically, we obtain closed formexpressions bounding the localisation error in terms of the parametersdescribing the acquisition setup. These theoretical results are independent of the localisation algorithm andthus provide fundamental limits on performance. Furthermore, the newframe-quantisation perspective is general enough to be extended to more complexcamera configurations.
arxiv-16500-208 | SHAPE: Linear-Time Camera Pose Estimation With Quadratic Error-Decay | http://arxiv.org/abs/1602.07535 | author:Alireza Ghasemi, Adam Scholefield, Martin Vetterli category:cs.CV published:2016-02-24 summary:We propose a novel camera pose estimation or perspective-n-point (PnP)algorithm, based on the idea of consistency regions and half-spaceintersections. Our algorithm has linear time-complexity and a squaredreconstruction error that decreases at least quadratically, as the number offeature point correspondences increase. Inspired by ideas from triangulation and frame quantisation theory, we defineconsistent reconstruction and then present SHAPE, our proposed consistent poseestimation algorithm. We compare this algorithm with state-of-the-art poseestimation techniques in terms of accuracy and error decay rate. Theexperimental results verify our hypothesis on the optimal worst-case quadraticdecay and demonstrate its promising performance compared to other approaches.
arxiv-16500-209 | A Bayesian Approach to the Data Description Problem | http://arxiv.org/abs/1602.07507 | author:Alireza Ghasemi, Hamid R. Rabiee, Mohammad T. Manzuri, M. H. Rohban category:cs.LG published:2016-02-24 summary:In this paper, we address the problem of data description using a Bayesianframework. The goal of data description is to draw a boundary around objects ofa certain class of interest to discriminate that class from the rest of thefeature space. Data description is also known as one-class learning and has awide range of applications. The proposed approach uses a Bayesian framework to precisely compute theclass boundary and therefore can utilize domain information in form of priorknowledge in the framework. It can also operate in the kernel space andtherefore recognize arbitrary boundary shapes. Moreover, the proposed methodcan utilize unlabeled data in order to improve accuracy of discrimination. We evaluate our method using various real-world datasets and compare it withother state of the art approaches of data description. Experiments showpromising results and improved performance over other data description andone-class learning algorithms.
arxiv-16500-210 | Active Learning from Positive and Unlabeled Data | http://arxiv.org/abs/1602.07495 | author:Alireza Ghasemi, Hamid R. Rabiee, Mohsen Fadaee, Mohammad T. Manzuri, Mohammad H. Rohban category:cs.LG published:2016-02-24 summary:During recent years, active learning has evolved into a popular paradigm forutilizing user's feedback to improve accuracy of learning algorithms. Activelearning works by selecting the most informative sample among unlabeled dataand querying the label of that point from user. Many different methods such asuncertainty sampling and minimum risk sampling have been utilized to select themost informative sample in active learning. Although many active learningalgorithms have been proposed so far, most of them work with binary ormulti-class classification problems and therefore can not be applied toproblems in which only samples from one class as well as a set of unlabeleddata are available. Such problems arise in many real-world situations and are known as theproblem of learning from positive and unlabeled data. In this paper we proposean active learning algorithm that can work when only samples of one class aswell as a set of unlabelled data are available. Our method works by separatelyestimating probability desnity of positive and unlabeled points and thencomputing expected value of informativeness to get rid of a hyper-parameter andhave a better measure of informativeness./ Experiments and empirical analysisshow promising results compared to other similar methods.
arxiv-16500-211 | Boosting patch-based scene text script identification with ensembles of conjoined networks | http://arxiv.org/abs/1602.07480 | author:Lluis Gomez, Anguelos Nicolaou, Dimosthenis Karatzas category:cs.CV published:2016-02-24 summary:This paper focuses on the problem of script identification in scene textimages. Facing this problem with state of the art CNN classifiers is notstraightforward, as they fail to address a key characteristic of scene textinstances: their extremely variable aspect ratio. Instead of resizing inputimages to a fixed aspect ratio as in the typical use of holistic CNNclassifiers, we propose here a patch-based classification framework in order topreserve discriminative parts of the image that are characteristic of itsclass. We describe a novel method based on the use of ensembles of conjoinednetworks to jointly learn discriminative stroke-parts representations and theirrelative importance in a patch-based classification scheme. Our experimentswith this learning procedure demonstrate state-of-the-art results in two publicscript identification datasets. In addition, we propose a new public benchmark dataset for the evaluation ofmulti-lingual scene text end-to-end reading systems. Experiments done in thisdataset demonstrate the key role of script identification in a completeend-to-end system that combines our script identification method with apreviously published text detector and an off-the-shelf OCR engine.
arxiv-16500-212 | Automatically Proving Mathematical Theorems with Evolutionary Algorithms and Proof Assistants | http://arxiv.org/abs/1602.07455 | author:Li-An Yang, Jui-Pin Liu, Chao-Hong Chen, Ying-ping Chen category:cs.NE cs.LO published:2016-02-24 summary:Mathematical theorems are human knowledge able to be accumulated in the formof symbolic representation, and proving theorems has been consideredintelligent behavior. Based on the BHK interpretation and the Curry-Howardisomorphism, proof assistants, software capable of interacting with human forconstructing formal proofs, have been developed in the past several decades.Since proofs can be considered and expressed as programs, proof assistantssimplify and verify a proof by computationally evaluating the programcorresponding to the proof. Thanks to the transformation from logic tocomputation, it is now possible to generate or search for formal proofsdirectly in the realm of computation. Evolutionary algorithms, known to beflexible and versatile, have been successfully applied to handle a variety ofscientific and engineering problems in numerous disciplines for also severaldecades. Examining the feasibility of establishing the link betweenevolutionary algorithms, as the program generator, and proof assistants, as theproof verifier, in order to automatically find formal proofs to a given logicsentence is the primary goal of this study. In the article, we describe indetail our first, ad-hoc attempt to fully automatically prove theorems as wellas the preliminary results. Ten simple theorems from various branches ofmathematics were proven, and most of these theorems cannot be proven by usingthe tactic auto alone in Coq, the adopted proof assistant. The implication andpotential influence of this study are discussed, and the developed source codewith the obtained experimental results are released as open source.
arxiv-16500-213 | A fine-grained approach to scene text script identification | http://arxiv.org/abs/1602.07475 | author:Lluis Gomez, Dimosthenis Karatzas category:cs.CV published:2016-02-24 summary:This paper focuses on the problem of script identification in unconstrainedscenarios. Script identification is an important prerequisite to recognition,and an indispensable condition for automatic text understanding systemsdesigned for multi-language environments. Although widely studied for documentimages and handwritten documents, it remains an almost unexplored territory forscene text images. We detail a novel method for script identification in natural images thatcombines convolutional features and the Naive-Bayes Nearest Neighborclassifier. The proposed framework efficiently exploits the discriminativepower of small stroke-parts, in a fine-grained classification framework. In addition, we propose a new public benchmark dataset for the evaluation ofjoint text detection and script identification in natural scenes. Experimentsdone in this new dataset demonstrate that the proposed method yields state ofthe art results, while it generalizes well to different datasets and variablenumber of scripts. The evidence provided shows that multi-lingual scene textrecognition in the wild is a viable proposition. Source code of the proposedmethod is made available online.
arxiv-16500-214 | Asymptotic consistency and order specification for logistic classifier chains in multi-label learning | http://arxiv.org/abs/1602.07466 | author:Paweł Teisseyre category:cs.LG stat.ML published:2016-02-24 summary:Classifier chains are popular and effective method to tackle a multi-labelclassification problem. The aim of this paper is to study the asymptoticproperties of the chain model in which the conditional probabilities are of thelogistic form. In particular we find conditions on the number of labels and thedistribution of feature vector under which the estimated mode of the jointdistribution of labels converges to the true mode. Best of our knowledge, thisimportant issue has not yet been studied in the context of multi-labellearning. We also investigate how the order of model building in a chaininfluences the estimation of the joint distribution of labels. We establish thelink between the problem of incorrect ordering in the chain and incorrect modelspecification. We propose a procedure of determining the optimal ordering oflabels in the chain, which is based on using measures of correct specificationand allows to find the ordering such that the consecutive logistic models arebest possibly specified. The other important question raised in this paper ishow accurately can we estimate the joint posterior probability when theordering of labels is wrong or the logistic models in the chain are incorrectlyspecified. The numerical experiments illustrate the theoretical results.
arxiv-16500-215 | Feature ranking for multi-label classification using Markov Networks | http://arxiv.org/abs/1602.07464 | author:Paweł Teisseyre category:cs.LG stat.ML published:2016-02-24 summary:We propose a simple and efficient method for ranking features in multi-labelclassification. The method produces a ranking of features showing theirrelevance in predicting labels, which in turn allows to choose a final subsetof features. The procedure is based on Markov Networks and allows to model thedependencies between labels and features in a direct way. In the first step webuild a simple network using only labels and then we test how much adding asingle feature affects the initial network. More specifically, in the firststep we use the Ising model whereas the second step is based on the scorestatistic, which allows to test a significance of added features very quickly.The proposed approach does not require transformation of label space, givesinterpretable results and allows for attractive visualization of dependencystructure. We give a theoretical justification of the procedure by discussingsome theoretical properties of the Ising model and the score statistic. We alsodiscuss feature ranking procedure based on fitting Ising model using $l_1$regularized logistic regressions. Numerical experiments show that the proposedmethods outperform the conventional approaches on the considered artificial andreal datasets.
arxiv-16500-216 | Max-Margin Nonparametric Latent Feature Models for Link Prediction | http://arxiv.org/abs/1602.07428 | author:Jun Zhu, Jiaming Song, Bei Chen category:cs.LG cs.SI stat.ME stat.ML published:2016-02-24 summary:Link prediction is a fundamental task in statistical network analysis. Recentadvances have been made on learning flexible nonparametric Bayesian latentfeature models for link prediction. In this paper, we present a max-marginlearning method for such nonparametric latent feature relational models. Ourapproach attempts to unite the ideas of max-margin learning and Bayesiannonparametrics to discover discriminative latent features for link prediction.It inherits the advances of nonparametric Bayesian methods to infer the unknownlatent social dimension, while for discriminative link prediction, it adoptsthe max-margin learning principle by minimizing a hinge-loss using the linearexpectation operator, without dealing with a highly nonlinear link likelihoodfunction. For posterior inference, we develop an efficient stochasticvariational inference algorithm under a truncated mean-field assumption. Ourmethods can scale up to large-scale real networks with millions of entities andtens of millions of positive links. We also provide a full Bayesianformulation, which can avoid tuning regularization hyper-parameters.Experimental results on a diverse range of real datasets demonstrate thebenefits inherited from max-margin learning and Bayesian nonparametricinference.
arxiv-16500-217 | Learning to Generate with Memory | http://arxiv.org/abs/1602.07416 | author:Chongxuan Li, Jun Zhu, Bo Zhang category:cs.LG cs.CV published:2016-02-24 summary:Memory units have been widely used to enrich the capabilities of deepnetworks on capturing long-term dependencies in reasoning and prediction tasks,but little investigation exists on deep generative models (DGMs) which are goodat inferring high-level invariant representations from unlabeled data. Thispaper presents a deep generative model with a possibly large external memoryand an attention mechanism to capture the local detail information that isoften lost in the bottom-up abstraction process in representation learning. Byadopting a smooth attention model, the whole network is trained end-to-end byoptimizing a variational bound of data likelihood via auto-encoding variationalBayesian methods, where an asymmetric recognition network is learnt jointly toinfer high-level invariant representations. The asymmetric architecture canreduce the competition between bottom-up invariant feature extraction andtop-down generation of instance details. Our experiments on several datasetsdemonstrate that memory can significantly boost the performance of DGMs andeven achieve state-of-the-art results on various tasks, including densityestimation, image generation, and missing value imputation.
arxiv-16500-218 | Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling | http://arxiv.org/abs/1602.07415 | author:Christopher De Sa, Kunle Olukotun, Christopher Ré category:cs.LG published:2016-02-24 summary:Gibbs sampling is a Markov Chain Monte Carlo technique commonly used forestimating marginal distributions. To speed up Gibbs sampling, there hasrecently been interest in parallelizing it by executing asynchronously. Whileempirical results suggest that many models can be efficiently sampledasynchronously, traditional Markov chain analysis does not apply to theasynchronous case, and thus asynchronous Gibbs sampling is poorly understood.In this paper, we derive a better understanding of the two main challenges ofasynchronous Gibbs: sampling bias and mixing time. We show experimentally thatour theoretical results match practical outcomes.
arxiv-16500-219 | On Study of the Binarized Deep Neural Network for Image Classification | http://arxiv.org/abs/1602.07373 | author:Song Wang, Dongchun Ren, Li Chen, Wei Fan, Jun Sun, Satoshi Naoi category:cs.NE cs.CV cs.LG published:2016-02-24 summary:Recently, the deep neural network (derived from the artificial neuralnetwork) has attracted many researchers' attention by its outstandingperformance. However, since this network requires high-performance GPUs andlarge storage, it is very hard to use it on individual devices. In order toimprove the deep neural network, many trials have been made by refining thenetwork structure or training strategy. Unlike those trials, in this paper, wefocused on the basic propagation function of the artificial neural network andproposed the binarized deep neural network. This network is a pure binarysystem, in which all the values and calculations are binarized. As a result,our network can save a lot of computational resource and storage. Therefore, itis possible to use it on various devices. Moreover, the experimental resultsproved the feasibility of the proposed network.
arxiv-16500-220 | Improved Accent Classification Combining Phonetic Vowels with Acoustic Features | http://arxiv.org/abs/1602.07394 | author:Zhenhao Ge category:cs.SD cs.CL published:2016-02-24 summary:Researches have shown accent classification can be improved by integratingsemantic information into pure acoustic approach. In this work, we combinephonetic knowledge, such as vowels, with enhanced acoustic features to build animproved accent classification system. The classifier is based on GaussianMixture Model-Universal Background Model (GMM-UBM), with normalized PerceptualLinear Predictive (PLP) features. The features are further optimized byPrinciple Component Analysis (PCA) and Hetroscedastic Linear DiscriminantAnalysis (HLDA). Using 7 major types of accented speech from the ForeignAccented English (FAE) corpus, the system achieves classification accuracy 54%with input test data as short as 20 seconds, which is competitive to the stateof the art in this field.
arxiv-16500-221 | A Compressed Sensing Based Decomposition of Electro-Dermal Activity Signals | http://arxiv.org/abs/1602.07754 | author:Swayambhoo Jain, Urvashi Oswal, Kevin S. Xu, Brian Eriksson, Jarvis Haupt category:stat.ML published:2016-02-24 summary:The measurement and analysis of Electro-Dermal Activity (EDA) offersapplications in diverse areas ranging from market research, to seizuredetection, to human stress analysis. Unfortunately, the analysis of EDA signalsis made difficult by the superposition of numerous components which can obscurethe signal information related to a user's response to a stimulus. We show howsimple pre-processing followed by a novel compressed sensing baseddecomposition can mitigate the effects of these noise components and helpreveal the underlying physiological signal. The proposed framework allows forfast decomposition of EDA signals with provable bounds on the recovery of userresponses. We test our procedure on both synthetic and real-world EDA signalsfrom wearable sensors, and demonstrate that our approach allows for moreaccurate recovery of user responses as compared to the existing techniques.
arxiv-16500-222 | Accent Classification with Phonetic Vowel Representation | http://arxiv.org/abs/1604.08095 | author:Zhenhao Ge, Yingyi Tan, Aravind Ganapathiraju category:cs.SD cs.CL published:2016-02-24 summary:Previous accent classification research focused mainly on detecting accentswith pure acoustic information without recognizing accented speech. This workcombines phonetic knowledge such as vowels with acoustic information to buildGuassian Mixture Model (GMM) classifier with Perceptual Linear Predictive (PLP)features, optimized by Hetroscedastic Linear Discriminant Analysis (HLDA). Withinput about 20-second accented speech, this system achieves classification rateof 51% on a 7-way classification system focusing on the major types of accentsin English, which is competitive to the state-of-the-art results in this field.
arxiv-16500-223 | Toward Mention Detection Robustness with Recurrent Neural Networks | http://arxiv.org/abs/1602.07749 | author:Thien Huu Nguyen, Avirup Sil, Georgiana Dinu, Radu Florian category:cs.CL published:2016-02-24 summary:One of the key challenges in natural language processing (NLP) is to yieldgood performance across application domains and languages. In this work, weinvestigate the robustness of the mention detection systems, one of thefundamental tasks in information extraction, via recurrent neural networks(RNNs). The advantage of RNNs over the traditional approaches is their capacityto capture long ranges of context and implicitly adapt the word embeddings,trained on a large corpus, into a task-specific word representation, but stillpreserve the original semantic generalization to be helpful across domains. Oursystematic evaluation for RNN architectures demonstrates that RNNs not onlyoutperform the best reported systems (up to 9\% relative error reduction) inthe general setting but also achieve the state-of-the-art performance in thecross-domain setting for English. Regarding other languages, RNNs aresignificantly better than the traditional methods on the similar task of namedentity recognition for Dutch (up to 22\% relative error reduction).
arxiv-16500-224 | Automatic Moth Detection from Trap Images for Pest Management | http://arxiv.org/abs/1602.07383 | author:Weiguang Ding, Graham Taylor category:cs.CV cs.LG cs.NE published:2016-02-24 summary:Monitoring the number of insect pests is a crucial component inpheromone-based pest management systems. In this paper, we propose an automaticdetection pipeline based on deep learning for identifying and counting pests inimages taken inside field traps. Applied to a commercial codling moth dataset,our method shows promising performance both qualitatively and quantitatively.Compared to previous attempts at pest detection, our approach uses nopest-specific engineering which enables it to adapt to other species andenvironments with minimal human effort. It is amenable to implementation onparallel hardware and therefore capable of deployment in settings wherereal-time performance is required.
arxiv-16500-225 | The Myopia of Crowds: A Study of Collective Evaluation on Stack Exchange | http://arxiv.org/abs/1602.07388 | author:Keith Burghardt, Emanuel F. Alsina, Michelle Girvan, William Rand, Kristina Lerman category:cs.HC cs.CY cs.LG physics.soc-ph published:2016-02-24 summary:Crowds can often make better decisions than individuals or small groups ofexperts by leveraging their ability to aggregate diverse information. Questionanswering sites, such as Stack Exchange, rely on the "wisdom of crowds" effectto identify the best answers to questions asked by users. We analyze data from250 communities on the Stack Exchange network to pinpoint factors affectingwhich answers are chosen as the best answers. Our results suggest that, ratherthan evaluate all available answers to a question, users rely on simplecognitive heuristics to choose an answer to vote for or accept. These cognitiveheuristics are linked to an answer's salience, such as the order in which it islisted and how much screen space it occupies. While askers appear to dependmore on heuristics, compared to voting users, when choosing an answer to acceptas the most helpful one, voters use acceptance itself as a heuristic: they aremore likely to choose the answer after it is accepted than before that verysame answer was accepted. These heuristics become more important in explainingand predicting behavior as the number of available answers increases. Ourfindings suggest that crowd judgments may become less reliable as the number ofanswers grow.
arxiv-16500-226 | How Deep Neural Networks Can Improve Emotion Recognition on Video Data | http://arxiv.org/abs/1602.07377 | author:Pooya Khorrami, Tom Le Paine, Kevin Brady, Charlie Dagli, Thomas S. Huang category:cs.CV published:2016-02-24 summary:We consider the task of dimensional emotion recognition on video data usingdeep learning. While several previous methods have shown the benefits oftraining temporal neural network models such as recurrent neural networks(RNNs) on hand-crafted features, few works have considered combiningconvolutional neural networks (CNNs) with RNNs. In this work, we present asystem that performs emotion recognition on video data using both CNNs andRNNs, and we also analyze how much each neural network component contributes tothe system's overall performance. We present our findings on videos from theAudio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze theeffects of several hyperparameters on overall performance while also achievingsuperior performance to the baseline and other competing methods.
arxiv-16500-227 | Domain Specific Author Attribution Based on Feedforward Neural Network Language Models | http://arxiv.org/abs/1602.07393 | author:Zhenhao Ge, Yufang Sun category:cs.CL cs.LG cs.NE published:2016-02-24 summary:Authorship attribution refers to the task of automatically determining theauthor based on a given sample of text. It is a problem with a long history andhas a wide range of application. Building author profiles using language modelsis one of the most successful methods to automate this task. New languagemodeling methods based on neural networks alleviate the curse of dimensionalityand usually outperform conventional N-gram methods. However, there have notbeen much research applying them to authorship attribution. In this paper, wepresent a novel setup of a Neural Network Language Model (NNLM) and apply it toa database of text samples from different authors. We investigate how the NNLMperforms on a task with moderate author set size and relatively limitedtraining and test data, and how the topics of the text samples affect theaccuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement offitness of a trained language model to the test data. Given 5 random testsentences, it also increases the author classification accuracy by 3.43% onaverage, compared with the N-gram methods using SRILM tools. An open sourceimplementation of our methodology is freely available athttps://github.com/zge/authorship-attribution/.
arxiv-16500-228 | Ultradense Word Embeddings by Orthogonal Transformation | http://arxiv.org/abs/1602.07572 | author:Sascha Rothe, Sebastian Ebert, Hinrich Schütze category:cs.CL published:2016-02-24 summary:Embeddings are generic representations that are useful for many NLP tasks. Inthis paper, we introduce DENSIFIER, a method that learns an orthogonaltransformation of the embedding space that focuses the information relevant fora task in an ultradense subspace of a dimensionality that is smaller by afactor of 100 than the original space. We show that ultradense embeddingsgenerated by DENSIFIER reach state of the art on a lexicon creation task inwhich words are annotated with three types of lexical information - sentiment,concreteness and frequency. On the SemEval2015 10B sentiment analysis task weshow that no information is lost when the ultradense subspace is used, buttraining is an order of magnitude more efficient due to the compactness of theultradense space.
arxiv-16500-229 | Multilingual Twitter Sentiment Classification: The Role of Human Annotators | http://arxiv.org/abs/1602.07563 | author:Igor Mozetic, Miha Grcar, Jasmina Smailovic category:cs.CL cs.AI published:2016-02-24 summary:What are the limits of automated Twitter sentiment classification? We analyzea large set of manually labeled tweets in different languages, use them astraining data, and construct automated classification models. It turns out thatthe quality of classification models depends much more on the quality and sizeof training data than on the type of the model trained. Experimental resultsindicate that there is no statistically significant difference between theperformance of the top classification models. We quantify the quality oftraining data by applying various annotator agreement measures, and identifythe weakest points of different datasets. We show that the model performanceapproaches the inter-annotator agreement when the size of the training set issufficiently large. However, it is crucial to regularly monitor the self- andinter-annotator agreements since this improves the training datasets andconsequently the model performance. Finally, we show that there is strongevidence that humans perceive the sentiment classes (negative, neutral, andpositive) as ordered.
arxiv-16500-230 | SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size | http://arxiv.org/abs/1602.07360 | author:Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer category:cs.CV cs.AI published:2016-02-24 summary:Recent research on deep neural networks has focused primarily on improvingaccuracy. For a given accuracy level, it is typically possible to identifymultiple DNN architectures that achieve that accuracy level. With equivalentaccuracy, smaller DNN architectures offer at least three advantages: (1)Smaller DNNs require less communication across servers during distributedtraining. (2) Smaller DNNs require less bandwidth to export a new model fromthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy onFPGAs and other hardware with limited memory. To provide all of theseadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNetachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.Additionally, with model compression techniques we are able to compressSqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here:https://github.com/DeepScale/SqueezeNet
arxiv-16500-231 | Group Equivariant Convolutional Networks | http://arxiv.org/abs/1602.07576 | author:Taco S. Cohen, Max Welling category:cs.LG stat.ML published:2016-02-24 summary:We introduce Group equivariant Convolutional Neural Networks (G-CNNs), anatural generalization of convolutional neural networks that reduces samplecomplexity by exploiting symmetries. By convolving over groups larger than thetranslation group, G-CNNs build representations that are equivariant to thesegroups, which makes it possible to greatly increase the degree of parametersharing. We show how G-CNNs can be implemented with negligible computationaloverhead for discrete groups such as the group of translations, reflections androtations by multiples of 90 degrees. G-CNNs achieve state of the art resultson rotated MNIST and significantly improve over a competitive baseline onaugmented and non-augmented CIFAR-10.
arxiv-16500-232 | Discrete Distribution Estimation under Local Privacy | http://arxiv.org/abs/1602.07387 | author:Peter Kairouz, Keith Bonawitz, Daniel Ramage category:stat.ML cs.LG published:2016-02-24 summary:The collection and analysis of user data drives improvements in the app andweb ecosystems, but comes with risks to privacy. This paper examines discretedistribution estimation under local privacy, a setting wherein serviceproviders can learn the distribution of a categorical statistic of interestwithout collecting the underlying data. We present new mechanisms, includinghashed K-ary Randomized Response (KRR), that empirically meet or exceed theutility of existing mechanisms at all privacy levels. New theoretical resultsdemonstrate the order-optimality of KRR and the existing RAPPOR mechanism atdifferent privacy regimes.
arxiv-16500-233 | Computer Aided Restoration of Handwritten Character Strokes | http://arxiv.org/abs/1602.07038 | author:Barak Sober, David Levin category:cs.GR cs.CV math.NA published:2016-02-23 summary:This work suggests a new variational approach to the task of computer aidedrestoration of incomplete characters, residing in a highly noisy document. Wemodel character strokes as the movement of a pen with a varying radius.Following this model, a cubic spline representation is being utilized toperform gradient descent steps, while maintaining interpolation at some initial(manually sampled) points. The proposed algorithm was utilized in the processof restoring approximately 1000 ancient Hebrew characters (dating to ca.8th-7th century BCE), some of which are presented herein and show that thealgorithm yields plausible results when applied on deteriorated documents.
arxiv-16500-234 | Sentence Similarity Learning by Lexical Decomposition and Composition | http://arxiv.org/abs/1602.07019 | author:Zhiguo Wang, Haitao Mi, Abraham Ittycheriah category:cs.CL published:2016-02-23 summary:Most conventional sentence similarity methods only focus on similar parts oftwo input sentences, and simply ignore the dissimilar parts, which usually giveus some clues and semantic meanings about the sentences. In this work, wepropose a model to take into account both the similarities and dissimilaritiesby decomposing and composing lexical semantics over sentences. The modelrepresents each word as a vector, and calculates a semantic matching vector foreach word based on all words in the other sentence. Then, each word vector isdecomposed into a similar component and a dissimilar component based on thesemantic matching vector. After this, a two-channel CNN model is employed tocapture features by composing the similar and dissimilar components. Finally, asimilarity score is estimated over the composed feature vectors. Experimentalresults show that our model gets the state-of-the-art performance on the answersentence selection task, and achieves a comparable result on the paraphraseidentification task.
arxiv-16500-235 | Mobile Big Data Analytics Using Deep Learning and Apache Spark | http://arxiv.org/abs/1602.07031 | author:Mohammad Abu Alsheikh, Dusit Niyato, Shaowei Lin, Hwee-Pink Tan, Zhu Han category:cs.DC cs.LG cs.NE published:2016-02-23 summary:The proliferation of mobile devices, such as smartphones and Internet ofThings (IoT) gadgets, results in the recent mobile big data (MBD) era.Collecting MBD is unprofitable unless suitable analytics and learning methodsare utilized for extracting meaningful information and hidden patterns fromdata. This article presents an overview and brief tutorial of deep learning inMBD analytics and discusses a scalable learning framework over Apache Spark.Specifically, a distributed deep learning is executed as an iterative MapReducecomputing on many Spark workers. Each Spark worker learns a partial deep modelon a partition of the overall MBD, and a master deep model is then built byaveraging the parameters of all partial models. This Spark-based frameworkspeeds up the learning of deep models consisting of many hidden layers andmillions of parameters. We use a context-aware activity recognition applicationwith a real-world dataset containing millions of samples to validate ourframework and assess its speedup effectiveness.
arxiv-16500-236 | Auditing Black-box Models by Obscuring Features | http://arxiv.org/abs/1602.07043 | author:Philip Adler, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, Suresh Venkatasubramanian category:stat.ML cs.LG published:2016-02-23 summary:Data-trained predictive models are widely used to assist in decision making.But they are used as black boxes that output a prediction or score. It istherefore hard to acquire a deeper understanding of model behavior: and inparticular how different attributes influence the model prediction. This isvery important when trying to interpret the behavior of complex models, orensure that certain problematic attributes (like race or gender) are not undulyinfluencing decisions. In this paper, we present a technique for auditing black-box models: we canstudy the extent to which existing models take advantage of particular featuresin the dataset without knowing how the models work. We show how a class oftechniques originally developed for the detection and repair of disparateimpact in classification models can be used to study the sensitivity of anymodel with respect to any feature subsets. Our approach does not require the black-box model to be retrained. This isimportant if (for example) the model is only accessible via an API, andcontrasts our work with other methods that investigate feature influence likefeature selection. We present experimental evidence for the effectiveness ofour procedure using a variety of publicly available datasets and models. Wealso validate our procedure using techniques from interpretable learning andfeature selection.
arxiv-16500-237 | An Improved Gap-Dependency Analysis of the Noisy Power Method | http://arxiv.org/abs/1602.07046 | author:Maria Florina Balcan, Simon S. Du, Yining Wang, Adams Wei Yu category:stat.ML cs.LG published:2016-02-23 summary:We consider the noisy power method algorithm, which has wide applications inmachine learning and statistics, especially those related to principalcomponent analysis (PCA) under resource (communication, memory or privacy)constraints. Existing analysis of the noisy power method shows anunsatisfactory dependency over the "consecutive" spectral gap$(\sigma_k-\sigma_{k+1})$ of an input data matrix, which could be very smalland hence limits the algorithm's applicability. In this paper, we present a newanalysis of the noisy power method that achieves improved gap dependency forboth sample complexity and noise tolerance bounds. More specifically, weimprove the dependency over $(\sigma_k-\sigma_{k+1})$ to dependency over$(\sigma_k-\sigma_{q+1})$, where $q$ is an intermediate algorithm parameter andcould be much larger than the target rank $k$. Our proofs are built upon anovel characterization of proximity between two subspaces that differ fromcanonical angle characterizations analyzed in previous works. Finally, we applyour improved bounds to distributed private PCA and memory-efficient streamingPCA and obtain bounds that are superior to existing results in the literature.
arxiv-16500-238 | A survey of sparse representation: algorithms and applications | http://arxiv.org/abs/1602.07017 | author:Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, David Zhang category:cs.CV cs.LG published:2016-02-23 summary:Sparse representation has attracted much attention from researchers in fieldsof signal processing, image processing, computer vision and patternrecognition. Sparse representation also has a good reputation in boththeoretical research and practical applications. Many different algorithms havebeen proposed for sparse representation. The main purpose of this article is toprovide a comprehensive study and an updated review on sparse representationand to supply a guidance for researchers. The taxonomy of sparse representationmethods can be studied from various viewpoints. For example, in terms ofdifferent norm minimizations used in sparsity constraints, the methods can beroughly categorized into five groups: sparse representation with $l_0$-normminimization, sparse representation with $l_p$-norm (0$<$p$<$1) minimization,sparse representation with $l_1$-norm minimization and sparse representationwith $l_{2,1}$-norm minimization. In this paper, a comprehensive overview ofsparse representation is provided. The available sparse representationalgorithms can also be empirically categorized into four groups: greedystrategy approximation, constrained optimization, proximity algorithm-basedoptimization, and homotopy algorithm-based sparse representation. Therationales of different algorithms in each category are analyzed and a widerange of sparse representation applications are summarized, which couldsufficiently reveal the potential nature of the sparse representation theory.Specifically, an experimentally comparative study of these sparserepresentation algorithms was presented. The Matlab code used in this paper canbe available at: http://www.yongxu.org/lunwen.html.
arxiv-16500-239 | Latent Skill Embedding for Personalized Lesson Sequence Recommendation | http://arxiv.org/abs/1602.07029 | author:Siddharth Reddy, Igor Labutov, Thorsten Joachims category:cs.LG cs.AI cs.CY published:2016-02-23 summary:Students in online courses generate large amounts of data that can be used topersonalize the learning process and improve quality of education. In thispaper, we present the Latent Skill Embedding (LSE), a probabilistic model ofstudents and educational content that can be used to recommend personalizedsequences of lessons with the goal of helping students prepare for specificassessments. Akin to collaborative filtering for recommender systems, thealgorithm does not require students or content to be described by features, butit learns a representation using access traces. We formulate this problem as aregularized maximum-likelihood embedding of students, lessons, and assessmentsfrom historical student-content interactions. An empirical evaluation onlarge-scale data from Knewton, an adaptive learning technology company, showsthat this approach predicts assessment results competitively with benchmarkmodels and is able to discriminate between lesson sequences that lead tomastery and failure.
arxiv-16500-240 | A Simple Approach to Sparse Clustering | http://arxiv.org/abs/1602.07277 | author:Ery Arias-Castro, Xiao Pu category:stat.ML published:2016-02-23 summary:We consider the problem of sparse clustering, where it is assumed that only asubset of the features are useful for clustering purposes. In the framework ofthe COSA method of Friedman and Meulman (2004), subsequently improved in theform of the Sparse K-means method of Witten and Tibshirani (2010), we propose avery natural and simpler hill-climbing approach that is competitive with thesetwo methods.
arxiv-16500-241 | The IBM 2016 Speaker Recognition System | http://arxiv.org/abs/1602.07291 | author:Seyed Omid Sadjadi, Sriram Ganapathy, Jason W. Pelecanos category:cs.SD cs.CL stat.ML published:2016-02-23 summary:In this paper we describe the recent advancements made in the IBM i-vectorspeaker recognition system for conversational speech. In particular, weidentify key techniques that contribute to significant improvements inperformance of our system, and quantify their contributions. The techniquesinclude: 1) a nearest-neighbor discriminant analysis (NDA) approach that isformulated to alleviate some of the limitations associated with theconventional linear discriminant analysis (LDA) that assumes Gaussianclass-conditional distributions, 2) the application of speaker- andchannel-adapted features, which are derived from an automatic speechrecognition (ASR) system, for speaker recognition, and 3) the use of a deepneural network (DNN) acoustic model with a large number of output units (~10ksenones) to compute the frame-level soft alignments required in the i-vectorestimation process. We evaluate these techniques on the NIST 2010 speakerrecognition evaluation (SRE) extended core conditions involving telephone andmicrophone trials. Experimental results indicate that: 1) the NDA is moreeffective (up to 35% relative improvement in terms of EER) than the traditionalparametric LDA for speaker recognition, 2) when compared to raw acousticfeatures (e.g., MFCCs), the ASR speaker-adapted features provide gains inspeaker recognition performance, and 3) increasing the number of output unitsin the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k)provides consistent improvements in performance (for example from 37% to 57%relative EER gains over our baseline GMM i-vector system). To our knowledge,results reported in this paper represent the best performances published todate on the NIST SRE 2010 extended core tasks.
arxiv-16500-242 | Stuck in a What? Adventures in Weight Space | http://arxiv.org/abs/1602.07320 | author:Zachary C. Lipton category:cs.LG published:2016-02-23 summary:Deep learning researchers commonly suggest that converged models are stuck inlocal minima. More recently, some researchers observed that under reasonableassumptions, the vast majority of critical points are saddle points, not trueminima. Both descriptions suggest that weights converge around a point inweight space, be it a local optima or merely a critical point. However, it'spossible that neither interpretation is accurate. As neural networks aretypically over-complete, it's easy to show the existence of vast continuousregions through weight space with equal loss. In this paper, we build on recentwork empirically characterizing the error surfaces of neural networks. Weanalyze training paths through weight space, presenting evidence that apparentconvergence of loss does not correspond to weights arriving at critical points,but instead to large movements through flat regions of weight space. While it'strivial to show that neural network error surfaces are globally non-convex, weshow that error surfaces are also locally non-convex, even after breakingsymmetry with a random initialization and also after partial training.
arxiv-16500-243 | Object Learning and Convex Cardinal Shape Composition | http://arxiv.org/abs/1602.07613 | author:Alireza Aghasi, Justin Romberg category:cs.CV math.OC published:2016-02-23 summary:This work mainly focuses on a novel segmentation and partitioning scheme,based on learning the principal elements of the optimal partitioner in theimage. The problem of interest is characterizing the objects present in animage as a composition of matching elements from a dictionary of prototypeshapes. The composition model allows set union and difference among theselected elements, while regularizing the problem by restricting their count toa fixed level. This is a combinatorial problem addressing which is not ingeneral computationally tractable. Convex cardinal shape composition (CSC) is arecent relaxation scheme presented as a proxy to the original problem. From atheoretical standpoint, this paper improves the results presented in theoriginal work, by deriving the general sufficient conditions under which CSCidentifies a target composition. We also provide qualitative results on whowell the CSC outcome approximates the combinatorial solution. From acomputational standpoint, two convex solvers, one supporting distributedprocessing for large-scale problems, and one cast as a linear program arepresented. Applications such as multi-resolution segmentation and recovery ofthe principal shape components are presented as the experiments supporting theproposed ideas.
arxiv-16500-244 | A Streaming Algorithm for Crowdsourced Data Classification | http://arxiv.org/abs/1602.07107 | author:Thomas Bonald, Richard Combes category:stat.ML cs.LG published:2016-02-23 summary:We propose a streaming algorithm for the binary classification of data basedon crowdsourcing. The algorithm learns the competence of each labeller bycomparing her labels to those of other labellers on the same tasks and usesthis information to minimize the prediction error rate on each task. We provideperformance guarantees of our algorithm for a fixed population of independentlabellers. In particular, we show that our algorithm is optimal in the sensethat the cumulative regret compared to the optimal decision with known labellererror probabilities is finite, independently of the number of tasks to label.The complexity of the algorithm is linear in the number of labellers and thenumber of tasks, up to some logarithmic factors. Numerical experimentsillustrate the performance of our algorithm compared to existing algorithms,including simple majority voting and expectation-maximization algorithms, onboth synthetic and real datasets.
arxiv-16500-245 | Search Improves Label for Active Learning | http://arxiv.org/abs/1602.07265 | author:Alina Beygelzimer, Daniel Hsu, John Langford, Chicheng Zhang category:cs.LG stat.ML published:2016-02-23 summary:We investigate active learning with access to two distinct oracles: Label(which is standard) and Search (which is not). The Search oracle models thesituation where a human searches a database to seed or counterexample anexisting solution. Search is stronger than Label while being natural toimplement in many situations. We show that an algorithm using both oracles canprovide exponentially large problem-dependent improvements over Label alone.
arxiv-16500-246 | Exploring the Neural Algorithm of Artistic Style | http://arxiv.org/abs/1602.07188 | author:Yaroslav Nikulin, Roman Novak category:cs.CV published:2016-02-23 summary:We explore the method of style transfer presented in the article "A NeuralAlgorithm of Artistic Style" by Leon A. Gatys, Alexander S. Ecker and MatthiasBethge (arXiv:1508.06576). We first demonstrate the power of the suggested style space on a fewexamples. We then vary different hyper-parameters and program properties thatwere not discussed in the original paper, among which are the recognitionnetwork used, starting point of the gradient descent and different ways topartition style and content layers. We also give a brief comparison of some ofthe existing algorithm implementations and deep learning frameworks used. To study the style space further we attempt to generate synthetic images bymaximizing a single entry in one of the Gram matrices $\mathcal{G}_l$ and someinteresting results are observed. Next, we try to mimic the sparsity andintensity distribution of Gram matrices obtained from a real painting andgenerate more complex textures. Finally, we propose two new style representations built on top of network'sfeatures and discuss how one could be used to achieve local and potentiallycontent-aware style transfer.
arxiv-16500-247 | The ImageNet Shuffle: Reorganized Pre-training for Video Event Detection | http://arxiv.org/abs/1602.07119 | author:Pascal Mettes, Dennis C. Koelma, Cees G. M. Snoek category:cs.CV published:2016-02-23 summary:This paper strives for video event detection using a representation learnedfrom deep convolutional neural networks. Different from the leading approaches,who all learn from the 1,000 classes defined in the ImageNet Large Scale VisualRecognition Challenge, we investigate how to leverage the complete ImageNethierarchy for pre-training deep networks. To deal with the problems ofover-specific classes and classes with few images, we introduce a bottom-up andtop-down approach for reorganization of the ImageNet hierarchy based on all its21,814 classes and more than 14 million images. Experiments on the TRECVIDMultimedia Event Detection 2013 and 2015 datasets show that videorepresentations derived from the layers of a deep neural network pre-trainedwith our reorganized hierarchy i) improves over standard pre-training, ii) iscomplementary among different reorganizations, iii) maintains the benefits offusion with other modalities, and iv) leads to state-of-the-art event detectionresults. The reorganized hierarchies and their derived Caffe models arepublicly available at http://tinyurl.com/imagenetshuffle.
arxiv-16500-248 | Submodular Learning and Covering with Response-Dependent Costs | http://arxiv.org/abs/1602.07120 | author:Sivan Sabato category:cs.LG stat.ML published:2016-02-23 summary:We consider interactive learning and covering problems, in a setting whereactions may incur different costs, depending on the outcomes of the action. Forinstance, in a clinical trial, selecting a patient for treatment might resultin improved health or adverse effects for this patients, and these two outcomeshave different costs. We consider a setting where these costs can be inferredfrom observable responses to the actions. We generalize previous analyses ofinteractive learning and covering to \emph{consistency aware} submodularobjectives, and propose a natural greedy algorithm for the setting ofresponse-dependent costs. We bound the approximation factor of this greedyalgorithm, for general submodular functions, as well as specifically for\emph{learning objectives}, and show that a different property of the costfunction controls the approximation factor in each of these scenarios. Wefurther show that in both settings, the approximation factor of this greedyalgorithm is near-optimal in the class of greedy algorithms. Experimentsdemonstrate the advantages of the proposed algorithm in the response-dependentcost setting.
arxiv-16500-249 | Explore First, Exploit Next: The True Shape of Regret in Bandit Problems | http://arxiv.org/abs/1602.07182 | author:Aurélien Garivier, Pierre Ménard, Gilles Stoltz category:math.ST cs.LG stat.TH published:2016-02-23 summary:We revisit lower bounds on the regret in the case of multi-armed banditproblems. We obtain non-asymptotic bounds and provide straightforward proofsbased only on well-known properties of Kullback-Leibler divergences. Thesebounds show that in an initial phase the regret grows almost linearly, and thatthe well-known logarithmic growth of the regret only holds in a final phase.The proof techniques come to the essence of the arguments used and they aredeprived of all unnecessary complications.
arxiv-16500-250 | Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning | http://arxiv.org/abs/1602.07261 | author:Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke category:cs.CV published:2016-02-23 summary:Very deep convolutional networks have been central to the largest advances inimage recognition performance in recent years. One example is the Inceptionarchitecture that has been shown to achieve very good performance at relativelylow computational cost. Recently, the introduction of residual connections inconjunction with a more traditional architecture has yielded state-of-the-artperformance in the 2015 ILSVRC challenge; its performance was similar to thelatest generation Inception-v3 network. This raises the question of whetherthere are any benefit in combining the Inception architecture with residualconnections. Here we give clear empirical evidence that training with residualconnections accelerates the training of Inception networks significantly. Thereis also some evidence of residual Inception networks outperforming similarlyexpensive Inception networks without residual connections by a thin margin. Wealso present several new streamlined architectures for both residual andnon-residual Inception networks. These variations improve the single-framerecognition performance on the ILSVRC 2012 classification task significantly.We further demonstrate how proper activation scaling stabilizes the training ofvery wide residual Inception networks. With an ensemble of three residual andone Inception-v4, we achieve 3.08 percent top-5 error on the test set of theImageNet classification (CLS) challenge
arxiv-16500-251 | Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series | http://arxiv.org/abs/1602.07109 | author:Maximilian Soelch, Justin Bayer, Marvin Ludersdorfer, Patrick van der Smagt category:stat.ML cs.LG published:2016-02-23 summary:Approximate variational inference has shown to be a powerful tool formodeling unknown complex probability distributions. Recent advances in thefield allow us to learn probabilistic models of sequences that ac- tivelyexploit spatial and temporal structure. We apply a Stochastic Recurrent Network(STORN) to learn robot time series data. Our evaluation demonstrates that wecan ro- bustly detect anomalies both off- and on-line.
arxiv-16500-252 | Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations | http://arxiv.org/abs/1602.07332 | author:Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li category:cs.CV cs.AI published:2016-02-23 summary:Despite progress in perceptual tasks such as image classification, computersstill perform poorly on cognitive tasks such as image description and questionanswering. Cognition is core to tasks that involve not just recognizing, butreasoning about our visual world. However, models used to tackle the richcontent in images for cognitive tasks are still being trained using the samedatasets designed for perceptual tasks. To achieve success at cognitive tasks,models need to understand the interactions and relationships between objects inan image. When asked "What vehicle is the person riding?", computers will needto identify the objects in an image as well as the relationships riding(man,carriage) and pulling(horse, carriage) in order to answer correctly that "theperson is riding a horse-drawn carriage". In this paper, we present the Visual Genome dataset to enable the modeling ofsuch relationships. We collect dense annotations of objects, attributes, andrelationships within each image to learn these models. Specifically, ourdataset contains over 100K images where each image has an average of 21objects, 18 attributes, and 18 pairwise relationships between objects. Wecanonicalize the objects, attributes, relationships, and noun phrases in regiondescriptions and questions answer pairs to WordNet synsets. Together, theseannotations represent the densest and largest dataset of image descriptions,objects, attributes, relationships, and question answers.
arxiv-16500-253 | Petrarch 2 : Petrarcher | http://arxiv.org/abs/1602.07236 | author:Clayton Norris category:cs.CL published:2016-02-23 summary:PETRARCH 2 is the fourth generation of a series of Event-Data coders stemmingfrom research by Phillip Schrodt. Each iteration has brought new functionalityand usability, and this is no exception.Petrarch 2 takes much of the power ofthe original Petrarch's dictionaries and redirects it into a faster and smartercore logic. Earlier iterations handled sentences largely as a list of words,incorporating some syntactic information here and there. Petrarch 2 now viewsthe sentence entirely on the syntactic level. It receives the syntactic parseof a sentence from the Stanford CoreNLP software, and stores this data as atree structure of linked nodes, where each node is a Phrase object.Prepositional, noun, and verb phrases each have their own version of thisPhrase class, which deals with the logic particular to those kinds of phrases.Since this is an event coder, the core of the logic focuses around the verbs:who is acting, who is being acted on, and what is happening. The theory behindthis new structure and its logic is founded in Generative Grammar, InformationTheory, and Lambda-Calculus Semantics.
arxiv-16500-254 | Parsimonious modeling with Information Filtering Networks | http://arxiv.org/abs/1602.07349 | author:Wolfram Barfuss, Guido Previde Massara, T. Di Matteo, Tomaso Aste category:cs.IT math.IT stat.ML published:2016-02-23 summary:We introduce a methodology to construct sparse models from data by usinginformation filtering networks as inference structure. This method iscomputationally very efficient and statistically robust because it is based{on} local, low-dimensional, inversions of the covariance matrix to generate aglobal sparse inverse. Compared with state-of-the-art methodologies such aslasso, our method is computationally more efficient producing in a fraction ofcomputation time models that have equivalent or better performances but with asparser and more meaningful inference structure. The local nature of thisapproach allow{s} dynamical partial updating when the properties of somevariables change without the need of recomputing the whole model. We discussperformances with financial data and financial applications to prediction,stress testing and risk allocation.
arxiv-16500-255 | Lens depth function and k-relative neighborhood graph: versatile tools for ordinal data analysis | http://arxiv.org/abs/1602.07194 | author:Matthäus Kleindessner, Ulrike von Luxburg category:stat.ML cs.DS cs.LG published:2016-02-23 summary:In recent years it has become popular to study machine learning problemsbased on ordinal distance information rather than numerical distancemeasurements. By ordinal distance information we refer to binary answers todistance comparisons such as d(A,B) < d(C,D). For many problems in machinelearning and statistics it is unclear how to solve them in such a scenario. Upto now, the main approach is to explicitly construct an ordinal embedding ofthe data points in the Euclidean space, an approach that has a number ofdrawbacks. In this paper, we propose algorithms for the problems of medoidestimation, outlier identification, classification and clustering when givenonly ordinal distance comparisons. They are based on estimating the lens depthfunction and the k-relative neighborhood graph on a data set. Our algorithmsare simple, can easily be parallelized, avoid many of the drawbacks of anordinal embedding approach and are much faster.
arxiv-16500-256 | Car Type Recognition with Deep Neural Networks | http://arxiv.org/abs/1602.07125 | author:Heikki Huttunen, Fatemeh Shokrollahi Yancheshmeh, Ke Chen category:cs.CV published:2016-02-23 summary:In this paper we study automatic recognition of cars of four types: Bus,Truck, Van and Small car. For this problem we consider two data drivenframeworks: a deep neural network and a support vector machine using SIFTfeatures. The accuracy of the methods is validated with a database of over 6500images, and the resulting prediction accuracy is over 97 %. This clearlyexceeds the accuracies of earlier studies that use manually engineered featureextraction pipelines.
arxiv-16500-257 | Higher-Order Low-Rank Regression | http://arxiv.org/abs/1602.06863 | author:Guillaume Rabusseau, Hachem Kadri category:cs.LG published:2016-02-22 summary:This paper proposes an efficient algorithm (HOLRR) to handle regression taskswhere the outputs have a tensor structure. We formulate the regression problemas the minimization of a least square criterion under a multilinear rankconstraint, a difficult non convex problem. HOLRR computes efficiently anapproximate solution of this problem, with solid theoretical guarantees. Akernel extension is also presented. Experiments on synthetic and real data showthat HOLRR outperforms multivariate and multilinear regression methods and isconsiderably faster than existing tensor methods.
arxiv-16500-258 | Principal Component Projection Without Principal Component Analysis | http://arxiv.org/abs/1602.06872 | author:Roy Frostig, Cameron Musco, Christopher Musco, Aaron Sidford category:cs.DS cs.LG stat.ML published:2016-02-22 summary:We show how to efficiently project a vector onto the top principal componentsof a matrix, without explicitly computing these components. Specifically, weintroduce an iterative algorithm that provably computes the projection usingfew calls to any black-box routine for ridge regression. By avoiding explicit principal component analysis (PCA), our algorithm is thefirst with no runtime dependence on the number of top principal components. Weshow that it can be used to give a fast iterative method for the popularprincipal component regression problem, giving the first major runtimeimprovement over the naive method of combining PCA with regression. To achieve our results, we first observe that ridge regression can be used toobtain a "smooth projection" onto the top principal components. We then sharpenthis approximation to true projection using a low-degree polynomialapproximation to the matrix step function. Step function approximation is atopic of long-term interest in scientific computing. We extend prior theory byconstructing polynomials with simple iterative structure and rigorouslyanalyzing their behavior under limited precision.
arxiv-16500-259 | Improving Trajectory Modelling for DNN-based Speech Synthesis by using Stacked Bottleneck Features and Minimum Generation Error Training | http://arxiv.org/abs/1602.06727 | author:Zhizheng Wu, Simon King category:cs.SD cs.CL cs.NE published:2016-02-22 summary:We propose two novel techniques --- stacking bottleneck features and minimumgeneration error training criterion --- to improve the performance of deepneural network (DNN)-based speech synthesis. The techniques address the relatedissues of frame-by-frame independence and ignorance of the relationship betweenstatic and dynamic features, within current typical DNN-based synthesisframeworks. Stacking bottleneck features, which are an acoustically--informedlinguistic representation, provides an efficient way to include more detailedlinguistic context at the input. The minimum generation error trainingcriterion minimises overall output trajectory error across an utterance, ratherthan minimising the error per frame independently, and thus takes into accountthe interaction between static and dynamic features. The two techniques can beeasily combined to further improve performance. We present both objective andsubjective results that demonstrate the effectiveness of the proposedtechniques. The subjective results show that combining the two techniques leadsto significantly more natural synthetic speech than from conventional DNN orlong short-term memory (LSTM) recurrent neural network (RNN) systems.
arxiv-16500-260 | Temporal Network Analysis of Literary Texts | http://arxiv.org/abs/1602.07275 | author:Sandra D. Prado, Silvio R. Dahmen, Ana L. C. Bazzan, Padraig Mac Carron, Ralph Kenna category:physics.soc-ph cs.CL published:2016-02-22 summary:We study temporal networks of characters in literature focusing on "Alice'sAdventures in Wonderland" (1865) by Lewis Carroll and the anonymous "La Chansonde Roland" (around 1100). The former, one of the most influential pieces ofnonsense literature ever written, describes the adventures of Alice in afantasy world with logic plays interspersed along the narrative. The latter, asong of heroic deeds, depicts the Battle of Roncevaux in 778 A.D. duringCharlemagne's campaign on the Iberian Peninsula. We apply methods recentlydeveloped by Taylor and coworkers \cite{Taylor+2015} to find time-averagedeigenvector centralities, Freeman indices and vitalities of characters. We showthat temporal networks are more appropriate than static ones for studyingstories, as they capture features that the time-independent approaches fail toyield.
arxiv-16500-261 | A Geometric Analysis of Phase Retrieval | http://arxiv.org/abs/1602.06664 | author:Ju Sun, Qing Qu, John Wright category:cs.IT math.IT math.OC stat.ML published:2016-02-22 summary:Can we recover a complex signal from its Fourier magnitudes? More generally,given a set of $m$ measurements, $y_k = \mathbf a_k^* \mathbf x$ for $k = 1,\dots, m$, is it possible to recover $\mathbf x \in \mathbb{C}^n$ (i.e.,length-$n$ complex vector)? This **generalized phase retrieval** (GPR) problemis a fundamental task in various disciplines, and has been the subject of muchrecent investigation. Natural nonconvex heuristics often work remarkably wellfor GPR in practice, but lack clear theoretical explanations. In this paper, wetake a step towards bridging this gap. We prove that when the measurementvectors $\mathbf a_k$'s are generic (i.i.d. complex Gaussian) and the number ofmeasurements is large enough ($m \ge C n \log^3 n$), with high probability, anatural least-squares formulation for GPR has the following benign geometricstructure: (1) there are no spurious local minimizers, and all globalminimizers are equal to the target signal $\mathbf x$, up to a global phase;and (2) the objective function has a negative curvature around each saddlepoint. This structure allows a number of iterative optimization methods toefficiently find a global minimizer, without special initialization. Tocorroborate the claim, we describe and analyze a second-order trust-regionalgorithm.
arxiv-16500-262 | Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation | http://arxiv.org/abs/1602.06886 | author:Akash Srivastava, James Zou, Ryan P. Adams, Charles Sutton category:stat.ML cs.LG published:2016-02-22 summary:A good clustering can help a data analyst to explore and understand a dataset, but what constitutes a good clustering may depend on domain-specific andapplication-specific criteria. These criteria can be difficult to formalize,even when it is easy for an analyst to know a good clustering when they seeone. We present a new approach to interactive clustering for data explorationcalled TINDER, based on a particularly simple feedback mechanism, in which ananalyst can reject a given clustering and request a new one, which is chosen tobe different from the previous clustering while fitting the data well. Weformalize this interaction in a Bayesian framework as a method for priorelicitation, in which each different clustering is produced by a priordistribution that is modified to discourage previously rejected clusterings. Weshow that TINDER successfully produces a diverse set of clusterings, each ofequivalent quality, that are much more diverse than would be obtained byrandomized restarts.
arxiv-16500-263 | Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm | http://arxiv.org/abs/1602.06929 | author:Prateek Jain, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, Aaron Sidford category:cs.LG cs.DS cs.NE stat.ML published:2016-02-22 summary:This work provides improved guarantees for streaming principle componentanalysis (PCA). Given $A_1, \ldots, A_n\in \mathbb{R}^{d\times d}$ sampledindependently from distributions satisfying $\mathbb{E}[A_i] = \Sigma$ for$\Sigma \succeq \mathbf{0}$, this work provides an $O(d)$-space linear-timesingle-pass streaming algorithm for estimating the top eigenvector of $\Sigma$.The algorithm nearly matches (and in certain cases improves upon) the accuracyobtained by the standard batch method that computes top eigenvector of theempirical covariance $\frac{1}{n} \sum_{i \in [n]} A_i$ as analyzed by thematrix Bernstein inequality. Moreover, to achieve constant accuracy, ouralgorithm improves upon the best previous known sample complexities ofstreaming algorithms by either a multiplicative factor of $O(d)$ or$1/\mathrm{gap}$ where $\mathrm{gap}$ is the relative distance between the toptwo eigenvalues of $\Sigma$. These results are achieved through a novel analysis of the classic Oja'salgorithm, one of the oldest and most popular algorithms for streaming PCA. Inparticular, this work shows that simply picking a random initial point $w_0$and applying the update rule $w_{i + 1} = w_i + \eta_i A_i w_i$ suffices toaccurately estimate the top eigenvector, with a suitable choice of $\eta_i$. Webelieve our result sheds light on how to efficiently perform streaming PCA bothin theory and in practice and we hope that our analysis may serve as the basisfor analyzing many variants and extensions of streaming PCA.
arxiv-16500-264 | From quantum foundations via natural language meaning to a theory of everything | http://arxiv.org/abs/1602.07618 | author:Bob Coecke category:cs.CL quant-ph published:2016-02-22 summary:In this paper we argue for a paradigmatic shift from `reductionism' to`togetherness'. In particular, we show how interaction between systems inquantum theory naturally carries over to modelling how word meanings interactin natural language. Since meaning in natural language, depending on thesubject domain, encompasses discussions within any scientific discipline, weobtain a template for theories such as social interaction, animal behaviour,and many others.
arxiv-16500-265 | Sparse Linear Regression via Generalized Orthogonal Least-Squares | http://arxiv.org/abs/1602.06916 | author:Abolfazl Hashemi, Haris Vikalo category:stat.ML cs.IT cs.LG math.IT published:2016-02-22 summary:Sparse linear regression, which entails finding a sparse solution to anunderdetermined system of linear equations, can formally be expressed as an$l_0$-constrained least-squares problem. The Orthogonal Least-Squares (OLS)algorithm sequentially selects the features (i.e., columns of the coefficientmatrix) to greedily find an approximate sparse solution. In this paper, ageneralization of Orthogonal Least-Squares which relies on a recursive relationbetween the components of the optimal solution to select L features at eachstep and solve the resulting overdetermined system of equations is proposed.Simulation results demonstrate that the generalized OLS algorithm iscomputationally efficient and achieves performance superior to that of existinggreedy algorithms broadly used in the literature.
arxiv-16500-266 | Blind score normalization method for PLDA based speaker recognition | http://arxiv.org/abs/1602.06967 | author:Danila Doroshin, Nikolay Lubimov, Marina Nastasenko, Mikhail Kotov category:cs.CL cs.LG cs.SD published:2016-02-22 summary:Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-artmethod for modeling $i$-vector space in speaker recognition task. However theperformance degradation is observed if enrollment data size differs from onespeaker to another. This paper presents a solution to such problem byintroducing new PLDA scoring normalization technique. Normalization parametersare derived in a blind way, so that, unlike traditional \textit{ZT-norm}, noextra development data is required. Moreover, proposed method has shown to beoptimal in terms of detection cost function. The experiments conducted on NISTSRE 2014 database demonstrate an improved accuracy in a mixed enrollment numbercondition.
arxiv-16500-267 | Understanding Visual Concepts with Continuation Learning | http://arxiv.org/abs/1602.06822 | author:William F. Whitney, Michael Chang, Tejas Kulkarni, Joshua B. Tenenbaum category:cs.LG published:2016-02-22 summary:We introduce a neural network architecture and a learning algorithm toproduce factorized symbolic representations. We propose to learn these conceptsby observing consecutive frames, letting all the components of the hiddenrepresentation except a small discrete set (gating units) be predicted from theprevious frame, and let the factors of variation in the next frame berepresented entirely by these discrete gated units (corresponding to symbolicrepresentations). We demonstrate the efficacy of our approach on datasets offaces undergoing 3D transformations and Atari 2600 games.
arxiv-16500-268 | Empath: Understanding Topic Signals in Large-Scale Text | http://arxiv.org/abs/1602.06979 | author:Ethan Fast, Binbin Chen, Michael Bernstein category:cs.CL cs.AI published:2016-02-22 summary:Human language is colored by a broad range of topics, but existing textanalysis tools only focus on a small number of them. We present Empath, a toolthat can generate and validate new lexical categories on demand from a smallset of seed terms (like "bleed" and "punch" to generate the category violence).Empath draws connotations between words and phrases by deep learning a neuralembedding across more than 1.8 billion words of modern fiction. Given a smallset of seed words that characterize a category, Empath uses its neuralembedding to discover new related terms, then validates the category with acrowd-powered filter. Empath also analyzes text across 200 built-in,pre-validated categories we have generated from common topics in our webdataset, like neglect, government, and social media. We show that Empath'sdata-driven, human validated categories are highly correlated (r=0.906) withsimilar categories in LIWC.
arxiv-16500-269 | Recovering the number of clusters in data sets with noise features using feature rescaling factors | http://arxiv.org/abs/1602.06989 | author:Renato Cordeiro de Amorim, Christian Hennig category:stat.ML cs.LG published:2016-02-22 summary:In this paper we introduce three methods for re-scaling data sets aiming atimproving the likelihood of clustering validity indexes to return the truenumber of spherical Gaussian clusters with additional noise features. Ourmethod obtains feature re-scaling factors taking into account the structure ofa given data set and the intuitive idea that different features may havedifferent degrees of relevance at different clusters. We experiment with the Silhouette (using squared Euclidean, Manhattan, andthe p$^{th}$ power of the Minkowski distance), Dunn's, Calinski-Harabasz andHartigan indexes on data sets with spherical Gaussian clusters with and withoutnoise features. We conclude that our methods indeed increase the chances ofestimating the true number of clusters in a data set.
arxiv-16500-270 | Semi-supervised Clustering for Short Text via Deep Representation Learning | http://arxiv.org/abs/1602.06797 | author:Zhiguo Wang, Haitao Mi, Abraham Ittycheriah category:cs.CL published:2016-02-22 summary:In this work, we propose a semi-supervised method for short text clustering,where we represent texts as distributed vectors with neural networks, and use asmall amount of labeled data to specify our intention for clustering. We designa novel objective to combine the representation learning process and thek-means clustering process together, and optimize the objective with bothlabeled data and unlabeled data iteratively until convergence through threesteps: (1) assign each short text to its nearest centroid based on itsrepresentation from the current neural networks; (2) re-estimate the clustercentroids based on cluster assignments from step (1); (3) update neuralnetworks according to the objective by keeping centroids and clusterassignments fixed. Experimental results on four datasets show that our methodworks significantly better than several other text clustering methods.
arxiv-16500-271 | A Statistical Model for Stroke Outcome Prediction and Treatment Planning | http://arxiv.org/abs/1602.07280 | author:Abhishek Sengupta, Vaibhav Rajan, Sakyajit Bhattacharya, G R K Sarma category:stat.AP cs.LG published:2016-02-22 summary:Stroke is a major cause of mortality and long--term disability in the world.Predictive outcome models in stroke are valuable for personalized treatment,rehabilitation planning and in controlled clinical trials. In this paper wedesign a new model to predict outcome in the short-term, the putativetherapeutic window for several treatments. Our regression-based model has aparametric form that is designed to address many challenges common in medicaldatasets like highly correlated variables and class imbalance. Empirically ourmodel outperforms the best--known previous models in predicting short--termoutcomes and in inferring the most effective treatments that improve outcome.
arxiv-16500-272 | An Effective and Efficient Approach for Clusterability Evaluation | http://arxiv.org/abs/1602.06687 | author:Margareta Ackerman, Andreas Adolfsson, Naomi Brownstein category:cs.LG stat.ML published:2016-02-22 summary:Clustering is an essential data mining tool that aims to discover inherentcluster structure in data. As such, the study of clusterability, whichevaluates whether data possesses such structure, is an integral part of clusteranalysis. Yet, despite their central role in the theory and application ofclustering, current notions of clusterability fall short in two crucial aspectsthat render them impractical; most are computationally infeasible and othersfail to classify the structure of real datasets. In this paper, we propose a novel approach to clusterability evaluation thatis both computationally efficient and successfully captures the structure ofreal data. Our method applies multimodality tests to the (one-dimensional) setof pairwise distances based on the original, potentially high-dimensional data.We present extensive analyses of our approach for both the Dip and Silvermanmultimodality tests on real data as well as 17,000 simulations, demonstratingthe success of our approach as the first practical notion of clusterability.
arxiv-16500-273 | Sparse Estimation of Multivariate Poisson Log-Normal Model and Inverse Covariance for Count Data | http://arxiv.org/abs/1602.07337 | author:Hao Wu, Xinwei Deng, Naren Ramakrishnan category:stat.ME cs.LG published:2016-02-22 summary:Modeling data with multivariate count responses is a challenge problem due tothe discrete nature of the responses. Such data are commonly observed in manyapplications such as biology, epidemiology and social studies. The existingmethods for univariate count response cannot be easily extended to themultivariate situation since the dependency among multiple responses needs tobe properly accommodated. In this paper, we propose a multivariate PoissonLog-Normal regression model for multivariate data with count responses. Anefficient Monte Carlo EM algorithm is also developed to facilitate the modelestimation. By simultaneously estimating the regression coefficients andinverse covariance matrix over the latent variables, the proposed regressionmodel takes advantages of association among multiple count responses to improvethe model prediction performance. Simulation studies are conducted tosystematically evaluate the performance of the proposed method in comparisonwith conventional methods. The proposed method is further used to analyze areal influenza-like illness data, showing accurate prediction and forecastingwith meaningful interpretation.
arxiv-16500-274 | Graph Regularized Low Rank Representation for Aerosol Optical Depth Retrieval | http://arxiv.org/abs/1602.06818 | author:Yubao Sun, Renlong Hang, Qingshan Liu, Fuping Zhu, Hucheng Pei category:cs.LG published:2016-02-22 summary:In this paper, we propose a novel data-driven regression model for aerosoloptical depth (AOD) retrieval. First, we adopt a low rank representation (LRR)model to learn a powerful representation of the spectral response. Then, graphregularization is incorporated into the LRR model to capture the localstructure information and the nonlinear property of the remote-sensing data.Since it is easy to acquire the rich satellite-retrieval results, we use themas a baseline to construct the graph. Finally, the learned featurerepresentation is feeded into support vector machine (SVM) to retrieve AOD.Experiments are conducted on two widely used data sets acquired by differentsensors, and the experimental results show that the proposed method can achievesuperior performance compared to the physical models and other state-of-the-artempirical models.
arxiv-16500-275 | Convexification of Learning from Constraints | http://arxiv.org/abs/1602.06746 | author:Iaroslav Shcherbatyi, Bjoern Andres category:cs.LG math.OC stat.ML published:2016-02-22 summary:Regularized empirical risk minimization with constrained labels (in contrastto fixed labels) is a remarkably general abstraction of learning. For commonloss and regularization functions, this optimization problem assumes the formof a mixed integer program (MIP) whose objective function is non-convex. Inthis form, the problem is resistant to standard optimization techniques. Weconstruct MIPs with the same solutions whose objective functions are convex.Specifically, we characterize the tightest convex extension of the objectivefunction, given by the Legendre-Fenchel biconjugate. Computing values of thistightest convex extension is NP-hard. However, by applying our characterizationto every function in an additive decomposition of the objective function, weobtain a class of looser convex extensions that can be computed efficiently.For some decompositions, common loss and regularization functions, we derive aclosed form.
arxiv-16500-276 | Denoising and Covariance Estimation of Single Particle Cryo-EM Images | http://arxiv.org/abs/1602.06632 | author:Tejal Bhamre, Teng Zhang, Amit Singer category:cs.CV q-bio.BM stat.ML published:2016-02-22 summary:The problem of image restoration in cryo-EM entails correcting for theeffects of the Contrast Transfer Function (CTF) and noise. Popular methods forimage restoration include `phase flipping', which corrects only for the Fourierphases but not amplitudes, and Wiener filtering, which requires the spectralsignal to noise ratio. We propose a new image restoration method which we call`Covariance Wiener Filtering' (CWF). In CWF, the covariance matrix of theprojection images is used within the classical Wiener filtering framework forsolving the image restoration deconvolution problem. Our estimation procedurefor the covariance matrix is new and successfully corrects for the CTF. Wedemonstrate the efficacy of CWF by applying it to restore both simulated andexperimental cryo-EM images. Results with experimental datasets demonstratethat CWF provides a good way to evaluate the particle images and to see whatthe dataset contains even without 2D classification and averaging.
arxiv-16500-277 | Creating Simplified 3D Models with High Quality Textures | http://arxiv.org/abs/1602.06645 | author:Song Liu, Wanqing Li, Philip Ogunbona, Yang-Wai Chow category:cs.GR cs.CV published:2016-02-22 summary:This paper presents an extension to the KinectFusion algorithm which allowscreating simplified 3D models with high quality RGB textures. This is achievedthrough (i) creating model textures using images from an HD RGB camera that iscalibrated with Kinect depth camera, (ii) using a modified scheme to updatemodel textures in an asymmetrical colour volume that contains a higher numberof voxels than that of the geometry volume, (iii) simplifying dense polygonmesh model using quadric-based mesh decimation algorithm, and (iv) creating andmapping 2D textures to every polygon in the output 3D model. The proposedmethod is implemented in real-time by means of GPU parallel processing.Visualization via ray casting of both geometry and colour volumes providesusers with a real-time feedback of the currently scanned 3D model. Experimentalresults show that the proposed method is capable of keeping the model texturequality even for a heavily decimated model and that, when reconstructing smallobjects, photorealistic RGB textures can still be reconstructed.
arxiv-16500-278 | Planogram Compliance Checking Based on Detection of Recurring Patterns | http://arxiv.org/abs/1602.06647 | author:Song Liu, Wanqing Li, Stephen Davis, Christian Ritz, Hongda Tian category:cs.CV published:2016-02-22 summary:In this paper, a novel method for automatic planogram compliance checking inretail chains is proposed without requiring product template images fortraining. Product layout is extracted from an input image by means ofunsupervised recurring pattern detection and matched via graph matching withthe expected product layout specified by a planogram to measure the level ofcompliance. A divide and conquer strategy is employed to improve the speed.Specifically, the input image is divided into several regions based on theplanogram. Recurring patterns are detected in each region respectively and thenmerged together to estimate the product layout. Experimental results on realdata have verified the efficacy of the proposed method. Compared with atemplate-based method, higher accuracies are achieved by the proposed methodover a wide range of products.
arxiv-16500-279 | Variational inference for Monte Carlo objectives | http://arxiv.org/abs/1602.06725 | author:Andriy Mnih, Danilo J. Rezende category:cs.LG stat.ML published:2016-02-22 summary:Recent progress in deep latent variable models has largely been driven by thedevelopment of flexible and scalable variational inference methods. Variationaltraining of this type involves maximizing a lower bound on the log-likelihood,using samples from the variational posterior to compute the required gradients.Recently, Burda et al. (2015) have derived a tighter lower bound using amulti-sample importance sampling estimate of the likelihood and showed thatoptimizing it yields models that use more of their capacity and achieve higherlikelihoods. This development showed the importance of such multi-sampleobjectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyzethe difficulty encountered when estimating the gradients involved. We thendevelop the first unbiased gradient estimator designed for importance-sampledobjectives and evaluate it at training generative and structured outputprediction models. The resulting estimator, which is based on low-varianceper-sample learning signals, is both simpler and more effective than the NVILestimator proposed for the single-sample variational objective, and iscompetitive with the currently used biased estimators.
arxiv-16500-280 | Structured Learning of Binary Codes with Column Generation | http://arxiv.org/abs/1602.06654 | author:Guosheng Lin, Fayao Liu, Chunhua Shen, Jianxin Wu, Heng Tao Shen category:cs.LG published:2016-02-22 summary:Hashing methods aim to learn a set of hash functions which map the originalfeatures to compact binary codes with similarity preserving in the Hammingspace. Hashing has proven a valuable tool for large-scale informationretrieval. We propose a column generation based binary code learning frameworkfor data-dependent hash function learning. Given a set of triplets that encodethe pairwise similarity comparison information, our column generation basedmethod learns hash functions that preserve the relative comparison relationswithin the large-margin learning framework. Our method iteratively learns thebest hash functions during the column generation procedure. Existing hashingmethods optimize over simple objectives such as the reconstruction error orgraph Laplacian related loss functions, instead of the performance evaluationcriteria of interest---multivariate performance measures such as the AUC andNDCG. Our column generation based method can be further generalized from thetriplet loss to a general structured learning based framework that allows oneto directly optimize multivariate performance measures. For optimizing generalranking measures, the resulting optimization problem can involve exponentiallyor infinitely many variables and constraints, which is more challenging thanstandard structured output learning. We use a combination of column generationand cutting-plane techniques to solve the optimization problem. To speed-up thetraining we further explore stage-wise training and propose to use a simplifiedNDCG loss for efficient inference. We demonstrate the generality of our methodby applying it to ranking prediction and image retrieval, and show that itoutperforms a few state-of-the-art hashing methods.
arxiv-16500-281 | Distributed Deep Learning Using Synchronous Stochastic Gradient Descent | http://arxiv.org/abs/1602.06709 | author:Dipankar Das, Sasikanth Avancha, Dheevatsa Mudigere, Karthikeyan Vaidynathan, Srinivas Sridharan, Dhiraj Kalamkar, Bharat Kaul, Pradeep Dubey category:cs.DC cs.LG published:2016-02-22 summary:We design and implement a distributed multinode synchronous SGD algorithm,without altering hyper parameters, or compressing data, or altering algorithmicbehavior. We perform a detailed analysis of scaling, and identify optimaldesign points for different networks. We demonstrate scaling of CNNs on 100s ofnodes, and present what we believe to be record training throughputs. A 512minibatch VGG-A CNN training run is scaled 90X on 128 nodes. Also 256 minibatchVGG-A and OverFeat-FAST networks are scaled 53X and 42X respectively on a 64node cluster. We also demonstrate the generality of our approach viabest-in-class 6.5X scaling for a 7-layer DNN on 16 nodes. Thereafter we attemptto democratize deep-learning by training on an Ethernet based AWS cluster andshow ~14X scaling on 16 nodes.
arxiv-16500-282 | Implicit LOD for processing, visualisation and classification in Point Cloud Servers | http://arxiv.org/abs/1602.06920 | author:Rémi Cura, Julien Perret, Nicolas Paparoditis category:cs.CG cs.CV cs.SE published:2016-02-22 summary:We propose a new paradigm to effortlessly get a portable geometric Level OfDetails (LOD) for a point cloud inside a Point Cloud Server. The point cloud isdivided into groups of points (patch), then each patch is reordered (MidOcordering) so that reading points following this order provides more and moredetails on the patch. This LOD have then multiple applications: point cloudsize reduction for visualisation (point cloud streaming) or speeding of slowalgorithm, fast density peak detection and correction as well as safeguard formethods that may be sensible to density variations. The LOD method also embedsinformation about the sensed object geometric nature, and thus can be used as acrude multi-scale dimensionality descriptor, enabling fast classification andon-the-fly filtering for basic classes.
arxiv-16500-283 | Inference Networks for Sequential Monte Carlo in Graphical Models | http://arxiv.org/abs/1602.06701 | author:Brooks Paige, Frank Wood category:stat.ML published:2016-02-22 summary:We introduce a new approach for amortizing inference in directed graphicalmodels by learning heuristic approximations to stochastic inverses, designedspecifically for use as proposal distributions in sequential Monte Carlomethods. We describe a procedure for constructing and learning a structuredneural network which represents an inverse factorization of the graphicalmodel, resulting in a conditional density estimator that takes as inputparticular values of the observed random variables, and returns anapproximation to the distribution of the latent variables. This recognitionmodel can be learned offline, independent from any particular dataset, prior toperforming inference. The output of these networks can be used asautomatically-learned high-quality proposal distributions to acceleratesequential Monte Carlo across a diverse range of problem settings.
arxiv-16500-284 | Preconditioning Kernel Matrices | http://arxiv.org/abs/1602.06693 | author:Kurt Cutajar, Michael A. Osborne, John P. Cunningham, Maurizio Filippone category:stat.ML stat.CO stat.ME published:2016-02-22 summary:The computational and storage complexity of kernel machines presents theprimary barrier to their scaling to large, modern, datasets. A common way totackle the scalability issue is to use the conjugate gradient algorithm, whichrelieves the constraints on both storage (the kernel matrix need not be stored)and computation (both stochastic gradients and parallelization can be used).Even so, conjugate gradient is not without its own issues: the conditioning ofkernel matrices is often such that conjugate gradients will have poorconvergence in practice. Preconditioning is a common approach to alleviatingthis issue. Here we propose preconditioned conjugate gradients for kernelmachines, and develop a broad range of preconditioners particularly useful forkernel matrices. We describe a scalable approach to both solving kernelmachines and learning their hyperparameters. We show this approach is exact inthe limit of iterations and outperforms state-of-the-art approximations for agiven computational budget.
arxiv-16500-285 | Orthogonal RNNs and Long-Memory Tasks | http://arxiv.org/abs/1602.06662 | author:Mikael Henaff, Arthur Szlam, Yann LeCun category:cs.NE cs.AI cs.LG stat.ML published:2016-02-22 summary:Although RNNs have been shown to be powerful tools for processing sequentialdata, finding architectures or optimization strategies that allow them to modelvery long term dependencies is still an active area of research. In this work,we carefully analyze two synthetic datasets originally outlined in (Hochreiterand Schmidhuber, 1997) which are used to evaluate the ability of RNNs to storeinformation over many time steps. We explicitly construct RNN solutions tothese problems, and using these constructions, illuminate both the problemsthemselves and the way in which RNNs store different types of information intheir hidden states. These constructions furthermore explain the success ofrecent methods that specify unitary initializations or constraints on thetransition matrices.
arxiv-16500-286 | Correlation Hashing Network for Efficient Cross-Modal Retrieval | http://arxiv.org/abs/1602.06697 | author:Yue Cao, Mingsheng Long, Jianmin Wang category:cs.CV published:2016-02-22 summary:Due to the storage and retrieval efficiency, hashing has been widely deployedto approximate nearest neighbor search for large-scale multimedia retrieval.Cross-modal hashing, which improves the quality of hash coding by exploitingthe semantic correlation across different modalities, has received increasingattention recently. For most existing cross-modal hashing methods, an object isfirst represented as of vector of hand-crafted or machine-learned features,followed by another separate quantization step that generates binary codes.However, suboptimal hash coding may be produced, because the quantization erroris not statistically minimized and the feature representation is not optimallycompatible with the binary coding. In this paper, we propose a novelCorrelation Hashing Network (CHN) architecture for cross-modal hashing, inwhich we jointly learn good data representation tailored to hash coding andformally control the quantization error. The CHN model is a hybrid deeparchitecture constituting four key components: (1) an image network withmultiple convolution-pooling layers to extract good image representations, anda text network with several fully-connected layers to extract good textrepresentations; (2) a fully-connected hashing layer to generatemodality-specific compact hash codes; (3) a squared cosine loss layer forcapturing both cross-modal correlation and within-modal correlation; and (4) anew cosine quantization loss for controlling the quality of the binarized hashcodes. Extensive experiments on standard cross-modal retrieval datasets showthe proposed CHN model yields substantial boosts over latest state-of-the-arthashing methods.
arxiv-16500-287 | Clustering subgaussian mixtures by semidefinite programming | http://arxiv.org/abs/1602.06612 | author:Dustin G. Mixon, Soledad Villar, Rachel Ward category:stat.ML cs.DS cs.IT cs.LG math.IT math.ST stat.TH published:2016-02-22 summary:We introduce a model-free relax-and-round algorithm for k-means clusteringbased on a semidefinite relaxation due to Peng and Wei. The algorithminterprets the SDP output as a denoised version of the original data and thenrounds this output to a hard clustering. We provide a generic method forproving performance guarantees for this algorithm, and we analyze the algorithmin the context of subgaussian mixture models. We also study the fundamentallimits of estimating Gaussian centers by k-means clustering in order to compareour approximation guarantee to the theoretically optimal k-means clusteringsolution.
arxiv-16500-288 | Active Task Selection for Multi-Task Learning | http://arxiv.org/abs/1602.06518 | author:Anastasia Pentina, Christoph H. Lampert category:stat.ML cs.LG published:2016-02-21 summary:In this paper we consider the problem of multi-task learning, in which alearner is given a collection of prediction tasks that need to be solved. Incontrast to previous work, we give up on the assumption that labeled trainingdata is available for all tasks. Instead, we propose an active task selectionframework, where based only on the unlabeled data, the learner can choose a,typically small, subset of tasks for which he gets some labeled examples. Forthe remaining tasks, which have no available annotation, solutions are found bytransferring information from the selected tasks. We analyze two transferstrategies and develop generalization bounds for each of them. Based on thistheoretical analysis we propose two algorithms for making the choice of labeledtasks in a principled way and show their effectiveness on synthetic and realdata.
arxiv-16500-289 | Learning, Visualizing, and Exploiting a Model for the Intrinsic Value of a Batted Ball | http://arxiv.org/abs/1603.00050 | author:Glenn Healey category:stat.AP cs.LG I.2.6 published:2016-02-21 summary:We present an algorithm for learning the intrinsic value of a batted ball inbaseball. This work addresses the fundamental problem of separating the valueof a batted ball at contact from factors such as the defense, weather, andballpark that can affect its observed outcome. The algorithm uses a Bayesianmodel to construct a continuous mapping from a vector of batted ball parametersto an intrinsic measure defined as the expected value of a linear weightsrepresentation for run value. A kernel method is used to build nonparametricestimates for the component probability density functions in Bayes theorem froma set of over one hundred thousand batted ball measurements recorded by theHITf/x system during the 2014 major league baseball (MLB) season.Cross-validation is used to determine the optimal vector of smoothingparameters for the density estimates. Properties of the mapping are visualizedby considering reduced-dimension subsets of the batted ball parameter space. Weuse the mapping to derive statistics for intrinsic quality of contact forbatters and pitchers which have the potential to improve the accuracy of playermodels and forecasting systems. We also show that the new approach leads to asimple automated measure of contact-adjusted defense and provides insight intothe impact of environmental variables on batted balls.
arxiv-16500-290 | Machine learning meets network science: dimensionality reduction for fast and efficient embedding of networks in the hyperbolic space | http://arxiv.org/abs/1602.06522 | author:Josephine Maria Thomas, Alessandro Muscoloni, Sara Ciucci, Ginestra Bianconi, Carlo Vittorio Cannistraci category:cs.AI cs.LG published:2016-02-21 summary:Complex network topologies and hyperbolic geometry seem specularly connected,and one of the most fascinating and challenging problems of recent complexnetwork theory is to map a given network to its hyperbolic space. ThePopularity Similarity Optimization (PSO) model represents - at the moment - theclimax of this theory. It suggests that the trade-off between node popularityand similarity is a mechanism to explain how complex network topologies emerge- as discrete samples - from the continuous world of hyperbolic geometry. Thehyperbolic space seems appropriate to represent real complex networks. In fact,it preserves many of their fundamental topological properties, and can beexploited for real applications such as, among others, link prediction andcommunity detection. Here, we observe for the first time that atopological-based machine learning class of algorithms - for nonlinearunsupervised dimensionality reduction - can directly approximate the network'snode angular coordinates of the hyperbolic model into a two-dimensional space,according to a similar topological organization that we named angularcoalescence. On the basis of this phenomenon, we propose a new class ofalgorithms that offers fast and accurate coalescent embedding of networks inthe hyperbolic space even for graphs with thousands of nodes.
arxiv-16500-291 | Semi-Markov Switching Vector Autoregressive Model-based Anomaly Detection in Aviation Systems | http://arxiv.org/abs/1602.06550 | author:Igor Melnyk, Arindam Banerjee, Bryan Matthews, Nikunj Oza category:cs.LG stat.AP stat.ML published:2016-02-21 summary:In this work we consider the problem of anomaly detection in heterogeneous,multivariate, variable-length time series datasets. Our focus is on theaviation safety domain, where data objects are flights and time series aresensor readings and pilot switches. In this context the goal is to detectanomalous flight segments, due to mechanical, environmental, or human factorsin order to identifying operationally significant events and provide insightsinto the flight operations and highlight otherwise unavailable potential safetyrisks and precursors to accidents. For this purpose, we propose a frameworkwhich represents each flight using a semi-Markov switching vectorautoregressive (SMS-VAR) model. Detection of anomalies is then based onmeasuring dissimilarities between the model's prediction and data observation.The framework is scalable, due to the inherent parallel nature of mostcomputations, and can be used to perform online anomaly detection. Extensiveexperimental results on simulated and real datasets illustrate that theframework can detect various types of anomalies along with the key parametersinvolved.
arxiv-16500-292 | Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques | http://arxiv.org/abs/1602.06516 | author:Debarghya Ghoshdastidar, Ambedkar Dukkipati category:cs.LG stat.ML published:2016-02-21 summary:Graph partitioning plays a central role in machine learning, and thedevelopment of graph partitioning algorithms is still an active area ofresearch. The immense demand for such algorithms arises due to the abundance ofapplications that involve pairwise interactions or similarities among entities.Recent studies in computer vision and databases systems have emphasized on thenecessity of considering multi-way interactions, and has led to the study of amore general problem in the form of hypergraph partitioning. This paper focuses on the problem of partitioning uniform hypergraphs, whicharises in applications such as subspace clustering, motion segmentation etc. Weshow that uniform hypergraph partitioning is equivalent to a tensor tracemaximization problem, and hence, a tensor based method is a natural answer tothis problem. We also propose a tensor spectral method that extends the widelyknown spectral clustering algorithm to the case of uniform hypergraphs. Whilethe theoretical guarantees of spectral clustering have been extensivelystudied, very little is known about the statistical properties of tensor basedmethods. To this end, we prove the consistency of the proposed algorithm undera planted partition model. The computational complexity of tensorial approaches has resulted in the useof various tensor sampling strategies. We present the first theoretical studyon the effect of sampling in tensor based hypergraph partitioning. Our resultjustifies the empirical success of iterative sampling techniques often used inpractice. We also present an iteratively sampled variant of the proposedalgorithm for the purpose of subspace clustering, and demonstrate theperformance of this method on a benchmark problem.
arxiv-16500-293 | Multi-task and Lifelong Learning of Kernels | http://arxiv.org/abs/1602.06531 | author:Anastasia Pentina, Shai Ben-David category:stat.ML cs.LG published:2016-02-21 summary:We consider a problem of learning kernels for use in SVM classification inthe multi-task and lifelong scenarios and provide generalization bounds on theerror of a large margin classifier. Our results show that, under mildconditions on the family of kernels used for learning, solving several relatedtasks simultaneously is beneficial over single task learning. In particular, asthe number of observed tasks grows, assuming that in the considered family ofkernels there exists one that yields low approximation error on all tasks, theoverhead associated with learning such a kernel vanishes and the complexityconverges to that of learning when this good kernel is given to the learner.
arxiv-16500-294 | Deep Learning in Finance | http://arxiv.org/abs/1602.06561 | author:J. B. Heaton, N. G. Polson, J. H. Witte category:cs.LG published:2016-02-21 summary:We explore the use of deep learning hierarchical models for problems infinancial prediction and classification. Financial prediction problems -- suchas those presented in designing and pricing securities, constructingportfolios, and risk management -- often involve large data sets with complexdata interactions that currently are difficult or impossible to specify in afull economic model. Applying deep learning methods to these problems canproduce more useful results than standard methods in finance. In particular,deep learning can detect and exploit interactions in the data that are, atleast currently, invisible to any existing financial economic theory.
arxiv-16500-295 | Automatic Building Extraction in Aerial Scenes Using Convolutional Networks | http://arxiv.org/abs/1602.06564 | author:Jiangye Yuan category:cs.CV published:2016-02-21 summary:Automatic building extraction from aerial and satellite imagery is highlychallenging due to extremely large variations of building appearances. Toattack this problem, we design a convolutional network with a final stage thatintegrates activations from multiple preceding stages for pixel-wiseprediction, and introduce the signed distance function of building boundariesas the output representation, which has an enhanced representation power. Weleverage abundant building footprint data available from geographic informationsystems (GIS) to compile training data. The trained network achieves superiorperformance on datasets that are significantly larger and more complex thanthose used in prior work, demonstrating that the proposed method provides apromising and scalable solution for automating this labor-intensive task.
arxiv-16500-296 | Estimating Structured Vector Autoregressive Model | http://arxiv.org/abs/1602.06606 | author:Igor Melnyk, Arindam Banerjee category:math.ST stat.ML stat.TH published:2016-02-21 summary:While considerable advances have been made in estimating high-dimensionalstructured models from independent data using Lasso-type models, limitedprogress has been made for settings when the samples are dependent. We considerestimating structured VAR (vector auto-regressive models), where the structurecan be captured by any suitable norm, e.g., Lasso, group Lasso, order weightedLasso, sparse group Lasso, etc. In VAR setting with correlated noise, althoughthere is strong dependence over time and covariates, we establish bounds on thenon-asymptotic estimation error of structured VAR parameters. Surprisingly, theestimation error is of the same order as that of the corresponding Lasso-typeestimator with independent samples, and the analysis holds for any norm. Ouranalysis relies on results in generic chaining, sub-exponential martingales,and spectral representation of VAR models. Experimental results on syntheticdata with a variety of structures as well as real aviation data are presented,validating theoretical results.
arxiv-16500-297 | Distributed Private Online Learning for Social Big Data Computing over Data Center Networks | http://arxiv.org/abs/1602.06489 | author:Chencheng Li, Pan Zhou, Yingxue Zhou, Kaigui Bian, Tao Jiang, Susanto Rahardja category:cs.DC cs.LG cs.SI published:2016-02-21 summary:With the rapid growth of Internet technologies, cloud computing and socialnetworks have become ubiquitous. An increasing number of people participate insocial networks and massive online social data are obtained. In order toexploit knowledge from copious amounts of data obtained and predict socialbehavior of users, we urge to realize data mining in social networks. Almostall online websites use cloud services to effectively process the large scaleof social data, which are gathered from distributed data centers. These dataare so large-scale, high-dimension and widely distributed that we propose adistributed sparse online algorithm to handle them. Additionally,privacy-protection is an important point in social networks. We should notcompromise the privacy of individuals in networks, while these social data arebeing learned for data mining. Thus we also consider the privacy problem inthis article. Our simulations shows that the appropriate sparsity of data wouldenhance the performance of our algorithm and the privacy-preserving method doesnot significantly hurt the performance of the proposed algorithm.
arxiv-16500-298 | Interactive Storytelling over Document Collections | http://arxiv.org/abs/1602.06566 | author:Dipayan Maiti, Mohammad Raihanul Islam, Scotland Leman, Naren Ramakrishnan category:cs.AI cs.LG stat.ML published:2016-02-21 summary:Storytelling algorithms aim to 'connect the dots' between disparate documentsby linking starting and ending documents through a series of intermediatedocuments. Existing storytelling algorithms are based on notions of coherenceand connectivity, and thus the primary way by which users can steer the storyconstruction is via design of suitable similarity functions. We present analternative approach to storytelling wherein the user can interactively anditeratively provide 'must use' constraints to preferentially support theconstruction of some stories over others. The three innovations in our approachare distance measures based on (inferred) topic distributions, the use ofconstraints to define sets of linear inequalities over paths, and theintroduction of slack and surplus variables to condition the topic distributionto preferentially emphasize desired terms over others. We describe experimentalresults to illustrate the effectiveness of our interactive storytellingapproach over multiple text datasets.
arxiv-16500-299 | 2-Bit Random Projections, NonLinear Estimators, and Approximate Near Neighbor Search | http://arxiv.org/abs/1602.06577 | author:Ping Li, Michael Mitzenmacher, Anshumali Shrivastava category:stat.ML cs.DS cs.LG published:2016-02-21 summary:The method of random projections has become a standard tool for machinelearning, data mining, and search with massive data at Web scale. The effectiveuse of random projections requires efficient coding schemes for quantizing(real-valued) projected data into integers. In this paper, we focus on a simple2-bit coding scheme. In particular, we develop accurate nonlinear estimators ofdata similarity based on the 2-bit strategy. This work will have importantpractical applications. For example, in the task of near neighbor search, acrucial step (often called re-ranking) is to compute or estimate datasimilarities once a set of candidate data points have been identified by hashtable techniques. This re-ranking step can take advantage of the proposedcoding scheme and estimator. As a related task, in this paper, we also study a simple uniform quantizationscheme for the purpose of building hash tables with projected data. Ouranalysis shows that typically only a small number of bits are needed. Forexample, when the target similarity level is high, 2 or 3 bits might besufficient. When the target similarity level is not so high, it is preferableto use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a goodchoice for the task of sublinear time approximate near neighbor search via hashtables. Combining these results, we conclude that 2-bit random projections should berecommended for approximate near neighbor search and similarity estimation.Extensive experimental results are provided.
arxiv-16500-300 | Recovering Structured Probability Matrices | http://arxiv.org/abs/1602.06586 | author:Qingqing Huang, Sham M. Kakade, Weihao Kong, Gregory Valiant category:cs.LG published:2016-02-21 summary:We consider the problem of accurately recovering a matrix B of size M by M ,which represents a probability distribution over M2 outcomes, given access toan observed matrix of "counts" generated by taking independent samples from thedistribution B. How can structural properties of the underlying matrix B beleveraged to yield computationally efficient and information theoreticallyoptimal reconstruction algorithms? When can accurate reconstruction beaccomplished in the sparse data regime? This basic problem lies at the core ofa number of questions that are currently being considered by differentcommunities, including building recommendation systems and collaborativefiltering in the sparse data regime, community detection in sparse randomgraphs, learning structured models such as topic models or hidden Markovmodels, and the efforts from the natural language processing community tocompute "word embeddings". Our results apply to the setting where B has a low rank structure. For thissetting, we propose an efficient algorithm that accurately recovers theunderlying M by M matrix using Theta(M) samples. This result easily translatesto Theta(M) sample algorithms for learning topic models and learning hiddenMarkov Models. These linear sample complexities are optimal, up to constantfactors, in an extremely strong sense: even testing basic properties of theunderlying matrix (such as whether it has rank 1 or 2) requires Omega(M)samples. We provide an even stronger lower bound where distinguishing whether asequence of observations were drawn from the uniform distribution over Mobservations versus being generated by an HMM with two hidden states requiresOmega(M) observations. This precludes sublinear-sample hypothesis tests forbasic properties, such as identity or uniformity, as well as sublinear sampleestimators for quantities such as the entropy rate of HMMs.
