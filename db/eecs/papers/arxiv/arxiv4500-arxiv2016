arxiv-4500-1 | Rectifying Self Organizing Maps for Automatic Concept Learning from Web Images | http://arxiv.org/pdf/1312.4384v1.pdf | author:Eren Golge, Pinar Duygulu category:cs.CV cs.LG cs.NE published:2013-12-16 summary:We attack the problem of learning concepts automatically from noisy web imagesearch results. Going beyond low level attributes, such as colour and texture,we explore weakly-labelled datasets for the learning of higher level concepts,such as scene categories. The idea is based on discovering commoncharacteristics shared among subsets of images by posing a method that is ableto organise the data while eliminating irrelevant instances. We propose a novelclustering and outlier detection method, namely Rectifying Self Organizing Maps(RSOM). Given an image collection returned for a concept query, RSOM providesclusters pruned from outliers. Each cluster is used to train a modelrepresenting a different characteristics of the concept. The proposed methodoutperforms the state-of-the-art studies on the task of learning low-levelconcepts, and it is competitive in learning higher level concepts as well. Itis capable to work at large scale with no supervision through exploiting theavailable sources.
arxiv-4500-2 | Single-trial estimation of stimulus and spike-history effects on time-varying ensemble spiking activity of multiple neurons: a simulation study | http://arxiv.org/pdf/1312.4382v1.pdf | author:Hideaki Shimazaki category:q-bio.NC stat.ML published:2013-12-16 summary:Neurons in cortical circuits exhibit coordinated spiking activity, and canproduce correlated synchronous spikes during behavior and cognition. Werecently developed a method for estimating the dynamics of correlated ensembleactivity by combining a model of simultaneous neuronal interactions (e.g., aspin-glass model) with a state-space method (Shimazaki et al. 2012 PLoS ComputBiol 8 e1002385). This method allows us to estimate stimulus-evoked dynamics ofneuronal interactions which is reproducible in repeated trials under identicalexperimental conditions. However, the method may not be suitable for detectingstimulus responses if the neuronal dynamics exhibits significant variabilityacross trials. In addition, the previous model does not include effects of pastspiking activity of the neurons on the current state of ensemble activity. Inthis study, we develop a parametric method for simultaneously estimating thestimulus and spike-history effects on the ensemble activity from single-trialdata even if the neurons exhibit dynamics that is largely unrelated to theseeffects. For this goal, we model ensemble neuronal activity as a latent processand include the stimulus and spike-history effects as exogenous inputs to thelatent process. We develop an expectation-maximization algorithm thatsimultaneously achieves estimation of the latent process, stimulus responses,and spike-history effects. The proposed method is useful to analyze aninteraction of internal cortical states and sensory evoked activity.
arxiv-4500-3 | Teleoperation System Using Past Image Records Considering Narrow Communication Band | http://arxiv.org/pdf/1312.4346v1.pdf | author:Noritaka Sato, Masataka Ito, Yoshifumi Morita, Fumitoshi Matsuno category:cs.RO cs.CV published:2013-12-16 summary:Teleoperation is necessary when the robot is applied to real missions, forexample surveillance, search and rescue. We proposed teleoperation system usingpast image records (SPIR). SPIR virtually generates the bird's-eye view imageby overlaying the CG model of the robot at the corresponding current positionon the background image which is captured from the camera mounted on the robotat a past time. The problem for SPIR is that the communication bandwidth isoften narrow in some teleoperation tasks. In this case, the candidates ofbackground image of SPIR are few and the position of the robot is oftendelayed. In this study, we propose zoom function for insufficiency ofcandidates of the background image and additional interpolation lines for thedelay of the position data of the robot. To evaluate proposed system, anoutdoor experiments are carried out. The outdoor experiment is conducted on atraining course of a driving school.
arxiv-4500-4 | Probable convexity and its application to Correlated Topic Models | http://arxiv.org/pdf/1312.4527v1.pdf | author:Khoat Than, Tu Bao Ho category:cs.LG stat.ML published:2013-12-16 summary:Non-convex optimization problems often arise from probabilistic modeling,such as estimation of posterior distributions. Non-convexity makes the problemsintractable, and poses various obstacles for us to design efficient algorithms.In this work, we attack non-convexity by first introducing the concept of\emph{probable convexity} for analyzing convexity of real functions inpractice. We then use the new concept to analyze an inference problem in the\emph{Correlated Topic Model} (CTM) and related nonconjugate models. Contraryto the existing belief of intractability, we show that this inference problemis concave under certain conditions. One consequence of our analyses is a novelalgorithm for learning CTM which is significantly more scalable and qualitativethan existing methods. Finally, we highlight that stochastic gradientalgorithms might be a practical choice to resolve efficiently non-convexproblems. This finding might find beneficial in many contexts which are beyondprobabilistic modeling.
arxiv-4500-5 | Vision-Guided Robot Hearing | http://arxiv.org/pdf/1311.2460v2.pdf | author:Xavier Alameda-Pineda, Radu Horaud category:cs.RO cs.CV published:2013-11-06 summary:Natural human-robot interaction in complex and unpredictable environments isone of the main research lines in robotics. In typical real-world scenarios,humans are at some distance from the robot and the acquired signals arestrongly impaired by noise, reverberations and other interfering sources. Inthis context, the detection and localisation of speakers plays a key role sinceit is the pillar on which several tasks (e.g.: speech recognition and speakertracking) rely. We address the problem of how to detect and localize peoplethat are both seen and heard by a humanoid robot. We introduce a hybriddeterministic/probabilistic model. Indeed, the deterministic component allowsus to map the visual information into the auditory space. By means of theprobabilistic component, the visual features guide the grouping of the auditoryfeatures in order to form AV objects. The proposed model and the associatedalgorithm are implemented in real-time (17 FPS) using a stereoscopic camerapair and two microphones embedded into the head of the humanoid robot NAO. Weperformed experiments on (i) synthetic data, (ii) a publicly available data setand (iii) data acquired using the robot. The results we obtained validate theapproach and encourage us to further investigate how vision can help robothearing.
arxiv-4500-6 | Near-Optimal Bayesian Active Learning with Noisy Observations | http://arxiv.org/pdf/1010.3091v2.pdf | author:Daniel Golovin, Andreas Krause, Debajyoti Ray category:cs.LG cs.AI cs.DS published:2010-10-15 summary:We tackle the fundamental problem of Bayesian active learning with noise,where we need to adaptively select from a number of expensive tests in order toidentify an unknown hypothesis sampled from a known prior distribution. In thecase of noise-free observations, a greedy algorithm called generalized binarysearch (GBS) is known to perform near-optimally. We show that if theobservations are noisy, perhaps surprisingly, GBS can perform very poorly. Wedevelop EC2, a novel, greedy active learning algorithm and prove that it iscompetitive with the optimal policy, thus obtaining the first competitivenessguarantees for Bayesian active learning with noisy observations. Our boundsrely on a recently discovered diminishing returns property called adaptivesubmodularity, generalizing the classical notion of submodular set functions toadaptive policies. Our results hold even if the tests have non-uniform cost andtheir noise is correlated. We also propose EffECXtive, a particularly fastapproximation of EC2, and evaluate it on a Bayesian experimental design probleminvolving human subjects, intended to tease apart competing economic theoriesof how people make decisions under uncertainty.
arxiv-4500-7 | Feature Graph Architectures | http://arxiv.org/pdf/1312.4209v1.pdf | author:Richard Davis, Sanjay Chawla, Philip Leong category:cs.LG published:2013-12-15 summary:In this article we propose feature graph architectures (FGA), which are deeplearning systems employing a structured initialisation and training methodbased on a feature graph which facilitates improved generalisation performancecompared with a standard shallow architecture. The goal is to explorealternative perspectives on the problem of deep network training. We evaluateFGA performance for deep SVMs on some experimental datasets, and show howgeneralisation and stability results may be derived for these models. Wedescribe the effect of permutations on the model accuracy, and give a criterionfor the optimal permutation in terms of feature correlations. The experimentalresults show that the algorithm produces robust and significant test setimprovements over a standard shallow SVM training method for a range ofdatasets. These gains are achieved with a moderate increase in time complexity.
arxiv-4500-8 | A Framework for Genetic Algorithms Based on Hadoop | http://arxiv.org/pdf/1312.0086v2.pdf | author:Filomena Ferrucci, M-Tahar Kechadi, Pasquale Salza, Federica Sarro category:cs.NE cs.DC published:2013-11-30 summary:Genetic Algorithms (GAs) are powerful metaheuristic techniques mostly used inmany real-world applications. The sequential execution of GAs requiresconsiderable computational power both in time and resources. Nevertheless, GAsare naturally parallel and accessing a parallel platform such as Cloud is easyand cheap. Apache Hadoop is one of the common services that can be used forparallel applications. However, using Hadoop to develop a parallel version ofGAs is not simple without facing its inner workings. Even though somesequential frameworks for GAs already exist, there is no framework supportingthe development of GA applications that can be executed in parallel. In thispaper is described a framework for parallel GAs on the Hadoop platform,following the paradigm of MapReduce. The main purpose of this framework is toallow the user to focus on the aspects of GA that are specific to the problemto be addressed, being sure that this task is going to be correctly executed onthe Cloud with a good performance. The framework has been also exploited todevelop an application for Feature Subset Selection problem. A preliminaryanalysis of the performance of the developed GA application has been performedusing three datasets and shown very promising performance.
arxiv-4500-9 | Face Detection from still and Video Images using Unsupervised Cellular Automata with K means clustering algorithm | http://arxiv.org/pdf/1312.6834v1.pdf | author:P. Kiran Sree, I. Ramesh Babu category:cs.CV published:2013-12-15 summary:Pattern recognition problem rely upon the features inherent in the pattern ofimages. Face detection and recognition is one of the challenging research areasin the field of computer vision. In this paper, we present a method to identifyskin pixels from still and video images using skin color. Face regions areidentified from this skin pixel region. Facial features such as eyes, nose andmouth are then located. Faces are recognized from color images using an RBFbased neural network. Unsupervised Cellular Automata with K means clusteringalgorithm is used to locate different facial elements. Orientation is correctedby using eyes. Parameters like inter eye distance, nose length, mouth position,Discrete Cosine Transform (DCT) coefficients etc. are computed and used for aRadial Basis Function (RBF) based neural network. This approach reliably worksfor face sequence with orientation in head, expressions etc.
arxiv-4500-10 | Autonomous Quantum Perceptron Neural Network | http://arxiv.org/pdf/1312.4149v1.pdf | author:Alaa Sagheer, Mohammed Zidan category:cs.NE published:2013-12-15 summary:Recently, with the rapid development of technology, there are a lot ofapplications require to achieve low-cost learning. However the computationalpower of classical artificial neural networks, they are not capable to providelow-cost learning. In contrast, quantum neural networks may be representing agood computational alternate to classical neural network approaches, based onthe computational power of quantum bit (qubit) over the classical bit. In thispaper we present a new computational approach to the quantum perceptron neuralnetwork can achieve learning in low-cost computation. The proposed approach hasonly one neuron can construct self-adaptive activation operators capable toaccomplish the learning process in a limited number of iterations and, thereby,reduce the overall computational cost. The proposed approach is capable toconstruct its own set of activation operators to be applied widely in bothquantum and classical applications to overcome the linearity limitation ofclassical perceptron. The computational power of the proposed approach isillustrated via solving variety of problems where promising and comparableresults are given.
arxiv-4500-11 | An introduction to synchronous self-learning Pareto strategy | http://arxiv.org/pdf/1312.4132v1.pdf | author:Ahmad Mozaffari, Alireza Fathi category:cs.NE published:2013-12-15 summary:In last decades optimization and control of complex systems that possessedvarious conflicted objectives simultaneously attracted an incremental interestof scientists. This is because of the vast applications of these systems invarious fields of real life engineering phenomena that are generally multimodal, non convex and multi criterion. Hence, many researchers utilizedversatile intelligent models such as Pareto based techniques, game theory(cooperative and non cooperative games), neuro evolutionary systems, fuzzylogic and advanced neural networks for handling these types of problems. Inthis paper a novel method called Synchronous Self Learning Pareto StrategyAlgorithm (SSLPSA) is presented which utilizes Evolutionary Computing (EC),Swarm Intelligence (SI) techniques and adaptive Classical Self Organizing Map(CSOM) simultaneously incorporating with a data shuffling behavior.Evolutionary Algorithms (EA) which attempt to simulate the phenomenon ofnatural evolution are powerful numerical optimization algorithms that reach anapproximate global maximum of a complex multi variable function over a widesearch space and swarm base technique can improved the intensity and therobustness in EA. CSOM is a neural network capable of learning and can improvethe quality of obtained optimal Pareto front. To prove the efficientperformance of proposed algorithm, authors utilized some well known benchmarktest functions. Obtained results indicate that the cited method is best suit inthe case of vector optimization.
arxiv-4500-12 | A robust Iris recognition method on adverse conditions | http://arxiv.org/pdf/1312.4124v1.pdf | author:Maryam Soltanali Khalili, Hamed Sadjedi category:cs.CV published:2013-12-15 summary:As a stable biometric system, iris has recently attracted great attentionamong the researchers. However, research is still needed to provide appropriatesolutions to ensure the resistance of the system against error factors. Thepresent study has tried to apply a mask to the image so that the unexpectedfactors affecting the location of the iris can be removed. So, pupillocalization will be faster and robust. Then to locate the exact location ofthe iris, a simple stage of boundary displacement due to the Canny edgedetector has been applied. Then, with searching left and right IRIS edge point,outer radios of IRIS will be detect. Through the process of extracting the irisfeatures, it has been sought to obtain the distinctive iris texture features byusing a discrete stationary wavelets transform 2-D (DSWT2). Using DSWT2 tooland symlet 4 wavelet, distinctive features are extracted. To reduce thecomputational cost, the features obtained from the application of the wavelethave been investigated and a feature selection procedure, using similaritycriteria, has been implemented. Finally, the iris matching has been performedusing a semi-correlation criterion. The accuracy of the proposed method forlocalization on CASIA-v1, CASIA-v3 is 99.73%, 98.24% and 97.04%, respectively.The accuracy of the feature extraction proposed method for CASIA3 iris imagesdatabase is 97.82%, which confirms the efficiency of the proposed method.
arxiv-4500-13 | A MapReduce based distributed SVM algorithm for binary classification | http://arxiv.org/pdf/1312.4108v1.pdf | author:Ferhat Özgür Çatak, Mehmet Erdal Balaban category:cs.LG cs.DC published:2013-12-15 summary:Although Support Vector Machine (SVM) algorithm has a high generalizationproperty to classify for unseen examples after training phase and it has smallloss value, the algorithm is not suitable for real-life classification andregression problems. SVMs cannot solve hundreds of thousands examples intraining dataset. In previous studies on distributed machine learningalgorithms, SVM is trained over a costly and preconfigured computerenvironment. In this research, we present a MapReduce based distributedparallel SVM training algorithm for binary classification problems. This workshows how to distribute optimization problem over cloud computing systems withMapReduce technique. In the second step of this work, we used statisticallearning theory to find the predictive hypothesis that minimize our empiricalrisks from hypothesis spaces that created with reduce function of MapReduce.The results of this research are important for training of big datasets for SVMalgorithm based classification problems. We provided that iterative training ofsplit dataset with MapReduce technique; accuracy of the classifier functionwill converge to global optimal classifier function's accuracy in finiteiteration size. The algorithm performance was measured on samples from letterrecognition and pen-based recognition of handwritten digits dataset.
arxiv-4500-14 | Domain adaptation for sequence labeling using hidden Markov models | http://arxiv.org/pdf/1312.4092v1.pdf | author:Edouard Grave, Guillaume Obozinski, Francis Bach category:cs.CL cs.LG published:2013-12-14 summary:Most natural language processing systems based on machine learning are notrobust to domain shift. For example, a state-of-the-art syntactic dependencyparser trained on Wall Street Journal sentences has an absolute drop inperformance of more than ten points when tested on textual data from the Web.An efficient solution to make these methods more robust to domain shift is tofirst learn a word representation using large amounts of unlabeled data fromboth domains, and then use this representation as features in a supervisedlearning algorithm. In this paper, we propose to use hidden Markov models tolearn word representations for part-of-speech tagging. In particular, we studythe influence of using data from the source, the target or both domains tolearn the representation and the different ways to represent words using anHMM.
arxiv-4500-15 | A natural-inspired optimization machine based on the annual migration of salmons in nature | http://arxiv.org/pdf/1312.4078v1.pdf | author:Ahmad Mozaffari, Alireza Fathi category:cs.NE published:2013-12-14 summary:Bio inspiration is a branch of artificial simulation science that showspervasive contributions to variety of engineering fields such as automatepattern recognition, systematic fault detection and applied optimization. Inthis paper, a new metaheuristic optimizing algorithm that is the simulation ofThe Great Salmon Run(TGSR) is developed. The obtained results imply on theacceptable performance of implemented method in optimization of complex nonconvex, multi dimensional and multi-modal problems. To prove the superiority ofTGSR in both robustness and quality, it is also compared with most of the wellknown proposed optimizing techniques such as Simulated Annealing (SA), ParallelMigrating Genetic Algorithm (PMGA), Differential Evolutionary Algorithm (DEA),Particle Swarm Optimization (PSO), Bee Algorithm (BA), Artificial Bee Colony(ABC), Firefly Algorithm (FA) and Cuckoo Search (CS). The obtained resultsconfirm the acceptable performance of the proposed method in both robustnessand quality for different bench-mark optimizing problems and also prove theauthors claim.
arxiv-4500-16 | Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm | http://arxiv.org/pdf/1312.4074v1.pdf | author:Srinjoy Ganguly, Digbalay Bose, Amit Konar category:cs.CV published:2013-12-14 summary:Clustering is an important facet of explorative data mining and findsextensive use in several fields. In this paper, we propose an extension of theclassical Fuzzy C-Means clustering algorithm. The proposed algorithm,abbreviated as VFC, adopts a multi-dimensional membership vector for each datapoint instead of the traditional, scalar membership value defined in theoriginal algorithm. The membership vector for each point is obtained byconsidering each feature of that point separately and obtaining individualmembership values for the same. We also propose an algorithm to efficientlyallocate the initial cluster centers close to the actual centers, so as tofacilitate rapid convergence. Further, we propose a scheme to achieve crispclustering using the VFC algorithm. The proposed, novel clustering scheme hasbeen tested on two standard data sets in order to analyze its performance. Wealso examine the efficacy of the proposed scheme by analyzing its performanceon image segmentation examples and comparing it with the classical FuzzyC-means clustering algorithm.
arxiv-4500-17 | CACO : Competitive Ant Colony Optimization, A Nature-Inspired Metaheuristic For Large-Scale Global Optimization | http://arxiv.org/pdf/1312.4044v1.pdf | author:M. A. El-Dosuky category:cs.NE published:2013-12-14 summary:Large-scale problems are nonlinear problems that need metaheuristics, orglobal optimization algorithms. This paper reviews nature-inspiredmetaheuristics, then it introduces a framework named Competitive Ant ColonyOptimization inspired by the chemical communications among insects. Then a casestudy is presented to investigate the proposed framework for large-scale globaloptimization.
arxiv-4500-18 | Identifying Influential Entries in a Matrix | http://arxiv.org/pdf/1310.3556v2.pdf | author:Abhisek Kundu, Srinivas Nambirajan, Petros Drineas category:cs.NA cs.LG stat.ML published:2013-10-14 summary:For any matrix A in R^(m x n) of rank \rho, we present a probabilitydistribution over the entries of A (the element-wise leverage scores ofequation (2)) that reveals the most influential entries in the matrix. From atheoretical perspective, we prove that sampling at most s = O ((m + n) \rho^2ln (m + n)) entries of the matrix (see eqn. (3) for the precise value of s)with respect to these scores and solving the nuclear norm minimization problemon the sampled entries, reconstructs A exactly. To the best of our knowledge,these are the strongest theoretical guarantees on matrix completion without anyincoherence assumptions on the matrix A. From an experimental perspective, weshow that entries corresponding to high element-wise leverage scores revealstructural properties of the data matrix that are of interest to domainscientists.
arxiv-4500-19 | High-Dimensional Covariance Decomposition into Sparse Markov and Independence Models | http://arxiv.org/pdf/1211.0919v2.pdf | author:Majid Janzamin, Animashree Anandkumar category:stat.ML math.ST stat.TH 62H12 published:2012-11-05 summary:Fitting high-dimensional data involves a delicate tradeoff between faithfulrepresentation and the use of sparse models. Too often, sparsity assumptions onthe fitted model are too restrictive to provide a faithful representation ofthe observed data. In this paper, we present a novel framework incorporatingsparsity in different domains.We decompose the observed covariance matrix intoa sparse Gaussian Markov model (with a sparse precision matrix) and a sparseindependence model (with a sparse covariance matrix). Our frameworkincorporates sparse covariance and sparse precision estimation as special casesand thus introduces a richer class of high-dimensional models. We characterizesufficient conditions for identifiability of the two models, \viz Markov andindependence models. We propose an efficient decomposition method based on amodification of the popular $\ell_1$-penalized maximum-likelihood estimator($\ell_1$-MLE). We establish that our estimator is consistent in both thedomains, i.e., it successfully recovers the supports of both Markov andindependence models, when the number of samples $n$ scales as $n = \Omega(d^2\log p)$, where $p$ is the number of variables and $d$ is the maximum nodedegree in the Markov model. Our experiments validate these results and alsodemonstrate that our models have better inference accuracy under simplealgorithms such as loopy belief propagation.
arxiv-4500-20 | ECOC-Based Training of Neural Networks for Face Recognition | http://arxiv.org/pdf/1312.3990v1.pdf | author:Nima Hatami, Reza Ebrahimpour, Reza Ghaderi category:cs.CV cs.LG published:2013-12-14 summary:Error Correcting Output Codes, ECOC, is an output representation methodcapable of discovering some of the errors produced in classification tasks.This paper describes the application of ECOC to the training of feed forwardneural networks, FFNN, for improving the overall accuracy of classificationsystems. Indeed, to improve the generalization of FFNN classifiers, this paperproposes an ECOC-Based training method for Neural Networks that use ECOC as theoutput representation, and adopts the traditional Back-Propagation algorithm,BP, to adjust weights of the network. Experimental results for face recognitionproblem on Yale database demonstrate the effectiveness of our method. With arejection scheme defined by a simple robustness rate, high reliability isachieved in this application.
arxiv-4500-21 | Classifiers With a Reject Option for Early Time-Series Classification | http://arxiv.org/pdf/1312.3989v1.pdf | author:Nima Hatami, Camelia Chira category:cs.CV cs.LG published:2013-12-14 summary:Early classification of time-series data in a dynamic environment is achallenging problem of great importance in signal processing. This paperproposes a classifier architecture with a reject option capable of onlinedecision making without the need to wait for the entire time series signal tobe present. The main idea is to classify an odor/gas signal with an acceptableaccuracy as early as possible. Instead of using posterior probability of aclassifier, the proposed method uses the "agreement" of an ensemble to decidewhether to accept or reject the candidate label. The introduced algorithm isapplied to the bio-chemistry problem of odor classification to build a novelElectronic-Nose called Forefront-Nose. Experimental results on wind tunneltest-bed facility confirms the robustness of the forefront-nose compared to thestandard classifiers from both earliness and recognition perspectives.
arxiv-4500-22 | An Extensive Evaluation of Filtering Misclassified Instances in Supervised Classification Tasks | http://arxiv.org/pdf/1312.3970v1.pdf | author:Michael R. Smith, Tony Martinez category:cs.LG stat.ML published:2013-12-13 summary:Removing or filtering outliers and mislabeled instances prior to training alearning algorithm has been shown to increase classification accuracy. Apopular approach for handling outliers and mislabeled instances is to removeany instance that is misclassified by a learning algorithm. However, anexamination of which learning algorithms to use for filtering as well as theireffects on multiple learning algorithms over a large set of data sets has notbeen done. Previous work has generally been limited due to the largecomputational requirements to run such an experiment, and, thus, theexamination has generally been limited to learning algorithms that arecomputationally inexpensive and using a small number of data sets. In thispaper, we examine 9 learning algorithms as filtering algorithms as well asexamining the effects of filtering in the 9 chosen learning algorithms on a setof 54 data sets. In addition to using each learning algorithm individually as afilter, we also use the set of learning algorithms as an ensemble filter anduse an adaptive algorithm that selects a subset of the learning algorithms forfiltering for a specific task and learning algorithm. We find that for mostcases, using an ensemble of learning algorithms for filtering produces thegreatest increase in classification accuracy. We also compare filtering with amajority voting ensemble. The voting ensemble significantly outperformsfiltering unless there are high amounts of noise present in the data set.Additionally, we find that a majority voting ensemble is robust to noise asfiltering with a voting ensemble does not increase the classification accuracyof the voting ensemble.
arxiv-4500-23 | Efficient coordinate-descent for orthogonal matrices through Givens rotations | http://arxiv.org/pdf/1312.0624v2.pdf | author:Uri Shalit, Gal Chechik category:cs.LG stat.ML published:2013-12-02 summary:Optimizing over the set of orthogonal matrices is a central component inproblems like sparse-PCA or tensor decomposition. Unfortunately, suchoptimization is hard since simple operations on orthogonal matrices easilybreak orthogonality, and correcting orthogonality usually costs a large amountof computation. Here we propose a framework for optimizing orthogonal matrices,that is the parallel of coordinate-descent in Euclidean spaces. It is based on{\em Givens-rotations}, a fast-to-compute operation that affects a small numberof entries in the learned matrix, and preserves orthogonality. We show twoapplications of this approach: an algorithm for tensor decomposition that isused in learning mixture models, and an algorithm for sparse-PCA. We study theparameter regime where a Givens rotation approach converges faster and achievesa superior model on a genome-wide brain-wide mRNA expression dataset.
arxiv-4500-24 | A Methodology for Player Modeling based on Machine Learning | http://arxiv.org/pdf/1312.3903v1.pdf | author:Marlos C. Machado category:cs.AI cs.LG published:2013-12-13 summary:AI is gradually receiving more attention as a fundamental feature to increasethe immersion in digital games. Among the several AI approaches, playermodeling is becoming an important one. The main idea is to understand and modelthe player characteristics and behaviors in order to develop a better AI. Inthis work, we discuss several aspects of this new field. We proposed a taxonomyto organize the area, discussing several facets of this topic, ranging fromimplementation decisions up to what a model attempts to describe. We thenclassify, in our taxonomy, some of the most important works in this field. Wealso presented a generic approach to deal with player modeling using ML, and weinstantiated this approach to model players' preferences in the gameCivilization IV. The instantiation of this approach has several steps. We firstdiscuss a generic representation, regardless of what is being modeled, andevaluate it performing experiments with the strategy game Civilization IV.Continuing the instantiation of the proposed approach we evaluated theapplicability of using game score information to distinguish differentpreferences. We presented a characterization of virtual agents in the game,comparing their behavior with their stated preferences. Once we havecharacterized these agents, we were able to observe that different preferencesgenerate different behaviors, measured by several game indicators. We thentackled the preference modeling problem as a binary classification task, with asupervised learning approach. We compared four different methods, based ondifferent paradigms (SVM, AdaBoost, NaiveBayes and JRip), evaluating them on aset of matches played by different virtual agents. We conclude our work usingthe learned models to infer human players' preferences. Using some of theevaluated classifiers we obtained accuracies over 60% for most of the inferredpreferences.
arxiv-4500-25 | Efficient Baseline-free Sampling in Parameter Exploring Policy Gradients: Super Symmetric PGPE | http://arxiv.org/pdf/1312.3811v1.pdf | author:Frank Sehnke category:cs.LG published:2013-12-13 summary:Policy Gradient methods that explore directly in parameter space are amongthe most effective and robust direct policy search methods and have drawn a lotof attention lately. The basic method from this field, Policy Gradients withParameter-based Exploration, uses two samples that are symmetric around thecurrent hypothesis to circumvent misleading reward in \emph{asymmetrical}reward distributed problems gathered with the usual baseline approach. Theexploration parameters are still updated by a baseline approach - leaving theexploration prone to asymmetric reward distributions. In this paper we willshow how the exploration parameters can be sampled quasi symmetric despitehaving limited instead of free parameters for exploration. We give atransformation approximation to get quasi symmetric samples with respect to theexploration without changing the overall sampling distribution. Finally we willdemonstrate that sampling symmetrically also for the exploration parameters issuperior in needs of samples and robustness than the original samplingapproach.
arxiv-4500-26 | ARIANNA: pAth Recognition for Indoor Assisted NavigatioN with Augmented perception | http://arxiv.org/pdf/1312.3724v1.pdf | author:Pierluigi Gallo, Ilenia Tinnirello, Laura Giarré, Domenico Garlisi, Daniele Croce, Adriano Fagiolini category:cs.CV cs.HC published:2013-12-13 summary:ARIANNA stands for pAth Recognition for Indoor Assisted Navigation withAugmented perception. It is a flexible and low cost navigation system for vi-sually impaired people. Arianna permits to navigate colored paths painted orsticked on the floor revealing their directions through vibrational feedback oncommercial smartphones.
arxiv-4500-27 | Consistent selection of tuning parameters via variable selection stability | http://arxiv.org/pdf/1208.3380v2.pdf | author:Wei Sun, Junhui Wang, Yixin Fang category:stat.ML stat.ME published:2012-08-16 summary:Penalized regression models are popularly used in high-dimensional dataanalysis to conduct variable selection and model fitting simultaneously.Whereas success has been widely reported in literature, their performanceslargely depend on the tuning parameters that balance the trade-off betweenmodel fitting and model sparsity. Existing tuning criteria mainly follow theroute of minimizing the estimated prediction error or maximizing the posteriormodel probability, such as cross-validation, AIC and BIC. This articleintroduces a general tuning parameter selection criterion based on a novelconcept of variable selection stability. The key idea is to select the tuningparameters so that the resultant penalized regression model is stable invariable selection. The asymptotic selection consistency is established forboth fixed and diverging dimensions. The effectiveness of the proposedcriterion is also demonstrated in a variety of simulated examples as well as anapplication to the prostate cancer data.
arxiv-4500-28 | A Latent Source Model for Nonparametric Time Series Classification | http://arxiv.org/pdf/1302.3639v5.pdf | author:George H. Chen, Stanislav Nikolov, Devavrat Shah category:stat.ML cs.LG cs.SI published:2013-02-14 summary:For classifying time series, a nearest-neighbor approach is widely used inpractice with performance often competitive with or better than more elaboratemethods such as neural networks, decision trees, and support vector machines.We develop theoretical justification for the effectiveness ofnearest-neighbor-like classification of time series. Our guiding hypothesis isthat in many applications, such as forecasting which topics will become trendson Twitter, there aren't actually that many prototypical time series to beginwith, relative to the number of time series we have access to, e.g., topicsbecome trends on Twitter only in a few distinct manners whereas we can collectmassive amounts of Twitter data. To operationalize this hypothesis, we proposea latent source model for time series, which naturally leads to a "weightedmajority voting" classification rule that can be approximated by anearest-neighbor classifier. We establish nonasymptotic performance guaranteesof both weighted majority voting and nearest-neighbor classification under ourmodel accounting for how much of the time series we observe and the modelcomplexity. Experimental results on synthetic data show weighted majorityvoting achieving the same misclassification rate as nearest-neighborclassification while observing less of the time series. We then use weightedmajority to forecast which news topics on Twitter become trends, where we areable to detect such "trending topics" in advance of Twitter 79% of the time,with a mean early advantage of 1 hour and 26 minutes, a true positive rate of95%, and a false positive rate of 4%.
arxiv-4500-29 | Oracle Inequalities for Convex Loss Functions with Non-Linear Targets | http://arxiv.org/pdf/1312.3525v1.pdf | author:Mehmet Caner, Anders Bredahl Kock category:math.ST stat.ML stat.TH published:2013-12-12 summary:This paper consider penalized empirical loss minimization of convex lossfunctions with unknown non-linear target functions. Using the elastic netpenalty we establish a finite sample oracle inequality which bounds the loss ofour estimator from above with high probability. If the unknown target is linearthis inequality also provides an upper bound of the estimation error of theestimated parameter vector. These are new results and they generalize theeconometrics and statistics literature. Next, we use the non-asymptotic resultsto show that the excess loss of our estimator is asymptotically of the sameorder as that of the oracle. If the target is linear we give sufficientconditions for consistency of the estimated parameter vector. Next, we brieflydiscuss how a thresholded version of our estimator can be used to performconsistent variable selection. We give two examples of loss functions coveredby our framework and show how penalized nonparametric series estimation iscontained as a special case and provide a finite sample upper bound on the meansquare error of the elastic net series estimator.
arxiv-4500-30 | Online Bayesian Passive-Aggressive Learning | http://arxiv.org/pdf/1312.3388v1.pdf | author:Tianlin Shi, Jun Zhu category:cs.LG published:2013-12-12 summary:Online Passive-Aggressive (PA) learning is an effective framework forperforming max-margin online learning. But the deterministic formulation andestimated single large-margin model could limit its capability in discoveringdescriptive structures underlying complex data. This pa- per presents onlineBayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PAand extends naturally to incorporate latent variables and perform nonparametricBayesian inference, thus providing great flexibility for explorative analysis.We apply BayesPA to topic modeling and derive efficient online learningalgorithms for max-margin topic models. We further develop nonparametricmethods to resolve the number of topics. Experimental results on real datasetsshow that our approaches significantly improve time efficiency whilemaintaining comparable results with the batch counterparts.
arxiv-4500-31 | Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic | http://arxiv.org/pdf/1312.3291v1.pdf | author:James Sharpnack, Akshay Krishnamurthy, Aarti Singh category:stat.ML 62G10 published:2013-12-11 summary:The detection of anomalous activity in graphs is a statistical problem thatarises in many applications, such as network surveillance, disease outbreakdetection, and activity monitoring in social networks. Beyond its wideapplicability, graph structured anomaly detection serves as a case study in thedifficulty of balancing computational complexity with statistical power. Inthis work, we develop from first principles the generalized likelihood ratiotest for determining if there is a well connected region of activation over thevertices in the graph in Gaussian noise. Because this test is computationallyinfeasible, we provide a relaxation, called the Lovasz extended scan statistic(LESS) that uses submodularity to approximate the intractable generalizedlikelihood ratio. We demonstrate a connection between LESS and maximuma-posteriori inference in Markov random fields, which provides us with apoly-time algorithm for LESS. Using electrical network theory, we are able tocontrol type 1 error for LESS and prove conditions under which LESS is riskconsistent. Finally, we consider specific graph models, the torus, k-nearestneighbor graphs, and epsilon-random graphs. We show that on these graphs ourresults provide near-optimal performance by matching our results to known lowerbounds.
arxiv-4500-32 | Implicit Sensitive Text Summarization based on Data Conveyed by Connectives | http://arxiv.org/pdf/1312.3258v1.pdf | author:Henda Chorfi Ouertani category:cs.CL published:2013-12-11 summary:So far and trying to reach human capabilities, research in automaticsummarization has been based on hypothesis that are both enabling and limiting.Some of these limitations are: how to take into account and reflect (in thegenerated summary) the implicit information conveyed in the text, the authorintention, the reader intention, the context influence, the general worldknowledge. Thus, if we want machines to mimic human abilities, then they willneed access to this same large variety of knowledge. The implicit is affectingthe orientation and the argumentation of the text and consequently its summary.Most of Text Summarizers (TS) are processing as compressing the initial dataand they necessarily suffer from information loss. TS are focusing on featuresof the text only, not on what the author intended or why the reader is readingthe text. In this paper, we address this problem and we present a systemfocusing on acquiring knowledge that is implicit. We principally spotlight theimplicit information conveyed by the argumentative connectives such as: but,even, yet and their effect on the summary.
arxiv-4500-33 | Towards The Development of a Bishnupriya Manipuri Corpus | http://arxiv.org/pdf/1312.3251v1.pdf | author:Nayan Jyoti Kalita, Navanath Saharia, Smriti Kumar Sinha category:cs.CL published:2013-12-11 summary:For any deep computational processing of language we need evidences, and onesuch set of evidences is corpus. This paper describes the development of atext-based corpus for the Bishnupriya Manipuri language. A Corpus is consideredas a building block for any language processing tasks. Due to the lack ofawareness like other Indian languages, it is also studied less frequently. As aresult the language still lacks a good corpus and basic language processingtools. As per our knowledge this is the first effort to develop a corpus forBishnupriya Manipuri language.
arxiv-4500-34 | Thickness Mapping of Eleven Retinal Layers in Normal Eyes Using Spectral Domain Optical Coherence Tomography | http://arxiv.org/pdf/1312.3199v1.pdf | author:Raheleh Kafieh, Hossein Rabbani, Fedra Hajizadeh, Michael D. Abramoff, Milan Sonka category:cs.CV published:2013-12-11 summary:Purpose. This study was conducted to determine the thickness map of elevenretinal layers in normal subjects by spectral domain optical coherencetomography (SD-OCT) and evaluate their association with sex and age. Methods.Mean regional retinal thickness of 11 retinal layers were obtained by automaticthree-dimensional diffusion-map-based method in 112 normal eyes of 76 Iraniansubjects. Results. The thickness map of central foveal area in layer 1, 3, and4 displayed the minimum thickness (P<0.005 for all). Maximum thickness wasobserved in nasal to the fovea of layer 1 (P<0.001) and in a circular patternin the parafoveal retinal area of layers 2, 3 and 4 and in central foveal areaof layer 6 (P<0.001). Temporal and inferior quadrants of the total retinalthickness and most of other quadrants of layer 1 were significantly greater inthe men than in the women. Surrounding eight sectors of total retinal thicknessand a limited number of sectors in layer 1 and 4 significantly correlated withage. Conclusion. SD-OCT demonstrated the three-dimensional thicknessdistribution of retinal layers in normal eyes. Thickness of layers varied withsex and age and in different sectors. These variables should be consideredwhile evaluating macular thickness.
arxiv-4500-35 | Semantic Types, Lexical Sorts and Classifiers | http://arxiv.org/pdf/1312.3168v1.pdf | author:Bruno Mery, Christian Retoré category:cs.CL published:2013-12-11 summary:We propose a cognitively and linguistically motivated set of sorts forlexical semantics in a compositional setting: the classifiers in languages thatdo have such pronouns. These sorts are needed to include lexical considerationsin a semantical analyser such as Boxer or Grail. Indeed, all proposed lexicalextensions of usual Montague semantics to model restriction of selection,felicitous and infelicitous copredication require a rich and refined typesystem whose base types are the lexical sorts, the basis of the many-sortedlogic in which semantical representations of sentences are stated. However,none of those approaches define precisely the actual base types or sorts to beused in the lexicon. In this article, we shall discuss some of the optionscommonly adopted by researchers in formal lexical semantics, and defend theview that classifiers in the languages which have such pronouns are anappealing solution, both linguistically and cognitively motivated.
arxiv-4500-36 | Fast Neighborhood Graph Search using Cartesian Concatenation | http://arxiv.org/pdf/1312.3062v1.pdf | author:Jingdong Wang, Jing Wang, Gang Zeng, Rui Gan, Shipeng Li, Baining Guo category:cs.CV published:2013-12-11 summary:In this paper, we propose a new data structure for approximate nearestneighbor search. This structure augments the neighborhood graph with a bridgegraph. We propose to exploit Cartesian concatenation to produce a large set ofvectors, called bridge vectors, from several small sets of subvectors. Eachbridge vector is connected with a few reference vectors near to it, forming abridge graph. Our approach finds nearest neighbors by simultaneously traversingthe neighborhood graph and the bridge graph in the best-first strategy. Thesuccess of our approach stems from two factors: the exact nearest neighborsearch over a large number of bridge vectors can be done quickly, and thereference vectors connected to a bridge (reference) vector near the query arealso likely to be near the query. Experimental results on searching over largescale datasets (SIFT, GIST and HOG) show that our approach outperformsstate-of-the-art ANN search algorithms in terms of efficiency and accuracy. Thecombination of our approach with the IVFADC system also shows superiorperformance over the BIGANN dataset of $1$ billion SIFT features compared withthe best previously published result.
arxiv-4500-37 | Fast Approximate $K$-Means via Cluster Closures | http://arxiv.org/pdf/1312.3061v1.pdf | author:Jingdong Wang, Jing Wang, Qifa Ke, Gang Zeng, Shipeng Li category:cs.CV published:2013-12-11 summary:$K$-means, a simple and effective clustering algorithm, is one of the mostwidely used algorithms in multimedia and computer vision community. Traditional$k$-means is an iterative algorithm---in each iteration new cluster centers arecomputed and each data point is re-assigned to its nearest center. The clusterre-assignment step becomes prohibitively expensive when the number of datapoints and cluster centers are large. In this paper, we propose a novel approximate $k$-means algorithm to greatlyreduce the computational complexity in the assignment step. Our approach ismotivated by the observation that most active points changing their clusterassignments at each iteration are located on or near cluster boundaries. Theidea is to efficiently identify those active points by pre-assembling the datainto groups of neighboring points using multiple random spatial partitiontrees, and to use the neighborhood information to construct a closure for eachcluster, in such a way only a small number of cluster candidates need to beconsidered when assigning a data point to its nearest cluster. Using complexityanalysis, image data clustering, and applications to image retrieval, we showthat our approach out-performs state-of-the-art approximate $k$-meansalgorithms in terms of clustering quality and efficiency.
arxiv-4500-38 | Heat kernel coupling for multiple graph analysis | http://arxiv.org/pdf/1312.3035v1.pdf | author:Michael M. Bronstein, Klaus Glashoff category:cs.CV published:2013-12-11 summary:In this paper, we introduce heat kernel coupling (HKC) as a method ofconstructing multimodal spectral geometry on weighted graphs of different sizewithout vertex-wise bijective correspondence. We show that Laplacian averagingcan be derived as a limit case of HKC, and demonstrate its applications onseveral problems from the manifold learning and pattern recognition domain.
arxiv-4500-39 | Active Player Modelling | http://arxiv.org/pdf/1312.2936v1.pdf | author:Julian Togelius, Noor Shaker, Georgios N. Yannakakis category:cs.LG published:2013-12-10 summary:We argue for the use of active learning methods for player modelling. Inactive learning, the learning algorithm chooses where to sample the searchspace so as to optimise learning progress. We hypothesise that player modellingbased on active learning could result in vastly more efficient learning, butwill require big changes in how data is collected. Some example active playermodelling scenarios are described. A particular form of active learning is alsoequivalent to an influential formalisation of (human and machine) curiosity,and games with active learning could therefore be seen as being curious aboutthe player. We further hypothesise that this form of curiosity is symmetric,and therefore that games that explore their players based on the principles ofactive learning will turn out to select game configurations that areinteresting to the player that is being explored.
arxiv-4500-40 | Automated Classification of L/R Hand Movement EEG Signals using Advanced Feature Extraction and Machine Learning | http://arxiv.org/pdf/1312.2877v1.pdf | author:Mohammad H. Alomari, Aya Samaha, Khaled AlKamha category:cs.NE cs.CV cs.HC published:2013-12-10 summary:In this paper, we propose an automated computer platform for the purpose ofclassifying Electroencephalography (EEG) signals associated with left and righthand movements using a hybrid system that uses advanced feature extractiontechniques and machine learning algorithms. It is known that EEG represents thebrain activity by the electrical voltage fluctuations along the scalp, andBrain-Computer Interface (BCI) is a device that enables the use of the brainneural activity to communicate with others or to control machines, artificiallimbs, or robots without direct physical movements. In our research work, weaspired to find the best feature extraction method that enables thedifferentiation between left and right executed fist movements through variousclassification algorithms. The EEG dataset used in this research was createdand contributed to PhysioNet by the developers of the BCI2000 instrumentationsystem. Data was preprocessed using the EEGLAB MATLAB toolbox and artifactsremoval was done using AAR. Data was epoched on the basis of Event-Related (De)Synchronization (ERD/ERS) and movement-related cortical potentials (MRCP)features. Mu/beta rhythms were isolated for the ERD/ERS analysis and deltarhythms were isolated for the MRCP analysis. The Independent Component Analysis(ICA) spatial filter was applied on related channels for noise reduction andisolation of both artifactually and neutrally generated EEG sources. The finalfeature vector included the ERD, ERS, and MRCP features in addition to themean, power and energy of the activations of the resulting independentcomponents of the epoched feature datasets. The datasets were inputted into twomachine-learning algorithms: Neural Networks (NNs) and Support Vector Machines(SVMs). Intensive experiments were carried out and optimum classificationperformances of 89.8 and 97.1 were obtained using NN and SVM, respectively.
arxiv-4500-41 | Wavelet and Fast Fourier Transform based analysis of Solar Image | http://arxiv.org/pdf/1311.6799v2.pdf | author:Sabyasachi Mukhopadhyay, Debadatta Dash, Swapnil Barmase, Prasanta K Panigrahi category:cs.CV cs.CE published:2013-10-30 summary:Both of Wavelet and Fast Fourier Transform are strong signal processing toolsin the field of Data Analysis. In this paper fast fourier transform (FFT) andWavelet Transform are employed to observe some important features of Solarimage (December, 2004). We have tried to find out the periodicity and coherenceof different sections of the solar image. We plotted the distribution of energyin solar surface by analyzing the solar image with scalograms and3D-coefficient plots.
arxiv-4500-42 | Performance Analysis Of Neural Network Models For Oxazolines And Oxazoles Derivatives Descriptor Dataset | http://arxiv.org/pdf/1312.2853v1.pdf | author:Doreswamy, Chanabasayya . M. Vastrad category:cs.CE cs.NE published:2013-12-10 summary:Neural networks have been used successfully to a broad range of areas such asbusiness, data mining, drug discovery and biology. In medicine, neural networkshave been applied widely in medical diagnosis, detection and evaluation of newdrugs and treatment cost estimation. In addition, neural networks have beginpractice in data mining strategies for the aim of prediction, knowledgediscovery. This paper will present the application of neural networks for theprediction and analysis of antitubercular activity of Oxazolines and Oxazolesderivatives. This study presents techniques based on the development of Singlehidden layer neural network (SHLFFNN), Gradient Descent Back propagation neuralnetwork (GDBPNN), Gradient Descent Back propagation with momentum neuralnetwork (GDBPMNN), Back propagation with Weight decay neural network (BPWDNN)and Quantile regression neural network (QRNN) of artificial neural network(ANN) models Here, we comparatively evaluate the performance of five neuralnetwork techniques. The evaluation of the efficiency of each model by ways ofbenchmark experiments is an accepted application. Cross-validation andresampling techniques are commonly used to derive point estimates of theperformances which are compared to identify methods with good properties.Predictive accuracy was evaluated using the root mean squared error (RMSE),Coefficient determination(???), mean absolute error(MAE), mean percentageerror(MPE) and relative square error(RSE). We found that all five neuralnetwork models were able to produce feasible models. QRNN model is outperformswith all statistical tests amongst other four models.
arxiv-4500-43 | mARC: Memory by Association and Reinforcement of Contexts | http://arxiv.org/pdf/1312.2844v1.pdf | author:Norbert Rimoux, Patrice Descourt category:cs.IR cs.CL nlin.AO nlin.CD published:2013-12-10 summary:This paper introduces the memory by Association and Reinforcement of Contexts(mARC). mARC is a novel data modeling technology rooted in the secondquantization formulation of quantum mechanics. It is an all-purpose incrementaland unsupervised data storage and retrieval system which can be applied to alltypes of signal or data, structured or unstructured, textual or not. mARC canbe applied to a wide range of information clas-sification and retrievalproblems like e-Discovery or contextual navigation. It can also for-mulated inthe artificial life framework a.k.a Conway "Game Of Life" Theory. In contrastto Conway approach, the objects evolve in a massively multidimensional space.In order to start evaluating the potential of mARC we have built a mARC-basedInternet search en-gine demonstrator with contextual functionality. We comparethe behavior of the mARC demonstrator with Google search both in terms ofperformance and relevance. In the study we find that the mARC search enginedemonstrator outperforms Google search by an order of magnitude in responsetime while providing more relevant results for some classes of queries.
arxiv-4500-44 | Explore or exploit? A generic model and an exactly solvable case | http://arxiv.org/pdf/1310.5114v3.pdf | author:Thomas Gueudré, Alexander Dobrinevski, Jean-Philippe Bouchaud category:cs.LG physics.soc-ph q-fin.GN published:2013-10-18 summary:Finding a good compromise between the exploitation of known resources and theexploration of unknown, but potentially more profitable choices, is a generalproblem, which arises in many different scientific disciplines. We propose astylized model for these exploration-exploitation situations, includingpopulation or economic growth, portfolio optimisation, evolutionary dynamics,or the problem of optimal pinning of vortices or dislocations in disorderedmaterials. We find the exact growth rate of this model for tree-like geometriesand prove the existence of an optimal migration rate in this case. Numericalsimulations in the one-dimensional case confirm the generic existence of anoptimum.
arxiv-4500-45 | Performance Analysis Of Regularized Linear Regression Models For Oxazolines And Oxazoles Derivitive Descriptor Dataset | http://arxiv.org/pdf/1312.2789v1.pdf | author:Doreswamy, Chanabasayya . M. Vastrad category:cs.LG published:2013-12-10 summary:Regularized regression techniques for linear regression have been created thelast few ten years to reduce the flaws of ordinary least squares regressionwith regard to prediction accuracy. In this paper, new methods for usingregularized regression in model choice are introduced, and we distinguish theconditions in which regularized regression develops our ability to discriminatemodels. We applied all the five methods that use penalty-based (regularization)shrinkage to handle Oxazolines and Oxazoles derivatives descriptor dataset withfar more predictors than observations. The lasso, ridge, elasticnet, lars andrelaxed lasso further possess the desirable property that they simultaneouslyselect relevant predictive descriptors and optimally estimate their effects.Here, we comparatively evaluate the performance of five regularized linearregression methods The assessment of the performance of each model by means ofbenchmark experiments is an established exercise. Cross-validation andresampling methods are generally used to arrive point evaluates theefficiencies which are compared to recognize methods with acceptable features.Predictive accuracy was evaluated using the root mean squared error (RMSE) andSquare of usual correlation between predictors and observed mean inhibitoryconcentration of antitubercular activity (R square). We found that all fiveregularized regression models were able to produce feasible models andefficient capturing the linearity in the data. The elastic net and lars hadsimilar accuracies as well as lasso and relaxed lasso had similar accuraciesbut outperformed ridge regression in terms of the RMSE and R square metrics.
arxiv-4500-46 | Accelerating Hessian-free optimization for deep neural networks by implicit preconditioning and sampling | http://arxiv.org/pdf/1309.1508v3.pdf | author:Tara N. Sainath, Lior Horesh, Brian Kingsbury, Aleksandr Y. Aravkin, Bhuvana Ramabhadran category:cs.LG cs.CL cs.NE math.OC stat.ML published:2013-09-05 summary:Hessian-free training has become a popular parallel second or- deroptimization technique for Deep Neural Network training. This study aims atspeeding up Hessian-free training, both by means of decreasing the amount ofdata used for training, as well as through reduction of the number of Krylovsubspace solver iterations used for implicit estimation of the Hessian. In thispaper, we develop an L-BFGS based preconditioning scheme that avoids the needto access the Hessian explicitly. Since L-BFGS cannot be regarded as afixed-point iteration, we further propose the employment of flexible Krylovsubspace solvers that retain the desired theoretical convergence guarantees oftheir conventional counterparts. Second, we propose a new sampling algorithm,which geometrically increases the amount of data utilized for gradient andKrylov subspace iteration calculations. On a 50-hr English Broadcast News task,we find that these methodologies provide roughly a 1.5x speed-up, whereas, on a300-hr Switchboard task, these techniques provide over a 2.3x speedup, with noloss in WER. These results suggest that even further speed-up is expected, asproblems scale and complexity grows.
arxiv-4500-47 | Improvements to deep convolutional neural networks for LVCSR | http://arxiv.org/pdf/1309.1501v3.pdf | author:Tara N. Sainath, Brian Kingsbury, Abdel-rahman Mohamed, George E. Dahl, George Saon, Hagen Soltau, Tomas Beran, Aleksandr Y. Aravkin, Bhuvana Ramabhadran category:cs.LG cs.CL cs.NE math.OC stat.ML published:2013-09-05 summary:Deep Convolutional Neural Networks (CNNs) are more powerful than Deep NeuralNetworks (DNN), as they are able to better reduce spectral variation in theinput signal. This has also been confirmed experimentally, with CNNs showingimprovements in word error rate (WER) between 4-12% relative compared to DNNsacross a variety of LVCSR tasks. In this paper, we describe different methodsto further improve CNN performance. First, we conduct a deep analysis comparinglimited weight sharing and full weight sharing with state-of-the-art features.Second, we apply various pooling strategies that have shown improvements incomputer vision to an LVCSR speech task. Third, we introduce a method toeffectively incorporate speaker adaptation, namely fMLLR, into log-melfeatures. Fourth, we introduce an effective strategy to use dropout duringHessian-free sequence training. We find that with these improvements,particularly with fMLLR and dropout, we are able to achieve an additional 2-3%relative improvement in WER on a 50-hour Broadcast News task over our previousbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%relative improvement over our previous best CNN baseline.
arxiv-4500-48 | Improving circuit miniaturization and its efficiency using Rough Set Theory | http://arxiv.org/pdf/1312.2710v1.pdf | author:Sarvesh SS Rawat, Dheeraj Dilip Mor, Anugrah Kumar, Sanjiban Shekar Roy, Rohit kumar category:cs.LG cs.AI published:2013-12-10 summary:High-speed, accuracy, meticulousness and quick response are notion of thevital necessities for modern digital world. An efficient electronic circuitunswervingly affects the maneuver of the whole system. Different tools arerequired to unravel different types of engineering tribulations. Improving theefficiency, accuracy and low power consumption in an electronic circuit isalways been a bottle neck problem. So the need of circuit miniaturization isalways there. It saves a lot of time and power that is wasted in switching ofgates, the wiring-crises is reduced, cross-sectional area of chip is reduced,the number of transistors that can implemented in chip is multiplied manyfolds. Therefore to trounce with this problem we have proposed an Artificialintelligence (AI) based approach that make use of Rough Set Theory for itsimplementation. Theory of rough set has been proposed by Z Pawlak in the year1982. Rough set theory is a new mathematical tool which deals with uncertaintyand vagueness. Decisions can be generated using rough set theory by reducingthe unwanted and superfluous data. We have condensed the number of gateswithout upsetting the productivity of the given circuit. This paper proposes anapproach with the help of rough set theory which basically lessens the numberof gates in the circuit, based on decision rules.
arxiv-4500-49 | Multi-Task Classification Hypothesis Space with Improved Generalization Bounds | http://arxiv.org/pdf/1312.2606v1.pdf | author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2013-12-09 summary:This paper presents a RKHS, in general, of vector-valued functions intendedto be used as hypothesis space for multi-task classification. It extendssimilar hypothesis spaces that have previously considered in the literature.Assuming this space, an improved Empirical Rademacher Complexity-basedgeneralization bound is derived. The analysis is itself extended to an MKLsetting. The connection between the proposed hypothesis space and a Group-Lassotype regularizer is discussed. Finally, experimental results, with someSVM-based Multi-Task Learning problems, underline the quality of the derivedbounds and validate the paper's analysis.
arxiv-4500-50 | Object Recognition System Design in Computer Vision: a Universal Approach | http://arxiv.org/pdf/1310.7170v2.pdf | author:Andrew Gleibman category:cs.CV published:2013-10-27 summary:The first contribution of this paper is architecture of a multipurposesystem, which delegates a range of object detection tasks to a classifier,applied in special grid positions of the tested image. The second contributionis Gray Level-Radius Co-occurrence Matrix, which describes local image textureand topology and, unlike common second order statistics methods, is robust toimage resolution. The third contribution is a parametrically controlledautomatic synthesis of unlimited number of numerical features forclassification. The fourth contribution is a method of optimizing parameters Cand gamma in LibSVM-based classifier, which is 20-100 times faster than thecommonly applied method. The work is essentially experimental, withdemonstration of various methods for definition of objects of interest inimages and video sequences.
arxiv-4500-51 | On the Performance of Filters for Reduction of Speckle Noise in SAR Images off the Coast of the Gulf of Guinea | http://arxiv.org/pdf/1312.2383v1.pdf | author:Griffith S. Klogo, Akpeko Gasonoo, Isaac K. E. Ampomah category:cs.CV published:2013-12-09 summary:Synthetic Aperture Radar (SAR) imagery to monitor oil spills are some methodsthat have been proposed for the West African sub-region. With the increase inthe number of oil exploration companies in Ghana (and her neighbors) and therise in the coastal activities in the sub-region, there is the need for propermonitoring of the environmental impact of these socio-economic activities onthe environment. Detection and near real-time information about oil spills arefundamental in reducing oil spill environmental impact. SAR images are prone tosome noise, which is predominantly speckle noise around the coastal areas. Thispaper evaluates the performance of the mean and median filters used in thepreprocessing filtering to reduce speckle noise in SAR images for most imageprocessing algorithms.
arxiv-4500-52 | A Unified Markov Chain Approach to Analysing Randomised Search Heuristics | http://arxiv.org/pdf/1312.2368v1.pdf | author:Jun He, Feidun He, Xin Yao category:math.OC cs.NE published:2013-12-09 summary:The convergence, convergence rate and expected hitting time play fundamentalroles in the analysis of randomised search heuristics. This paper presents aunified Markov chain approach to studying them. Using the approach, thesufficient and necessary conditions of convergence in distribution areestablished. Then the average convergence rate is introduced to randomisedsearch heuristics and its lower and upper bounds are derived. Finally, novelaverage drift analysis and backward drift analysis are proposed for boundingthe expected hitting time. A computational study is also conducted toinvestigate the convergence, convergence rate and expected hitting time. Thetheoretical study belongs to a prior and general study while the computationalstudy belongs to a posterior and case study.
arxiv-4500-53 | A preliminary survey on optimized multiobjective metaheuristic methods for data clustering using evolutionary approaches | http://arxiv.org/pdf/1312.2366v1.pdf | author:Ramachandra Rao Kurada, Dr. K Karteeka Pavan, Dr. AV Dattareya Rao category:cs.NE published:2013-12-09 summary:The present survey provides the state-of-the-art of research, copiouslydevoted to Evolutionary Approach (EAs) for clustering exemplified with adiversity of evolutionary computations. The Survey provides a nomenclature thathighlights some aspects that are very important in the context of evolutionarydata clustering. The paper missions the clustering trade-offs branched out withwide-ranging Multi Objective Evolutionary Approaches (MOEAs) methods. Finally,this study addresses the potential challenges of MOEA design and dataclustering, along with conclusions and recommendations for novice andresearchers by positioning most promising paths of future research. MOEAs havesubstantial success across a variety of MOP applications, from pedagogicalmultifunction optimization to real-world engineering design. The survey papernoticeably organizes the developments witnessed in the past three decades forEAs based metaheuristics to solve multiobjective optimization problems (MOP)and to derive significant progression in ruling high quality elucidations in asingle run. Data clustering is an exigent task, whose intricacy is caused by alack of unique and precise definition of a cluster. The discrete optimizationproblem uses the cluster space to derive a solution for Multiobjective dataclustering. Discovery of a majority or all of the clusters (of illogicalshapes) present in the data is a long-standing goal of unsupervised predictivelearning problems or exploratory pattern analysis.
arxiv-4500-54 | Event-Driven Contrastive Divergence for Spiking Neuromorphic Systems | http://arxiv.org/pdf/1311.0966v3.pdf | author:Emre Neftci, Srinjoy Das, Bruno Pedroni, Kenneth Kreutz-Delgado, Gert Cauwenberghs category:cs.NE q-bio.NC published:2013-11-05 summary:Restricted Boltzmann Machines (RBMs) and Deep Belief Networks have beendemonstrated to perform efficiently in a variety of applications, such asdimensionality reduction, feature learning, and classification. Theirimplementation on neuromorphic hardware platforms emulating large-scalenetworks of spiking neurons can have significant advantages from theperspectives of scalability, power dissipation and real-time interfacing withthe environment. However the traditional RBM architecture and the commonly usedtraining algorithm known as Contrastive Divergence (CD) are based on discreteupdates and exact arithmetics which do not directly map onto a dynamical neuralsubstrate. Here, we present an event-driven variation of CD to train a RBMconstructed with Integrate & Fire (I&F) neurons, that is constrained by thelimitations of existing and near future neuromorphic hardware platforms. Ourstrategy is based on neural sampling, which allows us to synthesize a spikingneural network that samples from a target Boltzmann distribution. The recurrentactivity of the network replaces the discrete steps of the CD algorithm, whileSpike Time Dependent Plasticity (STDP) carries out the weight updates in anonline, asynchronous fashion. We demonstrate our approach by training an RBMcomposed of leaky I&F neurons with STDP synapses to learn a generative model ofthe MNIST hand-written digit dataset, and by testing it in recognition,generation and cue integration tasks. Our results contribute to a machinelearning-driven approach for synthesizing networks of spiking neurons capableof carrying out practical, high-level functionality.
arxiv-4500-55 | Bayesian Structural Inference for Hidden Processes | http://arxiv.org/pdf/1309.1392v2.pdf | author:Christopher C. Strelioff, James P. Crutchfield category:stat.ML cs.LG math.ST nlin.CD stat.TH published:2013-09-05 summary:We introduce a Bayesian approach to discovering patterns in structurallycomplex processes. The proposed method of Bayesian Structural Inference (BSI)relies on a set of candidate unifilar HMM (uHMM) topologies for inference ofprocess structure from a data series. We employ a recently developed exactenumeration of topological epsilon-machines. (A sequel then removes thetopological restriction.) This subset of the uHMM topologies has the addedbenefit that inferred models are guaranteed to be epsilon-machines,irrespective of estimated transition probabilities. Properties ofepsilon-machines and uHMMs allow for the derivation of analytic expressions forestimating transition probabilities, inferring start states, and comparing theposterior probability of candidate model topologies, despite process internalstructure being only indirectly present in data. We demonstrate BSI'seffectiveness in estimating a process's randomness, as reflected by the Shannonentropy rate, and its structure, as quantified by the statistical complexity.We also compare using the posterior distribution over candidate models and thesingle, maximum a posteriori model for point estimation and show that theformer more accurately reflects uncertainty in estimated values. We apply BSIto in-class examples of finite- and infinite-order Markov processes, as well toan out-of-class, infinite-state hidden process.
arxiv-4500-56 | State Transition Algorithm | http://arxiv.org/pdf/1205.6548v4.pdf | author:Xiaojun Zhou, Chunhua Yang, Weihua Gui category:math.OC cs.NE published:2012-05-30 summary:In terms of the concepts of state and state transition, a new heuristicrandom search algorithm named state transition algorithm is proposed. Forcontinuous function optimization problems, four special transformationoperators called rotation, translation, expansion and axesion are designed.Adjusting measures of the transformations are mainly studied to keep thebalance of exploration and exploitation. Convergence analysis is also discussedabout the algorithm based on random search theory. In the meanwhile, tostrengthen the search ability in high dimensional space, communication strategyis introduced into the basic algorithm and intermittent exchange is presentedto prevent premature convergence. Finally, experiments are carried out for thealgorithms. With 10 common benchmark unconstrained continuous functions used totest the performance, the results show that state transition algorithms arepromising algorithms due to their good global search capability and convergenceproperty when compared with some popular algorithms.
arxiv-4500-57 | Scalable Object Detection using Deep Neural Networks | http://arxiv.org/pdf/1312.2249v1.pdf | author:Dumitru Erhan, Christian Szegedy, Alexander Toshev, Dragomir Anguelov category:cs.CV stat.ML published:2013-12-08 summary:Deep convolutional neural networks have recently achieved state-of-the-artperformance on a number of image recognition benchmarks, including the ImageNetLarge-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model onthe localization sub-task was a network that predicts a single bounding box anda confidence score for each object category in the image. Such a model capturesthe whole-image context around the objects but cannot handle multiple instancesof the same object in the image without naively replicating the number ofoutputs for each instance. In this work, we propose a saliency-inspired neuralnetwork model for detection, which predicts a set of class-agnostic boundingboxes along with a single score for each box, corresponding to its likelihoodof containing any object of interest. The model naturally handles a variablenumber of instances for each class and allows for cross-class generalization atthe highest levels of the network. We are able to obtain competitiverecognition performance on VOC2007 and ILSVRC2012, while using only the top fewpredicted locations in each image and a small number of neural networkevaluations.
arxiv-4500-58 | Nonparametric Estimation of Multi-View Latent Variable Models | http://arxiv.org/pdf/1311.3287v2.pdf | author:Le Song, Animashree Anandkumar, Bo Dai, Bo Xie category:cs.LG stat.ML published:2013-11-13 summary:Spectral methods have greatly advanced the estimation of latent variablemodels, generating a sequence of novel and efficient algorithms with strongtheoretical guarantees. However, current spectral algorithms are largelyrestricted to mixtures of discrete or Gaussian distributions. In this paper, wepropose a kernel method for learning multi-view latent variable models,allowing each mixture component to be nonparametric. The key idea of the methodis to embed the joint distribution of a multi-view latent variable into areproducing kernel Hilbert space, and then the latent parameters are recoveredusing a robust tensor power method. We establish that the sample complexity forthe proposed method is quadratic in the number of latent components and is alow order polynomial in the other relevant parameters. Thus, our non-parametrictensor approach to learning latent variable models enjoys good sample andcomputational efficiencies. Moreover, the non-parametric tensor power methodcompares favorably to EM algorithm and other existing spectral algorithms inour experiments.
arxiv-4500-59 | Sequential Monte Carlo Inference of Mixed Membership Stochastic Blockmodels for Dynamic Social Networks | http://arxiv.org/pdf/1312.2154v1.pdf | author:Tomoki Kobayashi, Koji Eguchi category:cs.SI cs.LG stat.ML published:2013-12-07 summary:Many kinds of data can be represented as a network or graph. It is crucial toinfer the latent structure underlying such a network and to predict unobservedlinks in the network. Mixed Membership Stochastic Blockmodel (MMSB) is apromising model for network data. Latent variables and unknown parameters inMMSB have been estimated through Bayesian inference with the entire network;however, it is important to estimate them online for evolving networks. In thispaper, we first develop online inference methods for MMSB through sequentialMonte Carlo methods, also known as particle filters. We then extend them fortime-evolving networks, taking into account the temporal dependency of thenetwork structure. We demonstrate through experiments that the time-dependentparticle filter outperformed several baselines in terms of predictionperformance in an online condition.
arxiv-4500-60 | End-to-end Phoneme Sequence Recognition using Convolutional Neural Networks | http://arxiv.org/pdf/1312.2137v1.pdf | author:Dimitri Palaz, Ronan Collobert, Mathew Magimai. -Doss category:cs.LG cs.CL cs.NE published:2013-12-07 summary:Most phoneme recognition state-of-the-art systems rely on a classical neuralnetwork classifiers, fed with highly tuned features, such as MFCC or PLPfeatures. Recent advances in ``deep learning'' approaches questioned suchsystems, but while some attempts were made with simpler features such asspectrograms, state-of-the-art systems still rely on MFCCs. This might beviewed as a kind of failure from deep learning approaches, which are oftenclaimed to have the ability to train with raw signals, alleviating the need ofhand-crafted features. In this paper, we investigate a convolutional neuralnetwork approach for raw speech signals. While convolutional architectures gottremendous success in computer vision or text processing, they seem to havebeen let down in the past recent years in the speech processing field. We showthat it is possible to learn an end-to-end phoneme sequence classifier systemdirectly from raw signal, with similar performance on the TIMIT and WSJdatasets than existing systems based on MFCC, questioning the need of complexhand-crafted features on large datasets.
arxiv-4500-61 | Robust Subspace System Identification via Weighted Nuclear Norm Optimization | http://arxiv.org/pdf/1312.2132v1.pdf | author:Dorsa Sadigh, Henrik Ohlsson, S. Shankar Sastry, Sanjit A. Seshia category:cs.SY cs.LG stat.ML published:2013-12-07 summary:Subspace identification is a classical and very well studied problem insystem identification. The problem was recently posed as a convex optimizationproblem via the nuclear norm relaxation. Inspired by robust PCA, we extend thisframework to handle outliers. The proposed framework takes the form of a convexoptimization problem with an objective that trades off fit, rank and sparsity.As in robust PCA, it can be problematic to find a suitable regularizationparameter. We show how the space in which a suitable parameter should be soughtcan be limited to a bounded open set of the two dimensional parameter space. Inpractice, this is very useful since it restricts the parameter space that isneeded to be surveyed.
arxiv-4500-62 | Towards Structural Natural Language Formalization: Mapping Discourse to Controlled Natural Language | http://arxiv.org/pdf/1312.2087v1.pdf | author:Nicholas H. Kirk category:cs.CL published:2013-12-07 summary:The author describes a conceptual study towards mapping grounded naturallanguage discourse representation structures to instances of controlledlanguage statements. This can be achieved via a pipeline of preexisting stateof the art technologies, namely natural language syntax to semantic discoursemapping, and a reduction of the latter to controlled language discourse, givena set of previously learnt reduction rules. Concludingly a description onevaluation, potential and limitations for ontology-based reasoning ispresented.
arxiv-4500-63 | Region and Location Based Indexing and Retrieval of MR-T2 Brain Tumor Images | http://arxiv.org/pdf/1312.2061v1.pdf | author:Krishna A N, B G Prasad category:cs.CV cs.IR published:2013-12-07 summary:In this paper, region based and location based retrieval systems have beenimplemented for retrieval of MR-T2 axial 2-D brain images. This is done byextracting and characterizing the tumor portion of 2-D brain slices by use of asuitable threshold computed over the entire image. Indexing and retrieval isthen performed by computing texture features based on gray-tonespatial-dependence matrix of segmented regions. A Hash structure is used toindex all images. A combined index is adopted to point to all similar images interms of the texture features. At query time, only those images that are in thesame hash bucket as those of the queried image are compared for similarity,thus reducing the search space and time.
arxiv-4500-64 | Semistability-Based Convergence Analysis for Paracontracting Multiagent Coordination Optimization | http://arxiv.org/pdf/1308.2930v6.pdf | author:Qing Hui, Haopeng Zhang category:cs.SY cs.NE math.OC 90C59, 93D99 published:2013-08-13 summary:This sequential technical report extends some of the previous results weposted at arXiv:1306.0225.
arxiv-4500-65 | A Component Lasso | http://arxiv.org/pdf/1311.4472v2.pdf | author:Nadine Hussami, Robert Tibshirani category:stat.ML cs.LG 62J07 published:2013-11-18 summary:We propose a new sparse regression method called the component lasso, basedon a simple idea. The method uses the connected-components structure of thesample covariance matrix to split the problem into smaller ones. It then solvesthe subproblems separately, obtaining a coefficient vector for each one. Then,it uses non-negative least squares to recombine the different vectors into asingle solution. This step is useful in selecting and reweighting componentsthat are correlated with the response. Simulated and real data examples showthat the component lasso can outperform standard regression methods such as thelasso and elastic net, achieving a lower mean squared error as well as bettersupport recovery.
arxiv-4500-66 | An Algorithmic Theory of Dependent Regularizers, Part 1: Submodular Structure | http://arxiv.org/pdf/1312.1970v1.pdf | author:Hoyt Koepke, Marina Meila category:stat.ML published:2013-12-06 summary:We present an exploration of the rich theoretical connections between severalclasses of regularized models, network flows, and recent results in submodularfunction theory. This work unifies key aspects of these problems under a commontheory, leading to novel methods for working with several important models ofinterest in statistics, machine learning and computer vision. In Part 1, we review the concepts of network flows and submodular functionoptimization theory foundational to our results. We then examine theconnections between network flows and the minimum-norm algorithm fromsubmodular optimization, extending and improving several current results. Thisleads to a concise representation of the structure of a large class of pairwiseregularized models important in machine learning, statistics and computervision. In Part 2, we describe the full regularization path of a class of penalizedregression problems with dependent variables that includes the graph-guidedLASSO and total variation constrained models. This description also motivates apractical algorithm. This allows us to efficiently find the regularization pathof the discretized version of TV penalized models. Ultimately, our newalgorithms scale up to high-dimensional problems with millions of variables.
arxiv-4500-67 | Partitioning into Expanders | http://arxiv.org/pdf/1309.3223v3.pdf | author:Shayan Oveis Gharan, Luca Trevisan category:cs.DS math.SP stat.ML published:2013-09-12 summary:Let G=(V,E) be an undirected graph, lambda_k be the k-th smallest eigenvalueof the normalized laplacian matrix of G. There is a basic fact in algebraicgraph theory that lambda_k > 0 if and only if G has at most k-1 connectedcomponents. We prove a robust version of this fact. If lambda_k>0, then forsome 1\leq \ell\leq k-1, V can be {\em partitioned} into l sets P_1,\ldots,P_lsuch that each P_i is a low-conductance set in G and induces a high conductanceinduced subgraph. In particular, \phi(P_i)=O(l^3\sqrt{\lambda_l}) and\phi(G[P_i]) >= \lambda_k/k^2). We make our results algorithmic by designing a simple polynomial timespectral algorithm to find such partitioning of G with a quadratic loss in theinside conductance of P_i's. Unlike the recent results on higher orderCheeger's inequality [LOT12,LRTV12], our algorithmic results do not use higherorder eigenfunctions of G. If there is a sufficiently large gap betweenlambda_k and lambda_{k+1}, more precisely, if \lambda_{k+1} >= \poly(k)lambda_{k}^{1/4} then our algorithm finds a k partitioning of V into setsP_1,...,P_k such that the induced subgraph G[P_i] has a significantly largerconductance than the conductance of P_i in G. Such a partitioning may representthe best k clustering of G. Our algorithm is a simple local search that onlyuses the Spectral Partitioning algorithm as a subroutine. We expect to seefurther applications of this simple algorithm in clustering applications.
arxiv-4500-68 | CEAI: CCM based Email Authorship Identification Model | http://arxiv.org/pdf/1312.2451v1.pdf | author:Sarwat Nizamani, Nasrullah Memon category:cs.LG published:2013-12-06 summary:In this paper we present a model for email authorship identification (EAI) byemploying a Cluster-based Classification (CCM) technique. Traditionally,stylometric features have been successfully employed in various authorshipanalysis tasks; we extend the traditional feature-set to include some moreinteresting and effective features for email authorship identification (e.g.the last punctuation mark used in an email, the tendency of an author to usecapitalization at the start of an email, or the punctuation after a greeting orfarewell). We also included Info Gain feature selection based content features.It is observed that the use of such features in the authorship identificationprocess has a positive impact on the accuracy of the authorship identificationtask. We performed experiments to justify our arguments and compared theresults with other base line models. Experimental results reveal that theproposed CCM-based email authorship identification model, along with theproposed feature set, outperforms the state-of-the-art support vector machine(SVM)-based models, as well as the models proposed by Iqbal et al. [1, 2]. Theproposed model attains an accuracy rate of 94% for 10 authors, 89% for 25authors, and 81% for 50 authors, respectively on Enron dataset, while 89.5%accuracy has been achieved on authors' constructed real email dataset. Theresults on Enron dataset have been achieved on quite a large number of authorsas compared to the models proposed by Iqbal et al. [1, 2].
arxiv-4500-69 | Semantic Measures for the Comparison of Units of Language, Concepts or Instances from Text and Knowledge Base Analysis | http://arxiv.org/pdf/1310.1285v2.pdf | author:Sébastien Harispe, Sylvie Ranwez, Stefan Janaqi, Jacky Montmain category:cs.CL published:2013-10-04 summary:Semantic measures are widely used today to estimate the strength of thesemantic relationship between elements of various types: units of language(e.g., words, sentences, documents), concepts or even instances semanticallycharacterized (e.g., diseases, genes, geographical locations). Semanticmeasures play an important role to compare such elements according to semanticproxies: texts and knowledge representations, which support their meaning ordescribe their nature. Semantic measures are therefore essential for designingintelligent agents which will for example take advantage of semantic analysisto mimic human ability to compare abstract or concrete objects. This paperproposes a comprehensive survey of the broad notion of semantic measure for thecomparison of units of language, concepts or instances based on semantic proxyanalyses. Semantic measures generalize the well-known notions of semanticsimilarity, semantic relatedness and semantic distance, which have beenextensively studied by various communities over the last decades (e.g.,Cognitive Sciences, Linguistics, and Artificial Intelligence to mention a few).
arxiv-4500-70 | Deep and Wide Multiscale Recursive Networks for Robust Image Labeling | http://arxiv.org/pdf/1310.0354v3.pdf | author:Gary B. Huang, Viren Jain category:cs.CV cs.LG published:2013-10-01 summary:Feedforward multilayer networks trained by supervised learning have recentlydemonstrated state of the art performance on image labeling problems such asboundary prediction and scene parsing. As even very low error rates can limitpractical usage of such systems, methods that perform closer to human accuracyremain desirable. In this work, we propose a new type of network with thefollowing properties that address what we hypothesize to be limiting aspects ofexisting methods: (1) a `wide' structure with thousands of features, (2) alarge field of view, (3) recursive iterations that exploit statisticaldependencies in label space, and (4) a parallelizable architecture that can betrained in a fraction of the time compared to benchmark multilayerconvolutional networks. For the specific image labeling problem of boundaryprediction, we also introduce a novel example weighting algorithm that improvessegmentation accuracy. Experiments in the challenging domain of connectomicreconstruction of neural circuity from 3d electron microscopy data show thatthese "Deep And Wide Multiscale Recursive" (DAWMR) networks lead to new levelsof image labeling performance. The highest performing architecture has twelvelayers, interwoven supervised and unsupervised stages, and uses an input fieldof view of 157,464 voxels ($54^3$) to make a prediction at each image location.We present an associated open source software package that enables the simpleand flexible creation of DAWMR networks.
arxiv-4500-71 | Towards Normalizing the Edit Distance Using a Genetic Algorithms Based Scheme | http://arxiv.org/pdf/1312.1760v1.pdf | author:Muhammad Marwan Muhammad Fuad category:cs.NE cs.AI published:2013-12-06 summary:The normalized edit distance is one of the distances derived from the editdistance. It is useful in some applications because it takes into account thelengths of the two strings compared. The normalized edit distance is notdefined in terms of edit operations but rather in terms of the edit path. Inthis paper we propose a new derivative of the edit distance that also takesinto consideration the lengths of the two strings, but the new distance isrelated directly to the edit distance. The particularity of the new distance isthat it uses the genetic algorithms to set the values of the parameters ituses. We conduct experiments to test the new distance and we obtain promisingresults.
arxiv-4500-72 | Particle Swarm Optimization of Information-Content Weighting of Symbolic Aggregate Approximation | http://arxiv.org/pdf/1312.1752v1.pdf | author:Muhammad Marwan Muhammad Fuad category:cs.NE cs.AI published:2013-12-06 summary:Bio-inspired optimization algorithms have been gaining more popularityrecently. One of the most important of these algorithms is particle swarmoptimization (PSO). PSO is based on the collective intelligence of a swam ofparticles. Each particle explores a part of the search space looking for theoptimal position and adjusts its position according to two factors; the firstis its own experience and the second is the collective experience of the wholeswarm. PSO has been successfully used to solve many optimization problems. Inthis work we use PSO to improve the performance of a well-known representationmethod of time series data which is the symbolic aggregate approximation (SAX).As with other time series representation methods, SAX results in loss ofinformation when applied to represent time series. In this paper we use PSO topropose a new minimum distance WMD for SAX to remedy this problem. Unlike theoriginal minimum distance, the new distance sets different weights to differentsegments of the time series according to their information content. Thisweighted minimum distance enhances the performance of SAX as we show throughexperiments using different time series datasets.
arxiv-4500-73 | Non-Asymptotic Analysis of Tangent Space Perturbation | http://arxiv.org/pdf/1111.4601v4.pdf | author:Daniel N. Kaslovsky, Francois G. Meyer category:cs.NA stat.ML published:2011-11-20 summary:Constructing an efficient parameterization of a large, noisy data set ofpoints lying close to a smooth manifold in high dimension remains a fundamentalproblem. One approach consists in recovering a local parameterization using thelocal tangent plane. Principal component analysis (PCA) is often the tool ofchoice, as it returns an optimal basis in the case of noise-free samples from alinear subspace. To process noisy data samples from a nonlinear manifold, PCAmust be applied locally, at a scale small enough such that the manifold isapproximately linear, but at a scale large enough such that structure may bediscerned from noise. Using eigenspace perturbation theory and non-asymptoticrandom matrix theory, we study the stability of the subspace estimated by PCAas a function of scale, and bound (with high probability) the angle it formswith the true tangent space. By adaptively selecting the scale that minimizesthis bound, our analysis reveals an appropriate scale for local tangent planerecovery. We also introduce a geometric uncertainty principle quantifying thelimits of noise-curvature perturbation for stable recovery. With the purpose ofproviding perturbation bounds that can be used in practice, we propose plug-inestimates that make it possible to directly apply the theoretical results toreal data sets.
arxiv-4500-74 | Curriculum Learning for Handwritten Text Line Recognition | http://arxiv.org/pdf/1312.1737v1.pdf | author:Jérôme Louradour, Christopher Kermorvant category:cs.LG published:2013-12-05 summary:Recurrent Neural Networks (RNN) have recently achieved the best performancein off-line Handwriting Text Recognition. At the same time, learning RNN bygradient descent leads to slow convergence, and training times are particularlylong when the training database consists of full lines of text. In this paper,we propose an easy way to accelerate stochastic gradient descent in thisset-up, and in the general context of learning to recognize sequences. Theprinciple is called Curriculum Learning, or shaping. The idea is to first learnto recognize short sequences before training on all available trainingsequences. Experiments on three different handwritten text databases (Rimes,IAM, OpenHaRT) show that a simple implementation of this strategy cansignificantly speed up the training of RNN for Text Recognition, and evensignificantly improve performance in some cases.
arxiv-4500-75 | Book embeddings of Reeb graphs | http://arxiv.org/pdf/1312.1725v1.pdf | author:Vitaliy Kurlin category:cs.CG cs.CV math.GT published:2013-12-05 summary:Let $X$ be a simplicial complex with a piecewise linear function$f:X\to\mathbb{R}$. The Reeb graph $Reeb(f,X)$ is the quotient of $X$, where wecollapse each connected component of $f^{-1}(t)$ to a single point. Let thenodes of $Reeb(f,X)$ be all homologically critical points where any homology ofthe corresponding component of the level set $f^{-1}(t)$ changes. Then we canlabel every arc of $Reeb(f,X)$ with the Betti numbers$(\beta_1,\beta_2,\dots,\beta_d)$ of the corresponding $d$-dimensionalcomponent of a level set. The homology labels give more information about theoriginal complex $X$ than the classical Reeb graph. We describe a canonicalembedding of a Reeb graph into a multi-page book (a star cross a line) and givea unique linear code of this book embedding.
arxiv-4500-76 | Max-Min Distance Nonnegative Matrix Factorization | http://arxiv.org/pdf/1312.1613v1.pdf | author:Jim Jing-Yan Wang category:stat.ML cs.LG cs.NA published:2013-12-05 summary:Nonnegative Matrix Factorization (NMF) has been a popular representationmethod for pattern classification problem. It tries to decompose a nonnegativematrix of data samples as the product of a nonnegative basic matrix and anonnegative coefficient matrix, and the coefficient matrix is used as the newrepresentation. However, traditional NMF methods ignore the class labels of thedata samples. In this paper, we proposed a supervised novel NMF algorithm toimprove the discriminative ability of the new representation. Using the classlabels, we separate all the data sample pairs into within-class pairs andbetween-class pairs. To improve the discriminate ability of the new NMFrepresentations, we hope that the maximum distance of the within-class pairs inthe new NMF space could be minimized, while the minimum distance of thebetween-class pairs pairs could be maximized. With this criterion, we constructan objective function and optimize it with regard to basic and coefficientmatrices and slack variables alternatively, resulting in a iterative algorithm.
arxiv-4500-77 | The optimality of attaching unlinked labels to unlinked meanings | http://arxiv.org/pdf/1310.5884v2.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL physics.soc-ph published:2013-10-22 summary:Vocabulary learning by children can be characterized by many biases. Whenencountering a new word, children as well as adults, are biased towardsassuming that it means something totally different from the words that theyalready know. To the best of our knowledge, the 1st mathematical proof of theoptimality of this bias is presented here. First, it is shown that this bias isa particular case of the maximization of mutual information between words andmeanings. Second, the optimality is proven within a more general informationtheoretic framework where mutual information maximization competes with otherinformation theoretic principles. The bias is a prediction from moderninformation theory. The relationship between information theoretic principlesand the principles of contrast and mutual exclusivity is also shown.
arxiv-4500-78 | Iterative Log Thresholding | http://arxiv.org/pdf/1312.1522v1.pdf | author:Dmitry Malioutov, Aleksandr Aravkin category:stat.ML math.OC published:2013-12-05 summary:Sparse reconstruction approaches using the re-weighted l1-penalty have beenshown, both empirically and theoretically, to provide a significant improvementin recovering sparse signals in comparison to the l1-relaxation. However,numerical optimization of such penalties involves solving problems withl1-norms in the objective many times. Using the direct link of reweightedl1-penalties to the concave log-regularizer for sparsity, we derive a simpleprox-like algorithm for the log-regularized formulation. The proximal splittingstep of the algorithm has a closed form solution, and we call the algorithm'log-thresholding' in analogy to soft thresholding for the l1-penalty. We establish convergence results, and demonstrate that log-thresholdingprovides more accurate sparse reconstructions compared to both soft and hardthresholding. Furthermore, the approach can be directly extended tooptimization over matrices with penalty for rank (i.e. the nuclear norm penaltyand its re-weigthed version), where we suggest a singular-valuelog-thresholding approach.
arxiv-4500-79 | Human Face Recognition using Gabor based Kernel Entropy Component Analysis | http://arxiv.org/pdf/1312.1685v1.pdf | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:In this paper, we present a novel Gabor wavelet based Kernel EntropyComponent Analysis (KECA) method by integrating the Gabor wavelettransformation (GWT) of facial images with the KECA method for enhanced facerecognition performance. Firstly, from the Gabor wavelet transformed images themost important discriminative desirable facial features characterized byspatial frequency, spatial locality and orientation selectivity to cope withthe variations due to illumination and facial expression changes were derived.After that KECA, relating to the Renyi entropy is extended to include cosinekernel function. The KECA with the cosine kernels is then applied on theextracted most important discriminating feature vectors of facial images toobtain only those real kernel ECA eigenvectors that are associated witheigenvalues having positive entropy contribution. Finally, these real KECAfeatures are used for image classification using the L1, L2 distance measures;the Mahalanobis distance measure and the cosine similarity measure. Thefeasibility of the Gabor based KECA method with the cosine kernel has beensuccessfully tested on both frontal and pose-angled face recognition, usingdatasets from the ORL, FRAV2D and the FERET database.
arxiv-4500-80 | A Face Recognition approach based on entropy estimate of the nonlinear DCT features in the Logarithm Domain together with Kernel Entropy Component Analysis | http://arxiv.org/pdf/1312.1520v1.pdf | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:This paper exploits the feature extraction capabilities of the discretecosine transform (DCT) together with an illumination normalization approach inthe logarithm domain that increase its robustness to variations in facialgeometry and illumination. Secondly in the same domain the entropy measures areapplied on the DCT coefficients so that maximum entropy preserving pixels canbe extracted as the feature vector. Thus the informative features of a face canbe extracted in a low dimensional space. Finally, the kernel entropy componentanalysis (KECA) with an extension of arc cosine kernels is applied on theextracted DCT coefficients that contribute most to the entropy estimate toobtain only those real kernel ECA eigenvectors that are associated witheigenvalues having high positive entropy contribution. The resulting system wassuccessfully tested on real image sequences and is robust to significantpartial occlusion and illumination changes, validated with the experiments onthe FERET, AR, FRAV2D and ORL face databases. Experimental comparison isdemonstrated to prove the superiority of the proposed approach in respect torecognition accuracy. Using specificity and sensitivity we find that the bestis achieved when Renyi entropy is applied on the DCT coefficients. Extensiveexperimental comparison is demonstrated to prove the superiority of theproposed approach in respect to recognition accuracy. Moreover, the proposedapproach is very simple, computationally fast and can be implemented in anyreal-time face recognition system.
arxiv-4500-81 | High Performance Human Face Recognition using Gabor based Pseudo Hidden Markov Model | http://arxiv.org/pdf/1312.1684v1.pdf | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:This paper introduces a novel methodology that combines the multi-resolutionfeature of the Gabor wavelet transformation (GWT) with the local interactionsof the facial structures expressed through the Pseudo Hidden Markov model(PHMM). Unlike the traditional zigzag scanning method for feature extraction acontinuous scanning method from top-left corner to right then top-down andright to left and so on until right-bottom of the image i.e. a spiral scanningtechnique has been proposed for better feature selection. Unlike traditionalHMMs, the proposed PHMM does not perform the state conditional independence ofthe visible observation sequence assumption. This is achieved via the conceptof local structures introduced by the PHMM used to extract facial bands andautomatically select the most informative features of a face image. Thus, thelong-range dependency problem inherent to traditional HMMs has been drasticallyreduced. Again with the use of most informative pixels rather than the wholeimage makes the proposed method reasonably faster for face recognition. Thismethod has been successfully tested on frontal face images from the ORL, FRAV2Dand FERET face databases where the images vary in pose, illumination,expression, and scale. The FERET data set contains 2200 frontal face images of200 subjects, while the FRAV2D data set consists of 1100 images of 100 subjectsand the full ORL database is considered. The results reported in thisapplication are far better than the recent and most referred systems.
arxiv-4500-82 | A Gabor block based Kernel Discriminative Common Vector (KDCV) approach using cosine kernels for Human Face Recognition | http://arxiv.org/pdf/1312.1517v1.pdf | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:In this paper a nonlinear Gabor Wavelet Transform (GWT) discriminant featureextraction approach for enhanced face recognition is proposed. Firstly, thelow-energized blocks from Gabor wavelet transformed images are extracted.Secondly, the nonlinear discriminating features are analyzed and extracted fromthe selected low-energized blocks by the generalized Kernel DiscriminativeCommon Vector (KDCV) method. The KDCV method is extended to include cosinekernel function in the discriminating method. The KDCV with the cosine kernelsis then applied on the extracted low energized discriminating feature vectorsto obtain the real component of a complex quantity for face recognition. Inorder to derive positive kernel discriminative vectors; we apply only thosekernel discriminative eigenvectors that are associated with non-zeroeigenvalues. The feasibility of the low energized Gabor block based generalizedKDCV method with cosine kernel function models has been successfully tested forimage classification using the L1, L2 distance measures; and the cosinesimilarity measure on both frontal and pose-angled face recognition.Experimental results on the FRAV2D and the FERET database demonstrate theeffectiveness of this new approach.
arxiv-4500-83 | Face Recognition using Hough Peaks extracted from the significant blocks of the Gradient Image | http://arxiv.org/pdf/1312.1683v1.pdf | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:This paper proposes a new technique for automatic face recognition usingintegrated peaks of the Hough transformed significant blocks of the binarygradient image. In this approach firstly the gradient of an image is calculatedand a threshold is set to obtain a binary gradient image, which is lesssensitive to noise and illumination changes. Secondly, significant blocks areextracted from the absolute gradient image, to extract pertinent informationwith the idea of dimension reduction. Finally the best fitted Hough peaks areextracted from the Hough transformed significant blocks for efficient facerecognition. Then these Hough peaks are concatenated together, which are usedas feature in classification process. The efficiency of the proposed method isdemonstrated by the experiment on 1100 images from the FRAV2D face database,2200 images from the FERET database, where the images vary in pose, expression,illumination and scale and 400 images from the ORL face database, where theimages slightly vary in pose. Our method has shown 93.3%, 88.5% and 99%recognition accuracy for the FRAV2D, FERET and the ORL database respectively.
arxiv-4500-84 | An adaptive block based integrated LDP,GLCM,and Morphological features for Face Recognition | http://arxiv.org/pdf/1312.1512v1.pdf | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:This paper proposes a technique for automatic face recognition usingintegrated multiple feature sets extracted from the significant blocks of agradient image. We discuss about the use of novel morphological, localdirectional pattern (LDP) and gray-level co-occurrence matrix GLCM basedfeature extraction technique to recognize human faces. Firstly, the newmorphological features i.e., features based on number of runs of pixels in fourdirections (N,NE,E,NW) are extracted, together with the GLCM based statisticalfeatures and LDP features that are less sensitive to the noise andnon-monotonic illumination changes, are extracted from the significant blocksof the gradient image. Then these features are concatenated together. Weintegrate the above mentioned methods to take full advantage of the threeapproaches. Extraction of the significant blocks from the absolute gradientimage and hence from the original image to extract pertinent information withthe idea of dimension reduction forms the basis of the work. The efficiency ofour method is demonstrated by the experiment on 1100 images from the FRAV2Dface database, 2200 images from the FERET database, where the images vary inpose, expression, illumination and scale and 400 images from the ORL facedatabase, where the images slightly vary in pose. Our method has shown 90.3%,93% and 98.75% recognition accuracy for the FRAV2D, FERET and the ORL databaserespectively.
arxiv-4500-85 | Approximating persistent homology for a cloud of $n$ points in a subquadratic time | http://arxiv.org/pdf/1312.1494v1.pdf | author:Vitaliy Kurlin category:cs.CG cs.CV math.AT published:2013-12-05 summary:The Vietoris-Rips filtration for an $n$-point metric space is a sequence oflarge simplicial complexes adding a topological structure to the otherwisedisconnected space. The persistent homology is a key tool in topological dataanalysis and studies topological features of data that persist over manyscales. The fastest algorithm for computing persistent homology of a filtrationhas time $O(M(u)+u^2\log^2 u)$, where $u$ is the number of updates (additionsor deletions of simplices), $M(u)=O(u^{2.376})$ is the time for multiplicationof $u\times u$ matrices. For a space of $n$ points given by their pairwisedistances, we approximate the Vietoris-Rips filtration by a zigzag filtrationconsisting of $u=o(n)$ updates, which is sublinear in $n$. The constant dependson a given error of approximation and on the doubling dimension of the metricspace. Then the persistent homology of this sublinear-size filtration can becomputed in time $o(n^2)$, which is subquadratic in $n$.
arxiv-4500-86 | Geometric Feature Based Face-Sketch Recognition | http://arxiv.org/pdf/1312.1462v1.pdf | author:Sourav Pramanik, Debotosh Bhattacharjee category:cs.CV published:2013-12-05 summary:This paper presents a novel facial sketch image or face-sketch recognitionapproach based on facial feature extraction. To recognize a face-sketch, wehave concentrated on a set of geometric face features like eyes, nose,eyebrows, lips, etc and their length and width ratio because it is difficult tomatch photos and sketches because they belong to two different modalities. Inthis system, first the facial features/components from training images areextracted, then ratios of length, width, and area etc. are calculated and thoseare stored as feature vectors for individual images. After that the meanfeature vectors are computed and subtracted from each feature vector forcentering of the feature vectors. In the next phase, feature vector for theincoming probe face-sketch is also computed in similar fashion. Here, K-NNclassifier is used to recognize probe face-sketch. It is experimentallyverified that the proposed method is robust against faces are in a frontalpose, with normal lighting and neutral expression and have no occlusions. Theexperiment has been conducted with 80 male and female face images fromdifferent face databases. It has useful applications for both law enforcementand digital entertainment.
arxiv-4500-87 | Multi-Sensor Image Fusion Based on Moment Calculation | http://arxiv.org/pdf/1312.1461v1.pdf | author:Sourav Pramanik, Debotosh Bhattacharjee category:cs.CV published:2013-12-05 summary:An image fusion method based on salient features is proposed in this paper.In this work, we have concentrated on salient features of the image for fusionin order to preserve all relevant information contained in the input images andtried to enhance the contrast in fused image and also suppressed noise to amaximum extent. In our system, first we have applied a mask on two input imagesin order to conserve the high frequency information along with some lowfrequency information and stifle noise to a maximum extent. Thereafter, foridentification of salience features from sources images, a local moment iscomputed in the neighborhood of a coefficient. Finally, a decision map isgenerated based on local moment in order to get the fused image. To verify ourproposed algorithm, we have tested it on 120 sensor image pairs collected fromManchester University UK database. The experimental results show that theproposed method can provide superior fused image in terms of severalquantitative fusion evaluation index.
arxiv-4500-88 | An Approach: Modality Reduction and Face-Sketch Recognition | http://arxiv.org/pdf/1312.1681v1.pdf | author:Sourav Pramanik, Dr. Debotosh Bhattacharjee category:cs.CV published:2013-12-05 summary:To recognize face sketch through face photo database is a challenging taskfor todays researchers. Because face photo images in training set and facesketch images in testing set have different modality. Difference between twoface photos of difference person is smaller than the difference between sameperson in a face photo and face sketched. In this paper, for reduction of themodality between face photo and face sketch we first bring face photo and facesketch images in a new dimension using 2D Discrete Haar wavelet transform withscale 3 followed by a negative approach. After that, extract features fromtransformed images using Principal Component Analysis (PCA). Thereafter, we useSVM classifier and K-NN classifier for better classification. Our proposedmethod is experimentally verified by its robustness against faces that arecaptured in a good lighting condition and in a frontal pose. The experiment hasbeen conducted with 100 male and female face images as training set and 100male and female face sketch images as testing set collected from CUHK trainingand testing cropped photos and CUHK training and testing cropped sketches.
arxiv-4500-89 | ABC-SG: A New Artificial Bee Colony Algorithm-Based Distance of Sequential Data Using Sigma Grams | http://arxiv.org/pdf/1312.1423v1.pdf | author:Muhammad Marwan Muhammad Fuad category:cs.NE cs.AI published:2013-12-05 summary:The problem of similarity search is one of the main problems in computerscience. This problem has many applications in text-retrieval, web search,computational biology, bioinformatics and others. Similarity between two dataobjects can be depicted using a similarity measure or a distance metric. Thereare numerous distance metrics in the literature, some are used for a particulardata type, and others are more general. In this paper we present a new distancemetric for sequential data which is based on the sum of n-grams. The novelty ofour distance is that these n-grams are weighted using artificial bee colony; arecent optimization algorithm based on the collective intelligence of a swarmof bees on their search for nectar. This algorithm has been used in optimizinga large number of numerical problems. We validate the new distanceexperimentally.
arxiv-4500-90 | Chebushev Greedy Algorithm in convex optimization | http://arxiv.org/pdf/1312.1244v1.pdf | author:Vladimir Temlyakov category:stat.ML math.OC published:2013-12-04 summary:Chebyshev Greedy Algorithm is a generalization of the well known OrthogonalMatching Pursuit defined in a Hilbert space to the case of Banach spaces. Weapply this algorithm for constructing sparse approximate solutions (withrespect to a given dictionary) to convex optimization problems. Rate ofconvergence results in a style of the Lebesgue-type inequalities are proved.
arxiv-4500-91 | Interpreting random forest classification models using a feature contribution method | http://arxiv.org/pdf/1312.1121v1.pdf | author:Anna Palczewska, Jan Palczewski, Richard Marchese Robinson, Daniel Neagu category:cs.LG published:2013-12-04 summary:Model interpretation is one of the key aspects of the model evaluationprocess. The explanation of the relationship between model variables andoutputs is relatively easy for statistical models, such as linear regressions,thanks to the availability of model parameters and their statisticalsignificance. For "black box" models, such as random forest, this informationis hidden inside the model structure. This work presents an approach forcomputing feature contributions for random forest classification models. Itallows for the determination of the influence of each variable on the modelprediction for an individual instance. By analysing feature contributions for atraining dataset, the most significant variables can be determined and theirtypical contribution towards predictions made for individual classes, i.e.,class-specific feature contribution "patterns", are discovered. These patternsrepresent a standard behaviour of the model and allow for an additionalassessment of the model reliability for a new data. Interpretation of featurecontributions for two UCI benchmark datasets shows the potential of theproposed methodology. The robustness of results is demonstrated through anextensive analysis of feature contributions calculated for a large number ofgenerated random forest models.
arxiv-4500-92 | Multiscale Dictionary Learning for Estimating Conditional Distributions | http://arxiv.org/pdf/1312.1099v1.pdf | author:Francesca Petralia, Joshua Vogelstein, David B. Dunson category:stat.ML cs.LG published:2013-12-04 summary:Nonparametric estimation of the conditional distribution of a response givenhigh-dimensional features is a challenging problem. It is important to allownot only the mean but also the variance and shape of the response density tochange flexibly with features, which are massive-dimensional. We propose amultiscale dictionary learning model, which expresses the conditional responsedensity as a convex combination of dictionary densities, with the densitiesused and their weights dependent on the path through a tree decomposition ofthe feature space. A fast graph partitioning algorithm is applied to obtain thetree decomposition, with Bayesian methods then used to adaptively prune andaverage over different sub-trees in a soft probabilistic manner. The algorithmscales efficiently to approximately one million features. State of the artpredictive performance is demonstrated for toy examples and two neuroscienceapplications including up to a million features.
arxiv-4500-93 | Algorithms and Hardness for Robust Subspace Recovery | http://arxiv.org/pdf/1211.1041v3.pdf | author:Moritz Hardt, Ankur Moitra category:cs.CC cs.DS cs.IT cs.LG math.IT published:2012-11-05 summary:We consider a fundamental problem in unsupervised learning called\emph{subspace recovery}: given a collection of $m$ points in $\mathbb{R}^n$,if many but not necessarily all of these points are contained in a$d$-dimensional subspace $T$ can we find it? The points contained in $T$ arecalled {\em inliers} and the remaining points are {\em outliers}. This problemhas received considerable attention in computer science and in statistics. Yetefficient algorithms from computer science are not robust to {\em adversarial}outliers, and the estimators from robust statistics are hard to compute in highdimensions. Are there algorithms for subspace recovery that are both robust to outliersand efficient? We give an algorithm that finds $T$ when it contains more than a$\frac{d}{n}$ fraction of the points. Hence, for say $d = n/2$ this estimatoris both easy to compute and well-behaved when there are a constant fraction ofoutliers. We prove that it is Small Set Expansion hard to find $T$ when thefraction of errors is any larger, thus giving evidence that our estimator is an{\em optimal} compromise between efficiency and robustness. As it turns out, this basic problem has a surprising number of connections toother areas including small set expansion, matroid theory and functionalanalysis that we make use of here.
arxiv-4500-94 | Distance Correlation Methods for Discovering Associations in Large Astrophysical Databases | http://arxiv.org/pdf/1308.3925v2.pdf | author:Elizabeth Martinez-Gomez, Mercedes T. Richards, Donald St. P. Richards category:astro-ph.CO math.ST stat.AP stat.ML stat.TH published:2013-08-19 summary:High-dimensional, large-sample astrophysical databases of galaxy clusters,such as the Chandra Deep Field South COMBO-17 database, provide measurements onmany variables for thousands of galaxies and a range of redshifts. Currentunderstanding of galaxy formation and evolution rests sensitively onrelationships between different astrophysical variables; hence an ability todetect and verify associations or correlations between variables is importantin astrophysical research. In this paper, we apply a recently definedstatistical measure called the distance correlation coefficient which can beused to identify new associations and correlations between astrophysicalvariables. The distance correlation coefficient applies to variables of anydimension; it can be used to determine smaller sets of variables that provideequivalent astrophysical information; it is zero only when variables areindependent; and it is capable of detecting nonlinear associations that areundetectable by the classical Pearson correlation coefficient. Hence, thedistance correlation coefficient provides more information than the Pearsoncoefficient. We analyze numerous pairs of variables in the COMBO-17 databasewith the distance correlation method and with the maximal informationcoefficient. We show that the Pearson coefficient can be estimated with higheraccuracy from the corresponding distance correlation coefficient than from themaximal information coefficient. For given values of the Pearson coefficient,the distance correlation method has a greater ability than the maximalinformation coefficient to resolve astrophysical data into highly concentratedV-shapes, which enhances classification and pattern identification. Theseresults are observed over a range of redshifts beyond the local universe andfor galaxies from elliptical to spiral.
arxiv-4500-95 | Sparse Linear Dynamical System with Its Application in Multivariate Clinical Time Series | http://arxiv.org/pdf/1311.7071v2.pdf | author:Zitao Liu, Milos Hauskrecht category:cs.AI cs.LG stat.ML published:2013-11-27 summary:Linear Dynamical System (LDS) is an elegant mathematical framework formodeling and learning multivariate time series. However, in general, it isdifficult to set the dimension of its hidden state space. A small number ofhidden states may not be able to model the complexities of a time series, whilea large number of hidden states can lead to overfitting. In this paper, westudy methods that impose an $\ell_1$ regularization on the transition matrixof an LDS model to alleviate the problem of choosing the optimal number ofhidden states. We incorporate a generalized gradient descent method into theMaximum a Posteriori (MAP) framework and use Expectation Maximization (EM) toiteratively achieve sparsity on the transition matrix of an LDS model. We showthat our Sparse Linear Dynamical System (SLDS) improves the predictiveperformance when compared to ordinary LDS on a multivariate clinical timeseries dataset.
arxiv-4500-96 | Feature Extraction of Human Lip Prints | http://arxiv.org/pdf/1312.0852v1.pdf | author:Samir Kumar Bandyopadhyay, S Arunkumar, Saptarshi Bhattacharjee category:cs.CV published:2013-12-03 summary:Methods have been used for identification of human by recognizing lip prints.Human lips have a number of elevation and depressions features called lipprints and examination of lip prints is referred to as cheiloscopy. Lip printsof each human being are unique in nature like many others features of human. Inthis paper lip print is first smoothened using a Gaussian Filter. Next SobelEdge Detector and Canny Edge Detector are used to detect the vertical andhorizontal groove pattern in the lip. This method of identification will beuseful both in criminal forensics and personal identification. It is ourassumption that study of lip prints and their types are well connected to playa song in a better way that are well accepted to people who loves to hearsongs.
arxiv-4500-97 | Medical Aid for Automatic Detection of Malaria | http://arxiv.org/pdf/1312.0940v1.pdf | author:Pramit Ghosh, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CY cs.CV published:2013-12-03 summary:The analysis and counting of blood cells in a microscope image can provideuseful information concerning to the health of a person. In particular,morphological analysis of red blood cells deformations can effectively detectimportant disease like malaria. Blood images, obtained by the microscope, whichis coupled with a digital camera, are analyzed by the computer for diagnosis orcan be transmitted easily to clinical centers than liquid blood samples.Automatic analysis system for the presence of Plasmodium in microscopic imageof blood can greatly help pathologists and doctors that typically inspect bloodfilms manually. Unfortunately, the analysis made by human experts is not rapidand not yet standardized due to the operators capabilities and tiredness. Thepaper shows how effectively and accurately it is possible to identify thePlasmodium in the blood film. In particular, the paper presents how to enhancethe microscopic image and filter out the unnecessary segments followed by thethreshold based segmentation and recognize the presence of Plasmodium. Theproposed system can be deployed in the remote area as a supporting aid fortelemedicine technology and only basic training is sufficient to operate it.This system achieved more than 98 percentage accuracy for the samples collectedto test this system.
arxiv-4500-98 | Automatic White Blood Cell Measuring Aid for Medical Diagnosis | http://arxiv.org/pdf/1312.0809v1.pdf | author:Pramit Ghosh, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CY cs.CV published:2013-12-03 summary:Blood related invasive pathological investigations play a major role indiagnosis of diseases. But in India and other third world countries there areno enough pathological infrastructures for medical diagnosis. Moreover, most ofthe remote places of those countries have neither pathologists nor physicians.Telemedicine partially solves the lack of physicians. But the pathologicalinvestigation infrastructure can not be integrated with the telemedicinetechnology. The objective of this work is to automate the blood relatedpathological investigation process. Detection of different white blood cellshas been automated in this work. This system can be deployed in the remote areaas a supporting aid for telemedicine technology and only high school educationis sufficient to operate it. The proposed system achieved 97.33 percentaccuracy for the samples collected to test this system.
arxiv-4500-99 | Template-Based Active Contours | http://arxiv.org/pdf/1312.0760v1.pdf | author:Jayanth Krishna Mogali, Adithya Kumar Pediredla, Chandra Sekhar Seelamantula category:cs.CV published:2013-12-03 summary:We develop a generalized active contour formalism for image segmentationbased on shape templates. The shape template is subjected to a restrictedaffine transformation (RAT) in order to segment the object of interest. RATallows for translation, rotation, and scaling, which give a total of fivedegrees of freedom. The proposed active contour comprises an inner and outercontour pair, which are closed and concentric. The active contour energy is acontrast function defined based on the intensities of pixels that lie insidethe inner contour and those that lie in the annulus between the inner and outercontours. We show that the contrast energy functional is optimal under certainconditions. The optimal RAT parameters are computed by maximizing the contrastfunction using a gradient descent optimizer. We show that the calculations aremade efficient through use of Green's theorem. The proposed formalism iscapable of handling a variety of shapes because for a chosen template,optimization is carried with respect to the RAT parameters only. The proposedformalism is validated on multiple images to show robustness to Gaussian andPoisson noise, to initialization, and to partial loss of structure in theobject to be segmented.
arxiv-4500-100 | Phase Transitions in Community Detection: A Solvable Toy Model | http://arxiv.org/pdf/1312.0631v1.pdf | author:Greg Ver Steeg, Cristopher Moore, Aram Galstyan, Armen E. Allahverdyan category:cs.SI physics.soc-ph stat.ML published:2013-12-02 summary:Recently, it was shown that there is a phase transition in the communitydetection problem. This transition was first computed using the cavity method,and has been proved rigorously in the case of $q=2$ groups. However, analyticcalculations using the cavity method are challenging since they require us tounderstand probability distributions of messages. We study analogoustransitions in so-called "zero-temperature inference" model, where thisdistribution is supported only on the most-likely messages. Furthermore,whenever several messages are equally likely, we break the tie by choosingamong them with equal probability. While the resulting analysis does not givethe correct values of the thresholds, it does reproduce some of the qualitativefeatures of the system. It predicts a first-order detectability transitionwhenever $q > 2$, while the finite-temperature cavity method shows that this isthe case only when $q > 4$. It also has a regime analogous to the "hard butdetectable" phase, where the community structure can be partially recovered,but only when the initial messages are sufficiently accurate. Finally, we studya semisupervised setting where we are given the correct labels for a fraction$\rho$ of the nodes. For $q > 2$, we find a regime where the accuracy jumpsdiscontinuously at a critical value of $\rho$.
arxiv-4500-101 | SpeedMachines: Anytime Structured Prediction | http://arxiv.org/pdf/1312.0579v1.pdf | author:Alexander Grubb, Daniel Munoz, J. Andrew Bagnell, Martial Hebert category:cs.LG published:2013-12-02 summary:Structured prediction plays a central role in machine learning applicationsfrom computational biology to computer vision. These models requiresignificantly more computation than unstructured models, and, in manyapplications, algorithms may need to make predictions within a computationalbudget or in an anytime fashion. In this work we propose an anytime techniquefor learning structured prediction that, at training time, incorporates bothstructural elements and feature computation trade-offs that affect test-timeinference. We apply our technique to the challenging problem of sceneunderstanding in computer vision and demonstrate efficient and anytimepredictions that gradually improve towards state-of-the-art classificationperformance as the allotted time increases.
arxiv-4500-102 | A Junction Tree Framework for Undirected Graphical Model Selection | http://arxiv.org/pdf/1304.4910v2.pdf | author:Divyanshu Vats, Robert Nowak category:stat.ML cs.AI cs.IT math.IT published:2013-04-17 summary:An undirected graphical model is a joint probability distribution defined onan undirected graph G*, where the vertices in the graph index a collection ofrandom variables and the edges encode conditional independence relationshipsamong random variables. The undirected graphical model selection (UGMS) problemis to estimate the graph G* given observations drawn from the undirectedgraphical model. This paper proposes a framework for decomposing the UGMSproblem into multiple subproblems over clusters and subsets of the separatorsin a junction tree. The junction tree is constructed using a graph thatcontains a superset of the edges in G*. We highlight three main properties ofusing junction trees for UGMS. First, different regularization parameters ordifferent UGMS algorithms can be used to learn different parts of the graph.This is possible since the subproblems we identify can be solved independentlyof each other. Second, under certain conditions, a junction tree based UGMSalgorithm can produce consistent results with fewer observations than the usualrequirements of existing algorithms. Third, both our theoretical andexperimental results show that the junction tree framework does a significantlybetter job at finding the weakest edges in a graph than existing methods. Thisproperty is a consequence of both the first and second properties. Finally, wenote that our framework is independent of the choice of the UGMS algorithm andcan be used as a wrapper around standard UGMS algorithms for more accurategraph estimation.
arxiv-4500-103 | Families of Parsimonious Finite Mixtures of Regression Models | http://arxiv.org/pdf/1312.0518v1.pdf | author:Utkarsh J. Dang, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2013-12-02 summary:Finite mixtures of regression models offer a flexible framework forinvestigating heterogeneity in data with functional dependencies. These modelscan be conveniently used for unsupervised learning on data with clearregression relationships. We extend such models by imposing aneigen-decomposition on the multivariate error covariance matrix. Byconstraining parts of this decomposition, we obtain families of parsimoniousmixtures of regressions and mixtures of regressions with concomitant variables.These families of models account for correlations between multiple responses.An expectation-maximization algorithm is presented for parameter estimation andperformance is illustrated on simulated and real data.
arxiv-4500-104 | High-Dimensional Screening Using Multiple Grouping of Variables | http://arxiv.org/pdf/1208.2043v3.pdf | author:Divyanshu Vats category:stat.ML cs.IT math.IT published:2012-08-09 summary:Screening is the problem of finding a superset of the set of non-zero entriesin an unknown p-dimensional vector \beta* given n noisy observations.Naturally, we want this superset to be as small as possible. We propose a novelframework for screening, which we refer to as Multiple Grouping (MuG), thatgroups variables, performs variable selection over the groups, and repeats thisprocess multiple number of times to estimate a sequence of sets that containsthe non-zero entries in \beta*. Screening is done by taking an intersection ofall these estimated sets. The MuG framework can be used in conjunction with anygroup based variable selection algorithm. In the high-dimensional setting,where p >> n, we show that when MuG is used with the group Lasso estimator,screening can be consistently performed without using any tuning parameter. Ournumerical simulations clearly show the merits of using the MuG framework inpractice.
arxiv-4500-105 | Bidirectional Recursive Neural Networks for Token-Level Labeling with Structure | http://arxiv.org/pdf/1312.0493v1.pdf | author:Ozan İrsoy, Claire Cardie category:cs.LG cs.CL stat.ML published:2013-12-02 summary:Recently, deep architectures, such as recurrent and recursive neural networkshave been successfully applied to various natural language processing tasks.Inspired by bidirectional recurrent neural networks which use representationsthat summarize the past and future around an instance, we propose a novelarchitecture that aims to capture the structural information around an input,and use it to label instances. We apply our method to the task of opinionexpression extraction, where we employ the binary parse tree of a sentence asthe structure, and word vector representations as the initial representation ofa single token. We conduct preliminary experiments to investigate itsperformance and compare it to the sequential approach.
arxiv-4500-106 | Precise Semidefinite Programming Formulation of Atomic Norm Minimization for Recovering d-Dimensional ($d\geq 2$) Off-the-Grid Frequencies | http://arxiv.org/pdf/1312.0485v1.pdf | author:Weiyu Xu, Jian-Feng Cai, Kumar Vijay Mishra, Myung Cho, Anton Kruger category:cs.IT math.IT math.OC stat.ML published:2013-12-02 summary:Recent research in off-the-grid compressed sensing (CS) has demonstratedthat, under certain conditions, one can successfully recover a spectrallysparse signal from a few time-domain samples even though the dictionary iscontinuous. In particular, atomic norm minimization was proposed in\cite{tang2012csotg} to recover $1$-dimensional spectrally sparse signal.However, in spite of existing research efforts \cite{chi2013compressive}, itwas still an open problem how to formulate an equivalent positive semidefiniteprogram for atomic norm minimization in recovering signals with $d$-dimensional($d\geq 2$) off-the-grid frequencies. In this paper, we settle this problem byproposing equivalent semidefinite programming formulations of atomic normminimization to recover signals with $d$-dimensional ($d\geq 2$) off-the-gridfrequencies.
arxiv-4500-107 | Practical Collapsed Stochastic Variational Inference for the HDP | http://arxiv.org/pdf/1312.0412v1.pdf | author:Arnim Bleier category:cs.LG published:2013-12-02 summary:Recent advances have made it feasible to apply the stochastic variationalparadigm to a collapsed representation of latent Dirichlet allocation (LDA).While the stochastic variational paradigm has successfully been applied to anuncollapsed representation of the hierarchical Dirichlet process (HDP), noattempts to apply this type of inference in a collapsed setting ofnon-parametric topic modeling have been put forward so far. In this paper weexplore such a collapsed stochastic variational Bayes inference for the HDP.The proposed online algorithm is easy to implement and accounts for theinference of hyper-parameters. First experiments show a promising improvementin predictive performance.
arxiv-4500-108 | Inferring Regulatory Networks by Combining Perturbation Screens and Steady State Gene Expression Profiles | http://arxiv.org/pdf/1312.0335v1.pdf | author:Ali Shojaie, Alexandra Jauhiainen, Michael Kallitsis, George Michailidis category:stat.ML q-bio.MN published:2013-12-02 summary:Reconstructing transcriptional regulatory networks is an important task infunctional genomics. Data obtained from experiments that perturb genes byknockouts or RNA interference contain useful information for addressing thisreconstruction problem. However, such data can be limited in size and/or areexpensive to acquire. On the other hand, observational data of the organism insteady state (e.g. wild-type) are more readily available, but theirinformational content is inadequate for the task at hand. We develop acomputational approach to appropriately utilize both data sources forestimating a regulatory network. The proposed approach is based on a three-stepalgorithm to estimate the underlying directed but cyclic network, that uses asinput both perturbation screens and steady state gene expression data. In thefirst step, the algorithm determines causal orderings of the genes that areconsistent with the perturbation data, by combining an exhaustive search methodwith a fast heuristic that in turn couples a Monte Carlo technique with a fastsearch algorithm. In the second step, for each obtained causal ordering, aregulatory network is estimated using a penalized likelihood based method,while in the third step a consensus network is constructed from the highestscored ones. Extensive computational experiments show that the algorithmperforms well in reconstructing the underlying network and clearly outperformscompeting approaches that rely only on a single data source. Further, it isestablished that the algorithm produces a consistent estimate of the regulatorynetwork.
arxiv-4500-109 | Reduced egomotion estimation drift using omnidirectional views | http://arxiv.org/pdf/1307.6962v2.pdf | author:Yalin Bastanlar category:cs.CV cs.RO published:2013-07-26 summary:Estimation of camera motion from a given image sequence becomes degraded asthe length of the sequence increases. In this letter, this phenomenon isdemonstrated and an approach to increase the estimation accuracy is proposed.The proposed method uses an omnidirectional camera in addition to theperspective one and takes advantage of its enlarged view by exploiting thecorrespondences between the omnidirectional and perspective images. Simulatedand real image experiments show that the proposed approach improves theestimation accuracy.
arxiv-4500-110 | Sparse arrays of signatures for online character recognition | http://arxiv.org/pdf/1308.0371v2.pdf | author:Benjamin Graham category:cs.CV cs.NE published:2013-08-01 summary:In mathematics the signature of a path is a collection of iterated integrals,commonly used for solving differential equations. We show that the pathsignature, used as a set of features for consumption by a convolutional neuralnetwork (CNN), improves the accuracy of online character recognition---that isthe task of reading characters represented as a collection of paths. Usingdatasets of letters, numbers, Assamese and Chinese characters, we show that thefirst, second, and even the third iterated integrals contain useful informationfor consumption by a CNN. On the CASIA-OLHWDB1.1 3755 Chinese character dataset, our approach gave atest error of 3.58%, compared with 5.61% for a traditional CNN [Ciresan etal.]. A CNN trained on the CASIA-OLHWDB1.0-1.2 datasets won the ICDAR2013Online Isolated Chinese Character recognition competition. Computationally, we have developed a sparse CNN implementation that make itpractical to train CNNs with many layers of max-pooling. Extending the MNISTdataset by translations, our sparse CNN gets a test error of 0.31%.
arxiv-4500-111 | No Free Lunch Theorem and Bayesian probability theory: two sides of the same coin. Some implications for black-box optimization and metaheuristics | http://arxiv.org/pdf/1311.6041v3.pdf | author:Loris Serafino category:cs.LG published:2013-11-23 summary:Challenging optimization problems, which elude acceptable solution viaconventional calculus methods, arise commonly in different areas of industrialdesign and practice. Hard optimization problems are those who manifest thefollowing behavior: a) high number of independent input variables; b) verycomplex or irregular multi-modal fitness; c) computational expensive fitnessevaluation. This paper will focus on some theoretical issues that have strongimplications for practice. I will stress how an interpretation of the No FreeLunch theorem leads naturally to a general Bayesian optimization framework. Thechoice of a prior over the space of functions is a critical and inevitable stepin every black-box optimization.
arxiv-4500-112 | A Typology of Collaboration Platform Users | http://arxiv.org/pdf/1312.0162v1.pdf | author:Anastasia Bezzubtseva, Dmitry I. Ignatov category:cs.CY cs.HC cs.SI stat.ML 68U35, 91D30 K.4.3 published:2013-11-30 summary:In this paper we present a review of the existing typologies of Internetservice users. We zoom in on social networking services including blogs andcrowdsourcing websites. Based on the results of the analysis of the consideredtypologies obtained by means of FCA we developed a new user typology of acertain class of Internet services, namely a collaboration innovation platform.Cluster analysis of data extracted from the collaboration platform Witology wasused to divide more than 500 participants into six groups based on threeactivity indicators: idea generation, commenting, and evaluation (assigningmarks) The obtained groups and their percentages appear to follow the "90 - 9 -1" rule.
arxiv-4500-113 | Group-Sparse Signal Denoising: Non-Convex Regularization, Convex Optimization | http://arxiv.org/pdf/1308.5038v2.pdf | author:Po-Yu Chen, Ivan W. Selesnick category:cs.CV cs.LG stat.ML published:2013-08-23 summary:Convex optimization with sparsity-promoting convex regularization is astandard approach for estimating sparse signals in noise. In order to promotesparsity more strongly than convex regularization, it is also standard practiceto employ non-convex optimization. In this paper, we take a third approach. Weutilize a non-convex regularization term chosen such that the total costfunction (consisting of data consistency and regularization terms) is convex.Therefore, sparsity is more strongly promoted than in the standard convexformulation, but without sacrificing the attractive aspects of convexoptimization (unique minimum, robust algorithms, etc.). We use this idea toimprove the recently developed 'overlapping group shrinkage' (OGS) algorithmfor the denoising of group-sparse signals. The algorithm is applied to theproblem of speech enhancement with favorable results in terms of both SNR andperceptual quality.
arxiv-4500-114 | Improving Texture Categorization with Biologically Inspired Filtering | http://arxiv.org/pdf/1312.0072v1.pdf | author:Ngoc-Son Vu, Thanh Phuong Nguyen, Christophe Garcia category:cs.CV published:2013-11-30 summary:Within the domain of texture classification, a lot of effort has been spenton local descriptors, leading to many powerful algorithms. However,preprocessing techniques have received much less attention despite theirimportant potential for improving the overall classification performance. Weaddress this question by proposing a novel, simple, yet very powerfulbiologically-inspired filtering (BF) which simulates the performance of humanretina. In the proposed approach, given a texture image, after applying a DoGfilter to detect the "edges", we first split the filtered image into two "maps"alongside the sides of its edges. The feature extraction step is then carriedout on the two "maps" instead of the input image. Our algorithm has severaladvantages such as simplicity, robustness to illumination and noise, anddiscriminative power. Experimental results on three large texture databasesshow that with an extremely low computational cost, the proposed methodimproves significantly the performance of many texture classification systems,notably in noisy environments. The source codes of the proposed algorithm canbe downloaded from https://sites.google.com/site/nsonvu/code.
arxiv-4500-115 | One-Class Classification: Taxonomy of Study and Review of Techniques | http://arxiv.org/pdf/1312.0049v1.pdf | author:Shehroz S. Khan, Michael G. Madden category:cs.LG cs.AI published:2013-11-30 summary:One-class classification (OCC) algorithms aim to build classification modelswhen the negative class is either absent, poorly sampled or not well defined.This unique situation constrains the learning of efficient classifiers bydefining class boundary just with the knowledge of positive class. The OCCproblem has been considered and applied under many research themes, such asoutlier/novelty detection and concept learning. In this paper we present aunified view of the general problem of OCC by presenting a taxonomy of studyfor OCC problems, which is based on the availability of training data,algorithms used and the application domains applied. We further delve into eachof the categories of the proposed taxonomy and present a comprehensiveliterature review of the OCC algorithms, techniques and methodologies with afocus on their significance, limitations and applications. We conclude ourpaper by discussing some open research problems in the field of OCC and presentour vision for future research.
arxiv-4500-116 | Stochastic Optimization of Smooth Loss | http://arxiv.org/pdf/1312.0048v1.pdf | author:Rong Jin category:cs.LG published:2013-11-30 summary:In this paper, we first prove a high probability bound rather than anexpectation bound for stochastic optimization with smooth loss. Furthermore,the existing analysis requires the knowledge of optimal classifier for tuningthe step size in order to achieve the desired bound. However, this informationis usually not accessible in advanced. We also propose a strategy to addressthe limitation.
arxiv-4500-117 | Combination of Diverse Ranking Models for Personalized Expedia Hotel Searches | http://arxiv.org/pdf/1311.7679v1.pdf | author:Xudong Liu, Bing Xu, Yuyu Zhang, Qiang Yan, Liang Pang, Qiang Li, Hanxiao Sun, Bin Wang category:cs.LG published:2013-11-29 summary:The ICDM Challenge 2013 is to apply machine learning to the problem of hotelranking, aiming to maximize purchases according to given hotel characteristics,location attractiveness of hotels, user's aggregated purchase history andcompetitive online travel agency information for each potential hotel choice.This paper describes the solution of team "binghsu & MLRush & BrickMover". Weconduct simple feature engineering work and train different models by eachindividual team member. Afterwards, we use listwise ensemble method to combineeach model's output. Besides describing effective model and features, we willdiscuss about the lessons we learned while using deep learning in thiscompetition.
arxiv-4500-118 | The Power of Asymmetry in Binary Hashing | http://arxiv.org/pdf/1311.7662v1.pdf | author:Behnam Neyshabur, Payman Yadollahpour, Yury Makarychev, Ruslan Salakhutdinov, Nathan Srebro category:cs.LG cs.CV cs.IR published:2013-11-29 summary:When approximating binary similarity using the hamming distance between shortbinary hashes, we show that even if the similarity is symmetric, we can haveshorter and more accurate hashes by using two distinct code maps. I.e. byapproximating the similarity between $x$ and $x'$ as the hamming distancebetween $f(x)$ and $g(x')$, for two distinct binary codes $f,g$, rather than asthe hamming distance between $f(x)$ and $f(x')$.
arxiv-4500-119 | Statistical estimation for optimization problems on graphs | http://arxiv.org/pdf/1311.7656v1.pdf | author:Mikhail Langovoy, Suvrit Sra category:stat.ML cs.DM math.OC stat.CO stat.ME published:2013-11-29 summary:Large graphs abound in machine learning, data mining, and several relatedareas. A useful step towards analyzing such graphs is that of obtaining certainsummary statistics - e.g., or the expected length of a shortest path betweentwo nodes, or the expected weight of a minimum spanning tree of the graph, etc.These statistics provide insight into the structure of a graph, and they canhelp predict global properties of a graph. Motivated thus, we propose to studystatistical properties of structured subgraphs (of a given graph), inparticular, to estimate the expected objective function value of acombinatorial optimization problem over these subgraphs. The general task isvery difficult, if not unsolvable; so for concreteness we describe a morespecific statistical estimation problem based on spanning trees. We hope thatour position paper encourages others to also study other types of graphicalstructures for which one can prove nontrivial statistical estimates.
arxiv-4500-120 | Adaptive nonparametric detection in cryo-electron microscopy | http://arxiv.org/pdf/1311.7650v1.pdf | author:Mikhail Langovoy, Michael Habeck, Bernhard Schoelkopf category:stat.AP stat.ME stat.ML published:2013-11-29 summary:Cryo-electron microscopy (cryo-EM) is an emerging experimental method tocharacterize the structure of large biomolecular assemblies. Single particlecryo-EM records 2D images (so-called micrographs) of projections of thethree-dimensional particle, which need to be processed to obtain thethree-dimensional reconstruction. A crucial step in the reconstruction processis particle picking which involves detection of particles in noisy 2Dmicrographs with low signal-to-noise ratios of typically 1:10 or even lower.Typically, each picture contains a large number of particles, and particleshave unknown irregular and nonconvex shapes.
arxiv-4500-121 | Visualizing and Understanding Convolutional Networks | http://arxiv.org/pdf/1311.2901v3.pdf | author:Matthew D Zeiler, Rob Fergus category:cs.CV published:2013-11-12 summary:Large Convolutional Network models have recently demonstrated impressiveclassification performance on the ImageNet benchmark. However there is no clearunderstanding of why they perform so well, or how they might be improved. Inthis paper we address both issues. We introduce a novel visualization techniquethat gives insight into the function of intermediate feature layers and theoperation of the classifier. We also perform an ablation study to discover theperformance contribution from different model layers. This enables us to findmodel architectures that outperform Krizhevsky \etal on the ImageNetclassification benchmark. We show our ImageNet model generalizes well to otherdatasets: when the softmax classifier is retrained, it convincingly beats thecurrent state-of-the-art results on Caltech-101 and Caltech-256 datasets.
arxiv-4500-122 | Shape from Texture using Locally Scaled Point Processes | http://arxiv.org/pdf/1311.7401v1.pdf | author:Eva-Maria Didden, Thordis Linda Thorarinsdottir, Alex Lenkoski, Christoph Schnörr category:stat.AP cs.CV published:2013-11-28 summary:Shape from texture refers to the extraction of 3D information from 2D imageswith irregular texture. This paper introduces a statistical framework to learnshape from texture where convex texture elements in a 2D image are representedthrough a point process. In a first step, the 2D image is preprocessed togenerate a probability map corresponding to an estimate of the unnormalizedintensity of the latent point process underlying the texture elements. Thelatent point process is subsequently inferred from the probability map in anon-parametric, model free manner. Finally, the 3D information is extractedfrom the point pattern by applying a locally scaled point process model wherethe local scaling function represents the deformation caused by the projectionof a 3D surface onto a 2D image.
arxiv-4500-123 | Gaussian Probabilities and Expectation Propagation | http://arxiv.org/pdf/1111.6832v2.pdf | author:John P. Cunningham, Philipp Hennig, Simon Lacoste-Julien category:stat.ML published:2011-11-29 summary:While Gaussian probability densities are omnipresent in applied mathematics,Gaussian cumulative probabilities are hard to calculate in any but theunivariate case. We study the utility of Expectation Propagation (EP) as anapproximate integration method for this problem. For rectangular integrationregions, the approximation is highly accurate. We also extend the derivationsto the more general case of polyhedral integration regions. However, we findthat in this polyhedral case, EP's answer, though often accurate, can be almostarbitrarily wrong. We consider these unexpected results empirically andtheoretically, both for the problem of Gaussian probabilities and for EP moregenerally. These results elucidate an interesting and non-obvious feature of EPnot yet studied in detail.
arxiv-4500-124 | Unobtrusive Low Cost Pupil Size Measurements using Web cameras | http://arxiv.org/pdf/1311.7327v1.pdf | author:Sergios Petridis, Theodoros Giannakopoulos, Costantine D. Spyropoulos category:cs.CV J.3; I.4.8 published:2013-11-28 summary:Unobtrusive every day health monitoring can be of important use for theelderly population. In particular, pupil size may be a valuable source ofinformation, since, apart from pathological cases, it can reveal the emotionalstate, the fatigue and the ageing. To allow for unobtrusive monitoring to gainacceptance, one should seek for efficient methods of monitoring using com- monlow-cost hardware. This paper describes a method for monitoring pupil sizesusing a common web camera in real time. Our method works by first detecting theface and the eyes area. Subsequently, optimal iris and sclera location andradius, modelled as ellipses, are found using efficient filtering. Finally, thepupil center and radius is estimated by optimal filtering within the area ofthe iris. Experimental result show both the efficiency and the effectiveness ofour approach.
arxiv-4500-125 | Glasgow's Stereo Image Database of Garments | http://arxiv.org/pdf/1311.7295v1.pdf | author:Gerardo Aragon-Camarasa, Susanne B. Oehler, Yuan Liu, Sun Li, Paul Cockshott, J. Paul Siebert category:cs.RO cs.CV published:2013-11-28 summary:To provide insight into cloth perception and manipulation with an activebinocular robotic vision system, we compiled a database of 80 stereo-paircolour images with corresponding horizontal and vertical disparity maps andmask annotations, for 3D garment point cloud rendering has been created andreleased. The stereo-image garment database is part of research conducted underthe EU-FP7 Clothes Perception and Manipulation (CloPeMa) project and belongs toa wider database collection released through CloPeMa (www.clopema.eu). Thisdatabase is based on 16 different off-the-shelve garments. Each garment hasbeen imaged in five different pose configurations on the project's binocularrobot head. A full copy of the database is made available for scientificresearch only at https://sites.google.com/site/ugstereodatabase/.
arxiv-4500-126 | Spatially-Adaptive Reconstruction in Computed Tomography using Neural Networks | http://arxiv.org/pdf/1311.7251v1.pdf | author:Joseph Shtok, Michael Zibulevsky, Michael Elad category:cs.CV cs.LG cs.NE published:2013-11-28 summary:We propose a supervised machine learning approach for boosting existingsignal and image recovery methods and demonstrate its efficacy on example ofimage reconstruction in computed tomography. Our technique is based on a localnonlinear fusion of several image estimates, all obtained by applying a chosenreconstruction algorithm with different values of its control parameters.Usually such output images have different bias/variance trade-off. The fusionof the images is performed by feed-forward neural network trained on a set ofknown examples. Numerical experiments show an improvement in reconstructionquality relatively to existing direct and iterative reconstruction methods.
arxiv-4500-127 | Finding a Maximum Clique using Ant Colony Optimization and Particle Swarm Optimization in Social Networks | http://arxiv.org/pdf/1311.7213v1.pdf | author:Mohammad Soleimani-Pouri, Alireza Rezvanian, Mohammad Reza Meybodi category:cs.SI cs.NE published:2013-11-28 summary:Interaction between users in online social networks plays a key role insocial network analysis. One on important types of social group is fullconnected relation between some users, which known as clique structure.Therefore finding a maximum clique is essential for some analysis. In thispaper, we proposed a new method using ant colony optimization algorithm andparticle swarm optimization algorithm. In the proposed method, in order toattain better results, it is improved process of pheromone update by particleswarm optimization. Simulation results on popular standard social networkbenchmarks in comparison standard ant colony optimization algorithm are shown arelative enhancement of proposed algorithm.
arxiv-4500-128 | An Alternate Approach for Designing a Domain Specific Image Search Prototype Using Histogram | http://arxiv.org/pdf/1401.2902v1.pdf | author:Sukanta Sinha, Rana Dattagupta, Debajyoti Mukhopadhyay category:cs.CV cs.IR published:2013-11-28 summary:Everyone knows that thousand of words are represented by a single image. As aresult image search has become a very popular mechanism for the Web searchers.Image search means, the search results are produced by the search engine shouldbe a set of images along with their Web page Unified Resource Locator. Now Websearcher can perform two types of image search, they are Text to Image andImage to Image search. In Text to Image search, search query should be a text.Based on the input text data system will generate a set of images along withtheir Web page URL as an output. On the other hand, in Image to Image search,search query should be an image and based on this image system will generate aset of images along with their Web page URL as an output. According to thecurrent scenarios, Text to Image search mechanism always not returns perfectresult. It matches the text data and then displays the corresponding images asan output, which is not always perfect. To resolve this problem, Webresearchers have introduced the Image to Image search mechanism. In this paper,we have also proposed an alternate approach of Image to Image search mechanismusing Histogram.
arxiv-4500-129 | Learning Semantic Representations for the Phrase Translation Model | http://arxiv.org/pdf/1312.0482v1.pdf | author:Jianfeng Gao, Xiaodong He, Wen-tau Yih, Li Deng category:cs.CL published:2013-11-28 summary:This paper presents a novel semantic-based phrase translation model. A pairof source and target phrases are projected into continuous-valued vectorrepresentations in a low-dimensional latent semantic space, where theirtranslation score is computed by the distance between the pair in this newspace. The projection is performed by a multi-layer neural network whoseweights are learned on parallel training data. The learning is aimed todirectly optimize the quality of end-to-end machine translation results.Experimental evaluation has been performed on two Europarl translation tasks,English-French and German-English. The results show that the new semantic-basedphrase translation model significantly improves the performance of astate-of-the-art phrase-based statistical machine translation sys-tem, leadingto a gain of 0.7-1.0 BLEU points.
arxiv-4500-130 | ADMM Algorithm for Graphical Lasso with an $\ell_{\infty}$ Element-wise Norm Constraint | http://arxiv.org/pdf/1311.7198v1.pdf | author:Karthik Mohan category:cs.LG math.OC stat.ML published:2013-11-28 summary:We consider the problem of Graphical lasso with an additional $\ell_{\infty}$element-wise norm constraint on the precision matrix. This problem hasapplications in high-dimensional covariance decomposition such as in\citep{Janzamin-12}. We propose an ADMM algorithm to solve this problem. Wealso use a continuation strategy on the penalty parameter to have a fastimplemenation of the algorithm.
arxiv-4500-131 | Real-time High Resolution Fusion of Depth Maps on GPU | http://arxiv.org/pdf/1311.7194v1.pdf | author:Dmitry Trifonov category:cs.GR cs.CV published:2013-11-28 summary:A system for live high quality surface reconstruction using a single movingdepth camera on a commodity hardware is presented. High accuracy and real-timeframe rate is achieved by utilizing graphics hardware computing capabilitiesvia OpenCL and by using sparse data structure for volumetric surfacerepresentation. Depth sensor pose is estimated by combining serial textureregistration algorithm with iterative closest points algorithm (ICP) aligningobtained depth map to the estimated scene model. Aligned surface is then fusedinto the scene. Kalman filter is used to improve fusion quality. Truncatedsigned distance function (TSDF) stored as block-based sparse buffer is used torepresent surface. Use of sparse data structure greatly increases accuracy ofscanned surfaces and maximum scanning area. Traditional GPU implementation ofvolumetric rendering and fusion algorithms were modified to exploit sparsity toachieve desired performance. Incorporation of texture registration for sensorpose estimation and Kalman filter for measurement integration improved accuracyand robustness of scanning process.
arxiv-4500-132 | A Novel Illumination-Invariant Loss for Monocular 3D Pose Estimation | http://arxiv.org/pdf/1311.7186v1.pdf | author:Srimal Jayawardena, Marcus Hutter, Nathan Brewer category:cs.CV published:2013-11-28 summary:The problem of identifying the 3D pose of a known object from a given 2Dimage has important applications in Computer Vision. Our proposed method ofregistering a 3D model of a known object on a given 2D photo of the object hasnumerous advantages over existing methods. It does not require prior training,knowledge of the camera parameters, explicit point correspondences or matchingfeatures between the image and model. Unlike techniques that estimate a partial3D pose (as in an overhead view of traffic or machine parts on a conveyorbelt), our method estimates the complete 3D pose of the object. It works on asingle static image from a given view under varying and unknown lightingconditions. For this purpose we derive a novel illumination-invariant distancemeasure between the 2D photo and projected 3D model, which is then minimised tofind the best pose parameters. Results for vehicle pose detection in realphotographs are presented.
arxiv-4500-133 | Using Multiple Samples to Learn Mixture Models | http://arxiv.org/pdf/1311.7184v1.pdf | author:Jason D Lee, Ran Gilad-Bachrach, Rich Caruana category:stat.ML cs.LG published:2013-11-28 summary:In the mixture models problem it is assumed that there are $K$ distributions$\theta_{1},\ldots,\theta_{K}$ and one gets to observe a sample from a mixtureof these distributions with unknown coefficients. The goal is to associateinstances with their generating distributions, or to identify the parameters ofthe hidden distributions. In this work we make the assumption that we haveaccess to several samples drawn from the same $K$ underlying distributions, butwith different mixing weights. As with topic modeling, having multiple samplesis often a reasonable assumption. Instead of pooling the data into one sample,we prove that it is possible to use the differences between the samples tobetter recover the underlying structure. We present algorithms that recover theunderlying structure under milder assumptions than the current state of artwhen either the dimensionality or the separation is high. The methods, whenapplied to topic modeling, allow generalization to words not present in thetraining data.
arxiv-4500-134 | Cross-Domain Sparse Coding | http://arxiv.org/pdf/1311.7080v1.pdf | author:Jim Jing-Yan Wang category:cs.CV stat.ML published:2013-11-27 summary:Sparse coding has shown its power as an effective data representation method.However, up to now, all the sparse coding approaches are limited within thesingle domain learning problem. In this paper, we extend the sparse coding tocross domain learning problem, which tries to learn from a source domain to atarget domain with significant different distribution. We impose the MaximumMean Discrepancy (MMD) criterion to reduce the cross-domain distributiondifference of sparse codes, and also regularize the sparse codes by the classlabels of the samples from both domains to increase the discriminative ability.The encouraging experiment results of the proposed cross-domain sparse codingalgorithm on two challenging tasks --- image classification of photograph andoil painting domains, and multiple user spam detection --- show the advantageof the proposed method over other cross-domain data representation methods.
arxiv-4500-135 | Image forgery detection based on the fusion of machine learning and block-matching methods | http://arxiv.org/pdf/1311.6934v1.pdf | author:Davide Cozzolino, Diego Gragnaniello, Luisa Verdoliva category:cs.CV published:2013-11-27 summary:Dense local descriptors and machine learning have been used with success inseveral applications, like classification of textures, steganalysis, andforgery detection. We develop a new image forgery detector building upon somedescriptors recently proposed in the steganalysis field suitably merging someof such descriptors, and optimizing a SVM classifier on the available trainingset. Despite the very good performance, very small forgeries are hardly everdetected because they contribute very little to the descriptors. Therefore wealso develop a simple, but extremely specific, copy-move detector based onregion matching and fuse decisions so as to reduce the missing detection rate.Overall results appear to be extremely encouraging.
arxiv-4500-136 | A novel framework for image forgery localization | http://arxiv.org/pdf/1311.6932v1.pdf | author:Davide Cozzolino, Diego Gragnaniello, Luisa Verdoliva category:cs.CV published:2013-11-27 summary:Image forgery localization is a very active and open research field for thedifficulty to handle the large variety of manipulations a malicious user canperform by means of more and more sophisticated image editing tools. Here, wepropose a localization framework based on the fusion of three very differenttools, based, respectively, on sensor noise, patch-matching, and machinelearning. The binary masks provided by these tools are finally fused based onsome suitable reliability indexes. According to preliminary experiments on thetraining set, the proposed framework provides often a very good localizationaccuracy and sometimes valuable clues for visual scrutiny.
arxiv-4500-137 | On Approximate Inference for Generalized Gaussian Process Models | http://arxiv.org/pdf/1311.6371v3.pdf | author:Lifeng Shang, Antoni B. Chan category:stat.ML cs.CV cs.LG published:2013-11-25 summary:A generalized Gaussian process model (GGPM) is a unifying framework thatencompasses many existing Gaussian process (GP) models, such as GP regression,classification, and counting. In the GGPM framework, the observation likelihoodof the GP model is itself parameterized using the exponential familydistribution (EFD). In this paper, we consider efficient algorithms forapproximate inference on GGPMs using the general form of the EFD. A particularGP model and its associated inference algorithms can then be formed by changingthe parameters of the EFD, thus greatly simplifying its creation fortask-specific output domains. We demonstrate the efficacy of this framework bycreating several new GP models for regressing to non-negative reals and to realintervals. We also consider a closed-form Taylor approximation for efficientinference on GGPMs, and elaborate on its connections with other model-specificheuristic closed-form approximations. Finally, we present a comprehensive setof experiments to compare approximate inference algorithms on a wide variety ofGGPMs.
arxiv-4500-138 | Color and Shape Content Based Image Classification using RBF Network and PSO Technique: A Survey | http://arxiv.org/pdf/1311.6881v1.pdf | author:Abhishek Pandey, Anjna Jayant Deen, Rajeev Pandey category:cs.CV cs.LG cs.NE published:2013-11-27 summary:The improvement of the accuracy of image query retrieval used imageclassification technique. Image classification is well known technique ofsupervised learning. The improved method of image classification increases theworking efficiency of image query retrieval. For the improvements ofclassification technique we used RBF neural network function for betterprediction of feature used in image retrieval.Colour content is represented bypixel values in image classification using radial base function(RBF) technique.This approach provides better result compare to SVM technique in imagerepresentation.Image is represented by matrix though RBF using pixel values ofcolour intensity of image. Firstly we using RGB colour model. In this colourmodel we use red, green and blue colour intensity values in matrix.SVM withpartical swarm optimization for image classification is implemented in contentof images which provide better Results based on the proposed approach are foundencouraging in terms of color image classification accuracy.
arxiv-4500-139 | Automatic Road Lighting System (ARLS) Model Based on Image Processing of Moving Object | http://arxiv.org/pdf/1107.0845v4.pdf | author:Suprijadi, Thomas Muliawan, Sparisoma Viridi category:cs.CV published:2011-07-05 summary:Using a vehicle toy (in next future called vehicle) as a moving object anautomatic road lighting system (ARLS) model is constructed. A digital videocamera with 25 fps is used to capture the vehicle motion as it moves in thetest segment of the road. Captured images are then processed to calculatevehicle speed. This information of the speed together with position of vehicleis then used to control the lighting system along the path that passes by thevehicle. Length of the road test segment is 1 m, the video camera is positionedabout 1.1 m above the test segment, and the vehicle toy dimension is 13 cm\times 9.3 cm. In this model, the maximum speed that ARLS can handle is about1.32 m/s, and the highest performance is obtained about 91% at speed 0.93 m/s.
arxiv-4500-140 | Learning Prices for Repeated Auctions with Strategic Buyers | http://arxiv.org/pdf/1311.6838v1.pdf | author:Kareem Amin, Afshin Rostamizadeh, Umar Syed category:cs.LG cs.GT published:2013-11-26 summary:Inspired by real-time ad exchanges for online display advertising, weconsider the problem of inferring a buyer's value distribution for a good whenthe buyer is repeatedly interacting with a seller through a posted-pricemechanism. We model the buyer as a strategic agent, whose goal is to maximizeher long-term surplus, and we are interested in mechanisms that maximize theseller's long-term revenue. We define the natural notion of strategic regret--- the lost revenue as measured against a truthful (non-strategic) buyer. Wepresent seller algorithms that are no-(strategic)-regret when the buyerdiscounts her future surplus --- i.e. the buyer prefers showing advertisementsto users sooner rather than later. We also give a lower bound on strategicregret that increases as the buyer's discounting weakens and shows, inparticular, that any seller algorithm will suffer linear strategic regret ifthere is no discounting.
arxiv-4500-141 | Dynamic Pricing with Limited Supply | http://arxiv.org/pdf/1108.4142v3.pdf | author:Moshe Babaioff, Shaddin Dughmi, Robert Kleinberg, Aleksandrs Slivkins category:cs.GT cs.DS cs.LG published:2011-08-20 summary:We consider the problem of dynamic pricing with limited supply. A seller has$k$ identical items for sale and is facing $n$ potential buyers ("agents") thatare arriving sequentially. Each agent is interested in buying one item. Eachagent's value for an item is an IID sample from some fixed distribution withsupport $[0,1]$. The seller offers a take-it-or-leave-it price to each arrivingagent (possibly different for different agents), and aims to maximize hisexpected revenue. We focus on "prior-independent" mechanisms -- ones that do not use anyinformation about the distribution. They are desirable because knowing thedistribution is unrealistic in many practical scenarios. We study how therevenue of such mechanisms compares to the revenue of the optimal offlinemechanism that knows the distribution ("offline benchmark"). We present a prior-independent dynamic pricing mechanism whose revenue is atmost $O((k \log n)^{2/3})$ less than the offline benchmark, for everydistribution that is regular. In fact, this guarantee holds without *any*assumptions if the benchmark is relaxed to fixed-price mechanisms. Further, weprove a matching lower bound. The performance guarantee for the same mechanismcan be improved to $O(\sqrt{k} \log n)$, with a distribution-dependentconstant, if $k/n$ is sufficiently small. We show that, in the worst case overall demand distributions, this is essentially the best rate that can beobtained with a distribution-specific constant. On a technical level, we exploit the connection to multi-armed bandits (MAB).While dynamic pricing with unlimited supply can easily be seen as an MABproblem, the intuition behind MAB approaches breaks when applied to the settingwith limited supply. Our high-level conceptual contribution is that even thelimited supply setting can be fruitfully treated as a bandit problem.
arxiv-4500-142 | Supervised Texture Classification Using a Novel Compression-Based Similarity Measure | http://arxiv.org/pdf/1207.3071v2.pdf | author:Mehrdad J. Gangeh, Ali Ghodsi, Mohamed S. Kamel category:cs.CV cs.LG published:2012-07-12 summary:Supervised pixel-based texture classification is usually performed in thefeature space. We propose to perform this task in (dis)similarity space byintroducing a new compression-based (dis)similarity measure. The proposedmeasure utilizes two dimensional MPEG-1 encoder, which takes into considerationthe spatial locality and connectivity of pixels in the images. The proposedformulation has been carefully designed based on MPEG encoder functionality. Tothis end, by design, it solely uses P-frame coding to find the (dis)similarityamong patches/images. We show that the proposed measure works properly on bothsmall and large patch sizes. Experimental results show that the proposedapproach significantly improves the performance of supervised pixel-basedtexture classification on Brodatz and outdoor images compared to othercompression-based dissimilarity measures as well as approaches performed infeature space. It also improves the computation speed by about 40% compared toits rivals.
arxiv-4500-143 | Kernelized Supervised Dictionary Learning | http://arxiv.org/pdf/1207.2488v4.pdf | author:Mehrdad J. Gangeh, Ali Ghodsi, Mohamed S. Kamel category:cs.CV cs.LG published:2012-07-10 summary:In this paper, we propose supervised dictionary learning (SDL) byincorporating information on class labels into the learning of the dictionary.To this end, we propose to learn the dictionary in a space where the dependencybetween the signals and their corresponding labels is maximized. To maximizethis dependency, the recently introduced Hilbert Schmidt independence criterion(HSIC) is used. One of the main advantages of this novel approach for SDL isthat it can be easily kernelized by incorporating a kernel, particularly adata-derived kernel such as normalized compression distance, into theformulation. The learned dictionary is compact and the proposed approach isfast. We show that it outperforms other unsupervised and supervised dictionarylearning approaches in the literature, using real-world data.
arxiv-4500-144 | Statistical Inference in Dynamic Treatment Regimes | http://arxiv.org/pdf/1006.5831v3.pdf | author:Eric B. Laber, Min Qian, Dan J. Lizotte, William E. Pelham, Susan A. Murphy category:stat.ME stat.ML stat.OT 47N30 published:2010-06-30 summary:Dynamic treatment regimes are of growing interest across the clinicalsciences as these regimes provide one way to operationalize and thus informsequential personalized clinical decision making. A dynamic treatment regime isa sequence of decision rules, with a decision rule per stage of clinicalintervention; each decision rule maps up-to-date patient information to arecommended treatment. We briefly review a variety of approaches for using datato construct the decision rules. We then review an interesting challenge, thatof nonregularity that often arises in this area. By nonregularity, we mean theparameters indexing the optimal dynamic treatment regime are nonsmoothfunctionals of the underlying generative distribution. A consequence is that no regular or asymptotically unbiased estimator ofthese parameters exists. Nonregularity arises in inference for parameters inthe optimal dynamic treatment regime; we illustrate the effect of nonregularityon asymptotic bias and via sensitivity of asymptotic, limiting, distributionsto local perturbations. We propose and evaluate a locally consistent AdaptiveConfidence Interval (ACI) for the parameters of the optimal dynamic treatmentregime. We use data from the Adaptive Interventions for Children with ADHDstudy as an illustrative example. We conclude by highlighting and discussingemerging theoretical problems in this area.
arxiv-4500-145 | Local and global asymptotic inference in smoothing spline models | http://arxiv.org/pdf/1212.6788v3.pdf | author:Zuofeng Shang, Guang Cheng category:math.ST stat.ML stat.TH published:2012-12-30 summary:This article studies local and global inference for smoothing splineestimation in a unified asymptotic framework. We first introduce a newtechnical tool called functional Bahadur representation, which significantlygeneralizes the traditional Bahadur representation in parametric models, thatis, Bahadur [Ann. Inst. Statist. Math. 37 (1966) 577-580]. Equipped with thistool, we develop four interconnected procedures for inference: (i) pointwiseconfidence interval; (ii) local likelihood ratio testing; (iii) simultaneousconfidence band; (iv) global likelihood ratio testing. In particular, ourconfidence intervals are proved to be asymptotically valid at any point in thesupport, and they are shorter on average than the Bayesian confidence intervalsproposed by Wahba [J. R. Stat. Soc. Ser. B Stat. Methodol. 45 (1983) 133-150]and Nychka [J. Amer. Statist. Assoc. 83 (1988) 1134-1143]. We also discuss aversion of the Wilks phenomenon arising from local/global likelihood ratiotesting. It is also worth noting that our simultaneous confidence bands are thefirst ones applicable to general quasi-likelihood models. Furthermore, issuesrelating to optimality and efficiency are carefully addressed. As a by-product,we discover a surprising relationship between periodic and nonperiodicsmoothing splines in terms of inference.
arxiv-4500-146 | A Novel Family of Adaptive Filtering Algorithms Based on The Logarithmic Cost | http://arxiv.org/pdf/1311.6809v1.pdf | author:Muhammed O. Sayin, N. Denizcan Vanli, Suleyman S. Kozat category:cs.LG published:2013-11-26 summary:We introduce a novel family of adaptive filtering algorithms based on arelative logarithmic cost. The new family intrinsically combines the higher andlower order measures of the error into a single continuous update based on theerror amount. We introduce important members of this family of algorithms suchas the least mean logarithmic square (LMLS) and least logarithmic absolutedifference (LLAD) algorithms that improve the convergence performance of theconventional algorithms. However, our approach and analysis are generic suchthat they cover other well-known cost functions as described in the paper. TheLMLS algorithm achieves comparable convergence performance with the least meanfourth (LMF) algorithm and extends the stability bound on the step size. TheLLAD and least mean square (LMS) algorithms demonstrate similar convergenceperformance in impulse-free noise environments while the LLAD algorithm isrobust against impulsive interferences and outperforms the sign algorithm (SA).We analyze the transient, steady state and tracking performance of theintroduced algorithms and demonstrate the match of the theoretical analyzes andsimulation results. We show the extended stability bound of the LMLS algorithmand analyze the robustness of the LLAD algorithm against impulsiveinterferences. Finally, we demonstrate the performance of our algorithms indifferent scenarios through numerical examples.
arxiv-4500-147 | The asymptotics of ranking algorithms | http://arxiv.org/pdf/1204.1688v3.pdf | author:John C. Duchi, Lester Mackey, Michael I. Jordan category:math.ST cs.LG stat.ML stat.TH published:2012-04-07 summary:We consider the predictive problem of supervised ranking, where the task isto rank sets of candidate items returned in response to queries. Although thereexist statistical procedures that come with guarantees of consistency in thissetting, these procedures require that individuals provide a complete rankingof all items, which is rarely feasible in practice. Instead, individualsroutinely provide partial preference information, such as pairwise comparisonsof items, and more practical approaches to ranking have aimed at modeling thispartial preference data directly. As we show, however, such an approach raisesserious theoretical challenges. Indeed, we demonstrate that many commonly usedsurrogate losses for pairwise comparison data do not yield consistency;surprisingly, we show inconsistency even in low-noise settings. With thesenegative results as motivation, we present a new approach to supervised rankingbased on aggregation of partial preferences, and we develop $U$-statistic-basedempirical risk minimization procedures. We present an asymptotic analysis ofthese new procedures, showing that they yield consistency results that parallelthose available for classification. We complement our theoretical results withan experiment studying the new procedures in a large-scale web-ranking task.
arxiv-4500-148 | Gradient Magnitude Similarity Deviation: A Highly Efficient Perceptual Image Quality Index | http://arxiv.org/pdf/1308.3052v2.pdf | author:Wufeng Xue, Lei Zhang, Xuanqin Mou, Alan C. Bovik category:cs.CV published:2013-08-14 summary:It is an important task to faithfully evaluate the perceptual quality ofoutput images in many applications such as image compression, image restorationand multimedia streaming. A good image quality assessment (IQA) model shouldnot only deliver high quality prediction accuracy but also be computationallyefficient. The efficiency of IQA metrics is becoming particularly important dueto the increasing proliferation of high-volume visual data in high-speednetworks. We present a new effective and efficient IQA model, called gradientmagnitude similarity deviation (GMSD). The image gradients are sensitive toimage distortions, while different local structures in a distorted image sufferdifferent degrees of degradations. This motivates us to explore the use ofglobal variation of gradient based local quality map for overall image qualityprediction. We find that the pixel-wise gradient magnitude similarity (GMS)between the reference and distorted images combined with a novel poolingstrategy the standard deviation of the GMS map can predict accuratelyperceptual image quality. The resulting GMSD algorithm is much faster than moststate-of-the-art IQA methods, and delivers highly competitive predictionaccuracy.
arxiv-4500-149 | Discriminative Density-ratio Estimation | http://arxiv.org/pdf/1311.4486v2.pdf | author:Yun-Qian Miao, Ahmed K. Farahat, Mohamed S. Kamel category:cs.LG published:2013-11-18 summary:The covariate shift is a challenging problem in supervised learning thatresults from the discrepancy between the training and test distributions. Aneffective approach which recently drew a considerable attention in the researchcommunity is to reweight the training samples to minimize that discrepancy. Inspecific, many methods are based on developing Density-ratio (DR) estimationtechniques that apply to both regression and classification problems. Althoughthese methods work well for regression problems, their performance onclassification problems is not satisfactory. This is due to a key observationthat these methods focus on matching the sample marginal distributions withoutpaying attention to preserving the separation between classes in the reweightedspace. In this paper, we propose a novel method for DiscriminativeDensity-ratio (DDR) estimation that addresses the aforementioned problem andaims at estimating the density-ratio of joint distributions in a class-wisemanner. The proposed algorithm is an iterative procedure that alternatesbetween estimating the class information for the test data and estimating newdensity ratio for each class. To incorporate the estimated class information ofthe test data, a soft matching technique is proposed. In addition, we employ aneffective criterion which adopts mutual information as an indicator to stop theiterative procedure while resulting in a decision boundary that lies in asparse region. Experiments on synthetic and benchmark datasets demonstrate thesuperiority of the proposed method in terms of both accuracy and robustness.
arxiv-4500-150 | Universal Codes from Switching Strategies | http://arxiv.org/pdf/1311.6536v1.pdf | author:Wouter M. Koolen, Steven de Rooij category:cs.IT cs.LG math.IT published:2013-11-26 summary:We discuss algorithms for combining sequential prediction strategies, a taskwhich can be viewed as a natural generalisation of the concept of universalcoding. We describe a graphical language based on Hidden Markov Models fordefining prediction strategies, and we provide both existing and new models asexamples. The models include efficient, parameterless models for switchingbetween the input strategies over time, including a model for the case whereswitches tend to occur in clusters, and finally a new model for the scenariowhere the prediction strategies have a known relationship, and where jumps aretypically between strongly related ones. This last model is relevant for codingtime series data where parameter drift is expected. As theoretical ontributionswe introduce an interpolation construction that is useful in the developmentand analysis of new algorithms, and we establish a new sophisticated lemma foranalysing the individual sequence regret of parameterised models.
arxiv-4500-151 | A Blockwise Descent Algorithm for Group-penalized Multiresponse and Multinomial Regression | http://arxiv.org/pdf/1311.6529v1.pdf | author:Noah Simon, Jerome Friedman, Trevor Hastie category:stat.CO stat.ML published:2013-11-26 summary:In this paper we purpose a blockwise descent algorithm for group-penalizedmultiresponse regression. Using a quasi-newton framework we extend this togroup-penalized multinomial regression. We give a publicly availableimplementation for these in R, and compare the speed of this algorithm to acompeting algorithm --- we show that our implementation is an order ofmagnitude faster than its competitor, and can solve gene-expression-sizedproblems in real time.
arxiv-4500-152 | Are all training examples equally valuable? | http://arxiv.org/pdf/1311.6510v1.pdf | author:Agata Lapedriza, Hamed Pirsiavash, Zoya Bylinskii, Antonio Torralba category:cs.CV cs.LG stat.ML published:2013-11-25 summary:When learning a new concept, not all training examples may prove equallyuseful for training: some may have higher or lower training value than others.The goal of this paper is to bring to the attention of the vision community thefollowing considerations: (1) some examples are better than others for trainingdetectors or classifiers, and (2) in the presence of better examples, someexamples may negatively impact performance and removing them may be beneficial.In this paper, we propose an approach for measuring the training value of anexample, and use it for ranking and greedily sorting examples. We test ourmethods on different vision tasks, models, datasets and classifiers. Ourexperiments show that the performance of current state-of-the-art detectors andclassifiers can be improved when training on a subset, rather than the wholetraining set.
arxiv-4500-153 | Parallel Coordinate Descent Methods for Big Data Optimization | http://arxiv.org/pdf/1212.0873v2.pdf | author:Peter Richtárik, Martin Takáč category:math.OC cs.AI stat.ML published:2012-12-04 summary:In this work we show that randomized (block) coordinate descent methods canbe accelerated by parallelization when applied to the problem of minimizing thesum of a partially separable smooth convex function and a simple separableconvex function. The theoretical speedup, as compared to the serial method, andreferring to the number of iterations needed to approximately solve the problemwith high probability, is a simple expression depending on the number ofparallel processors and a natural and easily computable measure of separabilityof the smooth component of the objective function. In the worst case, when nodegree of separability is present, there may be no speedup; in the best case,when the problem is separable, the speedup is equal to the number ofprocessors. Our analysis also works in the mode when the number of blocks beingupdated at each iteration is random, which allows for modeling situations withbusy or unreliable processors. We show that our algorithm is able to solve aLASSO problem involving a matrix with 20 billion nonzeros in 2 hours on a largememory node with 24 cores.
arxiv-4500-154 | On Recursive Edit Distance Kernels with Application to Time Series Classification | http://arxiv.org/pdf/1005.5141v12.pdf | author:Pierre-François Marteau, Sylvie Gibet category:cs.LG cs.IR published:2010-05-27 summary:This paper proposes some extensions to the work on kernels dedicated tostring or time series global alignment based on the aggregation of scoresobtained by local alignments. The extensions we propose allow to construct,from classical recursive definition of elastic distances, recursive editdistance (or time-warp) kernels that are positive definite if some sufficientconditions are satisfied. The sufficient conditions we end-up with are originaland weaker than those proposed in earlier works, although a recursiveregularizing term is required to get the proof of the positive definiteness asa direct consequence of the Haussler's convolution theorem. The classificationexperiment we conducted on three classical time warp distances (two of whichbeing metrics), using Support Vector Machine classifier, leads to concludethat, when the pairwise distance matrix obtained from the training data is\textit{far} from definiteness, the positive definite recursive elastic kernelsoutperform in general the distance substituting kernels for the classicalelastic distances we have tested.
arxiv-4500-155 | Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching | http://arxiv.org/pdf/1311.6425v1.pdf | author:Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo Musé, Guillermo Sapiro category:math.OC cs.LG stat.ML published:2013-11-25 summary:Graph matching is a challenging problem with very important applications in awide range of fields, from image and video analysis to biological andbiomedical problems. We propose a robust graph matching algorithm inspired insparsity-related techniques. We cast the problem, resembling group orcollaborative sparsity formulations, as a non-smooth convex optimizationproblem that can be efficiently solved using augmented Lagrangian techniques.The method can deal with weighted or unweighted graphs, as well as multimodaldata, where different graphs represent different types of data. The proposedapproach is also naturally integrated with collaborative graph inferencetechniques, solving general network inference problems where the observedvariables, possibly coming from different modalities, are not incorrespondence. The algorithm is tested and compared with state-of-the-artgraph matching techniques in both synthetic and real graphs. We also presentresults on multimodal graphs and applications to collaborative inference ofbrain connectivity from alignment-free functional magnetic resonance imaging(fMRI) data. The code is publicly available.
arxiv-4500-156 | Synchronous Context-Free Grammars and Optimal Linear Parsing Strategies | http://arxiv.org/pdf/1311.6421v1.pdf | author:Pierluigi Crescenzi, Daniel Gildea, Andrea Marino, Gianluca Rossi, Giorgio Satta category:cs.FL cs.CL published:2013-11-25 summary:Synchronous Context-Free Grammars (SCFGs), also known as syntax-directedtranslation schemata, are unlike context-free grammars in that they do not havea binary normal form. In general, parsing with SCFGs takes space and timepolynomial in the length of the input strings, but with the degree of thepolynomial depending on the permutations of the SCFG rules. We consider linearparsing strategies, which add one nonterminal at a time. We show that for agiven input permutation, the problems of finding the linear parsing strategywith the minimum space and time complexity are both NP-hard.
arxiv-4500-157 | Large-scale Multi-label Learning with Missing Labels | http://arxiv.org/pdf/1307.5101v3.pdf | author:Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, Inderjit S. Dhillon category:cs.LG published:2013-07-18 summary:The multi-label classification problem has generated significant interest inrecent years. However, existing approaches do not adequately address two keychallenges: (a) the ability to tackle problems with a large number (saymillions) of labels, and (b) the ability to handle data with missing labels. Inthis paper, we directly address both these problems by studying the multi-labelproblem in a generic empirical risk minimization (ERM) framework. Ourframework, despite being simple, is surprisingly able to encompass severalrecent label-compression based methods which can be derived as special cases ofour method. To optimize the ERM problem, we develop techniques that exploit thestructure of specific loss functions - such as the squared loss function - tooffer efficient algorithms. We further show that our learning framework admitsformal excess risk bounds even in the presence of missing labels. Our riskbounds are tight and demonstrate better generalization performance for low-rankpromoting trace-norm regularization when compared to (rank insensitive)Frobenius norm regularization. Finally, we present extensive empirical resultson a variety of benchmark datasets and show that our methods performsignificantly better than existing label compression based methods and canscale up to very large datasets such as the Wikipedia dataset.
arxiv-4500-158 | Learning Reputation in an Authorship Network | http://arxiv.org/pdf/1311.6334v1.pdf | author:Charanpal Dhanjal, Stéphan Clémençon category:cs.SI cs.IR cs.LG stat.ML published:2013-11-25 summary:The problem of searching for experts in a given academic field is hugelyimportant in both industry and academia. We study exactly this issue withrespect to a database of authors and their publications. The idea is to useLatent Semantic Indexing (LSI) and Latent Dirichlet Allocation (LDA) to performtopic modelling in order to find authors who have worked in a query field. Wethen construct a coauthorship graph and motivate the use of influencemaximisation and a variety of graph centrality measures to obtain a ranked listof experts. The ranked lists are further improved using a Markov Chain-basedrank aggregation approach. The complete method is readily scalable to largedatasets. To demonstrate the efficacy of the approach we report on an extensiveset of computational simulations using the Arnetminer dataset. An improvementin mean average precision is demonstrated over the baseline case of simplyusing the order of authors found by the topic models.
arxiv-4500-159 | Summary Statistics for Partitionings and Feature Allocations | http://arxiv.org/pdf/1310.0509v4.pdf | author:Işık Barış Fidaner, Ali Taylan Cemgil category:cs.LG stat.ML published:2013-10-01 summary:Infinite mixture models are commonly used for clustering. One can sample fromthe posterior of mixture assignments by Monte Carlo methods or find its maximuma posteriori solution by optimization. However, in some problems the posterioris diffuse and it is hard to interpret the sampled partitionings. In thispaper, we introduce novel statistics based on block sizes for representingsample sets of partitionings and feature allocations. We develop anelement-based definition of entropy to quantify segmentation among theirelements. Then we propose a simple algorithm called entropy agglomeration (EA)to summarize and visualize this information. Experiments on various infinitemixture posteriors as well as a feature allocation dataset demonstrate that theproposed statistics are useful in practice.
arxiv-4500-160 | Novelty Detection Under Multi-Instance Multi-Label Framework | http://arxiv.org/pdf/1311.6211v1.pdf | author:Qi Lou, Raviv Raich, Forrest Briggs, Xiaoli Z. Fern category:cs.LG published:2013-11-25 summary:Novelty detection plays an important role in machine learning and signalprocessing. This paper studies novelty detection in a new setting where thedata object is represented as a bag of instances and associated with multipleclass labels, referred to as multi-instance multi-label (MIML) learning.Contrary to the common assumption in MIML that each instance in a bag belongsto one of the known classes, in novelty detection, we focus on the scenariowhere bags may contain novel-class instances. The goal is to determine, for anygiven instance in a new bag, whether it belongs to a known class or a novelclass. Detecting novelty in the MIML setting captures many real-world phenomenaand has many potential applications. For example, in a collection of taggedimages, the tag may only cover a subset of objects existing in the images.Discovering an object whose class has not been previously tagged can be usefulfor the purpose of soliciting a label for the new object class. To address thisnovel problem, we present a discriminative framework for detecting new classinstances. Experiments demonstrate the effectiveness of our proposed method,and reveal that the presence of unlabeled novel instances in training bags ishelpful to the detection of such instances in testing stage.
arxiv-4500-161 | A Complete Characterization of Statistical Query Learning with Applications to Evolvability | http://arxiv.org/pdf/1002.3183v3.pdf | author:Vitaly Feldman category:cs.CC cs.LG published:2010-02-16 summary:Statistical query (SQ) learning model of Kearns (1993) is a naturalrestriction of the PAC learning model in which a learning algorithm is allowedto obtain estimates of statistical properties of the examples but cannot seethe examples themselves. We describe a new and simple characterization of thequery complexity of learning in the SQ learning model. Unlike the previouslyknown bounds on SQ learning our characterization preserves the accuracy and theefficiency of learning. The preservation of accuracy implies that that ourcharacterization gives the first characterization of SQ learning in theagnostic learning framework. The preservation of efficiency is achieved using anew boosting technique and allows us to derive a new approach to the design ofevolutionary algorithms in Valiant's (2006) model of evolvability. We use thisapproach to demonstrate the existence of a large class of monotone evolutionarylearning algorithms based on square loss performance estimation. These resultsdiffer significantly from the few known evolutionary algorithms and giveevidence that evolvability in Valiant's model is a more versatile phenomenonthan there had been previous reason to suspect.
arxiv-4500-162 | Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization | http://arxiv.org/pdf/1311.5750v2.pdf | author:Xiao-Tong Yuan, Ping Li, Tong Zhang category:cs.LG cs.NA stat.ML published:2013-11-22 summary:Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedurefor finding sparse solutions of underdetermined linear systems. This method hasbeen shown to have strong theoretical guarantee and impressive numericalperformance. In this paper, we generalize HTP from compressive sensing to ageneric problem setup of sparsity-constrained convex optimization. The proposedalgorithm iterates between a standard gradient descent step and a hardthresholding step with or without debiasing. We prove that our method enjoysthe strong guarantees analogous to HTP in terms of rate of convergence andparameter estimation accuracy. Numerical evidences show that our method issuperior to the state-of-the-art greedy selection methods in sparse logisticregression and sparse precision matrix estimation tasks.
arxiv-4500-163 | Sparse CCA via Precision Adjusted Iterative Thresholding | http://arxiv.org/pdf/1311.6186v1.pdf | author:Mengjie Chen, Chao Gao, Zhao Ren, Harrison H. Zhou category:math.ST stat.ME stat.ML stat.TH published:2013-11-24 summary:Sparse Canonical Correlation Analysis (CCA) has received considerableattention in high-dimensional data analysis to study the relationship betweentwo sets of random variables. However, there has been remarkably littletheoretical statistical foundation on sparse CCA in high-dimensional settingsdespite active methodological and applied research activities. In this paper,we introduce an elementary sufficient and necessary characterization such thatthe solution of CCA is indeed sparse, propose a computationally efficientprocedure, called CAPIT, to estimate the canonical directions, and show thatthe procedure is rate-optimal under various assumptions on nuisance parameters.The procedure is applied to a breast cancer dataset from The Cancer GenomeAtlas project. We identify methylation probes that are associated with genes,which have been previously characterized as prognosis signatures of themetastasis of breast cancer.
arxiv-4500-164 | Robust Low-rank Tensor Recovery: Models and Algorithms | http://arxiv.org/pdf/1311.6182v1.pdf | author:Donald Goldfarb, Zhiwei Qin category:stat.ML published:2013-11-24 summary:Robust tensor recovery plays an instrumental role in robustifying tensordecompositions for multilinear data analysis against outliers, grosscorruptions and missing values and has a diverse array of applications. In thispaper, we study the problem of robust low-rank tensor recovery in a convexoptimization framework, drawing upon recent advances in robust PrincipalComponent Analysis and tensor completion. We propose tailored optimizationalgorithms with global convergence guarantees for solving both the constrainedand the Lagrangian formulations of the problem. These algorithms are based onthe highly efficient alternating direction augmented Lagrangian and acceleratedproximal gradient methods. We also propose a nonconvex model that can oftenimprove the recovery results from the convex models. We investigate theempirical recoverability properties of the convex and nonconvex formulationsand compare the computational performance of the algorithms on simulated data.We demonstrate through a number of real applications the practicaleffectiveness of this convex optimization framework for robust low-rank tensorrecovery.
arxiv-4500-165 | Ranking and combining multiple predictors without labeled data | http://arxiv.org/pdf/1303.3257v3.pdf | author:Fabio Parisi, Francesco Strino, Boaz Nadler, Yuval Kluger category:stat.ML cs.LG published:2013-03-13 summary:In a broad range of classification and decision making problems, one is giventhe advice or predictions of several classifiers, of unknown reliability, overmultiple questions or queries. This scenario is different from the standardsupervised setting, where each classifier accuracy can be assessed usingavailable labeled data, and raises two questions: given only the predictions ofseveral classifiers over a large set of unlabeled test data, is it possible toa) reliably rank them; and b) construct a meta-classifier more accurate thanmost classifiers in the ensemble? Here we present a novel spectral approach toaddress these questions. First, assuming conditional independence betweenclassifiers, we show that the off-diagonal entries of their covariance matrixcorrespond to a rank-one matrix. Moreover, the classifiers can be ranked usingthe leading eigenvector of this covariance matrix, as its entries areproportional to their balanced accuracies. Second, via a linear approximationto the maximum likelihood estimator, we derive the Spectral Meta-Learner (SML),a novel ensemble classifier whose weights are equal to this eigenvectorentries. On both simulated and real data, SML typically achieves a higheraccuracy than most classifiers in the ensemble and can provide a betterstarting point than majority voting, for estimating the maximum likelihoodsolution. Furthermore, SML is robust to the presence of small malicious groupsof classifiers designed to veer the ensemble prediction away from the (unknown)ground truth.
arxiv-4500-166 | Detection of Partially Visible Objects | http://arxiv.org/pdf/1311.6758v1.pdf | author:Patrick Ott, Mark Everingham, Jiri Matas category:cs.CV published:2013-11-24 summary:An "elephant in the room" for most current object detection and localizationmethods is the lack of explicit modelling of partial visibility due toocclusion by other objects or truncation by the image boundary. Based on asliding window approach, we propose a detection method which explicitly modelspartial visibility by treating it as a latent variable. A novel non-maximumsuppression scheme is proposed which takes into account the inferred partialvisibility of objects while providing a globally optimal solution. The methodgives more detailed scene interpretations than conventional detectors in thatwe are able to identify the visible parts of an object. We report improvedaverage precision on the PASCAL VOC 2010 dataset compared to a baselinedetector.
arxiv-4500-167 | Skin Texture Recognition Using Neural Networks | http://arxiv.org/pdf/1311.6049v1.pdf | author:Nidhal K. El Abbadi, Nazar Dahir, Zaid Abd Alkareem category:cs.CV published:2013-11-23 summary:Skin recognition is used in many applications ranging from algorithms forface detection, hand gesture analysis, and to objectionable image filtering. Inthis work a skin recognition system was developed and tested. While many skinsegmentation algorithms relay on skin color, our work relies on both skin colorand texture features (features derives from the GLCM) to give a better and moreefficient recognition accuracy of skin textures. We used feed forward neuralnetworks to classify input textures images to be skin or non skin textures. Thesystem gave very encouraging results during the neural network generalizationface.
arxiv-4500-168 | On the Design and Analysis of Multiple View Descriptors | http://arxiv.org/pdf/1311.6048v1.pdf | author:Jingming Dong, Jonathan Balzer, Damek Davis, Joshua Hernandez, Stefano Soatto category:cs.CV published:2013-11-23 summary:We propose an extension of popular descriptors based on gradient orientationhistograms (HOG, computed in a single image) to multiple views. It hinges oninterpreting HOG as a conditional density in the space of sampled images, wherethe effects of nuisance factors such as viewpoint and illumination aremarginalized. However, such marginalization is performed with respect to a verycoarse approximation of the underlying distribution. Our extension leverages onthe fact that multiple views of the same scene allow separating intrinsic fromnuisance variability, and thus afford better marginalization of the latter. Theresult is a descriptor that has the same complexity of single-view HOG, and canbe compared in the same manner, but exploits multiple views to better trade offinsensitivity to nuisance variability with specificity to intrinsicvariability. We also introduce a novel multi-view wide-baseline matchingdataset, consisting of a mixture of real and synthetic objects with groundtruthed camera motion and dense three-dimensional geometry.
arxiv-4500-169 | Build Electronic Arabic Lexicon | http://arxiv.org/pdf/1311.6045v1.pdf | author:Nidhal El-Abbadi, Ahmed Nidhal Khdhair, Adel Al-Nasrawi category:cs.CL published:2013-11-23 summary:There are many known Arabic lexicons organized on different ways, each ofthem has a different number of Arabic words according to its organization way.This paper has used mathematical relations to count a number of Arabic words,which proofs the number of Arabic words presented by Al Farahidy. The paperalso presents new way to build an electronic Arabic lexicon by using a hashfunction that converts each word (as input) to correspond a unique integernumber (as output), these integer numbers will be used as an index to a lexiconentry.
arxiv-4500-170 | Dynamic Model of Facial Expression Recognition based on Eigen-face Approach | http://arxiv.org/pdf/1311.6007v1.pdf | author:Nikunj Bajaj, Aurobinda Routray, S L Happy category:cs.CV published:2013-11-23 summary:Emotions are best way of communicating information; and sometimes it carrymore information than words. Recently, there has been a huge interest inautomatic recognition of human emotion because of its wide spread applicationin security, surveillance, marketing, advertisement, and human-computerinteraction. To communicate with a computer in a natural way, it will bedesirable to use more natural modes of human communication based on voice,gestures and facial expressions. In this paper, a holistic approach for facialexpression recognition is proposed which captures the variation in facialfeatures in temporal domain and classifies the sequence of images in differentemotions. The proposed method uses Haar-like features to detect face in animage. The dimensionality of the eigenspace is reduced using PrincipalComponent Analysis (PCA). By projecting the subsequent face images intoprincipal eigen directions, the variation pattern of the obtained weight vectoris modeled to classify it into different emotions. Owing to the variations ofexpressions for different people and its intensity, a person specific methodfor emotion recognition is followed. Using the gray scale images of the frontalface, the system is able to classify four basic emotions such as happiness,sadness, surprise, and anger.
arxiv-4500-171 | Brain Tumor Detection Based On Symmetry Information | http://arxiv.org/pdf/1401.6127v1.pdf | author:Narkhede Sachin G, Vaishali Khairnar category:cs.CV published:2013-11-23 summary:Advances in computing technology have allowed researchers across many fieldsof endeavor to collect and maintain vast amounts of observational statisticaldata such as clinical data, biological patient data, data regarding access ofweb sites, financial data, and the like. This paper addresses some of thechallenging issues on brain magnetic resonance (MR) image tumor segmentationcaused by the weak correlation between magnetic resonance imaging (MRI)intensity and anatomical meaning. With the objective of utilizing moremeaningful information to improve brain tumor segmentation, an approach whichemploys bilateral symmetry information as an additional feature forsegmentation is proposed. This is motivated by potential performanceimprovement in the general automatic brain tumor segmentation systems which areimportant for many medical and scientific applications
arxiv-4500-172 | Fast Training of Effective Multi-class Boosting Using Coordinate Descent Optimization | http://arxiv.org/pdf/1311.5947v1.pdf | author:Guosheng Lin, Chunhua Shen, Anton van den Hengel, David Suter category:cs.CV cs.LG stat.CO published:2013-11-23 summary:Wepresentanovelcolumngenerationbasedboostingmethod for multi-classclassification. Our multi-class boosting is formulated in a single optimizationproblem as in Shen and Hao (2011). Different from most existing multi-classboosting methods, which use the same set of weak learners for all the classes,we train class specified weak learners (i.e., each class has a different set ofweak learners). We show that using separate weak learner sets for each classleads to fast convergence, without introducing additional computationaloverhead in the training procedure. To further make the training more efficientand scalable, we also propose a fast co- ordinate descent method for solvingthe optimization problem at each boosting iteration. The proposed coordinatedescent method is conceptually simple and easy to implement in that it is aclosed-form solution for each coordinate update. Experimental results on avariety of datasets show that, compared to a range of existing multi-classboosting meth- ods, the proposed method has much faster convergence rate andbetter generalization performance in most cases. We also empirically show thatthe proposed fast coordinate descent algorithm needs less training time thanthe MultiBoost algorithm in Shen and Hao (2011).
arxiv-4500-173 | Automatic Ranking of MT Outputs using Approximations | http://arxiv.org/pdf/1311.5836v1.pdf | author:Pooja Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-11-22 summary:Since long, research on machine translation has been ongoing. Still, we donot get good translations from MT engines so developed. Manual ranking of theseoutputs tends to be very time consuming and expensive. Identifying which one isbetter or worse than the others is a very taxing task. In this paper, we showan approach which can provide automatic ranks to MT outputs (translations)taken from different MT Engines and which is based on N-gram approximations. Weprovide a solution where no human intervention is required for ranking systems.Further we also show the evaluations of our results which show equivalentresults as that of human ranking.
arxiv-4500-174 | Dictionary-Learning-Based Reconstruction Method for Electron Tomography | http://arxiv.org/pdf/1311.5830v1.pdf | author:Baodong Liu, Hengyong Yu, Scott S. Verbridge, Lizhi Sun, Ge Wang category:cs.CV physics.med-ph published:2013-11-22 summary:Electron tomography usually suffers from so called missing wedge artifactscaused by limited tilt angle range. An equally sloped tomography (EST)acquisition scheme (which should be called the linogram sampling scheme) wasrecently applied to achieve 2.4-angstrom resolution. On the other hand, acompressive sensing-inspired reconstruction algorithm, known as adaptivedictionary based statistical iterative reconstruction (ADSIR), has beenreported for x-ray computed tomography. In this paper, we evaluate the EST,ADSIR and an ordered-subset simultaneous algebraic reconstruction technique(OS-SART), and compare the ES and equally angled (EA) data acquisition modes.Our results show that OS-SART is comparable to EST, and the ADSIR outperformsEST and OS-SART. Furthermore, the equally sloped projection data acquisitionmode has no advantage over the conventional equally angled mode in the context.
arxiv-4500-175 | Automated and Weighted Self-Organizing Time Maps | http://arxiv.org/pdf/1311.5763v1.pdf | author:Peter Sarlin category:cs.NE cs.HC published:2013-11-22 summary:This paper proposes schemes for automated and weighted Self-Organizing TimeMaps (SOTMs). The SOTM provides means for a visual approach to evolutionaryclustering, which aims at producing a sequence of clustering solutions. Thistask we denote as visual dynamic clustering. The implication of an automatedSOTM is not only a data-driven parametrization of the SOTM, but also thefeature of adjusting the training to the characteristics of the data at eachtime step. The aim of the weighted SOTM is to improve learning from moretrustworthy or important data with an instance-varying weight. The schemes forautomated and weighted SOTMs are illustrated on two real-world datasets: (i)country-level risk indicators to measure the evolution of global imbalances,and (ii) credit applicant data to measure the evolution of firm-level creditrisks.
arxiv-4500-176 | Learning Pairwise Graphical Models with Nonlinear Sufficient Statistics | http://arxiv.org/pdf/1311.5479v2.pdf | author:Xiao-Tong Yuan, Ping Li, Tong Zhang category:stat.ML published:2013-11-21 summary:We investigate a generic problem of learning pairwise exponential familygraphical models with pairwise sufficient statistics defined by a globalmapping function, e.g., Mercer kernels. This subclass of pairwise graphicalmodels allow us to flexibly capture complex interactions among variables beyondpairwise product. We propose two $\ell_1$-norm penalized maximum likelihoodestimators to learn the model parameters from i.i.d. samples. The first one isa joint estimator which estimates all the parameters simultaneously. The secondone is a node-wise conditional estimator which estimates the parametersindividually for each node. For both estimators, we show that under properconditions the extra flexibility gained in our model comes at almost no cost ofstatistical and computational efficiency. We demonstrate the advantages of ourmodel over state-of-the-art methods on synthetic and real datasets.
arxiv-4500-177 | Complexity measurement of natural and artificial languages | http://arxiv.org/pdf/1311.5427v2.pdf | author:Gerardo Febres, Klaus Jaffe, Carlos Gershenson category:cs.CL cs.IT math.IT nlin.AO physics.soc-ph published:2013-11-20 summary:We compared entropy for texts written in natural languages (English, Spanish)and artificial languages (computer software) based on a simple expression forthe entropy as a function of message length and specific word diversity. Codetext written in artificial languages showed higher entropy than text of similarlength expressed in natural languages. Spanish texts exhibit more symbolicdiversity than English ones. Results showed that algorithms based on complexitymeasures differentiate artificial from natural languages, and that textanalysis based on complexity measures allows the unveiling of important aspectsof their nature. We propose specific expressions to examine entropy relatedaspects of tests and estimate the values of entropy, emergence,self-organization and complexity based on specific diversity and messagelength.
arxiv-4500-178 | Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis | http://arxiv.org/pdf/1311.5422v2.pdf | author:Nikhil Rao, Christopher Cox, Robert Nowak, Timothy Rogers category:cs.LG stat.ML published:2013-11-20 summary:Multitask learning can be effective when features useful in one task are alsouseful for other tasks, and the group lasso is a standard method for selectinga common subset of features. In this paper, we are interested in a lessrestrictive form of multitask learning, wherein (1) the available features canbe organized into subsets according to a notion of similarity and (2) featuresuseful in one task are similar, but not necessarily identical, to the featuresbest suited for other tasks. The main contribution of this paper is a newprocedure called Sparse Overlapping Sets (SOS) lasso, a convex optimizationthat automatically selects similar features for related learning tasks. Errorbounds are derived for SOSlasso and its consistency is established for squarederror loss. In particular, SOSlasso is motivated by multi- subject fMRI studiesin which functional activity is classified using brain voxels as features.Experiments with real and synthetic data demonstrate the advantages of SOSlassocompared to the lasso and group lasso.
arxiv-4500-179 | A quantum genetic algorithm with quantum crossover and mutation operations | http://arxiv.org/pdf/1202.2026v5.pdf | author:Akira SaiToh, Robabeh Rahimi, Mikio Nakahara category:cs.NE quant-ph 68Q12 published:2012-02-09 summary:In the context of evolutionary quantum computing in the literal meaning, aquantum crossover operation has not been introduced so far. Here, we introducea novel quantum genetic algorithm which has a quantum crossover procedureperforming crossovers among all chromosomes in parallel for each generation. Acomplexity analysis shows that a quadratic speedup is achieved over itsclassical counterpart in the dominant factor of the run time to handle eachgeneration.
arxiv-4500-180 | Learning Non-Linear Feature Maps | http://arxiv.org/pdf/1311.5636v1.pdf | author:Dimitrios Athanasakis, John Shawe-Taylor, Delmiro Fernandez-Reyes category:cs.LG published:2013-11-22 summary:Feature selection plays a pivotal role in learning, particularly in areaswere parsimonious features can provide insight into the underlying process,such as biology. Recent approaches for non-linear feature selection employinggreedy optimisation of Centred Kernel Target Alignment(KTA), while exhibitingstrong results in terms of generalisation accuracy and sparsity, can becomecomputationally prohibitive for high-dimensional datasets. We propose randSel,a randomised feature selection algorithm, with attractive scaling properties.Our theoretical analysis of randSel provides strong probabilistic guaranteesfor the correct identification of relevant features. Experimental results onreal and artificial data, show that the method successfully identifieseffective features, performing better than a number of competitive approaches.
arxiv-4500-181 | Compressive Measurement Designs for Estimating Structured Signals in Structured Clutter: A Bayesian Experimental Design Approach | http://arxiv.org/pdf/1311.5599v1.pdf | author:Swayambhoo Jain, Akshay Soni, Jarvis Haupt category:stat.ML cs.LG published:2013-11-21 summary:This work considers an estimation task in compressive sensing, where the goalis to estimate an unknown signal from compressive measurements that arecorrupted by additive pre-measurement noise (interference, or clutter) as wellas post-measurement noise, in the specific setting where some (perhaps limited)prior knowledge on the signal, interference, and noise is available. Thespecific aim here is to devise a strategy for incorporating this priorinformation into the design of an appropriate compressive measurement strategy.Here, the prior information is interpreted as statistics of a priordistribution on the relevant quantities, and an approach based on BayesianExperimental Design is proposed. Experimental results on synthetic datademonstrate that the proposed approach outperforms traditional randomcompressive measurement designs, which are agnostic to the prior information,as well as several other knowledge-enhanced sensing matrix designs based onmore heuristic notions.
arxiv-4500-182 | Adaptive Learning of Region-based pLSA Model for Total Scene Annotation | http://arxiv.org/pdf/1311.5590v1.pdf | author:Yuzhu Zhou, Le Li, Honggang Zhang category:cs.CV published:2013-11-21 summary:In this paper, we present a region-based pLSA model to accomplish the task oftotal scene annotation. To be more specific, we not only properly generate alist of tags for each image, but also localizing each region with itscorresponding tag. We integrate advantages of different existing region-basedworks: employ efficient and powerful JSEG algorithm for segmentation so thateach region can easily express meaningful object information; the introductionof pLSA model can help better capturing semantic information behind thelow-level features. Moreover, we also propose an adaptive padding mechanism toautomatically choose the optimal padding strategy for each region, whichdirectly increases the overall system performance. Finally we conduct 3experiments to verify our ideas on Corel database and demonstrate theeffectiveness and accuracy of our system.
arxiv-4500-183 | Covariance Estimation in High Dimensions via Kronecker Product Expansions | http://arxiv.org/pdf/1302.2686v10.pdf | author:Theodoros Tsiligkaridis, Alfred O. Hero III category:stat.ME stat.ML published:2013-02-12 summary:This paper presents a new method for estimating high dimensional covariancematrices. The method, permuted rank-penalized least-squares (PRLS), is based ona Kronecker product series expansion of the true covariance matrix. Assuming ani.i.d. Gaussian random sample, we establish high dimensional rates ofconvergence to the true covariance as both the number of samples and the numberof variables go to infinity. For covariance matrices of low separation rank,our results establish that PRLS has significantly faster convergence than thestandard sample covariance matrix (SCM) estimator. The convergence ratecaptures a fundamental tradeoff between estimation error and approximationerror, thus providing a scalable covariance estimation framework in terms ofseparation rank, similar to low rank approximation of covariance matrices. TheMSE convergence rates generalize the high dimensional rates recently obtainedfor the ML Flip-flop algorithm for Kronecker product covariance estimation. Weshow that a class of block Toeplitz covariance matrices is approximatable bylow separation rank and give bounds on the minimal separation rank $r$ thatensures a given level of bias. Simulations are presented to validate thetheoretical bounds. As a real world application, we illustrate the utility ofthe proposed Kronecker covariance estimator for spatio-temporal linear leastsquares prediction of multivariate wind speed measurements.
arxiv-4500-184 | A Unified SVM Framework for Signal Estimation | http://arxiv.org/pdf/1311.5406v1.pdf | author:José Luis Rojo-Álvarez, Manel Martínez-Ramón, Jordi Muñoz-Marí, Gustavo Camps-Valls category:stat.ML stat.AP published:2013-11-21 summary:This paper presents a unified framework to tackle estimation problems inDigital Signal Processing (DSP) using Support Vector Machines (SVMs). The useof SVMs in estimation problems has been traditionally limited to its mere useas a black-box model. Noting such limitations in the literature, we takeadvantage of several properties of Mercer's kernels and functional analysis todevelop a family of SVM methods for estimation in DSP. Three types of signalmodel equations are analyzed. First, when a specific time-signal structure isassumed to model the underlying system that generated the data, the linearsignal model (so called Primal Signal Model formulation) is first stated andanalyzed. Then, non-linear versions of the signal structure can be readilydeveloped by following two different approaches. On the one hand, the signalmodel equation is written in reproducing kernel Hilbert spaces (RKHS) using thewell-known RKHS Signal Model formulation, and Mercer's kernels are readily usedin SVM non-linear algorithms. On the other hand, in the alternative and not socommon Dual Signal Model formulation, a signal expansion is made by using anauxiliary signal model equation given by a non-linear regression of each timeinstant in the observed time series. These building blocks can be used togenerate different novel SVM-based methods for problems of signal estimation,and we deal with several of the most important ones in DSP. We illustrate theusefulness of this methodology by defining SVM algorithms for linear andnon-linear system identification, spectral analysis, nonuniform interpolation,sparse deconvolution, and array processing. The performance of the developedSVM methods is compared to standard approaches in all these settings. Theexperimental results illustrate the generality, simplicity, and capabilities ofthe proposed SVM framework for DSP.
arxiv-4500-185 | Nonparametric Bayesian models of hierarchical structure in complex networks | http://arxiv.org/pdf/1311.1033v2.pdf | author:Mikkel N. Schmidt, Tue Herlau, Morten Mørup category:stat.ML published:2013-11-05 summary:Analyzing and understanding the structure of complex relational data isimportant in many applications including analysis of the connectivity in thehuman brain. Such networks can have prominent patterns on different scales,calling for a hierarchically structured model. We propose two non-parametricBayesian hierarchical network models based on Gibbs fragmentation tree priors,and demonstrate their ability to capture nested patterns in simulated networks.On real networks we demonstrate detection of hierarchical structure and showpredictive performance on par with the state of the art. We envision that ourmethods can be employed in exploratory analysis of large scale complex networksfor example to model human brain connectivity.
arxiv-4500-186 | Comparative Study Of Image Edge Detection Algorithms | http://arxiv.org/pdf/1311.4963v2.pdf | author:Shubham Saini, Bhavesh Kasliwal, Shraey Bhatia category:cs.CV published:2013-11-20 summary:Since edge detection is in the forefront of image processing for objectdetection, it is crucial to have a good understanding of edge detectionalgorithms. The reason for this is that edges form the outline of an object. Anedge is the boundary between an object and the background, and indicates theboundary between overlapping objects. This means that if the edges in an imagecan be identified accurately, all of the objects can be located and basicproperties such as area, perimeter, and shape can be measured. Since computervision involves the identification and classification of objects in an image,edge detection is an essential tool. We tested two edge detectors that usedifferent methods for detecting edges and compared their results under avariety of situations to determine which detector was preferable underdifferent sets of conditions.
arxiv-4500-187 | Texture descriptor combining fractal dimension and artificial crawlers | http://arxiv.org/pdf/1311.5290v1.pdf | author:Wesley Nunes Gonçalves, Bruno Brandoli Machado, Odemir Martinez Bruno category:cs.CV published:2013-11-21 summary:Texture is an important visual attribute used to describe images. There aremany methods available for texture analysis. However, they do not capture thedetails richness of the image surface. In this paper, we propose a new methodto describe textures using the artificial crawler model. This model assumesthat each agent can interact with the environment and each other. Since thisswarm system alone does not achieve a good discrimination, we developed a newmethod to increase the discriminatory power of artificial crawlers, togetherwith the fractal dimension theory. Here, we estimated the fractal dimension bythe Bouligand-Minkowski method due to its precision in quantifying structuralproperties of images. We validate our method on two texture datasets and theexperimental results reveal that our method leads to highly discriminativetextural features. The results indicate that our method can be used indifferent texture applications.
arxiv-4500-188 | Robust Compressed Sensing and Sparse Coding with the Difference Map | http://arxiv.org/pdf/1311.0053v2.pdf | author:Will Landecker, Rick Chartrand, Simon DeDeo category:cs.CV stat.ML published:2013-10-31 summary:In compressed sensing, we wish to reconstruct a sparse signal $x$ fromobserved data $y$. In sparse coding, on the other hand, we wish to find arepresentation of an observed signal $y$ as a sparse linear combination, withcoefficients $x$, of elements from an overcomplete dictionary. While manyalgorithms are competitive at both problems when $x$ is very sparse, it can bechallenging to recover $x$ when it is less sparse. We present the DifferenceMap, which excels at sparse recovery when sparseness is lower and noise ishigher. The Difference Map out-performs the state of the art withreconstruction from random measurements and natural image reconstruction viasparse coding.
arxiv-4500-189 | Streaming Variational Bayes | http://arxiv.org/pdf/1307.6769v2.pdf | author:Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson, Michael I. Jordan category:stat.ML cs.LG published:2013-07-25 summary:We present SDA-Bayes, a framework for (S)treaming, (D)istributed,(A)synchronous computation of a Bayesian posterior. The framework makesstreaming updates to the estimated posterior according to a user-specifiedapproximation batch primitive. We demonstrate the usefulness of our framework,with variational Bayes (VB) as the primitive, by fitting the latent Dirichletallocation model to two large-scale document collections. We demonstrate theadvantages of our algorithm over stochastic variational inference (SVI) bycomparing the two after a single pass through a known amount of data---a casewhere SVI may be applied---and in the streaming setting, where SVI does notapply.
arxiv-4500-190 | A survey on independence-based Markov networks learning | http://arxiv.org/pdf/1108.2283v2.pdf | author:Federico Schlüter category:cs.AI cs.LG published:2011-08-10 summary:This work reports the most relevant technical aspects in the problem oflearning the \emph{Markov network structure} from data. Such problem has becomeincreasingly important in machine learning, and many other application fieldsof machine learning. Markov networks, together with Bayesian networks, areprobabilistic graphical models, a widely used formalism for handlingprobability distributions in intelligent systems. Learning graphical modelsfrom data have been extensively applied for the case of Bayesian networks, butfor Markov networks learning it is not tractable in practice. However, thissituation is changing with time, given the exponential growth of computerscapacity, the plethora of available digital data, and the researching on newlearning technologies. This work stresses on a technology calledindependence-based learning, which allows the learning of the independencestructure of those networks from data in an efficient and sound manner,whenever the dataset is sufficiently large, and data is a representativesampling of the target distribution. In the analysis of such technology, thiswork surveys the current state-of-the-art algorithms for learning Markovnetworks structure, discussing its current limitations, and proposing a seriesof open problems where future works may produce some advances in the area interms of quality and efficiency. The paper concludes by opening a discussionabout how to develop a general formalism for improving the quality of thestructures learned, when data is scarce.
arxiv-4500-191 | Classification with Scattering Operators | http://arxiv.org/pdf/1011.3023v4.pdf | author:Joan Bruna, Stéphane Mallat category:cs.CV published:2010-11-12 summary:A scattering vector is a local descriptor including multiscale andmulti-direction co-occurrence information. It is computed with a cascade ofwavelet decompositions and complex modulus. This scattering representation islocally translation invariant and linearizes deformations. A supervisedclassification algorithm is computed with a PCA model selection on scatteringvectors. State of the art results are obtained for handwritten digitrecognition and texture classification.
arxiv-4500-192 | Gromov-Hausdorff stability of linkage-based hierarchical clustering methods | http://arxiv.org/pdf/1311.5068v1.pdf | author:A. Martínez-Pérez category:cs.LG published:2013-11-20 summary:A hierarchical clustering method is stable if small perturbations on the dataset produce small perturbations in the result. These perturbations are measuredusing the Gromov-Hausdorff metric. We study the problem of stability onlinkage-based hierarchical clustering methods. We obtain that, under some basicconditions, standard linkage-based methods are semi-stable. This means thatthey are stable if the input data is close enough to an ultrametric space. Weprove that, apart from exotic examples, introducing any unchaining condition inthe algorithm always produces unstable methods.
arxiv-4500-193 | Mapping cognitive ontologies to and from the brain | http://arxiv.org/pdf/1311.3859v2.pdf | author:Yannick Schwartz, Bertrand Thirion, Gaël Varoquaux category:stat.ML cs.LG q-bio.NC published:2013-11-15 summary:Imaging neuroscience links brain activation maps to behavior and cognitionvia correlational studies. Due to the nature of the individual experiments,based on eliciting neural response from a small number of stimuli, this link isincomplete, and unidirectional from the causal point of view. To come toconclusions on the function implied by the activation of brain regions, it isnecessary to combine a wide exploration of the various brain functions and someinversion of the statistical inference. Here we introduce a methodology foraccumulating knowledge towards a bidirectional link between observed brainactivity and the corresponding function. We rely on a large corpus of imagingstudies and a predictive engine. Technically, the challenges are to findcommonality between the studies without denaturing the richness of the corpus.The key elements that we contribute are labeling the tasks performed with acognitive ontology, and modeling the long tail of rare paradigms in the corpus.To our knowledge, our approach is the first demonstration of predicting thecognitive content of completely new brain images. To that end, we propose amethod that predicts the experimental paradigms across different studies.
arxiv-4500-194 | Optimal classification in sparse Gaussian graphic model | http://arxiv.org/pdf/1212.5332v2.pdf | author:Yingying Fan, Jiashun Jin, Zhigang Yao category:stat.ML math.ST stat.TH published:2012-12-21 summary:Consider a two-class classification problem where the number of features ismuch larger than the sample size. The features are masked by Gaussian noisewith mean zero and covariance matrix $\Sigma$, where the precision matrix$\Omega=\Sigma^{-1}$ is unknown but is presumably sparse. The useful features,also unknown, are sparse and each contributes weakly (i.e., rare and weak) tothe classification decision. By obtaining a reasonably good estimate of$\Omega$, we formulate the setting as a linear regression model. We propose atwo-stage classification method where we first select features by the method ofInnovated Thresholding (IT), and then use the retained features and Fisher'sLDA for classification. In this approach, a crucial problem is how to set thethreshold of IT. We approach this problem by adapting the recent innovation ofHigher Criticism Thresholding (HCT). We find that when useful features are rareand weak, the limiting behavior of HCT is essentially just as good as thelimiting behavior of ideal threshold, the threshold one would choose if theunderlying distribution of the signals is known (if only). Somewhatsurprisingly, when $\Omega$ is sufficiently sparse, its off-diagonalcoordinates usually do not have a major influence over the classificationdecision. Compared to recent work in the case where $\Omega$ is the identitymatrix [Proc. Natl. Acad. Sci. USA 105 (2008) 14790-14795; Philos. Trans. R.Soc. Lond. Ser. A Math. Phys. Eng. Sci. 367 (2009) 4449-4470], the currentsetting is much more general, which needs a new approach and much moresophisticated analysis. One key component of the analysis is the intimaterelationship between HCT and Fisher's separation. Another key component is thetight large-deviation bounds for empirical processes for data withunconventional correlation structures, where graph theory on vertex coloringplays an important role.
arxiv-4500-195 | Analyzing Evolutionary Optimization in Noisy Environments | http://arxiv.org/pdf/1311.4987v1.pdf | author:Chao Qian, Yang Yu, Zhi-Hua Zhou category:cs.AI cs.NE published:2013-11-20 summary:Many optimization tasks have to be handled in noisy environments, where wecannot obtain the exact evaluation of a solution but only a noisy one. Fornoisy optimization tasks, evolutionary algorithms (EAs), a kind of stochasticmetaheuristic search algorithm, have been widely and successfully applied.Previous work mainly focuses on empirical studying and designing EAs for noisyoptimization, while, the theoretical counterpart has been little investigated.In this paper, we investigate a largely ignored question, i.e., whether anoptimization problem will always become harder for EAs in a noisy environment.We prove that the answer is negative, with respect to the measurement of theexpected running time. The result implies that, for optimization tasks thathave already been quite hard to solve, the noise may not have a negativeeffect, and the easier a task the more negatively affected by the noise. On arepresentative problem where the noise has a strong negative effect, we examinetwo commonly employed mechanisms in EAs dealing with noise, the re-evaluationand the threshold selection strategies. The analysis discloses that the twostrategies, however, both are not effective, i.e., they do not make the EA morenoise tolerant. We then find that a small modification of the thresholdselection allows it to be proven as an effective strategy for dealing with thenoise in the problem.
arxiv-4500-196 | Neural Network Application on Foliage Plant Identification | http://arxiv.org/pdf/1311.5829v1.pdf | author:Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa category:cs.CV cs.NE published:2013-11-20 summary:Several researches in leaf identification did not include color informationas features. The main reason is caused by a fact that they used green coloredleaves as samples. However, for foliage plants, plants with colorful leaves,fancy patterns in their leaves, and interesting plants with unique shape, colorand also texture could not be neglected. For example, Epipremnum pinnatum'Aureum' and Epipremnum pinnatum 'Marble Queen' have similar patterns, sameshape, but different colors. Combination of shape, color, texture features, andother attribute contained on the leaf is very useful in leaf identification. Inthis research, Polar Fourier Transform and three kinds of geometric featureswere used to represent shape features, color moments that consist of mean,standard deviation, skewness were used to represent color features, texturefeatures are extracted from GLCMs, and vein features were added to improveperformance of the identification system. The identification system usesProbabilistic Neural Network (PNN) as a classifier. The result shows that thesystem gives average accuracy of 93.0833% for 60 kinds of foliage plants.
arxiv-4500-197 | Leaf Classification Using Shape, Color, and Texture Features | http://arxiv.org/pdf/1401.4447v1.pdf | author:Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa category:cs.CV cs.CY published:2013-11-20 summary:Several methods to identify plants have been proposed by several researchers.Commonly, the methods did not capture color information, because color was notrecognized as an important aspect to the identification. In this research,shape and vein, color, and texture features were incorporated to classify aleaf. In this case, a neural network called Probabilistic Neural network (PNN)was used as a classifier. The experimental result shows that the method forclassification gives average accuracy of 93.75% when it was tested on Flaviadataset, that contains 32 kinds of plant leaves. It means that the method givesbetter performance compared to the original work.
arxiv-4500-198 | Experiments of Distance Measurements in a Foliage Plant Retrieval System | http://arxiv.org/pdf/1401.3584v1.pdf | author:Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa category:cs.CV published:2013-11-20 summary:One of important components in an image retrieval system is selecting adistance measure to compute rank between two objects. In this paper, severaldistance measures were researched to implement a foliage plant retrievalsystem. Sixty kinds of foliage plants with various leaf color and shape wereused to test the performance of 7 different kinds of distance measures: cityblock distance, Euclidean distance, Canberra distance, Bray-Curtis distance, x2statistics, Jensen Shannon divergence and Kullback Leibler divergence. Theresults show that city block and Euclidean distance measures gave the bestperformance among the others.
arxiv-4500-199 | Hypothesis Testing for Automated Community Detection in Networks | http://arxiv.org/pdf/1311.2694v2.pdf | author:Peter J. Bickel, Purnamrita Sarkar category:stat.ML cs.LG cs.SI physics.soc-ph published:2013-11-12 summary:Community detection in networks is a key exploratory tool with applicationsin a diverse set of areas, ranging from finding communities in social andbiological networks to identifying link farms in the World Wide Web. Theproblem of finding communities or clusters in a network has received muchattention from statistics, physics and computer science. However, mostclustering algorithms assume knowledge of the number of clusters k. In thispaper we propose to automatically determine k in a graph generated from aStochastic Blockmodel. Our main contribution is twofold; first, wetheoretically establish the limiting distribution of the principal eigenvalueof the suitably centered and scaled adjacency matrix, and use that distributionfor our hypothesis test. Secondly, we use this test to design a recursivebipartitioning algorithm. Using quantifiable classification tasks on real worldnetworks with ground truth, we show that our algorithm outperforms existingprobabilistic models for learning overlapping clusters, and on unlabelednetworks, we show that we uncover nested community structure.
arxiv-4500-200 | Domain Adaptation of Majority Votes via Perturbed Variation-based Label Transfer | http://arxiv.org/pdf/1311.4833v1.pdf | author:Emilie Morvant category:stat.ML cs.LG published:2013-11-19 summary:We tackle the PAC-Bayesian Domain Adaptation (DA) problem. This arrives whenone desires to learn, from a source distribution, a good weighted majority vote(over a set of classifiers) on a different target distribution. In thiscontext, the disagreement between classifiers is known crucial to control. Innon-DA supervised setting, a theoretical bound - the C-bound - involves thisdisagreement and leads to a majority vote learning algorithm: MinCq. In thiswork, we extend MinCq to DA by taking advantage of an elegant divergencebetween distribution called the Perturbed Varation (PV). Firstly, justified bya new formulation of the C-bound, we provide to MinCq a target sample labeledthanks to a PV-based self-labeling focused on regions where the source andtarget marginal distributions are closer. Secondly, we propose an originalprocess for tuning the hyperparameters. Our framework shows very promisingresults on a toy problem.
arxiv-4500-201 | Universal Approximation Using Shuffled Linear Models | http://arxiv.org/pdf/1308.6498v2.pdf | author:Laurens Bliek category:math.DS cs.NE published:2013-08-29 summary:This paper proposes a specific type of Local Linear Model, the ShuffledLinear Model (SLM), that can be used as a universal approximator. Localoperating points are chosen randomly and linear models are used to approximatea function or system around these points. The model can also be interpreted asan extension to Extreme Learning Machines with Radial Basis Function nodes, oras a specific way of using Takagi-Sugeno fuzzy models. Using the availabletheory of Extreme Learning Machines, universal approximation of the SLM and anupper bound on the number of models are proved mathematically, and an efficientalgorithm is proposed.
arxiv-4500-202 | Image enhancement using fusion by wavelet transform and laplacian pyramid | http://arxiv.org/pdf/1401.6129v1.pdf | author:S. M. Mukane, Y. S. Ghodake, P. S. Khandagle category:cs.CV published:2013-11-19 summary:The idea of combining multiple image modalities to provide a single, enhancedimage is well established different fusion methods have been proposed inliterature. This paper is based on image fusion using laplacian pyramid andwavelet transform method. Images of same size are used for experimentation.Images used for the experimentation are standard images and averaging filter isused of equal weights in original images to burl. Performance of image fusiontechnique is measured by mean square error, normalized absolute error and peaksignal to noise ratio. From the performance analysis it has been observed thatMSE is decreased in case of both the methods where as PSNR increased, NAEdecreased in case of laplacian pyramid where as constant for wavelet transformmethod.
arxiv-4500-203 | Stochastic gradient descent on Riemannian manifolds | http://arxiv.org/pdf/1111.5280v4.pdf | author:Silvere Bonnabel category:math.OC cs.LG stat.ML published:2011-11-22 summary:Stochastic gradient descent is a simple approach to find the local minima ofa cost function whose evaluations are corrupted by noise. In this paper, wedevelop a procedure extending stochastic gradient descent algorithms to thecase where the function is defined on a Riemannian manifold. We prove that, asin the Euclidian case, the gradient descent algorithm converges to a criticalpoint of the cost function. The algorithm has numerous potential applications,and is illustrated here by four examples. In particular a novel gossipalgorithm on the set of covariance matrices is derived and tested numerically.
arxiv-4500-204 | Face Verification Using Kernel Principle Component Analysis | http://arxiv.org/pdf/1401.6108v1.pdf | author:V. Karthikeyan, Manjupriya, C. K. Chithra, M. Divya category:cs.CV published:2013-11-19 summary:In the beginning stage, face verification is done using easy method ofgeometric algorithm models, but the verification route has now developed into ascientific progress of complicated geometric representation and matchingprocess. In modern time the skill have enhanced face detection system into thevigorous focal point. Researchers currently undergoing strong research onfinding face recognition system for wider area information taken underhysterical elucidation dissimilarity. The proposed face recognition systemconsists of a narrative exposition indiscreet preprocessing method, a hybridFourier-based facial feature extraction and a score fusion scheme. We take inconventional the face detection in unlike cheer up circumstances and at unusualsetting. Image processing, Image detection, Feature removal and Face detectionare the methods used for Face Verification System . This paper focuses mainlyon the issue of toughness to lighting variations. The proposed system hasobtained an average of verification rate on Two-Dimensional images underdifferent lightening conditions.
arxiv-4500-205 | Face Verification System based on Integral Normalized Gradient Image(INGI) | http://arxiv.org/pdf/1401.6112v1.pdf | author:V. Karthikeyan, M. Divya, C. K. Chithra, K. Manju Priya category:cs.CV published:2013-11-19 summary:Character identification plays a vital role in the contemporary world ofImage processing. It can solve many composite problems and makes humans workeasier. An instance is Handwritten Character detection. Handwritten recognitionis not a novel expertise, but it has not gained community notice until Now. Theeventual aim of designing Handwritten Character recognition structure with anaccurateness rate of 100% is pretty illusionary. Tamil Handwritten Characterrecognition system uses the Neural Networks to distinguish them. Neural Networkand structural characteristics are used to instruct and recognize writtencharacters. After training and testing the exactness rate reached 99%. Thiscorrectness rate is extremely high. In this paper we are exploring imageprocessing through the Hilditch algorithm foundation and structuralcharacteristics of a character in the image. And we recognized some characterof the Tamil language, and we are trying to identify all the character of TamilIn our future works.
arxiv-4500-206 | Nonparametric Bayes dynamic modeling of relational data | http://arxiv.org/pdf/1311.4669v1.pdf | author:Daniele Durante, David B. Dunson category:stat.ML published:2013-11-19 summary:Symmetric binary matrices representing relations among entities are commonlycollected in many areas. Our focus is on dynamically evolving binary relationalmatrices, with interest being in inference on the relationship structure andprediction. We propose a nonparametric Bayesian dynamic model, which reducesdimensionality in characterizing the binary matrix through a lower-dimensionallatent space representation, with the latent coordinates evolving in continuoustime via Gaussian processes. By using a logistic mapping function from theprobability matrix space to the latent relational space, we obtain a flexibleand computational tractable formulation. Employing P\`olya-Gamma dataaugmentation, an efficient Gibbs sampler is developed for posteriorcomputation, with the dimension of the latent space automatically inferred. Weprovide some theoretical results on flexibility of the model, and illustrateperformance via simulation experiments. We also consider an application toco-movements in world financial markets.
arxiv-4500-207 | Analysis of Farthest Point Sampling for Approximating Geodesics in a Graph | http://arxiv.org/pdf/1311.4665v1.pdf | author:Pegah Kamousi, Sylvain Lazard, Anil Maheshwari, Stefanie Wuhrer category:cs.CG cs.CV cs.GR published:2013-11-19 summary:A standard way to approximate the distance between any two vertices $p$ and$q$ on a mesh is to compute, in the associated graph, a shortest path from $p$to $q$ that goes through one of $k$ sources, which are well-chosen vertices.Precomputing the distance between each of the $k$ sources to all vertices ofthe graph yields an efficient computation of approximate distances between anytwo vertices. One standard method for choosing $k$ sources, which has been usedextensively and successfully for isometry-invariant surface processing, is theso-called Farthest Point Sampling (FPS), which starts with a random vertex asthe first source, and iteratively selects the farthest vertex from the alreadyselected sources. In this paper, we analyze the stretch factor $\mathcal{F}_{FPS}$ ofapproximate geodesics computed using FPS, which is the maximum, over all pairsof distinct vertices, of their approximated distance over their geodesicdistance in the graph. We show that $\mathcal{F}_{FPS}$ can be bounded in termsof the minimal value $\mathcal{F}^*$ of the stretch factor obtained using anoptimal placement of $k$ sources as $\mathcal{F}_{FPS}\leq 2 r_e^2\mathcal{F}^*+ 2 r_e^2 + 8 r_e + 1$, where $r_e$ is the ratio of the lengths ofthe longest and the shortest edges of the graph. This provides some evidenceexplaining why farthest point sampling has been used successfully forisometry-invariant shape processing. Furthermore, we show that it isNP-complete to find $k$ sources that minimize the stretch factor.
arxiv-4500-208 | Hilditchs Algorithm Based Tamil Character Recognition | http://arxiv.org/pdf/1311.6740v1.pdf | author:V. Karthikeyan category:cs.CV published:2013-11-19 summary:Character identification plays a vital role in the contemporary world ofImage processing. It can solve many composite problems and makes humans workeasier. An instance is Handwritten Character detection. Handwritten recognitionis not a novel expertise, but it has not gained community notice until Now. Theeventual aim of designing Handwritten Character recognition structure with anaccurateness rate of 100% is pretty illusionary. Tamil Handwritten Characterrecognition system uses the Neural Networks to distinguish them. Neural Networkand structural characteristics are used to instruct and recognize writtencharacters. After training and testing the exactness rate reached 99%. Thiscorrectness rate is extremely high. In this paper we are exploring imageprocessing through the Hilditch algorithm foundation and structuralcharacteristics of a character in the image. And we recognized some characterof the Tamil language, and we are trying to identify all the character of TamilIn our future works.
arxiv-4500-209 | Near-Optimal Entrywise Sampling for Data Matrices | http://arxiv.org/pdf/1311.4643v1.pdf | author:Dimitris Achlioptas, Zohar Karnin, Edo Liberty category:cs.LG cs.IT cs.NA math.IT stat.ML published:2013-11-19 summary:We consider the problem of selecting non-zero entries of a matrix $A$ inorder to produce a sparse sketch of it, $B$, that minimizes $\A-B\_2$. Forlarge $m \times n$ matrices, such that $n \gg m$ (for example, representing $n$observations over $m$ attributes) we give sampling distributions that exhibitfour important properties. First, they have closed forms computable fromminimal information regarding $A$. Second, they allow sketching of matriceswhose non-zeros are presented to the algorithm in arbitrary order as a stream,with $O(1)$ computation per non-zero. Third, the resulting sketch matrices arenot only sparse, but their non-zero entries are highly compressible. Lastly,and most importantly, under mild assumptions, our distributions are provablycompetitive with the optimal offline distribution. Note that the probabilitiesin the optimal offline distribution may be complex functions of all the entriesin the matrix. Therefore, regardless of computational complexity, the optimaldistribution might be impossible to compute in the streaming model.
arxiv-4500-210 | Post-Proceedings of the First International Workshop on Learning and Nonmonotonic Reasoning | http://arxiv.org/pdf/1311.4639v1.pdf | author:Katsumi Inoue, Chiaki Sakama category:cs.AI cs.LG cs.LO published:2013-11-19 summary:Knowledge Representation and Reasoning and Machine Learning are two importantfields in AI. Nonmonotonic logic programming (NMLP) and Answer Set Programming(ASP) provide formal languages for representing and reasoning with commonsenseknowledge and realize declarative problem solving in AI. On the other side,Inductive Logic Programming (ILP) realizes Machine Learning in logicprogramming, which provides a formal background to inductive learning and thetechniques have been applied to the fields of relational learning and datamining. Generally speaking, NMLP and ASP realize nonmonotonic reasoning whilelack the ability of learning. By contrast, ILP realizes inductive learningwhile most techniques have been developed under the classical monotonic logic.With this background, some researchers attempt to combine techniques in thecontext of nonmonotonic ILP. Such combination will introduce a learningmechanism to programs and would exploit new applications on the NMLP side,while on the ILP side it will extend the representation language and enable usto use existing solvers. Cross-fertilization between learning and nonmonotonicreasoning can also occur in such as the use of answer set solvers for ILP,speed-up learning while running answer set solvers, learning action theories,learning transition rules in dynamical systems, abductive learning, learningbiological networks with inhibition, and applications involving default andnegation. This workshop is the first attempt to provide an open forum for theidentification of problems and discussion of possible collaborations amongresearchers with complementary expertise. The workshop was held on September15th of 2013 in Corunna, Spain. This post-proceedings contains five technicalpapers (out of six accepted papers) and the abstract of the invited talk by LucDe Raedt.
arxiv-4500-211 | Bandits with Switching Costs: T^{2/3} Regret | http://arxiv.org/pdf/1310.2997v2.pdf | author:Ofer Dekel, Jian Ding, Tomer Koren, Yuval Peres category:cs.LG math.PR published:2013-10-11 summary:We study the adversarial multi-armed bandit problem in a setting where theplayer incurs a unit cost each time he switches actions. We prove that theplayer's $T$-round minimax regret in this setting is$\widetilde{\Theta}(T^{2/3})$, thereby closing a fundamental gap in ourunderstanding of learning with bandit feedback. In the correspondingfull-information version of the problem, the minimax regret is known to grow ata much slower rate of $\Theta(\sqrt{T})$. The difference between these tworates provides the \emph{first} indication that learning with bandit feedbackcan be significantly harder than learning with full-information feedback(previous results only showed a different dependence on the number of actions,but not on $T$.) In addition to characterizing the inherent difficulty of the multi-armedbandit problem with switching costs, our results also resolve several otheropen problems in online learning. One direct implication is that learning withbandit feedback against bounded-memory adaptive adversaries has a minimaxregret of $\widetilde{\Theta}(T^{2/3})$. Another implication is that theminimax regret of online learning in adversarial Markov decision processes(MDPs) is $\widetilde{\Theta}(T^{2/3})$. The key to all of our results is a newrandomized construction of a multi-scale random walk, which is of independentinterest and likely to prove useful in additional settings.
arxiv-4500-212 | Guaranteed clustering and biclustering via semidefinite programming | http://arxiv.org/pdf/1202.3663v6.pdf | author:Brendan P. W. Ames category:math.OC cs.LG published:2012-02-16 summary:Identifying clusters of similar objects in data plays a significant role in awide range of applications. As a model problem for clustering, we consider thedensest k-disjoint-clique problem, whose goal is to identify the collection ofk disjoint cliques of a given weighted complete graph maximizing the sum of thedensities of the complete subgraphs induced by these cliques. In this paper, weestablish conditions ensuring exact recovery of the densest k cliques of agiven graph from the optimal solution of a particular semidefinite program. Inparticular, the semidefinite relaxation is exact for input graphs correspondingto data consisting of k large, distinct clusters and a smaller number ofoutliers. This approach also yields a semidefinite relaxation for thebiclustering problem with similar recovery guarantees. Given a set of objectsand a set of features exhibited by these objects, biclustering seeks tosimultaneously group the objects and features according to their expressionlevels. This problem may be posed as partitioning the nodes of a weightedbipartite complete graph such that the sum of the densities of the resultingbipartite complete subgraphs is maximized. As in our analysis of the densestk-disjoint-clique problem, we show that the correct partition of the objectsand features can be recovered from the optimal solution of a semidefiniteprogram in the case that the given data consists of several disjoint sets ofobjects exhibiting similar features. Empirical evidence from numericalexperiments supporting these theoretical guarantees is also provided.
arxiv-4500-213 | On Nonrigid Shape Similarity and Correspondence | http://arxiv.org/pdf/1311.5595v1.pdf | author:Alon Shtern, Ron Kimmel category:cs.CV cs.GR published:2013-11-18 summary:An important operation in geometry processing is finding the correspondencesbetween pairs of shapes. The Gromov-Hausdorff distance, a measure ofdissimilarity between metric spaces, has been found to be highly useful fornonrigid shape comparison. Here, we explore the applicability of related shapesimilarity measures to the problem of shape correspondence, adopting spectraltype distances. We propose to evaluate the spectral kernel distance, thespectral embedding distance and the novel spectral quasi-conformal distance,comparing the manifolds from different viewpoints. By matching the shapes inthe spectral domain, important attributes of surface structure are beingaligned. For the purpose of testing our ideas, we introduce a fully automaticframework for finding intrinsic correspondence between two shapes. The proposedmethod achieves state-of-the-art results on the Princeton isometric shapematching protocol applied, as usual, to the TOSCA and SCAPE benchmarks.
arxiv-4500-214 | Early Stage Influenza Detection from Twitter | http://arxiv.org/pdf/1309.7340v3.pdf | author:Jiwei Li, Claire Cardie category:cs.SI cs.CL published:2013-09-27 summary:Influenza is an acute respiratory illness that occurs virtually every yearand results in substantial disease, death and expense. Detection of Influenzain its earliest stage would facilitate timely action that could reduce thespread of the illness. Existing systems such as CDC and EISS which try tocollect diagnosis data, are almost entirely manual, resulting in about two-weekdelays for clinical data acquisition. Twitter, a popular microblogging service,provides us with a perfect source for early-stage flu detection due to itsreal- time nature. For example, when a flu breaks out, people that get the flumay post related tweets which enables the detection of the flu breakoutpromptly. In this paper, we investigate the real-time flu detection problem onTwitter data by proposing Flu Markov Network (Flu-MN): a spatio-temporalunsupervised Bayesian algorithm based on a 4 phase Markov Network, trying toidentify the flu breakout at the earliest stage. We test our model on realTwitter datasets from the United States along with baselines in multipleapplications, such as real-time flu breakout detection, future epidemic phaseprediction, or Influenza-like illness (ILI) physician visits. Experimentalresults show the robustness and effectiveness of our approach. We build up areal time flu reporting system based on the proposed approach, and we arehopeful that it would help government or health organizations in identifyingflu outbreaks and facilitating timely actions to decrease unnecessarymortality.
arxiv-4500-215 | From Maxout to Channel-Out: Encoding Information on Sparse Pathways | http://arxiv.org/pdf/1312.1909v1.pdf | author:Qi Wang, Joseph JaJa category:cs.NE cs.CV cs.LG stat.ML published:2013-11-18 summary:Motivated by an important insight from neural science, we propose a newframework for understanding the success of the recently proposed "maxout"networks. The framework is based on encoding information on sparse pathways andrecognizing the correct pathway at inference time. Elaborating further on thisinsight, we propose a novel deep network architecture, called "channel-out"network, which takes a much better advantage of sparse pathway encoding. Inchannel-out networks, pathways are not only formed a posteriori, but they arealso actively selected according to the inference outputs from the lowerlayers. From a mathematical perspective, channel-out networks can represent awider class of piece-wise continuous functions, thereby endowing the networkwith more expressive power than that of maxout networks. We test ourchannel-out networks on several well-known image classification benchmarks,setting new state-of-the-art performance on CIFAR-100 and STL-10, whichrepresent some of the "harder" image classification benchmarks.
arxiv-4500-216 | Ranking Algorithms by Performance | http://arxiv.org/pdf/1311.4319v1.pdf | author:Lars Kotthoff category:cs.AI cs.LG published:2013-11-18 summary:A common way of doing algorithm selection is to train a machine learningmodel and predict the best algorithm from a portfolio to solve a particularproblem. While this method has been highly successful, choosing only a singlealgorithm has inherent limitations -- if the choice was bad, no remedial actioncan be taken and parallelism cannot be exploited, to name but a few problems.In this paper, we investigate how to predict the ranking of the portfolioalgorithms on a particular problem. This information can be used to choose thesingle best algorithm, but also to allocate resources to the algorithmsaccording to their rank. We evaluate a range of approaches to predict theranking of a set of algorithms on a problem. We furthermore introduce aframework for categorizing ranking predictions that allows to judge theexpressiveness of the predictive output. Our experimental evaluationdemonstrates on a range of data sets from the literature that it is beneficialto consider the relationship between algorithms when predicting rankings. Wefurthermore show that relatively naive approaches deliver rankings of goodquality already.
arxiv-4500-217 | Guided Random Forest in the RRF Package | http://arxiv.org/pdf/1306.0237v3.pdf | author:Houtao Deng category:cs.LG published:2013-06-02 summary:Random Forest (RF) is a powerful supervised learner and has been popularlyused in many applications such as bioinformatics. In this work we propose the guided random forest (GRF) for feature selection.Similar to a feature selection method called guided regularized random forest(GRRF), GRF is built using the importance scores from an ordinary RF. However,the trees in GRRF are built sequentially, are highly correlated and do notallow for parallel computing, while the trees in GRF are built independentlyand can be implemented in parallel. Experiments on 10 high-dimensional genedata sets show that, with a fixed parameter value (without tuning theparameter), RF applied to features selected by GRF outperforms RF applied toall features on 9 data sets and 7 of them have significant differences at the0.05 level. Therefore, both accuracy and interpretability are significantlyimproved. GRF selects more features than GRRF, however, leads to betterclassification accuracy. Note in this work the guided random forest is guidedby the importance scores from an ordinary random forest, however, it can alsobe guided by other methods such as human insights (by specifying $\lambda_i$).GRF can be used in "RRF" v1.4 (and later versions), a package that alsoincludes the regularized random forest methods.
arxiv-4500-218 | Reflection methods for user-friendly submodular optimization | http://arxiv.org/pdf/1311.4296v1.pdf | author:Stefanie Jegelka, Francis Bach, Suvrit Sra category:cs.LG cs.NA cs.RO math.OC published:2013-11-18 summary:Recently, it has become evident that submodularity naturally captures widelyoccurring concepts in machine learning, signal processing and computer vision.Consequently, there is need for efficient optimization procedures forsubmodular functions, especially for minimization problems. While generalsubmodular minimization is challenging, we propose a new method that exploitsexisting decomposability of submodular functions. In contrast to previousapproaches, our method is neither approximate, nor impractical, nor does itneed any cumbersome parameter tuning. Moreover, it is easy to implement andparallelize. A key component of our method is a formulation of the discretesubmodular minimization problem as a continuous best approximation problem thatis solved through a sequence of reflections, and its solution can be easilythresholded to obtain an optimal discrete solution. This method solves both thecontinuous and discrete formulations of the problem, and therefore hasapplications in learning, inference, and reconstruction. In our experiments, weillustrate the benefits of our method on two image segmentation tasks.
arxiv-4500-219 | Contour polygonal approximation using shortest path in networks | http://arxiv.org/pdf/1311.4252v1.pdf | author:André Ricardo Backes, Dalcimar Casanova, Odemir Martinez Bruno category:cs.CV published:2013-11-18 summary:Contour polygonal approximation is a simplified representation of a contourby line segments, so that the main characteristics of the contour remain in asmall number of line segments. This paper presents a novel method for polygonalapproximation based on the Complex Networks theory. We convert each point ofthe contour into a vertex, so that we model a regular network. Then wetransform this network into a Small-World Complex Network by applying sometransformations over its edges. By analyzing of network properties, especiallythe geodesic path, we compute the polygonal approximation. The paper presentsthe main characteristics of the method, as well as its functionality. Weevaluate the proposed method using benchmark contours, and compare its resultswith other polygonal approximation methods.
arxiv-4500-220 | On the definition of a general learning system with user-defined operators | http://arxiv.org/pdf/1311.4235v1.pdf | author:Fernando Martínez-Plumed, Cèsar Ferri, José Hernández-Orallo, María-José Ramírez-Quintana category:cs.LG published:2013-11-18 summary:In this paper, we push forward the idea of machine learning systems whoseoperators can be modified and fine-tuned for each problem. This allows us topropose a learning paradigm where users can write (or adapt) their operators,according to the problem, data representation and the way the informationshould be navigated. To achieve this goal, data instances, backgroundknowledge, rules, programs and operators are all written in the same functionallanguage, Erlang. Since changing operators affect how the search space needs tobe explored, heuristics are learnt as a result of a decision process based onreinforcement learning where each action is defined as a choice of operator andrule. As a result, the architecture can be seen as a 'system for writingmachine learning systems' or to explore new operators where the policy reuse(as a kind of transfer learning) is allowed. States and actions are representedin a Q matrix which is actually a table, from which a supervised model islearnt. This makes it possible to have a more flexible mapping between old andnew problems, since we work with an abstraction of rules and actions. Weinclude some examples sharing reuse and the application of the system gErl toIQ problems. In order to evaluate gErl, we will test it against some structuredproblems: a selection of IQ test tasks and some experiments on some structuredprediction problems (list patterns).
arxiv-4500-221 | Towards Big Topic Modeling | http://arxiv.org/pdf/1311.4150v1.pdf | author:Jian-Feng Yan, Jia Zeng, Zhi-Qiang Liu, Yang Gao category:cs.LG cs.DC cs.IR stat.ML published:2013-11-17 summary:To solve the big topic modeling problem, we need to reduce both time andspace complexities of batch latent Dirichlet allocation (LDA) algorithms.Although parallel LDA algorithms on the multi-processor architecture have lowtime and space complexities, their communication costs among processors oftenscale linearly with the vocabulary size and the number of topics, leading to aserious scalability problem. To reduce the communication complexity amongprocessors for a better scalability, we propose a novel communication-efficientparallel topic modeling architecture based on power law, which consumes ordersof magnitude less communication time when the number of topics is large. Wecombine the proposed communication-efficient parallel architecture with theonline belief propagation (OBP) algorithm referred to as POBP for big topicmodeling tasks. Extensive empirical results confirm that POBP has the followingadvantages to solve the big topic modeling problem: 1) high accuracy, 2)communication-efficient, 3) fast speed, and 4) constant memory usage whencompared with recent state-of-the-art parallel LDA algorithms on themulti-processor architecture.
arxiv-4500-222 | Multilabel Classification through Random Graph Ensembles | http://arxiv.org/pdf/1310.8428v2.pdf | author:Hongyu Su, Juho Rousu category:cs.LG published:2013-10-31 summary:We present new methods for multilabel classification, relying on ensemblelearning on a collection of random output graphs imposed on the multilabel anda kernel-based structured output learner as the base classifier. For ensemblelearning, differences among the output graphs provide the required baseclassifier diversity and lead to improved performance in the increasing size ofthe ensemble. We study different methods of forming the ensemble prediction,including majority voting and two methods that perform inferences over thegraph structures before or after combining the base models into the ensemble.We compare the methods against the state-of-the-art machine learning approacheson a set of heterogeneous multilabel benchmark problems, including multilabelAdaBoost, convex multitask feature learning, as well as single target learningapproaches represented by Bagging and SVM. In our experiments, the random graphensembles are very competitive and robust, ranking first or second on most ofthe datasets. Overall, our results show that random graph ensembles are viablealternatives to flat multilabel and multitask learners.
arxiv-4500-223 | The Optimization of Running Queries in Relational Databases Using ANT-Colony Algorithm | http://arxiv.org/pdf/1311.4088v1.pdf | author:Adel Alinezhad Kolaei, Marzieh Ahmadzadeh category:cs.DB cs.NE published:2013-11-16 summary:The issue of optimizing queries is a cost-sensitive process and with respectto the number of associated tables in a query, its number of permutations growsexponentially. On one hand, in comparison with other operators in relationaldatabase, join operator is the most difficult and complicated one in terms ofoptimization for reducing its runtime. Accordingly, various algorithms have sofar been proposed to solve this problem. On the other hand, the success of anydatabase management system (DBMS) means exploiting the query model. In thecurrent paper, the heuristic ant algorithm has been proposed to solve thisproblem and improve the runtime of join operation. Experiments and observedresults reveal the efficiency of this algorithm compared to its similaralgorithms.
arxiv-4500-224 | A hybrid decision support system : application on healthcare | http://arxiv.org/pdf/1311.4086v1.pdf | author:Abdelhak Mansoul, Baghdad Atmani, Sofia Benbelkacem category:cs.AI cs.LG published:2013-11-16 summary:Many systems based on knowledge, especially expert systems for medicaldecision support have been developed. Only systems are based on productionrules, and cannot learn and evolve only by updating them. In addition, takinginto account several criteria induces an exorbitant number of rules to beinjected into the system. It becomes difficult to translate medical knowledgeor a support decision as a simple rule. Moreover, reasoning based on genericcases became classic and can even reduce the range of possible solutions. Toremedy that, we propose an approach based on using a multi-criteria decisionguided by a case-based reasoning (CBR) approach.
arxiv-4500-225 | A new bio-inspired method for remote sensing imagery classification | http://arxiv.org/pdf/1302.2606v2.pdf | author:Amghar Yasmina Teldja, Fizazi Hadria category:cs.NE cs.CV published:2013-02-11 summary:The problem of supervised classification of the satellite image is consideredto be the task of grouping pixels into a number of homogeneous regions in spaceintensity. This paper proposes a novel approach that combines a radial basicfunction clustering network with a growing neural gas include utility factorclassifier to yield improved solutions, obtained with previous networks. Thedouble objective technique is first used to the development of a method toperform the satellite images classification, and finally, the implementation toaddress the issue of the number of nodes in the hidden layer of the classicRadial Basis functions network. Results demonstrating the effectiveness of theproposed technique are provided for numeric remote sensing imagery. Moreover,the remotely sensed image of Oran city in Algeria has been classified using theproposed technique to establish its utility.
arxiv-4500-226 | A Comparative Study of Histogram Equalization Based Image Enhancement Techniques for Brightness Preservation and Contrast Enhancement | http://arxiv.org/pdf/1311.4033v1.pdf | author:Omprakash Patel, Yogendra P. S. Maravi, Sanjeev Sharma category:cs.CV published:2013-11-16 summary:Histogram Equalization is a contrast enhancement technique in the imageprocessing which uses the histogram of image. However histogram equalization isnot the best method for contrast enhancement because the mean brightness of theoutput image is significantly different from the input image. There are severalextensions of histogram equalization has been proposed to overcome thebrightness preservation challenge. Contrast enhancement using brightnesspreserving bi-histogram equalization (BBHE) and Dualistic sub image histogramequalization (DSIHE) which divides the image histogram into two parts based onthe input mean and median respectively then equalizes each sub histogramindependently. This paper provides review of different popular histogramequalization techniques and experimental study based on the absolute meanbrightness error (AMBE), peak signal to noise ratio (PSNR), Structuresimilarity index (SSI) and Entropy.
arxiv-4500-227 | Nonparametric Link Prediction in Large Scale Dynamic Networks | http://arxiv.org/pdf/1109.1077v3.pdf | author:Purnamrita Sarkar, Deepayan Chakrabarti, Michael Jordan category:stat.ML cs.SI physics.soc-ph published:2011-09-06 summary:We propose a nonparametric approach to link prediction in large-scale dynamicnetworks. Our model uses graph-based features of pairs of nodes as well asthose of their local neighborhoods to predict whether those nodes will belinked at each time step. The model allows for different types of evolution indifferent parts of the graph (e.g, growing or shrinking communities). We focuson large-scale graphs and present an implementation of our model that makes useof locality-sensitive hashing to allow it to be scaled to large problems.Experiments with simulated data as well as five real-world dynamic graphs showthat we outperform the state of the art, especially when sharp fluctuations ornonlinearities are present. We also establish theoretical properties of ourestimator, in particular consistency and weak convergence, the latter makinguse of an elaboration of Stein's method for dependency graphs.
arxiv-4500-228 | Inferring the Origin Locations of Tweets with Quantitative Confidence | http://arxiv.org/pdf/1305.3932v3.pdf | author:Reid Priedhorsky, Aron Culotta, Sara Y. Del Valle category:cs.SI cs.HC cs.LG published:2013-05-16 summary:Social Internet content plays an increasingly critical role in many domains,including public health, disaster management, and politics. However, itsutility is limited by missing geographic information; for example, fewer than1.6% of Twitter messages (tweets) contain a geotag. We propose a scalable,content-based approach to estimate the location of tweets using a novel yetsimple variant of gaussian mixture models. Further, because real-worldapplications depend on quantified uncertainty for such estimates, we proposenovel metrics of accuracy, precision, and calibration, and we evaluate ourapproach accordingly. Experiments on 13 million global, comprehensivelymulti-lingual tweets show that our approach yields reliable, well-calibratedresults competitive with previous computationally intensive methods. We alsoshow that a relatively small number of training data are required for goodestimates (roughly 30,000 tweets) and models are quite time-invariant(effective on tweets many weeks newer than the training set). Finally, we showthat toponyms and languages with small geographic footprint provide the mostuseful location signals.
arxiv-4500-229 | The STONE Transform: Multi-Resolution Image Enhancement and Real-Time Compressive Video | http://arxiv.org/pdf/1311.3405v2.pdf | author:Tom Goldstein, Lina Xu, Kevin F. Kelly, Richard Baraniuk category:cs.CV published:2013-11-14 summary:Compressed sensing enables the reconstruction of high-resolution signals fromunder-sampled data. While compressive methods simplify data acquisition, theyrequire the solution of difficult recovery problems to make use of theresulting measurements. This article presents a new sensing framework thatcombines the advantages of both conventional and compressive sensing. Using theproposed \stone transform, measurements can be reconstructed instantly atNyquist rates at any power-of-two resolution. The same data can then be"enhanced" to higher resolutions using compressive methods that leveragesparsity to "beat" the Nyquist limit. The availability of a fast directreconstruction enables compressive measurements to be processed on smallembedded devices. We demonstrate this by constructing a real-time compressivevideo camera.
arxiv-4500-230 | HEVAL: Yet Another Human Evaluation Metric | http://arxiv.org/pdf/1311.3961v1.pdf | author:Nisheeth Joshi, Iti Mathur, Hemant Darbari, Ajai Kumar category:cs.CL published:2013-11-15 summary:Machine translation evaluation is a very important activity in machinetranslation development. Automatic evaluation metrics proposed in literatureare inadequate as they require one or more human reference translations tocompare them with output produced by machine translation. This does not alwaysgive accurate results as a text can have several different translations. Humanevaluation metrics, on the other hand, lacks inter-annotator agreement andrepeatability. In this paper we have proposed a new human evaluation metricwhich addresses these issues. Moreover this metric also provides solid groundsfor making sound assumptions on the quality of the text produced by a machinetranslation.
arxiv-4500-231 | Describing Textures in the Wild | http://arxiv.org/pdf/1311.3618v2.pdf | author:Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, Andrea Vedaldi category:cs.CV published:2013-11-14 summary:Patterns and textures are defining characteristics of many natural objects: ashirt can be striped, the wings of a butterfly can be veined, and the skin ofan animal can be scaly. Aiming at supporting this analytical dimension in imageunderstanding, we address the challenging problem of describing textures withsemantic attributes. We identify a rich vocabulary of forty-seven texture termsand use them to describe a large dataset of patterns collected in the wild.Theresulting Describable Textures Dataset (DTD) is the basis to seek for the besttexture representation for recognizing describable texture attributes inimages. We port from object recognition to texture recognition the ImprovedFisher Vector (IFV) and show that, surprisingly, it outperforms specializedtexture descriptors not only on our problem, but also in established materialrecognition datasets. We also show that the describable attributes areexcellent texture descriptors, transferring between datasets and tasks; inparticular, combined with IFV, they significantly outperform thestate-of-the-art by more than 8 percent on both FMD and KTHTIPS-2b benchmarks.We also demonstrate that they produce intuitive descriptions of materials andInternet images.
arxiv-4500-232 | Mixing Energy Models in Genetic Algorithms for On-Lattice Protein Structure Prediction | http://arxiv.org/pdf/1311.3840v1.pdf | author:Mahmood A. Rashid, M. A. Hakim Newton, Md. Tamjidul Hoque, Abdul Sattar category:cs.CE cs.NE published:2013-11-15 summary:Protein structure prediction (PSP) is computationally a very challengingproblem. The challenge largely comes from the fact that the energy functionthat needs to be minimised in order to obtain the native structure of a givenprotein is not clearly known. A high resolution 20x20 energy model could bettercapture the behaviour of the actual energy function than a low resolutionenergy model such as hydrophobic polar. However, the fine grained details ofthe high resolution interaction energy matrix are often not very informativefor guiding the search. In contrast, a low resolution energy model couldeffectively bias the search towards certain promising directions. In thispaper, we develop a genetic algorithm that mainly uses a high resolution energymodel for protein structure evaluation but uses a low resolution HP energymodel in focussing the search towards exploring structures that havehydrophobic cores. We experimentally show that this mixing of energy modelsleads to significant lower energy structures compared to the state-of-the-artresults.
arxiv-4500-233 | Video Text Localization using Wavelet and Shearlet Transforms | http://arxiv.org/pdf/1307.4990v2.pdf | author:Purnendu Banerjee, B. B. Chaudhuri category:cs.CV published:2013-07-18 summary:Text in video is useful and important in indexing and retrieving the videodocuments efficiently and accurately. In this paper, we present a new method oftext detection using a combined dictionary consisting of wavelets and arecently introduced transform called shearlets. Wavelets provide optimallysparse expansion for point-like structures and shearlets provide optimallysparse expansions for curve-like structures. By combining these two features wehave computed a high frequency sub-band to brighten the text part. Then K-meansclustering is used for obtaining text pixels from the Standard Deviation (SD)of combined coefficient of wavelets and shearlets as well as the union ofwavelets and shearlets features. Text parts are obtained by groupingneighboring regions based on geometric properties of the classified outputframe of unsupervised K-means classification. The proposed method tested on astandard as well as newly collected database shows to be superior to someexisting methods.
arxiv-4500-234 | Periodicity Extraction using Superposition of Distance Matching Function and One-dimensional Haar Wavelet Transform | http://arxiv.org/pdf/1311.3808v1.pdf | author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV I.0; I.2.7 published:2013-11-15 summary:Periodicity of a texture is one of the important visual characteristics andis often used as a measure for textural discrimination at the structural level.Knowledge about periodicity of a texture is very essential in the field oftexture synthesis and texture compression and also in the design of frieze andwall papers. In this paper, we propose a method of periodicity extraction fromnoisy images based on superposition of distance matching function (DMF) andwavelet decomposition without de-noising the test images. Overall DMFs aresubjected to single-level Haar wavelet decomposition to obtain approximate anddetailed coefficients. Extracted coefficients help in determination ofperiodicities in row and column directions. We illustrate the usefulness andthe effectiveness of the proposed method in a texture synthesis application.
arxiv-4500-235 | Ensemble Relational Learning based on Selective Propositionalization | http://arxiv.org/pdf/1311.3735v1.pdf | author:Nicola Di Mauro, Floriana Esposito category:cs.LG cs.AI published:2013-11-15 summary:Dealing with structured data needs the use of expressive representationformalisms that, however, puts the problem to deal with the computationalcomplexity of the machine learning process. Furthermore, real world domainsrequire tools able to manage their typical uncertainty. Many statisticalrelational learning approaches try to deal with these problems by combining theconstruction of relevant relational features with a probabilistic tool. Whenthe combination is static (static propositionalization), the constructedfeatures are considered as boolean features and used offline as input to astatistical learner; while, when the combination is dynamic (dynamicpropositionalization), the feature construction and probabilistic tool arecombined into a single process. In this paper we propose a selectivepropositionalization method that search the optimal set of relational featuresto be used by a probabilistic learner in order to minimize a loss function. Thenew propositionalization approach has been combined with the random subspaceensemble method. Experiments on real-world datasets shows the validity of theproposed method.
arxiv-4500-236 | On Estimating Many Means, Selection Bias, and the Bootstrap | http://arxiv.org/pdf/1311.3709v1.pdf | author:Noah Simon, Richard Simon category:stat.ML stat.ME published:2013-11-15 summary:With recent advances in high throughput technology, researchers often findthemselves running a large number of hypothesis tests (thousands+) and esti-mating a large number of effect-sizes. Generally there is particular interestin those effects estimated to be most extreme. Unfortunately naive estimates ofthese effect-sizes (even after potentially accounting for multiplicity in atesting procedure) can be severely biased. In this manuscript we explore thisbias from a frequentist perspective: we give a formal definition, and show thatan oracle estimator using this bias dominates the naive maximum likelihoodestimate. We give a resampling estimator to approximate this oracle, and showthat it works well on simulated data. We also connect this to ideas inempirical Bayes.
arxiv-4500-237 | Evolutionary perspectives on collective decision making: Studying the implications of diversity and social network structure with agent-based simulations | http://arxiv.org/pdf/1311.3674v1.pdf | author:Hiroki Sayama, Shelley D. Dionne, Francis J. Yammarino category:cs.MA cs.NE cs.SI physics.soc-ph published:2013-11-14 summary:Collective, especially group-based, managerial decision making is crucial inorganizations. Using an evolutionary theory approach to collective decisionmaking, agent-based simulations were conducted to investigate how collectivedecision making would be affected by the agents' diversity in problemunderstanding and/or behavior in discussion, as well as by their social networkstructure. Simulation results indicated that groups with consistent problemunderstanding tended to produce higher utility values of ideas and displayedbetter decision convergence, but only if there was no group-level bias incollective problem understanding. Simulation results also indicated theimportance of balance between selection-oriented (i.e., exploitative) andvariation-oriented (i.e., explorative) behaviors in discussion to achievequality final decisions. Expanding the group size and introducing non-trivialsocial network structure generally improved the quality of ideas at the cost ofdecision convergence. Simulations with different social network topologiesrevealed that collective decision making on small-world networks with highlocal clustering tended to achieve highest decision quality more often than onrandom or scale-free networks. Implications of this evolutionary theory andsimulation approach for future managerial research on collective, group, andmulti-level decision making are discussed.
arxiv-4500-238 | Scalable Influence Estimation in Continuous-Time Diffusion Networks | http://arxiv.org/pdf/1311.3669v1.pdf | author:Nan Du, Le Song, Manuel Gomez Rodriguez, Hongyuan Zha category:cs.SI cs.LG H.2.8 published:2013-11-14 summary:If a piece of information is released from a media site, can it spread, in 1month, to a million web pages? This influence estimation problem is verychallenging since both the time-sensitive nature of the problem and the issueof scalability need to be addressed simultaneously. In this paper, we propose arandomized algorithm for influence estimation in continuous-time diffusionnetworks. Our algorithm can estimate the influence of every node in a networkwith V nodes and E edges to an accuracy of $\varepsilon$ using$n=O(1/\varepsilon^2)$ randomizations and up to logarithmic factorsO(nE+nV) computations. When used as a subroutine in a greedy influencemaximization algorithm, our proposed method is guaranteed to find a set ofnodes with an influence of at least (1-1/e)OPT-2$\varepsilon$, where OPT is theoptimal value. Experiments on both synthetic and real-world data show that theproposed method can easily scale up to networks of millions of nodes whilesignificantly improves over previous state-of-the-arts in terms of the accuracyof the estimated influence and the quality of the selected nodes in maximizingthe influence.
arxiv-4500-239 | Uniform random generation of large acyclic digraphs | http://arxiv.org/pdf/1202.6590v4.pdf | author:Jack Kuipers, Giusi Moffa category:stat.CO math.ST stat.ML stat.TH published:2012-02-29 summary:Directed acyclic graphs are the basic representation of the structureunderlying Bayesian networks, which represent multivariate probabilitydistributions. In many practical applications, such as the reverse engineeringof gene regulatory networks, not only the estimation of model parameters butthe reconstruction of the structure itself is of great interest. As well as forthe assessment of different structure learning algorithms in simulationstudies, a uniform sample from the space of directed acyclic graphs is requiredto evaluate the prevalence of certain structural features. Here we analyse howto sample acyclic digraphs uniformly at random through recursive enumeration,an approach previously thought too computationally involved. Based oncomplexity considerations, we discuss in particular how the enumerationdirectly provides an exact method, which avoids the convergence issues of thealternative Markov chain methods and is actually computationally much faster.The limiting behaviour of the distribution of acyclic digraphs then allows usto sample arbitrarily large graphs. Building on the ideas of recursiveenumeration based sampling we also introduce a novel hybrid Markov chain withmuch faster convergence than current alternatives while still being easy toadapt to various restrictions. Finally we discuss how to include suchrestrictions in the combinatorial enumeration and the new hybrid Markov chainmethod for efficient uniform sampling of the corresponding graphs.
arxiv-4500-240 | Flexible sampling of discrete data correlations without the marginal distributions | http://arxiv.org/pdf/1306.2685v3.pdf | author:Alfredo Kalaitzis, Ricardo Silva category:stat.ML cs.LG stat.CO published:2013-06-12 summary:Learning the joint dependence of discrete variables is a fundamental problemin machine learning, with many applications including prediction, clusteringand dimensionality reduction. More recently, the framework of copula modelinghas gained popularity due to its modular parametrization of jointdistributions. Among other properties, copulas provide a recipe for combiningflexible models for univariate marginal distributions with parametric familiessuitable for potentially high dimensional dependence structures. Moreradically, the extended rank likelihood approach of Hoff (2007) bypasseslearning marginal models completely when such information is ancillary to thelearning task at hand as in, e.g., standard dimensionality reduction problemsor copula parameter estimation. The main idea is to represent data by theirobservable rank statistics, ignoring any other information from the marginals.Inference is typically done in a Bayesian framework with Gaussian copulas, andit is complicated by the fact this implies sampling within a space where thenumber of constraints increases quadratically with the number of data points.The result is slow mixing when using off-the-shelf Gibbs sampling. We presentan efficient algorithm based on recent advances on constrained HamiltonianMarkov chain Monte Carlo that is simple to implement and does not requirepaying for a quadratic cost in sample size.
arxiv-4500-241 | Stratified Graphical Models - Context-Specific Independence in Graphical Models | http://arxiv.org/pdf/1309.6415v2.pdf | author:Henrik Nyman, Johan Pensar, Timo Koski, Jukka Corander category:stat.ML published:2013-09-25 summary:Theory of graphical models has matured over more than three decades toprovide the backbone for several classes of models that are used in a myriad ofapplications such as genetic mapping of diseases, credit risk evaluation,reliability and computer security, etc. Despite of their generic applicabilityand wide adoptance, the constraints imposed by undirected graphical models andBayesian networks have also been recognized to be unnecessarily stringent undercertain circumstances. This observation has led to the proposal of severalgeneralizations that aim at more relaxed constraints by which the models canimpose local or context-specific dependence structures. Here we consider anadditional class of such models, termed as stratified graphical models. Wedevelop a method for Bayesian learning of these models by deriving ananalytical expression for the marginal likelihood of data under a specificsubclass of decomposable stratified models. A non-reversible Markov chain MonteCarlo approach is further used to identify models that are highly supported bythe posterior distribution over the model space. Our method is illustrated andcompared with ordinary graphical models through application to several real andsynthetic datasets.
arxiv-4500-242 | High-dimensional learning of linear causal networks via inverse covariance estimation | http://arxiv.org/pdf/1311.3492v1.pdf | author:Po-Ling Loh, Peter Bühlmann category:stat.ML math.ST stat.TH 62F12 published:2013-11-14 summary:We establish a new framework for statistical estimation of directed acyclicgraphs (DAGs) when data are generated from a linear, possibly non-Gaussianstructural equation model. Our framework consists of two parts: (1) inferringthe moralized graph from the support of the inverse covariance matrix; and (2)selecting the best-scoring graph amongst DAGs that are consistent with themoralized graph. We show that when the error variances are known or estimatedto close enough precision, the true DAG is the unique minimizer of the scorecomputed using the reweighted squared l_2-loss. Our population-level resultshave implications for the identifiability of linear SEMs when the errorcovariances are specified up to a constant multiple. On the statistical side,we establish rigorous conditions for high-dimensional consistency of ourtwo-part algorithm, defined in terms of a "gap" between the true DAG and thenext best candidate. Finally, we demonstrate that dynamic programming may beused to select the optimal DAG in linear time when the treewidth of themoralized graph is bounded.
arxiv-4500-243 | Big Data and Cross-Document Coreference Resolution: Current State and Future Opportunities | http://arxiv.org/pdf/1311.3987v1.pdf | author:Seyed-Mehdi-Reza Beheshti, Srikumar Venugopal, Seung Hwan Ryu, Boualem Benatallah, Wei Wang category:cs.CL cs.DC cs.IR published:2013-11-14 summary:Information Extraction (IE) is the task of automatically extractingstructured information from unstructured/semi-structured machine-readabledocuments. Among various IE tasks, extracting actionable intelligence fromever-increasing amount of data depends critically upon Cross-DocumentCoreference Resolution (CDCR) - the task of identifying entity mentions acrossmultiple documents that refer to the same underlying entity. Recently, documentdatasets of the order of peta-/tera-bytes has raised many challenges forperforming effective CDCR such as scaling to large numbers of mentions andlimited representational power. The problem of analysing such datasets iscalled "big data". The aim of this paper is to provide readers with anunderstanding of the central concepts, subtasks, and the currentstate-of-the-art in CDCR process. We provide assessment of existingtools/techniques for CDCR subtasks and highlight big data challenges in each ofthem to help readers identify important and outstanding issues for furtherinvestigation. Finally, we provide concluding remarks and discuss possibledirections for future work.
arxiv-4500-244 | Anytime Belief Propagation Using Sparse Domains | http://arxiv.org/pdf/1311.3368v1.pdf | author:Sameer Singh, Sebastian Riedel, Andrew McCallum category:stat.ML cs.AI cs.LG published:2013-11-14 summary:Belief Propagation has been widely used for marginal inference, however it isslow on problems with large-domain variables and high-order factors. Previouswork provides useful approximations to facilitate inference on such models, butlacks important anytime properties such as: 1) providing accurate andconsistent marginals when stopped early, 2) improving the approximation whenrun longer, and 3) converging to the fixed point of BP. To this end, we proposea message passing algorithm that works on sparse (partially instantiated)domains, and converges to consistent marginals using dynamic messagescheduling. The algorithm grows the sparse domains incrementally, selecting thenext value to add using prioritization schemes based on the gradients of themarginal inference objective. Our experiments demonstrate local anytimeconsistency and fast convergence, providing significant speedups over BP toobtain low-error marginals: up to 25 times on grid models, and up to 6 times ona real-world natural language processing task.
arxiv-4500-245 | A Study of Actor and Action Semantic Retention in Video Supervoxel Segmentation | http://arxiv.org/pdf/1311.3318v1.pdf | author:Chenliang Xu, Richard F. Doell, Stephen José Hanson, Catherine Hanson, Jason J. Corso category:cs.CV published:2013-11-13 summary:Existing methods in the semantic computer vision community seem unable todeal with the explosion and richness of modern, open-source and social videocontent. Although sophisticated methods such as object detection orbag-of-words models have been well studied, they typically operate on low levelfeatures and ultimately suffer from either scalability issues or a lack ofsemantic meaning. On the other hand, video supervoxel segmentation has recentlybeen established and applied to large scale data processing, which potentiallyserves as an intermediate representation to high level video semanticextraction. The supervoxels are rich decompositions of the video content: theycapture object shape and motion well. However, it is not yet known if thesupervoxel segmentation retains the semantics of the underlying video content.In this paper, we conduct a systematic study of how well the actor and actionsemantics are retained in video supervoxel segmentation. Our study has humanobservers watching supervoxel segmentation videos and trying to discriminateboth actor (human or animal) and action (one of eight everyday actions). Wegather and analyze a large set of 640 human perceptions over 96 videos in 3different supervoxel scales. Furthermore, we conduct machine recognitionexperiments on a feature defined on supervoxel segmentation, called supervoxelshape context, which is inspired by the higher order processes in humanperception. Our ultimate findings suggest that a significant amount ofsemantics have been well retained in the video supervoxel segmentation and canbe used for further video analysis.
arxiv-4500-246 | On a non-local spectrogram for denoising one-dimensional signals | http://arxiv.org/pdf/1311.3269v1.pdf | author:Gonzalo Galiano, Julián Velasco category:cs.CV published:2013-11-13 summary:In previous works, we investigated the use of local filters based on partialdifferential equations (PDE) to denoise one-dimensional signals through theimage processing of time-frequency representations, such as the spectrogram. Inthis image denoising algorithms, the particularity of the image was hardlytaken into account. We turn, in this paper, to study the performance ofnon-local filters, like Neighborhood or Yaroslavsky filters, in the sameproblem. We show that, for certain iterative schemes involving the Neighborhoodfilter, the computational time is drastically reduced with respect toYaroslavsky or nonlinear PDE based filters, while the outputs of the filteringprocesses are similar. This is heuristically justified by the connectionbetween the (fast) Neighborhood filter applied to a spectrogram and thecorresponding Nonlocal Means filter (accurate) applied to the Wigner-Villedistribution of the signal. This correspondence holds only for time-frequencyrepresentations of one-dimensional signals, not to usual images, and in thissense the particularity of the image is exploited. We compare though a seriesof experiments on synthetic and biomedical signals the performance of local andnon-local filters.
arxiv-4500-247 | Stochastic inference with deterministic spiking neurons | http://arxiv.org/pdf/1311.3211v1.pdf | author:Mihai A. Petrovici, Johannes Bill, Ilja Bytschok, Johannes Schemmel, Karlheinz Meier category:q-bio.NC cs.NE physics.bio-ph 92-08 C.1.3; I.5.1 published:2013-11-13 summary:The seemingly stochastic transient dynamics of neocortical circuits observedin vivo have been hypothesized to represent a signature of ongoing stochasticinference. In vitro neurons, on the other hand, exhibit a highly deterministicresponse to various types of stimulation. We show that an ensemble ofdeterministic leaky integrate-and-fire neurons embedded in a spiking noisyenvironment can attain the correct firing statistics in order to sample from awell-defined target distribution. We provide an analytical derivation of theactivation function on the single cell level; for recurrent networks, weexamine convergence towards stationarity in computer simulations anddemonstrate sample-based Bayesian inference in a mixed graphical model. Thisestablishes a rigorous link between deterministic neuron models and functionalstochastic dynamics on the network level.
arxiv-4500-248 | Optimal Rates of Convergence for Latent Generalized Correlation Matrix Estimation in Transelliptical Distribution | http://arxiv.org/pdf/1305.6916v3.pdf | author:Fang Han, Han Liu category:stat.ML published:2013-05-29 summary:Correlation matrix plays a key role in many multivariate methods (e.g.,graphical model estimation and factor analysis). The current state-of-the-artin estimating large correlation matrices focuses on the use of Pearson's samplecorrelation matrix. Although Pearson's sample correlation matrix enjoys variousgood properties under Gaussian models, its not an effective estimator whenfacing heavy-tail distributions with possible outliers. As a robustalternative, \cite{han2012transelliptical} advocated the use of a transformedversion of the Kendall's tau sample correlation matrix in estimating highdimensional latent generalized correlation matrix under the transellipticaldistribution family (or elliptical copula). The transelliptical family assumesthat after unspecified marginal monotone transformations, the data follow anelliptical distribution. In this paper, we study the theoretical properties ofthe Kendall's tau sample correlation matrix and its transformed versionproposed in \cite{han2012transelliptical} for estimating the populationKendall's tau correlation matrix and the latent Pearson's correlation matrixunder both spectral and restricted spectral norms. With regard to the spectralnorm, we highlight the role of "effective rank" in quantifying the rate ofconvergence. With regard to the restricted spectral norm, we for the first timepresent a "sign subgaussian condition" which is sufficient to guarantee thatthe rank-based correlation matrix estimator attains the optimal rate ofconvergence. In both cases, we do not need any moment condition.
arxiv-4500-249 | Architecture of an Ontology-Based Domain-Specific Natural Language Question Answering System | http://arxiv.org/pdf/1311.3175v1.pdf | author:Athira P. M., Sreeja M., P. C. Reghu Raj category:cs.CL cs.IR published:2013-11-13 summary:Question answering (QA) system aims at retrieving precise information from alarge collection of documents against a query. This paper describes thearchitecture of a Natural Language Question Answering (NLQA) system for aspecific domain based on the ontological information, a step towards semanticweb question answering. The proposed architecture defines four basic modulessuitable for enhancing current QA capabilities with the ability of processingcomplex questions. The first module was the question processing, which analysesand classifies the question and also reformulates the user query. The secondmodule allows the process of retrieving the relevant documents. The next moduleprocesses the retrieved documents, and the last module performs the extractionand generation of a response. Natural language processing techniques are usedfor processing the question and documents and also for answer extraction.Ontology and domain knowledge are used for reformulating queries andidentifying the relations. The aim of the system is to generate short andspecific answer to the question that is asked in the natural language in aspecific domain. We have achieved 94 % accuracy of natural language questionanswering in our implementation.
arxiv-4500-250 | smart application for AMS using Face Recognition | http://arxiv.org/pdf/1401.6130v1.pdf | author:MuthuKalyani. K, VeeraMuthu. A category:cs.CY cs.CV published:2013-11-13 summary:Attendance Management System (AMS) can be made into smarter way by using facerecognition technique, where we use a CCTV camera to be fixed at the entrypoint of a classroom, which automatically captures the image of the person andchecks the observed image with the face database using android enhanced smartphone. It is typically used for two purposes. Firstly, marking attendance forstudent by comparing the face images produced recently and secondly,recognition of human who are strange to the environment i.e. an unauthorizedperson For verification of image, a newly emerging trend 3D Face Recognition isused which claims to provide more accuracy in matching the image databases andhas an ability to recognize a subject at different view angles.
arxiv-4500-251 | An Efficient Method for Recognizing the Low Quality Fingerprint Verification by Means of Cross Correlation | http://arxiv.org/pdf/1311.3076v1.pdf | author:V. Karthikeyan, V. J. Vijayalakshmi category:cs.CV published:2013-11-13 summary:In this paper, we propose an efficient method to provide personalidentification using fingerprint to get better accuracy even in noisycondition. The fingerprint matching based on the number of correspondingminutia pairings, has been in use for a long time, which is not very efficientfor recognizing the low quality fingerprints. To overcome this problem,correlation technique is used. The correlation-based fingerprint verificationsystem is capable of dealing with low quality images from which no minutiae canbe extracted reliably and with fingerprints that suffer from non-uniform shapedistortions, also in case of damaged and partial images. Orientation FieldMethodology (OFM) has been used as a preprocessing module, and it converts theimages into a field pattern based on the direction of the ridges, loops andbifurcations in the image of a fingerprint. The input image is then CrossCorrelated (CC) with all the images in the cluster and the highest correlatedimage is taken as the output. The result gives a good recognition rate, as theproposed scheme uses Cross Correlation of Field Orientation (CCFO = OFM + CC)for fingerprint identification.
arxiv-4500-252 | UW SPF: The University of Washington Semantic Parsing Framework | http://arxiv.org/pdf/1311.3011v1.pdf | author:Yoav Artzi, Luke Zettlemoyer category:cs.CL published:2013-11-13 summary:The University of Washington Semantic Parsing Framework (SPF) is a learningand inference framework for mapping natural language to formal representationof its meaning.
arxiv-4500-253 | Informed Source Separation: A Bayesian Tutorial | http://arxiv.org/pdf/1311.3001v1.pdf | author:Kevin H. Knuth category:stat.ML cs.LG published:2013-11-13 summary:Source separation problems are ubiquitous in the physical sciences; anysituation where signals are superimposed calls for source separation toestimate the original signals. In this tutorial I will discuss the Bayesianapproach to the source separation problem. This approach has a specificadvantage in that it requires the designer to explicitly describe the signalmodel in addition to any other information or assumptions that go into theproblem description. This leads naturally to the idea of informed sourceseparation, where the algorithm design incorporates relevant information aboutthe specific problem. This approach promises to enable researchers to designtheir own high-quality algorithms that are specifically tailored to the problemat hand.
arxiv-4500-254 | Learning Input and Recurrent Weight Matrices in Echo State Networks | http://arxiv.org/pdf/1311.2987v1.pdf | author:Hamid Palangi, Li Deng, Rabab K Ward category:cs.LG published:2013-11-13 summary:Echo State Networks (ESNs) are a special type of the temporally deep networkmodel, the Recurrent Neural Network (RNN), where the recurrent matrix iscarefully designed and both the recurrent and input matrices are fixed. An ESNuses the linearity of the activation function of the output units to simplifythe learning of the output matrix. In this paper, we devise a special techniquethat take advantage of this linearity in the output units of an ESN, to learnthe input and recurrent matrices. This has not been done in earlier ESNs due totheir well known difficulty in learning those matrices. Compared to thetechnique of BackPropagation Through Time (BPTT) in learning general RNNs, ourproposed method exploits linearity of activation function in the output unitsto formulate the relationships amongst the various matrices in an RNN. Theserelationships results in the gradient of the cost function having an analyticalform and being more accurate. This would enable us to compute the gradientsinstead of obtaining them by recursion as in BPTT. Experimental results onphone state classification show that learning one or both the input andrecurrent matrices in an ESN yields superior results compared to traditionalESNs that do not learn these matrices, especially when longer time steps areused.
arxiv-4500-255 | Authorship Attribution Using Word Network Features | http://arxiv.org/pdf/1311.2978v1.pdf | author:Shibamouli Lahiri, Rada Mihalcea category:cs.CL published:2013-11-12 summary:In this paper, we explore a set of novel features for authorship attributionof documents. These features are derived from a word network representation ofnatural language text. As has been noted in previous studies, natural languagetends to show complex network structure at word level, with low degrees ofseparation and scale-free (power law) degree distribution. There has also beenwork on authorship attribution that incorporates ideas from complex networks.The goal of our paper is to explore properties of these complex networks thatare suitable as features for machine-learning-based authorship attribution ofdocuments. We performed experiments on three different datasets, and obtainedpromising results.
arxiv-4500-256 | Approximate Inference in Continuous Determinantal Point Processes | http://arxiv.org/pdf/1311.2971v1.pdf | author:Raja Hafiz Affandi, Emily B. Fox, Ben Taskar category:stat.ML cs.LG stat.ME published:2013-11-12 summary:Determinantal point processes (DPPs) are random point processes well-suitedfor modeling repulsion. In machine learning, the focus of DPP-based models hasbeen on diverse subset selection from a discrete and finite base set. Thisdiscrete setting admits an efficient sampling algorithm based on theeigendecomposition of the defining kernel matrix. Recently, there has beengrowing interest in using DPPs defined on continuous spaces. While thediscrete-DPP sampler extends formally to the continuous case, computationally,the steps required are not tractable in general. In this paper, we present twoefficient DPP sampling schemes that apply to a wide range of kernel functions:one based on low rank approximations via Nystrom and random Fourier featuretechniques and another based on Gibbs sampling. We demonstrate the utility ofcontinuous DPPs in repulsive mixture modeling and synthesizing human posesspanning activity spaces.
arxiv-4500-257 | Structural Learning for Template-free Protein Folding | http://arxiv.org/pdf/1311.1422v2.pdf | author:Feng Zhao category:cs.LG cs.CE q-bio.QM published:2013-11-06 summary:The thesis is aimed to solve the template-free protein folding problem bytackling two important components: efficient sampling in vast conformationspace, and design of knowledge-based potentials with high accuracy. We haveproposed the first-order and second-order CRF-Sampler to sample structures fromthe continuous local dihedral angles space by modeling the lower and higherorder conditional dependency between neighboring dihedral angles given theprimary sequence information. A framework combining the Conditional RandomFields and the energy function is introduced to guide the local conformationsampling using long range constraints with the energy function. The relationship between the sequence profile and the local dihedral angledistribution is nonlinear. Hence we proposed the CNF-Folder to model thiscomplex relationship by applying a novel machine learning model ConditionalNeural Fields which utilizes the structural graphical model with the neuralnetwork. CRF-Samplers and CNF-Folder perform very well in CASP8 and CASP9. Further, a novel pairwise distance statistical potential (EPAD) is designedto capture the dependency of the energy profile on the positions of theinteracting amino acids as well as the types of those amino acids, opposing thecommon assumption that this energy profile depends only on the types of aminoacids. EPAD has also been successfully applied in the CASP 10 Free Modelingexperiment with CNF-Folder, especially outstanding on some uncommon structuredtargets.
arxiv-4500-258 | Moments and Root-Mean-Square Error of the Bayesian MMSE Estimator of Classification Error in the Gaussian Model | http://arxiv.org/pdf/1310.1519v2.pdf | author:Amin Zollanvari, Edward R. Dougherty category:stat.ML published:2013-10-05 summary:The most important aspect of any classifier is its error rate, because thisquantifies its predictive capacity. Thus, the accuracy of error estimation iscritical. Error estimation is problematic in small-sample classifier designbecause the error must be estimated using the same data from which theclassifier has been designed. Use of prior knowledge, in the form of a priordistribution on an uncertainty class of feature-label distributions to whichthe true, but unknown, feature-distribution belongs, can facilitate accurateerror estimation (in the mean-square sense) in circumstances where accuratecompletely model-free error estimation is impossible. This paper providesanalytic asymptotically exact finite-sample approximations for variousperformance metrics of the resulting Bayesian Minimum Mean-Square-Error (MMSE)error estimator in the case of linear discriminant analysis (LDA) in themultivariate Gaussian model. These performance metrics include the first,second, and cross moments of the Bayesian MMSE error estimator with the trueerror of LDA, and therefore, the Root-Mean-Square (RMS) error of the estimator.We lay down the theoretical groundwork for Kolmogorov double-asymptotics in aBayesian setting, which enables us to derive asymptotic expressions of thedesired performance metrics. From these we produce analytic finite-sampleapproximations and demonstrate their accuracy via numerical examples. Variousexamples illustrate the behavior of these approximations and their use indetermining the necessary sample size to achieve a desired RMS. TheSupplementary Material contains derivations for some equations and addedfigures.
arxiv-4500-259 | Multiple Closed-Form Local Metric Learning for K-Nearest Neighbor Classifier | http://arxiv.org/pdf/1311.3157v1.pdf | author:Jianbo Ye category:cs.LG published:2013-11-12 summary:Many researches have been devoted to learn a Mahalanobis distance metric,which can effectively improve the performance of kNN classification. Mostapproaches are iterative and computational expensive and linear rigidity stillcritically limits metric learning algorithm to perform better. We proposed acomputational economical framework to learn multiple metrics in closed-form.
arxiv-4500-260 | Aggregation of Affine Estimators | http://arxiv.org/pdf/1311.2799v1.pdf | author:Dong Dai, Philippe Rigollet, Lucy Xia, Tong Zhang category:math.ST cs.LG stat.TH 62G08 published:2013-11-12 summary:We consider the problem of aggregating a general collection of affineestimators for fixed design regression. Relevant examples include some commonlyused statistical estimators such as least squares, ridge and robust leastsquares estimators. Dalalyan and Salmon (2012) have established that, for thisproblem, exponentially weighted (EW) model selection aggregation leads to sharporacle inequalities in expectation, but similar bounds in deviation were notpreviously known. While results indicate that the same aggregation scheme maynot satisfy sharp oracle inequalities with high probability, we prove that aweaker notion of oracle inequality for EW that holds with high probability.Moreover, using a generalization of the newly introduced $Q$-aggregation schemewe also prove sharp oracle inequalities that hold with high probability.Finally, we apply our results to universal aggregation and show that ourproposed estimator leads simultaneously to all the best known bounds foraggregation, including $\ell_q$-aggregation, $q \in (0,1)$, with highprobability.
arxiv-4500-261 | When Does More Regularization Imply Fewer Degrees of Freedom? Sufficient Conditions and Counter Examples from Lasso and Ridge Regression | http://arxiv.org/pdf/1311.2791v1.pdf | author:Shachar Kaufman, Saharon Rosset category:math.ST stat.ML stat.TH published:2013-11-12 summary:Regularization aims to improve prediction performance of a given statisticalmodeling approach by moving to a second approach which achieves worse trainingerror but is expected to have fewer degrees of freedom, i.e., better agreementbetween training and prediction error. We show here, however, that thisexpected behavior does not hold in general. In fact, counter examples are giventhat show regularization can increase the degrees of freedom in simplesituations, including lasso and ridge regression, which are the most commonregularization approaches in use. In such situations, the regularizationincreases both training error and degrees of freedom, and is thus inherentlywithout merit. On the other hand, two important regularization scenarios aredescribed where the expected reduction in degrees of freedom is indeedguaranteed: (a) all symmetric linear smoothers, and (b) linear regressionversus convex constrained linear regression (as in the constrained variant ofridge regression and lasso).
arxiv-4500-262 | Equivalence of distance-based and RKHS-based statistics in hypothesis testing | http://arxiv.org/pdf/1207.6076v3.pdf | author:Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, Kenji Fukumizu category:stat.ME cs.LG math.ST stat.ML stat.TH published:2012-07-25 summary:We provide a unifying framework linking two classes of statistics used intwo-sample and independence testing: on the one hand, the energy distances anddistance covariances from the statistics literature; on the other, maximum meandiscrepancies (MMD), that is, distances between embeddings of distributions toreproducing kernel Hilbert spaces (RKHS), as established in machine learning.In the case where the energy distance is computed with a semimetric of negativetype, a positive definite kernel, termed distance kernel, may be defined suchthat the MMD corresponds exactly to the energy distance. Conversely, for anypositive definite kernel, we can interpret the MMD as energy distance withrespect to some negative-type semimetric. This equivalence readily extends todistance covariance using kernels on the product space. We determine the classof probability distributions for which the test statistics are consistentagainst all alternatives. Finally, we investigate the performance of the familyof distance kernels in two-sample and independence tests: we show in particularthat the energy distance most commonly employed in statistics is just onemember of a parametric family of kernels, and that other choices from thisfamily can yield more powerful tests.
arxiv-4500-263 | Deep neural networks for single channel source separation | http://arxiv.org/pdf/1311.2746v1.pdf | author:Emad M. Grais, Mehmet Umut Sen, Hakan Erdogan category:cs.NE cs.LG published:2013-11-12 summary:In this paper, a novel approach for single channel source separation (SCSS)using a deep neural network (DNN) architecture is introduced. Unlike previousstudies in which DNN and other classifiers were used for classifyingtime-frequency bins to obtain hard masks for each source, we use the DNN toclassify estimated source spectra to check for their validity duringseparation. In the training stage, the training data for the source signals areused to train a DNN. In the separation stage, the trained DNN is utilized toaid in estimation of each source in the mixed signal. Single channel sourceseparation problem is formulated as an energy minimization problem where eachsource spectra estimate is encouraged to fit the trained DNN model and themixed signal spectrum is encouraged to be written as a weighted sum of theestimated source spectra. The proposed approach works regardless of the energyscale differences between the source signals in the training and separationstages. Nonnegative matrix factorization (NMF) is used to initialize the DNNestimate for each source. The experimental results show that using DNNinitialized by NMF for source separation improves the quality of the separatedsignal compared with using NMF for source separation.
arxiv-4500-264 | Multivariate Bernoulli distribution | http://arxiv.org/pdf/1206.1874v2.pdf | author:Bin Dai, Shilin Ding, Grace Wahba category:stat.AP math.ST stat.ML stat.TH published:2012-06-08 summary:In this paper, we consider the multivariate Bernoulli distribution as a modelto estimate the structure of graphs with binary nodes. This distribution isdiscussed in the framework of the exponential family, and its statisticalproperties regarding independence of the nodes are demonstrated. Importantlythe model can estimate not only the main effects and pairwise interactionsamong the nodes but also is capable of modeling higher order interactions,allowing for the existence of complex clique effects. We compare themultivariate Bernoulli model with existing graphical inference models - theIsing model and the multivariate Gaussian model, where only the pairwiseinteractions are considered. On the other hand, the multivariate Bernoullidistribution has an interesting property in that independence anduncorrelatedness of the component random variables are equivalent. Both themarginal and conditional distributions of a subset of variables in themultivariate Bernoulli distribution still follow the multivariate Bernoullidistribution. Furthermore, the multivariate Bernoulli logistic model isdeveloped under generalized linear model theory by utilizing the canonical linkfunction in order to include covariate information on the nodes, edges andcliques. We also consider variable selection techniques such as LASSO in thelogistic model to impose sparsity structure on the graph. Finally, we discussextending the smoothing spline ANOVA approach to the multivariate Bernoullilogistic model to enable estimation of non-linear effects of the predictorvariables.
arxiv-4500-265 | Verifiable Source Code Documentation in Controlled Natural Language | http://arxiv.org/pdf/1311.2702v1.pdf | author:Tobias Kuhn, Alexandre Bergel category:cs.SE cs.AI cs.CL cs.HC cs.LO H.5.2; D.2.7 published:2013-11-12 summary:Writing documentation about software internals is rarely considered arewarding activity. It is highly time-consuming and the resulting documentationis fragile when the software is continuously evolving in a multi-developersetting. Unfortunately, traditional programming environments poorly support thewriting and maintenance of documentation. Consequences are severe as the lackof documentation on software structure negatively impacts the overall qualityof the software product. We show that using a controlled natural language witha reasoner and a query engine is a viable technique for verifying theconsistency and accuracy of documentation and source code. Using ACE, astate-of-the-art controlled natural language, we present positive results onthe comprehensibility and the general feasibility of creating and verifyingdocumentation. As a case study, we used automatic documentation verification toidentify and fix severe flaws in the architecture of a non-trivial piece ofsoftware. Moreover, a user experiment shows that our language is faster andeasier to learn and understand than other formal languages for softwaredocumentation.
arxiv-4500-266 | Sampling Based Approaches to Handle Imbalances in Network Traffic Dataset for Machine Learning Techniques | http://arxiv.org/pdf/1311.2677v1.pdf | author:Raman Singh, Harish Kumar, R. K. Singla category:cs.NI cs.CR cs.LG published:2013-11-12 summary:Network traffic data is huge, varying and imbalanced because various classesare not equally distributed. Machine learning (ML) algorithms for trafficanalysis uses the samples from this data to recommend the actions to be takenby the network administrators as well as training. Due to imbalances indataset, it is difficult to train machine learning algorithms for trafficanalysis and these may give biased or false results leading to seriousdegradation in performance of these algorithms. Various techniques can beapplied during sampling to minimize the effect of imbalanced instances. In thispaper various sampling techniques have been analysed in order to compare thedecrease in variation in imbalances of network traffic datasets sampled forthese algorithms. Various parameters like missing classes in samples,probability of sampling of the different instances have been considered forcomparison.
arxiv-4500-267 | Performing edge detection by difference of Gaussians using q-Gaussian kernels | http://arxiv.org/pdf/1311.2561v2.pdf | author:Lucas Assirati, Núbia R. da Silva, Lilian Berton, Alneu de A. Lopes, Odemir M. Bruno category:cs.CV published:2013-11-11 summary:In image processing, edge detection is a valuable tool to perform theextraction of features from an image. This detection reduces the amount ofinformation to be processed, since the redundant information (considered lessrelevant) can be unconsidered. The technique of edge detection consists ofdetermining the points of a digital image whose intensity changes sharply. Thischanges are due to the discontinuities of the orientation on a surface forexample. A well known method of edge detection is the Difference of Gaussians(DoG). The method consists of subtracting two Gaussians, where a kernel has astandard deviation smaller than the previous one. The convolution between thesubtraction of kernels and the input image results in the edge detection ofthis image. This paper introduces a method of extracting edges using DoG withkernels based on the q-Gaussian probability distribution, derived from theq-statistic proposed by Constantino Tsallis. To demonstrate the method'spotential, we compare the introduced method with the traditional DoG usingGaussians kernels. The results showed that the proposed method can extractedges with more accurate details.
arxiv-4500-268 | A comparative analysis of methods for estimating axon diameter using DWI | http://arxiv.org/pdf/1308.5269v2.pdf | author:Hamed Yousefi Mesri category:cs.NE published:2013-08-24 summary:The importance of studying the brain microstructure is described and theexisting and state of the art non-invasive methods for the investigation of thebrain microstructure using Diffusion Weighted Magnetic Resonance Imaging (DWI)is studied. In the next step, Cramer-Rao Lower Bound (CRLB) analysis isdescribed and utilised for assessment of the minimum estimation error anduncertainty level of different Diffusion Weighted Magnetic Resonance (DWMR)signal decay models. The analyses are performed considering the best scenariothrough which, we assume that the models are the appropriate representation ofthe measured phenomena. This includes the study of the sensitivity of theestimations to the measurement and model parameters. It is demonstrated thatnone of the existing models can achieve a reasonable minimum uncertainty levelunder typical measurement setup. At the end, the practical obstacles forachieving higher performance in clinical and experimental environments arestudied and their effects on feasibility of the methods are discussed.
arxiv-4500-269 | Volumetric Reconstruction Applied to Perceptual Studies of Size and Weight | http://arxiv.org/pdf/1311.2642v1.pdf | author:J. Balzer, M. Peters, S. Soatto category:cs.CV published:2013-11-11 summary:We explore the application of volumetric reconstruction from structured-lightsensors in cognitive neuroscience, specifically in the quantification of thesize-weight illusion, whereby humans tend to systematically perceive smallerobjects as heavier. We investigate the performance of two commercialstructured-light scanning systems in comparison to one we developedspecifically for this application. Our method has two main distinct features:First, it only samples a sparse series of viewpoints, unlike other systems suchas the Kinect Fusion. Second, instead of building a distance field for thepurpose of points-to-surface conversion directly, we pursue a first-orderapproach: the distance function is recovered from its gradient by a screenedPoisson reconstruction, which is very resilient to noise and yet preserveshigh-frequency signal components. Our experiments show that the quality ofmetric reconstruction from structured light sensors is subject to systematicbiases, and highlights the factors that influence it. Our main performanceindex rates estimates of volume (a proxy of size), for which we review awell-known formula applicable to incomplete meshes. Our code and data will bemade publicly available upon completion of the anonymous review process.
arxiv-4500-270 | Determining Leishmania Infection Levels by Automatic Analysis of Microscopy Images | http://arxiv.org/pdf/1311.2621v1.pdf | author:P. A Nogueira category:cs.CV published:2013-11-11 summary:Analysis of microscopy images is one important tool in many fields ofbiomedical research, as it allows the quantification of a multitude ofparameters at the cellular level. However, manual counting of these images isboth tiring and unreliable and ultimately very time-consuming for biomedicalresearchers. Not only does this slow down the overall research process, it alsointroduces counting errors due to a lack of objectivity and consistencyinherent to the researchers own human nature. This thesis addresses this issue by automatically determining infectionindexes of macrophages parasite by Leishmania in microscopy images usingcomputer vision and pattern recognition methodologies. Initially images aresubmitted to a pre-processing stage that consists in a normalization ofillumination conditions. Three algorithms are then applied in parallel to eachimage. Algorithm A intends to detect macrophage nuclei and consists ofsegmentation via adaptive multi-threshold, and classification of resultingregions using a set of collected features. Algorithm B intends to detectparasites and is similar to Algorithm A but the adaptive multi-threshold isparameterized with a different constraints vector. Algorithm C intends todetect the macrophages and parasites cytoplasm and consists of a cut-offversion of the previous two algorithms, where the classification step isskipped. Regions with multiple nuclei or parasites are processed by a votingsystem that employs both a Support Vector Machine and a set of region featuresfor determining the number of objects present in each region. The previous voteis then taken into account as the number of mixtures to be used in a GaussianMixture Model to decluster the said region. Finally each parasite is assignedto, at most, a single macrophage using minimum Euclidean distance to a cellnucleus, thus quantifying Leishmania infection levels.
arxiv-4500-271 | Stitched Panoramas from Toy Airborne Video Cameras | http://arxiv.org/pdf/1311.6500v1.pdf | author:Camille Goudeseune category:cs.CV published:2013-11-11 summary:Effective panoramic photographs are taken from vantage points that are high.High vantage points have recently become easier to reach as the cost ofquadrotor helicopters has dropped to nearly disposable levels. Although camerascarried by such aircraft weigh only a few grams, their low-quality video can beconverted into panoramas of high quality and high resolution. Also, the smallsize of these aircraft vastly reduces the risks inherent to flight.
arxiv-4500-272 | Motility at the origin of life: Its characterization and a model | http://arxiv.org/pdf/1311.2531v1.pdf | author:Tom Froese, Nathaniel Virgo, Takashi Ikegami category:cs.AI cs.NE nlin.AO nlin.PS q-bio.PE 35K57 published:2013-11-11 summary:Due to recent advances in synthetic biology and artificial life, the originof life is currently a hot topic of research. We review the literature andargue that the two traditionally competing "replicator-first" and"metabolism-first" approaches are merging into one integrated theory ofindividuation and evolution. We contribute to the maturation of this moreinclusive approach by highlighting some problematic assumptions that still leadto an impoverished conception of the phenomenon of life. In particular, weargue that the new consensus has so far failed to consider the relevance ofintermediate timescales. We propose that an adequate theory of life mustaccount for the fact that all living beings are situated in at least fourdistinct timescales, which are typically associated with metabolism, motility,development, and evolution. On this view, self-movement, adaptive behavior andmorphological changes could have already been present at the origin of life. Inorder to illustrate this possibility we analyze a minimal model of life-likephenomena, namely of precarious, individuated, dissipative structures that canbe found in simple reaction-diffusion systems. Based on our analysis we suggestthat processes in intermediate timescales could have already been operative inprebiotic systems. They may have facilitated and constrained changes occurringin the faster- and slower-paced timescales of chemical self-individuation andevolution by natural selection, respectively.
arxiv-4500-273 | Two-View Matching with View Synthesis Revisited | http://arxiv.org/pdf/1306.3855v2.pdf | author:Dmytro Mishkin, Michal Perdoch, Jiri Matas category:cs.CV published:2013-06-17 summary:Wide-baseline matching focussing on problems with extreme viewpoint change isconsidered. We introduce the use of view synthesis with affine-covariantdetectors to solve such problems and show that matching with the Hessian-Affineor MSER detectors outperforms the state-of-the-art ASIFT. To minimise the loss of speed caused by view synthesis, we propose theMatching On Demand with view Synthesis algorithm (MODS) that uses progressivelymore synthesized images and more (time-consuming) detectors until reliableestimation of geometry is possible. We show experimentally that the MODSalgorithm solves problems beyond the state-of-the-art and yet is comparable inspeed to standard wide-baseline matchers on simpler problems. Minor contributions include an improved method for tentative correspondenceselection, applicable both with and without view synthesis and a view synthesissetup greatly improving MSER robustness to blur and scale change that increaseits running time by 10% only.
arxiv-4500-274 | Predictable Feature Analysis | http://arxiv.org/pdf/1311.2503v1.pdf | author:Stefan Richthofer, Laurenz Wiskott category:cs.LG stat.ML published:2013-11-11 summary:Every organism in an environment, whether biological, robotic or virtual,must be able to predict certain aspects of its environment in order to surviveor perform whatever task is intended. It needs a model that is capable ofestimating the consequences of possible actions, so that planning, control, anddecision-making become feasible. For scientific purposes, such models areusually created in a problem specific manner using differential equations andother techniques from control- and system-theory. In contrast to that, we aimfor an unsupervised approach that builds up the desired model in aself-organized fashion. Inspired by Slow Feature Analysis (SFA), our approachis to extract sub-signals from the input, that behave as predictable aspossible. These "predictable features" are highly relevant for modeling,because predictability is a desired property of the neededconsequence-estimating model by definition. In our approach, we measurepredictability with respect to a certain prediction model. We focus here on thesolution of the arising optimization problem and present a tractable algorithmbased on algebraic methods which we call Predictable Feature Analysis (PFA). Weprove that the algorithm finds the globally optimal signal, if this signal canbe predicted with low error. To deal with cases where the optimal signal has asignificant prediction error, we provide a robust, heuristically motivatedvariant of the algorithm and verify it empirically. Additionally, we giveformal criteria a prediction-model must meet to be suitable for measuringpredictability in the PFA setting and also provide a suitable default-modelalong with a formal proof that it meets these criteria.
arxiv-4500-275 | Notes on Elementary Spectral Graph Theory. Applications to Graph Clustering Using Normalized Cuts | http://arxiv.org/pdf/1311.2492v1.pdf | author:Jean Gallier category:cs.CV published:2013-11-11 summary:These are notes on the method of normalized graph cuts and its applicationsto graph clustering. I provide a fairly thorough treatment of this deeplyoriginal method due to Shi and Malik, including complete proofs. I include thenecessary background on graphs and graph Laplacians. I then explain in detailhow the eigenvectors of the graph Laplacian can be used to draw a graph. Thisis an attractive application of graph Laplacians. The main thrust of this paperis the method of normalized cuts. I give a detailed account for K = 2 clusters,and also for K > 2 clusters, based on the work of Yu and Shi. Three points thatdo not appear to have been clearly articulated before are elaborated: 1. The solutions of the main optimization problem should be viewed as tuplesin the K-fold cartesian product of projective space RP^{N-1}. 2. When K > 2, the solutions of the relaxed problem should be viewed aselements of the Grassmannian G(K,N). 3. Two possible Riemannian distances are available to compare the closenessof solutions: (a) The distance on (RP^{N-1})^K. (b) The distance on theGrassmannian. I also clarify what should be the necessary and sufficient conditions for amatrix to represent a partition of the vertices of a graph to be clustered.
arxiv-4500-276 | Global Sensitivity Analysis with Dependence Measures | http://arxiv.org/pdf/1311.2483v1.pdf | author:Sébastien Da Veiga category:math.ST cs.LG stat.ML stat.TH published:2013-11-11 summary:Global sensitivity analysis with variance-based measures suffers from severaltheoretical and practical limitations, since they focus only on the variance ofthe output and handle multivariate variables in a limited way. In this paper,we introduce a new class of sensitivity indices based on dependence measureswhich overcomes these insufficiencies. Our approach originates from the idea tocompare the output distribution with its conditional counterpart when one ofthe input variables is fixed. We establish that this comparison yieldspreviously proposed indices when it is performed with Csiszar f-divergences, aswell as sensitivity indices which are well-known dependence measures betweenrandom variables. This leads us to investigate completely new sensitivityindices based on recent state-of-the-art dependence measures, such as distancecorrelation and the Hilbert-Schmidt independence criterion. We also emphasizethe potential of feature selection techniques relying on such dependencemeasures as alternatives to screening in high dimension.
arxiv-4500-277 | Exploiting correlation and budget constraints in Bayesian multi-armed bandit optimization | http://arxiv.org/pdf/1303.6746v4.pdf | author:Matthew W. Hoffman, Bobak Shahriari, Nando de Freitas category:stat.ML cs.LG published:2013-03-27 summary:We address the problem of finding the maximizer of a nonlinear smoothfunction, that can only be evaluated point-wise, subject to constraints on thenumber of permitted function evaluations. This problem is also known asfixed-budget best arm identification in the multi-armed bandit literature. Weintroduce a Bayesian approach for this problem and show that it empiricallyoutperforms both the existing frequentist counterpart and other Bayesianoptimization methods. The Bayesian approach places emphasis on detailedmodelling, including the modelling of correlations among the arms. As a result,it can perform well in situations where the number of arms is much larger thanthe number of allowed function evaluation, whereas the frequentist counterpartis inapplicable. This feature enables us to develop and deploy practicalapplications, such as automatic machine learning toolboxes. The paper presentscomprehensive comparisons of the proposed approach, Thompson sampling,classical Bayesian optimization techniques, more recent Bayesian banditapproaches, and state-of-the-art best arm identification methods. This is thefirst comparison of many of these methods in the literature and allows us toexamine the relative merits of their different features.
arxiv-4500-278 | An Empirical Evaluation of Sequence-Tagging Trainers | http://arxiv.org/pdf/1311.2378v1.pdf | author:P. Balamurugan, Shirish Shevade, S. Sundararajan, S. S Keerthi category:cs.LG published:2013-11-11 summary:The task of assigning label sequences to a set of observed sequences iscommon in computational linguistics. Several models for sequence labeling havebeen proposed over the last few years. Here, we focus on discriminative modelsfor sequence labeling. Many batch and online (updating model parameters aftervisiting each example) learning algorithms have been proposed in theliterature. On large datasets, online algorithms are preferred as batchlearning methods are slow. These online algorithms were designed to solveeither a primal or a dual problem. However, there has been no systematiccomparison of these algorithms in terms of their speed, generalizationperformance (accuracy/likelihood) and their ability to achieve steady stategeneralization performance fast. With this aim, we compare different algorithmsand make recommendations, useful for a practitioner. We conclude that theselection of an algorithm for sequence labeling depends on the evaluationcriterion used and its implementation simplicity.
arxiv-4500-279 | Efficient Optimization for Sparse Gaussian Process Regression | http://arxiv.org/pdf/1310.6007v3.pdf | author:Yanshuai Cao, Marcus A. Brubaker, David J. Fleet, Aaron Hertzmann category:cs.LG published:2013-10-22 summary:We propose an efficient optimization algorithm for selecting a subset oftraining data to induce sparsity for Gaussian process regression. The algorithmestimates an inducing set and the hyperparameters using a single objective,either the marginal likelihood or a variational free energy. The space and timecomplexity are linear in training set size, and the algorithm can be applied tolarge regression problems on discrete or continuous domains. Empiricalevaluation shows state-of-art performance in discrete cases and competitiveresults in the continuous case.
arxiv-4500-280 | Active Contour Models for Manifold Valued Image Segmentation | http://arxiv.org/pdf/1306.6269v2.pdf | author:Sumukh Bansal, Aditya Tatu category:cs.CV published:2013-06-26 summary:Image segmentation is the process of partitioning a image into differentregions or groups based on some characteristics like color, texture, motion orshape etc. Active contours is a popular variational method for objectsegmentation in images, in which the user initializes a contour which evolvesin order to optimize an objective function designed such that the desiredobject boundary is the optimal solution. Recently, imaging modalities thatproduce Manifold valued images have come up, for example, DT-MRI images, vectorfields. The traditional active contour model does not work on such images. Inthis paper, we generalize the active contour model to work on Manifold valuedimages. As expected, our algorithm detects regions with similar Manifold valuesin the image. Our algorithm also produces expected results on usual gray-scaleimages, since these are nothing but trivial examples of Manifold valued images.As another application of our general active contour model, we perform texturesegmentation on gray-scale images by first creating an appropriate Manifoldvalued image. We demonstrate segmentation results for manifold valued imagesand texture images.
arxiv-4500-281 | Tight Lower Bound on the Probability of a Binomial Exceeding its Expectation | http://arxiv.org/pdf/1306.1433v3.pdf | author:Spencer Greenberg, Mehryar Mohri category:cs.LG stat.ML published:2013-06-06 summary:We give the proof of a tight lower bound on the probability that a binomialrandom variable exceeds its expected value. The inequality plays an importantrole in a variety of contexts, including the analysis of relative deviationbounds in learning theory and generalization bounds for unbounded lossfunctions.
arxiv-4500-282 | Generalized Denoising Auto-Encoders as Generative Models | http://arxiv.org/pdf/1305.6663v4.pdf | author:Yoshua Bengio, Li Yao, Guillaume Alain, Pascal Vincent category:cs.LG published:2013-05-29 summary:Recent work has shown how denoising and contractive autoencoders implicitlycapture the structure of the data-generating density, in the case where thecorruption noise is Gaussian, the reconstruction error is the squared error,and the data is continuous-valued. This has led to various proposals forsampling from this implicitly learned density function, using Langevin andMetropolis-Hastings MCMC. However, it remained unclear how to connect thetraining procedure of regularized auto-encoders to the implicit estimation ofthe underlying data-generating distribution when the data are discrete, orusing other forms of corruption process and reconstruction errors. Anotherissue is the mathematical justification which is only valid in the limit ofsmall corruption noise. We propose here a different attack on the problem,which deals with all these issues: arbitrary (but noisy enough) corruption,arbitrary reconstruction loss (seen as a log-likelihood), handling bothdiscrete and continuous-valued variables, and removing the bias due tonon-infinitesimal corruption noise (or non-infinitesimal contractive penalty).
arxiv-4500-283 | A Quantitative Evaluation Framework for Missing Value Imputation Algorithms | http://arxiv.org/pdf/1311.2276v1.pdf | author:Vinod Nair, Rahul Kidambi, Sundararajan Sellamanickam, S. Sathiya Keerthi, Johannes Gehrke, Vijay Narayanan category:cs.LG published:2013-11-10 summary:We consider the problem of quantitatively evaluating missing value imputationalgorithms. Given a dataset with missing values and a choice of severalimputation algorithms to fill them in, there is currently no principled way torank the algorithms using a quantitative metric. We develop a framework basedon treating imputation evaluation as a problem of comparing two distributionsand show how it can be used to compute quantitative metrics. We present anefficient procedure for applying this framework to practical datasets,demonstrate several metrics derived from the existing literature on comparingdistributions, and propose a new metric called Neighborhood-based DissimilarityScore which is fast to compute and provides similar results. Results are shownon several datasets, metrics, and imputations algorithms.
arxiv-4500-284 | More data speeds up training time in learning halfspaces over sparse vectors | http://arxiv.org/pdf/1311.2271v1.pdf | author:Amit Daniely, Nati Linial, Shai Shalev Shwartz category:cs.LG published:2013-11-10 summary:The increased availability of data in recent years has led several authors toask whether it is possible to use data as a {\em computational} resource. Thatis, if more data is available, beyond the sample complexity limit, is itpossible to use the extra examples to speed up the computation time required toperform the learning task? We give the first positive answer to this question for a {\em naturalsupervised learning problem} --- we consider agnostic PAC learning ofhalfspaces over $3$-sparse vectors in $\{-1,1,0\}^n$. This class isinefficiently learnable using $O\left(n/\epsilon^2\right)$ examples. Our maincontribution is a novel, non-cryptographic, methodology for establishingcomputational-statistical gaps, which allows us to show that, under a widelybelieved assumption that refuting random $\mathrm{3CNF}$ formulas is hard, itis impossible to efficiently learn this class using only$O\left(n/\epsilon^2\right)$ examples. We further show that under strongerhardness assumptions, even $O\left(n^{1.499}/\epsilon^2\right)$ examples do notsuffice. On the other hand, we show a new algorithm that learns this classefficiently using $\tilde{\Omega}\left(n^2/\epsilon^2\right)$ examples. Thisformally establishes the tradeoff between sample and computational complexityfor a natural supervised learning problem.
arxiv-4500-285 | Semantic Sort: A Supervised Approach to Personalized Semantic Relatedness | http://arxiv.org/pdf/1311.2252v1.pdf | author:Ran El-Yaniv, David Yanay category:cs.CL cs.LG published:2013-11-10 summary:We propose and study a novel supervised approach to learning statisticalsemantic relatedness models from subjectively annotated training examples. Theproposed semantic model consists of parameterized co-occurrence statisticsassociated with textual units of a large background knowledge corpus. Wepresent an efficient algorithm for learning such semantic models from atraining sample of relatedness preferences. Our method is corpus independentand can essentially rely on any sufficiently large (unstructured) collection ofcoherent texts. Moreover, the approach facilitates the fitting of semanticmodels for specific users or groups of users. We present the results ofextensive range of experiments from small to large scale, indicating that theproposed method is effective and competitive with the state-of-the-art.
arxiv-4500-286 | Learning Gaussian Graphical Models with Observed or Latent FVSs | http://arxiv.org/pdf/1311.2241v1.pdf | author:Ying Liu, Alan S. Willsky category:cs.LG stat.ML published:2013-11-10 summary:Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widelyused in many applications, and the trade-off between the modeling capacity andthe efficiency of learning and inference has been an important researchproblem. In this paper, we study the family of GGMs with small feedback vertexsets (FVSs), where an FVS is a set of nodes whose removal breaks all thecycles. Exact inference such as computing the marginal distributions and thepartition function has complexity $O(k^{2}n)$ using message-passing algorithms,where k is the size of the FVS, and n is the total number of nodes. We proposeefficient structure learning algorithms for two cases: 1) All nodes areobserved, which is useful in modeling social or flight networks where the FVSnodes often correspond to a small number of high-degree nodes, or hubs, whilethe rest of the networks is modeled by a tree. Regardless of the maximumdegree, without knowing the full graph structure, we can exactly compute themaximum likelihood estimate in $O(kn^2+n^2\log n)$ if the FVS is known or inpolynomial time if the FVS is unknown but has bounded size. 2) The FVS nodesare latent variables, where structure learning is equivalent to decomposing ainverse covariance matrix (exactly or approximately) into the sum of atree-structured matrix and a low-rank matrix. By incorporating efficientinference into the learning steps, we can obtain a learning algorithm usingalternating low-rank correction with complexity $O(kn^{2}+n^{2}\log n)$ periteration. We also perform experiments using both synthetic data as well asreal data of flight delays to demonstrate the modeling capacity with FVSs ofvarious sizes.
arxiv-4500-287 | Pattern-Coupled Sparse Bayesian Learning for Recovery of Block-Sparse Signals | http://arxiv.org/pdf/1311.2150v1.pdf | author:Jun Fang, Yanning Shen, Hongbin Li, Pu Wang category:cs.IT cs.LG math.IT stat.ML published:2013-11-09 summary:We consider the problem of recovering block-sparse signals whose structuresare unknown \emph{a priori}. Block-sparse signals with nonzero coefficientsoccurring in clusters arise naturally in many practical scenarios. However, theknowledge of the block structure is usually unavailable in practice. In thispaper, we develop a new sparse Bayesian learning method for recovery ofblock-sparse signals with unknown cluster patterns. Specifically, apattern-coupled hierarchical Gaussian prior model is introduced to characterizethe statistical dependencies among coefficients, in which a set ofhyperparameters are employed to control the sparsity of signal coefficients.Unlike the conventional sparse Bayesian learning framework in which eachindividual hyperparameter is associated independently with each coefficient, inthis paper, the prior for each coefficient not only involves its ownhyperparameter, but also the hyperparameters of its immediate neighbors. Indoing this way, the sparsity patterns of neighboring coefficients are relatedto each other and the hierarchical model has the potential to encouragestructured-sparse solutions. The hyperparameters, along with the sparse signal,are learned by maximizing their posterior probability via anexpectation-maximization (EM) algorithm. Numerical results show that theproposed algorithm presents uniform superiority over other existing methods ina series of experiments.
arxiv-4500-288 | Large Margin Semi-supervised Structured Output Learning | http://arxiv.org/pdf/1311.2139v1.pdf | author:P. Balamurugan, Shirish Shevade, Sundararajan Sellamanickam category:cs.LG published:2013-11-09 summary:In structured output learning, obtaining labelled data for real-worldapplications is usually costly, while unlabelled examples are available inabundance. Semi-supervised structured classification has been developed tohandle large amounts of unlabelled structured data. In this work, we considersemi-supervised structural SVMs with domain constraints. The optimizationproblem, which in general is not convex, contains the loss terms associatedwith the labelled and unlabelled examples along with the domain constraints. Wepropose a simple optimization approach, which alternates between solving asupervised learning problem and a constraint matching problem. Solving theconstraint matching problem is difficult for structured prediction, and wepropose an efficient and effective hill-climbing method to solve it. Thealternating optimization is carried out within a deterministic annealingframework, which helps in effective constraint matching, and avoiding localminima which are not very useful. The algorithm is simple to implement andachieves comparable generalization performance on benchmark datasets.
arxiv-4500-289 | A Structured Prediction Approach for Missing Value Imputation | http://arxiv.org/pdf/1311.2137v1.pdf | author:Rahul Kidambi, Vinod Nair, Sundararajan Sellamanickam, S. Sathiya Keerthi category:cs.LG published:2013-11-09 summary:Missing value imputation is an important practical problem. There is a largebody of work on it, but there does not exist any work that formulates theproblem in a structured output setting. Also, most applications haveconstraints on the imputed data, for example on the distribution associatedwith each variable. None of the existing imputation methods use theseconstraints. In this paper we propose a structured output approach for missingvalue imputation that also incorporates domain constraints. We focus on largemargin models, but it is easy to extend the ideas to probabilistic models. Wedeal with the intractable inference step in learning via a piecewise trainingtechnique that is simple, efficient, and effective. Comparison with existingstate-of-the-art and baseline imputation methods shows that our method givessignificantly improved performance on the Hamming loss measure.
arxiv-4500-290 | Low-Rank Matrix and Tensor Completion via Adaptive Sampling | http://arxiv.org/pdf/1304.4672v3.pdf | author:Akshay Krishnamurthy, Aarti Singh category:stat.ML published:2013-04-17 summary:We study low rank matrix and tensor completion and propose novel algorithmsthat employ adaptive sampling schemes to obtain strong performance guarantees.Our algorithms exploit adaptivity to identify entries that are highlyinformative for learning the column space of the matrix (tensor) andconsequently, our results hold even when the row space is highly coherent, incontrast with previous analyses. In the absence of noise, we show that one canexactly recover a $n \times n$ matrix of rank $r$ from merely $\Omega(nr^{3/2}\log(r))$ matrix entries. We also show that one can recover an order $T$tensor using $\Omega(n r^{T-1/2}T^2 \log(r))$ entries. For noisy recovery, ouralgorithm consistently estimates a low rank matrix corrupted with noise using$\Omega(n r^{3/2} \textrm{polylog}(n))$ entries. We complement our study withsimulations that verify our theory and demonstrate the scalability of ouralgorithms.
arxiv-4500-291 | Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions | http://arxiv.org/pdf/1311.2110v1.pdf | author:Rishabh Iyer, Stefanie Jegelka, Jeff Bilmes category:cs.DS cs.DM cs.LG published:2013-11-08 summary:We investigate three related and important problems connected to machinelearning: approximating a submodular function everywhere, learning a submodularfunction (in a PAC-like setting [53]), and constrained minimization ofsubmodular functions. We show that the complexity of all three problems dependson the 'curvature' of the submodular function, and provide lower and upperbounds that refine and improve previous results [3, 16, 18, 52]. Our prooftechniques are fairly generic. We either use a black-box transformation of thefunction (for approximation and learning), or a transformation of algorithms touse an appropriate surrogate function (for minimization). Curiously, curvaturehas been known to influence approximations for submodular maximization [7, 55],but its effect on minimization, approximation and learning has hitherto beenopen. We complete this picture, and also support our theoretical claims byempirical results.
arxiv-4500-292 | An Experimental Comparison of Trust Region and Level Sets | http://arxiv.org/pdf/1311.2102v1.pdf | author:Lena Gorelick, Ismail BenAyed, Frank R. Schmidt, Yuri Boykov category:cs.CV published:2013-11-08 summary:High-order (non-linear) functionals have become very popular in segmentation,stereo and other computer vision problems. Level sets is a well establishedgeneral gradient descent framework, which is directly applicable tooptimization of such functionals and widely used in practice. Recently, anothergeneral optimization approach based on trust region methodology was proposedfor regional non-linear functionals. Our goal is a comprehensive experimentalcomparison of these two frameworks in regard to practical efficiency,robustness to parameters, and optimality. We experiment on a wide range ofproblems with non-linear constraints on segment volume, appearance and shape.
arxiv-4500-293 | Nonparametric Multi-group Membership Model for Dynamic Networks | http://arxiv.org/pdf/1311.2079v1.pdf | author:Myunghwan Kim, Jure Leskovec category:cs.SI physics.soc-ph stat.ML published:2013-11-08 summary:Relational data-like graphs, networks, and matrices-is often dynamic, wherethe relational structure evolves over time. A fundamental problem in theanalysis of time-varying network data is to extract a summary of the commonstructure and the dynamics of the underlying relations between the entities.Here we build on the intuition that changes in the network structure are drivenby the dynamics at the level of groups of nodes. We propose a nonparametricmulti-group membership model for dynamic networks. Our model contains threemain components: We model the birth and death of individual groups with respectto the dynamics of the network structure via a distance dependent Indian BuffetProcess. We capture the evolution of individual node group memberships via aFactorial Hidden Markov model. And, we explain the dynamics of the networkstructure by explicitly modeling the connectivity structure of groups. Wedemonstrate our model's capability of identifying the dynamics of latent groupsin a number of different types of network data. Experimental results show thatour model provides improved predictive performance over existing dynamicnetwork models on future network forecasting and missing link prediction.
arxiv-4500-294 | A new stopping criterion for the mean shift iterative algorithm | http://arxiv.org/pdf/1311.2014v1.pdf | author:Roberto Rodríguez, Esley Torres, Yasel Garcés, Osvaldo Pereira, Humberto Sossa category:cs.CV published:2013-11-08 summary:The mean shift iterative algorithm was proposed in 2006, for using theentropy as a stopping criterion. From then on, a theoretical base has beendeveloped and a group of applications has been carried out using thisalgorithm. This paper proposes a new stopping criterion for the mean shiftiterative algorithm, where stopping threshold via entropy is used now, but inanother way. Many segmentation experiments were carried out by utilizingstandard images and it was verified that a better segmentation was reached, andthat the algorithm had better stability. An analysis on the convergence,through a theorem, with the new stopping criterion was carried out. The goal ofthis paper is to compare the new stopping criterion with the old criterion. Forthis reason, the obtained results were not compared with other segmentationapproaches, since with the old stopping criterion were previously carried out.
arxiv-4500-295 | Fast Tracking via Spatio-Temporal Context Learning | http://arxiv.org/pdf/1311.1939v1.pdf | author:Kaihua Zhang, Lei Zhang, Ming-Hsuan Yang, David Zhang category:cs.CV published:2013-11-08 summary:In this paper, we present a simple yet fast and robust algorithm whichexploits the spatio-temporal context for visual tracking. Our approachformulates the spatio-temporal relationships between the object of interest andits local context based on a Bayesian framework, which models the statisticalcorrelation between the low-level features (i.e., image intensity and position)from the target and its surrounding regions. The tracking problem is posed bycomputing a confidence map, and obtaining the best target location bymaximizing an object location likelihood function. The Fast Fourier Transformis adopted for fast learning and detection in this work. Implemented in MATLABwithout code optimization, the proposed tracker runs at 350 frames per secondon an i7 machine. Extensive experimental results show that the proposedalgorithm performs favorably against state-of-the-art methods in terms ofefficiency, accuracy and robustness.
arxiv-4500-296 | Moment-based Uniform Deviation Bounds for $k$-means and Friends | http://arxiv.org/pdf/1311.1903v1.pdf | author:Matus Telgarsky, Sanjoy Dasgupta category:cs.LG stat.ML published:2013-11-08 summary:Suppose $k$ centers are fit to $m$ points by heuristically minimizing the$k$-means cost; what is the corresponding fit over the source distribution?This question is resolved here for distributions with $p\geq 4$ boundedmoments; in particular, the difference between the sample cost and distributioncost decays with $m$ and $p$ as $m^{\min\{-1/4, -1/2+2/p\}}$. The essentialtechnical contribution is a mechanism to uniformly control deviations in theface of unbounded parameter sets, cost functions, and source distributions. Tofurther demonstrate this mechanism, a soft clustering variant of $k$-means costis also considered, namely the log likelihood of a Gaussian mixture, subject tothe constraint that all covariance matrices have bounded spectrum. Lastly, arate with refined constants is provided for $k$-means instances possessing somecluster structure.
arxiv-4500-297 | Logique mathématique et linguistique formelle | http://arxiv.org/pdf/1311.1897v1.pdf | author:Christian Retoré category:math.LO cs.CL published:2013-11-08 summary:As the etymology of the word shows, logic is intimately related to language,as exemplified by the work of philosophers from Antiquity and from theMiddle-Age. At the beginning of the XX century, the crisis of the foundationsof mathematics invented mathematical logic and imposed logic as alanguage-based foundation for mathematics. How did the relations between logicand language evolved in this newly defined mathematical framework? After asurvey of the history of the relation between logic and linguistics,traditionally focused on semantics, we focus on some present issues: 1) grammaras a deductive system 2) the transformation of the syntactic structure of asentence to a logical formula representing its meaning 3) taking into accountthe context when interpreting words. This lecture shows that type theoryprovides a convenient framework both for natural language syntax and for theinterpretation of any of tis level (words, sentences, discourse).
arxiv-4500-298 | Stochastic blockmodel approximation of a graphon: Theory and consistent estimation | http://arxiv.org/pdf/1311.1731v2.pdf | author:Edoardo M Airoldi, Thiago B Costa, Stanley H Chan category:stat.ME cs.LG cs.SI stat.ML published:2013-11-07 summary:Non-parametric approaches for analyzing network data based on exchangeablegraph models (ExGM) have recently gained interest. The key object that definesan ExGM is often referred to as a graphon. This non-parametric perspective onnetwork modeling poses challenging questions on how to make inference on thegraphon underlying observed network data. In this paper, we propose acomputationally efficient procedure to estimate a graphon from a set ofobserved networks generated from it. This procedure is based on a stochasticblockmodel approximation (SBA) of the graphon. We show that, by approximatingthe graphon with a stochastic block model, the graphon can be consistentlyestimated, that is, the estimation error vanishes as the size of the graphapproaches infinity.
arxiv-4500-299 | Optimization, Learning, and Games with Predictable Sequences | http://arxiv.org/pdf/1311.1869v1.pdf | author:Alexander Rakhlin, Karthik Sridharan category:cs.LG cs.GT published:2013-11-08 summary:We provide several applications of Optimistic Mirror Descent, an onlinelearning algorithm based on the idea of predictable sequences. First, werecover the Mirror Prox algorithm for offline optimization, prove an extensionto Holder-smooth functions, and apply the results to saddle-point typeproblems. Next, we prove that a version of Optimistic Mirror Descent (which hasa close relation to the Exponential Weights algorithm) can be used by twostrongly-uncoupled players in a finite zero-sum matrix game to converge to theminimax equilibrium at the rate of O((log T)/T). This addresses a question ofDaskalakis et al 2011. Further, we consider a partial information version ofthe problem. We then apply the results to convex programming and exhibit asimple algorithm for the approximate Max Flow problem.
arxiv-4500-300 | Local Graph Clustering Beyond Cheeger's Inequality | http://arxiv.org/pdf/1304.8132v2.pdf | author:Zeyuan Allen Zhu, Silvio Lattanzi, Vahab Mirrokni category:cs.DS cs.LG stat.ML published:2013-04-30 summary:Motivated by applications of large-scale graph clustering, we studyrandom-walk-based LOCAL algorithms whose running times depend only on the sizeof the output cluster, rather than the entire graph. All previously known suchalgorithms guarantee an output conductance of $\tilde{O}(\sqrt{\phi(A)})$ whenthe target set $A$ has conductance $\phi(A)\in[0,1]$. In this paper, we improveit to $$\tilde{O}\bigg( \min\Big\{\sqrt{\phi(A)},\frac{\phi(A)}{\sqrt{\mathsf{Conn}(A)}} \Big\} \bigg)\enspace, $$ where theinternal connectivity parameter $\mathsf{Conn}(A) \in [0,1]$ is defined as thereciprocal of the mixing time of the random walk over the induced subgraph on$A$. For instance, using $\mathsf{Conn}(A) = \Omega(\lambda(A) / \log n)$ where$\lambda$ is the second eigenvalue of the Laplacian of the induced subgraph on$A$, our conductance guarantee can be as good as$\tilde{O}(\phi(A)/\sqrt{\lambda(A)})$. This builds an interesting connectionto the recent advance of the so-called improved Cheeger's Inequality [KKL+13],which says that global spectral algorithms can provide a conductance guaranteeof $O(\phi_{\mathsf{opt}}/\sqrt{\lambda_3})$ instead of$O(\sqrt{\phi_{\mathsf{opt}}})$. In addition, we provide theoretical guarantee on the clustering accuracy (interms of precision and recall) of the output set. We also prove that ouranalysis is tight, and perform empirical evaluation to support our theory onboth synthetic and real data. It is worth noting that, our analysis outperforms prior work when the clusteris well-connected. In fact, the better it is well-connected inside, the moresignificant improvement (both in terms of conductance and accuracy) we canobtain. Our results shed light on why in practice some random-walk-basedalgorithms perform better than its previous theory, and help guide futureresearch about local clustering.
