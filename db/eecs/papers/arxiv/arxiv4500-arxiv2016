arxiv-4500-1 | Domain-Specific Sentiment Word Extraction by Seed Expansion and Pattern Generation | http://arxiv.org/abs/1309.6722 | author:Tang Duyu, Qin Bing, Zhou LanJun, Wong KamFai, Zhao Yanyan, Liu Ting category:cs.CL published:2013-09-26 summary:This paper focuses on the automatic extraction of domain-specific sentimentword (DSSW), which is a fundamental subtask of sentiment analysis. Mostprevious work utilizes manual patterns for this task. However, the performanceof those methods highly relies on the labelled patterns or selected seeds. Inorder to overcome the above problem, this paper presents an automatic frameworkto detect large-scale domain-specific patterns for DSSW extraction. To thisend, sentiment seeds are extracted from massive dataset of user comments.Subsequently, these sentiment seeds are expanded by synonyms using abootstrapping mechanism. Simultaneously, a synonymy graph is built and thegraph propagation algorithm is applied on the built synonymy graph. Afterwards,syntactic and sequential relations between target words and high-rankedsentiment words are extracted automatically to construct large-scale patterns,which are further used to extracte DSSWs. The experimental results in threedomains reveal the effectiveness of our method.
arxiv-4500-2 | Building Bridges: Viewing Active Learning from the Multi-Armed Bandit Lens | http://arxiv.org/abs/1309.6830 | author:Ravi Ganti, Alexander G. Gray category:cs.LG stat.ML published:2013-09-26 summary:In this paper we propose a multi-armed bandit inspired, pool based activelearning algorithm for the problem of binary classification. By carefullyconstructing an analogy between active learning and multi-armed bandits, weutilize ideas such as lower confidence bounds, and self-concordantregularization from the multi-armed bandit literature to design our proposedalgorithm. Our algorithm is a sequential algorithm, which in each round assignsa sampling distribution on the pool, samples one point from this distribution,and queries the oracle for the label of this sampled point. The design of thissampling distribution is also inspired by the analogy between active learningand multi-armed bandits. We show how to derive lower confidence bounds requiredby our algorithm. Experimental comparisons to previously proposed activelearning algorithms show superior performance on some standard UCI datasets.
arxiv-4500-3 | Bethe-ADMM for Tree Decomposition based Parallel MAP Inference | http://arxiv.org/abs/1309.6829 | author:Qiang Fu, Huahua Wang, Arindam Banerjee category:cs.AI cs.LG stat.ML published:2013-09-26 summary:We consider the problem of maximum a posteriori (MAP) inference in discretegraphical models. We present a parallel MAP inference algorithm calledBethe-ADMM based on two ideas: tree-decomposition of the graph and thealternating direction method of multipliers (ADMM). However, unlike thestandard ADMM, we use an inexact ADMM augmented with a Bethe-divergence basedproximal function, which makes each subproblem in ADMM easy to solve inparallel using the sum-product algorithm. We rigorously prove globalconvergence of Bethe-ADMM. The proposed algorithm is extensively evaluated onboth synthetic and real datasets to illustrate its effectiveness. Further, theparallel Bethe-ADMM is shown to scale almost linearly with increasing number ofcores.
arxiv-4500-4 | Causal Discovery with Continuous Additive Noise Models | http://arxiv.org/abs/1309.6779 | author:Jonas Peters, Joris Mooij, Dominik Janzing, Bernhard Sch√∂lkopf category:stat.ML published:2013-09-26 summary:We consider the problem of learning causal directed acyclic graphs from anobservational joint distribution. One can use these graphs to predict theoutcome of interventional experiments, from which data are often not available.We show that if the observational distribution follows a structural equationmodel with an additive noise structure, the directed acyclic graph becomesidentifiable from the distribution under mild conditions. This constitutes aninteresting alternative to traditional methods that assume faithfulness andidentify only the Markov equivalence class of the graph, thus leaving someedges undirected. We provide practical algorithms for finitely many samples,RESIT (Regression with Subsequent Independence Test) and two methods based onan independence score. We prove that RESIT is correct in the population settingand provide an empirical evaluation.
arxiv-4500-5 | Distributed Online Learning in Social Recommender Systems | http://arxiv.org/abs/1309.6707 | author:Cem Tekin, Simpson Zhang, Mihaela van der Schaar category:cs.SI cs.LG stat.ML published:2013-09-26 summary:In this paper, we consider decentralized sequential decision making indistributed online recommender systems, where items are recommended to usersbased on their search query as well as their specific background includinghistory of bought items, gender and age, all of which comprise the contextinformation of the user. In contrast to centralized recommender systems, inwhich there is a single centralized seller who has access to the completeinventory of items as well as the complete record of sales and userinformation, in decentralized recommender systems each seller/learner only hasaccess to the inventory of items and user information for its own products andnot the products and user information of other sellers, but can get commissionif it sells an item of another seller. Therefore the sellers must distributedlyfind out for an incoming user which items to recommend (from the set of ownitems or items of another seller), in order to maximize the revenue from ownsales and commissions. We formulate this problem as a cooperative contextualbandit problem, analytically bound the performance of the sellers compared tothe best recommendation strategy given the complete realization of userarrivals and the inventory of items, as well as the context-dependent purchaseprobabilities of each item, and verify our results via numerical examples on adistributed data set adapted based on Amazon data. We evaluate the dependenceof the performance of a seller on the inventory of items the seller has, thenumber of connections it has with the other sellers, and the commissions whichthe seller gets by selling items of other sellers to its users.
arxiv-4500-6 | Convex Relaxations of Bregman Divergence Clustering | http://arxiv.org/abs/1309.6823 | author:Hao Cheng, Xinhua Zhang, Dale Schuurmans category:cs.LG stat.ML published:2013-09-26 summary:Although many convex relaxations of clustering have been proposed in the pastdecade, current formulations remain restricted to spherical Gaussian ordiscriminative models and are susceptible to imbalanced clusters. To addressthese shortcomings, we propose a new class of convex relaxations that can beflexibly applied to more general forms of Bregman divergence clustering. Bybasing these new formulations on normalized equivalence relations we retainadditional control on relaxation quality, which allows improvement inclustering quality. We furthermore develop optimization methods that improvescalability by exploiting recent implicit matrix norm methods. In practice, wefind that the new formulations are able to efficiently produce tighterclusterings that improve the accuracy of state of the art methods.
arxiv-4500-7 | Integrating Document Clustering and Topic Modeling | http://arxiv.org/abs/1309.6874 | author:Pengtao Xie, Eric P. Xing category:cs.LG cs.CL cs.IR stat.ML published:2013-09-26 summary:Document clustering and topic modeling are two closely related tasks whichcan mutually benefit each other. Topic modeling can project documents into atopic space which facilitates effective document clustering. Cluster labelsdiscovered by document clustering can be incorporated into topic models toextract local topics specific to each cluster and global topics shared by allclusters. In this paper, we propose a multi-grain clustering topic model(MGCTM) which integrates document clustering and topic modeling into a unifiedframework and jointly performs the two tasks to achieve the overall bestperformance. Our model tightly couples two components: a mixture component usedfor discovering latent groups in document collection and a topic modelcomponent used for mining multi-grain topics including local topics specific toeach cluster and global topics shared across clusters.We employ variationalinference to approximate the posterior of hidden variables and learn modelparameters. Experiments on two datasets demonstrate the effectiveness of ourmodel.
arxiv-4500-8 | Stochastic Rank Aggregation | http://arxiv.org/abs/1309.6852 | author:Shuzi Niu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng category:cs.LG cs.IR stat.ML published:2013-09-26 summary:This paper addresses the problem of rank aggregation, which aims to find aconsensus ranking among multiple ranking inputs. Traditional rank aggregationmethods are deterministic, and can be categorized into explicit and implicitmethods depending on whether rank information is explicitly or implicitlyutilized. Surprisingly, experimental results on real data sets show thatexplicit rank aggregation methods would not work as well as implicit methods,although rank information is critical for the task. Our analysis indicates thatthe major reason might be the unreliable rank information from incompleteranking inputs. To solve this problem, we propose to incorporate uncertaintyinto rank aggregation and tackle the problem in both unsupervised andsupervised scenario. We call this novel framework {stochastic rank aggregation}(St.Agg for short). Specifically, we introduce a prior distribution on ranks,and transform the ranking functions or objectives in traditional explicitmethods to their expectations over this distribution. Our experiments onbenchmark data sets show that the proposed St.Agg outperforms the baselines inboth unsupervised and supervised scenarios.
arxiv-4500-9 | The Supervised IBP: Neighbourhood Preserving Infinite Latent Feature Models | http://arxiv.org/abs/1309.6858 | author:Novi Quadrianto, Viktoriia Sharmanska, David A. Knowles, Zoubin Ghahramani category:cs.LG stat.ML published:2013-09-26 summary:We propose a probabilistic model to infer supervised latent variables in theHamming space from observed data. Our model allows simultaneous inference ofthe number of binary latent variables, and their values. The latent variablespreserve neighbourhood structure of the data in a sense that objects in thesame semantic concept have similar latent values, and objects in differentconcepts have dissimilar latent values. We formulate the supervised infinitelatent variable problem based on an intuitive principle of pulling objectstogether if they are of the same type, and pushing them apart if they are not.We then combine this principle with a flexible Indian Buffet Process prior onthe latent variables. We show that the inferred supervised latent variables canbe directly used to perform a nearest neighbour search for the purpose ofretrieval. We introduce a new application of dynamically extending hash codes,and show how to effectively couple the structure of the hash codes withcontinuously growing structure of the neighbourhood preserving infinite latentfeature space.
arxiv-4500-10 | Active Learning with Expert Advice | http://arxiv.org/abs/1309.6875 | author:Peilin Zhao, Steven Hoi, Jinfeng Zhuang category:cs.LG stat.ML published:2013-09-26 summary:Conventional learning with expert advice methods assumes a learner is alwaysreceiving the outcome (e.g., class labels) of every incoming training instanceat the end of each trial. In real applications, acquiring the outcome fromoracle can be costly or time consuming. In this paper, we address a new problemof active learning with expert advice, where the outcome of an instance isdisclosed only when it is requested by the online learner. Our goal is to learnan accurate prediction model by asking the oracle the number of questions assmall as possible. To address this challenge, we propose a framework of activeforecasters for online active learning with expert advice, which attempts toextend two regular forecasters, i.e., Exponentially Weighted Average Forecasterand Greedy Forecaster, to tackle the task of active learning with expertadvice. We prove that the proposed algorithms satisfy the Hannan consistencyunder some proper assumptions, and validate the efficacy of our technique by anextensive set of experiments.
arxiv-4500-11 | Identifying Finite Mixtures of Nonparametric Product Distributions and Causal Inference of Confounders | http://arxiv.org/abs/1309.6860 | author:Eleni Sgouritsa, Dominik Janzing, Jonas Peters, Bernhard Schoelkopf category:cs.LG cs.AI stat.ML published:2013-09-26 summary:We propose a kernel method to identify finite mixtures of nonparametricproduct distributions. It is based on a Hilbert space embedding of the jointdistribution. The rank of the constructed tensor is equal to the number ofmixture components. We present an algorithm to recover the components bypartitioning the data points into clusters such that the variables are jointlyconditionally independent given the cluster. This method can be used toidentify finite confounders.
arxiv-4500-12 | Determinantal Clustering Processes - A Nonparametric Bayesian Approach to Kernel Based Semi-Supervised Clustering | http://arxiv.org/abs/1309.6862 | author:Amar Shah, Zoubin Ghahramani category:cs.LG stat.ML published:2013-09-26 summary:Semi-supervised clustering is the task of clustering data points intoclusters where only a fraction of the points are labelled. The true number ofclusters in the data is often unknown and most models require this parameter asan input. Dirichlet process mixture models are appealing as they can infer thenumber of clusters from the data. However, these models do not deal with highdimensional data well and can encounter difficulties in inference. We present anovel nonparameteric Bayesian kernel based method to cluster data pointswithout the need to prespecify the number of clusters or to model complicateddensities from which data points are assumed to be generated from. The keyinsight is to use determinants of submatrices of a kernel matrix as a measureof how close together a set of points are. We explore some theoreticalproperties of the model and derive a natural Gibbs based algorithm with MCMChyperparameter learning. The model is implemented on a variety of synthetic andreal world data sets.
arxiv-4500-13 | One-class Collaborative Filtering with Random Graphs: Annotated Version | http://arxiv.org/abs/1309.6786 | author:Ulrich Paquet, Noam Koenigstein category:stat.ML cs.LG G.3 published:2013-09-26 summary:The bane of one-class collaborative filtering is interpreting and modellingthe latent signal from the missing class. In this paper we present a novelBayesian generative model for implicit collaborative filtering. It forms a corecomponent of the Xbox Live architecture, and unlike previous approaches,delineates the odds of a user disliking an item from simply not considering it.The latent signal is treated as an unobserved random graph connecting userswith items they might have encountered. We demonstrate how large-scaledistributed learning can be achieved through a combination of stochasticgradient descent and mean field variational inference over random graphsamples. A fine-grained comparison is done against a state of the art baselineon real world data.
arxiv-4500-14 | Bennett-type Generalization Bounds: Large-deviation Case and Faster Rate of Convergence | http://arxiv.org/abs/1309.6876 | author:Chao Zhang category:stat.ML cs.LG published:2013-09-26 summary:In this paper, we present the Bennett-type generalization bounds of thelearning process for i.i.d. samples, and then show that the generalizationbounds have a faster rate of convergence than the traditional results. Inparticular, we first develop two types of Bennett-type deviation inequality forthe i.i.d. learning process: one provides the generalization bounds based onthe uniform entropy number; the other leads to the bounds based on theRademacher complexity. We then adopt a new method to obtain the alternativeexpressions of the Bennett-type generalization bounds, which imply that thebounds have a faster rate o(N^{-1/2}) of convergence than the traditionalresults O(N^{-1/2}). Additionally, we find that the rate of the bounds willbecome faster in the large-deviation case, which refers to a situation wherethe empirical risk is far away from (at least not close to) the expected risk.Finally, we analyze the asymptotical convergence of the learning process andcompare our analysis with the existing results.
arxiv-4500-15 | On the Feature Discovery for App Usage Prediction in Smartphones | http://arxiv.org/abs/1309.7982 | author:Zhung-Xun Liao, Shou-Chung Li, Wen-Chih Peng, Philip S Yu category:cs.LG published:2013-09-26 summary:With the increasing number of mobile Apps developed, they are now closelyintegrated into daily life. In this paper, we develop a framework to predictmobile Apps that are most likely to be used regarding the current device statusof a smartphone. Such an Apps usage prediction framework is a crucialprerequisite for fast App launching, intelligent user experience, and powermanagement of smartphones. By analyzing real App usage log data, we discovertwo kinds of features: The Explicit Feature (EF) from sensing readings ofbuilt-in sensors, and the Implicit Feature (IF) from App usage relations. TheIF feature is derived by constructing the proposed App Usage Graph (abbreviatedas AUG) that models App usage transitions. In light of AUG, we are able todiscover usage relations among Apps. Since users may have different usagebehaviors on their smartphones, we further propose one personalized featureselection algorithm. We explore minimum description length (MDL) from thetraining data and select those features which need less length to describe thetraining data. The personalized feature selection can successfully reduce thelog size and the prediction time. Finally, we adopt the kNN classificationmodel to predict Apps usage. Note that through the features selected by theproposed personalized feature selection algorithm, we only need to keep thesefeatures, which in turn reduces the prediction time and avoids the curse ofdimensionality when using the kNN classifier. We conduct a comprehensiveexperimental study based on a real mobile App usage dataset. The resultsdemonstrate the effectiveness of the proposed framework and show the predictivecapability for App usage prediction.
arxiv-4500-16 | Sparse Nested Markov models with Log-linear Parameters | http://arxiv.org/abs/1309.6863 | author:Ilya Shpitser, Robin J. Evans, Thomas S. Richardson, James M. Robins category:cs.LG cs.AI stat.ML published:2013-09-26 summary:Hidden variables are ubiquitous in practical data analysis, and thereforemodeling marginal densities and doing inference with the resulting models is animportant problem in statistics, machine learning, and causal inference.Recently, a new type of graphical model, called the nested Markov model, wasdeveloped which captures equality constraints found in marginals of directedacyclic graph (DAG) models. Some of these constraints, such as the so called`Verma constraint', strictly generalize conditional independence. To makemodeling and inference with nested Markov models practical, it is necessary tolimit the number of parameters in the model, while still correctly capturingthe constraints in the marginal of a DAG model. Placing such limits is similarin spirit to sparsity methods for undirected graphical models, and regressionmodels. In this paper, we give a log-linear parameterization which allowssparse modeling with nested Markov models. We illustrate the advantages of thisparameterization with a simulation study.
arxiv-4500-17 | Estimating Undirected Graphs Under Weak Assumptions | http://arxiv.org/abs/1309.6933 | author:Larry Wasserman, Mladen Kolar, Alessandro Rinaldo category:math.ST cs.LG stat.ML stat.TH 62H12 published:2013-09-26 summary:We consider the problem of providing nonparametric confidence guarantees forundirected graphs under weak assumptions. In particular, we do not assumesparsity, incoherence or Normality. We allow the dimension $D$ to increase withthe sample size $n$. First, we prove lower bounds that show that if we wantaccurate inferences with low assumptions then there are limitations on thedimension as a function of sample size. When the dimension increases slowlywith sample size, we show that methods based on Normal approximations and onthe bootstrap lead to valid inferences and we provide Berry-Esseen bounds onthe accuracy of the Normal approximation. When the dimension is large relativeto sample size, accurate inferences for graphs under low assumptions are notpossible. Instead we propose to estimate something less demanding than theentire partial correlation graph. In particular, we consider: cluster graphs,restricted partial correlation graphs and correlation graphs.
arxiv-4500-18 | Finite-Time Analysis of Kernelised Contextual Bandits | http://arxiv.org/abs/1309.6869 | author:Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, Nelo Cristianini category:cs.LG stat.ML published:2013-09-26 summary:We tackle the problem of online reward maximisation over a large finite setof actions described by their contexts. We focus on the case when the number ofactions is too big to sample all of them even once. However we assume that wehave access to the similarities between actions' contexts and that the expectedreward is an arbitrary linear function of the contexts' images in the relatedreproducing kernel Hilbert space (RKHS). We propose KernelUCB, a kernelised UCBalgorithm, and give a cumulative regret bound through a frequentist analysis.For contextual bandits, the related algorithm GP-UCB turns out to be a specialcase of our algorithm, and our finite-time analysis improves the regret boundof GP-UCB for the agnostic case, both in the terms of the kernel-dependentquantity and the RKHS norm of the reward function. Moreover, for the linearkernel, our regret bound matches the lower bound for contextual linear bandits.
arxiv-4500-19 | Modeling Documents with Deep Boltzmann Machines | http://arxiv.org/abs/1309.6865 | author:Nitish Srivastava, Ruslan R Salakhutdinov, Geoffrey E. Hinton category:cs.LG cs.IR stat.ML published:2013-09-26 summary:We introduce a Deep Boltzmann Machine model suitable for modeling andextracting latent semantic representations from a large unstructured collectionof documents. We overcome the apparent difficulty of training a DBM withjudicious parameter tying. This parameter tying enables an efficientpretraining algorithm and a state initialization scheme that aids inference.The model can be trained just as efficiently as a standard Restricted BoltzmannMachine. Our experiments show that the model assigns better log probability tounseen data than the Replicated Softmax model. Features extracted from ourmodel outperform LDA, Replicated Softmax, and DocNADE models on documentretrieval and document classification tasks.
arxiv-4500-20 | Approximate Kalman Filter Q-Learning for Continuous State-Space MDPs | http://arxiv.org/abs/1309.6868 | author:Charles Tripp, Ross D. Shachter category:cs.LG stat.ML published:2013-09-26 summary:We seek to learn an effective policy for a Markov Decision Process (MDP) withcontinuous states via Q-Learning. Given a set of basis functions over stateaction pairs we search for a corresponding set of linear weights that minimizesthe mean Bellman residual. Our algorithm uses a Kalman filter model to estimatethose weights and we have developed a simpler approximate Kalman filter modelthat outperforms the current state of the art projected TD-Learning methods onseveral standard benchmark problems.
arxiv-4500-21 | Speedy Model Selection (SMS) for Copula Models | http://arxiv.org/abs/1309.6867 | author:Yaniv Tenzer, Gal Elidan category:cs.LG stat.ME published:2013-09-26 summary:We tackle the challenge of efficiently learning the structure of expressivemultivariate real-valued densities of copula graphical models. We start bytheoretically substantiating the conjecture that for many copula families themagnitude of Spearman's rank correlation coefficient is monotone in theexpected contribution of an edge in network, namely the negative copulaentropy. We then build on this theory and suggest a novel Bayesian approachthat makes use of a prior over values of Spearman's rho for learningcopula-based models that involve a mix of copula families. We demonstrate thegeneralization effectiveness of our highly efficient approach on sizable andvaried real-life datasets.
arxiv-4500-22 | Multiple-object tracking in cluttered and crowded public spaces | http://arxiv.org/abs/1309.6391 | author:Rhys Martin, Ognjen Arandjeloviƒá category:cs.CV published:2013-09-25 summary:This paper addresses the problem of tracking moving objects of variableappearance in challenging scenes rich with features and texture. Reliabletracking is of pivotal importance in surveillance applications. It is madeparticularly difficult by the nature of objects encountered in such scenes:these too change in appearance and scale, and are often articulated (e.g.humans). We propose a method which uses fast motion detection and segmentationas a constraint for both building appearance models and their robustpropagation (matching) in time. The appearance model is based on sets of localappearances automatically clustered using spatio-kinetic similarity, and isupdated with each new appearance seen. This integration of all seen appearancesof a tracked object makes it extremely resilient to errors caused by occlusionand the lack of permanence of due to low data quality, appearance change orbackground clutter. These theoretical strengths of our algorithm areempirically demonstrated on two hour long video footage of a busy citymarketplace.
arxiv-4500-23 | Contextually learnt detection of unusual motion-based behaviour in crowded public spaces | http://arxiv.org/abs/1309.6390 | author:Ognjen Arandjeloviƒá category:cs.CV published:2013-09-25 summary:In this paper we are interested in analyzing behaviour in crowded publicplaces at the level of holistic motion. Our aim is to learn, without userinput, strong scene priors or labelled data, the scope of "normal behaviour"for a particular scene and thus alert to novelty in unseen footage. The firstcontribution is a low-level motion model based on what we term trackletprimitives, which are scene-specific elementary motions. We propose aclustering-based algorithm for tracklet estimation from local approximations totracks of appearance features. This is followed by two methods for motionnovelty inference from tracklet primitives: (a) we describe an approach basedon a non-hierarchial ensemble of Markov chains as a means of capturingbehavioural characteristics at different scales, and (b) a more flexiblealternative which exhibits a higher generalizing power by accounting forconstraints introduced by intentionality and goal-oriented planning of humanmotion in a particular scene. Evaluated on a 2h long video of a busy citymarketplace, both algorithms are shown to be successful at inferring unusualbehaviour, the latter model achieving better performance for novelties at alarger spatial scale.
arxiv-4500-24 | Diffeomorphic Metric Mapping and Probabilistic Atlas Generation of Hybrid Diffusion Imaging based on BFOR Signal Basis | http://arxiv.org/abs/1309.6379 | author:Jia Du, A. Pasha Hosseinbor, Moo K. Chung, Barbara B. Bendlin, Gaurav Suryawanshi, Andrew L. Alexander, Anqi Qiu category:cs.CV published:2013-09-25 summary:We propose a large deformation diffeomorphic metric mapping algorithm toalign multiple b-value diffusion weighted imaging (mDWI) data, specificallyacquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We thenpropose a Bayesian model for estimating the white matter atlas from HYDIs. Weadopt the work given in Hosseinbor et al. (2012) and represent the q-spacediffusion signal with the Bessel Fourier orientation reconstruction (BFOR)signal basis. The BFOR framework provides the representation of mDWI in theq-space and thus reduces memory requirement. In addition, since the BFOR signalbasis is orthonormal, the L2 norm that quantifies the differences in theq-space signals of any two mDWI datasets can be easily computed as the sum ofthe squared differences in the BFOR expansion coefficients. In this work, weshow that the reorientation of the $q$-space signal due to spatialtransformation can be easily defined on the BFOR signal basis. We incorporatethe BFOR signal basis into the LDDMM framework and derive the gradient descentalgorithm for LDDMM-HYDI with explicit orientation optimization. Additionally,we extend the previous Bayesian atlas estimation framework for scalar-valuedimages to HYDIs and derive the expectation-maximization algorithm for solvingthe HYDI atlas estimation problem. Using real HYDI datasets, we show theBayesian model generates the white matter atlas with anatomical details.Moreover, we show that it is important to consider the variation of mDWIreorientation due to a small change in diffeomorphic transformation in theLDDMM-HYDI optimization and to incorporate the full information of HYDI foraligning mDWI.
arxiv-4500-25 | Should I Stay or Should I Go: Coordinating Biological Needs with Continuously-updated Assessments of the Environment | http://arxiv.org/abs/1309.6584 | author:Liane Gabora category:cs.NE cs.LG q-bio.NC published:2013-09-25 summary:This paper presents Wanderer, a model of how autonomous adaptive systemscoordinate internal biological needs with moment-by-moment assessments of theprobabilities of events in the external world. The extent to which Wanderermoves about or explores its environment reflects the relative activations oftwo competing motivational sub-systems: one represents the need to acquireenergy and it excites exploration, and the other represents the need to avoidpredators and it inhibits exploration. The environment contains food,predators, and neutral stimuli. Wanderer responds to these events in a way thatis adaptive in the short turn, and reassesses the probabilities of these eventsso that it can modify its long term behaviour appropriately. When food appears,Wanderer be-comes satiated and exploration temporarily decreases. When apredator appears, Wanderer both decreases exploration in the short term, andbecomes more "cautious" about exploring in the future. Wanderer also formsassociations between neutral features and salient ones (food and predators)when they are present at the same time, and uses these associations to guideits behaviour.
arxiv-4500-26 | Stratified Graphical Models - Context-Specific Independence in Graphical Models | http://arxiv.org/abs/1309.6415 | author:Henrik Nyman, Johan Pensar, Timo Koski, Jukka Corander category:stat.ML published:2013-09-25 summary:Theory of graphical models has matured over more than three decades toprovide the backbone for several classes of models that are used in a myriad ofapplications such as genetic mapping of diseases, credit risk evaluation,reliability and computer security, etc. Despite of their generic applicabilityand wide adoptance, the constraints imposed by undirected graphical models andBayesian networks have also been recognized to be unnecessarily stringent undercertain circumstances. This observation has led to the proposal of severalgeneralizations that aim at more relaxed constraints by which the models canimpose local or context-specific dependence structures. Here we consider anadditional class of such models, termed as stratified graphical models. Wedevelop a method for Bayesian learning of these models by deriving ananalytical expression for the marginal likelihood of data under a specificsubclass of decomposable stratified models. A non-reversible Markov chain MonteCarlo approach is further used to identify models that are highly supported bythe posterior distribution over the model space. Our method is illustrated andcompared with ordinary graphical models through application to several real andsynthetic datasets.
arxiv-4500-27 | An Inter-lingual Reference Approach For Multi-Lingual Ontology Matching | http://arxiv.org/abs/1309.6650 | author:Haytham Al-Feel, Ralph Schafermeier, Adrian Paschke category:cs.CL cs.DL published:2013-09-25 summary:Ontologies are considered as the backbone of the Semantic Web. With therising success of the Semantic Web, the number of participating communitiesfrom different countries is constantly increasing. The growing number ofontologies available in different natural languages leads to aninteroperability problem. In this paper, we discuss several approaches forontology matching; examine similarities and differences, identify weaknesses,and compare the existing automated approaches with the manual approaches forintegrating multilingual ontologies. In addition to that, we propose a newarchitecture for a multilingual ontology matching service. As a case study weused an example of two multilingual enterprise ontologies - the universityontology of Freie Universitaet Berlin and the ontology for Fayoum University inEgypt.
arxiv-4500-28 | Characterness: An Indicator of Text in the Wild | http://arxiv.org/abs/1309.6691 | author:Yao Li, Wenjing Jia, Chunhua Shen, Anton van den Hengel category:cs.CV published:2013-09-25 summary:Text in an image provides vital information for interpreting its contents,and text in a scene can aide with a variety of tasks from navigation, toobstacle avoidance, and odometry. Despite its value, however, identifyinggeneral text in images remains a challenging research problem. Motivated by theneed to consider the widely varying forms of natural text, we propose abottom-up approach to the problem which reflects the `characterness' of animage region. In this sense our approach mirrors the move from saliencydetection methods to measures of `objectness'. In order to measure thecharacterness we develop three novel cues that are tailored for characterdetection, and a Bayesian method for their integration. Because text is made upof sets of characters, we then design a Markov random field (MRF) model so asto exploit the inherent dependencies between characters. We experimentally demonstrate the effectiveness of our characterness cues aswell as the advantage of Bayesian multi-cue integration. The proposed textdetector outperforms state-of-the-art methods on a few benchmark scene textdetection datasets. We also show that our measurement of `characterness' issuperior than state-of-the-art saliency detection models when applied to thesame task.
arxiv-4500-29 | A Unified Framework for Representation-based Subspace Clustering of Out-of-sample and Large-scale Data | http://arxiv.org/abs/1309.6487 | author:Xi Peng, Huajin Tang, Lei Zhang, Zhang Yi, Shijie Xiao category:cs.LG cs.CV stat.ML published:2013-09-25 summary:Under the framework of spectral clustering, the key of subspace clustering isbuilding a similarity graph which describes the neighborhood relations amongdata points. Some recent works build the graph using sparse, low-rank, and$\ell_2$-norm-based representation, and have achieved state-of-the-artperformance. However, these methods have suffered from the following twolimitations. First, the time complexities of these methods are at leastproportional to the cube of the data size, which make those methods inefficientfor solving large-scale problems. Second, they cannot cope with out-of-sampledata that are not used to construct the similarity graph. To cluster eachout-of-sample datum, the methods have to recalculate the similarity graph andthe cluster membership of the whole data set. In this paper, we propose aunified framework which makes representation-based subspace clusteringalgorithms feasible to cluster both out-of-sample and large-scale data. Underour framework, the large-scale problem is tackled by converting it asout-of-sample problem in the manner of "sampling, clustering, coding, andclassifying". Furthermore, we give an estimation for the error bounds bytreating each subspace as a point in a hyperspace. Extensive experimentalresults on various benchmark data sets show that our methods outperform severalrecently-proposed scalable methods in clustering large-scale data set.
arxiv-4500-30 | Asymptotic normality and optimalities in estimation of large Gaussian graphical models | http://arxiv.org/abs/1309.6024 | author:Zhao Ren, Tingni Sun, Cun-Hui Zhang, Harrison H. Zhou category:math.ST stat.ME stat.ML stat.TH published:2013-09-24 summary:The Gaussian graphical model, a popular paradigm for studying relationshipamong variables in a wide range of applications, has attracted great attentionin recent years. This paper considers a fundamental question: When is itpossible to estimate low-dimensional parameters at parametric square-root ratein a large Gaussian graphical model? A novel regression approach is proposed toobtain asymptotically efficient estimation of each entry of a precision matrixunder a sparseness condition relative to the sample size. When the precisionmatrix is not sufficiently sparse, or equivalently the sample size is notsufficiently large, a lower bound is established to show that it is no longerpossible to achieve the parametric rate in the estimation of each entry. Thislower bound result, which provides an answer to the delicate sample sizequestion, is established with a novel construction of a subset of sparseprecision matrices in an application of Le Cam's lemma. Moreover, the proposedestimator is proven to have optimal convergence rate when the parametric ratecannot be achieved, under a minimal sample requirement. The proposed estimatoris applied to test the presence of an edge in the Gaussian graphical model orto recover the support of the entire model, to obtain adaptive rate-optimalestimation of the entire precision matrix as measured by the matrix $\ell_q$operator norm and to make inference in latent variables in the graphical model.All of this is achieved under a sparsity condition on the precision matrix anda side condition on the range of its spectrum. This significantly relaxes thecommonly imposed uniform signal strength condition on the precision matrix,irrepresentability condition on the Hessian tensor operator of the covariancematrix or the $\ell_1$ constraint on the precision matrix. Numerical resultsconfirm our theoretical findings. The ROC curve of the proposed algorithm,Asymptotic Normal Thresholding (ANT), for support recovery significantlyoutperforms that of the popular GLasso algorithm.
arxiv-4500-31 | Solving OSCAR regularization problems by proximal splitting algorithms | http://arxiv.org/abs/1309.6301 | author:Xiangrong Zeng, M√°rio A. T. Figueiredo category:cs.CV cs.LG stat.ML published:2013-09-24 summary:The OSCAR (octagonal selection and clustering algorithm for regression)regularizer consists of a L_1 norm plus a pair-wise L_inf norm (responsible forits grouping behavior) and was proposed to encourage group sparsity inscenarios where the groups are a priori unknown. The OSCAR regularizer has anon-trivial proximity operator, which limits its applicability. We reformulatethis regularizer as a weighted sorted L_1 norm, and propose its groupingproximity operator (GPO) and approximate proximity operator (APO), thus makingstate-of-the-art proximal splitting algorithms (PSAs) available to solveinverse problems with OSCAR regularization. The GPO is in fact the APO followedby additional grouping and averaging operations, which are costly in time andstorage, explaining the reason why algorithms with APO are much faster thanthat with GPO. The convergences of PSAs with GPO are guaranteed since GPO is anexact proximity operator. Although convergence of PSAs with APO is may not beguaranteed, we have experimentally found that APO behaves similarly to GPO whenthe regularization parameter of the pair-wise L_inf norm is set to anappropriately small value. Experiments on recovery of group-sparse signals(with unknown groups) show that PSAs with APO are very fast and accurate.
arxiv-4500-32 | Using Nuances of Emotion to Identify Personality | http://arxiv.org/abs/1309.6352 | author:Saif M. Mohammad, Svetlana Kiritchenko category:cs.CL published:2013-09-24 summary:Past work on personality detection has shown that frequency of lexicalcategories such as first person pronouns, past tense verbs, and sentiment wordshave significant correlations with personality traits. In this paper, for thefirst time, we show that fine affect (emotion) categories such as that ofexcitement, guilt, yearning, and admiration are significant indicators ofpersonality. Additionally, we perform experiments to show that the gainsprovided by the fine affect categories are not obtained by using coarse affectcategories alone or with specificity features alone. We employ these featuresin five SVM classifiers for detecting five personality traits through essays.We find that the use of fine emotion features leads to statisticallysignificant improvement over a competitive baseline, whereas the use of coarseaffect and specificity features does not.
arxiv-4500-33 | Non-negative Matrix Factorization with Linear Constraints for Single-Channel Speech Enhancement | http://arxiv.org/abs/1309.6047 | author:Nikolay Lyubimov, Mikhail Kotov category:cs.SD cs.CL published:2013-09-24 summary:This paper investigates a non-negative matrix factorization (NMF)-basedapproach to the semi-supervised single-channel speech enhancement problem whereonly non-stationary additive noise signals are given. The proposed methodrelies on sinusoidal model of speech production which is integrated inside NMFframework using linear constraints on dictionary atoms. This method is furtherdeveloped to regularize harmonic amplitudes. Simple multiplicative algorithmsare presented. The experimental evaluation was made on TIMIT corpus mixed withvarious types of noise. It has been shown that the proposed method outperformssome of the state-of-the-art noise suppression techniques in terms ofsignal-to-noise ratio.
arxiv-4500-34 | Random Forests on Distance Matrices for Imaging Genetics Studies | http://arxiv.org/abs/1309.6158 | author:Aaron Sim, Dimosthenis Tsagkrasoulis, Giovanni Montana category:stat.ML stat.AP published:2013-09-24 summary:We propose a non-parametric regression methodology, Random Forests onDistance Matrices (RFDM), for detecting genetic variants associated toquantitative phenotypes representing the human brain's structure or function,and obtained using neuroimaging techniques. RFDM, which is an extension ofdecision forests, requires a distance matrix as response that encodes allpair-wise phenotypic distances in the random sample. We discuss ways to learnsuch distances directly from the data using manifold learning techniques, andhow to define such distances when the phenotypes are non-vectorial objects suchas brain connectivity networks. We also describe an extension of RFDM to detectespistatic effects while keeping the computational complexity low. Extensivesimulation results and an application to an imaging genetics study ofAlzheimer's Disease are presented and discussed.
arxiv-4500-35 | Tracking Sentiment in Mail: How Genders Differ on Emotional Axes | http://arxiv.org/abs/1309.6347 | author:Saif M. Mohammad, Tony, Yang category:cs.CL published:2013-09-24 summary:With the widespread use of email, we now have access to unprecedented amountsof text that we ourselves have written. In this paper, we show how sentimentanalysis can be used in tandem with effective visualizations to quantify andtrack emotions in many types of mail. We create a large word--emotionassociation lexicon by crowdsourcing, and use it to compare emotions in loveletters, hate mail, and suicide notes. We show that there are markeddifferences across genders in how they use emotion words in work-place email.For example, women use many words from the joy--sadness axis, whereas menprefer terms from the fear--trust axis. Finally, we show visualizations thatcan help people track emotions in their emails.
arxiv-4500-36 | Acronym recognition and processing in 22 languages | http://arxiv.org/abs/1309.6185 | author:Maud Ehrmann, Leonida della Rocca, Ralf Steinberger, Hristo Tanev category:cs.CL published:2013-09-24 summary:We are presenting work on recognising acronyms of the form Long-Form(Short-Form) such as "International Monetary Fund (IMF)" in millions of newsarticles in twenty-two languages, as part of our more general effort torecognise entities and their variants in news text and to use them for theautomatic analysis of the news, including the linking of related news acrosslanguages. We show how the acronym recognition patterns, initially developedfor medical terms, needed to be adapted to the more general news domain and wepresent evaluation results. We describe our effort to automatically merge thenumerous long-form variants referring to the same short-form, while keepingnon-related long-forms separate. Finally, we provide extensive statistics onthe frequency and the distribution of short-form/long-form pairs acrosslanguages.
arxiv-4500-37 | Sentiment Analysis in the News | http://arxiv.org/abs/1309.6202 | author:Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov, Vanni Zavarella, Erik van der Goot, Matina Halkia, Bruno Pouliquen, Jenya Belyaeva category:cs.CL published:2013-09-24 summary:Recent years have brought a significant growth in the volume of research insentiment analysis, mostly on highly subjective text types (movie or productreviews). The main difference these texts have with news articles is that theirtarget is clearly defined and unique across the text. Following differentannotation efforts and the analysis of the issues encountered, we realised thatnews opinion mining is different from that of other text types. We identifiedthree subtasks that need to be addressed: definition of the target; separationof the good and bad news content from the good and bad sentiment expressed onthe target; and analysis of clearly marked opinion that is expressedexplicitly, not needing interpretation or the use of world knowledge.Furthermore, we distinguish three different possible views on newspaperarticles - author, reader and text, which have to be addressed differently atthe time of analysing sentiment. Given these definitions, we present work onmining opinions about entities in English language news, in which (a) we testthe relative suitability of various sentiment dictionaries and (b) we attemptto separate positive or negative opinion from good or bad news. In theexperiments described here, we tested whether or not subject domain-definingvocabulary should be ignored. Results showed that this idea is more appropriatein the context of news opinion mining and that the approaches taking this intoconsideration produce a better performance.
arxiv-4500-38 | JRC-Names: A freely available, highly multilingual named entity resource | http://arxiv.org/abs/1309.6162 | author:Ralf Steinberger, Bruno Pouliquen, Mijail Kabadjov, Erik van der Goot category:cs.CL published:2013-09-24 summary:This paper describes a new, freely available, highly multilingual namedentity resource for person and organisation names that has been compiled overseven years of large-scale multilingual news analysis combined with Wikipediamining, resulting in 205,000 per-son and organisation names plus about the samenumber of spelling variants written in over 20 different scripts and in manymore languages. This resource, produced as part of the Europe Media Monitoractivity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a numberof purposes. These include improving name search in databases or on theinternet, seeding machine learning systems to learn named entity recognitionrules, improve machine translation results, and more. We describe here how thisresource was created; we give statistics on its current size; we address theissue of morphological inflection; and we give details regarding itsfunctionality. Updates to this resource will be made available daily.
arxiv-4500-39 | A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion | http://arxiv.org/abs/1309.6013 | author:T. Tony Cai, Wen-Xin Zhou category:stat.ML math.ST stat.TH published:2013-09-24 summary:We consider in this paper the problem of noisy 1-bit matrix completion undera general non-uniform sampling distribution using the max-norm as a convexrelaxation for the rank. A max-norm constrained maximum likelihood estimate isintroduced and studied. The rate of convergence for the estimate is obtained.Information-theoretical methods are used to establish a minimax lower boundunder the general sampling model. The minimax upper and lower bounds togetheryield the optimal rate of convergence for the Frobenius norm loss.Computational algorithms and numerical performance are also discussed.
arxiv-4500-40 | Asymptotic Analysis of LASSOs Solution Path with Implications for Approximate Message Passing | http://arxiv.org/abs/1309.5979 | author:Ali Mousavi, Arian Maleki, Richard G. Baraniuk category:math.ST cs.IT math.IT stat.ML stat.TH published:2013-09-23 summary:This paper concerns the performance of the LASSO (also knows as basis pursuitdenoising) for recovering sparse signals from undersampled, randomized, noisymeasurements. We consider the recovery of the signal $x_o \in \mathbb{R}^N$from $n$ random and noisy linear observations $y= Ax_o + w$, where $A$ is themeasurement matrix and $w$ is the noise. The LASSO estimate is given by thesolution to the optimization problem $x_o$ with $\hat{x}_{\lambda} = \arg\min_x \frac{1}{2} \y-Ax\_2^2 + \lambda \x\_1$. Despite major progress inthe theoretical analysis of the LASSO solution, little is known about itsbehavior as a function of the regularization parameter $\lambda$. In this paperwe study two questions in the asymptotic setting (i.e., where $N \rightarrow\infty$, $n \rightarrow \infty$ while the ratio $n/N$ converges to a fixednumber in $(0,1)$): (i) How does the size of the active set$\\hat{x}_\lambda\_0/N$ behave as a function of $\lambda$, and (ii) How doesthe mean square error $\\hat{x}_{\lambda} - x_o\_2^2/N$ behave as a functionof $\lambda$? We then employ these results in a new, reliable algorithm forsolving LASSO based on approximate message passing (AMP).
arxiv-4500-41 | Fenchel Duals for Drifting Adversaries | http://arxiv.org/abs/1309.5904 | author:Suman K Bera, Anamitra R Choudhury, Syamantak Das, Sambuddha Roy, Jayram S. Thatchachar category:cs.LG published:2013-09-23 summary:We describe a primal-dual framework for the design and analysis of onlineconvex optimization algorithms for {\em drifting regret}. Existing literatureshows (nearly) optimal drifting regret bounds only for the $\ell_2$ and the$\ell_1$-norms. Our work provides a connection between these algorithms and theOnline Mirror Descent ($\omd$) updates; one key insight that results from ourwork is that in order for these algorithms to succeed, it suffices to have thegradient of the regularizer to be bounded (in an appropriate norm). Forsituations (like for the $\ell_1$ norm) where the vanilla regularizer does nothave this property, we have to {\em shift} the regularizer to ensure this.Thus, this helps explain the various updates presented in \cite{bansal10,buchbinder12}. We also consider the online variant of the problem with1-lookahead, and with movement costs in the $\ell_2$-norm. Our primal dualapproach yields nearly optimal competitive ratios for this problem.
arxiv-4500-42 | From Once Upon a Time to Happily Ever After: Tracking Emotions in Novels and Fairy Tales | http://arxiv.org/abs/1309.5909 | author:Saif Mohammad category:cs.CL published:2013-09-23 summary:Today we have access to unprecedented amounts of literary texts. However,search still relies heavily on key words. In this paper, we show how sentimentanalysis can be used in tandem with effective visualizations to quantify andtrack emotions in both individual books and across very large collections. Weintroduce the concept of emotion word density, and using the Brothers Grimmfairy tales as example, we show how collections of text can be organized forbetter search. Using the Google Books Corpus we show how to determine anentity's emotion associations from co-occurring words. Finally, we compareemotion words in fairy tales and novels, to show that fairy tales have a muchwider range of emotion word densities than novels.
arxiv-4500-43 | Ellipsoidal Rounding for Nonnegative Matrix Factorization Under Noisy Separability | http://arxiv.org/abs/1309.5701 | author:Tomohiko Mizutani category:stat.ML published:2013-09-23 summary:We present a numerical algorithm for nonnegative matrix factorization (NMF)problems under noisy separability. An NMF problem under separability can bestated as one of finding all vertices of the convex hull of data points. Theresearch interest of this paper is to find the vectors as close to the verticesas possible in a situation in which noise is added to the data points. Ouralgorithm is designed to capture the shape of the convex hull of data points byusing its enclosing ellipsoid. We show that the algorithm has correctness androbustness properties from theoretical and practical perspectives; correctnesshere means that if the data points do not contain any noise, the algorithm canfind the vertices of their convex hull; robustness means that if the datapoints contain noise, the algorithm can find the near-vertices. Finally, weapply the algorithm to document clustering, and report the experimentalresults.
arxiv-4500-44 | Implementation of a language driven Backpropagation algorithm | http://arxiv.org/abs/1309.5676 | author:I. V. Grossu, C. I. Ciuluvica category:cs.NE published:2013-09-23 summary:Inspired by the importance of both communication and feedback on errors inhuman learning, our main goal was to implement a similar mechanism insupervised learning of artificial neural networks. The starting point in ourstudy was the observation that words should accompany the input vectorsincluded in the training set, thus extending the ANN input space. This had asconsequence the necessity to take into consideration a modified sigmoidactivation function for neurons in the first hidden layer (in agreement with aspecific MLP apartment structure), and also a modified version of theBackpropagation algorithm, which allows using of unspecified (null) desiredoutput components. Following the belief that basic concepts should be tested onsimple examples, the previous mentioned mechanism was applied on both the XORproblem and a didactic color case study. In this context, we noticed theinteresting fact that the ANN was capable to categorize all desired inputvectors in the absence of their corresponding words, even though the trainingset included only word accompanied inputs, in both positive and negativeexamples. Further analysis along applying this approach to more complexscenarios is currently in progress, as we consider the proposed language-drivenalgorithm might contribute to a better understanding of learning in humans,opening as well the possibility to create a specific category of artificialneural networks, with abstraction capabilities.
arxiv-4500-45 | On the Success Rate of Crossover Operators for Genetic Programming with Offspring Selection | http://arxiv.org/abs/1309.5896 | author:Gabriel Kronberger, Stephan Winkler, Michael Affenzeller, Andreas Beham, Stefan Wagner category:cs.NE published:2013-09-23 summary:Genetic programming is a powerful heuristic search technique that is used fora number of real world applications to solve among others regression,classification, and time-series forecasting problems. A lot of progress towardsa theoretic description of genetic programming in form of schema theorems hasbeen made, but the internal dynamics and success factors of genetic programmingare still not fully understood. In particular, the effects of differentcrossover operators in combination with offspring selection are largelyunknown. This contribution sheds light on the ability of well-known GP crossoveroperators to create better offspring when applied to benchmark problems. Weconclude that standard (sub-tree swapping) crossover is a good default choicein combination with offspring selection, and that GP with offspring selectionand random selection of crossover operators can improve the performance of thealgorithm in terms of best solution quality when no solution size constraintsare applied.
arxiv-4500-46 | Smooth minimization of nonsmooth functions with parallel coordinate descent methods | http://arxiv.org/abs/1309.5885 | author:Olivier Fercoq, Peter Richt√°rik category:cs.DC math.OC stat.ML published:2013-09-23 summary:We study the performance of a family of randomized parallel coordinatedescent methods for minimizing the sum of a nonsmooth and separable convexfunctions. The problem class includes as a special case L1-regularized L1regression and the minimization of the exponential loss ("AdaBoost problem").We assume the input data defining the loss function is contained in a sparse$m\times n$ matrix $A$ with at most $\omega$ nonzeros in each row. Our methodsneed $O(n \beta/\tau)$ iterations to find an approximate solution with highprobability, where $\tau$ is the number of processors and $\beta = 1 +(\omega-1)(\tau-1)/(n-1)$ for the fastest variant. The notation hidesdependence on quantities such as the required accuracy and confidence levelsand the distance of the starting iterate from an optimal point. Since$\beta/\tau$ is a decreasing function of $\tau$, the method needs feweriterations when more processors are used. Certain variants of our algorithmsperform on average only $O(\nnz(A)/n)$ arithmetic operations during a singleiteration per processor and, because $\beta$ decreases when $\omega$ does,fewer iterations are needed for sparser problems.
arxiv-4500-47 | Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet | http://arxiv.org/abs/1309.5843 | author:Marco Guerini, Lorenzo Gatti, Marco Turchi category:cs.CL published:2013-09-23 summary:Assigning a positive or negative score to a word out of context (i.e. aword's prior polarity) is a challenging task for sentiment analysis. In theliterature, various approaches based on SentiWordNet have been proposed. Inthis paper, we compare the most often used techniques together with newlyproposed ones and incorporate all of them in a learning framework to seewhether blending them can further improve the estimation of prior polarityscores. Using two different versions of SentiWordNet and testing regression andclassification models across tasks and datasets, our learning approachconsistently outperforms the single metrics, providing a new state-of-the-artapproach in computing words' prior polarity for sentiment analysis. We concludeour investigation showing interesting biases in calculated prior polarityscores when word Part of Speech and annotator gender are considered.
arxiv-4500-48 | Data Mining using Unguided Symbolic Regression on a Blast Furnace Dataset | http://arxiv.org/abs/1309.5931 | author:Michael Kommenda, Gabriel Kronberger, Christoph Feilmayr, Michael Affenzeller category:cs.NE published:2013-09-23 summary:In this paper a data mining approach for variable selection and knowledgeextraction from datasets is presented. The approach is based on unguidedsymbolic regression (every variable present in the dataset is treated as thetarget variable in multiple regression runs) and a novel variable relevancemetric for genetic programming. The relevance of each input variable iscalculated and a model approximating the target variable is created. Thegenetic programming configurations with different target variables are executedmultiple times to reduce stochastic effects and the aggregated results aredisplayed as a variable interaction network. This interaction networkhighlights important system components and implicit relations between thevariables. The whole approach is tested on a blast furnace dataset, because ofthe complexity of the blast furnace and the many interrelations between thevariables. Finally the achieved results are discussed with respect to existingknowledge about the blast furnace process.
arxiv-4500-49 | A Kernel Classification Framework for Metric Learning | http://arxiv.org/abs/1309.5823 | author:Faqiang Wang, Wangmeng Zuo, Lei Zhang, Deyu Meng, David Zhang category:cs.LG I.5.1 published:2013-09-23 summary:Learning a distance metric from the given training samples plays a crucialrole in many machine learning tasks, and various models and optimizationalgorithms have been proposed in the past decade. In this paper, we generalizeseveral state-of-the-art metric learning methods, such as large margin nearestneighbor (LMNN) and information theoretic metric learning (ITML), into a kernelclassification framework. First, doublets and triplets are constructed from thetraining samples, and a family of degree-2 polynomial kernel functions areproposed for pairs of doublets or triplets. Then, a kernel classificationframework is established, which can not only generalize many popular metriclearning methods such as LMNN and ITML, but also suggest new metric learningmethods, which can be efficiently implemented, interestingly, by using thestandard support vector machine (SVM) solvers. Two novel metric learningmethods, namely doublet-SVM and triplet-SVM, are then developed under theproposed framework. Experimental results show that doublet-SVM and triplet-SVMachieve competitive classification accuracies with state-of-the-art metriclearning methods such as ITML and LMNN but with significantly less trainingtime.
arxiv-4500-50 | Efficient Sampling from Time-Varying Log-Concave Distributions | http://arxiv.org/abs/1309.5977 | author:Hariharan Narayanan, Alexander Rakhlin category:stat.ML stat.CO published:2013-09-23 summary:We propose a computationally efficient random walk on a convex body whichrapidly mixes and closely tracks a time-varying log-concave distribution. Wedevelop general theoretical guarantees on the required number of steps; thisnumber can be calculated on the fly according to the distance from and theshape of the next distribution. We then illustrate the technique on severalexamples. Within the context of exponential families, the proposed methodproduces samples from a posterior distribution which is updated as data arrivein a streaming fashion. The sampling technique can be used to tracktime-varying truncated distributions, as well as to obtain samples from achanging mixture model, fitted in a streaming fashion to data. In the settingof linear optimization, the proposed method has oracle complexity with bestknown dependence on the dimension for certain geometries. In the context ofonline learning and repeated games, the algorithm is an efficient method forimplementing no-regret mixture forecasting strategies. Remarkably, in some ofthese examples, only one step of the random walk is needed to track the nextdistribution.
arxiv-4500-51 | Feature Learning with Gaussian Restricted Boltzmann Machine for Robust Speech Recognition | http://arxiv.org/abs/1309.6176 | author:Xin Zheng, Zhiyong Wu, Helen Meng, Weifeng Li, Lianhong Cai category:cs.CL cs.LG cs.SD published:2013-09-23 summary:In this paper, we first present a new variant of Gaussian restrictedBoltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmannmachine (MGRBM), with its definition and learning algorithm. Then we proposeusing a learned GRBM or MGRBM to extract better features for robust speechrecognition. Our experiments on Aurora2 show that both GRBM-extracted andMGRBM-extracted feature performs much better than Mel-frequency cepstralcoefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN)acoustic model, and MGRBM-extracted feature is slightly better.
arxiv-4500-52 | A Hybrid Algorithm for Matching Arabic Names | http://arxiv.org/abs/1309.5657 | author:T. El-Shishtawy category:cs.CL published:2013-09-22 summary:In this paper, a new hybrid algorithm which combines both of token-based andcharacter-based approaches is presented. The basic Levenshtein approach hasbeen extended to token-based distance metric. The distance metric is enhancedto set the proper granularity level behavior of the algorithm. It smoothly mapsa threshold of misspellings differences at the character level, and theimportance of token level errors in terms of token's position and frequency.Using a large Arabic dataset, the experimental results show that the proposedalgorithm overcomes successfully many types of errors such as: typographicalerrors, omission or insertion of middle name components, omission ofnon-significant popular name components, and different writing styles charactervariations. When compared the results with other classical algorithms, usingthe same dataset, the proposed algorithm was found to increase the minimumsuccess level of best tested algorithms, while achieving higher upper limits .
arxiv-4500-53 | Stochastic Bound Majorization | http://arxiv.org/abs/1309.5605 | author:Anna Choromanska, Tony Jebara category:cs.LG published:2013-09-22 summary:Recently a majorization method for optimizing partition functions oflog-linear models was proposed alongside a novel quadratic variationalupper-bound. In the batch setting, it outperformed state-of-the-art first- andsecond-order optimization methods on various learning tasks. We propose astochastic version of this bound majorization method as well as a low-rankmodification for high-dimensional data-sets. The resulting stochasticsecond-order method outperforms stochastic gradient descent (across variationsand various tunings) both in terms of the number of iterations and computationtime till convergence while finding a better quality parameter setting. Theproposed method bridges first- and second-order stochastic optimization methodsby maintaining a computational complexity that is linear in the data dimensionand while exploiting second order information about the pseudo-global curvatureof the objective function (as opposed to the local curvature in the Hessian).
arxiv-4500-54 | LDC Arabic Treebanks and Associated Corpora: Data Divisions Manual | http://arxiv.org/abs/1309.5652 | author:Mona Diab, Nizar Habash, Owen Rambow, Ryan Roth category:cs.CL published:2013-09-22 summary:The Linguistic Data Consortium (LDC) has developed hundreds of data corporafor natural language processing (NLP) research. Among these are a number ofannotated treebank corpora for Arabic. Typically, these corpora consist of asingle collection of annotated documents. NLP research, however, usuallyrequires multiple data sets for the purposes of training models, developingtechniques, and final evaluation. Therefore it becomes necessary to divide thecorpora used into the required data sets (divisions). This document details aset of rules that have been defined to enable consistent divisions for old andnew Arabic treebanks (ATB) and related corpora.
arxiv-4500-55 | Stochastic First- and Zeroth-order Methods for Nonconvex Stochastic Programming | http://arxiv.org/abs/1309.5549 | author:Saeed Ghadimi, Guanghui Lan category:math.OC cs.CC stat.ML published:2013-09-22 summary:In this paper, we introduce a new stochastic approximation (SA) typealgorithm, namely the randomized stochastic gradient (RSG) method, for solvingan important class of nonlinear (possibly nonconvex) stochastic programming(SP) problems. We establish the complexity of this method for computing anapproximate stationary point of a nonlinear programming problem. We also showthat this method possesses a nearly optimal rate of convergence if the problemis convex. We discuss a variant of the algorithm which consists of applying apost-optimization phase to evaluate a short list of solutions generated byseveral independent runs of the RSG method, and show that such modificationallows to improve significantly the large-deviation properties of thealgorithm. These methods are then specialized for solving a class ofsimulation-based optimization problems in which only stochastic zeroth-orderinformation is available.
arxiv-4500-56 | Spike Synchronization Dynamics of Small-World Networks | http://arxiv.org/abs/1309.5660 | author:Derek Harter category:cs.NE nlin.AO q-bio.NC published:2013-09-22 summary:In this research report, we examine the effects of small-world networkorganization on spike synchronization dynamics in networks of Izhikevichspiking units. We interpolate network organizations from regular ring lattices,through the small-world region, to random networks, and measure global spikesynchronization dynamics. We examine how average path length and clusteringeffect the dynamics of global and neighborhood clique spike organization andpropagation. We show that the emergence of global synchronization undergoes aphase transition in the small-world region, between the clustering and pathlength phase transitions that are known to exist. We add additional realisticconstraints on the dynamics by introducing propagation delays of spikingsignals proportional to wiring length. The addition of delays interferes withthe ability of random networks to sustain global synchronization, in relationto the breakdown of clustering in the networks. The addition of delays furtherenhances the finding that small-world organization is beneficial for balancingneighborhood synchronized waves of organization with global synchronizationdynamics.
arxiv-4500-57 | Reweighted message passing revisited | http://arxiv.org/abs/1309.5655 | author:Vladimir Kolmogorov category:cs.AI cs.CV cs.LG published:2013-09-22 summary:We propose a new family of message passing techniques for MAP estimation ingraphical models which we call {\em Sequential Reweighted Message Passing}(SRMP). Special cases include well-known techniques such as {\em Min-SumDiffusion} (MSD) and a faster {\em Sequential Tree-Reweighted Message Passing}(TRW-S). Importantly, our derivation is simpler than the original derivation ofTRW-S, and does not involve a decomposition into trees. This allows easygeneralizations. We present such a generalization for the case of higher-ordergraphical models, and test it on several real-world problems with promisingresults.
arxiv-4500-58 | Generic Image Classification Approaches Excel on Face Recognition | http://arxiv.org/abs/1309.5594 | author:Fumin Shen, Chunhua Shen category:cs.CV published:2013-09-22 summary:The main finding of this work is that the standard image classificationpipeline, which consists of dictionary learning, feature encoding, spatialpyramid pooling and linear classification, outperforms all state-of-the-artface recognition methods on the tested benchmark datasets (we have tested onAR, Extended Yale B, the challenging FERET, and LFW-a datasets). Thissurprising and prominent result suggests that those advances in generic imageclassification can be directly applied to improve face recognition systems. Inother words, face recognition may not need to be viewed as a separate objectclassification problem. While recently a large body of residual based face recognition methods focuson developing complex dictionary learning algorithms, in this work we show thata dictionary of randomly extracted patches (even from non-face images) canachieve very promising results using the image classification pipeline. Thatmeans, the choice of dictionary learning methods may not be important. Instead,we find that learning multiple dictionaries using different low-level imagefeatures often improve the final classification accuracy. Our proposed facerecognition approach offers the best reported results on the widely-used facerecognition benchmark datasets. In particular, on the challenging FERET andLFW-a datasets, we improve the best reported accuracies in the literature byabout 20% and 30% respectively.
arxiv-4500-59 | Multiple Instance Learning with Bag Dissimilarities | http://arxiv.org/abs/1309.5643 | author:Veronika Cheplygina, David M. J. Tax, Marco Loog category:stat.ML cs.LG published:2013-09-22 summary:Multiple instance learning (MIL) is concerned with learning from sets (bags)of objects (instances), where the individual instance labels are ambiguous. Inthis setting, supervised learning cannot be applied directly. Often,specialized MIL methods learn by making additional assumptions about therelationship of the bag labels and instance labels. Such assumptions may fit aparticular dataset, but do not generalize to the whole range of MIL problems.Other MIL methods shift the focus of assumptions from the labels to the overall(dis)similarity of bags, and therefore learn from bags directly. We propose torepresent each bag by a vector of its dissimilarities to other bags in thetraining set, and treat these dissimilarities as a feature representation. Weshow several alternatives to define a dissimilarity between bags and discusswhich definitions are more suitable for particular MIL problems. Theexperimental results show that the proposed approach is computationallyinexpensive, yet very competitive with state-of-the-art algorithms on a widerange of MIL datasets.
arxiv-4500-60 | Latent Fisher Discriminant Analysis | http://arxiv.org/abs/1309.5427 | author:Gang Chen category:cs.LG cs.CV stat.ML I.2.10 published:2013-09-21 summary:Linear Discriminant Analysis (LDA) is a well-known method for dimensionalityreduction and classification. Previous studies have also extended thebinary-class case into multi-classes. However, many applications, such asobject detection and keyframe extraction cannot provide consistentinstance-label pairs, while LDA requires labels on instance level for training.Thus it cannot be directly applied for semi-supervised classification problem.In this paper, we overcome this limitation and propose a latent variable Fisherdiscriminant analysis model. We relax the instance-level labeling intobag-level, is a kind of semi-supervised (video-level labels of event type arerequired for semantic frame extraction) and incorporates a data-driven priorover the latent variables. Hence, our method combines the latent variableinference and dimension reduction in an unified bayesian framework. We test ourmethod on MUSK and Corel data sets and yield competitive results compared tothe baseline approach. We also demonstrate its capacity on the challengingTRECVID MED11 dataset for semantic keyframe extraction and conduct ahuman-factors ranking-based experimental evaluation, which clearly demonstratesour proposed method consistently extracts more semantically meaningfulkeyframes than challenging baselines.
arxiv-4500-61 | Scan-based Compressed Terahertz Imaging and Real-Time Reconstruction via the Complex-valued Fast Block Sparse Bayesian Learning Algorithm | http://arxiv.org/abs/1309.6195 | author:Benyuan Liu, Hongqi Fan, Zaiqi Lu, Qiang Fu category:cs.CV published:2013-09-20 summary:Compressed Sensing based Terahertz imaging (CS-THz) is a computationalimaging technique. It uses only one THz receiver to accumulate the randommodulated image measurements where the original THz image is reconstruct fromthese measurements using compressed sensing solvers. The advantage of theCS-THz is its reduced acquisition time compared with the raster scan mode.However, when it applied to large-scale two-dimensional (2D) imaging, theincreased dimension resulted in both high computational complexity andexcessive memory usage. In this paper, we introduced a novel CS-based THzimaging system that progressively compressed the THz image column by column.Therefore, the CS-THz system could be simplified with a much smaller sizedmodulator and reduced dimension. In order to utilize the block structure andthe correlation of adjacent columns of the THz image, a complex-valued blocksparse Bayesian learning algorithm was proposed. We conducted systematicevaluation of state-of-the-art CS algorithms under the scan based CS-THzarchitecture. The compression ratios and the choices of the sensing matriceswere analyzed in detail using both synthetic and real-life THz images.Simulation results showed that both the scan based architecture and theproposed recovery algorithm were superior and efficient for large scale CS-THzapplications.
arxiv-4500-62 | mTim: Rapid and accurate transcript reconstruction from RNA-Seq data | http://arxiv.org/abs/1309.5211 | author:Georg Zeller, Nico Goernitz, Andre Kahles, Jonas Behr, Pramod Mudrakarta, Soeren Sonnenburg, Gunnar Raetsch category:q-bio.GN stat.ML published:2013-09-20 summary:Recent advances in high-throughput cDNA sequencing (RNA-Seq) technology haverevolutionized transcriptome studies. A major motivation for RNA-Seq is to mapthe structure of expressed transcripts at nucleotide resolution. With accuratecomputational tools for transcript reconstruction, this technology may alsobecome useful for genome (re-)annotation, which has mostly relied on de novogene finding where gene structures are primarily inferred from the genomesequence. We developed a machine-learning method, called mTim (margin-basedtranscript inference method) for transcript reconstruction from RNA-Seq readalignments that is based on discriminatively trained hidden Markov supportvector machines. In addition to features derived from read alignments, itutilizes characteristic genomic sequences, e.g. around splice sites, to improvetranscript predictions. mTim inferred transcripts that were highly accurate andrelatively robust to alignment errors in comparison to those from Cufflinks, awidely used transcript assembly method.
arxiv-4500-63 | JRC EuroVoc Indexer JEX - A freely available multi-label categorisation tool | http://arxiv.org/abs/1309.5223 | author:Ralf Steinberger, Mohamed Ebrahim, Marco Turchi category:cs.CL H.3.1; H.3.6 published:2013-09-20 summary:EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700hierarchically organised subject domains used by European Institutions and manyauthorities in Member States of the European Union (EU) for the classificationand retrieval of official documents. JEX is JRC-developed multi-labelclassification software that learns from manually labelled data toautomatically assign EuroVoc descriptors to new documents in a profile-basedcategory-ranking task. The JEX release consists of trained classifiers for 22official EU languages, of parallel training data in the same languages, of aninterface that allows viewing and amending the assignment results, and of amodule that allows users to re-train the tool on their own documentcollections. JEX allows advanced users to change the document representation soas to possibly improve the categorisation result through linguisticpre-processing. JEX can be used as a tool for interactive EuroVoc descriptorassignment to increase speed and consistency of the human categorisationprocess, or it can be used fully automatically. The output of JEX is alanguage-independent EuroVoc feature vector lending itself also as input tovarious other Language Technology tasks, including cross-lingual clustering andclassification, cross-lingual plagiarism detection, sentence selection andranking, and more.
arxiv-4500-64 | DGT-TM: A freely Available Translation Memory in 22 Languages | http://arxiv.org/abs/1309.5226 | author:Ralf Steinberger, Andreas Eisele, Szymon Klocek, Spyridon Pilos, Patrick Schl√ºter category:cs.CL I.2.7 published:2013-09-20 summary:The European Commission's (EC) Directorate General for Translation, togetherwith the EC's Joint Research Centre, is making available a large translationmemory (TM; i.e. sentences and their professionally produced translations)covering twenty-two official European Union (EU) languages and their 231language pairs. Such a resource is typically used by translation professionalsin combination with TM software to improve speed and consistency of theirtranslations. However, this resource has also many uses for translation studiesand for language technology applications, including Statistical MachineTranslation (SMT), terminology extraction, Named Entity Recognition (NER),multilingual classification and clustering, and many more. In this referencepaper for DGT-TM, we introduce this new resource, provide statistics regardingits size, and explain how it was produced and how to use it.
arxiv-4500-65 | Scalable Anomaly Detection in Large Homogenous Populations | http://arxiv.org/abs/1309.5803 | author:Henrik Ohlsson, Tianshi Chen, Sina Khoshfetrat Pakazad, Lennart Ljung, S. Shankar Sastry category:cs.LG cs.DC cs.SY math.OC published:2013-09-20 summary:Anomaly detection in large populations is a challenging but highly relevantproblem. The problem is essentially a multi-hypothesis problem, with ahypothesis for every division of the systems into normal and anomal systems.The number of hypothesis grows rapidly with the number of systems andapproximate solutions become a necessity for any problems of practicalinterests. In the current paper we take an optimization approach to thismulti-hypothesis problem. We first observe that the problem is equivalent to anon-convex combinatorial optimization problem. We then relax the problem to aconvex problem that can be solved distributively on the systems and that stayscomputationally tractable as the number of systems increase. An interestingproperty of the proposed method is that it can under certain conditions beshown to give exactly the same result as the combinatorial multi-hypothesisproblem and the relaxation is hence tight.
arxiv-4500-66 | An introduction to the Europe Media Monitor family of applications | http://arxiv.org/abs/1309.5290 | author:Ralf Steinberger, Bruno Pouliquen, Erik van der Goot category:cs.CL published:2013-09-20 summary:Most large organizations have dedicated departments that monitor the media tokeep up-to-date with relevant developments and to keep an eye on how they arerepresented in the news. Part of this media monitoring work can be automated.In the European Union with its 23 official languages, it is particularlyimportant to cover media reports in many languages in order to capture thecomplementary news content published in the different countries. It is alsoimportant to be able to access the news content across languages and to mergethe extracted information. We present here the four publicly accessible systemsof the Europe Media Monitor (EMM) family of applications, which cover between19 and 50 languages (see http://press.jrc.it/overview.html). We give anoverview of their functionality and discuss some of the implications of thefact that they cover quite so many languages. We discuss design issuesnecessary to be able to achieve this high multilinguality, as well as thebenefits of this multilinguality.
arxiv-4500-67 | Nonmyopic View Planning for Active Object Detection | http://arxiv.org/abs/1309.5401 | author:Nikolay Atanasov, Bharath Sankaran, Jerome Le Ny, George J. Pappas, Kostas Daniilidis category:cs.RO cs.CV cs.SY published:2013-09-20 summary:One of the central problems in computer vision is the detection ofsemantically important objects and the estimation of their pose. Most of thework in object detection has been based on single image processing and itsperformance is limited by occlusions and ambiguity in appearance and geometry.This paper proposes an active approach to object detection by controlling thepoint of view of a mobile depth camera. When an initial static detection phaseidentifies an object of interest, several hypotheses are made about its classand orientation. The sensor then plans a sequence of views, which balances theamount of energy used to move with the chance of identifying the correcthypothesis. We formulate an active hypothesis testing problem, which includessensor mobility, and solve it using a point-based approximate POMDP algorithm.The validity of our approach is verified through simulation and real-worldexperiments with the PR2 robot. The results suggest that our approachoutperforms the widely-used greedy view point selection and provides asignificant improvement over static object detection.
arxiv-4500-68 | Saying What You're Looking For: Linguistics Meets Video Search | http://arxiv.org/abs/1309.5174 | author:Andrei Barbu, N. Siddharth, Jeffrey Mark Siskind category:cs.CV cs.CL cs.IR published:2013-09-20 summary:We present an approach to searching large video corpora for video clips whichdepict a natural-language query in the form of a sentence. This approach usescompositional semantics to encode subtle meaning that is lost in other systems,such as the difference between two sentences which have identical words butentirely different meaning: "The person rode the horse} vs. \emph{The horserode the person". Given a video-sentence pair and a natural-language parser,along with a grammar that describes the space of sentential queries, we producea score which indicates how well the video depicts the sentence. We producesuch a score for each video clip in a corpus and return a ranked list of clips.Furthermore, this approach addresses two fundamental problems simultaneously:detecting and tracking objects, and recognizing whether those tracks depict thequery. Because both tracking and object detection are unreliable, this usesknowledge about the intended sentential query to focus the tracker on therelevant participants and ensures that the resulting tracks are described bythe sentential query. While earlier work was limited to single-word querieswhich correspond to either verbs or nouns, we show how one can search forcomplex queries which contain multiple phrases, such as prepositional phrases,and modifiers, such as adverbs. We demonstrate this approach by searching for141 queries involving people and horses interacting with each other in 10full-length Hollywood movies.
arxiv-4500-69 | Colourful Language: Measuring Word-Colour Associations | http://arxiv.org/abs/1309.5942 | author:Saif Mohammad category:cs.CL published:2013-09-20 summary:Since many real-world concepts are associated with colour, for example dangerwith red, linguistic information is often complimented with the use ofappropriate colours in information visualization and product marketing. Yet,there is no comprehensive resource that captures concept-colour associations.We present a method to create a large word-colour association lexicon bycrowdsourcing. We focus especially on abstract concepts and emotions to showthat even though they cannot be physically visualized, they too tend to havestrong colour associations. Finally, we show how word-colour associationsmanifest themselves in language, and quantify usefulness of co-occurrence andpolarity cues in automatically detecting colour associations.
arxiv-4500-70 | Even the Abstract have Colour: Consensus in Word-Colour Associations | http://arxiv.org/abs/1309.5391 | author:Saif M. Mohammad category:cs.CL published:2013-09-20 summary:Colour is a key component in the successful dissemination of information.Since many real-world concepts are associated with colour, for example dangerwith red, linguistic information is often complemented with the use ofappropriate colours in information visualization and product marketing. Yet,there is no comprehensive resource that captures concept-colour associations.We present a method to create a large word-colour association lexicon bycrowdsourcing. A word-choice question was used to obtain sense-levelannotations and to ensure data quality. We focus especially on abstractconcepts and emotions to show that even they tend to have strong colourassociations. Thus, using the right colours can not only improve semanticcoherence, but also inspire the desired emotional response.
arxiv-4500-71 | Recognizing Speech in a Novel Accent: The Motor Theory of Speech Perception Reframed | http://arxiv.org/abs/1309.5319 | author:Cl√©ment Moulin-Frier, M. A. Arbib category:cs.CL cs.LG q-bio.NC published:2013-09-20 summary:The motor theory of speech perception holds that we perceive the speech ofanother in terms of a motor representation of that speech. However, when wehave learned to recognize a foreign accent, it seems plausible that recognitionof a word rarely involves reconstruction of the speech gestures of the speakerrather than the listener. To better assess the motor theory and thisobservation, we proceed in three stages. Part 1 places the motor theory ofspeech perception in a larger framework based on our earlier models of theadaptive formation of mirror neurons for grasping, and for viewing extensionsof that mirror system as part of a larger system for neuro-linguisticprocessing, augmented by the present consideration of recognizing speech in anovel accent. Part 2 then offers a novel computational model of how a listenercomes to understand the speech of someone speaking the listener's nativelanguage with a foreign accent. The core tenet of the model is that thelistener uses hypotheses about the word the speaker is currently uttering toupdate probabilities linking the sound produced by the speaker to phonemes inthe native language repertoire of the listener. This, on average, improves therecognition of later words. This model is neutral regarding the nature of therepresentations it uses (motor vs. auditory). It serve as a reference point forthe discussion in Part 3, which proposes a dual-stream neuro-linguisticarchitecture to revisits claims for and against the motor theory of speechperception and the relevance of mirror neurons, and extracts some implicationsfor the reframing of the motor theory.
arxiv-4500-72 | Network Anomaly Detection: A Survey and Comparative Analysis of Stochastic and Deterministic Methods | http://arxiv.org/abs/1309.4844 | author:Jing Wang, Daniel Rossell, Christos G. Cassandras, Ioannis Ch. Paschalidis category:stat.ML cs.LG cs.NI published:2013-09-19 summary:We present five methods to the problem of network anomaly detection. Thesemethods cover most of the common techniques in the anomaly detection field,including Statistical Hypothesis Tests (SHT), Support Vector Machines (SVM) andclustering analysis. We evaluate all methods in a simulated network thatconsists of nominal data, three flow-level anomalies and one packet-levelattack. Through analyzing the results, we point out the advantages anddisadvantages of each method and conclude that combining the results of theindividual methods can yield improved anomaly detection results.
arxiv-4500-73 | An ant colony optimization algorithm for job shop scheduling problem | http://arxiv.org/abs/1309.5110 | author:Edson Fl√≥rez, Wilfredo G√≥mez, Lola Bautista category:cs.AI cs.NE published:2013-09-19 summary:The nature has inspired several metaheuristics, outstanding among these isAnt Colony Optimization (ACO), which have proved to be very effective andefficient in problems of high complexity (NP-hard) in combinatorialoptimization. This paper describes the implementation of an ACO model algorithmknown as Elitist Ant System (EAS), applied to a combinatorial optimizationproblem called Job Shop Scheduling Problem (JSSP). We propose a method thatseeks to reduce delays designating the operation immediately available, butconsidering the operations that lack little to be available and have a greateramount of pheromone. The performance of the algorithm was evaluated forproblems of JSSP reference, comparing the quality of the solutions obtainedregarding the best known solution of the most effective methods. The solutionswere of good quality and obtained with a remarkable efficiency by having tomake a very low number of objective function evaluations.
arxiv-4500-74 | Blind Deconvolution via Maximum Kurtosis Adaptive Filtering | http://arxiv.org/abs/1309.5004 | author:Deborah Pereg, Doron Benzvi category:cs.CV published:2013-09-19 summary:In this paper, we present an algorithm for identifying a parametricallydescribed destructive unknown system based on a non-gaussianity measure. It isknown that under certain conditions the output of a linear system is moregaussian than the input. Hence, an inverse filter is searched, such that itsoutput is minimally gaussian. We use the kurtosis as a measure of thenon-gaussianity of the signal. A maximum of the kurtosis as a function of thedeconvolving filter coefficients is searched. The search is done iterativelyusing the gradient ascent algorithm, and the coefficients at the maximum pointcorrespond to the inverse filter coefficients. This filter may be applied tothe distorted signal to obtain the original undistorted signal. While a similarapproach has been used before, it was always directed at a particular kind of asignal, commonly of impulsive characteristics. In this paper a successfulattempt has been made to apply the algorithm to a wider range of signals, suchas to process distorted audio signals and destructed images. This innovativeimplementation required the revelation of a way to preprocess the distortedsignal at hand. The experimental results show very good performance in terms ofrecovering audio signals and blurred images, both for an FIR and IIR distortingfilters.
arxiv-4500-75 | Predictive PAC Learning and Process Decompositions | http://arxiv.org/abs/1309.4859 | author:Cosma Rohilla Shalizi, Aryeh Kontorovich category:stat.ML published:2013-09-19 summary:We informally call a stochastic process learnable if it admits ageneralization error approaching zero in probability for any concept class withfinite VC-dimension (IID processes are the simplest example). A mixture oflearnable processes need not be learnable itself, and certainly itsgeneralization error need not decay at the same rate. In this paper, we arguethat it is natural in predictive PAC to condition not on the past observationsbut on the mixture component of the sample path. This definition not onlymatches what a realistic learner might demand, but also allows us to sidestepseveral otherwise grave problems in learning from dependent data. Inparticular, we give a novel PAC generalization bound for mixtures of learnableprocesses with a generalization error that is not worse than that of eachmixture component. We also provide a characterization of mixtures of absolutelyregular ($\beta$-mixing) processes, of independent probability-theoreticinterest.
arxiv-4500-76 | HOL(y)Hammer: Online ATP Service for HOL Light | http://arxiv.org/abs/1309.4962 | author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO cs.MS published:2013-09-19 summary:HOL(y)Hammer is an online AI/ATP service for formal (computer-understandable)mathematics encoded in the HOL Light system. The service allows its users toupload and automatically process an arbitrary formal development (project)based on HOL Light, and to attack arbitrary conjectures that use the conceptsdefined in some of the uploaded projects. For that, the service uses severalautomated reasoning systems combined with several premise selection methodstrained on all the project proofs. The projects that are readily available onthe server for such query answering include the recent versions of theFlyspeck, Multivariate Analysis and Complex Analysis libraries. The serviceruns on a 48-CPU server, currently employing in parallel for each task 7 AI/ATPcombinations and 4 decision procedures that contribute to its overallperformance. The system is also available for local installation by interestedusers, who can customize it for their own proof development. An Emacs interfaceallowing parallel asynchronous queries to the service is also provided. Theoverall structure of the service is outlined, problems that arise and theirsolutions are discussed, and an initial account of using the system is given.
arxiv-4500-77 | Exploration and Exploitation in Visuomotor Prediction of Autonomous Agents | http://arxiv.org/abs/1309.7959 | author:Laurens Bliek category:cs.LG cs.CV math.DS published:2013-09-19 summary:This paper discusses various techniques to let an agent learn how to predictthe effects of its own actions on its sensor data autonomously, and theirusefulness to apply them to visual sensors. An Extreme Learning Machine is usedfor visuomotor prediction, while various autonomous control techniques that canaid the prediction process by balancing exploration and exploitation arediscussed and tested in a simple system: a camera moving over a 2D greyscaleimage.
arxiv-4500-78 | A Comparative Analysis of Ensemble Classifiers: Case Studies in Genomics | http://arxiv.org/abs/1309.5047 | author:Sean Whalen, Gaurav Pandey category:cs.LG q-bio.GN stat.ML published:2013-09-19 summary:The combination of multiple classifiers using ensemble methods isincreasingly important for making progress in a variety of difficult predictionproblems. We present a comparative analysis of several ensemble methods throughtwo case studies in genomics, namely the prediction of genetic interactions andprotein functions, to demonstrate their efficacy on real-world datasets anddraw useful conclusions about their behavior. These methods include simpleaggregation, meta-learning, cluster-based meta-learning, and ensemble selectionusing heterogeneous classifiers trained on resampled data to improve thediversity of their predictions. We present a detailed analysis of these methodsacross 4 genomics datasets and find the best of these methods offerstatistically significant improvements over the state of the art in theirrespective domains. In addition, we establish a novel connection betweenensemble selection and meta-learning, demonstrating how both of these disparatemethods establish a balance between ensemble diversity and performance.
arxiv-4500-79 | Temporal-Difference Learning to Assist Human Decision Making during the Control of an Artificial Limb | http://arxiv.org/abs/1309.4714 | author:Ann L. Edwards, Alexandra Kearney, Michael Rory Dawson, Richard S. Sutton, Patrick M. Pilarski category:cs.AI cs.LG cs.RO published:2013-09-18 summary:In this work we explore the use of reinforcement learning (RL) to help withhuman decision making, combining state-of-the-art RL algorithms with anapplication to prosthetics. Managing human-machine interaction is a problem ofconsiderable scope, and the simplification of human-robot interfaces isespecially important in the domains of biomedical technology and rehabilitationmedicine. For example, amputees who control artificial limbs are often requiredto quickly switch between a number of control actions or modes of operation inorder to operate their devices. We suggest that by learning to anticipate(predict) a user's behaviour, artificial limbs could take on an active role ina human's control decisions so as to reduce the burden on their users.Recently, we showed that RL in the form of general value functions (GVFs) couldbe used to accurately detect a user's control intent prior to their explicitcontrol choices. In the present work, we explore the use of temporal-differencelearning and GVFs to predict when users will switch their control influencebetween the different motor functions of a robot arm. Experiments wereperformed using a multi-function robot arm that was controlled by musclesignals from a user's body (similar to conventional artificial limb control).Our approach was able to acquire and maintain forecasts about a user'sswitching decisions in real time. It also provides an intuitive and reward-freeway for users to correct or reinforce the decisions made by the machinelearning system. We expect that when a system is certain enough about itspredictions, it can begin to take over switching decisions from the user tostreamline control and potentially decrease the time and effort needed tocomplete tasks. This preliminary study therefore suggests a way to naturallyintegrate human- and machine-based decision making systems.
arxiv-4500-80 | A novel approach to nose-tip and eye corners detection using H-K Curvature Analysis in case of 3D images | http://arxiv.org/abs/1309.4582 | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-18 summary:In this paper we present a novel method that combines a HK curvature-basedapproach for three-dimensional (3D) face detection in different poses (X-axis,Y-axis and Z-axis). Salient face features, such as the eyes and nose, aredetected through an analysis of the curvature of the entire facial surface. Allthe experiments have been performed on the FRAV3D Database. After applying theproposed algorithm to the 3D facial surface we have obtained considerably goodresults i.e. on 752 3D face images our method detected the eye corners for 543face images, thus giving a 72.20% of eye corners detection and 743 face imagesfor nose-tip detection thus giving a 98.80% of good nose tip localization
arxiv-4500-81 | Detection of pose orientation across single and multiple axes in case of 3D face images | http://arxiv.org/abs/1309.4577 | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-18 summary:In this paper, we propose a new approach that takes as input a 3D face imageacross X, Y and Z axes as well as both Y and X axes and gives output as itspose i.e. it tells whether the face is oriented with respect the X, Y or Z axesor is it oriented across multiple axes with angles of rotation up to 42 degree.All the experiments have been performed on the FRAV3D, GAVADB and Bosphorusdatabase which has two figures of each individual across multiple axes. Afterapplying the proposed algorithm to the 3D facial surface from FRAV3D on 848 3Dfaces, 566 3D faces were correctly recognized for pose thus giving 67% ofcorrect identification rate. We had experimented on 420 images from the GAVADBdatabase, and only 336 images were detected for correct pose identificationrate i.e. 80% and from Bosphorus database on 560 images only 448 images weredetected for correct pose identification i.e. 80%.abstract goes here.
arxiv-4500-82 | Text segmentation with character-level text embeddings | http://arxiv.org/abs/1309.4628 | author:Grzegorz Chrupa≈Ça category:cs.CL published:2013-09-18 summary:Learning word representations has recently seen much success in computationallinguistics. However, assuming sequences of word tokens as input to linguisticanalysis is often unjustified. For many languages word segmentation is anon-trivial task and naturally occurring text is sometimes a mixture of naturallanguage strings and other character data. We propose to learn textrepresentations directly from raw character sequences by training a Simplerecurrent Network to predict the next character in text. The network uses itshidden layer to evolve abstract representations of the character sequences itsees. To demonstrate the usefulness of the learned text embeddings, we use themas features in a supervised character level text segmentation and labelingtask: recognizing spans of text containing programming language code. By usingthe embeddings as features we are able to substantially improve over a baselinewhich uses only surface character n-grams.
arxiv-4500-83 | Bayesian rules and stochastic models for high accuracy prediction of solar radiation | http://arxiv.org/abs/1309.4999 | author:Cyril Voyant, C. Darras, Marc Muselli, Christophe Paoli, Marie Laure Nivet, Philippe Poggi category:cs.LG stat.AP published:2013-09-18 summary:It is essential to find solar predictive methods to massively insertrenewable energies on the electrical distribution grid. The goal of this studyis to find the best methodology allowing predicting with high accuracy thehourly global radiation. The knowledge of this quantity is essential for thegrid manager or the private PV producer in order to anticipate fluctuationsrelated to clouds occurrences and to stabilize the injected PV power. In thispaper, we test both methodologies: single and hybrid predictors. In the firstclass, we include the multi-layer perceptron (MLP), auto-regressive and movingaverage (ARMA), and persistence models. In the second class, we mix thesepredictors with Bayesian rules to obtain ad-hoc models selections, and Bayesianaverages of outputs related to single models. If MLP and ARMA are equivalent(nRMSE close to 40.5% for the both), this hybridization allows a nRMSE gainupper than 14 percentage points compared to the persistence estimation(nRMSE=37% versus 51%).
arxiv-4500-84 | A novel approach for nose tip detection using smoothing by weighted median filtering applied to 3D face images in variant poses | http://arxiv.org/abs/1309.4573 | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-18 summary:This paper is based on an application of smoothing of 3D face images followedby feature detection i.e. detecting the nose tip. The present method uses aweighted mesh median filtering technique for smoothing. In this presentsmoothing technique we have built the neighborhood surrounding a particularpoint in 3D face and replaced that with the weighted value of the surroundingpoints in 3D face image. After applying the smoothing technique to the 3D faceimages our experimental results show that we have obtained considerableimprovement as compared to the algorithm without smoothing. We have used herethe maximum intensity algorithm for detecting the nose-tip and this methodcorrectly detects the nose-tip in case of any pose i.e. along X, Y, and Z axes.The present technique gave us worked successfully on 535 out of 542 3D faceimages as compared to the method without smoothing which worked only on 521 3Dface images out of 542 face images. Thus we have obtained a 98.70% performancerate over 96.12% performance rate of the algorithm without smoothing. All theexperiments have been performed on the FRAV3D database.
arxiv-4500-85 | Sparsity Based Poisson Denoising with Dictionary Learning | http://arxiv.org/abs/1309.4306 | author:Raja Giryes, Michael Elad category:cs.CV stat.ML published:2013-09-17 summary:The problem of Poisson denoising appears in various imaging applications,such as low-light photography, medical imaging and microscopy. In cases of highSNR, several transformations exist so as to convert the Poisson noise into anadditive i.i.d. Gaussian noise, for which many effective algorithms areavailable. However, in a low SNR regime, these transformations aresignificantly less accurate, and a strategy that relies directly on the truenoise statistics is required. A recent work by Salmon et al. took this route,proposing a patch-based exponential image representation model based on GMM(Gaussian mixture model), leading to state-of-the-art results. In this paper,we propose to harness sparse-representation modeling to the image patches,adopting the same exponential idea. Our scheme uses a greedy pursuit withboot-strapping based stopping condition and dictionary learning within thedenoising process. The reconstruction performance of the proposed scheme iscompetitive with leading methods in high SNR, and achieving state-of-the-artresults in cases of low SNR.
arxiv-4500-86 | Calculation of Entailed Rank Constraints in Partially Non-Linear and Cyclic Models | http://arxiv.org/abs/1309.7004 | author:Peter L. Spirtes category:cs.AI stat.ML published:2013-09-17 summary:The Trek Separation Theorem (Sullivant et al. 2010) states necessary andsufficient conditions for a linear directed acyclic graphical model to entailfor all possible values of its linear coefficients that the rank of varioussub-matrices of the covariance matrix is less than or equal to n, for any givenn. In this paper, I extend the Trek Separation Theorem in two ways: I provethat the same necessary and sufficient conditions apply even when thegenerating model is partially non-linear and contains some cycles. Thisjustifies application of constraint-based causal search algorithms such as theBuildPureClusters algorithm (Silva et al. 2006) for discovering the causalstructure of latent variable models to data generated by a wider class ofcausal models that may contain non-linear and cyclic relations among the latentvariables.
arxiv-4500-87 | Photon counting compressive depth mapping | http://arxiv.org/abs/1309.4385 | author:Gregory A. Howland, Daniel J. Lum, Matthew R. Ware, John C. Howell category:physics.optics cs.CV published:2013-09-17 summary:We demonstrate a compressed sensing, photon counting lidar system based onthe single-pixel camera. Our technique recovers both depth and intensity mapsfrom a single under-sampled set of incoherent, linear projections of a scene ofinterest at ultra-low light levels around 0.5 picowatts. Only two-dimensionalreconstructions are required to image a three-dimensional scene. We demonstrateintensity imaging and depth mapping at 256 x 256 pixel transverse resolutionwith acquisition times as short as 3 seconds. We also show novelty filtering,reconstructing only the difference between two instances of a scene. Finally,we acquire 32 x 32 pixel real-time video for three-dimensional object trackingat 14 frames-per-second.
arxiv-4500-88 | Exploiting Similarities among Languages for Machine Translation | http://arxiv.org/abs/1309.4168 | author:Tomas Mikolov, Quoc V. Le, Ilya Sutskever category:cs.CL published:2013-09-17 summary:Dictionaries and phrase tables are the basis of modern statistical machinetranslation systems. This paper develops a method that can automate the processof generating and extending dictionaries and phrase tables. Our method cantranslate missing word and phrase entries by learning language structures basedon large monolingual data and mapping between languages from small bilingualdata. It uses distributed representation of words and learns a linear mappingbetween vector spaces of languages. Despite its simplicity, our method issurprisingly effective: we can achieve almost 90% precision@5 for translationof words between English and Spanish. This method makes little assumption aboutthe languages, so it can be used to extend and refine dictionaries andtranslation tables for any language pairs.
arxiv-4500-89 | GRED: Graph-Regularized 3D Shape Reconstruction from Highly Anisotropic and Noisy Images | http://arxiv.org/abs/1309.4426 | author:Christian Widmer, Philipp Drewe, Xinghua Lou, Shefali Umrania, Stephanie Heinrich, Gunnar R√§tsch category:cs.CV published:2013-09-17 summary:Analysis of microscopy images can provide insight into many biologicalprocesses. One particularly challenging problem is cell nuclear segmentation inhighly anisotropic and noisy 3D image data. Manually localizing and segmentingeach and every cell nuclei is very time consuming, which remains a bottleneckin large scale biological experiments. In this work we present a tool forautomated segmentation of cell nuclei from 3D fluorescent microscopic data. Ourtool is based on state-of-the-art image processing and machine learningtechniques and supports a friendly graphical user interface (GUI). We show thatour tool is as accurate as manual annotation but greatly reduces the time forthe registration.
arxiv-4500-90 | A Non-Local Means Filter for Removing the Poisson Noise | http://arxiv.org/abs/1309.4151 | author:Qiyu Jin, Ion Grama, Quansheng Liu category:stat.AP cs.CV published:2013-09-17 summary:A new image denoising algorithm to deal with the Poisson noise model isgiven, which is based on the idea of Non-Local Mean. By using the "Oracle"concept, we establish a theorem to show that the Non-Local Means Filter caneffectively deal with Poisson noise with some modification. Under thetheoretical result, we construct our new algorithm called Non-Local MeansPoisson Filter and demonstrate in theory that the filter converges at the usualoptimal rate. The filter is as simple as the classic Non-Local Means and thesimulation results show that our filter is very competitive.
arxiv-4500-91 | On Convergent Finite Difference Schemes for Variational - PDE Based Image Processing | http://arxiv.org/abs/1310.7443 | author:V. B. S. Prasath, Juan C. Moreno category:cs.CV math.NA I.4.3 published:2013-09-16 summary:We study an adaptive anisotropic Huber functional based image restorationscheme. By using a combination of L2-L1 regularization functions, an adaptiveHuber functional based energy minimization model provides denoising with edgepreservation in noisy digital images. We study a convergent finite differencescheme based on continuous piecewise linear functions and use a variablesplitting scheme, namely the Split Bregman, to obtain the discrete minimizer.Experimental results are given in image denoising and comparison with additiveoperator splitting, dual fixed point, and projected gradient schemes illustratethat the best convergence rates are obtained for our algorithm.
arxiv-4500-92 | Attribute-Efficient Evolvability of Linear Functions | http://arxiv.org/abs/1309.4132 | author:Elaine Angelino, Varun Kanade category:cs.LG q-bio.PE published:2013-09-16 summary:In a seminal paper, Valiant (2006) introduced a computational model forevolution to address the question of complexity that can arise throughDarwinian mechanisms. Valiant views evolution as a restricted form ofcomputational learning, where the goal is to evolve a hypothesis that is closeto the ideal function. Feldman (2008) showed that (correlational) statisticalquery learning algorithms could be framed as evolutionary mechanisms inValiant's model. P. Valiant (2012) considered evolvability of real-valuedfunctions and also showed that weak-optimization algorithms that useweak-evaluation oracles could be converted to evolutionary mechanisms. In this work, we focus on the complexity of representations of evolutionarymechanisms. In general, the reductions of Feldman and P. Valiant may result inintermediate representations that are arbitrarily complex (polynomial-sizedcircuits). We argue that biological constraints often dictate that therepresentations have low complexity, such as constant depth and fan-incircuits. We give mechanisms for evolving sparse linear functions under a largeclass of smooth distributions. These evolutionary algorithms areattribute-efficient in the sense that the size of the representations and thenumber of generations required depend only on the sparsity of the targetfunction and the accuracy parameter, but have no dependence on the total numberof attributes.
arxiv-4500-93 | Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel | http://arxiv.org/abs/1309.4111 | author:Tai Qin, Karl Rohe category:stat.ML cs.LG math.ST stat.TH published:2013-09-16 summary:Spectral clustering is a fast and popular algorithm for finding clusters innetworks. Recently, Chaudhuri et al. (2012) and Amini et al.(2012) proposedinspired variations on the algorithm that artificially inflate the node degreesfor improved statistical performance. The current paper extends the previousstatistical estimation results to the more canonical spectral clusteringalgorithm in a way that removes any assumption on the minimum degree andprovides guidance on the choice of the tuning parameter. Moreover, our resultsshow how the "star shape" in the eigenvectors--a common feature of empiricalnetworks--can be explained by the Degree-Corrected Stochastic Blockmodel andthe Extended Planted Partition model, two statistical models that allow forhighly heterogeneous degrees. Throughout, the paper characterizes and justifiesseveral of the variations of the spectral clustering algorithm in terms ofthese models.
arxiv-4500-94 | Learning a Loopy Model For Semantic Segmentation Exactly | http://arxiv.org/abs/1309.4061 | author:Andreas Christian Mueller, Sven Behnke category:cs.LG cs.CV published:2013-09-16 summary:Learning structured models using maximum margin techniques has become anindispensable tool for com- puter vision researchers, as many computer visionapplications can be cast naturally as an image labeling problem. Pixel-based orsuperpixel-based conditional random fields are particularly popular examples.Typ- ically, neighborhood graphs, which contain a large number of cycles, areused. As exact inference in loopy graphs is NP-hard in general, learning thesemodels without approximations is usually deemed infeasible. In this work weshow that, despite the theoretical hardness, it is possible to learn loopymodels exactly in practical applications. To this end, we analyze the use ofmultiple approximate inference techniques together with cutting plane trainingof structural SVMs. We show that our proposed method yields exact solutionswith an optimality guarantees in a computer vision application, for littleadditional computational cost. We also propose a dynamic caching scheme toaccelerate training further, yielding runtimes that are comparable withapproximate methods. We hope that this insight can lead to a reconsideration ofthe tractability of loopy models in computer vision.
arxiv-4500-95 | Why SOV might be initially preferred and then lost or recovered? A theoretical framework | http://arxiv.org/abs/1309.4058 | author:Ramon Ferrer-i-Cancho category:cs.CL nlin.AO physics.soc-ph q-bio.NC published:2013-09-16 summary:Little is known about why SOV order is initially preferred and then discardedor recovered. Here we present a framework for understanding these and manyrelated word order phenomena: the diversity of dominant orders, the existenceof free words orders, the need of alternative word orders and word orderreversions and cycles in evolution. Under that framework, word order isregarded as a multiconstraint satisfaction problem in which at least twoconstraints are in conflict: online memory minimization and maximumpredictability.
arxiv-4500-96 | Domain and Function: A Dual-Space Model of Semantic Relations and Compositions | http://arxiv.org/abs/1309.4035 | author:Peter D. Turney category:cs.CL cs.AI cs.LG published:2013-09-16 summary:Given appropriate representations of the semantic relations between carpenterand wood and between mason and stone (for example, vectors in a vector spacemodel), a suitable algorithm should be able to recognize that these relationsare highly similar (carpenter is to wood as mason is to stone; the relationsare analogous). Likewise, with representations of dog, house, and kennel, analgorithm should be able to recognize that the semantic composition of dog andhouse, dog house, is highly similar to kennel (dog house and kennel aresynonymous). It seems that these two tasks, recognizing relations andcompositions, are closely connected. However, up to now, the best models forrelations are significantly different from the best models for compositions. Inthis paper, we introduce a dual-space model that unifies these two tasks. Thismodel matches the performance of the best previous models for relations andcompositions. The dual-space model consists of a space for measuring domainsimilarity and a space for measuring function similarity. Carpenter and woodshare the same domain, the domain of carpentry. Mason and stone share the samedomain, the domain of masonry. Carpenter and mason share the same function, thefunction of artisans. Wood and stone share the same function, the function ofmaterials. In the composition dog house, kennel has some domain overlap withboth dog and house (the domains of pets and buildings). The function of kennelis similar to the function of house (the function of shelters). By combiningdomain and function similarities in various ways, we can model relations,compositions, and other aspects of semantics.
arxiv-4500-97 | The Cyborg Astrobiologist: Matching of Prior Textures by Image Compression for Geological Mapping and Novelty Detection | http://arxiv.org/abs/1309.4024 | author:P. C. McGuire, A. Bonnici, K. R. Bruner, C. Gross, J. Orm√∂, R. A. Smosna, S. Walter, L. Wendt category:cs.CV astro-ph.EP astro-ph.IM cs.LG published:2013-09-16 summary:(abridged) We describe an image-comparison technique of Heidemann and Ritterthat uses image compression, and is capable of: (i) detecting novel textures ina series of images, as well as of: (ii) alerting the user to the similarity ofa new image to a previously-observed texture. This image-comparison techniquehas been implemented and tested using our Astrobiology Phone-cam system, whichemploys Bluetooth communication to send images to a local laptop server in thefield for the image-compression analysis. We tested the system in a field sitedisplaying a heterogeneous suite of sandstones, limestones, mudstones andcoalbeds. Some of the rocks are partly covered with lichen. The image-matchingprocedure of this system performed very well with data obtained through ourfield test, grouping all images of yellow lichens together and grouping allimages of a coal bed together, and giving a 91% accuracy for similaritydetection. Such similarity detection could be employed to make maps ofdifferent geological units. The novelty-detection performance of our system wasalso rather good (a 64% accuracy). Such novelty detection may become valuablein searching for new geological units, which could be of astrobiologicalinterest. The image-comparison technique is an unsupervised technique that isnot capable of directly classifying an image as containing a particulargeological feature; labeling of such geological features is done post facto byhuman geologists associated with this study, for the purpose of analyzing thesystem's performance. By providing more advanced capabilities for similaritydetection and novelty detection, this image-compression technique could beuseful in giving more scientific autonomy to robotic planetary rovers, and inassisting human astronauts in their geological exploration and assessment.
arxiv-4500-98 | Performance Investigation of Feature Selection Methods | http://arxiv.org/abs/1309.3949 | author:Anuj sharma, Shubhamoy Dey category:cs.IR cs.CL cs.LG published:2013-09-16 summary:Sentiment analysis or opinion mining has become an open research domain afterproliferation of Internet and Web 2.0 social media. People express theirattitudes and opinions on social media including blogs, discussion forums,tweets, etc. and, sentiment analysis concerns about detecting and extractingsentiment or opinion from online text. Sentiment based text classification isdifferent from topical text classification since it involves discriminationbased on expressed opinion on a topic. Feature selection is significant forsentiment analysis as the opinionated text may have high dimensions, which canadversely affect the performance of sentiment analysis classifier. This paperexplores applicability of feature selection methods for sentiment analysis andinvestigates their performance for classification in term of recall, precisionand accuracy. Five feature selection methods (Document Frequency, InformationGain, Gain Ratio, Chi Squared, and Relief-F) and three popular sentimentfeature lexicons (HM, GI and Opinion Lexicon) are investigated on movie reviewscorpus with a size of 2000 documents. The experimental results show thatInformation Gain gave consistent results and Gain Ratio performs overall bestfor sentimental feature selection while sentiment lexicons gave poorperformance. Furthermore, we found that performance of the classifier dependson appropriate number of representative feature selected from text.
arxiv-4500-99 | Using Self-Organizing Maps for Sentiment Analysis | http://arxiv.org/abs/1309.3946 | author:Anuj Sharma, Shubhamoy Dey category:cs.IR cs.CL cs.NE published:2013-09-16 summary:Web 2.0 services have enabled people to express their opinions, experienceand feelings in the form of user-generated content. Sentiment analysis oropinion mining involves identifying, classifying and aggregating opinions asper their positive or negative polarity. This paper investigates the efficacyof different implementations of Self-Organizing Maps (SOM) for sentiment basedvisualization and classification of online reviews. Specifically, this paperimplements the SOM algorithm for both supervised and unsupervised learning fromtext documents. The unsupervised SOM algorithm is implemented for sentimentbased visualization and classification tasks. For supervised sentimentanalysis, a competitive learning algorithm known as Learning VectorQuantization is used. Both algorithms are also compared with their respectivemulti-pass implementations where a quick rough ordering pass is followed by afine tuning pass. The experimental results on the online movie review data setshow that SOMs are well suited for sentiment based classification and sentimentpolarity visualization.
arxiv-4500-100 | A Neural Network based Approach for Predicting Customer Churn in Cellular Network Services | http://arxiv.org/abs/1309.3945 | author:Anuj Sharma, Dr. Prabin Kumar Panigrahi category:cs.NE cs.CE published:2013-09-16 summary:Marketing literature states that it is more costly to engage a new customerthan to retain an existing loyal customer. Churn prediction models aredeveloped by academics and practitioners to effectively manage and controlcustomer churn in order to retain existing customers. As churn management is animportant activity for companies to retain loyal customers, the ability tocorrectly predict customer churn is necessary. As the cellular network servicesmarket becoming more competitive, customer churn management has become acrucial task for mobile communication operators. This paper proposes a neuralnetwork based approach to predict customer churn in subscription of cellularwireless services. The results of experiments indicate that neural networkbased approach can predict customer churn.
arxiv-4500-101 | Sequential Design for Optimal Stopping Problems | http://arxiv.org/abs/1309.3832 | author:Robert B. Gramacy, Mike Ludkovski category:q-fin.CP q-fin.PR stat.ML published:2013-09-16 summary:We propose a new approach to solve optimal stopping problems via simulation.Working within the backward dynamic programming/Snell envelope framework, weaugment the methodology of Longstaff-Schwartz that focuses on approximating thestopping strategy. Namely, we introduce adaptive generation of the stochasticgrids anchoring the simulated sample paths of the underlying state process.This allows for active learning of the classifiers partitioning the state spaceinto the continuation and stopping regions. To this end, we examine sequentialdesign schemes that adaptively place new design points close to the stoppingboundaries. We then discuss dynamic regression algorithms that can implementsuch recursive estimation and local refinement of the classifiers. The newalgorithm is illustrated with a variety of numerical experiments, showing thatan order of magnitude savings in terms of design size can be achieved. We alsocompare with existing benchmarks in the context of pricing multi-dimensionalBermudan options.
arxiv-4500-102 | A Metric-learning based framework for Support Vector Machines and Multiple Kernel Learning | http://arxiv.org/abs/1309.3877 | author:Huyen Do, Alexandros Kalousis category:cs.LG published:2013-09-16 summary:Most metric learning algorithms, as well as Fisher's Discriminant Analysis(FDA), optimize some cost function of different measures of within-andbetween-class distances. On the other hand, Support Vector Machines(SVMs) andseveral Multiple Kernel Learning (MKL) algorithms are based on the SVM largemargin theory. Recently, SVMs have been analyzed from SVM and metric learning,and to develop new algorithms that build on the strengths of each. Inspired bythe metric learning interpretation of SVM, we develop here a newmetric-learning based SVM framework in which we incorporate metric learningconcepts within SVM. We extend the optimization problem of SVM to include somemeasure of the within-class distance and along the way we develop a newwithin-class distance measure which is appropriate for SVM. In addition, weadopt the same approach for MKL and show that it can be also formulated as aMahalanobis metric learning problem. Our end result is a number of SVM/MKLalgorithms that incorporate metric learning concepts. We experiment with themon a set of benchmark datasets and observe important predictive performanceimprovements.
arxiv-4500-103 | Visual-Semantic Scene Understanding by Sharing Labels in a Context Network | http://arxiv.org/abs/1309.3809 | author:Ishani Chakraborty, Ahmed Elgammal category:cs.CV cs.LG stat.ML published:2013-09-16 summary:We consider the problem of naming objects in complex, natural scenescontaining widely varying object appearance and subtly different names.Informed by cognitive research, we propose an approach based on sharing contextbased object hypotheses between visual and lexical spaces. To this end, wepresent the Visual Semantic Integration Model (VSIM) that represents objectlabels as entities shared between semantic and visual contexts and infers a newimage by updating labels through context switching. At the core of VSIM is asemantic Pachinko Allocation Model and a visual nearest neighbor LatentDirichlet Allocation Model. For inference, we derive an iterative DataAugmentation algorithm that pools the label probabilities and maximizes thejoint label posterior of an image. Our model surpasses the performance ofstate-of-art methods in several visual tasks on the challenging SUN09 dataset.
arxiv-4500-104 | Evaluation the efficiency of artificial bee colony and the firefly algorithm in solving the continuous optimization problem | http://arxiv.org/abs/1310.7961 | author:Seyyed Reza Khaze, Isa maleki, Sohrab Hojjatkhah, Ali Bagherinia category:cs.NE cs.AI published:2013-09-16 summary:Now the Meta-Heuristic algorithms have been used vastly in solving theproblem of continuous optimization. In this paper the Artificial Bee Colony(ABC) algorithm and the Firefly Algorithm (FA) are valuated. And for presentingthe efficiency of the algorithms and also for more analysis of them, thecontinuous optimization problems which are of the type of the problems of vastlimit of answer and the close optimized points are tested. So, in this paperthe efficiency of the ABC algorithm and FA are presented for solving thecontinuous optimization problems and also the said algorithms are studied fromthe accuracy in reaching the optimized solution and the resulting time and thereliability of the optimized answer points of view.
arxiv-4500-105 | SEEDS: Superpixels Extracted via Energy-Driven Sampling | http://arxiv.org/abs/1309.3848 | author:Michael Van den Bergh, Xavier Boix, Gemma Roig, Luc Van Gool category:cs.CV published:2013-09-16 summary:Superpixel algorithms aim to over-segment the image by grouping pixels thatbelong to the same object. Many state-of-the-art superpixel algorithms rely onminimizing objective functions to enforce color ho- mogeneity. The optimizationis accomplished by sophis- ticated methods that progressively build thesuperpix- els, typically by adding cuts or growing superpixels. As a result,they are computationally too expensive for real-time applications. We introducea new approach based on a simple hill-climbing optimization. Starting from aninitial superpixel partitioning, it continuously refines the superpixels bymodifying the boundaries. We define a robust and fast to evaluate energyfunction, based on enforcing color similarity between the bound- aries and thesuperpixel color histogram. In a series of experiments, we show that we achievean excellent com- promise between accuracy and efficiency. We are able toachieve a performance comparable to the state-of- the-art, but in real-time ona single Intel i7 CPU at 2.8GHz.
arxiv-4500-106 | Estimation of intrinsic volumes from digital grey-scale images | http://arxiv.org/abs/1309.3842 | author:Anne Marie Svane category:math.ST cs.CV stat.TH 62H35 published:2013-09-16 summary:Local algorithms are common tools for estimating intrinsic volumes fromblack-and-white digital images. However, these algorithms are typically biasedin the design based setting, even when the resolution tends to infinity.Moreover, images recorded in practice are most often blurred grey-scale imagesrather than black-and-white. In this paper, an extended definition of localalgorithms, applying directly to grey-scale images without thresholding, issuggested. We investigate the asymptotics of these new algorithms when theresolution tends to infinity and apply this to construct estimators for surfacearea and integrated mean curvature that are asymptotically unbiased in certainnatural settings.
arxiv-4500-107 | An iterative algorithm for computed tomography image reconstruction from limited-angle projections | http://arxiv.org/abs/1310.7448 | author:Yuli Sun, Jinxu Tao, Conggui Liu category:cs.CV G.1.1 published:2013-09-16 summary:In application of tomography imaging, limited-angle problem is a quitepractical and important issue. In this paper, an iterativereprojection-reconstruction (IRR) algorithm using a modified Papoulis-Gerchberg(PG) iterative scheme is developed for reconstruction from limited-angleprojections which contain noise. The proposed algorithm has two iterativeupdate processes, one is the extrapolation of unknown data, and the other isthe modification of the known noisy observation data. And the algorithmintroduces scaling factors to control the two processes, respectively. Theconvergence of the algorithm is guaranteed, and the method of choosing thescaling factors is given with energy constraints. The simulation resultdemonstrates our conclusions and indicates that the algorithm proposed in thispaper can obviously improve the reconstruction quality.
arxiv-4500-108 | Multiplicative Approximations, Optimal Hypervolume Distributions, and the Choice of the Reference Point | http://arxiv.org/abs/1309.3816 | author:Tobias Friedrich, Frank Neumann, Christian Thyssen category:cs.NE published:2013-09-16 summary:Many optimization problems arising in applications have to consider severalobjective functions at the same time. Evolutionary algorithms seem to be a verynatural choice for dealing with multi-objective problems as the population ofsuch an algorithm can be used to represent the trade-offs with respect to thegiven objective functions. In this paper, we contribute to the theoreticalunderstanding of evolutionary algorithms for multi-objective problems. Weconsider indicator-based algorithms whose goal is to maximize the hypervolumefor a given problem by distributing {\mu} points on the Pareto front. To gainnew theoretical insights into the behavior of hypervolume-based algorithms wecompare their optimization goal to the goal of achieving an optimalmultiplicative approximation ratio. Our studies are carried out for differentPareto front shapes of bi-objective problems. For the class of linear frontsand a class of convex fronts, we prove that maximizing the hypervolume givesthe best possible approximation ratio when assuming that the extreme pointshave to be included in both distributions of the points on the Pareto front.Furthermore, we investigate the choice of the reference point on theapproximation behavior of hypervolume-based approaches and examine Paretofronts of different shapes by numerical calculations.
arxiv-4500-109 | Hierarchical Clustering of Hyperspectral Images using Rank-Two Nonnegative Matrix Factorization | http://arxiv.org/abs/1310.7441 | author:Nicolas Gillis, Da Kuang, Haesun Park category:cs.CV cs.IR math.OC published:2013-09-14 summary:In this paper, we design a hierarchical clustering algorithm forhigh-resolution hyperspectral images. At the core of the algorithm, a newrank-two nonnegative matrix factorizations (NMF) algorithm is used to split theclusters, which is motivated by convex geometry concepts. The method startswith a single cluster containing all pixels, and, at each step, (i) selects acluster in such a way that the error at the next step is minimized, and (ii)splits the selected cluster into two disjoint clusters using rank-two NMF insuch a way that the clusters are well balanced and stable. The proposed methodcan also be used as an endmember extraction algorithm in the presence of purepixels. The effectiveness of this approach is illustrated on several syntheticand real-world hyperspectral images, and shown to outperform standardclustering techniques such as k-means, spherical k-means and standard NMF.
arxiv-4500-110 | Optimized projections for compressed sensing via rank-constrained nearest correlation matrix | http://arxiv.org/abs/1309.3676 | author:Nicolae Cleju category:cs.IT cs.LG math.IT stat.ML published:2013-09-14 summary:Optimizing the acquisition matrix is useful for compressed sensing of signalsthat are sparse in overcomplete dictionaries, because the acquisition matrixcan be adapted to the particular correlations of the dictionary atoms. In thispaper a novel formulation of the optimization problem is proposed, in the formof a rank-constrained nearest correlation matrix problem. Furthermore,improvements for three existing optimization algorithms are introduced, whichare shown to be particular instances of the proposed formulation. Simulationresults show notable improvements and superior robustness in sparse signalrecovery.
arxiv-4500-111 | Local Support Vector Machines:Formulation and Analysis | http://arxiv.org/abs/1309.3699 | author:Ravi Ganti, Alexander Gray category:stat.ML published:2013-09-14 summary:We provide a formulation for Local Support Vector Machines (LSVMs) thatgeneralizes previous formulations, and brings out the explicit connections tolocal polynomial learning used in nonparametric estimation literature. Weinvestigate the simplest type of LSVMs called Local Linear Support VectorMachines (LLSVMs). For the first time we establish conditions under whichLLSVMs make Bayes consistent predictions at each test point $x_0$. We alsoestablish rates at which the local risk of LLSVMs converges to the minimumvalue of expected local risk at each point $x_0$. Using stability arguments weestablish generalization error bounds for LLSVMs.
arxiv-4500-112 | Group Learning and Opinion Diffusion in a Broadcast Network | http://arxiv.org/abs/1309.3697 | author:Yang Liu, Mingyan Liu category:cs.LG published:2013-09-14 summary:We analyze the following group learning problem in the context of opiniondiffusion: Consider a network with $M$ users, each facing $N$ options. In adiscrete time setting, at each time step, each user chooses $K$ out of the $N$options, and receive randomly generated rewards, whose statistics depend on theoptions chosen as well as the user itself, and are unknown to the users. Eachuser aims to maximize their expected total rewards over a certain time horizonthrough an online learning process, i.e., a sequence of exploration (samplingthe return of each option) and exploitation (selecting empirically goodoptions) steps. Within this context we consider two group learning scenarios, (1) users withuniform preferences and (2) users with diverse preferences, and examine how auser should construct its learning process to best extract information fromother's decisions and experiences so as to maximize its own reward. Performanceis measured in {\em weak regret}, the difference between the user's totalreward and the reward from a user-specific best single-action policy (i.e.,always selecting the set of options generating the highest mean rewards forthis user). Within each scenario we also consider two cases: (i) when usersexchange full information, meaning they share the actual rewards they obtainedfrom their choices, and (ii) when users exchange limited information, e.g.,only their choices but not rewards obtained from these choices.
arxiv-4500-113 | A method for nose-tip based 3D face registration using maximum intensity algorithm | http://arxiv.org/abs/1309.3425 | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak kr. Basu category:cs.CV published:2013-09-13 summary:In this paper we present a novel technique of registering 3D images acrosspose. In this context, we have taken into account the images which are alignedacross X, Y and Z axes. We have first determined the angle across which theimage is rotated with respect to X, Y and Z axes and then translation isperformed on the images. After testing the proposed method on 472 images fromthe FRAV3D database, the method correctly registers 358 images thus giving aperformance rate of 75.84%.
arxiv-4500-114 | A Novel Approach in detecting pose orientation of a 3D face required for face | http://arxiv.org/abs/1309.3418 | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-13 summary:In this paper we present a novel approach that takes as input a 3D image andgives as output its pose i.e. it tells whether the face is oriented withrespect the X, Y or Z axes with angles of rotation up to 40 degree. All theexperiments have been performed on the FRAV3D Database. After applying theproposed algorithm to the 3D facial surface we have obtained i.e. on 848 3Dface images our method detected the pose correctly for 566 face images,thusgiving an approximately 67 % of correct pose detection.
arxiv-4500-115 | Mixed Membership Models for Time Series | http://arxiv.org/abs/1309.3533 | author:Emily B. Fox, Michael I. Jordan category:stat.ME cs.LG stat.ML published:2013-09-13 summary:In this article we discuss some of the consequences of the mixed membershipperspective on time series analysis. In its most abstract form, a mixedmembership model aims to associate an individual entity with some set ofattributes based on a collection of observed data. Although much of theliterature on mixed membership models considers the setting in whichexchangeable collections of data are associated with each member of a set ofentities, it is equally natural to consider problems in which an entire timeseries is viewed as an entity and the goal is to characterize the time seriesin terms of a set of underlying dynamic attributes or "dynamic regimes".Indeed, this perspective is already present in the classical hidden Markovmodel, where the dynamic regimes are referred to as "states", and thecollection of states realized in a sample path of the underlying process can beviewed as a mixed membership characterization of the observed time series. Ourgoal here is to review some of the richer modeling possibilities for timeseries that are provided by recent developments in the mixed membershipframework.
arxiv-4500-116 | Partitioning into Expanders | http://arxiv.org/abs/1309.3223 | author:Shayan Oveis Gharan, Luca Trevisan category:cs.DS math.SP stat.ML published:2013-09-12 summary:Let G=(V,E) be an undirected graph, lambda_k be the k-th smallest eigenvalueof the normalized laplacian matrix of G. There is a basic fact in algebraicgraph theory that lambda_k > 0 if and only if G has at most k-1 connectedcomponents. We prove a robust version of this fact. If lambda_k>0, then forsome 1\leq \ell\leq k-1, V can be {\em partitioned} into l sets P_1,\ldots,P_lsuch that each P_i is a low-conductance set in G and induces a high conductanceinduced subgraph. In particular, \phi(P_i)=O(l^3\sqrt{\lambda_l}) and\phi(G[P_i]) >= \lambda_k/k^2). We make our results algorithmic by designing a simple polynomial timespectral algorithm to find such partitioning of G with a quadratic loss in theinside conductance of P_i's. Unlike the recent results on higher orderCheeger's inequality [LOT12,LRTV12], our algorithmic results do not use higherorder eigenfunctions of G. If there is a sufficiently large gap betweenlambda_k and lambda_{k+1}, more precisely, if \lambda_{k+1} >= \poly(k)lambda_{k}^{1/4} then our algorithm finds a k partitioning of V into setsP_1,...,P_k such that the induced subgraph G[P_i] has a significantly largerconductance than the conductance of P_i in G. Such a partitioning may representthe best k clustering of G. Our algorithm is a simple local search that onlyuses the Spectral Partitioning algorithm as a subroutine. We expect to seefurther applications of this simple algorithm in clustering applications.
arxiv-4500-117 | Mapping Mutable Genres in Structurally Complex Volumes | http://arxiv.org/abs/1309.3323 | author:Ted Underwood, Michael L. Black, Loretta Auvil, Boris Capitanu category:cs.CL cs.DL published:2013-09-12 summary:To mine large digital libraries in humanistically meaningful ways, scholarsneed to divide them by genre. This is a task that classification algorithms arewell suited to assist, but they need adjustment to address the specificchallenges of this domain. Digital libraries pose two problems of scale notusually found in the article datasets used to test these algorithms. 1) Becauselibraries span several centuries, the genres being identified may changegradually across the time axis. 2) Because volumes are much longer thanarticles, they tend to be internally heterogeneous, and the classification taskneeds to begin with segmentation. We describe a multi-layered solution thattrains hidden Markov models to segment volumes, and uses ensembles ofoverlapping classifiers to address historical change. We test this approach ona collection of 469,200 volumes drawn from HathiTrust Digital Library. Todemonstrate the humanistic value of these methods, we extract 32,209 volumes offiction from the digital library, and trace the changing proportions of first-and third-person narration in the corpus. We note that narrative points of viewseem to have strong associations with particular themes and genres.
arxiv-4500-118 | Temporal Autoencoding Improves Generative Models of Time Series | http://arxiv.org/abs/1309.3103 | author:Chris H√§usler, Alex Susemihl, Martin P Nawrot, Manfred Opper category:stat.ML cs.LG published:2013-09-12 summary:Restricted Boltzmann Machines (RBMs) are generative models which can learnuseful representations from samples of a dataset in an unsupervised fashion.They have been widely employed as an unsupervised pre-training method inmachine learning. RBMs have been modified to model time series in two mainways: The Temporal RBM stacks a number of RBMs laterally and introducestemporal dependencies between the hidden layer units; The Conditional RBM, onthe other hand, considers past samples of the dataset as a conditional bias andlearns a representation which takes these into account. Here we propose a newtraining method for both the TRBM and the CRBM, which enforces the dynamicstructure of temporal datasets. We do so by treating the temporal models asdenoising autoencoders, considering past frames of the dataset as corruptedversions of the present frame and minimizing the reconstruction error of thepresent data by the model. We call this approach Temporal Autoencoding. Thisleads to a significant improvement in the performance of both models in afilling-in-frames task across a number of datasets. The error reduction formotion capture data is 56\% for the CRBM and 80\% for the TRBM. Taking theposterior mean prediction instead of single samples further improves themodel's estimates, decreasing the error by as much as 91\% for the CRBM onmotion capture data. We also trained the model to perform forecasting on alarge number of datasets and have found TA pretraining to consistently improvethe performance of the forecasts. Furthermore, by looking at the predictionerror across time, we can see that this improvement reflects a betterrepresentation of the dynamics of the data as opposed to a bias towardsreconstructing the observed data on a short time scale.
arxiv-4500-119 | Convex relaxations of structured matrix factorizations | http://arxiv.org/abs/1309.3117 | author:Francis Bach category:cs.LG math.OC published:2013-09-12 summary:We consider the factorization of a rectangular matrix $X $ into a positivelinear combination of rank-one factors of the form $u v^\top$, where $u$ and$v$ belongs to certain sets $\mathcal{U}$ and $\mathcal{V}$, that may encodespecific structures regarding the factors, such as positivity or sparsity. Inthis paper, we show that computing the optimal decomposition is equivalent tocomputing a certain gauge function of $X$ and we provide a detailed analysis ofthese gauge functions and their polars. Since these gauge functions aretypically hard to compute, we present semi-definite relaxations and severalalgorithms that may recover approximate decompositions with approximationguarantees. We illustrate our results with simulations on findingdecompositions with elements in $\{0,1\}$. As side contributions, we present adetailed analysis of variational quadratic representations of norms as well asa new iterative basis pursuit algorithm that can deal with inexact first-orderoracles.
arxiv-4500-120 | Efficient Orthogonal Tensor Decomposition, with an Application to Latent Variable Model Learning | http://arxiv.org/abs/1309.3233 | author:Franz J. Kir√°ly category:stat.ML cs.LG math.ST stat.TH published:2013-09-12 summary:Decomposing tensors into orthogonal factors is a well-known task instatistics, machine learning, and signal processing. We study orthogonal outerproduct decompositions where the factors in the summands in the decompositionare required to be orthogonal across summands, by relating this orthogonaldecomposition to the singular value decompositions of the flattenings. We showthat it is a non-trivial assumption for a tensor to have such an orthogonaldecomposition, and we show that it is unique (up to natural symmetries) in caseit exists, in which case we also demonstrate how it can be efficiently andreliably obtained by a sequence of singular value decompositions. Wedemonstrate how the factoring algorithm can be applied for parameteridentification in latent variable and mixture models.
arxiv-4500-121 | Recovery guarantees for exemplar-based clustering | http://arxiv.org/abs/1309.3256 | author:Abhinav Nellore, Rachel Ward category:stat.ML cs.CV cs.LG published:2013-09-12 summary:For a certain class of distributions, we prove that the linear programmingrelaxation of $k$-medoids clustering---a variant of $k$-means clustering wheremeans are replaced by exemplars from within the dataset---distinguishes pointsdrawn from nonoverlapping balls with high probability once the number of pointsdrawn and the separation distance between any two balls are sufficiently large.Our results hold in the nontrivial regime where the separation distance issmall enough that points drawn from different balls may be closer to each otherthan points drawn from the same ball; in this case, clustering by thresholdingpairwise distances between points can fail. We also exhibit numerical evidenceof high-probability recovery in a substantially more permissive regime.
arxiv-4500-122 | Modeling Based on Elman Wavelet Neural Network for Class-D Power Amplifiers | http://arxiv.org/abs/1309.3214 | author:Li Wang, Jie Shao, Yaqin Zhong, Weisong Zhao, Reza Malekian category:cs.NE published:2013-09-12 summary:In Class-D Power Amplifiers (CDPAs), the power supply noise can intermodulatewith the input signal, manifesting into power-supply induced intermodulationdistortion (PS-IMD) and due to the memory effects of the system, there existasymmetries in the PS-IMDs. In this paper, a new behavioral modeling based onthe Elman Wavelet Neural Network (EWNN) is proposed to study the nonlineardistortion of the CDPAs. In EWNN model, the Morlet wavelet functions areemployed as the activation function and there is a normalized operation in thehidden layer, the modification of the scale factor and translation factor inthe wavelet functions are ignored to avoid the fluctuations of the errorcurves. When there are 30 neurons in the hidden layer, to achieve the samesquare sum error (SSE) $\epsilon_{min}=10^{-3}$, EWNN needs 31 iteration steps,while the basic Elman neural network (BENN) model needs 86 steps. TheVolterra-Laguerre model has 605 parameters to be estimated but still can'tachieve the same magnitude accuracy of EWNN. Simulation results show that theproposed approach of EWNN model has fewer parameters and higher accuracy thanthe Volterra-Laguerre model and its convergence rate is much faster than theBENN model.
arxiv-4500-123 | The Classification Accuracy of Multiple-Metric Learning Algorithm on Multi-Sensor Fusion | http://arxiv.org/abs/1309.3006 | author:Firouz Abdullah Al-Wassai, N. V. Kalyankar category:cs.CV published:2013-09-11 summary:This paper focuses on two main issues; first one is the impact of SimilaritySearch to learning the training sample in metric space, and searching based onsupervised learning classi-fication. In particular, four metrics spacesearching are based on spatial information that are introduced as thefollowing; Cheby-shev Distance (CD); Bray Curtis Distance (BCD); ManhattanDistance (MD) and Euclidean Distance(ED) classifiers. The second issueinvestigates the performance of combination of mul-ti-sensor images on thesupervised learning classification accura-cy. QuickBird multispectral data (MS)and panchromatic data (PAN) have been used in this study to demonstrate theenhance-ment and accuracy assessment of fused image over the original images.The supervised classification results of fusion image generated better than theMS did. QuickBird and the best results with ED classifier than the other did.
arxiv-4500-124 | Decision Trees for Function Evaluation - Simultaneous Optimization of Worst and Expected Cost | http://arxiv.org/abs/1309.2796 | author:Ferdinando Cicalese, Eduardo Laber, Aline Medeiros Saettler category:cs.DS cs.AI cs.LG published:2013-09-11 summary:In several applications of automatic diagnosis and active learning a centralproblem is the evaluation of a discrete function by adaptively querying thevalues of its variables until the values read uniquely determine the value ofthe function. In general, the process of reading the value of a variable mightinvolve some cost, computational or even a fee to be paid for the experimentrequired for obtaining the value. This cost should be taken into account whendeciding the next variable to read. The goal is to design a strategy forevaluating the function incurring little cost (in the worst case or inexpectation according to a prior distribution on the possible variables'assignments). Our algorithm builds a strategy (decision tree) which attains alogarithmic approxima- tion simultaneously for the expected and worst costspent. This is best possible under the assumption that $P \neq NP.$
arxiv-4500-125 | Sparse and Functional Principal Components Analysis | http://arxiv.org/abs/1309.2895 | author:Genevera I. Allen category:stat.ML published:2013-09-11 summary:Regularized principal components analysis, especially Sparse PCA andFunctional PCA, has become widely used for dimension reduction inhigh-dimensional settings. Many examples of massive data, however, may benefitfrom estimating both sparse AND functional factors. These include neuroimagingdata where there are discrete brain regions of activation (sparsity) but theseregions tend to be smooth spatially (functional). Here, we introduce anoptimization framework that can encourage both sparsity and smoothness of therow and/or column PCA factors. This framework generalizes many of the existingapproaches to Sparse PCA, Functional PCA and two-way Sparse PCA and FunctionalPCA, as these are all special cases of our method. In particular, our methodpermits flexible combinations of sparsity and smoothness that lead toimprovements in feature selection and signal recovery as well as moreinterpretable PCA factors. We demonstrate our method on simulated data and aneuroimaging example on EEG data. This work provides a unified framework forregularized PCA that can form the foundation for a cohesive approach toregularization in high-dimensional multivariate analysis.
arxiv-4500-126 | General Purpose Textual Sentiment Analysis and Emotion Detection Tools | http://arxiv.org/abs/1309.2853 | author:Alexandre Denis, Samuel Cruz-Lara, Nadia Bellalem category:cs.CL published:2013-09-11 summary:Textual sentiment analysis and emotion detection consists in retrieving thesentiment or emotion carried by a text or document. This task can be useful inmany domains: opinion mining, prediction, feedbacks, etc. However, building ageneral purpose tool for doing sentiment analysis and emotion detection raisesa number of issues, theoretical issues like the dependence to the domain or tothe language but also pratical issues like the emotion representation forinteroperability. In this paper we present our sentiment/emotion analysistools, the way we propose to circumvent the di culties and the applicationsthey are used for.
arxiv-4500-127 | High-dimensional cluster analysis with the Masked EM Algorithm | http://arxiv.org/abs/1309.2848 | author:Shabnam N. Kadir, Dan F. M. Goodman, Kenneth D. Harris category:q-bio.QM cs.LG q-bio.NC stat.AP published:2013-09-11 summary:Cluster analysis faces two problems in high dimensions: first, the `curse ofdimensionality' that can lead to overfitting and poor generalizationperformance; and second, the sheer time taken for conventional algorithms toprocess large amounts of high-dimensional data. In many applications, only asmall subset of features provide information about the cluster membership ofany one data point, however this informative feature subset may not be the samefor all data points. Here we introduce a `Masked EM' algorithm for fittingmixture of Gaussians models in such cases. We show that the algorithm performsclose to optimally on simulated Gaussian data, and in an application of `spikesorting' of high channel-count neuronal recordings.
arxiv-4500-128 | Enhancements of Multi-class Support Vector Machine Construction from Binary Learners using Generalization Performance | http://arxiv.org/abs/1309.2765 | author:Patoomsiri Songsiri, Thimaporn Phetkaew, Boonserm Kijsirikul category:cs.LG stat.ML published:2013-09-11 summary:We propose several novel methods for enhancing the multi-class SVMs byapplying the generalization performance of binary classifiers as the core idea.This concept will be applied on the existing algorithms, i.e., the DecisionDirected Acyclic Graph (DDAG), the Adaptive Directed Acyclic Graphs (ADAG), andMax Wins. Although in the previous approaches there have been many attempts touse some information such as the margin size and the number of support vectorsas performance estimators for binary SVMs, they may not accurately reflect theactual performance of the binary SVMs. We show that the generalization abilityevaluated via a cross-validation mechanism is more suitable to directly extractthe actual performance of binary SVMs. Our methods are built around thisperformance measure, and each of them is crafted to overcome the weakness ofthe previous algorithm. The proposed methods include the Reordering AdaptiveDirected Acyclic Graph (RADAG), Strong Elimination of the classifiers (SE),Weak Elimination of the classifiers (WE), and Voting based Candidate Filtering(VCF). Experimental results demonstrate that our methods give significantlyhigher accuracy than all of the traditional ones. Especially, WE providessignificantly superior results compared to Max Wins which is recognized as thestate of the art algorithm in terms of both accuracy and classification speedwith two times faster in average.
arxiv-4500-129 | Robust Periocular Recognition By Fusing Sparse Representations of Color and Geometry Information | http://arxiv.org/abs/1309.2752 | author:Juan C. Moreno, V. B. S. Prasath, Gil Santos, Hugo Proenca category:cs.CV published:2013-09-11 summary:In this paper, we propose a re-weighted elastic net (REN) model for biometricrecognition. The new model is applied to data separated into geometric andcolor spatial components. The geometric information is extracted using a fastcartoon - texture decomposition model based on a dual formulation of the totalvariation norm allowing us to carry information about the overall geometry ofimages. Color components are defined using linear and nonlinear color spaces,namely the red-green-blue (RGB), chromaticity-brightness (CB) andhue-saturation-value (HSV). Next, according to a Bayesian fusion-scheme, sparserepresentations for classification purposes are obtained. The scheme isnumerically solved using a gradient projection (GP) algorithm. In the empiricalvalidation of the proposed model, we have chosen the periocular region, whichis an emerging trait known for its robustness against low quality data. Ourresults were obtained in the publicly available UBIRIS.v2 data set and showconsistent improvements in recognition effectiveness when compared to relatedstate-of-the-art techniques.
arxiv-4500-130 | A multi-stream hmm approach to offline handwritten arabic word recognition | http://arxiv.org/abs/1309.2506 | author:Ahlam Maqqor, Akram Halli, Khaled Satori category:cs.CV published:2013-09-10 summary:In This paper we presented new approach for cursive Arabic text recognitionsystem. The objective is to propose methodology analytical offline recognitionof handwritten Arabic for rapid implementation. The first part in the writingrecognition system is the preprocessing phase is the preprocessing phase toprepare the data was introduces and extracts a set of simple statisticalfeatures by two methods : from a window which is sliding long that text linethe right to left and the approach VH2D (consists in projecting every characteron the abscissa, on the ordinate and the diagonals 45{\deg} and 135{\deg}) . Itthen injects the resulting feature vectors to Hidden Markov Model (HMM) andcombined the two HMM by multi-stream approach.
arxiv-4500-131 | Maximizing submodular functions using probabilistic graphical models | http://arxiv.org/abs/1309.2593 | author:K. S. Sesh Kumar, Francis Bach category:cs.LG math.OC published:2013-09-10 summary:We consider the problem of maximizing submodular functions; while thisproblem is known to be NP-hard, several numerically efficient local searchtechniques with approximation guarantees are available. In this paper, wepropose a novel convex relaxation which is based on the relationship betweensubmodular functions, entropies and probabilistic graphical models. In agraphical model, the entropy of the joint distribution decomposes as a sum ofmarginal entropies of subsets of variables; moreover, for any distribution, theentropy of the closest distribution factorizing in the graphical model providesan bound on the entropy. For directed graphical models, this last propertyturns out to be a direct consequence of the submodularity of the entropyfunction, and allows the generalization of graphical-model-based upper boundsto any submodular functions. These upper bounds may then be jointly maximizedwith respect to a set, while minimized with respect to the graph, leading to aconvex variational inference scheme for maximizing submodular functions, basedon outer approximations of the marginal polytope and maximum likelihood boundedtreewidth structures. By considering graphs of increasing treewidths, we maythen explore the trade-off between computational complexity and tightness ofthe relaxation. We also present extensions to constrained problems andmaximizing the difference of submodular functions, which include all possibleset functions.
arxiv-4500-132 | Minimizing Finite Sums with the Stochastic Average Gradient | http://arxiv.org/abs/1309.2388 | author:Mark Schmidt, Nicolas Le Roux, Francis Bach category:math.OC cs.LG stat.CO stat.ML published:2013-09-10 summary:We propose the stochastic average gradient (SAG) method for optimizing thesum of a finite number of smooth convex functions. Like stochastic gradient(SG) methods, the SAG method's iteration cost is independent of the number ofterms in the sum. However, by incorporating a memory of previous gradientvalues the SAG method achieves a faster convergence rate than black-box SGmethods. The convergence rate is improved from O(1/k^{1/2}) to O(1/k) ingeneral, and when the sum is strongly-convex the convergence rate is improvedfrom the sub-linear O(1/k) to a linear convergence rate of the form O(p^k) forp \textless{} 1. Further, in many cases the convergence rate of the new methodis also faster than black-box deterministic gradient methods, in terms of thenumber of gradient evaluations. Numerical experiments indicate that the newalgorithm often dramatically outperforms existing SG and deterministic gradientmethods, and that the performance may be further improved through the use ofnon-uniform sampling strategies.
arxiv-4500-133 | Compressed Sensing for Block-Sparse Smooth Signals | http://arxiv.org/abs/1309.2505 | author:Shahzad Gishkori, Geert Leus category:stat.ML cs.IT math.IT math.ST stat.TH published:2013-09-10 summary:We present reconstruction algorithms for smooth signals with block sparsityfrom their compressed measurements. We tackle the issue of varying group sizevia group-sparse least absolute shrinkage selection operator (LASSO) as well asvia latent group LASSO regularizations. We achieve smoothness in the signal viafusion. We develop low-complexity solvers for our proposed formulations throughthe alternating direction method of multipliers.
arxiv-4500-134 | Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization | http://arxiv.org/abs/1309.2375 | author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG cs.NA stat.CO published:2013-09-10 summary:We introduce a proximal version of the stochastic dual coordinate ascentmethod and show how to accelerate the method using an inner-outer iterationprocedure. We analyze the runtime of the framework and obtain rates thatimprove state-of-the-art results for various key machine learning optimizationproblems including SVM, logistic regression, ridge regression, Lasso, andmulticlass SVM. Experiments validate our theoretical findings.
arxiv-4500-135 | Implementation of nlization framework for verbs, pronouns and determiners with eugene | http://arxiv.org/abs/1309.2471 | author:Harinder Singh, Parteek Kumar category:cs.CL published:2013-09-10 summary:UNL system is designed and implemented by a nonprofit organization, UNDLFoundation at Geneva in 1999. UNL applications are application softwares thatallow end users to accomplish natural language tasks, such as translating,summarizing, retrieving or extracting information, etc. Two major web basedapplication softwares are Interactive ANalyzer (IAN), which is a naturallanguage analysis system. It represents natural language sentences as semanticnetworks in the UNL format. Other application software is dEep-to-sUrfaceGENErator (EUGENE), which is an open-source interactive NLizer. It generatesnatural language sentences out of semantic networks represented in the UNLformat. In this paper, NLization framework with EUGENE is focused, while usingUNL system for accomplishing the task of machine translation. In wholeNLization process, EUGENE takes a UNL input and delivers an output in naturallanguage without any human intervention. It is language-independent and has tobe parametrized to the natural language input through a dictionary and agrammar, provided as separate interpretable files. In this paper, it isexplained that how UNL input is syntactically and semantically analyzed withthe UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronounsand determiners for Punjabi natural language.
arxiv-4500-136 | Exponentially Fast Parameter Estimation in Networks Using Distributed Dual Averaging | http://arxiv.org/abs/1309.2350 | author:Shahin Shahrampour, Ali Jadbabaie category:cs.LG cs.SI math.OC stat.ML published:2013-09-10 summary:In this paper we present an optimization-based view of distributed parameterestimation and observational social learning in networks. Agents receive asequence of random, independent and identically distributed (i.i.d.) signals,each of which individually may not be informative about the underlying truestate, but the signals together are globally informative enough to make thetrue state identifiable. Using an optimization-based characterization ofBayesian learning as proximal stochastic gradient descent (withKullback-Leibler divergence from a prior as a proximal function), we show howto efficiently use a distributed, online variant of Nesterov's dual averagingmethod to solve the estimation with purely local information. When the truestate is globally identifiable, and the network is connected, we prove thatagents eventually learn the true parameter using a randomized gossip scheme. Wedemonstrate that with high probability the convergence is exponentially fastwith a rate dependent on the KL divergence of observations under the true statefrom observations under the second likeliest state. Furthermore, our work alsohighlights the possibility of learning under continuous adaptation of networkwhich is a consequence of employing constant, unit stepsize for the algorithm.
arxiv-4500-137 | Single image super resolution in spatial and wavelet domain | http://arxiv.org/abs/1309.2057 | author:Sapan Naik, Nikunj Patel category:cs.CV published:2013-09-09 summary:Recently single image super resolution is very important research area togenerate high resolution image from given low resolution image. Algorithms ofsingle image resolution are mainly based on wavelet domain and spatial domain.Filters support to model the regularity of natural images is exploited inwavelet domain while edges of images get sharp during up sampling in spatialdomain. Here single image super resolution algorithm is presented which basedon both spatial and wavelet domain and take the advantage of both. Algorithm isiterative and use back projection to minimize reconstruction error. Waveletbased denoising method is also introduced to remove noise.
arxiv-4500-138 | Structure Learning of Probabilistic Logic Programs by Searching the Clause Space | http://arxiv.org/abs/1309.2080 | author:Elena Bellodi, Fabrizio Riguzzi category:cs.LG cs.AI published:2013-09-09 summary:Learning probabilistic logic programming languages is receiving an increasingattention and systems are available for learning the parameters (PRISM,LeProbLog, LFI-ProbLog and EMBLEM) or both the structure and the parameters(SEM-CP-logic and SLIPCASE) of these languages. In this paper we present thealgorithm SLIPCOVER for "Structure LearnIng of Probabilistic logic programs bysearChing OVER the clause space". It performs a beam search in the space ofprobabilistic clauses and a greedy search in the space of theories, using thelog likelihood of the data as the guiding heuristics. To estimate the loglikelihood SLIPCOVER performs Expectation Maximization with EMBLEM. Thealgorithm has been tested on five real world datasets and compared withSLIPCASE, SEM-CP-logic, Aleph and two algorithms for learning Markov LogicNetworks (Learning using Structural Motifs (LSM) and ALEPH++ExactL1). SLIPCOVERachieves higher areas under the precision-recall and ROC curves in most cases.
arxiv-4500-139 | Real-Time and Continuous Hand Gesture Spotting: an Approach Based on Artificial Neural Networks | http://arxiv.org/abs/1309.2084 | author:Pedro Neto, D√°rio Pereira, Norberto Pires, Paulo Moreira category:cs.RO cs.CV published:2013-09-09 summary:New and more natural human-robot interfaces are of crucial interest to theevolution of robotics. This paper addresses continuous and real-time handgesture spotting, i.e., gesture segmentation plus gesture recognition. Gesturepatterns are recognized by using artificial neural networks (ANNs) specificallyadapted to the process of controlling an industrial robot. Since in continuousgesture recognition the communicative gestures appear intermittently with thenoncommunicative, we are proposing a new architecture with two ANNs in seriesto recognize both kinds of gesture. A data glove is used as interfacetechnology. Experimental results demonstrated that the proposed solutionpresents high recognition rates (over 99% for a library of ten gestures andover 96% for a library of thirty gestures), low training and learning time anda good capacity to generalize from particular situations.
arxiv-4500-140 | Application of Artificial Neural Networks in Estimating Participation in Elections | http://arxiv.org/abs/1309.2183 | author:Seyyed Reza Khaze, Mohammad Masdari, Sohrab Hojjatkhah category:cs.NE cs.CY published:2013-09-09 summary:It is approved that artificial neural networks can be considerable effectivein anticipating and analyzing flows in which traditional methods and staticsare not able to solve. in this article, by using two-layer feedforward networkwith tan-sigmoid transmission function in input and output layers, we cananticipate participation rate of public in kohgiloye and boyerahmad province infuture presidential election of islamic republic of iran with 91% accuracy. theassessment standards of participation such as confusion matrix and roc diagramshave been approved our claims.
arxiv-4500-141 | Contour Manifolds and Optimal Transport | http://arxiv.org/abs/1309.2240 | author:Bernhard Schmitzer, Christoph Schn√∂rr category:math.DG cs.CV published:2013-09-09 summary:Describing shapes by suitable measures in object segmentation, as proposed in[24], allows to combine the advantages of the representations as parametrizedcontours and indicator functions. The pseudo-Riemannian structure of optimaltransport can be used to model shapes in ways similar as with contours, whilethe Kantorovich functional enables the application of convex optimizationmethods for global optimality of the segmentation functional. In this paper we provide a mathematical study of the shape measurerepresentation and its relation to the contour description. In particular weshow that the pseudo-Riemannian structure of optimal transport, when restrictedto the set of shape measures, yields a manifold which is diffeomorphic to themanifold of closed contours. A discussion of the metric induced by optimaltransport and the corresponding geodesic equation is given.
arxiv-4500-142 | Spectral Clustering with Imbalanced Data | http://arxiv.org/abs/1309.2303 | author:Jing Qian, Venkatesh Saligrama category:stat.ML published:2013-09-09 summary:Spectral clustering is sensitive to how graphs are constructed from dataparticularly when proximal and imbalanced clusters are present. We show thatRatio-Cut (RCut) or normalized cut (NCut) objectives are not tailored toimbalanced data since they tend to emphasize cut sizes over cut values. Wepropose a graph partitioning problem that seeks minimum cut partitions underminimum size constraints on partitions to deal with imbalanced data. Ourapproach parameterizes a family of graphs, by adaptively modulating nodedegrees on a fixed node set, to yield a set of parameter dependent cutsreflecting varying levels of imbalance. The solution to our problem is thenobtained by optimizing over these parameters. We present rigorous limit cutanalysis results to justify our approach. We demonstrate the superiority of ourmethod through unsupervised and semi-supervised experiments on synthetic andreal data sets.
arxiv-4500-143 | Large-scale optimization with the primal-dual column generation method | http://arxiv.org/abs/1309.2168 | author:Jacek Gondzio, Pablo Gonz√°lez-Brevis, Pedro Munari category:math.OC cs.LG cs.NA published:2013-09-09 summary:The primal-dual column generation method (PDCGM) is a general-purpose columngeneration technique that relies on the primal-dual interior point method tosolve the restricted master problems. The use of this interior point methodvariant allows to obtain suboptimal and well-centered dual solutions whichnaturally stabilizes the column generation. As recently presented in theliterature, reductions in the number of calls to the oracle and in the CPUtimes are typically observed when compared to the standard column generation,which relies on extreme optimal dual solutions. However, these results arebased on relatively small problems obtained from linear relaxations ofcombinatorial applications. In this paper, we investigate the behaviour of thePDCGM in a broader context, namely when solving large-scale convex optimizationproblems. We have selected applications that arise in important real-lifecontexts such as data analysis (multiple kernel learning problem),decision-making under uncertainty (two-stage stochastic programming problems)and telecommunication and transportation networks (multicommodity network flowproblem). In the numerical experiments, we use publicly available benchmarkinstances to compare the performance of the PDCGM against recent results fordifferent methods presented in the literature, which were the best availableresults to date. The analysis of these results suggests that the PDCGM offersan attractive alternative over specialized methods since it remains competitivein terms of number of iterations and CPU times even for large-scaleoptimization problems.
arxiv-4500-144 | The Linearized Bregman Method via Split Feasibility Problems: Analysis and Generalizations | http://arxiv.org/abs/1309.2094 | author:Dirk A. Lorenz, Frank Sch√∂pfer, Stephan Wenger category:math.OC cs.CV cs.NA math.NA published:2013-09-09 summary:The linearized Bregman method is a method to calculate sparse solutions tosystems of linear equations. We formulate this problem as a split feasibilityproblem, propose an algorithmic framework based on Bregman projections andprove a general convergence result for this framework. Convergence of thelinearized Bregman method will be obtained as a special case. Our approach alsoallows for several generalizations such as other objective functions,incremental iterations, incorporation of non-gaussian noise models or boxconstraints.
arxiv-4500-145 | Learning Transformations for Clustering and Classification | http://arxiv.org/abs/1309.2074 | author:Qiang Qiu, Guillermo Sapiro category:cs.CV cs.LG stat.ML published:2013-09-09 summary:A low-rank transformation learning framework for subspace clustering andclassification is here proposed. Many high-dimensional data, such as faceimages and motion sequences, approximately lie in a union of low-dimensionalsubspaces. The corresponding subspace clustering problem has been extensivelystudied in the literature to partition such high-dimensional data into clusterscorresponding to their underlying low-dimensional subspaces. However,low-dimensional intrinsic structures are often violated for real-worldobservations, as they can be corrupted by errors or deviate from ideal models.We propose to address this by learning a linear transformation on subspacesusing matrix rank, via its convex surrogate nuclear norm, as the optimizationcriteria. The learned linear transformation restores a low-rank structure fordata from the same subspace, and, at the same time, forces a a maximallyseparated structure for data from different subspaces. In this way, we reducevariations within subspaces, and increase separation between subspaces for amore robust subspace clustering. This proposed learned robust subspaceclustering framework significantly enhances the performance of existingsubspace clustering methods. Basic theoretical results here presented help tofurther support the underlying framework. To exploit the low-rank structures ofthe transformed subspaces, we further introduce a fast subspace clusteringtechnique, which efficiently combines robust PCA with sparse modeling. Whenclass labels are present at the training stage, we show this low-ranktransformation framework also significantly enhances classificationperformance. Extensive experiments using public datasets are presented, showingthat the proposed approach significantly outperforms state-of-the-art methodsfor subspace clustering and classification.
arxiv-4500-146 | The placement of the head that minimizes online memory: a complex systems approach | http://arxiv.org/abs/1309.1939 | author:Ramon Ferrer-i-Cancho category:cs.CL nlin.AO physics.soc-ph published:2013-09-08 summary:It is well known that the length of a syntactic dependency determines itsonline memory cost. Thus, the problem of the placement of a head and itsdependents (complements or modifiers) that minimizes online memory isequivalent to the problem of the minimum linear arrangement of a star tree.However, how that length is translated into cognitive cost is not known. Thisstudy shows that the online memory cost is minimized when the head is placed atthe center, regardless of the function that transforms length into cost,provided only that this function is strictly monotonically increasing. Onlinememory defines a quasi-convex adaptive landscape with a single central minimumif the number of elements is odd and two central minima if that number is even.We discuss various aspects of the dynamics of word order of subject (S), verb(V) and object (O) from a complex systems perspective and suggest that wordorders tend to evolve by swapping adjacent constituents from an initial orearly SOV configuration that is attracted towards a central word order byonline memory minimization. We also suggest that the stability of SVO is due toat least two factors, the quasi-convex shape of the adaptive landscape in theonline memory dimension and online memory adaptations that avoid regression toSOV. Although OVS is also optimal for placing the verb at the center, its lowfrequency is explained by its long distance to the seminal SOV in thepermutation space.
arxiv-4500-147 | A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries | http://arxiv.org/abs/1309.1952 | author:Alekh Agarwal, Animashree Anandkumar, Praneeth Netrapalli category:stat.ML cs.LG math.OC published:2013-09-08 summary:We consider the problem of learning overcomplete dictionaries in the contextof sparse coding, where each sample selects a sparse subset of dictionaryelements. Our main result is a strategy to approximately recover the unknowndictionary using an efficient algorithm. Our algorithm is a clustering-styleprocedure, where each cluster is used to estimate a dictionary element. Theresulting solution can often be further cleaned up to obtain a high accuracyestimate, and we provide one simple scenario where $\ell_1$-regularizedregression can be used for such a second stage.
arxiv-4500-148 | Variational Bayes Approximations for Clustering via Mixtures of Normal Inverse Gaussian Distributions | http://arxiv.org/abs/1309.1901 | author:Sanjeena Subedi, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2013-09-07 summary:Parameter estimation for model-based clustering using a finite mixture ofnormal inverse Gaussian (NIG) distributions is achieved through variationalBayes approximations. Univariate NIG mixtures and multivariate NIG mixtures areconsidered. The use of variational Bayes approximations here is a substantialdeparture from the traditional EM approach and alleviates some of theassociated computational complexities and uncertainties. Our variationalalgorithm is applied to simulated and real data. The paper concludes withdiscussion and suggestions for future work.
arxiv-4500-149 | A General Two-Step Approach to Learning-Based Hashing | http://arxiv.org/abs/1309.1853 | author:Guosheng Lin, Chunhua Shen, David Suter, Anton van den Hengel category:cs.LG cs.CV published:2013-09-07 summary:Most existing approaches to hashing apply a single form of hash function, andan optimization process which is typically deeply coupled to this specificform. This tight coupling restricts the flexibility of the method to respond tothe data, and can result in complex optimization problems that are difficult tosolve. Here we propose a flexible yet simple framework that is able toaccommodate different types of loss functions and hash functions. Thisframework allows a number of existing approaches to hashing to be placed incontext, and simplifies the development of new problem-specific hashingmethods. Our framework decomposes hashing learning problem into two steps: hashbit learning and hash function learning based on the learned bits. The firststep can typically be formulated as binary quadratic problems, and the secondstep can be accomplished by training standard binary classifiers. Both problemshave been extensively studied in the literature. Our extensive experimentsdemonstrate that the proposed framework is effective, flexible and outperformsthe state-of-the-art.
arxiv-4500-150 | Radar shadow detection in SAR images using DEM and projections | http://arxiv.org/abs/1309.1830 | author:V. B. S. Prasath, O. Haddad category:cs.CV 68U10 I.4.8 published:2013-09-07 summary:Synthetic aperture radar (SAR) images are widely used in target recognitiontasks nowadays. In this letter, we propose an automatic approach for radarshadow detection and extraction from SAR images utilizing geometric projectionsalong with the digital elevation model (DEM) which corresponds to the givengeo-referenced SAR image. First, the DEM is rotated into the radar geometry sothat each row would match that of a radar line of sight. Next, we extract theshadow regions by processing row by row until the image is covered fully. Wetest the proposed shadow detection approach on different DEMs and a simulated1D signals and 2D hills and volleys modeled by various variance based Gaussianfunctions. Experimental results indicate the proposed algorithm produces goodresults in detecting shadows in SAR images with high resolution.
arxiv-4500-151 | Preparing Korean Data for the Shared Task on Parsing Morphologically Rich Languages | http://arxiv.org/abs/1309.1649 | author:Jinho D. Choi category:cs.CL published:2013-09-06 summary:This document gives a brief description of Korean data prepared for the SPMRL2013 shared task. A total of 27,363 sentences with 350,090 tokens are used forthe shared task. All constituent trees are collected from the KAIST Treebankand transformed to the Penn Treebank style. All dependency trees are convertedfrom the transformed constituent trees using heuristics and labeling rules de-signed specifically for the KAIST Treebank. In addition to the gold-standardmorphological analysis provided by the KAIST Treebank, two sets of automaticmorphological analysis are provided for the shared task, one is generated bythe HanNanum morphological analyzer, and the other is generated by the Sejongmorphological analyzer.
arxiv-4500-152 | Nano-scale reservoir computing | http://arxiv.org/abs/1309.1521 | author:Oliver Obst, Adrian Trinchi, Simon G. Hardin, Matthew Chadwick, Ivan Cole, Tim H. Muster, Nigel Hoschke, Diet Ostry, Don Price, Khoa N. Pham, Tim Wark category:cs.ET cs.NE nlin.AO published:2013-09-06 summary:This work describes preliminary steps towards nano-scale reservoir computingusing quantum dots. Our research has focused on the development of anaccumulator-based sensing system that reacts to changes in the environment, aswell as the development of a software simulation. The investigated systemsgenerate nonlinear responses to inputs that make them suitable for a physicalimplementation of a neural network. This development will enableminiaturisation of the neurons to the molecular level, leading to a range ofapplications including monitoring of changes in materials or structures. Thesystem is based around the optical properties of quantum dots. The paper willreport on experimental work on systems using Cadmium Selenide (CdSe) quantumdots and on the various methods to render the systems sensitive to pH, redoxpotential or specific ion concentration. Once the quantum dot-based systems arerendered sensitive to these triggers they can provide a distributed array thatcan monitor and transmit information on changes within the material.
arxiv-4500-153 | Topology preserving thinning for cell complexes | http://arxiv.org/abs/1309.1628 | author:Pawe≈Ç D≈Çotko, Ruben Specogna category:cs.CV published:2013-09-06 summary:A topology preserving skeleton is a synthetic representation of an objectthat retains its topology and many of its significant morphological properties.The process of obtaining the skeleton, referred to as skeletonization orthinning, is a very active research area. It plays a central role in reducingthe amount of information to be processed during image analysis andvisualization, computer-aided diagnosis or by pattern recognition algorithms. This paper introduces a novel topology preserving thinning algorithm whichremoves \textit{simple cells}---a generalization of simple points---of a givencell complex. The test for simple cells is based on \textit{acyclicity tables}automatically produced in advance with homology computations. Using acyclicitytables render the implementation of thinning algorithms straightforward.Moreover, the fact that tables are automatically filled for all possibleconfigurations allows to rigorously prove the generality of the algorithm andto obtain fool-proof implementations. The novel approach enables, for the firsttime, according to our knowledge, to thin a general unstructured simplicialcomplex. Acyclicity tables for cubical and simplicial complexes and an opensource implementation of the thinning algorithm are provided as additionalmaterial to allow their immediate use in the vast number of practicalapplications arising in medical imaging and beyond.
arxiv-4500-154 | Convergence of Nearest Neighbor Pattern Classification with Selective Sampling | http://arxiv.org/abs/1309.1761 | author:Shaun N. Joseph, Seif Omar Abu Bakr, Gabriel Lugo category:cs.LG stat.ML 60G25 F.2.2; G.3 published:2013-09-06 summary:In the panoply of pattern classification techniques, few enjoy the intuitiveappeal and simplicity of the nearest neighbor rule: given a set of samples insome metric domain space whose value under some function is known, we estimatethe function anywhere in the domain by giving the value of the nearest sampleper the metric. More generally, one may use the modal value of the m nearestsamples, where m is a fixed positive integer (although m=1 is known to beadmissible in the sense that no larger value is asymptotically superior interms of prediction error). The nearest neighbor rule is nonparametric andextremely general, requiring in principle only that the domain be a metricspace. The classic paper on the technique, proving convergence underindependent, identically-distributed (iid) sampling, is due to Cover and Hart(1967). Because taking samples is costly, there has been much research inrecent years on selective sampling, in which each sample is selected from apool of candidates ranked by a heuristic; the heuristic tries to guess whichcandidate would be the most "informative" sample. Lindenbaum et al. (2004)apply selective sampling to the nearest neighbor rule, but their approachsacrifices the austere generality of Cover and Hart; furthermore, theirheuristic algorithm is complex and computationally expensive. Here we reportrecent results that enable selective sampling in the original Cover-Hartsetting. Our results pose three selection heuristics and prove that theirnearest neighbor rule predictions converge to the true pattern. Two of thealgorithms are computationally cheap, with complexity growing linearly in thenumber of samples. We believe that these results constitute an importantadvance in the art.
arxiv-4500-155 | A Comparism of the Performance of Supervised and Unsupervised Machine Learning Techniques in evolving Awale/Mancala/Ayo Game Player | http://arxiv.org/abs/1309.1543 | author:O. A. Randle, O. O. Ogunduyile, T. Zuva, N. A. Fashola category:cs.LG cs.GT published:2013-09-06 summary:Awale games have become widely recognized across the world, for theirinnovative strategies and techniques which were used in evolving the agents(player) and have produced interesting results under various conditions. Thispaper will compare the results of the two major machine learning techniques byreviewing their performance when using minimax, endgame database, a combinationof both techniques or other techniques, and will determine which are the besttechniques.
arxiv-4500-156 | Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application | http://arxiv.org/abs/1309.1541 | author:Weiran Wang, Miguel √Å. Carreira-Perpi√±√°n category:cs.LG math.OC stat.ML published:2013-09-06 summary:We provide an elementary proof of a simple, efficient algorithm for computingthe Euclidean projection of a point onto the probability simplex. We also showan application in Laplacian K-modes clustering.
arxiv-4500-157 | Practical Matrix Completion and Corruption Recovery using Proximal Alternating Robust Subspace Minimization | http://arxiv.org/abs/1309.1539 | author:Yu-Xiang Wang, Choon Meng Lee, Loong-Fah Cheong, Kim-Chuan Toh category:cs.CV published:2013-09-06 summary:Low-rank matrix completion is a problem of immense practical importance.Recent works on the subject often use nuclear norm as a convex surrogate of therank function. Despite its solid theoretical foundation, the convex version ofthe problem often fails to work satisfactorily in real-life applications. Realdata often suffer from very few observations, with support not meeting therandom requirements, ubiquitous presence of noise and potentially grosscorruptions, sometimes with these simultaneously occurring. This paper proposes a Proximal Alternating Robust Subspace Minimization(PARSuMi) method to tackle the three problems. The proximal alternating schemeexplicitly exploits the rank constraint on the completed matrix and uses the$\ell_0$ pseudo-norm directly in the corruption recovery step. We show that theproposed method for the non-convex and non-smooth model converges to astationary point. Although it is not guaranteed to find the global optimalsolution, in practice we find that our algorithm can typically arrive at a goodlocal minimizer when it is supplied with a reasonably good starting point basedon convex optimization. Extensive experiments with challenging synthetic andreal data demonstrate that our algorithm succeeds in a much larger range ofpractical problems where convex optimization fails, and it also outperformsvarious state-of-the-art algorithms.
arxiv-4500-158 | Guided Self-Organization of Input-Driven Recurrent Neural Networks | http://arxiv.org/abs/1309.1524 | author:Oliver Obst, Joschka Boedecker category:cs.NE cs.AI nlin.AO published:2013-09-06 summary:We review attempts that have been made towards understanding thecomputational properties and mechanisms of input-driven dynamical systems likeRNNs, and reservoir computing networks in particular. We provide details onmethods that have been developed to give quantitative answers to the questionsabove. Following this, we show how self-organization may be used to improvereservoirs for better performance, in some cases guided by the measurespresented before. We also present a possible way to quantify task performanceusing an information-theoretic approach, and finally discuss promising futuredirections aimed at a better understanding of how these systems perform theircomputations and how to best guide self-organized processes for theiroptimization.
arxiv-4500-159 | Rank-frequency relation for Chinese characters | http://arxiv.org/abs/1309.1536 | author:W. B. Deng, A. E. Allahverdyan, B. Li, Q. A. Wang category:cs.CL published:2013-09-06 summary:We show that the Zipf's law for Chinese characters perfectly holds forsufficiently short texts (few thousand different characters). The scenario ofits validity is similar to the Zipf's law for words in short English texts. Forlong Chinese texts (or for mixtures of short Chinese texts), rank-frequencyrelations for Chinese characters display a two-layer, hierarchic structure thatcombines a Zipfian power-law regime for frequent characters (first layer) withan exponential-like regime for less frequent characters (second layer). Forthese two layers we provide different (though related) theoretical descriptionsthat include the range of low-frequency characters (hapax legomena). Thecomparative analysis of rank-frequency relations for Chinese characters versusEnglish words illustrates the extent to which the characters play for Chinesewriters the same role as the words for those writing within alphabeticalsystems.
arxiv-4500-160 | Accelerating Hessian-free optimization for deep neural networks by implicit preconditioning and sampling | http://arxiv.org/abs/1309.1508 | author:Tara N. Sainath, Lior Horesh, Brian Kingsbury, Aleksandr Y. Aravkin, Bhuvana Ramabhadran category:cs.LG cs.CL cs.NE math.OC stat.ML published:2013-09-05 summary:Hessian-free training has become a popular parallel second or- deroptimization technique for Deep Neural Network training. This study aims atspeeding up Hessian-free training, both by means of decreasing the amount ofdata used for training, as well as through reduction of the number of Krylovsubspace solver iterations used for implicit estimation of the Hessian. In thispaper, we develop an L-BFGS based preconditioning scheme that avoids the needto access the Hessian explicitly. Since L-BFGS cannot be regarded as afixed-point iteration, we further propose the employment of flexible Krylovsubspace solvers that retain the desired theoretical convergence guarantees oftheir conventional counterparts. Second, we propose a new sampling algorithm,which geometrically increases the amount of data utilized for gradient andKrylov subspace iteration calculations. On a 50-hr English Broadcast News task,we find that these methodologies provide roughly a 1.5x speed-up, whereas, on a300-hr Switchboard task, these techniques provide over a 2.3x speedup, with noloss in WER. These results suggest that even further speed-up is expected, asproblems scale and complexity grows.
arxiv-4500-161 | Improvements to deep convolutional neural networks for LVCSR | http://arxiv.org/abs/1309.1501 | author:Tara N. Sainath, Brian Kingsbury, Abdel-rahman Mohamed, George E. Dahl, George Saon, Hagen Soltau, Tomas Beran, Aleksandr Y. Aravkin, Bhuvana Ramabhadran category:cs.LG cs.CL cs.NE math.OC stat.ML published:2013-09-05 summary:Deep Convolutional Neural Networks (CNNs) are more powerful than Deep NeuralNetworks (DNN), as they are able to better reduce spectral variation in theinput signal. This has also been confirmed experimentally, with CNNs showingimprovements in word error rate (WER) between 4-12% relative compared to DNNsacross a variety of LVCSR tasks. In this paper, we describe different methodsto further improve CNN performance. First, we conduct a deep analysis comparinglimited weight sharing and full weight sharing with state-of-the-art features.Second, we apply various pooling strategies that have shown improvements incomputer vision to an LVCSR speech task. Third, we introduce a method toeffectively incorporate speaker adaptation, namely fMLLR, into log-melfeatures. Fourth, we introduce an effective strategy to use dropout duringHessian-free sequence training. We find that with these improvements,particularly with fMLLR and dropout, we are able to achieve an additional 2-3%relative improvement in WER on a 50-hour Broadcast News task over our previousbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%relative improvement over our previous best CNN baseline.
arxiv-4500-162 | A Small Universal Petri Net | http://arxiv.org/abs/1309.1274 | author:Dmitry A. Zaitsev category:cs.FL cs.CC cs.DC cs.NE published:2013-09-05 summary:A universal deterministic inhibitor Petri net with 14 places, 29 transitionsand 138 arcs was constructed via simulation of Neary and Woods' weaklyuniversal Turing machine with 2 states and 4 symbols; the total time complexityis exponential in the running time of their weak machine. To simulate the blankwords of the weakly universal Turing machine, a couple of dedicated transitionsinsert their codes when reaching edges of the working zone. To complete a chainof a given Petri net encoding to be executed by the universal Petri net, atranslation of a bi-tag system into a Turing machine was constructed. Theconstructed Petri net is universal in the standard sense; a weaker form ofuniversality for Petri nets was not introduced in this work.
arxiv-4500-163 | Semistochastic Quadratic Bound Methods | http://arxiv.org/abs/1309.1369 | author:Aleksandr Y. Aravkin, Anna Choromanska, Tony Jebara, Dimitri Kanevsky category:stat.ML cs.LG math.NA stat.CO published:2013-09-05 summary:Partition functions arise in a variety of settings, including conditionalrandom fields, logistic regression, and latent gaussian models. In this paper,we consider semistochastic quadratic bound (SQB) methods for maximum likelihoodinference based on partition function optimization. Batch methods based on thequadratic bound were recently proposed for this class of problems, andperformed favorably in comparison to state-of-the-art techniques.Semistochastic methods fall in between batch algorithms, which use all thedata, and stochastic gradient type methods, which use small random selectionsat each iteration. We build semistochastic quadratic bound-based methods, andprove both global convergence (to a stationary point) under very weakassumptions, and linear convergence rate under stronger assumptions on theobjective. To make the proposed methods faster and more stable, we considerinexact subproblem minimization and batch-size selection schemes. The efficacyof SQB methods is demonstrated via comparison with several state-of-the-arttechniques on commonly used datasets.
arxiv-4500-164 | Noisy Sparse Subspace Clustering | http://arxiv.org/abs/1309.1233 | author:Yu-Xiang Wang, Huan Xu category:stat.ML published:2013-09-05 summary:This paper considers the problem of subspace clustering under noise.Specifically, we study the behavior of Sparse Subspace Clustering (SSC) wheneither adversarial or random noise is added to the unlabelled input datapoints, which are assumed to be in a union of low-dimensional subspaces. Weshow that a modified version of SSC is \emph{provably effective} in correctlyidentifying the underlying subspaces, even with noisy data. This extendstheoretical guarantee of this algorithm to more practical settings and providesjustification to the success of SSC in a class of real applications.
arxiv-4500-165 | Bayesian Structural Inference for Hidden Processes | http://arxiv.org/abs/1309.1392 | author:Christopher C. Strelioff, James P. Crutchfield category:stat.ML cs.LG math.ST nlin.CD stat.TH published:2013-09-05 summary:We introduce a Bayesian approach to discovering patterns in structurallycomplex processes. The proposed method of Bayesian Structural Inference (BSI)relies on a set of candidate unifilar HMM (uHMM) topologies for inference ofprocess structure from a data series. We employ a recently developed exactenumeration of topological epsilon-machines. (A sequel then removes thetopological restriction.) This subset of the uHMM topologies has the addedbenefit that inferred models are guaranteed to be epsilon-machines,irrespective of estimated transition probabilities. Properties ofepsilon-machines and uHMMs allow for the derivation of analytic expressions forestimating transition probabilities, inferring start states, and comparing theposterior probability of candidate model topologies, despite process internalstructure being only indirectly present in data. We demonstrate BSI'seffectiveness in estimating a process's randomness, as reflected by the Shannonentropy rate, and its structure, as quantified by the statistical complexity.We also compare using the posterior distribution over candidate models and thesingle, maximum a posteriori model for point estimation and show that theformer more accurately reflects uncertainty in estimated values. We apply BSIto in-class examples of finite- and infinite-order Markov processes, as well toan out-of-class, infinite-state hidden process.
arxiv-4500-166 | Concentration in unbounded metric spaces and algorithmic stability | http://arxiv.org/abs/1309.1007 | author:Aryeh Kontorovich category:math.PR cs.LG math.FA 60D99 published:2013-09-04 summary:We prove an extension of McDiarmid's inequality for metric spaces withunbounded diameter. To this end, we introduce the notion of the {\emsubgaussian diameter}, which is a distribution-dependent refinement of themetric diameter. Our technique provides an alternative approach to that ofKutin and Niyogi's method of weakly difference-bounded functions, and yieldsnontrivial, dimension-free results in some interesting cases where the formerdoes not. As an application, we give apparently the first generalization boundin the algorithmic stability setting that holds for unbounded loss functions.We furthermore extend our concentration inequality to strongly mixingprocesses.
arxiv-4500-167 | Confidence-constrained joint sparsity recovery under the Poisson noise model | http://arxiv.org/abs/1309.1193 | author:E. Chunikhina, R. Raich, T. Nguyen category:stat.ML cs.LG published:2013-09-04 summary:Our work is focused on the joint sparsity recovery problem where the commonsparsity pattern is corrupted by Poisson noise. We formulate theconfidence-constrained optimization problem in both least squares (LS) andmaximum likelihood (ML) frameworks and study the conditions for perfectreconstruction of the original row sparsity and row sparsity pattern. However,the confidence-constrained optimization problem is non-convex. Using convexrelaxation, an alternative convex reformulation of the problem is proposed. Weevaluate the performance of the proposed approach using simulation results onsynthetic data and show the effectiveness of proposed row sparsity and rowsparsity pattern recovery framework.
arxiv-4500-168 | Efficient binary tomographic reconstruction | http://arxiv.org/abs/1309.0985 | author:Stephane Roux, Hugo Leclerc, Fran√ßois Hild category:cs.CV published:2013-09-04 summary:Tomographic reconstruction of a binary image from few projections isconsidered. A novel {\em heuristic} algorithm is proposed, the central elementof which is a nonlinear transformation $\psi(p)=\log(p/(1-p))$ of theprobability $p$ that a pixel of the sought image be 1-valued. It consists ofbackprojections based on $\psi(p)$ and iterative corrections. Application ofthis algorithm to a series of artificial test cases leads to exact binaryreconstructions, (i.e recovery of the binary image for each single pixel) fromthe knowledge of projection data over a few directions. Images up to $10^6$pixels are reconstructed in a few seconds. A series of test cases is performedfor comparison with previous methods, showing a better efficiency and reducedcomputation times.
arxiv-4500-169 | Minutiae Based Thermal Face Recognition using Blood Perfusion Data | http://arxiv.org/abs/1309.0999 | author:Ayan Seal, Mita Nasipuri, Debotosh Bhattacharjee, Dipak Kumar Basu category:cs.CV published:2013-09-04 summary:This paper describes an efficient approach for human face recognition basedon blood perfusion data from infra-red face images. Blood perfusion data arecharacterized by the regional blood flow in human tissue and therefore do notdepend entirely on surrounding temperature. These data bear a great potentialfor deriving discriminating facial thermogram for better classification andrecognition of face images in comparison to optical image data. Blood perfusiondata are related to distribution of blood vessels under the face skin. Adistribution of blood vessels are unique for each person and as a set ofextracted minutiae points from a blood perfusion data of a human face should beunique for that face. There may be several such minutiae point sets for asingle face but all of these correspond to that particular face only. Entireface image is partitioned into equal blocks and the total number of minutiaepoints from each block is computed to construct final vector. Therefore, thesize of the feature vectors is found to be same as total number of blocksconsidered. For classification, a five layer feed-forward backpropagationneural network has been used. A number of experiments were conducted toevaluate the performance of the proposed face recognition system with varyingblock sizes. Experiments have been performed on the database created at our ownlaboratory. The maximum success of 91.47% recognition has been achieved withblock size 8X8.
arxiv-4500-170 | Some Options for L1-Subspace Signal Processing | http://arxiv.org/abs/1309.1194 | author:Panos P. Markopoulos, George N. Karystinos, Dimitris A. Pados category:stat.ML published:2013-09-04 summary:We describe ways to define and calculate $L_1$-norm signal subspaces whichare less sensitive to outlying data than $L_2$-calculated subspaces. We focuson the computation of the $L_1$ maximum-projection principal component of adata matrix containing N signal samples of dimension D and conclude that thegeneral problem is formally NP-hard in asymptotically large N, D. We prove,however, that the case of engineering interest of fixed dimension D andasymptotically large sample support N is not and we present an optimalalgorithm of complexity $O(N^D)$. We generalize to multiple$L_1$-max-projection components and present an explicit optimal $L_1$ subspacecalculation algorithm in the form of matrix nuclear-norm evaluations. Weconclude with illustrations of $L_1$-subspace signal processing in the fieldsof data dimensionality reduction and direction-of-arrival estimation.
arxiv-4500-171 | Analysing Quality of English-Hindi Machine Translation Engine Outputs Using Bayesian Classification | http://arxiv.org/abs/1309.1129 | author:Rashmi Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-09-04 summary:This paper considers the problem for estimating the quality of machinetranslation outputs which are independent of human intervention and aregenerally addressed using machine learning techniques.There are variousmeasures through which a machine learns translations quality. AutomaticEvaluation metrics produce good co-relation at corpus level but cannot producethe same results at the same segment or sentence level. In this paper 16features are extracted from the input sentences and their translations and aquality score is obtained based on Bayesian inference produced from trainingdata.
arxiv-4500-172 | Learning to answer questions | http://arxiv.org/abs/1309.1125 | author:Ana Cristina Mendes, Lu√≠sa Coheur, S√©rgio Curto category:cs.CL published:2013-09-04 summary:We present an open-domain Question-Answering system that learns to answerquestions based on successful past interactions. We follow a pattern-basedapproach to Answer-Extraction, where (lexico-syntactic) patterns that relate aquestion to its answer are automatically learned and used to answer futurequestions. Results show that our approach contributes to the system's bestperformance when it is conjugated with typical Answer-Extraction strategies.Moreover, it allows the system to learn with the answered questions and torectify wrong or unsolved past questions.
arxiv-4500-173 | Boosting in Location Space | http://arxiv.org/abs/1309.1080 | author:Damian Eads, David Helmbold, Ed Rosten category:cs.CV published:2013-09-04 summary:The goal of object detection is to find objects in an image. An objectdetector accepts an image and produces a list of locations as $(x,y)$ pairs.Here we introduce a new concept: {\bf location-based boosting}. Location-basedboosting differs from previous boosting algorithms because it optimizes a newspatial loss function to combine object detectors, each of which may havemarginal performance, into a single, more accurate object detector. Astructured representation of object locations as a list of $(x,y)$ pairs is amore natural domain for object detection than the spatially unstructuredrepresentation produced by classifiers. Furthermore, this formulation allows usto take advantage of the intuition that large areas of the background areuninteresting and it is not worth expending computational effort on them. Thisresults in a more scalable algorithm because it does not need to take measuresto prevent the background data from swamping the foreground data such assubsampling or applying an ad-hoc weighting to the pixels. We first present thetheory of location-based boosting, and then motivate it with empirical resultson a challenging data set.
arxiv-4500-174 | Thermal Human face recognition based on Haar wavelet transform and series matching technique | http://arxiv.org/abs/1309.1156 | author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak kr. Basu category:cs.CV published:2013-09-04 summary:Thermal infrared (IR) images represent the heat patterns emitted from hotobject and they do not consider the energies reflected from an object. Objectsliving or non-living emit different amounts of IR energy according to theirbody temperature and characteristics. Humans are homoeothermic and hencecapable of maintaining constant temperature under different surroundingtemperature. Face recognition from thermal (IR) images should focus on changesof temperature on facial blood vessels. These temperature changes can beregarded as texture features of images and wavelet transform is a very goodtool to analyze multi-scale and multi-directional texture. Wavelet transform isalso used for image dimensionality reduction, by removing redundancies andpreserving original features of the image. The sizes of the facial images arenormally large. So, the wavelet transform is used before image similarity ismeasured. Therefore this paper describes an efficient approach of human facerecognition based on wavelet transform from thermal IR images. The systemconsists of three steps. At the very first step, human thermal IR face image ispreprocessed and the face region is only cropped from the entire image.Secondly, Haar wavelet is used to extract low frequency band from the croppedface region. Lastly, the image classification between the training images andthe test images is done, which is based on low-frequency components. Theproposed approach is tested on a number of human thermal infrared face imagescreated at our own laboratory and Terravic Facial IR Database. Experimentalresults indicated that the thermal infra red face images can be recognized bythe proposed system effectively. The maximum success of 95% recognition hasbeen achieved.
arxiv-4500-175 | Minutiae Based Thermal Human Face Recognition using Label Connected Component Algorithm | http://arxiv.org/abs/1309.1155 | author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-04 summary:In this paper, a thermal infra red face recognition system for humanidentification and verification using blood perfusion data and back propagationfeed forward neural network is proposed. The system consists of three steps. Atthe very first step face region is cropped from the colour 24-bit input images.Secondly face features are extracted from the croped region, which will betaken as the input of the back propagation feed forward neural network in thethird step and classification and recognition is carried out. The proposedapproaches are tested on a number of human thermal infra red face imagescreated at our own laboratory. Experimental results reveal the higher degreeperformance
arxiv-4500-176 | Advances in the Logical Representation of Lexical Semantics | http://arxiv.org/abs/1309.1014 | author:Bruno Mery, Christian Retor√© category:cs.CL published:2013-09-04 summary:The integration of lexical semantics and pragmatics in the analysis of themeaning of natural lan- guage has prompted changes to the global frameworkderived from Montague. In those works, the original lexicon, in which wordswere assigned an atomic type of a single-sorted logic, has been re- placed by aset of many-facetted lexical items that can compose their meaning with salientcontextual properties using a rich typing system as a guide. Having related ourproposal for such an expanded framework \LambdaTYn, we present some recentadvances in the logical formalisms associated, including constraints on lexicaltransformations and polymorphic quantifiers, and ongoing discussions andresearch on the granularity of the type system and the limits of transitivity.
arxiv-4500-177 | A Comparative Study of Human thermal face recognition based on Haar wavelet transform (HWT) and Local Binary Pattern (LBP) | http://arxiv.org/abs/1309.1009 | author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2013-09-04 summary:Thermal infra-red (IR) images focus on changes of temperature distribution onfacial muscles and blood vessels. These temperature changes can be regarded astexture features of images. A comparative study of face recognition methodsworking in thermal spectrum is carried out in this paper. In these study twolocal-matching methods based on Haar wavelet transform and Local Binary Pattern(LBP) are analyzed. Wavelet transform is a good tool to analyze multi-scale,multi-direction changes of texture. Local binary patterns (LBP) are a type offeature used for classification in computer vision. Firstly, human thermal IRface image is preprocessed and cropped the face region only from the entireimage. Secondly, two different approaches are used to extract the features fromthe cropped face region. In the first approach, the training images and thetest images are processed with Haar wavelet transform and the LL band and theaverage of LH/HL/HH bands sub-images are created for each face image. Then atotal confidence matrix is formed for each face image by taking a weighted sumof the corresponding pixel values of the LL band and average band. For LBPfeature extraction, each of the face images in training and test datasets isdivided into 161 numbers of sub images, each of size 8X8 pixels. For each suchsub images, LBP features are extracted which are concatenated in row wisemanner. PCA is performed separately on the individual feature set fordimensionality reeducation. Finally two different classifiers are used toclassify face images. One such classifier multi-layer feed forward neuralnetwork and another classifier is minimum distance classifier. The Experimentshave been performed on the database created at our own laboratory and TerravicFacial IR Database.
arxiv-4500-178 | Automated Thermal Face recognition based on Minutiae Extraction | http://arxiv.org/abs/1309.1000 | author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kr. Basu category:cs.CV published:2013-09-04 summary:In this paper an efficient approach for human face recognition based on theuse of minutiae points in thermal face image is proposed. The thermogram ofhuman face is captured by thermal infra-red camera. Image processing methodsare used to pre-process the captured thermogram, from which differentphysiological features based on blood perfusion data are extracted. Bloodperfusion data are related to distribution of blood vessels under the faceskin. In the present work, three different methods have been used to get theblood perfusion image, namely bit-plane slicing and medial axis transform,morphological erosion and medial axis transform, sobel edge operators.Distribution of blood vessels is unique for each person and a set of extractedminutiae points from a blood perfusion data of a human face should be uniquefor that face. Two different methods are discussed for extracting minutiaepoints from blood perfusion data. For extraction of features entire face imageis partitioned into equal size blocks and the total number of minutiae pointsfrom each block is computed to construct final feature vector. Therefore, thesize of the feature vectors is found to be same as total number of blocksconsidered. A five layer feed-forward back propagation neural network is usedas the classification tool. A number of experiments were conducted to evaluatethe performance of the proposed face recognition methodologies with varyingblock size on the database created at our own laboratory. It has been foundthat the first method supercedes the other two producing an accuracy of 97.62%with block size 16X16 for bit-plane 4.
arxiv-4500-179 | Online Tensor Methods for Learning Latent Variable Models | http://arxiv.org/abs/1309.0787 | author:Furong Huang, U. N. Niranjan, Mohammad Umar Hakeem, Animashree Anandkumar category:cs.LG cs.DC cs.SI stat.ML published:2013-09-03 summary:We introduce an online tensor decomposition based approach for two latentvariable modeling problems namely, (1) community detection, in which we learnthe latent communities that the social actors in social networks belong to, and(2) topic modeling, in which we infer hidden topics of text articles. Weconsider decomposition of moment tensors using stochastic gradient descent. Weconduct optimization of multilinear operations in SGD and avoid directlyforming the tensors, to save computational and storage costs. We presentoptimized algorithm in two platforms. Our GPU-based implementation exploits theparallelism of SIMD architectures to allow for maximum speed-up by a carefuloptimization of storage and data transfer, whereas our CPU-based implementationuses efficient sparse matrix computations and is suitable for large sparsedatasets. For the community detection problem, we demonstrate accuracy andcomputational efficiency on Facebook, Yelp and DBLP datasets, and for the topicmodeling problem, we also demonstrate good performance on the New York Timesdataset. We compare our results to the state-of-the-art algorithms such as thevariational method, and report a gain of accuracy and a gain of several ordersof magnitude in the execution time.
arxiv-4500-180 | SKYNET: an efficient and robust neural network training tool for machine learning in astronomy | http://arxiv.org/abs/1309.0790 | author:Philip Graff, Farhan Feroz, Michael P. Hobson, Anthony N. Lasenby category:astro-ph.IM cs.LG cs.NE stat.ML published:2013-09-03 summary:We present the first public release of our generic neural network trainingalgorithm, called SkyNet. This efficient and robust machine learning tool isable to train large and deep feed-forward neural networks, includingautoencoders, for use in a wide range of supervised and unsupervised learningapplications, such as regression, classification, density estimation,clustering and dimensionality reduction. SkyNet uses a `pre-training' method toobtain a set of network parameters that has empirically been shown to be closeto a good solution, followed by further optimisation using a regularisedvariant of Newton's method, where the level of regularisation is determined andadjusted automatically; the latter uses second-order derivative information toimprove convergence, but without the need to evaluate or store the full Hessianmatrix, by using a fast approximate method to calculate Hessian-vectorproducts. This combination of methods allows for the training of complicatednetworks that are difficult to optimise using standard backpropagationtechniques. SkyNet employs convergence criteria that naturally preventoverfitting, and also includes a fast algorithm for estimating the accuracy ofnetwork outputs. The utility and flexibility of SkyNet are demonstrated byapplication to a number of toy problems, and to astronomical problems focusingon the recovery of structure from blurred and noisy images, the identificationof gamma-ray bursters, and the compression and denoising of galaxy images. TheSkyNet software, which is implemented in standard ANSI C and fully parallelisedusing MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/.
arxiv-4500-181 | Understanding Evolutionary Potential in Virtual CPU Instruction Set Architectures | http://arxiv.org/abs/1309.0719 | author:David M. Bryson, Charles Ofria category:cs.NE published:2013-09-03 summary:We investigate fundamental decisions in the design of instruction setarchitectures for linear genetic programs that are used as both model systemsin evolutionary biology and underlying solution representations in evolutionarycomputation. We subjected digital organisms with each tested architecture toseven different computational environments designed to present a range ofevolutionary challenges. Our goal was to engineer a general purposearchitecture that would be effective under a broad range of evolutionaryconditions. We evaluated six different types of architectural features for thevirtual CPUs: (1) genetic flexibility: we allowed digital organisms to moreprecisely modify the function of genetic instructions, (2) memory: we providedan increased number of registers in the virtual CPUs, (3) decoupled sensors andactuators: we separated input and output operations to enable greater controlover data flow. We also tested a variety of methods to regulate expression: (4)explicit labels that allow programs to dynamically refer to specific genomepositions, (5) position-relative search instructions, and (6) multiple new flowcontrol instructions, including conditionals and jumps. Each of these featuresalso adds complication to the instruction set and risks slowing evolution dueto epistatic interactions. Two features (multiple argument specification andseparated I/O) demonstrated substantial improvements int the majority of testenvironments. Some of the remaining tested modifications were detrimental,thought most exhibit no systematic effects on evolutionary potential,highlighting the robustness of digital evolution. Combined, these observationsenhance our understanding of how instruction architecture impacts evolutionarypotential, enabling the creation of architectures that support more rapidevolution of complex solutions to a broad range of challenges.
arxiv-4500-182 | On the Robustness of Temporal Properties for Stochastic Models | http://arxiv.org/abs/1309.0866 | author:Ezio Bartocci, Luca Bortolussi, Laura Nenzi, Guido Sanguinetti category:cs.LO cs.AI cs.LG cs.SY published:2013-09-03 summary:Stochastic models such as Continuous-Time Markov Chains (CTMC) and StochasticHybrid Automata (SHA) are powerful formalisms to model and to reason about thedynamics of biological systems, due to their ability to capture thestochasticity inherent in biological processes. A classical question in formalmodelling with clear relevance to biological modelling is the model checkingproblem. i.e. calculate the probability that a behaviour, expressed forinstance in terms of a certain temporal logic formula, may occur in a givenstochastic process. However, one may not only be interested in the notion ofsatisfiability, but also in the capacity of a system to mantain a particularemergent behaviour unaffected by the perturbations, caused e.g. from extrinsicnoise, or by possible small changes in the model parameters. To address thisissue, researchers from the verification community have recently proposedseveral notions of robustness for temporal logic providing suitable definitionsof distance between a trajectory of a (deterministic) dynamical system and theboundaries of the set of trajectories satisfying the property of interest. Thecontributions of this paper are twofold. First, we extend the notion ofrobustness to stochastic systems, showing that this naturally leads to adistribution of robustness scores. By discussing two examples, we show how toapproximate the distribution of the robustness score and its key indicators:the average robustness and the conditional average robustness. Secondly, weshow how to combine these indicators with the satisfaction probability toaddress the system design problem, where the goal is to optimize some controlparameters of a stochastic model in order to best maximize robustness of thedesired specifications.
arxiv-4500-183 | BayesOpt: A Library for Bayesian optimization with Robotics Applications | http://arxiv.org/abs/1309.0671 | author:Ruben Martinez-Cantin category:cs.RO cs.AI cs.LG cs.MS published:2013-09-03 summary:The purpose of this paper is twofold. On one side, we present a generalframework for Bayesian optimization and we compare it with some related fieldsin active learning and Bayesian numerical analysis. On the other hand, Bayesianoptimization and related problems (bandits, sequential experimental design) arehighly dependent on the surrogate model that is selected. However, there is noclear standard in the literature. Thus, we present a fast and flexible toolboxthat allows to test and combine different models and criteria with littleeffort. It includes most of the state-of-the-art contributions, algorithms andmodels. Its speed also removes part of the stigma that Bayesian optimizationmethods are only good for "expensive functions". The software is free and itcan be used in many operating systems and computer languages.
arxiv-4500-184 | Relative Comparison Kernel Learning with Auxiliary Kernels | http://arxiv.org/abs/1309.0489 | author:Eric Heim, Hamed Valizadegan, Milos Hauskrecht category:cs.LG published:2013-09-02 summary:In this work we consider the problem of learning a positive semidefinitekernel matrix from relative comparisons of the form: "object A is more similarto object B than it is to C", where comparisons are given by humans. Existingsolutions to this problem assume many comparisons are provided to learn a highquality kernel. However, this can be considered unrealistic for many real-worldtasks since relative assessments require human input, which is often costly ordifficult to obtain. Because of this, only a limited number of thesecomparisons may be provided. In this work, we explore methods for aiding theprocess of learning a kernel with the help of auxiliary kernels built from moreeasily extractable information regarding the relationships among objects. Wepropose a new kernel learning approach in which the target kernel is defined asa conic combination of auxiliary kernels and a kernel whose elements arelearned directly. We formulate a convex optimization to solve for this targetkernel that adds only minor overhead to methods that use no auxiliaryinformation. Empirical results show that in the presence of few trainingrelative comparisons, our method can learn kernels that generalize to moreout-of-sample comparisons than methods that do not utilize auxiliaryinformation, as well as similar methods that learn metrics over objects.
arxiv-4500-185 | Parallel machine scheduling with step deteriorating jobs and setup times by a hybrid discrete cuckoo search algorithm | http://arxiv.org/abs/1309.1453 | author:Peng Guo, Wenming Cheng, Yi Wang category:math.OC cs.DS cs.NE published:2013-09-02 summary:This article considers the parallel machine scheduling problem withstep-deteriorating jobs and sequence-dependent setup times. The objective is tominimize the total tardiness by determining the allocation and sequence of jobson identical parallel machines. In this problem, the processing time of eachjob is a step function dependent upon its starting time. An individual extendedtime is penalized when the starting time of a job is later than a specificdeterioration date. The possibility of deterioration of a job makes theparallel machine scheduling problem more challenging than ordinary ones. Amixed integer programming model for the optimal solution is derived. Due to itsNP-hard nature, a hybrid discrete cuckoo search algorithm is proposed to solvethis problem. In order to generate a good initial swarm, a modified heuristicnamed the MBHG is incorporated into the initialization of population. Severaldiscrete operators are proposed in the random walk of L\'{e}vy Flights and thecrossover search. Moreover, a local search procedure based on variableneighborhood descent is integrated into the algorithm as a hybrid strategy inorder to improve the quality of elite solutions. Computational experiments areexecuted on two sets of randomly generated test instances. The results showthat the proposed hybrid algorithm can yield better solutions in comparisonwith the commercial solver CPLEX with one hour time limit, discrete cuckoosearch algorithm and the existing variable neighborhood search algorithm.
arxiv-4500-186 | Scalable Probabilistic Entity-Topic Modeling | http://arxiv.org/abs/1309.0337 | author:Neil Houlsby, Massimiliano Ciaramita category:stat.ML cs.IR cs.LG published:2013-09-02 summary:We present an LDA approach to entity disambiguation. Each topic is associatedwith a Wikipedia article and topics generate either content words or entitymentions. Training such models is challenging because of the topic andvocabulary size, both in the millions. We tackle these problems using a noveldistributed inference and representation framework based on a parallel Gibbssampler guided by the Wikipedia link graph, and pipelines of MapReduce allowingfast and memory-frugal processing of large datasets. We report state-of-the-artperformance on a public dataset.
arxiv-4500-187 | Tagging Scientific Publications using Wikipedia and Natural Language Processing Tools. Comparison on the ArXiv Dataset | http://arxiv.org/abs/1309.0326 | author:Micha≈Ç ≈Åopuszy≈Ñski, ≈Åukasz Bolikowski category:cs.CL cs.DL published:2013-09-02 summary:In this work, we compare two simple methods of tagging scientificpublications with labels reflecting their content. As a first source of labelsWikipedia is employed, second label set is constructed from the noun phrasesoccurring in the analyzed corpus. We examine the statistical properties and theeffectiveness of both approaches on the dataset consisting of abstracts from0.7 million of scientific documents deposited in the ArXiv preprint collection.We believe that obtained tags can be later on applied as useful documentfeatures in various machine learning tasks (document similarity, clustering,topic modelling, etc.).
arxiv-4500-188 | A Study on Unsupervised Dictionary Learning and Feature Encoding for Action Classification | http://arxiv.org/abs/1309.0309 | author:Xiaojiang Peng, Qiang Peng, Yu Qiao, Junzhou Chen, Mehtab Afzal category:cs.CV published:2013-09-02 summary:Many efforts have been devoted to develop alternative methods to traditionalvector quantization in image domain such as sparse coding and soft-assignment.These approaches can be split into a dictionary learning phase and a featureencoding phase which are often closely connected. In this paper, we investigatethe effects of these phases by separating them for video-based actionclassification. We compare several dictionary learning methods and featureencoding schemes through extensive experiments on KTH and HMDB51 datasets.Experimental results indicate that sparse coding performs consistently betterthan the other encoding methods in large complex dataset (i.e., HMDB51), and itis robust to different dictionaries. For small simple dataset (i.e., KTH) withless variation, however, all the encoding strategies perform competitively. Inaddition, we note that the strength of sophisticated encoding approaches comesnot from their corresponding dictionaries but the encoding mechanisms, and wecan just use randomly selected exemplars as dictionaries for video-based actionclassification.
arxiv-4500-189 | Unmixing Incoherent Structures of Big Data by Randomized or Greedy Decomposition | http://arxiv.org/abs/1309.0302 | author:Tianyi Zhou, Dacheng Tao category:stat.ML cs.DS cs.LG published:2013-09-02 summary:Learning big data by matrix decomposition always suffers from expensivecomputation, mixing of complicated structures and noise. In this paper, westudy more adaptive models and efficient algorithms that decompose a datamatrix as the sum of semantic components with incoherent structures. We firstlyintroduce "GO decomposition (GoDec)", an alternating projection methodestimating the low-rank part $L$ and the sparse part $S$ from data matrix$X=L+S+G$ corrupted by noise $G$. Two acceleration strategies are proposed toobtain scalable unmixing algorithm on big data: 1) Bilateral random projection(BRP) is developed to speed up the update of $L$ in GoDec by a closed-formbuilt from left and right random projections of $X-S$ in lower dimensions; 2)Greedy bilateral (GreB) paradigm updates the left and right factors of $L$ in amutually adaptive and greedy incremental manner, and achieve significantimprovement in both time and sample complexities. Then we proposes threenontrivial variants of GoDec that generalizes GoDec to more general data typeand whose fast algorithms can be derived from the two strategies......
arxiv-4500-190 | Learning to Rank for Blind Image Quality Assessment | http://arxiv.org/abs/1309.0213 | author:Fei Gao, Dacheng Tao, Xinbo Gao, Xuelong Li category:cs.CV published:2013-09-01 summary:Blind image quality assessment (BIQA) aims to predict perceptual imagequality scores without access to reference images. State-of-the-art BIQAmethods typically require subjects to score a large number of images to train arobust model. However, the acquisition of image quality scores has severallimitations: 1) scores are not precise, because subjects are usually uncertainabout which score most precisely represents the perceptual quality of a givenimage; 2) subjective judgements of quality may be biased by image content; 3)the quality scales between different distortion categories are inconsistent;and 4) it is challenging to obtain a large scale database, or to extendexisting databases, because of the inconvenience of collecting sufficientimages, training the subjects, conducting subjective experiments, andrealigning human quality evaluations. To combat these limitations, this paperexplores and exploits preference image pairs such as "the quality of image Iais better than that of image Ib" for training a robust BIQA model. Thepreference label, representing the relative quality of two images, is generallyprecise and consistent, and is not sensitive to image content, distortion type,or subject identity; such PIPs can be generated at very low cost. The proposedBIQA method is one of learning to rank. We first formulate the problem oflearning the mapping from the image features to the preference label as one ofclassification. In particular, we investigate the utilization of a multiplekernel learning algorithm based on group lasso (MKLGL) to provide a solution. Asimple but effective strategy to estimate perceptual image quality scores isthen presented. Experiments show that the proposed BIQA method is highlyeffective and achieves comparable performance to state-of-the-art BIQAalgorithms. Moreover, the proposed method can be easily extended to newdistortion categories.
arxiv-4500-191 | Demodulation of Sparse PPM Signals with Low Samples Using Trained RIP Matrix | http://arxiv.org/abs/1309.5854 | author:Seyed Hossein Hosseini, Mahrokh G. Shayesteh, Mehdi Chehel Amirani category:cs.OH cs.IT cs.LG math.IT published:2013-09-01 summary:Compressed sensing (CS) theory considers the restricted isometry property(RIP) as a sufficient condition for measurement matrix which guarantees therecovery of any sparse signal from its compressed measurements. The RIPcondition also preserves enough information for classification of sparsesymbols, even with fewer measurements. In this work, we utilize RIP bound asthe cost function for training a simple neural network in order to exploit thenear optimal measurements or equivalently near optimal features forclassification of a known set of sparse symbols. As an example, we considerdemodulation of pulse position modulation (PPM) signals. The results indicatethat the proposed method has much better performance than the randommeasurements and requires less samples than the optimum matched filterdemodulator, at the expense of some performance loss. Further, the proposedapproach does not need equalizer for multipath channels in contrast to theconventional receiver.
arxiv-4500-192 | Multi-Column Deep Neural Networks for Offline Handwritten Chinese Character Classification | http://arxiv.org/abs/1309.0261 | author:Dan Cire≈üan, J√ºrgen Schmidhuber category:cs.CV published:2013-09-01 summary:Our Multi-Column Deep Neural Networks achieve best known recognition rates onChinese characters from the ICDAR 2011 and 2013 offline handwritingcompetitions, approaching human performance.
arxiv-4500-193 | High-Accuracy Total Variation for Compressed Video Sensing | http://arxiv.org/abs/1309.0270 | author:Mahdi S. Hosseini, Konstantinos N. Plataniotis category:math.OC cs.CV published:2013-09-01 summary:Numerous total variation (TV) regularizers, engaged in image restorationproblem, encode the gradients by means of simple $[-1,1]$ FIR filter. Despiteits low computational processing, this filter severely deviates signal's highfrequency components pertinent to edge/discontinuous information and causeseveral deficiency issues known as texture and geometric loss. This paperaddresses this problem by proposing an alternative model to the TVregularization problem via high order accuracy differential FIR filters topreserve rapid transitions in signal recovery. A numerical encoding scheme isdesigned to extend the TV model into multidimensional representation (tensorialdecomposition). We adopt this design to regulate the spatial and temporalredundancy in compressed video sensing problem to jointly recover frames fromunder-sampled measurements. We then seek the solution via alternating directionmethods of multipliers and find a unique solution to quadratic minimizationstep with capability of handling different boundary conditions. The resultingalgorithm uses much lower sampling rate and highly outperforms alternativestate-of-the-art methods. This is evaluated both in terms of restorationaccuracy and visual quality of the recovered frames.
arxiv-4500-194 | Ensemble approaches for improving community detection methods | http://arxiv.org/abs/1309.0242 | author:Johan Dahlin, Pontus Svenson category:physics.soc-ph cs.LG cs.SI stat.ML published:2013-09-01 summary:Statistical estimates can often be improved by fusion of data from severaldifferent sources. One example is so-called ensemble methods which have beensuccessfully applied in areas such as machine learning for classification andclustering. In this paper, we present an ensemble method to improve communitydetection by aggregating the information found in an ensemble of communitystructures. This ensemble can found by re-sampling methods, multiple runs of astochastic community detection method, or by several different communitydetection algorithms applied to the same network. The proposed method isevaluated using random networks with community structures and compared with twocommonly used community detection methods. The proposed method when applied ona stochastic community detection algorithm performs well with low computationalcomplexity, thus offering both a new approach to community detection and anadditional community detection method.
arxiv-4500-195 | API design for machine learning software: experiences from the scikit-learn project | http://arxiv.org/abs/1309.0238 | author:Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake Vanderplas, Arnaud Joly, Brian Holt, Ga√´l Varoquaux category:cs.LG cs.MS published:2013-09-01 summary:Scikit-learn is an increasingly popular machine learning li- brary. Writtenin Python, it is designed to be simple and efficient, accessible tonon-experts, and reusable in various contexts. In this paper, we present anddiscuss our design choices for the application programming interface (API) ofthe project. In particular, we describe the simple and elegant interface sharedby all learning and processing units in the library and then discuss itsadvantages in terms of composition and reusability. The paper also comments onimplementation details specific to the Python ecosystem and analyzes obstaclesfaced by users and developers of the library.
arxiv-4500-196 | Non-Asymptotic Convergence Analysis of Inexact Gradient Methods for Machine Learning Without Strong Convexity | http://arxiv.org/abs/1309.0113 | author:Anthony Man-Cho So category:math.OC cs.LG published:2013-08-31 summary:Many recent applications in machine learning and data fitting call for thealgorithmic solution of structured smooth convex optimization problems.Although the gradient descent method is a natural choice for this task, itrequires exact gradient computations and hence can be inefficient when theproblem size is large or the gradient is difficult to evaluate. Therefore,there has been much interest in inexact gradient methods (IGMs), in which anefficiently computable approximate gradient is used to perform the update ineach iteration. Currently, non-asymptotic linear convergence results for IGMsare typically established under the assumption that the objective function isstrongly convex, which is not satisfied in many applications of interest; whilelinear convergence results that do not require the strong convexity assumptionare usually asymptotic in nature. In this paper, we combine the best of thesetwo types of results and establish---under the standard assumption that thegradient approximation errors decrease linearly to zero---the non-asymptoticlinear convergence of IGMs when applied to a class of structured convexoptimization problems. Such a class covers settings where the objectivefunction is not necessarily strongly convex and includes the least squares andlogistic regression problems. We believe that our techniques will find furtherapplications in the non-asymptotic convergence analysis of other first-ordermethods.
arxiv-4500-197 | A Robust Alternating Direction Method for Constrained Hybrid Variational Deblurring Model | http://arxiv.org/abs/1309.0123 | author:Ryan Wen Liu, Tian Xu category:cs.CV 65K10, 68U10 I.4.4; G.1.6 published:2013-08-31 summary:In this work, a new constrained hybrid variational deblurring model isdeveloped by combining the non-convex first- and second-order total variationregularizers. Moreover, a box constraint is imposed on the proposed model toguarantee high deblurring performance. The developed constrained hybridvariational model could achieve a good balance between preserving image detailsand alleviating ringing artifacts. In what follows, we present thecorresponding numerical solution by employing an iteratively reweightedalgorithm based on alternating direction method of multipliers. Theexperimental results demonstrate the superior performance of the proposedmethod in terms of quantitative and qualitative image quality assessments.
arxiv-4500-198 | Concentration Inequalities for Bounded Random Vectors | http://arxiv.org/abs/1309.0003 | author:Xinjia Chen category:math.PR cs.LG math.ST stat.TH published:2013-08-30 summary:We derive simple concentration inequalities for bounded random vectors, whichgeneralize Hoeffding's inequalities for bounded scalar random variables. Asapplications, we apply the general results to multinomial and Dirichletdistributions to obtain multivariate concentration inequalities.
arxiv-4500-199 | Separable Approximations and Decomposition Methods for the Augmented Lagrangian | http://arxiv.org/abs/1308.6774 | author:Rachael Tappenden, Peter Richtarik, Burak Buke category:math.OC cs.DC cs.NA stat.ML published:2013-08-30 summary:In this paper we study decomposition methods based on separableapproximations for minimizing the augmented Lagrangian. In particular, we studyand compare the Diagonal Quadratic Approximation Method (DQAM) of Mulvey andRuszczy\'{n}ski and the Parallel Coordinate Descent Method (PCDM) ofRicht\'arik and Tak\'a\v{c}. We show that the two methods are equivalent forfeasibility problems up to the selection of a single step-size parameter.Furthermore, we prove an improved complexity bound for PCDM under strongconvexity, and show that this bound is at least $8(L'/\bar{L})(\omega-1)^2$times better than the best known bound for DQAM, where $\omega$ is the degreeof partial separability and $L'$ and $\bar{L}$ are the maximum and average ofthe block Lipschitz constants of the gradient of the quadratic penaltyappearing in the augmented Lagrangian.
arxiv-4500-200 | Discriminative Parameter Estimation for Random Walks Segmentation | http://arxiv.org/abs/1308.6721 | author:Pierre-Yves Baudin, Danny Goodman, Puneet Kumar, Noura Azzabou, Pierre G. Carlier, Nikos Paragios, M. Pawan Kumar category:cs.CV cs.LG published:2013-08-30 summary:The Random Walks (RW) algorithm is one of the most e - cient and easy-to-useprobabilistic segmentation methods. By combining contrast terms with priorterms, it provides accurate segmentations of medical images in a fullyautomated manner. However, one of the main drawbacks of using the RW algorithmis that its parameters have to be hand-tuned. we propose a novel discriminativelearning framework that estimates the parameters using a training dataset. Themain challenge we face is that the training samples are not fully supervised.Speci cally, they provide a hard segmentation of the images, instead of aproba- bilistic segmentation. We overcome this challenge by treating the opti-mal probabilistic segmentation that is compatible with the given hardsegmentation as a latent variable. This allows us to employ the latent supportvector machine formulation for parameter estimation. We show that our approachsigni cantly outperforms the baseline methods on a challenging datasetconsisting of real clinical 3D MRI volumes of skeletal muscles.
arxiv-4500-201 | Image Set based Collaborative Representation for Face Recognition | http://arxiv.org/abs/1308.6687 | author:Pengfei Zhu, Wangmeng Zuo, Lei Zhang, Simon C. K. Shiu, David Zhang category:cs.CV published:2013-08-30 summary:With the rapid development of digital imaging and communication technologies,image set based face recognition (ISFR) is becoming increasingly important. Onekey issue of ISFR is how to effectively and efficiently represent the queryface image set by using the gallery face image sets. The set-to-set distancebased methods ignore the relationship between gallery sets, while representingthe query set images individually over the gallery sets ignores the correlationbetween query set images. In this paper, we propose a novel image set basedcollaborative representation and classification method for ISFR. By modelingthe query set as a convex or regularized hull, we represent this hullcollaboratively over all the gallery sets. With the resolved representationcoefficients, the distance between the query set and each gallery set can thenbe calculated for classification. The proposed model naturally and effectivelyextends the image based collaborative representation to an image set based one,and our extensive experiments on benchmark ISFR databases show the superiorityof the proposed method to state-of-the-art ISFR methods under different setsizes in terms of both recognition rate and efficiency.
arxiv-4500-202 | Inconsistency of Pitman-Yor process mixtures for the number of components | http://arxiv.org/abs/1309.0024 | author:Jeffrey W. Miller, Matthew T. Harrison category:math.ST stat.ML stat.TH published:2013-08-30 summary:In many applications, a finite mixture is a natural model, but it can bedifficult to choose an appropriate number of components. To circumvent thischoice, investigators are increasingly turning to Dirichlet process mixtures(DPMs), and Pitman-Yor process mixtures (PYMs), more generally. While thesemodels may be well-suited for Bayesian density estimation, many investigatorsare using them for inferences about the number of components, by consideringthe posterior on the number of components represented in the observed data. Weshow that this posterior is not consistent --- that is, on data from a finitemixture, it does not concentrate at the true number of components. This resultapplies to a large class of nonparametric mixtures, including DPMs and PYMs,over a wide variety of families of component distributions, includingessentially all discrete families, as well as continuous exponential familiessatisfying mild regularity conditions (such as multivariate Gaussians).
arxiv-4500-203 | Online Ranking: Discrete Choice, Spearman Correlation and Other Feedback | http://arxiv.org/abs/1308.6797 | author:Nir Ailon category:cs.LG cs.GT stat.ML published:2013-08-30 summary:Given a set $V$ of $n$ objects, an online ranking system outputs at each timestep a full ranking of the set, observes a feedback of some form and suffers aloss. We study the setting in which the (adversarial) feedback is an element in$V$, and the loss is the position (0th, 1st, 2nd...) of the item in theoutputted ranking. More generally, we study a setting in which the feedback isa subset $U$ of at most $k$ elements in $V$, and the loss is the sum of thepositions of those elements. We present an algorithm of expected regret $O(n^{3/2}\sqrt{Tk})$ over a timehorizon of $T$ steps with respect to the best single ranking in hindsight. Thisimproves previous algorithms and analyses either by a factor of either$\Omega(\sqrt{k})$, a factor of $\Omega(\sqrt{\log n})$ or by improving runningtime from quadratic to $O(n\log n)$ per round. We also prove a matching lowerbound. Our techniques also imply an improved regret bound for online rankaggregation over the Spearman correlation measure, and to other more complexranking loss functions.
arxiv-4500-204 | A Low-Dimensional Representation for Robust Partial Isometric Correspondences Computation | http://arxiv.org/abs/1308.6804 | author:Alan Brunton, Michael Wand, Stefanie Wuhrer, Hans-Peter Seidel, Tino Weinkauf category:cs.CV cs.GR published:2013-08-30 summary:Intrinsic isometric shape matching has become the standard approach for poseinvariant correspondence estimation among deformable shapes. Most existingapproaches assume global consistency, i.e., the metric structure of the wholemanifold must not change significantly. While global isometric matching is wellunderstood, only a few heuristic solutions are known for partial matching.Partial matching is particularly important for robustness to topological noise(incomplete data and contacts), which is a common problem in real-world 3Dscanner data. In this paper, we introduce a new approach to partial, intrinsicisometric matching. Our method is based on the observation that isometries arefully determined by purely local information: a map of a single point and itstangent space fixes an isometry for both global and the partial maps. From thisidea, we develop a new representation for partial isometric maps based onequivalence classes of correspondences between pairs of points and theirtangent spaces. From this, we derive a local propagation algorithm that findsuch mappings efficiently. In contrast to previous heuristics based on RANSACor expectation maximization, our method is based on a simple and soundtheoretical model and fully deterministic. We apply our approach to registerpartial point clouds and compare it to the state-of-the-art methods, where weobtain significant improvements over global methods for real-world data andstronger guarantees than previous heuristic partial matching algorithms.
arxiv-4500-205 | A New Algorithm of Speckle Filtering using Stochastic Distances | http://arxiv.org/abs/1308.6487 | author:Leonardo Torres, Tamer Cavalcante, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML published:2013-08-29 summary:This paper presents a new approach for filter design based on stochasticdistances and tests between distributions. A window is defined around eachpixel, overlapping samples are compared and only those which pass agoodness-of-fit test are used to compute the filtered value. The technique isapplied to intensity SAR data with homogeneous regions using the Gamma model.The proposal is compared with the Lee's filter using a protocol based on MonteCarlo. Among the criteria used to quantify the quality of filters, we employthe equivalent number of looks, line and edge preservation. Moreover, we alsoassessed the filters by the Universal Image Quality Index and the Pearson'scorrelation on edges regions.
arxiv-4500-206 | Universal Approximation Using Shuffled Linear Models | http://arxiv.org/abs/1308.6498 | author:Laurens Bliek category:math.DS cs.NE published:2013-08-29 summary:This paper proposes a specific type of Local Linear Model, the ShuffledLinear Model (SLM), that can be used as a universal approximator. Localoperating points are chosen randomly and linear models are used to approximatea function or system around these points. The model can also be interpreted asan extension to Extreme Learning Machines with Radial Basis Function nodes, oras a specific way of using Takagi-Sugeno fuzzy models. Using the availabletheory of Extreme Learning Machines, universal approximation of the SLM and anupper bound on the number of models are proved mathematically, and an efficientalgorithm is proposed.
arxiv-4500-207 | A Synergistic Approach for Recovering Occlusion-Free Textured 3D Maps of Urban Facades from Heterogeneous Cartographic Data | http://arxiv.org/abs/1308.6401 | author:Karim Hammoudi, Fadi Dornaika, Bahman Soheilian, Bruno Vallet, John McDonald, Nicolas Paparoditis category:cs.CV I.4; I.5 published:2013-08-29 summary:In this paper we present a practical approach for generating anocclusion-free textured 3D map of urban facades by the synergistic use ofterrestrial images, 3D point clouds and area-based information. Particularly indense urban environments, the high presence of urban objects in front of thefacades causes significant difficulties for several stages in computationalbuilding modeling. Major challenges lie on the one hand in extracting complete3D facade quadrilateral delimitations and on the other hand in generatingocclusion-free facade textures. For these reasons, we describe astraightforward approach for completing and recovering facade geometry andtextures by exploiting the data complementarity of terrestrial multi-sourceimagery and area-based information.
arxiv-4500-208 | Linear and Parallel Learning of Markov Random Fields | http://arxiv.org/abs/1308.6342 | author:Yariv Dror Mizrahi, Misha Denil, Nando de Freitas category:stat.ML cs.LG published:2013-08-29 summary:We introduce a new embarrassingly parallel parameter learning algorithm forMarkov random fields with untied parameters which is efficient for a largeclass of practical models. Our algorithm parallelizes naturally over cliquesand, for graphs of bounded degree, its complexity is linear in the number ofcliques. Unlike its competitors, our algorithm is fully parallel and forlog-linear models it is also data efficient, requiring only the localsufficient statistics of the data to estimate parameters.
arxiv-4500-209 | Joint Video and Text Parsing for Understanding Events and Answering Queries | http://arxiv.org/abs/1308.6628 | author:Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, Song-Chun Zhu category:cs.CV cs.CL cs.MM published:2013-08-29 summary:We propose a framework for parsing video and text jointly for understandingevents and answering user queries. Our framework produces a parse graph thatrepresents the compositional structures of spatial information (objects andscenes), temporal information (actions and events) and causal information(causalities between events and fluents) in the video and text. The knowledgerepresentation of our framework is based on a spatial-temporal-causal And-Orgraph (S/T/C-AOG), which jointly models possible hierarchical compositions ofobjects, scenes and events as well as their interactions and mutual contexts,and specifies the prior probabilistic distribution of the parse graphs. Wepresent a probabilistic generative model for joint parsing that captures therelations between the input video/text, their corresponding parse graphs andthe joint parse graph. Based on the probabilistic model, we propose a jointparsing system consisting of three modules: video parsing, text parsing andjoint inference. Video parsing and text parsing produce two parse graphs fromthe input video and text respectively. The joint inference module produces ajoint parse graph by performing matching, deduction and revision on the videoand text parse graphs. The proposed framework has the following objectives:Firstly, we aim at deep semantic parsing of video and text that goes beyond thetraditional bag-of-words approaches; Secondly, we perform parsing and reasoningacross the spatial, temporal and causal dimensions based on the joint S/T/C-AOGrepresentation; Thirdly, we show that deep joint parsing facilitates subsequentapplications such as generating narrative text descriptions and answeringqueries in the forms of who, what, when, where and why. We empiricallyevaluated our system based on comparison against ground-truth as well asaccuracy of query answering and obtained satisfactory results.
arxiv-4500-210 | GNCGCP - Graduated NonConvexity and Graduated Concavity Procedure | http://arxiv.org/abs/1308.6388 | author:Zhi-Yong Liu, Hong Qiao category:cs.CV published:2013-08-29 summary:In this paper we propose the Graduated NonConvexity and Graduated ConcavityProcedure (GNCGCP) as a general optimization framework to approximately solvethe combinatorial optimization problems on the set of partial permutationmatrices. GNCGCP comprises two sub-procedures, graduated nonconvexity (GNC)which realizes a convex relaxation and graduated concavity (GC) which realizesa concave relaxation. It is proved that GNCGCP realizes exactly a type ofconvex-concave relaxation procedure (CCRP), but with a much simpler formulationwithout needing convex or concave relaxation in an explicit way. Actually,GNCGCP involves only the gradient of the objective function and is thereforevery easy to use in practical applications. Two typical NP-hard problems,(sub)graph matching and quadratic assignment problem (QAP), are employed todemonstrate its simplicity and state-of-the-art performance.
arxiv-4500-211 | Collecting Coupons with Random Initial Stake | http://arxiv.org/abs/1308.6384 | author:Benjamin Doerr, Carola Doerr category:cs.DM cs.DS cs.NE F.2.2 published:2013-08-29 summary:Motivated by a problem in the theory of randomized search heuristics, we givea very precise analysis for the coupon collector problem where the collectorstarts with a random set of coupons (chosen uniformly from all sets). We show that the expected number of rounds until we have a coupon of eachtype is $nH_{n/2} - 1/2 \pm o(1)$, where $H_{n/2}$ denotes the $(n/2)$thharmonic number when $n$ is even, and $H_{n/2}:= (1/2) H_{\lfloor n/2 \rfloor}+ (1/2) H_{\lceil n/2 \rceil}$ when $n$ is odd. Consequently, the couponcollector with random initial stake is by half a round faster than the onestarting with exactly $n/2$ coupons (apart from additive $o(1)$ terms). This result implies that classic simple heuristic called \emph{randomizedlocal search} needs an expected number of $nH_{n/2} - 1/2 \pm o(1)$ iterationsto find the optimum of any monotonic function defined on bit-strings of length$n$.
arxiv-4500-212 | Learning-Based Procedural Content Generation | http://arxiv.org/abs/1308.6415 | author:Jonathan Roberts, Ke Chen category:cs.AI cs.HC cs.LG cs.NE published:2013-08-29 summary:Procedural content generation (PCG) has recently become one of the hottesttopics in computational intelligence and AI game researches. Among a variety ofPCG techniques, search-based approaches overwhelmingly dominate PCG developmentat present. While SBPCG leads to promising results and successful applications,it poses a number of challenges ranging from representation to evaluation ofthe content being generated. In this paper, we present an alternative yetgeneric PCG framework, named learning-based procedure content generation(LBPCG), to provide potential solutions to several challenging problems inexisting PCG techniques. By exploring and exploiting information gained in gamedevelopment and public beta test via data-driven learning, our framework cangenerate robust content adaptable to end-user or target players on-line withminimal interruption to their experience. Furthermore, we develop enablingtechniques to implement the various models required in our framework. For aproof of concept, we have developed a prototype based on the classic opensource first-person shooter game, Quake. Simulation results suggest that ourframework is promising in generating quality content.
arxiv-4500-213 | Categorizing ancient documents | http://arxiv.org/abs/1308.6311 | author:Nizar Zaghden, Remy Mullot, Mohamed Adel Alimi category:cs.CV published:2013-08-28 summary:The analysis of historical documents is still a topical issue given theimportance of information that can be extracted and also the importance givenby the institutions to preserve their heritage. The main idea in order tocharacterize the content of the images of ancient documents after attempting toclean the image is segmented blocks texts from the same image and tries to findsimilar blocks in either the same image or the entire image database. Mostapproaches of offline handwriting recognition proceed by segmenting words intosmaller pieces (usually characters) which are recognized separately.Recognition of a word then requires the recognition of all characters (OCR)that compose it. Our work focuses mainly on the characterization of classes inimages of old documents. We use Som toolbox for finding classes in documents.We applied also fractal dimensions and points of interest to categorize andmatch ancient documents.
arxiv-4500-214 | Text recognition in both ancient and cartographic documents | http://arxiv.org/abs/1308.6309 | author:Nizar Zaghden, Badreddine Khelifi, Adel M. Alimi, Remy Mullot category:cs.CV published:2013-08-28 summary:This paper deals with the recognition and matching of text in bothcartographic maps and ancient documents. The purpose of this work is to findsimilar text regions based on statistical and global features. A phase ofnormalization is done first, in object to well categorize the same quantity ofinformation. A phase of wordspotting is done next by combining local and globalfeatures. We make different experiments by combining the different techniquesof extracting features in order to obtain better results in recognition phase.We applied fontspotting on both ancient documents and cartographic ones. Wealso applied the wordspotting in which we adopted a new technique which triesto compare the images of character and not the entire images words. We presentthe precision and recall values obtained with three methods for the new methodof wordspotting applied on characters only.
arxiv-4500-215 | A proposition of a robust system for historical document images indexation | http://arxiv.org/abs/1308.6319 | author:Nizar Zaghden, Remy Mullot, Mohamed Adel Alimi category:cs.CV published:2013-08-28 summary:Characterizing noisy or ancient documents is a challenging problem up to now.Many techniques have been done in order to effectuate feature extraction andimage indexation for such documents. Global approaches are in general lessrobust and exact than local approaches. That's why, we propose in this paper, ahybrid system based on global approach(fractal dimension), and a local onebased on SIFT descriptor. The Scale Invariant Feature Transform seems to dowell with our application since it's rotation invariant and relatively robustto changing illumination.In the first step the calculation of fractal dimensionis applied to images in order to eliminate images which have distant featuresthan image request characteristics. Next, the SIFT is applied to show whichimages match well the request. However the average matching time using thehybrid approach is better than "fractal dimension" and "SIFT descriptor" ifthey are used alone.
arxiv-4500-216 | Computing Lexical Contrast | http://arxiv.org/abs/1308.6300 | author:Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, Peter D. Turney category:cs.CL published:2013-08-28 summary:Knowing the degree of semantic contrast between words has widespreadapplication in natural language processing, including machine translation,information retrieval, and dialogue systems. Manually-created lexicons focus onopposites, such as {\rm hot} and {\rm cold}. Opposites are of many kinds suchas antipodals, complementaries, and gradable. However, existing lexicons oftendo not classify opposites into the different kinds. They also do not explicitlylist word pairs that are not opposites but yet have some degree of contrast inmeaning, such as {\rm warm} and {\rm cold} or {\rm tropical} and {\rmfreezing}. We propose an automatic method to identify contrasting word pairsthat is based on the hypothesis that if a pair of words, $A$ and $B$, arecontrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and$C$ are strongly related and $B$ and $D$ are strongly related. (For example,there exists the pair of opposites {\rm hot} and {\rm cold} such that {\rmtropical} is related to {\rm hot,} and {\rm freezing} is related to {\rmcold}.) We will call this the contrast hypothesis. We begin with a largecrowdsourcing experiment to determine the amount of human agreement on theconcept of oppositeness and its different kinds. In the process, we flesh outkey features of different kinds of opposites. We then present an automatic andempirical measure of lexical contrast that relies on the contrast hypothesis,corpus statistics, and the structure of a {\it Roget}-like thesaurus. We showthat the proposed measure of lexical contrast obtains high precision and largecoverage, outperforming existing methods.
arxiv-4500-217 | Crowdsourcing a Word-Emotion Association Lexicon | http://arxiv.org/abs/1308.6297 | author:Saif M. Mohammad, Peter D. Turney category:cs.CL published:2013-08-28 summary:Even though considerable attention has been given to the polarity of words(positive and negative) and the creation of large polarity lexicons, researchin emotion analysis has had to rely on limited and small emotion lexicons. Inthis paper we show how the combined strength and wisdom of the crowds can beused to generate a large, high-quality, word-emotion and word-polarityassociation lexicon quickly and inexpensively. We enumerate the challenges inemotion annotation in a crowdsourcing scenario and propose solutions to addressthem. Most notably, in addition to questions about emotions associated withterms, we show how the inclusion of a word choice question can discouragemalicious data entry, help identify instances where the annotator may not befamiliar with the target term (allowing us to reject such annotations), andhelp obtain annotations at sense level (rather than at word level). Weconducted experiments on how to formulate the emotion-annotation questions, andshow that asking if a term is associated with an emotion leads to markedlyhigher inter-annotator agreement than that obtained by asking if a term evokesan emotion.
arxiv-4500-218 | Prediction of breast cancer recurrence using Classification Restricted Boltzmann Machine with Dropping | http://arxiv.org/abs/1308.6324 | author:Jakub M. Tomczak category:cs.LG published:2013-08-28 summary:In this paper, we apply Classification Restricted Boltzmann Machine(ClassRBM) to the problem of predicting breast cancer recurrence. According tothe Polish National Cancer Registry, in 2010 only, the breast cancer causedalmost 25% of all diagnosed cases of cancer in Poland. We propose how to useClassRBM for predicting breast cancer return and discovering relevant inputs(symptoms) in illness reappearance. Next, we outline a general probabilisticframework for learning Boltzmann machines with masks, which we refer to asDropping. The fashion of generating masks leads to different learning methods,i.e., DropOut, DropConnect. We propose a new method called DropPart which is ageneralization of DropConnect. In DropPart the Beta distribution instead ofBernoulli distribution in DropConnect is used. At the end, we carry out anexperiment using real-life dataset consisting of 949 cases, provided by theInstitute of Oncology Ljubljana.
arxiv-4500-219 | New Algorithms for Learning Incoherent and Overcomplete Dictionaries | http://arxiv.org/abs/1308.6273 | author:Sanjeev Arora, Rong Ge, Ankur Moitra category:cs.DS cs.LG stat.ML published:2013-08-28 summary:In sparse recovery we are given a matrix $A$ (the dictionary) and a vector ofthe form $A X$ where $X$ is sparse, and the goal is to recover $X$. This is acentral notion in signal processing, statistics and machine learning. But inapplications such as sparse coding, edge detection, compression and superresolution, the dictionary $A$ is unknown and has to be learned from randomexamples of the form $Y = AX$ where $X$ is drawn from an appropriatedistribution --- this is the dictionary learning problem. In most settings, $A$is overcomplete: it has more columns than rows. This paper presents apolynomial-time algorithm for learning overcomplete dictionaries; the onlypreviously known algorithm with provable guarantees is the recent work ofSpielman, Wang and Wright who gave an algorithm for the full-rank case, whichis rarely the case in applications. Our algorithm applies to incoherentdictionaries which have been a central object of study since they wereintroduced in seminal work of Donoho and Huo. In particular, a dictionary is$\mu$-incoherent if each pair of columns has inner product at most $\mu /\sqrt{n}$. The algorithm makes natural stochastic assumptions about the unknown sparsevector $X$, which can contain $k \leq c \min(\sqrt{n}/\mu \log n, m^{1/2-\eta})$ non-zero entries (for any $\eta > 0$). This is close to the best $k$allowable by the best sparse recovery algorithms even if one knows thedictionary $A$ exactly. Moreover, both the running time and sample complexitydepend on $\log 1/\epsilon$, where $\epsilon$ is the target accuracy, and soour algorithms converge very quickly to the true dictionary. Our algorithm canalso tolerate substantial amounts of noise provided it is incoherent withrespect to the dictionary (e.g., Gaussian). In the noisy setting, our runningtime and sample complexity depend polynomially on $1/\epsilon$, and this isnecessary.
arxiv-4500-220 | Compound Poisson Processes, Latent Shrinkage Priors and Bayesian Nonconvex Penalization | http://arxiv.org/abs/1308.6069 | author:Zhihua Zhang, Jin Li category:stat.ML stat.ME published:2013-08-28 summary:In this paper we discuss Bayesian nonconvex penalization for sparse learningproblems. We explore a nonparametric formulation for latent shrinkageparameters using subordinators which are one-dimensional L\'{e}vy processes. Weparticularly study a family of continuous compound Poisson subordinators and afamily of discrete compound Poisson subordinators. We exemplify four specificsubordinators: Gamma, Poisson, negative binomial and squared Besselsubordinators. The Laplace exponents of the subordinators are Bernsteinfunctions, so they can be used as sparsity-inducing nonconvex penaltyfunctions. We exploit these subordinators in regression problems, yielding ahierarchical model with multiple regularization parameters. We devise ECME(Expectation/Conditional Maximization Either) algorithms to simultaneouslyestimate regression coefficients and regularization parameters. The empiricalevaluation of simulated data shows that our approach is feasible and effectivein high-dimensional data analysis.
arxiv-4500-221 | NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets | http://arxiv.org/abs/1308.6242 | author:Saif M. Mohammad, Svetlana Kiritchenko, Xiaodan Zhu category:cs.CL published:2013-08-28 summary:In this paper, we describe how we created two state-of-the-art SVMclassifiers, one to detect the sentiment of messages such as tweets and SMS(message-level task) and one to detect the sentiment of a term within asubmissions stood first in both tasks on tweets, obtaining an F-score of 69.02in the message-level task and 88.93 in the term-level task. We implemented avariety of surface-form, semantic, and sentiment features. with sentiment-wordhashtags, and one from tweets with emoticons. In the message-level task, thelexicon-based features provided a gain of 5 F-score points over all others.Both of our systems can be replicated us available resources.
arxiv-4500-222 | Bayesian Conditional Gaussian Network Classifiers with Applications to Mass Spectra Classification | http://arxiv.org/abs/1308.6181 | author:Victor Bellon, Jesus Cerquides, Ivo Grosse category:cs.LG stat.ML published:2013-08-28 summary:Classifiers based on probabilistic graphical models are very effective. Incontinuous domains, maximum likelihood is usually used to assess thepredictions of those classifiers. When data is scarce, this can easily lead tooverfitting. In any probabilistic setting, Bayesian averaging (BA) providestheoretically optimal predictions and is known to be robust to overfitting. Inthis work we introduce Bayesian Conditional Gaussian Network Classifiers, whichefficiently perform exact Bayesian averaging over the parameters. We evaluatethe proposed classifiers against the maximum likelihood alternatives proposedso far over standard UCI datasets, concluding that performing BA improves thequality of the assessed probabilities (conditional log likelihood) whilstmaintaining the error rate. Overfitting is more likely to occur in domains where the number of data itemsis small and the number of variables is large. These two conditions are met inthe realm of bioinformatics, where the early diagnosis of cancer from massspectra is a relevant task. We provide an application of our classificationframework to that problem, comparing it with the standard maximum likelihoodalternative, where the improvement of quality in the assessed probabilities isconfirmed.
arxiv-4500-223 | Clustering, Classification, Discriminant Analysis, and Dimension Reduction via Generalized Hyperbolic Mixtures | http://arxiv.org/abs/1308.6315 | author:Katherine Morris, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2013-08-28 summary:A method for dimension reduction with clustering, classification, ordiscriminant analysis is introduced. This mixture model-based approach is basedon fitting generalized hyperbolic mixtures on a reduced subspace within theparadigm of model-based clustering, classification, or discriminant analysis. Areduced subspace of the data is derived by considering the extent to whichgroup means and group covariances vary. The members of the subspace arisethrough linear combinations of the original data, and are ordered by importancevia the associated eigenvalues. The observations can be projected onto thesubspace, resulting in a set of variables that captures most of the clusteringinformation available. The use of generalized hyperbolic mixtures gives arobust framework capable of dealing with skewed clusters. Although dimensionreduction is increasingly in demand across many application areas, the authorsare most familiar with biological applications and so two of the five real dataexamples are within that sphere. Simulated data are also used for illustration.The approach introduced herein can be considered the most general such approachavailable, and so we compare results to three special and limiting cases.Comparisons with several well established techniques illustrate its promisingperformance.
arxiv-4500-224 | Preventing Disclosure of Sensitive Knowledge by Hiding Inference | http://arxiv.org/abs/1308.6744 | author:A. S. Syed Navaz, M. Ravi, T. Prabhu category:cs.CR cs.DB cs.LG published:2013-08-28 summary:Data Mining is a way of extracting data or uncovering hidden patterns ofinformation from databases. So, there is a need to prevent the inference rulesfrom being disclosed such that the more secure data sets cannot be identifiedfrom non sensitive attributes. This can be done through removing or addingcertain item sets in the transactions Sanitization. The purpose is to hide theInference rules, so that the user may not be able to discover any valuableinformation from other non sensitive data and any organisation can release allsamples of their data without the fear of Knowledge Discovery In Databaseswhich can be achieved by investigating frequently occurring item sets, rulesthat can be mined from them with the objective of hiding them. Another way isto release only limited samples in the new database so that there is noinformation loss and it also satisfies the legitimate needs of the users. Themajor problem is uncovering hidden patterns, which causes a threat to thedatabase security. Sensitive data are inferred from non-sensitive data based onthe semantics of the application the user has, commonly known as the inferenceproblem. Two fundamental approaches to protect sensitive rules from disclosureare that, preventing rules from being generated by hiding the frequent sets ofdata items and reducing the importance of the rules by setting their confidencebelow a user-specified threshold.
arxiv-4500-225 | Brain MRI Segmentation with Fast and Globally Convex Multiphase Active Contours | http://arxiv.org/abs/1308.6056 | author:Juan C. Moreno, V. B. S. Prasath, Hugo Proenca, K. Palaniappan category:cs.CV 68U10 I.4.6 published:2013-08-28 summary:Multiphase active contour based models are useful in identifying multipleregions with different characteristics such as the mean values of regions. Thisis relevant in brain magnetic resonance images (MRIs), allowing thedifferentiation of white matter against gray matter. We consider a well definedglobally convex formulation of Vese and Chan multiphase active contour modelfor segmenting brain MRI images. A well-established theory and an efficientdual minimization scheme are thoroughly described which guarantees optimalsolutions and provides stable segmentations. Moreover, under the dualminimization implementation our model perfectly describes disjoint regions byavoiding local minima solutions. Experimental results indicate that theproposed approach provides better accuracy than other related multiphase activecontour algorithms even under severe noise, intensity inhomogeneities, andpartial volume effects.
arxiv-4500-226 | Hierarchized block wise image approximation by greedy pursuit strategies | http://arxiv.org/abs/1308.5876 | author:Laura Rebollo-Neira, Ryszard Maciol, Shabnam Bibi category:cs.CV 68U10, 94A08 G.1.2 published:2013-08-27 summary:An approach for effective implementation of greedy selection methodologies,to approximate an image partitioned into blocks, is proposed. The method isspecially designed for approximating partitions on a transformed image. Itevolves by selecting, at each iteration step, i) the elements for approximatingeach of the blocks partitioning the image and ii) the hierarchized sequence inwhich the blocks are approximated to reach the required global condition onsparsity.
arxiv-4500-227 | Multi-Objective Particle Swarm Optimization for Facility Location Problem in Wireless Mesh Networks | http://arxiv.org/abs/1308.5807 | author:Tarik Mountassir Bou, Abdelkrim Haqiq, Samir Bennani category:cs.NI cs.NE published:2013-08-27 summary:Wireless mesh networks have seen a real progress due of their implementationat a low cost. They present one of Next Generation Networks technologies andcan serve as home, companies and universities networks. In this paper, wepropose and discuss a new multi-objective model for nodes deploymentoptimization in Multi-Radio Multi-Channel Wireless Mesh Networks. We exploitthe trade-off between network cost and the overall network performance. Thisoptimization problem is solved simultaneously by using a meta-heuristic methodthat returns a non-dominated set of near optimal solutions. A comparative studywas driven to evaluate the efficiency of the proposed model.
arxiv-4500-228 | Improving Sparse Associative Memories by Escaping from Bogus Fixed Points | http://arxiv.org/abs/1308.6003 | author:Zhe Yao, Vincent Gripon, Michael Rabbat category:cs.NE cs.IT math.IT published:2013-08-27 summary:The Gripon-Berrou neural network (GBNN) is a recently invented recurrentneural network embracing a LDPC-like sparse encoding setup which makes itextremely resilient to noise and errors. A natural use of GBNN is as anassociative memory. There are two activation rules for the neuron dynamics,namely sum-of-sum and sum-of-max. The latter outperforms the former in terms ofretrieval rate by a huge margin. In prior discussions and experiments, it isbelieved that although sum-of-sum may lead the network to oscillate, sum-of-maxalways converges to an ensemble of neuron cliques corresponding to previouslystored patterns. However, this is not entirely correct. In fact, sum-of-maxoften converges to bogus fixed points where the ensemble only comprises a smallsubset of the converged state. By taking advantage of this overlooked fact, wecan greatly improve the retrieval rate. We discuss this particular issue andpropose a number of heuristics to push sum-of-max beyond these bogus fixedpoints. To tackle the problem directly and completely, a novel post-processingalgorithm is also developed and customized to the structure of GBNN.Experimental results show that the new algorithm achieves a huge performanceboost in terms of both retrieval rate and run-time, compared to the standardsum-of-max and all the other heuristics.
arxiv-4500-229 | Backhaul-Aware Interference Management in the Uplink of Wireless Small Cell Networks | http://arxiv.org/abs/1308.5835 | author:Sumudu Samarakoon, Mehdi Bennis, Walid Saad, Matti Latva-aho category:cs.NI cs.GT cs.LG published:2013-08-27 summary:The design of distributed mechanisms for interference management is one ofthe key challenges in emerging wireless small cell networks whose backhaul iscapacity limited and heterogeneous (wired, wireless and a mix thereof). In thispaper, a novel, backhaul-aware approach to interference management in wirelesssmall cell networks is proposed. The proposed approach enables macrocell userequipments (MUEs) to optimize their uplink performance, by exploiting thepresence of neighboring small cell base stations. The problem is formulated asa noncooperative game among the MUEs that seek to optimize their delay-ratetradeoff, given the conditions of both the radio access network and the --possibly heterogeneous -- backhaul. To solve this game, a novel, distributedlearning algorithm is proposed using which the MUEs autonomously choose theiroptimal uplink transmission strategies, given a limited amount of availableinformation. The convergence of the proposed algorithm is shown and itsproperties are studied. Simulation results show that, under various types ofbackhauls, the proposed approach yields significant performance gains, in termsof both average throughput and delay for the MUEs, when compared to existingbenchmark algorithms.
arxiv-4500-230 | Frequency Recognition in SSVEP-based BCI using Multiset Canonical Correlation Analysis | http://arxiv.org/abs/1308.5609 | author:Yu Zhang, Guoxu Zhou, Jing Jin, Xingyu Wang, Andrzej Cichocki category:stat.ML published:2013-08-26 summary:Canonical correlation analysis (CCA) has been one of the most popular methodsfor frequency recognition in steady-state visual evoked potential (SSVEP)-basedbrain-computer interfaces (BCIs). Despite its efficiency, a potential problemis that using pre-constructed sine-cosine waves as the required referencesignals in the CCA method often does not result in the optimal recognitionaccuracy due to their lack of features from the real EEG data. To address thisproblem, this study proposes a novel method based on multiset canonicalcorrelation analysis (MsetCCA) to optimize the reference signals used in theCCA method for SSVEP frequency recognition. The MsetCCA method learns multiplelinear transforms that implement joint spatial filtering to maximize theoverall correlation among canonical variates, and hence extracts SSVEP commonfeatures from multiple sets of EEG data recorded at the same stimulusfrequency. The optimized reference signals are formed by combination of thecommon features and completely based on training data. Experimental study withEEG data from ten healthy subjects demonstrates that the MsetCCA methodimproves the recognition accuracy of SSVEP frequency in comparison with the CCAmethod and other two competing methods (multiway CCA (MwayCCA) and phaseconstrained CCA (PCCA)), especially for a small number of channels and a shorttime window length. The superiority indicates that the proposed MsetCCA methodis a new promising candidate for frequency recognition in SSVEP-based BCIs.
arxiv-4500-231 | Linear models and linear mixed effects models in R with linguistic applications | http://arxiv.org/abs/1308.5499 | author:Bodo Winter category:cs.CL published:2013-08-26 summary:This text is a conceptual introduction to mixed effects modeling withlinguistic applications, using the R programming environment. The reader isintroduced to linear modeling and assumptions, as well as to mixedeffects/multilevel modeling, including a discussion of random intercepts,random slopes and likelihood ratio tests. The example used throughout the textfocuses on the phonetic analysis of voice pitch data.
arxiv-4500-232 | Sparse and Non-Negative BSS for Noisy Data | http://arxiv.org/abs/1308.5546 | author:J√©r√©my Rapin, J√©r√¥me Bobin, Anthony Larue, Jean-Luc Starck category:stat.ML cs.LG 94A12 I.5.4 published:2013-08-26 summary:Non-negative blind source separation (BSS) has raised interest in variousfields of research, as testified by the wide literature on the topic ofnon-negative matrix factorization (NMF). In this context, it is fundamentalthat the sources to be estimated present some diversity in order to beefficiently retrieved. Sparsity is known to enhance such contrast between thesources while producing very robust approaches, especially to noise. In thispaper we introduce a new algorithm in order to tackle the blind separation ofnon-negative sparse sources from noisy measurements. We first show thatsparsity and non-negativity constraints have to be carefully applied on thesought-after solution. In fact, improperly constrained solutions are unlikelyto be stable and are therefore sub-optimal. The proposed algorithm, named nGMCA(non-negative Generalized Morphological Component Analysis), makes use ofproximal calculus techniques to provide properly constrained solutions. Theperformance of nGMCA compared to other state-of-the-art algorithms isdemonstrated by numerical experiments encompassing a wide variety of settings,with negligible parameter tuning. In particular, nGMCA is shown to providerobustness to noise and performs well on synthetic mixtures of real NMRspectra.
arxiv-4500-233 | The Generalized Mean Information Coefficient | http://arxiv.org/abs/1308.5712 | author:Alexander Luedtke, Linh Tran category:stat.ML published:2013-08-26 summary:Reshef & Reshef recently published a paper in which they present a methodcalled the Maximal Information Coefficient (MIC) that can detect all forms ofstatistical dependence between pairs of variables as sample size goes toinfinity. While this method has been praised by some, it has also beencriticized for its lack of power in finite samples. We seek to modify MIC sothat it has higher power in detecting associations for limited sample sizes.Here we present the Generalized Mean Information Coefficient (GMIC), ageneralization of MIC which incorporates a tuning parameter that can be used tomodify the complexity of the association favored by the measure. We define GMICand prove it maintains several key asymptotic properties of MIC. Its increasedpower over MIC is demonstrated using a simulation of eight different functionalrelationships at sixty different noise levels. The results are compared to thePearson correlation, distance correlation, and MIC. Simulation results suggestthat while generally GMIC has slightly lower power than the distancecorrelation measure, it achieves higher power than MIC for many forms ofunderlying association. For some functional relationships, GMIC surpasses allother statistics calculated. Preliminary results suggest choosing a moderatevalue of the tuning parameter for GMIC will yield a test that is robust acrossunderlying relationships. GMIC is a promising new method that mitigates thepower issues suffered by MIC, at the possible expense of equitability.Nonetheless, distance correlation was in our simulations more powerful for manyforms of underlying relationships. At a minimum, this work motivates furtherconsideration of maximal information-based nonparametric exploration (MINE)methods as statistical tests of independence.
arxiv-4500-234 | Detection of copy-move forgery in digital images based on DCT | http://arxiv.org/abs/1308.5661 | author:Nathalie Diane Wandji, Sun Xingming, Moise Fah Kue category:cs.CV cs.CR published:2013-08-26 summary:With rapid advances in digital information processing systems, and morespecifically in digital image processing software, there is a widespreaddevelopment of advanced tools and techniques for digital image forgery. One ofthe techniques most commonly used is the Copy-move forgery which proceeds bycopying a part of an image and pasting it into the same image, in order tomaliciously hide an object or a region. In this paper, we propose a method todetect this specific kind of counterfeit. Firstly, the color image is convertedfrom RGB color space to YCbCr color space and then the R, G, B and Y-componentare splitted into fixed-size overlapping blocks and, features are extractedfrom the R, G and B-components image blocks on one hand and on the other, fromthe DCT representation of the R, G, B and Ycomponent image block. The featurevectors obtained are then lexicographically sorted to make similar image blocksneighbors and duplicated image blocks are identified using Euclidean distanceas similarity criterion. Experimental results showed that the proposed methodcan detect the duplicated regions when there is more than one copy move forgedarea in the image and even in case of slight rotations, JPEG compression,shift, scale, blur and noise addition.
arxiv-4500-235 | A Comparison of Algorithms for Learning Hidden Variables in Normal Graphs | http://arxiv.org/abs/1308.5576 | author:Francesco A. N. Palmieri category:stat.ML cs.IT cs.SY math.IT published:2013-08-26 summary:A Bayesian factor graph reduced to normal form consists in theinterconnection of diverter units (or equal constraint units) andSingle-Input/Single-Output (SISO) blocks. In this framework localizedadaptation rules are explicitly derived from a constrained maximum likelihood(ML) formulation and from a minimum KL-divergence criterion using KKTconditions. The learning algorithms are compared with two other updatingequations based on a Viterbi-like and on a variational approximationrespectively. The performance of the various algorithm is verified on syntheticdata sets for various architectures. The objective of this paper is to providethe programmer with explicit algorithms for rapid deployment of Bayesian graphsin the applications.
arxiv-4500-236 | A Literature Review: Stemming Algorithms for Indian Languages | http://arxiv.org/abs/1308.5423 | author:M. Thangarasu, R. Manavalan category:cs.CL published:2013-08-25 summary:Stemming is the process of extracting root word from the given inflectionword. It also plays significant role in numerous application of NaturalLanguage Processing (NLP). The stemming problem has addressed in many contextsand by researchers in many disciplines. This expository paper presents surveyof some of the latest developments on stemming algorithms in data mining andalso presents with some of the solutions for various Indian language stemmingalgorithms along with the results.
arxiv-4500-237 | Stability of Phase Retrievable Frames | http://arxiv.org/abs/1308.5465 | author:Radu Balan category:math.FA cs.CV stat.ML published:2013-08-25 summary:In this paper we study the property of phase retrievability by redundantsysems of vectors under perturbations of the frame set. Specifically we showthat if a set $\fc$ of $m$ vectors in the complex Hilbert space of dimension nallows for vector reconstruction from magnitudes of its coefficients, thenthere is a perturbation bound $\rho$ so that any frame set within $\rho$ from$\fc$ has the same property. In particular this proves the recent constructionin \cite{BH13} is stable under perturbations. By the same token we reduce thecritical cardinality conjectured in \cite{BCMN13a} to proving a stabilityresult for non phase-retrievable frames.
arxiv-4500-238 | A comparative analysis of methods for estimating axon diameter using DWI | http://arxiv.org/abs/1308.5269 | author:Hamed Yousefi Mesri category:cs.NE published:2013-08-24 summary:The importance of studying the brain microstructure is described and theexisting and state of the art non-invasive methods for the investigation of thebrain microstructure using Diffusion Weighted Magnetic Resonance Imaging (DWI)is studied. In the next step, Cramer-Rao Lower Bound (CRLB) analysis isdescribed and utilised for assessment of the minimum estimation error anduncertainty level of different Diffusion Weighted Magnetic Resonance (DWMR)signal decay models. The analyses are performed considering the best scenariothrough which, we assume that the models are the appropriate representation ofthe measured phenomena. This includes the study of the sensitivity of theestimations to the measurement and model parameters. It is demonstrated thatnone of the existing models can achieve a reasonable minimum uncertainty levelunder typical measurement setup. At the end, the practical obstacles forachieving higher performance in clinical and experimental environments arestudied and their effects on feasibility of the methods are discussed.
arxiv-4500-239 | The Lovasz-Bregman Divergence and connections to rank aggregation, clustering, and web ranking | http://arxiv.org/abs/1308.5275 | author:Rishabh Iyer, Jeff Bilmes category:cs.LG cs.IR stat.ML published:2013-08-24 summary:We extend the recently introduced theory of Lovasz-Bregman (LB) divergences(Iyer & Bilmes, 2012) in several ways. We show that they represent a distortionbetween a 'score' and an 'ordering', thus providing a new view of rankaggregation and order based clustering with interesting connections to webranking. We show how the LB divergences have a number of properties akin tomany permutation based metrics, and in fact have as special cases forms verysimilar to the Kendall-$\tau$ metric. We also show how the LB divergencessubsume a number of commonly used ranking measures in information retrieval,like the NDCG and AUC. Unlike the traditional permutation based metrics,however, the LB divergence naturally captures a notion of "confidence" in theorderings, thus providing a new representation to applications involvingaggregating scores as opposed to just orderings. We show how a number ofrecently used web ranking models are forms of Lovasz-Bregman rank aggregationand also observe that a natural form of Mallow's model using the LB divergencehas been used as conditional ranking models for the 'Learning to Rank' problem.
arxiv-4500-240 | Ensemble of Distributed Learners for Online Classification of Dynamic Data Streams | http://arxiv.org/abs/1308.5281 | author:Luca Canzian, Yu Zhang, Mihaela van der Schaar category:cs.LG published:2013-08-24 summary:We present an efficient distributed online learning scheme to classify datacaptured from distributed, heterogeneous, and dynamic data sources. Our schemeconsists of multiple distributed local learners, that analyze different streamsof data that are correlated to a common event that needs to be classified. Eachlearner uses a local classifier to make a local prediction. The localpredictions are then collected by each learner and combined using a weightedmajority rule to output the final prediction. We propose a novel onlineensemble learning algorithm to update the aggregation rule in order to adapt tothe underlying data dynamics. We rigorously determine a bound for the worstcase misclassification probability of our algorithm which depends on themisclassification probabilities of the best static aggregation rule, and of thebest local classifier. Importantly, the worst case misclassificationprobability of our algorithm tends asymptotically to 0 if the misclassificationprobability of the best static aggregation rule or the misclassificationprobability of the best local classifier tend to 0. Then we extend ouralgorithm to address challenges specific to the distributed implementation andwe prove new bounds that apply to these settings. Finally, we test our schemeby performing an evaluation study on several data sets. When applied to datasets widely used by the literature dealing with dynamic data streams andconcept drift, our scheme exhibits performance gains ranging from 34% to 71%with respect to state of the art solutions.
arxiv-4500-241 | Edge-detection applied to moving sand dunes on Mars | http://arxiv.org/abs/1308.5315 | author:Amelia Carolina Sparavigna category:cs.CV published:2013-08-24 summary:Here we discuss the application of an edge detection filter, the Sobel filterof GIMP, to the recently discovered motion of some sand dunes on Mars. Thefilter allows a good comparison of an image HiRISE of 2007 and an image of 1999recorded by the Mars Global Surveyor of the dunes in the Nili Patera caldera,measuring therefore the motion of the dunes on a longer period of time thanthat previously investigated.
arxiv-4500-242 | Monitoring with uncertainty | http://arxiv.org/abs/1308.5329 | author:Ezio Bartocci, Radu Grosu category:cs.LO cs.LG cs.SY published:2013-08-24 summary:We discuss the problem of runtime verification of an instrumented programthat misses to emit and to monitor some events. These gaps can occur when amonitoring overhead control mechanism is introduced to disable the monitor ofan application with real-time constraints. We show how to use statisticalmodels to learn the application behavior and to "fill in" the introduced gaps.Finally, we present and discuss some techniques developed in the last threeyears to estimate the probability that a property of interest is violated inthe presence of an incomplete trace.
arxiv-4500-243 | A stochastic hybrid model of a biological filter | http://arxiv.org/abs/1308.5338 | author:Andrea Ocone, Guido Sanguinetti category:cs.LG cs.CE q-bio.MN published:2013-08-24 summary:We present a hybrid model of a biological filter, a genetic circuit whichremoves fast fluctuations in the cell's internal representation of the extracellular environment. The model takes the classic feed-forward loop (FFL) motifand represents it as a network of continuous protein concentrations and binary,unobserved gene promoter states. We address the problem of statisticalinference and parameter learning for this class of models from partial,discrete time observations. We show that the hybrid representation leads to anefficient algorithm for approximate statistical inference in this circuit, andshow its effectiveness on a simulated data set.
arxiv-4500-244 | Likelihood Adaptively Modified Penalties | http://arxiv.org/abs/1308.5036 | author:Yang Feng, Tengfei Li, Zhiliang Ying category:stat.ME math.ST stat.ML stat.TH published:2013-08-23 summary:A new family of penalty functions, adaptive to likelihood, is introduced formodel selection in general regression models. It arises naturally throughassuming certain types of prior distribution on the regression parameters. Tostudy stability properties of the penalized maximum likelihood estimator, twotypes of asymptotic stability are defined. Theoretical properties, includingthe parameter estimation consistency, model selection consistency, andasymptotic stability, are established under suitable regularity conditions. Anefficient coordinate-descent algorithm is proposed. Simulation results and realdata analysis show that the proposed method has competitive performance incomparison with existing ones.
arxiv-4500-245 | Suspicious Object Recognition Method in Video Stream Based on Visual Attention | http://arxiv.org/abs/1308.5063 | author:Panqu Wang, Yan Zhang category:cs.CV published:2013-08-23 summary:We propose a state of the art method for intelligent object recognition andvideo surveillance based on human visual attention. Bottom up and top downattention are applied respectively in the process of acquiring interestedobject(saliency map) and object recognition. The revision of 4 channel PFTmethod is proposed for bottom up attention and enhances the speed and accuracy.Inhibit of return (IOR) is applied in judging the sequence of saliency objectpop out. Euclidean distance of color distribution, object center coordinatesand speed are considered in judging whether the target is match and suspicious.The extensive tests on videos and images show that our method in video analysishas high accuracy and fast speed compared with traditional method. The methodcan be applied into many fields such as video surveillance and security.
arxiv-4500-246 | How Did Humans Become So Creative? A Computational Approach | http://arxiv.org/abs/1308.5032 | author:Liane Gabora, Steve DiPaola category:cs.NE cs.AI cs.MA q-bio.NC published:2013-08-23 summary:This paper summarizes efforts to computationally model two transitions in theevolution of human creativity: its origins about two million years ago, and the'big bang' of creativity about 50,000 years ago. Using a computational model ofcultural evolution in which neural network based agents evolve ideas foractions through invention and imitation, we tested the hypothesis that humancreativity began with onset of the capacity for recursive recall. We comparedruns in which agents were limited to single-step actions to runs in which theyused recursive recall to chain simple actions into complex ones. Chainingresulted in higher diversity, open-ended novelty, no ceiling on the meanfitness of actions, and greater ability to make use of learning. Using acomputational model of portrait painting, we tested the hypothesis that theexplosion of creativity in the Middle/Upper Paleolithic was due to onset ofcon-textual focus: the capacity to shift between associative and analyticthought. This resulted in faster convergence on portraits that resembled thesitter, employed painterly techniques, and were rated as preferable. Weconclude that recursive recall and contextual focus provide a computationallyplausible explanation of how humans evolved the means to transform this planet.
arxiv-4500-247 | A hybrid evolutionary algorithm with importance sampling for multi-dimensional optimization | http://arxiv.org/abs/1308.5033 | author:Guanghui Huang, Zhifeng Pan category:cs.NE published:2013-08-23 summary:A hybrid evolutionary algorithm with importance sampling method is proposedfor multi-dimensional optimization problems in this paper. In order to make useof the information provided in the search process, a set of visited solutionsis selected to give scores for intervals in each dimension, and they areupdated as algorithm proceeds. Those intervals with higher scores are regardedas good intervals, which are used to estimate the joint distribution of optimalsolutions through an interaction between the pool of good genetics, which arethe individuals with smaller fitness values. And the sampling probabilities forgood genetics are determined through an interaction between those estimatedgood intervals. It is a cross validation mechanism which determines thesampling probabilities for good intervals and genetics, and the resultedprobabilities are used to design crossover, mutation and other stochasticoperators with importance sampling method. As the selection of genetics andintervals is not directly dependent on the values of fitness, the resultedoffsprings may avoid the trap of local optima. And a purely random EA is alsocombined into the proposed algorithm to maintain the diversity of population.30 benchmark test functions are used to evaluate the performance of theproposed algorithm, and it is found that the proposed hybrid algorithm is anefficient algorithm for multi-dimensional optimization problems considered inthis paper.
arxiv-4500-248 | Artificial Immune Systems (INTROS 2) | http://arxiv.org/abs/1308.5138 | author:Uwe Aickelin, Dipankar Dasgupta, Feng Gu category:cs.NE cs.ET published:2013-08-23 summary:The biological immune system is a robust, complex, adaptive system thatdefends the body from foreign pathogens. It is able to categorize all cells (ormolecules) within the body as self or non-self substances. It does this withthe help of a distributed task force that has the intelligence to take actionfrom a local and also a global perspective using its network of chemicalmessengers for communication. There are two major branches of the immunesystem. The innate immune system is an unchanging mechanism that detects anddestroys certain invading organisms, whilst the adaptive immune system respondsto previously unknown foreign cells and builds a response to them that canremain in the body over a long period of time. This remarkable informationprocessing biological system has caught the attention of computer science inrecent years. A novel computational intelligence technique, inspired by immunology, hasemerged, called Artificial Immune Systems. Several concepts from the immunesystem have been extracted and applied for solution to real world science andengineering problems. In this tutorial, we briefly describe the immune systemmetaphors that are relevant to existing Artificial Immune Systems methods. Wewill then show illustrative real-world problems suitable for Artificial ImmuneSystems and give a step-by-step algorithm walkthrough for one such problem. Acomparison of the Artificial Immune Systems to other well-known algorithms,areas for future work, tips & tricks and a list of resources will round thistutorial off. It should be noted that as Artificial Immune Systems is still ayoung and evolving field, there is not yet a fixed algorithm template and henceactual implementations might differ somewhat from time to time and from thoseexamples given here.
arxiv-4500-249 | Group-Sparse Signal Denoising: Non-Convex Regularization, Convex Optimization | http://arxiv.org/abs/1308.5038 | author:Po-Yu Chen, Ivan W. Selesnick category:cs.CV cs.LG stat.ML published:2013-08-23 summary:Convex optimization with sparsity-promoting convex regularization is astandard approach for estimating sparse signals in noise. In order to promotesparsity more strongly than convex regularization, it is also standard practiceto employ non-convex optimization. In this paper, we take a third approach. Weutilize a non-convex regularization term chosen such that the total costfunction (consisting of data consistency and regularization terms) is convex.Therefore, sparsity is more strongly promoted than in the standard convexformulation, but without sacrificing the attractive aspects of convexoptimization (unique minimum, robust algorithms, etc.). We use this idea toimprove the recently developed 'overlapping group shrinkage' (OGS) algorithmfor the denoising of group-sparse signals. The algorithm is applied to theproblem of speech enhancement with favorable results in terms of both SNR andperceptual quality.
arxiv-4500-250 | Performance Measurement Under Increasing Environmental Uncertainty In The Context of Interval Type-2 Fuzzy Logic Based Robotic Sailing | http://arxiv.org/abs/1308.5133 | author:Naisan Benatar, Uwe Aickelin, Jonathan M. Garibald category:cs.RO cs.NE cs.SY published:2013-08-23 summary:Performance measurement of robotic controllers based on fuzzy logic,operating under uncertainty, is a subject area which has been somewhat ignoredin the current literature. In this paper standard measures such as RMSE areshown to be inappropriate for use under conditions where the environmentaluncertainty changes significantly between experiments. An overview of currentmethods which have been applied by other authors is presented, followed by adesign of a more sophisticated method of comparison. This method is thenapplied to a robotic control problem to observe its outcome compared with asingle measure. Results show that the technique described provides a morerobust method of performance comparison than less complex methods allowingbetter comparisons to be drawn.
arxiv-4500-251 | Complexity of evolutionary equilibria in static fitness landscapes | http://arxiv.org/abs/1308.5094 | author:Artem Kaznatcheev category:q-bio.PE cs.NE F.2.2; J.3 published:2013-08-23 summary:A fitness landscape is a genetic space -- with two genotypes adjacent if theydiffer in a single locus -- and a fitness function. Evolutionary dynamicsproduce a flow on this landscape from lower fitness to higher; reachingequilibrium only if a local fitness peak is found. I use computationalcomplexity to question the common assumption that evolution on static fitnesslandscapes can quickly reach a local fitness peak. I do this by showing thatthe popular NK model of rugged fitness landscapes is PLS-complete for K >= 2;the reduction from Weighted 2SAT is a bijection on adaptive walks, so there areNK fitness landscapes where every adaptive path from some vertices is ofexponential length. Alternatively -- under the standard complexity theoreticassumption that there are problems in PLS not solvable in polynomial time --this means that there are no evolutionary dynamics (known, or to be discovered,and not necessarily following adaptive paths) that can converge to a localfitness peak on all NK landscapes with K = 2. Applying results from theanalysis of simplex algorithms, I show that there exist single-peakedlandscapes with no reciprocal sign epistasis where the expected length of anadaptive path following strong selection weak mutation dynamics is$e^{O(n^{1/3})}$ even though an adaptive path to the optimum of length lessthan n is available from every vertex. The technical results are written to beaccessible to mathematical biologists without a computer science background,and the biological literature is summarized for the convenience ofnon-biologists with the aim to open a constructive dialogue between the twodisciplines.
arxiv-4500-252 | Manopt, a Matlab toolbox for optimization on manifolds | http://arxiv.org/abs/1308.5200 | author:Nicolas Boumal, Bamdev Mishra, P. -A. Absil, Rodolphe Sepulchre category:cs.MS cs.LG math.OC stat.ML published:2013-08-23 summary:Optimization on manifolds is a rapidly developing branch of nonlinearoptimization. Its focus is on problems where the smooth geometry of the searchspace can be leveraged to design efficient numerical algorithms. In particular,optimization on manifolds is well-suited to deal with rank and orthogonalityconstraints. Such structured constraints appear pervasively in machine learningapplications, including low-rank matrix completion, sensor networklocalization, camera network registration, independent component analysis,metric learning, dimensionality reduction and so on. The Manopt toolbox,available at www.manopt.org, is a user-friendly, documented piece of softwarededicated to simplify experimenting with state of the art Riemannianoptimization algorithms. We aim particularly at reaching practitioners outsideour field.
arxiv-4500-253 | Online Douglas-Rachford splitting method | http://arxiv.org/abs/1308.4757 | author:Ziqiang Shi category:cs.NA cs.LG stat.ML published:2013-08-22 summary:Online learning has emerged as powerful tool in large scale optimization. Inthis work, we generalize the Douglas-Rachford splitting method for minimizingcomposite functions to online settings.
arxiv-4500-254 | Joint modeling of multiple time series via the beta process with application to motion capture segmentation | http://arxiv.org/abs/1308.4747 | author:Emily B. Fox, Michael C. Hughes, Erik B. Sudderth, Michael I. Jordan category:stat.ME stat.ML published:2013-08-22 summary:We propose a Bayesian nonparametric approach to the problem of jointlymodeling multiple related time series. Our model discovers a latent set ofdynamical behaviors shared among the sequences, and segments each time seriesinto regions defined by a subset of these behaviors. Using a beta processprior, the size of the behavior set and the sharing pattern are both inferredfrom data. We develop Markov chain Monte Carlo (MCMC) methods based on theIndian buffet process representation of the predictive distribution of the betaprocess. Our MCMC inference algorithm efficiently adds and removes behaviorsvia novel split-merge moves as well as data-driven birth and death proposals,avoiding the need to consider a truncated model. We demonstrate promisingresults on unsupervised segmentation of human motion capture data.
arxiv-4500-255 | A Unified Framework for Multi-Sensor HDR Video Reconstruction | http://arxiv.org/abs/1308.4908 | author:Joel Kronander, Stefan Gustavson, Gerhard Bonnet, Anders Ynnerman, Jonas Unger category:cs.CV cs.GR cs.MM published:2013-08-22 summary:One of the most successful approaches to modern high quality HDR-videocapture is to use camera setups with multiple sensors imaging the scene througha common optical system. However, such systems pose several challenges for HDRreconstruction algorithms. Previous reconstruction techniques have considereddebayering, denoising, resampling (align- ment) and exposure fusion as separateproblems. In contrast, in this paper we present a unifying approach, performingHDR assembly directly from raw sensor data. Our framework includes a cameranoise model adapted to HDR video and an algorithm for spatially adaptive HDRreconstruction based on fitting of local polynomial approximations to observedsensor data. The method is easy to implement and allows reconstruction to anarbitrary resolution and output mapping. We present an implementation in CUDAand show real-time performance for an experimental 4 Mpixel multi-sensor HDRvideo system. We further show that our algorithm has clear advantages overexisting methods, both in terms of flexibility and reconstruction quality.
arxiv-4500-256 | A review on handwritten character and numeral recognition for Roman, Arabic, Chinese and Indian scripts | http://arxiv.org/abs/1308.4902 | author:Aini Najwa Azmi, Dewi Nasien, Siti Mariyam Shamsuddin category:cs.CV published:2013-08-22 summary:There are a lot of intensive researches on handwritten character recognition(HCR) for almost past four decades. The research has been done on some ofpopular scripts such as Roman, Arabic, Chinese and Indian. In this paper wepresent a review on HCR work on the four popular scripts. We have summarizedmost of the published paper from 2005 to recent and also analyzed the variousmethods in creating a robust HCR system. We also added some future direction ofresearch on HCR.
arxiv-4500-257 | The Sample-Complexity of General Reinforcement Learning | http://arxiv.org/abs/1308.4828 | author:Tor Lattimore, Marcus Hutter, Peter Sunehag category:cs.LG published:2013-08-22 summary:We present a new algorithm for general reinforcement learning where the trueenvironment is known to belong to a finite class of N arbitrary models. Thealgorithm is shown to be near-optimal for all but O(N log^2 N) time-steps withhigh probability. Infinite classes are also considered where we show thatcompactness is a key criterion for determining the existence of uniformsample-complexity bounds. A matching lower bound is given for the finite case.
arxiv-4500-258 | Minimal Dirichlet energy partitions for graphs | http://arxiv.org/abs/1308.4915 | author:Braxton Osting, Chris D. White, Edouard Oudet category:math.OC cs.LG stat.ML published:2013-08-22 summary:Motivated by a geometric problem, we introduce a new non-convex graphpartitioning objective where the optimality criterion is given by the sum ofthe Dirichlet eigenvalues of the partition components. A relaxed formulation isidentified and a novel rearrangement algorithm is proposed, which we show isstrictly decreasing and converges in a finite number of iterations to a localminimum of the relaxed objective function. Our method is applied to severalclustering problems on graphs constructed from synthetic data, MNISThandwritten digits, and manifold discretizations. The model has asemi-supervised extension and provides a natural representative for theclusters as well.
arxiv-4500-259 | Automatic Labeling for Entity Extraction in Cyber Security | http://arxiv.org/abs/1308.4941 | author:Robert A. Bridges, Corinne L. Jones, Michael D. Iannacone, Kelly M. Testa, John R. Goodall category:cs.IR cs.CL published:2013-08-22 summary:Timely analysis of cyber-security information necessitates automatedinformation extraction from unstructured text. While state-of-the-artextraction methods produce extremely accurate results, they require ampletraining data, which is generally unavailable for specialized applications,such as detecting security related entities; moreover, manual annotation ofcorpora is very costly and often not a viable solution. In response, we developa very precise method to automatically label text from several data sources byleveraging related, domain-specific, structured data and provide public accessto a corpus annotated with cyber-security entities. Next, we implement aMaximum Entropy Model trained with the average perceptron on a portion of ourcorpus ($\sim$750,000 words) and achieve near perfect precision, recall, andaccuracy, with training times under 17 seconds.
arxiv-4500-260 | Learning Deep Representation Without Parameter Inference for Nonlinear Dimensionality Reduction | http://arxiv.org/abs/1308.4922 | author:Xiao-Lei Zhang category:cs.LG stat.ML published:2013-08-22 summary:Unsupervised deep learning is one of the most powerful representationlearning techniques. Restricted Boltzman machine, sparse coding, regularizedauto-encoders, and convolutional neural networks are pioneering building blocksof deep learning. In this paper, we propose a new building block -- distributedrandom models. The proposed method is a special full implementation of theproduct of experts: (i) each expert owns multiple hidden units and differentexperts have different numbers of hidden units; (ii) the model of each expertis a k-center clustering, whose k-centers are only uniformly sampled examples,and whose output (i.e. the hidden units) is a sparse code that only thesimilarity values from a few nearest neighbors are reserved. The relationshipbetween the pioneering building blocks, several notable research branches andthe proposed method is analyzed. Experimental results show that the proposeddeep model can learn better representations than deep belief networks andmeanwhile can train a much larger network with much less time than deep beliefnetworks.
arxiv-4500-261 | Sentiment in New York City: A High Resolution Spatial and Temporal View | http://arxiv.org/abs/1308.5010 | author:Karla Z. Bertrand, Maya Bialik, Kawandeep Virdee, Andreas Gros, Yaneer Bar-Yam category:physics.soc-ph cs.CL cs.CY published:2013-08-22 summary:Measuring public sentiment is a key task for researchers and policymakersalike. The explosion of available social media data allows for a moretime-sensitive and geographically specific analysis than ever before. In thispaper we analyze data from the micro-blogging site Twitter and generate asentiment map of New York City. We develop a classifier specifically tuned for140-character Twitter messages, or tweets, using key words, phrases andemoticons to determine the mood of each tweet. This method, combined withgeotagging provided by users, enables us to gauge public sentiment on extremelyfine-grained spatial and temporal scales. We find that public mood is generallyhighest in public parks and lowest at transportation hubs, and locate otherareas of strong sentiment such as cemeteries, medical centers, a jail, and asewage facility. Sentiment progressively improves with proximity to TimesSquare. Periodic patterns of sentiment fluctuate on both a daily and a weeklyscale: more positive tweets are posted on weekends than on weekdays, with adaily peak in sentiment around midnight and a nadir between 9:00 a.m. and noon.
arxiv-4500-262 | PACE: Pattern Accurate Computationally Efficient Bootstrapping for Timely Discovery of Cyber-Security Concepts | http://arxiv.org/abs/1308.4648 | author:Nikki McNeil, Robert A. Bridges, Michael D. Iannacone, Bogdan Czejdo, Nicolas Perez, John R. Goodall category:cs.IR cs.CL IEEE published:2013-08-21 summary:Public disclosure of important security information, such as knowledge ofvulnerabilities or exploits, often occurs in blogs, tweets, mailing lists, andother online sources months before proper classification into structureddatabases. In order to facilitate timely discovery of such knowledge, wepropose a novel semi-supervised learning algorithm, PACE, for identifying andclassifying relevant entities in text sources. The main contribution of thispaper is an enhancement of the traditional bootstrapping method for entityextraction by employing a time-memory trade-off that simultaneously circumventsa costly corpus search while strengthening pattern nomination, which shouldincrease accuracy. An implementation in the cyber-security domain is discussedas well as challenges to Natural Language Processing imposed by the securitydomain.
arxiv-4500-263 | An Investigation of the Sampling-Based Alignment Method and Its Contributions | http://arxiv.org/abs/1308.4479 | author:Juan Luo, Yves Lepage category:cs.CL published:2013-08-21 summary:By investigating the distribution of phrase pairs in phrase translationtables, the work in this paper describes an approach to increase the number ofn-gram alignments in phrase translation tables output by a sampling-basedalignment method. This approach consists in enforcing the alignment of n-gramsin distinct translation subtables so as to increase the number of n-grams.Standard normal distribution is used to allot alignment time among translationsubtables, which results in adjustment of the distribution of n- grams. Thisleads to better evaluation results on statistical machine translation tasksthan the original sampling-based alignment approach. Furthermore, thetranslation quality obtained by merging phrase translation tables computed fromthe sampling-based alignment method and from MGIZA++ is examined.
arxiv-4500-264 | Distributed Online Learning via Cooperative Contextual Bandits | http://arxiv.org/abs/1308.4568 | author:Cem Tekin, Mihaela van der Schaar category:cs.LG stat.ML published:2013-08-21 summary:In this paper we propose a novel framework for decentralized, online learningby many learners. At each moment of time, an instance characterized by acertain context may arrive to each learner; based on the context, the learnercan select one of its own actions (which gives a reward and providesinformation) or request assistance from another learner. In the latter case,the requester pays a cost and receives the reward but the provider learns theinformation. In our framework, learners are modeled as cooperative contextualbandits. Each learner seeks to maximize the expected reward from its arrivals,which involves trading off the reward received from its own actions, theinformation learned from its own actions, the reward received from the actionsrequested of others and the cost paid for these actions - taking into accountwhat it has learned about the value of assistance from each other learner. Wedevelop distributed online learning algorithms and provide analytic bounds tocompare the efficiency of these with algorithms with the complete knowledge(oracle) benchmark (in which the expected reward of every action in everycontext is known by every learner). Our estimates show that regret - the lossincurred by the algorithm - is sublinear in time. Our theoretical framework canbe used in many practical applications including Big Data mining, eventdetection in surveillance sensor networks and distributed online recommendationsystems.
arxiv-4500-265 | Decentralized Online Big Data Classification - a Bandit Framework | http://arxiv.org/abs/1308.4565 | author:Cem Tekin, Mihaela van der Schaar category:cs.LG cs.MA published:2013-08-21 summary:Distributed, online data mining systems have emerged as a result ofapplications requiring analysis of large amounts of correlated andhigh-dimensional data produced by multiple distributed data sources. We proposea distributed online data classification framework where data is gathered bydistributed data sources and processed by a heterogeneous set of distributedlearners which learn online, at run-time, how to classify the different datastreams either by using their locally available classification functions or byhelping each other by classifying each other's data. Importantly, since thedata is gathered at different locations, sending the data to another learner toprocess incurs additional costs such as delays, and hence this will be onlybeneficial if the benefits obtained from a better classification will exceedthe costs. We assume that the classification functions available to eachprocessing element are fixed, but their prediction accuracy for various typesof incoming data are unknown and can change dynamically over time, and thusthey need to be learned online. We model the problem of joint classification bythe distributed and heterogeneous learners from multiple data sources as adistributed contextual bandit problem where each data is characterized by aspecific context. We develop distributed online learning algorithms for whichwe can prove that they have sublinear regret. Compared to prior work indistributed online data mining, our work is the first to provide analyticregret results characterizing the performance of the proposed algorithms.
arxiv-4500-266 | A study of retrieval algorithms of sparse messages in networks of neural cliques | http://arxiv.org/abs/1308.4506 | author:Ala Aboudib, Vincent Gripon, Xiaoran Jiang category:cs.NE published:2013-08-21 summary:Associative memories are data structures addressed using part of the contentrather than an index. They offer good fault reliability and biologicalplausibility. Among different families of associative memories, sparse ones areknown to offer the best efficiency (ratio of the amount of bits stored to thatof bits used by the network itself). Their retrieval process performance hasbeen shown to benefit from the use of iterations. However classical algorithmsrequire having prior knowledge about the data to retrieve such as the number ofnonzero symbols. We introduce several families of algorithms to enhance theretrieval process performance in recently proposed sparse associative memoriesbased on binary neural networks. We show that these algorithms provide betterperformance, along with better plausibility than existing techniques. We alsoanalyze the required number of iterations and derive corresponding curves.
arxiv-4500-267 | Can inferred provenance and its visualisation be used to detect erroneous annotation? A case study using UniProtKB | http://arxiv.org/abs/1308.4618 | author:Michael J. Bell, Matthew Collison, Phillip Lord category:cs.CL cs.CE cs.DL q-bio.QM published:2013-08-21 summary:A constant influx of new data poses a challenge in keeping the annotation inbiological databases current. Most biological databases contain significantquantities of textual annotation, which often contains the richest source ofknowledge. Many databases reuse existing knowledge, during the curation processannotations are often propagated between entries. However, this is often notmade explicit. Therefore, it can be hard, potentially impossible, for a readerto identify where an annotation originated from. Within this work we attempt toidentify annotation provenance and track its subsequent propagation.Specifically, we exploit annotation reuse within the UniProt Knowledgebase(UniProtKB), at the level of individual sentences. We describe a visualisationapproach for the provenance and propagation of sentences in UniProtKB whichenables a large-scale statistical analysis. Initially levels of sentence reusewithin UniProtKB were analysed, showing that reuse is heavily prevalent, whichenables the tracking of provenance and propagation. By analysing sentencesthroughout UniProtKB, a number of interesting propagation patterns wereidentified, covering over 100, 000 sentences. Over 8000 sentences remain in thedatabase after they have been removed from the entries where they originallyoccurred. Analysing a subset of these sentences suggest that approximately 30%are erroneous, whilst 35% appear to be inconsistent. These results suggest thatbeing able to visualise sentence propagation and provenance can aid in thedetermination of the accuracy and quality of textual annotation. Source codeand supplementary data are available from the authors website.
arxiv-4500-268 | A proposal for a Chinese keyboard for cellphones, smartphones, ipads and tablets | http://arxiv.org/abs/1308.4965 | author:Maurice Margenstern, Lan Wu category:cs.HC cs.CL 69U99, 94A99 H.1.2; H.5.2 published:2013-08-21 summary:In this paper, we investigate the possibility to use two tilings of thehyperbolic plane as basic frame for devising a way to input texts in Chinesecharacters into messages of cellphones, smartphones, ipads and tablets.
arxiv-4500-269 | Invertibility and Robustness of Phaseless Reconstruction | http://arxiv.org/abs/1308.4718 | author:Radu Balan, Yang Wang category:math.FA cs.CV stat.ML published:2013-08-21 summary:This paper is concerned with the question of reconstructing a vector in afinite-dimensional real Hilbert space when only the magnitudes of thecoefficients of the vector under a redundant linear map are known. We analyzevarious Lipschitz bounds of the nonlinear analysis map and we establishtheoretical performance bounds of any reconstruction algorithm. We show thatrobust and stable reconstruction requires additional redundancy than thecritical threshold.
arxiv-4500-270 | Scalable Convex Methods for Flexible Low-Rank Matrix Modeling | http://arxiv.org/abs/1308.4211 | author:William Fithian, Rahul Mazumder category:stat.ML published:2013-08-20 summary:We propose a general framework for reduced-rank modeling of matrix-valueddata. By applying a generalized nuclear norm penalty we directly modellow-dimensional latent variables associated with rows and columns. Ourframework flexibly incorporates row and column features, smoothing kernels, andother sources of side information by penalizing deviations from the row andcolumn models. Under general con- ditions these models can be estimatedscalably using convex optimization. The computational bottleneck - one singularvalue decomposition per iteration of a large but easy-to-apply matrix - can bescaled to large problems by representing the matrix implicitly and using warmstarts [Mazumder and Hastie, 2013]. Our framework generalizes traditionalconvex matrix completion and multi-task learning methods as well as MAPestimation under a large class of popular hierarchical Bayesian models.
arxiv-4500-271 | Towards Adapting ImageNet to Reality: Scalable Domain Adaptation with Implicit Low-rank Transformations | http://arxiv.org/abs/1308.4200 | author:Erik Rodner, Judy Hoffman, Jeff Donahue, Trevor Darrell, Kate Saenko category:cs.CV cs.LG stat.ML published:2013-08-20 summary:Images seen during test time are often not from the same distribution asimages used for learning. This problem, known as domain shift, occurs whentraining classifiers from object-centric internet image databases and trying toapply them directly to scene understanding tasks. The consequence is oftensevere performance degradation and is one of the major barriers for theapplication of classifiers in real-world systems. In this paper, we show how tolearn transform-based domain adaptation classifiers in a scalable manner. Thekey idea is to exploit an implicit rank constraint, originated from amax-margin domain adaptation formulation, to make optimization tractable.Experiments show that the transformation between domains can be veryefficiently learned from data and easily applied to new categories. This beginsto bridge the gap between large-scale internet image collections and objectimages captured in everyday life environments.
arxiv-4500-272 | Influences Combination of Multi-Sensor Images on Classification Accuracy | http://arxiv.org/abs/1308.4440 | author:AL-Wassai Firouz, N. V. Kalyankar category:cs.CV published:2013-08-20 summary:This paper focuses on two main issues; first one is the impact of combinationof multi-sensor images on the supervised learning classification accuracy usingsegment Fusion (SF). The second issue attempts to undertake the study ofsupervised machine learning classification technique of remote sensing imagesby using four classifiers like Parallelepiped (Pp), Mahalanobis Distance (MD),Maximum-Likelihood (ML) and Euclidean Distance(ED) classifiers, and theiraccuracies have been evaluated on their respected classification to choose thebest technique for classification of remote sensing images. QuickBirdmultispectral data (MS) and panchromatic data (PAN) have been used in thisstudy to demonstrate the enhancement and accuracy assessment of fused imageover the original images using ALwassaiProcess software. According toexperimental result of this study, is that the test results indicate thesupervised classification results of fusion image, which generated better thanthe MS did. As well as the result with Euclidean classifier is robust andprovides better results than the other classifiers do, despite of the popularbelief that the maximum-likelihood classifier is the most accurate classifier.
arxiv-4500-273 | Pylearn2: a machine learning research library | http://arxiv.org/abs/1308.4214 | author:Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Fr√©d√©ric Bastien, Yoshua Bengio category:stat.ML cs.LG cs.MS published:2013-08-20 summary:Pylearn2 is a machine learning research library. This does not just mean thatit is a collection of machine learning algorithms that share a common API; itmeans that it has been designed for flexibility and extensibility in order tofacilitate research projects that involve new or unusual use cases. In thispaper we give a brief history of the library, an overview of its basicphilosophy, a summary of the library's architecture, and a description of howthe Pylearn2 community functions socially.
arxiv-4500-274 | Nested Nonnegative Cone Analysis | http://arxiv.org/abs/1308.4206 | author:Lingsong Zhang, J. S. Marron, Shu Lu category:stat.ME cs.LG published:2013-08-20 summary:Motivated by the analysis of nonnegative data objects, a novel NestedNonnegative Cone Analysis (NNCA) approach is proposed to overcome somedrawbacks of existing methods. The application of traditional PCA/SVD method tononnegative data often cause the approximation matrix leave the nonnegativecone, which leads to non-interpretable and sometimes nonsensical results. Thenonnegative matrix factorization (NMF) approach overcomes this issue, howeverthe NMF approximation matrices suffer several drawbacks: 1) the factorizationmay not be unique, 2) the resulting approximation matrix at a specific rank maynot be unique, and 3) the subspaces spanned by the approximation matrices atdifferent ranks may not be nested. These drawbacks will cause troubles indetermining the number of components and in multi-scale (in ranks)interpretability. The NNCA approach proposed in this paper naturally generatesa nested structure, and is shown to be unique at each rank. Simulations areused in this paper to illustrate the drawbacks of the traditional methods, andthe usefulness of the NNCA method.
arxiv-4500-275 | SAR Image Despeckling Algorithms using Stochastic Distances and Nonlocal Means | http://arxiv.org/abs/1308.4338 | author:Leonardo Torres, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML published:2013-08-20 summary:This paper presents two approaches for filter design based on stochasticdistances for intensity speckle reduction. A window is defined around eachpixel, overlapping samples are compared and only those which pass agoodness-of-fit test are used to compute the filtered value. The tests stemfrom stochastic divergences within the Information Theory framework. Thetechnique is applied to intensity Synthetic Aperture Radar (SAR) data withhomogeneous regions using the Gamma model. The first approach uses aNagao-Matsuyama-type procedure for setting the overlapping samples, and thesecond uses the nonlocal method. The proposals are compared with the ImprovedSigma filter and with anisotropic diffusion for speckled data (SRAD) using aprotocol based on Monte Carlo simulation. Among the criteria used to quantifythe quality of filters, we employ the equivalent number of looks, and line andedge preservation. Moreover, we also assessed the filters by the UniversalImage Quality Index and by the Pearson correlation between edges. Applicationsto real images are also discussed. The proposed methods show good results.
arxiv-4500-276 | Optimal Algorithms for Testing Closeness of Discrete Distributions | http://arxiv.org/abs/1308.3946 | author:Siu-On Chan, Ilias Diakonikolas, Gregory Valiant, Paul Valiant category:cs.DS cs.IT cs.LG math.IT published:2013-08-19 summary:We study the question of closeness testing for two discrete distributions.More precisely, given samples from two distributions $p$ and $q$ over an$n$-element set, we wish to distinguish whether $p=q$ versus $p$ is at least$\eps$-far from $q$, in either $\ell_1$ or $\ell_2$ distance. Batu et al. gavethe first sub-linear time algorithms for these problems, which matched thelower bounds of Valiant up to a logarithmic factor in $n$, and a polynomialfactor of $\eps.$ In this work, we present simple (and new) testers for both the $\ell_1$ and$\ell_2$ settings, with sample complexity that is information-theoreticallyoptimal, to constant factors, both in the dependence on $n$, and the dependenceon $\eps$; for the $\ell_1$ testing problem we establish that the samplecomplexity is $\Theta(\max\{n^{2/3}/\eps^{4/3}, n^{1/2}/\eps^2 \}).$
arxiv-4500-277 | A balanced k-means algorithm for weighted point sets | http://arxiv.org/abs/1308.4004 | author:Steffen Borgwardt, Andreas Brieden, Peter Gritzmann category:math.OC cs.LG stat.ML published:2013-08-19 summary:The classical k-means algorithm for paritioning n points in R^d into ksubsets is one of the most popular and widely spread clustering methods inscientific and business applications. The present paper gives a generalizationthat is capable of handling weighted point sets and prescribed lower and upperbounds on the cluster sizes. The new algorithm replaces the assignment step ofk-means by the computation of a weight-balanced least-squares assignment. Thisis modelled as a linear program over a weight-balanced partition polytope whoseoptimal vertices correspond to clusterings that allow strongly feasible powerdiagrams. We use this correspondence to derive a worst-case upper bound n^O(dk)for the number of operations. This is similar to the known upper bound fork-means, polynomial for fixed k and d, and in view of the known complexityresults for k-means, essentially the best one can expect. Further, we show thekernelizability of our approach.
arxiv-4500-278 | A Study on Stroke Rehabilitation through Task-Oriented Control of a Haptic Device via Near-Infrared Spectroscopy-Based BCI | http://arxiv.org/abs/1308.4017 | author:Berdakh Abibullaev, Jinung An, Seung-Hyun Lee, Jeon-Il Moon category:stat.ML cs.HC q-bio.NC published:2013-08-19 summary:This paper presents a study in task-oriented approach to strokerehabilitation by controlling a haptic device via near-infraredspectroscopy-based brain-computer interface (BCI). The task is to command thehaptic device to move in opposing directions of leftward and rightwardmovement. Our study consists of data acquisition, signal preprocessing, andclassification. In data acquisition, we conduct experiments based on twodifferent mental tasks: one on pure motor imagery, and another on combinedmotor imagery and action observation. The experiments were conducted in bothoffline and online modes. In the signal preprocessing, we use localizationmethod to eliminate channels that are irrelevant to the mental task, as well asperform feature extraction for subsequent classification. We propose multiplesupport vector machine classifiers with a majority-voting scheme for improvedclassification results. And lastly, we present test results to demonstrate theefficacy of our proposed approach to possible stroke rehabilitation practice.
arxiv-4500-279 | Seeing What You're Told: Sentence-Guided Activity Recognition In Video | http://arxiv.org/abs/1308.4189 | author:N. Siddharth, Andrei Barbu, Jeffrey Mark Siskind category:cs.CV cs.AI cs.CL published:2013-08-19 summary:We present a system that demonstrates how the compositional structure ofevents, in concert with the compositional structure of language, can interplaywith the underlying focusing mechanisms in video action recognition, therebyproviding a medium, not only for top-down and bottom-up integration, but alsofor multi-modal integration between vision and language. We show how the rolesplayed by participants (nouns), their characteristics (adjectives), the actionsperformed (verbs), the manner of such actions (adverbs), and changing spatialrelations between participants (prepositions) in the form of whole sententialdescriptions mediated by a grammar, guides the activity-recognition process.Further, the utility and expressiveness of our framework is demonstrated byperforming three separate tasks in the domain of multi-activity videos:sentence-guided focus of attention, generation of sentential descriptions ofvideo, and query-based video search, simply by leveraging the framework indifferent manners.
arxiv-4500-280 | Distance Correlation Methods for Discovering Associations in Large Astrophysical Databases | http://arxiv.org/abs/1308.3925 | author:Elizabeth Martinez-Gomez, Mercedes T. Richards, Donald St. P. Richards category:astro-ph.CO math.ST stat.AP stat.ML stat.TH published:2013-08-19 summary:High-dimensional, large-sample astrophysical databases of galaxy clusters,such as the Chandra Deep Field South COMBO-17 database, provide measurements onmany variables for thousands of galaxies and a range of redshifts. Currentunderstanding of galaxy formation and evolution rests sensitively onrelationships between different astrophysical variables; hence an ability todetect and verify associations or correlations between variables is importantin astrophysical research. In this paper, we apply a recently definedstatistical measure called the distance correlation coefficient which can beused to identify new associations and correlations between astrophysicalvariables. The distance correlation coefficient applies to variables of anydimension; it can be used to determine smaller sets of variables that provideequivalent astrophysical information; it is zero only when variables areindependent; and it is capable of detecting nonlinear associations that areundetectable by the classical Pearson correlation coefficient. Hence, thedistance correlation coefficient provides more information than the Pearsoncoefficient. We analyze numerous pairs of variables in the COMBO-17 databasewith the distance correlation method and with the maximal informationcoefficient. We show that the Pearson coefficient can be estimated with higheraccuracy from the corresponding distance correlation coefficient than from themaximal information coefficient. For given values of the Pearson coefficient,the distance correlation method has a greater ability than the maximalinformation coefficient to resolve astrophysical data into highly concentratedV-shapes, which enhances classification and pattern identification. Theseresults are observed over a range of redshifts beyond the local universe andfor galaxies from elliptical to spiral.
arxiv-4500-281 | Support Recovery for the Drift Coefficient of High-Dimensional Diffusions | http://arxiv.org/abs/1308.4077 | author:Jose Bento, Morteza Ibrahimi category:cs.IT cs.LG math.IT math.PR math.ST stat.TH published:2013-08-19 summary:Consider the problem of learning the drift coefficient of a $p$-dimensionalstochastic differential equation from a sample path of length $T$. We assumethat the drift is parametrized by a high-dimensional vector, and study thesupport recovery problem when both $p$ and $T$ can tend to infinity. Inparticular, we prove a general lower bound on the sample-complexity $T$ byusing a characterization of mutual information as a time integral ofconditional variance, due to Kadota, Zakai, and Ziv. For linear stochasticdifferential equations, the drift coefficient is parametrized by a $p\times p$matrix which describes which degrees of freedom interact under the dynamics. Inthis case, we analyze a $\ell_1$-regularized least squares estimator and provean upper bound on $T$ that nearly matches the lower bound on specific classesof sparse matrices.
arxiv-4500-282 | A Likelihood Ratio Approach for Probabilistic Inequalities | http://arxiv.org/abs/1308.4123 | author:Xinjia Chen category:math.PR cs.LG math.ST stat.TH published:2013-08-18 summary:We propose a new approach for deriving probabilistic inequalities based onbounding likelihood ratios. We demonstrate that this approach is more generaland powerful than the classical method frequently used for derivingconcentration inequalities such as Chernoff bounds. We discover that theproposed approach is inherently related to statistical concepts such asmonotone likelihood ratio, maximum likelihood, and the method of moments forparameter estimation. A connection between the proposed approach and the largedeviation theory is also established. We show that, without using momentgenerating functions, tightest possible concentration inequalities may bereadily derived by the proposed approach. We have derived new concentrationinequalities using the proposed approach, which cannot be obtained by theclassical approach based on moment generating functions.
arxiv-4500-283 | Natural Language Web Interface for Database (NLWIDB) | http://arxiv.org/abs/1308.3830 | author:Rukshan Alexander, Prashanthi Rukshan, Sinnathamby Mahesan category:cs.CL cs.DB cs.HC published:2013-08-18 summary:It is a long term desire of the computer users to minimize the communicationgap between the computer and a human. On the other hand, almost all ICTapplications store information in to databases and retrieve from them.Retrieving information from the database requires knowledge of technicallanguages such as Structured Query Language. However majority of the computerusers who interact with the databases do not have a technical background andare intimidated by the idea of using languages such as SQL. For above reasons,a Natural Language Web Interface for Database (NLWIDB) has been developed. TheNLWIDB allows the user to query the database in a language more like English,through a convenient interface over the Internet.
arxiv-4500-284 | Consensus Sequence Segmentation | http://arxiv.org/abs/1308.3839 | author:Tamal Chowdhury, Rabindra Rakshit, Arko Banerjee category:cs.CL 68T10 published:2013-08-18 summary:In this paper we introduce a method to detect words or phrases in a givensequence of alphabets without knowing the lexicon. Our linear time unsupervisedalgorithm relies entirely on statistical relationships among alphabets in theinput sequence to detect location of word boundaries. We compare our algorithmto previous approaches from unsupervised sequence segmentation literature andprovide superior segmentation over number of benchmarks.
arxiv-4500-285 | Reference Distance Estimator | http://arxiv.org/abs/1308.3818 | author:Yanpeng Li category:cs.LG stat.ML published:2013-08-18 summary:A theoretical study is presented for a simple linear classifier calledreference distance estimator (RDE), which assigns the weight of each feature jas P(rj)-P(r), where r is a reference feature relevant to the target class y.The analysis shows that if r performs better than random guess in predicting yand is conditionally independent with each feature j, the RDE will have thesame classification performance as that from P(yj)-P(y), a classifier trainedwith the gold standard y. Since the estimation of P(rj)-P(r) does not requirelabeled data, under the assumption above, RDE trained with a large number ofunlabeled examples would be close to that trained with infinite labeledexamples. For the case the assumption does not hold, we theoretically analyzethe factors that influence the closeness of the RDE to the perfect one underthe assumption, and present an algorithm to select reference features andcombine multiple RDEs from different reference features using both labeled andunlabeled data. The experimental results on 10 text classification tasks showthat the semi-supervised learning method improves supervised methods using5,000 labeled examples and 13 million unlabeled ones, and in many tasks, itsperformance is even close to a classifier trained with 13 million labeledexamples. In addition, the bounds in the theorems provide good estimation ofthe classification performance and can be useful for new algorithm design.
arxiv-4500-286 | Development of Comprehensive Devnagari Numeral and Character Database for Offline Handwritten Character Recognition | http://arxiv.org/abs/1309.5357 | author:Vikas J. Dongre, Vijay H. Mankar category:cs.CV published:2013-08-17 summary:In handwritten character recognition, benchmark database plays an importantrole in evaluating the performance of various algorithms and the resultsobtained by various researchers. In Devnagari script, there is lack of suchofficial benchmark. This paper focuses on the generation of offline benchmarkdatabase for Devnagari handwritten numerals and characters. The present workgenerated 5137 and 20305 isolated samples for numeral and character database,respectively, from 750 writers of all ages, sex, education, and profession. Theoffline sample images are stored in TIFF image format as it occupies lessmemory. Also, the data is presented in binary level so that memory requirementis further reduced. It will facilitate research on handwriting recognition ofDevnagari script through free access to the researchers.
arxiv-4500-287 | Comment on "robustness and regularization of support vector machines" by H. Xu, et al., (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009, arXiv:0803.3490) | http://arxiv.org/abs/1308.3750 | author:Yahya Forghani, Hadi Sadoghi Yazdi category:cs.LG published:2013-08-17 summary:This paper comments on the published work dealing with robustness andregularization of support vector machines (Journal of Machine LearningResearch, vol. 10, pp. 1485-1510, 2009) [arXiv:0803.3490] by H. Xu, etc. Theyproposed a theorem to show that it is possible to relate robustness in thefeature space and robustness in the sample space directly. In this paper, wepropose a counter example that rejects their theorem.
arxiv-4500-288 | Graph Colouring Problem Based on Discrete Imperialist Competitive Algorithm | http://arxiv.org/abs/1308.3784 | author:Hojjat Emami, Shahriar Lotfi category:cs.AI cs.NE published:2013-08-17 summary:In graph theory, Graph Colouring Problem (GCP) is an assignment of colours tovertices of any given graph such that the colours on adjacent vertices aredifferent. The GCP is known to be an optimization and NP-hard problem.Imperialist Competitive Algorithm (ICA) is a meta-heuristic optimization andstochastic search strategy which is inspired from socio-political phenomenon ofimperialistic competition. The ICA contains two main operators: theassimilation and the imperialistic competition. The ICA has excellentcapabilities such as high convergence rate and better global optimumachievement. In this research, a discrete version of ICA is proposed to dealwith the solution of GCP. We call this algorithm as the DICA. The performanceof the proposed method is compared with Genetic Algorithm (GA) on sevenwell-known graph colouring benchmarks. Experimental results demonstrate thesuperiority of the DICA for the benchmarks. This means DICA can produce optimaland valid solutions for different GCP instances.
arxiv-4500-289 | Implementation Of Back-Propagation Neural Network For Isolated Bangla Speech Recognition | http://arxiv.org/abs/1308.3785 | author:Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan category:cs.CL cs.NE published:2013-08-17 summary:This paper is concerned with the development of Back-propagation NeuralNetwork for Bangla Speech Recognition. In this paper, ten bangla digits wererecorded from ten speakers and have been recognized. The features of thesespeech digits were extracted by the method of Mel Frequency CepstralCoefficient (MFCC) analysis. The mfcc features of five speakers were used totrain the network with Back propagation algorithm. The mfcc features of tenbangla digit speeches, from 0 to 9, of another five speakers were used to testthe system. All the methods and algorithms used in this research wereimplemented using the features of Turbo C and C++ languages. From ourinvestigation it is seen that the developed system can successfully encode andanalyze the mfcc features of the speech signal to recognition. The developedsystem achieved recognition rate about 96.332% for known speakers (i.e.,speaker dependent) and 92% for unknown speakers (i.e., speaker independent).
arxiv-4500-290 | Adaptive Independent Sticky MCMC algorithms | http://arxiv.org/abs/1308.3779 | author:L. Martino, R. Casarin, F. Leisen, D. Luengo category:stat.CO stat.ML published:2013-08-17 summary:In this work, we introduce a novel class of adaptive Monte Carlo methods,called adaptive independent sticky MCMC algorithms, for efficient sampling froma generic target probability density function (pdf). The new class ofalgorithms employs adaptive non-parametric proposal densities which becomecloser and closer to the target as the number of iterations increases. Theproposal pdf is built using interpolation procedures based on a set of supportpoints which is constructed iteratively based on previously drawn samples. Thealgorithm's efficiency is ensured by a test that controls the evolution of theset of support points. This extra stage controls the computational cost and theconvergence of the proposal density to the target. Each part of the novelfamily of algorithms is discussed and several examples are provided. Althoughthe novel algorithms are presented for univariate target densities, we showthat they can be easily extended to the multivariate context within aGibbs-type sampler. The ergodicity is ensured and discussed. Exhaustivenumerical examples illustrate the efficiency of sticky schemes, both as astand-alone methods to sample from complicated one-dimensional pdfs and withinGibbs in order to draw from multi-dimensional target distributions.
arxiv-4500-291 | Standardizing Interestingness Measures for Association Rules | http://arxiv.org/abs/1308.3740 | author:Mateen Shaikh, Paul D. McNicholas, M. Luiza Antonie, T. Brendan Murphy category:stat.AP cs.LG stat.ML published:2013-08-16 summary:Interestingness measures provide information that can be used to prune orselect association rules. A given value of an interestingness measure is ofteninterpreted relative to the overall range of the values that theinterestingness measure can take. However, properties of individual associationrules restrict the values an interestingness measure can achieve. Aninteresting measure can be standardized to take this into account, but this hasonly been done for one interestingness measure to date, i.e., the lift.Standardization provides greater insight than the raw value and may even alterresearchers' perception of the data. We derive standardized analogues of threeinterestingness measures and use real and simulated data to compare them totheir raw versions, each other, and the standardized lift.
arxiv-4500-292 | Genetic Algorithm for Solving Simple Mathematical Equality Problem | http://arxiv.org/abs/1308.4675 | author:Denny Hermawanto category:cs.NE published:2013-08-16 summary:This paper explains genetic algorithm for novice in this field. Basicphilosophy of genetic algorithm and its flowchart are described. Step by stepnumerical computation of genetic algorithm for solving simple mathematicalequality problem will be briefly explained
arxiv-4500-293 | Knapsack Constrained Contextual Submodular List Prediction with Application to Multi-document Summarization | http://arxiv.org/abs/1308.3541 | author:Jiaji Zhou, Stephane Ross, Yisong Yue, Debadeepta Dey, J. Andrew Bagnell category:cs.LG published:2013-08-16 summary:We study the problem of predicting a set or list of options under knapsackconstraint. The quality of such lists are evaluated by a submodular rewardfunction that measures both quality and diversity. Similar to DAgger (Ross etal., 2010), by a reduction to online learning, we show how to adapt twosequence prediction models to imitate greedy maximization under knapsackconstraint problems: CONSEQOPT (Dey et al., 2012) and SCP (Ross et al., 2013).Experiments on extractive multi-document summarization show that our approachoutperforms existing state-of-the-art methods.
arxiv-4500-294 | Fast Stochastic Alternating Direction Method of Multipliers | http://arxiv.org/abs/1308.3558 | author:Leon Wenliang Zhong, James T. Kwok category:cs.LG cs.NA published:2013-08-16 summary:In this paper, we propose a new stochastic alternating direction method ofmultipliers (ADMM) algorithm, which incrementally approximates the fullgradient in the linearized ADMM formulation. Besides having a low per-iterationcomplexity as existing stochastic ADMM algorithms, the proposed algorithmimproves the convergence rate on convex problems from $O(\frac 1 {\sqrt{T}})$to $O(\frac 1 T)$, where $T$ is the number of iterations. This matches theconvergence rate of the batch ADMM algorithm, but without the need to visit allthe samples in each iteration. Experiments on the graph-guided fused lassodemonstrate that the new algorithm is significantly faster thanstate-of-the-art stochastic and batch ADMM algorithms.
arxiv-4500-295 | High dimensional Sparse Gaussian Graphical Mixture Model | http://arxiv.org/abs/1308.3381 | author:Anani Lotsi, Ernst Wit category:stat.ML cs.LG published:2013-08-15 summary:This paper considers the problem of networks reconstruction fromheterogeneous data using a Gaussian Graphical Mixture Model (GGMM). It is wellknown that parameter estimation in this context is challenging due to largenumbers of variables coupled with the degeneracy of the likelihood. We proposeas a solution a penalized maximum likelihood technique by imposing an $l_{1}$penalty on the precision matrix. Our approach shrinks the parameters therebyresulting in better identifiability and variable selection. We use theExpectation Maximization (EM) algorithm which involves the graphical LASSO toestimate the mixing coefficients and the precision matrices. We show that undercertain regularity conditions the Penalized Maximum Likelihood (PML) estimatesare consistent. We demonstrate the performance of the PML estimator throughsimulations and we show the utility of our method for high dimensional dataanalysis in a genomic application.
arxiv-4500-296 | Computational Rationalization: The Inverse Equilibrium Problem | http://arxiv.org/abs/1308.3506 | author:Kevin Waugh, Brian D. Ziebart, J. Andrew Bagnell category:cs.GT cs.LG stat.ML published:2013-08-15 summary:Modeling the purposeful behavior of imperfect agents from a small number ofobservations is a challenging task. When restricted to the single-agentdecision-theoretic setting, inverse optimal control techniques assume thatobserved behavior is an approximately optimal solution to an unknown decisionproblem. These techniques learn a utility function that explains the examplebehavior and can then be used to accurately predict or imitate future behaviorin similar observed or unobserved situations. In this work, we consider similar tasks in competitive and cooperativemulti-agent domains. Here, unlike single-agent settings, a player cannotmyopically maximize its reward; it must speculate on how the other agents mayact to influence the game's outcome. Employing the game-theoretic notion ofregret and the principle of maximum entropy, we introduce a technique forpredicting and generalizing behavior.
arxiv-4500-297 | Axioms for graph clustering quality functions | http://arxiv.org/abs/1308.3383 | author:Twan van Laarhoven, Elena Marchiori category:cs.CV cs.LG stat.ML published:2013-08-15 summary:We investigate properties that intuitively ought to be satisfied by graphclustering quality functions, that is, functions that assign a score to aclustering of a graph. Graph clustering, also known as network communitydetection, is often performed by optimizing such a function. Two axiomstailored for graph clustering quality functions are introduced, and the fouraxioms introduced in previous work on distance based clustering arereformulated and generalized for the graph setting. We show that modularity, astandard quality function for graph clustering, does not satisfy all of thesesix properties. This motivates the derivation of a new family of qualityfunctions, adaptive scale modularity, which does satisfy the proposed axioms.Adaptive scale modularity has two parameters, which give greater flexibility inthe kinds of clusterings that can be found. Standard graph clustering qualityfunctions, such as normalized cut and unnormalized cut, are obtained as specialcases of adaptive scale modularity. In general, the results of our investigation indicate that the consideredaxiomatic framework covers existing `good' quality functions for graphclustering, and can be used to derive an interesting new family of qualityfunctions.
arxiv-4500-298 | Stochastic Optimization for Machine Learning | http://arxiv.org/abs/1308.3509 | author:Andrew Cotter category:cs.LG published:2013-08-15 summary:It has been found that stochastic algorithms often find good solutions muchmore rapidly than inherently-batch approaches. Indeed, a very useful rule ofthumb is that often, when solving a machine learning problem, an iterativetechnique which relies on performing a very large number ofrelatively-inexpensive updates will often outperform one which performs asmaller number of much "smarter" but computationally-expensive updates. In this thesis, we will consider the application of stochastic algorithms totwo of the most important machine learning problems. Part i is concerned withthe supervised problem of binary classification using kernelized linearclassifiers, for which the data have labels belonging to exactly two classes(e.g. "has cancer" or "doesn't have cancer"), and the learning problem is tofind a linear classifier which is best at predicting the label. In Part ii, wewill consider the unsupervised problem of Principal Component Analysis, forwhich the learning task is to find the directions which contain most of thevariance of the data distribution. Our goal is to present stochastic algorithms for both problems which are,above all, practical--they work well on real-world data, in some cases betterthan all known competing algorithms. A secondary, but still very important,goal is to derive theoretical bounds on the performance of these algorithmswhich are at least competitive with, and often better than, those known forother approaches.
arxiv-4500-299 | Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation | http://arxiv.org/abs/1308.3432 | author:Yoshua Bengio, Nicholas L√©onard, Aaron Courville category:cs.LG published:2013-08-15 summary:Stochastic neurons and hard non-linearities can be useful for a number ofreasons in deep learning models, but in many cases they pose a challengingproblem: how to estimate the gradient of a loss function with respect to theinput of such stochastic or non-smooth neurons? I.e., can we "back-propagate"through these stochastic neurons? We examine this question, existingapproaches, and compare four families of solutions, applicable in differentsettings. One of them is the minimum variance unbiased gradient estimator forstochatic binary neurons (a special case of the REINFORCE algorithm). A secondapproach, introduced here, decomposes the operation of a binary stochasticneuron into a stochastic binary part and a smooth differentiable part, whichapproximates the expected effect of the pure stochatic binary neuron to firstorder. A third approach involves the injection of additive or multiplicativenoise in a computational graph that is otherwise differentiable. A fourthapproach heuristically copies the gradient with respect to the stochasticoutput directly as an estimator of the gradient with respect to the sigmoidargument (we call this the straight-through estimator). To explore a contextwhere these estimators are useful, we consider a small-scale version of {\emconditional computation}, where sparse stochastic units form a distributedrepresentation of gaters that can turn off in combinatorially many ways largechunks of the computation performed in the rest of the neural network. In thiscase, it is important that the gating units produce an actual 0 most of thetime. The resulting sparsity can be potentially be exploited to greatly reducethe computational cost of large deep networks for which conditional computationwould be useful.
arxiv-4500-300 | Innovative Second-Generation Wavelets Construction With Recurrent Neural Networks for Solar Radiation Forecasting | http://arxiv.org/abs/1308.3524 | author:Giacomo Capizzi, Christian Napoli, Francesco Bonanno category:cs.NE published:2013-08-15 summary:Solar radiation prediction is an important challenge for the electricalengineer because it is used to estimate the power developed by commercialphotovoltaic modules. This paper deals with the problem of solar radiationprediction based on observed meteorological data. A 2-day forecast is obtainedby using novel wavelet recurrent neural networks (WRNNs). In fact, these WRNNSare used to exploit the correlation between solar radiation andtimescale-related variations of wind speed, humidity, and temperature. Theinput to the selected WRNN is provided by timescale-related bands of waveletcoefficients obtained from meteorological time series. The experimental setupavailable at the University of Catania, Italy, provided this information. Thenovelty of this approach is that the proposed WRNN performs the prediction inthe wavelet domain and, in addition, also performs the inverse wavelettransform, giving the predicted signal as output. The obtained simulationresults show a very low root-mean-square error compared to the results of thesolar radiation prediction approaches obtained by hybrid neural networksreported in the recent literature.
