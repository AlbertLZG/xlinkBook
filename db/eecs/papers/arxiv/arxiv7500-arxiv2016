arxiv-7500-1 | Subspace Restricted Boltzmann Machine | http://arxiv.org/pdf/1407.4422v1.pdf | author:Jakub M. Tomczak, Adam Gonczarek category:cs.LG published:2014-07-16 summary:The subspace Restricted Boltzmann Machine (subspaceRBM) is a third-orderBoltzmann machine where multiplicative interactions are between one visible andtwo hidden units. There are two kinds of hidden units, namely, gate units andsubspace units. The subspace units reflect variations of a pattern in data andthe gate unit is responsible for activating the subspace units. Additionally,the gate unit can be seen as a pooling feature. We evaluate the behavior ofsubspaceRBM through experiments with MNIST digit recognition task, measuringreconstruction error and classification error.
arxiv-7500-2 | Collaborative Filtering Ensemble for Personalized Name Recommendation | http://arxiv.org/pdf/1407.4832v1.pdf | author:Bernat Coma-Puig, Ernesto Diaz-Aviles, Wolfgang Nejdl category:cs.IR cs.AI cs.LG H.3.3; I.2.6 published:2014-07-16 summary:Out of thousands of names to choose from, picking the right one for yourchild is a daunting task. In this work, our objective is to help parents makingan informed decision while choosing a name for their baby. We follow arecommender system approach and combine, in an ensemble, the individualrankings produced by simple collaborative filtering algorithms in order toproduce a personalized list of names that meets the individual parents' taste.Our experiments were conducted using real-world data collected from the querylogs of 'nameling' (nameling.net), an online portal for searching and exploringnames, which corresponds to the dataset released in the context of the ECMLPKDD Discover Challenge 2013. Our approach is intuitive, easy to implement, andfeatures fast training and prediction steps.
arxiv-7500-3 | Probabilistic Group Testing under Sum Observations: A Parallelizable 2-Approximation for Entropy Loss | http://arxiv.org/pdf/1407.4446v3.pdf | author:Weidong Han, Purnima Rajan, Peter I. Frazier, Bruno M. Jedynak category:cs.IT cs.LG math.IT math.OC math.ST stat.ML stat.TH published:2014-07-16 summary:We consider the problem of group testing with sum observations and noiselessanswers, in which we aim to locate multiple objects by querying the number ofobjects in each of a sequence of chosen sets. We study a probabilistic settingwith entropy loss, in which we assume a joint Bayesian prior density on thelocations of the objects and seek to choose the sets queried to minimize theexpected entropy of the Bayesian posterior distribution after a fixed number ofquestions. We present a new non-adaptive policy, called the dyadic policy, showit is optimal among non-adaptive policies, and is within a factor of two ofoptimal among adaptive policies. This policy is quick to compute, itsnonadaptive nature makes it easy to parallelize, and our bounds show itperforms well even when compared with adaptive policies. We also study anadaptive greedy policy, which maximizes the one-step expected reduction inentropy, and show that it performs at least as well as the dyadic policy,offering greater query efficiency but reduced parallelism. Numericalexperiments demonstrate that both procedures outperform a divide-and-conquerbenchmark policy from the literature, called sequential bifurcation, and showhow these procedures may be applied in a stylized computer vision problem.
arxiv-7500-4 | A marginal sampler for $σ$-Stable Poisson-Kingman mixture models | http://arxiv.org/pdf/1407.4211v3.pdf | author:María Lomelí, Stefano Favaro, Yee Whye Teh category:stat.CO stat.ML published:2014-07-16 summary:We investigate the class of $\sigma$-stable Poisson-Kingman randomprobability measures (RPMs) in the context of Bayesian nonparametric mixturemodeling. This is a large class of discrete RPMs which encompasses most of thethe popular discrete RPMs used in Bayesian nonparametrics, such as theDirichlet process, Pitman-Yor process, the normalized inverse Gaussian processand the normalized generalized Gamma process. We show how certain samplingproperties and marginal characterizations of $\sigma$-stable Poisson-KingmanRPMs can be usefully exploited for devising a Markov chain Monte Carlo (MCMC)algorithm for making inference in Bayesian nonparametric mixture modeling.Specifically, we introduce a novel and efficient MCMC sampling scheme in anaugmented space that has a fixed number of auxiliary variables per iteration.We apply our sampling scheme for a density estimation and clustering tasks withunidimensional and multidimensional datasets, and we compare it againstcompeting sampling schemes.
arxiv-7500-5 | Sequential Logistic Principal Component Analysis (SLPCA): Dimensional Reduction in Streaming Multivariate Binary-State System | http://arxiv.org/pdf/1407.4430v1.pdf | author:Zhaoyi Kang, Costas J. Spanos category:stat.ML cs.LG stat.AP published:2014-07-16 summary:Sequential or online dimensional reduction is of interests due to theexplosion of streaming data based applications and the requirement of adaptivestatistical modeling, in many emerging fields, such as the modeling of energyend-use profile. Principal Component Analysis (PCA), is the classical way ofdimensional reduction. However, traditional Singular Value Decomposition (SVD)based PCA fails to model data which largely deviates from Gaussiandistribution. The Bregman Divergence was recently introduced to achieve ageneralized PCA framework. If the random variable under dimensional reductionfollows Bernoulli distribution, which occurs in many emerging fields, thegeneralized PCA is called Logistic PCA (LPCA). In this paper, we extend thebatch LPCA to a sequential version (i.e. SLPCA), based on the sequential convexoptimization theory. The convergence property of this algorithm is discussedcompared to the batch version of LPCA (i.e. BLPCA), as well as its performancein reducing the dimension for multivariate binary-state systems. Itsapplication in building energy end-use profile modeling is also investigated.
arxiv-7500-6 | Large scale canonical correlation analysis with iterative least squares | http://arxiv.org/pdf/1407.4508v2.pdf | author:Yichao Lu, Dean P. Foster category:stat.ML published:2014-07-16 summary:Canonical Correlation Analysis (CCA) is a widely used statistical tool withboth well established theory and favorable performance for a wide range ofmachine learning problems. However, computing CCA for huge datasets can be veryslow since it involves implementing QR decomposition or singular valuedecomposition of huge matrices. In this paper we introduce L-CCA, a iterativealgorithm which can compute CCA fast on huge sparse datasets. Theory on boththe asymptotic convergence and finite time accuracy of L-CCA are established.The experiments also show that L-CCA outperform other fast CCA approximationschemes on two real datasets.
arxiv-7500-7 | Mobile Camera Array Calibration for Light Field Acquisition | http://arxiv.org/pdf/1407.4206v1.pdf | author:Yichao Xu, Kazuki Maeno, Hajime Nagahara, Rin-ichiro Taniguchi category:cs.CV published:2014-07-16 summary:The light field camera is useful for computer graphics and visionapplications. Calibration is an essential step for these applications. Aftercalibration, we can rectify the captured image by using the calibrated cameraparameters. However, the large camera array calibration method, which assumesthat all cameras are on the same plane, ignores the orientation and intrinsicparameters. The multi-camera calibration technique usually assumes that theworking volume and viewpoints are fixed. In this paper, we describe acalibration algorithm suitable for a mobile camera array based light fieldacquisition system. The algorithm performs in Zhang's style by moving acheckerboard, and computes the initial parameters in closed form. Globaloptimization is then applied to refine all the parameters simultaneously. Ourimplementation is rather flexible in that users can assign the number ofviewpoints and refinement of intrinsic parameters is optional. Experiments onboth simulated data and real data acquired by a commercial product show thatour method yields good results. Digital refocusing application shows thecalibrated light field can well focus to the target object we desired.
arxiv-7500-8 | In Defense of MinHash Over SimHash | http://arxiv.org/pdf/1407.4416v1.pdf | author:Anshumali Shrivastava, Ping Li category:stat.CO cs.DS cs.IR cs.LG stat.ML published:2014-07-16 summary:MinHash and SimHash are the two widely adopted Locality Sensitive Hashing(LSH) algorithms for large-scale data processing applications. Deciding whichLSH to use for a particular problem at hand is an important question, which hasno clear answer in the existing literature. In this study, we provide atheoretical answer (validated by experiments) that MinHash virtually alwaysoutperforms SimHash when the data are binary, as common in practice such assearch. The collision probability of MinHash is a function of resemblance similarity($\mathcal{R}$), while the collision probability of SimHash is a function ofcosine similarity ($\mathcal{S}$). To provide a common basis for comparison, weevaluate retrieval results in terms of $\mathcal{S}$ for both MinHash andSimHash. This evaluation is valid as we can prove that MinHash is a valid LSHwith respect to $\mathcal{S}$, by using a general inequality $\mathcal{S}^2\leq\mathcal{R}\leq \frac{\mathcal{S}}{2-\mathcal{S}}$. Our worst case analysis canshow that MinHash significantly outperforms SimHash in high similarity region. Interestingly, our intensive experiments reveal that MinHash is alsosubstantially better than SimHash even in datasets where most of the datapoints are not too similar to each other. This is partly because, in practicaldata, often $\mathcal{R}\geq \frac{\mathcal{S}}{z-\mathcal{S}}$ holds where $z$is only slightly larger than 2 (e.g., $z\leq 2.1$). Our restricted worst caseanalysis by assuming $\frac{\mathcal{S}}{z-\mathcal{S}}\leq \mathcal{R}\leq\frac{\mathcal{S}}{2-\mathcal{S}}$ shows that MinHash indeed significantlyoutperforms SimHash even in low similarity region. We believe the results in this paper will provide valuable guidelines forsearch in practice, especially when the data are sparse.
arxiv-7500-9 | On the Complexity of Best Arm Identification in Multi-Armed Bandit Models | http://arxiv.org/pdf/1407.4443v1.pdf | author:Emilie Kaufmann, Olivier Cappé, Aurélien Garivier category:stat.ML cs.LG published:2014-07-16 summary:The stochastic multi-armed bandit model is a simple abstraction that hasproven useful in many different contexts in statistics and machine learning.Whereas the achievable limit in terms of regret minimization is now well known,our aim is to contribute to a better understanding of the performance in termsof identifying the m best arms. We introduce generic notions of complexity forthe two dominant frameworks considered in the literature: fixed-budget andfixed-confidence settings. In the fixed-confidence setting, we provide thefirst known distribution-dependent lower bound on the complexity that involvesinformation-theoretic quantities and holds when m is larger than 1 undergeneral assumptions. In the specific case of two armed-bandits, we deriverefined lower bounds in both the fixed-confidence and fixed-budget settings,along with matching algorithms for Gaussian and Bernoulli bandit models. Theseresults show in particular that the complexity of the fixed-budget setting maybe smaller than the complexity of the fixed-confidence setting, contradictingthe familiar behavior observed when testing fully specified alternatives. Inaddition, we also provide improved sequential stopping rules that haveguaranteed error probabilities and shorter average running times. The proofsrely on two technical results that are of independent interest : a deviationlemma for self-normalized sums (Lemma 19) and a novel change of measureinequality for bandit models (Lemma 1).
arxiv-7500-10 | Kernel Nonnegative Matrix Factorization Without the Curse of the Pre-image - Application to Unmixing Hyperspectral Images | http://arxiv.org/pdf/1407.4420v2.pdf | author:Fei Zhu, Paul Honeine, Maya Kallas category:cs.CV cs.IT cs.LG cs.NE math.IT stat.ML published:2014-07-16 summary:The nonnegative matrix factorization (NMF) is widely used in signal and imageprocessing, including bio-informatics, blind source separation andhyperspectral image analysis in remote sensing. A great challenge arises whendealing with a nonlinear formulation of the NMF. Within the framework of kernelmachines, the models suggested in the literature do not allow therepresentation of the factorization matrices, which is a fallout of the curseof the pre-image. In this paper, we propose a novel kernel-based model for theNMF that does not suffer from the pre-image problem, by investigating theestimation of the factorization matrices directly in the input space. Fordifferent kernel functions, we describe two schemes for iterative algorithms:an additive update rule based on a gradient descent scheme and a multiplicativeupdate rule in the same spirit as in the Lee and Seung algorithm. Within theproposed framework, we develop several extensions to incorporate constraints,including sparseness, smoothness, and spatial regularization with atotal-variation-like penalty. The effectiveness of the proposed method isdemonstrated with the problem of unmixing hyperspectral images, usingwell-known real images and results with state-of-the-art techniques.
arxiv-7500-11 | Online Asynchronous Distributed Regression | http://arxiv.org/pdf/1407.4373v1.pdf | author:Gérard Biau, Ryad Zenine category:math.ST stat.ML stat.TH published:2014-07-16 summary:Distributed computing offers a high degree of flexibility to accommodatemodern learning constraints and the ever increasing size of datasets involvedin massive data issues. Drawing inspiration from the theory of distributedcomputation models developed in the context of gradient-type optimizationalgorithms, we present a consensus-based asynchronous distributed approach fornonparametric online regression and analyze some of its asymptotic properties.Substantial numerical evidence involving up to 28 parallel processors isprovided on synthetic datasets to assess the excellent performance of ourmethod, both in terms of computation time and prediction accuracy.
arxiv-7500-12 | Aggregate channel features for multi-view face detection | http://arxiv.org/pdf/1407.4023v2.pdf | author:Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li category:cs.CV published:2014-07-15 summary:Face detection has drawn much attention in recent decades since the seminalwork by Viola and Jones. While many subsequences have improved the work withmore powerful learning algorithms, the feature representation used for facedetection still can't meet the demand for effectively and efficiently handlingfaces with large appearance variance in the wild. To solve this bottleneck, weborrow the concept of channel features to the face detection domain, whichextends the image channel to diverse types like gradient magnitude and orientedgradient histograms and therefore encodes rich information in a simple form. Weadopt a novel variant called aggregate channel features, make a fullexploration of feature design, and discover a multi-scale version of featureswith better performance. To deal with poses of faces in the wild, we propose amulti-view detection approach featuring score re-ranking and detectionadjustment. Following the learning pipelines in Viola-Jones framework, themulti-view face detector using aggregate channel features shows competitiveperformance against state-of-the-art algorithms on AFW and FDDB testsets, whileruns at 42 FPS on VGA images.
arxiv-7500-13 | Bayesian Network Structure Learning Using Quantum Annealing | http://arxiv.org/pdf/1407.3897v2.pdf | author:Bryan O'Gorman, Alejandro Perdomo-Ortiz, Ryan Babbush, Alan Aspuru-Guzik, Vadim Smelyanskiy category:quant-ph cs.LG published:2014-07-15 summary:We introduce a method for the problem of learning the structure of a Bayesiannetwork using the quantum adiabatic algorithm. We do so by introducing anefficient reformulation of a standard posterior-probability scoring function ongraphs as a pseudo-Boolean function, which is equivalent to a system of 2-bodyIsing spins, as well as suitable penalty terms for enforcing the constraintsnecessary for the reformulation; our proposed method requires $\mathcal O(n^2)$qubits for $n$ Bayesian network variables. Furthermore, we prove lower boundson the necessary weighting of these penalty terms. The logical structureresulting from the mapping has the appealing property that it isinstance-independent for a given number of Bayesian network variables, as wellas being independent of the number of data cases.
arxiv-7500-14 | Subjectivity, Bayesianism, and Causality | http://arxiv.org/pdf/1407.4139v4.pdf | author:Pedro A. Ortega category:cs.AI stat.ME stat.ML published:2014-07-15 summary:Bayesian probability theory is one of the most successful frameworks to modelreasoning under uncertainty. Its defining property is the interpretation ofprobabilities as degrees of belief in propositions about the state of the worldrelative to an inquiring subject. This essay examines the notion ofsubjectivity by drawing parallels between Lacanian theory and Bayesianprobability theory, and concludes that the latter must be enriched with causalinterventions to model agency. The central contribution of this work is anabstract model of the subject that accommodates causal interventions in ameasure-theoretic formalisation. This formalisation is obtained through agame-theoretic Ansatz based on modelling the inside and outside of the subjectas an extensive-form game with imperfect information between two players.Finally, I illustrate the expressiveness of this model with an example ofcausal induction.
arxiv-7500-15 | Fast matrix completion without the condition number | http://arxiv.org/pdf/1407.4070v1.pdf | author:Moritz Hardt, Mary Wootters category:cs.LG cs.DS stat.ML published:2014-07-15 summary:We give the first algorithm for Matrix Completion whose running time andsample complexity is polynomial in the rank of the unknown target matrix,linear in the dimension of the matrix, and logarithmic in the condition numberof the matrix. To the best of our knowledge, all previous algorithms eitherincurred a quadratic dependence on the condition number of the unknown matrixor a quadratic dependence on the dimension of the matrix in the running time.Our algorithm is based on a novel extension of Alternating Minimization whichwe show has theoretical guarantees under standard assumptions even in thepresence of noise.
arxiv-7500-16 | An iterative approach to Hough transform without re-voting | http://arxiv.org/pdf/1407.3969v1.pdf | author:Giorgio Ricca, Mauro C. Beltrametti, Anna Maria Massone category:cs.CV 68T45, 68U10 published:2014-07-15 summary:Many bone shapes in the human skeleton are characterized by profiles that canbe associated to equations of algebraic curves. Fixing the parameters in thecurve equation, by means of a classical pattern recognition procedure like theHough transform technique, it is then possible to associate an equation to aspecific bone profile. However, most skeleton districts are more accuratelydescribed by piecewise defined curves. This paper utilizes an iterativeapproach of the Hough transform without re-voting, to provide an efficientprocedure for describing the profile of a bone in the human skeleton as acollection of different but continuously attached curves.
arxiv-7500-17 | Uncertainty And Evolutionary Optimization: A Novel Approach | http://arxiv.org/pdf/1407.4000v2.pdf | author:Maumita Bhattacharya, R. Islam, A. Mahmood category:cs.NE 68T99 F.1.1 published:2014-07-15 summary:Evolutionary algorithms (EA) have been widely accepted as efficient solversfor complex real world optimization problems, including engineeringoptimization. However, real world optimization problems often involve uncertainenvironment including noisy and/or dynamic environments, which pose majorchallenges to EA-based optimization. The presence of noise interferes with theevaluation and the selection process of EA, and thus adversely affects itsperformance. In addition, as presence of noise poses challenges to theevaluation of the fitness function, it may need to be estimated instead ofbeing evaluated. Several existing approaches attempt to address this problem,such as introduction of diversity (hyper mutation, random immigrants, specialoperators) or incorporation of memory of the past (diploidy, case basedmemory). However, these approaches fail to adequately address the problem. Inthis paper we propose a Distributed Population Switching Evolutionary Algorithm(DPSEA) method that addresses optimization of functions with noisy fitnessusing a distributed population switching architecture, to simulate adistributed self-adaptive memory of the solution space. Local regression isused in the pseudo-populations to estimate the fitness. Successful applicationsto benchmark test problems ascertain the proposed method's superior performancein terms of both robustness and accuracy.
arxiv-7500-18 | Machine Learning Classification of SDSS Transient Survey Images | http://arxiv.org/pdf/1407.4118v3.pdf | author:L. du Buisson, N. Sivanandam, B. A. Bassett, M. Smith category:astro-ph.IM astro-ph.CO cs.CV published:2014-07-15 summary:We show that multiple machine learning algorithms can match human performancein classifying transient imaging data from the Sloan Digital Sky Survey (SDSS)supernova survey into real objects and artefacts. This is a first step in anytransient science pipeline and is currently still done by humans, but futuresurveys such as the Large Synoptic Survey Telescope (LSST) will necessitatefully machine-enabled solutions. Using features trained from eigenimageanalysis (principal component analysis, PCA) of single-epoch g, r andi-difference images, we can reach a completeness (recall) of 96 per cent, whileonly incorrectly classifying at most 18 per cent of artefacts as real objects,corresponding to a precision (purity) of 84 per cent. In general, randomforests performed best, followed by the k-nearest neighbour and the SkyNetartificial neural net algorithms, compared to other methods such as na\"iveBayes and kernel support vector machine. Our results show that PCA-basedmachine learning can match human success levels and can naturally be extendedby including multiple epochs of data, transient colours and host galaxyinformation which should allow for significant further improvements, especiallyat low signal-to-noise.
arxiv-7500-19 | Controlled Natural Language Processing as Answer Set Programming: an Experiment | http://arxiv.org/pdf/1408.2466v1.pdf | author:Rolf Schwitter category:cs.CL cs.AI published:2014-07-15 summary:Most controlled natural languages (CNLs) are processed with the help of apipeline architecture that relies on different software components. Weinvestigate in this paper in an experimental way how well answer setprogramming (ASP) is suited as a unifying framework for parsing a CNL, derivinga formal representation for the resulting syntax trees, and for reasoning withthat representation. We start from a list of input tokens in ASP notation andshow how this input can be transformed into a syntax tree using an ASP grammarand then into reified ASP rules in form of a set of facts. These facts are thenprocessed by an ASP meta-interpreter that allows us to infer new knowledge.
arxiv-7500-20 | Part-based R-CNNs for Fine-grained Category Detection | http://arxiv.org/pdf/1407.3867v1.pdf | author:Ning Zhang, Jeff Donahue, Ross Girshick, Trevor Darrell category:cs.CV published:2014-07-15 summary:Semantic part localization can facilitate fine-grained categorization byexplicitly isolating subtle appearance differences associated with specificobject parts. Methods for pose-normalized representations have been proposed,but generally presume bounding box annotations at test time due to thedifficulty of object detection. We propose a model for fine-grainedcategorization that overcomes these limitations by leveraging deepconvolutional features computed on bottom-up region proposals. Our methodlearns whole-object and part detectors, enforces learned geometric constraintsbetween them, and predicts a fine-grained category from a pose-normalizedrepresentation. Experiments on the Caltech-UCSD bird dataset confirm that ourmethod outperforms state-of-the-art fine-grained categorization methods in anend-to-end evaluation without requiring a bounding box at test time.
arxiv-7500-21 | Globally Optimal Joint Image Segmentation and Shape Matching Based on Wasserstein Modes | http://arxiv.org/pdf/1407.3956v2.pdf | author:Bernhard Schmitzer, Christoph Schnörr category:cs.CV 49Q10, 62H35 published:2014-07-15 summary:A functional for joint variational object segmentation and shape matching isdeveloped. The formulation is based on optimal transport w.r.t. geometricdistance and local feature similarity. Geometric invariance and modelling ofobject-typical statistical variations is achieved by introducing degrees offreedom that describe transformations and deformations of the shape template.The shape model is mathematically equivalent to contour-based approaches butinference can be performed without conversion between the contour and regionrepresentations, allowing combination with other convex segmentation approachesand simplifying optimization. While the overall functional is non-convex,non-convexity is confined to a low-dimensional variable. We propose a locallyoptimal alternating optimization scheme and a globally optimal branch and boundscheme, based on adaptive convex relaxation. Combining both methods allows toeliminate the delicate initialization problem inherent to many contour basedapproaches while remaining computationally practical. The properties of thefunctional, its ability to adapt to a wide range of input data structures andthe different optimization schemes are illustrated and compared by numericalexperiments.
arxiv-7500-22 | Analysis of purely random forests bias | http://arxiv.org/pdf/1407.3939v1.pdf | author:Sylvain Arlot, Robin Genuer category:math.ST cs.LG stat.ME stat.TH published:2014-07-15 summary:Random forests are a very effective and commonly used statistical method, buttheir full theoretical analysis is still an open problem. As a first step,simplified models such as purely random forests have been introduced, in orderto shed light on the good performance of random forests. In this paper, westudy the approximation error (the bias) of some purely random forest models ina regression framework, focusing in particular on the influence of the numberof trees in the forest. Under some regularity assumptions on the regressionfunction, we show that the bias of an infinite forest decreases at a fasterrate (with respect to the size of each tree) than a single tree. As aconsequence, infinite forests attain a strictly better risk rate (with respectto the sample size) than single trees. Furthermore, our results allow to derivea minimum number of trees sufficient to reach the same rate as an infiniteforest. As a by-product of our analysis, we also show a link between the biasof purely random forests and the bias of some kernel estimators.
arxiv-7500-23 | Automatic discovery of cell types and microcircuitry from neural connectomics | http://arxiv.org/pdf/1407.4137v1.pdf | author:Eric Jonas, Konrad Kording category:q-bio.NC stat.ML published:2014-07-15 summary:Neural connectomics has begun producing massive amounts of data,necessitating new analysis methods to discover the biological and computationalstructure. It has long been assumed that discovering neuron types and theirrelation to microcircuitry is crucial to understanding neural function. Here wedeveloped a nonparametric Bayesian technique that identifies neuron types andmicrocircuitry patterns in connectomics data. It combines the informationtraditionally used by biologists, including connectivity, cell body locationand the spatial distribution of synapses, in a principled andprobabilistically-coherent manner. We show that the approach recovers knownneuron types in the retina and enables predictions of connectivity, better thansimpler algorithms. It also can reveal interesting structure in the nervoussystem of C. elegans, and automatically discovers the structure of amicroprocessor. Our approach extracts structural meaning from connectomics,enabling new approaches of automatically deriving anatomical insights fromthese emerging datasets.
arxiv-7500-24 | Self Organization Map based Texture Feature Extraction for Efficient Medical Image Categorization | http://arxiv.org/pdf/1408.4143v1.pdf | author:Marghny H. Mohamed, Mohammed M. Abdelsamea category:cs.CV cs.NE published:2014-07-14 summary:Texture is one of the most important properties of visual surface that helpsin discriminating one object from another or an object from background. Theself-organizing map (SOM) is an excellent tool in exploratory phase of datamining. It projects its input space on prototypes of a low-dimensional regulargrid that can be effectively utilized to visualize and explore properties ofthe data. This paper proposes an enhancement extraction method for accurateextracting features for efficient image representation it based on SOM neuralnetwork. In this approach, we apply three different partitioning approaches asa region of interested (ROI) selection methods for extracting differentaccurate textural features from medical image as a primary step of ourextraction method. Fisherfaces feature selection is used, for selectingdiscriminated features form extracted textural features. Experimental resultshowed the high accuracy of medical image categorization with our proposedextraction method. Experiments held on Mammographic Image Analysis Society(MIAS) dataset.
arxiv-7500-25 | An Enhancement Neighborhood connected Segmentation for 2D-Cellular Image | http://arxiv.org/pdf/1407.3664v1.pdf | author:Mohammed M. Abdelsamea category:cs.CV published:2014-07-14 summary:A good segmentation result depends on a set of "correct" choice for theseeds. When the input images are noisy, the seeds may fall on atypical pixelsthat are not representative of the region statistics. This can lead toerroneous segmentation results. In this paper, an automatic seeded regiongrowing algorithm is proposed for cellular image segmentation. First, theregions of interest (ROIs) extracted from the preprocessed image. Second, theinitial seeds are automatically selected based on ROIs extracted from theimage. Third, the most reprehensive seeds are selected using a machine learningalgorithm. Finally, the cellular image is segmented into regions where eachregion corresponds to a seed. The aim of the proposed is to automaticallyextract the Region of Interests (ROI) from in the cellular images in terms ofovercoming the explosion, under segmentation and over segmentation problems.Experimental results show that the proposed algorithm can improve the segmentedimage and the segmented results are less noisy as compared to some existingalgorithms.
arxiv-7500-26 | Finding Motif Sets in Time Series | http://arxiv.org/pdf/1407.3685v1.pdf | author:Anthony Bagnall, Jon Hills, Jason Lines category:cs.LG cs.DB published:2014-07-14 summary:Time-series motifs are representative subsequences that occur frequently in atime series; a motif set is the set of subsequences deemed to be instances of agiven motif. We focus on finding motif sets. Our motivation is to detect motifsets in household electricity-usage profiles, representing repeated patterns ofhousehold usage. We propose three algorithms for finding motif sets. Two are greedy algorithmsbased on pairwise comparison, and the third uses a heuristic measure of setquality to find the motif set directly. We compare these algorithms onsimulated datasets and on electricity-usage data. We show that Scan MK, thesimplest way of using the best-matching pair to find motif sets, is lessaccurate on our synthetic data than Set Finder and Cluster MK, although thelatter is very sensitive to parameter settings. We qualitatively analyse theoutputs for the electricity-usage data and demonstrate that both Scan MK andSet Finder can discover useful motif sets in such data.
arxiv-7500-27 | Optimizing Auto-correlation for Fast Target Search in Large Search Space | http://arxiv.org/pdf/1407.3535v2.pdf | author:Arif Mahmood, Ajmal Mian, Robyn Owens category:cs.CV published:2014-07-14 summary:In remote sensing image-blurring is induced by many sources such asatmospheric scatter, optical aberration, spatial and temporal sensorintegration. The natural blurring can be exploited to speed up target search byfast template matching. In this paper, we synthetically induce additionalnon-uniform blurring to further increase the speed of the matching process. Toavoid loss of accuracy, the amount of synthetic blurring is varied spatiallyover the image according to the underlying content. We extend transitivealgorithm for fast template matching by incorporating controlled image blur. Tothis end we propose an Efficient Group Size (EGS) algorithm which minimizes thenumber of similarity computations for a particular search image. A largerefficient group size guarantees less computations and more speedup. EGSalgorithm is used as a component in our proposed Optimizing auto-correlation(OptA) algorithm. In OptA a search image is iteratively non-uniformly blurredwhile ensuring no accuracy degradation at any image location. In each iterationefficient group size and overall computations are estimated by using theproposed EGS algorithm. The OptA algorithm stops when the number ofcomputations cannot be further decreased without accuracy degradation. Theproposed algorithm is compared with six existing state of the art exhaustiveaccuracy techniques using correlation coefficient as the similarity measure.Experiments on satellite and aerial image datasets demonstrate theeffectiveness of the proposed algorithm.
arxiv-7500-28 | Performance Guarantees for Schatten-$p$ Quasi-Norm Minimization in Recovery of Low-Rank Matrices | http://arxiv.org/pdf/1407.3716v2.pdf | author:Mohammadreza Malek-Mohammadi, Massoud Babaie-Zadeh, Mikael Skoglund category:cs.IT math.IT stat.ML published:2014-07-14 summary:We address some theoretical guarantees for Schatten-$p$ quasi-normminimization ($p \in (0,1]$) in recovering low-rank matrices from compressedlinear measurements. Firstly, using null space properties of the measurementoperator, we provide a sufficient condition for exact recovery of low-rankmatrices. This condition guarantees unique recovery of matrices of ranks equalor larger than what is guaranteed by nuclear norm minimization. Secondly, thissufficient condition leads to a theorem proving that all restricted isometryproperty (RIP) based sufficient conditions for $\ell_p$ quasi-norm minimizationgeneralize to Schatten-$p$ quasi-norm minimization. Based on this theorem, weprovide a few RIP-based recovery conditions.
arxiv-7500-29 | Spatiotemporal Stacked Sequential Learning for Pedestrian Detection | http://arxiv.org/pdf/1407.3686v1.pdf | author:Alejandro González, Sebastian Ramos, David Vázquez, Antonio M. López, Jaume Amores category:cs.CV published:2014-07-14 summary:Pedestrian classifiers decide which image windows contain a pedestrian. Inpractice, such classifiers provide a relatively high response at neighborwindows overlapping a pedestrian, while the responses around potential falsepositives are expected to be lower. An analogous reasoning applies for imagesequences. If there is a pedestrian located within a frame, the same pedestrianis expected to appear close to the same location in neighbor frames. Therefore,such a location has chances of receiving high classification scores duringseveral frames, while false positives are expected to be more spurious. In thispaper we propose to exploit such correlations for improving the accuracy ofbase pedestrian classifiers. In particular, we propose to use two-stageclassifiers which not only rely on the image descriptors required by the baseclassifiers but also on the response of such base classifiers in a givenspatiotemporal neighborhood. More specifically, we train pedestrian classifiersusing a stacked sequential learning (SSL) paradigm. We use a new pedestriandataset we have acquired from a car to evaluate our proposal at different framerates. We also test on a well known dataset: Caltech. The obtained results showthat our SSL proposal boosts detection accuracy significantly with a minimalimpact on the computational cost. Interestingly, SSL improves more the accuracyat the most dangerous situations, i.e. when a pedestrian is close to thecamera.
arxiv-7500-30 | Toward Network-based Keyword Extraction from Multitopic Web Documents | http://arxiv.org/pdf/1407.3636v1.pdf | author:Sabina Šišović, Sanda Martinčić-Ipšić, Ana Meštrović category:cs.CL cs.IR published:2014-07-14 summary:In this paper we analyse the selectivity measure calculated from the complexnetwork in the task of the automatic keyword extraction. Texts, collected fromdifferent web sources (portals, forums), are represented as directed andweighted co-occurrence complex networks of words. Words are nodes and links areestablished between two nodes if they are directly co-occurring within thesentence. We test different centrality measures for ranking nodes - keywordcandidates. The promising results are achieved using the selectivity measure.Then we propose an approach which enables extracting word pairs according tothe values of the in/out selectivity and weight measures combined withfiltering.
arxiv-7500-31 | Depth Reconstruction from Sparse Samples: Representation, Algorithm, and Sampling | http://arxiv.org/pdf/1407.3840v4.pdf | author:Lee-Kang Liu, Stanley H. Chan, Truong Q. Nguyen category:cs.CV published:2014-07-14 summary:The rapid development of 3D technology and computer vision applications havemotivated a thrust of methodologies for depth acquisition and estimation.However, most existing hardware and software methods have limited performancedue to poor depth precision, low resolution and high computational cost. Inthis paper, we present a computationally efficient method to recover densedepth maps from sparse measurements. We make three contributions. First, weprovide empirical evidence that depth maps can be encoded much more sparselythan natural images by using common dictionaries such as wavelets andcontourlets. We also show that a combined wavelet-contourlet dictionaryachieves better performance than using either dictionary alone. Second, wepropose an alternating direction method of multipliers (ADMM) to achieve fastreconstruction. A multi-scale warm start procedure is proposed to speed up theconvergence. Third, we propose a two-stage randomized sampling scheme tooptimally choose the sampling locations, thus maximizing the reconstructionperformance for any given sampling budget. Experimental results show that theproposed method produces high quality dense depth estimates, and is robust tonoisy measurements. Applications to real data in stereo matching aredemonstrated.
arxiv-7500-32 | Finding representative sets of optimizations for adaptive multiversioning applications | http://arxiv.org/pdf/1407.4075v1.pdf | author:Lianjie Luo, Yang Chen, Chengyong Wu, Shun Long, Grigori Fursin category:cs.PL cs.LG published:2014-07-14 summary:Iterative compilation is a widely adopted technique to optimize programs fordifferent constraints such as performance, code size and power consumption inrapidly evolving hardware and software environments. However, in case ofstatically compiled programs, it is often restricted to optimizations for aspecific dataset and may not be applicable to applications that exhibitdifferent run-time behavior across program phases, multiple datasets or whenexecuted in heterogeneous, reconfigurable and virtual environments. Severalframeworks have been recently introduced to tackle these problems and enablerun-time optimization and adaptation for statically compiled programs based onstatic function multiversioning and monitoring of online program behavior. Inthis article, we present a novel technique to select a minimal set ofrepresentative optimization variants (function versions) for such frameworkswhile avoiding performance loss across available datasets and code-sizeexplosion. We developed a novel mapping mechanism using popular decision treeor rule induction based machine learning techniques to rapidly select best codeversions at run-time based on dataset features and minimize selection overhead.These techniques enable creation of self-tuning static binaries or librariesadaptable to changing behavior and environments at run-time using stagedcompilation that do not require complex recompilation frameworks whileeffectively outperforming traditional single-version non-adaptable code.
arxiv-7500-33 | Benchmarking Named Entity Disambiguation approaches for Streaming Graphs | http://arxiv.org/pdf/1407.3751v1.pdf | author:Sutanay Choudhury, Chase Dowling category:cs.CL cs.IR published:2014-07-14 summary:Named Entity Disambiaguation (NED) is a central task for applications dealingwith natural language text. Assume that we have a graph based knowledge base(subsequently referred as Knowledge Graph) where nodes represent various realworld entities such as people, location, organization and concepts. Given datasources such as social media streams and web pages Entity Linking is the taskof mapping named entities that are extracted from the data to those present inthe Knowledge Graph. This is an inherently difficult task due to severalreasons. Almost all these data sources are generated without any formalontology; the unstructured nature of the input, limited context and theambiguity involved when multiple entities are mapped to the same name make thisa hard task. This report looks at two state of the art systems employing twodistinctive approaches: graph based Accurate Online Disambiguation of Entities(AIDA) and Mined Evidence Named Entity Disambiguation (MENED), which employs astatistical inference approach. We compare both approaches using the data setand queries provided by the Knowledge Base Population (KBP) track at 2011 NISTText Analytics Conference (TAC). This report begins with an overview of therespective approaches, followed by detailed description of the experimentalsetup. It concludes with our findings from the benchmarking exercise.
arxiv-7500-34 | A Framework for Exploring Non-Linear Functional Connectivity and Causality in the Human Brain: Mutual Connectivity Analysis (MCA) of Resting-State Functional MRI with Convergent Cross-Mapping and Non-Metric Clustering | http://arxiv.org/pdf/1407.3809v1.pdf | author:Axel Wismüller, Xixi Wang, Adora M. DSouza, Mahesh B. Nagarajan category:cs.NE q-bio.NC published:2014-07-14 summary:We present a computational framework for analysis and visualization ofnon-linear functional connectivity in the human brain from resting statefunctional MRI (fMRI) data for purposes of recovering the underlying networkcommunity structure and exploring causality between network components. Ourproposed methodology of non-linear mutual connectivity analysis (MCA) involvestwo computational steps. First, the pair-wise cross-prediction performancebetween resting state fMRI pixel time series within the brain is evaluated. Theunderlying network structure is subsequently recovered from the affinity matrixconstructed through MCA using non-metric network partitioning/clustering withthe so-called Louvain method. We demonstrate our methodology in the task ofidentifying regions of the motor cortex associated with hand movement onresting state fMRI data acquired from eight slice locations in four subjects.For comparison, we also localized regions of the motor cortex through atask-based fMRI sequence involving a finger-tapping stimulus paradigm. Finally,we integrate convergent cross mapping (CCM) into the first step of MCA forinvestigating causality between regions of the motor cortex. Results regardingcausation between regions of the motor cortex revealed a significantdirectional variability and were not readily interpretable in a consistentmanner across all subjects. However, our results on whole-slice fMRI analysisdemonstrate that MCA-based model-free recovery of regions associated with theprimary motor cortex and supplementary motor area are in close agreement withlocalization of similar regions achieved with a task-based fMRI acquisition.Thus, we conclude that our computational framework MCA can extract andvisualize valuable information concerning the underlying network structure andcausation between different regions of the brain in resting state fMRI.
arxiv-7500-35 | Measuring Atmospheric Scattering from Digital Images of Urban Scenery using Temporal Polarization-Based Vision | http://arxiv.org/pdf/1407.3540v1.pdf | author:Tarek El-Gaaly, Joshua Gluckman category:cs.CV published:2014-07-14 summary:Particulate Matter (PM) is a form of air pollution that visually degradesurban scenery and is hazardous to human health and the environment. Currentmonitoring devices are limited in measuring average PM over large areas.Quantifying the visual effects of haze in digital images of urban scenery andcorrelating these effects to PM levels is a vital step in more practicallymonitoring our environment. Current image haze extraction algorithms removehaze from the scene for the sole purpose of enhancing vision. We present twoalgorithms which bridge the gap between image haze extraction and environmentalmonitoring. We provide a means of measuring atmospheric scattering from imagesof urban scenery by incorporating temporal knowledge. In doing so, we alsopresent a method of recovering an accurate depthmap of the scene and recoveringthe scene without the visual effects of haze. We compare our algorithm to threeknown haze removal methods. The algorithms are composed of an optimization overa model of haze formation in images and an optimization using a constraint ofconstant depth over a sequence of images taken over time. These algorithms notonly measure atmospheric scattering, but also recover a more accurate depthmapand dehazed image. The measurements of atmospheric scattering this researchproduces, can be directly correlated to PM levels and therefore pave the way tomonitoring the health of the environment by visual means. Accurate atmosphericsensing from digital images is a challenging and under-researched problem. Thiswork provides an important step towards a more practical and accurate visualmeans of measuring PM from digital images.
arxiv-7500-36 | On the Power of Adaptivity in Matrix Completion and Approximation | http://arxiv.org/pdf/1407.3619v1.pdf | author:Akshay Krishnamurthy, Aarti Singh category:stat.ML cs.LG published:2014-07-14 summary:We consider the related tasks of matrix completion and matrix approximationfrom missing data and propose adaptive sampling procedures for both problems.We show that adaptive sampling allows one to eliminate standard incoherenceassumptions on the matrix row space that are necessary for passive samplingprocedures. For exact recovery of a low-rank matrix, our algorithm judiciouslyselects a few columns to observe in full and, with few additional measurements,projects the remaining columns onto their span. This algorithm exactly recoversan $n \times n$ rank $r$ matrix using $O(nr\mu_0 \log^2(r))$ observations,where $\mu_0$ is a coherence parameter on the column space of the matrix. Inaddition to completely eliminating any row space assumptions that have pervadedthe literature, this algorithm enjoys a better sample complexity than anyexisting matrix completion algorithm. To certify that this improvement is dueto adaptive sampling, we establish that row space coherence is necessary forpassive sampling algorithms to achieve non-trivial sample complexity bounds. For constructing a low-rank approximation to a high-rank input matrix, wepropose a simple algorithm that thresholds the singular values of a zero-filledversion of the input matrix. The algorithm computes an approximation that isnearly as good as the best rank-$r$ approximation using $O(nr\mu \log^2(n))$samples, where $\mu$ is a slightly different coherence parameter on the matrixcolumns. Again we eliminate assumptions on the row space.
arxiv-7500-37 | Robots that can adapt like animals | http://arxiv.org/pdf/1407.3501v4.pdf | author:Antoine Cully, Jeff Clune, Danesh Tarapore, Jean-Baptiste Mouret category:cs.RO cs.AI cs.LG cs.NE q-bio.NC published:2014-07-13 summary:As robots leave the controlled environments of factories to autonomouslyfunction in more complex, natural environments, they will have to respond tothe inevitable fact that they will become damaged. However, while animals canquickly adapt to a wide variety of injuries, current robots cannot "thinkoutside the box" to find a compensatory behavior when damaged: they are limitedto their pre-specified self-sensing abilities, can diagnose only anticipatedfailure modes, and require a pre-programmed contingency plan for every type ofpotential damage, an impracticality for complex robots. Here we introduce anintelligent trial and error algorithm that allows robots to adapt to damage inless than two minutes, without requiring self-diagnosis or pre-specifiedcontingency plans. Before deployment, a robot exploits a novel algorithm tocreate a detailed map of the space of high-performing behaviors: This maprepresents the robot's intuitions about what behaviors it can perform and theirvalue. If the robot is damaged, it uses these intuitions to guide atrial-and-error learning algorithm that conducts intelligent experiments torapidly discover a compensatory behavior that works in spite of the damage.Experiments reveal successful adaptations for a legged robot injured in fivedifferent ways, including damaged, broken, and missing legs, and for a roboticarm with joints broken in 14 different ways. This new technique will enablemore robust, effective, autonomous robots, and suggests principles that animalsmay use to adapt to injury.
arxiv-7500-38 | A Spectral Algorithm for Inference in Hidden Semi-Markov Models | http://arxiv.org/pdf/1407.3422v3.pdf | author:Igor Melnyk, Arindam Banerjee category:stat.ML cs.LG published:2014-07-12 summary:Hidden semi-Markov models (HSMMs) are latent variable models which allowlatent state persistence and can be viewed as a generalization of the popularhidden Markov models (HMMs). In this paper, we introduce a novel spectralalgorithm to perform inference in HSMMs. Unlike expectation maximization (EM),our approach correctly estimates the probability of given observation sequencebased on a set of training sequences. Our approach is based on estimatingmoments from the sample, whose number of dimensions depends onlylogarithmically on the maximum length of the hidden state persistence.Moreover, the algorithm requires only a few matrix inversions and is thereforecomputationally efficient. Empirical evaluations on synthetic and real datademonstrate the advantage of the algorithm over EM in terms of speed andaccuracy, especially for large datasets.
arxiv-7500-39 | Offline to Online Conversion | http://arxiv.org/pdf/1407.3334v1.pdf | author:Marcus Hutter category:cs.LG cs.IT math.IT math.ST stat.CO stat.TH published:2014-07-12 summary:We consider the problem of converting offline estimators into an onlinepredictor or estimator with small extra regret. Formally this is the problem ofmerging a collection of probability measures over strings of length 1,2,3,...into a single probability measure over infinite sequences. We describe variousapproaches and their pros and cons on various examples. As a side-result wegive an elementary non-heuristic purely combinatoric derivation of Turing'sfamous estimator. Our main technical contribution is to determine thecomputational complexity of online estimators with good guarantees in general.
arxiv-7500-40 | Extreme State Aggregation Beyond MDPs | http://arxiv.org/pdf/1407.3341v1.pdf | author:Marcus Hutter category:cs.AI cs.LG published:2014-07-12 summary:We consider a Reinforcement Learning setup where an agent interacts with anenvironment in observation-reward-action cycles without any (esp.\ MDP)assumptions on the environment. State aggregation and more generally featurereinforcement learning is concerned with mapping histories/raw-states toreduced/aggregated states. The idea behind both is that the resulting reducedprocess (approximately) forms a small stationary finite-state MDP, which canthen be efficiently solved or learnt. We considerably generalize existingaggregation results by showing that even if the reduced process is not an MDP,the (q-)value functions and (optimal) policies of an associated MDP with samestate-space size solve the original problem, as long as the solution canapproximately be represented as a function of the reduced states. This impliesan upper bound on the required state space size that holds uniformly for all RLproblems. It may also explain why RL algorithms designed for MDPs sometimesperform well beyond MDPs.
arxiv-7500-41 | Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations | http://arxiv.org/pdf/1407.3399v2.pdf | author:Xianjie Chen, Alan Yuille category:cs.CV published:2014-07-12 summary:We present a method for estimating articulated human pose from a singlestatic image based on a graphical model with novel pairwise relations that makeadaptive use of local image measurements. More precisely, we specify agraphical model for human pose which exploits the fact the local imagemeasurements can be used both to detect parts (or joints) and also to predictthe spatial relationships between them (Image Dependent Pairwise Relations).These spatial relationships are represented by a mixture model. We use DeepConvolutional Neural Networks (DCNNs) to learn conditional probabilities forthe presence of parts and their spatial relationships within image patches.Hence our model combines the representational flexibility of graphical modelswith the efficiency and statistical power of DCNNs. Our method significantlyoutperforms the state of the art methods on the LSP and FLIC datasets and alsoperforms very well on the Buffy dataset without any training.
arxiv-7500-42 | Charge Scheduling of an Energy Storage System under Time-of-use Pricing and a Demand Charge | http://arxiv.org/pdf/1407.3077v1.pdf | author:Yourim Yoon, Yong-Hyuk Kim category:cs.NE published:2014-07-11 summary:A real-coded genetic algorithm is used to schedule the charging of an energystorage system (ESS), operated in tandem with renewable power by an electricityconsumer who is subject to time-of-use pricing and a demand charge. Simulationsbased on load and generation profiles of typical residential customers showthat an ESS scheduled by our algorithm can reduce electricity costs byapproximately 17%, compared to a system without an ESS, and by 8% compared to ascheduling algorithm based on net power.
arxiv-7500-43 | Multiple chaotic central pattern generators with learning for legged locomotion and malfunction compensation | http://arxiv.org/pdf/1407.3269v1.pdf | author:Guanjiao Ren, Weihai Chen, Sakyasingha Dasgupta, Christoph Kolodziejski, Florentin Wörgötter, Poramate Manoonpong category:cs.AI cs.LG cs.NE cs.RO I.2.9; I.2.6 published:2014-07-11 summary:An originally chaotic system can be controlled into various periodicdynamics. When it is implemented into a legged robot's locomotion control as acentral pattern generator (CPG), sophisticated gait patterns arise so that therobot can perform various walking behaviors. However, such a single chaotic CPGcontroller has difficulties dealing with leg malfunction. Specifically, in thescenarios presented here, its movement permanently deviates from the desiredtrajectory. To address this problem, we extend the single chaotic CPG tomultiple CPGs with learning. The learning mechanism is based on a simulatedannealing algorithm. In a normal situation, the CPGs synchronize and theirdynamics are identical. With leg malfunction or disability, the CPGs losesynchronization leading to independent dynamics. In this case, the learningmechanism is applied to automatically adjust the remaining legs' oscillationfrequencies so that the robot adapts its locomotion to deal with themalfunction. As a consequence, the trajectory produced by the multiple chaoticCPGs resembles the original trajectory far better than the one produced by onlya single CPG. The performance of the system is evaluated first in a physicalsimulation of a quadruped as well as a hexapod robot and finally in a realsix-legged walking machine called AMOSII. The experimental results presentedhere reveal that using multiple CPGs with learning is an effective approach foradaptive locomotion generation where, for instance, different body parts haveto perform independent movements for malfunction compensation.
arxiv-7500-44 | Altitude Training: Strong Bounds for Single-Layer Dropout | http://arxiv.org/pdf/1407.3289v2.pdf | author:Stefan Wager, William Fithian, Sida Wang, Percy Liang category:stat.ML cs.LG math.ST stat.TH published:2014-07-11 summary:Dropout training, originally designed for deep neural networks, has beensuccessful on high-dimensional single-layer natural language tasks. This paperproposes a theoretical explanation for this phenomenon: we show that, under agenerative Poisson topic model with long documents, dropout training improvesthe exponent in the generalization bound for empirical risk minimization.Dropout achieves this gain much like a marathon runner who practices ataltitude: once a classifier learns to perform reasonably well on trainingexamples that have been artificially corrupted by dropout, it will do very wellon the uncorrupted test set. We also show that, under similar conditions,dropout preserves the Bayes decision boundary and should therefore induceminimal bias in high dimensions.
arxiv-7500-45 | Deep Networks with Internal Selective Attention through Feedback Connections | http://arxiv.org/pdf/1407.3068v2.pdf | author:Marijn Stollenga, Jonathan Masci, Faustino Gomez, Juergen Schmidhuber category:cs.CV cs.LG cs.NE 68T45 published:2014-07-11 summary:Traditional convolutional neural networks (CNN) are stationary andfeedforward. They neither change their parameters during evaluation nor usefeedback from higher to lower layers. Real brains, however, do. So does ourDeep Attention Selective Network (dasNet) architecture. DasNets feedbackstructure can dynamically alter its convolutional filter sensitivities duringclassification. It harnesses the power of sequential processing to improveclassification performance, by allowing the network to iteratively focus itsinternal attention on some of its convolutional filters. Feedback is trainedthrough direct policy search in a huge million-dimensional parameter space,through scalable natural evolution strategies (SNES). On the CIFAR-10 andCIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.
arxiv-7500-46 | Optimally Stabilized PET Image Denoising Using Trilateral Filtering | http://arxiv.org/pdf/1407.3193v1.pdf | author:Awais Mansoor, Ulas Bagci, Daniel J. Mollura category:cs.CV published:2014-07-11 summary:Low-resolution and signal-dependent noise distribution in positron emissiontomography (PET) images makes denoising process an inevitable step prior toqualitative and quantitative image analysis tasks. Conventional PET denoisingmethods either over-smooth small-sized structures due to resolution limitationor make incorrect assumptions about the noise characteristics. Therefore,clinically important quantitative information may be corrupted. To addressthese challenges, we introduced a novel approach to remove signal-dependentnoise in the PET images where the noise distribution was considered asPoisson-Gaussian mixed. Meanwhile, the generalized Anscombe's transformation(GAT) was used to stabilize varying nature of the PET noise. Other than noisestabilization, it is also desirable for the noise removal filter to preservethe boundaries of the structures while smoothing the noisy regions. Indeed, itis important to avoid significant loss of quantitative information such asstandard uptake value (SUV)-based metrics as well as metabolic lesion volume.To satisfy all these properties, we extended bilateral filtering method intotrilateral filtering through multiscaling and optimal Gaussianization process.The proposed method was tested on more than 50 PET-CT images from variouspatients having different cancers and achieved the superior performancecompared to the widely used denoising techniques in the literature.
arxiv-7500-47 | An SVM Based Approach for Cardiac View Planning | http://arxiv.org/pdf/1407.3026v1.pdf | author:Ramasubramanian Sundararajan, Hima Patel, Dattesh Shanbhag, Vivek Vaidya category:cs.LG cs.CV published:2014-07-11 summary:We consider the problem of automatically prescribing oblique planes (shortaxis, 4 chamber and 2 chamber views) in Cardiac Magnetic Resonance Imaging(MRI). A concern with technologist-driven acquisitions of these planes is thequality and time taken for the total examination. We propose an automatedsolution incorporating anatomical features external to the cardiac region. Thesolution uses support vector machine regression models wherein complexity andfeature selection are optimized using multi-objective genetic algorithms.Additionally, we examine the robustness of our approach by training our modelson images with additive Rician-Gaussian mixtures at varying Signal to Noise(SNR) levels. Our approach has shown promising results, with an angulardeviation of less than 15 degrees on 90% cases across oblique planes, measuredin terms of average 6-fold cross validation performance -- this is generallywithin acceptable bounds of variation as specified by clinicians.
arxiv-7500-48 | CIDI-Lung-Seg: A Single-Click Annotation Tool for Automatic Delineation of Lungs from CT Scans | http://arxiv.org/pdf/1407.3176v1.pdf | author:Awais Mansoor, Ulas Bagci, Brent Foster, Ziyue Xu, Deborah Douglas, Jeffrey M. Solomon, Jayaram K. Udupa, Daniel J. Mollura category:cs.CV published:2014-07-11 summary:Accurate and fast extraction of lung volumes from computed tomography (CT)scans remains in a great demand in the clinical environment because theavailable methods fail to provide a generic solution due to wide anatomicalvariations of lungs and existence of pathologies. Manual annotation, currentgold standard, is time consuming and often subject to human bias. On the otherhand, current state-of-the-art fully automated lung segmentation methods failto make their way into the clinical practice due to their inability toefficiently incorporate human input for handling misclassifications and praxis.This paper presents a lung annotation tool for CT images that is interactive,efficient, and robust. The proposed annotation tool produces an "as accurate aspossible" initial annotation based on the fuzzy-connectedness imagesegmentation, followed by efficient manual fixation of the initial extractionif deemed necessary by the practitioner. To provide maximum flexibility to theusers, our annotation tool is supported in three major operating systems(Windows, Linux, and the Mac OS X). The quantitative results comparing our freesoftware with commercially available lung segmentation tools show higher degreeof consistency and precision of our software with a considerable potential toenhance the performance of routine clinical tasks.
arxiv-7500-49 | Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation | http://arxiv.org/pdf/1407.3179v1.pdf | author:Awais Mansoor, Ulas Bagci, Daniel J. Mollura category:cs.CV published:2014-07-11 summary:Accurate delineation of pathological lungs from computed tomography (CT)images remains mostly unsolved because available methods fail to provide areliable generic solution due to high variability of abnormality appearance.Local descriptor-based classification methods have shown to work well inannotating pathologies; however, these methods are usually computationallyintensive which restricts their widespread use in real-time or near-real-timeclinical applications. In this paper, we present a novel approach for fast,accurate, reliable segmentation of pathological lungs from CT scans bycombining region-based segmentation method with local descriptor classificationthat is performed on an optimized sampling grid. Our method works in twostages; during stage one, we adapted the fuzzy connectedness (FC) imagesegmentation algorithm to perform initial lung parenchyma extraction. In thesecond stage, texture-based local descriptors are utilized to segment abnormalimaging patterns using a near optimal keypoint analysis by employing centroidof supervoxel as grid points. The quantitative results show that ourpathological lung segmentation method is fast, robust, and improves on currentstandards and has potential to enhance the performance of routine clinicaltasks.
arxiv-7500-50 | Image Inpainting Using Directional Tensor Product Complex Tight Framelets | http://arxiv.org/pdf/1407.3234v1.pdf | author:Yi Shen, Bin Han, Elena Braverman category:cs.IT cs.CV math.IT published:2014-07-11 summary:In this paper we are particularly interested in the image inpainting problemusing directional complex tight wavelet frames. Under the assumption that framecoefficients of images are sparse, several iterative thresholding algorithmsfor the image inpainting problem have been proposed in the literature. Theoutputs of such iterative algorithms are closely linked to solutions of severalconvex minimization models using the balanced approach which simultaneouslycombines the $l_1$-regularization for sparsity of frame coefficients and the$l_2$-regularization for smoothness of the solution. Due to the redundancy of atight frame, elements of a tight frame could be highly correlated andtherefore, their corresponding frame coefficients of an image are expected toclose to each other. This is called the grouping effect in statistics. In thispaper, we establish the grouping effect property for frame-based convexminimization models using the balanced approach. This result on grouping effectpartially explains the effectiveness of models using the balanced approach forseveral image restoration problems. Inspired by recent development ondirectional tensor product complex tight framelets (TP-CTFs) and theirimpressive performance for the image denoising problem, in this paper wepropose an iterative thresholding algorithm using a single tight frame derivedfrom TP-CTFs for the image inpainting problem. Experimental results show thatour proposed algorithm can handle well both cartoons and texturessimultaneously and performs comparably and often better than several well-knownframe-based iterative thresholding algorithms for the image inpainting problemwithout noise. For the image inpainting problem with additive zero-mean i.i.d.Gaussian noise, our proposed algorithm using TP-CTFs performs superior thanother known state-of-the-art frame-based image inpainting algorithms.
arxiv-7500-51 | Biclustering Via Sparse Clustering | http://arxiv.org/pdf/1407.3010v1.pdf | author:Qian Liu, Guanhua Chen, Michael R. Kosorok, Eric Bair category:stat.ME stat.ML published:2014-07-11 summary:In many situations it is desirable to identify clusters that differ withrespect to only a subset of features. Such clusters may represent homogeneoussubgroups of patients with a disease, such as cancer or chronic pain. We definea bicluster to be a submatrix U of a larger data matrix X such that thefeatures and observations in U differ from those not contained in U. Forexample, the observations in U could have different means or variances withrespect to the features in U. We propose a general framework for biclusteringbased on the sparse clustering method of Witten and Tibshirani (2010). Wedevelop a method for identifying features that belong to biclusters. Thisframework can be used to identify biclusters that differ with respect to themeans of the features, the variance of the features, or more generaldifferences. We apply these methods to several simulated and real-world datasets and compare the results of our method with several previously publishedmethods. The results of our method compare favorably with existing methods withrespect to both predictive accuracy and computing time.
arxiv-7500-52 | A Proposed Infrastructure for Adding Online Interaction to Any Evolutionary Domain | http://arxiv.org/pdf/1407.3000v1.pdf | author:Paul Szerlip, Kenneth O. Stanley category:cs.NE published:2014-07-11 summary:To address the difficulty of creating online collaborative evolutionarysystems, this paper presents a new prototype library called WorldwideInfrastructure for Neuroevolution (WIN) and its accompanying site WIN Online(http://winark.org/). The WIN library is a collection of software packagesbuilt on top of Node.js that reduce the complexity of creating fullypersistent, online, and interactive (or automated) evolutionary platformsaround any domain. WIN Online is the public interface for WIN, providing anonline collection of domains built with the WIN library that lets novice andexpert users browse and meaningfully contribute to ongoing experiments. Thelong term goal of WIN is to make it trivial to connect any platform to theworld, providing both a stream of online users, and archives of data anddiscoveries for later extension by humans or computers.
arxiv-7500-53 | Density Adaptive Parallel Clustering | http://arxiv.org/pdf/1407.3242v1.pdf | author:Marcello La Rocca category:cs.DS cs.LG stat.ML 91C20 published:2014-07-11 summary:In this paper we are going to introduce a new nearest neighbours basedapproach to clustering, and compare it with previous solutions; the resultingalgorithm, which takes inspiration from both DBscan and minimum spanning treeapproaches, is deterministic but proves simpler, faster and doesnt require toset in advance a value for k, the number of clusters.
arxiv-7500-54 | Hidden Markov Model Based Part of Speech Tagger for Sinhala Language | http://arxiv.org/pdf/1407.2989v1.pdf | author:A. J. P. M. P. Jayaweera, N. G. J. Dias category:cs.CL I.2.7 published:2014-07-10 summary:In this paper we present a fundamental lexical semantics of Sinhala languageand a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhalalanguage. In any Natural Language processing task, Part of Speech is a veryvital topic, which involves analysing of the construction, behaviour and thedynamics of the language, which the knowledge could utilized in computationallinguistics analysis and automation applications. Though Sinhala is amorphologically rich and agglutinative language, in which words are inflectedwith various grammatical features, tagging is very essential for furtheranalysis of the language. Our research is based on statistical based approach,in which the tagging process is done by computing the tag sequence probabilityand the word-likelihood probability from the given corpus, where the linguisticknowledge is automatically extracted from the annotated corpus. The currenttagger could reach more than 90% of accuracy for known words.
arxiv-7500-55 | Learning Privately with Labeled and Unlabeled Examples | http://arxiv.org/pdf/1407.2662v3.pdf | author:Amos Beimel, Kobbi Nissim, Uri Stemmer category:cs.LG cs.CR published:2014-07-10 summary:A private learner is an algorithm that given a sample of labeled individualexamples outputs a generalizing hypothesis while preserving the privacy of eachindividual. In 2008, Kasiviswanathan et al. (FOCS 2008) gave a genericconstruction of private learners, in which the sample complexity is (generally)higher than what is needed for non-private learners. This gap in the samplecomplexity was then further studied in several followup papers, showing that(at least in some cases) this gap is unavoidable. Moreover, those papersconsidered ways to overcome the gap, by relaxing either the privacy or thelearning guarantees of the learner. We suggest an alternative approach, inspired by the (non-private) models ofsemi-supervised learning and active-learning, where the focus is on the samplecomplexity of labeled examples whereas unlabeled examples are of asignificantly lower cost. We consider private semi-supervised learners thatoperate on a random sample, where only a (hopefully small) portion of thissample is labeled. The learners have no control over which of the sampleelements are labeled. Our main result is that the labeled sample complexity ofprivate learners is characterized by the VC dimension. We present two generic constructions of private semi-supervised learners. Thefirst construction is of learners where the labeled sample complexity isproportional to the VC dimension of the concept class, however, the unlabeledsample complexity of the algorithm is as big as the representation length ofdomain elements. Our second construction presents a new technique fordecreasing the labeled sample complexity of a given private learner, whileroughly maintaining its unlabeled sample complexity. In addition, we show thatin some settings the labeled sample complexity does not depend on the privacyparameters of the learner.
arxiv-7500-56 | FAME: Face Association through Model Evolution | http://arxiv.org/pdf/1407.2987v1.pdf | author:Eren Golge, Pinar Duygulu category:cs.CV cs.AI cs.IR cs.LG published:2014-07-10 summary:We attack the problem of learning face models for public faces fromweakly-labelled images collected from web through querying a name. The data isvery noisy even after face detection, with several irrelevant facescorresponding to other people. We propose a novel method, Face Associationthrough Model Evolution (FAME), that is able to prune the data in an iterativeway, for the face models associated to a name to evolve. The idea is based oncapturing discriminativeness and representativeness of each instance andeliminating the outliers. The final models are used to classify faces on noveldatasets with possibly different characteristics. On benchmark datasets, ourresults are comparable to or better than state-of-the-art studies for the taskof face identification.
arxiv-7500-57 | Real-Time Impulse Noise Suppression from Images Using an Efficient Weighted-Average Filtering | http://arxiv.org/pdf/1408.3139v1.pdf | author:Hossein Hosseini, Farzad Hessar, Farokh Marvasti category:cs.CV published:2014-07-10 summary:In this paper, we propose a method for real-time high density impulse noisesuppression from images. In our method, we first apply an impulse detector toidentify the corrupted pixels and then employ an innovative weighted-averagefilter to restore them. The filter takes the nearest neighboring interpolatedimage as the initial image and computes the weights according to the relativepositions of the corrupted and uncorrupted pixels. Experimental results showthat the proposed method outperforms the best existing methods in both PSNRmeasure and visual quality and is quite suitable for real-time applications.
arxiv-7500-58 | On the Convergence of the Mean Shift Algorithm in the One-Dimensional Space | http://arxiv.org/pdf/1407.2961v1.pdf | author:Youness Aliyari Ghassabeh category:cs.CV published:2014-07-10 summary:The mean shift algorithm is a non-parametric and iterative technique that hasbeen used for finding modes of an estimated probability density function. Ithas been successfully employed in many applications in specific areas ofmachine vision, pattern recognition, and image processing. Although the meanshift algorithm has been used in many applications, a rigorous proof of itsconvergence is still missing in the literature. In this paper we address theconvergence of the mean shift algorithm in the one-dimensional space and provethat the sequence generated by the mean shift algorithm is a monotone andconvergent sequence.
arxiv-7500-59 | An eigenanalysis of data centering in machine learning | http://arxiv.org/pdf/1407.2904v1.pdf | author:Paul Honeine category:stat.ML cs.CV cs.LG math.SP math.ST stat.TH published:2014-07-10 summary:Many pattern recognition methods rely on statistical information fromcentered data, with the eigenanalysis of an empirical central moment, such asthe covariance matrix in principal component analysis (PCA), as well as partialleast squares regression, canonical-correlation analysis and Fisherdiscriminant analysis. Recently, many researchers advocate working onnon-centered data. This is the case for instance with the singular valuedecomposition approach, with the (kernel) entropy component analysis, with theinformation-theoretic learning framework, and even with nonnegative matrixfactorization. Moreover, one can also consider a non-centered PCA by using thesecond-order non-central moment. The main purpose of this paper is to bridge the gap between these twoviewpoints in designing machine learning methods. To provide a study at thecornerstone of kernel-based machines, we conduct an eigenanalysis of the innerproduct matrices from centered and non-centered data. We derive several resultsconnecting their eigenvalues and their eigenvectors. Furthermore, we explorethe outer product matrices, by providing several results connecting the largesteigenvectors of the covariance matrix and its non-centered counterpart. Theseresults lay the groundwork to several extensions beyond conventional centering,with the weighted mean shift, the rank-one update, and the multidimensionalscaling. Experiments conducted on simulated and real data illustrate therelevance of this work.
arxiv-7500-60 | A New Optimal Stepsize For Approximate Dynamic Programming | http://arxiv.org/pdf/1407.2676v2.pdf | author:Ilya O. Ryzhov, Peter I. Frazier, Warren B. Powell category:math.OC cs.AI cs.LG cs.SY stat.ML published:2014-07-10 summary:Approximate dynamic programming (ADP) has proven itself in a wide range ofapplications spanning large-scale transportation problems, health care, revenuemanagement, and energy systems. The design of effective ADP algorithms has manydimensions, but one crucial factor is the stepsize rule used to update a valuefunction approximation. Many operations research applications arecomputationally intensive, and it is important to obtain good results quickly.Furthermore, the most popular stepsize formulas use tunable parameters and canproduce very poor results if tuned improperly. We derive a new stepsize rulethat optimizes the prediction error in order to improve the short-termperformance of an ADP algorithm. With only one, relatively insensitive tunableparameter, the new rule adapts to the level of noise in the problem andproduces faster convergence in numerical experiments.
arxiv-7500-61 | Beyond Disagreement-based Agnostic Active Learning | http://arxiv.org/pdf/1407.2657v2.pdf | author:Chicheng Zhang, Kamalika Chaudhuri category:cs.LG stat.ML published:2014-07-10 summary:We study agnostic active learning, where the goal is to learn a classifier ina pre-specified hypothesis class interactively with as few label queries aspossible, while making no assumptions on the true function generating thelabels. The main algorithms for this problem are {\em{disagreement-based activelearning}}, which has a high label requirement, and {\em{margin-based activelearning}}, which only applies to fairly restricted settings. A major challengeis to find an algorithm which achieves better label complexity, is consistentin an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on twonovel contributions -- a reduction from consistent active learning toconfidence-rated prediction with guaranteed error, and a novel confidence-ratedpredictor.
arxiv-7500-62 | On the Optimality of Averaging in Distributed Statistical Learning | http://arxiv.org/pdf/1407.2724v2.pdf | author:Jonathan Rosenblatt, Boaz Nadler category:stat.ML math.ST stat.TH published:2014-07-10 summary:A common approach to statistical learning with big-data is to randomly splitit among $m$ machines and learn the parameter of interest by averaging the $m$individual estimates. In this paper, focusing on empirical risk minimization,or equivalently M-estimation, we study the statistical error incurred by thisstrategy. We consider two large-sample settings: First, a classical settingwhere the number of parameters $p$ is fixed, and the number of samples permachine $n\to\infty$. Second, a high-dimensional regime where both$p,n\to\infty$ with $p/n \to \kappa \in (0,1)$. For both regimes and undersuitable assumptions, we present asymptotically exact expressions for thisestimation error. In the fixed-$p$ setting, under suitable assumptions, weprove that to leading order averaging is as accurate as the centralizedsolution. We also derive the second order error terms, and show that these canbe non-negligible, notably for non-linear models. The high-dimensional setting,in contrast, exhibits a qualitatively different behavior: data splitting incursa first-order accuracy loss, which to leading order increases linearly with thenumber of machines. The dependence of our error approximations on the number ofmachines traces an interesting accuracy-complexity tradeoff, allowing thepractitioner an informed choice on the number of machines to deploy. Finally,we confirm our theoretical analysis with several simulations.
arxiv-7500-63 | Asynchronous Anytime Sequential Monte Carlo | http://arxiv.org/pdf/1407.2864v1.pdf | author:Brooks Paige, Frank Wood, Arnaud Doucet, Yee Whye Teh category:stat.CO stat.ML published:2014-07-10 summary:We introduce a new sequential Monte Carlo algorithm we call the particlecascade. The particle cascade is an asynchronous, anytime alternative totraditional particle filtering algorithms. It uses no barrier synchronizationswhich leads to improved particle throughput and memory efficiency. It is ananytime algorithm in the sense that it can be run forever to emit an unboundednumber of particles while keeping within a fixed memory budget. We prove thatthe particle cascade is an unbiased marginal likelihood estimator which meansthat it can be straightforwardly plugged into existing pseudomarginal methods.
arxiv-7500-64 | ARTOS -- Adaptive Real-Time Object Detection System | http://arxiv.org/pdf/1407.2721v2.pdf | author:Björn Barz, Erik Rodner, Joachim Denzler category:cs.CV published:2014-07-10 summary:ARTOS is all about creating, tuning, and applying object detection modelswith just a few clicks. In particular, ARTOS facilitates learning of models forvisual object detection by eliminating the burden of having to collect andannotate a large set of positive and negative samples manually and in additionit implements a fast learning technique to reduce the time needed for thelearning step. A clean and friendly GUI guides the user through the process of modelcreation, adaptation of learned models to different domains using in-situimages, and object detection on both offline images and images from a videostream. A library written in C++ provides the main functionality of ARTOS witha C-style procedural interface, so that it can be easily integrated with anyother project.
arxiv-7500-65 | Private Learning and Sanitization: Pure vs. Approximate Differential Privacy | http://arxiv.org/pdf/1407.2674v1.pdf | author:Amos Beimel, Kobbi Nissim, Uri Stemmer category:cs.LG cs.CR stat.ML published:2014-07-10 summary:We compare the sample complexity of private learning [Kasiviswanathan et al.2008] and sanitization~[Blum et al. 2008] under pure $\epsilon$-differentialprivacy [Dwork et al. TCC 2006] and approximate$(\epsilon,\delta)$-differential privacy [Dwork et al. Eurocrypt 2006]. We showthat the sample complexity of these tasks under approximate differentialprivacy can be significantly lower than that under pure differential privacy. We define a family of optimization problems, which we call Quasi-ConcavePromise Problems, that generalizes some of our considered tasks. We observethat a quasi-concave promise problem can be privately approximated using asolution to a smaller instance of a quasi-concave promise problem. This allowsus to construct an efficient recursive algorithm solving such problemsprivately. Specifically, we construct private learners for point functions,threshold functions, and axis-aligned rectangles in high dimension. Similarly,we construct sanitizers for point functions and threshold functions. We also examine the sample complexity of label-private learners, a relaxationof private learning where the learner is required to only protect the privacyof the labels in the sample. We show that the VC dimension completelycharacterizes the sample complexity of such learners, that is, the samplecomplexity of learning with label privacy is equal (up to constants) tolearning without privacy.
arxiv-7500-66 | Quality Estimation Of Machine Translation Outputs Through Stemming | http://arxiv.org/pdf/1407.2694v1.pdf | author:Pooja Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2014-07-10 summary:Machine Translation is the challenging problem for Indian languages. Everyday we can see some machine translators being developed, but getting a highquality automatic translation is still a very distant dream . The correcttranslated sentence for Hindi language is rarely found. In this paper, we areemphasizing on English-Hindi language pair, so in order to preserve the correctMT output we present a ranking system, which employs some machine learningtechniques and morphological features. In ranking no human intervention isrequired. We have also validated our results by comparing it with humanranking.
arxiv-7500-67 | Compressed sensing for longitudinal MRI: An adaptive-weighted approach | http://arxiv.org/pdf/1407.2602v3.pdf | author:Lior Weizman, Yonina C. Eldar, Dafna Ben Bashat category:physics.med-ph cs.CV published:2014-07-10 summary:Purpose: Repeated brain MRI scans are performed in many clinical scenarios,such as follow up of patients with tumors and therapy response assessment. Inthis paper, the authors show an approach to utilize former scans of the patientfor the acceleration of repeated MRI scans. Methods: The proposed approach utilizes the possible similarity of therepeated scans in longitudinal MRI studies. Since similarity is not guaranteed,sampling and reconstruction are adjusted during acquisition to match the actualsimilarity between the scans. The baseline MR scan is utilized both in thesampling stage, via adaptive sampling, and in the reconstruction stage, withweighted reconstruction. In adaptive sampling, k-space sampling locations areoptimized during acquisition. Weighted reconstruction uses the locations of thenonzero coefficients in the sparse domains as a prior in the recovery process.The approach was tested on 2D and 3D MRI scans of patients with brain tumors. Results: The longitudinal adaptive CS MRI (LACS-MRI) scheme providesreconstruction quality which outperforms other CS-based approaches for rapidMRI. Examples are shown on patients with brain tumors and demonstrate improvedspatial resolution. Compared with data sampled at Nyquist rate, LACS-MRIexhibits Signal-to-Error Ratio (SER) of 24.8dB with undersampling factor of16.6 in 3D MRI. Conclusions: The authors have presented a novel method for imagereconstruction utilizing similarity of scans in longitudinal MRI studies, wherepossible. The proposed approach can play a major part and significantly reducescanning time in many applications that consist of disease follow-up andmonitoring of longitudinal changes in brain MRI.
arxiv-7500-68 | A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation | http://arxiv.org/pdf/1407.2697v1.pdf | author:Aaron J. Defazio, Tiberio S. Caetano category:cs.LG stat.ML published:2014-07-10 summary:A key problem in statistics and machine learning is the determination ofnetwork structure from data. We consider the case where the structure of thegraph to be reconstructed is known to be scale-free. We show that in such casesit is natural to formulate structured sparsity inducing priors using submodularfunctions, and we use their Lov\'asz extension to obtain a convex relaxation.For tractable classes such as Gaussian graphical models, this leads to a convexoptimization problem that can be efficiently solved. We show that our methodresults in an improvement in the accuracy of reconstructed networks forsynthetic data. We also show how our prior encourages scale-freereconstructions on a bioinfomatics dataset.
arxiv-7500-69 | Rate-Optimal Detection of Very Short Signal Segments | http://arxiv.org/pdf/1407.2812v1.pdf | author:T. Tony Cai, Ming Yuan category:stat.ML cs.IT math.IT math.ST stat.TH published:2014-07-10 summary:Motivated by a range of applications in engineering and genomics, we considerin this paper detection of very short signal segments in three settings:signals with known shape, arbitrary signals, and smooth signals. Optimal ratesof detection are established for the three cases and rate-optimal detectors areconstructed. The detectors are easily implementable and are based on scanningwith linear and quadratic statistics. Our analysis reveals both similaritiesand differences in the strategy and fundamental difficulty of detection amongthese three settings.
arxiv-7500-70 | Offline handwritten signature identification using adaptive window positioning techniques | http://arxiv.org/pdf/1407.2700v1.pdf | author:Ghazali Sulong, Anwar Yahy Ebrahim, Muhammad Jehanzeb category:cs.CV published:2014-07-10 summary:The paper presents to address this challenge, we have proposed the use ofAdaptive Window Positioning technique which focuses on not just the meaning ofthe handwritten signature but also on the individuality of the writer. Thisinnovative technique divides the handwritten signature into 13 small windows ofsize nxn(13x13).This size should be large enough to contain ample informationabout the style of the author and small enough to ensure a good identificationperformance.The process was tested with a GPDS data set containing 4870signature samples from 90 different writers by comparing the robust features ofthe test signature with that of the user signature using an appropriateclassifier. Experimental results reveal that adaptive window positioningtechnique proved to be the efficient and reliable method for accurate signaturefeature extraction for the identification of offline handwritten signatures.Thecontribution of this technique can be used to detect signatures signed underemotional duress.
arxiv-7500-71 | Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems | http://arxiv.org/pdf/1407.2710v1.pdf | author:Aaron J. Defazio, Tibério S. Caetano, Justin Domke category:cs.LG stat.ML published:2014-07-10 summary:Recent advances in optimization theory have shown that smooth strongly convexfinite sums can be minimized faster than by treating them as a black box"batch" problem. In this work we introduce a new method in this class with atheoretical convergence rate four times faster than existing methods, for sumswith sufficiently many terms. This method is also amendable to a samplingwithout replacement scheme that in practice gives further speed-ups. We giveempirical results showing state of the art performance.
arxiv-7500-72 | A multi-instance learning algorithm based on a stacked ensemble of lazy learners | http://arxiv.org/pdf/1407.2736v1.pdf | author:Ramasubramanian Sundararajan, Hima Patel, Manisha Srivastava category:cs.LG published:2014-07-10 summary:This document describes a novel learning algorithm that classifies "bags" ofinstances rather than individual instances. A bag is labeled positive if itcontains at least one positive instance (which may or may not be specificallyidentified), and negative otherwise. This class of problems is known asmulti-instance learning problems, and is useful in situations where the classlabel at an instance level may be unavailable or imprecise or difficult toobtain, or in situations where the problem is naturally posed as one ofclassifying instance groups. The algorithm described here is an ensemble-basedmethod, wherein the members of the ensemble are lazy learning classifierslearnt using the Citation Nearest Neighbour method. Diversity among theensemble members is achieved by optimizing their parameters using amulti-objective optimization method, with the objectives being to maximizeClass 1 accuracy and minimize false positive rate. The method has been found tobe effective on the Musk1 benchmark dataset.
arxiv-7500-73 | What you need to know about the state-of-the-art computational models of object-vision: A tour through the models | http://arxiv.org/pdf/1407.2776v1.pdf | author:Seyed-Mahdi Khaligh-Razavi category:cs.CV cs.AI cs.LG q-bio.NC published:2014-07-10 summary:Models of object vision have been of great interest in computer vision andvisual neuroscience. During the last decades, several models have beendeveloped to extract visual features from images for object recognition tasks.Some of these were inspired by the hierarchical structure of primate visualsystem, and some others were engineered models. The models are varied inseveral aspects: models that are trained by supervision, models trained withoutsupervision, and models (e.g. feature extractors) that are fully hard-wired anddo not need training. Some of the models come with a deep hierarchicalstructure consisting of several layers, and some others are shallow and comewith only one or two layers of processing. More recently, new models have beendeveloped that are not hand-tuned but trained using millions of images, throughwhich they learn how to extract informative task-related features. Here I willsurvey all these different models and provide the reader with an intuitive, aswell as a more detailed, understanding of the underlying computations in eachof the models.
arxiv-7500-74 | Bandits Warm-up Cold Recommender Systems | http://arxiv.org/pdf/1407.2806v1.pdf | author:Jérémie Mary, Romaric Gaudel, Preux Philippe category:cs.LG cs.IR stat.ML published:2014-07-10 summary:We address the cold start problem in recommendation systems assuming nocontextual information is available neither about users, nor items. We considerthe case in which we only have access to a set of ratings of items by users.Most of the existing works consider a batch setting, and use cross-validationto tune parameters. The classical method consists in minimizing the root meansquare error over a training subset of the ratings which provides afactorization of the matrix of ratings, interpreted as a latent representationof items and users. Our contribution in this paper is 5-fold. First, weexplicit the issues raised by this kind of batch setting for users or itemswith very few ratings. Then, we propose an online setting closer to the actualuse of recommender systems; this setting is inspired by the bandit framework.The proposed methodology can be used to turn any recommender system dataset(such as Netflix, MovieLens,...) into a sequential dataset. Then, we explicit astrong and insightful link between contextual bandit algorithms and matrixfactorization; this leads us to a new algorithm that tackles theexploration/exploitation dilemma associated to the cold start problem in astrikingly new perspective. Finally, experimental evidence confirm that ouralgorithm is effective in dealing with the cold start problem on publiclyavailable datasets. Overall, the goal of this paper is to bridge the gapbetween recommender systems based on matrix factorizations and those based oncontextual bandits.
arxiv-7500-75 | XML Matchers: approaches and challenges | http://arxiv.org/pdf/1407.2845v1.pdf | author:Santa Agreste, Pasquale De Meo, Emilio Ferrara, Domenico Ursino category:cs.DB cs.AI cs.IR cs.LG published:2014-07-10 summary:Schema Matching, i.e. the process of discovering semantic correspondencesbetween concepts adopted in different data source schemas, has been a key topicin Database and Artificial Intelligence research areas for many years. In thepast, it was largely investigated especially for classical database models(e.g., E/R schemas, relational databases, etc.). However, in the latest years,the widespread adoption of XML in the most disparate application fields pusheda growing number of researchers to design XML-specific Schema Matchingapproaches, called XML Matchers, aiming at finding semantic matchings betweenconcepts defined in DTDs and XSDs. XML Matchers do not just take well-knowntechniques originally designed for other data models and apply them onDTDs/XSDs, but they exploit specific XML features (e.g., the hierarchicalstructure of a DTD/XSD) to improve the performance of the Schema Matchingprocess. The design of XML Matchers is currently a well-established researcharea. The main goal of this paper is to provide a detailed description andclassification of XML Matchers. We first describe to what extent thespecificities of DTDs/XSDs impact on the Schema Matching task. Then weintroduce a template, called XML Matcher Template, that describes the maincomponents of an XML Matcher, their role and behavior. We illustrate how eachof these components has been implemented in some popular XML Matchers. Weconsider our XML Matcher Template as the baseline for objectively comparingapproaches that, at first glance, might appear as unrelated. The introductionof this template can be useful in the design of future XML Matchers. Finally,we analyze commercial tools implementing XML Matchers and introduce twochallenging issues strictly related to this topic, namely XML source clusteringand uncertainty management in XML Matchers.
arxiv-7500-76 | On Gridless Sparse Methods for Line Spectral Estimation From Complete and Incomplete Data | http://arxiv.org/pdf/1407.2490v2.pdf | author:Zai Yang, Lihua Xie category:cs.IT math.IT stat.ML published:2014-07-09 summary:This paper is concerned about sparse, continuous frequency estimation in linespectral estimation, and focused on developing gridless sparse methods whichovercome grid mismatches and correspond to limiting scenarios of existinggrid-based approaches, e.g., $\ell_1$ optimization and SPICE, with aninfinitely dense grid. We generalize AST (atomic-norm soft thresholding) to thecase of nonconsecutively sampled data (incomplete data) inspired by recentatomic norm based techniques. We present a gridless version of SPICE (gridlessSPICE, or GLS), which is applicable to both complete and incomplete datawithout the knowledge of noise level. We further prove the equivalence betweenGLS and atomic norm-based techniques under different assumptions of noise.Moreover, we extend GLS to a systematic framework consisting of model orderselection and robust frequency estimation, and present feasible algorithms forAST and GLS. Numerical simulations are provided to validate our theoreticalanalysis and demonstrate performance of our methods compared to existing ones.
arxiv-7500-77 | Identifying Cover Songs Using Information-Theoretic Measures of Similarity | http://arxiv.org/pdf/1407.2433v3.pdf | author:Peter Foster, Simon Dixon, Anssi Klapuri category:cs.IR cs.LG stat.ML published:2014-07-09 summary:This paper investigates methods for quantifying similarity between audiosignals, specifically for the task of of cover song detection. We consider aninformation-theoretic approach, where we compute pairwise measures ofpredictability between time series. We compare discrete-valued approachesoperating on quantised audio features, to continuous-valued approaches. In thediscrete case, we propose a method for computing the normalised compressiondistance, where we account for correlation between time series. In thecontinuous case, we propose to compute information-based measures of similarityas statistics of the prediction error between time series. We evaluate ourmethods on two cover song identification tasks using a data set comprised of300 Jazz standards and using the Million Song Dataset. For both datasets, weobserve that continuous-valued approaches outperform discrete-valuedapproaches. We consider approaches to estimating the normalised compressiondistance (NCD) based on string compression and prediction, where we observethat our proposed normalised compression distance with alignment (NCDA)improves average performance over NCD, for sequential compression algorithms.Finally, we demonstrate that continuous-valued distances may be combined toimprove performance with respect to baseline approaches. Using a large-scalefilter-and-refine approach, we demonstrate state-of-the-art performance forcover song identification using the Million Song Dataset.
arxiv-7500-78 | RankMerging: A supervised learning-to-rank framework to predict links in large social network | http://arxiv.org/pdf/1407.2515v3.pdf | author:Lionel Tabourier, Daniel Faria Bernardes, Anne-Sophie Libert, Renaud Lambiotte category:cs.SI cs.IR cs.LG physics.soc-ph published:2014-07-09 summary:Uncovering unknown or missing links in social networks is a difficult taskbecause of their sparsity and because links may represent different types ofrelationships, characterized by different structural patterns. In this paper,we define a simple yet efficient supervised learning-to-rank framework, calledRankMerging, which aims at combining information provided by variousunsupervised rankings. We illustrate our method on three different kinds ofsocial networks and show that it substantially improves the performances ofunsupervised metrics of ranking. We also compare it to other combinationstrategies based on standard methods. Finally, we explore various aspects ofRankMerging, such as feature selection and parameter estimation and discuss itsarea of relevance: the prediction of an adjustable number of links on largenetworks.
arxiv-7500-79 | Classifiers fusion method to recognize handwritten persian numerals | http://arxiv.org/pdf/1407.2572v2.pdf | author:Reza Azad, Babak Azad, Iraj Mogharreb, Shahram Jamali category:cs.CV published:2014-07-09 summary:Recognition of Persian handwritten characters has been considered as asignificant field of research for the last few years under pattern analysingtechnique. In this paper, a new approach for robust handwritten Persiannumerals recognition using strong feature set and a classifier fusion method isscrutinized to increase the recognition percentage. For implementing theclassifier fusion technique, we have considered k nearest neighbour (KNN),linear classifier (LC) and support vector machine (SVM) classifiers. Theinnovation of this tactic is to attain better precision with few features usingclassifier fusion method. For evaluation of the proposed method we considered aPersian numerals database with 20,000 handwritten samples. Spending 15,000samples for training stage, we verified our technique on other 5,000 samples,and the correct recognition ratio achieved approximately 99.90%. Additional, wegot 99.97% exactness using four-fold cross validation procedure on 20,000databases.
arxiv-7500-80 | Counting Markov Blanket Structures | http://arxiv.org/pdf/1407.2483v2.pdf | author:Shyam Visweswaran, Gregory F. Cooper category:stat.ML cs.AI cs.LG published:2014-07-09 summary:Learning Markov blanket (MB) structures has proven useful in performingfeature selection, learning Bayesian networks (BNs), and discovering causalrelationships. We present a formula for efficiently determining the number ofMB structures given a target variable and a set of other variables. Asexpected, the number of MB structures grows exponentially. However, we showquantitatively that there are many fewer MB structures that contain the targetvariable than there are BN structures that contain it. In particular, the ratioof BN structures to MB structures appears to increase exponentially in thenumber of variables.
arxiv-7500-81 | Collaborative Recommendation with Auxiliary Data: A Transfer Learning View | http://arxiv.org/pdf/1407.2919v1.pdf | author:Weike Pan category:cs.IR cs.LG published:2014-07-09 summary:Intelligent recommendation technology has been playing an increasinglyimportant role in various industry applications such as e-commerce productpromotion and Internet advertisement display. Besides users' feedbacks (e.g.,numerical ratings) on items as usually exploited by some typical recommendationalgorithms, there are often some additional data such as users' social circlesand other behaviors. Such auxiliary data are usually related to users'preferences on items behind the numerical ratings. Collaborative recommendationwith auxiliary data (CRAD) aims to leverage such additional information so asto improve the personalization services, which have received much attentionfrom both researchers and practitioners. Transfer learning (TL) is proposed to extract and transfer knowledge fromsome auxiliary data in order to assist the learning task on some target data.In this paper, we consider the CRAD problem from a transfer learning view,especially on how to achieve knowledge transfer from some auxiliary data.First, we give a formal definition of transfer learning for CRAD (TL-CRAD).Second, we extend the existing categorization of TL techniques (i.e., adaptive,collective and integrative knowledge transfer algorithm styles) with threeknowledge transfer strategies (i.e., prediction rule, regularization andconstraint). Third, we propose a novel generic knowledge transfer framework forTL-CRAD. Fourth, we describe some representative works of each specificknowledge transfer strategy of each algorithm style in detail, which areexpected to inspire further works. Finally, we conclude the paper with somesummary discussions and several future directions.
arxiv-7500-82 | PatchLift: Fast and Exact Computation of Patch Distances using Lifting, with Applications to Non-Local Means | http://arxiv.org/pdf/1407.2343v1.pdf | author:Kunal Narayan Chaudhury category:cs.CV published:2014-07-09 summary:In this paper, we propose a fast algorithm called PatchLift for computingdistances between patches extracted from a one-dimensional signal. PatchLift isbased on the observation that the patch distances can be expressed in terms ofsimple moving sums of an image, which is derived from the one-dimensionalsignal via lifting. We apply PatchLift to develop a separable extension of theclassical Non-Local Means (NLM) algorithm which is at least 100 times fasterthan NLM for standard parameter settings. The PSNR obtained using the proposedextension is typically close to (and often larger than) the PSNRs obtainedusing the original NLM. We provide some simulations results to demonstrate theacceleration achieved using separability and PatchLift.
arxiv-7500-83 | The jump set under geometric regularisation. Part 2: Higher-order approaches | http://arxiv.org/pdf/1407.2334v1.pdf | author:Tuomo Valkonen category:math.FA cs.CV published:2014-07-09 summary:In Part 1, we developed a new technique based on Lipschitz pushforwards forproving the jump set containment property $\mathcal{H}^{m-1}(J_u \setminusJ_f)=0$ of solutions $u$ to total variation denoising. We demonstrated that thetechnique also applies to Huber-regularised TV. Now, in this Part 2, we extendthe technique to higher-order regularisers. We are not quite able to prove theproperty for total generalised variation (TGV) based on the symmetrisedgradient for the second-order term. We show that the property holds under threeconditions: First, the solution $u$ is locally bounded. Second, thesecond-order variable is of locally bounded variation, $w \in\mbox{BV}_\mbox{loc}(\Omega; \mathbb{R}^m)$, instead of just boundeddeformation, $w \in \mbox{BD}(\Omega)$. Third, $w$ does not jump on $J_u$parallel to it. The second condition can be achieved for non-symmetric TGV.Both the second and third condition can be achieved if we change the Radon (or$L^1$) norm of the symmetrised gradient $Ew$ into an $L^p$ norm, $p>1$, inwhich case Korn's inequality holds. We also consider the application of thetechnique to infimal convolution TV, and study the limiting behaviour of thesingular part of $D u$, as the second parameter of $\mbox{TGV}^2$ goes to zero.Unsurprisingly, it vanishes, but in numerical discretisations the situationlooks quite different. Finally, our work additionally includes a result onTGV-strict approximation in $\mbox{BV}(\Omega)$.
arxiv-7500-84 | Online Stroke and Akshara Recognition GUI in Assamese Language Using Hidden Markov Model | http://arxiv.org/pdf/1407.2390v1.pdf | author:SRM Prasanna, Rituparna Devi, Deepjoy Das, Subhankar Ghosh, Krishna Naik category:cs.CV published:2014-07-09 summary:The work describes the development of Online Assamese Stroke & AksharaRecognizer based on a set of language rules. In handwriting literature strokesare composed of two coordinate trace in between pen down and pen up labels. TheAssamese aksharas are combination of a number of strokes, the maximum number ofstrokes taken to make a combination being eight. Based on these combinationseight language rule models have been made which are used to test if a set ofstrokes form a valid akshara. A Hidden Markov Model is used to train 181different stroke patterns which generates a model used during stroke leveltesting. Akshara level testing is performed by integrating a GUI (provided byCDAC-Pune) with the Binaries of HTK toolkit classifier, HMM train model and thelanguage rules using a dynamic linked library (dll). We have got a stroke levelperformance of 94.14% and akshara level performance of 84.2%.
arxiv-7500-85 | A Statistical Modeling Approach to Computer-Aided Quantification of Dental Biofilm | http://arxiv.org/pdf/1407.2630v1.pdf | author:Awais Mansoor, Valery Patsekin, Dale Scherl, J. Paul Robinson, Bartlomiej Rajwa category:cs.CV published:2014-07-09 summary:Biofilm is a formation of microbial material on tooth substrata. Severalmethods to quantify dental biofilm coverage have recently been reported in theliterature, but at best they provide a semi-automated approach toquantification with significant input from a human grader that comes with thegraders bias of what are foreground, background, biofilm, and tooth.Additionally, human assessment indices limit the resolution of thequantification scale; most commercial scales use five levels of quantificationfor biofilm coverage (0%, 25%, 50%, 75%, and 100%). On the other hand, currentstate-of-the-art techniques in automatic plaque quantification fail to maketheir way into practical applications owing to their inability to incorporatehuman input to handle misclassifications. This paper proposes a new interactivemethod for biofilm quantification in Quantitative light-induced fluorescence(QLF) images of canine teeth that is independent of the perceptual bias of thegrader. The method partitions a QLF image into segments of uniform texture andintensity called superpixels; every superpixel is statistically modeled as arealization of a single 2D Gaussian Markov random field (GMRF) whose parametersare estimated; the superpixel is then assigned to one of three classes(background, biofilm, tooth substratum) based on the training set of data. Thequantification results show a high degree of consistency and precision. At thesame time, the proposed method gives pathologists full control to post-processthe automatic quantification by flipping misclassified superpixels to adifferent state (background, tooth, biofilm) with a single click, providinggreater usability than simply marking the boundaries of biofilm and tooth asdone by current state-of-the-art methods.
arxiv-7500-86 | Learning Probabilistic Programs | http://arxiv.org/pdf/1407.2646v1.pdf | author:Yura N. Perov, Frank D. Wood category:cs.AI cs.LG stat.ML published:2014-07-09 summary:We develop a technique for generalising from data in which models aresamplers represented as program text. We establish encouraging empiricalresults that suggest that Markov chain Monte Carlo probabilistic programminginference techniques coupled with higher-order probabilistic programminglanguages are now sufficiently powerful to enable successful inference of thiskind in nontrivial domains. We also introduce a new notion of probabilisticprogram compilation and show how the same machinery might be used in the futureto compile probabilistic programs for efficient reusable predictive inference.
arxiv-7500-87 | Learning Deep Structured Models | http://arxiv.org/pdf/1407.2538v3.pdf | author:Liang-Chieh Chen, Alexander G. Schwing, Alan L. Yuille, Raquel Urtasun category:cs.LG published:2014-07-09 summary:Many problems in real-world applications involve predicting several randomvariables which are statistically related. Markov random fields (MRFs) are agreat mathematical tool to encode such relationships. The goal of this paper isto combine MRFs with deep learning algorithms to estimate complexrepresentations while taking into account the dependencies between the outputrandom variables. Towards this goal, we propose a training algorithm that isable to learn structured models jointly with deep features that form the MRFpotentials. Our approach is efficient as it blends learning and inference andmakes use of GPU acceleration. We demonstrate the effectiveness of ouralgorithm in the tasks of predicting words from noisy images, as well asmulti-class classification of Flickr photographs. We show that joint learningof the deep features and the MRF parameters results in significant performancegains.
arxiv-7500-88 | Classifying Fonts and Calligraphy Styles Using Complex Wavelet Transform | http://arxiv.org/pdf/1407.2649v1.pdf | author:Alican Bozkurt, Pinar Duygulu, A. Enis Cetin category:cs.CV published:2014-07-09 summary:Recognizing fonts has become an important task in document analysis, due tothe increasing number of available digital documents in different fonts andemphases. A generic font-recognition system independent of language, script andcontent is desirable for processing various types of documents. At the sametime, categorizing calligraphy styles in handwritten manuscripts is importantfor palaeographic analysis, but has not been studied sufficiently in theliterature. We address the font-recognition problem as analysis andcategorization of textures. We extract features using complex wavelet transformand use support vector machines for classification. Extensive experimentalevaluations on different datasets in four languages and comparisons withstate-of-the-art studies show that our proposed method achieves higherrecognition accuracy while being computationally simpler. Furthermore, on a newdataset generated from Ottoman manuscripts, we show that the proposed methodcan also be used for categorizing Ottoman calligraphy with high accuracy.
arxiv-7500-89 | A Survey of Named Entity Recognition in Assamese and other Indian Languages | http://arxiv.org/pdf/1407.2918v1.pdf | author:Gitimoni Talukdar, Pranjal Protim Borah, Arup Baruah category:cs.CL published:2014-07-09 summary:Named Entity Recognition is always important when dealing with major NaturalLanguage Processing tasks such as information extraction, question-answering,machine translation, document summarization etc so in this paper we put forwarda survey of Named Entities in Indian Languages with particular reference toAssamese. There are various rule-based and machine learning approachesavailable for Named Entity Recognition. At the very first of the paper we givean idea of the available approaches for Named Entity Recognition and then wediscuss about the related research in this field. Assamese like other Indianlanguages is agglutinative and suffers from lack of appropriate resources asNamed Entity Recognition requires large data sets, gazetteer list, dictionaryetc and some useful feature like capitalization as found in English cannot befound in Assamese. Apart from this we also describe some of the issues faced inAssamese while doing Named Entity Recognition.
arxiv-7500-90 | A Critical Reassessment of Evolutionary Algorithms on the cryptanalysis of the simplified data encryption standard algorithm | http://arxiv.org/pdf/1407.1993v1.pdf | author:Fabien Teytaud, Cyril Fonlupt category:cs.CR cs.NE published:2014-07-08 summary:In this paper we analyze the cryptanalysis of the simplified data encryptionstandard algorithm using meta-heuristics and in particular genetic algorithms.The classic fitness function when using such an algorithm is to compare n-gramstatistics of a the decrypted message with those of the target message. We showthat using such a function is irrelevant in case of Genetic Algorithm, simplybecause there is no correlation between the distance to the real key (theoptimum) and the value of the fitness, in other words, there is no hiddengradient. In order to emphasize this assumption we experimentally show that agenetic algorithm perform worse than a random search on the cryptanalysis ofthe simplified data encryption standard algorithm.
arxiv-7500-91 | Inter-Rater Agreement Study on Readability Assessment in Bengali | http://arxiv.org/pdf/1407.1976v1.pdf | author:Shanta Phani, Shibamouli Lahiri, Arindam Biswas category:cs.CL published:2014-07-08 summary:An inter-rater agreement study is performed for readability assessment inBengali. A 1-7 rating scale was used to indicate different levels ofreadability. We obtained moderate to fair agreement among seven independentannotators on 30 text passages written by four eminent Bengali authors. As a byproduct of our study, we obtained a readability-annotated ground truth datasetin Bengali. .
arxiv-7500-92 | Regression-Based Image Alignment for General Object Categories | http://arxiv.org/pdf/1407.1957v1.pdf | author:Hilton Bristow, Simon Lucey category:cs.CV published:2014-07-08 summary:Gradient-descent methods have exhibited fast and reliable performance forimage alignment in the facial domain, but have largely been ignored by thebroader vision community. They require the image function be smooth and(numerically) differentiable -- properties that hold for pixel-basedrepresentations obeying natural image statistics, but not for more generalclasses of non-linear feature transforms. We show that transforms such as DenseSIFT can be incorporated into a Lucas Kanade alignment framework by predictingdescent directions via regression. This enables robust matching of instancesfrom general object categories whilst maintaining desirable properties of LucasKanade such as the capacity to handle high-dimensional warp parametrizationsand a fast rate of convergence. We present alignment results on a number ofobjects from ImageNet, and an extension of the method to unsupervised jointalignment of objects from a corpus of images.
arxiv-7500-93 | Lexpresso: a Controlled Natural Language | http://arxiv.org/pdf/1407.1933v1.pdf | author:Adam Saulwick category:cs.CL cs.AI published:2014-07-08 summary:This paper presents an overview of `Lexpresso', a Controlled Natural Languagedeveloped at the Defence Science & Technology Organisation as a bidirectionalnatural language interface to a high-level information fusion system. The paperdescribes Lexpresso's main features including lexical coverage, expressivenessand range of linguistic syntactic and semantic structures. It also touches onits tight integration with a formal semantic formalism and tentativelyclassifies it against the PENS system.
arxiv-7500-94 | Orientation covariant aggregation of local descriptors with embeddings | http://arxiv.org/pdf/1407.2170v2.pdf | author:Giorgos Tolias, Teddy Furon, Hervé Jégou category:cs.CV published:2014-07-08 summary:Image search systems based on local descriptors typically achieve orientationinvariance by aligning the patches on their dominant orientations. Albeitsuccessful, this choice introduces too much invariance because it does notguarantee that the patches are rotated consistently. This paper introduces anaggregation strategy of local descriptors that achieves this covarianceproperty by jointly encoding the angle in the aggregation stage in a continuousmanner. It is combined with an efficient monomial embedding to provide acodebook-free method to aggregate local descriptors into a single vectorrepresentation. Our strategy is also compatible and employed with severalpopular encoding methods, in particular bag-of-words, VLAD and the Fishervector. Our geometric-aware aggregation strategy is effective for image search,as shown by experiments performed on standard benchmarks for image andparticular object retrieval, namely Holidays and Oxford buildings.
arxiv-7500-95 | Assamese-English Bilingual Machine Translation | http://arxiv.org/pdf/1407.2019v1.pdf | author:Kalyanee Kanchan Baruah, Pranjal Das, Abdul Hannan, Shikhar Kr. Sarma category:cs.CL published:2014-07-08 summary:Machine translation is the process of translating text from one language toanother. In this paper, Statistical Machine Translation is done on Assamese andEnglish language by taking their respective parallel corpus. A statisticalphrase based translation toolkit Moses is used here. To develop the languagemodel and to align the words we used two another tools IRSTLM, GIZArespectively. BLEU score is used to check our translation system performance,how good it is. A difference in BLEU scores is obtained while translatingsentences from Assamese to English and vice-versa. Since Indian languages aremorphologically very rich hence translation is relatively harder from Englishto Assamese resulting in a low BLEU score. A statistical transliteration systemis also introduced with our translation system to deal basically with propernouns, OOV (out of vocabulary) words which are not present in our corpus.
arxiv-7500-96 | Meteorological time series forecasting with pruned multi-layer perceptron and 2-stage Levenberg-Marquardt method | http://arxiv.org/pdf/1407.2169v1.pdf | author:Cyril Voyant, Wani W. Tamas, Marie Laure Nivet, Gilles Notton, Christophe Paoli, Aurélia Balu, Marc Muselli category:cs.NE cs.SY published:2014-07-08 summary:A Multi-Layer Perceptron (MLP) defines a family of artificial neural networksoften used in TS modeling and forecasting. Because of its "black box" aspect,many researchers refuse to use it. Moreover, the optimization (often based onthe exhaustive approach where "all" configurations are tested) and learningphases of this artificial intelligence tool (often based on theLevenberg-Marquardt algorithm; LMA) are weaknesses of this approach(exhaustively and local minima). These two tasks must be repeated depending onthe knowledge of each new problem studied, making the process, long, laboriousand not systematically robust. In this paper a pruning process is proposed.This method allows, during the training phase, to carry out an inputs selectingmethod activating (or not) inter-nodes connections in order to verify ifforecasting is improved. We propose to use iteratively the popular dampedleast-squares method to activate inputs and neurons. A first pass is applied to10% of the learning sample to determine weights significantly different from 0and delete other. Then a classical batch process based on LMA is used with thenew MLP. The validation is done using 25 measured meteorological TS andcross-comparing the prediction results of the classical LMA and the 2-stageLMA.
arxiv-7500-97 | Inferring latent structures via information inequalities | http://arxiv.org/pdf/1407.2256v1.pdf | author:R. Chaves, L. Luft, T. O. Maciel, D. Gross, D. Janzing, B. Schölkopf category:stat.ML quant-ph published:2014-07-08 summary:One of the goals of probabilistic inference is to decide whether anempirically observed distribution is compatible with a candidate Bayesiannetwork. However, Bayesian networks with hidden variables give rise to highlynon-trivial constraints on the observed distribution. Here, we propose aninformation-theoretic approach, based on the insight that conditions onentropies of Bayesian networks take the form of simple linear inequalities. Wedescribe an algorithm for deriving entropic tests for latent structures. Thewell-known conditional independence tests appear as a special case. While theapproach applies for generic Bayesian networks, we presently adopt the causalview, and show the versatility of the framework by treating several relevantproblems from that domain: detecting common ancestors, quantifying the strengthof causal influence, and inferring the direction of causation from two-variablemarginals.
arxiv-7500-98 | Tracking Individual Targets in High Density Crowd Scenes Analysis of a Video Recording in Hajj 2009 | http://arxiv.org/pdf/1407.2044v2.pdf | author:Mohamed H. Dridi category:cs.CV physics.soc-ph published:2014-07-08 summary:In this paper we present a number of methods (manual, semi-automatic andautomatic) for tracking individual targets in high density crowd scenes wherethousand of people are gathered. The necessary data about the motion ofindividuals and a lot of other physical information can be extracted fromconsecutive image sequences in different ways, including optical flow and blockmotion estimation. One of the famous methods for tracking moving objects is theblock matching method. This way to estimate subject motion requires thespecification of a comparison window which determines the scale of theestimate. In this work we present a real-time method for pedestrian recognitionand tracking in sequences of high resolution images obtained by a stationary(high definition) camera located in different places on the Haram mosque inMecca. The objective is to estimate pedestrian velocities as a function of thelocal density.The resulting data of tracking moving pedestrians based on videosequences are presented in the following section. Through the evaluated systemthe spatio-temporal coordinates of each pedestrian during the Tawaf ritual areestablished. The pilgrim velocities as function of the local densities in theMataf area (Haram Mosque Mecca) are illustrated and very precisely documented.
arxiv-7500-99 | Learning Discriminative Stein Kernel for SPD Matrices and Its Applications | http://arxiv.org/pdf/1407.1974v3.pdf | author:Jianjia Zhang, Lei Wang, Luping Zhou, Wanqing Li category:cs.CV published:2014-07-08 summary:Stein kernel has recently shown promising performance on classifying imagesrepresented by symmetric positive definite (SPD) matrices. It evaluates thesimilarity between two SPD matrices through their eigenvalues. In this paper,we argue that directly using the original eigenvalues may be problematicbecause: i) Eigenvalue estimation becomes biased when the number of samples isinadequate, which may lead to unreliable kernel evaluation; ii) Moreimportantly, eigenvalues only reflect the property of an individual SPD matrix.They are not necessarily optimal for computing Stein kernel when the goal is todiscriminate different sets of SPD matrices. To address the two issues in oneshot, we propose a discriminative Stein kernel, in which an extra parametervector is defined to adjust the eigenvalues of the input SPD matrices. Theoptimal parameter values are sought by optimizing a proxy of classificationperformance. To show the generality of the proposed method, three differentkernel learning criteria that are commonly used in the literature are employedrespectively as a proxy. A comprehensive experimental study is conducted on avariety of image classification tasks to compare our proposed discriminativeStein kernel with the original Stein kernel and other commonly used methods forevaluating the similarity between SPD matrices. The experimental resultsdemonstrate that, the discriminative Stein kernel can attain greaterdiscrimination and better align with classification tasks by altering theeigenvalues. This makes it produce higher classification performance than theoriginal Stein kernel and other commonly used methods.
arxiv-7500-100 | Novel methods for multilinear data completion and de-noising based on tensor-SVD | http://arxiv.org/pdf/1407.1785v2.pdf | author:Zemin Zhang, Gregory Ely, Shuchin Aeron, Ning Hao, Misha Kilmer category:cs.CV published:2014-07-07 summary:In this paper we propose novel methods for completion (from limited samples)and de-noising of multilinear (tensor) data and as an application consider 3-Dand 4- D (color) video data completion and de-noising. We exploit the recentlyproposed tensor-Singular Value Decomposition (t-SVD)[11]. Based on t-SVD, thenotion of multilinear rank and a related tensor nuclear norm was proposed in[11] to characterize informational and structural complexity of multilineardata. We first show that videos with linear camera motion can be representedmore efficiently using t-SVD compared to the approaches based on vectorizing orflattening of the tensors. Since efficiency in representation impliesefficiency in recovery, we outline a tensor nuclear norm penalized algorithmfor video completion from missing entries. Application of the proposedalgorithm for video recovery from missing entries is shown to yield a superiorperformance over existing methods. We also consider the problem of tensorrobust Principal Component Analysis (PCA) for de-noising 3-D video data fromsparse random corruptions. We show superior performance of our method comparedto the matrix robust PCA adapted to this setting as proposed in [4].
arxiv-7500-101 | Low Complexity Regularization of Linear Inverse Problems | http://arxiv.org/pdf/1407.1598v2.pdf | author:Samuel Vaiter, Gabriel Peyré, Jalal M. Fadili category:math.OC cs.IT math.IT stat.ML published:2014-07-07 summary:Inverse problems and regularization theory is a central theme in contemporarysignal processing, where the goal is to reconstruct an unknown signal frompartial indirect, and possibly noisy, measurements of it. A now standard methodfor recovering the unknown signal is to solve a convex optimization problemthat enforces some prior knowledge about its structure. This has provedefficient in many problems routinely encountered in imaging sciences,statistics and machine learning. This chapter delivers a review of recentadvances in the field where the regularization prior promotes solutionsconforming to some notion of simplicity/low-complexity. These priors encompassas popular examples sparsity and group sparsity (to capture the compressibilityof natural signals and images), total variation and analysis sparsity (topromote piecewise regularity), and low-rank (as natural extension of sparsityto matrix-valued data). Our aim is to provide a unified treatment of all theseregularizations under a single umbrella, namely the theory of partialsmoothness. This framework is very general and accommodates all low-complexityregularizers just mentioned, as well as many others. Partial smoothness turnsout to be the canonical way to encode low-dimensional models that can be linearspaces or more general smooth manifolds. This review is intended to serve as aone stop shop toward the understanding of the theoretical properties of theso-regularized solutions. It covers a large spectrum including: (i) recoveryguarantees and stability to noise, both in terms of $\ell^2$-stability andmodel (manifold) identification; (ii) sensitivity analysis to perturbations ofthe parameters involved (in particular the observations), with applications tounbiased risk estimation ; (iii) convergence properties of the forward-backwardproximal splitting scheme, that is particularly well suited to solve thecorresponding large-scale regularized optimization problem.
arxiv-7500-102 | Analyzing the Performance of Multilayer Neural Networks for Object Recognition | http://arxiv.org/pdf/1407.1610v2.pdf | author:Pulkit Agrawal, Ross Girshick, Jitendra Malik category:cs.CV cs.NE published:2014-07-07 summary:In the last two years, convolutional neural networks (CNNs) have achieved animpressive suite of results on standard recognition datasets and tasks.CNN-based features seem poised to quickly replace engineered representations,such as SIFT and HOG. However, compared to SIFT and HOG, we understand muchless about the nature of the features learned by large CNNs. In this paper, weexperimentally probe several aspects of CNN feature learning in an attempt tohelp practitioners gain useful, evidence-backed intuitions about how to applyCNNs to computer vision problems.
arxiv-7500-103 | WordRep: A Benchmark for Research on Learning Word Representations | http://arxiv.org/pdf/1407.1640v1.pdf | author:Bin Gao, Jiang Bian, Tie-Yan Liu category:cs.CL cs.LG published:2014-07-07 summary:WordRep is a benchmark collection for the research on learning distributedword representations (or word embeddings), released by Microsoft Research. Inthis paper, we describe the details of the WordRep collection and show how touse it in different types of machine learning research related to wordembedding. Specifically, we describe how the evaluation tasks in WordRep areselected, how the data are sampled, and how the evaluation tool is built. Wethen compare several state-of-the-art word representations on WordRep, reporttheir evaluation performance, and make discussions on the results. After that,we discuss new potential research topics that can be supported by WordRep, inaddition to algorithm comparison. We hope that this paper can help people gaindeeper understanding of WordRep, and enable more interesting research onlearning distributed word representations and related topics.
arxiv-7500-104 | Spectral norm of random tensors | http://arxiv.org/pdf/1407.1870v1.pdf | author:Ryota Tomioka, Taiji Suzuki category:math.ST stat.ML stat.TH published:2014-07-07 summary:We show that the spectral norm of a random $n_1\times n_2\times \cdots \timesn_K$ tensor (or higher-order array) scales as$O\left(\sqrt{(\sum_{k=1}^{K}n_k)\log(K)}\right)$ under some sub-Gaussianassumption on the entries. The proof is based on a covering number argument.Since the spectral norm is dual to the tensor nuclear norm (the tightest convexrelaxation of the set of rank one tensors), the bound implies that the convexrelaxation yields sample complexity that is linear in (the sum of) the numberof dimensions, which is much smaller than other recently proposed convexrelaxations of tensor rank that use unfolding.
arxiv-7500-105 | Recommending Learning Algorithms and Their Associated Hyperparameters | http://arxiv.org/pdf/1407.1890v1.pdf | author:Michael R. Smith, Logan Mitchell, Christophe Giraud-Carrier, Tony Martinez category:cs.LG stat.ML published:2014-07-07 summary:The success of machine learning on a given task dependson, among otherthings, which learning algorithm is selected and its associatedhyperparameters. Selecting an appropriate learning algorithm and setting itshyperparameters for a given data set can be a challenging task, especially forusers who are not experts in machine learning. Previous work has examined usingmeta-features to predict which learning algorithm and hyperparameters should beused. However, choosing a set of meta-features that are predictive of algorithmperformance is difficult. Here, we propose to apply collaborative filteringtechniques to learning algorithm and hyperparameter selection, and find thatdoing so avoids determining which meta-features to use and outperformstraditional meta-learning approaches in many cases.
arxiv-7500-106 | The Primal-Dual Hybrid Gradient Method for Semiconvex Splittings | http://arxiv.org/pdf/1407.1723v1.pdf | author:Thomas Möllenhoff, Evgeny Strekalovskiy, Michael Moeller, Daniel Cremers category:math.NA cs.CV cs.NA math.OC published:2014-07-07 summary:This paper deals with the analysis of a recent reformulation of theprimal-dual hybrid gradient method [Zhu and Chan 2008, Pock, Cremers, Bischofand Chambolle 2009, Esser, Zhang and Chan 2010, Chambolle and Pock 2011], whichallows to apply it to nonconvex regularizers as first proposed for truncatedquadratic penalization in [Strekalovskiy and Cremers 2014]. Particularly, itinvestigates variational problems for which the energy to be minimized can bewritten as $G(u) + F(Ku)$, where $G$ is convex, $F$ semiconvex, and $K$ is alinear operator. We study the method and prove convergence in the case wherethe nonconvexity of $F$ is compensated by the strong convexity of the $G$. Theconvergence proof yields an interesting requirement for the choice of algorithmparameters, which we show to not only be sufficient, but necessary.Additionally, we show boundedness of the iterates under much weaker conditions.Finally, we demonstrate effectiveness and convergence of the algorithm beyondthe theoretical guarantees in several numerical experiments.
arxiv-7500-107 | KNET: A General Framework for Learning Word Embedding using Morphological Knowledge | http://arxiv.org/pdf/1407.1687v3.pdf | author:Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, Tie-Yan Liu category:cs.CL cs.LG published:2014-07-07 summary:Neural network techniques are widely applied to obtain high-qualitydistributed representations of words, i.e., word embeddings, to address textmining, information retrieval, and natural language processing tasks. Recently,efficient methods have been proposed to learn word embeddings from context thatcaptures both semantic and syntactic relationships between words. However, itis challenging to handle unseen words or rare words with insufficient context.In this paper, inspired by the study on word recognition process in cognitivepsychology, we propose to take advantage of seemingly less obvious butessentially important morphological knowledge to address these challenges. Inparticular, we introduce a novel neural network architecture called KNET thatleverages both contextual information and morphological word similarity builtbased on morphological knowledge to learn word embeddings. Meanwhile, thelearning architecture is also able to refine the pre-defined morphologicalknowledge and obtain more accurate word similarity. Experiments on ananalogical reasoning task and a word similarity task both demonstrate that theproposed KNET framework can greatly enhance the effectiveness of wordembeddings.
arxiv-7500-108 | Simultaneous Detection and Segmentation | http://arxiv.org/pdf/1407.1808v1.pdf | author:Bharath Hariharan, Pablo Arbeláez, Ross Girshick, Jitendra Malik category:cs.CV published:2014-07-07 summary:We aim to detect all instances of a category in an image and, for eachinstance, mark the pixels that belong to it. We call this task SimultaneousDetection and Segmentation (SDS). Unlike classical bounding box detection, SDSrequires a segmentation and not just a box. Unlike classical semanticsegmentation, we require individual object instances. We build on recent workthat uses convolutional neural networks to classify category-independent regionproposals (R-CNN [16]), introducing a novel architecture tailored for SDS. Wethen use category-specific, top- down figure-ground predictions to refine ourbottom-up proposals. We show a 7 point boost (16% relative) over our baselineson SDS, a 5 point boost (10% relative) over state-of-the-art on semanticsegmentation, and state-of-the-art performance in object detection. Finally, weprovide diagnostic tools that unpack performance and provide directions forfuture work.
arxiv-7500-109 | Downscaling near-surface atmospheric fields with multi-objective Genetic Programming | http://arxiv.org/pdf/1407.1768v1.pdf | author:Tanja Zerenner, Victor Venema, Petra Friederichs, Clemens Simmer category:physics.ao-ph cs.NE published:2014-07-07 summary:The coupling of models for the different components of theSoil-Vegetation-Atmosphere-System is required to investigate componentinteractions and feedback processes. However, the component models foratmosphere, land-surface and subsurface are usually operated at differentresolutions in space and time owing to the dominant processes. Thecomputationally often more expensive atmospheric models, for instance, aretypically employed at a coarser resolution than land-surface and subsurfacemodels. Thus up- and downscaling procedures are required at the interfacebetween the atmospheric model and the land-surface/subsurface models. We applymulti-objective Genetic Programming (GP) to a training data set ofhigh-resolution atmospheric model runs to learn equations or short programsthat reconstruct the fine-scale fields (e.g., 400 m resolution) of thenear-surface atmospheric state variables from the coarse atmospheric modeloutput (e.g., 2.8 km resolution). Like artificial neural networks, GP canflexibly incorporate multivariate and nonlinear relations, but offers theadvantage that the solutions are human readable and thus can be checked forphysical consistency. Using the Strength Pareto Approach for multi-objectivefitness assignment allows us to consider multiple characteristics of thefine-scale fields during the learning procedure.
arxiv-7500-110 | Les noms propres se traduisent-ils ? Étude d'un corpus multilingue | http://arxiv.org/pdf/1407.1605v1.pdf | author:Émeline Lecuit, Denis Maurel, Dusko Vitas category:cs.CL published:2014-07-07 summary:In this paper, we tackle the problem of the translation of proper names. Weintroduce our hypothesis according to which proper names can be translated moreoften than most people seem to think. Then, we describe the construction of aparallel multilingual corpus used to illustrate our point. We eventuallyevaluate both the advantages and limits of this corpus in our study.
arxiv-7500-111 | Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent | http://arxiv.org/pdf/1407.1537v4.pdf | author:Zeyuan Allen-Zhu, Lorenzo Orecchia category:cs.DS cs.LG cs.NA math.OC stat.ML published:2014-07-06 summary:First-order methods play a central role in large-scale convex optimization.Even though many variations exist, each suited to a particular problem form,almost all such methods fundamentally rely on two types of algorithmic stepsand two corresponding types of analysis: gradient-descent steps, which yieldprimal progress, and mirror-descent steps, which yield dual progress. In thispaper, we observe that the performances of these two types of step arecomplementary, so that faster algorithms can be designed by linearly couplingthe two steps. In particular, we obtain a simple accelerated gradient method for the classof smooth convex optimization problems. The first such method was proposed byNesterov back to 1983, but to the best of our knowledge, the proof of the fastconvergence of accelerated gradient methods has not found a clearinterpretation and is still regarded by many as crucially relying on "algebraictricks". We apply our novel insights to construct a new accelerated gradientmethod as a natural linear coupling of gradient descent and mirror descent andto write its proof of convergence as a simple combination of the convergenceanalyses of the two underlying descent steps. We believe that the complementary view and the linear coupling technique inthis paper will prove very useful in the design of first-order methods as itallows us to design fast algorithms in a conceptually easier way. For instance,our technique greatly facilitates the recent breakthroughs in solving packingand covering linear programs [AO14, AO15].
arxiv-7500-112 | Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method | http://arxiv.org/pdf/1407.1543v2.pdf | author:Boaz Barak, Jonathan A. Kelner, David Steurer category:cs.DS cs.LG stat.ML published:2014-07-06 summary:We give a new approach to the dictionary learning (also known as "sparsecoding") problem of recovering an unknown $n\times m$ matrix $A$ (for $m \geqn$) from examples of the form \[ y = Ax + e, \] where $x$ is a random vector in$\mathbb R^m$ with at most $\tau m$ nonzero coordinates, and $e$ is a randomnoise vector in $\mathbb R^n$ with bounded magnitude. For the case $m=O(n)$,our algorithm recovers every column of $A$ within arbitrarily good constantaccuracy in time $m^{O(\log m/\log(\tau^{-1}))}$, in particular achievingpolynomial time if $\tau = m^{-\delta}$ for any $\delta>0$, and time $m^{O(\logm)}$ if $\tau$ is (a sufficiently small) constant. Prior algorithms withcomparable assumptions on the distribution required the vector $x$ to be muchsparser---at most $\sqrt{n}$ nonzero coordinates---and there were intrinsicbarriers preventing these algorithms from applying for denser $x$. We achieve this by designing an algorithm for noisy tensor decomposition thatcan recover, under quite general conditions, an approximate rank-onedecomposition of a tensor $T$, given access to a tensor $T'$ that is$\tau$-close to $T$ in the spectral norm (when considered as a matrix). To ourknowledge, this is the first algorithm for tensor decomposition that works inthe constant spectral-norm noise regime, where there is no guarantee that thelocal optima of $T$ and $T'$ have similar structures. Our algorithm is based on a novel approach to using and analyzing the Sum ofSquares semidefinite programming hierarchy (Parrilo 2000, Lasserre 2001), andit can be viewed as an indication of the utility of this very general andpowerful tool for unsupervised learning problems.
arxiv-7500-113 | The jump set under geometric regularisation. Part 1: Basic technique and first-order denoising | http://arxiv.org/pdf/1407.1531v2.pdf | author:Tuomo Valkonen category:math.FA cs.CV published:2014-07-06 summary:Let $u \in \mbox{BV}(\Omega)$ solve the total variation denoising problemwith $L^2$-squared fidelity and data $f$. Caselles et al. [Multiscale Model.Simul. 6 (2008), 879--894] have shown the containment $\mathcal{H}^{m-1}(J_u\setminus J_f)=0$ of the jump set $J_u$ of $u$ in that of $f$. Their proofunfortunately depends heavily on the co-area formula, as do many results inthis area, and as such is not directly extensible to higher-order,curvature-based, and other advanced geometric regularisers, such as totalgeneralised variation (TGV) and Euler's elastica. These have received increasedattention in recent times due to their better practical regularisationproperties compared to conventional total variation or wavelets. We proveanalogous jump set containment properties for a general class of regularisers.We do this with novel Lipschitz transformation techniques, and do not requirethe co-area formula. In the present Part 1 we demonstrate the general techniqueon first-order regularisers, while in Part 2 we will extend it to higher-orderregularisers. In particular, we concentrate in this part on TV and, as anovelty, Huber-regularised TV. We also demonstrate that the technique wouldapply to non-convex TV models as well as the Perona-Malik anisotropicdiffusion, if these approaches were well-posed to begin with.
arxiv-7500-114 | Large-Scale Multi-Label Learning with Incomplete Label Assignments | http://arxiv.org/pdf/1407.1538v1.pdf | author:Xiangnan Kong, Zhaoming Wu, Li-Jia Li, Ruofei Zhang, Philip S. Yu, Hang Wu, Wei Fan category:cs.LG published:2014-07-06 summary:Multi-label learning deals with the classification problems where eachinstance can be assigned with multiple labels simultaneously. Conventionalmulti-label learning approaches mainly focus on exploiting label correlations.It is usually assumed, explicitly or implicitly, that the label sets fortraining instances are fully labeled without any missing labels. However, inmany real-world multi-label datasets, the label assignments for traininginstances can be incomplete. Some ground-truth labels can be missed by thelabeler from the label set. This problem is especially typical when the numberinstances is very large, and the labeling cost is very high, which makes italmost impossible to get a fully labeled training set. In this paper, we studythe problem of large-scale multi-label learning with incomplete labelassignments. We propose an approach, called MPU, based upon positive andunlabeled stochastic gradient descent and stacked models. Unlike prior works,our method can effectively and efficiently consider missing labels and labelcorrelations simultaneously, and is very scalable, that has linear timecomplexities over the size of the data. Extensive experiments on two real-worldmulti-label datasets show that our MPU model consistently outperform othercommonly-used baselines.
arxiv-7500-115 | Large-scale Supervised Hierarchical Feature Learning for Face Recognition | http://arxiv.org/pdf/1407.1490v1.pdf | author:Jianguo Li, Yurong Chen category:cs.CV published:2014-07-06 summary:This paper proposes a novel face recognition algorithm based on large-scalesupervised hierarchical feature learning. The approach consists of two parts:hierarchical feature learning and large-scale model learning. The hierarchicalfeature learning searches feature in three levels of granularity in asupervised way. First, face images are modeled by receptive field theory, andthe representation is an image with many channels of Gaussian receptive maps.We activate a few most distinguish channels by supervised learning. Second, theface image is further represented by patches of picked channels, and we searchfrom the over-complete patch pool to activate only those most discriminantpatches. Third, the feature descriptor of each patch is further projected tolower dimension subspace with discriminant subspace analysis. Learned feature of activated patches are concatenated to get a full facerepresentation.A linear classifier is learned to separate face pairs from samesubjects and different subjects. As the number of face pairs are extremelylarge, we introduce ADMM (alternative direction method of multipliers) to trainthe linear classifier on a computing cluster. Experiments show that moretraining samples will bring notable accuracy improvement. We conduct experiments on FRGC and LFW. Results show that the proposedapproach outperforms existing algorithms under the same protocol notably.Besides, the proposed approach is small in memory footprint, and low incomputing cost, which makes it suitable for embedded applications.
arxiv-7500-116 | Generalized Higher-Order Tensor Decomposition via Parallel ADMM | http://arxiv.org/pdf/1407.1399v1.pdf | author:Fanhua Shang, Yuanyuan Liu, James Cheng category:cs.NA cs.LG published:2014-07-05 summary:Higher-order tensors are becoming prevalent in many scientific areas such ascomputer vision, social network analysis, data mining and neuroscience.Traditional tensor decomposition approaches face three major challenges: modelselecting, gross corruptions and computational efficiency. To address theseproblems, we first propose a parallel trace norm regularized tensordecomposition method, and formulate it as a convex optimization problem. Thismethod does not require the rank of each mode to be specified beforehand, andcan automatically determine the number of factors in each mode through ouroptimization scheme. By considering the low-rank structure of the observedtensor, we analyze the equivalent relationship of the trace norm between alow-rank tensor and its core tensor. Then, we cast a non-convex tensordecomposition model into a weighted combination of multiple much smaller-scalematrix trace norm minimization. Finally, we develop two parallel alternatingdirection methods of multipliers (ADMM) to solve our problems. Experimentalresults verify that our regularized formulation is effective, and our methodsare robust to noise or outliers.
arxiv-7500-117 | Homophilic Clustering by Locally Asymmetric Geometry | http://arxiv.org/pdf/1407.1352v1.pdf | author:Deli Zhao, Xiaoou Tang category:cs.CV published:2014-07-05 summary:Clustering is indispensable for data analysis in many scientific disciplines.Detecting clusters from heavy noise remains challenging, particularly forhigh-dimensional sparse data. Based on graph-theoretic framework, the presentpaper proposes a novel algorithm to address this issue. The locally asymmetricgeometries of neighborhoods between data points result in a directed similaritygraph to model the structural connectivity of data points. Performingsimilarity propagation on this directed graph simply by its adjacency matrixpowers leads to an interesting discovery, in the sense that if the in-degreesare ordered by the corresponding sorted out-degrees, they will beself-organized to be homophilic layers according to the different distributionsof cluster densities, which is dubbed the Homophilic In-degree figure (the HIfigure). With the HI figure, we can easily single out all cores of clusters,identify the boundary between cluster and noise, and visualize the intrinsicstructures of clusters. Based on the in-degree homophily, we also develop asimple efficient algorithm of linear space complexity to cluster noisy data.Extensive experiments on toy and real-world scientific data validate theeffectiveness of our algorithms.
arxiv-7500-118 | A New Approach for Super resolution by Using Web Images and FFT Based Image Registration | http://arxiv.org/pdf/1407.3675v1.pdf | author:Archana Vijayan, Vincy Salam category:cs.CV published:2014-07-05 summary:Preserving accuracy is a challenging issue in super resolution images. Inthis paper, we propose a new FFT based image registration algorithm and asparse based super resolution algorithm to improve the accuracy of superresolution image. Given a low resolution image, our approach initially extractsthe local descriptors from the input and then the local descriptors from thewhole correlated images using the SIFT algorithm. Once this is completed, itwill compare the local descriptors on the basis of a threshold value. Theretrieved images could be having different focal length, illumination,inclination and size. To overcome the above differences of the retrievedimages, we propose a new FFT based image registration algorithm. After theregistration stage, we apply a sparse based super resolution on the images forrecreating images with better resolution compared to the input. Based on thePSSNR calculation and SSIM comparison, we can see that the new methodologycreates a better image than the traditional methods.
arxiv-7500-119 | Image Fusion Using LEP Filtering and Bilinear Interpolation | http://arxiv.org/pdf/1407.3986v1.pdf | author:Haritha Raveendran, Deepa Thomas category:cs.CV published:2014-07-05 summary:Image Fusion is the process in which core information from a set of componentimages is merged to form a single image, which is more informative and completethan the component input images in quality and appearance. This paper presentsa fast and effective image fusion method for creating high quality fused imagesby merging component images. In the proposed method, the input image is brokendown to a two-scale image representation with a base layer having large scalevariations in intensity, and a detail layer containing small scale details.Here fusion of the base and detail layers is implemented by means of a LocalEdge preserving filtering based technique. The proposed method is an efficientimage fusion technique in which the noise component is very low and quality ofthe resultant image is high so that it can be used for applications likemedical image processing, requiring very accurate edge preserved images.Performance is tested by calculating PSNR and SSIM of images. The benefit ofthe proposed method is that it removes noise without altering the underlyingstructures of the image. This paper also presents an image zooming techniqueusing bilinear interpolation in which a portion of the input image is croppedand bilinear interpolation is applied. Experimental results showed that thewhen PSNR value is calculated, the noise is found to be very low for theresultant image portion.
arxiv-7500-120 | Reinforcement Learning Based Algorithm for the Maximization of EV Charging Station Revenue | http://arxiv.org/pdf/1407.1291v2.pdf | author:Stoyan Dimitrov, Redouane Lguensat category:cs.CE cs.LG math.OC stat.AP published:2014-07-04 summary:This paper presents an online reinforcement learning based application whichincreases the revenue of one particular electric vehicles (EV) station,connected to a renewable source of energy. Moreover, the proposed applicationadapts to changes in the trends of the station's average number of customersand their types. Most of the parameters in the model are simulatedstochastically and the algorithm used is a Q-learning algorithm. A computersimulation was implemented which demonstrates and confirms the utility of themodel.
arxiv-7500-121 | Expanding the Family of Grassmannian Kernels: An Embedding Perspective | http://arxiv.org/pdf/1407.1123v1.pdf | author:Mehrtash T. Harandi, Mathieu Salzmann, Sadeep Jayasumana, Richard Hartley, Hongdong Li category:cs.CV cs.LG stat.ML published:2014-07-04 summary:Modeling videos and image-sets as linear subspaces has proven beneficial formany visual recognition tasks. However, it also incurs challenges arising fromthe fact that linear subspaces do not obey Euclidean geometry, but lie on aspecial type of Riemannian manifolds known as Grassmannian. To leverage thetechniques developed for Euclidean spaces (e.g, support vector machines) withsubspaces, several recent studies have proposed to embed the Grassmannian intoa Hilbert space by making use of a positive definite kernel. Unfortunately,only two Grassmannian kernels are known, none of which -as we will show- isuniversal, which limits their ability to approximate a target functionarbitrarily well. Here, we introduce several positive definite Grassmanniankernels, including universal ones, and demonstrate their superiority overpreviously-known kernels in various tasks, such as classification, clustering,sparse coding and hashing.
arxiv-7500-122 | Robust Optimization using Machine Learning for Uncertainty Sets | http://arxiv.org/pdf/1407.1097v1.pdf | author:Theja Tulabandhula, Cynthia Rudin category:math.OC cs.LG stat.ML published:2014-07-04 summary:Our goal is to build robust optimization problems for making decisions basedon complex data from the past. In robust optimization (RO) generally, the goalis to create a policy for decision-making that is robust to our uncertaintyabout the future. In particular, we want our policy to best handle the theworst possible situation that could arise, out of an uncertainty set ofpossible situations. Classically, the uncertainty set is simply chosen by theuser, or it might be estimated in overly simplistic ways with strongassumptions; whereas in this work, we learn the uncertainty set from datacollected in the past. The past data are drawn randomly from an (unknown)possibly complicated high-dimensional distribution. We propose a newuncertainty set design and show how tools from statistical learning theory canbe employed to provide probabilistic guarantees on the robustness of thepolicy.
arxiv-7500-123 | Optimizing Ranking Measures for Compact Binary Code Learning | http://arxiv.org/pdf/1407.1151v1.pdf | author:Guosheng Lin, Chunhua Shen, Jianxin Wu category:cs.LG cs.CV published:2014-07-04 summary:Hashing has proven a valuable tool for large-scale information retrieval.Despite much success, existing hashing methods optimize over simple objectivessuch as the reconstruction error or graph Laplacian related loss functions,instead of the performance evaluation criteria of interest---multivariateperformance measures such as the AUC and NDCG. Here we present a generalframework (termed StructHash) that allows one to directly optimize multivariateperformance measures. The resulting optimization problem can involveexponentially or infinitely many variables and constraints, which is morechallenging than standard structured output learning. To solve the StructHashoptimization problem, we use a combination of column generation andcutting-plane techniques. We demonstrate the generality of StructHash byapplying it to ranking prediction and image retrieval, and show that itoutperforms a few state-of-the-art hashing methods.
arxiv-7500-124 | Recognition of Isolated Words using Zernike and MFCC features for Audio Visual Speech Recognition | http://arxiv.org/pdf/1407.1165v1.pdf | author:Prashant Bordea, Amarsinh Varpeb, Ramesh Manzac, Pravin Yannawara category:cs.CV cs.CL published:2014-07-04 summary:Automatic Speech Recognition (ASR) by machine is an attractive research topicin signal processing domain and has attracted many researchers to contribute inthis area. In recent year, there have been many advances in automatic speechreading system with the inclusion of audio and visual speech features torecognize words under noisy conditions. The objective of audio-visual speechrecognition system is to improve recognition accuracy. In this paper wecomputed visual features using Zernike moments and audio feature using MelFrequency Cepstral Coefficients (MFCC) on vVISWa (Visual Vocabulary ofIndependent Standard Words) dataset which contains collection of isolated setof city names of 10 speakers. The visual features were normalized and dimensionof features set was reduced by Principal Component Analysis (PCA) in order torecognize the isolated word utterance on PCA space.The performance ofrecognition of isolated words based on visual only and audio only featuresresults in 63.88 and 100 respectively.
arxiv-7500-125 | Identifying Higher-order Combinations of Binary Features | http://arxiv.org/pdf/1407.1176v1.pdf | author:Felipe Llinares, Mahito Sugiyama, Karsten M. Borgwardt category:stat.ML cs.LG published:2014-07-04 summary:Finding statistically significant interactions between binary variables iscomputationally and statistically challenging in high-dimensional settings, dueto the combinatorial explosion in the number of hypotheses. Terada et al.recently showed how to elegantly address this multiple testing problem byexcluding non-testable hypotheses. Still, it remains unclear how their approachscales to large datasets. We here proposed strategies to speed up the approach by Terada et al. andevaluate them thoroughly in 11 real-world benchmark datasets. We observe thatone approach, incremental search with early stopping, is orders of magnitudefaster than the current state-of-the-art approach.
arxiv-7500-126 | From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for SPD Matrices | http://arxiv.org/pdf/1407.1120v2.pdf | author:Mehrtash T. Harandi, Mathieu Salzmann, Richard Hartley category:cs.CV published:2014-07-04 summary:Representing images and videos with Symmetric Positive Definite (SPD)matrices and considering the Riemannian geometry of the resulting space hasproven beneficial for many recognition tasks. Unfortunately, computation on theRiemannian manifold of SPD matrices --especially of high-dimensional ones--comes at a high cost that limits the applicability of existing techniques. Inthis paper we introduce an approach that lets us handle high-dimensional SPDmatrices by constructing a lower-dimensional, more discriminative SPD manifold.To this end, we model the mapping from the high-dimensional SPD manifold to thelow-dimensional one with an orthonormal projection. In particular, we searchfor a projection that yields a low-dimensional manifold with maximumdiscriminative power encoded via an affinity-weighted similarity measure basedon metrics on the manifold. Learning can then be expressed as an optimizationproblem on a Grassmann manifold. Our evaluation on several classification tasksshows that our approach leads to a significant accuracy gain overstate-of-the-art methods.
arxiv-7500-127 | Inverse Graphics with Probabilistic CAD Models | http://arxiv.org/pdf/1407.1339v1.pdf | author:Tejas D. Kulkarni, Vikash K. Mansinghka, Pushmeet Kohli, Joshua B. Tenenbaum category:cs.CV cs.AI stat.ML published:2014-07-04 summary:Recently, multiple formulations of vision problems as probabilisticinversions of generative models based on computer graphics have been proposed.However, applications to 3D perception from natural images have focused onlow-dimensional latent scenes, due to challenges in both modeling andinference. Accounting for the enormous variability in 3D object shape and 2Dappearance via realistic generative models seems intractable, as does invertingeven simple versions of the many-to-many computations that link 3D scenes to 2Dimages. This paper proposes and evaluates an approach that addresses keyaspects of both these challenges. We show that it is possible to solvechallenging, real-world 3D vision problems by approximate inference ingenerative models for images based on rendering the outputs of probabilisticCAD (PCAD) programs. Our PCAD object geometry priors generate deformable 3Dmeshes corresponding to plausible objects and apply affine transformations toplace them in a scene. Image likelihoods are based on similarity in a featurespace based on standard mid-level image representations from the visionliterature. Our inference algorithm integrates single-site and locally blockedMetropolis-Hastings proposals, Hamiltonian Monte Carlo and discriminativedata-driven proposals learned from training data generated from our models. Weapply this approach to 3D human pose estimation and object shape reconstructionfrom single images, achieving quantitative and qualitative performanceimprovements over state-of-the-art baselines.
arxiv-7500-128 | Calibration of Multiple Fish-Eye Cameras Using a Wand | http://arxiv.org/pdf/1407.1267v1.pdf | author:Qiang Fu, Quan Quan, Kai-Yuan Cai category:cs.CV published:2014-07-04 summary:Fish-eye cameras are becoming increasingly popular in computer vision, buttheir use for 3D measurement is limited partly due to the lack of an accurate,efficient and user-friendly calibration procedure. For such a purpose, wepropose a method to calibrate the intrinsic and extrinsic parameters (includingradial distortion parameters) of two/multiple fish-eye cameras simultaneouslyby using a wand under general motions. Thanks to the generic camera model used,the proposed calibration method is also suitable for two/multiple conventionalcameras and mixed cameras (e.g. two conventional cameras and a fish-eyecamera). Simulation and real experiments demonstrate the effectiveness of theproposed method. Moreover, we develop the camera calibration toolbox, which isavailable online.
arxiv-7500-129 | Weakly Supervised Action Labeling in Videos Under Ordering Constraints | http://arxiv.org/pdf/1407.1208v1.pdf | author:Piotr Bojanowski, Rémi Lajugie, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid, Josef Sivic category:cs.CV cs.LG published:2014-07-04 summary:We are given a set of video clips, each one annotated with an {\em ordered}list of actions, such as "walk" then "sit" then "answer phone" extracted from,for example, the associated text script. We seek to temporally localize theindividual actions in each clip as well as to learn a discriminative classifierfor each action. We formulate the problem as a weakly supervised temporalassignment with ordering constraints. Each video clip is divided into smalltime intervals and each time interval of each video clip is assigned one actionlabel, while respecting the order in which the action labels appear in thegiven annotations. We show that the action label assignment can be determinedtogether with learning a classifier for each action in a discriminative manner.We evaluate the proposed model on a new and challenging dataset of 937 videoclips with a total of 787720 frames containing sequences of 16 differentactions from 69 Hollywood movies.
arxiv-7500-130 | Improving Performance of Self-Organising Maps with Distance Metric Learning Method | http://arxiv.org/pdf/1407.1201v1.pdf | author:Piotr Płoński, Krzysztof Zaremba category:cs.LG cs.NE published:2014-07-04 summary:Self-Organising Maps (SOM) are Artificial Neural Networks used in PatternRecognition tasks. Their major advantage over other architectures is humanreadability of a model. However, they often gain poorer accuracy. Mostly usedmetric in SOM is the Euclidean distance, which is not the best approach to someproblems. In this paper, we study an impact of the metric change on the SOM'sperformance in classification problems. In order to change the metric of theSOM we applied a distance metric learning method, so-called 'Large MarginNearest Neighbour'. It computes the Mahalanobis matrix, which assures smalldistance between nearest neighbour points from the same class and separation ofpoints belonging to different classes by large margin. Results are presented onseveral real data sets, containing for example recognition of written digits,spoken letters or faces.
arxiv-7500-131 | Anomaly Detection Based on Aggregation of Indicators | http://arxiv.org/pdf/1407.0880v2.pdf | author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG published:2014-07-03 summary:Automatic anomaly detection is a major issue in various areas. Beyond meredetection, the identification of the origin of the problem that produced theanomaly is also essential. This paper introduces a general methodology that canassist human operators who aim at classifying monitoring signals. The main ideais to leverage expert knowledge by generating a very large number ofindicators. A feature selection method is used to keep only the mostdiscriminant indicators which are used as inputs of a Naive Bayes classifier.The parameters of the classifier have been optimized indirectly by theselection process. Simulated data designed to reproduce some of the anomalytypes observed in real world engines.
arxiv-7500-132 | Projecting Ising Model Parameters for Fast Mixing | http://arxiv.org/pdf/1407.0749v2.pdf | author:Justin Domke, Xianghang Liu category:cs.LG stat.ML published:2014-07-03 summary:Inference in general Ising models is difficult, due to high treewidth makingtree-based algorithms intractable. Moreover, when interactions are strong,Gibbs sampling may take exponential time to converge to the stationarydistribution. We present an algorithm to project Ising model parameters onto aparameter set that is guaranteed to be fast mixing, under several divergences.We find that Gibbs sampling using the projected parameters is more accuratethan with the original parameters when interaction strengths are strong andwhen limited time is available for sampling.
arxiv-7500-133 | Global convergence of splitting methods for nonconvex composite optimization | http://arxiv.org/pdf/1407.0753v6.pdf | author:Guoyin Li, Ting Kei Pong category:math.OC cs.LG math.NA stat.ML published:2014-07-03 summary:We consider the problem of minimizing the sum of a smooth function $h$ with abounded Hessian, and a nonsmooth function. We assume that the latter functionis a composition of a proper closed function $P$ and a surjective linear map$\cal M$, with the proximal mappings of $\tau P$, $\tau > 0$, simple tocompute. This problem is nonconvex in general and encompasses many importantapplications in engineering and machine learning. In this paper, we examinedtwo types of splitting methods for solving this nonconvex optimization problem:alternating direction method of multipliers and proximal gradient algorithm.For the direct adaptation of the alternating direction method of multipliers,we show that, if the penalty parameter is chosen sufficiently large and thesequence generated has a cluster point, then it gives a stationary point of thenonconvex problem. We also establish convergence of the whole sequence under anadditional assumption that the functions $h$ and $P$ are semi-algebraic.Furthermore, we give simple sufficient conditions to guarantee boundedness ofthe sequence generated. These conditions can be satisfied for a wide range ofapplications including the least squares problem with the $\ell_{1/2}$regularization. Finally, when $\cal M$ is the identity so that the proximalgradient algorithm can be efficiently applied, we show that any cluster pointis stationary under a slightly more flexible constant step-size rule than whatis known in the literature for a nonconvex $h$.
arxiv-7500-134 | Structured Learning via Logistic Regression | http://arxiv.org/pdf/1407.0754v1.pdf | author:Justin Domke category:cs.LG stat.ML published:2014-07-03 summary:A successful approach to structured learning is to write the learningobjective as a joint function of linear parameters and inference messages, anditerate between updates to each. This paper observes that if the inferenceproblem is "smoothed" through the addition of entropy terms, for fixedmessages, the learning objective reduces to a traditional (non-structured)logistic regression problem with respect to parameters. In these logisticregression problems, each training example has a bias term determined by thecurrent set of messages. Based on this insight, the structured energy functioncan be extended from linear factors to any function class where an "oracle"exists to minimize a logistic loss.
arxiv-7500-135 | BiofilmQuant: A Computer-Assisted Tool for Dental Biofilm Quantification | http://arxiv.org/pdf/1407.0765v1.pdf | author:Awais Mansoor, Valery Patsekin, Dale Scherl, J. Paul Robinson, Bartlomiej Rajwa category:cs.CV published:2014-07-03 summary:Dental biofilm is the deposition of microbial material over a toothsubstratum. Several methods have recently been reported in the literature forbiofilm quantification; however, at best they provide a barely automatedsolution requiring significant input needed from the human expert. On thecontrary, state-of-the-art automatic biofilm methods fail to make their wayinto clinical practice because of the lack of effective mechanism toincorporate human input to handle praxis or misclassified regions. Manualdelineation, the current gold standard, is time consuming and subject to expertbias. In this paper, we introduce a new semi-automated software tool,BiofilmQuant, for dental biofilm quantification in quantitative light-inducedfluorescence (QLF) images. The software uses a robust statistical modelingapproach to automatically segment the QLF image into three classes (background,biofilm, and tooth substratum) based on the training data. This initialsegmentation has shown a high degree of consistency and precision on more than200 test QLF dental scans. Further, the proposed software provides theclinicians full control to fix any misclassified areas using a single click. Inaddition, BiofilmQuant also provides a complete solution for the longitudinalquantitative analysis of biofilm of the full set of teeth, providing greaterease of usability.
arxiv-7500-136 | Strengthening the Effectiveness of Pedestrian Detection with Spatially Pooled Features | http://arxiv.org/pdf/1407.0786v1.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV published:2014-07-03 summary:We propose a simple yet effective approach to the problem of pedestriandetection which outperforms the current state-of-the-art. Our new features arebuilt on the basis of low-level visual features and spatial pooling.Incorporating spatial pooling improves the translational invariance and thusthe robustness of the detection process. We then directly optimise the partialarea under the ROC curve (\pAUC) measure, which concentrates detectionperformance in the range of most practical importance. The combination of thesefactors leads to a pedestrian detector which outperforms all competitors on allof the standard benchmark datasets. We advance state-of-the-art results bylowering the average miss rate from $13\%$ to $11\%$ on the INRIA benchmark,$41\%$ to $37\%$ on the ETH benchmark, $51\%$ to $42\%$ on the TUD-Brusselsbenchmark and $36\%$ to $29\%$ on the Caltech-USA benchmark.
arxiv-7500-137 | Online Submodular Maximization under a Matroid Constraint with Application to Learning Assignments | http://arxiv.org/pdf/1407.1082v1.pdf | author:Daniel Golovin, Andreas Krause, Matthew Streeter category:cs.LG published:2014-07-03 summary:Which ads should we display in sponsored search in order to maximize ourrevenue? How should we dynamically rank information sources to maximize thevalue of the ranking? These applications exhibit strong diminishing returns:Redundancy decreases the marginal utility of each ad or information source. Weshow that these and other problems can be formalized as repeatedly selecting anassignment of items to positions to maximize a sequence of monotone submodularfunctions that arrive one by one. We present an efficient algorithm for thisgeneral problem and analyze it in the no-regret model. Our algorithm possessesstrong theoretical guarantees, such as a performance ratio that converges tothe optimal constant of 1 - 1/e. We empirically evaluate our algorithm on tworeal-world online optimization problems on the web: ad allocation withsubmodular utilities, and dynamically ranking blogs to detect informationcascades. Finally, we present a second algorithm that handles the more generalcase in which the feasible sets are given by a matroid constraint, while stillmaintaining a 1 - 1/e asymptotic performance ratio.
arxiv-7500-138 | Multiple Moving Object Recognitions in video based on Log Gabor-PCA Approach | http://arxiv.org/pdf/1407.0935v1.pdf | author:M. T Gopalakrishna, M. Ravishankar, D. R Rameshbabu category:cs.CV 68T45 I.4.8 published:2014-07-03 summary:Object recognition in the video sequence or images is one of the sub-field ofcomputer vision. Moving object recognition from a video sequence is anappealing topic with applications in various areas such as airport safety,intrusion surveillance, video monitoring, intelligent highway, etc. Movingobject recognition is the most challenging task in intelligent videosurveillance system. In this regard, many techniques have been proposed basedon different methods. Despite of its importance, moving object recognition incomplex environments is still far from being completely solved for lowresolution videos, foggy videos, and also dim video sequences. All in all,these make it necessary to develop exceedingly robust techniques. This paperintroduces multiple moving object recognition in the video sequence based onLoG Gabor-PCA approach and Angle based distance Similarity measures techniquesused to recognize the object as a human, vehicle etc. Number of experiments areconducted for indoor and outdoor video sequences of standard datasets and alsoour own collection of video sequences comprising of partial night vision videosequences. Experimental results show that our proposed approach achieves anexcellent recognition rate. Results obtained are satisfactory and competent.
arxiv-7500-139 | Solving QVIs for Image Restoration with Adaptive Constraint Sets | http://arxiv.org/pdf/1407.0921v1.pdf | author:Frank Lenzen, Jan Lellmann, Florian Becker, Christoph Schnörr category:math.OC cs.CV math.NA published:2014-07-03 summary:We consider a class of quasi-variational inequalities (QVIs) for adaptiveimage restoration, where the adaptivity is described via solution-dependentconstraint sets. In previous work we studied both theoretical and numericalissues. While we were able to show the existence of solutions for a relativelybroad class of problems, we encountered problems concerning uniqueness of thesolution as well as convergence of existing algorithms for solving QVIs. Inparticular, it seemed that with increasing image size the growing conditionnumber of the involved differential operator poses severe problems. In thepresent paper we prove uniqueness for a larger class of problems and inparticular independent of the image size. Moreover, we provide a numericalalgorithm with proved convergence. Experimental results support our theoreticalfindings.
arxiv-7500-140 | Enhanced EZW Technique for Compression of Image by Setting Detail Retaining Pass Number | http://arxiv.org/pdf/1407.3673v1.pdf | author:Isha Tyagi, Ashish Nautiyal, Vishwanath Bijalwan, Meenu Balodhi category:cs.CV published:2014-07-03 summary:For keeping the data secured and maintained, compression is most essentialaspect. For which efficiency is the important part to be researchedcontinuously until the satisfactory result is achieved. the optimized ratio ofdata is necessary for compression and embedded transmission. In this paper themain objective is to improve the execution time evolved in EZW compression.
arxiv-7500-141 | Reducing Offline Evaluation Bias in Recommendation Systems | http://arxiv.org/pdf/1407.0822v1.pdf | author:Arnaud De Myttenaere, Bénédicte Le Grand, Boris Golden, Fabrice Rossi category:cs.IR cs.LG stat.ML published:2014-07-03 summary:Recommendation systems have been integrated into the majority of large onlinesystems. They tailor those systems to individual users by filtering and rankinginformation according to user profiles. This adaptation process influences theway users interact with the system and, as a consequence, increases thedifficulty of evaluating a recommendation algorithm with historical data (viaoffline evaluation). This paper analyses this evaluation bias and proposes asimple item weighting solution that reduces its impact. The efficiency of theproposed solution is evaluated on real world data extracted from Viadeoprofessional social network.
arxiv-7500-142 | A Data-Driven Approach for Tag Refinement and Localization in Web Videos | http://arxiv.org/pdf/1407.0623v3.pdf | author:Lamberto Ballan, Marco Bertini, Giuseppe Serra, Alberto Del Bimbo category:cs.CV cs.IR cs.MM published:2014-07-02 summary:Tagging of visual content is becoming more and more widespread as web-basedservices and social networks have popularized tagging functionalities amongtheir users. These user-generated tags are used to ease browsing andexploration of media collections, e.g. using tag clouds, or to retrievemultimedia content. However, not all media are equally tagged by users. Usingthe current systems is easy to tag a single photo, and even tagging a part of aphoto, like a face, has become common in sites like Flickr and Facebook. On theother hand, tagging a video sequence is more complicated and time consuming, sothat users just tag the overall content of a video. In this paper we present amethod for automatic video annotation that increases the number of tagsoriginally provided by users, and localizes them temporally, associating tagsto keyframes. Our approach exploits collective knowledge embedded inuser-generated tags and web sources, and visual similarity of keyframes andimages uploaded to social sites like YouTube and Flickr, as well as web sourceslike Google and Bing. Given a keyframe, our method is able to select on the flyfrom these visual sources the training exemplars that should be the mostrelevant for this test sample, and proceeds to transfer labels across similarimages. Compared to existing video tagging approaches that require trainingclassifiers for each tag, our system has few parameters, is easy to implementand can deal with an open vocabulary scenario. We demonstrate the approach ontag refinement and localization on DUT-WEBV, a large dataset of web videos, andshow state-of-the-art results.
arxiv-7500-143 | Fast Algorithm for Low-rank matrix recovery in Poisson noise | http://arxiv.org/pdf/1407.0726v2.pdf | author:Yang Cao, Yao Xie category:stat.ML cs.LG math.ST stat.TH published:2014-07-02 summary:This paper describes a fast algorithm for recovering low-rank matrices fromtheir linear measurements contaminated with Poisson noise: the Poisson noiseMaximum Likelihood Singular Value thresholding (PMLSV) algorithm. We propose aconvex optimization formulation with a cost function consisting of the sum of alikelihood function and a regularization function which the nuclear norm of thematrix. Instead of solving the optimization problem directly by semi-definiteprogram (SDP), we derive an iterative singular value thresholding algorithm byexpanding the likelihood function. We demonstrate the good performance of theproposed algorithm on recovery of solar flare images with Poisson noise: thealgorithm is more efficient than solving SDP using the interior-point algorithmand it generates a good approximate solution compared to that solved from SDP.
arxiv-7500-144 | Info-Greedy sequential adaptive compressed sensing | http://arxiv.org/pdf/1407.0731v4.pdf | author:Gabor Braun, Sebastian Pokutta, Yao Xie category:cs.IT math.IT math.ST stat.ML stat.TH published:2014-07-02 summary:We present an information-theoretic framework for sequential adaptivecompressed sensing, Info-Greedy Sensing, where measurements are chosen tomaximize the extracted information conditioned on the previous measurements. Weshow that the widely used bisection approach is Info-Greedy for a family of$k$-sparse signals by connecting compressed sensing and blackbox complexity ofsequential query algorithms, and present Info-Greedy algorithms for Gaussianand Gaussian Mixture Model (GMM) signals, as well as ways to design sparseInfo-Greedy measurements. Numerical examples demonstrate the good performanceof the proposed algorithms using simulated and real data: Info-Greedy Sensingshows significant improvement over random projection for signals with sparseand low-rank covariance matrices, and adaptivity brings robustness when thereis a mismatch between the assumed and the true distributions.
arxiv-7500-145 | Deep Poselets for Human Detection | http://arxiv.org/pdf/1407.0717v1.pdf | author:Lubomir Bourdev, Fei Yang, Rob Fergus category:cs.CV published:2014-07-02 summary:We address the problem of detecting people in natural scenes using a partapproach based on poselets. We propose a bootstrapping method that allows us tocollect millions of weakly labeled examples for each poselet type. We use theseexamples to train a Convolutional Neural Net to discriminate different poselettypes and separate them from the background class. We then use the trained CNNas a way to represent poselet patches with a Pose Discriminative Feature (PDF)vector -- a compact 256-dimensional feature vector that is effective atdiscriminating pose from appearance. We train the poselet model on top of PDFfeatures and combine them with object-level CNNs for detection and bounding boxprediction. The resulting model leads to state-of-the-art performance for humandetection on the PASCAL datasets.
arxiv-7500-146 | Continuous On-line Evolution of Agent Behaviours with Cartesian Genetic Programming | http://arxiv.org/pdf/1407.0698v1.pdf | author:Davide Nunes, Luis Antunes category:cs.NE cs.MA published:2014-07-02 summary:Evolutionary Computation has been successfully used to synthesise controllersfor embodied agents and multi-agent systems in general. Notwithstanding this,continuous on-line adaptation by the means of evolutionary algorithms is stillunder-explored, especially outside the evolutionary robotics domain. In thispaper, we present an on-line evolutionary programming algorithm that searchesin the agent design space for the appropriate behavioural policies to cope withthe underlying environment. We discuss the current problems of continuous agentadaptation, present our on-line evolution testbed for evolutionary simulation.
arxiv-7500-147 | Nonparametric Hierarchical Clustering of Functional Data | http://arxiv.org/pdf/1407.0612v1.pdf | author:Marc Boullé, Romain Guigourès, Fabrice Rossi category:stat.ML cs.LG published:2014-07-02 summary:In this paper, we deal with the problem of curves clustering. We propose anonparametric method which partitions the curves into clusters and discretizesthe dimensions of the curve points into intervals. The cross-product of thesepartitions forms a data-grid which is obtained using a Bayesian model selectionapproach while making no assumptions regarding the curves. Finally, apost-processing technique, aiming at reducing the number of clusters in orderto improve the interpretability of the clustering, is proposed. It consists inoptimally merging the clusters step by step, which corresponds to anagglomerative hierarchical classification whose dissimilarity measure is thevariation of the criterion. Interestingly this measure is none other than thesum of the Kullback-Leibler divergences between clusters distributions beforeand after the merges. The practical interest of the approach for functionaldata exploratory analysis is presented and compared with an alternativeapproach on an artificial and a real world data set.
arxiv-7500-148 | How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need? | http://arxiv.org/pdf/1407.0611v1.pdf | author:Fabrice Rossi category:stat.ML cs.LG cs.NE published:2014-07-02 summary:In numerous applicative contexts, data are too rich and too complex to berepresented by numerical vectors. A general approach to extend machine learningand data mining techniques to such data is to really on a dissimilarity or on akernel that measures how different or similar two objects are. This approachhas been used to define several variants of the Self Organizing Map (SOM). Thispaper reviews those variants in using a common set of notations in order tooutline differences and similarities between them. It discusses the advantagesand drawbacks of the variants, as well as the actual relevance of thedissimilarity/kernel SOM for practical applications.
arxiv-7500-149 | Higher-Order Quantum-Inspired Genetic Algorithms | http://arxiv.org/pdf/1407.0977v1.pdf | author:Robert Nowotniak, Jacek Kucharski category:cs.NE quant-ph published:2014-07-02 summary:This paper presents a theory and an empirical evaluation of Higher-OrderQuantum-Inspired Genetic Algorithms. Fundamental notions of the theory havebeen introduced, and a novel Order-2 Quantum-Inspired Genetic Algorithm (QIGA2)has been presented. Contrary to all QIGA algorithms which represent quantumgenes as independent qubits, in higher-order QIGAs quantum registers are usedto represent genes strings which allows modelling of genes relations usingquantum phenomena. Performance comparison has been conducted on a benchmark of20 deceptive combinatorial optimization problems. It has been presented thatusing higher quantum orders is beneficial for genetic algorithm efficiency, andthe new QIGA2 algorithm outperforms the old QIGA algorithm which was tuned inhighly compute intensive metaoptimization process.
arxiv-7500-150 | Systematic Derivation of Behaviour Characterisations in Evolutionary Robotics | http://arxiv.org/pdf/1407.0577v1.pdf | author:Jorge Gomes, Pedro Mariano, Anders Lyhne Christensen category:cs.NE cs.MA cs.RO published:2014-07-02 summary:Evolutionary techniques driven by behavioural diversity, such as noveltysearch, have shown significant potential in evolutionary robotics. Thesetechniques rely on priorly specified behaviour characterisations to estimatethe similarity between individuals. Characterisations are typically defined inan ad hoc manner based on the experimenter's intuition and knowledge about thetask. Alternatively, generic characterisations based on the sensor-effectorvalues of the agents are used. In this paper, we propose a novel approach thatallows for systematic derivation of behaviour characterisations forevolutionary robotics, based on a formal description of the agents and theirenvironment. Systematically derived behaviour characterisations (SDBCs) gobeyond generic characterisations in that they can contain task-specificfeatures related to the internal state of the agents, environmental features,and relations between them. We evaluate SDBCs with novelty search in threesimulated collective robotics tasks. Our results show that SDBCs yield aperformance comparable to the task-specific characterisations, in terms of bothsolution quality and behaviour space exploration.
arxiv-7500-151 | Novelty Search in Competitive Coevolution | http://arxiv.org/pdf/1407.0576v1.pdf | author:Jorge Gomes, Pedro Mariano, Anders Lyhne Christensen category:cs.NE cs.MA published:2014-07-02 summary:One of the main motivations for the use of competitive coevolution systems istheir ability to capitalise on arms races between competing species to evolveincreasingly sophisticated solutions. Such arms races can, however, be hard tosustain, and it has been shown that the competing species often convergeprematurely to certain classes of behaviours. In this paper, we investigate ifand how novelty search, an evolutionary technique driven by behaviouralnovelty, can overcome convergence in coevolution. We propose three methods forapplying novelty search to coevolutionary systems with two species: (i) scoreboth populations according to behavioural novelty; (ii) score one populationaccording to novelty, and the other according to fitness; and (iii) score bothpopulations with a combination of novelty and fitness. We evaluate the methodsin a predator-prey pursuit task. Our results show that novelty-based approachescan evolve a significantly more diverse set of solutions, when compared totraditional fitness-based coevolution.
arxiv-7500-152 | Classification-based Approximate Policy Iteration: Experiments and Extended Discussions | http://arxiv.org/pdf/1407.0449v1.pdf | author:Amir-massoud Farahmand, Doina Precup, André M. S. Barreto, Mohammad Ghavamzadeh category:cs.LG cs.SY math.OC stat.ML I.2.6; I.2.8 published:2014-07-02 summary:Tackling large approximate dynamic programming or reinforcement learningproblems requires methods that can exploit regularities, or intrinsicstructure, of the problem in hand. Most current methods are geared towardsexploiting the regularities of either the value function or the policy. Weintroduce a general classification-based approximate policy iteration (CAPI)framework, which encompasses a large class of algorithms that can exploitregularities of both the value function and the policy space, depending on whatis advantageous. This framework has two main components: a generic valuefunction estimator and a classifier that learns a policy based on the estimatedvalue function. We establish theoretical guarantees for the sample complexityof CAPI-style algorithms, which allow the policy evaluation step to beperformed by a wide variety of algorithms (including temporal-difference-stylemethods), and can handle nonparametric representations of policies. Our boundson the estimation error of the performance loss are tighter than existingresults. We also illustrate this approach empirically on several problems,including a large HIV control task.
arxiv-7500-153 | Support Consistency of Direct Sparse-Change Learning in Markov Networks | http://arxiv.org/pdf/1407.0581v10.pdf | author:Song Liu, Taiji Suzuki, Raissa Relator, Jun Sese, Masashi Sugiyama, Kenji Fukumizu category:stat.ML published:2014-07-02 summary:We study the problem of learning sparse structure changes between two Markovnetworks $P$ and $Q$. Rather than fitting two Markov networks separately to twosets of data and figuring out their differences, a recent work proposed tolearn changes \emph{directly} via estimating the ratio between two Markovnetwork models. In this paper, we give sufficient conditions for\emph{successful change detection} with respect to the sample size $n_p, n_q$,the dimension of data $m$, and the number of changed edges $d$. When using anunbounded density ratio model we prove that the true sparse changes can beconsistently identified for $n_p = \Omega(d^2 \log \frac{m^2+m}{2})$ and $n_q =\Omega({n_p^2})$, with an exponentially decaying upper-bound on learning error.Such sample complexity can be improved to $\min(n_p, n_q) = \Omega(d^2 \log\frac{m^2+m}{2})$ when the boundedness of the density ratio model is assumed.Our theoretical guarantee can be applied to a wide range of discrete/continuousMarkov networks.
arxiv-7500-154 | Cortical spatio-temporal dimensionality reduction for visual grouping | http://arxiv.org/pdf/1407.0733v2.pdf | author:Giacomo Cocci, Davide Barbieri, Giovanna Citti, Alessandro Sarti category:cs.CV cs.NE q-bio.NC stat.ML published:2014-07-02 summary:The visual systems of many mammals, including humans, is able to integratethe geometric information of visual stimuli and to perform cognitive tasksalready at the first stages of the cortical processing. This is thought to bethe result of a combination of mechanisms, which include feature extraction atsingle cell level and geometric processing by means of cells connectivity. Wepresent a geometric model of such connectivities in the space of detectedfeatures associated to spatio-temporal visual stimuli, and show how they can beused to obtain low-level object segmentation. The main idea is that of defininga spectral clustering procedure with anisotropic affinities over datasetsconsisting of embeddings of the visual stimuli into higher dimensional spaces.Neural plausibility of the proposed arguments will be discussed.
arxiv-7500-155 | Geometric Tight Frame based Stylometry for Art Authentication of van Gogh Paintings | http://arxiv.org/pdf/1407.0439v3.pdf | author:Haixia Liu, Raymond H. Chan, Yuan Yao category:cs.LG cs.CV published:2014-07-02 summary:This paper is about authenticating genuine van Gogh paintings from forgeries.The authentication process depends on two key steps: feature extraction andoutlier detection. In this paper, a geometric tight frame and some simplestatistics of the tight frame coefficients are used to extract features fromthe paintings. Then a forward stage-wise rank boosting is used to select asmall set of features for more accurate classification so that van Goghpaintings are highly concentrated towards some center point while forgeries arespread out as outliers. Numerical results show that our method can achieve86.08% classification accuracy under the leave-one-out cross-validationprocedure. Our method also identifies five features that are much morepredominant than other features. Using just these five features forclassification, our method can give 88.61% classification accuracy which is thehighest so far reported in literature. Evaluation of the five features is alsoperformed on two hundred datasets generated by bootstrap sampling withreplacement. The median and the mean are 88.61% and 87.77% respectively. Ourresults show that a small set of statistics of the tight frame coefficientsalong certain orientations can serve as discriminative features for van Goghpaintings. It is more important to look at the tail distributions of suchdirectional coefficients than mean values and standard deviations. It reflectsa highly consistent style in van Gogh's brushstroke movements, where manyforgeries demonstrate a more diverse spread in these features.
arxiv-7500-156 | A Bayes consistent 1-NN classifier | http://arxiv.org/pdf/1407.0208v2.pdf | author:Aryeh Kontorovich, Roi Weiss category:cs.LG stat.ML published:2014-07-01 summary:We show that a simple modification of the 1-nearest neighbor classifieryields a strongly Bayes consistent learner. Prior to this work, the onlystrongly Bayes consistent proximity-based method was the k-nearest neighborclassifier, for k growing appropriately with sample size. We will argue that amargin-regularized 1-NN enjoys considerable statistical and algorithmicadvantages over the k-NN classifier. These include user-friendly finite-sampleerror bounds, as well as time- and memory-efficient learning and test-pointevaluation algorithms with a principled speed-accuracy tradeoff. Encouragingempirical results are reported.
arxiv-7500-157 | Randomized Block Coordinate Descent for Online and Stochastic Optimization | http://arxiv.org/pdf/1407.0107v3.pdf | author:Huahua Wang, Arindam Banerjee category:cs.LG published:2014-07-01 summary:Two types of low cost-per-iteration gradient descent methods have beenextensively studied in parallel. One is online or stochastic gradient descent(OGD/SGD), and the other is randomzied coordinate descent (RBCD). In thispaper, we combine the two types of methods together and propose onlinerandomized block coordinate descent (ORBCD). At each iteration, ORBCD onlycomputes the partial gradient of one block coordinate of one mini-batchsamples. ORBCD is well suited for the composite minimization problem where onefunction is the average of the losses of a large number of samples and theother is a simple regularizer defined on high dimensional variables. We showthat the iteration complexity of ORBCD has the same order as OGD or SGD. Forstrongly convex functions, by reducing the variance of stochastic gradients, weshow that ORBCD can converge at a geometric rate in expectation, matching theconvergence rate of SGD with variance reduction and RBCD.
arxiv-7500-158 | Significant Subgraph Mining with Multiple Testing Correction | http://arxiv.org/pdf/1407.0316v3.pdf | author:Mahito Sugiyama, Felipe Llinares López, Niklas Kasenburg, Karsten M. Borgwardt category:stat.ME cs.LG stat.ML published:2014-07-01 summary:The problem of finding itemsets that are statistically significantly enrichedin a class of transactions is complicated by the need to correct for multiplehypothesis testing. Pruning untestable hypotheses was recently proposed as astrategy for this task of significant itemset mining. It was shown to lead togreater statistical power, the discovery of more truly significant itemsets,than the standard Bonferroni correction on real-world datasets. An openquestion, however, is whether this strategy of excluding untestable hypothesesalso leads to greater statistical power in subgraph mining, in which the numberof hypotheses is much larger than in itemset mining. Here we answer thisquestion by an empirical investigation on eight popular graph benchmarkdatasets. We propose a new efficient search strategy, which always returns thesame solution as the state-of-the-art approach and is approximately two ordersof magnitude faster. Moreover, we exploit the dependence between subgraphs byconsidering the effective number of tests and thereby further increase thestatistical power.
arxiv-7500-159 | SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives | http://arxiv.org/pdf/1407.0202v3.pdf | author:Aaron Defazio, Francis Bach, Simon Lacoste-Julien category:cs.LG math.OC stat.ML published:2014-07-01 summary:In this work we introduce a new optimisation method called SAGA in the spiritof SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradientalgorithms with fast linear convergence rates. SAGA improves on the theorybehind SAG and SVRG, with better theoretical convergence rates, and has supportfor composite objectives where a proximal operator is used on the regulariser.Unlike SDCA, SAGA supports non-strongly convex problems directly, and isadaptive to any inherent strong convexity of the problem. We give experimentalresults showing the effectiveness of our method.
arxiv-7500-160 | Mathematical Language Processing Project | http://arxiv.org/pdf/1407.0167v1.pdf | author:Robert Pagael, Moritz Schubotz category:cs.DL cs.CL cs.IR published:2014-07-01 summary:In natural language, words and phrases themselves imply the semantics. Incontrast, the meaning of identifiers in mathematical formulae is undefined.Thus scientists must study the context to decode the meaning. The MathematicalLanguage Processing (MLP) project aims to support that process. In this paper,we compare two approaches to discover identifier-definition tuples. At first weuse a simple pattern matching approach. Second, we present the MLP approachthat uses part-of-speech tag based distances as well as sentence positions tocalculate identifier-definition probabilities. The evaluation of ourprototypical system, applied on the Wikipedia text corpus, shows that ourapproach augments the user experience substantially. While hovering theidentifiers in the formula, tool-tips with the most probable definitions occur.Tests with random samples show that the displayed definitions provide a goodmatch with the actual meaning of the identifiers.
arxiv-7500-161 | Mind the Nuisance: Gaussian Process Classification using Privileged Noise | http://arxiv.org/pdf/1407.0179v1.pdf | author:Daniel Hernández-Lobato, Viktoriia Sharmanska, Kristian Kersting, Christoph H. Lampert, Novi Quadrianto category:stat.ML cs.LG published:2014-07-01 summary:The learning with privileged information setting has recently attracted a lotof attention within the machine learning community, as it allows theintegration of additional knowledge into the training process of a classifier,even when this comes in the form of a data modality that is not available attest time. Here, we show that privileged information can naturally be treatedas noise in the latent function of a Gaussian Process classifier (GPC). Thatis, in contrast to the standard GPC setting, the latent function is not just anuisance but a feature: it becomes a natural measure of confidence about thetraining data by modulating the slope of the GPC sigmoid likelihood function.Extensive experiments on public datasets show that the proposed GPC methodusing privileged noise, called GPC+, improves over a standard GPC withoutprivileged knowledge, and also over the current state-of-the-art SVM-basedmethod, SVM+. Moreover, we show that advanced neural networks and deep learningmethods can be compressed as privileged information.
arxiv-7500-162 | Imaging with Kantorovich-Rubinstein discrepancy | http://arxiv.org/pdf/1407.0221v1.pdf | author:Jan Lellmann, Dirk A. Lorenz, Carola Schönlieb, Tuomo Valkonen category:cs.CV math.NA published:2014-07-01 summary:We propose the use of the Kantorovich-Rubinstein norm from optimal transportin imaging problems. In particular, we discuss a variational regularisationmodel endowed with a Kantorovich-Rubinstein discrepancy term and totalvariation regularization in the context of image denoising and cartoon-texturedecomposition. We point out connections of this approach to several otherrecently proposed methods such as total generalized variation and normscapturing oscillating patterns. We also show that the respective optimizationproblem can be turned into a convex-concave saddle point problem with simpleconstraints and hence, can be solved by standard tools. Numerical examplesexhibit interesting features and favourable performance for denoising andcartoon-texture decomposition.
arxiv-7500-163 | Identifying Outliers in Large Matrices via Randomized Adaptive Compressive Sampling | http://arxiv.org/pdf/1407.0312v3.pdf | author:Xingguo Li, Jarvis Haupt category:cs.IT cs.LG math.IT stat.ML published:2014-07-01 summary:This paper examines the problem of locating outlier columns in a large,otherwise low-rank, matrix. We propose a simple two-step adaptive sensing andinference approach and establish theoretical guarantees for its performance;our results show that accurate outlier identification is achievable using veryfew linear summaries of the original data matrix -- as few as the squared rankof the low-rank component plus the number of outliers, times constant andlogarithmic factors. We demonstrate the performance of our approachexperimentally in two stylized applications, one motivated by robustcollaborative filtering tasks, and the other by saliency map estimation tasksarising in computer vision and automated surveillance, and also investigateextensions to settings where the data are noisy, or possibly incomplete.
arxiv-7500-164 | Supervised learning in Spiking Neural Networks with Limited Precision: SNN/LP | http://arxiv.org/pdf/1407.0265v1.pdf | author:Evangelos Stromatias, John Marsland category:cs.NE published:2014-07-01 summary:A new supervised learning algorithm, SNN/LP, is proposed for Spiking NeuralNetworks. This novel algorithm uses limited precision for both synaptic weightsand synaptic delays; 3 bits in each case. Also a genetic algorithm is used forthe supervised training. The results are comparable or better than previouslypublished work. The results are applicable to the realization of large scalehardware neural networks. One of the trained networks is implemented inprogrammable hardware.
arxiv-7500-165 | A New Path to Construct Parametric Orientation Field: Sparse FOMFE Model and Compressed Sparse FOMFE Model | http://arxiv.org/pdf/1407.0342v1.pdf | author:Jinwei Xu, Jiankun Hu, Xiuping Jia category:cs.CV cs.CR published:2014-07-01 summary:Orientation field, representing the fingerprint ridge structure direction,plays a crucial role in fingerprint-related image processing tasks. Orientationfield is able to be constructed by either non-parametric or parametric methods.In this paper, the advantages and disadvantages regarding to the existingnon-parametric and parametric approaches are briefly summarized. With thefurther investigation for constructing the orientation field by parametrictechnique, two new models - sparse FOMFE model and compressed sparse FOMFEmodel are introduced, based on the rapidly developing signal sparserepresentation and compressed sensing theories. The experiments on high-qualityfingerprint image dataset (plain and rolled print) and poor-quality fingerprintimage dataset (latent print) demonstrate their feasibilities to construct theorientation field in a sparse or even compressed sparse mode. The comparisonsamong the state-of-art orientation field modeling approaches show that theproposed two models have the potential availability in big data-orientedfingerprint indexing tasks.
arxiv-7500-166 | A Dynamic Simulation-Optimization Model for Adaptive Management of Urban Water Distribution System Contamination Threats | http://arxiv.org/pdf/1407.0424v1.pdf | author:Amin Rasekh, Kelly Brumbelow category:cs.OH cs.NE published:2014-07-01 summary:Urban water distribution systems hold a critical and strategic position inpreserving public health and industrial growth. Despite the ubiquity of theseurban systems, aging infrastructure, and increased risk of terrorism, decisionsupport models for a timely and adaptive contamination emergency response stillremain at an undeveloped stage. Emergency response is characterized as aprogressive, interactive, and adaptive process that involves parallelactivities of processing streaming information and executing response actions.This study develops a dynamic decision support model that adaptively simulatesthe time-varying emergency environment and tracks changing best healthprotection response measures at every stage of an emergency in real-time.Feedback mechanisms between the contaminated network, emergency managers, andconsumers are incorporated in a dynamic simulation model to capturetime-varying characteristics of an emergency environment. Anevolutionary-computation-based dynamic optimization model is developed toadaptively identify time-dependant optimal health protection measures during anemergency. This dynamic simulation-optimization model treats perceivedcontaminant source attributes as time-varying parameters to account forperceived contamination source updates as more data stream in over time.Performance of the developed dynamic decision support model is analyzed anddemonstrated using a mid-size virtual city that resembles the dynamics andcomplexity of real-world urban systems. This adaptive emergency responseoptimization model is intended to be a major component of an all-inclusivecyberinfrastructure for efficient contamination threat management, which iscurrently under development.
arxiv-7500-167 | DC approximation approaches for sparse optimization | http://arxiv.org/pdf/1407.0286v2.pdf | author:Hoai An Le Thi, Tao Pham Dinh, Hoai Minh Le, Xuan Thanh Vo category:cs.NA cs.LG stat.ML 90C26, 90C90 published:2014-07-01 summary:Sparse optimization refers to an optimization problem involving the zero-normin objective or constraints. In this paper, nonconvex approximation approachesfor sparse optimization have been studied with a unifying point of view in DC(Difference of Convex functions) programming framework. Considering a common DCapproximation of the zero-norm including all standard sparse inducing penaltyfunctions, we studied the consistency between global minimums (resp. localminimums) of approximate and original problems. We showed that, in severalcases, some global minimizers (resp. local minimizers) of the approximateproblem are also those of the original problem. Using exact penalty techniquesin DC programming, we proved stronger results for some particularapproximations, namely, the approximate problem, with suitable parameters, isequivalent to the original problem. The efficiency of several sparse inducingpenalty functions have been fully analyzed. Four DCA (DC Algorithm) schemeswere developed that cover all standard algorithms in nonconvex sparseapproximation approaches as special versions. They can be viewed as, an $\ell_{1}$-perturbed algorithm / reweighted-$\ell _{1}$ algorithm / reweighted-$\ell_{1}$ algorithm. We offer a unifying nonconvex approximation approach, withsolid theoretical tools as well as efficient algorithms based on DC programmingand DCA, to tackle the zero-norm and sparse optimization. As an application, weimplemented our methods for the feature selection in SVM (Support VectorMachine) problem and performed empirical comparative numerical experiments onthe proposed algorithms with various approximation functions.
arxiv-7500-168 | Building DNN Acoustic Models for Large Vocabulary Speech Recognition | http://arxiv.org/pdf/1406.7806v2.pdf | author:Andrew L. Maas, Peng Qi, Ziang Xie, Awni Y. Hannun, Christopher T. Lengerich, Daniel Jurafsky, Andrew Y. Ng category:cs.CL cs.LG cs.NE stat.ML published:2014-06-30 summary:Deep neural networks (DNNs) are now a central component of nearly allstate-of-the-art speech recognition systems. Building neural network acousticmodels requires several design decisions including network architecture, size,and training loss function. This paper offers an empirical investigation onwhich aspects of DNN acoustic model design are most important for speechrecognition system performance. We report DNN classifier performance and finalspeech recognizer word error rates, and compare DNNs using several metrics toquantify factors influencing differences in task performance. Our first set ofexperiments use the standard Switchboard benchmark corpus, which containsapproximately 300 hours of conversational telephone speech. We compare standardDNNs to convolutional networks, and present the first experiments usinglocally-connected, untied neural networks for acoustic modeling. Weadditionally build systems on a corpus of 2,100 hours of training data bycombining the Switchboard and Fisher corpora. This larger corpus allows us tomore thoroughly examine performance of large DNN models -- with up to ten timesmore parameters than those typically used in speech recognition systems. Ourresults suggest that a relatively simple DNN architecture and optimizationtechnique produces strong results. These findings, along with previous work,help establish a set of best practices for building DNN hybrid speechrecognition systems with maximum likelihood training. Our experiments in DNNoptimization additionally serve as a case study for training DNNs withdiscriminative loss functions for speech tasks, as well as DNN classifiers moregenerally.
arxiv-7500-169 | Direct Density-Derivative Estimation and Its Application in KL-Divergence Approximation | http://arxiv.org/pdf/1406.7638v1.pdf | author:Hiroaki Sasaki, Yung-Kyun Noh, Masashi Sugiyama category:stat.ML published:2014-06-30 summary:Estimation of density derivatives is a versatile tool in statistical dataanalysis. A naive approach is to first estimate the density and then computeits derivative. However, such a two-step approach does not work well because agood density estimator does not necessarily mean a good density-derivativeestimator. In this paper, we give a direct method to approximate the densityderivative without estimating the density itself. Our proposed estimator allowsanalytic and computationally efficient approximation of multi-dimensionalhigh-order density derivatives, with the ability that all hyper-parameters canbe chosen objectively by cross-validation. We further show that the proposeddensity-derivative estimator is useful in improving the accuracy ofnon-parametric KL-divergence estimation via metric learning. The practicalsuperiority of the proposed method is experimentally demonstrated in changedetection and feature selection.
arxiv-7500-170 | Dispersion and Line Formation in Artificial Swarm Intelligence | http://arxiv.org/pdf/1407.0014v1.pdf | author:Donghwa Jeong, Kiju Lee category:cs.NE published:2014-06-30 summary:One of the major motifs in collective or swarm intelligence is that, eventhough individuals follow simple rules, the resulting global behavior can becomplex and intelligent. In artificial swarm systems, such as swarm robots, thegoal is to use systems that are as simple and cheap as possible, deploy many ofthem, and coordinate them to conduct complex tasks that each individual cannotaccomplish. Shape formation in artificial intelligence systems is usuallyrequired for specific task-oriented performance, including 1) forming sensinggrids, 2) exploring and mapping in space, underwater, or hazardousenvironments, and 3) forming a barricade for surveillance or protecting an areaor a person. This paper presents a dynamic model of an artificial swarm systembased on a virtual spring damper model and algorithms for dispersion without aleader and line formation with an interim leader using only the distanceestimation among the neighbors.
arxiv-7500-171 | Navigating Robot Swarms Using Collective Intelligence Learned from Golden Shiner Fish | http://arxiv.org/pdf/1407.0008v1.pdf | author:Grace Gao category:cs.NE published:2014-06-30 summary:Navigating networked robot swarms often requires knowing where to go, sensingthe environment, and path-planning based on the destination and barriers in theenvironment. Such a process is computationally intensive. Moreover, as thenetwork scales up, the computational load increases quadratically, or evenexponentially. Unlike these man-made systems, most biological systems scalelinearly in complexity. Furthermore, the scale of a biological swarm can evenenable collective intelligence. One example comes from observations of goldenshiner fish. Golden shiners naturally prefer darkness and school together. Eachindividual golden shiner does not know where the darkness is. Neither does itsense the light gradients in the environment. However, by moving together as aschool, they always end up in the shady area. We apply such collectiveintelligence learned from golden shiner fish to navigating robot swarms. Eachindividual robot's dynamic is based on the gold shiners' movement strategy---arandom walk with its speed modulated by the light intensity and its directionaffected by its neighbors. The theoretical analysis and simulation results showthat our method 1) promises to navigate a robot swarm with little situationalknowledge, 2) simplifies control and decision-making for each individual robot,3) requires minimal or even no information exchange within the swarm, and 4) ishighly distributed, adaptive, and robust.
arxiv-7500-172 | Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian Process Hyper-Parameters | http://arxiv.org/pdf/1406.7758v1.pdf | author:Ziyu Wang, Nando de Freitas category:stat.ML cs.LG published:2014-06-30 summary:Bayesian optimisation has gained great popularity as a tool for optimisingthe parameters of machine learning algorithms and models. Somewhat ironically,setting up the hyper-parameters of Bayesian optimisation methods is notoriouslyhard. While reasonable practical solutions have been advanced, they can oftenfail to find the best optima. Surprisingly, there is little theoreticalanalysis of this crucial problem in the literature. To address this, we derivea cumulative regret bound for Bayesian optimisation with Gaussian processes andunknown kernel hyper-parameters in the stochastic setting. The bound, whichapplies to the expected improvement acquisition function and sub-Gaussianobservation noise, provides us with guidelines on how to design hyper-parameterestimation methods. A simple simulation demonstrates the importance offollowing these guidelines.
arxiv-7500-173 | Subjective and Objective Quality Assessment of Image: A Survey | http://arxiv.org/pdf/1406.7799v1.pdf | author:Pedram Mohammadi, Abbas Ebrahimi-Moghadam, Shahram Shirani category:cs.MM cs.CV published:2014-06-30 summary:With the increasing demand for image-based applications, the efficient andreliable evaluation of image quality has increased in importance. Measuring theimage quality is of fundamental importance for numerous image processingapplications, where the goal of image quality assessment (IQA) methods is toautomatically evaluate the quality of images in agreement with human qualityjudgments. Numerous IQA methods have been proposed over the past years tofulfill this goal. In this paper, a survey of the quality assessment methodsfor conventional image signals, as well as the newly emerged ones, whichincludes the high dynamic range (HDR) and 3-D images, is presented. Acomprehensive explanation of the subjective and objective IQA and theirclassification is provided. Six widely used subjective quality datasets, andperformance measures are reviewed. Emphasis is given to the full-referenceimage quality assessment (FR-IQA) methods, and 9 often-used quality measures(including mean squared error (MSE), structural similarity index (SSIM),multi-scale structural similarity index (MS-SSIM), visual information fidelity(VIF), most apparent distortion (MAD), feature similarity measure (FSIM),feature similarity measure for color images (FSIMC), dynamic range independentmeasure (DRIM), and tone-mapped images quality index (TMQI)) are carefullydescribed, and their performance and computation time on four subjectivequality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA isprovided and the issues related to this area of research are reviewed.
arxiv-7500-174 | Information Transfer in Swarms with Leaders | http://arxiv.org/pdf/1407.0007v1.pdf | author:Yu Sun, Louis F. Rossi, Chien-Chung Shen, Jennifer Miller, X. Rosalind Wang, Joseph T. Lizier, Mikhail Prokopenko, Upul Senanayake category:cs.NE published:2014-06-30 summary:Swarm dynamics is the study of collections of agents that interact with oneanother without central control. In natural systems, insects, birds, fish andother large mammals function in larger units to increase the overall fitness ofthe individuals. Their behavior is coordinated through local interactions toenhance mate selection, predator detection, migratory route identification andso forth [Andersson and Wallander 2003; Buhl et al. 2006; Nagy et al. 2010;Partridge 1982; Sumpter et al. 2008]. In artificial systems, swarms ofautonomous agents can augment human activities such as search and rescue, andenvironmental monitoring by covering large areas with multiple nodes [Alami etal. 2007; Caruso et al. 2008; Ogren et al. 2004; Paley et al. 2007; Sibley etal. 2002]. In this paper, we explore the interplay between swarm dynamics,covert leadership and theoretical information transfer. A leader is a member ofthe swarm that acts upon information in addition to what is provided by localinteractions. Depending upon the leadership model, leaders can use theirexternal information either all the time or in response to local conditions[Couzin et al. 2005; Sun et al. 2013]. A covert leader is a leader that istreated no differently than others in the swarm, so leaders and followersparticipate equally in whatever interaction model is used [Rossi et al. 2007].In this study, we use theoretical information transfer as a means of analyzingswarm interactions to explore whether or not it is possible to distinguishbetween followers and leaders based on interactions within the swarm. We findthat covert leaders can be distinguished from followers in a swarm because theyreceive less transfer entropy than followers.
arxiv-7500-175 | Block matching algorithm for motion estimation based on Artificial Bee Colony (ABC) | http://arxiv.org/pdf/1407.0061v1.pdf | author:Erik Cuevas, Daniel Zaldivar, Marco Perez, Humberto Sossa, Valentin Osuna category:cs.NE published:2014-06-30 summary:Block matching (BM) motion estimation plays a very important role in videocoding. In a BM approach, image frames in a video sequence are divided intoblocks. For each block in the current frame, the best matching block isidentified inside a region of the previous frame, aiming to minimize the sum ofabsolute differences (SAD). Unfortunately, the SAD evaluation iscomputationally expensive and represents the most consuming operation in the BMprocess. Therefore, BM motion estimation can be approached as an optimizationproblem, where the goal is to find the best matching block within a searchspace. The simplest available BM method is the full search algorithm (FSA)which finds the most accurate motion vector through an exhaustive computationof SAD values for all elements of the search window. Recently, several fast BMalgorithms have been proposed to reduce the number of SAD operations bycalculating only a fixed subset of search locations at the price of pooraccuracy. In this paper, a new algorithm based on Artificial Bee Colony (ABC)optimization is proposed to reduce the number of search locations in the BMprocess. In our algorithm, the computation of search locations is drasticallyreduced by considering a fitness calculation strategy which indicates when itis feasible to calculate or only estimate new search locations. Since theproposed algorithm does not consider any fixed search pattern or any othermovement assumption as most of other BM approaches do, a high probability forfinding the true minimum (accurate motion vector) is expected. Conductedsimulations show that the proposed method achieves the best balance over otherfast BM algorithms, in terms of both estimation accuracy and computationalcost.
arxiv-7500-176 | Simple connectome inference from partial correlation statistics in calcium imaging | http://arxiv.org/pdf/1406.7865v4.pdf | author:Antonio Sutera, Arnaud Joly, Vincent François-Lavet, Zixiao Aaron Qiu, Gilles Louppe, Damien Ernst, Pierre Geurts category:stat.ML cs.CE cs.LG published:2014-06-30 summary:In this work, we propose a simple yet effective solution to the problem ofconnectome inference in calcium imaging data. The proposed algorithm consistsof two steps. First, processing the raw signals to detect neural peakactivities. Second, inferring the degree of association between neurons frompartial correlation statistics. This paper summarises the methodology that ledus to win the Connectomics Challenge, proposes a simplified version of ourmethod, and finally compares our results with respect to other inferencemethods.
arxiv-7500-177 | Relevance Singular Vector Machine for low-rank matrix sensing | http://arxiv.org/pdf/1407.0013v1.pdf | author:Martin Sundin, Saikat Chatterjee, Magnus Jansson, Cristian R. Rojas category:cs.NA cs.LG math.ST stat.TH published:2014-06-30 summary:In this paper we develop a new Bayesian inference method for low rank matrixreconstruction. We call the new method the Relevance Singular Vector Machine(RSVM) where appropriate priors are defined on the singular vectors of theunderlying matrix to promote low rank. To accelerate computations, anumerically efficient approximation is developed. The proposed algorithms areapplied to matrix completion and matrix reconstruction problems and theirperformance is studied numerically.
arxiv-7500-178 | Pixel-wise Orthogonal Decomposition for Color Illumination Invariant and Shadow-free Image | http://arxiv.org/pdf/1407.0010v2.pdf | author:Liangqiong Qu, Jiandong Tian, Zhi Han, Yandong Tang category:cs.CV published:2014-06-30 summary:In this paper, we propose a novel, effective and fast method to obtain acolor illumination invariant and shadow-free image from a single outdoor image.Different from state-of-the-art methods for shadow-free image that either needshadow detection or statistical learning, we set up a linear equation set foreach pixel value vector based on physically-based shadow invariants, deduce apixel-wise orthogonal decomposition for its solutions, and then get anillumination invariant vector for each pixel value vector on an image. Theillumination invariant vector is the unique particular solution of the linearequation set, which is orthogonal to its free solutions. With this illuminationinvariant vector and Lab color space, we propose an algorithm to generate ashadow-free image which well preserves the texture and color information of theoriginal image. A series of experiments on a diverse set of outdoor images andthe comparisons with the state-of-the-art methods validate our method.
arxiv-7500-179 | An optimization algorithm for multimodal functions inspired by collective animal behavior | http://arxiv.org/pdf/1406.7811v1.pdf | author:Erik Cuevas, Mauricio Gonzalez category:cs.NE published:2014-06-30 summary:Interest in multimodal function optimization is expanding rapidly since realworld optimization problems often demand locating multiple optima within asearch space. This article presents a new multimodal optimization algorithmnamed as the Collective Animal Behavior (CAB). Animal groups, such as schoolsof fish, flocks of birds, swarms of locusts and herds of wildebeest, exhibit avariety of behaviors including swarming about a food source, milling around acentral location or migrating over large distances in aligned groups. Thesecollective behaviors are often advantageous to groups, allowing them toincrease their harvesting efficiency to follow better migration routes, toimprove their aerodynamic and to avoid predation. In the proposed algorithm,searcher agents are a group of animals which interact to each other based onthe biological laws of collective motion. Experimental results demonstrate thatthe proposed algorithm is capable of finding global and local optima ofbenchmark multimodal optimization problems with a higher efficiency incomparison to other methods reported in the literature.
arxiv-7500-180 | Rates of Convergence for Nearest Neighbor Classification | http://arxiv.org/pdf/1407.0067v2.pdf | author:Kamalika Chaudhuri, Sanjoy Dasgupta category:cs.LG math.ST stat.ML stat.TH published:2014-06-30 summary:Nearest neighbor methods are a popular class of nonparametric estimators withseveral desirable properties, such as adaptivity to different distance scalesin different regions of space. Prior work on convergence rates for nearestneighbor classification has not fully reflected these subtle properties. Weanalyze the behavior of these estimators in metric spaces and providefinite-sample, distribution-dependent rates of convergence under minimalassumptions. As a by-product, we are able to establish the universalconsistency of nearest neighbor in a broader range of data spaces than waspreviously known. We illustrate our upper and lower bounds by introducingsmoothness classes that are customized for nearest neighbor classification.
arxiv-7500-181 | Learning Laplacian Matrix in Smooth Graph Signal Representations | http://arxiv.org/pdf/1406.7842v3.pdf | author:Xiaowen Dong, Dorina Thanou, Pascal Frossard, Pierre Vandergheynst category:cs.LG cs.SI stat.ML published:2014-06-30 summary:The construction of a meaningful graph plays a crucial role in the success ofmany graph-based representations and algorithms for handling structured data,especially in the emerging field of graph signal processing. However, ameaningful graph is not always readily available from the data, nor easy todefine depending on the application domain. In particular, it is oftendesirable in graph signal processing applications that a graph is chosen suchthat the data admit certain regularity or smoothness on the graph. In thispaper, we address the problem of learning graph Laplacians, which is equivalentto learning graph topologies, such that the input data form graph signals withsmooth variations on the resulting topology. To this end, we adopt a factoranalysis model for the graph signals and impose a Gaussian probabilistic prioron the latent variables that control these signals. We show that the Gaussianprior leads to an efficient representation that favors the smoothness propertyof the graph signals. We then propose an algorithm for learning graphs thatenforces such property and is based on minimizing the variations of the signalson the learned graph. Experiments on both synthetic and real world datademonstrate that the proposed graph learning framework can efficiently infermeaningful graph topologies from signal observations under the smoothnessprior.
arxiv-7500-182 | Adaptive Image Denoising by Targeted Databases | http://arxiv.org/pdf/1407.5055v3.pdf | author:Enming Luo, Stanley H. Chan, Truong Q. Nguyen category:cs.CV stat.ME published:2014-06-30 summary:We propose a data-dependent denoising procedure to restore noisy images.Different from existing denoising algorithms which search for patches fromeither the noisy image or a generic database, the new algorithm finds patchesfrom a database that contains only relevant patches. We formulate the denoisingproblem as an optimal filter design problem and make two contributions. First,we determine the basis function of the denoising filter by solving a groupsparsity minimization problem. The optimization formulation generalizesexisting denoising algorithms and offers systematic analysis of theperformance. Improvement methods are proposed to enhance the patch searchprocess. Second, we determine the spectral coefficients of the denoising filterby considering a localized Bayesian prior. The localized prior leverages thesimilarity of the targeted database, alleviates the intensive Bayesiancomputation, and links the new method to the classical linear minimum meansquared error estimation. We demonstrate applications of the proposed method ina variety of scenarios, including text images, multiview images and faceimages. Experimental results show the superiority of the new algorithm overexisting methods.
arxiv-7500-183 | Sparse Recovery via Differential Inclusions | http://arxiv.org/pdf/1406.7728v5.pdf | author:Stanley Osher, Feng Ruan, Jiechao Xiong, Yuan Yao, Wotao Yin category:math.ST stat.ML stat.TH published:2014-06-30 summary:In this paper, we recover sparse signals from their noisy linear measurementsby solving nonlinear differential inclusions, which is based on the notion ofinverse scale space (ISS) developed in applied mathematics. Our goal here is tobring this idea to address a challenging problem in statistics, \emph{i.e.}finding the oracle estimator which is unbiased and sign-consistent usingdynamics. We call our dynamics \emph{Bregman ISS} and \emph{Linearized BregmanISS}. A well-known shortcoming of LASSO and any convex regularizationapproaches lies in the bias of estimators. However, we show that under properconditions, there exists a bias-free and sign-consistent point on the solutionpaths of such dynamics, which corresponds to a signal that is the unbiasedestimate of the true signal and whose entries have the same signs as those ofthe true signs, \emph{i.e.} the oracle estimator. Therefore, their solutionpaths are regularization paths better than the LASSO regularization path, sincethe points on the latter path are biased when sign-consistency is reached. Wealso show how to efficiently compute their solution paths in both continuousand discretized settings: the full solution paths can be exactly computed pieceby piece, and a discretization leads to \emph{Linearized Bregman iteration},which is a simple iterative thresholding rule and easy to parallelize.Theoretical guarantees such as sign-consistency and minimax optimal $l_2$-errorbounds are established in both continuous and discrete settings for specificpoints on the paths. Early-stopping rules for identifying these points aregiven. The key treatment relies on the development of differential inequalitiesfor differential inclusions and their discretizations, which extends theprevious results and leads to exponentially fast recovering of sparse signalsbefore selecting wrong ones.
arxiv-7500-184 | Infinite Structured Hidden Semi-Markov Models | http://arxiv.org/pdf/1407.0044v1.pdf | author:Jonathan H. Huggins, Frank Wood category:stat.ME stat.AP stat.ML published:2014-06-30 summary:This paper reviews recent advances in Bayesian nonparametric techniques forconstructing and performing inference in infinite hidden Markov models. Wefocus on variants of Bayesian nonparametric hidden Markov models that enhance aposteriori state-persistence in particular. This paper also introduces a newBayesian nonparametric framework for generating left-to-right and otherstructured, explicit-duration infinite hidden Markov models that we call theinfinite structured hidden semi-Markov model.
arxiv-7500-185 | PAINTER: a spatio-spectral image reconstruction algorithm for optical interferometry | http://arxiv.org/pdf/1407.1885v2.pdf | author:Antony Schutz, André Ferrari, David Mary, Férréol Soulez, Éric Thiébaut, Martin Vannier category:astro-ph.IM cs.CV published:2014-06-29 summary:Astronomical optical interferometers sample the Fourier transform of theintensity distribution of a source at the observation wavelength. Because ofrapid perturbations caused by atmospheric turbulence, the phases of the complexFourier samples (visibilities) cannot be directly exploited. Consequently,specific image reconstruction methods have been devised in the last fewdecades. Modern polychromatic optical interferometric instruments are nowpaving the way to multiwavelength imaging. This paper is devoted to thederivation of a spatio-spectral (3D) image reconstruction algorithm, coinedPAINTER (Polychromatic opticAl INTErferometric Reconstruction software). Thealgorithm relies on an iterative process, which alternates estimation ofpolychromatic images and of complex visibilities. The complex visibilities arenot only estimated from squared moduli and closure phases, but alsodifferential phases, which helps to better constrain the polychromaticreconstruction. Simulations on synthetic data illustrate the efficiency of thealgorithm and in particular the relevance of injecting a differential phasesmodel in the reconstruction.
arxiv-7500-186 | Jabalin: a Comprehensive Computational Model of Modern Standard Arabic Verbal Morphology Based on Traditional Arabic Prosody | http://arxiv.org/pdf/1406.7483v1.pdf | author:Alicia Gonzalez Martinez, Susana Lopez Hervas, Doaa Samy, Carlos G. Arques, Antonio Moreno Sandoval category:cs.CL published:2014-06-29 summary:The computational handling of Modern Standard Arabic is a challenge in thefield of natural language processing due to its highly rich morphology.However, several authors have pointed out that the Arabic morphological systemis in fact extremely regular. The existing Arabic morphological analyzers haveexploited this regularity to variable extent, yet we believe there is stillsome scope for improvement. Taking inspiration in traditional Arabic prosody,we have designed and implemented a compact and simple morphological systemwhich in our opinion takes further advantage of the regularities encountered inthe Arabic morphological system. The output of the system is a large-scalelexicon of inflected forms that has subsequently been used to create an OnlineInterface for a morphological analyzer of Arabic verbs. The Jabalin OnlineInterface is available at http://elvira.lllf.uam.es/jabalin/, hosted at theLLI-UAM lab. The generation system is also available under a GNU GPL 3 license.
arxiv-7500-187 | Fusion Based Holistic Road Scene Understanding | http://arxiv.org/pdf/1406.7525v1.pdf | author:Wenqi Huang, Xiaojin Gong category:cs.CV published:2014-06-29 summary:This paper addresses the problem of holistic road scene understanding basedon the integration of visual and range data. To achieve the grand goal, wepropose an approach that jointly tackles object-level image segmentation andsemantic region labeling within a conditional random field (CRF) framework.Specifically, we first generate semantic object hypotheses by clustering 3Dpoints, learning their prior appearance models, and using a deep learningmethod for reasoning their semantic categories. The learned priors, togetherwith spatial and geometric contexts, are incorporated in CRF. With thisformulation, visual and range data are fused thoroughly, and moreover, thecoupled segmentation and semantic labeling problem can be inferred via GraphCuts. Our approach is validated on the challenging KITTI dataset that containsdiverse complicated road scenarios. Both quantitative and qualitativeevaluations demonstrate its effectiveness.
arxiv-7500-188 | Estimating the distribution of Galaxy Morphologies on a continuous space | http://arxiv.org/pdf/1406.7536v1.pdf | author:Giuseppe Vinci, Peter Freeman, Jeffrey Newman, Larry Wasserman, Christopher Genovese category:astro-ph.GA astro-ph.CO stat.AP stat.CO stat.ML 85-08 published:2014-06-29 summary:The incredible variety of galaxy shapes cannot be summarized by human defineddiscrete classes of shapes without causing a possibly large loss ofinformation. Dictionary learning and sparse coding allow us to reduce the highdimensional space of shapes into a manageable low dimensional continuous vectorspace. Statistical inference can be done in the reduced space via probabilitydistribution estimation and manifold estimation.
arxiv-7500-189 | Exploring Task Mappings on Heterogeneous MPSoCs using a Bias-Elitist Genetic Algorithm | http://arxiv.org/pdf/1406.7539v1.pdf | author:Wei Quan, Andy D. Pimentel category:cs.PF cs.NE C.4 published:2014-06-29 summary:Exploration of task mappings plays a crucial role in achieving highperformance in heterogeneous multi-processor system-on-chip (MPSoC) platforms.The problem of optimally mapping a set of tasks onto a set of givenheterogeneous processors for maximal throughput has been known, in general, tobe NP-complete. The problem is further exacerbated when multiple applications(i.e., bigger task sets) and the communication between tasks are alsoconsidered. Previous research has shown that Genetic Algorithms (GA) typicallyare a good choice to solve this problem when the solution space is relativelysmall. However, when the size of the problem space increases, classic geneticalgorithms still suffer from the problem of long evolution times. To addressthis problem, this paper proposes a novel bias-elitist genetic algorithm thatis guided by domain-specific heuristics to speed up the evolution process.Experimental results reveal that our proposed algorithm is able to handle largescale task mapping problems and produces high-quality mapping solutions in onlya short time period.
arxiv-7500-190 | Human Communication Systems Evolve by Cultural Selection | http://arxiv.org/pdf/1406.7558v1.pdf | author:Nicolas Fay, Monica Tamariz, T Mark Ellison, Dale Barr category:cs.SI cs.CL physics.soc-ph published:2014-06-29 summary:Human communication systems, such as language, evolve culturally; theircomponents undergo reproduction and variation. However, a role for selection incultural evolutionary dynamics is less clear. Often neutral evolution (alsoknown as 'drift') models, are used to explain the evolution of humancommunication systems, and cultural evolution more generally. Under thisaccount, cultural change is unbiased: for instance, vocabulary, baby names andpottery designs have been found to spread through random copying. While drift is the null hypothesis for models of cultural evolution it doesnot always adequately explain empirical results. Alternative models includecultural selection, which assumes variant adoption is biased. Theoreticalmodels of human communication argue that during conversation interlocutors arebiased to adopt the same labels and other aspects of linguistic representation(including prosody and syntax). This basic alignment mechanism has beenextended by computer simulation to account for the emergence of linguisticconventions. When agents are biased to match the linguistic behavior of theirinterlocutor, a single variant can propagate across an entire population ofinteracting computer agents. This behavior-matching account operates at thelevel of the individual. We call it the Conformity-biased model. Under adifferent selection account, called content-biased selection, functionalselection or replicator selection, variant adoption depends upon the intrinsicvalue of the particular variant (e.g., ease of learning or use). This secondalternative account operates at the level of the cultural variant. FollowingBoyd and Richerson we call it the Content-biased model. The present paper teststhe drift model and the two biased selection models' ability to explain thespread of communicative signal variants in an experimental micro-society.
arxiv-7500-191 | Thompson Sampling for Learning Parameterized Markov Decision Processes | http://arxiv.org/pdf/1406.7498v3.pdf | author:Aditya Gopalan, Shie Mannor category:stat.ML cs.LG published:2014-06-29 summary:We consider reinforcement learning in parameterized Markov Decision Processes(MDPs), where the parameterization may induce correlation across transitionprobabilities or rewards. Consequently, observing a particular state transitionmight yield useful information about other, unobserved, parts of the MDP. Wepresent a version of Thompson sampling for parameterized reinforcement learningproblems, and derive a frequentist regret bound for priors over generalparameter spaces. The result shows that the number of instants where suboptimalactions are chosen scales logarithmically with time, with high probability. Itholds for prior distributions that put significant probability near the truemodel, without any additional, specific closed-form structure such as conjugateor product-form priors. The constant factor in the logarithmic scaling encodesthe information complexity of learning the MDP in terms of the Kullback-Leiblergeometry of the parameter space.
arxiv-7500-192 | Complexity Measures and Concept Learning | http://arxiv.org/pdf/1406.7424v3.pdf | author:Andreas D. Pape, Kenneth J. Kurtz, Hiroki Sayama category:cs.IT cs.LG math.IT published:2014-06-28 summary:The nature of concept learning is a core question in cognitive science.Theories must account for the relative difficulty of acquiring differentconcepts by supervised learners. For a canonical set of six category types, twodistinct orderings of classification difficulty have been found. One ordering,which we call paradigm-specific, occurs when adult human learners classifyobjects with easily distinguishable characteristics such as size, shape, andshading. The general order occurs in all other known cases: when adult humansclassify objects with characteristics that are not readily distinguished (e.g.,brightness, saturation, hue); for children and monkeys; and when categorizationdifficulty is extrapolated from errors in identification learning. Theparadigm-specific order was found to be predictable mathematically by measuringthe logical complexity of tasks, i.e., how concisely the solution can berepresented by logical rules. However, logical complexity explains only the paradigm-specific order but notthe general order. Here we propose a new difficulty measurement, informationcomplexity, that calculates the amount of uncertainty remaining when a subsetof the dimensions are specified. This measurement is based on Shannon entropy.We show that, when the metric extracts minimal uncertainties, this newmeasurement predicts the paradigm-specific order for the canonical six categorytypes, and when the metric extracts average uncertainties, this new measurementpredicts the general order. Moreover, for learning category types beyond thecanonical six, we find that the minimal-uncertainty formulation correctlypredicts the paradigm-specific order as well or better than existing metrics(Boolean complexity and GIST) in most cases.
arxiv-7500-193 | Efficient Learning in Large-Scale Combinatorial Semi-Bandits | http://arxiv.org/pdf/1406.7443v3.pdf | author:Zheng Wen, Branislav Kveton, Azin Ashkan category:cs.LG cs.AI stat.ML published:2014-06-28 summary:A stochastic combinatorial semi-bandit is an online learning problem where ateach step a learning agent chooses a subset of ground items subject tocombinatorial constraints, and then observes stochastic weights of these itemsand receives their sum as a payoff. In this paper, we consider efficientlearning in large-scale combinatorial semi-bandits with linear generalization,and as a solution, propose two learning algorithms called Combinatorial LinearThompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Bothalgorithms are computationally efficient as long as the offline version of thecombinatorial problem can be solved efficiently. We establish that CombLinTSand CombLinUCB are also provably statistically efficient under reasonableassumptions, by developing regret bounds that are independent of the problemscale (number of items) and sublinear in time. We also evaluate CombLinTS on avariety of problems with thousands of items. Our experiment results demonstratethat CombLinTS is scalable, robust to the choice of algorithm parameters, andsignificantly outperforms the best of our baselines.
arxiv-7500-194 | A framework for improving the performance of verification algorithms with a low false positive rate requirement and limited training data | http://arxiv.org/pdf/1406.7360v2.pdf | author:Ognjen Arandjelovic category:cs.CV published:2014-06-28 summary:In this paper we address the problem of matching patterns in the so-calledverification setting in which a novel, query pattern is verified against asingle training pattern: the decision sought is whether the two match (i.e.belong to the same class) or not. Unlike previous work which has universallyfocused on the development of more discriminative distance functions betweenpatterns, here we consider the equally important and pervasive task ofselecting a distance threshold which fits a particular operational requirement- specifically, the target false positive rate (FPR). First, we argue ontheoretical grounds that a data-driven approach is inherently ill-conditionedwhen the desired FPR is low, because by the very nature of the challenge only asmall portion of training data affects or is affected by the desired threshold.This leads us to propose a general, statistical model-based method instead. Ourapproach is based on the interpretation of an inter-pattern distance asimplicitly defining a pattern embedding which approximately distributespatterns according to an isotropic multi-variate normal distribution in somespace. This interpretation is then used to show that the distribution oftraining inter-pattern distances is the non-central chi2 distribution,differently parameterized for each class. Thus, to make the class-specificthreshold choice we propose a novel analysis-by-synthesis iterative algorithmwhich estimates the three free parameters of the model (for each class) usingtask-specific constraints. The validity of the premises of our work and theeffectiveness of the proposed method are demonstrated by applying the method tothe task of set-based face verification on a large database of pseudo-randomhead motion videos.
arxiv-7500-195 | Contrastive Feature Induction for Efficient Structure Learning of Conditional Random Fields | http://arxiv.org/pdf/1406.7445v1.pdf | author:Ni Lao, Jun Zhu category:cs.LG published:2014-06-28 summary:Structure learning of Conditional Random Fields (CRFs) can be cast into anL1-regularized optimization problem. To avoid optimizing over a fully linkedmodel, gain-based or gradient-based feature selection methods start from anempty model and incrementally add top ranked features to it. However, forhigh-dimensional problems like statistical relational learning, training timeof these incremental methods can be dominated by the cost of evaluating thegain or gradient of a large collection of candidate features. In this study wepropose a fast feature evaluation algorithm called Contrastive FeatureInduction (CFI), which only evaluates a subset of features that involve bothvariables with high signals (deviation from mean) and variables with higherrors (residue). We prove that the gradient of candidate features can berepresented solely as a function of signals and errors, and that CFI is anefficient approximation of gradient-based evaluation methods. Experiments onsynthetic and real data sets show competitive learning speed and accuracy ofCFI on pairwise CRFs, compared to state-of-the-art structure learning methodssuch as full optimization over all features, and Grafting.
arxiv-7500-196 | Convex Analysis of Mixtures for Separating Non-negative Well-grounded Sources | http://arxiv.org/pdf/1406.7349v3.pdf | author:Yitan Zhu, Niya Wang, David J. Miller, Yue Wang category:stat.ML q-bio.QM published:2014-06-28 summary:Blind Source Separation (BSS) has proven to be a powerful tool for theanalysis of composite patterns in engineering and science. We introduce ConvexAnalysis of Mixtures (CAM) for separating non-negative well-grounded sources,which learns the mixing matrix by identifying the lateral edges of the convexdata scatter plot. We prove a sufficient and necessary condition foridentifying the mixing matrix through edge detection, which also serves as thefoundation for CAM to be applied not only to the exact-determined andover-determined cases, but also to the under-determined case. We show theoptimality of the edge detection strategy, even for cases where sourcewell-groundedness is not strictly satisfied. The CAM algorithm integratesplug-in noise filtering using sector-based clustering, an efficient geometricconvex analysis scheme, and stability-based model order selection. Wedemonstrate the principle of CAM on simulated data and numerically mixednatural images. The superior performance of CAM against a panel of benchmarkBSS techniques is demonstrated on numerically mixed gene expression data. Wethen apply CAM to dissect dynamic contrast-enhanced magnetic resonance imagingdata taken from breast tumors and time-course microarray gene expression dataderived from in-vivo muscle regeneration in mice, both producing biologicallyplausible decomposition results.
arxiv-7500-197 | Learning to Deblur | http://arxiv.org/pdf/1406.7444v1.pdf | author:Christian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Schölkopf category:cs.CV cs.LG published:2014-06-28 summary:We describe a learning-based approach to blind image deconvolution. It uses adeep layered architecture, parts of which are borrowed from recent work onneural network learning, and parts of which incorporate computations that arespecific to image deconvolution. The system is trained end-to-end on a set ofartificially generated training examples, enabling competitive performance inblind deconvolution, both with respect to quality and runtime.
arxiv-7500-198 | Comparison of SVM Optimization Techniques in the Primal | http://arxiv.org/pdf/1406.7429v1.pdf | author:Jonathan Katzman, Diane Duros category:cs.LG published:2014-06-28 summary:This paper examines the efficacy of different optimization techniques in aprimal formulation of a support vector machine (SVM). Three main techniques arecompared. The dataset used to compare all three techniques was the SentimentAnalysis on Movie Reviews dataset, from kaggle.com.
arxiv-7500-199 | Intelligent Emergency Message Broadcasting in VANET Using PSO | http://arxiv.org/pdf/1406.7399v1.pdf | author:Ghassan Samara, Tareq Alhmiedat category:cs.NI cs.NE published:2014-06-28 summary:The new type of Mobile Ad hoc Network which is called Vehicular Ad hocNetworks (VANET) created a fertile environment for research. In this research,a protocol Particle Swarm Optimization Contention Based Broadcast (PCBB) isproposed, for fast andeffective dissemination of emergency messages within ageographical area to distribute the emergency message and achieve the safetysystem, this research will help the VANET system to achieve its safety goals inintelligent and efficient way.
arxiv-7500-200 | Unimodal Bandits without Smoothness | http://arxiv.org/pdf/1406.7447v2.pdf | author:Richard Combes, Alexandre Proutiere category:cs.LG published:2014-06-28 summary:We consider stochastic bandit problems with a continuous set of arms andwhere the expected reward is a continuous and unimodal function of the arm. Nofurther assumption is made regarding the smoothness and the structure of theexpected reward function. For these problems, we propose the StochasticPentachotomy (SP) algorithm, and derive finite-time upper bounds on its regretand optimization error. In particular, we show that, for any expected rewardfunction $\mu$ that behaves as $\mu(x)=\mu(x^\star)-Cx-x^\star^\xi$ locallyaround its maximizer $x^\star$ for some $\xi, C>0$, the SP algorithm isorder-optimal. Namely its regret and optimization error scale as$O(\sqrt{T\log(T)})$ and $O(\sqrt{\log(T)/T})$, respectively, when the timehorizon $T$ grows large. These scalings are achieved without the knowledge of$\xi$ and $C$. Our algorithm is based on asymptotically optimal sequentialstatistical tests used to successively trim an interval that contains the bestarm with high probability. To our knowledge, the SP algorithm constitutes thefirst sequential arm selection rule that achieves a regret and optimizationerror scaling as $O(\sqrt{T})$ and $O(1/\sqrt{T})$, respectively, up to alogarithmic factor for non-smooth expected reward functions, as well as forsmooth functions with unknown smoothness.
arxiv-7500-201 | Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning | http://arxiv.org/pdf/1406.7362v1.pdf | author:Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2014-06-28 summary:Many state-of-the-art results obtained with deep networks are achieved withthe largest models that could be trained, and if more computation power wasavailable, we might be able to exploit much larger datasets in order to improvegeneralization ability. Whereas in learning algorithms such as decision treesthe ratio of capacity (e.g., the number of parameters) to computation is veryfavorable (up to exponentially more parameters than computation), the ratio isessentially 1 for deep neural networks. Conditional computation has beenproposed as a way to increase the capacity of a deep neural network withoutincreasing the amount of computation required, by activating some parametersand computation "on-demand", on a per-example basis. In this note, we propose anovel parametrization of weight matrices in neural networks which has thepotential to increase up to exponentially the ratio of the number of parametersto computation. The proposed approach is based on turning on some parameters(weight matrices) when specific bit patterns of hidden unit activations areobtained. In order to better control for the overfitting that might result, wepropose a parametrization that is tree-structured, where each node of the treecorresponds to a prefix of a sequence of sign bits, or gating units, associatedwith hidden units.
arxiv-7500-202 | Stock Market Prediction from WSJ: Text Mining via Sparse Matrix Factorization | http://arxiv.org/pdf/1406.7330v1.pdf | author:Felix Ming Fai Wong, Zhenming Liu, Mung Chiang category:cs.LG q-fin.ST published:2014-06-27 summary:We revisit the problem of predicting directional movements of stock pricesbased on news articles: here our algorithm uses daily articles from The WallStreet Journal to predict the closing stock prices on the same day. We proposea unified latent space model to characterize the "co-movements" between stockprices and news articles. Unlike many existing approaches, our new model isable to simultaneously leverage the correlations: (a) among stock prices, (b)among news articles, and (c) between stock prices and news articles. Thus, ourmodel is able to make daily predictions on more than 500 stocks (most of whichare not even mentioned in any news article) while having low complexity. Wecarry out extensive backtesting on trading strategies based on our algorithm.The result shows that our model has substantially better accuracy rate (55.7%)compared to many widely used algorithms. The return (56%) and Sharpe ratio dueto a trading strategy based on our model are also much higher than baselineindices.
arxiv-7500-203 | On the Use of Different Feature Extraction Methods for Linear and Non Linear kernels | http://arxiv.org/pdf/1406.7314v1.pdf | author:Imen Trabelsi, Dorra Ben Ayed category:cs.CL cs.LG published:2014-06-27 summary:The speech feature extraction has been a key focus in robust speechrecognition research; it significantly affects the recognition performance. Inthis paper, we first study a set of different features extraction methods suchas linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC)and perceptual linear prediction (PLP) with several features normalizationtechniques like rasta filtering and cepstral mean subtraction (CMS). Based onthis, a comparative evaluation of these features is performed on the task oftext independent speaker identification using a combination between gaussianmixture models (GMM) and linear and non-linear kernels based on support vectormachine (SVM).
arxiv-7500-204 | A Multi Level Data Fusion Approach for Speaker Identification on Telephone Speech | http://arxiv.org/pdf/1407.0380v1.pdf | author:Imen Trabelsi, Dorra Ben Ayed category:cs.SD cs.LG published:2014-06-27 summary:Several speaker identification systems are giving good performance with cleanspeech but are affected by the degradations introduced by noisy audioconditions. To deal with this problem, we investigate the use of complementaryinformation at different levels for computing a combined match score for theunknown speaker. In this work, we observe the effect of two supervised machinelearning approaches including support vectors machines (SVM) and na\"ive bayes(NB). We define two feature vector sets based on mel frequency cepstralcoefficients (MFCC) and relative spectral perceptual linear predictivecoefficients (RASTA-PLP). Each feature is modeled using the Gaussian MixtureModel (GMM). Several ways of combining these information sources givesignificant improvements in a text-independent speaker identification taskusing a very large telephone degraded NTIMIT database.
arxiv-7500-205 | An Incentive Compatible Multi-Armed-Bandit Crowdsourcing Mechanism with Quality Assurance | http://arxiv.org/pdf/1406.7157v3.pdf | author:Shweta Jain, Sujit Gujar, Satyanath Bhat, Onno Zoeter, Y. Narahari category:cs.GT cs.LG published:2014-06-27 summary:Consider a requester who wishes to crowdsource a series of identical binarylabeling tasks to a pool of workers so as to achieve an assured accuracy foreach task, in a cost optimal way. The workers are heterogeneous with unknownbut fixed qualities and their costs are private. The problem is to select foreach task an optimal subset of workers so that the outcome obtained from theselected workers guarantees a target accuracy level. The problem is achallenging one even in a non strategic setting since the accuracy ofaggregated label depends on unknown qualities. We develop a novel multi-armedbandit (MAB) mechanism for solving this problem. First, we propose a framework,Assured Accuracy Bandit (AAB), which leads to an MAB algorithm, ConstrainedConfidence Bound for a Non Strategic setting (CCB-NS). We derive an upper boundon the number of time steps the algorithm chooses a sub-optimal set thatdepends on the target accuracy level and true qualities. A more challengingsituation arises when the requester not only has to learn the qualities of theworkers but also elicit their true costs. We modify the CCB-NS algorithm toobtain an adaptive exploration separated algorithm which we call { \emConstrained Confidence Bound for a Strategic setting (CCB-S)}. CCB-S algorithmproduces an ex-post monotone allocation rule and thus can be transformed intoan ex-post incentive compatible and ex-post individually rational mechanismthat learns the qualities of the workers and guarantees a given target accuracylevel in a cost optimal way. We provide a lower bound on the number of timesany algorithm should select a sub-optimal set and we see that the lower boundmatches our upper bound upto a constant factor. We provide insights on thepractical implementation of this framework through an illustrative example andwe show the efficacy of our algorithms through simulations.
arxiv-7500-206 | Density-Based Diffusion for Soft Clustering | http://arxiv.org/pdf/1406.7130v2.pdf | author:Thomas Bonis, Steve Oudot category:stat.ML published:2014-06-27 summary:In this paper we advocate the use of diffusion processes guided by density toperform soft clustering tasks. Our approach interpolates between classical modeseeking and spectral clustering, being parametrized by a temperature parameter$\beta>0$ controlling the amount of random motion added to the gradient ascent.In practice we simulate the diffusion process in the continuous domain byrandom walks in neighborhood graphs built on the input data. We prove theconvergence of this scheme under mild sampling conditions, and we deriveguarantees for the clustering obtained in terms of the cluster membershipdistributions. Our theoretical results are cooroborated by preliminaryexperiments on manufactured data and on real data.
arxiv-7500-207 | Optimal Population Codes for Control and Estimation | http://arxiv.org/pdf/1406.7179v1.pdf | author:Alex Susemihl, Ron Meir, Manfred Opper category:stat.ML cs.IT math.IT q-bio.NC published:2014-06-27 summary:Agents acting in the natural world aim at selecting appropriate actions basedon noisy and partial sensory observations. Many behaviors leading to decisionmak- ing and action selection in a closed loop setting are naturally phrasedwithin a control theoretic framework. Within the framework of optimal ControlTheory, one is usually given a cost function which is minimized by selecting acontrol law based on the observations. While in standard control settings thesensors are assumed fixed, biological systems often gain from the extraflexibility of optimiz- ing the sensors themselves. However, this sensoryadaptation is geared towards control rather than perception, as is oftenassumed. In this work we show that sen- sory adaptation for control differsfrom sensory adaptation for perception, even for simple control setups. Thisimplies, consistently with recent experimental results, that when studyingsensory adaptation, it is essential to account for the task being performed.
arxiv-7500-208 | On a new formulation of nonlocal image filters involving the relative rearrangement | http://arxiv.org/pdf/1406.7128v1.pdf | author:Gonzalo Galiano, Julián Velasco category:cs.CV 68U10 published:2014-06-27 summary:Nonlocal filters are simple and powerful techniques for image denoising. Inthis paper we study the reformulation of a broad class of nonlocal filters interms of two functional rearrangements: the decreasing and the relativerearrangements. Independently of the dimension of the image, we reformulate these filters asintegral operators defined in a one-dimensional space corresponding to thelevel sets measures. We prove the equivalency between the original and the rearranged versions ofthe filters and propose a discretization in terms of constant-wiseinterpolators, which we prove to be convergent to the solution of thecontinuous setting. For some particular cases, this new formulation allows us to perform adetailed analysis of the filtering properties. Among others, we prove that thefiltered image is a contrast change of the original image, and that thefiltering procedure behaves asymptotically as a shock filter combined with aborder diffusive term, responsible for the staircaising effect and the loss ofcontrast.
arxiv-7500-209 | Template Matching based Object Detection Using HOG Feature Pyramid | http://arxiv.org/pdf/1406.7120v1.pdf | author:Anish Acharya category:cs.CV published:2014-06-27 summary:This article provides a step by step development of designing a ObjectDetection scheme using the HOG based Feature Pyramid aligned with the conceptof Template Matching.
arxiv-7500-210 | 3D planar patch extraction from stereo using probabilistic region growing | http://arxiv.org/pdf/1406.7112v1.pdf | author:Vasileios Zografos category:cs.CV published:2014-06-27 summary:This article presents a novel 3D planar patch extraction method using aprobabilistic region growing algorithm. Our method works by simultaneouslyinitiating multiple planar patches from seed points, the latter determined byan intensity-based 2D segmentation algorithm in the stereo-pair images. Thepatches are grown incrementally and in parallel as 3D scene points areconsidered for membership, using a probabilistic distance likelihood measure.In addition, we have incorporated prior information based on the noise model inthe 2D images and the scene configuration but also include the intensityinformation resulting from the initial segmentation. This method works wellacross many different data-sets, involving real and synthetic examples of bothregularly and non-regularly sampled data, and is fast enough that may be usedfor robot navigation tasks of path detection and obstacle avoidance.
arxiv-7500-211 | Adaptive texture energy measure method | http://arxiv.org/pdf/1406.7075v1.pdf | author:Omer Faruk Ertugrul category:cs.CV published:2014-06-27 summary:Recent developments in image quality, data storage, and computationalcapacity have heightened the need for texture analysis in image process. Todate various methods have been developed and introduced for assessing texturesin images. One of the most popular texture analysis methods is the TextureEnergy Measure (TEM) and it has been used for detecting edges, levels, waves,spots and ripples by employing predefined TEM masks to images. Despite severalsuccess- ful studies, TEM has a number of serious weaknesses in use. The majordrawback is; the masks are predefined therefore they cannot be adapted toimage. A new method, Adaptive Texture Energy Measure Method (aTEM), was offeredto over- come this disadvantage of TEM by using adaptive masks by adjusting thecontrast, sharpening and orientation angle of the mask. To assess theapplicability of aTEM, it is compared with TEM. The accuracy of theclassification of butterfly, flower seed and Brodatz datasets are 0.08, 0.3292and 0.3343, respectively by TEM and 0.0053, 0.2417 and 0.3153, respectively byaTEM. The results of this study indicate that aTEM is a successful method fortexture analysis.
arxiv-7500-212 | Proximal Quasi-Newton for Computationally Intensive L1-regularized M-estimators | http://arxiv.org/pdf/1406.7321v2.pdf | author:Kai Zhong, Ian E. H. Yen, Inderjit S. Dhillon, Pradeep Ravikumar category:stat.ML published:2014-06-27 summary:We consider the class of optimization problems arising from computationallyintensive L1-regularized M-estimators, where the function or gradient valuesare very expensive to compute. A particular instance of interest is theL1-regularized MLE for learning Conditional Random Fields (CRFs), which are apopular class of statistical models for varied structured prediction problemssuch as sequence labeling, alignment, and classification with label taxonomy.L1-regularized MLEs for CRFs are particularly expensive to optimize sincecomputing the gradient values requires an expensive inference step. In thiswork, we propose the use of a carefully constructed proximal quasi-Newtonalgorithm for such computationally intensive M-estimation problems, where weemploy an aggressive active set selection technique. In a key contribution ofthe paper, we show that the proximal quasi-Newton method is provablysuper-linearly convergent, even in the absence of strong convexity, byleveraging a restricted variant of strong convexity. In our experiments, theproposed algorithm converges considerably faster than current state-of-the-arton the problems of sequence labeling and hierarchical classification.
arxiv-7500-213 | Adaptive Mesh Representation and Restoration of Biomedical Images | http://arxiv.org/pdf/1406.7062v1.pdf | author:Ke Liu, Ming Xu, Zeyun Yu category:cs.CV published:2014-06-27 summary:The triangulation of images has become an active research area in recentyears for its compressive representation and ease of image processing andvisualization. However, little work has been done on how to faithfully recoverimage intensities from a triangulated mesh of an image, a process also known asimage restoration or decoding from meshes. The existing methods such as linearinterpolation, least-square interpolation, or interpolation based on radialbasis functions (RBFs) work to some extent, but often yield blurred features(edges, corners, etc.). The main reason for this problem is due to theisotropically-defined Euclidean distance that is taken into consideration inthese methods, without considering the anisotropicity of feature intensities inan image. Moreover, most existing methods use intensities defined at mesh nodeswhose intensities are often ambiguously defined on or near image edges (orfeature boundaries). In the current paper, a new method of restoring an imagefrom its triangulation representation is proposed, by utilizing anisotropicradial basis functions (ARBFs). This method considers not only the geometrical(Euclidean) distances but also the local feature orientations (anisotropicintensities). Additionally, this method is based on the intensities of meshfaces instead of mesh nodes and thus provides a more robust restoration. Thetwo strategies together guarantee excellent feature-preserving restoration ofan image with arbitrary super-resolutions from its triangulationrepresentation, as demonstrated by various experiments provided in the paper.
arxiv-7500-214 | Architecture of a Web-based Predictive Editor for Controlled Natural Language Processing | http://arxiv.org/pdf/1408.0016v1.pdf | author:Stephen Guy, Rolf Schwitter category:cs.CL cs.AI published:2014-06-27 summary:In this paper, we describe the architecture of a web-based predictive texteditor being developed for the controlled natural language PENG$^{ASP)$. Thiscontrolled language can be used to write non-monotonic specifications that havethe same expressive power as Answer Set Programs. In order to support thewriting process of these specifications, the predictive text editorcommunicates asynchronously with the controlled natural language processor thatgenerates lookahead categories and additional auxiliary information for theauthor of a specification text. The text editor can display multiple sets oflookahead categories simultaneously for different possible sentencecompletions, anaphoric expressions, and supports the addition of new contentwords to the lexicon.
arxiv-7500-215 | Reconstructing subclonal composition and evolution from whole genome sequencing of tumors | http://arxiv.org/pdf/1406.7250v3.pdf | author:Amit G. Deshwar, Shankar Vembu, Christina K. Yung, Gun Ho Jang, Lincoln Stein, Quaid Morris category:q-bio.PE cs.LG stat.ML published:2014-06-27 summary:Tumors often contain multiple subpopulations of cancerous cells defined bydistinct somatic mutations. We describe a new method, PhyloWGS, that can beapplied to WGS data from one or more tumor samples to reconstruct completegenotypes of these subpopulations based on variant allele frequencies (VAFs) ofpoint mutations and population frequencies of structural variations. Weintroduce a principled phylogenic correction for VAFs in loci affected by copynumber alterations and we show that this correction greatly improves subclonalreconstruction compared to existing methods.
arxiv-7500-216 | Face Image Classification by Pooling Raw Features | http://arxiv.org/pdf/1406.6811v2.pdf | author:Fumin Shen, Chunhua Shen, Heng Tao Shen category:cs.CV published:2014-06-26 summary:We propose a very simple, efficient yet surprisingly effective featureextraction method for face recognition (about 20 lines of Matlab code), whichis mainly inspired by spatial pyramid pooling in generic image classification.We show that features formed by simply pooling local patches over a multi-levelpyramid, coupled with a linear classifier, can significantly outperform mostrecent face recognition methods. The simplicity of our feature extractionprocedure is demonstrated by the fact that no learning is involved (except PCAwhitening). We show that, multi-level spatial pooling and dense extraction ofmulti-scale patches play critical roles in face image classification. Theextracted facial features can capture strong structural information ofindividual faces with no label information being used. We also find that,pre-processing on local image patches such as contrast normalization can havean important impact on the classification accuracy. In particular, on thechallenging face recognition datasets of FERET and LFW-a, our method improvesprevious best results by more than 10% and 20%, respectively.
arxiv-7500-217 | Face Identification with Second-Order Pooling | http://arxiv.org/pdf/1406.6818v2.pdf | author:Fumin Shen, Chunhua Shen, Heng Tao Shen category:cs.CV published:2014-06-26 summary:Automatic face recognition has received significant performance improvementby developing specialised facial image representations. On the other hand,generic object recognition has rarely been applied to the face recognition.Spatial pyramid pooling of features encoded by an over-complete dictionary hasbeen the key component of many state-of-the-art image classification systems.Inspired by its success, in this work we develop a new face imagerepresentation method inspired by the second-order pooling in Carreira et al.[1], which was originally proposed for image segmentation. The proposed methoddiffers from the previous methods in that, we encode the densely extractedlocal patches by a small-size dictionary; and the facial image signatures areobtained by pooling the second-order statistics of the encoded features. Weshow the importance of pooling on encoded features, which is bypassed by theoriginal second-order pooling method to avoid the high computational cost.Equipped with a simple linear classifier, the proposed method outperforms thestate-of-the-art face identification performance by large margins. For example,on the LFW databases, the proposed method performs better than the previousbest by around 13% accuracy.
arxiv-7500-218 | How good are detection proposals, really? | http://arxiv.org/pdf/1406.6962v2.pdf | author:Jan Hosang, Rodrigo Benenson, Bernt Schiele category:cs.CV published:2014-06-26 summary:Current top performing Pascal VOC object detectors employ detection proposalsto guide the search for objects thereby avoiding exhaustive sliding windowsearch across images. Despite the popularity of detection proposals, it isunclear which trade-offs are made when using them during object detection. Weprovide an in depth analysis of ten object proposal methods along with fourbaselines regarding ground truth annotation recall (on Pascal VOC 2007 andImageNet 2013), repeatability, and impact on DPM detector performance. Ourfindings show common weaknesses of existing methods, and provide insights tochoose the most adequate method for different settings.
arxiv-7500-219 | An improved computer vision method for detecting white blood cells | http://arxiv.org/pdf/1406.6946v2.pdf | author:Erik Cuevas, Margarita Diaz, Miguel Manzanares, Daniel Zaldivar, Marco Perez category:cs.CV published:2014-06-26 summary:The automatic detection of White Blood Cells (WBC) still remains as anunsolved issue in medical imaging. The analysis of WBC images has engagedresearchers from fields of medicine and computer vision alike. Since WBC can beapproximated by an ellipsoid form, an ellipse detector algorithm may besuccessfully applied in order to recognize them. This paper presents analgorithm for the automatic detection of WBC embedded into complicated andcluttered smear images that considers the complete process as a multi-ellipsedetection problem. The approach, based on the Differential Evolution (DE)algorithm, transforms the detection task into an optimization problem whereindividuals emulate candidate ellipses. An objective function evaluates if suchcandidate ellipses are really present in the edge image of the smear. Guided bythe values of such function, the set of encoded candidate ellipses(individuals) are evolved using the DE algorithm so that they can fit into theWBC enclosed within the edge-only map of the image. Experimental results fromwhite blood cell images with a varying range of complexity are included tovalidate the efficiency of the proposed technique in terms of accuracy androbustness.
arxiv-7500-220 | Deep Learning Multi-View Representation for Face Recognition | http://arxiv.org/pdf/1406.6947v1.pdf | author:Zhenyao Zhu, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-06-26 summary:Various factors, such as identities, views (poses), and illuminations, arecoupled in face images. Disentangling the identity and view representations isa major challenge in face recognition. Existing face recognition systems eitheruse handcrafted features or learn features discriminatively to improverecognition accuracy. This is different from the behavior of human brain.Intriguingly, even without accessing 3D data, human not only can recognize faceidentity, but can also imagine face images of a person under differentviewpoints given a single 2D image, making face perception in the brain robustto view changes. In this sense, human brain has learned and encoded 3D facemodels from 2D images. To take into account this instinct, this paper proposesa novel deep neural net, named multi-view perceptron (MVP), which can untanglethe identity and view features, and infer a full spectrum of multi-view imagesin the meanwhile, given a single 2D face image. The identity features of MVPachieve superior performance on the MultiPIE dataset. MVP is also capable tointerpolate and predict images under viewpoints that are unobserved in thetraining data.
arxiv-7500-221 | Edge Label Inference in Generalized Stochastic Block Models: from Spectral Theory to Impossibility Results | http://arxiv.org/pdf/1406.6897v1.pdf | author:Jiaming Xu, Laurent Massoulié, Marc Lelarge category:math.ST stat.ML stat.TH published:2014-06-26 summary:The classical setting of community detection consists of networks exhibitinga clustered structure. To more accurately model real systems we consider aclass of networks (i) whose edges may carry labels and (ii) which may lack aclustered structure. Specifically we assume that nodes possess latentattributes drawn from a general compact space and edges between two nodes arerandomly generated and labeled according to some unknown distribution as afunction of their latent attributes. Our goal is then to infer the edge labeldistributions from a partially observed network. We propose a computationallyefficient spectral algorithm and show it allows for asymptotically correctinference when the average node degree could be as low as logarithmic in thetotal number of nodes. Conversely, if the average node degree is below aspecific constant threshold, we show that no algorithm can achieve betterinference than guessing without using the observations. As a byproduct of ouranalysis, we show that our model provides a general procedure to constructrandom graph models with a spectrum asymptotic to a pre-specified eigenvaluedistribution such as a power-law distribution.
arxiv-7500-222 | A Fully Automated Latent Fingerprint Matcher with Embedded Self-learning Segmentation Module | http://arxiv.org/pdf/1406.6854v1.pdf | author:Jinwei Xu, Jiankun Hu, Xiuping Jia category:cs.CR cs.CV published:2014-06-26 summary:Latent fingerprint has the practical value to identify the suspects who haveunintentionally left a trace of fingerprint in the crime scenes. However,designing a fully automated latent fingerprint matcher is a very challengingtask as it needs to address many challenging issues including the separation ofoverlapping structured patterns over the partial and poor quality latentfingerprint image, and finding a match against a large background database thatwould have different resolutions. Currently there is no fully automated latentfingerprint matcher available to the public and most literature reports haveutilized a specialized latent fingerprint matcher COTS3 which is not accessibleto the public. This will make it infeasible to assess and compare the relevantresearch work which is vital for this research community. In this study, wetarget to develop a fully automated latent matcher for adaptive detection ofthe region of interest and robust matching of latent prints. Unlike themanually conducted matching procedure, the proposed latent matcher can run likea sealed black box without any manual intervention. This matcher consists ofthe following two modules: (i) the dictionary learning-based region of interest(ROI) segmentation scheme; and (ii) the genetic algorithm-based minutiae setmatching unit. Experimental results on NIST SD27 latent fingerprint databasedemonstrates that the proposed matcher outperforms the currently publicstate-of-art latent fingerprint matcher.
arxiv-7500-223 | FrameNet Resource Grammar Library for GF | http://arxiv.org/pdf/1406.6844v1.pdf | author:Normunds Gruzitis, Peteris Paikens, Guntis Barzdins category:cs.CL published:2014-06-26 summary:In this paper we present an ongoing research investigating the possibilityand potential of integrating frame semantics, particularly FrameNet, in theGrammatical Framework (GF) application grammar development. An importantcomponent of GF is its Resource Grammar Library (RGL) that encapsulates thelow-level linguistic knowledge about morphology and syntax of currently morethan 20 languages facilitating rapid development of multilingual applications.In the ideal case, porting a GF application grammar to a new language wouldonly require introducing the domain lexicon - translation equivalents that areinterlinked via common abstract terms. While it is possible for a highlyrestricted CNL, developing and porting a less restricted CNL requires aboveaverage linguistic knowledge about the particular language, and above averageGF experience. Specifying a lexicon is mostly straightforward in the case ofnouns (incl. multi-word units), however, verbs are the most complex category(in terms of both inflectional paradigms and argument structure), and addingthem to a GF application grammar is not a straightforward task. In this paperwe are focusing on verbs, investigating the possibility of creating amultilingual FrameNet-based GF library. We propose an extension to the currentRGL, allowing GF application developers to define clauses on the semanticlevel, thus leaving the language-specific syntactic mapping to this extension.We demonstrate our approach by reengineering the MOLTO Phrasebook applicationgrammar.
arxiv-7500-224 | Overlapping Community Detection Optimization and Nash Equilibrium | http://arxiv.org/pdf/1406.6832v1.pdf | author:Michel Crampes, Michel Plantié category:cs.SI physics.soc-ph stat.ML published:2014-06-26 summary:Community detection using both graphs and social networks is the focus ofmany algorithms. Recent methods aimed at optimizing the so-called modularityfunction proceed by maximizing relations within communities while minimizinginter-community relations. However, given the NP-completeness of the problem, these algorithms areheuristics that do not guarantee an optimum. In this paper, we introduce a newalgorithm along with a function that takes an approximate solution and modifiesit in order to reach an optimum. This reassignment function is considered a'potential function' and becomes a necessary condition to asserting that thecomputed optimum is indeed a Nash Equilibrium. We also use this function tosimultaneously show partitioning and overlapping communities, two detection andvisualization modes of great value in revealing interesting features of asocial network. Our approach is successfully illustrated through severalexperiments on either real unipartite, multipartite or directed graphs ofmedium and large-sized datasets.
arxiv-7500-225 | Online learning in MDPs with side information | http://arxiv.org/pdf/1406.6812v1.pdf | author:Yasin Abbasi-Yadkori, Gergely Neu category:cs.LG stat.ML published:2014-06-26 summary:We study online learning of finite Markov decision process (MDP) problemswhen a side information vector is available. The problem is motivated byapplications such as clinical trials, recommendation systems, etc. Suchapplications have an episodic structure, where each episode corresponds to apatient/customer. Our objective is to compete with the optimal dynamic policythat can take side information into account. We propose a computationally efficient algorithm and show that its regret isat most $O(\sqrt{T})$, where $T$ is the number of rounds. To best of ourknowledge, this is the first regret bound for this setting.
arxiv-7500-226 | Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks | http://arxiv.org/pdf/1406.6909v2.pdf | author:Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox category:cs.LG cs.CV cs.NE published:2014-06-26 summary:Deep convolutional networks have proven to be very successful in learningtask specific features that allow for unprecedented performance on variouscomputer vision tasks. Training of such networks follows mostly the supervisedlearning paradigm, where sufficiently many input-output pairs are required fortraining. Acquisition of large training sets is one of the key challenges, whenapproaching a new task. In this paper, we aim for generic feature learning andpresent an approach for training a convolutional network using only unlabeleddata. To this end, we train the network to discriminate between a set ofsurrogate classes. Each surrogate class is formed by applying a variety oftransformations to a randomly sampled 'seed' image patch. In contrast tosupervised network training, the resulting feature representation is not classspecific. It rather provides robustness to the transformations that have beenapplied during training. This generic feature representation allows forclassification results that outperform the state of the art for unsupervisedlearning on several popular datasets (STL-10, CIFAR-10, Caltech-101,Caltech-256). While such generic features cannot compete with class specificfeatures from supervised training on a classification task, we show that theyare advantageous on geometric matching problems, where they also outperform theSIFT descriptor.
arxiv-7500-227 | A scaled gradient projection method for Bayesian learning in dynamical systems | http://arxiv.org/pdf/1406.6603v3.pdf | author:Silvia Bonettini, Alessandro Chiuso, Marco Prato category:math.NA cs.LG stat.ML published:2014-06-25 summary:A crucial task in system identification problems is the selection of the mostappropriate model class, and is classically addressed resorting tocross-validation or using asymptotic arguments. As recently suggested in theliterature, this can be addressed in a Bayesian framework, where modelcomplexity is regulated by few hyperparameters, which can be estimated viamarginal likelihood maximization. It is thus of primary importance to designeffective optimization methods to solve the corresponding optimization problem.If the unknown impulse response is modeled as a Gaussian process with asuitable kernel, the maximization of the marginal likelihood leads to achallenging nonconvex optimization problem, which requires a stable andeffective solution strategy. In this paper we address this problem by means ofa scaled gradient projection algorithm, in which the scaling matrix and thesteplength parameter play a crucial role to provide a meaning solution in acomputational time comparable with second order methods. In particular, wepropose both a generalization of the split gradient approach to design thescaling matrix in the presence of box constraints, and an effectiveimplementation of the gradient and objective function. The extensive numericalexperiments carried out on several test problems show that our method is veryeffective in providing in few tenths of a second solutions of the problems withaccuracy comparable with state-of-the-art approaches. Moreover, the flexibilityof the proposed strategy makes it easily adaptable to a wider range of problemsarising in different areas of machine learning, signal processing and systemidentification.
arxiv-7500-228 | On the Convergence Rate of Decomposable Submodular Function Minimization | http://arxiv.org/pdf/1406.6474v3.pdf | author:Robert Nishihara, Stefanie Jegelka, Michael I. Jordan category:math.OC cs.DM cs.DS cs.LG cs.NA published:2014-06-25 summary:Submodular functions describe a variety of discrete problems in machinelearning, signal processing, and computer vision. However, minimizingsubmodular functions poses a number of algorithmic challenges. Recent workintroduced an easy-to-use, parallelizable algorithm for minimizing submodularfunctions that decompose as the sum of "simple" submodular functions.Empirically, this algorithm performs extremely well, but no theoreticalanalysis was given. In this paper, we show that the algorithm convergeslinearly, and we provide upper and lower bounds on the rate of convergence. Ourproof relies on the geometry of submodular polyhedra and draws on results fromspectral graph theory.
arxiv-7500-229 | Compressive Imaging and Characterization of Sparse Light Deflection Maps | http://arxiv.org/pdf/1406.6425v2.pdf | author:Prasad Sudhakar, Laurent Jacques, Xavier Dubois, Philippe Antoine, Luc Joannes category:cs.CV published:2014-06-25 summary:Light rays incident on a transparent object of uniform refractive indexundergo deflections, which uniquely characterize the surface geometry of theobject. Associated with each point on the surface is a deflection map (orspectrum) which describes the pattern of deflections in various directions.This article presents a novel method to efficiently acquire and reconstructsparse deflection spectra induced by smooth object surfaces. To this end, weleverage the framework of Compressed Sensing (CS) in a particularimplementation of a schlieren deflectometer, i.e., an optical system providinglinear measurements of deflection spectra with programmable spatial lightmodulation patterns. We design those modulation patterns on the principle ofspread spectrum CS for reducing the number of observations. The ability of ourdevice to simultaneously observe the deflection spectra on a densediscretization of the object surface is related to a Multiple MeasurementVector (MMV) model. This scheme allows us to estimate both the noise power andthe instrumental point spread function. We formulate the spectrum reconstruction task as the solving of a linearinverse problem regularized by an analysis sparsity prior using a translationinvariant wavelet frame. Our results demonstrate the capability and advantagesof using a CS based approach for deflectometric imaging both on simulated dataand experimental deflectometric data. Finally, the paper presents an extension of our method showing how we canextract the main deflection direction in each point of the object surface froma few compressive measurements, without needing any costly reconstructionprocedures. This compressive characterization is then confirmed withexperimental results on simple plano-convex and multifocal intra-ocular lensesstudying the evolution of the main deflection as a function of the object pointlocation.
arxiv-7500-230 | $ N^4 $-Fields: Neural Network Nearest Neighbor Fields for Image Transforms | http://arxiv.org/pdf/1406.6558v2.pdf | author:Yaroslav Ganin, Victor Lempitsky category:cs.CV published:2014-06-25 summary:We propose a new architecture for difficult image processing operations, suchas natural edge detection or thin object segmentation. The architecture isbased on a simple combination of convolutional neural networks with the nearestneighbor search. We focus our attention on the situations when the desired imagetransformation is too hard for a neural network to learn explicitly. We showthat in such situations, the use of the nearest neighbor search on top of thenetwork output allows to improve the results considerably and to account forthe underfitting effect during the neural network training. The approach isvalidated on three challenging benchmarks, where the performance of theproposed architecture matches or exceeds the state-of-the-art.
arxiv-7500-231 | A Quantitative Neural Coding Model of Sensory Memory | http://arxiv.org/pdf/1406.6453v1.pdf | author:Peilei Liu, Ting Wang category:cs.NE q-bio.NC published:2014-06-25 summary:The coding mechanism of sensory memory on the neuron scale is one of the mostimportant questions in neuroscience. We have put forward a quantitative neuralnetwork model, which is self organized, self similar, and self adaptive, justlike an ecosystem following Darwin theory. According to this model, neuralcoding is a mult to one mapping from objects to neurons. And the whole cerebrumis a real-time statistical Turing Machine, with powerful representing andlearning ability. This model can reconcile some important disputations, suchas: temporal coding versus rate based coding, grandmother cell versuspopulation coding, and decay theory versus interference theory. And it has alsoprovided explanations for some key questions such as memory consolidation,episodic memory, consciousness, and sentiment. Philosophical significance isindicated at last.
arxiv-7500-232 | Weakly-supervised Discovery of Visual Pattern Configurations | http://arxiv.org/pdf/1406.6507v1.pdf | author:Hyun Oh Song, Yong Jae Lee, Stefanie Jegelka, Trevor Darrell category:cs.CV cs.LG published:2014-06-25 summary:The increasing prominence of weakly labeled data nurtures a growing demandfor object detection methods that can cope with minimal supervision. We proposean approach that automatically identifies discriminative configurations ofvisual patterns that are characteristic of a given object class. We formulatethe problem as a constrained submodular optimization problem and demonstratethe benefits of the discovered configurations in remedying mislocalizations andfinding informative positive and negative training examples. Together, theselead to state-of-the-art weakly-supervised detection results on the challengingPASCAL VOC dataset.
arxiv-7500-233 | A Bimodal Co-Sparse Analysis Model for Image Processing | http://arxiv.org/pdf/1406.6538v1.pdf | author:Martin Kiechle, Tim Habigt, Simon Hawe, Martin Kleinsteuber category:cs.CV published:2014-06-25 summary:The success of many computer vision tasks lies in the ability to exploit theinterdependency between different image modalities such as intensity and depth.Fusing corresponding information can be achieved on several levels, and onepromising approach is the integration at a low level. Moreover, sparse signalmodels have successfully been used in many vision applications. Within thisarea of research, the so called co-sparse analysis model has attractedconsiderably less attention than its well-known counterpart, the sparsesynthesis model, although it has been proven to be very useful in various imageprocessing applications. In this paper, we propose a co-sparse analysis modelthat is able to capture the interdependency of two image modalities. It isbased on the assumption that a pair of analysis operators exists, so that theco-supports of the corresponding bimodal image structures are correlated. Wepropose an algorithm that is able to learn such a coupled pair of operatorsfrom registered and noise-free training data. Furthermore, we explain how thismodel can be applied to solve linear inverse problems in image processing andhow it can be used for image registration tasks. This paper extends the work ofsome of the authors by two major contributions. Firstly, a modification of thelearning process is proposed that a priori guarantees unit norm and zero-meanof the rows of the operator. This accounts for the intuition that contrast inimage modalities carries the most information. Secondly, the model is used in anovel bimodal image registration algorithm which estimates the transformationparameters of unregistered images of different modalities.
arxiv-7500-234 | Multi Circle Detection on Images Using Artificial Bee Colony (ABC) Optimization | http://arxiv.org/pdf/1406.6560v1.pdf | author:Erik Cuevas, Felipe Sencion-Echauri, Daniel Zaldivar, Marco Perez Cisneros category:cs.CV cs.NE published:2014-06-25 summary:Hough transform (HT) has been the most common method for circle detection,exhibiting robustness, but adversely demanding considerable computationaleffort and large memory requirements. Alternative approaches include heuristicmethods that employ iterative optimization procedures for detecting multiplecircles. Since only one circle can be marked at each optimization cycle,multiple executions must be enforced in order to achieve multi detection. Thispaper presents an algorithm for automatic detection of multiple circular shapesthat considers the overall process as a multi-modal optimization problem. Theapproach is based on the artificial bee colony (ABC) algorithm, a swarmoptimization algorithm inspired by the intelligent foraging behavior of honeybees. Unlike the original ABC algorithm, the proposed approach presents theaddition of a memory for discarded solutions. Such memory allows holdingimportant information regarding other local optima which might have emergedduring the optimization process. The detector uses a combination of threenon-collinear edge points as parameters to determine circle candidates. Amatching function (nectar- amount) determines if such circle candidates(bee-food-sources) are actually present in the image. Guided by the values ofsuch matching functions, the set of encoded candidate circles are evolvedthrough the ABC algorithm so that the best candidate (global optimum) can befitted into an actual circle within the edge only image. Then, an analysis ofthe incorporated memory is executed in order to identify potential localoptima, i.e., other circles.
arxiv-7500-235 | Mass-Univariate Hypothesis Testing on MEEG Data using Cross-Validation | http://arxiv.org/pdf/1406.6720v1.pdf | author:Seyed Mostafa Kia category:stat.ML cs.LG math.ST stat.TH published:2014-06-25 summary:Recent advances in statistical theory, together with advances in thecomputational power of computers, provide alternative methods to domass-univariate hypothesis testing in which a large number of univariate tests,can be properly used to compare MEEG data at a large number of time-frequencypoints and scalp locations. One of the major problematic aspects of this kindof mass-univariate analysis is due to high number of accomplished hypothesistests. Hence procedures that remove or alleviate the increased probability offalse discoveries are crucial for this type of analysis. Here, I propose a newmethod for mass-univariate analysis of MEEG data based on cross-validationscheme. In this method, I suggest a hierarchical classification procedure underk-fold cross-validation to detect which sensors at which time-bin and whichfrequency-bin contributes in discriminating between two different stimuli ortasks. To achieve this goal, a new feature extraction method based on thediscrete cosine transform (DCT) employed to get maximum advantage of all threedata dimensions. Employing cross-validation and hierarchy architecturealongside the DCT feature space makes this method more reliable and at the sametime enough sensitive to detect the narrow effects in brain activities.
arxiv-7500-236 | Support vector machine classification of dimensionally reduced structural MRI images for dementia | http://arxiv.org/pdf/1406.6568v1.pdf | author:V. A. Miller, S. Erlien, J. Piersol category:cs.CV cs.LG physics.med-ph published:2014-06-25 summary:We classify very-mild to moderate dementia in patients (CDR ranging from 0 to2) using a support vector machine classifier acting on dimensionally reducedfeature set derived from MRI brain scans of the 416 subjects available in theOASIS-Brains dataset. We use image segmentation and principal componentanalysis to reduce the dimensionality of the data. Our resulting feature setcontains 11 features for each subject. Performance of the classifiers isevaluated using 10-fold cross-validation. Using linear and (gaussian) kernels,we obtain a training classification accuracy of 86.4% (90.1%), test accuracy of85.0% (85.7%), test precision of 68.7% (68.5%), test recall of 68.0% (74.0%),and test Matthews correlation coefficient of 0.594 (0.616).
arxiv-7500-237 | 3DUNDERWORLD-SLS: An Open-Source Structured-Light Scanning System for Rapid Geometry Acquisition | http://arxiv.org/pdf/1406.6595v1.pdf | author:Kyriakos Herakleous, Charalambos Poullis category:cs.CV published:2014-06-25 summary:Recently, there has been an increase in the demand of virtual 3D objectsrepresenting real-life objects. A plethora of methods and systems have alreadybeen proposed for the acquisition of the geometry of real-life objects rangingfrom those which employ active sensor technology, passive sensor technology ora combination of various techniques. In this paper we present the development of a 3D scanning system which isbased on the principle of structured-light, without having particularrequirements for specialized equipment. We discuss the intrinsic details andinherent difficulties of structured-light scanning techniques and present oursolutions. Finally, we introduce our open-source scanning system"3DUNDERWORLD-SLS" which implements the proposed techniques. We have performedextensive testing with a wide range of models and report the results.Furthermore, we present a comprehensive evaluation of the system and acomparison with a high-end commercial 3D scanner.
arxiv-7500-238 | Computational Lower Bounds for Community Detection on Random Graphs | http://arxiv.org/pdf/1406.6625v3.pdf | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:math.ST cs.CC stat.ML stat.TH published:2014-06-25 summary:This paper studies the problem of detecting the presence of a small densecommunity planted in a large Erd\H{o}s-R\'enyi random graph $\mathcal{G}(N,q)$,where the edge probability within the community exceeds $q$ by a constantfactor. Assuming the hardness of the planted clique detection problem, we showthat the computational complexity of detecting the community exhibits thefollowing phase transition phenomenon: As the graph size $N$ grows and thegraph becomes sparser according to $q=N^{-\alpha}$, there exists a criticalvalue of $\alpha = \frac{2}{3}$, below which there exists a computationallyintensive procedure that can detect far smaller communities than anycomputationally efficient procedure, and above which a linear-time procedure isstatistically optimal. The results also lead to the average-case hardnessresults for recovering the dense community and approximating the densest$K$-subgraph.
arxiv-7500-239 | When is it Better to Compare than to Score? | http://arxiv.org/pdf/1406.6618v1.pdf | author:Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, Martin Wainwright category:stat.ML cs.LG published:2014-06-25 summary:When eliciting judgements from humans for an unknown quantity, one often hasthe choice of making direct-scoring (cardinal) or comparative (ordinal)measurements. In this paper we study the relative merits of either choice,providing empirical and theoretical guidelines for the selection of ameasurement scheme. We provide empirical evidence based on experiments onAmazon Mechanical Turk that in a variety of tasks, (pairwise-comparative)ordinal measurements have lower per sample noise and are typically faster toelicit than cardinal ones. Ordinal measurements however typically provide lessinformation. We then consider the popular Thurstone and Bradley-Terry-Luce(BTL) models for ordinal measurements and characterize the minimax error ratesfor estimating the unknown quantity. We compare these minimax error rates tothose under cardinal measurement models and quantify for what noise levelsordinal measurements are better. Finally, we revisit the data collected fromour experiments and show that fitting these models confirms this prediction:for tasks where the noise in ordinal measurements is sufficiently low, theordinal approach results in smaller errors in the estimation.
arxiv-7500-240 | Active Learning and Best-Response Dynamics | http://arxiv.org/pdf/1406.6633v1.pdf | author:Maria-Florina Balcan, Chris Berlind, Avrim Blum, Emma Cohen, Kaushik Patnaik, Le Song category:cs.LG cs.GT published:2014-06-25 summary:We examine an important setting for engineered systems in which low-powerdistributed sensors are each making highly noisy measurements of some unknowntarget function. A center wants to accurately learn this function by querying asmall number of sensors, which ordinarily would be impossible due to the highnoise rate. The question we address is whether local communication amongsensors, together with natural best-response dynamics in anappropriately-defined game, can denoise the system without destroying the truesignal and allow the center to succeed from only a small number of activequeries. By using techniques from game theory and empirical processes, we provepositive (and negative) results on the denoising power of several naturaldynamics. We then show experimentally that when combined with recent agnosticactive learning algorithms, this process can achieve low error from very fewqueries, performing substantially better than active or passive learningwithout these denoising dynamics as well as passive learning with denoising.
arxiv-7500-241 | Causality Networks | http://arxiv.org/pdf/1406.6651v1.pdf | author:Ishanu Chattopadhyay category:cs.LG cs.IT math.IT q-fin.ST stat.ML 68Q32 published:2014-06-25 summary:While correlation measures are used to discern statistical relationshipsbetween observed variables in almost all branches of data-driven scientificinquiry, what we are really interested in is the existence of causaldependence. Designing an efficient causality test, that may be carried out inthe absence of restrictive pre-suppositions on the underlying dynamicalstructure of the data at hand, is non-trivial. Nevertheless, ability tocomputationally infer statistical prima facie evidence of causal dependence mayyield a far more discriminative tool for data analysis compared to thecalculation of simple correlations. In the present work, we present a newnon-parametric test of Granger causality for quantized or symbolic data streamsgenerated by ergodic stationary sources. In contrast to state-of-art binarytests, our approach makes precise and computes the degree of causal dependencebetween data streams, without making any restrictive assumptions, linearity orotherwise. Additionally, without any a priori imposition of specific dynamicalstructure, we infer explicit generative models of causal cross-dependence,which may be then used for prediction. These explicit models are represented asgeneralized probabilistic automata, referred to crossed automata, and are shownto be sufficient to capture a fairly general class of causal dependence. Theproposed algorithms are computationally efficient in the PAC sense; $i.e.$, wefind good models of cross-dependence with high probability, with polynomialrun-times and sample complexities. The theoretical results are applied toweekly search-frequency data from Google Trends API for a chosen set ofsocially "charged" keywords. The causality network inferred from this datasetreveals, quite expectedly, the causal importance of certain keywords. It isalso illustrated that correlation analysis fails to gather such insight.
arxiv-7500-242 | Learning the ergodic decomposition | http://arxiv.org/pdf/1406.6670v1.pdf | author:Nabil Al-Najjar, Eran Shmaya category:math.ST math.PR stat.ML stat.TH published:2014-06-25 summary:A Bayesian agent learns about the structure of a stationary process from ob-serving past outcomes. We prove that his predictions about the near futurebecome ap- proximately those he would have made if he knew the long runempirical frequencies of the process.
arxiv-7500-243 | Generalized Mixability via Entropic Duality | http://arxiv.org/pdf/1406.6130v1.pdf | author:Mark D. Reid, Rafael M. Frongillo, Robert C. Williamson, Nishant Mehta category:cs.LG published:2014-06-24 summary:Mixability is a property of a loss which characterizes when fast convergenceis possible in the game of prediction with expert advice. We show that a keyproperty of mixability generalizes, and the exp and log operations present inthe usual theory are not as special as one might have thought. In doing this weintroduce a more general notion of $\Phi$-mixability where $\Phi$ is a generalentropy (\ie, any convex function on probabilities). We show how a propertyshared by the convex dual of any such entropy yields a natural algorithm (theminimizer of a regret bound) which, analogous to the classical aggregatingalgorithm, is guaranteed a constant regret when used with $\Phi$-mixablelosses. We characterize precisely which $\Phi$ have $\Phi$-mixable losses andput forward a number of conjectures about the optimality and relationshipsbetween different choices of entropy.
arxiv-7500-244 | Studying Collective Human Decision Making and Creativity with Evolutionary Computation | http://arxiv.org/pdf/1406.6291v1.pdf | author:Hiroki Sayama, Shelley D. Dionne category:cs.NE cs.MA nlin.AO published:2014-06-24 summary:We report a summary of our interdisciplinary research project "EvolutionaryPerspective on Collective Decision Making" that was conducted through closecollaboration between computational, organizational and social scientists atBinghamton University. We redefined collective human decision making andcreativity as evolution of ecologies of ideas, where populations of ideasevolve via continual applications of evolutionary operators such asreproduction, recombination, mutation, selection, and migration of ideas, eachconducted by participating humans. Based on this evolutionary perspective, wegenerated hypotheses about collective human decision making using agent-basedcomputer simulations. The hypotheses were then tested through severalexperiments with real human subjects. Throughout this project, we utilizedevolutionary computation (EC) in non-traditional ways---(1) as a theoreticalframework for reinterpreting the dynamics of idea generation and selection, (2)as a computational simulation model of collective human decision makingprocesses, and (3) as a research tool for collecting high-resolutionexperimental data of actual collaborative design and decision making from humansubjects. We believe our work demonstrates untapped potential of EC forinterdisciplinary research involving human and social dynamics.
arxiv-7500-245 | Image Completion for View Synthesis Using Markov Random Fields and Efficient Belief Propagation | http://arxiv.org/pdf/1406.6273v1.pdf | author:Julian Habigt, Klaus Diepold category:cs.CV published:2014-06-24 summary:View synthesis is a process for generating novel views from a scene which hasbeen recorded with a 3-D camera setup. It has important applications in 3-Dpost-production and 2-D to 3-D conversion. However, a central problem in thegeneration of novel views lies in the handling of disocclusions. Backgroundcontent, which was occluded in the original view, may become unveiled in thesynthesized view. This leads to missing information in the generated view whichhas to be filled in a visually plausible manner. We present an inpaintingalgorithm for disocclusion filling in synthesized views based on Markov randomfields and efficient belief propagation. We compare the result to twostate-of-the-art algorithms and demonstrate a significant improvement in imagequality.
arxiv-7500-246 | Recurrent Models of Visual Attention | http://arxiv.org/pdf/1406.6247v1.pdf | author:Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu category:cs.LG cs.CV stat.ML published:2014-06-24 summary:Applying convolutional neural networks to large images is computationallyexpensive because the amount of computation scales linearly with the number ofimage pixels. We present a novel recurrent neural network model that is capableof extracting information from an image or video by adaptively selecting asequence of regions or locations and only processing the selected regions athigh resolution. Like convolutional neural networks, the proposed model has adegree of translation invariance built-in, but the amount of computation itperforms can be controlled independently of the input image size. While themodel is non-differentiable, it can be trained using reinforcement learningmethods to learn task-specific policies. We evaluate our model on several imageclassification tasks, where it significantly outperforms a convolutional neuralnetwork baseline on cluttered images, and on a dynamic visual control problem,where it learns to track a simple object without an explicit training signalfor doing so.
arxiv-7500-247 | Saccadic Eye Movements and the Generalized Pareto Distribution | http://arxiv.org/pdf/1406.6201v1.pdf | author:Reiner Lenz category:cs.CV I.5.4 published:2014-06-24 summary:We describe a statistical analysis of the eye tracker measurements in adatabase with 15 observers viewing 1003 images under free-viewing conditions.In contrast to the common approach of investigating the properties of thefixation points we analyze the properties of the transition phases betweenfixations. We introduce hyperbolic geometry as a tool to measure the steplength between consecutive eye positions. We show that the step lengths,measured in hyperbolic and euclidean geometry, follow a generalized Paretodistribution. The results based on the hyperbolic distance are more robust thanthose based on euclidean geometry. We show how the structure of the space ofgeneralized Pareto distributions can be used to characterize and identifyindividual observers.
arxiv-7500-248 | Combining predictions from linear models when training and test inputs differ | http://arxiv.org/pdf/1406.6200v1.pdf | author:Thijs van Ommen category:stat.ME cs.LG stat.ML published:2014-06-24 summary:Methods for combining predictions from different models in a supervisedlearning setting must somehow estimate/predict the quality of a model'spredictions at unknown future inputs. Many of these methods (often implicitly)make the assumption that the test inputs are identical to the training inputs,which is seldom reasonable. By failing to take into account that predictionwill generally be harder for test inputs that did not occur in the trainingset, this leads to the selection of too complex models. Based on a novel,unbiased expression for KL divergence, we propose XAIC and its special caseFAIC as versions of AIC intended for prediction that use different degrees ofknowledge of the test inputs. Both methods substantially differ from and mayoutperform all the known versions of AIC even when the training and test inputsare iid, and are especially useful for deterministic inputs and under covariateshift. Our experiments on linear models suggest that if the test and traininginputs differ substantially, then XAIC and FAIC predictively outperform AIC,BIC and several other methods including Bayesian model averaging.
arxiv-7500-249 | Composite Likelihood Estimation for Restricted Boltzmann machines | http://arxiv.org/pdf/1406.6176v1.pdf | author:Muneki Yasuda, Shun Kataoka, Yuji Waizumi, Kazuyuki Tanaka category:cs.LG published:2014-06-24 summary:Learning the parameters of graphical models using the maximum likelihoodestimation is generally hard which requires an approximation. Maximum compositelikelihood estimations are statistical approximations of the maximum likelihoodestimation which are higher-order generalizations of the maximumpseudo-likelihood estimation. In this paper, we propose a composite likelihoodmethod and investigate its property. Furthermore, we apply our compositelikelihood method to restricted Boltzmann machines.
arxiv-7500-250 | A Concise Information-Theoretic Derivation of the Baum-Welch algorithm | http://arxiv.org/pdf/1406.7002v1.pdf | author:Alireza Nejati, Charles Unsworth category:cs.IT cs.LG math.IT published:2014-06-24 summary:We derive the Baum-Welch algorithm for hidden Markov models (HMMs) through aninformation-theoretical approach using cross-entropy instead of the Lagrangemultiplier approach which is universal in machine learning literature. Theproposed approach provides a more concise derivation of the Baum-Welch methodand naturally generalizes to multiple observations.
arxiv-7500-251 | Mining Recurrent Concepts in Data Streams using the Discrete Fourier Transform | http://arxiv.org/pdf/1406.6114v1.pdf | author:Sakthithasan Sripirakas, Russel Pears category:cs.LG published:2014-06-24 summary:In this research we address the problem of capturing recurring concepts in adata stream environment. Recurrence capture enables the re-use of previouslylearned classifiers without the need for re-learning while providing for betteraccuracy during the concept recurrence interval. We capture concepts byapplying the Discrete Fourier Transform (DFT) to Decision Tree classifiers toobtain highly compressed versions of the trees at concept drift points in thestream and store such trees in a repository for future use. Our empiricalresults on real world and synthetic data exhibiting varying degrees ofrecurrence show that the Fourier compressed trees are more robust to noise andare able to capture recurring concepts with higher precision than a metalearning approach that chooses to re-use classifiers in their originallyoccurring form.
arxiv-7500-252 | Incremental Clustering: The Case for Extra Clusters | http://arxiv.org/pdf/1406.6398v1.pdf | author:Margareta Ackerman, Sanjoy Dasgupta category:cs.LG published:2014-06-24 summary:The explosion in the amount of data available for analysis often necessitatesa transition from batch to incremental clustering methods, which process oneelement at a time and typically store only a small subset of the data. In thispaper, we initiate the formal analysis of incremental clustering methodsfocusing on the types of cluster structure that they are able to detect. Wefind that the incremental setting is strictly weaker than the batch model,proving that a fundamental class of cluster structures that can readily bedetected in the batch setting is impossible to identify using any incrementalmethod. Furthermore, we show how the limitations of incremental clustering canbe overcome by allowing additional clusters.
arxiv-7500-253 | Image patch analysis and clustering of sunspots: a dimensionality reduction approach | http://arxiv.org/pdf/1406.6390v1.pdf | author:Kevin R. Moon, Jimmy J. Li, Veronique Delouille, Fraser Watson, Alfred O. Hero III category:cs.CV astro-ph.SR published:2014-06-24 summary:Sunspots, as seen in white light or continuum images, are associated withregions of high magnetic activity on the Sun, visible on magnetogram images.Their complexity is correlated with explosive solar activity and so classifyingthese active regions is useful for predicting future solar activity. Currentclassification of sunspot groups is visually based and suffers from bias.Supervised learning methods can reduce human bias but fail to optimallycapitalize on the information present in sunspot images. This paper uses twoimage modalities (continuum and magnetogram) to characterize the spatial andmodal interactions of sunspot and magnetic active region images and presents anew approach to cluster the images. Specifically, in the framework of imagepatch analysis, we estimate the number of intrinsic parameters required todescribe the spatial and modal dependencies, the correlation between the twomodalities and the corresponding spatial patterns, and examine the phenomena atdifferent scales within the images. To do this, we use linear and nonlinearintrinsic dimension estimators, canonical correlation analysis, andmultiresolution analysis of intrinsic dimension.
arxiv-7500-254 | A multilevel thresholding algorithm using Electromagnetism Optimization | http://arxiv.org/pdf/1406.6336v1.pdf | author:Diego Oliva, Erik Cuevas, Gonzalo Pajares, Daniel Zaldivar, Valentin Osuna category:cs.CV published:2014-06-24 summary:Segmentation is one of the most important tasks in image processing. Itconsist in classify the pixels into two or more groups depending on theirintensity levels and a threshold value. The quality of the segmentation dependson the method applied to select the threshold. The use of the classicalimplementations for multilevel thresholding is computationally expensive sincethey exhaustively search the best values to optimize the objective function.Under such conditions, the use of optimization evolutionary approaches has beenextended. The Electromagnetism Like algorithm (EMO) is an evolutionary methodwhich mimics the attraction repulsion mechanism among charges to evolve themembers of a population. Different to other algorithms, EMO exhibitsinteresting search capabilities whereas maintains a low computational overhead.In this paper, a multilevel thresholding (MT) algorithm based on the EMO isintroduced. The approach combines the good search capabilities of EMO algorithmwith objective functions proposed by the popular MT methods of Otsu and Kapur.The algorithm takes random samples from a feasible search space inside theimage histogram. Such samples build each particle in the EMO context whereasits quality is evaluated considering the objective that is function employed bythe Otsu or Kapur method. Guided by these objective values the set of candidatesolutions are evolved through the EMO operators until an optimal solution isfound. The approach generates a multilevel segmentation algorithm which caneffectively identify the threshold values of a digital image in a reducednumber of iterations. Experimental results show performance evidence of theimplementation of EMO for digital image segmentation.
arxiv-7500-255 | Dense Correspondences Across Scenes and Scales | http://arxiv.org/pdf/1406.6323v1.pdf | author:Moria Tau, Tal Hassner category:cs.CV published:2014-06-24 summary:We seek a practical method for establishing dense correspondences between twoimages with similar content, but possibly different 3D scenes. One of thechallenges in designing such a system is the local scale differences of objectsappearing in the two images. Previous methods often considered only smallsubsets of image pixels; matching only pixels for which stable scales may bereliably estimated. More recently, others have considered densecorrespondences, but with substantial costs associated with generating, storingand matching scale invariant descriptors. Our work here is motivated by theobservation that pixels in the image have contexts -- the pixels around them --which may be exploited in order to estimate local scales reliably andrepeatably. Specifically, we make the following contributions. (i) We show thatscales estimated in sparse interest points may be propagated to neighboringpixels where this information cannot be reliably determined. Doing so allowsscale invariant descriptors to be extracted anywhere in the image, not just indetected interest points. (ii) We present three different means for propagatingthis information: using only the scales at detected interest points, using theunderlying image information to guide the propagation of this informationacross each image, separately, and using both images simultaneously. Finally,(iii), we provide extensive results, both qualitative and quantitative,demonstrating that accurate dense correspondences can be obtained even betweenvery different images, with little computational costs beyond those required byexisting methods.
arxiv-7500-256 | Incorporating Near-Infrared Information into Semantic Image Segmentation | http://arxiv.org/pdf/1406.6147v1.pdf | author:Neda Salamati, Diane Larlus, Gabriela Csurka, Sabine Süsstrunk category:cs.CV published:2014-06-24 summary:Recent progress in computational photography has shown that we can acquirenear-infrared (NIR) information in addition to the normal visible (RGB) band,with only slight modifications to standard digital cameras. Due to theproximity of the NIR band to visible radiation, NIR images share manyproperties with visible images. However, as a result of the material dependentreflection in the NIR part of the spectrum, such images reveal differentcharacteristics of the scene. We investigate how to effectively exploit thesedifferences to improve performance on the semantic image segmentation task.Based on a state-of-the-art segmentation framework and a novel manuallysegmented image database (both indoor and outdoor scenes) that contain4-channel images (RGB+NIR), we study how to best incorporate the specificcharacteristics of the NIR response. We show that adding NIR leads to improvedperformance for classes that correspond to a specific type of material in bothoutdoor and indoor scenes. We also discuss the results with respect to thephysical properties of the NIR response.
arxiv-7500-257 | Fast algorithm for robust subspace recovery | http://arxiv.org/pdf/1406.6145v1.pdf | author:Gilad Lerman, Tyler Maunu category:cs.LG cs.CV stat.AP stat.ML published:2014-06-24 summary:This paper presents a fast algorithm for robust subspace recovery. Thedatasets considered include points drawn around a low-dimensional subspace of ahigher dimensional ambient space, and a possibly large portion of points thatdo not lie nearby this subspace. The proposed algorithm, which we refer to asFast Median Subspace (FMS), is designed to robustly determine the underlyingsubspace of such datasets, while having lower computational complexity thanexisting methods. Numerical experiments on synthetic and real data demonstrateits competitive speed and accuracy.
arxiv-7500-258 | Techniques for clustering interaction data as a collection of graphs | http://arxiv.org/pdf/1406.6319v3.pdf | author:Nam H. Lee, Carey Priebe, Youngser Park, I-Jeng Wang, Michael Rosen category:stat.ML published:2014-06-24 summary:A natural approach to analyze interaction data of form"what-connects-to-what-when" is to create a time-series (or rather a sequence)of graphs through temporal discretization (bandwidth selection) and spatialdiscretization (vertex contraction). Such discretization together withnon-negative factorization techniques can be useful for obtaining clustering ofgraphs. Motivating application of performing clustering of graphs (as opposedto vertex clustering) can be found in neuroscience and in social networkanalysis, and it can also be used to enhance community detection (i.e., vertexclustering) by way of conditioning on the cluster labels. In this paper, weformulate a problem of clustering of graphs as a model selection problem. Ourapproach involves information criteria, non-negative matrix factorization andsingular value thresholding, and we illustrate our techniques using real andsimulated data.
arxiv-7500-259 | Reliable ABC model choice via random forests | http://arxiv.org/pdf/1406.6288v3.pdf | author:Pierre Pudlo, Jean-Michel Marin, Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier, Christian P. Robert category:stat.ML q-bio.PE stat.CO stat.ME published:2014-06-24 summary:Approximate Bayesian computation (ABC) methods provide an elaborate approachto Bayesian inference on complex models, including model choice. Boththeoretical arguments and simulation experiments indicate, however, that modelposterior probabilities may be poorly evaluated by standard ABC techniques. Wepropose a novel approach based on a machine learning tool named random foreststo conduct selection among the highly complex models covered by ABC algorithms.We thus modify the way Bayesian model selection is both understood andoperated, in that we rephrase the inferential goal as a classification problem,first predicting the model that best fits the data with random forests andpostponing the approximation of the posterior probability of the predicted MAPfor a second stage also relying on random forests. Compared with earlierimplementations of ABC model choice, the ABC random forest approach offersseveral potential improvements: (i) it often has a larger discriminative poweramong the competing models, (ii) it is more robust against the number andchoice of statistics summarizing the data, (iii) the computing effort isdrastically reduced (with a gain in computation efficiency of at least fifty),and (iv) it includes an approximation of the posterior probability of theselected model. The call to random forests will undoubtedly extend the range ofsize of datasets and complexity of models that ABC can handle. We illustratethe power of this novel methodology by analyzing controlled experiments as wellas genuine population genetics datasets. The proposed methodologies areimplemented in the R package abcrf available on the CRAN.
arxiv-7500-260 | Offline Handwritten MODI Character Recognition Using HU, Zernike Moments and Zoning | http://arxiv.org/pdf/1406.6140v4.pdf | author:Sadanand A. Kulkarni, Prashant L. Borde, Ramesh R. Manza, Pravin L. Yannawar category:cs.CV published:2014-06-24 summary:HOCR is abbreviated as Handwritten Optical Character Recognition. HOCR is aprocess of recognition of different handwritten characters from a digital imageof documents. Handwritten automatic character recognition has attracted manyresearchers all over the world to contribute handwritten character recognitiondomain. Shape identification and feature extraction is very important part ofany character recognition system and success of method is highly dependent onselection of features. However feature extraction is the most important step indefining the shape of the character as precisely and as uniquely as possible.This is indeed the most important step and complex task as well and achievedsuccess by using invariance property, irrespective of position and orientation.Zernike moments describes shape, identify rotation invariant due to itsOrthogonality property. MODI is an ancient script of India had cursive andcomplex representation of characters. The work described in this paper presentsefficiency of Zernike moments over Hu 7 moment with zoning for automaticrecognition of handwritten MODI characters. Offline approach is used in thispaper because MODI Script was very popular and widely used for writing purposetill 19th century before Devanagari was officially adopted.
arxiv-7500-261 | Automatic Dimension Selection for a Non-negative Factorization Approach to Clustering Multiple Random Graphs | http://arxiv.org/pdf/1406.6315v2.pdf | author:Nam H. Lee, I-Jeng Wang, Youngser Park, Care E. Priebe, Michael Rosen category:stat.ML published:2014-06-24 summary:We consider a problem of grouping multiple graphs into several clusters usingsingular value thesholding and non-negative factorization. We derive a modelselection information criterion to estimate the number of clusters. Wedemonstrate our approach using "Swimmer data set" as well as simulated dataset, and compare its performance with two standard clustering algorithms.
arxiv-7500-262 | Scalable Topical Phrase Mining from Text Corpora | http://arxiv.org/pdf/1406.6312v2.pdf | author:Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare Voss, Jiawei Han category:cs.CL cs.IR cs.LG published:2014-06-24 summary:While most topic modeling algorithms model text corpora with unigrams, humaninterpretation often relies on inherent grouping of terms into phrases. Assuch, we consider the problem of discovering topical phrases of mixed lengths.Existing work either performs post processing to the inference results ofunigram-based topic models, or utilizes complex n-gram-discovery topic models.These methods generally produce low-quality topical phrases or suffer from poorscalability on even moderately-sized datasets. We propose a different approachthat is both computationally efficient and effective. Our solution combines anovel phrase mining framework to segment a document into single and multi-wordphrases, and a new topic model that operates on the induced document partition.Our approach discovers high quality topical phrases with negligible extra costto the bag-of-words topic model in a variety of datasets including researchpublication titles, abstracts, reviews, and news articles.
arxiv-7500-263 | Further heuristics for $k$-means: The merge-and-split heuristic and the $(k,l)$-means | http://arxiv.org/pdf/1406.6314v1.pdf | author:Frank Nielsen, Richard Nock category:cs.LG cs.CV cs.IR stat.ML published:2014-06-23 summary:Finding the optimal $k$-means clustering is NP-hard in general and manyheuristics have been designed for minimizing monotonically the $k$-meansobjective. We first show how to extend Lloyd's batched relocation heuristic andHartigan's single-point relocation heuristic to take into account empty-clusterand single-point cluster events, respectively. Those events tend toincreasingly occur when $k$ or $d$ increases, or when performing severalrestarts. First, we show that those special events are a blessing because theyallow to partially re-seed some cluster centers while further minimizing the$k$-means objective function. Second, we describe a novel heuristic,merge-and-split $k$-means, that consists in merging two clusters and splittingthis merged cluster again with two new centers provided it improves the$k$-means objective. This novel heuristic can improve Hartigan's $k$-means whenit has converged to a local minimum. We show empirically that thismerge-and-split $k$-means improves over the Hartigan's heuristic which is the{\em de facto} method of choice. Finally, we propose the $(k,l)$-meansobjective that generalizes the $k$-means objective by associating the datapoints to their $l$ closest cluster centers, and show how to either directlyconvert or iteratively relax the $(k,l)$-means into a $k$-means in order toreach better local minima.
arxiv-7500-264 | A Unified Quantitative Model of Vision and Audition | http://arxiv.org/pdf/1406.5807v1.pdf | author:Peilei Liu, Ting Wang category:cs.CV q-bio.NC q-bio.QM I.5.4; I.5.2 published:2014-06-23 summary:We have put forwards a unified quantitative framework of vision and audition,based on existing data and theories. According to this model, the retina is afeedforward network self-adaptive to inputs in a specific period. After fullygrown, cells become specialized detectors based on statistics of stimulushistory. This model has provided explanations for perception mechanisms ofcolour, shape, depth and motion. Moreover, based on this ground we have putforwards a bold conjecture that single ear can detect sound direction. This iscomplementary to existing theories and has provided better explanations forsound localization.
arxiv-7500-265 | VideoSET: Video Summary Evaluation through Text | http://arxiv.org/pdf/1406.5824v1.pdf | author:Serena Yeung, Alireza Fathi, Li Fei-Fei category:cs.CV cs.CL cs.IR published:2014-06-23 summary:In this paper we present VideoSET, a method for Video Summary Evaluationthrough Text that can evaluate how well a video summary is able to retain thesemantic information contained in its original video. We observe that semanticsis most easily expressed in words, and develop a text-based approach for theevaluation. Given a video summary, a text representation of the video summaryis first generated, and an NLP-based metric is then used to measure itssemantic distance to ground-truth text summaries written by humans. We showthat our technique has higher agreement with human judgment than pixel-baseddistance metrics. We also release text annotations and ground-truth textsummaries for a number of publicly available video datasets, for use by thecomputer vision community.
arxiv-7500-266 | Committees of deep feedforward networks trained with few data | http://arxiv.org/pdf/1406.5947v1.pdf | author:Bogdan Miclut, Thomas Kaester, Thomas Martinetz, Erhardt Barth category:cs.CV cs.NE published:2014-06-23 summary:Deep convolutional neural networks are known to give good results on imageclassification tasks. In this paper we present a method to improve theclassification result by combining multiple such networks in a committee. Weadopt the STL-10 dataset which has very few training examples and show that ourmethod can achieve results that are better than the state of the art. Thenetworks are trained layer-wise and no backpropagation is used. We also explorethe effects of dataset augmentation by mirroring, rotation, and scaling.
arxiv-7500-267 | Multi-utility Learning: Structured-output Learning with Multiple Annotation-specific Loss Functions | http://arxiv.org/pdf/1406.5910v1.pdf | author:Roman Shapovalov, Dmitry Vetrov, Anton Osokin, Pushmeet Kohli category:cs.CV cs.LG published:2014-06-23 summary:Structured-output learning is a challenging problem; particularly so becauseof the difficulty in obtaining large datasets of fully labelled instances fortraining. In this paper we try to overcome this difficulty by presenting amulti-utility learning framework for structured prediction that can learn fromtraining instances with different forms of supervision. We propose a unifiedtechnique for inferring the loss functions most suitable for quantifying theconsistency of solutions with the given weak annotation. We demonstrate theeffectiveness of our framework on the challenging semantic image segmentationproblem for which a wide variety of annotations can be used. For instance, thepopular training datasets for semantic segmentation are composed of images withhard-to-generate full pixel labellings, as well as images with easy-to-obtainweak annotations, such as bounding boxes around objects, or image-level labelsthat specify which object categories are present in an image. Experimentalevaluation shows that the use of annotation-specific loss functionsdramatically improves segmentation accuracy compared to the baseline systemwhere only one type of weak annotation is used.
arxiv-7500-268 | Exact fit of simple finite mixture models | http://arxiv.org/pdf/1406.6038v2.pdf | author:Dirk Tasche category:stat.ML q-fin.RM 62P30, 62F10 published:2014-06-23 summary:How to forecast next year's portfolio-wide credit default rate based on lastyear's default observations and the current score distribution? A classicalapproach to this problem consists of fitting a mixture of the conditional scoredistributions observed last year to the current score distribution. This is aspecial (simple) case of a finite mixture model where the mixture componentsare fixed and only the weights of the components are estimated. The optimumweights provide a forecast of next year's portfolio-wide default rate. We pointout that the maximum-likelihood (ML) approach to fitting the mixturedistribution not only gives an optimum but even an exact fit if we allow themixture components to vary but keep their density ratio fix. From thisobservation we can conclude that the standard default rate forecast based onlast year's conditional default rates will always be located between lastyear's portfolio-wide default rate and the ML forecast for next year. As anapplication example, then cost quantification is discussed. We also discuss howthe mixture model based estimation methods can be used to forecast total loss.This involves the reinterpretation of an individual classification problem as acollective quantification problem.
arxiv-7500-269 | Improved Frame Level Features and SVM Supervectors Approach for the Recogniton of Emotional States from Speech: Application to categorical and dimensional states | http://arxiv.org/pdf/1406.6101v1.pdf | author:Imen Trabelsi, Dorra Ben Ayed, Noureddine Ellouze category:cs.CL cs.LG published:2014-06-23 summary:The purpose of speech emotion recognition system is to classify speakersutterances into different emotional states such as disgust, boredom, sadness,neutral and happiness. Speech features that are commonly used in speech emotionrecognition rely on global utterance level prosodic features. In our work, weevaluate the impact of frame level feature extraction. The speech samples arefrom Berlin emotional database and the features extracted from these utterancesare energy, different variant of mel frequency cepstrum coefficients, velocityand acceleration features.
arxiv-7500-270 | From Black-Scholes to Online Learning: Dynamic Hedging under Adversarial Environments | http://arxiv.org/pdf/1406.6084v1.pdf | author:Henry Lam, Zhenming Liu category:cs.DS cs.LG q-fin.PR F.2; I.2.6 published:2014-06-23 summary:We consider a non-stochastic online learning approach to price financialoptions by modeling the market dynamic as a repeated game between the nature(adversary) and the investor. We demonstrate that such framework yieldsanalogous structure as the Black-Scholes model, the widely popular optionpricing model in stochastic finance, for both European and American optionswith convex payoffs. In the case of non-convex options, we constructapproximate pricing algorithms, and demonstrate that their efficiency can beanalyzed through the introduction of an artificial probability measure, inparallel to the so-called risk-neutral measure in the finance literature, eventhough our framework is completely adversarial. Continuous-time convergenceresults and extensions to incorporate price jumps are also presented.
arxiv-7500-271 | A Statistical Perspective on Randomized Sketching for Ordinary Least-Squares | http://arxiv.org/pdf/1406.5986v2.pdf | author:Garvesh Raskutti, Michael Mahoney category:stat.ML published:2014-06-23 summary:We consider statistical as well as algorithmic aspects of solving large-scaleleast-squares (LS) problems using randomized sketching algorithms. For a LSproblem with input data $(X, Y) \in \mathbb{R}^{n \times p} \times\mathbb{R}^n$, sketching algorithms use a sketching matrix, $S\in\mathbb{R}^{r\times n}$ with $r \ll n$. Then, rather than solving the LS problem using thefull data $(X,Y)$, sketching algorithms solve the LS problem using only thesketched data $(SX, SY)$. Prior work has typically adopted an algorithmicperspective, in that it has made no statistical assumptions on the input $X$and $Y$, and instead it has been assumed that the data $(X,Y)$ are fixed andworst-case (WC). Prior results show that, when using sketching matrices such asrandom projections and leverage-score sampling algorithms, with $p < r \ll n$,the WC error is the same as solving the original problem, up to a smallconstant. From a statistical perspective, we typically consider themean-squared error performance of randomized sketching algorithms, when data$(X, Y)$ are generated according to a statistical model $Y = X \beta +\epsilon$, where $\epsilon$ is a noise process. We provide a rigorouscomparison of both perspectives leading to insights on how they differ. To dothis, we first develop a framework for assessing algorithmic and statisticalaspects of randomized sketching methods. We then consider the statisticalprediction efficiency (PE) and the statistical residual efficiency (RE) of thesketched LS estimator; and we use our framework to provide upper bounds forseveral types of random projection and random sampling sketching algorithms.Among other results, we show that the RE can be upper bounded when $p < r \lln$ while the PE typically requires the sample size $r$ to be substantiallylarger. Lower bounds developed in subsequent results show that our upper boundson PE can not be improved.
arxiv-7500-272 | Stationary Mixing Bandits | http://arxiv.org/pdf/1406.6020v1.pdf | author:Julien Audiffren, Liva Ralaivola category:cs.LG published:2014-06-23 summary:We study the bandit problem where arms are associated with stationaryphi-mixing processes and where rewards are therefore dependent: the questionthat arises from this setting is that of recovering some independence byignoring the value of some rewards. As we shall see, the bandit problem wetackle requires us to address the exploration/exploitation/independencetrade-off. To do so, we provide a UCB strategy together with a general regretanalysis for the case where the size of the independence blocks (the ignoredrewards) is fixed and we go a step beyond by providing an algorithm that isable to compute the size of the independence blocks from the data. Finally, wegive an analysis of our bandit problem in the restless case, i.e., in thesituation where the time counters for all mixing processes simultaneouslyevolve.
arxiv-7500-273 | Reinforcement and Imitation Learning via Interactive No-Regret Learning | http://arxiv.org/pdf/1406.5979v1.pdf | author:Stephane Ross, J. Andrew Bagnell category:cs.LG stat.ML published:2014-06-23 summary:Recent work has demonstrated that problems-- particularly imitation learningand structured prediction-- where a learner's predictions influence theinput-distribution it is tested on can be naturally addressed by an interactiveapproach and analyzed using no-regret online learning. These approaches toimitation learning, however, neither require nor benefit from information aboutthe cost of actions. We extend existing results in two directions: first, wedevelop an interactive imitation learning approach that leverages costinformation; second, we extend the technique to address reinforcement learning.The results provide theoretical support to the commonly observed successes ofonline approximate policy iteration. Our approach suggests a broad new familyof algorithms and provides a unifying view of existing techniques for imitationand reinforcement learning.
arxiv-7500-274 | Divide-and-Conquer Learning by Anchoring a Conical Hull | http://arxiv.org/pdf/1406.5752v1.pdf | author:Tianyi Zhou, Jeff Bilmes, Carlos Guestrin category:stat.ML cs.LG published:2014-06-22 summary:We reduce a broad class of machine learning problems, usually addressed by EMor sampling, to the problem of finding the $k$ extremal rays spanning theconical hull of a data point set. These $k$ "anchors" lead to a global solutionand a more interpretable model that can even outperform EM and sampling ongeneralization error. To find the $k$ anchors, we propose a noveldivide-and-conquer learning scheme "DCA" that distributes the problem to$\mathcal O(k\log k)$ same-type sub-problems on different low-D randomhyperplanes, each can be solved by any solver. For the 2D sub-problem, wepresent a non-iterative solver that only needs to compute an array of cosinevalues and its max/min entries. DCA also provides a faster subroutine for othermethods to check whether a point is covered in a conical hull, which improvesalgorithm design in multiple dimensions and brings significant speedup tolearning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering,then show its competitive performance and scalability over other methods onrich datasets.
arxiv-7500-275 | Convex Optimization Learning of Faithful Euclidean Distance Representations in Nonlinear Dimensionality Reduction | http://arxiv.org/pdf/1406.5736v1.pdf | author:Chao Ding, Hou-Duo Qi category:stat.ML cs.LG math.OC published:2014-06-22 summary:Classical multidimensional scaling only works well when the noisy distancesobserved in a high dimensional space can be faithfully represented by Euclideandistances in a low dimensional space. Advanced models such as Maximum VarianceUnfolding (MVU) and Minimum Volume Embedding (MVE) use Semi-DefiniteProgramming (SDP) to reconstruct such faithful representations. While those SDPmodels are capable of producing high quality configuration numerically, theysuffer two major drawbacks. One is that there exist no theoretically guaranteedbounds on the quality of the configuration. The other is that they are slow incomputation when the data points are beyond moderate size. In this paper, wepropose a convex optimization model of Euclidean distance matrices. Weestablish a non-asymptotic error bound for the random graph model withsub-Gaussian noise, and prove that our model produces a matrix estimator ofhigh accuracy when the order of the uniform sample size is roughly the degreeof freedom of a low-rank matrix up to a logarithmic factor. Our resultspartially explain why MVU and MVE often work well. Moreover, we develop a fastinexact accelerated proximal gradient method. Numerical experiments show thatthe model can produce configurations of high quality on large data points thatthe SDP approach would struggle to cope with.
arxiv-7500-276 | Recovery of Images with Missing Pixels using a Gradient Compressive Sensing Algorithm | http://arxiv.org/pdf/1407.3695v1.pdf | author:Isidora Stanković category:cs.CV published:2014-06-22 summary:This paper investigates the possibility of reconstruction of imagesconsidering that they are sparse in the DCT transformation domain. Twoapproaches are considered. One when the image is pre-processed in the DCTdomain, using 8x8 blocks. The image is made sparse by setting the smallest DCTcoefficients to zero. In the other case the original image is consideredwithout pre-processing, assuming the sparsity as intrinsic property of theanalyzed image. A gradient based algorithm is used to recover a large number ofmissing pixels in the image. The case of a salt-and-paper noise affecting alarge number of pixels is easily reduced to the case of missing pixels andconsidered within the same framework. The reconstruction of images affectedwith salt-and-paper impulsive is compared with the images filtered using amedian filter. The same algorithm can be used considering transformation of thewhole image. Reconstructions of black and white and colour images areconsidered.
arxiv-7500-277 | Natural Color Image Enhancement based on Modified Multiscale Retinex Algorithm and Performance Evaluation usingWavelet Energy | http://arxiv.org/pdf/1406.5710v1.pdf | author:M. C Hanumantharaju, M. Ravishankar, D. R Rameshbabu category:cs.CV 68U10 I.4.3 published:2014-06-22 summary:This paper presents a new color image enhancement technique based on modifiedMultiScale Retinex(MSR) algorithm and visual quality of the enhanced images areevaluated using a new metric, namely, wavelet energy. The color imageenhancement is achieved by down sampling the value component of HSV color spaceconverted image into three scales (normal, medium and fine) following thecontrast stretching operation. These down sampled value components are enhancedusing the MSR algorithm. The value component is reconstructed by averaging eachpixels of the lower scale image with that of the upper scale image subsequentto up sampling the lower scale image. This process replaces dark pixel by theaverage pixels of both the lower scale and upper scale, while retaining thebright pixels. The quality of the reconstructed images in the proposed methodis found to be good and far better then the other researchers method. Theperformance of the proposed scheme is evaluated using new wavelet domain basedassessment criterion, referred as wavelet energy. This scheme computes theenergy of both original and enhanced image in wavelet domain. The number ofedge details as well as wavelet energy is less in a poor quality image comparedwith naturally enhanced image. Experimental results presented confirms that theproposed wavelet energy based color image quality assessment techniqueefficiently characterizes both the local and global details of enhanced image.
arxiv-7500-278 | Constant Factor Approximation for Balanced Cut in the PIE model | http://arxiv.org/pdf/1406.5665v1.pdf | author:Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan category:cs.DS cs.LG published:2014-06-22 summary:We propose and study a new semi-random semi-adversarial model for BalancedCut, a planted model with permutation-invariant random edges (PIE). Our modelis much more general than planted models considered previously. Consider a setof vertices V partitioned into two clusters $L$ and $R$ of equal size. Let $G$be an arbitrary graph on $V$ with no edges between $L$ and $R$. Let$E_{random}$ be a set of edges sampled from an arbitrary permutation-invariantdistribution (a distribution that is invariant under permutation of vertices in$L$ and in $R$). Then we say that $G + E_{random}$ is a graph withpermutation-invariant random edges. We present an approximation algorithm for the Balanced Cut problem that findsa balanced cut of cost $O(E_{random}) + n \text{polylog}(n)$ in this model.In the regime when $E_{random} = \Omega(n \text{polylog}(n))$, this is aconstant factor approximation with respect to the cost of the planted cut.
arxiv-7500-279 | Deep Fragment Embeddings for Bidirectional Image Sentence Mapping | http://arxiv.org/pdf/1406.5679v1.pdf | author:Andrej Karpathy, Armand Joulin, Li Fei-Fei category:cs.CV cs.CL cs.LG published:2014-06-22 summary:We introduce a model for bidirectional retrieval of images and sentencesthrough a multi-modal embedding of visual and natural language data. Unlikeprevious models that directly map images or sentences into a common embeddingspace, our model works on a finer level and embeds fragments of images(objects) and fragments of sentences (typed dependency tree relations) into acommon space. In addition to a ranking objective seen in previous work, thisallows us to add a new fragment alignment objective that learns to directlyassociate these fragments across modalities. Extensive experimental evaluationshows that reasoning on both the global level of images and sentences and thefiner level of their respective fragments significantly improves performance onimage-sentence retrieval tasks. Additionally, our model provides interpretablepredictions since the inferred inter-modal fragment alignment is explicit.
arxiv-7500-280 | Environmental Sensing by Wearable Device for Indoor Activity and Location Estimation | http://arxiv.org/pdf/1406.5765v1.pdf | author:Ming Jin, Han Zou, Kevin Weekly, Ruoxi Jia, Alexandre M. Bayen, Costas J. Spanos category:cs.HC stat.ML published:2014-06-22 summary:We present results from a set of experiments in this pilot study toinvestigate the causal influence of user activity on various environmentalparameters monitored by occupant carried multi-purpose sensors. Hypotheses withrespect to each type of measurements are verified, including temperature,humidity, and light level collected during eight typical activities: sitting inlab / cubicle, indoor walking / running, resting after physical activity,climbing stairs, taking elevators, and outdoor walking. Our main contributionis the development of features for activity and location recognition based onenvironmental measurements, which exploit location- and activity-specificcharacteristics and capture the trends resulted from the underlyingphysiological process. The features are statistically shown to have goodseparability and are also information-rich. Fusing environmental sensingtogether with acceleration is shown to achieve classification accuracy as highas 99.13%. For building applications, this study motivates a sensor fusionparadigm for learning individualized activity, location, and environmentalpreferences for energy management and user comfort.
arxiv-7500-281 | Correlation Clustering with Noisy Partial Information | http://arxiv.org/pdf/1406.5667v2.pdf | author:Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan category:cs.DS cs.LG published:2014-06-22 summary:In this paper, we propose and study a semi-random model for the CorrelationClustering problem on arbitrary graphs G. We give two approximation algorithmsfor Correlation Clustering instances from this model. The first algorithm findsa solution of value $(1+ \delta) optcost + O_{\delta}(n\log^3 n)$ with highprobability, where $optcost$ is the value of the optimal solution (for every$\delta > 0$). The second algorithm finds the ground truth clustering with anarbitrarily small classification error $\eta$ (under some additionalassumptions on the instance).
arxiv-7500-282 | 3D ShapeNets: A Deep Representation for Volumetric Shapes | http://arxiv.org/pdf/1406.5670v3.pdf | author:Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao category:cs.CV published:2014-06-22 summary:3D shape is a crucial but heavily underutilized cue in today's computervision systems, mostly due to the lack of a good generic shape representation.With the recent availability of inexpensive 2.5D depth sensors (e.g. MicrosoftKinect), it is becoming increasingly important to have a powerful 3D shaperepresentation in the loop. Apart from category recognition, recovering full 3Dshapes from view-based 2.5D depth maps is also a critical part of visualunderstanding. To this end, we propose to represent a geometric 3D shape as aprobability distribution of binary variables on a 3D voxel grid, using aConvolutional Deep Belief Network. Our model, 3D ShapeNets, learns thedistribution of complex 3D shapes across different object categories andarbitrary poses from raw CAD data, and discovers hierarchical compositionalpart representations automatically. It naturally supports joint objectrecognition and shape completion from 2.5D depth maps, and it enables activeobject recognition through view planning. To train our 3D deep learning model,we construct ModelNet -- a large-scale 3D CAD model dataset. Extensiveexperiments show that our 3D deep representation enables significantperformance improvement over the-state-of-the-arts in a variety of tasks.
arxiv-7500-283 | SPSD Matrix Approximation vis Column Selection: Theories, Algorithms, and Extensions | http://arxiv.org/pdf/1406.5675v6.pdf | author:Shusen Wang, Luo Luo, Zhihua Zhang category:cs.LG published:2014-06-22 summary:Symmetric positive semidefinite (SPSD) matrix approximation is an importantproblem with applications in kernel methods. However, existing SPSD matrixapproximation methods such as the Nystr\"om method only have weak error bounds.In this paper we conduct in-depth studies of an SPSD matrix approximation modeland establish strong relative-error bounds. We call it the prototype model forit has more efficient and effective extensions, and some of its extensions havehigh scalability. Though the prototype model itself is not suitable forlarge-scale data, it is still useful to study its properties, on which theanalysis of its extensions relies. This paper offers novel theoretical analysis, efficient algorithms, and ahighly accurate extension. First, we establish a lower error bound for theprototype model and improve the error bound of an existing column selectionalgorithm to match the lower bound. In this way, we obtain the first optimalcolumn selection algorithm for the prototype model. We also prove that theprototype model is exact under certain conditions. Second, we develop a simplecolumn selection algorithm with a provable error bound. Third, we propose aso-called spectral shifting model to make the approximation more accurate whenthe eigenvalues of the matrix decay slowly, and the improvement istheoretically quantified. The spectral shifting method can also be applied toimprove other SPSD matrix approximation models.
arxiv-7500-284 | A CNL for Contract-Oriented Diagrams | http://arxiv.org/pdf/1406.5691v1.pdf | author:John J. Camilleri, Gabriele Paganelli, Gerardo Schneider category:cs.CL cs.FL published:2014-06-22 summary:We present a first step towards a framework for defining and manipulatingnormative documents or contracts described as Contract-Oriented (C-O) Diagrams.These diagrams provide a visual representation for such texts, giving thepossibility to express a signatory's obligations, permissions and prohibitions,with or without timing constraints, as well as the penalties resulting from thenon-fulfilment of a contract. This work presents a CNL for verbalising C-ODiagrams, a web-based tool allowing editing in this CNL, and another forvisualising and manipulating the diagrams interactively. We then show how theseproof-of-concept tools can be used by applying them to a small example.
arxiv-7500-285 | On the Maximum Entropy Property of the First-Order Stable Spline Kernel and its Implications | http://arxiv.org/pdf/1406.5706v2.pdf | author:Francesca Paola Carli category:math.ST cs.LG stat.ML stat.TH published:2014-06-22 summary:A new nonparametric approach for system identification has been recentlyproposed where the impulse response is seen as the realization of a zero--meanGaussian process whose covariance, the so--called stable spline kernel,guarantees that the impulse response is almost surely stable. Maximum entropyproperties of the stable spline kernel have been pointed out in the literature.In this paper we provide an independent proof that relies on the theory ofmatrix extension problems in the graphical model literature and leads to aclosed form expression for the inverse of the first order stable spline kernelas well as to a new factorization in the form $UWU^\top$ with $U$ uppertriangular and $W$ diagonal. Interestingly, all first--order stable splinekernels share the same factor $U$ and $W$ admits a closed form representationin terms of the kernel hyperparameter, making the factorization computationallyinexpensive. Maximum likelihood properties of the stable spline kernel are alsohighlighted. These results can be applied both to improve the stability and toreduce the computational complexity associated with the computation of stablespline estimators.
arxiv-7500-286 | CNN: Single-label to Multi-label | http://arxiv.org/pdf/1406.5726v3.pdf | author:Yunchao Wei, Wei Xia, Junshi Huang, Bingbing Ni, Jian Dong, Yao Zhao, Shuicheng Yan category:cs.CV published:2014-06-22 summary:Convolutional Neural Network (CNN) has demonstrated promising performance insingle-label image classification tasks. However, how CNN best copes withmulti-label images still remains an open problem, mainly due to the complexunderlying object layouts and insufficient multi-label training images. In thiswork, we propose a flexible deep CNN infrastructure, calledHypotheses-CNN-Pooling (HCP), where an arbitrary number of object segmenthypotheses are taken as the inputs, then a shared CNN is connected with eachhypothesis, and finally the CNN output results from different hypotheses areaggregated with max pooling to produce the ultimate multi-label predictions.Some unique characteristics of this flexible deep CNN infrastructure include:1) no ground truth bounding box information is required for training; 2) thewhole HCP infrastructure is robust to possibly noisy and/or redundanthypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN maybe well pre-trained with a large-scale single-label image dataset, e.g.ImageNet; and 5) it may naturally output multi-label prediction results.Experimental results on Pascal VOC2007 and VOC2012 multi-label image datasetswell demonstrate the superiority of the proposed HCP infrastructure over otherstate-of-the-arts. In particular, the mAP reaches 84.2% by HCP only and 90.3%after the fusion with our complementary result in [47] based on hand-craftedfeatures on the VOC2012 dataset, which significantly outperforms thestate-of-the-arts with a large margin of more than 7%.
arxiv-7500-287 | Factors of Transferability for a Generic ConvNet Representation | http://arxiv.org/pdf/1406.5774v3.pdf | author:Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, Stefan Carlsson category:cs.CV published:2014-06-22 summary:Evidence is mounting that Convolutional Networks (ConvNets) are the mosteffective representation learning method for visual recognition tasks. In thecommon scenario, a ConvNet is trained on a large labeled dataset (source) andthe feed-forward units activation of the trained network, at a certain layer ofthe network, is used as a generic representation of an input image for a taskwith relatively smaller training set (target). Recent studies have shown thisform of representation transfer to be suitable for a wide range of targetvisual recognition tasks. This paper introduces and investigates severalfactors affecting the transferability of such representations. It includesparameters for training of the source ConvNet such as its architecture,distribution of the training data, etc. and also the parameters of featureextraction such as layer of the trained ConvNet, dimensionality reduction, etc.Then, by optimizing these factors, we show that significant improvements can beachieved on various (17) visual recognition tasks. We further show that thesevisual recognition tasks can be categorically ordered based on their distancefrom the source task such that a correlation between the performance of tasksand their distance from the source task w.r.t. the proposed factors isobserved.
arxiv-7500-288 | On semidefinite relaxations for the block model | http://arxiv.org/pdf/1406.5647v3.pdf | author:Arash A. Amini, Elizaveta Levina category:cs.LG cs.SI stat.ML published:2014-06-21 summary:The stochastic block model (SBM) is a popular tool for community detection innetworks, but fitting it by maximum likelihood (MLE) involves a computationallyinfeasible optimization problem. We propose a new semidefinite programming(SDP) solution to the problem of fitting the SBM, derived as a relaxation ofthe MLE. We put ours and previously proposed SDPs in a unified framework, asrelaxations of the MLE over various sub-classes of the SBM, revealing aconnection to sparse PCA. Our main relaxation, which we call SDP-1, is tighterthan other recently proposed SDP relaxations, and thus previously establishedtheoretical guarantees carry over. However, we show that SDP-1 exactly recoverstrue communities over a wider class of SBMs than those covered by currentresults. In particular, the assumption of strong assortativity of the SBM,implicit in consistency conditions for previously proposed SDPs, can be relaxedto weak assortativity for our approach, thus significantly broadening the classof SBMs covered by the consistency results. We also show that strongassortativity is indeed a necessary condition for exact recovery for previouslyproposed SDP approaches and not an artifact of the proofs. Our analysis of SDPsis based on primal-dual witness constructions, which provides some insight intothe nature of the solutions of various SDPs. We show how to combine featuresfrom SDP-1 and already available SDPs to achieve the most flexibility in termsof both assortativity and block-size constraints, as our relaxation has thetendency to produce communities of similar sizes. This tendency makes it theideal tool for fitting network histograms, a method gaining popularity in thegraphon estimation literature, as we illustrate on an example of a socialnetworks of dolphins. We also provide empirical evidence that SDPs outperformspectral methods for fitting SBMs with a large number of blocks.
arxiv-7500-289 | From conformal to probabilistic prediction | http://arxiv.org/pdf/1406.5600v1.pdf | author:Vladimir Vovk, Ivan Petej, Valentina Fedorova category:cs.LG 68T10 published:2014-06-21 summary:This paper proposes a new method of probabilistic prediction, which is basedon conformal prediction. The method is applied to the standard USPS data setand gives encouraging results.
arxiv-7500-290 | Graphical structure of conditional independencies in determinantal point processes | http://arxiv.org/pdf/1406.5577v2.pdf | author:Tvrtko Tadić category:math.PR math.ST stat.ML stat.TH published:2014-06-21 summary:Determinantal point process have recently been used as models in machinelearning and this has raised questions regarding the characterizations ofconditional independence. In this paper we investigate characterizations ofconditional independence. We describe some conditional independencies throughthe conditions on the kernel of a determinantal point process, and show manycan be obtained using the graph induced by a kernel of the $L$-ensemble.
arxiv-7500-291 | PAC-Bayes Analysis of Multi-view Learning | http://arxiv.org/pdf/1406.5614v1.pdf | author:Shiliang Sun, John Shawe-Taylor category:cs.LG cs.AI stat.ML published:2014-06-21 summary:This paper presents four PAC-Bayes bounds to analyse the generalisationperformance of multi-view classifiers. These bounds adopt data dependentGaussian priors which emphasize the classifiers with high view agreements. Thecentre of the prior for the first two bounds is the origin, while the centre ofthe prior for the last two bounds is given by a data dependent vector. Anotherimportant ingredient to obtain these bounds is two derived logarithmicdeterminant inequalities whose difference lies at whether the dimensionality ofdata is involved. We evaluate the multi-view PAC-Bayes bounds on benchmark datawith preliminary experimental results indicating their usefulness.
arxiv-7500-292 | An Open Source Pattern Recognition Toolbox for MATLAB | http://arxiv.org/pdf/1406.5565v1.pdf | author:Kenneth D. Morton Jr., Peter Torrione, Leslie Collins, Sam Keene category:stat.ML cs.CV cs.LG cs.MS published:2014-06-21 summary:Pattern recognition and machine learning are becoming integral parts ofalgorithms in a wide range of applications. Different algorithms and approachesfor machine learning include different tradeoffs between performance andcomputation, so during algorithm development it is often necessary to explore avariety of different approaches to a given task. A toolbox with a unifiedframework across multiple pattern recognition techniques enables algorithmdevelopers the ability to rapidly evaluate different choices prior todeployment. MATLAB is a widely used environment for algorithm development andprototyping, and although several MATLAB toolboxes for pattern recognition arecurrently available these are either incomplete, expensive, or restrictivelylicensed. In this work we describe a MATLAB toolbox for pattern recognition andmachine learning known as the PRT (Pattern Recognition Toolbox), licensed underthe permissive MIT license. The PRT includes many popular techniques for datapreprocessing, supervised learning, clustering, regression and featureselection, as well as a methodology for combining these components using asimple, uniform syntax. The resulting algorithms can be evaluated usingcross-validation and a variety of scoring metrics to ensure robust performancewhen the algorithm is deployed. This paper presents an overview of the PRT aswell as an example of usage on Fisher's Iris dataset.
arxiv-7500-293 | Thermodynamic-RAM Technology Stack | http://arxiv.org/pdf/1406.5633v1.pdf | author:M. Alexander Nugent, Timothy W. Molter category:cs.NE published:2014-06-21 summary:We introduce a technology stack or specification describing the multiplelevels of abstraction and specialization needed to implement a neuromorphicprocessor based on the theory of AHaH Computing. This specific implementationis called Thermodynamic-RAM (kT-RAM). Bringing us closer to brain-like neuralcomputation, kT-RAM will provide a general-purpose adaptive hardware resourceto existing computing platforms enabling fast and low-power machine learningcapabilities that are currently hampered by the separation of memory andprocessing. The motivation for defining the technology stack is two-fold.First, explaining kT-RAM is much easier if it is broken down into smaller, moremanageable pieces. Secondly, groups interested in realizing kT-RAM can choose alevel to contribute to that matches their interest and expertise. The levels ofthe Thermodynamic-RAM technology stack include the memristor, Knowm-Synapse,AHaH Node, kT-RAM, kT-RAM instruction set, sparse spike encoding, kT-RAMemulator, and SENSE Server.
arxiv-7500-294 | Interactively Test Driving an Object Detector: Estimating Performance on Unlabeled Data | http://arxiv.org/pdf/1406.5653v1.pdf | author:Rushil Anirudh, Pavan Turaga category:cs.CV published:2014-06-21 summary:In this paper, we study the problem of `test-driving' a detector, i.e.allowing a human user to get a quick sense of how well the detector generalizesto their specific requirement. To this end, we present the first system thatestimates detector performance interactively without extensive ground truthingusing a human in the loop. We approach this as a problem of estimatingproportions and show that it is possible to make accurate inferences on theproportion of classes or groups within a large data collection by observingonly $5-10\%$ of samples from the data. In estimating the false detections (forprecision), the samples are chosen carefully such that the overallcharacteristics of the data collection are preserved. Next, inspired by its usein estimating disease propagation we apply pooled testing approaches toestimate missed detections (for recall) from the dataset. The estimates thusobtained are close to the ones obtained using ground truth, thus reducing theneed for extensive labeling which is expensive and time consuming.
arxiv-7500-295 | Minimax-optimal Inference from Partial Rankings | http://arxiv.org/pdf/1406.5638v1.pdf | author:Bruce Hajek, Sewoong Oh, Jiaming Xu category:stat.ML math.ST stat.TH published:2014-06-21 summary:This paper studies the problem of inferring a global preference based on thepartial rankings provided by many users over different subsets of itemsaccording to the Plackett-Luce model. A question of particular interest is howto optimally assign items to users for ranking and how many item assignmentsare needed to achieve a target estimation error. For a given assignment ofitems to users, we first derive an oracle lower bound of the estimation errorthat holds even for the more general Thurstone models. Then we show that theCram\'er-Rao lower bound and our upper bounds inversely depend on the spectralgap of the Laplacian of an appropriately defined comparison graph. When thesystem is allowed to choose the item assignment, we propose a random assignmentscheme. Our oracle lower bound and upper bounds imply that it isminimax-optimal up to a logarithmic factor among all assignment schemes and thelower bound can be achieved by the maximum likelihood estimator as well aspopular rank-breaking schemes that decompose partial rankings into pairwisecomparisons. The numerical experiments corroborate our theoretical findings.
arxiv-7500-296 | A survey on phrase structure learning methods for text classification | http://arxiv.org/pdf/1406.5598v1.pdf | author:Reshma Prasad, Mary Priya Sebastian category:cs.CL published:2014-06-21 summary:Text classification is a task of automatic classification of text into one ofthe predefined categories. The problem of text classification has been widelystudied in different communities like natural language processing, data miningand information retrieval. Text classification is an important constituent inmany information management tasks like topic identification, spam filtering,email routing, language identification, genre classification, readabilityassessment etc. The performance of text classification improves notably whenphrase patterns are used. The use of phrase patterns helps in capturingnon-local behaviours and thus helps in the improvement of text classificationtask. Phrase structure extraction is the first step to continue with the phrasepattern identification. In this survey, detailed study of phrase structurelearning methods have been carried out. This will enable future work in severalNLP tasks, which uses syntactic information from phrase structure like grammarcheckers, question answering, information extraction, machine translation, textclassification. The paper also provides different levels of classification anddetailed comparison of the phrase structure learning methods.
arxiv-7500-297 | Spectral Ranking using Seriation | http://arxiv.org/pdf/1406.5370v4.pdf | author:Fajwel Fogel, Alexandre d'Aspremont, Milan Vojnovic category:cs.LG cs.AI stat.ML published:2014-06-20 summary:We describe a seriation algorithm for ranking a set of items given pairwisecomparisons between these items. Intuitively, the algorithm assigns similarrankings to items that compare similarly with all others. It does so byconstructing a similarity matrix from pairwise comparisons, using seriationmethods to reorder this matrix and construct a ranking. We first show that thisspectral seriation algorithm recovers the true ranking when all pairwisecomparisons are observed and consistent with a total order. We then show thatranking reconstruction is still exact when some pairwise comparisons arecorrupted or missing, and that seriation based spectral ranking is more robustto noise than classical scoring methods. Finally, we bound the ranking errorwhen only a random subset of the comparions are observed. An additional benefitof the seriation formulation is that it allows us to solve semi-supervisedranking problems. Experiments on both synthetic and real datasets demonstratethat seriation based spectral ranking achieves competitive and in some casessuperior performance compared to classical ranking methods.
arxiv-7500-298 | Web-Scale Training for Face Identification | http://arxiv.org/pdf/1406.5266v2.pdf | author:Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf category:cs.CV published:2014-06-20 summary:Scaling machine learning methods to very large datasets has attractedconsiderable attention in recent years, thanks to easy access to ubiquitoussensing and data from the web. We study face recognition and show that threedistinct properties have surprising effects on the transferability of deepconvolutional networks (CNN): (1) The bottleneck of the network serves as animportant transfer learning regularizer, and (2) in contrast to the commonwisdom, performance saturation may exist in CNN's (as the number of trainingsamples grows); we propose a solution for alleviating this by replacing thenaive random subsampling of the training set with a bootstrapping process.Moreover, (3) we find a link between the representation norm and the ability todiscriminate in a target domain, which sheds lights on how such networksrepresent faces. Based on these discoveries, we are able to improve facerecognition accuracy on the widely used LFW benchmark, both in the verification(1:1) and identification (1:N) protocols, and directly compare, for the firsttime, with the state of the art Commercially-Off-The-Shelf system and show asizable leap in performance.
arxiv-7500-299 | Semi-Supervised Learning with Deep Generative Models | http://arxiv.org/pdf/1406.5298v2.pdf | author:Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling category:cs.LG stat.ML published:2014-06-20 summary:The ever-increasing size of modern data sets combined with the difficulty ofobtaining label information has made semi-supervised learning one of theproblems of significant practical importance in modern data analysis. Werevisit the approach to semi-supervised learning with generative models anddevelop new models that allow for effective generalisation from small labelleddata sets to large unlabelled ones. Generative approaches have thus far beeneither inflexible, inefficient or non-scalable. We show that deep generativemodels and approximate Bayesian inference exploiting recent advances invariational methods can be used to provide significant improvements, makinggenerative approaches highly competitive for semi-supervised learning.
arxiv-7500-300 | Playing with Duality: An Overview of Recent Primal-Dual Approaches for Solving Large-Scale Optimization Problems | http://arxiv.org/pdf/1406.5429v2.pdf | author:Nikos Komodakis, Jean-Christophe Pesquet category:cs.NA cs.CV cs.LG math.OC published:2014-06-20 summary:Optimization methods are at the core of many problems in signal/imageprocessing, computer vision, and machine learning. For a long time, it has beenrecognized that looking at the dual of an optimization problem may drasticallysimplify its solution. Deriving efficient strategies which jointly brings intoplay the primal and the dual problems is however a more recent idea which hasgenerated many important new contributions in the last years. These noveldevelopments are grounded on recent advances in convex analysis, discreteoptimization, parallel processing, and non-smooth optimization with emphasis onsparsity issues. In this paper, we aim at presenting the principles ofprimal-dual approaches, while giving an overview of numerical methods whichhave been proposed in different contexts. We show the benefits which can bedrawn from primal-dual algorithms both for solving large-scale convexoptimization problems and discrete ones, and we provide various applicationexamples to illustrate their usefulness.
