arxiv-7500-1 | Implicit segmentation of Kannada characters in offline handwriting recognition using hidden Markov models | http://arxiv.org/pdf/1410.4341v1.pdf | author:Manasij Venkatesh, Vikas Majjagi, Deepu Vijayasenan category:cs.LG cs.CV published:2014-10-16 summary:We describe a method for classification of handwritten Kannada charactersusing Hidden Markov Models (HMMs). Kannada script is agglutinative, wheresimple shapes are concatenated horizontally to form a character. This resultsin a large number of characters making the task of classification difficult.Character segmentation plays a significant role in reducing the number ofclasses. Explicit segmentation techniques suffer when overlapping shapes arepresent, which is common in the case of handwritten text. We use HMMs to takeadvantage of the agglutinative nature of Kannada script, which allows us toperform implicit segmentation of characters along with recognition. All theexperiments are performed on the Chars74k dataset that consists of 657handwritten characters collected across multiple users. Gradient-based featuresare extracted from individual characters and are used to train character HMMs.The use of implicit segmentation technique at the character level resulted inan improvement of around 10%. This system also outperformed an existing systemtested on the same dataset by around 16%. Analysis based on learning curvesshowed that increasing the training data could result in better accuracy.Accordingly, we collected additional data and obtained an improvement of 4%with 6 additional samples.
arxiv-7500-2 | Super-resolution method using sparse regularization for point-spread function recovery | http://arxiv.org/pdf/1410.7679v1.pdf | author:Fred Maurice Ngolè Mboula, Jean-Luc Starck, Samuel Ronayette, Koryo Okumura, Jérôme Amiaux category:cs.CV astro-ph.IM published:2014-10-16 summary:In large-scale spatial surveys, such as the forthcoming ESA Euclid mission,images may be undersampled due to the optical sensors sizes. Therefore, one mayconsider using a super-resolution (SR) method to recover aliased frequencies,prior to further analysis. This is particularly relevant for point-sourceimages, which provide direct measurements of the instrument point-spreadfunction (PSF). We introduce SPRITE, SParse Recovery of InsTrumental rEsponse,which is an SR algorithm using a sparse analysis prior. We show that such aprior provides significant improvements over existing methods, especially onlow SNR PSFs.
arxiv-7500-3 | Lasso Screening Rules via Dual Polytope Projection | http://arxiv.org/pdf/1211.3966v3.pdf | author:Jie Wang, Peter Wonka, Jieping Ye category:cs.LG stat.ML published:2012-11-16 summary:Lasso is a widely used regression technique to find sparse representations.When the dimension of the feature space and the number of samples are extremelylarge, solving the Lasso problem remains challenging. To improve the efficiencyof solving large-scale Lasso problems, El Ghaoui and his colleagues haveproposed the SAFE rules which are able to quickly identify the inactivepredictors, i.e., predictors that have $0$ components in the solution vector.Then, the inactive predictors or features can be removed from the optimizationproblem to reduce its scale. By transforming the standard Lasso to its dualform, it can be shown that the inactive predictors include the set of inactiveconstraints on the optimal dual solution. In this paper, we propose anefficient and effective screening rule via Dual Polytope Projections (DPP),which is mainly based on the uniqueness and nonexpansiveness of the optimaldual solution due to the fact that the feasible set in the dual space is aconvex and closed polytope. Moreover, we show that our screening rule can beextended to identify inactive groups in group Lasso. To the best of ourknowledge, there is currently no "exact" screening rule for group Lasso. Wehave evaluated our screening rule using synthetic and real data sets. Resultsshow that our rule is more effective in identifying inactive predictors thanexisting state-of-the-art screening rules for Lasso.
arxiv-7500-4 | Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets | http://arxiv.org/pdf/1410.4210v1.pdf | author:Jie Wang, Jieping Ye category:cs.LG published:2014-10-15 summary:Sparse-Group Lasso (SGL) has been shown to be a powerful regression techniquefor simultaneously discovering group and within-group sparse patterns by usinga combination of the $\ell_1$ and $\ell_2$ norms. However, in large-scaleapplications, the complexity of the regularizers entails great computationalchallenges. In this paper, we propose a novel Two-Layer Feature REductionmethod (TLFre) for SGL via a decomposition of its dual feasible set. Thetwo-layer reduction is able to quickly identify the inactive groups and theinactive features, respectively, which are guaranteed to be absent from thesparse representation and can be removed from the optimization. Existingfeature reduction methods are only applicable for sparse models with onesparsity-inducing regularizer. To our best knowledge, TLFre is the first onethat is capable of dealing with multiple sparsity-inducing regularizers.Moreover, TLFre has a very low computational cost and can be integrated withany existing solvers. We also develop a screening method---called DPC(DecomPosition of Convex set)---for the nonnegative Lasso problem. Experimentson both synthetic and real data sets show that TLFre and DPC improve theefficiency of SGL and nonnegative Lasso by several orders of magnitude.
arxiv-7500-5 | Learning Distributed Word Representations for Natural Logic Reasoning | http://arxiv.org/pdf/1410.4176v1.pdf | author:Samuel R. Bowman, Christopher Potts, Christopher D. Manning category:cs.CL published:2014-10-15 summary:Natural logic offers a powerful relational conception of meaning that is anatural counterpart to distributed semantic representations, which have provenvaluable in a wide range of sophisticated language tasks. However, it remainsan open question whether it is possible to train distributed representations tosupport the rich, diverse logical reasoning captured by natural logic. Weaddress this question using two neural network-based models for learningembeddings: plain neural networks and neural tensor networks. Our experimentsevaluate the models' ability to learn the basic algebra of natural logicrelations from simulated data and from the WordNet noun graph. The overallpositive results are promising for the future of learned distributedrepresentations in the applied modeling of logical semantics.
arxiv-7500-6 | Open-set Person Re-identification | http://arxiv.org/pdf/1408.0872v2.pdf | author:Shengcai Liao, Zhipeng Mo, Jianqing Zhu, Yang Hu, Stan Z. Li category:cs.CV published:2014-08-05 summary:Person re-identification is becoming a hot research for developing bothmachine learning algorithms and video surveillance applications. The task ofperson re-identification is to determine which person in a gallery has the sameidentity to a probe image. This task basically assumes that the subject of theprobe image belongs to the gallery, that is, the gallery contains this person.However, in practical applications such as searching a suspect in a video, thisassumption is usually not true. In this paper, we consider the open-set personre-identification problem, which includes two sub-tasks, detection andidentification. The detection sub-task is to determine the presence of theprobe subject in the gallery, and the identification sub-task is to determinewhich person in the gallery has the same identity as the accepted probe. Wepresent a database collected from a video surveillance setting of 6 cameras,with 200 persons and 7,413 images segmented. Based on this database, we developa benchmark protocol for evaluating the performance under the open-set personre-identification scenario. Several popular metric learning algorithms forperson re-identification have been evaluated as baselines. From the baselineperformance, we observe that the open-set person re-identification problem isstill largely unresolved, thus further attention and effort is needed.
arxiv-7500-7 | Separating the Real from the Synthetic: Minutiae Histograms as Fingerprints of Fingerprints | http://arxiv.org/pdf/1304.5409v3.pdf | author:Carsten Gottschlich, Stephan Huckemann category:cs.CV cs.AI cs.DB published:2013-04-19 summary:In this study we show that by the current state-of-the-art syntheticallygenerated fingerprints can easily be discriminated from real fingerprints. Wepropose a method based on second order extended minutiae histograms (MHs) whichcan distinguish between real and synthetic prints with very high accuracy. MHsprovide a fixed-length feature vector for a fingerprint which are invariantunder rotation and translation. This 'test of realness' can be applied tosynthetic fingerprints produced by any method. In this work, tests areconducted on the 12 publicly available databases of FVC2000, FVC2002 andFVC2004 which are well established benchmarks for evaluating the performance offingerprint recognition algorithms; 3 of these 12 databases consist ofartificial fingerprints generated by the SFinGe software. Additionally, weevaluate the discriminative performance on a database of synthetic fingerprintsgenerated by the software of Bicz versus real fingerprint images. We concludewith suggestions for the improvement of synthetic fingerprint generation.
arxiv-7500-8 | Complexity Issues and Randomization Strategies in Frank-Wolfe Algorithms for Machine Learning | http://arxiv.org/pdf/1410.4062v1.pdf | author:Emanuele Frandi, Ricardo Nanculef, Johan Suykens category:stat.ML cs.LG cs.NA math.OC published:2014-10-15 summary:Frank-Wolfe algorithms for convex minimization have recently gainedconsiderable attention from the Optimization and Machine Learning communities,as their properties make them a suitable choice in a variety of applications.However, as each iteration requires to optimize a linear model, a cleverimplementation is crucial to make such algorithms viable on large-scaledatasets. For this purpose, approximation strategies based on a random samplinghave been proposed by several researchers. In this work, we perform anexperimental study on the effectiveness of these techniques, analyze possiblealternatives and provide some guidelines based on our results.
arxiv-7500-9 | Online Tracking of Skin Colour Regions Against a Complex Background | http://arxiv.org/pdf/1410.4017v1.pdf | author:Subhadip Basu, S. Chakraborty, K. Mukherjee, S. K. Pandit category:cs.CV published:2014-10-15 summary:Online tracking of human activity against a complex background is achallenging task for many applications. In this paper, we have developed arobust technique for localizing skin colour regions from unconstrained imageframes. A simple and fast segmentation algorithm is used to train a multiplayerperceptron (MLP) for detection of skin colours. Stepper motors are synchronizedwith the MLP to track the movement of the skin colour regions.
arxiv-7500-10 | A two-pass fuzzy-geno approach to pattern classification | http://arxiv.org/pdf/1410.4013v1.pdf | author:Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2014-10-15 summary:The work presents an extension of the fuzzy approach to 2-D shape recognition[1] through refinement of initial or coarse classification decisions under atwo pass approach. In this approach, an unknown pattern is classified byrefining possible classification decisions obtained through coarseclassification of the same. To build a fuzzy model of a pattern classhorizontal and vertical fuzzy partitions on the sample images of the class areoptimized using genetic algorithm. To make coarse classification decisionsabout an unknown pattern, the fuzzy representation of the pattern is comparedwith models of all pattern classes through a specially designed similaritymeasure. Coarse classification decisions are refined in the second pass toobtain the final classification decision of the unknown pattern. To do so,optimized horizontal and vertical fuzzy partitions are again created on certainregions of the image frame, specific to each group of similar type of patternclasses. It is observed through experiments that the technique improves theoverall recognition rate from 86.2%, in the first pass, to 90.4% after thesecond pass, with 500 training samples of handwritten digits.
arxiv-7500-11 | Online interpretation of numeric sign language using 2-d skeletal model | http://arxiv.org/pdf/1410.4012v1.pdf | author:Subhadip Basu, S. Dey, K. Mukherjee, T. S. Jana category:cs.CV published:2014-10-15 summary:Gesturing is one of the natural modes of human communication. Signs producedby gestures can have a basic meaning coupled with additional information thatis layered over the basic meaning of the sign. Sign language is an importantexample of communicative gestures that are highly structured and well acceptedacross the world as a communication medium for deaf and dumb. In this paper, anonline recognition scheme is proposed to interpret the standard numeric signlanguage comprising of 10 basic hand symbols. A web camera is used to capturethe real time hand movements as input to the system. The basic meaning of thehand gesture is extracted from the input data frame by analysing the shape ofthe hand, considering its orientation, movement and location to be fixed. Theinput hand shape is processed to identify the palm structure, fingertips andtheir relative positions and the presence of the extended thumb. A2-dimensional skeletal model is generated from the acquired shape informationto represent and subsequently interpret the basic meaning of the hand gesture.
arxiv-7500-12 | Thompson sampling with the online bootstrap | http://arxiv.org/pdf/1410.4009v1.pdf | author:Dean Eckles, Maurits Kaptein category:cs.LG stat.CO stat.ML 68W27, 62L05 G.3; I.2.6 published:2014-10-15 summary:Thompson sampling provides a solution to bandit problems in which newobservations are allocated to arms with the posterior probability that an armis optimal. While sometimes easy to implement and asymptotically optimal,Thompson sampling can be computationally demanding in large scale banditproblems, and its performance is dependent on the model fit to the observeddata. We introduce bootstrap Thompson sampling (BTS), a heuristic method forsolving bandit problems which modifies Thompson sampling by replacing theposterior distribution used in Thompson sampling by a bootstrap distribution.We first explain BTS and show that the performance of BTS is competitive toThompson sampling in the well-studied Bernoulli bandit case. Subsequently, wedetail why BTS using the online bootstrap is more scalable than regularThompson sampling, and we show through simulation that BTS is more robust to amisspecified error distribution. BTS is an appealing modification of Thompsonsampling, especially when samples from the posterior are otherwise notavailable or are costly.
arxiv-7500-13 | Shape and Color Object Tracking for Real-Time Robotic Navigation | http://arxiv.org/pdf/1410.3970v1.pdf | author:Haythem Ghazouani category:cs.CV published:2014-10-15 summary:This paper presents a real-time approach for single-colored ball detectionand tracking. The approach consists of two main phases. In a first offlinecalibration phase, the intrinsic parameters of the camera and the radialdistortion are estimated, and a classification of colors is learned from asample image of colored balls. The second phase consists of four main steps:(1) color segmentation of the input image into several regions based on theoffline classification, (2) robust estimation of the circle parameters (3)refinement of the circle parameters, and (4) ball tracking. The experimentalresults showed that the approach presents a good compromise between suitabilityfor real-time navigation and robustness to occlusions, background congestionand colors interference in the scene.
arxiv-7500-14 | Probabilistic Interpretation of Linear Solvers | http://arxiv.org/pdf/1402.2058v2.pdf | author:Philipp Hennig category:math.OC cs.LG cs.NA math.NA math.PR stat.ML 90C53, 65F10 published:2014-02-10 summary:This manuscript proposes a probabilistic framework for algorithms thatiteratively solve unconstrained linear problems $Bx = b$ with positive definite$B$ for $x$. The goal is to replace the point estimates returned by existingmethods with a Gaussian posterior belief over the elements of the inverse of$B$, which can be used to estimate errors. Recent probabilistic interpretationsof the secant family of quasi-Newton optimization algorithms are extended.Combined with properties of the conjugate gradient algorithm, this leads touncertainty-calibrated methods with very limited cost overhead over conjugategradients, a self-contained novel interpretation of the quasi-Newton andconjugate gradient algorithms, and a foundation for new nonlinear optimizationmethods.
arxiv-7500-15 | A Logic-based Approach to Generatively Defined Discriminative Modeling | http://arxiv.org/pdf/1410.3935v1.pdf | author:Taisuke Sato, Keiichi Kubota, Yoshitaka Kameya category:cs.LG published:2014-10-15 summary:Conditional random fields (CRFs) are usually specified by graphical modelsbut in this paper we propose to use probabilistic logic programs and specifythem generatively. Our intension is first to provide a unified approach to CRFsfor complex modeling through the use of a Turing complete language and secondto offer a convenient way of realizing generative-discriminative pairs inmachine learning to compare generative and discriminative models and choose thebest model. We implemented our approach as the D-PRISM language by modifyingPRISM, a logic-based probabilistic modeling language for generative modeling,while exploiting its dynamic programming mechanism for efficient probabilitycomputation. We tested D-PRISM with logistic regression, a linear-chain CRF anda CRF-CFG and empirically confirmed their excellent discriminative performancecompared to their generative counterparts, i.e.\ naive Bayes, an HMM and aPCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRFversions of Bayesian network classifiers and probabilistic left-corner grammarsrespectively and easily implementable in D-PRISM. We empirically showed thatthey outperform their generative counterparts as expected.
arxiv-7500-16 | Detection of Salient Regions in Crowded Scenes | http://arxiv.org/pdf/1410.3932v1.pdf | author:Mei Kuan Lim, Chee Seng Chan, Dorothy Monekosso, Paolo Remagnino category:cs.CV published:2014-10-15 summary:The increasing number of cameras and a handful of human operators to monitorthe video inputs from hundreds of cameras leave the system ill equipped tofulfil the task of detecting anomalies. Thus, there is a dire need toautomatically detect regions that require immediate attention for a moreeffective and proactive surveillance. We propose a framework that utilises thetemporal variations in the flow field of a crowd scene to automatically detectsalient regions, while eliminating the need to have prior knowledge of thescene or training. We deem the flow fields to be a dynamic system and adopt thestability theory of dynamical systems, to determine the motion dynamics withina given area. In the context of this work, salient regions refer to areas withhigh motion dynamics, where points in a particular region are unstable.Experimental results on public, crowd scenes have shown the effectiveness ofthe proposed method in detecting salient regions which correspond to unstableflow, occlusions, bottlenecks, entries and exits.
arxiv-7500-17 | Spotting Suspicious Link Behavior with fBox: An Adversarial Perspective | http://arxiv.org/pdf/1410.3915v1.pdf | author:Neil Shah, Alex Beutel, Brian Gallagher, Christos Faloutsos category:cs.LG cs.IR cs.SI published:2014-10-15 summary:How can we detect suspicious users in large online networks? Onlinepopularity of a user or product (via follows, page-likes, etc.) can bemonetized on the premise of higher ad click-through rates or increased sales.Web services and social networks which incentivize popularity thus suffer froma major problem of fake connections from link fraudsters looking to make aquick buck. Typical methods of catching this suspicious behavior use spectraltechniques to spot large groups of often blatantly fraudulent (but sometimeshonest) users. However, small-scale, stealthy attacks may go unnoticed due tothe nature of low-rank eigenanalysis used in practice. In this work, we take an adversarial approach to find and prove claims aboutthe weaknesses of modern, state-of-the-art spectral methods and propose fBox,an algorithm designed to catch small-scale, stealth attacks that slip below theradar. Our algorithm has the following desirable properties: (a) it hastheoretical underpinnings, (b) it is shown to be highly effective on real dataand (c) it is scalable (linear on the input size). We evaluate fBox on a large,public 41.7 million node, 1.5 billion edge who-follows-whom social graph fromTwitter in 2010 and with high precision identify many suspicious accounts whichhave persisted without suspension even to this day.
arxiv-7500-18 | High Order Structure Descriptors for Scene Images | http://arxiv.org/pdf/1410.3910v1.pdf | author:Wenya Zhu, Xiankai Lu, Tao Xu, Ziyi Zhao category:cs.CV published:2014-10-15 summary:Structure information is ubiquitous in natural scene images and it plays animportant role in scene representation. In this paper, third order structurestatistics (TOSS) and fourth order structure statistics (FOSS) are exploited toencode higher order structure information. Afterwards, based on the radial andnormal slice of TOSS and FOSS, we propose the high order structure feature:third order structure feature (TOSF) and fourth order structure feature (FOSF).It is well known that scene images are well characterized by particulararrangements of their local structures, we divide the scene image into thenon-overlapping sub-regions and compute the proposed higher order structuralfeatures among them. Then a scene classification is performed by using SVMclassifier with these higher order structure features. The experimental resultsshow that higher order structure statistics can deliver image structureinformation well and its spatial envelope has strong discriminative ability.
arxiv-7500-19 | Efficient Image Categorization with Sparse Fisher Vector | http://arxiv.org/pdf/1410.3905v1.pdf | author:Xiankai Lu, Zheng Fang, Tao Xu, Haiting Zhang, Hongya Tuo category:cs.CV published:2014-10-15 summary:In object recognition, Fisher vector (FV) representation is one of thestate-of-art image representations ways at the expense of dense, highdimensional features and increased computation time. A simplification of FV isattractive, so we propose Sparse Fisher vector (SFV). By incorporating localitystrategy, we can accelerate the Fisher coding step in image categorizationwhich is implemented from a collective of local descriptors. Combining withpooling step, we explore the relationship between coding step and pooling stepto give a theoretical explanation about SFV. Experiments on benchmark datasetshave shown that SFV leads to a speedup of several-fold of magnitude compareswith FV, while maintaining the categorization performance. In addition, wedemonstrate how SFV preserves the consistence in representation of similarlocal features.
arxiv-7500-20 | Tighter Low-rank Approximation via Sampling the Leveraged Element | http://arxiv.org/pdf/1410.3886v1.pdf | author:Srinadh Bhojanapalli, Prateek Jain, Sujay Sanghavi category:cs.DS cs.LG stat.ML published:2014-10-14 summary:In this work, we propose a new randomized algorithm for computing a low-rankapproximation to a given matrix. Taking an approach different from existingliterature, our method first involves a specific biased sampling, with anelement being chosen based on the leverage scores of its row and column, andthen involves weighted alternating minimization over the factored form of theintended low-rank matrix, to minimize error only on these samples. Our methodcan leverage input sparsity, yet produce approximations in {\em spectral} (asopposed to the weaker Frobenius) norm; this combines the best aspects ofotherwise disparate current results, but with a dependence on the conditionnumber $\kappa = \sigma_1/\sigma_r$. In particular we require $O(nnz(M) +\frac{n\kappa^2 r^5}{\epsilon^2})$ computations to generate a rank-$r$approximation to $M$ in spectral norm. In contrast, the best existing methodrequires $O(nnz(M)+ \frac{nr^2}{\epsilon^4})$ time to compute an approximationin Frobenius norm. Besides the tightness in spectral norm, we have a betterdependence on the error $\epsilon$. Our method is naturally and highlyparallelizable. Our new approach enables two extensions that are interesting on their own.The first is a new method to directly compute a low-rank approximation (inefficient factored form) to the product of two given matrices; it computes asmall random set of entries of the product, and then executes weightedalternating minimization (as before) on these. The sampling strategy isdifferent because now we cannot access leverage scores of the product matrix(but instead have to work with input matrices). The second extension is animproved algorithm with smaller communication complexity for the distributedPCA setting (where each server has small set of rows of the matrix, and want tocompute low rank approximation with small amount of communication with otherservers).
arxiv-7500-21 | Reducing the Effects of Detrimental Instances | http://arxiv.org/pdf/1406.2237v2.pdf | author:Michael R. Smith, Tony Martinez category:stat.ML cs.LG published:2014-06-09 summary:Not all instances in a data set are equally beneficial for inducing a modelof the data. Some instances (such as outliers or noise) can be detrimental.However, at least initially, the instances in a data set are generallyconsidered equally in machine learning algorithms. Many current approaches forhandling noisy and detrimental instances make a binary decision about whetheran instance is detrimental or not. In this paper, we 1) extend this paradigm byweighting the instances on a continuous scale and 2) present a methodology formeasuring how detrimental an instance may be for inducing a model of the data.We call our method of identifying and weighting detrimental instances reduceddetrimental instance learning (RDIL). We examine RIDL on a set of 54 data setsand 5 learning algorithms and compare RIDL with other weighting and filteringapproaches. RDIL is especially useful for learning algorithms where everyinstance can affect the classification boundary and the training instances areconsidered individually, such as multilayer perceptrons trained withbackpropagation (MLPs). Our results also suggest that a more accurate estimateof which instances are detrimental can have a significant positive impact forhandling them.
arxiv-7500-22 | A Non-Local Structure Tensor Based Approach for Multicomponent Image Recovery Problems | http://arxiv.org/pdf/1403.5403v2.pdf | author:Giovanni Chierchia, Nelly Pustelnik, Beatrice Pesquet-Popescu, Jean-Christophe Pesquet category:cs.CV cs.NA math.OC published:2014-03-21 summary:Non-Local Total Variation (NLTV) has emerged as a useful tool in variationalmethods for image recovery problems. In this paper, we extend the NLTV-basedregularization to multicomponent images by taking advantage of the StructureTensor (ST) resulting from the gradient of a multicomponent image. The proposedapproach allows us to penalize the non-local variations, jointly for thedifferent components, through various $\ell_{1,p}$ matrix norms with $p \ge 1$.To facilitate the choice of the hyper-parameters, we adopt a constrained convexoptimization approach in which we minimize the data fidelity term subject to aconstraint involving the ST-NLTV regularization. The resulting convexoptimization problem is solved with a novel epigraphical projection method.This formulation can be efficiently implemented thanks to the flexibilityoffered by recent primal-dual proximal algorithms. Experiments are carried outfor multispectral and hyperspectral images. The results demonstrate theinterest of introducing a non-local structure tensor regularization and showthat the proposed approach leads to significant improvements in terms ofconvergence speed over current state-of-the-art methods.
arxiv-7500-23 | An exact mapping between the Variational Renormalization Group and Deep Learning | http://arxiv.org/pdf/1410.3831v1.pdf | author:Pankaj Mehta, David J. Schwab category:stat.ML cs.LG cs.NE published:2014-10-14 summary:Deep learning is a broad set of techniques that uses multiple layers ofrepresentation to automatically learn relevant features directly fromstructured data. Recently, such techniques have yielded record-breaking resultson a diverse set of difficult machine learning tasks in computer vision, speechrecognition, and natural language processing. Despite the enormous success ofdeep learning, relatively little is understood theoretically about why thesetechniques are so successful at feature learning and compression. Here, we showthat deep learning is intimately related to one of the most important andsuccessful techniques in theoretical physics, the renormalization group (RG).RG is an iterative coarse-graining scheme that allows for the extraction ofrelevant features (i.e. operators) as a physical system is examined atdifferent length scales. We construct an exact mapping from the variationalrenormalization group, first introduced by Kadanoff, and deep learningarchitectures based on Restricted Boltzmann Machines (RBMs). We illustratethese ideas using the nearest-neighbor Ising Model in one and two-dimensions.Our results suggests that deep learning algorithms may be employing ageneralized RG-like scheme to learn relevant features from data.
arxiv-7500-24 | POLYGLOT-NER: Massive Multilingual Named Entity Recognition | http://arxiv.org/pdf/1410.3791v1.pdf | author:Rami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, Steven Skiena category:cs.CL cs.LG I.2.7; I.2.6 published:2014-10-14 summary:The increasing diversity of languages used on the web introduces a new levelof complexity to Information Retrieval (IR) systems. We can no longer assumethat textual content is written in one language or even the same languagefamily. In this paper, we demonstrate how to build massive multilingualannotators with minimal human expertise and intervention. We describe a systemthat builds Named Entity Recognition (NER) annotators for 40 major languagesusing Wikipedia and Freebase. Our approach does not require NER human annotateddatasets or language specific resources like treebanks, parallel corpora, andorthographic rules. The novelty of approach lies therein - using only languageagnostic techniques, while achieving competitive performance. Our method learns distributed word representations (word embeddings) whichencode semantic and syntactic features of words in each language. Then, weautomatically generate datasets from Wikipedia link structure and Freebaseattributes. Finally, we apply two preprocessing stages (oversampling and exactsurface form matching) which do not require any linguistic expertise. Our evaluation is two fold: First, we demonstrate the system performance onhuman annotated datasets. Second, for languages where no gold-standardbenchmarks are available, we propose a new method, distant evaluation, based onstatistical machine translation.
arxiv-7500-25 | Crowd Saliency Detection via Global Similarity Structure | http://arxiv.org/pdf/1410.3756v1.pdf | author:Mei Kuan Lim, Ven Jyn Kok, Chen Change Loy, Chee Seng Chan category:cs.CV stat.ML published:2014-10-14 summary:It is common for CCTV operators to overlook inter- esting events taking placewithin the crowd due to large number of people in the crowded scene (i.e.marathon, rally). Thus, there is a dire need to automate the detection ofsalient crowd regions acquiring immediate attention for a more effective andproactive surveillance. This paper proposes a novel framework to identify andlocalize salient regions in a crowd scene, by transforming low-level featuresextracted from crowd motion field into a global similarity structure. Theglobal similarity structure representation allows the discovery of theintrinsic manifold of the motion dynamics, which could not be captured by thelow-level representation. Ranking is then performed on the global similaritystructure to identify a set of extrema. The proposed approach is unsupervisedso learning stage is eliminated. Experimental results on public datasetsdemonstrates the effectiveness of exploiting such extrema in identifyingsalient regions in various crowd scenarios that exhibit crowding, localirregular motion, and unique motion areas such as sources and sinks.
arxiv-7500-26 | Enhanced Random Forest with Image/Patch-Level Learning for Image Understanding | http://arxiv.org/pdf/1410.3752v1.pdf | author:Wai Lam Hoo, Tae-Kyun Kim, Yuru Pei, Chee Seng Chan category:cs.CV stat.ML published:2014-10-14 summary:Image understanding is an important research domain in the computer visiondue to its wide real-world applications. For an image understanding frameworkthat uses the Bag-of-Words model representation, the visual codebook is anessential part. Random forest (RF) as a tree-structure discriminative codebookhas been a popular choice. However, the performance of the RF can be degradedif the local patch labels are poorly assigned. In this paper, we tackle thisproblem by a novel way to update the RF codebook learning for a morediscriminative codebook with the introduction of the soft class labels,estimated from the pLSA model based on a feedback scheme. The feedback schemeis performed on both the image and patch levels respectively, which is incontrast to the state- of-the-art RF codebook learning that focused on eitherimage or patch level only. Experiments on 15-Scene and C-Pascal datasets hadshown the effectiveness of the proposed method in image understanding task.
arxiv-7500-27 | A Fusion Approach for Efficient Human Skin Detection | http://arxiv.org/pdf/1410.3751v1.pdf | author:Wei Ren Tan, Chee Seng Chan, Pratheepan Yogarajah, Joan Condell category:cs.CV stat.ML published:2014-10-14 summary:A reliable human skin detection method that is adaptable to different humanskin colours and illu- mination conditions is essential for better human skinsegmentation. Even though different human skin colour detection solutions havebeen successfully applied, they are prone to false skin detection and are notable to cope with the variety of human skin colours across different ethnic.Moreover, existing methods require high computational cost. In this paper, wepropose a novel human skin de- tection approach that combines a smoothed 2Dhistogram and Gaussian model, for automatic human skin detection in colourimage(s). In our approach an eye detector is used to refine the skin model fora specific person. The proposed approach reduces computational costs as notraining is required; and it improves the accuracy of skin detection despitewide variation in ethnicity and illumination. To the best of our knowledge,this is the first method to employ fusion strategy for this purpose.Qualitative and quantitative results on three standard public datasets and acomparison with state-of-the-art methods have shown the effectiveness androbustness of the proposed approach.
arxiv-7500-28 | Zero-Shot Object Recognition System based on Topic Model | http://arxiv.org/pdf/1410.3748v1.pdf | author:Wai Lam Hoo, Chee Seng Chan category:cs.CV stat.ML published:2014-10-14 summary:Object recognition systems usually require fully complete manually labeledtraining data to train the classifier. In this paper, we study the problem ofobject recognition where the training samples are missing during the classifierlearning stage, a task also known as zero-shot learning. We propose a novelzero-shot learning strategy that utilizes the topic model and hierarchicalclass concept. Our proposed method advanced where cumbersome human annotationstage (i.e. attribute-based classification) is eliminated. We achievecomparable performance with state-of-the-art algorithms in four publicdatasets: PubFig (67.09%), Cifar-100 (54.85%), Caltech-256 (52.14%), andAnimals with Attributes (49.65%) when unseen classes exist in theclassification task.
arxiv-7500-29 | Refined Particle Swarm Intelligence Method for Abrupt Motion Tracking | http://arxiv.org/pdf/1410.3744v1.pdf | author:Mei Kuan Lim, Chee Seng Chan, Dorothy Monekosso, Paolo Remagnino category:cs.CV cs.NE published:2014-10-14 summary:Conventional tracking solutions are not feasible in handling abrupt motion asthey are based on smooth motion assumption or an accurate motion model. Abruptmotion is not subject to motion continuity and smoothness. To assuage this, wedeem tracking as an optimisation problem and propose a novel abrupt motiontracker that based on swarm intelligence - the SwaTrack. Unlike existingswarm-based filtering methods, we first of all introduce an optimisedswarm-based sampling strategy to tradeoff between the exploration andexploitation of the search space in search for the optimal proposaldistribution. Secondly, we propose Dynamic Acceleration Parameters (DAP) allowon the fly tuning of the best mean and variance of the distribution forsampling. Such innovating idea of combining these strategies in an ingeniousway in the PSO framework to handle the abrupt motion, which so far no existingworks are found. Experimental results in both quantitative and qualitative hadshown the effectiveness of the proposed method in tracking abrupt motions.
arxiv-7500-30 | Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding | http://arxiv.org/pdf/1410.3726v1.pdf | author:Chern Hong Lim, Anhar Risnumawan, Chee Seng Chan category:cs.CV cs.AI cs.IR published:2014-10-14 summary:Ambiguity or uncertainty is a pervasive element of many real world decisionmaking processes. Variation in decisions is a norm in this situation when thesame problem is posed to different subjects. Psychological and metaphysicalresearch had proven that decision making by human is subjective. It isinfluenced by many factors such as experience, age, background, etc. Sceneunderstanding is one of the computer vision problems that fall into thiscategory. Conventional methods relax this problem by assuming scene images aremutually exclusive; and therefore, focus on developing different approaches toperform the binary classification tasks. In this paper, we show that sceneimages are non-mutually exclusive, and propose the Fuzzy Qualitative RankClassifier (FQRC) to tackle the aforementioned problems. The proposed FQRCprovides a ranking interpretation instead of binary decision. Evaluations interm of qualitative and quantitative using large numbers and challenging publicscene datasets have shown the effectiveness of our proposed method in modelingthe non-mutually exclusive scene images.
arxiv-7500-31 | A graph Laplacian regularization for hyperspectral data unmixing | http://arxiv.org/pdf/1410.3699v1.pdf | author:Rita Ammanouil, André Ferrari, Cédric Richard category:cs.CV published:2014-10-14 summary:This paper introduces a graph Laplacian regularization in the hyperspectralunmixing formulation. The proposed regularization relies upon the constructionof a graph representation of the hyperspectral image. Each node in the graphrepresents a pixel's spectrum, and edges connect spectrally and spatiallysimilar pixels. The proposed graph framework promotes smoothness in theestimated abundance maps and collaborative estimation between homogeneous areasof the image. The resulting convex optimization problem is solved using theAlternating Direction Method of Multipliers (ADMM). A special attention isgiven to the computational complexity of the algorithm, and Graph-cut methodsare proposed in order to reduce the computational burden. Finally, simulationsconducted on synthetic data illustrate the effectiveness of the graph Laplacianregularization with respect to other classical regularizations forhyperspectral unmixing.
arxiv-7500-32 | Sparsity Based Poisson Denoising with Dictionary Learning | http://arxiv.org/pdf/1309.4306v3.pdf | author:Raja Giryes, Michael Elad category:cs.CV stat.ML published:2013-09-17 summary:The problem of Poisson denoising appears in various imaging applications,such as low-light photography, medical imaging and microscopy. In cases of highSNR, several transformations exist so as to convert the Poisson noise into anadditive i.i.d. Gaussian noise, for which many effective algorithms areavailable. However, in a low SNR regime, these transformations aresignificantly less accurate, and a strategy that relies directly on the truenoise statistics is required. A recent work by Salmon et al. took this route,proposing a patch-based exponential image representation model based on GMM(Gaussian mixture model), leading to state-of-the-art results. In this paper,we propose to harness sparse-representation modeling to the image patches,adopting the same exponential idea. Our scheme uses a greedy pursuit withboot-strapping based stopping condition and dictionary learning within thedenoising process. The reconstruction performance of the proposed scheme iscompetitive with leading methods in high SNR, and achieving state-of-the-artresults in cases of low SNR.
arxiv-7500-33 | A stochastic behavior analysis of stochastic restricted-gradient descent algorithm in reproducing kernel Hilbert spaces | http://arxiv.org/pdf/1410.3595v1.pdf | author:Masa-aki Takizawa, Masahiro Yukawa, Cedric Richard category:cs.LG stat.ML published:2014-10-14 summary:This paper presents a stochastic behavior analysis of a kernel-basedstochastic restricted-gradient descent method. The restricted gradient gives asteepest ascent direction within the so-called dictionary subspace. Theanalysis provides the transient and steady state performance in the meansquared error criterion. It also includes stability conditions in the mean andmean-square sense. The present study is based on the analysis of the kernelnormalized least mean square (KNLMS) algorithm initially proposed by Chen etal. Simulation results validate the analysis.
arxiv-7500-34 | Direct Processing of Document Images in Compressed Domain | http://arxiv.org/pdf/1410.2959v2.pdf | author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV published:2014-10-11 summary:With the rapid increase in the volume of Big data of this digital era, faxdocuments, invoices, receipts, etc are traditionally subjected to compressionfor the efficiency of data storage and transfer. However, in order to processthese documents, they need to undergo the stage of decompression which indentsadditional computing resources. This limitation induces the motivation toresearch on the possibility of directly processing of compressed images. Inthis research paper, we summarize the research work carried out to performdifferent operations straight from run-length compressed documents withoutgoing through the stage of decompression. The different operations demonstratedare feature extraction; text-line, word and character segmentation; documentblock segmentation; and font size detection, all carried out in the compressedversion of the document. Feature extraction methods demonstrate how to extractthe conventionally defined features such as projection profile, run-histogramand entropy, directly from the compressed document data. Document segmentationinvolves the extraction of compressed segments of text-lines, words andcharacters using the vertical and horizontal projection profile features.Further an attempt is made to segment randomly a block of interest from thecompressed document and subsequently facilitate absolute and relativecharacterization of the segmented block which finds real time applications inautomatic processing of Bank Cheques, Challans, etc, in compressed domain.Finally an application to detect font size at text line level is alsoinvestigated. All the proposed algorithms are validated experimentally withsufficient data set of compressed documents.
arxiv-7500-35 | Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits | http://arxiv.org/pdf/1402.0555v2.pdf | author:Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, Robert E. Schapire category:cs.LG stat.ML published:2014-02-04 summary:We present a new algorithm for the contextual bandit learning problem, wherethe learner repeatedly takes one of $K$ actions in response to the observedcontext, and observes the reward only for that chosen action. Our methodassumes access to an oracle for solving fully supervised cost-sensitiveclassification problems and achieves the statistically optimal regret guaranteewith only $\tilde{O}(\sqrt{KT/\log N})$ oracle calls across all $T$ rounds,where $N$ is the number of policies in the policy class we compete against. Bydoing so, we obtain the most practical contextual bandit learning algorithmamongst approaches that work for general policy classes. We further conduct aproof-of-concept experiment which demonstrates the excellent computational andprediction performance of (an online variant of) our algorithm relative toseveral baselines.
arxiv-7500-36 | Testing Poisson Binomial Distributions | http://arxiv.org/pdf/1410.3386v2.pdf | author:Jayadev Acharya, Constantinos Daskalakis category:cs.DS cs.IT cs.LG math.IT published:2014-10-13 summary:A Poisson Binomial distribution over $n$ variables is the distribution of thesum of $n$ independent Bernoullis. We provide a sample near-optimal algorithmfor testing whether a distribution $P$ supported on $\{0,...,n\}$ to which wehave sample access is a Poisson Binomial distribution, or far from all PoissonBinomial distributions. The sample complexity of our algorithm is $O(n^{1/4})$to which we provide a matching lower bound. We note that our sample complexityimproves quadratically upon that of the naive "learn followed by tolerant-test"approach, while instance optimal identity testing [VV14] is not applicablesince we are looking to simultaneously test against a whole family ofdistributions.
arxiv-7500-37 | A Short Introduction to NILE | http://arxiv.org/pdf/1311.6063v4.pdf | author:Sheng Yu, Tianxi Cai category:cs.CL published:2013-11-23 summary:In this paper, we briefly introduce the Narrative Information LinearExtraction (NILE) system, a natural language processing library for clinicalnarratives. NILE is an experiment of our ideas on efficient and effectivemedical language processing. We introduce the overall design of NILE and itsmajor components, and show the performance of it in real projects.
arxiv-7500-38 | Enhanced Higgs to $τ^+τ^-$ Searches with Deep Learning | http://arxiv.org/pdf/1410.3469v1.pdf | author:Pierre Baldi, Peter Sadowski, Daniel Whiteson category:hep-ph cs.LG hep-ex published:2014-10-13 summary:The Higgs boson is thought to provide the interaction that imparts mass tothe fundamental fermions, but while measurements at the Large Hadron Collider(LHC) are consistent with this hypothesis, current analysis techniques lack thestatistical power to cross the traditional 5$\sigma$ significance barrierwithout more data. \emph{Deep learning} techniques have the potential toincrease the statistical power of this analysis by \emph{automatically}learning complex, high-level data representations. In this work, deep neuralnetworks are used to detect the decay of the Higgs to a pair of tau leptons. ABayesian optimization algorithm is used to tune the network architecture andtraining algorithm hyperparameters, resulting in a deep network of eightnon-linear processing layers that improves upon the performance of shallowclassifiers even without the use of features specifically engineered byphysicists for this application. The improvement in discovery significance isequivalent to an increase in the accumulated dataset of 25\%.
arxiv-7500-39 | Computing Topology Preservation of RBF Transformations for Landmark-Based Image Registration | http://arxiv.org/pdf/1410.3426v1.pdf | author:R. Cavoretto, A. De Rossi, H. Qiao, B. Quatember, W. Recheis, M. Mayr category:math.NA cs.CV published:2014-10-13 summary:In image registration, a proper transformation should be topology preserving.Especially for landmark-based image registration, if the displacement of onelandmark is larger enough than those of neighbourhood landmarks, topologyviolation will be occurred. This paper aim to analyse the topology preservationof some Radial Basis Functions (RBFs) which are used to model deformations inimage registration. Mat\'{e}rn functions are quite common in the statisticliterature (see, e.g. \cite{Matern86,Stein99}). In this paper, we use them tosolve the landmark-based image registration problem. We present the topologypreservation properties of RBFs in one landmark and four landmarks modelrespectively. Numerical results of three kinds of Mat\'{e}rn transformationsare compared with results of Gaussian, Wendland's, and Wu's functions.
arxiv-7500-40 | Homotopy equivalence of finite digital images | http://arxiv.org/pdf/1408.2584v2.pdf | author:Jason Haarmann, Meg P. Murphy, Casey S. Peters, P. Christopher Staecker category:math.GN cs.CG cs.CV 55P10, 68R10 I.4.m published:2014-08-11 summary:For digital images, there is an established homotopy equivalence relationwhich parallels that of classical topology. Many classical homotopy equivalenceinvariants, such as the Euler characteristic and the homology groups, do notremain invariants in the digital setting. This paper develops a numericaldigital homotopy invariant and begins to catalog all possible connected digitalimages on a small number of points, up to homotopy equivalence.
arxiv-7500-41 | Fast Multilevel Support Vector Machines | http://arxiv.org/pdf/1410.3348v1.pdf | author:Talayeh Razzaghi, Ilya Safro category:stat.ML cs.LG published:2014-10-13 summary:Solving different types of optimization models (including parameters fitting)for support vector machines on large-scale training data is often an expensivecomputational task. This paper proposes a multilevel algorithmic framework thatscales efficiently to very large data sets. Instead of solving the wholetraining set in one optimization process, the support vectors are obtained andgradually refined at multiple levels of coarseness of the data. The proposedframework includes: (a) construction of hierarchy of large-scale data coarserepresentations, and (b) a local processing of updating the hyperplanethroughout this hierarchy. Our multilevel framework substantially improves thecomputational time without loosing the quality of classifiers. The algorithmsare demonstrated for both regular and weighted support vector machines.Experimental results are presented for balanced and imbalanced classificationproblems. Quality improvement on several imbalanced data sets has beenobserved.
arxiv-7500-42 | Mining Block I/O Traces for Cache Preloading with Sparse Temporal Non-parametric Mixture of Multivariate Poisson | http://arxiv.org/pdf/1410.3463v1.pdf | author:Lavanya Sita Tekumalla, Chiranjib Bhattacharyya category:cs.OS cs.LG cs.SY published:2014-10-13 summary:Existing caching strategies, in the storage domain, though well suited toexploit short range spatio-temporal patterns, are unable to leverage long-rangemotifs for improving hitrates. Motivated by this, we investigate novel Bayesiannon-parametric modeling(BNP) techniques for count vectors, to capture longrange correlations for cache preloading, by mining Block I/O traces. Suchtraces comprise of a sequence of memory accesses that can be aggregated intohigh-dimensional sparse correlated count vector sequences. While there are several state of the art BNP algorithms for clustering andtheir temporal extensions for prediction, there has been no work on exploringthese for correlated count vectors. Our first contribution addresses this gapby proposing a DP based mixture model of Multivariate Poisson (DP-MMVP) and itstemporal extension(HMM-DP-MMVP) that captures the full covariance structure ofmultivariate count data. However, modeling full covariance structure for countvectors is computationally expensive, particularly for high dimensional data.Hence, we exploit sparsity in our count vectors, and as our main contribution,introduce the Sparse DP mixture of multivariate Poisson(Sparse-DP-MMVP),generalizing our DP-MMVP mixture model, also leading to more efficientinference. We then discuss a temporal extension to our model for cachepreloading. We take the first step towards mining historical data, to capture long rangepatterns in storage traces for cache preloading. Experimentally, we show adramatic improvement in hitrates on benchmark traces and lay the groundwork forfurther research in storage domain to reduce latencies using data miningtechniques to capture long range motifs.
arxiv-7500-43 | Neural Hypernetwork Approach for Pulmonary Embolism diagnosis | http://arxiv.org/pdf/1409.5743v2.pdf | author:Matteo Rucco, David M. S. Rodrigues, Emanuela Merelli, Jeffrey H. Johnson, Lorenzo Falsetti, Cinzia Nitti, Aldo Salvi category:physics.med-ph cs.LG q-bio.QM stat.ML published:2014-09-19 summary:This work introduces an integrative approach based on Q-analysis with machinelearning. The new approach, called Neural Hypernetwork, has been applied to acase study of pulmonary embolism diagnosis. The objective of the application ofneural hyper-network to pulmonary embolism (PE) is to improve diagnose forreducing the number of CT-angiography needed. Hypernetworks, based ontopological simplicial complex, generalize the concept of two-relation tomany-body relation. Furthermore, Hypernetworks provide a significantgeneralization of network theory, enabling the integration of relationalstructure, logic and analytic dynamics. Another important results is thatQ-analysis stays close to the data, while other approaches manipulate data,projecting them into metric spaces or applying some filtering functions tohighlight the intrinsic relations. A pulmonary embolism (PE) is a blockage ofthe main artery of the lung or one of its branches, frequently fatal. Our studyuses data on 28 diagnostic features of 1,427 people considered to be at risk ofPE. The resulting neural hypernetwork correctly recognized 94% of thosedeveloping a PE. This is better than previous results that have been obtainedwith other methods (statistical selection of features, partial least squaresregression, topological data analysis in a metric space).
arxiv-7500-44 | Propagation Kernels | http://arxiv.org/pdf/1410.3314v1.pdf | author:Marion Neumann, Roman Garnett, Christian Bauckhage, Kristian Kersting category:stat.ML cs.LG published:2014-10-13 summary:We introduce propagation kernels, a general graph-kernel framework forefficiently measuring the similarity of structured data. Propagation kernelsare based on monitoring how information spreads through a set of given graphs.They leverage early-stage distributions from propagation schemes such as randomwalks to capture structural information encoded in node labels, attributes, andedge information. This has two benefits. First, off-the-shelf propagationschemes can be used to naturally construct kernels for many graph types,including labeled, partially labeled, unlabeled, directed, and attributedgraphs. Second, by leveraging existing efficient and informative propagationschemes, propagation kernels can be considerably faster than state-of-the-artapproaches without sacrificing predictive performance. We will also show thatif the graphs at hand have a regular structure, for instance when modelingimage or video data, one can exploit this regularity to scale the kernelcomputation to large databases of graphs with thousands of nodes. We supportour contributions by exhaustive experiments on a number of real-world graphsfrom a variety of application domains.
arxiv-7500-45 | Tag Relevance Fusion for Social Image Retrieval | http://arxiv.org/pdf/1410.3462v1.pdf | author:Xirong Li category:cs.IR cs.CV H.3.3 published:2014-10-13 summary:Due to the subjective nature of social tagging, measuring the relevance ofsocial tags with respect to the visual content is crucial for retrieving theincreasing amounts of social-networked images. Witnessing the limit of a singlemeasurement of tag relevance, we introduce in this paper tag relevance fusionas an extension to methods for tag relevance estimation. We present asystematic study, covering tag relevance fusion in early and late stages, andin supervised and unsupervised settings. Experiments on a large present-daybenchmark set show that tag relevance fusion leads to better image retrieval.Moreover, unsupervised tag relevance fusion is found to be practically aseffective as supervised tag relevance fusion, but without the need of anytraining efforts. This finding suggests the potential of tag relevance fusionfor real-world deployment.
arxiv-7500-46 | Sentiment Analysis based on User Tag for Traditional Chinese Medicine in Weibo | http://arxiv.org/pdf/1410.3460v1.pdf | author:Junhui Shen, Peiyan Zhu, Rui Fan, Wei Tan category:cs.CL cs.SI published:2014-10-13 summary:With the acceptance of Western culture and science, Traditional ChineseMedicine (TCM) has become a controversial issue in China. So, it's important tostudy the public's sentiment and opinion on TCM. The rapid development ofonline social network, such as twitter, make it convenient and efficient tosample hundreds of millions of people for the aforementioned sentiment study.To the best of our knowledge, the present work is the first attempt thatapplies sentiment analysis to the domain of TCM on Sina Weibo (a twitter-likemicroblogging service in China). In our work, firstly we collect tweets topicabout TCM from Sina Weibo, and label the tweets as supporting TCM and opposingTCM automatically based on user tag. Then, a support vector machine classifierhas been built to predict the sentiment of TCM tweets without labels. Finally,we present a method to adjust the classifier result. The performance ofF-measure attained with our method is 97%.
arxiv-7500-47 | Markov Random Fields and Mass Spectra Discrimination | http://arxiv.org/pdf/1410.3234v1.pdf | author:Ao Kong, Robert Azencott category:stat.ML stat.AP stat.CO 62P10, 68T10 published:2014-10-13 summary:For mass spectra acquired from cancer patients by MALDI or SELDI techniques,automated discrimination between cancer types or stages has often beenimplemented by machine learnings. These techniques typically generate"black-box" classifiers, which are difficult to interpret biologically. Wedevelop new and efficient signature discovery algorithms leading tointerpretable signatures combining the discriminating power of explicitlyselected small groups of biomarkers, identified by their m/z ratios. Ourapproach is based on rigorous stochastic modeling of "homogeneous" datasets ofmass spectra by a versatile class of parameterized Markov Random Fields. Wepresent detailed algorithms validated by precise theoretical results. We alsooutline the successful tests of our approach to generate efficient explicitsignatures for six benchmark discrimination tasks, based on mass spectraacquired from colorectal cancer patients, as well as from ovarian cancerpatients.
arxiv-7500-48 | PAC-Bayesian AUC classification and scoring | http://arxiv.org/pdf/1410.1771v2.pdf | author:James Ridgway, Pierre Alquier, Nicolas Chopin, Feng Liang category:stat.ML stat.CO 62H30 published:2014-10-07 summary:We develop a scoring and classification procedure based on the PAC-Bayesianapproach and the AUC (Area Under Curve) criterion. We focus initially on theclass of linear score functions. We derive PAC-Bayesian non-asymptotic boundsfor two types of prior for the score parameters: a Gaussian prior, and aspike-and-slab prior; the latter makes it possible to perform featureselection. One important advantage of our approach is that it is amenable topowerful Bayesian computational tools. We derive in particular a SequentialMonte Carlo algorithm, as an efficient method which may be used as a goldstandard, and an Expectation-Propagation algorithm, as a much faster butapproximate method. We also extend our method to a class of non-linear scorefunctions, essentially leading to a nonparametric procedure, by considering aGaussian process prior.
arxiv-7500-49 | Learning without Concentration for General Loss Functions | http://arxiv.org/pdf/1410.3192v1.pdf | author:Shahar Mendelson category:stat.ML K.3.2 published:2014-10-13 summary:We study prediction and estimation problems using empirical riskminimization, relative to a general convex loss function. We obtain sharp errorrates even when concentration is false or is very restricted, for example, inheavy-tailed scenarios. Our results show that the error rate depends on twoparameters: one captures the intrinsic complexity of the class, and essentiallyleads to the error rate in a noise-free (or realizable) problem; the othermeasures interactions between class members the target and the loss, and isdominant when the problem is far from realizable. We also explain how one maydeal with outliers by choosing the loss in a way that is calibrated to theintrinsic complexity of the class and to the noise-level of the problem (thelatter is measured by the distance between the target and the class).
arxiv-7500-50 | Multi-Scale Local Shape Analysis and Feature Selection in Machine Learning Applications | http://arxiv.org/pdf/1410.3169v1.pdf | author:Paul Bendich, Ellen Gasparovic, John Harer, Rauf Izmailov, Linda Ness category:cs.CG cs.LG math.AT stat.ML published:2014-10-13 summary:We introduce a method called multi-scale local shape analysis, or MLSA, forextracting features that describe the local structure of points within adataset. The method uses both geometric and topological features at multiplelevels of granularity to capture diverse types of local information forsubsequent machine learning algorithms operating on the dataset. Usingsynthetic and real dataset examples, we demonstrate significant performanceimprovement of classification algorithms constructed for these datasets withcorrespondingly augmented features.
arxiv-7500-51 | Graph matching: relax or not? | http://arxiv.org/pdf/1401.7623v5.pdf | author:Yonathan Aflalo, Alex Bronstein, Ron Kimmel category:cs.DS cs.CG cs.CV math.OC published:2014-01-29 summary:We consider the problem of exact and inexact matching of weighted undirectedgraphs, in which a bijective correspondence is sought to minimize a quadraticweight disagreement. This computationally challenging problem is often relaxedas a convex quadratic program, in which the space of permutations is replacedby the space of doubly-stochastic matrices. However, the applicability of sucha relaxation is poorly understood. We define a broad class of friendly graphscharacterized by an easily verifiable spectral property. We prove that forfriendly graphs, the convex relaxation is guaranteed to find the exactisomorphism or certify its inexistence. This result is further extended toapproximately isomorphic graphs, for which we develop an explicit bound on theamount of weight disagreement under which the relaxation is guaranteed to findthe globally optimal approximate isomorphism. We also show that in many cases,the graph matching problem can be further harmlessly relaxed to a convexquadratic program with only n separable linear equality constraints, which issubstantially more efficient than the standard relaxation involving 2n equalityand n^2 inequality constraints. Finally, we show that our results are stillvalid for unfriendly graphs if additional information in the form of seeds orattributes is allowed, with the latter satisfying an easy to verify spectralcharacteristic.
arxiv-7500-52 | Sparse Matrix-based Random Projection for Classification | http://arxiv.org/pdf/1312.3522v3.pdf | author:Weizhi Lu, Weiyu Li, Kidiyo Kpalma, Joseph Ronsin category:cs.LG cs.CV stat.ML published:2013-12-12 summary:As a typical dimensionality reduction technique, random projection can besimply implemented with linear projection, while maintaining the pairwisedistances of high-dimensional data with high probability. Considering thistechnique is mainly exploited for the task of classification, this paper isdeveloped to study the construction of random matrix from the viewpoint offeature selection, rather than of traditional distance preservation. Thisyields a somewhat surprising theoretical result, that is, the sparse randommatrix with exactly one nonzero element per column, can present better featureselection performance than other more dense matrices, if the projectiondimension is sufficiently large (namely, not much smaller than the number offeature elements); otherwise, it will perform comparably to others. For randomprojection, this theoretical result implies considerable improvement on bothcomplexity and performance, which is widely confirmed with the classificationexperiments on both synthetic data and real data.
arxiv-7500-53 | Machine Learning Techniques in Cognitive Radio Networks | http://arxiv.org/pdf/1410.3145v1.pdf | author:Peter Hossain, Adaulfo Komisarczuk, Garin Pawetczak, Sarah Van Dijk, Isabella Axelsen category:cs.LG cs.NI published:2014-10-12 summary:Cognitive radio is an intelligent radio that can be programmed and configureddynamically to fully use the frequency resources that are not used by licensedusers. It defines the radio devices that are capable of learning and adaptingto their transmission to the external radio environment, which means it hassome kind of intelligence for monitoring the radio environment, learning theenvironment and make smart decisions. In this paper, we are reviewing someexamples of the usage of machine learning techniques in cognitive radionetworks for implementing the intelligent radio.
arxiv-7500-54 | Hierarchical models for neural population dynamics in the presence of non-stationarity | http://arxiv.org/pdf/1410.3111v1.pdf | author:Mijung Park, Jakob H. Macke category:stat.ML q-bio.NC published:2014-10-12 summary:Neural population activity often exhibits rich variability and temporalstructure. This variability is thought to arise from single-neuronstochasticity, neural dynamics on short time-scales, as well as frommodulations of neural firing properties on long time-scales, often referred toas "non-stationarity". To better understand the nature of co-variability inneural circuits and their impact on cortical information processing, we needstatistical models that are able to capture multiple sources of variability ondifferent time-scales. Here, we introduce a hierarchical statistical model ofneural population activity which models both neural population dynamics as wellas inter-trial modulations in firing rates. In addition, we extend the model toallow us to capture non-stationarities in the population dynamics itself (i.e.,correlations across neurons). We develop variational inference methods for learning model parameters, anddemonstrate that the method can recover non-stationarities in both averagefiring rates and correlation structure. Applied to neural population recordingsfrom anesthetized macaque primary visual cortex, our models provide a betteraccount of the structure of neural firing than stationary dynamics models.
arxiv-7500-55 | Tree-Structure Bayesian Compressive Sensing for Video | http://arxiv.org/pdf/1410.3080v1.pdf | author:Xin Yuan, Patrick Llull, David J. Brady, Lawrence Carin category:cs.CV published:2014-10-12 summary:A Bayesian compressive sensing framework is developed for videoreconstruction based on the color coded aperture compressive temporal imaging(CACTI) system. By exploiting the three dimension (3D) tree structure of thewavelet and Discrete Cosine Transformation (DCT) coefficients, a Bayesiancompressive sensing inversion algorithm is derived to reconstruct (up to 22)color video frames from a single monochromatic compressive measurement. Bothsimulated and real datasets are adopted to verify the performance of theproposed algorithm.
arxiv-7500-56 | Recommending Investors for Crowdfunding Projects | http://arxiv.org/pdf/1409.7489v2.pdf | author:Jisun An, Daniele Quercia, Jon Crowcroft category:cs.SI cs.CY cs.HC physics.soc-ph stat.ML published:2014-09-26 summary:To bring their innovative ideas to market, those embarking in new ventureshave to raise money, and, to do so, they have often resorted to banks andventure capitalists. Nowadays, they have an additional option: that ofcrowdfunding. The name refers to the idea that funds come from a network ofpeople on the Internet who are passionate about supporting others' projects.One of the most popular crowdfunding sites is Kickstarter. In it, creators postdescriptions of their projects and advertise them on social media sites (mainlyTwitter), while investors look for projects to support. The most common reasonfor project failure is the inability of founders to connect with a sufficientnumber of investors, and that is mainly because hitherto there has not been anyautomatic way of matching creators and investors. We thus set out to proposedifferent ways of recommending investors found on Twitter for specificKickstarter projects. We do so by conducting hypothesis-driven analyses ofpledging behavior and translate the corresponding findings into differentrecommendation strategies. The best strategy achieves, on average, 84% ofaccuracy in predicting a list of potential investors' Twitter accounts for anygiven project. Our findings also produced key insights about the whys andwherefores of investors deciding to support innovative efforts.
arxiv-7500-57 | Q-learning for Optimal Control of Continuous-time Systems | http://arxiv.org/pdf/1410.2954v1.pdf | author:Biao Luo, Derong Liu, Tingwen Huang category:cs.SY stat.ML published:2014-10-11 summary:In this paper, two Q-learning (QL) methods are proposed and their convergencetheories are established for addressing the model-free optimal control problemof general nonlinear continuous-time systems. By introducing the Q-function forcontinuous-time systems, policy iteration based QL (PIQL) and value iterationbased QL (VIQL) algorithms are proposed for learning the optimal control policyfrom real system data rather than using mathematical system model. It is provedthat both PIQL and VIQL methods generate a nonincreasing Q-function sequence,which converges to the optimal Q-function. For implementation of the QLalgorithms, the method of weighted residuals is applied to derived theparameters update rule. The developed PIQL and VIQL algorithms are essentiallyoff-policy reinforcement learning approachs, where the system data can becollected arbitrary and thus the exploration ability is increased. With thedata collected from the real system, the QL methods learn the optimal controlpolicy offline, and then the convergent control policy will be employed to realsystem. The effectiveness of the developed QL algorithms are verified throughcomputer simulation.
arxiv-7500-58 | On model selection consistency of regularized M-estimators | http://arxiv.org/pdf/1305.7477v8.pdf | author:Jason D. Lee, Yuekai Sun, Jonathan E. Taylor category:math.ST cs.LG math.OC stat.ME stat.ML stat.TH published:2013-05-31 summary:Regularized M-estimators are used in diverse areas of science and engineeringto fit high-dimensional models with some low-dimensional structure. Usually thelow-dimensional structure is encoded by the presence of the (unknown)parameters in some low-dimensional model subspace. In such settings, it isdesirable for estimates of the model parameters to be \emph{model selectionconsistent}: the estimates also fall in the model subspace. We develop ageneral framework for establishing consistency and model selection consistencyof regularized M-estimators and show how it applies to some special cases ofinterest in statistical learning. Our analysis identifies two key properties ofregularized M-estimators, referred to as geometric decomposability andirrepresentability, that ensure the estimators are consistent and modelselection consistent.
arxiv-7500-59 | Use of Computer Vision to Detect Tangles in Tangled Objects | http://arxiv.org/pdf/1405.4802v2.pdf | author:Paritosh Parmar category:cs.CV published:2014-05-19 summary:Untangling of structures like ropes and wires by autonomous robots can beuseful in areas such as personal robotics, industries and electrical wiring &repairing by robots. This problem can be tackled by using computer visionsystem in robot. This paper proposes a computer vision based method foranalyzing visual data acquired from camera for perceiving the overlap of wires,ropes, hoses i.e. detecting tangles. Information obtained after processingimage according to the proposed method comprises of position of tangles intangled object and which wire passes over which wire. This information can thenbe used to guide robot to untangle wire/s. Given an image, preprocessing isdone to remove noise. Then edges of wire are detected. After that, the image isdivided into smaller blocks and each block is checked for wire overlap/s andfinding other relevant information. TANGLED-100 dataset was introduced, whichconsists of images of tangled linear deformable objects. Method discussed inhere was tested on the TANGLED-100 dataset. Accuracy achieved duringexperiments was found to be 74.9%. Robotic simulations were carried out todemonstrate the use of the proposed method in applications of robot. Proposedmethod is a general method that can be used by robots working in differentsituations.
arxiv-7500-60 | Do Deep Nets Really Need to be Deep? | http://arxiv.org/pdf/1312.6184v7.pdf | author:Lei Jimmy Ba, Rich Caruana category:cs.LG cs.NE published:2013-12-21 summary:Currently, deep neural networks are the state of the art on problems such asspeech recognition and computer vision. In this extended abstract, we show thatshallow feed-forward networks can learn the complex functions previouslylearned by deep nets and achieve accuracies previously only achievable withdeep models. Moreover, in some cases the shallow neural nets can learn thesedeep functions using a total number of parameters similar to the original deepmodel. We evaluate our method on the TIMIT phoneme recognition task and areable to train shallow fully-connected nets that perform similarly to complex,well-engineered, deep convolutional architectures. Our success in trainingshallow neural nets to mimic deeper models suggests that there probably existbetter algorithms for training shallow feed-forward nets than those currentlyavailable.
arxiv-7500-61 | Riesz Logic | http://arxiv.org/pdf/1410.2910v1.pdf | author:Daoud Clarke category:cs.LO cs.CL published:2014-10-10 summary:We introduce Riesz Logic, whose models are abelian lattice ordered groups,which generalise Riesz spaces (vector lattices), and show soundness andcompleteness. Our motivation is to provide a logic for distributional semanticsof natural language, where words are typically represented as elements of avector space whose dimensions correspond to contexts in which words may occur.This basis provides a lattice ordering on the space, and this ordering may beinterpreted as "distributional entailment". Several axioms of Riesz Logic arefamiliar from Basic Fuzzy Logic, and we show how the models of these two logicsmay be related; Riesz Logic may thus be considered a new fuzzy logic. Inaddition to applications in natural language processing, there is potential forapplying the theory to neuro-fuzzy systems.
arxiv-7500-62 | Approximate False Positive Rate Control in Selection Frequency for Random Forest | http://arxiv.org/pdf/1410.2838v1.pdf | author:Ender Konukoglu, Melanie Ganz category:cs.LG stat.ME published:2014-10-10 summary:Random Forest has become one of the most popular tools for feature selection.Its ability to deal with high-dimensional data makes this algorithm especiallyuseful for studies in neuroimaging and bioinformatics. Despite its popularityand wide use, feature selection in Random Forest still lacks a crucialingredient: false positive rate control. To date there is no efficient,principled and computationally light-weight solution to this shortcoming. As aresult, researchers using Random Forest for feature selection have to resort tousing heuristically set thresholds on feature rankings. This article builds anapproximate probabilistic model for the feature selection process in randomforest training, which allows us to compute an estimated false positive ratefor a given threshold on selection frequency. Hence, it presents a principledway to determine thresholds for the selection of relevant features without anyadditional computational load. Experimental analysis with synthetic datademonstrates that the proposed approach can limit false positive rates on theorder of the desired values and keep false negative rates low. Results showthat this holds even in the presence of a complex correlation structure betweenfeatures. Its good statistical properties and light-weight computational needsmake this approach widely applicable to feature selection for a wide-range ofapplications.
arxiv-7500-63 | New SVD based initialization strategy for Non-negative Matrix Factorization | http://arxiv.org/pdf/1410.2786v1.pdf | author:Hanli Qiao category:cs.LG cs.NA published:2014-10-10 summary:There are two problems need to be dealt with for Non-negative MatrixFactorization (NMF): choose a suitable rank of the factorization and provide agood initialization method for NMF algorithms. This paper aims to solve thesetwo problems using Singular Value Decomposition (SVD). At first we extract thenumber of main components as the rank, actually this method is inspired from[1, 2]. Second, we use the singular value and its vectors to initialize NMFalgorithm. In 2008, Boutsidis and Gollopoulos [3] provided the method titledNNDSVD to enhance initialization of NMF algorithms. They extracted the positivesection and respective singular triplet information of the unit matrices{C(j)}k j=1 which were obtained from singular vector pairs. This strategy aimsto use positive section to cope with negative elements of the singular vectors,but in experiments we found that even replacing negative elements by theirabsolute values could get better results than NNDSVD. Hence, we give anothermethod based SVD to fulfil initialization for NMF algorithms (SVD-NMF).Numerical experiments on two face databases ORL and YALE [16, 17] show that ourmethod is better than NNDSVD.
arxiv-7500-64 | A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and Low-resolution | http://arxiv.org/pdf/1409.5114v2.pdf | author:Shuxin Ouyang, Timothy Hospedales, Yi-Zhe Song, Xueming Li category:cs.CV published:2014-09-17 summary:Heterogeneous face recognition (HFR) refers to matching face imagery acrossdifferent domains. It has received much interest from the research community asa result of its profound implications in law enforcement. A wide variety of newinvariant features, cross-modality matching models and heterogeneous datasetsbeing established in recent years. This survey provides a comprehensive reviewof established techniques and recent developments in HFR. Moreover, we offer adetailed account of datasets and benchmarks commonly used for evaluation. Wefinish by assessing the state of the field and discussing promising directionsfor future research.
arxiv-7500-65 | Compressed Sensing With Side Information: Geometrical Interpretation and Performance Bounds | http://arxiv.org/pdf/1410.2724v1.pdf | author:João F. C. Mota, Nikos Deligiannis, Miguel R. D. Rodrigues category:cs.IT math.IT math.OC stat.ML published:2014-10-10 summary:We address the problem of Compressed Sensing (CS) with side information.Namely, when reconstructing a target CS signal, we assume access to a similarsignal. This additional knowledge, the side information, is integrated into CSvia L1-L1 and L1-L2 minimization. We then provide lower bounds on the number ofmeasurements that these problems require for successful reconstruction of thetarget signal. If the side information has good quality, the number ofmeasurements is significantly reduced via L1-L1 minimization, but not so muchvia L1-L2 minimization. We provide geometrical interpretations and experimentalresults illustrating our findings.
arxiv-7500-66 | Challenge IEEE-ISBI/TCB : Application of Covariance matrices and wavelet marginals | http://arxiv.org/pdf/1410.2663v1.pdf | author:Florian Yger category:cs.CV published:2014-10-10 summary:This short memo aims at explaining our approach for the challenge IEEE-ISBIon Bone Texture Characterization. In this work, we focus on the use ofcovariance matrices and wavelet marginals in an SVM classifier.
arxiv-7500-67 | Contrastive Unsupervised Word Alignment with Non-Local Features | http://arxiv.org/pdf/1410.2082v2.pdf | author:Yang Liu, Maosong Sun category:cs.CL published:2014-10-08 summary:Word alignment is an important natural language processing task thatindicates the correspondence between natural languages. Recently, unsupervisedlearning of log-linear models for word alignment has received considerableattention as it combines the merits of generative and discriminativeapproaches. However, a major challenge still remains: it is intractable tocalculate the expectations of non-local features that are critical forcapturing the divergence between natural languages. We propose a contrastiveapproach that aims to differentiate observed training examples from noises. Itnot only introduces prior knowledge to guide unsupervised learning but alsocancels out partition functions. Based on the observation that the probabilitymass of log-linear models for word alignment is usually highly concentrated, wepropose to use top-n alignments to approximate the expectations with respect toposterior distributions. This allows for efficient and accurate calculation ofexpectations of non-local features. Experiments show that our approach achievessignificant improvements over state-of-the-art unsupervised word alignmentmethods.
arxiv-7500-68 | Distributed Estimation, Information Loss and Exponential Families | http://arxiv.org/pdf/1410.2653v1.pdf | author:Qiang Liu, Alexander Ihler category:stat.ML published:2014-10-09 summary:Distributed learning of probabilistic models from multiple data repositorieswith minimum communication is increasingly important. We study a simplecommunication-efficient learning framework that first calculates the localmaximum likelihood estimates (MLE) based on the data subsets, and then combinesthe local MLEs to achieve the best possible approximation to the global MLEgiven the whole dataset. We study this framework's statistical properties,showing that the efficiency loss compared to the global setting relates to howmuch the underlying distribution families deviate from full exponentialfamilies, drawing connection to the theory of information loss by Fisher, Raoand Efron. We show that the "full-exponential-family-ness" represents the lowerbound of the error rate of arbitrary combinations of local MLEs, and isachieved by a KL-divergence-based combination method but not by a more commonlinear combination method. We also study the empirical properties of bothmethods, showing that the KL method significantly outperforms linearcombination in practical settings with issues such as model misspecification,non-convexity, and heterogeneous data partitions.
arxiv-7500-69 | Hybrid approaches for automatic vowelization of Arabic texts | http://arxiv.org/pdf/1410.2646v1.pdf | author:Mohamed Bebah, Chennoufi Amine, Mazroui Azzeddine, Lakhouaja Abdelhak category:cs.CL 68T50 published:2014-10-09 summary:Hybrid approaches for automatic vowelization of Arabic texts are presented inthis article. The process is made up of two modules. In the first one, amorphological analysis of the text words is performed using the open sourcemorphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed outof context, are its different possible vowelizations. The integration of thisAnalyzer in our vowelization system required the addition of a lexical databasecontaining the most frequent words in Arabic language. Using a statisticalapproach based on two hidden Markov models (HMM), the second module aims toeliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabicwords are the observed states and the vowelized words are the hidden states.The observed states of the second HMM are identical to those of the first, butthe hidden states are the lists of possible diacritics of the word without itsArabic letters. Our system uses Viterbi algorithm to select the optimal pathamong the solutions proposed by Al Khalil Morpho Sys. Our approach opens animportant way to improve the performance of automatic vowelization of Arabictexts for other uses in automatic natural language processing.
arxiv-7500-70 | Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares | http://arxiv.org/pdf/1410.2596v1.pdf | author:Trevor Hastie, Rahul Mazumder, Jason Lee, Reza Zadeh category:stat.ME stat.ML published:2014-10-09 summary:The matrix-completion problem has attracted a lot of attention, largely as aresult of the celebrated Netflix competition. Two popular approaches forsolving the problem are nuclear-norm-regularized matrix approximation (Candesand Tao, 2009, Mazumder, Hastie and Tibshirani, 2010), and maximum-marginmatrix factorization (Srebro, Rennie and Jaakkola, 2005). These two proceduresare in some cases solving equivalent problems, but with quite differentalgorithms. In this article we bring the two approaches together, leading to anefficient algorithm for large matrix factorization and completion thatoutperforms both of these. We develop a software package "softImpute" in R forimplementing our approaches, and a distributed version for very large matricesusing the "Spark" cluster programming environment.
arxiv-7500-71 | A unified approach for multi-object triangulation, tracking and camera calibration | http://arxiv.org/pdf/1410.2535v1.pdf | author:Jeremie Houssineau, Daniel Clark, Spela Ivekovic, Chee Sing Lee, Jose Franco category:cs.CV stat.ME published:2014-10-09 summary:Object triangulation, 3-D object tracking, feature correspondence, and cameracalibration are key problems for estimation from camera networks. This paperaddresses these problems within a unified Bayesian framework for jointmulti-object tracking and sensor registration. Given that using standardfiltering approaches for state estimation from cameras is problematic, analternative parametrisation is exploited, called disparity space. The disparityspace-based approach for triangulation and object tracking is shown to be moreeffective than non-linear versions of the Kalman filter and particle filteringfor non-rectified cameras. The approach for feature correspondence is based onthe Probability Hypothesis Density (PHD) filter, and hence inherits the abilityto update without explicit measurement association, to initiate new targets,and to discriminate between target and clutter. The PHD filtering approach thenforms the basis of a camera calibration method from static or moving objects.Results are shown on simulated data.
arxiv-7500-72 | Genetic Stereo Matching Algorithm with Fuzzy Fitness | http://arxiv.org/pdf/1410.2474v1.pdf | author:Haythem Ghazouani category:cs.CV published:2014-10-09 summary:This paper presents a genetic stereo matching algorithm with fuzzy evaluationfunction. The proposed algorithm presents a new encoding scheme in which achromosome is represented by a disparity matrix. Evolution is controlled by afuzzy fitness function able to deal with noise and uncertain camerameasurements, and uses classical evolutionary operators. The result of thealgorithm is accurate dense disparity maps obtained in a reasonablecomputational time suitable for real-time applications as shown in experimentalresults.
arxiv-7500-73 | Learning Invariant Color Features for Person Re-Identification | http://arxiv.org/pdf/1410.1035v2.pdf | author:Rahul Rama Varior, Gang Wang, Jiwen Lu category:cs.CV published:2014-10-04 summary:Matching people across multiple camera views known as personre-identification, is a challenging problem due to the change in visualappearance caused by varying lighting conditions. The perceived color of thesubject appears to be different with respect to illumination. Previous worksuse color as it is or address these challenges by designing color spacesfocusing on a specific cue. In this paper, we propose a data driven approachfor learning color patterns from pixels sampled from images across two cameraviews. The intuition behind this work is that, even though pixel values of samecolor would be different across views, they should be encoded with the samevalues. We model color feature generation as a learning problem by jointlylearning a linear transformation and a dictionary to encode pixel values. Wealso analyze different photometric invariant color spaces. Using color as theonly cue, we compare our approach with all the photometric invariant colorspaces and show superior performance over all of them. Combining with otherlearned low-level and high-level features, we obtain promising results inViPER, Person Re-ID 2011 and CAVIAR4REID datasets.
arxiv-7500-74 | Bayesian CP Factorization of Incomplete Tensors with Automatic Rank Determination | http://arxiv.org/pdf/1401.6497v2.pdf | author:Qibin Zhao, Liqing Zhang, Andrzej Cichocki category:cs.LG cs.CV stat.ML published:2014-01-25 summary:CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerfultechnique for tensor completion through explicitly capturing the multilinearlatent factors. The existing CP algorithms require the tensor rank to bemanually specified, however, the determination of tensor rank remains achallenging problem especially for CP rank. In addition, existing approaches donot take into account uncertainty information of latent factors, as well asmissing entries. To address these issues, we formulate CP factorization using ahierarchical probabilistic model and employ a fully Bayesian treatment byincorporating a sparsity-inducing prior over multiple latent factors and theappropriate hyperpriors over all hyperparameters, resulting in automatic rankdetermination. To learn the model, we develop an efficient deterministicBayesian inference algorithm, which scales linearly with data size. Our methodis characterized as a tuning parameter-free approach, which can effectivelyinfer underlying multilinear factors with a low-rank constraint, while alsoproviding predictive distributions over missing entries. Extensive simulationson synthetic data illustrate the intrinsic capability of our method to recoverthe ground-truth of CP rank and prevent the overfitting problem, even when alarge amount of entries are missing. Moreover, the results from real-worldapplications, including image inpainting and facial image synthesis,demonstrate that our method outperforms state-of-the-art approaches for bothtensor factorization and tensor completion in terms of predictive performance.
arxiv-7500-75 | Recognition of cDNA microarray image Using Feedforward artificial neural network | http://arxiv.org/pdf/1410.2381v1.pdf | author:R. M. Farouk, S. Badr, M. Sayed Elahl category:cs.CV cs.NE published:2014-10-09 summary:The complementary DNA (cDNA) sequence is considered to be the magic biometrictechnique for personal identification. In this paper, we present a new methodfor cDNA recognition based on the artificial neural network (ANN). Microarrayimaging is used for the concurrent identification of thousands of genes. Wehave segmented the location of the spots in a cDNA microarray. Thus, a preciselocalization and segmenting of a spot are essential to obtain a more accurateintensity measurement, leading to a more precise expression measurement of agene. The segmented cDNA microarray image is resized and it is used as an inputfor the proposed artificial neural network. For matching and recognition, wehave trained the artificial neural network. Recognition results are given forthe galleries of cDNA sequences . The numerical results show that, the proposedmatching technique is an effective in the cDNA sequences process. We alsocompare our results with previous results and find out that, the proposedtechnique is an effective matching performance.
arxiv-7500-76 | Proceedings of the second "international Traveling Workshop on Interactions between Sparse models and Technology" (iTWIST'14) | http://arxiv.org/pdf/1410.0719v2.pdf | author:L. Jacques, C. De Vleeschouwer, Y. Boursier, P. Sudhakar, C. De Mol, A. Pizurica, S. Anthoine, P. Vandergheynst, P. Frossard, C. Bilen, S. Kitic, N. Bertin, R. Gribonval, N. Boumal, B. Mishra, P. -A. Absil, R. Sepulchre, S. Bundervoet, C. Schretter, A. Dooms, P. Schelkens, O. Chabiron, F. Malgouyres, J. -Y. Tourneret, N. Dobigeon, P. Chainais, C. Richard, B. Cornelis, I. Daubechies, D. Dunson, M. Dankova, P. Rajmic, K. Degraux, V. Cambareri, B. Geelen, G. Lafruit, G. Setti, J. -F. Determe, J. Louveaux, F. Horlin, A. Drémeau, P. Heas, C. Herzet, V. Duval, G. Peyré, A. Fawzi, M. Davies, N. Gillis, S. A. Vavasis, C. Soussen, L. Le Magoarou, J. Liang, J. Fadili, A. Liutkus, D. Martina, S. Gigan, L. Daudet, M. Maggioni, S. Minsker, N. Strawn, C. Mory, F. Ngole, J. -L. Starck, I. Loris, S. Vaiter, M. Golbabaee, D. Vukobratovic category:cs.NA cs.CV cs.IT cs.LG math.IT math.OC math.ST stat.TH published:2014-10-02 summary:The implicit objective of the biennial "international - Traveling Workshop onInteractions between Sparse models and Technology" (iTWIST) is to fostercollaboration between international scientific teams by disseminating ideasthrough both specific oral/poster presentations and free discussions. For itssecond edition, the iTWIST workshop took place in the medieval and picturesquetown of Namur in Belgium, from Wednesday August 27th till Friday August 29th,2014. The workshop was conveniently located in "The Arsenal" building withinwalking distance of both hotels and town center. iTWIST'14 has gathered about70 international participants and has featured 9 invited talks, 10 oralpresentations, and 14 posters on the following themes, all related to thetheory, application and generalization of the "sparsity paradigm":Sparsity-driven data sensing and processing; Union of low dimensionalsubspaces; Beyond linear and convex inverse problem; Matrix/manifold/graphsensing/processing; Blind inverse problems and dictionary learning; Sparsityand computational neuroscience; Information theory, geometry and randomness;Complexity/accuracy tradeoffs in numerical methods; Sparsity? What's next?;Sparse machine learning and inference.
arxiv-7500-77 | Automatic Training Data Synthesis for Handwriting Recognition Using the Structural Crossing-Over Technique | http://arxiv.org/pdf/1412.6018v1.pdf | author:Sirisak Visessenee, Sanparith Marukatat, Rachada Kongkachandra category:cs.CV cs.LG published:2014-10-09 summary:The paper presents a novel technique called "Structural Crossing-Over" tosynthesize qualified data for training machine learning-based handwritingrecognition. The proposed technique can provide a greater variety of patternsof training data than the existing approaches such as elastic distortion andtangent-based affine transformation. A couple of training characters arechosen, then they are analyzed by their similar and different structures, andfinally are crossed over to generate the new characters. The experiments areset to compare the performances of tangent-based affine transformation and theproposed approach in terms of the variety of generated characters and percentof recognition errors. The standard MNIST corpus including 60,000 trainingcharacters and 10,000 test characters is employed in the experiments. Theproposed technique uses 1,000 characters to synthesize 60,000 characters, andthen uses these data to train and test the benchmark handwriting recognitionsystem that exploits Histogram of Gradient (HOG) as features and Support VectorMachine (SVM) as recognizer. The experimental result yields 8.06% of errors. Itsignificantly outperforms the tangent-based affine transformation and theoriginal MNIST training data, which are 11.74% and 16.55%, respectively.
arxiv-7500-78 | Generalization Analysis for Game-Theoretic Machine Learning | http://arxiv.org/pdf/1410.3341v1.pdf | author:Haifang Li, Fei Tian, Wei Chen, Tao Qin, Tie-Yan Liu category:cs.LG cs.GT published:2014-10-09 summary:For Internet applications like sponsored search, cautions need to be takenwhen using machine learning to optimize their mechanisms (e.g., auction) sinceself-interested agents in these applications may change their behaviors (andthus the data distribution) in response to the mechanisms. To tackle thisproblem, a framework called game-theoretic machine learning (GTML) was recentlyproposed, which first learns a Markov behavior model to characterize agents'behaviors, and then learns the optimal mechanism by simulating agents' behaviorchanges in response to the mechanism. While GTML has demonstrated practicalsuccess, its generalization analysis is challenging because the behavior dataare non-i.i.d. and dependent on the mechanism. To address this challenge,first, we decompose the generalization error for GTML into the behaviorlearning error and the mechanism learning error; second, for the behaviorlearning error, we obtain novel non-asymptotic error bounds for both parametricand non-parametric behavior learning methods; third, for the mechanism learningerror, we derive a uniform convergence bound based on a new concept callednested covering number of the mechanism space and the generalization analysistechniques developed for mixing sequences. To the best of our knowledge, thisis the first work on the generalization analysis of GTML, and we believe it hasgeneral implications to the theoretical analysis of other complicated machinelearning problems.
arxiv-7500-79 | A Scalable, Lexicon Based Technique for Sentiment Analysis | http://arxiv.org/pdf/1410.2265v1.pdf | author:Chetan Kaushik, Atul Mishra category:cs.IR cs.CL published:2014-10-08 summary:Rapid increase in the volume of sentiment rich social media on the web hasresulted in an increased interest among researchers regarding SentimentalAnalysis and opinion mining. However, with so much social media available onthe web, sentiment analysis is now considered as a big data task. Hence theconventional sentiment analysis approaches fails to efficiently handle the vastamount of sentiment data available now a days. The main focus of the researchwas to find such a technique that can efficiently perform sentiment analysis onbig data sets. A technique that can categorize the text as positive, negativeand neutral in a fast and accurate manner. In the research, sentiment analysiswas performed on a large data set of tweets using Hadoop and the performance ofthe technique was measured in form of speed and accuracy. The experimentalresults shows that the technique exhibits very good efficiency in handling bigsentiment data sets.
arxiv-7500-80 | Joint Estimation of Multiple Graphical Models from High Dimensional Time Series | http://arxiv.org/pdf/1311.0219v2.pdf | author:Huitong Qiu, Fang Han, Han Liu, Brian Caffo category:stat.ML published:2013-11-01 summary:In this manuscript we consider the problem of jointly estimating multiplegraphical models in high dimensions. We assume that the data are collected fromn subjects, each of which consists of T possibly dependent observations. Thegraphical models of subjects vary, but are assumed to change smoothlycorresponding to a measure of closeness between subjects. We propose a kernelbased method for jointly estimating all graphical models. Theoretically, undera double asymptotic framework, where both (T,n) and the dimension d canincrease, we provide the explicit rate of convergence in parameter estimation.It characterizes the strength one can borrow across different individuals andimpact of data dependence on parameter estimation. Empirically, experiments onboth synthetic and real resting state functional magnetic resonance imaging(rs-fMRI) data illustrate the effectiveness of the proposed method.
arxiv-7500-81 | Bayesian tracking and parameter learning for non-linear multiple target tracking models | http://arxiv.org/pdf/1410.2046v1.pdf | author:Lan Jiang, Sumeetpal S. Singh, Sinan Yıldırım category:stat.AP stat.CO stat.ML published:2014-10-08 summary:We propose a new Bayesian tracking and parameter learning algorithm fornon-linear non-Gaussian multiple target tracking (MTT) models. We design aMarkov chain Monte Carlo (MCMC) algorithm to sample from the posteriordistribution of the target states, birth and death times, and association ofobservations to targets, which constitutes the solution to the trackingproblem, as well as the model parameters. In the numerical section, we presentperformance comparisons with several competing techniques and demonstratesignificant performance improvements in all cases.
arxiv-7500-82 | Supervised learning Methods for Bangla Web Document Categorization | http://arxiv.org/pdf/1410.2045v1.pdf | author:Ashis Kumar Mandal, Rikta Sen category:cs.CL cs.LG published:2014-10-08 summary:This paper explores the use of machine learning approaches, or morespecifically, four supervised learning Methods, namely Decision Tree(C 4.5),K-Nearest Neighbour (KNN), Na\"ive Bays (NB), and Support Vector Machine (SVM)for categorization of Bangla web documents. This is a task of automaticallysorting a set of documents into categories from a predefined set. Whereas awide range of methods have been applied to English text categorization,relatively few studies have been conducted on Bangla language textcategorization. Hence, we attempt to analyze the efficiency of those fourmethods for categorization of Bangla documents. In order to validate, Banglacorpus from various websites has been developed and used as examples for theexperiment. For Bangla, empirical results support that all four methods producesatisfactory performance with SVM attaining good result in terms of highdimensional and relatively noisy document feature vectors.
arxiv-7500-83 | Deep Learning in Neural Networks: An Overview | http://arxiv.org/pdf/1404.7828v4.pdf | author:Juergen Schmidhuber category:cs.NE cs.LG published:2014-04-30 summary:In recent years, deep artificial neural networks (including recurrent ones)have won numerous contests in pattern recognition and machine learning. Thishistorical survey compactly summarises relevant work, much of it from theprevious millennium. Shallow and deep learners are distinguished by the depthof their credit assignment paths, which are chains of possibly learnable,causal links between actions and effects. I review deep supervised learning(also recapitulating the history of backpropagation), unsupervised learning,reinforcement learning & evolutionary computation, and indirect search forshort programs encoding deep and large networks.
arxiv-7500-84 | Projecting Ising Model Parameters for Fast Mixing | http://arxiv.org/pdf/1407.0749v2.pdf | author:Justin Domke, Xianghang Liu category:cs.LG stat.ML published:2014-07-03 summary:Inference in general Ising models is difficult, due to high treewidth makingtree-based algorithms intractable. Moreover, when interactions are strong,Gibbs sampling may take exponential time to converge to the stationarydistribution. We present an algorithm to project Ising model parameters onto aparameter set that is guaranteed to be fast mixing, under several divergences.We find that Gibbs sampling using the projected parameters is more accuratethan with the original parameters when interaction strengths are strong andwhen limited time is available for sampling.
arxiv-7500-85 | GLAD: Group Anomaly Detection in Social Media Analysis- Extended Abstract | http://arxiv.org/pdf/1410.1940v1.pdf | author:Qi, Yu, Xinran He, Yan Liu category:cs.LG cs.SI H.2.8 published:2014-10-07 summary:Traditional anomaly detection on social media mostly focuses on individualpoint anomalies while anomalous phenomena usually occur in groups. Therefore itis valuable to study the collective behavior of individuals and detect groupanomalies. Existing group anomaly detection approaches rely on the assumptionthat the groups are known, which can hardly be true in real world social mediaapplications. In this paper, we take a generative approach by proposing ahierarchical Bayes model: Group Latent Anomaly Detection (GLAD) model. GLADtakes both pair-wise and point-wise data as input, automatically infers thegroups and detects group anomalies simultaneously. To account for the dynamicproperties of the social media data, we further generalize GLAD to its dynamicextension d-GLAD. We conduct extensive experiments to evaluate our models onboth synthetic and real world datasets. The empirical results demonstrate thatour approach is effective and robust in discovering latent groups and detectinggroup anomalies.
arxiv-7500-86 | Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation | http://arxiv.org/pdf/1409.1257v2.pdf | author:Jean Pouget-Abadie, Dzmitry Bahdanau, Bart van Merrienboer, Kyunghyun Cho, Yoshua Bengio category:cs.CL cs.LG cs.NE stat.ML published:2014-09-03 summary:The authors of (Cho et al., 2014a) have shown that the recently introducedneural network translation systems suffer from a significant drop intranslation quality when translating long sentences, unlike existingphrase-based translation systems. In this paper, we propose a way to addressthis issue by automatically segmenting an input sentence into phrases that canbe easily translated by the neural network translation model. Once each segmenthas been independently translated by the neural machine translation model, thetranslated clauses are concatenated to form a final translation. Empiricalresults show a significant improvement in translation quality for longsentences.
arxiv-7500-87 | On the Properties of Neural Machine Translation: Encoder-Decoder Approaches | http://arxiv.org/pdf/1409.1259v2.pdf | author:Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio category:cs.CL stat.ML published:2014-09-03 summary:Neural machine translation is a relatively new approach to statisticalmachine translation based purely on neural networks. The neural machinetranslation models often consist of an encoder and a decoder. The encoderextracts a fixed-length representation from a variable-length input sentence,and the decoder generates a correct translation from this representation. Inthis paper, we focus on analyzing the properties of the neural machinetranslation using two models; RNN Encoder--Decoder and a newly proposed gatedrecursive convolutional neural network. We show that the neural machinetranslation performs relatively well on short sentences without unknown words,but its performance degrades rapidly as the length of the sentence and thenumber of unknown words increase. Furthermore, we find that the proposed gatedrecursive convolutional network learns a grammatical structure of a sentenceautomatically.
arxiv-7500-88 | Generalization Bounds for Learning with Linear, Polygonal, Quadratic and Conic Side Knowledge | http://arxiv.org/pdf/1405.7764v3.pdf | author:Theja Tulabandhula, Cynthia Rudin category:stat.ML cs.LG published:2014-05-30 summary:In this paper, we consider a supervised learning setting where side knowledgeis provided about the labels of unlabeled examples. The side knowledge has theeffect of reducing the hypothesis space, leading to tighter generalizationbounds, and thus possibly better generalization. We consider several types ofside knowledge, the first leading to linear and polygonal constraints on thehypothesis space, the second leading to quadratic constraints, and the lastleading to conic constraints. We show how different types of domain knowledgecan lead directly to these kinds of side knowledge. We prove bounds oncomplexity measures of the hypothesis space for quadratic and conic sideknowledge, and show that these bounds are tight in a specific sense for thequadratic case.
arxiv-7500-89 | On Classification with Bags, Groups and Sets | http://arxiv.org/pdf/1406.0281v2.pdf | author:Veronika Cheplygina, David M. J. Tax, Marco Loog category:stat.ML cs.CV cs.LG published:2014-06-02 summary:Many classification problems can be difficult to formulate directly in termsof the traditional supervised setting, where both training and test samples areindividual feature vectors. There are cases in which samples are betterdescribed by sets of feature vectors, that labels are only available for setsrather than individual samples, or, if individual labels are available, thatthese are not independent. To better deal with such problems, severalextensions of supervised learning have been proposed, where either trainingand/or test objects are sets of feature vectors. However, having been proposedrather independently of each other, their mutual similarities and differenceshave hitherto not been mapped out. In this work, we provide an overview of suchlearning scenarios, propose a taxonomy to illustrate the relationships betweenthem, and discuss directions for further research in these areas.
arxiv-7500-90 | Mumford-Shah and Potts Regularization for Manifold-Valued Data with Applications to DTI and Q-Ball Imaging | http://arxiv.org/pdf/1410.1699v1.pdf | author:Andreas Weinmann, Laurent Demaret, Martin Storath category:math.NA cs.CV math.OC physics.med-ph published:2014-10-07 summary:Mumford-Shah and Potts functionals are powerful variational models forregularization which are widely used in signal and image processing; typicalapplications are edge-preserving denoising and segmentation. Being bothnon-smooth and non-convex, they are computationally challenging even for scalardata. For manifold-valued data, the problem becomes even more involved sincetypical features of vector spaces are not available. In this paper, we proposealgorithms for Mumford-Shah and for Potts regularization of manifold-valuedsignals and images. For the univariate problems, we derive solvers based ondynamic programming combined with (convex) optimization techniques formanifold-valued data. For the class of Cartan-Hadamard manifolds (whichincludes the data space in diffusion tensor imaging), we show that ouralgorithms compute global minimizers for any starting point. For themultivariate Mumford-Shah and Potts problems (for image regularization) wepropose a splitting into suitable subproblems which we can solve exactly usingthe techniques developed for the corresponding univariate problems. Our methoddoes not require any a priori restrictions on the edge set and we do not haveto discretize the data space. We apply our method to diffusion tensor imaging(DTI) as well as Q-ball imaging. Using the DTI model, we obtain a segmentationof the corpus callosum.
arxiv-7500-91 | Annotation as a New Paradigm in Research Archiving | http://arxiv.org/pdf/1412.6069v1.pdf | author:Dirk Roorda, Charles van den Heuvel category:cs.DL cs.CL published:2014-10-07 summary:We outline a paradigm to preserve results of digital scholarship, whetherthey are query results, feature values, or topic assignments. This paradigm ischaracterized by using annotations as multifunctional carriers and making themportable. The testing grounds we have chosen are two significant enterprises,one in the history of science, and one in Hebrew scholarship. The first one(CKCC) focuses on the results of a project where a Dutch consortium ofuniversities, research institutes, and cultural heritage institutionsexperimented for 4 years with language techniques and topic modeling methodswith the aim to analyze the emergence of scholarly debates. The data: a complexset of about 20.000 letters. The second one (DTHB) is a multi-year effort toexpress the linguistic features of the Hebrew bible in a text database, whichis still growing in detail and sophistication. Versions of this database arepackaged in commercial bible study software. We state that the results of theseforms of scholarship require new knowledge management and archive practices.Only when researchers can build efficiently on each other's (intermediate)results, they can achieve the aggregations of quality data by which newquestions can be answered, and hidden patterns visualized. Archives arerequired to find a balance between preserving authoritative versions of sourcesand supporting collaborative efforts in digital scholarship. Annotations arepromising vehicles for preserving and reusing research results. Keywordsannotation, portability, archiving, queries, features, topics, keywords,Republic of Letters, Hebrew text databases.
arxiv-7500-92 | Predicting Nearly As Well As the Optimal Twice Differentiable Regressor | http://arxiv.org/pdf/1401.6413v2.pdf | author:N. Denizcan Vanli, Muhammed O. Sayin, Suleyman S. Kozat category:cs.LG stat.ML published:2014-01-23 summary:We study nonlinear regression of real valued data in an individual sequencemanner, where we provide results that are guaranteed to hold without anystatistical assumptions. We address the convergence and undertraining issues ofconventional nonlinear regression methods and introduce an algorithm thatelegantly mitigates these issues via an incremental hierarchical structure,(i.e., via an incremental decision tree). Particularly, we present a piecewiselinear (or nonlinear) regression algorithm that partitions the regressor spacein a data driven manner and learns a linear model at each region. Unlike theconventional approaches, our algorithm gradually increases the number ofdisjoint partitions on the regressor space in a sequential manner according tothe observed data. Through this data driven approach, our algorithmsequentially and asymptotically achieves the performance of the optimal twicedifferentiable regression function for any data sequence with an unknown andarbitrary length. The computational complexity of the introduced algorithm isonly logarithmic in the data length under certain regularity conditions. Weprovide the explicit description of the algorithm and demonstrate thesignificant gains for the well-known benchmark real data sets and chaoticsignals.
arxiv-7500-93 | Term-Weighting Learning via Genetic Programming for Text Classification | http://arxiv.org/pdf/1410.0640v3.pdf | author:Hugo Jair Escalante, Mauricio A. García-Limón, Alicia Morales-Reyes, Mario Graff, Manuel Montes-y-Gómez, Eduardo F. Morales category:cs.NE cs.LG 68T50, 68T10 published:2014-10-02 summary:This paper describes a novel approach to learning term-weighting schemes(TWSs) in the context of text classification. In text mining a TWS determinesthe way in which documents will be represented in a vector space model, beforeapplying a classifier. Whereas acceptable performance has been obtained withstandard TWSs (e.g., Boolean and term-frequency schemes), the definition ofTWSs has been traditionally an art. Further, it is still a difficult task todetermine what is the best TWS for a particular problem and it is not clearyet, whether better schemes, than those currently available, can be generatedby combining known TWS. We propose in this article a genetic program that aimsat learning effective TWSs that can improve the performance of current schemesin text classification. The genetic program learns how to combine a set ofbasic units to give rise to discriminative TWSs. We report an extensiveexperimental study comprising data sets from thematic and non-thematic textclassification as well as from image classification. Our study shows thevalidity of the proposed method; in fact, we show that TWSs learned with thegenetic program outperform traditional schemes and other TWSs proposed inrecent works. Further, we show that TWSs learned from a specific domain can beeffectively used for other tasks.
arxiv-7500-94 | Top Rank Optimization in Linear Time | http://arxiv.org/pdf/1410.1462v1.pdf | author:Nan Li, Rong Jin, Zhi-Hua Zhou category:cs.LG cs.AI cs.IR published:2014-10-06 summary:Bipartite ranking aims to learn a real-valued ranking function that orderspositive instances before negative instances. Recent efforts of bipartiteranking are focused on optimizing ranking accuracy at the top of the rankedlist. Most existing approaches are either to optimize task specific metrics orto extend the ranking loss by emphasizing more on the error associated with thetop ranked instances, leading to a high computational cost that is super-linearin the number of training instances. We propose a highly efficient approach,titled TopPush, for optimizing accuracy at the top that has computationalcomplexity linear in the number of training instances. We present a novelanalysis that bounds the generalization error for the top ranked instances forthe proposed approach. Empirical study shows that the proposed approach ishighly competitive to the state-of-the-art approaches and is 10-100 timesfaster.
arxiv-7500-95 | An Aerial Image Recognition Framework using Discrimination and Redundancy Quality Measure | http://arxiv.org/pdf/1410.2188v1.pdf | author:Yuxin Hu, Luming Zhang category:cs.CV published:2014-10-06 summary:Aerial image categorization plays an indispensable role in remote sensing andartificial intelligence. In this paper, we propose a new aerial imagecategorization framework, focusing on organizing the local patches of eachaerial image into multiple discriminative subgraphs. The subgraphs reflect boththe geometric property and the color distribution of an aerial image. First,each aerial image is decomposed into a collection of regions in terms of theircolor intensities. Thereby region connected graph (RCG), which models theconnection between the spatial neighboring regions, is constructed to encodethe spatial context of an aerial image. Second, a subgraph mining technique isadopted to discover the frequent structures in the RCGs constructed from thetraining aerial images. Thereafter, a set of refined structures are selectedamong the frequent ones toward being highly discriminative and low redundant.Lastly, given a new aerial image, its sub-RCGs corresponding to the refinedstructures are extracted. They are further quantized into a discriminativevector for SVM classification. Thorough experimental results validate thee?ectiveness of the proposed method. In addition, the visualized minedsubgraphs show that the discriminative topologies of each aerial image arediscovered.
arxiv-7500-96 | Effective persistent homology of digital images | http://arxiv.org/pdf/1412.6154v1.pdf | author:Ana Romero, Julio Rubio, Francis Sergeraert category:cs.CV published:2014-10-06 summary:In this paper, three Computational Topology methods (namely effectivehomology, persistent homology and discrete vector fields) are mixed together toproduce algorithms for homological digital image processing. The algorithmshave been implemented as extensions of the Kenzo system and have shown a goodperformance when applied on some actual images extracted from a public dataset.
arxiv-7500-97 | Memristive Threshold Logic Circuit Design of Fast Moving Object Detection | http://arxiv.org/pdf/1410.1267v1.pdf | author:Akshay Kumar Maan, Dinesh Sasi Kumar, Sherin Sugathan, Alex Pappachen James category:cs.CV cs.AR cs.ET published:2014-10-06 summary:Real-time detection of moving objects involves memorisation of features inthe template image and their comparison with those in the test image. At highsampling rates, such techniques face the problems of high algorithmiccomplexity and component delays. We present a new resistive switching basedthreshold logic cell which encodes the pixels of a template image. The cellcomprises a voltage divider circuit that programs the resistances of thememristors arranged in a single node threshold logic network and the output isencoded as a binary value using a CMOS inverter gate. When a test image isapplied to the template-programmed cell, a mismatch in the respective pixels isseen as a change in the output voltage of the cell. The proposed cell whencompared with CMOS equivalent implementation shows improved performance inarea, leakage power, power dissipation and delay.
arxiv-7500-98 | Sequential Monte Carlo for Graphical Models | http://arxiv.org/pdf/1402.0330v4.pdf | author:Christian A. Naesseth, Fredrik Lindsten, Thomas B. Schön category:stat.ME stat.ML published:2014-02-03 summary:We propose a new framework for how to use sequential Monte Carlo (SMC)algorithms for inference in probabilistic graphical models (PGM). Via asequential decomposition of the PGM we find a sequence of auxiliarydistributions defined on a monotonically increasing sequence of probabilityspaces. By targeting these auxiliary distributions using SMC we are able toapproximate the full joint distribution defined by the PGM. One of the keymerits of the SMC sampler is that it provides an unbiased estimate of thepartition function of the model. We also show how it can be used within aparticle Markov chain Monte Carlo framework in order to constructhigh-dimensional block-sampling algorithms for general PGMs.
arxiv-7500-99 | Coupling Top-down and Bottom-up Methods for 3D Human Pose and Shape Estimation from Monocular Image Sequences | http://arxiv.org/pdf/1410.0117v2.pdf | author:Atul Kanaujia category:cs.CV published:2014-10-01 summary:Until recently Intelligence, Surveillance, and Reconnaissance (ISR) focusedon acquiring behavioral information of the targets and their activities.Continuous evolution of intelligence being gathered of the human centricactivities has put increased focus on the humans, especially inferring theirinnate characteristics - size, shapes and physiology. These bio-signaturesextracted from the surveillance sensors can be used to deduce age, ethnicity,gender and actions, and further characterize human actions in unseen scenarios.However, recovery of pose and shape of humans in such monocular videos isinherently an ill-posed problem, marked by frequent depth and view basedambiguities due to self-occlusion, foreshortening and misalignment. Thelikelihood function often yields a highly multimodal posterior that isdifficult to propagate even using the most advanced particle filtering(PF)algorithms. Motivated by the recent success of the discriminative approaches toefficiently predict 3D poses directly from the 2D images, we present severalprincipled approaches to integrate predictive cues using learned regressionmodels to sustain multimodality of the posterior during tracking. Additionally,these learned priors can be actively adapted to the test data using alikelihood based feedback mechanism. Estimated 3D poses are then used to fit 3Dhuman shape model to each frame independently for inferring anthropometricbio-signatures. The proposed system is fully automated, robust to noisy testdata and has ability to swiftly recover from tracking failures even afterconfronting with significant errors. We evaluate the system on a large numberof monocular human motion sequences.
arxiv-7500-100 | Learning Topology and Dynamics of Large Recurrent Neural Networks | http://arxiv.org/pdf/1410.1174v1.pdf | author:Yiyuan She, Yuejia He, Dapeng Wu category:stat.ML stat.CO published:2014-10-05 summary:Large-scale recurrent networks have drawn increasing attention recentlybecause of their capabilities in modeling a large variety of real-worldphenomena and physical mechanisms. This paper studies how to identify allauthentic connections and estimate system parameters of a recurrent network,given a sequence of node observations. This task becomes extremely challengingin modern network applications, because the available observations are usuallyvery noisy and limited, and the associated dynamical system is stronglynonlinear. By formulating the problem as multivariate sparse sigmoidalregression, we develop simple-to-implement network learning algorithms, withrigorous convergence guarantee in theory, for a variety of sparsity-promotingpenalty forms. A quantile variant of progressive recurrent network screening isproposed for efficient computation and allows for direct cardinality control ofnetwork topology in estimation. Moreover, we investigate recurrent networkstability conditions in Lyapunov's sense, and integrate such stabilityconstraints into sparse network learning. Experiments show excellentperformance of the proposed algorithms in network topology identification andforecasting.
arxiv-7500-101 | 1-HKUST: Object Detection in ILSVRC 2014 | http://arxiv.org/pdf/1409.6155v3.pdf | author:Cewu Lu, Hao Chen, Qifeng Chen, Hei Law, Yao Xiao, Chi-Keung Tang category:cs.CV published:2014-09-22 summary:The Imagenet Large Scale Visual Recognition Challenge (ILSVRC) is the one ofthe most important big data challenges to date. We participated in the objectdetection track of ILSVRC 2014 and received the fourth place among the 38teams. We introduce in our object detection system a number of novel techniquesin localization and recognition. For localization, initial candidate proposalsare generated using selective search, and a novel bounding boxes regressionmethod is used for better object localization. For recognition, to represent acandidate proposal, we adopt three features, namely, RCNN feature, IFV feature,and DPM feature. Given these features, category-specific combination functionsare learned to improve the object recognition rate. In addition, object contextin the form of background priors and object interaction priors are learned andapplied in our system. Our ILSVRC 2014 results are reported alongside with theresults of other participating teams.
arxiv-7500-102 | Training Algorithm for Neuro-Fuzzy Network Based on Singular Spectrum Analysis | http://arxiv.org/pdf/1410.1151v1.pdf | author:Yulia S. Maslennikova, Vladimir V. Bochkarev category:cs.NE stat.ME I.5.1 published:2014-10-05 summary:In this article, we propose a combination of an noise-reduction algorithmbased on Singular Spectrum Analysis (SSA) and a standard feedforward neuralprediction model. Basically, the proposed algorithm consists of two differentsteps: data preprocessing based on the SSA filtering method and step-by-steptraining procedure in which we use a simple feedforward multilayer neuralnetwork with backpropagation learning. The proposed noise-reduction proceduresuccessfully removes most of the noise. That increases long-term predictabilityof the processed dataset comparison with the raw dataset. The method wasapplied to predict the International sunspot number RZ time series. The resultsshow that our combined technique has better performances than those offered bythe same network directly applied to raw dataset.
arxiv-7500-103 | Corpora Preparation and Stopword List Generation for Arabic data in Social Network | http://arxiv.org/pdf/1410.1135v1.pdf | author:Walaa Medhat, Ahmed H. Yousef, Hoda Korashy category:cs.CL published:2014-10-05 summary:This paper proposes a methodology to prepare corpora in Arabic language fromonline social network (OSN) and review site for Sentiment Analysis (SA) task.The paper also proposes a methodology for generating a stopword list from theprepared corpora. The aim of the paper is to investigate the effect of removingstopwords on the SA task. The problem is that the stopwords lists generatedbefore were on Modern Standard Arabic (MSA) which is not the common languageused in OSN. We have generated a stopword list of Egyptian dialect and acorpus-based list to be used with the OSN corpora. We compare the efficiency oftext classification when using the generated lists along with previouslygenerated lists of MSA and combining the Egyptian dialect list with the MSAlist. The text classification was performed using Na\"ive Bayes and DecisionTree classifiers and two feature selection approaches, unigrams and bigram. Theexperiments show that the general lists containing the Egyptian dialects wordsgive better performance than using lists of MSA stopwords only.
arxiv-7500-104 | Statistique et Big Data Analytics; Volumétrie, L'Attaque des Clones | http://arxiv.org/pdf/1405.6676v2.pdf | author:Philippe Besse, Nathalie Villa-Vialaneix category:stat.OT cs.LG math.ST stat.TH published:2014-05-26 summary:This article assumes acquired the skills and expertise of a statistician inunsupervised (NMF, k-means, SVD) and supervised learning (regression, CART,random forest). What skills and knowledge do a statistician must acquire toreach the "Volume" scale of big data? After a quick overview of the differentstrategies available and especially of those imposed by Hadoop, the algorithmsof some available learning methods are outlined in order to understand how theyare adapted to the strong stresses of the Map-Reduce functionalities
arxiv-7500-105 | Towards Distribution-Free Multi-Armed Bandits with Combinatorial Strategies | http://arxiv.org/pdf/1307.5438v3.pdf | author:Xiang-yang Li, Shaojie Tang, Yaqin Zhou category:cs.LG published:2013-07-20 summary:In this paper we study a generalized version of classical multi-armed bandits(MABs) problem by allowing for arbitrary constraints on constituent bandits ateach decision point. The motivation of this study comes from many situationsthat involve repeatedly making choices subject to arbitrary constraints in anuncertain environment: for instance, regularly deciding which advertisements todisplay online in order to gain high click-through-rate without knowing userpreferences, or what route to drive home each day under uncertain weather andtraffic conditions. Assume that there are $K$ unknown random variables (RVs),i.e., arms, each evolving as an \emph{i.i.d} stochastic process over time. Ateach decision epoch, we select a strategy, i.e., a subset of RVs, subject toarbitrary constraints on constituent RVs. We then gain a reward that is a linear combination of observations onselected RVs. The performance of prior results for this problem heavily depends on thedistribution of strategies generated by corresponding learning policy. Forexample, if the reward-difference between the best and second best strategyapproaches zero, prior result may lead to arbitrarily large regret. Meanwhile, when there are exponential number of possible strategies at eachdecision point, naive extension of a prior distribution-free policy would causepoor performance in terms of regret, computation and space complexity. To this end, we propose an efficient Distribution-Free Learning (DFL) policythat achieves zero regret, regardless of the probability distribution of theresultant strategies. Our learning policy has both $O(K)$ time complexity and $O(K)$ spacecomplexity. In successive generations, we show that even if finding the optimalstrategy at each decision point is NP-hard, our policy still allows forapproximated solutions while retaining near zero-regret.
arxiv-7500-106 | Language-based Examples in the Statistics Classroom | http://arxiv.org/pdf/1410.2149v1.pdf | author:Roger Bilisoly category:cs.CL published:2014-10-05 summary:Statistics pedagogy values using a variety of examples. Thanks to textresources on the Web, and since statistical packages have the ability toanalyze string data, it is now easy to use language-based examples in astatistics class. Three such examples are discussed here. First, many types ofwordplay (e.g., crosswords and hangman) involve finding words with letters thatsatisfy a certain pattern. Second, linguistics has shown that idiomatic pairsof words often appear together more frequently than chance. For example, in theBrown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E-10.)Third, a pangram contains all the letters of the alphabet at least once. Theseare searched for in Charles Dickens' A Christmas Carol, and their lengths arecompared to the expected value given by the unequal probability couponcollector's problem as well as simulations.
arxiv-7500-107 | Explain Images with Multimodal Recurrent Neural Networks | http://arxiv.org/pdf/1410.1090v1.pdf | author:Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille category:cs.CV cs.CL cs.LG published:2014-10-04 summary:In this paper, we present a multimodal Recurrent Neural Network (m-RNN) modelfor generating novel sentence descriptions to explain the content of images. Itdirectly models the probability distribution of generating a word givenprevious words and the image. Image descriptions are generated by sampling fromthis distribution. The model consists of two sub-networks: a deep recurrentneural network for sentences and a deep convolutional network for images. Thesetwo sub-networks interact with each other in a multimodal layer to form thewhole m-RNN model. The effectiveness of our model is validated on threebenchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our modeloutperforms the state-of-the-art generative method. In addition, the m-RNNmodel can be applied to retrieval tasks for retrieving images or sentences, andachieves significant performance improvement over the state-of-the-art methodswhich directly optimize the ranking objective function for retrieval.
arxiv-7500-108 | Generating abbreviations using Google Books library | http://arxiv.org/pdf/1410.1080v1.pdf | author:Valery D. Solovyev, Vladimir V. Bochkarev category:cs.CL stat.AP 91F20, 62P25 I.2.7; J.5 published:2014-10-04 summary:The article describes the original method of creating a dictionary ofabbreviations based on the Google Books Ngram Corpus. The dictionary ofabbreviations is designed for Russian, yet as its methodology is universal itcan be applied to any language. The dictionary can be used to define thefunction of the period during text segmentation in various applied systems oftext processing. The article describes difficulties encountered in the processof its construction as well as the ways to overcome them. A model of evaluatinga probability of first and second type errors (extraction accuracy andfullness) is constructed. Certain statistical data for the use of abbreviationsare provided.
arxiv-7500-109 | Gamma Processes, Stick-Breaking, and Variational Inference | http://arxiv.org/pdf/1410.1068v1.pdf | author:Anirban Roychowdhury, Brian Kulis category:stat.ML cs.LG published:2014-10-04 summary:While most Bayesian nonparametric models in machine learning have focused onthe Dirichlet process, the beta process, or their variants, the gamma processhas recently emerged as a useful nonparametric prior in its own right. Currentinference schemes for models involving the gamma process are restricted toMCMC-based methods, which limits their scalability. In this paper, we present avariational inference framework for models involving gamma process priors. Ourapproach is based on a novel stick-breaking constructive definition of thegamma process. We prove correctness of this stick-breaking process by using thecharacterization of the gamma process as a completely random measure (CRM), andwe explicitly derive the rate measure of our construction using Poisson processmachinery. We also derive error bounds on the truncation of the infiniteprocess required for variational inference, similar to the truncation analysesfor other nonparametric models based on the Dirichlet and beta processes. Ourrepresentation is then used to derive a variational inference algorithm for aparticular Bayesian nonparametric latent structure formulation known as theinfinite Gamma-Poisson model, where the latent variables are drawn from a gammaprocess prior with Poisson likelihoods. Finally, we present results for ouralgorithms on nonnegative matrix factorization tasks on document corpora, andshow that we compare favorably to both sampling-based techniques andvariational approaches based on beta-Bernoulli priors.
arxiv-7500-110 | Facial Feature Point Detection: A Comprehensive Survey | http://arxiv.org/pdf/1410.1037v1.pdf | author:Nannan Wang, Xinbo Gao, Dacheng Tao, Xuelong Li category:cs.CV published:2014-10-04 summary:This paper presents a comprehensive survey of facial feature point detectionwith the assistance of abundant manually labeled images. Facial feature pointdetection favors many applications such as face recognition, animation,tracking, hallucination, expression analysis and 3D face modeling. Existingmethods can be categorized into the following four groups: constrained localmodel (CLM)-based, active appearance model (AAM)-based, regression-based, andother methods. CLM-based methods consist of a shape model and a number of localexperts, each of which is utilized to detect a facial feature point. AAM-basedmethods fit a shape model to an image by minimizing texture synthesis errors.Regression-based methods directly learn a mapping function from facial imageappearance to facial feature points. Besides the above three major categoriesof methods, there are also minor categories of methods which we classify intoother methods: graphical model-based methods, joint face alignment methods,independent facial feature point detectors, and deep learning-based methods.Though significant progress has been made, facial feature point detection islimited in its success by wild and real-world conditions: variations acrossposes, expressions, illuminations, and occlusions. A comparative illustrationand analysis of representative methods provide us a holistic understanding anddeep insight into facial feature point detection, which also motivates us toexplore promising future directions.
arxiv-7500-111 | Minimax Analysis of Active Learning | http://arxiv.org/pdf/1410.0996v1.pdf | author:Steve Hanneke, Liu Yang category:cs.LG math.ST stat.ML stat.TH published:2014-10-03 summary:This work establishes distribution-free upper and lower bounds on the minimaxlabel complexity of active learning with general hypothesis classes, undervarious noise models. The results reveal a number of surprising facts. Inparticular, under the noise model of Tsybakov (2004), the minimax labelcomplexity of active learning with a VC class is always asymptotically smallerthan that of passive learning, and is typically significantly smaller than thebest previously-published upper bounds in the active learning literature. Inhigh-noise regimes, it turns out that all active learning problems of a givenVC dimension have roughly the same minimax label complexity, which contrastswith well-known results for bounded noise. In low-noise regimes, we find thatthe label complexity is well-characterized by a simple combinatorial complexitymeasure we call the star number. Interestingly, we find that almost all of thecomplexity measures previously explored in the active learning literature haveworst-case values exactly equal to the star number. We also propose new activelearning strategies that nearly achieve these minimax label complexities.
arxiv-7500-112 | Contributions of natural ventilation on thermal performance of alternative floor plan designs | http://arxiv.org/pdf/1410.0948v1.pdf | author:Eugénio Rodrigues, Adélio R. Gaspar, Álvaro Gomes, Manuel Gameiro da Silva category:cs.NE cs.SE D.2.2; G.1.6 published:2014-10-03 summary:During the earliest phase of architectural design process, practitionersafter analyzing the client's design program, legal requirements, topographicconstraints, and preferences synthesize these requirements into architecturalfloor plan drawings. Design decisions taken in this phase may significantlycontribute to the building performance. On account of this reason, it isimportant to estimate and compare alternative solutions, when it is stillmanageable to change the building design. The authors have been developing a prototype tool to assist architects duringthis initial design phase. It is made up of two algorithms. The first algorithmgenerates alternative floor plans according to the architect's preferences andrequirements, and the client's design program. It consists in one evolutionarystrategy approach enhanced with local search technique to allocate rooms onseveral levels in the two-dimensional space. The second algorithm evaluates,ranks, and optimizes those floor plans according to thermal performancecriteria. The prototype tool is coupled with dynamic simulation program, whichestimates the thermal behavior of each solution. A sequential variableoptimization is used to change several geometric values of differentarchitectural elements in the floor plans to explore the improvement potential. In the present communication, the two algorithms are used in an iterativeprocess to generate and optimize the thermal performance of alternative floorplans. In the building simulation specifications of EnergyPlus program, theairflow network model has been used in order to adequately model the airinfiltration and the airflows through indoor spaces. A case study of asingle-family house with three rooms in a single level is presented.
arxiv-7500-113 | Cortical spatio-temporal dimensionality reduction for visual grouping | http://arxiv.org/pdf/1407.0733v2.pdf | author:Giacomo Cocci, Davide Barbieri, Giovanna Citti, Alessandro Sarti category:cs.CV cs.NE q-bio.NC stat.ML published:2014-07-02 summary:The visual systems of many mammals, including humans, is able to integratethe geometric information of visual stimuli and to perform cognitive tasksalready at the first stages of the cortical processing. This is thought to bethe result of a combination of mechanisms, which include feature extraction atsingle cell level and geometric processing by means of cells connectivity. Wepresent a geometric model of such connectivities in the space of detectedfeatures associated to spatio-temporal visual stimuli, and show how they can beused to obtain low-level object segmentation. The main idea is that of defininga spectral clustering procedure with anisotropic affinities over datasetsconsisting of embeddings of the visual stimuli into higher dimensional spaces.Neural plausibility of the proposed arguments will be discussed.
arxiv-7500-114 | Probit Normal Correlated Topic Models | http://arxiv.org/pdf/1410.0908v1.pdf | author:Xingchen Yu, Ernest Fokoue category:stat.ML cs.IR cs.LG 62H25, 62H30 published:2014-10-03 summary:The logistic normal distribution has recently been adapted via thetransformation of multivariate Gaus- sian variables to model the topicaldistribution of documents in the presence of correlations among topics. In thispaper, we propose a probit normal alternative approach to modelling correlatedtopical structures. Our use of the probit model in the context of topicdiscovery is novel, as many authors have so far con- centrated solely of thelogistic model partly due to the formidable inefficiency of the multinomialprobit model even in the case of very small topical spaces. We hereincircumvent the inefficiency of multinomial probit estimation by using anadaptation of the diagonal orthant multinomial probit in the topic modelscontext, resulting in the ability of our topic modelling scheme to handlecorpuses with a large number of latent topics. An additional and very importantbenefit of our method lies in the fact that unlike with the logistic normalmodel whose non-conjugacy leads to the need for sophisticated sampling schemes,our ap- proach exploits the natural conjugacy inherent in the auxiliaryformulation of the probit model to achieve greater simplicity. The applicationof our proposed scheme to a well known Associated Press corpus not only helpsdiscover a large number of meaningful topics but also reveals the capturing ofcompellingly intuitive correlations among certain topics. Besides, our proposedapproach lends itself to even further scalability thanks to various existinghigh performance algorithms and architectures capable of handling millions ofdocuments.
arxiv-7500-115 | Fast Prediction with SVM Models Containing RBF Kernels | http://arxiv.org/pdf/1403.0736v3.pdf | author:Marc Claesen, Frank De Smet, Johan A. K. Suykens, Bart De Moor category:stat.ML cs.LG published:2014-03-04 summary:We present an approximation scheme for support vector machine models that usean RBF kernel. A second-order Maclaurin series approximation is used forexponentials of inner products between support vectors and test instances. Theapproximation is applicable to all kernel methods featuring sums of kernelevaluations and makes no assumptions regarding data normalization. Theprediction speed of approximated models no longer relates to the amount ofsupport vectors but is quadratic in terms of the number of input dimensions. Ifthe number of input dimensions is small compared to the amount of supportvectors, the approximated model is significantly faster in prediction and has asmaller memory footprint. An optimized C++ implementation was made to assessthe gain in prediction speed in a set of practical tests. We additionallyprovide a method to verify the approximation accuracy, prior to training modelsor during run-time, to ensure the loss in accuracy remains acceptable andwithin known bounds.
arxiv-7500-116 | Group Orbit Optimization: A Unified Approach to Data Normalization | http://arxiv.org/pdf/1410.0868v1.pdf | author:Shuchang Zhou, Zhihua Zhang, Xiaobing Feng category:cs.NA cs.CV math.NA 15-02 G.1.3; I.5.4 published:2014-10-03 summary:In this paper we propose and study an optimization problem over a matrixgroup orbit that we call \emph{Group Orbit Optimization} (GOO). We prove thatGOO can be used to induce matrix decomposition techniques such as singularvalue decomposition (SVD), LU decomposition, QR decomposition, Schurdecomposition and Cholesky decomposition, etc. This gives rise to a unifiedframework for matrix decomposition and allows us to bridge these matrixdecomposition methods. Moreover, we generalize GOO for tensor decomposition. Asa concrete application of GOO, we devise a new data decomposition method over aspecial linear group to normalize point cloud data. Experiment results showthat our normalization method is able to obtain recovery well from distortionslike shearing, rotation and squeezing.
arxiv-7500-117 | Individualized Rank Aggregation using Nuclear Norm Regularization | http://arxiv.org/pdf/1410.0860v1.pdf | author:Yu Lu, Sahand N. Negahban category:stat.ML published:2014-10-03 summary:In recent years rank aggregation has received significant attention from themachine learning community. The goal of such a problem is to combine the(partially revealed) preferences over objects of a large population into asingle, relatively consistent ordering of those objects. However, in manycases, we might not want a single ranking and instead opt for individualrankings. We study a version of the problem known as collaborative ranking. Inthis problem we assume that individual users provide us with pairwisepreferences (for example purchasing one item over another). From thosepreferences we wish to obtain rankings on items that the users have not had anopportunity to explore. The results here have a very interesting connection tothe standard matrix completion problem. We provide a theoretical justificationfor a nuclear norm regularized optimization procedure, and providehigh-dimensional scaling results that show how the error in estimating userpreferences behaves as the number of observations increase.
arxiv-7500-118 | Feature Learning from Incomplete EEG with Denoising Autoencoder | http://arxiv.org/pdf/1410.0818v1.pdf | author:Junhua Li, Zbigniew Struzik, Liqing Zhang, Andrzej Cichocki category:cs.CV q-bio.NC published:2014-10-03 summary:An alternative pathway for the human brain to communicate with the outsideworld is by means of a brain computer interface (BCI). A BCI can decodeelectroencephalogram (EEG) signals of brain activities, and then send a commandor an intent to an external interactive device, such as a wheelchair. Theeffectiveness of the BCI depends on the performance in decoding the EEG.Usually, the EEG is contaminated by different kinds of artefacts (e.g.,electromyogram (EMG), background activity), which leads to a low decodingperformance. A number of filtering methods can be utilized to remove or weakenthe effects of artefacts, but they generally fail when the EEG contains extremeartefacts. In such cases, the most common approach is to discard the whole datasegment containing extreme artefacts. This causes the fatal drawback that theBCI cannot output decoding results during that time. In order to solve thisproblem, we employ the Lomb-Scargle periodogram to estimate the spectral powerfrom incomplete EEG (after removing only parts contaminated by artefacts), andDenoising Autoencoder (DAE) for learning. The proposed method is evaluated withmotor imagery EEG data. The results show that our method can successfullydecode incomplete EEG to good effect.
arxiv-7500-119 | Learning manifold to regularize nonnegative matrix factorization | http://arxiv.org/pdf/1410.2191v1.pdf | author:Jim Jing-Yan Wang, Xin Gao category:cs.LG published:2014-10-03 summary:Inthischapterwediscusshowtolearnanoptimalmanifoldpresentationto regularizenonegative matrix factorization (NMF) for data representation problems.NMF,whichtriestorepresentanonnegativedatamatrixasaproductoftwolowranknonnegative matrices, has been a popular method for data representation due toits ability to explore the latent part-based structure of data. Recent studyshows that lots of data distributions have manifold structures, and we shouldrespect the manifold structure when the data are represented. Recently,manifold regularized NMF used a nearest neighbor graph to regulate the learningof factorization parameter matrices and has shown its advantage overtraditional NMF methods for data representation problems. However, how toconstruct an optimal graph to present the manifold prop- erly remains adifficultproblem due to the graph modelselection, noisy features, and nonlineardistributed data. In this chapter, we introduce three effective methods tosolve these problems of graph construction for manifold regularized NMF.Multiple graph learning is proposed to solve the problem of graph modelselection, adaptive graph learning via feature selection is proposed to solvethe problem of constructing a graph from noisy features, while multi-kernellearning-based graph construction is used to solve the problem of learning agraph from nonlinearly distributed data.
arxiv-7500-120 | Language Modeling with Power Low Rank Ensembles | http://arxiv.org/pdf/1312.7077v2.pdf | author:Ankur P. Parikh, Avneesh Saluja, Chris Dyer, Eric P. Xing category:cs.CL cs.LG stat.ML published:2013-12-26 summary:We present power low rank ensembles (PLRE), a flexible framework for n-gramlanguage modeling where ensembles of low rank matrices and tensors are used toobtain smoothed probability estimates of words in context. Our method can beunderstood as a generalization of n-gram modeling to non-integer n, andincludes standard techniques such as absolute discounting and Kneser-Neysmoothing as special cases. PLRE training is efficient and our approachoutperforms state-of-the-art modified Kneser Ney baselines in terms ofperplexity on large corpora as well as on BLEU score in a downstream machinetranslation task.
arxiv-7500-121 | Linear State-Space Model with Time-Varying Dynamics | http://arxiv.org/pdf/1410.0555v2.pdf | author:Jaakko Luttinen, Tapani Raiko, Alexander Ilin category:stat.ML published:2014-10-02 summary:This paper introduces a linear state-space model with time-varying dynamics.The time dependency is obtained by forming the state dynamics matrix as atime-varying linear combination of a set of matrices. The time dependency ofthe weights in the linear combination is modelled by another linear Gaussiandynamical model allowing the model to learn how the dynamics of the processchanges. Previous approaches have used switching models which have a small setof possible state dynamics matrices and the model selects one of those matricesat each time, thus jumping between them. Our model forms the dynamics as alinear combination and the changes can be smooth and more continuous. The modelis motivated by physical processes which are described by linear partialdifferential equations whose parameters vary in time. An example of such aprocess could be a temperature field whose evolution is driven by a varyingwind direction. The posterior inference is performed using variational Bayesianapproximation. The experiments on stochastic advection-diffusion processes andreal-world weather processes show that the model with time-varying dynamics canoutperform previously introduced approaches.
arxiv-7500-122 | Generalized Laguerre Reduction of the Volterra Kernel for Practical Identification of Nonlinear Dynamic Systems | http://arxiv.org/pdf/1410.0741v1.pdf | author:Brett W. Israelsen, Dale A. Smith category:cs.LG published:2014-10-03 summary:The Volterra series can be used to model a large subset of nonlinear, dynamicsystems. A major drawback is the number of coefficients required model suchsystems. In order to reduce the number of required coefficients, Laguerrepolynomials are used to estimate the Volterra kernels. Existing literatureproposes algorithms for a fixed number of Volterra kernels, and Laguerreseries. This paper presents a novel algorithm for generalized calculation ofthe finite order Volterra-Laguerre (VL) series for a MIMO system. An exampleaddresses the utility of the algorithm in practical application.
arxiv-7500-123 | Sparse Additive Model using Symmetric Nonnegative Definite Smoothers | http://arxiv.org/pdf/1409.2552v3.pdf | author:Yan Li category:stat.ML cs.LG published:2014-09-08 summary:We introduce a new algorithm, called adaptive sparse backfitting algorithm,for solving high dimensional Sparse Additive Model (SpAM) utilizing symmetric,non-negative definite smoothers. Unlike the previous sparse backfittingalgorithm, our method is essentially a block coordinate descent algorithm thatguarantees to converge to the optimal solution. It bridges the gap between thepopulation backfitting algorithm and that of the data version. We also provevariable selection consistency under suitable conditions. Numerical studies onboth synthesis and real data are conducted to show that adaptive sparsebackfitting algorithm outperforms previous sparse backfitting algorithm infitting and predicting high dimensional nonparametric models.
arxiv-7500-124 | Bayesian Network Structure Learning Using Quantum Annealing | http://arxiv.org/pdf/1407.3897v2.pdf | author:Bryan O'Gorman, Alejandro Perdomo-Ortiz, Ryan Babbush, Alan Aspuru-Guzik, Vadim Smelyanskiy category:quant-ph cs.LG published:2014-07-15 summary:We introduce a method for the problem of learning the structure of a Bayesiannetwork using the quantum adiabatic algorithm. We do so by introducing anefficient reformulation of a standard posterior-probability scoring function ongraphs as a pseudo-Boolean function, which is equivalent to a system of 2-bodyIsing spins, as well as suitable penalty terms for enforcing the constraintsnecessary for the reformulation; our proposed method requires $\mathcal O(n^2)$qubits for $n$ Bayesian network variables. Furthermore, we prove lower boundson the necessary weighting of these penalty terms. The logical structureresulting from the mapping has the appealing property that it isinstance-independent for a given number of Bayesian network variables, as wellas being independent of the number of data cases.
arxiv-7500-125 | Deep Directed Generative Autoencoders | http://arxiv.org/pdf/1410.0630v1.pdf | author:Sherjil Ozair, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2014-10-02 summary:For discrete data, the likelihood $P(x)$ can be rewritten exactly andparametrized into $P(X = x) = P(X = x H = f(x)) P(H = f(x))$ if $P(X H)$has enough capacity to put no probability mass on any $x'$ for which $f(x')\neqf(x)$, where $f(\cdot)$ is a deterministic discrete function. The log of thefirst factor gives rise to the log-likelihood reconstruction error of anautoencoder with $f(\cdot)$ as the encoder and $P(XH)$ as the (probabilistic)decoder. The log of the second term can be seen as a regularizer on the encodedactivations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decodercan be represented by a deep neural network and trained to maximize the averageof the optimal log-likelihood $\log p(x)$. The objective is to learn an encoder$f(\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than$X$ itself, estimated by $P(H)$. This "flattens the manifold" or concentratesprobability mass in a smaller number of (relevant) dimensions over which thedistribution factorizes. Generating samples from the model is straightforwardusing ancestral sampling. One challenge is that regular back-propagation cannotbe used to obtain the gradient on the parameters of the encoder, but we findthat using the straight-through estimator works well here. We also find thatalthough optimizing a single level of such architecture may be difficult, muchbetter results can be obtained by pre-training and stacking them, graduallytransforming the data distribution into one that is more easily captured by asimple parametric model.
arxiv-7500-126 | Dictionary learning for fast classification based on soft-thresholding | http://arxiv.org/pdf/1402.1973v2.pdf | author:Alhussein Fawzi, Mike Davies, Pascal Frossard category:cs.CV cs.LG stat.ML published:2014-02-09 summary:Classifiers based on sparse representations have recently been shown toprovide excellent results in many visual recognition and classification tasks.However, the high cost of computing sparse representations at test time is amajor obstacle that limits the applicability of these methods in large-scaleproblems, or in scenarios where computational power is restricted. We considerin this paper a simple yet efficient alternative to sparse coding for featureextraction. We study a classification scheme that applies the soft-thresholdingnonlinear mapping in a dictionary, followed by a linear classifier. A novelsupervised dictionary learning algorithm tailored for this low complexityclassification architecture is proposed. The dictionary learning problem, whichjointly learns the dictionary and linear classifier, is cast as a difference ofconvex (DC) program and solved efficiently with an iterative DC solver. Weconduct experiments on several datasets, and show that our learning algorithmthat leverages the structure of the classification problem outperforms genericlearning procedures. Our simple classifier based on soft-thresholding alsocompetes with the recent sparse coding classifiers, when the dictionary islearned appropriately. The adopted classification scheme further requires lesscomputational time at the testing stage, compared to other classifiers. Theproposed scheme shows the potential of the adequately trained soft-thresholdingmapping for classification and paves the way towards the development of veryefficient classification methods for vision problems.
arxiv-7500-127 | A probabilistic evolutionary optimization approach to compute quasiparticle braids | http://arxiv.org/pdf/1410.0602v1.pdf | author:Roberto Santana, Ross B. McDonald, Helmut G. Katzgraber category:quant-ph cs.NE published:2014-10-02 summary:Topological quantum computing is an alternative framework for avoiding thequantum decoherence problem in quantum computation. The problem of executing agate in this framework can be posed as the problem of braiding quasiparticles.Because these are not Abelian, the problem can be reduced to finding an optimalproduct of braid generators where the optimality is defined in terms of thegate approximation and the braid's length. In this paper we propose the use ofdifferent variants of estimation of distribution algorithms to deal with theproblem. Furthermore, we investigate how the regularities of the braidoptimization problem can be translated into statistical regularities by meansof the Boltzmann distribution. We show that our best algorithm is able toproduce many solutions that approximates the target gate with an accuracy inthe order of $10^{-6}$, and have lengths up to 9 times shorter than thoseexpected from braids of the same accuracy obtained with other methods.
arxiv-7500-128 | Mapping Energy Landscapes of Non-Convex Learning Problems | http://arxiv.org/pdf/1410.0576v1.pdf | author:Maria Pavlovskaia, Kewei Tu, Song-Chun Zhu category:stat.ML cs.LG published:2014-10-02 summary:In many statistical learning problems, the target functions to be optimizedare highly non-convex in various model spaces and thus are difficult toanalyze. In this paper, we compute \emph{Energy Landscape Maps} (ELMs) whichcharacterize and visualize an energy function with a tree structure, in whicheach leaf node represents a local minimum and each non-leaf node represents thebarrier between adjacent energy basins. The ELM also associates each node withthe estimated probability mass and volume for the corresponding energy basin.We construct ELMs by adopting the generalized Wang-Landau algorithm andmulti-domain sampler that simulates a Markov chain traversing the model spaceby dynamically reweighting the energy function. We construct ELMs in the modelspace for two classic statistical learning problems: i) clustering withGaussian mixture models or Bernoulli templates; and ii) bi-clustering. Wepropose a way to measure the difficulties (or complexity) of these learningproblems and study how various conditions affect the landscape complexity, suchas separability of the clusters, the number of examples, and the level ofsupervision; and we also visualize the behaviors of different algorithms, suchas K-mean, EM, two-step EM and Swendsen-Wang cuts, in the energy landscapes.
arxiv-7500-129 | Stochastic Discriminative EM | http://arxiv.org/pdf/1410.1784v1.pdf | author:Andres R. Masegosa category:cs.LG published:2014-10-02 summary:Stochastic discriminative EM (sdEM) is an online-EM-type algorithm fordiscriminative training of probabilistic generative models belonging to theexponential family. In this work, we introduce and justify this algorithm as astochastic natural gradient descent method, i.e. a method which accounts forthe information geometry in the parameter space of the statistical model. Weshow how this learning algorithm can be used to train probabilistic generativemodels by minimizing different discriminative loss functions, such as thenegative conditional log-likelihood and the Hinge loss. The resulting modelstrained by sdEM are always generative (i.e. they define a joint probabilitydistribution) and, in consequence, allows to deal with missing data and latentvariables in a principled way either when being learned or when makingpredictions. The performance of this method is illustrated by several textclassification problems for which a multinomial naive Bayes and a latentDirichlet allocation based classifier are learned using differentdiscriminative loss functions.
arxiv-7500-130 | Deep Sequential Neural Network | http://arxiv.org/pdf/1410.0510v1.pdf | author:Ludovic Denoyer, Patrick Gallinari category:cs.LG cs.NE published:2014-10-02 summary:Neural Networks sequentially build high-level features through theirsuccessive layers. We propose here a new neural network model where each layeris associated with a set of candidate mappings. When an input is processed, ateach layer, one mapping among these candidates is selected according to asequential decision process. The resulting model is structured according to aDAG like architecture, so that a path from the root to a leaf node defines asequence of transformations. Instead of considering global transformations,like in classical multilayer networks, this model allows us for learning a setof local transformations. It is thus able to process data with differentcharacteristics through specific sequences of such local transformations,increasing the expression power of this model w.r.t a classical multilayerednetwork. The learning algorithm is inspired from policy gradient techniquescoming from the reinforcement learning domain and is used here instead of theclassical back-propagation based gradient descent techniques. Experiments ondifferent datasets show the relevance of this approach.
arxiv-7500-131 | Generating functionals for computational intelligence: the Fisher information as an objective function for self-limiting Hebbian learning rules | http://arxiv.org/pdf/1410.0507v1.pdf | author:Rodrigo Echeveste, Claudius Gros category:q-bio.NC cs.NE published:2014-10-02 summary:Generating functionals may guide the evolution of a dynamical system andconstitute a possible route for handling the complexity of neural networks asrelevant for computational intelligence. We propose and explore a new objectivefunction, which allows to obtain plasticity rules for the afferent synapticweights. The adaption rules are Hebbian, self-limiting, and result from theminimization of the Fisher information with respect to the synaptic flux. Weperform a series of simulations examining the behavior of the new learningrules in various circumstances. The vector of synaptic weights aligns with theprincipal direction of input activities, whenever one is present. A lineardiscrimination is performed when there are two or more principal directions;directions having bimodal firing-rate distributions, being characterized by anegative excess kurtosis, are preferred. We find robust performance and fullhomeostatic adaption of the synaptic weights results as a by-product of thesynaptic flux minimization. This self-limiting behavior allows for stableonline learning for arbitrary durations. The neuron acquires new informationwhen the statistics of input activities is changed at a certain point of thesimulation, showing however, a distinct resilience to unlearn previouslyacquired knowledge. Learning is fast when starting with randomly drawn synapticweights and substantially slower when the synaptic weights are already fullyadapted.
arxiv-7500-132 | Recognition of Handwritten Bangla Basic Characters and Digits using Convex Hull based Feature Set | http://arxiv.org/pdf/1410.0478v1.pdf | author:Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri category:cs.CV published:2014-10-02 summary:In dealing with the problem of recognition of handwritten character patternsof varying shapes and sizes, selection of a proper feature set is important toachieve high recognition performance. The current research aims to evaluate theperformance of the convex hull based feature set, i.e. 125 features in allcomputed over different bays attributes of the convex hull of a pattern, foreffective recognition of isolated handwritten Bangla basic characters anddigits. On experimentation with a database of 10000 samples, the maximumrecognition rate of 76.86% is observed for handwritten Bangla characters. ForBangla numerals the maximum success rate of 99.45%. is achieved on a databaseof 12000 sample. The current work validates the usefulness of a new kind offeature set for recognition of handwritten Bangla basic characters andnumerals.
arxiv-7500-133 | Identification of Dynamic functional brain network states Through Tensor Decomposition | http://arxiv.org/pdf/1410.0446v1.pdf | author:Arash Golibagh Mahyari, Selin Aviyente category:cs.NE q-bio.NC published:2014-10-02 summary:With the advances in high resolution neuroimaging, there has been a growinginterest in the detection of functional brain connectivity. Complex networktheory has been proposed as an attractive mathematical representation offunctional brain networks. However, most of the current studies of functionalbrain networks have focused on the computation of graph theoretic indices forstatic networks, i.e. long-time averages of connectivity networks. It iswell-known that functional connectivity is a dynamic process and theconstruction and reorganization of the networks is key to understanding humancognition. Therefore, there is a growing need to track dynamic functional brainnetworks and identify time intervals over which the network isquasi-stationary. In this paper, we present a tensor decomposition based methodto identify temporally invariant 'network states' and find a common topographicrepresentation for each state. The proposed methods are applied toelectroencephalogram (EEG) data during the study of error-related negativity(ERN).
arxiv-7500-134 | Scalable Nonlinear Learning with Adaptive Polynomial Expansions | http://arxiv.org/pdf/1410.0440v1.pdf | author:Alekh Agarwal, Alina Beygelzimer, Daniel Hsu, John Langford, Matus Telgarsky category:cs.LG stat.ML published:2014-10-02 summary:Can we effectively learn a nonlinear representation in time comparable tolinear learning? We describe a new algorithm that explicitly and adaptivelyexpands higher-order interaction features over base linear representations. Thealgorithm is designed for extreme computational efficiency, and an extensiveexperimental study shows that its computation/prediction tradeoff abilitycompares very favorably against strong baselines.
arxiv-7500-135 | Methods and Models for Interpretable Linear Classification | http://arxiv.org/pdf/1405.4047v2.pdf | author:Berk Ustun, Cynthia Rudin category:stat.ME cs.LG stat.ML published:2014-05-16 summary:We present an integer programming framework to build accurate andinterpretable discrete linear classification models. Unlike existingapproaches, our framework is designed to provide practitioners with the controland flexibility they need to tailor accurate and interpretable models for adomain of choice. To this end, our framework can produce models that are fullyoptimized for accuracy, by minimizing the 0--1 classification loss, and thataddress multiple aspects of interpretability, by incorporating a range ofdiscrete constraints and penalty functions. We use our framework to producemodels that are difficult to create with existing methods, such as scoringsystems and M-of-N rule tables. In addition, we propose specially designedoptimization methods to improve the scalability of our framework throughdecomposition and data reduction. We show that discrete linear classifiers canattain the training accuracy of any other linear classifier, and provide anOccam's Razor type argument as to why the use of small discrete coefficientscan provide better generalization. We demonstrate the performance andflexibility of our framework through numerical experiments and a case study inwhich we construct a highly tailored clinical tool for sleep apnea diagnosis.
arxiv-7500-136 | Learning to Transfer Privileged Information | http://arxiv.org/pdf/1410.0389v1.pdf | author:Viktoriia Sharmanska, Novi Quadrianto, Christoph H. Lampert category:cs.CV stat.ML published:2014-10-01 summary:We introduce a learning framework called learning using privilegedinformation (LUPI) to the computer vision field. We focus on the prototypicalcomputer vision problem of teaching computers to recognize objects in images.We want the computers to be able to learn faster at the expense of providingextra information during training time. As additional information about theimage data, we look at several scenarios that have been studied in computervision before: attributes, bounding boxes and image tags. The information isprivileged as it is available at training time but not at test time. We exploretwo maximum-margin techniques that are able to make use of this additionalsource of information, for binary and multiclass object classification. Weinterpret these methods as learning easiness and hardness of the objects in theprivileged space and then transferring this knowledge to train a betterclassifier in the original space. We provide a thorough analysis and comparisonof information transfer from privileged to the original data spaces for bothLUPI methods. Our experiments show that incorporating privileged informationcan improve the classification accuracy. Finally, we conduct user studies tounderstand which samples are easy and which are hard for human learning, andexplore how this information is related to easy and hard samples when learninga classifier.
arxiv-7500-137 | Domain adaptation of weighted majority votes via perturbed variation-based self-labeling | http://arxiv.org/pdf/1410.0334v1.pdf | author:Emilie Morvant category:stat.ML cs.LG published:2014-10-01 summary:In machine learning, the domain adaptation problem arrives when the test(target) and the train (source) data are generated from differentdistributions. A key applied issue is thus the design of algorithms able togeneralize on a new distribution, for which we have no label information. Wefocus on learning classification models defined as a weighted majority voteover a set of real-val ued functions. In this context, Germain et al. (2013)have shown that a measure of disagreement between these functions is crucial tocontrol. The core of this measure is a theoretical bound--the C-bound (Lacasseet al., 2007)--which involves the disagreement and leads to a well performingmajority vote learning algorithm in usual non-adaptative supervised setting:MinCq. In this work, we propose a framework to extend MinCq to a domainadaptation scenario. This procedure takes advantage of the recent perturbedvariation divergence between distributions proposed by Harel and Mannor (2012).Justified by a theoretical bound on the target risk of the vote, we provide toMinCq a target sample labeled thanks to a perturbed variation-basedself-labeling focused on the regions where the source and target marginalsappear similar. We also study the influence of our self-labeling, from which wededuce an original process for tuning the hyperparameters. Finally, ourframework called PV-MinCq shows very promising results on a rotation andtranslation synthetic problem.
arxiv-7500-138 | Deformable Part Models are Convolutional Neural Networks | http://arxiv.org/pdf/1409.5403v2.pdf | author:Ross Girshick, Forrest Iandola, Trevor Darrell, Jitendra Malik category:cs.CV published:2014-09-18 summary:Deformable part models (DPMs) and convolutional neural networks (CNNs) aretwo widely used tools for visual recognition. They are typically viewed asdistinct approaches: DPMs are graphical models (Markov random fields), whileCNNs are "black-box" non-linear classifiers. In this paper, we show that a DPMcan be formulated as a CNN, thus providing a novel synthesis of the two ideas.Our construction involves unrolling the DPM inference algorithm and mappingeach step to an equivalent (and at times novel) CNN layer. From thisperspective, it becomes natural to replace the standard image features used inDPM with a learned feature extractor. We call the resulting model DeepPyramidDPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantlyoutperforms DPMs based on histograms of oriented gradients features (HOG) andslightly outperforms a comparable version of the recently introduced R-CNNdetection system, while running an order of magnitude faster.
arxiv-7500-139 | Using social network graph analysis for interest detection | http://arxiv.org/pdf/1410.0316v1.pdf | author:Brian Lee Yung Rowe category:cs.SI cs.CL physics.soc-ph published:2014-10-01 summary:A person's interests exist as an internal state and are difficult to define.Since only external actions are observable, a proxy must be used thatrepresents someone's interests. Techniques like collaborative filtering,behavioral targeting, and hashtag analysis implicitly model an individual'sinterests. I argue that these models are limited to shallow, temporaryinterests, which do not reflect people's deeper interests or passions. Ipropose an alternative model of interests that takes advantage of a user'ssocial graph. The basic principle is that people only follow those thatinterest them, so the social graph is an effective and robust proxy forpeople's interests.
arxiv-7500-140 | LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible | http://arxiv.org/pdf/1410.0286v1.pdf | author:Dirk Roorda, Gino Kalkman, Martijn Naaijer, Andreas van Cranenburgh category:cs.CL published:2014-10-01 summary:The Linguistic Annotation Framework (LAF) provides a general, extensiblestand-off markup system for corpora. This paper discusses LAF-Fabric, a newtool to analyse LAF resources in general with an extension to process theHebrew Bible in particular. We first walk through the history of the HebrewBible as text database in decennium-wide steps. Then we describe how LAF-Fabricmay serve as an analysis tool for this corpus. Finally, we describe threeanalytic projects/workflows that benefit from the new LAF representation: 1) the study of linguistic variation: extract cooccurrence data of commonnouns between the books of the Bible (Martijn Naaijer); 2) the study of thegrammar of Hebrew poetry in the Psalms: extract clause typology (Gino Kalkman);3) construction of a parser of classical Hebrew by Data Oriented Parsing:generate tree structures from the database (Andreas van Cranenburgh).
arxiv-7500-141 | Pattern Encoding on the Poincare Sphere | http://arxiv.org/pdf/1410.0243v1.pdf | author:Aleksandra Pizurica category:cs.CV published:2014-10-01 summary:This paper presents a convenient graphical tool for encoding visual patterns(such as image patches and image atoms) as point constellations in a spacespanned by perceptual features and with a clear geometrical interpretation.General theory and a practical pattern encoding scheme are presented, inspiredby encoding polarization states of a light wave on the Poincare sphere. Thisnew pattern encoding scheme can be useful for many applications in imageprocessing and computer vision. Here, three possible applications areillustrated, in clustering perceptually similar patterns, visualizingproperties of learned dictionaries of image atoms and generating newdictionaries of image atoms from spherical codes.
arxiv-7500-142 | Reservoir Computing using Cellular Automata | http://arxiv.org/pdf/1410.0162v1.pdf | author:Ozgur Yilmaz category:cs.NE published:2014-10-01 summary:We introduce a novel framework of reservoir computing. Cellular automaton isused as the reservoir of dynamical systems. Input is randomly projected ontothe initial conditions of automaton cells and nonlinear computation isperformed on the input via application of a rule in the automaton for a periodof time. The evolution of the automaton creates a space-time volume of theautomaton state space, and it is used as the reservoir. The proposed frameworkis capable of long short-term memory and it requires orders of magnitude lesscomputation compared to Echo State Networks. Also, for additive cellularautomaton rules, reservoir features can be combined using Boolean operations,which provides a direct way for concept building and symbolic processing, andit is much more efficient compared to state-of-the-art approaches.
arxiv-7500-143 | Deep Tempering | http://arxiv.org/pdf/1410.0123v1.pdf | author:Guillaume Desjardins, Heng Luo, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML published:2014-10-01 summary:Restricted Boltzmann Machines (RBMs) are one of the fundamental buildingblocks of deep learning. Approximate maximum likelihood training of RBMstypically necessitates sampling from these models. In many training scenarios,computationally efficient Gibbs sampling procedures are crippled by poormixing. In this work we propose a novel method of sampling from Boltzmannmachines that demonstrates a computationally efficient way to promote mixing.Our approach leverages an under-appreciated property of deep generative modelssuch as the Deep Belief Network (DBN), where Gibbs sampling from deeper levelsof the latent variable hierarchy results in dramatically increased ergodicity.Our approach is thus to train an auxiliary latent hierarchical model, based onthe DBN. When used in conjunction with parallel-tempering, the method isasymptotically guaranteed to simulate samples from the target RBM. Experimentalresults confirm the effectiveness of this sampling strategy in the context ofRBM training.
arxiv-7500-144 | Riemannian Multi-Manifold Modeling | http://arxiv.org/pdf/1410.0095v1.pdf | author:Xu Wang, Konstantinos Slavakis, Gilad Lerman category:stat.ML cs.CV cs.LG published:2014-10-01 summary:This paper advocates a novel framework for segmenting a dataset in aRiemannian manifold $M$ into clusters lying around low-dimensional submanifoldsof $M$. Important examples of $M$, for which the proposed clustering algorithmis computationally efficient, are the sphere, the set of positive definitematrices, and the Grassmannian. The clustering problem with these examples of$M$ is already useful for numerous application domains such as actionidentification in video sequences, dynamic texture clustering, brain fibersegmentation in medical imaging, and clustering of deformed images. Theproposed clustering algorithm constructs a data-affinity matrix by thoroughlyexploiting the intrinsic geometry and then applies spectral clustering. Theintrinsic local geometry is encoded by local sparse coding and more importantlyby directional information of local tangent spaces and geodesics. Theoreticalguarantees are established for a simplified variant of the algorithm even whenthe clusters intersect. To avoid complication, these guarantees assume that theunderlying submanifolds are geodesic. Extensive validation on synthetic andreal data demonstrates the resiliency of the proposed method against deviationsfrom the theoretical model as well as its superior performance overstate-of-the-art techniques.
arxiv-7500-145 | Unsupervised Bump Hunting Using Principal Components | http://arxiv.org/pdf/1409.8630v1.pdf | author:Daniel A Díaz-Pachón, Jean-Eudes Dazard, J. Sunil Rao category:stat.ML 65C60 published:2014-09-30 summary:Principal Components Analysis is a widely used technique for dimensionreduction and characterization of variability in multivariate populations. Ourinterest lies in studying when and why the rotation to principal components canbe used effectively within a response-predictor set relationship in the contextof mode hunting. Specifically focusing on the Patient Rule Induction Method(PRIM), we first develop a fast version of this algorithm (fastPRIM) undernormality which facilitates the theoretical studies to follow. Using basicgeometrical arguments, we then demonstrate how the PC rotation of the predictorspace alone can in fact generate improved mode estimators. Simulation resultsare used to illustrate our findings.
arxiv-7500-146 | A Study of Image Analysis with Tangent Distance | http://arxiv.org/pdf/1401.2529v3.pdf | author:Elif Vural, Pascal Frossard category:cs.CV published:2014-01-11 summary:The computation of the geometric transformation between a reference and atarget image, known as registration or alignment, corresponds to the projectionof the target image onto the transformation manifold of the reference image(the set of images generated by its geometric transformations). It, however,often takes a nontrivial form such that the exact computation of projections onthe manifold is difficult. The tangent distance method is an effectivealgorithm to solve this problem by exploiting a linear approximation of themanifold. As theoretical studies about the tangent distance algorithm have beenlargely overlooked, we present in this work a detailed performance analysis ofthis useful algorithm, which can eventually help its implementation. Weconsider a popular image registration setting using a multiscale pyramid oflowpass filtered versions of the (possibly noisy) reference and target images,which is particularly useful for recovering large transformations. We firstshow that the alignment error has a nonmonotonic variation with the filtersize, due to the opposing effects of filtering on both manifold nonlinearityand image noise. We then study the convergence of the multiscale tangentdistance method to the optimal solution. We finally examine the performance ofthe tangent distance method in image classification applications. Ourtheoretical findings are confirmed by experiments on image transformationmodels involving translations, rotations and scalings. Our study is the firstdetailed study of the tangent distance algorithm that leads to a betterunderstanding of its efficacy and to the proper selection of its designparameters.
arxiv-7500-147 | Distributed Detection : Finite-time Analysis and Impact of Network Topology | http://arxiv.org/pdf/1409.8606v1.pdf | author:Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie category:math.OC cs.LG cs.SI stat.ML published:2014-09-30 summary:This paper addresses the problem of distributed detection in multi-agentnetworks. Agents receive private signals about an unknown state of the world.The underlying state is globally identifiable, yet informative signals may bedispersed throughout the network. Using an optimization-based framework, wedevelop an iterative local strategy for updating individual beliefs. Incontrast to the existing literature which focuses on asymptotic learning, weprovide a finite-time analysis. Furthermore, we introduce a Kullback-Leiblercost to compare the efficiency of the algorithm to its centralized counterpart.Our bounds on the cost are expressed in terms of network size, spectral gap,centrality of each agent and relative entropy of agents' signal structures. Akey observation is that distributing more informative signals to central agentsresults in a faster learning rate. Furthermore, optimizing the weights, we canspeed up learning by improving the spectral gap. We also quantify the effect oflink failures on learning speed in symmetric networks. We finally providenumerical simulations which verify our theoretical results.
arxiv-7500-148 | Data Imputation through the Identification of Local Anomalies | http://arxiv.org/pdf/1409.8576v1.pdf | author:Huseyin Ozkan, Ozgun S. Pelvan, Suleyman S. Kozat category:cs.LG stat.ML published:2014-09-30 summary:We introduce a comprehensive and statistical framework in a model freesetting for a complete treatment of localized data corruptions due to severenoise sources, e.g., an occluder in the case of a visual recording. Within thisframework, we propose i) a novel algorithm to efficiently separate, i.e.,detect and localize, possible corruptions from a given suspicious data instanceand ii) a Maximum A Posteriori (MAP) estimator to impute the corrupted data. Asa generalization to Euclidean distance, we also propose a novel distancemeasure, which is based on the ranked deviations among the data attributes andempirically shown to be superior in separating the corruptions. Our algorithmfirst splits the suspicious instance into parts through a binary partitioningtree in the space of data attributes and iteratively tests those parts todetect local anomalies using the nominal statistics extracted from anuncorrupted (clean) reference data set. Once each part is labeled as anomalousvs normal, the corresponding binary patterns over this tree that characterizecorruptions are identified and the affected attributes are imputed. Under acertain conditional independency structure assumed for the binary patterns, weanalytically show that the false alarm rate of the introduced algorithm indetecting the corruptions is independent of the data and can be directly setwithout any parameter tuning. The proposed framework is tested over severalwell-known machine learning data sets with synthetically generated corruptions;and experimentally shown to produce remarkable improvements in terms ofclassification purposes with strong corruption separation capabilities. Ourexperiments also indicate that the proposed algorithms outperform the typicalapproaches and are robust to varying training phase conditions.
arxiv-7500-149 | Efficient multivariate sequence classification | http://arxiv.org/pdf/1409.8211v2.pdf | author:Pavel P. Kuksa category:cs.LG published:2014-09-29 summary:Kernel-based approaches for sequence classification have been successfullyapplied to a variety of domains, including the text categorization, imageclassification, speech analysis, biological sequence analysis, time series andmusic classification, where they show some of the most accurate results. Typical kernel functions for sequences in these domains (e.g., bag-of-words,mismatch, or subsequence kernels) are restricted to {\em discrete univariate}(i.e. one-dimensional) string data, such as sequences of words in the textanalysis, codeword sequences in the image analysis, or nucleotide or amino acidsequences in the DNA and protein sequence analysis. However, original sequencedata are often of real-valued multivariate nature, i.e. are not univariate anddiscrete as required by typical $k$-mer based sequence kernel functions. In this work, we consider the problem of the {\em multivariate} sequenceclassification such as classification of multivariate music sequences, ormultidimensional protein sequence representations. To this end, we extend {\emunivariate} kernel functions typically used in sequence analysis and proposeefficient {\em multivariate} similarity kernel method (MVDFQ-SK) based on (1) adirect feature quantization (DFQ) of each sequence dimension in the original{\em real-valued} multivariate sequences and (2) applying novel multivariatediscrete kernel measures on these multivariate discrete DFQ sequencerepresentations to more accurately capture similarity relationships amongsequences and improve classification performance. Experiments using the proposed MVDFQ-SK kernel method show excellentclassification performance on three challenging music classification tasks aswell as protein sequence classification with significant 25-40% improvementsover univariate kernel methods and existing state-of-the-art sequenceclassification methods.
arxiv-7500-150 | A Deep Learning Approach to Data-driven Parameterizations for Statistical Parametric Speech Synthesis | http://arxiv.org/pdf/1409.8558v1.pdf | author:Prasanna Kumar Muthukumar, Alan W. Black category:cs.CL cs.LG cs.NE published:2014-09-30 summary:Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstralcoefficients as the vocal tract parameterization of the speech signal. MelCepstral coefficients were never intended to work in a parametric speechsynthesis framework, but as yet, there has been little success in creating abetter parameterization that is more suited to synthesis. In this paper, we usedeep learning algorithms to investigate a data-driven parameterizationtechnique that is designed for the specific requirements of synthesis. Wecreate an invertible, low-dimensional, noise-robust encoding of the Mel LogSpectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA isthen unwrapped and used as the initialization for a Multi-Layer Perceptron(MLP). The MLP is fine-tuned by training it to reconstruct the input at theoutput layer. This MLP is then split down the middle to form encoding anddecoding networks. These networks produce a parameterization of the Mel LogSpectrum that is intended to better fulfill the requirements of synthesis.Results are reported for experiments conducted using this resultingparameterization with the ClusterGen speech synthesizer.
arxiv-7500-151 | Non-Myopic Learning in Repeated Stochastic Games | http://arxiv.org/pdf/1409.8498v1.pdf | author:Jacob W. Crandall category:cs.GT cs.AI cs.LG published:2014-09-30 summary:This paper addresses learning in repeated stochastic games (RSGs) playedagainst unknown associates. Learning in RSGs is extremely challenging due totheir inherently large strategy spaces. Furthermore, these games typically havemultiple (often infinite) equilibria, making attempts to solve them viaequilibrium analysis and rationality assumptions wholly insufficient. As such,previous learning algorithms for RSGs either learn very slowly or makeextremely limiting assumptions about the game structure or associates'behaviors. In this paper, we propose and evaluate the notion of gameabstraction by experts (Gabe) for two-player general-sum RSGs. Gabe reduces anRSG to a multi-armed bandit problem, which can then be solved using an expertalgorithm. Gabe maintains many aspects of the original game, including securityand Pareto optimal Nash equilibria. We demonstrate that Gabe substantiallyoutperforms existing algorithms in many scenarios.
arxiv-7500-152 | Predicting missing links via correlation between nodes | http://arxiv.org/pdf/1409.8485v1.pdf | author:Hao Liao, An Zeng, Yi-Cheng Zhang category:physics.soc-ph cs.SI stat.ML published:2014-09-30 summary:As a fundamental problem in many different fields, link prediction aims toestimate the likelihood of an existing link between two nodes based on theobserved information. Since this problem is related to many applicationsranging from uncovering missing data to predicting the evolution of networks,link prediction has been intensively investigated recently and many methodshave been proposed so far. The essential challenge of link prediction is toestimate the similarity between nodes. Most of the existing methods are basedon the common neighbor index and its variants. In this paper, we propose tocalculate the similarity between nodes by the correlation coefficient. Thismethod is found to be very effective when applied to calculate similarity basedon high order paths. We finally fuse the correlation-based method with theresource allocation method, and find that the combined method can substantiallyoutperform the existing methods, especially in sparse networks.
arxiv-7500-153 | An agent-driven semantical identifier using radial basis neural networks and reinforcement learning | http://arxiv.org/pdf/1409.8484v1.pdf | author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana category:cs.NE cs.AI cs.CL cs.LG cs.MA published:2014-09-30 summary:Due to the huge availability of documents in digital form, and the deceptionpossibility raise bound to the essence of digital documents and the way theyare spread, the authorship attribution problem has constantly increased itsrelevance. Nowadays, authorship attribution,for both information retrieval andanalysis, has gained great importance in the context of security, trust andcopyright preservation. This work proposes an innovative multi-agent drivenmachine learning technique that has been developed for authorship attribution.By means of a preprocessing for word-grouping and time-period related analysisof the common lexicon, we determine a bias reference level for the recurrencefrequency of the words within analysed texts, and then train a Radial BasisNeural Networks (RBPNN)-based classifier to identify the correct author. Themain advantage of the proposed approach lies in the generality of the semanticanalysis, which can be applied to different contexts and lexical domains,without requiring any modification. Moreover, the proposed system is able toincorporate an external input, meant to tune the classifier, and thenself-adjust by means of continuous learning reinforcement.
arxiv-7500-154 | Nonstochastic Multi-Armed Bandits with Graph-Structured Feedback | http://arxiv.org/pdf/1409.8428v1.pdf | author:Noga Alon, Nicolò Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, Ohad Shamir category:cs.LG stat.ML published:2014-09-30 summary:We present and study a partial-information model of online learning, where adecision maker repeatedly chooses from a finite set of actions, and observessome subset of the associated losses. This naturally models several situationswhere the losses of different actions are related, and knowing the loss of oneaction provides information on the loss of other actions. Moreover, itgeneralizes and interpolates between the well studied full-information setting(where all losses are revealed) and the bandit setting (where only the loss ofthe action chosen by the player is revealed). We provide several algorithmsaddressing different variants of our setting, and provide tight regret boundsdepending on combinatorial properties of the information feedback structure.
arxiv-7500-155 | Variational inference of latent state sequences using Recurrent Networks | http://arxiv.org/pdf/1406.1655v2.pdf | author:Justin Bayer, Christian Osendorfer category:stat.ML cs.LG published:2014-06-06 summary:Recent advances in the estimation of deep directed graphical models andrecurrent networks let us contribute to the removal of a blind spot in the areaof probabilistc modelling of time series. The proposed methods i) can inferdistributed latent state-space trajectories with nonlinear transitions, ii)scale to large data sets thanks to the use of a stochastic objective and fast,approximate inference, iii) enable the design of rich emission models which iv)will naturally lead to structured outputs. Two different paths of introducinglatent state sequences are pursued, leading to the variational recurrent autoencoder (VRAE) and the variational one step predictor (VOSP). The use ofindependent Wiener processes as priors on the latent state sequence is a viablecompromise between efficient computation of the Kullback-Leibler divergencefrom the variational approximation of the posterior and maintaining areasonable belief in the dynamics. We verify our methods empirically, obtainingresults close or superior to the state of the art. We also show qualitativeresults for denoising and missing value imputation.
arxiv-7500-156 | Computational Beauty: Aesthetic Judgment at the Intersection of Art and Science | http://arxiv.org/pdf/1410.2488v1.pdf | author:Emily L. Spratt, Ahmed Elgammal category:cs.CV published:2014-09-30 summary:In part one of the Critique of Judgment, Immanuel Kant wrote that "thejudgment of taste...is not a cognitive judgment, and so not logical, but isaesthetic."\cite{Kant} While the condition of aesthetic discernment has longbeen the subject of philosophical discourse, the role of the arbiters of thatjudgment has more often been assumed than questioned. The art historian,critic, connoisseur, and curator have long held the esteemed position of theaesthetic judge, their training, instinct, and eye part of the inimitablesubjective processes that Kant described as occurring upon artistic evaluation.Although the concept of intangible knowledge in regard to aesthetic theory hasbeen much explored, little discussion has arisen in response to the developmentof new types of artificial intelligence as a challenge to the seeminglyineffable abilities of the human observer. This paper examines the developmentsin the field of computer vision analysis of paintings from canonical movementswith the history of Western art and the reaction of art historians to theapplication of this technology in the field. Through an investigation of theethical consequences of this innovative technology, the unquestioned authorityof the art expert is challenged and the subjective nature of aesthetic judgmentis brought to philosophical scrutiny once again.
arxiv-7500-157 | A Novel Scheme for Intelligent Recognition of Pornographic Images | http://arxiv.org/pdf/1402.5792v3.pdf | author:Seyed Mostafa Kia, Hossein Rahmani, Reza Mortezaei, Mohsen Ebrahimi Moghaddam, Amer Namazi category:cs.CV published:2014-02-24 summary:Harmful contents are rising in internet day by day and this motivates theessence of more research in fast and reliable obscene and immoral materialfiltering. Pornographic image recognition is an important component in eachfiltering system. In this paper, a new approach for detecting pornographicimages is introduced. In this approach, two new features are suggested. Thesetwo features in combination with other simple traditional features providedecent difference between porn and non-porn images. In addition, we appliedfuzzy integral based information fusion to combine MLP (Multi-Layer Perceptron)and NF (Neuro-Fuzzy) outputs. To test the proposed method, performance ofsystem was evaluated over 18354 download images from internet. The attainedprecision was 93% in TP and 8% in FP on training dataset, and 87% and 5.5% ontest dataset. Achieved results verify the performance of proposed system versusother related works.
arxiv-7500-158 | Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models - a Gentle Tutorial | http://arxiv.org/pdf/1402.1412v2.pdf | author:Yarin Gal, Mark van der Wilk category:stat.ML published:2014-02-06 summary:In this tutorial we explain the inference procedures developed for the sparseGaussian process (GP) regression and Gaussian process latent variable model(GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias &Lawrence (2010) is brief, hence getting a full picture of it requirescollecting results from several different sources and a substantial amount ofalgebra to fill-in the gaps. Our main goal is thus to collect all the resultsand full derivations into one place to help speed up understanding this work.In doing so we present a re-parametrisation of the inference that allows it tobe carried out in parallel. A secondary goal for this document is, therefore,to accompany our paper and open-source implementation of the parallel inferencescheme for the models. We hope that this document will bridge the gap betweenthe equations as implemented in code and those published in the originalpapers, in order to make it easier to extend existing work. We assume priorknowledge of Gaussian processes and variational inference, but we also includereferences for further reading where appropriate.
arxiv-7500-159 | Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models | http://arxiv.org/pdf/1402.1389v2.pdf | author:Yarin Gal, Mark van der Wilk, Carl E. Rasmussen category:stat.ML cs.LG published:2014-02-06 summary:Gaussian processes (GPs) are a powerful tool for probabilistic inference overfunctions. They have been applied to both regression and non-lineardimensionality reduction, and offer desirable properties such as uncertaintyestimates, robustness to over-fitting, and principled ways for tuninghyper-parameters. However the scalability of these models to big datasetsremains an active topic of research. We introduce a novel re-parametrisation ofvariational inference for sparse GP regression and latent variable models thatallows for an efficient distributed algorithm. This is done by exploiting thedecoupling of the data given the inducing points to re-formulate the evidencelower bound in a Map-Reduce setting. We show that the inference scales wellwith data and computational resources, while preserving a balanced distributionof the load among the nodes. We further demonstrate the utility in scalingGaussian processes to big data. We show that GP performance improves withincreasing amounts of data in regression (on flight data with 2 millionrecords) and latent variable modelling (on MNIST). The results show that GPsperform better than many common models often used for big data.
arxiv-7500-160 | Bayesian and regularization approaches to multivariable linear system identification: the role of rank penalties | http://arxiv.org/pdf/1409.8327v1.pdf | author:Giulia Prando, Alessandro Chiuso, Gianluigi Pillonetto category:cs.SY cs.LG stat.ML published:2014-09-29 summary:Recent developments in linear system identification have proposed the use ofnon-parameteric methods, relying on regularization strategies, to handle theso-called bias/variance trade-off. This paper introduces an impulse responseestimator which relies on an $\ell_2$-type regularization including arank-penalty derived using the log-det heuristic as a smooth approximation tothe rank function. This allows to account for different properties of theestimated impulse response (e.g. smoothness and stability) while alsopenalizing high-complexity models. This also allows to account and enforcecoupling between different input-output channels in MIMO systems. According tothe Bayesian paradigm, the parameters defining the relative weight of the tworegularization terms as well as the structure of the rank penalty are estimatedoptimizing the marginal likelihood. Once these hyperameters have beenestimated, the impulse response estimate is available in closed form.Experiments show that the proposed method is superior to the estimator relyingon the "classic" $\ell_2$-regularization alone as well as those based in atomicand nuclear norm.
arxiv-7500-161 | Arabic Spelling Correction using Supervised Learning | http://arxiv.org/pdf/1409.8309v1.pdf | author:Youssef Hassan, Mohamed Aly, Amir Atiya category:cs.LG cs.CL published:2014-09-29 summary:In this work, we address the problem of spelling correction in the Arabiclanguage utilizing the new corpus provided by QALB (Qatar Arabic Language Bank)project which is an annotated corpus of sentences with errors and theircorrections. The corpus contains edit, add before, split, merge, add after,move and other error types. We are concerned with the first four error types asthey contribute more than 90% of the spelling errors in the corpus. Theproposed system has many models to address each error type on its own and thenintegrating all the models to provide an efficient and robust system thatachieves an overall recall of 0.59, precision of 0.58 and F1 score of 0.58including all the error types on the development set. Our system participatedin the QALB 2014 shared task "Automatic Arabic Error Correction" and achievedan F1 score of 0.6, earning the sixth place out of nine participants.
arxiv-7500-162 | Short-Term Predictability of Photovoltaic Production over Italy | http://arxiv.org/pdf/1409.8202v1.pdf | author:Matteo De Felice, Marcello Petitta, Paolo M. Ruti category:cs.LG stat.AP published:2014-09-29 summary:Photovoltaic (PV) power production increased drastically in Europe throughoutthe last years. About the 6% of electricity in Italy comes from PV and for anefficient management of the power grid an accurate and reliable forecasting ofproduction would be needed. Starting from a dataset of electricity productionof 65 Italian solar plants for the years 2011-2012 we investigate thepossibility to forecast daily production from one to ten days of lead timewithout using on site measurements. Our study is divided in two parts: anassessment of the predictability of meteorological variables using weatherforecasts and an analysis on the application of data-driven modelling inpredicting solar power production. We calibrate a SVM model using availableobservations and then we force the same model with the predicted variables fromweather forecasts with a lead time from one to ten days. As expected, solarpower production is strongly influenced by cloudiness and clear sky, in fact weobserve that while during summer we obtain a general error under the 10%(slightly lower in south Italy), during winter the error is abundantly abovethe 20%.
arxiv-7500-163 | Freshness-Aware Thompson Sampling | http://arxiv.org/pdf/1409.8572v1.pdf | author:Djallel Bouneffouf category:cs.IR cs.LG I.2 published:2014-09-29 summary:To follow the dynamicity of the user's content, researchers have recentlystarted to model interactions between users and the Context-Aware RecommenderSystems (CARS) as a bandit problem where the system needs to deal withexploration and exploitation dilemma. In this sense, we propose to study thefreshness of the user's content in CARS through the bandit problem. Weintroduce in this paper an algorithm named Freshness-Aware Thompson Sampling(FA-TS) that manages the recommendation of fresh document according to theuser's risk of the situation. The intensive evaluation and the detailedanalysis of the experimental results reveals several important discoveries inthe exploration/exploitation (exr/exp) behaviour.
arxiv-7500-164 | A Neural Networks Committee for the Contextual Bandit Problem | http://arxiv.org/pdf/1409.8191v1.pdf | author:Robin Allesiardo, Raphael Feraud, Djallel Bouneffouf category:cs.NE cs.LG I.2 published:2014-09-29 summary:This paper presents a new contextual bandit algorithm, NeuralBandit, whichdoes not need hypothesis on stationarity of contexts and rewards. Severalneural networks are trained to modelize the value of rewards knowing thecontext. Two variants, based on multi-experts approach, are proposed to chooseonline the parameters of multi-layer perceptrons. The proposed algorithms aresuccessfully tested on a large dataset with and without stationarity ofrewards.
arxiv-7500-165 | Communication-Efficient Distributed Dual Coordinate Ascent | http://arxiv.org/pdf/1409.1458v2.pdf | author:Martin Jaggi, Virginia Smith, Martin Takáč, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, Michael I. Jordan category:cs.LG math.OC stat.ML 90C25, 68W15 G.1.6; C.1.4 published:2014-09-04 summary:Communication remains the most significant bottleneck in the performance ofdistributed optimization algorithms for large-scale machine learning. In thispaper, we propose a communication-efficient framework, CoCoA, that uses localcomputation in a primal-dual setting to dramatically reduce the amount ofnecessary communication. We provide a strong convergence rate analysis for thisclass of algorithms, as well as experiments on real-world distributed datasetswith implementations in Spark. In our experiments, we find that as compared tostate-of-the-art mini-batch versions of SGD and SDCA algorithms, CoCoAconverges to the same .001-accurate solution quality on average 25x as quickly.
arxiv-7500-166 | Random forests with random projections of the output space for high dimensional multi-label classification | http://arxiv.org/pdf/1404.3581v4.pdf | author:Arnaud Joly, Pierre Geurts, Louis Wehenkel category:stat.ML cs.LG published:2014-04-14 summary:We adapt the idea of random projections applied to the output space, so as toenhance tree-based ensemble methods in the context of multi-labelclassification. We show how learning time complexity can be reduced withoutaffecting computational complexity and accuracy of predictions. We also showthat random output space projections may be used in order to reach differentbias-variance tradeoffs, over a broad panel of benchmark problems, and thatthis may lead to improved accuracy while reducing significantly thecomputational burden of the learning stage.
arxiv-7500-167 | Controversy and Sentiment in Online News | http://arxiv.org/pdf/1409.8152v1.pdf | author:Yelena Mejova, Amy X. Zhang, Nicholas Diakopoulos, Carlos Castillo category:cs.CY cs.CL published:2014-09-29 summary:How do news sources tackle controversial issues? In this work, we take adata-driven approach to understand how controversy interplays with emotionalexpression and biased language in the news. We begin by introducing a newdataset of controversial and non-controversial terms collected usingcrowdsourcing. Then, focusing on 15 major U.S. news outlets, we comparemillions of articles discussing controversial and non-controversial issues overa span of 7 months. We find that in general, when it comes to controversialissues, the use of negative affect and biased language is prevalent, while theuse of strong emotion is tempered. We also observe many differences across newssources. Using these findings, we show that we can indicate to what extent anissue is controversial, by comparing it with other issues in terms of how theyare portrayed across different media.
arxiv-7500-168 | A Bayesian Tensor Factorization Model via Variational Inference for Link Prediction | http://arxiv.org/pdf/1409.8276v1.pdf | author:Beyza Ermis, A. Taylan Cemgil category:cs.LG cs.NA stat.ML published:2014-09-29 summary:Probabilistic approaches for tensor factorization aim to extract meaningfulstructure from incomplete data by postulating low rank constraints. Recently,variational Bayesian (VB) inference techniques have successfully been appliedto large scale models. This paper presents full Bayesian inference via VB onboth single and coupled tensor factorization models. Our method can be run evenfor very large models and is easily implemented. It exhibits better predictionperformance than existing approaches based on maximum likelihood on severalreal-world datasets for missing link prediction problem.
arxiv-7500-169 | Robustness and Generalization for Metric Learning | http://arxiv.org/pdf/1209.1086v3.pdf | author:Aurélien Bellet, Amaury Habrard category:cs.LG stat.ML published:2012-09-05 summary:Metric learning has attracted a lot of interest over the last decade, but thegeneralization ability of such methods has not been thoroughly studied. In thispaper, we introduce an adaptation of the notion of algorithmic robustness(previously introduced by Xu and Mannor) that can be used to derivegeneralization bounds for metric learning. We further show that a weak notionof robustness is in fact a necessary and sufficient condition for a metriclearning algorithm to generalize. To illustrate the applicability of theproposed framework, we derive generalization results for a large family ofexisting metric learning algorithms, including some sparse formulations thatare not covered by previous results.
arxiv-7500-170 | CRF-based Named Entity Recognition @ICON 2013 | http://arxiv.org/pdf/1409.8008v1.pdf | author:Arjun Das, Utpal Garain category:cs.CL published:2014-09-29 summary:This paper describes performance of CRF based systems for Named EntityRecognition (NER) in Indian language as a part of ICON 2013 shared task. Inthis task we have considered a set of language independent features for all thelanguages. Only for English a language specific feature, i.e. capitalization,has been added. Next the use of gazetteer is explored for Bengali, Hindi andEnglish. The gazetteers are built from Wikipedia and other sources. Testresults show that the system achieves the highest F measure of 88% for Englishand the lowest F measure of 69% for both Tamil and Telugu. Note that for theleast performing two languages no gazetteer was used. NER in Bengali and Hindifinds accuracy (F measure) of 87% and 79%, respectively.
arxiv-7500-171 | Improving the Performance of English-Tamil Statistical Machine Translation System using Source-Side Pre-Processing | http://arxiv.org/pdf/1409.8581v1.pdf | author:M. Anand Kumar, V. Dhanalakshmi, K. P. Soman, V. Sharmiladevi category:cs.CL published:2014-09-29 summary:Machine Translation is one of the major oldest and the most active researcharea in Natural Language Processing. Currently, Statistical Machine Translation(SMT) dominates the Machine Translation research. Statistical MachineTranslation is an approach to Machine Translation which uses models to learntranslation patterns directly from data, and generalize them to translate a newunseen text. The SMT approach is largely language independent, i.e. the modelscan be applied to any language pair. Statistical Machine Translation (SMT)attempts to generate translations using statistical methods based on bilingualtext corpora. Where such corpora are available, excellent results can beattained translating similar texts, but such corpora are still not availablefor many language pairs. Statistical Machine Translation systems, in general,have difficulty in handling the morphology on the source or the target sideespecially for morphologically rich languages. Errors in morphology or syntaxin the target language can have severe consequences on meaning of the sentence.They change the grammatical function of words or the understanding of thesentence through the incorrect tense information in verb. Baseline SMT alsoknown as Phrase Based Statistical Machine Translation (PBSMT) system does notuse any linguistic information and it only operates on surface word form.Recent researches shown that adding linguistic information helps to improve theaccuracy of the translation with less amount of bilingual corpora. Addinglinguistic information can be done using the Factored Statistical MachineTranslation system through pre-processing steps. This paper investigates abouthow English side pre-processing is used to improve the accuracy ofEnglish-Tamil SMT system.
arxiv-7500-172 | Sequential Complexity as a Descriptor for Musical Similarity | http://arxiv.org/pdf/1402.6926v3.pdf | author:Peter Foster, Matthias Mauch, Simon Dixon category:cs.IR cs.LG cs.SD published:2014-02-27 summary:We propose string compressibility as a descriptor of temporal structure inaudio, for the purpose of determining musical similarity. Our descriptors arebased on computing track-wise compression rates of quantised audio features,using multiple temporal resolutions and quantisation granularities. To verifythat our descriptors capture musically relevant information, we incorporate ourdescriptors into similarity rating prediction and song year prediction tasks.We base our evaluation on a dataset of 15500 track excerpts of Western popularmusic, for which we obtain 7800 web-sourced pairwise similarity ratings. Toassess the agreement among similarity ratings, we perform an evaluation undercontrolled conditions, obtaining a rank correlation of 0.33 between intersectedsets of ratings. Combined with bag-of-features descriptors, we obtainperformance gains of 31.1% and 10.9% for similarity rating prediction and songyear prediction. For both tasks, analysis of selected descriptors reveals thatrepresenting features at multiple time scales benefits prediction accuracy.
arxiv-7500-173 | MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation | http://arxiv.org/pdf/1409.7963v1.pdf | author:Arjun Jain, Jonathan Tompson, Yann LeCun, Christoph Bregler category:cs.CV cs.LG cs.NE published:2014-09-28 summary:In this work, we propose a novel and efficient method for articulated humanpose estimation in videos using a convolutional network architecture, whichincorporates both color and motion features. We propose a new human body posedataset, FLIC-motion, that extends the FLIC dataset with additional motionfeatures. We apply our architecture to this dataset and report significantlybetter performance than current state-of-the-art pose detection systems.
arxiv-7500-174 | Combining human and machine learning for morphological analysis of galaxy images | http://arxiv.org/pdf/1409.7935v1.pdf | author:Evan Kuminski, Joe George, John Wallin, Lior Shamir category:astro-ph.IM astro-ph.GA cs.CV cs.LG published:2014-09-28 summary:The increasing importance of digital sky surveys collecting many millions ofgalaxy images has reinforced the need for robust methods that can performmorphological analysis of large galaxy image databases. Citizen scienceinitiatives such as Galaxy Zoo showed that large datasets of galaxy images canbe analyzed effectively by non-scientist volunteers, but since databasesgenerated by robotic telescopes grow much faster than the processing power ofany group of citizen scientists, it is clear that computer analysis isrequired. Here we propose to use citizen science data for training machinelearning systems, and show experimental results demonstrating that machinelearning systems can be trained with citizen science data. Our findings showthat the performance of machine learning depends on the quality of the data,which can be improved by using samples that have a high degree of agreementbetween the citizen scientists. The source code of the method is publiclyavailable.
arxiv-7500-175 | Comparing apples to apples in the evaluation of binary coding methods | http://arxiv.org/pdf/1405.1005v2.pdf | author:Mohammad Rastegari, Shobeir Fakhraei, Jonghyun Choi, David Jacobs, Larry S. Davis category:cs.CV cs.LG published:2014-05-05 summary:We discuss methodological issues related to the evaluation of unsupervisedbinary code construction methods for nearest neighbor search. These issues havebeen widely ignored in literature. These coding methods attempt to preserveeither Euclidean distance or angular (cosine) distance in the binary embeddingspace. We explain why when comparing a method whose goal is preserving cosinesimilarity to one designed for preserving Euclidean distance, the originalfeatures should be normalized by mapping them to the unit hypersphere beforelearning the binary mapping functions. To compare a method whose goal is topreserves Euclidean distance to one that preserves cosine similarity, theoriginal feature data must be mapped to a higher dimension by including a biasterm in binary mapping functions. These conditions ensure the fair comparisonbetween different binary code methods for the task of nearest neighbor search.Our experiments show under these conditions the very simple methods (e.g. LSHand ITQ) often outperform recent state-of-the-art methods (e.g. MDSH andOK-means).
arxiv-7500-176 | PAINTER: a spatio-spectral image reconstruction algorithm for optical interferometry | http://arxiv.org/pdf/1407.1885v2.pdf | author:Antony Schutz, André Ferrari, David Mary, Férréol Soulez, Éric Thiébaut, Martin Vannier category:astro-ph.IM cs.CV published:2014-06-29 summary:Astronomical optical interferometers sample the Fourier transform of theintensity distribution of a source at the observation wavelength. Because ofrapid perturbations caused by atmospheric turbulence, the phases of the complexFourier samples (visibilities) cannot be directly exploited. Consequently,specific image reconstruction methods have been devised in the last fewdecades. Modern polychromatic optical interferometric instruments are nowpaving the way to multiwavelength imaging. This paper is devoted to thederivation of a spatio-spectral (3D) image reconstruction algorithm, coinedPAINTER (Polychromatic opticAl INTErferometric Reconstruction software). Thealgorithm relies on an iterative process, which alternates estimation ofpolychromatic images and of complex visibilities. The complex visibilities arenot only estimated from squared moduli and closure phases, but alsodifferential phases, which helps to better constrain the polychromaticreconstruction. Simulations on synthetic data illustrate the efficiency of thealgorithm and in particular the relevance of injecting a differential phasesmodel in the reconstruction.
arxiv-7500-177 | The automatic creation of concept maps from documents written using morphologically rich languages | http://arxiv.org/pdf/1210.7599v2.pdf | author:Krunoslav Zubrinic, Damir Kalpic, Mario Milicevic category:cs.IR cs.AI cs.CL published:2012-10-29 summary:Concept map is a graphical tool for representing knowledge. They have beenused in many different areas, including education, knowledge management,business and intelligence. Constructing of concept maps manually can be acomplex task; an unskilled person may encounter difficulties in determining andpositioning concepts relevant to the problem area. An application thatrecommends concept candidates and their position in a concept map cansignificantly help the user in that situation. This paper gives an overview ofdifferent approaches to automatic and semi-automatic creation of concept mapsfrom textual and non-textual sources. The concept map mining process isdefined, and one method suitable for the creation of concept maps fromunstructured textual sources in highly inflected languages such as the Croatianlanguage is described in detail. Proposed method uses statistical and datamining techniques enriched with linguistic tools. With minor adjustments, thatmethod can also be used for concept map mining from textual sources in othermorphologically rich languages.
arxiv-7500-178 | Audio Surveillance: a Systematic Review | http://arxiv.org/pdf/1409.7787v1.pdf | author:Marco Crocco, Marco Cristani, Andrea Trucco, Vittorio Murino category:cs.SD cs.CV cs.MM published:2014-09-27 summary:Despite surveillance systems are becoming increasingly ubiquitous in ourliving environment, automated surveillance, currently based on video sensorymodality and machine intelligence, lacks most of the time the robustness andreliability required in several real applications. To tackle this issue, audiosensory devices have been taken into account, both alone or in combination withvideo, giving birth, in the last decade, to a considerable amount of research.In this paper audio-based automated surveillance methods are organized into acomprehensive survey: a general taxonomy, inspired by the more widespread videosurveillance field, is proposed in order to systematically describe the methodscovering background subtraction, event classification, object tracking andsituation analysis. For each of these tasks, all the significant works arereviewed, detailing their pros and cons and the context for which they havebeen proposed. Moreover, a specific section is devoted to audio features,discussing their expressiveness and their employment in the above describedtasks. Differently, from other surveys on audio processing and analysis, thepresent one is specifically targeted to automated surveillance, highlightingthe target applications of each described methods and providing the readertables and schemes useful to retrieve the most suited algorithms for a specificrequirement.
arxiv-7500-179 | Maximum mutual information regularized classification | http://arxiv.org/pdf/1409.7780v1.pdf | author:Jim Jing-Yan Wang, Yi Wang, Shiguang Zhao, Xin Gao category:cs.LG published:2014-09-27 summary:In this paper, a novel pattern classification approach is proposed byregularizing the classifier learning to maximize mutual information between theclassification response and the true class label. We argue that, with thelearned classifier, the uncertainty of the true class label of a data sampleshould be reduced by knowing its classification response as much as possible.The reduced uncertainty is measured by the mutual information between theclassification response and the true class label. To this end, when learning alinear classifier, we propose to maximize the mutual information betweenclassification responses and true class labels of training samples, besidesminimizing the classification error and reduc- ing the classifier complexity.An objective function is constructed by modeling mutual information withentropy estimation, and it is optimized by a gradi- ent descend method in aniterative algorithm. Experiments on two real world pattern classificationproblems show the significant improvements achieved by maximum mutualinformation regularization.
arxiv-7500-180 | Combating Corrupt Messages in Sparse Clustered Associative Memories | http://arxiv.org/pdf/1409.7758v1.pdf | author:Zhe Yao, Vincent Gripon, Michael Rabbat category:cs.NE published:2014-09-27 summary:In this paper we analyze and extend the neural network based associativememory proposed by Gripon and Berrou. This associative memory resembles thecelebrated Willshaw model with an added partite cluster structure. In theliterature, two retrieving schemes have been proposed for the network dynamics,namely sum-of-sum and sum-of-max. They both offer considerably betterperformance than Willshaw and Hopfield networks, when comparable retrievalscenarios are considered. Former discussions and experiments concentrate on theerasure scenario, where a partial message is used as a probe to the network, inthe hope of retrieving the full message. In this regard, sum-of-max outperformssum-of-sum in terms of retrieval rate by a large margin. However, we observethat when noise and errors are present and the network is queried by a corruptprobe, sum-of-max faces a severe limitation as its stringent activation ruleprevents a neuron from reviving back into play once deactivated. In thismanuscript, we categorize and analyze different error scenarios so that boththe erasure and the corrupt scenarios can be treated consistently. We make anamendment to the network structure to improve the retrieval rate, at the costof an extra scalar per neuron. Afterwards, five different approaches areproposed to deal with corrupt probes. As a result, we extend the networkcapability, and also increase the robustness of the retrieving procedure. Wethen experimentally compare all these proposals and discuss pros and cons ofeach approach under different types of errors. Simulation results show that ifcarefully designed, the network is able to preserve both a high retrieval rateand a low running time simultaneously, even when queried by a corrupt probe.
arxiv-7500-181 | Active Dictionary Learning in Sparse Representation Based Classification | http://arxiv.org/pdf/1409.5763v2.pdf | author:Jin Xu, Haibo He, Hong Man category:cs.CV 68T05 published:2014-09-19 summary:Sparse representation, which uses dictionary atoms to reconstruct inputvectors, has been studied intensively in recent years. A proper dictionary is akey for the success of sparse representation. In this paper, an activedictionary learning (ADL) method is introduced, in which classification errorand reconstruction error are considered as the active learning criteria inselection of the atoms for dictionary construction. The learned dictionariesare caculated in sparse representation based classification (SRC). Theclassification accuracy and reconstruction error are used to evaluate theproposed dictionary learning method. The performance of the proposed dictionarylearning method is compared with other methods, including unsuperviseddictionary learning and whole-training-data dictionary. The experimentalresults based on the UCI data sets and face data set demonstrate the efficiencyof the proposed method.
arxiv-7500-182 | How close are we to understanding image-based saliency? | http://arxiv.org/pdf/1409.7686v1.pdf | author:Matthias Kümmerer, Thomas Wallis, Matthias Bethge category:cs.CV q-bio.NC stat.AP published:2014-09-26 summary:Within the set of the many complex factors driving gaze placement, theproperities of an image that are associated with fixations under free viewingconditions have been studied extensively. There is a general impression thatthe field is close to understanding this particular association. Here we framesaliency models probabilistically as point processes, allowing the calculationof log-likelihoods and bringing saliency evaluation into the domain ofinformation. We compared the information gain of state-of-the-art models to agold standard and find that only one third of the explainable spatialinformation is captured. We additionally provide a principled method to showwhere and how models fail to capture information in the fixations. Thus,contrary to previous assertions, purely spatial saliency remains a significantchallenge.
arxiv-7500-183 | Order-invariant prior specification in Bayesian factor analysis | http://arxiv.org/pdf/1409.7672v1.pdf | author:Dennis Leung, Mathias Drton category:stat.ME stat.ML published:2014-09-26 summary:In (exploratory) factor analysis, the loading matrix is identified only up toorthogonal rotation. For identifiability, one thus often takes the loadingmatrix to be lower triangular with positive diagonal entries. In Bayesianinference, a standard practice is then to specify a prior under which theloadings are independent, the off-diagonal loadings are normally distributed,and the diagonal loadings follow a truncated normal distribution. This priorspecification, however, depends in an important way on how the variables andassociated rows of the loading matrix are ordered. We show how a minormodification of the approach allows one to compute with the identifiable lowertriangular loading matrix but maintain invariance properties under reorderingof the variables.
arxiv-7500-184 | Deconvolution of High-Dimensional Mixtures via Boosting, with Application to Diffusion-Weighted MRI of Human Brain | http://arxiv.org/pdf/1409.7134v2.pdf | author:Charles Zheng, Franco Pestilli, Ariel Rokem category:stat.ML published:2014-09-25 summary:Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractographyare the only methods to measure the structure of the white matter in the livinghuman brain. The diffusion signal has been modelled as the combinedcontribution from many individual fascicles of nerve fibers passing througheach location in the white matter. Typically, this is done via basis pursuit,but estimation of the exact directions is limited due to discretization. Thedifficulties inherent in modeling DWI data are shared by many other problemsinvolving fitting non-parametric mixture models. Ekanadaham et al. proposed anapproach, continuous basis pursuit, to overcome discretization error in the1-dimensional case (e.g., spike-sorting). Here, we propose a more generalalgorithm that fits mixture models of any dimensionality withoutdiscretization. Our algorithm uses the principles of L2-boost, together withrefitting of the weights and pruning of the parameters. The addition of thesesteps to L2-boost both accelerates the algorithm and assures its accuracy. Werefer to the resulting algorithm as elastic basis pursuit, or EBP, since itexpands and contracts the active set of kernels as needed. We show that incontrast to existing approaches to fitting mixtures, our boosting framework (1)enables the selection of the optimal bias-variance tradeoff along the solutionpath, and (2) scales with high-dimensional problems. In simulations of DWI, wefind that EBP yields better parameter estimates than a non-negative leastsquares (NNLS) approach, or the standard model used in DWI, the tensor model,which serves as the basis for diffusion tensor imaging (DTI). We demonstratethe utility of the method in DWI data acquired in parts of the brain containingcrossings of multiple fascicles of nerve fibers.
arxiv-7500-185 | Topic Similarity Networks: Visual Analytics for Large Document Sets | http://arxiv.org/pdf/1409.7591v1.pdf | author:Arun S. Maiya, Robert M. Rolfe category:cs.CL cs.HC cs.IR cs.SI stat.ML published:2014-09-26 summary:We investigate ways in which to improve the interpretability of LDA topicmodels by better analyzing and visualizing their outputs. We focus on examiningwhat we refer to as topic similarity networks: graphs in which nodes representlatent topics in text collections and links represent similarity among topics.We describe efficient and effective approaches to both building and labelingsuch networks. Visualizations of topic models based on these networks are shownto be a powerful means of exploring, characterizing, and summarizing largecollections of unstructured text documents. They help to "tease out"non-obvious connections among different sets of documents and provide insightsinto how topics form larger themes. We demonstrate the efficacy andpracticality of these approaches through two case studies: 1) NSF grants forbasic research spanning a 14 year period and 2) the entire English portion ofWikipedia.
arxiv-7500-186 | An Analysis on Selection for High-Resolution Approximations in Many-Objective Optimization | http://arxiv.org/pdf/1409.7478v1.pdf | author:Hernan Aguirre, Arnaud Liefooghe, Sébastien Verel, Kiyoshi Tanaka category:cs.NE published:2014-09-26 summary:This work studies the behavior of three elitist multi- and many-objectiveevolutionary algorithms generating a high-resolution approximation of thePareto optimal set. Several search-assessment indicators are defined to tracethe dynamics of survival selection and measure the ability to simultaneouslykeep optimal solutions and discover new ones under different population sizes,set as a fraction of the size of the Pareto optimal set.
arxiv-7500-187 | Short-term solar irradiance and irradiation forecasts via different time series techniques: A preliminary study | http://arxiv.org/pdf/1409.7476v1.pdf | author:Cédric Join, Cyril Voyant, Michel Fliess, Marc Muselli, Marie Laure Nivet, Christophe Paoli, Frédéric Chaxel category:cs.LG physics.ao-ph published:2014-09-26 summary:This communication is devoted to solar irradiance and irradiation short-termforecasts, which are useful for electricity production. Several different timeseries approaches are employed. Our results and the corresponding numericalsimulations show that techniques which do not need a large amount of historicaldata behave better than those which need them, especially when those data arequite noisy.
arxiv-7500-188 | Extracting man-made objects from remote sensing images via fast level set evolutions | http://arxiv.org/pdf/1409.7474v1.pdf | author:Zhongbin Li, Wenzhong Shi, Qunming Wang, Zelang Miao category:cs.CV 68T10, 68T45 published:2014-09-26 summary:Object extraction from remote sensing images has long been an intensiveresearch topic in the field of surveying and mapping. Most existing methods aredevoted to handling just one type of object and little attention has been paidto improving the computational efficiency. In recent years, level set evolution(LSE) has been shown to be very promising for object extraction in thecommunity of image processing and computer vision because it can handletopological changes automatically while achieving high accuracy. However, theapplication of state-of-the-art LSEs is compromised by laborious parametertuning and expensive computation. In this paper, we proposed two fast LSEs forman-made object extraction from high spatial resolution remote sensing images.The traditional mean curvature-based regularization term is replaced by aGaussian kernel and it is mathematically sound to do that. Thus a larger timestep can be used in the numerical scheme to expedite the proposed LSEs. Incontrast to existing methods, the proposed LSEs are significantly faster. Mostimportantly, they involve much fewer parameters while achieving betterperformance. The advantages of the proposed LSEs over other state-of-the-artapproaches have been verified by a range of experiments.
arxiv-7500-189 | Using graph transformation algorithms to generate natural language equivalents of icons expressing medical concepts | http://arxiv.org/pdf/1411.4614v1.pdf | author:Pascal Vaillant, Jean-Baptiste Lamy category:cs.CL published:2014-09-26 summary:A graphical language addresses the need to communicate medical information ina synthetic way. Medical concepts are expressed by icons conveying fast visualinformation about patients' current state or about the known effects of drugs.In order to increase the visual language's acceptance and usability, a naturallanguage generation interface is currently developed. In this context, thispaper describes the use of an informatics method ---graph transformation--- toprepare data consisting of concepts in an OWL-DL ontology for use in a naturallanguage generation component. The OWL concept may be considered as astar-shaped graph with a central node. The method transforms it into a graphrepresenting the deep semantic structure of a natural language phrase. Thiswork may be of future use in other contexts where ontology concepts have to bemapped to half-formalized natural language expressions.
arxiv-7500-190 | Semi-supervised Spectral Clustering for Classification | http://arxiv.org/pdf/1405.5737v2.pdf | author:Arif Mahmood, Ajmal S. Mian category:cs.CV published:2014-05-22 summary:We propose a Classification Via Clustering (CVC) algorithm which enablesexisting clustering methods to be efficiently employed in classificationproblems. In CVC, training and test data are co-clustered and class-clusterdistributions are used to find the label of the test data. To determine anefficient number of clusters, a Semi-supervised Hierarchical Clustering (SHC)algorithm is proposed. Clusters are obtained by hierarchically applying two-wayNCut by using signs of the Fiedler vector of the normalized graph Laplacian. Tothis end, a Direct Fiedler Vector Computation algorithm is proposed. The graphcut is based on the data structure and does not consider labels. Labels areused only to define the stopping criterion for graph cut. We propose clusteringto be performed on the Grassmannian manifolds facilitating the formation ofspectral ensembles. The proposed algorithm outperformed state-of-the-artimage-set classification algorithms on five standard datasets.
arxiv-7500-191 | Autoencoder Trees | http://arxiv.org/pdf/1409.7461v1.pdf | author:Ozan İrsoy, Ethem Alpaydın category:cs.LG stat.ML published:2014-09-26 summary:We discuss an autoencoder model in which the encoding and decoding functionsare implemented by decision trees. We use the soft decision tree where internalnodes realize soft multivariate splits given by a gating function and theoverall output is the average of all leaves weighted by the gating values ontheir path. The encoder tree takes the input and generates a lower dimensionalrepresentation in the leaves and the decoder tree takes this and reconstructsthe original input. Exploiting the continuity of the trees, autoencoder treesare trained with stochastic gradient descent. On handwritten digit and newsdata, we see that the autoencoder trees yield good reconstruction errorcompared to traditional autoencoder perceptrons. We also see that theautoencoder tree captures hierarchical representations at differentgranularities of the data on its different levels and the leaves capture thelocalities in the input space.
arxiv-7500-192 | Beyond Maximum Likelihood: from Theory to Practice | http://arxiv.org/pdf/1409.7458v1.pdf | author:Jiantao Jiao, Kartik Venkat, Yanjun Han, Tsachy Weissman category:stat.ME cs.DS cs.IT math.IT stat.ML published:2014-09-26 summary:Maximum likelihood is the most widely used statistical estimation technique.Recent work by the authors introduced a general methodology for theconstruction of estimators for functionals in parametric models, anddemonstrated improvements - both in theory and in practice - over the maximumlikelihood estimator (MLE), particularly in high dimensional scenariosinvolving parameter dimension comparable to or larger than the number ofsamples. This approach to estimation, building on results from approximationtheory, is shown to yield minimax rate-optimal estimators for a wide class offunctionals, implementable with modest computational requirements. In anutshell, a message of this recent work is that, for a wide class offunctionals, the performance of these essentially optimal estimators with $n$samples is comparable to that of the MLE with $n \ln n$ samples. In the present paper, we highlight the applicability of the aforementionedmethodology to statistical problems beyond functional estimation, and show thatit can yield substantial gains. For example, we demonstrate that for learningtree-structured graphical models, our approach achieves a significant reductionof the required data size compared with the classical Chow--Liu algorithm,which is an implementation of the MLE, to achieve the same accuracy. The keystep in improving the Chow--Liu algorithm is to replace the empirical mutualinformation with the estimator for mutual information proposed by the authors.Further, applying the same replacement approach to classical Bayesian networkclassification, the resulting classifiers uniformly outperform the previousclassifiers on 26 widely used datasets.
arxiv-7500-193 | Two-stage Geometric Information Guided Image Reconstruction | http://arxiv.org/pdf/1409.7450v1.pdf | author:Jing Qin, Weihong Guo category:math.OC cs.CV published:2014-09-26 summary:In compressive sensing, it is challenging to reconstruct image of highquality from very few noisy linear projections. Existing methods mostly workwell on piecewise constant images but not so well on piecewise smooth imagessuch as natural images, medical images that contain a lot of details. Wepropose a two-stage method called GeoCS to recover images with rich geometricinformation from very limited amount of noisy measurements. The method adoptsthe shearlet transform that is mathematically proven to be optimal in sparselyrepresenting images containing anisotropic features such as edges, corners,spikes etc. It also uses the weighted total variation (TV) sparsity withspatially variant weights to preserve sharp edges but to reduce the staircaseeffects of TV. Geometric information extracted from the results of stage Iserves as an initial prior for stage II which alternates image reconstructionand geometric information update in a mutually beneficial way. GeoCS has beentested on incomplete spectral Fourier samples. It is applicable to other typesof measurements as well. Experimental results on various complicated imagesshow that GeoCS is efficient and generates high-quality images.
arxiv-7500-194 | Community Detection in Sparse Random Networks | http://arxiv.org/pdf/1308.2955v2.pdf | author:Ery Arias-Castro, Nicolas Verzelen category:math.ST stat.ML stat.TH published:2013-08-13 summary:We consider the problem of detecting a tight community in a sparse randomnetwork. This is formalized as testing for the existence of a dense randomsubgraph in a random graph. Under the null hypothesis, the graph is arealization of an Erd\"os-R\'enyi graph on $N$ vertices and with connectionprobability $p_0$; under the alternative, there is an unknown subgraph on $n$vertices where the connection probability is p1 > p0. In Arias-Castro andVerzelen (2012), we focused on the asymptotically dense regime where p0 islarge enough that np0>(n/N)^{o(1)}. We consider here the asymptotically sparseregime where p0 is small enough that np0<(n/N)^{c0} for some c0>0. As before,we derive information theoretic lower bounds, and also establish theperformance of various tests. Compared to our previous work, the arguments forthe lower bounds are based on the same technology, but are substantially moretechnical in the details; also, the methods we study are different: besides avariant of the scan statistic, we study other statistics such as the size ofthe largest connected component, the number of triangles, the eigengap of theadjacency matrix, etc. Our detection bounds are sharp, except in the Poissonregime where we were not able to fully characterize the constant arising in thebound.
arxiv-7500-195 | A Deep Graph Embedding Network Model for Face Recognition | http://arxiv.org/pdf/1409.7313v1.pdf | author:Yufei Gan, Teng Yang, Chu He category:cs.CV published:2014-09-25 summary:In this paper, we propose a new deep learning network "GENet", it combinesthe multi-layer network architec- ture and graph embedding framework. Firstly,we use simplest unsupervised learning PCA/LDA as first layer to generate thelow- level feature. Secondly, many cascaded dimensionality reduction layersbased on graph embedding framework are applied to GENet. Finally, a linear SVMclassifier is used to classify dimension-reduced features. The experimentsindicate that higher classification accuracy can be obtained by this algorithmon the CMU-PIE, ORL, Extended Yale B dataset.
arxiv-7500-196 | Image Classification with A Deep Network Model based on Compressive Sensing | http://arxiv.org/pdf/1409.7307v1.pdf | author:Yufei Gan, Tong Zhuo, Chu He category:cs.CV published:2014-09-25 summary:To simplify the parameter of the deep learning network, a cascadedcompressive sensing model "CSNet" is implemented for image classification.Firstly, we use cascaded compressive sensing network to learn feature from thedata. Secondly, CSNet generates the feature by binary hashing and block-wisehistograms. Finally, a linear SVM classifier is used to classify thesefeatures. The experiments on the MNIST dataset indicate that higherclassification accuracy can be obtained by this algorithm.
arxiv-7500-197 | Performance of Stanford and Minipar Parser on Biomedical Texts | http://arxiv.org/pdf/1409.7386v1.pdf | author:Rushdi Shams category:cs.CL published:2014-09-25 summary:In this paper, the performance of two dependency parsers, namely Stanford andMinipar, on biomedical texts has been reported. The performance of te parsersto assignm dependencies between two biomedical concepts that are already provedto be connected is not satisfying. Both Stanford and Minipar, being statisticalparsers, fail to assign dependency relation between two connected concepts ifthey are distant by at least one clause. Minipar's performance, in terms ofprecision, recall and the F-score of the attachment score (e.g., correctlyidentified head in a dependency), to parse biomedical text is also measuredtaking the Stanford's as a gold standard. The results suggest that Minipar isnot suitable yet to parse biomedical texts. In addition, a qualitativeinvestigation reveals that the difference between working principles of theparsers also play a vital role for Minipar's degraded performance.
arxiv-7500-198 | Semi-supervised Classification for Natural Language Processing | http://arxiv.org/pdf/1409.7612v1.pdf | author:Rushdi Shams category:cs.CL cs.LG published:2014-09-25 summary:Semi-supervised classification is an interesting idea where classificationmodels are learned from both labeled and unlabeled data. It has severaladvantages over supervised classification in natural language processingdomain. For instance, supervised classification exploits only labeled data thatare expensive, often difficult to get, inadequate in quantity, and requirehuman experts for annotation. On the other hand, unlabeled data are inexpensiveand abundant. Despite the fact that many factors limit the wide-spread use ofsemi-supervised classification, it has become popular since its level ofperformance is empirically as good as supervised classification. This studyexplores the possibilities and achievements as well as complexity andlimitations of semi-supervised classification for several natural langueprocessing tasks like parsing, biomedical information processing, textclassification, and summarization.
arxiv-7500-199 | Identification of jump Markov linear models using particle filters | http://arxiv.org/pdf/1409.7287v1.pdf | author:Andreas Svensson, Thomas B. Schön, Fredrik Lindsten category:stat.CO math.OC stat.ML published:2014-09-25 summary:Jump Markov linear models consists of a finite number of linear state spacemodels and a discrete variable encoding the jumps (or switches) between thedifferent linear models. Identifying jump Markov linear models makes for achallenging problem lacking an analytical solution. We derive a new expectationmaximization (EM) type algorithm that produce maximum likelihood estimates ofthe model parameters. Our development hinges upon recent progress in combiningparticle filters with Markov chain Monte Carlo methods in solving the nonlinearstate smoothing problem inherent in the EM formulation. Key to our developmentis that we exploit a conditionally linear Gaussian substructure in the model,allowing for an efficient algorithm.
arxiv-7500-200 | The meaning-frequency law in Zipfian optimization models of communication | http://arxiv.org/pdf/1409.7275v1.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL physics.soc-ph published:2014-09-25 summary:According to Zipf's meaning-frequency law, words that are more frequent tendto have more meanings. Here it is shown that a linear dependency between thefrequency of a form and its number of meanings is found in a family of modelsof Zipf's law for word frequencies. This is evidence for a weak version of themeaning-frequency law. Interestingly, that weak law (a) is not an inevitable ofproperty of the assumptions of the family and (b) is found at least in thenarrow regime where those models exhibit Zipf's law for word frequencies.
arxiv-7500-201 | Ctrax extensions for tracking in difficult lighting conditions | http://arxiv.org/pdf/1409.7272v1.pdf | author:Ulrich Stern, Chung-Hui Yang category:q-bio.QM cs.CV published:2014-09-25 summary:The fly tracking software Ctrax by Branson et al. is popular for positionaltracking of animals both within and beyond the fly community. Ctrax was notdesigned to handle tracking in difficult lighting conditions with strongshadows or recurring "on"/"off" changes in lighting - a condition that willlikely become increasingly common due to the advent of red-shiftedchannelrhodopsin. We describe Ctrax extensions we developed that address thisproblem. The extensions enabled good tracking accuracy in three types ofdifficult lighting conditions in our lab. Our technique handling shadows relieson "single animal tracking"; the other techniques should be widely applicable.
arxiv-7500-202 | Generating Conceptual Metaphors from Proposition Stores | http://arxiv.org/pdf/1409.7619v1.pdf | author:Ekaterina Ovchinnikova, Vladimir Zaytsev, Suzanne Wertheim, Ross Israel category:cs.CL published:2014-09-25 summary:Contemporary research on computational processing of linguistic metaphors isdivided into two main branches: metaphor recognition and metaphorinterpretation. We take a different line of research and present an automatedmethod for generating conceptual metaphors from linguistic data. Given thegenerated conceptual metaphors, we find corresponding linguistic metaphors incorpora. In this paper, we describe our approach and its evaluation usingEnglish and Russian data.
arxiv-7500-203 | The risks of mixing dependency lengths from sequences of different length | http://arxiv.org/pdf/1304.3841v2.pdf | author:Ramon Ferrer-i-Cancho, Haitao Liu category:cs.CL published:2013-04-13 summary:Mixing dependency lengths from sequences of different length is a commonpractice in language research. However, the empirical distribution ofdependency lengths of sentences of the same length differs from that ofsentences of varying length and the distribution of dependency lengths dependson sentence length for real sentences and also under the null hypothesis thatdependencies connect vertices located in random positions of the sequence. Thissuggests that certain results, such as the distribution of syntactic dependencylengths mixing dependencies from sentences of varying length, could be a mereconsequence of that mixing. Furthermore, differences in the global averages ofdependency length (mixing lengths from sentences of varying length) for twodifferent languages do not simply imply a priori that one language optimizesdependency lengths better than the other because those differences could be dueto differences in the distribution of sentence lengths and other factors.
arxiv-7500-204 | Heterogeneous Metric Learning with Content-based Regularization for Software Artifact Retrieval | http://arxiv.org/pdf/1409.7165v1.pdf | author:Liang Wu, Hui Xiong, Liang Du, Bo Liu, Guandong Xu, Yong Ge, Yanjie Fu, Yuanchun Zhou, Jianhui Li category:cs.LG cs.IR cs.SE published:2014-09-25 summary:The problem of software artifact retrieval has the goal to effectively locatesoftware artifacts, such as a piece of source code, in a large code repository.This problem has been traditionally addressed through the textual query. Inother words, information retrieval techniques will be exploited based on thetextual similarity between queries and textual representation of softwareartifacts, which is generated by collecting words from comments, identifiers,and descriptions of programs. However, in addition to these semanticinformation, there are rich information embedded in source codes themselves.These source codes, if analyzed properly, can be a rich source for enhancingthe efforts of software artifact retrieval. To this end, in this paper, wedevelop a feature extraction method on source codes. Specifically, this methodcan capture both the inherent information in the source codes and the semanticinformation hidden in the comments, descriptions, and identifiers of the sourcecodes. Moreover, we design a heterogeneous metric learning approach, whichallows to integrate code features and text features into the same latentsemantic space. This, in turn, can help to measure the artifact similarity byexploiting the joint power of both code and text features. Finally, extensiveexperiments on real-world data show that the proposed method can help toimprove the performances of software artifact retrieval with a significantmargin.
arxiv-7500-205 | Deep Learning Representation using Autoencoder for 3D Shape Retrieval | http://arxiv.org/pdf/1409.7164v1.pdf | author:Zhuotun Zhu, Xinggang Wang, Song Bai, Cong Yao, Xiang Bai category:cs.CV published:2014-09-25 summary:We study the problem of how to build a deep learning representation for 3Dshape. Deep learning has shown to be very effective in variety of visualapplications, such as image classification and object detection. However, ithas not been successfully applied to 3D shape recognition. This is because 3Dshape has complex structure in 3D space and there are limited number of 3Dshapes for feature learning. To address these problems, we project 3D shapesinto 2D space and use autoencoder for feature learning on the 2D images. Highaccuracy 3D shape retrieval performance is obtained by aggregating the featureslearned on 2D images. In addition, we show the proposed deep learning featureis complementary to conventional local image descriptors. By combing the globaldeep learning representation and the local descriptor representation, ourmethod can obtain the state-of-the-art performance on 3D shape retrievalbenchmarks.
arxiv-7500-206 | Deeply-Supervised Nets | http://arxiv.org/pdf/1409.5185v2.pdf | author:Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu category:stat.ML cs.LG cs.NE published:2014-09-18 summary:Our proposed deeply-supervised nets (DSN) method simultaneously minimizesclassification error while making the learning process of hidden layers directand transparent. We make an attempt to boost the classification performance bystudying a new formulation in deep networks. Three aspects in convolutionalneural networks (CNN) style architectures are being looked at: (1) transparencyof the intermediate layers to the overall classification; (2)discriminativeness and robustness of learned features, especially in the earlylayers; (3) effectiveness in training due to the presence of the exploding andvanishing gradients. We introduce "companion objective" to the individualhidden layers, in addition to the overall objective at the output layer (adifferent strategy to layer-wise pre-training). We extend techniques fromstochastic gradient methods to analyze our algorithm. The advantage of ourmethod is evident and our experimental result on benchmark datasets showssignificant performance gain over existing methods (e.g. all state-of-the-artresults on MNIST, CIFAR-10, CIFAR-100, and SVHN).
arxiv-7500-207 | Learning rates of $l^q$ coefficient regularization learning with Gaussian kernel | http://arxiv.org/pdf/1312.5465v3.pdf | author:Shaobo Lin, Jinshan Zeng, Jian Fang, Zongben Xu category:cs.LG stat.ML 68T05 F.2.1 published:2013-12-19 summary:Regularization is a well recognized powerful strategy to improve theperformance of a learning machine and $l^q$ regularization schemes with$0<q<\infty$ are central in use. It is known that different $q$ leads todifferent properties of the deduced estimators, say, $l^2$ regularization leadsto smooth estimators while $l^1$ regularization leads to sparse estimators.Then, how does the generalization capabilities of $l^q$ regularization learningvary with $q$? In this paper, we study this problem in the framework ofstatistical learning theory and show that implementing $l^q$ coefficientregularization schemes in the sample dependent hypothesis space associated withGaussian kernel can attain the same almost optimal learning rates for all$0<q<\infty$. That is, the upper and lower bounds of learning rates for $l^q$regularization learning are asymptotically identical for all $0<q<\infty$. Ourfinding tentatively reveals that, in some modeling contexts, the choice of $q$might not have a strong impact with respect to the generalization capability.From this perspective, $q$ can be arbitrarily specified, or specified merely byother no generalization criteria like smoothness, computational complexity,sparsity, etc..
arxiv-7500-208 | Semantically-Informed Syntactic Machine Translation: A Tree-Grafting Approach | http://arxiv.org/pdf/1409.7085v1.pdf | author:Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie J. Dorr, Nathaniel W. Filardo, Lori Levin, Scott Miller, Christine Piatko category:cs.CL cs.LG stat.ML published:2014-09-24 summary:We describe a unified and coherent syntactic framework for supporting asemantically-informed syntactic approach to statistical machine translation.Semantically enriched syntactic tags assigned to the target-language trainingtexts improved translation quality. The resulting system significantlyoutperformed a linguistically naive baseline model (Hiero), and reached thehighest scores yet reported on the NIST 2009 Urdu-English translation task.This finding supports the hypothesis (posed by many researchers in the MTcommunity, e.g., in DARPA GALE) that both syntactic and semantic informationare critical for improving translation quality---and further demonstrates thatlarge gains can be achieved for low-resource languages with different wordorder than English.
arxiv-7500-209 | Variational Pseudolikelihood for Regularized Ising Inference | http://arxiv.org/pdf/1409.7074v1.pdf | author:Charles K. Fisher category:cs.LG stat.ML published:2014-09-24 summary:I propose a variational approach to maximum pseudolikelihood inference of theIsing model. The variational algorithm is more computationally efficient, anddoes a better job predicting out-of-sample correlations than $L_2$ regularizedmaximum pseudolikelihood inference as well as mean field and isolated spin pairapproximations with pseudocount regularization. The key to the approach is avariational energy that regularizes the inference problem by shrinking thecouplings towards zero, while still allowing some large couplings to explainstrong correlations. The utility of the variational pseudolikelihood approachis illustrated by training an Ising model to represent the letters A-J usingsamples of letters from different computer fonts.
arxiv-7500-210 | Unsupervised learning of regression mixture models with unknown number of components | http://arxiv.org/pdf/1409.6981v1.pdf | author:Faicel Chamroukhi category:stat.ME cs.LG stat.ML published:2014-09-24 summary:Regression mixture models are widely studied in statistics, machine learningand data analysis. Fitting regression mixtures is challenging and is usuallyperformed by maximum likelihood by using the expectation-maximization (EM)algorithm. However, it is well-known that the initialization is crucial for EM.If the initialization is inappropriately performed, the EM algorithm may leadto unsatisfactory results. The EM algorithm also requires the number ofclusters to be given a priori; the problem of selecting the number of mixturecomponents requires using model selection criteria to choose one from a set ofpre-estimated candidate models. We propose a new fully unsupervised algorithmto learn regression mixture models with unknown number of components. Thedeveloped unsupervised learning approach consists in a penalized maximumlikelihood estimation carried out by a robust expectation-maximization (EM)algorithm for fitting polynomial, spline and B-spline regressions mixtures. Theproposed learning approach is fully unsupervised: 1) it simultaneously infersthe model parameters and the optimal number of the regression mixturecomponents from the data as the learning proceeds, rather than in a two-foldscheme as in standard model-based clustering using afterward model selectioncriteria, and 2) it does not require accurate initialization unlike thestandard EM for regression mixtures. The developed approach is applied to curveclustering problems. Numerical experiments on simulated data show that theproposed robust EM algorithm performs well and provides accurate results interms of robustness with regard initialization and retrieving the optimalpartition with the actual number of clusters. An application to real data inthe framework of functional data clustering, confirms the benefit of theproposed approach for practical applications.
arxiv-7500-211 | One-class Collaborative Filtering with Random Graphs: Annotated Version | http://arxiv.org/pdf/1309.6786v4.pdf | author:Ulrich Paquet, Noam Koenigstein category:stat.ML cs.LG G.3 published:2013-09-26 summary:The bane of one-class collaborative filtering is interpreting and modellingthe latent signal from the missing class. In this paper we present a novelBayesian generative model for implicit collaborative filtering. It forms a corecomponent of the Xbox Live architecture, and unlike previous approaches,delineates the odds of a user disliking an item from simply not considering it.The latent signal is treated as an unobserved random graph connecting userswith items they might have encountered. We demonstrate how large-scaledistributed learning can be achieved through a combination of stochasticgradient descent and mean field variational inference over random graphsamples. A fine-grained comparison is done against a state of the art baselineon real world data.
arxiv-7500-212 | Recent Progress in Image Deblurring | http://arxiv.org/pdf/1409.6838v1.pdf | author:Ruxin Wang, Dacheng Tao category:cs.CV published:2014-09-24 summary:This paper comprehensively reviews the recent development of imagedeblurring, including non-blind/blind, spatially invariant/variant deblurringtechniques. Indeed, these techniques share the same objective of inferring alatent sharp image from one or several corresponding blurry images, while theblind deblurring techniques are also required to derive an accurate blurkernel. Considering the critical role of image restoration in modern imagingsystems to provide high-quality images under complex environments such asmotion, undesirable lighting conditions, and imperfect system components, imagedeblurring has attracted growing attention in recent years. From the viewpointof how to handle the ill-posedness which is a crucial issue in deblurringtasks, existing methods can be grouped into five categories: Bayesian inferenceframework, variational methods, sparse representation-based methods,homography-based modeling, and region-based methods. In spite of achieving acertain level of development, image deblurring, especially the blind case, islimited in its success by complex application conditions which make the blurkernel hard to obtain and be spatially variant. We provide a holisticunderstanding and deep insight into image deblurring in this review. Ananalysis of the empirical evidence for representative methods, practicalissues, as well as a discussion of promising future directions are alsopresented.
arxiv-7500-213 | Quantized Estimation of Gaussian Sequence Models in Euclidean Balls | http://arxiv.org/pdf/1409.6833v1.pdf | author:Yuancheng Zhu, John Lafferty category:math.ST stat.ML stat.TH published:2014-09-24 summary:A central result in statistical theory is Pinsker's theorem, whichcharacterizes the minimax rate in the normal means model of nonparametricestimation. In this paper, we present an extension to Pinsker's theorem whereestimation is carried out under storage or communication constraints. Inparticular, we place limits on the number of bits used to encode an estimator,and analyze the excess risk in terms of this constraint, the signal size, andthe noise level. We give sharp upper and lower bounds for the case of aEuclidean ball, which establishes the Pareto-optimal minimax tradeoff betweenstorage and risk in this setting.
arxiv-7500-214 | Improving Cross-domain Recommendation through Probabilistic Cluster-level Latent Factor Model--Extended Version | http://arxiv.org/pdf/1409.6805v1.pdf | author:Siting Ren, Sheng Gao category:cs.IR cs.LG stat.ML published:2014-09-24 summary:Cross-domain recommendation has been proposed to transfer user behaviorpattern by pooling together the rating data from multiple domains to alleviatethe sparsity problem appearing in single rating domains. However, previousmodels only assume that multiple domains share a latent common rating patternbased on the user-item co-clustering. To capture diversities among differentdomains, we propose a novel Probabilistic Cluster-level Latent Factor (PCLF)model to improve the cross-domain recommendation performance. Experiments onseveral real world datasets demonstrate that our proposed model outperforms thestate-of-the-art methods for the cross-domain recommendation task.
arxiv-7500-215 | A Concept Learning Approach to Multisensory Object Perception | http://arxiv.org/pdf/1409.6745v1.pdf | author:Ifeoma Nwogu, Goker Erdogan, Ilker Yildirim, Robert Jacobs category:cs.CV published:2014-09-23 summary:This paper presents a computational model of concept learning using Bayesianinference for a grammatically structured hypothesis space, and test the modelon multisensory (visual and haptics) recognition of 3D objects. The study isperformed on a set of artificially generated 3D objects known as fribbles,which are complex, multipart objects with categorical structures. The goal ofthis work is to develop a working multisensory representational model thatintegrates major themes on concepts and concepts learning from the cognitivescience literature. The model combines the representational power of aprobabilistic generative grammar with the inferential power of Bayesianinduction.
arxiv-7500-216 | Local case-control sampling: Efficient subsampling in imbalanced data sets | http://arxiv.org/pdf/1306.3706v2.pdf | author:William Fithian, Trevor Hastie category:stat.CO stat.ML published:2013-06-16 summary:For classification problems with significant class imbalance, subsampling canreduce computational costs at the price of inflated variance in estimatingmodel parameters. We propose a method for subsampling efficiently for logisticregression by adjusting the class balance locally in feature space via anaccept-reject scheme. Our method generalizes standard case-control sampling,using a pilot estimate to preferentially select examples whose responses areconditionally rare given their features. The biased subsampling is corrected bya post-hoc analytic adjustment to the parameters. The method is simple andrequires one parallelizable scan over the full data set. Standard case-controlsampling is inconsistent under model misspecification for the populationrisk-minimizing coefficients $\theta^*$. By contrast, our estimator isconsistent for $\theta^*$ provided that the pilot estimate is. Moreover, undercorrect specification and with a consistent, independent pilot estimate, ourestimator has exactly twice the asymptotic variance of the full-sample MLE -even if the selected subsample comprises a miniscule fraction of the full dataset, as happens when the original data are severely imbalanced. The factor oftwo improves to $1+\frac{1}{c}$ if we multiply the baseline acceptanceprobabilities by $c>1$ (and weight points with acceptance probability greaterthan 1), taking roughly $\frac{1+c}{2}$ times as many data points into thesubsample. Experiments on simulated and real data show that our method cansubstantially outperform standard case-control subsampling.
arxiv-7500-217 | HSR: L1/2 Regularized Sparse Representation for Fast Face Recognition using Hierarchical Feature Selection | http://arxiv.org/pdf/1409.6448v1.pdf | author:Bo Han, Bo He, Tingting Sun, Mengmeng Ma, Amaury Lendasse category:cs.CV cs.LG published:2014-09-23 summary:In this paper, we propose a novel method for fast face recognition calledL1/2 Regularized Sparse Representation using Hierarchical Feature Selection(HSR). By employing hierarchical feature selection, we can compress the scaleand dimension of global dictionary, which directly contributes to the decreaseof computational cost in sparse representation that our approach is stronglyrooted in. It consists of Gabor wavelets and Extreme Learning MachineAuto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local featurescan be extracted at multiple scales and orientations to form Gabor-featurebased image, which in turn improves the recognition rate. Besides, in thepresence of occluded face image, the scale of Gabor-feature based globaldictionary can be compressed accordingly because redundancies exist inGabor-feature based occlusion dictionary. For ELM-AE part, the dimension ofGabor-feature based global dictionary can be compressed becausehigh-dimensional face images can be rapidly represented by low-dimensionalfeature. By introducing L1/2 regularization, our approach can produce sparserand more robust representation compared to regularized Sparse Representationbased Classification (SRC), which also contributes to the decrease of thecomputational cost in sparse representation. In comparison with related worksuch as SRC and Gabor-feature based SRC (GSRC), experimental results on avariety of face databases demonstrate the great advantage of our method forcomputational cost. Moreover, we also achieve approximate or even betterrecognition rate.
arxiv-7500-218 | RMSE-ELM: Recursive Model based Selective Ensemble of Extreme Learning Machines for Robustness Improvement | http://arxiv.org/pdf/1408.2004v3.pdf | author:Bo Han, Bo He, Mengmeng Ma, Tingting Sun, Tianhong Yan, Amaury Lendasse category:cs.LG cs.NE published:2014-08-09 summary:Extreme learning machine (ELM) as an emerging branch of shallow networks hasshown its excellent generalization and fast learning speed. However, forblended data, the robustness of ELM is weak because its weights and biases ofhidden nodes are set randomly. Moreover, the noisy data exert a negativeeffect. To solve this problem, a new framework called RMSE-ELM is proposed inthis paper. It is a two-layer recursive model. In the first layer, theframework trains lots of ELMs in different groups concurrently, then employsselective ensemble to pick out an optimal set of ELMs in each group, which canbe merged into a large group of ELMs called candidate pool. In the secondlayer, selective ensemble is recursively used on candidate pool to acquire thefinal ensemble. In the experiments, we apply UCI blended datasets to confirmthe robustness of our new approach in two key aspects (mean square error andstandard deviation). The space complexity of our method is increased to somedegree, but the results have shown that RMSE-ELM significantly improvesrobustness with slightly computational time compared with representativemethods (ELM, OP-ELM, GASEN-ELM, GASEN-BP and E-GASEN). It becomes a potentialframework to solve robustness issue of ELM for high-dimensional blended data inthe future.
arxiv-7500-219 | The two-dimensional Gabor function adapted to natural image statistics: An analytical model of simple-cell responses in the early visual system | http://arxiv.org/pdf/1304.0023v3.pdf | author:Peter Loxley category:cs.CV published:2013-03-29 summary:The two-dimensional Gabor function is adapted to natural image statistics bylearning the joint distribution of the Gabor function parameters. The jointdistribution is then approximated to yield an analytical model of simple-cellreceptive fields. Adapting a basis of Gabor functions is found to take an orderof magnitude less computation than learning an equivalent non-parameterizedbasis. Derived learning rules are shown to be capable of adapting Gaborparameters to the statistics of images of man-made and natural environments.Learning is found to be most pronounced in three Gabor parameters thatrepresent the size, aspect-ratio, and spatial frequency of the two-dimensionalGabor function. These three parameters are characterized by non-uniformmarginal distributions with heavy tails -- most likely due to scale invariancein natural images -- and all three parameters are strongly correlated:resulting in a basis of multiscale Gabor functions with similar aspect-ratios,and size-dependent spatial frequencies. The Gabor orientation and phaseparameters do not appear to gain anything from learning over natural images.Different tuning strategies are found by controlling learning through the Gaborparameter learning rates. Two opposing strategies include well-resolvedorientation and well-resolved spatial frequency. On image reconstruction, abasis of Gabor functions with fitted marginal distributions is shown tosignificantly outperform a basis of Gabor functions generated from uniformlysampled parameters. An additional increase in performance results when thestrong correlations are included. However, the best analytical model does notyet achieve the performance of the learned model. A comparison with estimatesfor biological simple cells shows that the Gabor function adapted to naturalimage statistics correctly predicts some key receptive field properties.
arxiv-7500-220 | Unified Structured Learning for Simultaneous Human Pose Estimation and Garment Attribute Classification | http://arxiv.org/pdf/1404.4923v3.pdf | author:Jie Shen, Guangcan Liu, Jia Chen, Yuqiang Fang, Jianbin Xie, Yong Yu, Shuicheng Yan category:cs.CV published:2014-04-19 summary:In this paper, we utilize structured learning to simultaneously address twointertwined problems: human pose estimation (HPE) and garment attributeclassification (GAC), which are valuable for a variety of computer vision andmultimedia applications. Unlike previous works that usually handle the twoproblems separately, our approach aims to produce a jointly optimal estimationfor both HPE and GAC via a unified inference procedure. To this end, we adopt apreprocessing step to detect potential human parts from each image (i.e., a setof "candidates") that allows us to have a manageable input space. In this way,the simultaneous inference of HPE and GAC is converted to a structured learningproblem, where the inputs are the collections of candidate ensembles, theoutputs are the joint labels of human parts and garment attributes, and thejoint feature representation involves various cues such as pose-specificfeatures, garment-specific features, and cross-task features that encodecorrelations between human parts and garment attributes. Furthermore, weexplore the "strong edge" evidence around the potential human parts so as toderive more powerful representations for oriented human parts. Such evidencescan be seamlessly integrated into our structured learning model as a kind ofenergy function, and the learning process could be performed by standardstructured Support Vector Machines (SVM) algorithm. However, the jointstructure of the two problems is a cyclic graph, which hinders efficientinference. To resolve this issue, we compute instead approximate optima byusing an iterative procedure, where in each iteration the variables of oneproblem are fixed. In this way, satisfactory solutions can be efficientlycomputed by dynamic programming. Experimental results on two benchmark datasetsshow the state-of-the-art performance of our approach.
arxiv-7500-221 | Analyzing the Performance of Multilayer Neural Networks for Object Recognition | http://arxiv.org/pdf/1407.1610v2.pdf | author:Pulkit Agrawal, Ross Girshick, Jitendra Malik category:cs.CV cs.NE published:2014-07-07 summary:In the last two years, convolutional neural networks (CNNs) have achieved animpressive suite of results on standard recognition datasets and tasks.CNN-based features seem poised to quickly replace engineered representations,such as SIFT and HOG. However, compared to SIFT and HOG, we understand muchless about the nature of the features learned by large CNNs. In this paper, weexperimentally probe several aspects of CNN feature learning in an attempt tohelp practitioners gain useful, evidence-backed intuitions about how to applyCNNs to computer vision problems.
arxiv-7500-222 | Detecting People in Cubist Art | http://arxiv.org/pdf/1409.6235v1.pdf | author:Shiry Ginosar, Daniel Haas, Timothy Brown, Jitendra Malik category:cs.CV published:2014-09-22 summary:Although the human visual system is surprisingly robust to extreme distortionwhen recognizing objects, most evaluations of computer object detection methodsfocus only on robustness to natural form deformations such as people's posechanges. To determine whether algorithms truly mirror the flexibility of humanvision, they must be compared against human vision at its limits. For example,in Cubist abstract art, painted objects are distorted by object fragmentationand part-reorganization, to the point that human vision often fails torecognize them. In this paper, we evaluate existing object detection methods onthese abstract renditions of objects, comparing human annotators to fourstate-of-the-art object detectors on a corpus of Picasso paintings. Our resultsdemonstrate that while human perception significantly outperforms currentmethods, human perception and part-based models exhibit a similarly gracefuldegradation in object detection performance as the objects become increasinglyabstract and fragmented, corroborating the theory of part-based objectrepresentation in the brain.
arxiv-7500-223 | Expectation Propagation | http://arxiv.org/pdf/1409.6179v1.pdf | author:Jack Raymond, Andre Manoel, Manfred Opper category:stat.ML published:2014-09-22 summary:Variational inference is a powerful concept that underlies many iterativeapproximation algorithms; expectation propagation, mean-field methods andbelief propagations were all central themes at the school that can be perceivedfrom this unifying framework. The lectures of Manfred Opper introduce thearchetypal example of Expectation Propagation, before establishing theconnection with the other approximation methods. Corrections by expansion aboutthe expectation propagation are then explained. Finally some advanced inferencetopics and applications are explored in the final sections.
arxiv-7500-224 | A Bayesian Network View on Acoustic Model-Based Techniques for Robust Speech Recognition | http://arxiv.org/pdf/1310.3099v2.pdf | author:Roland Maas, Christian Huemmer, Armin Sehr, Walter Kellermann category:cs.LG cs.CL stat.ML published:2013-10-11 summary:This article provides a unifying Bayesian network view on various approachesfor acoustic model adaptation, missing feature, and uncertainty decoding thatare well-known in the literature of robust automatic speech recognition. Therepresentatives of these classes can often be deduced from a Bayesian networkthat extends the conventional hidden Markov models used in speech recognition.These extensions, in turn, can in many cases be motivated from an underlyingobservation model that relates clean and distorted feature vectors. Byconverting the observation models into a Bayesian network representation, weformulate the corresponding compensation rules leading to a unified view onknown derivations as well as to new formulations for certain approaches. Thegeneric Bayesian perspective provided in this contribution thus highlightsstructural differences and similarities between the analyzed approaches.
arxiv-7500-225 | Distributed Clustering and Learning Over Networks | http://arxiv.org/pdf/1409.6111v1.pdf | author:Xiaochuan Zhao, Ali H. Sayed category:math.OC cs.LG cs.MA cs.SY stat.ML published:2014-09-22 summary:Distributed processing over networks relies on in-network processing andcooperation among neighboring agents. Cooperation is beneficial when agentsshare a common objective. However, in many applications agents may belong todifferent clusters that pursue different objectives. Then, indiscriminatecooperation will lead to undesired results. In this work, we propose anadaptive clustering and learning scheme that allows agents to learn whichneighbors they should cooperate with and which other neighbors they shouldignore. In doing so, the resulting algorithm enables the agents to identifytheir clusters and to attain improved learning and estimation accuracy overnetworks. We carry out a detailed mean-square analysis and assess the errorprobabilities of Types I and II, i.e., false alarm and mis-detection, for theclustering mechanism. Among other results, we establish that theseprobabilities decay exponentially with the step-sizes so that the probabilityof correct clustering can be made arbitrarily close to one.
arxiv-7500-226 | Action Classification with Locality-constrained Linear Coding | http://arxiv.org/pdf/1408.3810v2.pdf | author:Hossein Rahmani, Arif Mahmood, Du Huynh, Ajmal Mian category:cs.CV published:2014-08-17 summary:We propose an action classification algorithm which uses Locality-constrainedLinear Coding (LLC) to capture discriminative information of human bodyvariations in each spatiotemporal subsequence of a video sequence. Our proposedmethod divides the input video into equally spaced overlapping spatiotemporalsubsequences, each of which is decomposed into blocks and then cells. We usethe Histogram of Oriented Gradient (HOG3D) feature to encode the information ineach cell. We justify the use of LLC for encoding the block descriptor bydemonstrating its superiority over Sparse Coding (SC). Our sequence descriptoris obtained via a logistic regression classifier with L2 regularization. Weevaluate and compare our algorithm with ten state-of-the-art algorithms on fivebenchmark datasets. Experimental results show that, on average, our algorithmgives better accuracy than these ten algorithms.
arxiv-7500-227 | HOPC: Histogram of Oriented Principal Components of 3D Pointclouds for Action Recognition | http://arxiv.org/pdf/1408.3809v4.pdf | author:Hossein Rahmani, Arif Mahmood, Du Q. Huynh, Ajmal Mian category:cs.CV published:2014-08-17 summary:Existing techniques for 3D action recognition are sensitive to viewpointvariations because they extract features from depth images which changesignificantly with viewpoint. In contrast, we directly process the pointcloudsand propose a new technique for action recognition which is more robust tonoise, action speed and viewpoint variations. Our technique consists of a noveldescriptor and keypoint detection algorithm. The proposed descriptor isextracted at a point by encoding the Histogram of Oriented Principal Components(HOPC) within an adaptive spatio-temporal support volume around that point.Based on this descriptor, we present a novel method to detect Spatio-TemporalKey-Points (STKPs) in 3D pointcloud sequences. Experimental results show thatthe proposed descriptor and STKP detector outperform state-of-the-artalgorithms on three benchmark human activity datasets. We also introduce a newmultiview public dataset and show the robustness of our proposed method toviewpoint variations.
arxiv-7500-228 | Spatially-sparse convolutional neural networks | http://arxiv.org/pdf/1409.6070v1.pdf | author:Benjamin Graham category:cs.CV cs.NE published:2014-09-22 summary:Convolutional neural networks (CNNs) perform well on problems such ashandwriting recognition and image classification. However, the performance ofthe networks is often limited by budget and time constraints, particularly whentrying to train deep networks. Motivated by the problem of online handwriting recognition, we developed aCNN for processing spatially-sparse inputs; a character drawn with a one-pixelwide pen on a high resolution grid looks like a sparse matrix. Taking advantageof the sparsity allowed us more efficiently to train and test large, deep CNNs.On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a testerror of 3.82%. Although pictures are not sparse, they can be thought of as sparse by addingpadding. Applying a deep convolutional network using sparsity has resulted in asubstantial reduction in test error on the CIFAR small picture datasets: 6.28%on CIFAR-10 and 24.30% for CIFAR-100.
arxiv-7500-229 | On the Maximum Entropy Property of the First-Order Stable Spline Kernel and its Implications | http://arxiv.org/pdf/1406.5706v2.pdf | author:Francesca Paola Carli category:math.ST cs.LG stat.ML stat.TH published:2014-06-22 summary:A new nonparametric approach for system identification has been recentlyproposed where the impulse response is seen as the realization of a zero--meanGaussian process whose covariance, the so--called stable spline kernel,guarantees that the impulse response is almost surely stable. Maximum entropyproperties of the stable spline kernel have been pointed out in the literature.In this paper we provide an independent proof that relies on the theory ofmatrix extension problems in the graphical model literature and leads to aclosed form expression for the inverse of the first order stable spline kernelas well as to a new factorization in the form $UWU^\top$ with $U$ uppertriangular and $W$ diagonal. Interestingly, all first--order stable splinekernels share the same factor $U$ and $W$ admits a closed form representationin terms of the kernel hyperparameter, making the factorization computationallyinexpensive. Maximum likelihood properties of the stable spline kernel are alsohighlighted. These results can be applied both to improve the stability and toreduce the computational complexity associated with the computation of stablespline estimators.
arxiv-7500-230 | Approximation errors of online sparsification criteria | http://arxiv.org/pdf/1409.6046v1.pdf | author:Paul Honeine category:stat.ML cs.CV cs.IT cs.LG cs.NE math.IT published:2014-09-21 summary:Many machine learning frameworks, such as resource-allocating networks,kernel-based methods, Gaussian processes, and radial-basis-function networks,require a sparsification scheme in order to address the online learningparadigm. For this purpose, several online sparsification criteria have beenproposed to restrict the model definition on a subset of samples. The mostknown criterion is the (linear) approximation criterion, which discards anysample that can be well represented by the already contributing samples, anoperation with excessive computational complexity. Several computationallyefficient sparsification criteria have been introduced in the literature, suchas the distance, the coherence and the Babel criteria. In this paper, weprovide a framework that connects these sparsification criteria to the issue ofapproximating samples, by deriving theoretical bounds on the approximationerrors. Moreover, we investigate the error of approximating any feature, byproposing upper-bounds on the approximation error for each of theaforementioned sparsification criteria. Two classes of features are describedin detail, the empirical mean and the principal axes in the kernel principalcomponent analysis.
arxiv-7500-231 | Analyzing sparse dictionaries for online learning with kernels | http://arxiv.org/pdf/1409.6045v1.pdf | author:Paul Honeine category:stat.ML cs.CV cs.IT cs.LG math.IT published:2014-09-21 summary:Many signal processing and machine learning methods share essentially thesame linear-in-the-parameter model, with as many parameters as availablesamples as in kernel-based machines. Sparse approximation is essential in manydisciplines, with new challenges emerging in online learning with kernels. Tothis end, several sparsity measures have been proposed in the literature toquantify sparse dictionaries and constructing relevant ones, the most prolificones being the distance, the approximation, the coherence and the Babelmeasures. In this paper, we analyze sparse dictionaries based on thesemeasures. By conducting an eigenvalue analysis, we show that these sparsitymeasures share many properties, including the linear independence condition andinducing a well-posed optimization problem. Furthermore, we prove that thereexists a quasi-isometry between the parameter (i.e., dual) space and thedictionary's induced feature space.
arxiv-7500-232 | Domain Adaptive Neural Networks for Object Recognition | http://arxiv.org/pdf/1409.6041v1.pdf | author:Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang category:cs.CV cs.NE published:2014-09-21 summary:We propose a simple neural network model to deal with the domain adaptationproblem in object recognition. Our model incorporates the Maximum MeanDiscrepancy (MMD) measure as a regularization in the supervised learning toreduce the distribution mismatch between the source and target domains in thelatent space. From experiments, we demonstrate that the MMD regularization isan effective tool to provide good domain adaptation models on both SURFfeatures and raw image pixels of a particular image data set. We also show thatour proposed model, preceded by the denoising auto-encoder pretraining,achieves better performance than recent benchmark models on the same data sets.This work represents the first study of MMD measure in the context of neuralnetworks.
arxiv-7500-233 | A High-Level Model of Neocortical Feedback Based on an Event Window Segmentation Algorithm | http://arxiv.org/pdf/1409.6023v1.pdf | author:Jerry R. Van Aken category:cs.NE I.2.7 published:2014-09-21 summary:The author previously presented an event window segmentation (EWS) algorithm[5] that uses purely statistical methods to learn to recognize recurringpatterns in an input stream of events. In the following discussion, the EWSalgorithm is first extended to make predictions about future events. Next, thisextended algorithm is used to construct a high-level, simplified model of aneocortical hierarchy. An event stream enters at the bottom of the hierarchy,and drives processing activity upward in the hierarchy. Successively higherregions in the hierarchy learn to recognize successively deeper levels ofpatterns in these events as they propagate from the bottom of the hierarchy.The lower levels in the hierarchy use the predictions from the levels above tostrengthen their own predictions. A C++ source code listing of the modelimplementation and test program is included as an appendix.
arxiv-7500-234 | A KdV-like advection-dispersion equation with some remarkable properties | http://arxiv.org/pdf/1109.3745v3.pdf | author:Abhijit Sen, Dilip P. Ahalpara, Anantanarayanan Thyagaraja, Govind S. Krishnaswami category:nlin.PS cs.NE math.AP published:2011-09-17 summary:We discuss a new non-linear PDE, u_t + (2 u_xx/u) u_x = epsilon u_xxx,invariant under scaling of dependent variable and referred to here as SIdV. Itis one of the simplest such translation and space-time reflection-symmetricfirst order advection-dispersion equations. This PDE (with dispersioncoefficient unity) was discovered in a genetic programming search for equationssharing the KdV solitary wave solution. It provides a bridge between non-linearadvection, diffusion and dispersion. Special cases include the mKdV and lineardispersive equations. We identify two conservation laws, though initialinvestigations indicate that SIdV does not follow from a polynomial Lagrangianof the KdV sort. Nevertheless, it possesses solitary and periodic travellingwaves. Moreover, numerical simulations reveal recurrence properties usuallyassociated with integrable systems. KdV and SIdV are the simplest in aninfinite dimensional family of equations sharing the KdV solitary wave. SIdVand its generalizations may serve as a testing ground for numerical andanalytical techniques and be a rich source for further explorations.
arxiv-7500-235 | Capturing "attrition intensifying" structural traits from didactic interaction sequences of MOOC learners | http://arxiv.org/pdf/1409.5887v1.pdf | author:Tanmay Sinha, Nan Li, Patrick Jermann, Pierre Dillenbourg category:cs.CY cs.LG cs.SI published:2014-09-20 summary:This work is an attempt to discover hidden structural configurations inlearning activity sequences of students in Massive Open Online Courses (MOOCs).Leveraging combined representations of video clickstream interactions and forumactivities, we seek to fundamentally understand traits that are predictive ofdecreasing engagement over time. Grounded in the interdisciplinary field ofnetwork science, we follow a graph based approach to successfully extractindicators of active and passive MOOC participation that reflect persistenceand regularity in the overall interaction footprint. Using these richeducational semantics, we focus on the problem of predicting student attrition,one of the major highlights of MOOC literature in the recent years. Our resultsindicate an improvement over a baseline ngram based approach in capturing"attrition intensifying" features from the learning activities that MOOClearners engage in. Implications for some compelling future research arediscussed.
arxiv-7500-236 | R3MC: A Riemannian three-factor algorithm for low-rank matrix completion | http://arxiv.org/pdf/1306.2672v2.pdf | author:B. Mishra, R. Sepulchre category:math.OC cs.LG published:2013-06-11 summary:We exploit the versatile framework of Riemannian optimization on quotientmanifolds to develop R3MC, a nonlinear conjugate-gradient method for low-rankmatrix completion. The underlying search space of fixed-rank matrices isendowed with a novel Riemannian metric that is tailored to the least-squarescost. Numerical comparisons suggest that R3MC robustly outperformsstate-of-the-art algorithms across different problem instances, especiallythose that combine scarcely sampled and ill-conditioned data.
arxiv-7500-237 | Tight Error Bounds for Structured Prediction | http://arxiv.org/pdf/1409.5834v1.pdf | author:Amir Globerson, Tim Roughgarden, David Sontag, Cafer Yildirim category:cs.LG cs.DS stat.ML published:2014-09-19 summary:Structured prediction tasks in machine learning involve the simultaneousprediction of multiple labels. This is typically done by maximizing a scorefunction on the space of labels, which decomposes as a sum of pairwiseelements, each depending on two specific labels. Intuitively, the more pairwiseterms are used, the better the expected accuracy. However, there is currentlyno theoretical account of this intuition. This paper takes a significant stepin this direction. We formulate the problem as classifying the vertices of a known graph$G=(V,E)$, where the vertices and edges of the graph are labelled and correlatesemi-randomly with the ground truth. We show that the prospects for achievinglow expected Hamming error depend on the structure of the graph $G$ ininteresting ways. For example, if $G$ is a very poor expander, like a path,then large expected Hamming error is inevitable. Our main positive result showsthat, for a wide class of graphs including 2D grid graphs common in machinevision applications, there is a polynomial-time algorithm with small andinformation-theoretically near-optimal expected error. Our results provide afirst step toward a theoretical justification for the empirical success of theefficient approximate inference algorithms that are used for structuredprediction in models where exact inference is intractable.
arxiv-7500-238 | Hyperspectral and Multispectral Image Fusion based on a Sparse Representation | http://arxiv.org/pdf/1409.5729v1.pdf | author:Qi Wei, José Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV published:2014-09-19 summary:This paper presents a variational based approach to fusing hyperspectral andmultispectral images. The fusion process is formulated as an inverse problemwhose solution is the target image assumed to live in a much lower dimensionalsubspace. A sparse regularization term is carefully designed, relying on adecomposition of the scene on a set of dictionaries. The dictionary atoms andthe corresponding supports of active coding coefficients are learned from theobserved images. Then, conditionally on these dictionaries and supports, thefusion problem is solved via alternating optimization with respect to thetarget image (using the alternating direction method of multipliers) and thecoding coefficients. Simulation results demonstrate the efficiency of theproposed algorithm when compared with the state-of-the-art fusion methods.
arxiv-7500-239 | Statistical Estimation: From Denoising to Sparse Regression and Hidden Cliques | http://arxiv.org/pdf/1409.5557v1.pdf | author:Eric W. Tramel, Santhosh Kumar, Andrei Giurgiu, Andrea Montanari category:cs.IT math.IT stat.ML published:2014-09-19 summary:These notes review six lectures given by Prof. Andrea Montanari on the topicof statistical estimation for linear models. The first two lectures cover theprinciples of signal recovery from linear measurements in terms of minimaxrisk. Subsequent lectures demonstrate the application of these principles toseveral practical problems in science and engineering. Specifically, thesetopics include denoising of error-laden signals, recovery of compressivelysensed signals, reconstruction of low-rank matrices, and also the discovery ofhidden cliques within large networks.
arxiv-7500-240 | Effective Spectral Unmixing via Robust Representation and Learning-based Sparsity | http://arxiv.org/pdf/1409.0685v3.pdf | author:Feiyun Zhu, Ying Wang, Bin Fan, Gaofeng Meng, Chunhong Pan category:cs.CV published:2014-09-02 summary:Hyperspectral unmixing (HU) plays a fundamental role in a wide range ofhyperspectral applications. It is still challenging due to the common presenceof outlier channels and the large solution space. To address the above twoissues, we propose a novel model by emphasizing both robust representation andlearning-based sparsity. Specifically, we apply the $\ell_{2,1}$-norm tomeasure the representation error, preventing outlier channels from dominatingour objective. In this way, the side effects of outlier channels are greatlyrelieved. Besides, we observe that the mixed level of each pixel varies overimage grids. Based on this observation, we exploit a learning-based sparsitymethod to simultaneously learn the HU results and a sparse guidance map. Viathis guidance map, the sparsity constraint in the $\ell_{p}\!\left(\!0\!<\!p\!\leq\!1\right)$-norm is adaptively imposed according to the learnt mixedlevel of each pixel. Compared with state-of-the-art methods, our model isbetter suited to the real situation, thus expected to achieve better HUresults. The resulted objective is highly non-convex and non-smooth, and so itis hard to optimize. As a profound theoretical contribution, we propose anefficient algorithm to solve it. Meanwhile, the convergence proof and thecomputational complexity analysis are systematically provided. Extensiveevaluations verify that our method is highly promising for the HU task---itachieves very accurate guidance maps and much better HU results compared withstate-of-the-art methods.
arxiv-7500-241 | Learning Negative Mixture Models by Tensor Decompositions | http://arxiv.org/pdf/1403.4224v2.pdf | author:Guillaume Rabusseau, François Denis category:cs.LG published:2014-03-17 summary:This work considers the problem of estimating the parameters of negativemixture models, i.e. mixture models that possibly involve negative weights. Thecontributions of this paper are as follows. (i) We show that every rationalprobability distributions on strings, a representation which occurs naturallyin spectral learning, can be computed by a negative mixture of at most twoprobabilistic automata (or HMMs). (ii) We propose a method to estimate theparameters of negative mixture models having a specific tensor structure intheir low order observable moments. Building upon a recent paper on tensordecompositions for learning latent variable models, we extend this work to thebroader setting of tensors having a symmetric decomposition with positive andnegative weights. We introduce a generalization of the tensor power method forcomplex valued tensors, and establish theoretical convergence guarantees. (iii)We show how our approach applies to negative Gaussian mixture models, for whichwe provide some experiments.
arxiv-7500-242 | Using crowdsourcing system for creating site-specific statistical machine translation engine | http://arxiv.org/pdf/1409.5502v1.pdf | author:Alexander Kalinin, George Savchenko category:cs.CL published:2014-09-19 summary:A crowdsourcing translation approach is an effective tool for globalizationof site content, but it is also an important source of parallel linguisticdata. For the given site, processed with a crowdsourcing system, asentence-aligned corpus can be fetched, which covers a very narrow domain ofterminology and language patterns - a site-specific domain. These data can beused for training and estimation of site-specific statistical machinetranslation engine
arxiv-7500-243 | When Does a Mixture of Products Contain a Product of Mixtures? | http://arxiv.org/pdf/1206.0387v5.pdf | author:Guido F. Montufar, Jason Morton category:stat.ML math.CO G.3 published:2012-06-02 summary:We derive relations between theoretical properties of restricted Boltzmannmachines (RBMs), popular machine learning models which form the building blocksof deep learning models, and several natural notions from discrete mathematicsand convex geometry. We give implications and equivalences relatingRBM-representable probability distributions, perfectly reconstructible inputs,Hamming modes, zonotopes and zonosets, point configurations in hyperplanearrangements, linear threshold codes, and multi-covering numbers of hypercubes.As a motivating application, we prove results on the relative representationalpower of mixtures of product distributions and products of mixtures of pairs ofproduct distributions (RBMs) that formally justify widely held intuitions aboutdistributed representations. In particular, we show that a mixture of productsrequiring an exponentially larger number of parameters is needed to representthe probability distributions which can be obtained as products of mixtures.
arxiv-7500-244 | Particle Metropolis-Hastings using gradient and Hessian information | http://arxiv.org/pdf/1311.0686v4.pdf | author:Johan Dahlin, Fredrik Lindsten, Thomas B. Schön category:stat.CO stat.ML published:2013-11-04 summary:Particle Metropolis-Hastings (PMH) allows for Bayesian parameter inference innonlinear state space models by combining Markov chain Monte Carlo (MCMC) andparticle filtering. The latter is used to estimate the intractable likelihood.In its original formulation, PMH makes use of a marginal MCMC proposal for theparameters, typically a Gaussian random walk. However, this can lead to a poorexploration of the parameter space and an inefficient use of the generatedparticles. We propose a number of alternative versions of PMH that incorporate gradientand Hessian information about the posterior into the proposal. This informationis more or less obtained as a byproduct of the likelihood estimation. Indeed,we show how to estimate the required information using a fixed-lag particlesmoother, with a computational cost growing linearly in the number ofparticles. We conclude that the proposed methods can: (i) decrease the lengthof the burn-in phase, (ii) increase the mixing of the Markov chain at thestationary phase, and (iii) make the proposal distribution scale invariantwhich simplifies tuning.
arxiv-7500-245 | SAME but Different: Fast and High-Quality Gibbs Parameter Estimation | http://arxiv.org/pdf/1409.5402v1.pdf | author:Huasha Zhao, Biye Jiang, John Canny category:cs.LG stat.ML K.3.2; D.1.3 published:2014-09-18 summary:Gibbs sampling is a workhorse for Bayesian inference but has severallimitations when used for parameter estimation, and is often much slower thannon-sampling inference methods. SAME (State Augmentation for MarginalEstimation) \cite{Doucet99,Doucet02} is an approach to MAP parameter estimationwhich gives improved parameter estimates over direct Gibbs sampling. SAME canbe viewed as cooling the posterior parameter distribution and allows annealedsearch for the MAP parameters, often yielding very high quality (lower loss)estimates. But it does so at the expense of additional samples per iterationand generally slower performance. On the other hand, SAME dramaticallyincreases the parallelism in the sampling schedule, and is an excellent matchfor modern (SIMD) hardware. In this paper we explore the application of SAME tographical model inference on modern hardware. We show that combining SAME withfactored sample representation (or approximation) gives throughput competitivewith the fastest symbolic methods, but with potentially better quality. Wedescribe experiments on Latent Dirichlet Allocation, achieving speeds similarto the fastest reported methods (online Variational Bayes) and lowercross-validated loss than other LDA implementations. The method is simple toimplement and should be applicable to many other models.
arxiv-7500-246 | Visual Landmark Recognition from Internet Photo Collections: A Large-Scale Evaluation | http://arxiv.org/pdf/1409.5400v1.pdf | author:Tobias Weyand, Bastian Leibe category:cs.CV published:2014-09-18 summary:The task of a visual landmark recognition system is to identify photographedbuildings or objects in query photos and to provide the user with relevantinformation on them. With their increasing coverage of the world's landmarkbuildings and objects, Internet photo collections are now being used as asource for building such systems in a fully automatic fashion. This processtypically consists of three steps: clustering large amounts of images by theobjects they depict; determining object names from user-provided tags; andbuilding a robust, compact, and efficient recognition index. To this date,however, there is little empirical information on how well current approachesfor those steps perform in a large-scale open-set mining and recognition task.Furthermore, there is little empirical information on how recognitionperformance varies for different types of landmark objects and where there isstill potential for improvement. With this paper, we intend to fill these gaps.Using a dataset of 500k images from Paris, we analyze each component of thelandmark recognition pipeline in order to answer the following questions: Howmany and what kinds of objects can be discovered automatically? How can we bestuse the resulting image clusters to recognize the object in a query? How canthe object be efficiently represented in memory for recognition? How reliablycan semantic information be extracted? And finally: What are the limitingfactors in the resulting pipeline from query to semantics? We evaluate howdifferent choices of methods and parameters for the individual pipeline stepsaffect overall system performance and examine their effects for different querycategories such as buildings, paintings or sculptures.
arxiv-7500-247 | Fused Lasso Additive Model | http://arxiv.org/pdf/1409.5391v1.pdf | author:Ashley Petersen, Daniela Witten, Noah Simon category:stat.ME stat.ML published:2014-09-18 summary:We consider the problem of predicting an outcome variable using $p$covariates that are measured on $n$ independent observations, in the setting inwhich flexible and interpretable fits are desirable. We propose the fused lassoadditive model (FLAM), in which each additive function is estimated to bepiecewise constant with a small number of adaptively-chosen knots. FLAM is thesolution to a convex optimization problem, for which a simple algorithm withguaranteed convergence to the global optimum is provided. FLAM is shown to beconsistent in high dimensions, and an unbiased estimator of its degrees offreedom is proposed. We evaluate the performance of FLAM in a simulation studyand on two data sets.
arxiv-7500-248 | Learning and approximation capability of orthogonal super greedy algorithm | http://arxiv.org/pdf/1409.5330v1.pdf | author:Jian Fang, Shaobo Lin, Zongben Xu category:cs.LG F.2.2 published:2014-09-18 summary:We consider the approximation capability of orthogonal super greedyalgorithms (OSGA) and its applications in supervised learning. OSGA isconcerned with selecting more than one atoms in each iteration step, which, ofcourse, greatly reduces the computational burden when compared with theconventional orthogonal greedy algorithm (OGA). We prove that even for functionclasses that are not the convex hull of the dictionary, OSGA does not degradethe approximation capability of OGA provided the dictionary is incoherent.Based on this, we deduce a tight generalization error bound for OSGA learning.Our results show that in the realm of supervised learning, OSGA provides apossibility to further reduce the computational burden of OGA in the premise ofmaintaining its prominent generalization capability.
arxiv-7500-249 | Virtual Electrode Recording Tool for EXtracellular potentials (VERTEX): Comparing multi-electrode recordings from simulated and biological mammalian cortical tissue | http://arxiv.org/pdf/1409.5326v1.pdf | author:Richard J. Tomsett, Matt Ainsworth, Alexander Thiele, Mehdi Sanayei, Xing Chen, Alwin Gieselmann, Miles A. Whittington, Mark O. Cunningham, Marcus Kaiser category:q-bio.NC cs.AI cs.NE published:2014-09-18 summary:Local field potentials (LFPs) sampled with extracellular electrodes arefrequently used as a measure of population neuronal activity. However, relatingsuch measurements to underlying neuronal behaviour and connectivity isnon-trivial. To help study this link, we developed the Virtual ElectrodeRecording Tool for EXtracellular potentials (VERTEX). We first identified areduced neuron model that retained the spatial and frequency filteringcharacteristics of extracellular potentials from neocortical neurons. We thendeveloped VERTEX as an easy-to-use Matlab tool for simulating LFPs from largepopulations (>100 000 neurons). A VERTEX-based simulation successfullyreproduced features of the LFPs from an in vitro multi-electrode arrayrecording of macaque neocortical tissue. Our model, with virtual electrodesplaced anywhere in 3D, allows direct comparisons with the in vitro recordingsetup. We envisage that VERTEX will stimulate experimentalists, clinicians, andcomputational neuroscientists to use models to understand the mechanismsunderlying measured brain dynamics in health and disease.
arxiv-7500-250 | Feature Engineering for Knowledge Base Construction | http://arxiv.org/pdf/1407.6439v3.pdf | author:Christopher Ré, Amir Abbas Sadeghian, Zifei Shan, Jaeho Shin, Feiran Wang, Sen Wu, Ce Zhang category:cs.DB cs.CL cs.LG published:2014-07-24 summary:Knowledge base construction (KBC) is the process of populating a knowledgebase, i.e., a relational database together with inference rules, withinformation extracted from documents and structured sources. KBC blurs thedistinction between two traditional database problems, information extractionand information integration. For the last several years, our group has beenbuilding knowledge bases with scientific collaborators. Using our approach, wehave built knowledge bases that have comparable and sometimes better qualitythan those constructed by human volunteers. In contrast to these knowledgebases, which took experts a decade or more human years to construct, many ofour projects are constructed by a single graduate student. Our approach to KBC is based on joint probabilistic inference and learning,but we do not see inference as either a panacea or a magic bullet: inference isa tool that allows us to be systematic in how we construct, debug, and improvethe quality of such systems. In addition, inference allows us to constructthese systems in a more loosely coupled way than traditional approaches. Tosupport this idea, we have built the DeepDive system, which has the design goalof letting the user "think about features---not algorithms." We think ofDeepDive as declarative in that one specifies what they want but not how to getit. We describe our approach with a focus on feature engineering, which weargue is an understudied problem relative to its importance to end-to-endquality.
arxiv-7500-251 | How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation | http://arxiv.org/pdf/1407.7906v3.pdf | author:Yoshua Bengio category:cs.LG published:2014-07-29 summary:We propose to exploit {\em reconstruction} as a layer-local training signalfor deep learning. Reconstructions can be propagated in a form of targetpropagation playing a role similar to back-propagation but helping to reducethe reliance on derivatives in order to perform credit assignment across manylevels of possibly strong non-linearities (which is difficult forback-propagation). A regularized auto-encoder tends produce a reconstructionthat is a more likely version of its input, i.e., a small move in the directionof higher likelihood. By generalizing gradients, target propagation may alsoallow to train deep networks with discrete hidden units. If the auto-encodertakes both a representation of input and target (or of any side information) ininput, then its reconstruction of input representation provides a targettowards a representation that is more likely, conditioned on all the sideinformation. A deep auto-encoder decoding path generalizes gradient propagationin a learned way that can could thus handle not just infinitesimal changes butlarger, discrete changes, hopefully allowing credit assignment through a longchain of non-linear operations. In addition to each layer being a goodauto-encoder, the encoder also learns to please the upper layers bytransforming the data into a space where it is easier to model by them,flattening manifolds and disentangling factors. The motivations and theoreticaljustifications for this approach are laid down in this paper, along withconjectures that will have to be verified either mathematically orexperimentally, including a hypothesis stating that such auto-encoder mediatedtarget propagation could play in brains the role of credit assignment throughmany non-linear, noisy and discrete transformations.
arxiv-7500-252 | Deep Regression for Face Alignment | http://arxiv.org/pdf/1409.5230v1.pdf | author:Baoguang Shi, Xiang Bai, Wenyu Liu, Jingdong Wang category:cs.CV published:2014-09-18 summary:In this paper, we present a deep regression approach for face alignment. Thedeep architecture consists of a global layer and multi-stage local layers. Weapply the back-propagation algorithm with the dropout strategy to jointlyoptimize the regression parameters. We show that the resulting deep regressorgradually and evenly approaches the true facial landmarks stage by stage,avoiding the tendency to yield over-strong early stage regressors whileover-weak later stage regressors. Experimental results show that our approachachieves the state-of-the-art
arxiv-7500-253 | Fingerprint Classification Based on Depth Neural Network | http://arxiv.org/pdf/1409.5188v1.pdf | author:Ruxin Wang, Congying Han, Yanping Wu, Tiande Guo category:cs.CV published:2014-09-18 summary:Fingerprint classification is an effective technique for reducing thecandidate numbers of fingerprints in the stage of matching in automaticfingerprint identification system (AFIS). In recent years, deep learning is anemerging technology which has achieved great success in many fields, such asimage processing, natural language processing and so on. In this paper, we onlychoose the orientation field as the input feature and adopt a new method(stacked sparse autoencoders) based on depth neural network for fingerprintclassification. For the four-class problem, we achieve a classification of 93.1percent using the depth network structure which has three hidden layers (with1.8% rejection) in the NIST-DB4 database. And then we propose a novel methodusing two classification probabilities for fuzzy classification which caneffectively enhance the accuracy of classification. By only adjusting theprobability threshold, we get the accuracy of classification is 96.1% (settingthreshold is 0.85), 97.2% (setting threshold is 0.90) and 98.0% (settingthreshold is 0.95). Using the fuzzy method, we obtain higher accuracy thanother methods.
arxiv-7500-254 | Model-based Kernel Sum Rule | http://arxiv.org/pdf/1409.5178v1.pdf | author:Yu Nishiyama, Motonobu Kanagawa, Arthur Gretton, Kenji Fukumizu category:stat.ML stat.ME published:2014-09-18 summary:In this study, we enrich the framework of nonparametric kernel Bayesianinference via the flexible incorporation of certain probabilistic models, suchas additive Gaussian noise models. Nonparametric inference expressed in termsof kernel means, which is called kernel Bayesian inference, has been studiedusing basic rules such as the kernel sum rule (KSR), kernel chain rule, kernelproduct rule, and kernel Bayes' rule (KBR). However, the current framework usedfor kernel Bayesian inference deals only with nonparametric inference and itcannot allow inference when combined with probabilistic models. In this study,we introduce a novel KSR, called model-based KSR (Mb-KSR), which exploits theknowledge obtained from some probabilistic models of conditional distributions.The incorporation of Mb-KSR into nonparametric kernel Bayesian inferencefacilitates more flexible kernel Bayesian inference than nonparametricinference. We focus on combinations of Mb-KSR, Non-KSR, and KBR, and we proposea filtering algorithm for state space models, which combines nonparametriclearning of the observation process using kernel means and additive Gaussiannoise models of the transition dynamics. The idea of the Mb-KSR for additiveGaussian noise models can be extended to more general noise model cases,including a conjugate pair with a positive-definite kernel and a probabilisticmodel.
arxiv-7500-255 | A Method for Stopping Active Learning Based on Stabilizing Predictions and the Need for User-Adjustable Stopping | http://arxiv.org/pdf/1409.5165v1.pdf | author:Michael Bloodgood, K. Vijay-Shanker category:cs.LG cs.CL stat.ML published:2014-09-17 summary:A survey of existing methods for stopping active learning (AL) reveals theneeds for methods that are: more widely applicable; more aggressive in savingannotations; and more stable across changing datasets. A new method forstopping AL based on stabilizing predictions is presented that addresses theseneeds. Furthermore, stopping methods are required to handle a broad range ofdifferent annotation/performance tradeoff valuations. Despite this, theexisting body of work is dominated by conservative methods with little (if any)attention paid to providing users with control over the behavior of stoppingmethods. The proposed method is shown to fill a gap in the level ofaggressiveness available for stopping AL and supports providing users withcontrol over stopping behavior.
arxiv-7500-256 | Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation | http://arxiv.org/pdf/1406.2984v2.pdf | author:Jonathan Tompson, Arjun Jain, Yann LeCun, Christoph Bregler category:cs.CV published:2014-06-11 summary:This paper proposes a new hybrid architecture that consists of a deepConvolutional Network and a Markov Random Field. We show how this architectureis successfully applied to the challenging problem of articulated human poseestimation in monocular images. The architecture can exploit structural domainconstraints such as geometric relationships between body joint locations. Weshow that joint training of these two model paradigms improves performance andallows us to significantly outperform existing state-of-the-art techniques.
arxiv-7500-257 | Visual Words for Automatic Lip-Reading | http://arxiv.org/pdf/1409.6689v1.pdf | author:Ahmad Basheer Hassanat category:cs.CV published:2014-09-17 summary:Lip reading is used to understand or interpret speech without hearing it, atechnique especially mastered by people with hearing difficulties. The abilityto lip read enables a person with a hearing impairment to communicate withothers and to engage in social activities, which otherwise would be difficult.Recent advances in the fields of computer vision, pattern recognition, andsignal processing has led to a growing interest in automating this challengingtask of lip reading. Indeed, automating the human ability to lip read, aprocess referred to as visual speech recognition, could open the door for othernovel applications. This thesis investigates various issues faced by anautomated lip-reading system and proposes a novel "visual words" based approachto automatic lip reading. The proposed approach includes a novel automatic facelocalisation scheme and a lip localisation method.
arxiv-7500-258 | Down-Sampling coupled to Elastic Kernel Machines for Efficient Recognition of Isolated Gestures | http://arxiv.org/pdf/1408.3944v2.pdf | author:Pierre-François Marteau, Sylvie Gibet, Clement Reverdy category:cs.LG cs.HC published:2014-08-18 summary:In the field of gestural action recognition, many studies have focused ondimensionality reduction along the spatial axis, to reduce both the variabilityof gestural sequences expressed in the reduced space, and the computationalcomplexity of their processing. It is noticeable that very few of these methodshave explicitly addressed the dimensionality reduction along the time axis.This is however a major issue with regard to the use of elastic distancescharacterized by a quadratic complexity. To partially fill this apparent gap,we present in this paper an approach based on temporal down-sampling associatedto elastic kernel machine learning. We experimentally show, on two data setsthat are widely referenced in the domain of human gesture recognition, and verydifferent in terms of quality of motion capture, that it is possible tosignificantly reduce the number of skeleton frames while maintaining a goodrecognition rate. The method proves to give satisfactory results at a levelcurrently reached by state-of-the-art methods on these data sets. Thecomputational complexity reduction makes this approach eligible for real-timeapplications.
arxiv-7500-259 | Distance Shrinkage and Euclidean Embedding via Regularized Kernel Estimation | http://arxiv.org/pdf/1409.5009v1.pdf | author:Luwan Zhang, Grace Wahba, Ming Yuan category:stat.ML math.ST stat.ME stat.TH published:2014-09-17 summary:Although recovering an Euclidean distance matrix from noisy observations is acommon problem in practice, how well this could be done remains largelyunknown. To fill in this void, we study a simple distance matrix estimate basedupon the so-called regularized kernel estimate. We show that such an estimatecan be characterized as simply applying a constant amount of shrinkage to allobserved pairwise distances. This fact allows us to establish risk bounds forthe estimate implying that the true distances can be estimated consistently inan average sense as the number of objects increases. In addition, such acharacterization suggests an efficient algorithm to compute the distance matrixestimator, as an alternative to the usual second order cone programming knownnot to scale well for large problems. Numerical experiments and an applicationin visualizing the diversity of Vpu protein sequences from a recent HIV-1 studyfurther demonstrate the practical merits of the proposed method.
arxiv-7500-260 | An Agent-Based Algorithm exploiting Multiple Local Dissimilarities for Clusters Mining and Knowledge Discovery | http://arxiv.org/pdf/1409.4988v1.pdf | author:Filippo Maria Bianchi, Enrico Maiorino, Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.LG cs.DC cs.MA published:2014-09-17 summary:We propose a multi-agent algorithm able to automatically discover relevantregularities in a given dataset, determining at the same time the set ofconfigurations of the adopted parametric dissimilarity measure yielding compactand separated clusters. Each agent operates independently by performing aMarkovian random walk on a suitable weighted graph representation of the inputdataset. Such a weighted graph representation is induced by the specificparameter configuration of the dissimilarity measure adopted by the agent,which searches and takes decisions autonomously for one cluster at a time.Results show that the algorithm is able to discover parameter configurationsthat yield a consistent and interpretable collection of clusters. Moreover, wedemonstrate that our algorithm shows comparable performances with other similarstate-of-the-art algorithms when facing specific clustering problems.
arxiv-7500-261 | Adaptive Tag Selection for Image Annotation | http://arxiv.org/pdf/1409.4995v1.pdf | author:Xixi He, Xirong Li, Gang Yang, Jieping Xu, Qin Jin category:cs.CV published:2014-09-17 summary:Not all tags are relevant to an image, and the number of relevant tags isimage-dependent. Although many methods have been proposed for imageauto-annotation, the question of how to determine the number of tags to beselected per image remains open. The main challenge is that for a large tagvocabulary, there is often a lack of ground truth data for acquiring optimalcutoff thresholds per tag. In contrast to previous works that pre-specify thenumber of tags to be selected, we propose in this paper adaptive tag selection.The key insight is to divide the vocabulary into two disjoint subsets, namely aseen set consisting of tags having ground truth available for optimizing theirthresholds and a novel set consisting of tags without any ground truth. Such adivision allows us to estimate how many tags shall be selected from the novelset according to the tags that have been selected from the seen set. Theeffectiveness of the proposed method is justified by our participation in theImageCLEF 2014 image annotation task. On a set of 2,065 test images with groundtruth available for 207 tags, the benchmark evaluation shows that compared tothe popular top-$k$ strategy which obtains an F-score of 0.122, adaptive tagselection achieves a higher F-score of 0.223. Moreover, by treating theunderlying image annotation system as a black box, the new method can be usedas an easy plug-in to boost the performance of existing systems.
arxiv-7500-262 | Reinforcement Learning Based Algorithm for the Maximization of EV Charging Station Revenue | http://arxiv.org/pdf/1407.1291v2.pdf | author:Stoyan Dimitrov, Redouane Lguensat category:cs.CE cs.LG math.OC stat.AP published:2014-07-04 summary:This paper presents an online reinforcement learning based application whichincreases the revenue of one particular electric vehicles (EV) station,connected to a renewable source of energy. Moreover, the proposed applicationadapts to changes in the trends of the station's average number of customersand their types. Most of the parameters in the model are simulatedstochastically and the algorithm used is a Q-learning algorithm. A computersimulation was implemented which demonstrates and confirms the utility of themodel.
arxiv-7500-263 | Tensity Research Based on the Information of Eye Movement | http://arxiv.org/pdf/1409.4958v1.pdf | author:Yi Wang category:cs.RO cs.CV published:2014-09-17 summary:User's mental state is concerned gradually, during the interaction course ofhuman robot. As the measurement and identification method of psychologicalstate, tension, has certain practical significance role. At presents there isno suitable method of measuring the tension. Firstly, sum up some availabilityof eye movement index. And then parameters extraction on eye movementcharacteristics of normal illumination is studied, including the location ofthe face, eyes location, access to the pupil diameter, the eye pupil centercharacteristic parameters. And with the judgment of the tension in eye images,extract exact information of gaze direction. Finally, through the experiment toprove the proposed method is effective.
arxiv-7500-264 | Scalable Verification of Markov Decision Processes | http://arxiv.org/pdf/1310.3609v4.pdf | author:Axel Legay, Sean Sedwards, Louis-Marie Traonouez category:cs.DS cs.DC cs.LG cs.LO published:2013-10-14 summary:Markov decision processes (MDP) are useful to model concurrent processoptimisation problems, but verifying them with numerical methods is oftenintractable. Existing approximative approaches do not scale well and arelimited to memoryless schedulers. Here we present the basis of scalableverification for MDPSs, using an O(1) memory representation ofhistory-dependent schedulers. We thus facilitate scalable learning techniquesand the use of massively parallel verification.
arxiv-7500-265 | Ensembles of Random Sphere Cover Classifiers | http://arxiv.org/pdf/1409.4936v1.pdf | author:Anthony Bagnall, Reda Younsi category:cs.LG cs.AI stat.ML published:2014-09-17 summary:We propose and evaluate alternative ensemble schemes for a new instance basedlearning classifier, the Randomised Sphere Cover (RSC) classifier. RSC fusesinstances into spheres, then bases classification on distance to spheres ratherthan distance to instances. The randomised nature of RSC makes it ideal for usein ensembles. We propose two ensemble methods tailored to the RSC classifier;$\alpha \beta$RSE, an ensemble based on instance resampling and $\alpha$RSSE, asubspace ensemble. We compare $\alpha \beta$RSE and $\alpha$RSSE to tree basedensembles on a set of UCI datasets and demonstrates that RSC ensembles performsignificantly better than some of these ensembles, and not significantly worsethan the others. We demonstrate via a case study on six gene expression datasets that $\alpha$RSSE can outperform other subspace ensemble methods on highdimensional data when used in conjunction with an attribute filter. Finally, weperform a set of Bias/Variance decomposition experiments to analyse the sourceof improvement in comparison to a base classifier.
arxiv-7500-266 | Statistical inference with probabilistic graphical models | http://arxiv.org/pdf/1409.4928v1.pdf | author:Angélique Drémeau, Christophe Schülke, Yingying Xu, Devavrat Shah category:cs.LG stat.ML published:2014-09-17 summary:These are notes from the lecture of Devavrat Shah given at the autumn school"Statistical Physics, Optimization, Inference, and Message-Passing Algorithms",that took place in Les Houches, France from Monday September 30th, 2013, tillFriday October 11th, 2013. The school was organized by Florent Krzakala fromUPMC & ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, LenkaZdeborova from CEA Saclay & CNRS, and Riccardo Zecchina from PolitecnicoTorino. This lecture of Devavrat Shah (MIT) covers the basics of inference andlearning. It explains how inference problems are represented within structuresknown as graphical models. The theoretical basis of the belief propagationalgorithm is then explained and derived. This lecture sets the stage forgeneralizations and applications of message passing algorithms.
arxiv-7500-267 | Efficiently Detecting Overlapping Communities through Seeding and Semi-Supervised Learning | http://arxiv.org/pdf/1401.5888v4.pdf | author:Changxing Shang, Shengzhong Feng, Zhongying Zhao, Jianping Fan category:cs.SI cs.LG physics.soc-ph published:2014-01-23 summary:Seeding then expanding is a commonly used scheme to discover overlappingcommunities in a network. Most seeding methods are either too complex to scaleto large networks or too simple to select high-quality seeds, and thenon-principled functions used by most expanding methods lead to poorperformance when applied to diverse networks. This paper proposes a new methodthat transforms a network into a corpus where each edge is treated as adocument, and all nodes of the network are treated as terms of the corpus. Aneffective seeding method is also proposed that selects seeds as a training set,then a principled expanding method based on semi-supervised learning is appliedto classify edges. We compare our new algorithm with four other communitydetection algorithms on a wide range of synthetic and empirical networks.Experimental results show that the new algorithm can significantly improveclustering performance in most cases. Furthermore, the time complexity of thenew algorithm is linear to the number of edges, and this low complexity makesthe new algorithm scalable to large networks.
arxiv-7500-268 | Face Identification with Second-Order Pooling | http://arxiv.org/pdf/1406.6818v2.pdf | author:Fumin Shen, Chunhua Shen, Heng Tao Shen category:cs.CV published:2014-06-26 summary:Automatic face recognition has received significant performance improvementby developing specialised facial image representations. On the other hand,generic object recognition has rarely been applied to the face recognition.Spatial pyramid pooling of features encoded by an over-complete dictionary hasbeen the key component of many state-of-the-art image classification systems.Inspired by its success, in this work we develop a new face imagerepresentation method inspired by the second-order pooling in Carreira et al.[1], which was originally proposed for image segmentation. The proposed methoddiffers from the previous methods in that, we encode the densely extractedlocal patches by a small-size dictionary; and the facial image signatures areobtained by pooling the second-order statistics of the encoded features. Weshow the importance of pooling on encoded features, which is bypassed by theoriginal second-order pooling method to avoid the high computational cost.Equipped with a simple linear classifier, the proposed method outperforms thestate-of-the-art face identification performance by large margins. For example,on the LFW databases, the proposed method performs better than the previousbest by around 13% accuracy.
arxiv-7500-269 | Face Image Classification by Pooling Raw Features | http://arxiv.org/pdf/1406.6811v2.pdf | author:Fumin Shen, Chunhua Shen, Heng Tao Shen category:cs.CV published:2014-06-26 summary:We propose a very simple, efficient yet surprisingly effective featureextraction method for face recognition (about 20 lines of Matlab code), whichis mainly inspired by spatial pyramid pooling in generic image classification.We show that features formed by simply pooling local patches over a multi-levelpyramid, coupled with a linear classifier, can significantly outperform mostrecent face recognition methods. The simplicity of our feature extractionprocedure is demonstrated by the fact that no learning is involved (except PCAwhitening). We show that, multi-level spatial pooling and dense extraction ofmulti-scale patches play critical roles in face image classification. Theextracted facial features can capture strong structural information ofindividual faces with no label information being used. We also find that,pre-processing on local image patches such as contrast normalization can havean important impact on the classification accuracy. In particular, on thechallenging face recognition datasets of FERET and LFW-a, our method improvesprevious best results by more than 10% and 20%, respectively.
arxiv-7500-270 | Continuum limit of total variation on point clouds | http://arxiv.org/pdf/1403.6355v3.pdf | author:Nicolás García Trillos, Dejan Slepčev category:math.ST math.AP stat.ML stat.TH published:2014-03-25 summary:We consider point clouds obtained as random samples of a measure on aEuclidean domain. A graph representing the point cloud is obtained by assigningweights to edges based on the distance between the points they connect. Ourgoal is to develop mathematical tools needed to study the consistency, as thenumber of available data points increases, of graph-based machine learningalgorithms for tasks such as clustering. In particular, we study when is thecut capacity, and more generally total variation, on these graphs a goodapproximation of the perimeter (total variation) in the continuum setting. Weaddress this question in the setting of $\Gamma$-convergence. We obtain almostoptimal conditions on the scaling, as number of points increases, of the sizeof the neighborhood over which the points are connected by an edge for the$\Gamma$-convergence to hold. Taking the limit is enabled by a transportationbased metric which allows to suitably compare functionals defined on differentpoint clouds.
arxiv-7500-271 | Going Deeper with Convolutions | http://arxiv.org/pdf/1409.4842v1.pdf | author:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich category:cs.CV published:2014-09-17 summary:We propose a deep convolutional neural network architecture codenamed"Inception", which was responsible for setting the new state of the art forclassification and detection in the ImageNet Large-Scale Visual RecognitionChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is theimproved utilization of the computing resources inside the network. This wasachieved by a carefully crafted design that allows for increasing the depth andwidth of the network while keeping the computational budget constant. Tooptimize quality, the architectural decisions were based on the Hebbianprinciple and the intuition of multi-scale processing. One particularincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22layers deep network, the quality of which is assessed in the context ofclassification and detection.
arxiv-7500-272 | Taking into Account the Differences between Actively and Passively Acquired Data: The Case of Active Learning with Support Vector Machines for Imbalanced Datasets | http://arxiv.org/pdf/1409.4835v1.pdf | author:Michael Bloodgood, K. Vijay-Shanker category:cs.LG cs.CL stat.ML published:2014-09-17 summary:Actively sampled data can have very different characteristics than passivelysampled data. Therefore, it's promising to investigate using differentinference procedures during AL than are used during passive learning (PL). Thisgeneral idea is explored in detail for the focused case of AL withcost-weighted SVMs for imbalanced data, a situation that arises for many HLTtasks. The key idea behind the proposed InitPA method for addressing imbalanceis to base cost models during AL on an estimate of overall corpus imbalancecomputed via a small unbiased sample rather than the imbalance in the labeledtraining data, which is the leading method used during PL.
arxiv-7500-273 | Your click decides your fate: Inferring Information Processing and Attrition Behavior from MOOC Video Clickstream Interactions | http://arxiv.org/pdf/1407.7131v2.pdf | author:Tanmay Sinha, Patrick Jermann, Nan Li, Pierre Dillenbourg category:cs.HC cs.LG published:2014-07-26 summary:In this work, we explore video lecture interaction in Massive Open OnlineCourses (MOOCs), which is central to student learning experience on theseeducational platforms. As a research contribution, we operationalize videolecture clickstreams of students into cognitively plausible higher levelbehaviors, and construct a quantitative information processing index, which canaid instructors to better understand MOOC hurdles and reason aboutunsatisfactory learning outcomes. Our results illustrate how such a metricinspired by cognitive psychology can help answer critical questions regardingstudents' engagement, their future click interactions and participationtrajectories that lead to in-video & course dropouts. Implications for researchand practice are discussed
arxiv-7500-274 | Anomaly Detection Based on Indicators Aggregation | http://arxiv.org/pdf/1409.4747v1.pdf | author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG published:2014-09-16 summary:Automatic anomaly detection is a major issue in various areas. Beyond meredetection, the identification of the source of the problem that produced theanomaly is also essential. This is particularly the case in aircraft enginehealth monitoring where detecting early signs of failure (anomalies) andhelping the engine owner to implement efficiently the adapted maintenanceoperations (fixing the source of the anomaly) are of crucial importance toreduce the costs attached to unscheduled maintenance. This paper introduces ageneral methodology that aims at classifying monitoring signals into normalones and several classes of abnormal ones. The main idea is to leverage expertknowledge by generating a very large number of binary indicators. Eachindicator corresponds to a fully parametrized anomaly detector built fromparametric anomaly scores designed by experts. A feature selection method isused to keep only the most discriminant indicators which are used at inputs ofa Naive Bayes classifier. This give an interpretable classifier based oninterpretable anomaly detectors whose parameters have been optimized indirectlyby the selection process. The proposed methodology is evaluated on simulateddata designed to reproduce some of the anomaly types observed in real worldengines.
arxiv-7500-275 | Anomaly Detection Based on Aggregation of Indicators | http://arxiv.org/pdf/1407.0880v2.pdf | author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG published:2014-07-03 summary:Automatic anomaly detection is a major issue in various areas. Beyond meredetection, the identification of the origin of the problem that produced theanomaly is also essential. This paper introduces a general methodology that canassist human operators who aim at classifying monitoring signals. The main ideais to leverage expert knowledge by generating a very large number ofindicators. A feature selection method is used to keep only the mostdiscriminant indicators which are used as inputs of a Naive Bayes classifier.The parameters of the classifier have been optimized indirectly by theselection process. Simulated data designed to reproduce some of the anomalytypes observed in real world engines.
arxiv-7500-276 | Supervised Classification Using Sparse Fisher's LDA | http://arxiv.org/pdf/1301.4976v2.pdf | author:Irina Gaynanova, James G. Booth, Martin T. Wells category:stat.ML stat.CO published:2013-01-21 summary:It is well known that in a supervised classification setting when the numberof features is smaller than the number of observations, Fisher's lineardiscriminant rule is asymptotically Bayes. However, there are numerous modernapplications where classification is needed in the high-dimensional setting.Naive implementation of Fisher's rule in this case fails to provide goodresults because the sample covariance matrix is singular. Moreover, byconstructing a classifier that relies on all features the interpretation of theresults is challenging. Our goal is to provide robust classification thatrelies only on a small subset of important features and accounts for theunderlying correlation structure. We apply a lasso-type penalty to thediscriminant vector to ensure sparsity of the solution and use a shrinkage typeestimator for the covariance matrix. The resulting optimization problem issolved using an iterative coordinate ascent algorithm. Furthermore, we analyzethe effect of nonconvexity on the sparsity level of the solution and highlightthe difference between the penalized and the constrained versions of theproblem. The simulation results show that the proposed method performsfavorably in comparison to alternatives. The method is used to classifyleukemia patients based on DNA methylation features.
arxiv-7500-277 | A Mixtures-of-Experts Framework for Multi-Label Classification | http://arxiv.org/pdf/1409.4698v1.pdf | author:Charmgil Hong, Iyad Batal, Milos Hauskrecht category:cs.LG I.2.6 published:2014-09-16 summary:We develop a novel probabilistic approach for multi-label classification thatis based on the mixtures-of-experts architecture combined with recentlyintroduced conditional tree-structured Bayesian networks. Our approach capturesdifferent input-output relations from multi-label data using the efficienttree-structured classifiers, while the mixtures-of-experts architecture aims tocompensate for the tree-structured restrictions and build a more accuratemodel. We develop and present algorithms for learning the model from data andfor performing multi-label predictions on future data instances. Experiments onmultiple benchmark datasets demonstrate that our approach achieves highlycompetitive results and outperforms the existing state-of-the-art multi-labelclassification methods.
arxiv-7500-278 | DISA at ImageCLEF 2014 Revised: Search-based Image Annotation with DeCAF Features | http://arxiv.org/pdf/1409.4627v1.pdf | author:Petra Budikova, Jan Botorek, Michal Batko, Pavel Zezula category:cs.IR cs.CV published:2014-09-16 summary:This paper constitutes an extension to the report on DISA-MU teamparticipation in the ImageCLEF 2014 Scalable Concept Image Annotation Task aspublished in [3]. Specifically, we introduce a new similarity search componentthat was implemented into the system, report on the results achieved byutilizing this component, and analyze the influence of different similaritysearch parameters on the annotation quality.
arxiv-7500-279 | Predictive Capacity of Meteorological Data - Will it rain tomorrow | http://arxiv.org/pdf/1409.5079v1.pdf | author:Bilal Ahmed category:cs.LG published:2014-09-16 summary:With the availability of high precision digital sensors and cheap storagemedium, it is not uncommon to find large amounts of data collected on almostall measurable attributes, both in nature and man-made habitats. Weather inparticular has been an area of keen interest for researchers to develop moreaccurate and reliable prediction models. This paper presents a set ofexperiments which involve the use of prevalent machine learning techniques tobuild models to predict the day of the week given the weather data for thatparticular day i.e. temperature, wind, rain etc., and test their reliabilityacross four cities in Australia {Brisbane, Adelaide, Perth, Hobart}. Theresults provide a comparison of accuracy of these machine learning techniquesand their reliability to predict the day of the week by analysing the weatherdata. We then apply the models to predict weather conditions based on theavailable data.
arxiv-7500-280 | The Role of Emotions in Propagating Brands in Social Networks | http://arxiv.org/pdf/1409.4617v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:cs.SI cs.CL stat.ML published:2014-09-16 summary:A key aspect of word of mouth marketing are emotions. Emotions in texts helppropagating messages in conventional advertising. In word of mouth scenarios,emotions help to engage consumers and incite to propagate the message further.While the function of emotions in offline marketing in general and word ofmouth marketing in particular is rather well understood, online marketing canonly offer a limited view on the function of emotions. In this contribution weseek to close this gap. We therefore investigate how emotions function insocial media. To do so, we collected more than 30,000 brand marketing messagesfrom the Google+ social networking site. Using state of the art computationallinguistics classifiers, we compute the sentiment of these messages. Startingout with Poisson regression-based baseline models, we seek to replicate earlierfindings using this large data set. We extend upon earlier research bycomputing multi-level mixed effects models that compare the function ofemotions across different industries. We find that while the well known notionof activating emotions propagating messages holds in general for our data aswell. But there are significant differences between the observed industries.
arxiv-7500-281 | Multivariate Comparison of Classification Algorithms | http://arxiv.org/pdf/1409.4566v1.pdf | author:Olcay Taner Yildiz, Ethem Alpaydin category:stat.ML cs.LG published:2014-09-16 summary:Statistical tests that compare classification algorithms are univariate anduse a single performance measure, e.g., misclassification error, $F$ measure,AUC, and so on. In multivariate tests, comparison is done using multiplemeasures simultaneously. For example, error is the sum of false positives andfalse negatives and a univariate test on error cannot make a distinctionbetween these two sources, but a 2-variate test can. Similarly, instead ofcombining precision and recall in $F$ measure, we can have a 2-variate test on(precision, recall). We use Hotelling's multivariate $T^2$ test for comparingtwo algorithms, and when we have three or more algorithms we use themultivariate analysis of variance (MANOVA) followed by pairwise post hoc tests.In our experiments, we see that multivariate tests have higher power thanunivariate tests, that is, they can detect differences that univariate testscannot. We also discuss how multivariate analysis allows us to automaticallyextract performance measures that best distinguish the behavior of multiplealgorithms.
arxiv-7500-282 | Improving files availability for BitTorrent using a diffusion model | http://arxiv.org/pdf/1409.4565v1.pdf | author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana category:cs.NI cs.NE published:2014-09-16 summary:The BitTorrent mechanism effectively spreads file fragments by copying therarest fragments first. We propose to apply a mathematical model for thediffusion of fragments on a P2P in order to take into account both the effectsof peer distances and the changing availability of peers while time goes on.Moreover, we manage to provide a forecast on the availability of a torrentthanks to a neural network that models the behaviour of peers on the P2Psystem. The combination of the mathematical model and the neural networkprovides a solution for choosing file fragments that need to be copied first,in order to ensure their continuous availability, counteracting possibledisconnections by some peers.
arxiv-7500-283 | A Combined Method Of Fractal And GLCM Features For MRI And CT Scan Images Classification | http://arxiv.org/pdf/1409.4559v1.pdf | author:Redouan Korchiyne, Sidi Mohamed Farssi, Abderrahmane Sbihi, Rajaa Touahni, Mustapha Tahiri Alaoui category:cs.CV published:2014-09-16 summary:Fractal analysis has been shown to be useful in image processing forcharacterizing shape and gray-scale complexity. The fractal feature is acompact descriptor used to give a numerical measure of the degree ofirregularity of the medical images. This descriptor property does not giveownership of the local image structure. In this paper, we present a combinationof this parameter based on Box Counting with GLCM Features. This powerfulcombination has proved good results especially in classification of medicaltexture from MRI and CT Scan images of trabecular bone. This method has thepotential to improve clinical diagnostics tests for osteoporosis pathologies.
arxiv-7500-284 | Semidefinite Programming Based Preconditioning for More Robust Near-Separable Nonnegative Matrix Factorization | http://arxiv.org/pdf/1310.2273v2.pdf | author:Nicolas Gillis, Stephen A. Vavasis category:stat.ML cs.LG math.OC published:2013-10-08 summary:Nonnegative matrix factorization (NMF) under the separability assumption canprovably be solved efficiently, even in the presence of noise, and has beenshown to be a powerful technique in document classification and hyperspectralunmixing. This problem is referred to as near-separable NMF and requires thatthere exists a cone spanned by a small subset of the columns of the inputnonnegative matrix approximately containing all columns. In this paper, wepropose a preconditioning based on semidefinite programming making the inputmatrix well-conditioned. This in turn can improve significantly the performanceof near-separable NMF algorithms which is illustrated on the popular successiveprojection algorithm (SPA). The new preconditioned SPA is provably more robustto noise, and outperforms SPA on several synthetic data sets. We also show howan active-set method allow us to apply the preconditioning on large-scalereal-world hyperspectral images.
arxiv-7500-285 | Collapsed Variational Bayes Inference of Infinite Relational Model | http://arxiv.org/pdf/1409.4757v1.pdf | author:Katsuhiko Ishiguro, Issei Sato, Naonori Ueda category:cs.LG stat.ML published:2014-09-16 summary:The Infinite Relational Model (IRM) is a probabilistic model for relationaldata clustering that partitions objects into clusters based on observedrelationships. This paper presents Averaged CVB (ACVB) solutions for IRM,convergence-guaranteed and practically useful fast Collapsed Variational Bayes(CVB) inferences. We first derive ordinary CVB and CVB0 for IRM based on thelower bound maximization. CVB solutions yield deterministic iterativeprocedures for inferring IRM given the truncated number of clusters. Ourproposal includes CVB0 updates of hyperparameters including the concentrationparameter of the Dirichlet Process, which has not been studied in theliterature. To make the CVB more practically useful, we further study the CVBinference in two aspects. First, we study the convergence issues and develop aconvergence-guaranteed algorithm for any CVB-based inferences called ACVB,which enables automatic convergence detection and frees non-expertpractitioners from difficult and costly manual monitoring of inferenceprocesses. Second, we present a few techniques for speeding up IRM inferences.In particular, we describe the linear time inference of CVB0, allowing the IRMfor larger relational data uses. The ACVB solutions of IRM showed comparable orbetter performance compared to existing inference methods in experiments, andprovide deterministic, faster, and easier convergence detection.
arxiv-7500-286 | A Robust and Efficient Method for Improving Accuracy of License Plate Characters Recognition | http://arxiv.org/pdf/1407.6705v2.pdf | author:Reza Azad, Hamid Reza Shayegh, Hamed Amiri category:cs.CV published:2014-07-24 summary:License Plate Recognition (LPR) plays an important role on the trafficmonitoring and parking management. A robust and efficient method for enhancingaccuracy of license plate characters recognition based on K Nearest Neighbours(K-NN) classifier is presented in this paper. The system first prepares acontour form of the extracted character, then the angle and distance featureinformation about the character is extracted and finally K-NN classifier isused to character recognition. Angle and distance features of a character havebeen computed based on distribution of points on the bitmap image of character.In K-NN method, the Euclidean distance between testing point and referencepoints is calculated in order to find the k-nearest neighbours. We evaluatedour method on the available dataset that contain 1200 sample. Using 70% samplesfor training, we tested our method on whole samples and obtained 99% correctrecognition rate.Further, we achieved average 99.41% accuracy usingthree/strategy validation technique on 1200 dataset.
arxiv-7500-287 | Voting for Deceptive Opinion Spam Detection | http://arxiv.org/pdf/1409.4504v1.pdf | author:Tao Wang, Hua Zhu category:cs.CL cs.SI published:2014-09-16 summary:Consumers' purchase decisions are increasingly influenced by user-generatedonline reviews. Accordingly, there has been growing concern about the potentialfor posting deceptive opinion spam fictitious reviews that have beendeliberately written to sound authentic, to deceive the readers. Existingapproaches mainly focus on developing automatic supervised learning basedmethods to help users identify deceptive opinion spams. This work, we used the LSI and Sprinkled LSI technique to reduce thedimension for deception detection. We make our contribution to demonstrate whatLSI is capturing in latent semantic space and reveal how deceptive opinions canbe recognized automatically from truthful opinions. Finally, we proposed avoting scheme which integrates different approaches to further improve theclassification performance.
arxiv-7500-288 | Real-time Crowd Tracking using Parameter Optimized Mixture of Motion Models | http://arxiv.org/pdf/1409.4481v1.pdf | author:Aniket Bera, David Wolinski, Julien Pettré, Dinesh Manocha category:cs.CV published:2014-09-16 summary:We present a novel, real-time algorithm to track the trajectory of eachpedestrian in moderately dense crowded scenes. Our formulation is based on anadaptive particle-filtering scheme that uses a combination of variousmulti-agent heterogeneous pedestrian simulation models. We automaticallycompute the optimal parameters for each of these different models based onprior tracked data and use the best model as motion prior for ourparticle-filter based tracking algorithm. We also use our "mixture of motionmodels" for adaptive particle selection and accelerate the performance of theonline tracking algorithm. The motion model parameter estimation is formulatedas an optimization problem, and we use an approach that solves thiscombinatorial optimization problem in a model independent manner and hencescalable to any multi-agent pedestrian motion model. We evaluate theperformance of our approach on different crowd video datasets and highlight theimprovement in accuracy over homogeneous motion models and a baselinemean-shift based tracker. In practice, our formulation can compute trajectoriesof tens of pedestrians on a multi-core desktop CPU in in real time and offerhigher accuracy as compared to prior real time pedestrian tracking algorithms.
arxiv-7500-289 | Convolutional Networks for Image Processing by Coupled Oscillator Arrays | http://arxiv.org/pdf/1409.4469v1.pdf | author:Dmitri E. Nikonov, Ian A. Young, George I. Bourianoff category:nlin.PS cs.CV published:2014-09-15 summary:A coupled oscillator array is shown to approximate convolutions with Gaborfilters for image processing tasks. Pixelated image fragments and filterfunctions are converted to voltages, differenced, and input into acorresponding array of weakly coupled Voltage Controlled Oscillators (VCOs).This is referred to as Frequency Shift Keying (FSK). Upon synchronization ofthe array, the common node amplitude provides a metric for the degree of matchbetween the image fragment and the filter function. The optimal oscillatorparameters for synchronization are determined and favor a moderate value of theQ-factor.
arxiv-7500-290 | The Randomized Causation Coefficient | http://arxiv.org/pdf/1409.4366v1.pdf | author:David Lopez-Paz, Krikamol Muandet, Benjamin Recht category:stat.ML published:2014-09-15 summary:We are interested in learning causal relationships between pairs of randomvariables, purely from observational data. To effectively address this task,the state-of-the-art relies on strong assumptions regarding the mechanismsmapping causes to effects, such as invertibility or the existence of additivenoise, which only hold in limited situations. On the contrary, this short paperproposes to learn how to perform causal inference directly from data, andwithout the need of feature engineering. In particular, we pose causality as akernel mean embedding classification problem, where inputs are samples fromarbitrary probability distributions on pairs of random variables, and labelsare types of causal relationships. We validate the performance of our method onsynthetic and real-world data against the state-of-the-art. Moreover, wesubmitted our algorithm to the ChaLearn's "Fast Causation CoefficientChallenge" competition, with which we won the fastest code prize and rankedthird in the overall leaderboard.
arxiv-7500-291 | Computational Algorithms Based on the Paninian System to Process Euphonic Conjunctions for Word Searches | http://arxiv.org/pdf/1409.4364v1.pdf | author:S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan category:cs.CL published:2014-09-15 summary:Searching for words in Sanskrit E-text is a problem that is accompanied bycomplexities introduced by features of Sanskrit such as euphonic conjunctionsor sandhis. A word could occur in an E-text in a transformed form owing to theoperation of rules of sandhi. Simple word search would not yield thesetransformed forms of the word. Further, there is no search engine in theliterature that can comprehensively search for words in Sanskrit E-texts takingeuphonic conjunctions into account. This work presents an optimal binaryrepresentational schema for letters of the Sanskrit alphabet along withalgorithms to efficiently process the sandhi rules of Sanskrit grammar. Thework further presents an algorithm that uses the sandhi processing algorithm toperform a comprehensive word search on E-text.
arxiv-7500-292 | A Binary Schema and Computational Algorithms to Process Vowel-based Euphonic Conjunctions for Word Searches | http://arxiv.org/pdf/1409.4354v1.pdf | author:S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan category:cs.CL published:2014-09-15 summary:Comprehensively searching for words in Sanskrit E-text is a non-trivialproblem because words could change their forms in different contexts. One suchcontext is sandhi or euphonic conjunctions, which cause a word to change owingto the presence of adjacent letters or words. The change wrought by thesepossible conjunctions can be so significant in Sanskrit that a simple searchfor the word in its given form alone can significantly reduce the success levelof the search. This work presents a representational schema that representsletters in a binary format and reduces Paninian rules of euphonic conjunctionsto simple bit set-unset operations. The work presents an efficient algorithm toprocess vowel-based sandhis using this schema. It further presents anotheralgorithm that uses the sandhi processor to generate the possible transformedword forms of a given word to use in a comprehensive word search.
arxiv-7500-293 | On the optimality of shape and data representation in the spectral domain | http://arxiv.org/pdf/1409.4349v1.pdf | author:Yonathan Aflalo, Haim Brezis, Ron Kimmel category:cs.CV published:2014-09-15 summary:A proof of the optimality of the eigenfunctions of the Laplace-Beltramioperator (LBO) in representing smooth functions on surfaces is provided andadapted to the field of applied shape and data analysis. It is based on theCourant-Fischer min-max principle adapted to our case. % The theorem we presentsupports the new trend in geometry processing of treating geometric structuresby using their projection onto the leading eigenfunctions of the decompositionof the LBO. Utilisation of this result can be used for constructing numericallyefficient algorithms to process shapes in their spectrum. We review a couple ofapplications as possible practical usage cases of the proposed optimalitycriteria. % We refer to a scale invariant metric, which is also invariant tobending of the manifold. This novel pseudo-metric allows constructing an LBO bywhich a scale invariant eigenspace on the surface is defined. We demonstratethe efficiency of an intermediate metric, defined as an interpolation betweenthe scale invariant and the regular one, in representing geometric structureswhile capturing both coarse and fine details. Next, we review a numericalacceleration technique for classical scaling, a member of a family offlattening methods known as multidimensional scaling (MDS). There, theoptimality is exploited to efficiently approximate all geodesic distancesbetween pairs of points on a given surface, and thereby match and comparebetween almost isometric surfaces. Finally, we revisit the classical principalcomponent analysis (PCA) definition by coupling its variational form with aDirichlet energy on the data manifold. By pairing the PCA with the LBO we canhandle cases that go beyond the scope defined by the observation set that ishandled by regular PCA.
arxiv-7500-294 | Novel and Fast Algorithm for Extracting License Plate Location Based on Edge Analysis | http://arxiv.org/pdf/1407.6496v2.pdf | author:Reza Azad, Mohammad Baghdadi category:cs.CV published:2014-07-24 summary:Nowadays in developing or developed countries, the Intelligent TransportationSystem (ITS) technology has attracted so much attention to itself. LicensePlate Recognition (LPR) systems have many applications in ITSs, such as thepayment of parking fee, controlling the traffic volume, traffic datacollection, etc. This paper presents a new and fast method for license plateextraction based on edge analysis. our proposed method consist of four stage,which are edge detection, non-useable edge and noise removing, edge analysisand morphology-based license plate extraction. In the result part, the proposedalgorithm is applied on vehicle database and the accuracy rate reached 98%.From the experimental results it is shown that the proposed method gives fairlyacceptable level of accuracy for practical license plate recognition system.
arxiv-7500-295 | Speeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of Pruning Classifiers | http://arxiv.org/pdf/1409.4205v1.pdf | author:B. Conejo, N. Komodakis, S. Leprince, J. P. Avouac category:cs.CV published:2014-09-15 summary:We propose a general and versatile framework that significantly speeds-upgraphical model optimization while maintaining an excellent solution accuracy.The proposed approach relies on a multi-scale pruning scheme that is able toprogressively reduce the solution space by use of a novel strategy based on acoarse-to-fine cascade of learnt classifiers. We thoroughly experiment withclassic computer vision related MRF problems, where our framework constantlyyields a significant time speed-up (with respect to the most efficientinference methods) and obtains a more accurate solution than directlyoptimizing the MRF.
arxiv-7500-296 | 3D Visual Tracking with Particle and Kalman Filters | http://arxiv.org/pdf/1006.4910v2.pdf | author:Burak Bayramli category:cs.CV published:2010-06-25 summary:One of the most visually demonstrable and straightforward uses of filteringis in the field of Computer Vision. In this document we will try to outline theissues encountered while designing and implementing a particle and kalmanfilter based tracking system.
arxiv-7500-297 | Selecting Near-Optimal Approximate State Representations in Reinforcement Learning | http://arxiv.org/pdf/1405.2652v6.pdf | author:Ronald Ortner, Odalric-Ambrym Maillard, Daniil Ryabko category:cs.LG published:2014-05-12 summary:We consider a reinforcement learning setting introduced in (Maillard et al.,NIPS 2011) where the learner does not have explicit access to the states of theunderlying Markov decision process (MDP). Instead, she has access to severalmodels that map histories of past interactions to states. Here we improve overknown regret bounds in this setting, and more importantly generalize to thecase where the models given to the learner do not contain a true modelresulting in an MDP representation but only approximations of it. We also giveimproved error bounds for state aggregation.
arxiv-7500-298 | An Algorithm Based on Empirical Methods, for Text-to-Tuneful-Speech Synthesis of Sanskrit Verse | http://arxiv.org/pdf/1409.4169v1.pdf | author:Rama N., Meenakshi Lakshmanan category:cs.CL published:2014-09-15 summary:The rendering of Sanskrit poetry from text to speech is a problem that hasnot been solved before. One reason may be the complications in the languageitself. We present unique algorithms based on extensive empirical analysis, tosynthesize speech from a given text input of Sanskrit verses. Using apre-recorded audio units database which is itself tremendously reduced in sizecompared to the colossal size that would otherwise be required, the algorithmswork on producing the best possible, tunefully rendered chanting of the givenverse. His would enable the visually impaired and those with readingdisabilities to easily access the contents of Sanskrit verses otherwiseavailable only in writing.
arxiv-7500-299 | Serialising the ISO SynAF Syntactic Object Model | http://arxiv.org/pdf/1108.0631v3.pdf | author:Laurent Romary, Amir Zeldes, Florian Zipser category:cs.CL published:2011-08-02 summary:This paper introduces, an XML format developed to serialise the object modeldefined by the ISO Syntactic Annotation Framework SynAF. Based on widespreadbest practices we adapt a popular XML format for syntactic annotation,TigerXML, with additional features to support a variety of syntactic phenomenaincluding constituent and dependency structures, binding, and different nodetypes such as compounds or empty elements. We also define interfaces to otherformats and standards including the Morpho-syntactic Annotation Framework MAFand the ISOCat Data Category Registry. Finally a case study of the GermanTreebank TueBa-D/Z is presented, showcasing the handling of constituentstructures, topological fields and coreference annotation in tandem.
arxiv-7500-300 | Active Metric Learning from Relative Comparisons | http://arxiv.org/pdf/1409.4155v1.pdf | author:Sicheng Xiong, Rómer Rosales, Yuanli Pei, Xiaoli Z. Fern category:cs.LG published:2014-09-15 summary:This work focuses on active learning of distance metrics from relativecomparison information. A relative comparison specifies, for a data pointtriplet $(x_i,x_j,x_k)$, that instance $x_i$ is more similar to $x_j$ than to$x_k$. Such constraints, when available, have been shown to be useful towarddefining appropriate distance metrics. In real-world applications, acquiringconstraints often require considerable human effort. This motivates us to studyhow to select and query the most useful relative comparisons to achieveeffective metric learning with minimum user effort. Given an underlying classconcept that is employed by the user to provide such constraints, we present aninformation-theoretic criterion that selects the triplet whose answer leads tothe highest expected gain in information about the classes of a set ofexamples. Directly applying the proposed criterion requires examining $O(n^3)$triplets with $n$ instances, which is prohibitive even for datasets of moderatesize. We show that a randomized selection strategy can be used to reduce theselection pool from $O(n^3)$ to $O(n)$, allowing us to scale up to larger-sizeproblems. Experiments show that the proposed method consistently outperformstwo baseline policies.
