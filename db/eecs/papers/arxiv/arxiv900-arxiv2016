arxiv-900-1 | Message-Passing Algorithms for Channel Estimation and Decoding Using Approximate Inference | http://arxiv.org/pdf/1202.1467v2.pdf | author:Mihai-Alin Badiu, Gunvor Elisabeth Kirkelund, Carles Navarro Manchón, Erwin Riegler, Bernard Henri Fleury category:cs.IT math.IT stat.ML published:2012-02-07 summary:We design iterative receiver schemes for a generic wireless communicationsystem by treating channel estimation and information decoding as an inferenceproblem in graphical models. We introduce a recently proposed inferenceframework that combines belief propagation (BP) and the mean field (MF)approximation and includes these algorithms as special cases. We also show thatthe expectation propagation and expectation maximization algorithms can beembedded in the BP-MF framework with slight modifications. By applying theconsidered inference algorithms to our probabilistic model, we derive fourdifferent message-passing receiver schemes. Our numerical evaluationdemonstrates that the receiver based on the BP-MF framework and its variantbased on BP-EM yield the best compromise between performance, computationalcomplexity and numerical stability among all candidate algorithms.
arxiv-900-2 | FST Based Morphological Analyzer for Hindi Language | http://arxiv.org/pdf/1207.5409v1.pdf | author:Deepak Kumar, Manjeet Singh, Seema Shukla category:cs.CL cs.IR published:2012-07-23 summary:Hindi being a highly inflectional language, FST (Finite State Transducer)based approach is most efficient for developing a morphological analyzer forthis language. The work presented in this paper uses the SFST (Stuttgart FiniteState Transducer) tool for generating the FST. A lexicon of root words iscreated. Rules are then added for generating inflectional and derivationalwords from these root words. The Morph Analyzer developed was used in a Part OfSpeech (POS) Tagger based on Stanford POS Tagger. The system was first trainedusing a manually tagged corpus and MAXENT (Maximum Entropy) approach ofStanford POS tagger was then used for tagging input sentences. Themorphological analyzer gives approximately 97% correct results. POS taggergives an accuracy of approximately 87% for the sentences that have the wordsknown to the trained model file, and 80% accuracy for the sentences that havethe words unknown to the trained model file.
arxiv-900-3 | Towards a theory of statistical tree-shape analysis | http://arxiv.org/pdf/1207.5371v1.pdf | author:Aasa Feragen, Pechin Lo, Marleen de Bruijne, Mads Nielsen, Francois Lauze category:stat.ME cs.CV math.MG published:2012-07-23 summary:In order to develop statistical methods for shapes with a tree-structure, weconstruct a shape space framework for tree-like shapes and study metrics on theshape space. This shape space has singularities, corresponding to topologicaltransitions in the represented trees. We study two closely related metrics onthe shape space, TED and QED. QED is a quotient Euclidean distance arisingnaturally from the shape space formulation, while TED is the classical treeedit distance. Using Gromov's metric geometry we gain new insight into thegeometries defined by TED and QED. We show that the new metric QED has nicegeometric properties which facilitate statistical analysis, such as existenceand local uniqueness of geodesics and averages. TED, on the other hand, doesnot share the geometric advantages of QED, but has nice algorithmic properties.We provide a theoretical framework and experimental results on synthetic datatrees as well as airway trees from pulmonary CT scans. This way, we effectivelyillustrate that our framework has both the theoretical and qualitativeproperties necessary to build a theory of statistical tree-shape analysis.
arxiv-900-4 | A Robust Signal Classification Scheme for Cognitive Radio | http://arxiv.org/pdf/1207.5342v1.pdf | author:Hanwen Cao, Jürgen Peissig category:cs.IT cs.LG cs.NI math.IT C.2.1 published:2012-07-23 summary:This paper presents a robust signal classification scheme for achievingcomprehensive spectrum sensing of multiple coexisting wireless systems. It isbuilt upon a group of feature-based signal detection algorithms enhanced by theproposed dimension cancelation (DIC) method for mitigating the noiseuncertainty problem. The classification scheme is implemented on our testbedconsisting real-world wireless devices. The simulation and experimentalperformances agree with each other well and shows the e?ectiveness androbustness of the proposed scheme.
arxiv-900-5 | Guarantees of Augmented Trace Norm Models in Tensor Recovery | http://arxiv.org/pdf/1207.5326v1.pdf | author:Ziqiang Shi, Jiqing Han, Tieran Zheng, Shiwen Deng, Ji Li category:cs.IT cs.CV math.IT published:2012-07-23 summary:This paper studies the recovery guarantees of the models of minimizing$\\mathcal{X}\_*+\frac{1}{2\alpha}\\mathcal{X}\_F^2$ where $\mathcal{X}$ isa tensor and $\\mathcal{X}\_*$ and $\\mathcal{X}\_F$ are the trace andFrobenius norm of respectively. We show that they can efficiently recoverlow-rank tensors. In particular, they enjoy exact guarantees similar to thoseknown for minimizing $\\mathcal{X}\_*$ under the conditions on the sensingoperator such as its null-space property, restricted isometry property, orspherical section property. To recover a low-rank tensor $\mathcal{X}^0$,minimizing $\\mathcal{X}\_*+\frac{1}{2\alpha}\\mathcal{X}\_F^2$ returns thesame solution as minimizing $\\mathcal{X}\_*$ almost whenever$\alpha\geq10\mathop {\max}\limits_{i}\X^0_{(i)}\_2$.
arxiv-900-6 | Meta-Learning of Exploration/Exploitation Strategies: The Multi-Armed Bandit Case | http://arxiv.org/pdf/1207.5208v1.pdf | author:Francis Maes, Damien Ernst, Louis Wehenkel category:cs.AI cs.LG stat.ML published:2012-07-22 summary:The exploration/exploitation (E/E) dilemma arises naturally in many subfieldsof Science. Multi-armed bandit problems formalize this dilemma in its canonicalform. Most current research in this field focuses on generic solutions that canbe applied to a wide range of problems. However, in practice, it is often thecase that a form of prior information is available about the specific class oftarget problems. Prior knowledge is rarely used in current solutions due to thelack of a systematic approach to incorporate it into the E/E strategy. To address a specific class of E/E problems, we propose to proceed in threesteps: (i) model prior knowledge in the form of a probability distribution overthe target class of E/E problems; (ii) choose a large hypothesis space ofcandidate E/E strategies; and (iii), solve an optimization problem to find acandidate E/E strategy of maximal average performance over a sample of problemsdrawn from the prior distribution. We illustrate this meta-learning approach with two different hypothesisspaces: one where E/E strategies are numerically parameterized and anotherwhere E/E strategies are represented as small symbolic formulas. We proposeappropriate optimization algorithms for both cases. Our experiments, withtwo-armed Bernoulli bandit problems and various playing budgets, show that themeta-learnt E/E strategies outperform generic strategies of the literature(UCB1, UCB1-Tuned, UCB-v, KL-UCB and epsilon greedy); they also evaluate therobustness of the learnt E/E strategies, by tests carried out on arms whoserewards follow a truncated Gaussian distribution.
arxiv-900-7 | A Short Note on Gaussian Process Modeling for Large Datasets using Graphics Processing Units | http://arxiv.org/pdf/1203.1269v2.pdf | author:Mark Franey, Pritam Ranjan, Hugh Chipman category:stat.CO stat.ML published:2012-03-06 summary:The graphics processing unit (GPU) has emerged as a powerful and costeffective processor for general performance computing. GPUs are capable of anorder of magnitude more floating-point operations per second as compared tomodern central processing units (CPUs), and thus provide a great deal ofpromise for computationally intensive statistical applications. Fitting complexstatistical models with a large number of parameters and/or for large datasetsis often very computationally expensive. In this study, we focus on Gaussianprocess (GP) models -- statistical models commonly used for emulating expensivecomputer simulators. We demonstrate that the computational cost of implementingGP models can be significantly reduced by using a CPU+GPU heterogeneouscomputing system over an analogous implementation on a traditional computingsystem with no GPU acceleration. Our small study suggests that GP models arefertile ground for further implementation on CPU+GPU systems.
arxiv-900-8 | Causal Inference on Time Series using Structural Equation Models | http://arxiv.org/pdf/1207.5136v1.pdf | author:Jonas Peters, Dominik Janzing, Bernhard Schölkopf category:stat.ML cs.LG stat.ME published:2012-07-21 summary:Causal inference uses observations to infer the causal structure of the datagenerating system. We study a class of functional models that we call TimeSeries Models with Independent Noise (TiMINo). These models require independentresidual time series, whereas traditional methods like Granger causalityexploit the variance of residuals. There are two main contributions: (1)Theoretical: By restricting the model class (e.g. to additive noise) we canprovide a more general identifiability result than existing ones. This resultincorporates lagged and instantaneous effects that can be nonlinear and do notneed to be faithful, and non-instantaneous feedbacks between the time series.(2) Practical: If there are no feedback loops between time series, we proposean algorithm based on non-linear independence tests of time series. When thedata are causally insufficient, or the data generating process does not satisfythe model assumptions, this algorithm may still give partial results, butmostly avoids incorrect answers. An extension to (non-instantaneous) feedbacksis possible, but not discussed. It outperforms existing methods on artificialand real data. Code can be provided upon request.
arxiv-900-9 | Piecewise Linear Patch Reconstruction for Segmentation and Description of Non-smooth Image Structures | http://arxiv.org/pdf/1207.5113v1.pdf | author:Junyan Wang, Kap Luk Chan category:cs.CV published:2012-07-21 summary:In this paper, we propose a unified energy minimization model for thesegmentation of non-smooth image structures. The energy of piecewise linearpatch reconstruction is considered as an objective measure of the quality ofthe segmentation of non-smooth structures. The segmentation is achieved byminimizing the single energy without any separate process of featureextraction. We also prove that the error of segmentation is bounded by theproposed energy functional, meaning that minimizing the proposed energy leadsto reducing the error of segmentation. As a by-product, our method produces adictionary of optimized orthonormal descriptors for each segmented region. Theunique feature of our method is that it achieves the simultaneous segmentationand description for non-smooth image structures under the same optimizationframework. The experiments validate our theoretical claims and show the clearsuperior performance of our methods over other related methods for segmentationof various image textures. We show that our model can be coupled with thepiecewise smooth model to handle both smooth and non-smooth structures, and wedemonstrate that the proposed model is capable of coping with multipledifferent regions through the one-against-all strategy.
arxiv-900-10 | Learning Probabilistic Systems from Tree Samples | http://arxiv.org/pdf/1207.5091v1.pdf | author:Anvesh Komuravelli, Corina S. Pasareanu, Edmund M. Clarke category:cs.LO cs.LG published:2012-07-21 summary:We consider the problem of learning a non-deterministic probabilistic systemconsistent with a given finite set of positive and negative tree samples.Consistency is defined with respect to strong simulation conformance. Wepropose learning algorithms that use traditional and a new "stochastic"state-space partitioning, the latter resulting in the minimum number of states.We then use them to solve the problem of "active learning", that uses aknowledgeable teacher to generate samples as counterexamples to simulationequivalence queries. We show that the problem is undecidable in general, butthat it becomes decidable under a suitable condition on the teacher which comesnaturally from the way samples are generated from failed simulation checks. Thelatter problem is shown to be undecidable if we impose an additional conditionon the learner to always conjecture a "minimum state" hypothesis. We thereforepropose a semi-algorithm using stochastic partitions. Finally, we apply theproposed (semi-) algorithms to infer intermediate assumptions in an automatedassume-guarantee verification framework for probabilistic systems.
arxiv-900-11 | Absolute Uniqueness of Phase Retrieval with Random Illumination | http://arxiv.org/pdf/1110.5097v7.pdf | author:Albert Fannjiang category:physics.optics cs.CV math-ph math.MP published:2011-10-23 summary:Random illumination is proposed to enforce absolute uniqueness and resolveall types of ambiguity, trivial or nontrivial, from phase retrieval. Almostsure irreducibility is proved for any complex-valued object of a full ranksupport. While the new irreducibility result can be viewed as a probabilisticversion of the classical result by Bruck, Sodin and Hayes, it provides a novelperspective and an effective method for phase retrieval. In particular, almost sure uniqueness, up to a global phase, is proved forcomplex-valued objects under general two-point conditions. Under a tight sectorconstraint absolute uniqueness is proved to hold with probability exponentiallyclose to unity as the object sparsity increases. Under a magnitude constraintwith random amplitude illumination, uniqueness modulo global phase is proved tohold with probability exponentially close to unity as object sparsityincreases. For general complex-valued objects without any constraint, almostsure uniqueness up to global phase is established with two sets of Fouriermagnitude data under two independent illuminations. Numerical experimentssuggest that random illumination essentially alleviates most, if not all,numerical problems commonly associated with the standard phasing algorithms.
arxiv-900-12 | A Novel Metric Approach Evaluation For The Spatial Enhancement Of Pan-Sharpened Images | http://arxiv.org/pdf/1207.5064v1.pdf | author:Firouz Abdullah Al-Wassai, Dr. N. V. Kalyankar category:cs.CV published:2012-07-20 summary:Various and different methods can be used to produce high-resolutionmultispectral images from high-resolution panchromatic image (PAN) andlow-resolution multispectral images (MS), mostly on the pixel level. TheQuality of image fusion is an essential determinant of the value of processingimages fusion for many applications. Spatial and spectral qualities are the twoimportant indexes that used to evaluate the quality of any fused image.However, the jury is still out of fused image's benefits if it compared withits original images. In addition, there is a lack of measures for assessing theobjective quality of the spatial resolution for the fusion methods. So, anobjective quality of the spatial resolution assessment for fusion images isrequired. Therefore, this paper describes a new approach proposed to estimatethe spatial resolution improve by High Past Division Index (HPDI) uponcalculating the spatial-frequency of the edge regions of the image and it dealswith a comparison of various analytical techniques for evaluating the Spatialquality, and estimating the colour distortion added by image fusion including:MG, SG, FCC, SD, En, SNR, CC and NRMSE. In addition, this paper devotes toconcentrate on the comparison of various image fusion techniques based on pixeland feature fusion technique.
arxiv-900-13 | Parameter and Structure Learning in Nested Markov Models | http://arxiv.org/pdf/1207.5058v1.pdf | author:Ilya Shpitser, Thomas S. Richardson, James M. Robins, Robin Evans category:stat.ML math.ST stat.TH published:2012-07-20 summary:The constraints arising from DAG models with latent variables can benaturally represented by means of acyclic directed mixed graphs (ADMGs). Suchgraphs contain directed and bidirected arrows, and contain no directed cycles.DAGs with latent variables imply independence constraints in the distributionresulting from a 'fixing' operation, in which a joint distribution is dividedby a conditional. This operation generalizes marginalizing and conditioning.Some of these constraints correspond to identifiable 'dormant' independenceconstraints, with the well known 'Verma constraint' as one example. Recently,models defined by a set of the constraints arising after fixing from a DAG withlatents, were characterized via a recursive factorization and a nested Markovproperty. In addition, a parameterization was given in the discrete case. Inthis paper we use this parameterization to describe a parameter fittingalgorithm, and a search and score structure learning algorithm for these nestedMarkov models. We apply our algorithms to a variety of datasets.
arxiv-900-14 | Multisegmentation through wavelets: Comparing the efficacy of Daubechies vs Coiflets | http://arxiv.org/pdf/1207.5007v1.pdf | author:Madhur Srivastava, Yashwant Yashu, Satish K. Singh, Prasanta K. Panigrahi category:cs.CV published:2012-07-20 summary:In this paper, we carry out a comparative study of the efficacy of waveletsbelonging to Daubechies and Coiflet family in achieving image segmentationthrough a fast statistical algorithm.The fact that wavelets belonging toDaubechies family optimally capture the polynomial trends and those of Coifletfamily satisfy mini-max condition, makes this comparison interesting. In thecontext of the present algorithm, it is found that the performance of Coifletwavelets is better, as compared to Daubechies wavelet.
arxiv-900-15 | Developing Embodied Multisensory Dialogue Agents | http://arxiv.org/pdf/1111.7190v3.pdf | author:Michał B. Paradowski category:cs.AI cs.CL published:2011-11-29 summary:A few decades of work in the AI field have focused efforts on developing anew generation of systems which can acquire knowledge via interaction with theworld. Yet, until very recently, most such attempts were underpinned byresearch which predominantly regarded linguistic phenomena as separated fromthe brain and body. This could lead one into believing that to emulatelinguistic behaviour, it suffices to develop 'software' operating on abstractrepresentations that will work on any computational machine. This picture isinaccurate for several reasons, which are elucidated in this paper and extendbeyond sensorimotor and semantic resonance. Beginning with a review ofresearch, I list several heterogeneous arguments against disembodied language,in an attempt to draw conclusions for developing embodied multisensory agentswhich communicate verbally and non-verbally with their environment. Withouttaking into account both the architecture of the human brain, and embodiment,it is unrealistic to replicate accurately the processes which take place duringlanguage acquisition, comprehension, production, or during non-linguisticactions. While robots are far from isomorphic with humans, they could benefitfrom strengthened associative connections in the optimization of theirprocesses and their reactivity and sensitivity to environmental stimuli, and insituated human-machine interaction. The concept of multisensory integrationshould be extended to cover linguistic input and the complementary informationcombined from temporally coincident sensory impressions.
arxiv-900-16 | Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural Network | http://arxiv.org/pdf/1207.4931v1.pdf | author:G. N. Tripathi, V. Rihani category:cs.RO cs.AI cs.LG cs.NE published:2012-07-20 summary:The paper presents the electronic design and motion planning of a robot basedon decision making regarding its straight motion and precise turn usingArtificial Neural Network (ANN). The ANN helps in learning of robot so that itperforms motion autonomously. The weights calculated are implemented inmicrocontroller. The performance has been tested to be excellent.
arxiv-900-17 | Distributed Strongly Convex Optimization | http://arxiv.org/pdf/1207.3031v2.pdf | author:Konstantinos I. Tsianos, Michael G. Rabbat category:cs.DC cs.LG stat.ML published:2012-07-12 summary:A lot of effort has been invested into characterizing the convergence ratesof gradient based algorithms for non-linear convex optimization. Recently,motivated by large datasets and problems in machine learning, the interest hasshifted towards distributed optimization. In this work we present a distributedalgorithm for strongly convex constrained optimization. Each node in a networkof n computers converges to the optimum of a strongly convex, L-Lipchitzcontinuous, separable objective at a rate O(log (sqrt(n) T) / T) where T is thenumber of iterations. This rate is achieved in the online setting where thedata is revealed one at a time to the nodes, and in the batch setting whereeach node has access to its full local dataset from the start. The sameconvergence rate is achieved in expectation when the subgradients used at eachnode are corrupted with additive zero-mean noise.
arxiv-900-18 | Facial Expression Representation and Recognition Using 2DHLDA, Gabor Wavelets, and Ensemble Learning | http://arxiv.org/pdf/1004.0378v7.pdf | author:Mahmoud Khademi, Mohammad H. Kiapour, Mehran Safayani, Mohammad T. Manzuri, M. Shojaei category:cs.CV cs.LG I.5 published:2010-04-02 summary:In this paper, a novel method for representation and recognition of thefacial expressions in two-dimensional image sequences is presented. We apply avariation of two-dimensional heteroscedastic linear discriminant analysis(2DHLDA) algorithm, as an efficient dimensionality reduction technique, toGabor representation of the input sequence. 2DHLDA is an extension of thetwo-dimensional linear discriminant analysis (2DLDA) approach and it removesthe equal within-class covariance. By applying 2DHLDA in two directions, weeliminate the correlations between both image columns and image rows. Then, weperform a one-dimensional LDA on the new features. This combined method canalleviate the small sample size problem and instability encountered by HLDA.Also, employing both geometric and appearance features and using an ensemblelearning scheme based on data fusion, we create a classifier which canefficiently classify the facial expressions. The proposed method is robust toillumination changes and it can properly represent temporal information as wellas subtle changes in facial muscles. We provide experiments on Cohn-Kanadedatabase that show the superiority of the proposed method. KEYWORDS:two-dimensional heteroscedastic linear discriminant analysis (2DHLDA), subspacelearning, facial expression analysis, Gabor wavelets, ensemble learning.
arxiv-900-19 | Local Component Analysis for Nonparametric Bayes Classifier | http://arxiv.org/pdf/1010.4951v2.pdf | author:Mahmoud Khademi, Mohammad T. Manzuri-Shalmani, Meharn safayani category:cs.CV cs.LG published:2010-10-24 summary:The decision boundaries of Bayes classifier are optimal because they lead tomaximum probability of correct decision. It means if we knew the priorprobabilities and the class-conditional densities, we could design a classifierwhich gives the lowest probability of error. However, in classification basedon nonparametric density estimation methods such as Parzen windows, thedecision regions depend on the choice of parameters such as window width.Moreover, these methods suffer from curse of dimensionality of the featurespace and small sample size problem which severely restricts their practicalapplications. In this paper, we address these problems by introducing a noveldimension reduction and classification method based on local componentanalysis. In this method, by adopting an iterative cross-validation algorithm,we simultaneously estimate the optimal transformation matrices (for dimensionreduction) and classifier parameters based on local information. The proposedmethod can classify the data with complicated boundary and also alleviate thecourse of dimensionality dilemma. Experiments on real data show the superiorityof the proposed algorithm in term of classification accuracies for patternclassification applications like age, facial expression and characterrecognition. Keywords: Bayes classifier, curse of dimensionality dilemma,Parzen window, pattern classification, subspace learning.
arxiv-900-20 | Automorphism Groups of Graphical Models and Lifted Variational Inference | http://arxiv.org/pdf/1207.4814v1.pdf | author:Hung Hai Bui, Tuyen N. Huynh, Sebastian Riedel category:cs.AI cs.LG math.CO stat.CO stat.ML published:2012-07-19 summary:Using the theory of group action, we first introduce the concept of theautomorphism group of an exponential family or a graphical model, thusformalizing the general notion of symmetry of a probabilistic model. Thisautomorphism group provides a precise mathematical framework for liftedinference in the general exponential family. Its group action partitions theset of random variables and feature functions into equivalent classes (calledorbits) having identical marginals and expectations. Then the inference problemis effectively reduced to that of computing marginals or expectations for eachclass, thus avoiding the need to deal with each individual variable or feature.We demonstrate the usefulness of this general framework in lifting two classesof variational approximation for MAP inference: local LP relaxation and localLP relaxation with cycle constraints; the latter yields the first liftedinference that operate on a bound tighter than local constraints. Initialexperimental results demonstrate that lifted MAP inference with cycleconstraints achieved the state of the art performance, obtaining much betterobjective function values than local approximation while remaining relativelyefficient.
arxiv-900-21 | Hierarchical Clustering using Randomly Selected Similarities | http://arxiv.org/pdf/1207.4748v1.pdf | author:Brian Eriksson category:stat.ML cs.IT cs.LG math.IT published:2012-07-19 summary:The problem of hierarchical clustering items from pairwise similarities isfound across various scientific disciplines, from biology to networking. Often,applications of clustering techniques are limited by the cost of obtainingsimilarities between pairs of items. While prior work has been developed toreconstruct clustering using a significantly reduced set of pairwisesimilarities via adaptive measurements, these techniques are only applicablewhen choice of similarities are available to the user. In this paper, weexamine reconstructing hierarchical clustering under similarity observationsat-random. We derive precise bounds which show that a significant fraction ofthe hierarchical clustering can be recovered using fewer than all the pairwisesimilarities. We find that the correct hierarchical clustering down to aconstant fraction of the total number of items (i.e., clusters sized O(N)) canbe found using only O(N log N) randomly selected pairwise similarities inexpectation.
arxiv-900-22 | Models of Disease Spectra | http://arxiv.org/pdf/1207.4674v1.pdf | author:Iead Rezek, Christian Beckmann category:stat.ML published:2012-07-19 summary:Case vs control comparisons have been the classical approach to the study ofneurological diseases. However, most patients will not fall cleanly into eithergroup. Instead, clinicians will typically find patients that cannot beclassified as having clearly progressed into the disease state. For thosesubjects, very little can be said about their brain function on the basis ofanalyses of group differences. To describe the intermediate brain functionrequires models that interpolate between the disease states. We have chosenGaussian Processes (GP) regression to obtain a continuous spectrum of brainactivation and to extract the unknown disease progression profile. Our modelsincorporate spatial distribution of measures of activation, e.g. thecorrelation of an fMRI trace with an input stimulus, and so constituteultra-high multi-variate GP regressors. We applied GPs to model fMRI imagephenotypes across Alzheimer's Disease (AD) behavioural measures, e.g. MMSE, ACEetc. scores, and obtained predictions at non-observed MMSE/ACE values. Theoverall model confirmed the known reduction in the spatial extent of activityin response to reading versus false-font stimulation. The predictiveuncertainty indicated the worsening confidence intervals at behavioural scoresdistance from those used for GP training. Thus, the model indicated the type ofpatient (what behavioural score) that would need to included in the trainingdata to improve models predictions.
arxiv-900-23 | Thompson Sampling: An Asymptotically Optimal Finite Time Analysis | http://arxiv.org/pdf/1205.4217v2.pdf | author:Emilie Kaufmann, Nathaniel Korda, Rémi Munos category:stat.ML cs.LG published:2012-05-18 summary:The question of the optimality of Thompson Sampling for solving thestochastic multi-armed bandit problem had been open since 1933. In this paperwe answer it positively for the case of Bernoulli rewards by providing thefirst finite-time analysis that matches the asymptotic rate given in the Laiand Robbins lower bound for the cumulative regret. The proof is accompanied bya numerical comparison with other optimal policies, experiments that have beenlacking in the literature until now for the Bernoulli case.
arxiv-900-24 | Recovering Epipolar Geometry from Images of Smooth Surfaces | http://arxiv.org/pdf/1106.0823v4.pdf | author:Oleg Kupervasser category:cs.CV cs.AI 68T45 published:2011-06-04 summary:We present four methods for recovering the epipolar geometry from images ofsmooth surfaces. In the existing methods for recovering epipolar geometrycorresponding feature points are used that cannot be found in such images. Thefirst method is based on finding corresponding characteristic points created byillumination (ICPM - illumination characteristic points' method (PM)). Thesecond method is based on correspondent tangency points created by tangentsfrom epipoles to outline of smooth bodies (OTPM - outline tangent PM). Thesetwo methods are exact and give correct results for real images, becausepositions of the corresponding illumination characteristic points andcorresponding outline are known with small errors. But the second method islimited either to special type of scenes or to restricted camera motion. Wealso consider two more methods which are termed CCPM (curve characteristic PM)and CTPM (curve tangent PM), for searching epipolar geometry for images ofsmooth bodies based on a set of level curves with constant illuminationintensity. The CCPM method is based on searching correspondent points onisophoto curves with the help of correlation of curvatures between these lines.The CTPM method is based on property of the tangential to isophoto curveepipolar line to map into the tangential to correspondent isophoto curvesepipolar line. The standard method (SM) based on knowledge of pairs of thealmost exact correspondent points. The methods have been implemented and testedby SM on pairs of real images. Unfortunately, the last two methods give us onlya finite subset of solutions including "good" solution. Exception is "epipolesin infinity". The main reason is inaccuracy of assumption of constantbrightness for smooth bodies. But outline and illumination characteristicpoints are not influenced by this inaccuracy. So, the first pair of methodsgives exact results.
arxiv-900-25 | Clustering of Local Optima in Combinatorial Fitness Landscapes | http://arxiv.org/pdf/1207.4632v1.pdf | author:Gabriela Ochoa, Sébastien Verel, Fabio Daolio, Marco Tomassini category:cs.NE cs.AI published:2012-07-19 summary:Using the recently proposed model of combinatorial landscapes: local optimanetworks, we study the distribution of local optima in two classes of instancesof the quadratic assignment problem. Our results indicate that the two probleminstance classes give rise to very different configuration spaces. For theso-called real-like class, the optima networks possess a clear modularstructure, while the networks belonging to the class of random uniforminstances are less well partitionable into clusters. We briefly discuss theconsequences of the findings for heuristically searching the correspondingproblem spaces.
arxiv-900-26 | Analyzing the Effect of Objective Correlation on the Efficient Set of MNK-Landscapes | http://arxiv.org/pdf/1207.4631v1.pdf | author:Sébastien Verel, Arnaud Liefooghe, Laetitia Jourdan, Clarisse Dhaenens category:cs.NE cs.AI published:2012-07-19 summary:In multiobjective combinatorial optimization, there exists two main classesof metaheuristics, based either on multiple aggregations, or on a dominancerelation. As in the single objective case, the structure of the search spacecan explain the difficulty for multiobjective metaheuristics, and guide thedesign of such methods. In this work we analyze the properties ofmultiobjective combinatorial search spaces. In particular, we focus on thefeatures related the efficient set, and we pay a particular attention to thecorrelation between objectives. Few benchmark takes such objective correlationinto account. Here, we define a general method to design multiobjectiveproblems with correlation. As an example, we extend the well-knownmultiobjective NK-landscapes. By measuring different properties of the searchspace, we show the importance of considering the objective correlation on thedesign of metaheuristics.
arxiv-900-27 | On the Neutrality of Flowshop Scheduling Fitness Landscapes | http://arxiv.org/pdf/1207.4629v1.pdf | author:Marie-Eleonore Marmion, Clarisse Dhaenens, Laetitia Jourdan, Arnaud Liefooghe, Sébastien Verel category:cs.NE cs.AI published:2012-07-19 summary:Solving efficiently complex problems using metaheuristics, and in particularlocal searches, requires incorporating knowledge about the problem to solve. Inthis paper, the permutation flowshop problem is studied. It is well known thatin such problems, several solutions may have the same fitness value. As thisneutrality property is an important one, it should be taken into account duringthe design of optimization methods. Then in the context of the permutationflowshop, a deep landscape analysis focused on the neutrality property isdriven and propositions on the way to use this neutrality to guide efficientlythe search are given.
arxiv-900-28 | On the Effect of Connectedness for Biobjective Multiple and Long Path Problems | http://arxiv.org/pdf/1207.4628v1.pdf | author:Sébastien Verel, Arnaud Liefooghe, Jérémie Humeau, Laetitia Jourdan, Clarisse Dhaenens category:cs.NE cs.AI published:2012-07-19 summary:Recently, the property of connectedness has been claimed to give a strongmotivation on the design of local search techniques for multiobjectivecombinatorial optimization (MOCO). Indeed, when connectedness holds, a basicPareto local search, initialized with at least one non-dominated solution,allows to identify the efficient set exhaustively. However, this becomesquickly infeasible in practice as the number of efficient solutions typicallygrows exponentially with the instance size. As a consequence, we generally haveto deal with a limited-size approximation, where a good sample set has to befound. In this paper, we propose the biobjective multiple and long pathproblems to show experimentally that, on the first problems, even if theefficient set is connected, a local search may be outperformed by a simpleevolutionary algorithm in the sampling of the efficient set. At the opposite,on the second problems, a local search algorithm may successfully approximate adisconnected efficient set. Then, we argue that connectedness is not the singleproperty to study for the design of local search heuristics for MOCO. This workopens new discussions on a proper definition of the multiobjective fitnesslandscape.
arxiv-900-29 | The Road to VEGAS: Guiding the Search over Neutral Networks | http://arxiv.org/pdf/1207.4626v1.pdf | author:Marie-Eleonore Marmion, Clarisse Dhaenens, Laetitia Jourdan, Arnaud Liefooghe, Sébastien Verel category:cs.NE published:2012-07-19 summary:VEGAS (Varying Evolvability-Guided Adaptive Search) is a new methodologyproposed to deal with the neutrality property of some optimization problems. tsmain feature is to consider the whole neutral network rather than an arbitrarysolution. Moreover, VEGAS is designed to escape from plateaus based on theevolvability of solution and a multi-armed bandit. Experiments are conducted onNK-landscapes with neutrality. Results show the importance of considering thewhole neutral network and of guiding the search cleverly. The impact of thelevel of neutrality and of the exploration-exploitation trade-off are deeplyanalyzed.
arxiv-900-30 | Appropriate Nouns with Obligatory Modifiers | http://arxiv.org/pdf/1207.4625v1.pdf | author:E. Laporte category:cs.CL published:2012-07-19 summary:The notion of appropriate sequence as introduced by Z. Harris provides apowerful syntactic way of analysing the detailed meaning of various sentences,including ambiguous ones. In an adjectival sentence like 'The leather wasyellow', the introduction of an appropriate noun, here 'colour', specifieswhich quality the adjective describes. In some other adjectival sentences withan appropriate noun, that noun plays the same part as 'colour' and seems to berelevant to the description of the adjective. These appropriate nouns canusually be used in elementary sentences like 'The leather had some colour', butin many cases they have a more or less obligatory modifier. For example, youcan hardly mention that an object has a colour without qualifying that colourat all. About 300 French nouns are appropriate in at least one adjectivalsentence and have an obligatory modifier. They enter in a number of sentencestructures related by several syntactic transformations. The appropriateness ofthe noun and the fact that the modifier is obligatory are reflected in thesetransformations. The description of these syntactic phenomena provides a basisfor a classification of these nouns. It also concerns the lexical properties ofthousands of predicative adjectives, and in particular the relations betweenthe sentence without the noun : 'The leather was yellow' and the adjectivalsentence with the noun : 'The colour of the leather was yellow'.
arxiv-900-31 | Local stability of Belief Propagation algorithm with multiple fixed points | http://arxiv.org/pdf/1207.4597v1.pdf | author:Victorin Martin, Jean-Marc Lasgouttes, Cyril Furtlehner category:stat.ML cs.LG published:2012-07-19 summary:A number of problems in statistical physics and computer science can beexpressed as the computation of marginal probabilities over a Markov randomfield. Belief propagation, an iterative message-passing algorithm, computesexactly such marginals when the underlying graph is a tree. But it has gainedits popularity as an efficient way to approximate them in the more generalcase, even if it can exhibits multiple fixed points and is not guaranteed toconverge. In this paper, we express a new sufficient condition for localstability of a belief propagation fixed point in terms of the graph structureand the beliefs values at the fixed point. This gives credence to the usualunderstanding that Belief Propagation performs better on sparse graphs.
arxiv-900-32 | Protein Function Prediction Based on Kernel Logistic Regression with 2-order Graphic Neighbor Information | http://arxiv.org/pdf/1207.4463v1.pdf | author:Jingwei Liu category:q-bio.QM cs.LG q-bio.MN published:2012-07-18 summary:To enhance the accuracy of protein-protein interaction function prediction, a2-order graphic neighbor information feature extraction method based onundirected simple graph is proposed in this paper, which extends the 1-ordergraphic neighbor featureextraction method. And the chi-square test statisticalmethod is also involved in feature combination. To demonstrate theeffectiveness of our 2-order graphic neighbor feature, four logistic regressionmodels (logistic regression (abbrev. LR), diffusion kernel logistic regression(abbrev. DKLR), polynomial kernel logistic regression (abbrev. PKLR), andradial basis function (RBF) based kernel logistic regression (abbrev. RBF KLR))are investigated on the two feature sets. The experimental results of proteinfunction prediction of Yeast Proteome Database (YPD) using the theprotein-protein interaction data of Munich Information Center for ProteinSequences (MIPS) show that 2-order graphic neighbor information of proteins cansignificantly improve the average overall percentage of protein functionprediction especially with RBF KLR. And, with a new 5-top chi-square featurecombination method, RBF KLR can achieve 99.05% average overall percentage on2-order neighbor feature combination set.
arxiv-900-33 | First-improvement vs. Best-improvement Local Optima Networks of NK Landscapes | http://arxiv.org/pdf/1207.4455v1.pdf | author:Gabriela Ochoa, Sébastien Verel, Marco Tomassini category:cs.NE cs.AI published:2012-07-18 summary:This paper extends a recently proposed model for combinatorial landscapes:Local Optima Networks (LON), to incorporate a first-improvement (greedy-ascent)hill-climbing algorithm, instead of a best-improvement (steepest-ascent) one,for the definition and extraction of the basins of attraction of the landscapeoptima. A statistical analysis comparing best and first improvement networkmodels for a set of NK landscapes, is presented and discussed. Our resultssuggest structural differences between the two models with respect to both thenetwork connectivity, and the nature of the basins of attraction. The impact ofthese differences in the behavior of search heuristics based on first and bestimprovement local search is thoroughly discussed.
arxiv-900-34 | Pareto Local Optima of Multiobjective NK-Landscapes with Correlated Objectives | http://arxiv.org/pdf/1207.4452v1.pdf | author:Sébastien Verel, Arnaud Liefooghe, Laetitia Jourdan, Clarisse Dhaenens category:cs.NE cs.AI published:2012-07-18 summary:In this paper, we conduct a fitness landscape analysis for multiobjectivecombinatorial optimization, based on the local optima of multiobjectiveNK-landscapes with objective correlation. In single-objective optimization, ithas become clear that local optima have a strong impact on the performance ofmetaheuristics. Here, we propose an extension to the multiobjective case, basedon the Pareto dominance. We study the co-influence of the problem dimension,the degree of non-linearity, the number of objectives and the correlationdegree between objective functions on the number of Pareto local optima.
arxiv-900-35 | Set-based Multiobjective Fitness Landscapes: A Preliminary Study | http://arxiv.org/pdf/1207.4451v1.pdf | author:Sébastien Verel, Arnaud Liefooghe, Clarisse Dhaenens category:cs.NE cs.AI published:2012-07-18 summary:Fitness landscape analysis aims to understand the geometry of a givenoptimization problem in order to design more efficient search algorithms.However, there is a very little knowledge on the landscape of multiobjectiveproblems. In this work, following a recent proposal by Zitzler et al. (2010),we consider multiobjective optimization as a set problem. Then, we give ageneral definition of set-based multiobjective fitness landscapes. Anexperimental set-based fitness landscape analysis is conducted on themultiobjective NK-landscapes with objective correlation. The aim is to adaptand to enhance the comprehensive design of set-based multiobjective searchapproaches, motivated by an a priori analysis of the corresponding set problemproperties.
arxiv-900-36 | NILS: a Neutrality-based Iterated Local Search and its application to Flowshop Scheduling | http://arxiv.org/pdf/1207.4450v1.pdf | author:Marie-Eleonore Marmion, Clarisse Dhaenens, Laetitia Jourdan, Arnaud Liefooghe, Sébastien Verel category:cs.NE cs.AI published:2012-07-18 summary:This paper presents a new methodology that exploits specific characteristicsfrom the fitness landscape. In particular, we are interested in the property ofneutrality, that deals with the fact that the same fitness value is assigned tonumerous solutions from the search space. Many combinatorial optimizationproblems share this property, that is generally very inhibiting for localsearch algorithms. A neutrality-based iterated local search, that allowsneutral walks to move on the plateaus, is proposed and experimented on apermutation flowshop scheduling problem with the aim of minimizing themakespan. Our experiments show that the proposed approach is able to findimproving solutions compared with a classical iterated local search. Moreover,the tradeoff between the exploitation of neutrality and the exploration of newparts of the search space is deeply analyzed.
arxiv-900-37 | DAMS: Distributed Adaptive Metaheuristic Selection | http://arxiv.org/pdf/1207.4448v1.pdf | author:Bilel Derbel, Sébastien Verel category:cs.NE cs.AI published:2012-07-18 summary:We present a distributed generic algorithm called DAMS dedicated to adaptiveoptimization in distributed environments. Given a set of metaheuristic, thegoal of DAMS is to coordinate their local execution on distributed nodes inorder to optimize the global performance of the distributed system. DAMS isbased on three-layer architecture allowing node to decide distributively whatlocal information to communicate, and what metaheuristic to apply while theoptimization process is in progress. The adaptive features of DAMS are firstaddressed in a very general setting. A specific DAMS called SBM is thendescribed and analyzed from both a parallel and an adaptive point of view. SBMis a simple, yet efficient, adaptive distributed algorithm using anexploitation component allowing nodes to select the metaheuristic with the bestlocally observed performance, and an exploration component allowing nodes todetect the metaheuristic with the actual best performance. The efficiency ofBSM-DAMS is demonstrated through experimentations and comparisons with otheradaptive strategies (sequential and distributed).
arxiv-900-38 | Communities of Minima in Local Optima Networks of Combinatorial Spaces | http://arxiv.org/pdf/1207.4445v1.pdf | author:Fabio Daolio, Marco Tomassini, Sébastien Verel, Gabriela Ochoa category:cs.NE cs.AI published:2012-07-18 summary:In this work we present a new methodology to study the structure of theconfiguration spaces of hard combinatorial problems. It consists in buildingthe network that has as nodes the locally optimal configurations and as edgesthe weighted oriented transitions between their basins of attraction. We applythe approach to the detection of communities in the optima networks produced bytwo different classes of instances of a hard combinatorial optimizationproblem: the quadratic assignment problem (QAP). We provide evidence indicatingthat the two problem instance classes give rise to very different configurationspaces. For the so-called real-like class, the networks possess a clear modularstructure, while the optima networks belonging to the class of random uniforminstances are less well partitionable into clusters. This is convincinglysupported by using several statistical tests. Finally, we shortly discuss theconsequences of the findings for heuristically searching the correspondingproblem spaces.
arxiv-900-39 | Complex-network analysis of combinatorial spaces: The NK landscape case | http://arxiv.org/pdf/1207.4442v1.pdf | author:Marco Tomassini, Sébastien Verel, Gabriela Ochoa category:cs.NE nlin.AO published:2012-07-18 summary:We propose a network characterization of combinatorial fitness landscapes byadapting the notion of inherent networks proposed for energy surfaces. We usethe well-known family of NK landscapes as an example. In our case the inherentnetwork is the graph whose vertices represent the local maxima in thelandscape, and the edges account for the transition probabilities between theircorresponding basins of attraction. We exhaustively extracted such networks onrepresentative NK landscape instances, and performed a statisticalcharacterization of their properties. We found that most of these networkproperties are related to the search difficulty on the underlying NK landscapeswith varying values of K.
arxiv-900-40 | Unachievable Region in Precision-Recall Space and Its Effect on Empirical Evaluation | http://arxiv.org/pdf/1206.4667v2.pdf | author:Kendrick Boyd, Vitor Santos Costa, Jesse Davis, David Page category:cs.LG cs.AI cs.IR published:2012-06-18 summary:Precision-recall (PR) curves and the areas under them are widely used tosummarize machine learning results, especially for data sets exhibiting classskew. They are often used analogously to ROC curves and the area under ROCcurves. It is known that PR curves vary as class skew changes. What was notrecognized before this paper is that there is a region of PR space that iscompletely unachievable, and the size of this region depends only on the skew.This paper precisely characterizes the size of that region and discusses itsimplications for empirical evaluation methodology in machine learning.
arxiv-900-41 | Stochastic optimization and sparse statistical recovery: An optimal algorithm for high dimensions | http://arxiv.org/pdf/1207.4421v1.pdf | author:Alekh Agarwal, Sahand Negahban, Martin J. Wainwright category:stat.ML cs.LG math.OC published:2012-07-18 summary:We develop and analyze stochastic optimization algorithms for problems inwhich the expected loss is strongly convex, and the optimum is (approximately)sparse. Previous approaches are able to exploit only one of these twostructures, yielding an $\order(\pdim/T)$ convergence rate for strongly convexobjectives in $\pdim$ dimensions, and an $\order(\sqrt{(\spindex \log\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Ouralgorithm is based on successively solving a series of $\ell_1$-regularizedoptimization problems using Nesterov's dual averaging algorithm. We establishthat the error of our solution after $T$ iterations is at most$\order((\spindex \log\pdim)/T)$, with natural extensions to approximatesparsity. Our results apply to locally Lipschitz losses including the logistic,exponential, hinge and least-squares losses. By recourse to statistical minimaxresults, we show that our convergence rates are optimal up to multiplicativeconstant factors. The effectiveness of our approach is also confirmed innumerical simulations, in which we compare to several baselines on aleast-squares regression problem.
arxiv-900-42 | Better Mixing via Deep Representations | http://arxiv.org/pdf/1207.4404v1.pdf | author:Yoshua Bengio, Grégoire Mesnil, Yann Dauphin, Salah Rifai category:cs.LG published:2012-07-18 summary:It has previously been hypothesized, and supported with some experimentalevidence, that deeper representations, when well trained, tend to do a betterjob at disentangling the underlying factors of variation. We study thefollowing related conjecture: better representations, in the sense of betterdisentangling, can be exploited to produce faster-mixing Markov chains.Consequently, mixing would be more efficient at higher levels ofrepresentation. To better understand why and how this is happening, we proposea secondary conjecture: the higher-level samples fill more uniformly the spacethey occupy and the high-density manifolds tend to unfold when represented athigher levels. The paper discusses these hypotheses and tests themexperimentally through visualization and measurements of mixing andinterpolating between samples.
arxiv-900-43 | Universal Properties of Mythological Networks | http://arxiv.org/pdf/1205.4324v2.pdf | author:Pádraig Mac Carron, Ralph Kenna category:physics.soc-ph cs.CL cs.SI published:2012-05-19 summary:As in statistical physics, the concept of universality plays an important,albeit qualitative, role in the field of comparative mythology. Here we applystatistical mechanical tools to analyse the networks underlying three iconicmythological narratives with a view to identifying common and distinguishingquantitative features. Of the three narratives, an Anglo-Saxon and a Greek textare mostly believed by antiquarians to be partly historically based while thethird, an Irish epic, is often considered to be fictional. Here we show thatnetwork analysis is able to discriminate real from imaginary social networksand place mythological narratives on the spectrum between them. Moreover, theperceived artificiality of the Irish narrative can be traced back to anomalousfeatures associated with six characters. Considering these as amalgams ofseveral entities or proxies, renders the plausibility of the Irish textcomparable to the others from a network-theoretic point of view.
arxiv-900-44 | Empirical review of standard benchmark functions using evolutionary global optimization | http://arxiv.org/pdf/1207.4318v1.pdf | author:Johannes M. Dieterich, Bernd Hartke category:cs.NE published:2012-07-18 summary:We have employed a recent implementation of genetic algorithms to study arange of standard benchmark functions for global optimization. It turns outthat some of them are not very useful as challenging test functions, since theyneither allow for a discrimination between different variants of geneticoperators nor exhibit a dimensionality scaling resembling that of real-worldproblems, for example that of global structure optimization of atomic andmolecular clusters. The latter properties seem to be simulated better by twoother types of benchmark functions. One type is designed to be deceptive,exemplified here by Lunacek's function. The other type offers additionaladvantages of markedly increased complexity and of broad tunability in searchspace characteristics. For the latter type, we use an implementation based onrandomly distributed Gaussians. We advocate the use of the latter types of testfunctions for algorithm development and benchmarking.
arxiv-900-45 | Assessment of SAR Image Filtering using Adaptive Stack Filters | http://arxiv.org/pdf/1207.4308v1.pdf | author:Maria E. Buemi, Marta Mejail, Julio Jacobo, Alejandro C. Frery, Heitor S. Ramos category:cs.CV published:2012-07-18 summary:Stack filters are a special case of non-linear filters. They have a goodperformance for filtering images with different types of noise while preservingedges and details. A stack filter decomposes an input image into several binaryimages according to a set of thresholds. Each binary image is then filtered bya Boolean function, which characterizes the filter. Adaptive stack filters canbe designed to be optimal; they are computed from a pair of images consistingof an ideal noiseless image and its noisy version. In this work we study theperformance of adaptive stack filters when they are applied to SyntheticAperture Radar (SAR) images. This is done by evaluating the quality of thefiltered images through the use of suitable image quality indexes and bymeasuring the classification accuracy of the resulting images.
arxiv-900-46 | Frame Interpretation and Validation in a Open Domain Dialogue System | http://arxiv.org/pdf/1207.4307v1.pdf | author:Artur Ventura, Nuno Diegues, David Martins de Matos category:cs.CL cs.RO I.2.7; I.2.9 published:2012-07-18 summary:Our goal in this paper is to establish a means for a dialogue platform to beable to cope with open domains considering the possible interaction between theembodied agent and humans. To this end we present an algorithm capable ofprocessing natural language utterances and validate them against knowledgestructures of an intelligent agent's mind. Our algorithm leverages dialoguetechniques in order to solve ambiguities and acquire knowledge about unknownentities.
arxiv-900-47 | Expectation-Propagation for Likelihood-Free Inference | http://arxiv.org/pdf/1107.5959v2.pdf | author:Simon Barthelmé, Nicolas Chopin category:stat.CO stat.ML published:2011-07-29 summary:Many models of interest in the natural and social sciences have noclosed-form likelihood function, which means that they cannot be treated usingthe usual techniques of statistical inference. In the case where such modelscan be efficiently simulated, Bayesian inference is still possible thanks tothe Approximate Bayesian Computation (ABC) algorithm. Although many refinementshave been suggested, ABC inference is still far from routine. ABC is oftenexcruciatingly slow due to very low acceptance rates. In addition, ABC requiresintroducing a vector of "summary statistics", the choice of which is relativelyarbitrary, and often require some trial and error, making the whole processquite laborious for the user. We introduce in this work the EP-ABC algorithm, which is an adaptation to thelikelihood-free context of the variational approximation algorithm known asExpectation Propagation (Minka, 2001). The main advantage of EP-ABC is that itis faster by a few orders of magnitude than standard algorithms, whileproducing an overall approximation error which is typically negligible. Asecond advantage of EP-ABC is that it replaces the usual global ABC constrainton the vector of summary statistics computed on the whole dataset, by n localconstraints of the form that apply separately to each data-point. As aconsequence, it is often possible to do away with summary statistics entirely.In that case, EP-ABC approximates directly the evidence (marginal likelihood)of the model. Comparisons are performed in three real-world applications which are typicalof likelihood-free inference, including one application in neuroscience whichis novel, and possibly too challenging for standard ABC techniques.
arxiv-900-48 | Incremental Learning of 3D-DCT Compact Representations for Robust Visual Tracking | http://arxiv.org/pdf/1207.3389v2.pdf | author:Xi Li, Anthony Dick, Chunhua Shen, Anton van den Hengel, Hanzi Wang category:cs.CV cs.LG published:2012-07-14 summary:Visual tracking usually requires an object appearance model that is robust tochanging illumination, pose and other factors encountered in video. In thispaper, we construct an appearance model using the 3D discrete cosine transform(3D-DCT). The 3D-DCT is based on a set of cosine basis functions, which aredetermined by the dimensions of the 3D signal and thus independent of the inputvideo data. In addition, the 3D-DCT can generate a compact energy spectrumwhose high-frequency coefficients are sparse if the appearance samples aresimilar. By discarding these high-frequency coefficients, we simultaneouslyobtain a compact 3D-DCT based object representation and a signalreconstruction-based similarity measure (reflecting the information loss fromsignal reconstruction). To efficiently update the object representation, wepropose an incremental 3D-DCT algorithm, which decomposes the 3D-DCT intosuccessive operations of the 2D discrete cosine transform (2D-DCT) and 1Ddiscrete cosine transform (1D-DCT) on the input video data.
arxiv-900-49 | Content Based Multimedia Information Retrieval to Support Digital Libraries | http://arxiv.org/pdf/1207.4259v1.pdf | author:Mohammad Nabil Almunawar category:cs.IR cs.CV published:2012-07-18 summary:Content-based multimedia information retrieval is an interesting researcharea since it allows retrieval based on inherent characteristic of multimediaobjects. For example retrieval based on visual characteristics such as colour,shapes or textures of objects in images or retrieval based on spatialrelationships among objects in the media (images or video clips). This paperreviews some work done in image and video retrieval and then proposes anintegrated model that can handle images and video clips uniformly. Using thismodel retrieval on images or video clips can be done based on the sameframework.
arxiv-900-50 | A Two-Stage Combined Classifier in Scale Space Texture Classification | http://arxiv.org/pdf/1207.4089v1.pdf | author:Mehrdad J. Gangeh, Robert P. W. Duin, Bart M. ter Haar Romeny, Mohamed S. Kamel category:cs.CV cs.LG published:2012-07-17 summary:Textures often show multiscale properties and hence multiscale techniques areconsidered useful for texture analysis. Scale-space theory as a biologicallymotivated approach may be used to construct multiscale textures. In this papervarious ways are studied to combine features on different scales for textureclassification of small image patches. We use the N-jet of derivatives up tothe second order at different scales to generate distinct patternrepresentations (DPR) of feature subsets. Each feature subset in the DPR isgiven to a base classifier (BC) of a two-stage combined classifier. Thedecisions made by these BCs are combined in two stages over scales andderivatives. Various combining systems and their significances and differencesare discussed. The learning curves are used to evaluate the performances. Wefound for small sample sizes combining classifiers performs significantlybetter than combining feature spaces (CFS). It is also shown that combiningclassifiers performs better than the support vector machine on CFS inmultiscale texture classification.
arxiv-900-51 | Nonlinear Laplacian spectral analysis: Capturing intermittent and low-frequency spatiotemporal patterns in high-dimensional data | http://arxiv.org/pdf/1202.6103v2.pdf | author:Dimitrios Giannakis, Andrew J. Majda category:cs.LG published:2012-02-28 summary:We present a technique for spatiotemporal data analysis called nonlinearLaplacian spectral analysis (NLSA), which generalizes singular spectrumanalysis (SSA) to take into account the nonlinear manifold structure of complexdata sets. The key principle underlying NLSA is that the functions used torepresent temporal patterns should exhibit a degree of smoothness on thenonlinear data manifold M; a constraint absent from classical SSA. NLSAenforces such a notion of smoothness by requiring that temporal patterns belongin low-dimensional Hilbert spaces V_l spanned by the leading l Laplace-Beltramieigenfunctions on M. These eigenfunctions can be evaluated efficiently in highambient-space dimensions using sparse graph-theoretic algorithms. Moreover,they provide orthonormal bases to expand a family of linear maps, whosesingular value decomposition leads to sets of spatiotemporal patterns atprogressively finer resolution on the data manifold. The Riemannian measure ofM and an adaptive graph kernel width enhances the capability of NLSA to detectimportant nonlinear processes, including intermittency and rare events. Theminimum dimension of V_l required to capture these features while avoidingoverfitting is estimated here using spectral entropy criteria.
arxiv-900-52 | Computation of the Hausdorff distance between sets of line segments in parallel | http://arxiv.org/pdf/1207.3962v1.pdf | author:Helmut Alt, Ludmila Scharf category:cs.CG cs.CV cs.DC published:2012-07-17 summary:We show that the Hausdorff distance for two sets of non-intersecting linesegments can be computed in parallel in $O(\log^2 n)$ time using O(n)processors in a CREW-PRAM computation model. We discuss how some parts of thesequential algorithm can be performed in parallel using previously knownparallel algorithms; and identify the so-far unsolved part of the problem forthe parallel computation, which is the following: Given two sets of$x$-monotone curve segments, red and blue, for each red segment find itsextremal intersection points with the blue set, i.e. points with the minimaland maximal $x$-coordinate. Each segment set is assumed to be intersectionfree. For this intersection problem we describe a parallel algorithm whichcompletes the Hausdorff distance computation within the stated time andprocessor bounds.
arxiv-900-53 | Polarimetric SAR Image Segmentation with B-Splines and a New Statistical Model | http://arxiv.org/pdf/1207.3944v1.pdf | author:Alejandro C. Frery, Julio Jacobo-Berlles, Juliana Gambini, Marta Mejail category:cs.CV stat.ML published:2012-07-17 summary:We present an approach for polarimetric Synthetic Aperture Radar (SAR) imageregion boundary detection based on the use of B-Spline active contours and anew model for polarimetric SAR data: the GHP distribution. In order to detectthe boundary of a region, initial B-Spline curves are specified, eitherautomatically or manually, and the proposed algorithm uses a deformablecontours technique to find the boundary. In doing this, the parameters of thepolarimetric GHP model for the data are estimated, in order to find thetransition points between the region being segmented and the surrounding area.This is a local algorithm since it works only on the region to be segmented.Results of its performance are presented.
arxiv-900-54 | The representer theorem for Hilbert spaces: a necessary and sufficient condition | http://arxiv.org/pdf/1205.1928v3.pdf | author:Francesco Dinuzzo, Bernhard Schölkopf category:math.FA cs.LG published:2012-05-09 summary:A family of regularization functionals is said to admit a linear representertheorem if every member of the family admits minimizers that lie in a fixedfinite dimensional subspace. A recent characterization states that a generalclass of regularization functionals with differentiable regularizer admits alinear representer theorem if and only if the regularization term is anon-decreasing function of the norm. In this report, we improve over suchresult by replacing the differentiability assumption with lower semi-continuityand deriving a proof that is independent of the dimensionality of the space.
arxiv-900-55 | Automatic Segmentation of Manipuri (Meiteilon) Word into Syllabic Units | http://arxiv.org/pdf/1207.3932v1.pdf | author:Kishorjit Nongmeikapam, Vidya Raj RK, Oinam Imocha Singh, Sivaji Bandyopadhyay category:cs.CL I.2.7 published:2012-07-17 summary:The work of automatic segmentation of a Manipuri language (or Meiteilon) wordinto syllabic units is demonstrated in this paper. This language is a scheduledIndian language of Tibeto-Burman origin, which is also a very highlyagglutinative language. This language usages two script: a Bengali script andMeitei Mayek (Script). The present work is based on the second script. Analgorithm is designed so as to identify mainly the syllables of Manipuri originword. The result of the algorithm shows a Recall of 74.77, Precision of 91.21and F-Score of 82.18 which is a reasonable score with the first attempt of suchkind for this language.
arxiv-900-56 | Measurability Aspects of the Compactness Theorem for Sample Compression Schemes | http://arxiv.org/pdf/1205.5819v2.pdf | author:Damjan Kalajdzievski category:stat.ML cs.LG published:2012-05-25 summary:It was proved in 1998 by Ben-David and Litman that a concept space has asample compression scheme of size d if and only if every finite subspace has asample compression scheme of size d. In the compactness theorem, measurabilityof the hypotheses of the created sample compression scheme is not guaranteed;at the same time measurability of the hypotheses is a necessary condition forlearnability. In this thesis we discuss when a sample compression scheme,created from com- pression schemes on finite subspaces via the compactnesstheorem, have measurable hypotheses. We show that if X is a standard Borelspace with a d-maximum and universally separable concept class C, then (X,C)has a sample compression scheme of size d with universally Borel measurablehypotheses. Additionally we introduce a new variant of compression schemecalled a copy sample compression scheme.
arxiv-900-57 | Image Labeling on a Network: Using Social-Network Metadata for Image Classification | http://arxiv.org/pdf/1207.3809v1.pdf | author:Julian McAuley, Jure Leskovec category:cs.CV cs.SI physics.soc-ph published:2012-07-16 summary:Large-scale image retrieval benchmarks invariably consist of images from theWeb. Many of these benchmarks are derived from online photo sharing networks,like Flickr, which in addition to hosting images also provide a highlyinteractive social community. Such communities generate rich metadata that cannaturally be harnessed for image classification and retrieval. Here we studyfour popular benchmark datasets, extending them with social-network metadata,such as the groups to which each image belongs, the comment thread associatedwith the image, who uploaded it, their location, and their network of friends.Since these types of data are inherently relational, we propose a model thatexplicitly accounts for the interdependencies between images sharing commonproperties. We model the task as a binary labeling problem on a network, anduse structured learning techniques to learn model parameters. We find thatsocial-network metadata are useful in a variety of classification tasks, inmany cases outperforming methods based on image content.
arxiv-900-58 | Towards a Self-Organized Agent-Based Simulation Model for Exploration of Human Synaptic Connections | http://arxiv.org/pdf/1207.3760v1.pdf | author:Önder Gürcan, Carole Bernon, Kemal S. Türker category:cs.NE cs.AI cs.LG nlin.AO published:2012-07-16 summary:In this paper, the early design of our self-organized agent-based simulationmodel for exploration of synaptic connections that faithfully generates what isobserved in natural situation is given. While we take inspiration fromneuroscience, our intent is not to create a veridical model of processes inneurodevelopmental biology, nor to represent a real biological system. Instead,our goal is to design a simulation model that learns acting in the same way ofhuman nervous system by using findings on human subjects using reflexmethodologies in order to estimate unknown connections.
arxiv-900-59 | Morphological Filtering in Shape Spaces: Applications using Tree-Based Image Representations | http://arxiv.org/pdf/1204.4758v2.pdf | author:Yongchao Xu, Thierry Géraud, Laurent Najman category:cs.CV math.OA published:2012-04-20 summary:Connected operators are filtering tools that act by merging elementaryregions of an image. A popular strategy is based on tree-based imagerepresentations: for example, one can compute an attribute on each node of thetree and keep only the nodes for which the attribute is sufficiently strong.This operation can be seen as a thresholding of the tree, seen as a graph whosenodes are weighted by the attribute. Rather than being satisfied with a merethresholding, we propose to expand on this idea, and to apply connected filterson this latest graph. Consequently, the filtering is done not in the space ofthe image, but on the space of shapes build from the image. Such a processingis a generalization of the existing tree-based connected operators. Indeed, theframework includes classical existing connected operators by attributes. Italso allows us to propose a class of novel connected operators from theleveling family, based on shape attributes. Finally, we also propose a novelclass of self-dual connected operators that we call morphological shapings.
arxiv-900-60 | Ultrametric Model of Mind, II: Application to Text Content Analysis | http://arxiv.org/pdf/1201.2719v3.pdf | author:Fionn Murtagh category:cs.AI cs.CL 68T01 published:2012-01-13 summary:In a companion paper, Murtagh (2012), we discussed how Matte Blanco's worklinked the unrepressed unconscious (in the human) to symmetric logic andthought processes. We showed how ultrametric topology provides a most usefulrepresentational and computational framework for this. Now we look at theextent to which we can find ultrametricity in text. We use coherent andmeaningful collections of nearly 1000 texts to show how we can measure inherentultrametricity. On the basis of our findings we hypothesize that inherentultrametricty is a basis for further exploring unconscious thought processes.
arxiv-900-61 | Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood | http://arxiv.org/pdf/1207.3649v1.pdf | author:Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari category:stat.ML published:2012-07-16 summary:We consider probabilistic multinomial probit classification using Gaussianprocess (GP) priors. The challenges with the multiclass GP classification arethe integration over the non-Gaussian posterior distribution, and the increaseof the number of unknown latent variables as the number of target classesgrows. Expectation propagation (EP) has proven to be a very accurate method forapproximate inference but the existing EP approaches for the multinomial probitGP classification rely on numerical quadratures or independence assumptionsbetween the latent values from different classes to facilitate thecomputations. In this paper, we propose a novel nested EP approach which doesnot require numerical quadratures, and approximates accurately allbetween-class posterior dependencies of the latent values, but still scaleslinearly in the number of classes. The predictive accuracy of the nested EPapproach is compared to Laplace, variational Bayes, and Markov chain MonteCarlo (MCMC) approximations with various benchmark data sets. In theexperiments nested EP was the most consistent method with respect to MCMCsampling, but the differences between the compared methods were small if onlythe classification accuracy is concerned.
arxiv-900-62 | Fusing image representations for classification using support vector machines | http://arxiv.org/pdf/1207.3607v1.pdf | author:Can Demirkesen, Hocine Cherifi category:cs.CV cs.LG published:2012-07-16 summary:In order to improve classification accuracy different image representationsare usually combined. This can be done by using two different fusing schemes.In feature level fusion schemes, image representations are combined before theclassification process. In classifier fusion, the decisions taken separatelybased on individual representations are fused to make a decision. In this paperthe main methods derived for both strategies are evaluated. Our experimentalresults show that classifier fusion performs better. Specifically Bayes beliefintegration is the best performing strategy for image classification task.
arxiv-900-63 | Accuracy Measures for the Comparison of Classifiers | http://arxiv.org/pdf/1207.3790v1.pdf | author:Vincent Labatut, Hocine Cherifi category:cs.LG published:2012-07-16 summary:The selection of the best classification algorithm for a given dataset is avery widespread problem. It is also a complex one, in the sense it requires tomake several important methodological choices. Among them, in this work wefocus on the measure used to assess the classification performance and rank thealgorithms. We present the most popular measures and discuss their properties.Despite the numerous measures proposed over the years, many of them turn out tobe equivalent in this specific case, to have interpretation problems, or to beunsuitable for our purpose. Consequently, classic overall success rate ormarginal rates should be preferred for this specific task.
arxiv-900-64 | Qualitative Comparison of Community Detection Algorithms | http://arxiv.org/pdf/1207.3603v1.pdf | author:Günce Orman, Vincent Labatut, Hocine Cherifi category:cs.SI cs.CV physics.soc-ph published:2012-07-16 summary:Community detection is a very active field in complex networks analysis,consisting in identifying groups of nodes more densely interconnectedrelatively to the rest of the network. The existing algorithms are usuallytested and compared on real-world and artificial networks, their performancebeing assessed through some partition similarity measure. However, artificialnetworks realism can be questioned, and the appropriateness of those measuresis not obvious. In this study, we take advantage of recent advances concerningthe characterization of community structures to tackle these questions. Wefirst generate networks thanks to the most realistic model available to date.Their analysis reveals they display only some of the properties observed inreal-world community structures. We then apply five community detectionalgorithms on these networks and find out the performance assessedquantitatively does not necessarily agree with a qualitative analysis of theidentified communities. It therefore seems both approaches should be applied toperform a relevant comparison of the algorithms.
arxiv-900-65 | Autofocus Correction of Azimuth Phase Error and Residual Range Cell Migration in Spotlight SAR Polar Format Imagery | http://arxiv.org/pdf/1207.7245v1.pdf | author:Xinhua Mao, Daiyin Zhu, Zhaoda Zhu category:astro-ph.IM cs.CV published:2012-07-16 summary:Synthetic aperture radar (SAR) images are often blurred by phaseperturbations induced by uncompensated sensor motion and /or unknownpropagation effects caused by turbulent media. To get refocused images,autofocus proves to be useful post-processing technique applied to estimate andcompensate the unknown phase errors. However, a severe drawback of theconventional autofocus algorithms is that they are only capable of removingone-dimensional azimuth phase errors (APE). As the resolution becomes finer,residual range cell migration (RCM), which makes the defocus inherentlytwo-dimensional, becomes a new challenge. In this paper, correction of APE andresidual RCM are presented in the framework of polar format algorithm (PFA).First, an insight into the underlying mathematical mechanism of polarreformatting is presented. Then based on this new formulation, the effect ofpolar reformatting on the uncompensated APE and residual RCM is investigated indetail. By using the derived analytical relationship between APE and residualRCM, an efficient two-dimensional (2-D) autofocus method is proposed.Experimental results indicate the effectiveness of the proposed method.
arxiv-900-66 | Diagnosing client faults using SVM-based intelligent inference from TCP packet traces | http://arxiv.org/pdf/1207.3560v1.pdf | author:Chathuranga Widanapathirana, Y. Ahmet Sekercioglu, Paul G. Fitzpatrick, Milosh V. Ivanovich, Jonathan C. Li category:cs.NI cs.AI cs.LG published:2012-07-16 summary:We present the Intelligent Automated Client Diagnostic (IACD) system, whichonly relies on inference from Transmission Control Protocol (TCP) packet tracesfor rapid diagnosis of client device problems that cause network performanceissues. Using soft-margin Support Vector Machine (SVM) classifiers, the system(i) distinguishes link problems from client problems, and (ii) identifiescharacteristics unique to client faults to report the root cause of the clientdevice problem. Experimental evaluation demonstrated the capability of the IACDsystem to distinguish between faulty and healthy links and to diagnose theclient faults with 98% accuracy in healthy links. The system can perform faultdiagnosis independent of the client's specific TCP implementation, enablingdiagnosis capability on diverse range of client computers.
arxiv-900-67 | Improved brain pattern recovery through ranking approaches | http://arxiv.org/pdf/1207.3520v1.pdf | author:Fabian Pedregosa, Alexandre Gramfort, Gaël Varoquaux, Bertrand Thirion, Christophe Pallier, Elodie Cauvet category:cs.LG stat.ML published:2012-07-15 summary:Inferring the functional specificity of brain regions from functionalMagnetic Resonance Images (fMRI) data is a challenging statistical problem.While the General Linear Model (GLM) remains the standard approach for brainmapping, supervised learning techniques (a.k.a.} decoding) have proven to beuseful to capture multivariate statistical effects distributed across voxelsand brain regions. Up to now, much effort has been made to improve decoding byincorporating prior knowledge in the form of a particular regularization term.In this paper we demonstrate that further improvement can be made by accountingfor non-linearities using a ranking approach rather than the commonly usedleast-square regression. Through simulation, we compare the recovery propertiesof our approach to linear models commonly used in fMRI based decoding. Wedemonstrate the superiority of ranking with a real fMRI dataset.
arxiv-900-68 | Adversarial Evaluation for Models of Natural Language | http://arxiv.org/pdf/1207.0245v2.pdf | author:Noah A. Smith category:cs.CL published:2012-07-01 summary:We now have a rich and growing set of modeling tools and algorithms forinducing linguistic structure from text that is less than fully annotated. Inthis paper, we discuss some of the weaknesses of our current methodology. Wepresent a new abstract framework for evaluating natural language processing(NLP) models in general and unsupervised NLP models in particular. The centralidea is to make explicit certain adversarial roles among researchers, so thatthe different roles in an evaluation are more clearly defined and performers ofall roles are offered ways to make measurable contributions to the larger goal.Adopting this approach may help to characterize model successes and failures byencouraging earlier consideration of error analysis. The framework can beinstantiated in a variety of ways, simulating some familiar intrinsic andextrinsic evaluations as well as some new evaluations.
arxiv-900-69 | Distinct word length frequencies: distributions and symbol entropies | http://arxiv.org/pdf/1207.2334v2.pdf | author:Reginald D. Smith category:cs.CL published:2012-07-10 summary:The distribution of frequency counts of distinct words by length in alanguage's vocabulary will be analyzed using two methods. The first, will lookat the empirical distributions of several languages and derive a distributionthat reasonably explains the number of distinct words as a function of length.We will be able to derive the frequency count, mean word length, and varianceof word length based on the marginal probability of letters and spaces. Thesecond, based on information theory, will demonstrate that the conditionalentropies can also be used to estimate the frequency of distinct words of agiven length in a language. In addition, it will be shown how these techniquescan also be applied to estimate higher order entropies using vocabulary wordlength.
arxiv-900-70 | Optimization of Real, Hermitian Quadratic Forms: Real, Complex Hopfield-Amari Neural Network | http://arxiv.org/pdf/1206.5651v2.pdf | author:Garimella Ramamurthy, Bondalapati Nischal category:cs.NE published:2012-06-25 summary:In this research paper, the problem of optimization of quadratic formsassociated with the dynamics of Hopfield-Amari neural network is considered. Anelegant (and short) proof of the states at which local/global minima ofquadratic form are attained is provided. A theorem associated with local/globalminimization of quadratic energy function using the Hopfield-Amari neuralnetwork is discussed. The results are generalized to a "Complex Hopfield neuralnetwork" dynamics over the complex hypercube (using a "complex signumfunction"). It is also reasoned through two theorems that there is no loss ofgenerality in assuming the threshold vector to be a zero vector in the case ofreal as well as a "Complex Hopfield neural network". Some structured quadraticforms like Toeplitz form and Complex Toeplitz form are discussed.
arxiv-900-71 | Approximated Computation of Belief Functions for Robust Design Optimization | http://arxiv.org/pdf/1207.3442v1.pdf | author:Massimiliano Vasile, Edmondo Minisci, Quirien Wijnands category:cs.CE cs.NE cs.SY math.OC math.PR published:2012-07-14 summary:This paper presents some ideas to reduce the computational cost ofevidence-based robust design optimization. Evidence Theory crystallizes boththe aleatory and epistemic uncertainties in the design parameters, providingtwo quantitative measures, Belief and Plausibility, of the credibility of thecomputed value of the design budgets. The paper proposes some techniques tocompute an approximation of Belief and Plausibility at a cost that is afraction of the one required for an accurate calculation of the two values.Some simple test cases will show how the proposed techniques scale with thedimension of the problem. Finally a simple example of spacecraft system designis presented.
arxiv-900-72 | MahNMF: Manhattan Non-negative Matrix Factorization | http://arxiv.org/pdf/1207.3438v1.pdf | author:Naiyang Guan, Dacheng Tao, Zhigang Luo, John Shawe-Taylor category:stat.ML cs.LG cs.NA 65K10 published:2012-07-14 summary:Non-negative matrix factorization (NMF) approximates a non-negative matrix$X$ by a product of two non-negative low-rank factor matrices $W$ and $H$. NMFand its extensions minimize either the Kullback-Leibler divergence or theEuclidean distance between $X$ and $W^T H$ to model the Poisson noise or theGaussian noise. In practice, when the noise distribution is heavy tailed, theycannot perform well. This paper presents Manhattan NMF (MahNMF) which minimizesthe Manhattan distance between $X$ and $W^T H$ for modeling the heavy tailedLaplacian noise. Similar to sparse and low-rank matrix decompositions, MahNMFrobustly estimates the low-rank part and the sparse part of a non-negativematrix and thus performs effectively when data are contaminated by outliers. Weextend MahNMF for various practical applications by developing box-constrainedMahNMF, manifold regularized MahNMF, group sparse MahNMF, elastic net inducingMahNMF, and symmetric MahNMF. The major contribution of this paper lies in twofast optimization algorithms for MahNMF and its extensions: the rank-oneresidual iteration (RRI) method and Nesterov's smoothing method. In particular,by approximating the residual matrix by the outer product of one row of W andone row of $H$ in MahNMF, we develop an RRI method to iteratively update eachvariable of $W$ and $H$ in a closed form solution. Although RRI is efficientfor small scale MahNMF and some of its extensions, it is neither scalable tolarge scale matrices nor flexible enough to optimize all MahNMF extensions.Since the objective functions of MahNMF and its extensions are neither convexnor smooth, we apply Nesterov's smoothing method to recursively optimize onefactor matrix with another matrix fixed. By setting the smoothing parameterinversely proportional to the iteration number, we improve the approximationaccuracy iteratively for both MahNMF and its extensions.
arxiv-900-73 | Robust Mission Design Through Evidence Theory and Multi-Agent Collaborative Search | http://arxiv.org/pdf/1207.3437v1.pdf | author:Massimiliano Vasile category:cs.CE cs.NE cs.SY math.OC math.PR published:2012-07-14 summary:In this paper, the preliminary design of a space mission is approachedintroducing uncertainties on the design parameters and formulating theresulting reliable design problem as a multiobjective optimization problem.Uncertainties are modelled through evidence theory and the belief, orcredibility, in the successful achievement of mission goals is maximised alongwith the reliability of constraint satisfaction. The multiobjectiveoptimisation problem is solved through a novel algorithm based on thecollaboration of a population of agents in search for the set of highlyreliable solutions. Two typical problems in mission analysis are used toillustrate the proposed methodology.
arxiv-900-74 | Split HMC for Gaussian Process Models | http://arxiv.org/pdf/1201.3973v2.pdf | author:Shiwei Lan, Babak Shahbaba category:stat.CO stat.ML published:2012-01-19 summary:In this paper, we discuss an extension of the Split Hamiltonian Monte Carlo(Split HMC) method for Gaussian process model (GPM). This method is based onsplitting the Hamiltonian in a way that allows much of the movement around thestate space to be done at low computational cost. To this end, we approximatethe negative log density (i.e., the energy function) of the distribution ofinterest by a quadratic function U0 for which Hamiltonian dynamics can besolved analytically. The overall energy function U is then written as U0 + U1,where U1 is the approximation error. The Hamiltonian is then split into twoparts; one part is based on U0 is handled analytically, the other part is basedon U1 for which we approximate Hamiltonian's equations by discretizing time. Weuse simulated and real data to compare the performance of our method to thestandard HMC. We find that splitting the Hamiltonian for GP models could leadto substantial improvement (up to 10 folds) of sampling efficiency, which ismeasured in terms of the amount of time required for producing an independentsample with high acceptance probability from posterior distributions.
arxiv-900-75 | Dimension Reduction by Mutual Information Feature Extraction | http://arxiv.org/pdf/1207.3394v1.pdf | author:Ali Shadvar category:cs.LG cs.CV published:2012-07-14 summary:During the past decades, to study high-dimensional data in a large variety ofproblems, researchers have proposed many Feature Extraction algorithms. One ofthe most effective approaches for optimal feature extraction is based on mutualinformation (MI). However it is not always easy to get an accurate estimationfor high dimensional MI. In terms of MI, the optimal feature extraction iscreating a feature set from the data which jointly have the largest dependencyon the target class and minimum redundancy. In this paper, acomponent-by-component gradient ascent method is proposed for featureextraction which is based on one-dimensional MI estimates. We will refer tothis algorithm as Mutual Information Feature Extraction (MIFX). The performanceof this proposed method is evaluated using UCI databases. The results indicatethat MIFX provides a robust performance over different data sets which arealmost always the best or comparable to the best ones.
arxiv-900-76 | Deconvolution of vibroacoustic images using a simulation model based on a three dimensional point spread function | http://arxiv.org/pdf/1207.3370v1.pdf | author:Talita Perciano, Matthew Urban, Nelson D. A. Mascarenhas, Mostafa Fatemi, Alejandro C. Frery, Glauber T. Silva category:cs.CV published:2012-07-13 summary:Vibro-acoustography (VA) is a medical imaging method based on thedifference-frequency generation produced by the mixture of two focusedultrasound beams. VA has been applied to different problems in medical imagingsuch as imaging bones, microcalcifications in the breast, mass lesions, andcalcified arteries. The obtained images may have a resolution of 0.7--0.8 mm.Current VA systems based on confocal or linear array transducers generateC-scan images at the beam focal plane. Images on the axial plane are alsopossible, however the system resolution along depth worsens when compared tothe lateral one. Typical axial resolution is about 1.0 cm. Furthermore, theelevation resolution of linear array systems is larger than that in lateraldirection. This asymmetry degrades C-scan images obtained using linear arrays.The purpose of this article is to study VA image restoration based on a 3Dpoint spread function (PSF) using classical deconvolution algorithms: Wiener,constrained least-squares (CLSs), and geometric mean filters. To assess thefilters' performance, we use an image quality index that accounts forcorrelation loss, luminance and contrast distortion. Results for simulated VAimages show that the quality index achieved with the Wiener filter is 0.9 (1indicates perfect restoration). This filter yielded the best result incomparison with the other ones. Moreover, the deconvolution algorithms wereapplied to an experimental VA image of a phantom composed of three stretched0.5 mm wires. Experiments were performed using transducer driven at twofrequencies, 3075 kHz and 3125 kHz, which resulted in the difference-frequencyof 50 kHz. Restorations with the theoretical line spread function (LSF) did notrecover sufficient information to identify the wires in the images. However,using an estimated LSF the obtained results displayed enough information tospot the wires in the images.
arxiv-900-77 | Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers | http://arxiv.org/pdf/1202.2160v2.pdf | author:Clément Farabet, Camille Couprie, Laurent Najman, Yann LeCun category:cs.CV cs.LG published:2012-02-10 summary:Scene parsing, or semantic segmentation, consists in labeling each pixel inan image with the category of the object it belongs to. It is a challengingtask that involves the simultaneous detection, segmentation and recognition ofall the objects in the image. The scene parsing method proposed here starts by computing a tree of segmentsfrom a graph of pixel dissimilarities. Simultaneously, a set of dense featurevectors is computed which encodes regions of multiple sizes centered on eachpixel. The feature extractor is a multiscale convolutional network trained fromraw pixels. The feature vectors associated with the segments covered by eachnode in the tree are aggregated and fed to a classifier which produces anestimate of the distribution of object categories contained in the segment. Asubset of tree nodes that cover the image are then selected so as to maximizethe average "purity" of the class distributions, hence maximizing the overalllikelihood that each segment will contain a single object. The convolutionalnetwork feature extractor is trained end-to-end from raw pixels, alleviatingthe need for engineered features. After training, the system is parameter free. The system yields record accuracies on the Stanford Background Dataset (8classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170classes) while being an order of magnitude faster than competing approaches,producing a 320 \times 240 image labeling in less than 1 second.
arxiv-900-78 | Learning the Pseudoinverse Solution to Network Weights | http://arxiv.org/pdf/1207.3368v1.pdf | author:Jonathan Tapson, Andre van Schaik category:cs.NE published:2012-07-13 summary:The last decade has seen the parallel emergence in computational neuroscienceand machine learning of neural network structures which spread the input signalrandomly to a higher dimensional space; perform a nonlinear activation; andthen solve for a regression or classification output by means of a mathematicalpseudoinverse operation. In the field of neuromorphic engineering, thesemethods are increasingly popular for synthesizing biologically plausible neuralnetworks, but the "learning method" - computation of the pseudoinverse bysingular value decomposition - is problematic both for biological plausibilityand because it is not an online or an adaptive method. We present an online orincremental method of computing the pseudoinverse, which we argue isbiologically plausible as a learning method, and which can be made adaptablefor non-stationary data streams. The method is significantly morememory-efficient than the conventional computation of pseudoinverses bysingular value decomposition.
arxiv-900-79 | Comparative Study for Inference of Hidden Classes in Stochastic Block Models | http://arxiv.org/pdf/1207.2328v2.pdf | author:Pan Zhang, Florent Krzakala, Jörg Reichardt, Lenka Zdeborová category:cs.LG stat.ML published:2012-07-10 summary:Inference of hidden classes in stochastic block model is a classical problemwith important applications. Most commonly used methods for this probleminvolve na\"{\i}ve mean field approaches or heuristic spectral methods.Recently, belief propagation was proposed for this problem. In thiscontribution we perform a comparative study between the three methods onsynthetically created networks. We show that belief propagation shows muchbetter performance when compared to na\"{\i}ve mean field and spectralapproaches. This applies to accuracy, computational efficiency and the tendencyto overfit the data.
arxiv-900-80 | Tracking Tetrahymena Pyriformis Cells using Decision Trees | http://arxiv.org/pdf/1207.3127v1.pdf | author:Quan Wang, Yan Ou, A. Agung Julius, Kim L. Boyer, Min Jun Kim category:cs.CV published:2012-07-13 summary:Matching cells over time has long been the most difficult step in celltracking. In this paper, we approach this problem by recasting it as aclassification problem. We construct a feature set for each cell, and compute afeature difference vector between a cell in the current frame and a cell in aprevious frame. Then we determine whether the two cells represent the same cellover time by training decision trees as our binary classifiers. With the outputof decision trees, we are able to formulate an assignment problem for our cellassociation task and solve it using a modified version of the Hungarianalgorithm.
arxiv-900-81 | A Hierarchical Graphical Model for Record Linkage | http://arxiv.org/pdf/1207.4180v1.pdf | author:Pradeep Ravikumar, William Cohen category:cs.LG cs.IR stat.ML published:2012-07-12 summary:The task of matching co-referent records is known among other names as rocordlinkage. For large record-linkage problems, often there is little or no labeleddata available, but unlabeled data shows a reasonable clear structure. For suchproblems, unsupervised or semi-supervised methods are preferable to supervisedmethods. In this paper, we describe a hierarchical graphical model frameworkfor the linakge-problem in an unsupervised setting. In addition to proposingnew methods, we also cast existing unsupervised probabilistic record-linkagemethods in this framework. Some of the techniques we propose to minimizeoverfitting in the above model are of interest in the general graphical modelsetting. We describe a method for incorporating monotinicity constraints in agraphical model. We also outline a bootstrapping approach of using"single-field" classifiers to noisily label latent variables in a hierarchicalmodel. Experimental results show that our proposed unsupervised methods performquite competitively even with fully supervised record-linkage methods.
arxiv-900-82 | Probabilistic index maps for modeling natural signals | http://arxiv.org/pdf/1207.4179v1.pdf | author:Nebojsa Jojic, Yaron Caspi, Manuel Reyes-Gomez category:cs.CV published:2012-07-12 summary:One of the major problems in modeling natural signals is that signals withvery similar structure may locally have completely different measurements,e.g., images taken under different illumination conditions, or the speechsignal captured in different environments. While there have been manysuccessful attempts to address these problems in application-specific settings,we believe that underlying a large set of problems in signal representation isa representational deficiency of intensity-derived local measurements that arethe basis of most efficient models. We argue that interesting structure insignals is better captured when the signal is de- fined as a matrix whoseentries are discrete indices to a separate palette of possible measurements. Inorder to model the variability in signal structure, we define a signal classnot by a single index map, but by a probability distribution over the indexmaps, which can be estimated from the data, and which we call probabilisticindex maps. The existing algorithm can be adapted to work with thisrepresentation. Furthermore, the probabilistic index map representation leadsto algorithms with computational costs proportional to either the size of thepalette or the log of the size of the palette, making the cost of significantlyincreased invariance to non-structural changes quite bearable. We illustratethe benefits of the probabilistic index map representation in severalapplications in computer vision and speech processing.
arxiv-900-83 | Biogeography-Based Informative Gene Selection and Cancer Classification Using SVM and Random Forests | http://arxiv.org/pdf/1207.3285v1.pdf | author:Sarvesh Nikumbh, Shameek Ghosh, Valadi Jayaraman category:cs.NE stat.ML published:2012-07-12 summary:Microarray cancer gene expression data comprise of very high dimensions.Reducing the dimensions helps in improving the overall analysis andclassification performance. We propose two hybrid techniques, Biogeography -based Optimization - Random Forests (BBO - RF) and BBO - SVM (Support VectorMachines) with gene ranking as a heuristic, for microarray gene expressionanalysis. This heuristic is obtained from information gain filter rankingprocedure. The BBO algorithm generates a population of candidate subset ofgenes, as part of an ecosystem of habitats, and employs the migration andmutation processes across multiple generations of the population to improve theclassification accuracy. The fitness of each gene subset is assessed by theclassifiers - SVM and Random Forests. The performances of these hybridtechniques are evaluated on three cancer gene expression datasets retrievedfrom the Kent Ridge Biomedical datasets collection and the libSVM datarepository. Our results demonstrate that genes selected by the proposedtechniques yield classification accuracies comparable to previously reportedalgorithms.
arxiv-900-84 | Detecting Activations over Graphs using Spanning Tree Wavelet Bases | http://arxiv.org/pdf/1206.0937v3.pdf | author:James Sharpnack, Akshay Krishnamurthy, Aarti Singh category:stat.ML cs.IT math.IT math.ST stat.TH published:2012-06-05 summary:We consider the detection of activations over graphs under Gaussian noise,where signals are piece-wise constant over the graph. Despite the wideapplicability of such a detection algorithm, there has been little success inthe development of computationally feasible methods with proveable theoreticalguarantees for general graph topologies. We cast this as a hypothesis testingproblem, and first provide a universal necessary condition for asymptoticdistinguishability of the null and alternative hypotheses. We then introducethe spanning tree wavelet basis over graphs, a localized basis that reflectsthe topology of the graph, and prove that for any spanning tree, this approachcan distinguish null from alternative in a low signal-to-noise regime. Lastly,we improve on this result and show that using the uniform spanning tree in thebasis construction yields a randomized test with stronger theoreticalguarantees that in many cases matches our necessary conditions. Specifically,we obtain near-optimal performance in edge transitive graphs, $k$-nearestneighbor graphs, and $\epsilon$-graphs.
arxiv-900-85 | Hypothesis Testing in Speckled Data with Stochastic Distances | http://arxiv.org/pdf/1207.2959v1.pdf | author:Abraão D. C. Nascimento, Renato J. Cintra, Alejandro C. Frery category:stat.ML cs.GR published:2012-07-12 summary:Images obtained with coherent illumination, as is the case of sonar,ultrasound-B, laser and Synthetic Aperture Radar -- SAR, are affected byspeckle noise which reduces the ability to extract information from the data.Specialized techniques are required to deal with such imagery, which has beenmodeled by the G0 distribution and under which regions with different degreesof roughness and mean brightness can be characterized by two parameters; athird parameter, the number of looks, is related to the overall signal-to-noiseratio. Assessing distances between samples is an important step in imageanalysis; they provide grounds of the separability and, therefore, of theperformance of classification procedures. This work derives and compares eightstochastic distances and assesses the performance of hypothesis tests thatemploy them and maximum likelihood estimation. We conclude that tests based onthe triangular distance have the closest empirical size to the theoretical one,while those based on the arithmetic-geometric distances have the best power.Since the power of tests based on the triangular distance is close to optimum,we conclude that the safest choice is using this distance for hypothesistesting, even when compared with classical distances as Kullback-Leibler andBhattacharyya.
arxiv-900-86 | ROI Segmentation for Feature Extraction from Human Facial Images | http://arxiv.org/pdf/1207.2922v1.pdf | author:Surbhi, Vishal Arora category:cs.CV cs.HC published:2012-07-12 summary:Human Computer Interaction (HCI) is the biggest goal of computer visionresearchers. Features form the different facial images are able to provide avery deep knowledge about the activities performed by the different facialmovements. In this paper we presented a technique for feature extraction fromvarious regions of interest with the help of Skin color segmentation technique,Thresholding, knowledge based technique for face recognition.
arxiv-900-87 | Camera identification by grouping images from database, based on shared noise patterns | http://arxiv.org/pdf/1207.2641v2.pdf | author:Teun Baar, Wiger van Houten, Zeno Geradts category:cs.CV published:2012-07-11 summary:Previous research showed that camera specific noise patterns, so-calledPRNU-patterns, are extracted from images and related images could be found. Inthis particular research the focus is on grouping images from a database, basedon a shared noise pattern as an identification method for cameras. Using themethod as described in this article, groups of images, created using the samecamera, could be linked from a large database of images. Using MATLABprogramming, relevant image noise patterns are extracted from images muchquicker than common methods by the use of faster noise extraction filters andimprovements to reduce the calculation costs. Relating noise patterns, with acorrelation above a certain threshold value, can quickly be matched. Hereby,from a database of images, groups of relating images could be linked and themethod could be used to scan a large number of images for suspect noisepatterns.
arxiv-900-88 | Building high-level features using large scale unsupervised learning | http://arxiv.org/pdf/1112.6209v5.pdf | author:Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng category:cs.LG published:2011-12-29 summary:We consider the problem of building high- level, class-specific featuredetectors from only unlabeled data. For example, is it possible to learn a facedetector using only unlabeled images? To answer this, we train a 9-layeredlocally connected sparse autoencoder with pooling and local contrastnormalization on a large dataset of images (the model has 1 bil- lionconnections, the dataset has 10 million 200x200 pixel images downloaded fromthe Internet). We train this network using model parallelism and asynchronousSGD on a clus- ter with 1,000 machines (16,000 cores) for three days. Contraryto what appears to be a widely-held intuition, our experimental re- sultsreveal that it is possible to train a face detector without having to labelimages as containing a face or not. Control experiments show that this featuredetector is robust not only to translation but also to scaling and out-of-planerotation. We also find that the same network is sensitive to other high-levelconcepts such as cat faces and human bod- ies. Starting with these learnedfeatures, we trained our network to obtain 15.8% accu- racy in recognizing20,000 object categories from ImageNet, a leap of 70% relative im- provementover the previous state-of-the-art.
arxiv-900-89 | Clustering based approach extracting collocations | http://arxiv.org/pdf/1207.2714v1.pdf | author:Mohamed Achraf Ben Mohamed, Mounir Zrigui, Mohsen Maraoui category:cs.CL published:2012-07-11 summary:The following study presents a collocation extraction approach based onclustering technique. This study uses a combination of several classicalmeasures which cover all aspects of a given corpus then it suggests separatingbigrams found in the corpus in several disjoint groups according to theprobability of presence of collocations. This will allow excluding groups wherethe presence of collocations is very unlikely and thus reducing in a meaningfulway the search space.
arxiv-900-90 | Genetic agent approach for improving on-the-fly web map generalization | http://arxiv.org/pdf/1207.2697v1.pdf | author:Brahim lejdel, Okba kazar category:cs.MA cs.CG cs.NE published:2012-07-11 summary:The utilization of web mapping becomes increasingly important in the domainof cartography. Users want access to spatial data on the web specific to theirneeds. For this reason, different approaches were appeared for generatingon-the-fly the maps demanded by users, but those not suffice for guide aflexible and efficient process. Thus, new approach must be developed forimproving this process according to the user needs. This work focuses ondefining a new strategy which improves on-the-fly map generalization processand resolves the spatial conflicts. This approach uses the multiplerepresentation and cartographic generalization. The map generalization processis based on the implementation of multi- agent system where each agent wasequipped with a genetic patrimony.
arxiv-900-91 | Variational Chernoff Bounds for Graphical Models | http://arxiv.org/pdf/1207.4172v1.pdf | author:Pradeep Ravikumar, John Lafferty category:cs.LG stat.ML published:2012-07-11 summary:Recent research has made significant progress on the problem of bounding logpartition functions for exponential family graphical models. Such bounds haveassociated dual parameters that are often used as heuristic estimates of themarginal probabilities required in inference and learning. However thesevariational estimates do not give rigorous bounds on marginal probabilities,nor do they give estimates for probabilities of more general events than simplemarginals. In this paper we build on this recent work by deriving rigorousupper and lower bounds on event probabilities for graphical models. Ourapproach is based on the use of generalized Chernoff bounds to express boundson event probabilities in terms of convex optimization problems; theseoptimization problems, in turn, require estimates of generalized log partitionfunctions. Simulations indicate that this technique can result in useful,rigorous bounds to complement the heuristic variational estimates, withcomparable computational cost.
arxiv-900-92 | The Author-Topic Model for Authors and Documents | http://arxiv.org/pdf/1207.4169v1.pdf | author:Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, Padhraic Smyth category:cs.IR cs.LG stat.ML published:2012-07-11 summary:We introduce the author-topic model, a generative model for documents thatextends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to includeauthorship information. Each author is associated with a multinomialdistribution over topics and each topic is associated with a multinomialdistribution over words. A document with multiple authors is modeled as adistribution over topics that is a mixture of the distributions associated withthe authors. We apply the model to a collection of 1,700 NIPS conference papersand 160,000 CiteSeer abstracts. Exact inference is intractable for thesedatasets and we use Gibbs sampling to estimate the topic and authordistributions. We compare the performance with two other generative models fordocuments, which are special cases of the author-topic model: LDA (a topicmodel) and a simple author model in which each author is associated with adistribution over words rather than a distribution over topics. We show topicsrecovered by the author-topic model, and demonstrate applications to computingsimilarity between authors and entropy of author output.
arxiv-900-93 | Predictive State Representations: A New Theory for Modeling Dynamical Systems | http://arxiv.org/pdf/1207.4167v1.pdf | author:Satinder Singh, Michael James, Matthew Rudary category:cs.AI cs.LG published:2012-07-11 summary:Modeling dynamical systems, both for control purposes and to make predictionsabout their behavior, is ubiquitous in science and engineering. Predictivestate representations (PSRs) are a recently introduced class of models fordiscrete-time dynamical systems. The key idea behind PSRs and the closelyrelated OOMs (Jaeger's observable operator models) is to represent the state ofthe system as a set of predictions of observable outcomes of experiments onecan do in the system. This makes PSRs rather different from history-basedmodels such as nth-order Markov models and hidden-state-based models such asHMMs and POMDPs. We introduce an interesting construct, the systemdynamicsmatrix, and show how PSRs can be derived simply from it. We also use thisconstruct to show formally that PSRs are more general than both nth-orderMarkov models and HMMs/POMDPs. Finally, we discuss the main difference betweenPSRs and OOMs and conclude with directions for future work.
arxiv-900-94 | Factored Latent Analysis for far-field tracking data | http://arxiv.org/pdf/1207.4164v1.pdf | author:Chris Stauffer category:cs.LG stat.ML published:2012-07-11 summary:This paper uses Factored Latent Analysis (FLA) to learn a factorized,segmental representation for observations of tracked objects over time.Factored Latent Analysis is latent class analysis in which the observationspace is subdivided and each aspect of the original space is represented by aseparate latent class model. One could simply treat these factors as completelyindependent and ignore their interdependencies or one could concatenate themtogether and attempt to learn latent class structure for the completeobservation space. Alternatively, FLA allows the interdependencies to beexploited in estimating an effective model, which is also capable ofrepresenting a factored latent state. In this paper, FLA is used to learn a setof factored latent classes to represent different modalities of observations oftracked objects. Different characteristics of the state of tracked objects areeach represented by separate latent class models, including normalized size,normalized speed, normalized direction, and position. This model also enableseffective temporal segmentation of these sequences. This method is data-driven,unsupervised using only pairwise observation statistics. This data-driven andunsupervised activity classi- fication technique exhibits good performance inmultiple challenging environments.
arxiv-900-95 | On the Choice of Regions for Generalized Belief Propagation | http://arxiv.org/pdf/1207.4158v1.pdf | author:Max Welling category:cs.AI cs.LG published:2012-07-11 summary:Generalized belief propagation (GBP) has proven to be a promising techniquefor approximate inference tasks in AI and machine learning. However, the choiceof a good set of clusters to be used in GBP has remained more of an art then ascience until this day. This paper proposes a sequential approach to adding newclusters of nodes and their interactions (i.e. "regions") to the approximation.We first review and analyze the recently introduced region graphs and find thatthree kinds of operations ("split", "merge" and "death") leave the free energyand (under some conditions) the fixed points of GBP invariant. This leads tothe notion of "weakly irreducible" regions as the natural candidates to beadded to the approximation. Computational complexity of the GBP algorithm iscontrolled by restricting attention to regions with small "region-width".Combining the above with an efficient (i.e. local in the graph) measure topredict the improved accuracy of GBP leads to the sequential "region pursuit"algorithm for adding new regions bottom-up to the region graph. Experimentsshow that this algorithm can indeed perform close to optimally.
arxiv-900-96 | An Integrated, Conditional Model of Information Extraction and Coreference with Applications to Citation Matching | http://arxiv.org/pdf/1207.4157v1.pdf | author:Ben Wellner, Andrew McCallum, Fuchun Peng, Michael Hay category:cs.LG cs.DL cs.IR stat.ML published:2012-07-11 summary:Although information extraction and coreference resolution appear together inmany applications, most current systems perform them as ndependent steps. Thispaper describes an approach to integrated inference for extraction andcoreference based on conditionally-trained undirected graphical models. Wediscuss the advantages of conditional probability training, and of acoreference model structure based on graph partitioning. On a data set ofresearch paper citations, we show significant reduction in error by usingextraction uncertainty to improve coreference citation matching accuracy, andusing coreference to improve the accuracy of the extracted fields.
arxiv-900-97 | Graph partition strategies for generalized mean field inference | http://arxiv.org/pdf/1207.4156v1.pdf | author:Eric P. Xing, Michael I. Jordan, Stuart Russell category:cs.LG stat.ML published:2012-07-11 summary:An autonomous variational inference algorithm for arbitrary graphical modelsrequires the ability to optimize variational approximations over the space ofmodel parameters as well as over the choice of tractable families used for thevariational approximation. In this paper, we present a novel combination ofgraph partitioning algorithms with a generalized mean field (GMF) inferencealgorithm. This combination optimizes over disjoint clustering of variables andperforms inference using those clusters. We provide a formal analysis of therelationship between the graph cut and the GMF approximation, and exploreseveral graph partition strategies empirically. Our empirical results providerather clear support for a weighted version of MinCut as a useful clusteringalgorithm for GMF inference, which is consistent with the implications from theformal analysis.
arxiv-900-98 | Similarity-Driven Cluster Merging Method for Unsupervised Fuzzy Clustering | http://arxiv.org/pdf/1207.4155v1.pdf | author:Xuejian Xiong, Kap Chan, Kian Lee Tan category:cs.LG stat.ML published:2012-07-11 summary:In this paper, a similarity-driven cluster merging method is proposed forunsuper-vised fuzzy clustering. The cluster merging method is used to resolvethe problem of cluster validation. Starting with an overspecified number ofclusters in the data, pairs of similar clusters are merged based on theproposed similarity-driven cluster merging criterion. The similarity betweenclusters is calculated by a fuzzy cluster similarity matrix, while an adaptivethreshold is used for merging. In addition, a modified generalized ob- jectivefunction is used for prototype-based fuzzy clustering. The function includesthe p-norm distance measure as well as principal components of the clusters.The number of the principal components is determined automatically from thedata being clustered. The properties of this unsupervised fuzzy clusteringalgorithm are illustrated by several experiments.
arxiv-900-99 | Maximum Entropy for Collaborative Filtering | http://arxiv.org/pdf/1207.4152v1.pdf | author:Lawrence Zitnick, Takeo Kanade category:cs.IR cs.LG published:2012-07-11 summary:Within the task of collaborative filtering two challenges for computingconditional probabilities exist. First, the amount of training data availableis typically sparse with respect to the size of the domain. Thus, support forhigher-order interactions is generally not present. Second, the variables thatwe are conditioning upon vary for each query. That is, users label differentvariables during each query. For this reason, there is no consistent input tooutput mapping. To address these problems we purpose a maximum entropy approachusing a non-standard measure of entropy. This approach can be simplified tosolving a set of linear equations that can be efficiently solved.
arxiv-900-100 | PAC-learning bounded tree-width Graphical Models | http://arxiv.org/pdf/1207.4151v1.pdf | author:Mukund Narasimhan, Jeff A. Bilmes category:cs.LG cs.DS stat.ML published:2012-07-11 summary:We show that the class of strongly connected graphical models with treewidthat most k can be properly efficiently PAC-learnt with respect to theKullback-Leibler Divergence. Previous approaches to this problem, such as thoseof Chow ([1]), and Ho gen ([7]) have shown that this class is PAC-learnable byreducing it to a combinatorial optimization problem. However, for k > 1, thisproblem is NP-complete ([15]), and so unless P=NP, these approaches will takeexponential amounts of time. Our approach differs significantly from these, inthat it first attempts to find approximate conditional independencies bysolving (polynomially many) submodular optimization problems, and then using adynamic programming formulation to combine the approximate conditionalindependence information to derive a graphical model with underlying graph ofthe tree-width specified. This gives us an efficient (polynomial time in thenumber of random variables) PAC-learning algorithm which requires onlypolynomial number of samples of the true distribution, and only polynomialrunning time.
arxiv-900-101 | From Fields to Trees | http://arxiv.org/pdf/1207.4149v1.pdf | author:Firas Hamze, Nando de Freitas category:stat.CO cs.LG published:2012-07-11 summary:We present new MCMC algorithms for computing the posterior distributions andexpectations of the unknown variables in undirected graphical models withregular structure. For demonstration purposes, we focus on Markov Random Fields(MRFs). By partitioning the MRFs into non-overlapping trees, it is possible tocompute the posterior distribution of a particular tree exactly by conditioningon the remaining tree. These exact solutions allow us to construct efficientblocked and Rao-Blackwellised MCMC algorithms. We show empirically that treesampling is considerably more efficient than other partitioned sampling schemesand the naive Gibbs sampler, even in cases where loopy belief propagation failsto converge. We prove that tree sampling exhibits lower variance than the naiveGibbs sampler and other naive partitioning schemes using the theoreticalmeasure of maximal correlation. We also construct new information theory toolsfor comparing different MCMC schemes and show that, under these, tree samplingis more efficient.
arxiv-900-102 | Dynamical Systems Trees | http://arxiv.org/pdf/1207.4148v1.pdf | author:Andrew Howard, Tony S. Jebara category:cs.LG stat.ML published:2012-07-11 summary:We propose dynamical systems trees (DSTs) as a flexible class of models fordescribing multiple processes that interact via a hierarchy of aggregatingparent chains. DSTs extend Kalman filters, hidden Markov models and nonlineardynamical systems to an interactive group scenario. Various individualprocesses interact as communities and sub-communities in a tree structure thatis unrolled in time. To accommodate nonlinear temporal activity, eachindividual leaf process is modeled as a dynamical system containing discreteand/or continuous hidden states with discrete and/or Gaussian emissions.Subsequent higher level parent processes act like hidden Markov models andmediate the interaction between leaf processes or between other parentprocesses in the hierarchy. Aggregator chains are parents of child processesthat they combine and mediate, yielding a compact overall parameterization. Weprovide tractable inference and learning algorithms for arbitrary DSTtopologies via an efficient structured mean-field algorithm. The diverseapplicability of DSTs is demonstrated by experiments on gene expression dataand by modeling group behavior in the setting of an American football game.
arxiv-900-103 | A Bayesian Approach toward Active Learning for Collaborative Filtering | http://arxiv.org/pdf/1207.4146v1.pdf | author:Rong Jin, Luo Si category:cs.LG cs.IR stat.ML published:2012-07-11 summary:Collaborative filtering is a useful technique for exploiting the preferencepatterns of a group of users to predict the utility of items for the activeuser. In general, the performance of collaborative filtering depends on thenumber of rated examples given by the active user. The more the number of ratedexamples given by the active user, the more accurate the predicted ratings willbe. Active learning provides an effective way to acquire the most informativerated examples from active users. Previous work on active learning forcollaborative filtering only considers the expected loss function based on theestimated model, which can be misleading when the estimated model isinaccurate. This paper takes one step further by taking into account of theposterior distribution of the estimated model, which results in more robustactive learning algorithm. Empirical studies with datasets of movie ratingsshow that when the number of ratings from the active user is restricted to besmall, active learning methods only based on the estimated model don't performwell while the active learning method using the model distribution achievessubstantially better performance.
arxiv-900-104 | A Generative Bayesian Model for Aggregating Experts' Probabilities | http://arxiv.org/pdf/1207.4144v1.pdf | author:Joseph Kahn category:cs.LG stat.ML published:2012-07-11 summary:In order to improve forecasts, a decisionmaker often combines probabilitiesgiven by various sources, such as human experts and machine learningclassifiers. When few training data are available, aggregation can be improvedby incorporating prior knowledge about the event being forecasted and aboutsalient properties of the experts. To this end, we develop a generativeBayesian aggregation model for probabilistic classi cation. The model includesan event-specific prior, measures of individual experts' bias, calibration,accuracy, and a measure of dependence betweeen experts. Rather than requireabsolute measures, we show that aggregation may be expressed in terms ofrelative accuracy between experts. The model results in a weighted logarithmicopinion pool (LogOps) that satis es consistency criteria such as the externalBayesian property. We derive analytic solutions for independent and forexchangeable experts. Empirical tests demonstrate the model's use, comparingits accuracy with other aggregation methods.
arxiv-900-105 | Conditional Chow-Liu Tree Structures for Modeling Discrete-Valued Vector Time Series | http://arxiv.org/pdf/1207.4142v1.pdf | author:Sergey Kirshner, Padhraic Smyth, Andrew Robertson category:cs.LG stat.ML published:2012-07-11 summary:We consider the problem of modeling discrete-valued vector time series datausing extensions of Chow-Liu tree models to capture both dependencies acrosstime and dependencies across variables. Conditional Chow-Liu tree models areintroduced, as an extension to standard Chow-Liu trees, for modelingconditional rather than joint densities. We describe learning algorithms forsuch models and show how they can be used to learn parsimonious representationsfor the output distributions in hidden Markov models. These models are appliedto the important problem of simulating and forecasting daily precipitationoccurrence for networks of rain stations. To demonstrate the effectiveness ofthe models, we compare their performance versus a number of alternatives usinghistorical precipitation data from Southwestern Australia and the WesternUnited States. We illustrate how the structure and parameters of the models canbe used to provide an improved meteorological interpretation of such data.
arxiv-900-106 | An Extended Cencov-Campbell Characterization of Conditional Information Geometry | http://arxiv.org/pdf/1207.4139v1.pdf | author:Guy Lebanon category:cs.LG stat.ML published:2012-07-11 summary:We formulate and prove an axiomatic characterization of conditionalinformation geometry, for both the normalized and the nonnormalized cases. Thischaracterization extends the axiomatic derivation of the Fisher geometry byCencov and Campbell to the cone of positive conditional models, and as aspecial case to the manifold of conditional distributions. Due to the closeconnection between the conditional I-divergence and the product Fisherinformation metric the characterization provides a new axiomatic interpretationof the primal problems underlying logistic regression and AdaBoost.
arxiv-900-107 | Active Model Selection | http://arxiv.org/pdf/1207.4138v1.pdf | author:Omid Madani, Daniel J. Lizotte, Russell Greiner category:cs.LG stat.ML published:2012-07-11 summary:Classical learning assumes the learner is given a labeled data sample, fromwhich it learns a model. The field of Active Learning deals with the situationwhere the learner begins not with a training sample, but instead with resourcesthat it can use to obtain information to help identify the optimal model. Tobetter understand this task, this paper presents and analyses the simplified"(budgeted) active model selection" version, which captures the pureexploration aspect of many active learning problems in a clean and simpleproblem formulation. Here the learner can use a fixed budget of "model probes"(where each probe evaluates the specified model on a random indistinguishableinstance) to identify which of a given set of possible models has the highestexpected accuracy. Our goal is a policy that sequentially determines whichmodel to probe next, based on the information observed so far. We present aformal description of this task, and show that it is NPhard in general. We theninvestigate a number of algorithms for this task, including several existingones (eg, "Round-Robin", "Interval Estimation", "Gittins") as well as somenovel ones (e.g., "Biased-Robin"), describing first their approximationproperties and then their empirical performance on various problem instances.We observe empirically that the simple biased-robin algorithm significantlyoutperforms the other algorithms in the case of identical costs and priors.
arxiv-900-108 | Bayesian Learning in Undirected Graphical Models: Approximate MCMC algorithms | http://arxiv.org/pdf/1207.4134v1.pdf | author:Iain Murray, Zoubin Ghahramani category:cs.LG stat.ML published:2012-07-11 summary:Bayesian learning in undirected graphical modelscomputing posteriordistributions over parameters and predictive quantities is exceptionallydifficult. We conjecture that for general undirected models, there are notractable MCMC (Markov Chain Monte Carlo) schemes giving the correctequilibrium distribution over parameters. While this intractability, due to thepartition function, is familiar to those performing parameter optimisation,Bayesian learning of posterior distributions over undirected model parametershas been unexplored and poses novel challenges. we propose several approximateMCMC schemes and test on fully observed binary models (Boltzmann machines) fora small coronary heart disease data set and larger artificial systems. Whileapproximations must perform well on the model, their interaction with thesampling scheme is also important. Samplers based on variational mean- fieldapproximations generally performed poorly, more advanced methods using loopypropagation, brief sampling and stochastic dynamics lead to acceptableparameter posteriors. Finally, we demonstrate these techniques on a Markovrandom field with hidden variables.
arxiv-900-109 | "Ideal Parent" Structure Learning for Continuous Variable Networks | http://arxiv.org/pdf/1207.4133v1.pdf | author:Iftach Nachman, Gal Elidan, Nir Friedman category:cs.LG stat.ML published:2012-07-11 summary:In recent years, there is a growing interest in learning Bayesian networkswith continuous variables. Learning the structure of such networks is acomputationally expensive procedure, which limits most applications toparameter learning. This problem is even more acute when learning networks withhidden variables. We present a general method for significantly speeding thestructure search algorithm for continuous variable networks with commonparametric distributions. Importantly, our method facilitates the addition ofnew hidden variables into the network structure efficiently. We demonstrate themethod on several data sets, both for learning structure on fully observabledata, and for introducing new hidden variables during structure search.
arxiv-900-110 | MOB-ESP and other Improvements in Probability Estimation | http://arxiv.org/pdf/1207.4132v1.pdf | author:Rodney Nielsen category:cs.LG cs.AI stat.ML published:2012-07-11 summary:A key prerequisite to optimal reasoning under uncertainty in intelligentsystems is to start with good class probability estimates. This paper improveson the current best probability estimation trees (Bagged-PETs) and alsopresents a new ensemble-based algorithm (MOB-ESP). Comparisons are made usingseveral benchmark datasets and multiple metrics. These experiments show thatMOB-ESP outputs significantly more accurate class probabilities than either thebaseline BPETs algorithm or the enhanced version presented here (EB-PETs).These results are based on metrics closely associated with the average accuracyof the predictions. MOB-ESP also provides much better probability rankings thanB-PETs. The paper further suggests how these estimation techniques can beapplied in concert with a broader category of classifiers.
arxiv-900-111 | Exponential Families for Conditional Random Fields | http://arxiv.org/pdf/1207.4131v1.pdf | author:Yasemin Altun, Alex Smola, Thomas Hofmann category:cs.LG stat.ML published:2012-07-11 summary:In this paper we de ne conditional random elds in reproducing kernel Hilbertspaces and show connections to Gaussian Process classi cation. More specically, we prove decomposition results for undirected graphical models and wegive constructions for kernels. Finally we present e cient means of solving theoptimization problem using reduced rank decompositions and we show howstationarity can be exploited e ciently in the optimization process.
arxiv-900-112 | Recovering Articulated Object Models from 3D Range Data | http://arxiv.org/pdf/1207.4129v1.pdf | author:Dragomir Anguelov, Daphne Koller, Hoi-Cheung Pang, Praveen Srinivasan, Sebastian Thrun category:cs.CV published:2012-07-11 summary:We address the problem of unsupervised learning of complex articulated objectmodels from 3D range data. We describe an algorithm whose input is a set ofmeshes corresponding to different configurations of an articulated object. Thealgorithm automatically recovers a decomposition of the object intoapproximately rigid parts, the location of the parts in the different objectinstances, and the articulated object skeleton linking the parts. Our algorithmfirst registers allthe meshes using an unsupervised non-rigid techniquedescribed in a companion paper. It then segments the meshes using a graphicalmodel that captures the spatial contiguity of parts. The segmentation is doneusing the EM algorithm, iterating between finding a decomposition of the objectinto rigid parts, and finding the location of the parts in the objectinstances. Although the graphical model is densely connected, the objectdecomposition step can be performed optimally and efficiently, allowing us toidentify a large number of object parts while avoiding local maxima. Wedemonstrate the algorithm on real world datasets, recovering a 15-partarticulated model of a human puppet from just 7 different puppetconfigurations, as well as a 4 part model of a fiexing arm where significantnon-rigid deformation was present.
arxiv-900-113 | Applying Discrete PCA in Data Analysis | http://arxiv.org/pdf/1207.4125v1.pdf | author:Wray L. Buntine, Aleks Jakulin category:cs.LG stat.ML published:2012-07-11 summary:Methods for analysis of principal components in discrete data have existedfor some time under various names such as grade of membership modelling,probabilistic latent semantic analysis, and genotype inference with admixture.In this paper we explore a number of extensions to the common theory, andpresent some application of these methods to some common statistical tasks. Weshow that these methods can be interpreted as a discrete version of ICA. Wedevelop a hierarchical version yielding components at different levels ofdetail, and additional techniques for Gibbs sampling. We compare the algorithmson a text prediction task using support vector machines, and to informationretrieval.
arxiv-900-114 | Iterative Conditional Fitting for Gaussian Ancestral Graph Models | http://arxiv.org/pdf/1207.4118v1.pdf | author:Mathias Drton, Thomas S. Richardson category:stat.ME cs.LG stat.ML published:2012-07-11 summary:Ancestral graph models, introduced by Richardson and Spirtes (2002),generalize both Markov random fields and Bayesian networks to a class of graphswith a global Markov property that is closed under conditioning andmarginalization. By design, ancestral graphs encode precisely the conditionalindependence structures that can arise from Bayesian networks with selectionand unobserved (hidden/latent) variables. Thus, ancestral graph models providea potentially very useful framework for exploratory model selection whenunobserved variables might be involved in the data-generating process but noparticular hidden structure can be specified. In this paper, we present theIterative Conditional Fitting (ICF) algorithm for maximum likelihood estimationin Gaussian ancestral graph models. The name reflects that in each step of theprocedure a conditional distribution is estimated, subject to constraints,while a marginal distribution is held fixed. This approach is in duality to thewell-known Iterative Proportional Fitting algorithm, in which marginaldistributions are fitted while conditional distributions are held fixed.
arxiv-900-115 | On-line Prediction with Kernels and the Complexity Approximation Principle | http://arxiv.org/pdf/1207.4113v1.pdf | author:Alex Gammerman, Yuri Kalnishkan, Vladimir Vovk category:cs.LG stat.ML published:2012-07-11 summary:The paper describes an application of Aggregating Algorithm to the problem ofregression. It generalizes earlier results concerned with plain linearregression to kernel techniques and presents an on-line algorithm whichperforms nearly as well as any oblivious kernel predictor. The paper containsthe derivation of an estimate on the performance of this algorithm. Theestimate is then used to derive an application of the Complexity ApproximationPrinciple to kernel methods.
arxiv-900-116 | Algebraic Statistics in Model Selection | http://arxiv.org/pdf/1207.4112v1.pdf | author:Luis David Garcia category:cs.LG stat.ML published:2012-07-11 summary:We develop the necessary theory in computational algebraic geometry to placeBayesian networks into the realm of algebraic statistics. We present analgebra{statistics dictionary focused on statistical modeling. In particular,we link the notion of effiective dimension of a Bayesian network with thenotion of algebraic dimension of a variety. We also obtain the independence andnon{independence constraints on the distributions over the observable variablesimplied by a Bayesian network with hidden variables, via a generating set of anideal of polynomials associated to the network. These results extend previouswork on the subject. Finally, the relevance of these results for modelselection is discussed.
arxiv-900-117 | The Minimum Information Principle for Discriminative Learning | http://arxiv.org/pdf/1207.4110v1.pdf | author:Amir Globerson, Naftali Tishby category:cs.LG stat.ML published:2012-07-11 summary:Exponential models of distributions are widely used in machine learning forclassiffication and modelling. It is well known that they can be interpreted asmaximum entropy models under empirical expectation constraints. In this work,we argue that for classiffication tasks, mutual information is a more suitableinformation theoretic measure to be optimized. We show how the principle ofminimum mutual information generalizes that of maximum entropy, and provides acomprehensive framework for building discriminative classiffiers. A gametheoretic interpretation of our approach is then given, and severalgeneralization bounds provided. We present iterative algorithms for solving theminimum information problem and its convex dual, and demonstrate theirperformance on various classiffication tasks. The results show that minimuminformation classiffiers outperform the corresponding maximum entropy models.
arxiv-900-118 | Nugget Discovery with a Multi-objective Cultural Algorithm | http://arxiv.org/pdf/1207.2630v1.pdf | author:Sujatha Srinivasan, Sivakumar Ramakrishnan category:cs.NE I.5.2; I.2.0 published:2012-07-11 summary:Partial classification popularly known as nugget discovery comes underdescriptive knowledge discovery. It involves mining rules for a target class ofinterest. Classification "If-Then" rules are the most sought out by decisionmakers since they are the most comprehensible form of knowledge mined by datamining techniques. The rules have certain properties namely the rule metricswhich are used to evaluate them. Mining rules with user specified propertiescan be considered as a multi-objective optimization problem since the ruleshave to satisfy more than one property to be used by the user. Culturalalgorithm (CA) with its knowledge sources have been used in solving manyoptimization problems. However research gap exists in using cultural algorithmfor multi-objective optimization of rules. In the current study amulti-objective cultural algorithm is proposed for partial classification.Results of experiments on benchmark data sets reveal good performance.
arxiv-900-119 | A Novel Approach Coloured Object Tracker with Adaptive Model and Bandwidth using Mean Shift Algorithm | http://arxiv.org/pdf/1207.2602v1.pdf | author:Seyed Amir Mohammadi, Mohammad Reza Mahzoun category:cs.CV published:2012-07-11 summary:The traditional color-based mean-shift tracking algorithm is popular amongtracking methods due to its simple and efficient procedure, however, the lackof dynamism in its target model makes it unsuitable for tracking objects whichhave changes in their sizes and shapes. In this paper, we propose a fast novelthreephase colored object tracker algorithm based on mean shift idea whileutilizing adaptive model. The proposed method can improve the mentionedweaknesses of the original mean-shift algorithm. The experimental results showthat the new method is feasible, robust and has acceptable speed in comparisonwith other algorithms.15 page,
arxiv-900-120 | Efficient Prediction of DNA-Binding Proteins Using Machine Learning | http://arxiv.org/pdf/1207.2600v1.pdf | author:Sokyna Qatawneh, Afaf Alneaimi, Thamer Rawashdeh, Mmohammad Muhairat, Rami Qahwaji, Stan Ipson category:cs.CV q-bio.QM published:2012-07-11 summary:DNA-binding proteins are a class of proteins which have a specific or generalaffinity to DNA and include three important components: transcription factors;nucleases, and histones. DNA-binding proteins also perform important roles inmany types of cellular activities. In this paper we describe machine learningsystems for the prediction of DNA- binding proteins where a Support VectorMachine and a Cascade Correlation Neural Network are optimized and thencompared to determine the learning algorithm that achieves the best predictionperformance. The information used for classification is derived fromcharacteristics that include overall charge, patch size and amino acidscomposition. In total 121 DNA- binding proteins and 238 non-binding proteinsare used to build and evaluate the system. For SVM using the ANOVA Kernel withJack-knife evaluation, an accuracy of 86.7% has been achieved with 91.1% forsensitivity and 85.3% for specificity. For CCNN optimized over the entiredataset with Jack knife evaluation we report an accuracy of 75.4%, while thevalues of specificity and sensitivity achieved were 72.3% and 82.6%,respectively.
arxiv-900-121 | Automated Training and Maintenance through Kinect | http://arxiv.org/pdf/1207.2597v1.pdf | author:Saket Warade, Jagannath Aghav, Petitpierre Claude, Sandeep Udayagiri category:cs.CV cs.ET cs.GR cs.HC published:2012-07-11 summary:In this paper, we have worked on reducing burden on mechanic involvingcomplex automobile maintenance activities that are performed in centralisedworkshops. We have presented a system prototype that combines Augmented Realitywith Kinect. With the use of Kinect, very high quality sensors are available atconsiderably low costs, thus reducing overall expenditure for system design.The system can be operated either in Speech mode or in Gesture mode. The systemcan be controlled by various audio commands if user opts for Speech mode. Thesame controlling can also be done by using a set of Gestures in Gesture mode. Gesture recognition is the task performed by Kinect system. This system,bundled with RGB and Depth camera, processes the skeletal data by keeping trackof 20 different body joints. Recognizing Gestures is done by verifying usermovements and checking them against predefined condition. Augmented Realitymodule captures real-time image data streams from high resolution camera. Thismodule then generates 3D model that is superimposed on real time data.
arxiv-900-122 | Face Recognition Algorithms based on Transformed Shape Features | http://arxiv.org/pdf/1207.2537v1.pdf | author:Sambhunath Biswas, Amrita Biswas category:cs.CV published:2012-07-11 summary:Human face recognition is, indeed, a challenging task, especially under theillumination and pose variations. We examine in the present paper effectivenessof two simple algorithms using coiflet packet and Radon transforms to recognizehuman faces from some databases of still gray level images, under theenvironment of illumination and pose variations. Both the algorithms convert2-D gray level training face images into their respective depth maps orphysical shape which are subsequently transformed by Coiflet packet and Radontransforms to compute energy for feature extraction. Experiments show that suchtransformed shape features are robust to illumination and pose variations. Withthe features extracted, training classes are optimally separated through lineardiscriminant analysis (LDA), while classification for test face images is madethrough a k-NN classifier, based on L1 norm and Mahalanobis distance measures.Proposed algorithms are then tested on face images that differ inillumination,expression or pose separately, obtained from threedatabases,namely, ORL, Yale and Essex-Grimace databases. Results, so obtained,are compared with two different existing algorithms.Performance usingDaubechies wavelets is also examined. It is seen that the proposed Coifletpacket and Radon transform based algorithms have significant performance,especially under different illumination conditions and pose variation.Comparison shows the proposed algorithms are superior.
arxiv-900-123 | Efficient Tracking of Large Classes of Experts | http://arxiv.org/pdf/1110.2755v3.pdf | author:András Gyorgy, Tamás Linder, Gábor Lugosi category:cs.LG cs.IT math.IT 68Q32, 68P30 I.2.6; E.4 published:2011-10-12 summary:In the framework of prediction of individual sequences, sequential predictionmethods are to be constructed that perform nearly as well as the best expertfrom a given class. We consider prediction strategies that compete with theclass of switching strategies that can segment a given sequence into severalblocks, and follow the advice of a different "base" expert in each block. Asusual, the performance of the algorithm is measured by the regret defined asthe excess loss relative to the best switching strategy selected in hindsightfor the particular sequence to be predicted. In this paper we constructprediction strategies of low computational cost for the case where the set ofbase experts is large. In particular we provide a method that can transform anyprediction algorithm $\A$ that is designed for the base class into a trackingalgorithm. The resulting tracking algorithm can take advantage of theprediction performance and potential computational efficiency of $\A$ in thesense that it can be implemented with time and space complexity only$O(n^{\gamma} \ln n)$ times larger than that of $\A$, where $n$ is the timehorizon and $\gamma \ge 0$ is a parameter of the algorithm. With $\A$ properlychosen, our algorithm achieves a regret bound of optimal order for $\gamma>0$,and only $O(\ln n)$ times larger than the optimal order for $\gamma=0$ for alltypical regret bound types we examined. For example, for predicting binarysequences with switching parameters under the logarithmic loss, our methodachieves the optimal $O(\ln n)$ regret rate with time complexity$O(n^{1+\gamma}\ln n)$ for any $\gamma\in (0,1)$.
arxiv-900-124 | A Spectral Learning Approach to Range-Only SLAM | http://arxiv.org/pdf/1207.2491v1.pdf | author:Byron Boots, Geoffrey J. Gordon category:cs.LG cs.RO stat.ML published:2012-07-10 summary:We present a novel spectral learning algorithm for simultaneous localizationand mapping (SLAM) from range data with known correspondences. This algorithmis an instance of a general spectral system identification framework, fromwhich it inherits several desirable properties, including statisticalconsistency and no local optima. Compared with popular batch optimization ormultiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectralapproach offers guaranteed low computational requirements and good trackingperformance. Compared with popular extended Kalman filter (EKF) or extendedinformation filter (EIF) approaches, and many MHT ones, our approach does notneed to linearize a transition or measurement model; such linearizations cancause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularlyfor the highly non-Gaussian posteriors encountered in range-only SLAM. Weprovide a theoretical analysis of our method, including finite-sample errorbounds. Finally, we demonstrate on a real-world robotic SLAM problem that ouralgorithm is not only theoretically justified, but works well in practice: in acomparison of multiple methods, the lowest errors come from a combination ofour algorithm with batch optimization, but our method alone produces nearly asgood a result at far lower computational cost.
arxiv-900-125 | Non-Convex Rank Minimization via an Empirical Bayesian Approach | http://arxiv.org/pdf/1207.2440v1.pdf | author:David Wipf category:stat.ML cs.CV cs.IT math.IT published:2012-07-10 summary:In many applications that require matrix solutions of minimal rank, theunderlying cost function is non-convex leading to an intractable, NP-hardoptimization problem. Consequently, the convex nuclear norm is frequently usedas a surrogate penalty term for matrix rank. The problem is that in manypractical scenarios there is no longer any guarantee that we can correctlyestimate generative low-rank matrices of interest, theoretical special casesnotwithstanding. Consequently, this paper proposes an alternative empiricalBayesian procedure build upon a variational approximation that, unlike thenuclear norm, retains the same globally minimizing point estimate as the rankfunction under many useful constraints. However, locally minimizing solutionsare largely smoothed away via marginalization, allowing the algorithm tosucceed when standard convex relaxations completely fail. While the proposedmethodology is generally applicable to a wide range of low-rank applications,we focus our attention on the robust principal component analysis problem(RPCA), which involves estimating an unknown low-rank matrix with unknownsparse corruptions. Theoretical and empirical evidence are presented to showthat our method is potentially superior to related MAP-based approaches, forwhich the convex principle component pursuit (PCP) algorithm (Candes et al.,2011) can be viewed as a special case.
arxiv-900-126 | Vision-Based Navigation I: A navigation filter for fusing DTM/correspondence updates | http://arxiv.org/pdf/1107.0399v4.pdf | author:Oleg Kupervasser, Vladimir Voronov category:cs.CV cs.AI 68T45 published:2011-07-02 summary:An algorithm for pose and motion estimation using corresponding features inimages and a digital terrain map is proposed. Using a Digital Terrain (orDigital Elevation) Map (DTM/DEM) as a global reference enables recovering theabsolute position and orientation of the camera. In order to do this, the DTMis used to formulate a constraint between corresponding features in twoconsecutive frames. The utilization of data is shown to improve the robustnessand accuracy of the inertial navigation algorithm. Extended Kalman filter wasused to combine results of inertial navigation algorithm and proposedvision-based navigation algorithm. The feasibility of this algorithms isestablished through numerical simulations.
arxiv-900-127 | A Multi-Agents Architecture to Learn Vision Operators and their Parameters | http://arxiv.org/pdf/1207.2426v1.pdf | author:Issam Qaffou, Mohammed Sadgal, Abdelaziz Elfazziki category:cs.CV published:2012-07-10 summary:In a vision system, every task needs that the operators to apply should be{\guillemotleft} well chosen {\guillemotright} and their parameters should bealso {\guillemotleft} well adjusted {\guillemotright}. The diversity ofoperators and the multitude of their parameters constitute a big challenge forusers. As it is very difficult to make the {\guillemotleft} right{\guillemotright} choice, lack of a specific rule, many disadvantages appearand affect the computation time and especially the quality of results. In thispaper we present a multi-agent architecture to learn the best operators toapply and their best parameters for a class of images. Our architectureconsists of three types of agents: User Agent, Operator Agent and ParameterAgent. The User Agent determines the phases of treatment, a library ofoperators and the possible values of their parameters. The Operator Agentconstructs all possible combinations of operators and the Parameter Agent, thecore of the architecture, adjusts the parameters of each combination bytreating a large number of images. Through the reinforcement learningmechanism, our architecture does not consider only the system opportunities butalso the user preferences.
arxiv-900-128 | Dual-Space Analysis of the Sparse Linear Model | http://arxiv.org/pdf/1207.2422v1.pdf | author:David Wipf, Yi Wu category:stat.ML cs.CV cs.IT math.IT published:2012-07-10 summary:Sparse linear (or generalized linear) models combine a standard likelihoodfunction with a sparse prior on the unknown coefficients. These priors canconveniently be expressed as a maximization over zero-mean Gaussians withdifferent variance hyperparameters. Standard MAP estimation (Type I) involvesmaximizing over both the hyperparameters and coefficients, while an empiricalBayesian alternative (Type II) first marginalizes the coefficients and thenmaximizes over the hyperparameters, leading to a tractable posteriorapproximation. The underlying cost functions can be related via a dual-spaceframework from Wipf et al. (2011), which allows both the Type I or Type IIobjectives to be expressed in either coefficient or hyperparmeter space. Thisperspective is useful because some analyses or extensions are more conducive todevelopment in one space or the other. Herein we consider the estimation of atrade-off parameter balancing sparsity and data fit. As this parameter iseffectively a variance, natural estimators exist by assessing the problem inhyperparameter (variance) space, transitioning natural ideas from Type II tosolve what is much less intuitive for Type I. In contrast, for analyses ofupdate rules and sparsity properties of local and global solutions, as well asextensions to more general likelihood models, we can leverage coefficient-spacetechniques developed for Type I and apply them to Type II. For example, thisallows us to prove that Type II-inspired techniques can be successfulrecovering sparse coefficients when unfavorable restricted isometry properties(RIP) lead to failure of popular L1 reconstructions. It also facilitates theanalysis of Type II when non-Gaussian likelihood models lead to intractableintegrations.
arxiv-900-129 | Improvement of ISOM by using filter | http://arxiv.org/pdf/1207.2268v1.pdf | author:Imen Chaabouni, Wiem Fourati, Med Salim Bouhlel category:cs.MM cs.CV published:2012-07-10 summary:Image compression helps in storing the transmitted data in proficient way bydecreasing its redundancy. This technique helps in transferring more digital ormultimedia data over internet as it increases the storage space. It isimportant to maintain the image quality even if it is compressed to certainextent. Depend upon this the image compression is classified into twocategories : lossy and lossless image compression. There are many lossy digitalimage compression techniques exists. Among this Incremental Self Organizing Mapis a familiar one. The good pictures quality can be retrieved if imagedenoising technique is used for compression and also provides bettercompression ratio. Image denoising is an important pre-processing step for manyimage analysis and computer vision system. It refers to the task of recoveringa good estimate of the true image from a degraded observation without alteringand changing useful structure in the image such as discontinuities and edges.Many approaches have been proposed to remove the noise effectively whilepreserving the original image details and features as much as possible. Thispaper proposes a technique for image compression using Incremental SelfOrganizing Map (ISOM) with Discret Wavelet Transform (DWT) by applyingfiltering techniques which play a crucial role in enhancing the quality of areconstructed image. The experimental result shows that the proposed techniqueobtained better compression ratio value.
arxiv-900-130 | Challenges for Distributional Compositional Semantics | http://arxiv.org/pdf/1207.2265v1.pdf | author:Daoud Clarke category:cs.CL cs.AI published:2012-07-10 summary:This paper summarises the current state-of-the art in the study ofcompositionality in distributional semantics, and major challenges for thisarea. We single out generalised quantifiers and intensional semantics as areason which to focus attention for the development of the theory. Once suitabletheories have been developed, algorithms will be needed to apply the theory totasks. Evaluation is a major problem; we single out application to recognisingtextual entailment and machine translation for this purpose.
arxiv-900-131 | A Genetic Algorithm Approach for Solving a Flexible Job Shop Scheduling Problem | http://arxiv.org/pdf/1207.2253v1.pdf | author:Sayedmohammadreza Vaghefinezhad, Kuan Yew Wong category:math.OC cs.NE published:2012-07-10 summary:Flexible job shop scheduling has been noticed as an effective manufacturingsystem to cope with rapid development in today's competitive environment.Flexible job shop scheduling problem (FJSSP) is known as a NP-hard problem inthe field of optimization. Considering the dynamic state of the real worldmakes this problem more and more complicated. Most studies in the field ofFJSSP have only focused on minimizing the total makespan. In this paper, amathematical model for FJSSP has been developed. The objective function ismaximizing the total profit while meeting some constraints. Time-varying rawmaterial costs and selling prices and dissimilar demands for each period, havebeen considered to decrease gaps between reality and the model. A manufacturerthat produces various parts of gas valves has been used as a case study. Itsscheduling problem for multi-part, multi-period, and multi-operation withparallel machines has been solved by using genetic algorithm (GA). The bestobtained answer determines the economic amount of production by differentmachines that belong to predefined operations for each part to satisfy customerdemand in each period.
arxiv-900-132 | An Introduction to Artificial Prediction Markets for Classification | http://arxiv.org/pdf/1102.1465v6.pdf | author:Adrian Barbu, Nathan Lay category:stat.ML cs.LG math.ST stat.TH published:2011-02-07 summary:Prediction markets are used in real life to predict outcomes of interest suchas presidential elections. This paper presents a mathematical theory ofartificial prediction markets for supervised learning of conditionalprobability estimators. The artificial prediction market is a novel method forfusing the prediction information of features or trained classifiers, where thefusion result is the contract price on the possible outcomes. The market can betrained online by updating the participants' budgets using training examples.Inspired by the real prediction markets, the equations that govern the marketare derived from simple and reasonable assumptions. Efficient numericalalgorithms are presented for solving these equations. The obtained artificialprediction market is shown to be a maximum likelihood estimator. It generalizeslinear aggregation, existent in boosting and random forest, as well as logisticregression and some kernel methods. Furthermore, the market mechanism allowsthe aggregation of specialized classifiers that participate only on specificinstances. Experimental comparisons show that the artificial prediction marketsoften outperform random forest and implicit online learning on synthetic dataand real UCI datasets. Moreover, an extensive evaluation for pelvic andabdominal lymph node detection in CT data shows that the prediction marketimproves adaboost's detection rate from 79.6% to 81.2% at 3 falsepositives/volume.
arxiv-900-133 | Approximated Structured Prediction for Learning Large Scale Graphical Models | http://arxiv.org/pdf/1006.2899v2.pdf | author:Tamir Hazan, Raquel Urtasun category:cs.LG cs.AI published:2010-06-15 summary:This manuscripts contains the proofs for "A Primal-Dual Message-PassingAlgorithm for Approximated Large Scale Structured Prediction".
arxiv-900-134 | Determining a rotation of a tetrahedron from a projection | http://arxiv.org/pdf/1111.7100v2.pdf | author:Richard J. Gardner, Paolo Gronchi, Thorsten Theobald category:math.MG cs.CG cs.CV published:2011-11-30 summary:The following problem, arising from medical imaging, is addressed: Supposethat $T$ is a known tetrahedron in $\R^3$ with centroid at the origin. Alsoknown is the orthogonal projection $U$ of the vertices of the image $\phi T$ of$T$ under an unknown rotation $\phi$ about the origin. Under what circumstancescan $\phi$ be determined from $T$ and $U$?
arxiv-900-135 | Shortest path distance in random k-nearest neighbor graphs | http://arxiv.org/pdf/1206.6381v2.pdf | author:Morteza Alamgir, Ulrike von Luxburg category:cs.LG stat.ML published:2012-06-27 summary:Consider a weighted or unweighted k-nearest neighbor graph that has beenbuilt on n data points drawn randomly according to some density p on R^d. Westudy the convergence of the shortest path distance in such graphs as thesample size tends to infinity. We prove that for unweighted kNN graphs, thisdistance converges to an unpleasant distance function on the underlying spacewhose properties are detrimental to machine learning. We also study thebehavior of the shortest path distance in weighted kNN graphs.
arxiv-900-136 | Estimating a Causal Order among Groups of Variables in Linear Models | http://arxiv.org/pdf/1207.1977v1.pdf | author:Doris Entner, Patrik O. Hoyer category:stat.ML cs.LG stat.ME published:2012-07-09 summary:The machine learning community has recently devoted much attention to theproblem of inferring causal relationships from statistical data. Most of thiswork has focused on uncovering connections among scalar random variables. Wegeneralize existing methods to apply to collections of multi-dimensional randomvectors, focusing on techniques applicable to linear models. The performance ofthe resulting algorithms is evaluated and compared in simulations, which showthat our methods can, in many cases, provide useful information on causalrelationships even for relatively small sample sizes.
arxiv-900-137 | Forecasting electricity consumption by aggregating specialized experts | http://arxiv.org/pdf/1207.1965v1.pdf | author:Marie Devaine, Pierre Gaillard, Yannig Goude, Gilles Stoltz category:stat.ML cs.LG stat.AP published:2012-07-09 summary:We consider the setting of sequential prediction of arbitrary sequences basedon specialized experts. We first provide a review of the relevant literatureand present two theoretical contributions: a general analysis of the specialistaggregation rule of Freund et al. (1997) and an adaptation of fixed-share rulesof Herbster and Warmuth (1998) in this setting. We then apply these rules tothe sequential short-term (one-day-ahead) forecasting of electricityconsumption; to do so, we consider two data sets, a Slovakian one and a Frenchone, respectively concerned with hourly and half-hourly predictions. We followa general methodology to perform the stated empirical studies and detail inparticular tuning issues of the learning parameters. The introduced aggregationrules demonstrate an improved accuracy on the data sets at hand; theimprovements lie in a reduced mean squared error but also in a more robustbehavior with respect to large occasional errors.
arxiv-900-138 | Spatial And Spectral Quality Evaluation Based On Edges Regions Of Satellite Image Fusion | http://arxiv.org/pdf/1207.1922v1.pdf | author:Firouz Abdullah Al-Wassai, N. V. Kalyankar, Ali A. Al-Zaky category:cs.CV published:2012-07-08 summary:The Quality of image fusion is an essential determinant of the value ofprocessing images fusion for many applications. Spatial and spectral qualitiesare the two important indexes that used to evaluate the quality of any fusedimage. However, the jury is still out of fused image's benefits if it comparedwith its original images. In addition, there is a lack of measures forassessing the objective quality of the spatial resolution for the fusionmethods. Therefore, an objective quality of the spatial resolution assessmentfor fusion images is required. Most important details of the image are in edgesregions, but most standards of image estimation do not depend upon specifyingthe edges in the image and measuring their edges. However, they depend upon thegeneral estimation or estimating the uniform region, so this study deals withnew method proposed to estimate the spatial resolution by Contrast StatisticalAnalysis (CSA) depending upon calculating the contrast of the edge, non edgeregions and the rate for the edges regions. Specifying the edges in the imageis made by using Soble operator with different threshold values. In addition,estimating the color distortion added by image fusion based on HistogramAnalysis of the edge brightness values of all RGB-color bands and Lcomponent.
arxiv-900-139 | Nonparametric Edge Detection in Speckled Imagery | http://arxiv.org/pdf/1207.1915v1.pdf | author:Edwin Girón, Alejandro C. Frery, Francisco Cribari-Neto category:stat.AP cs.CV stat.ML published:2012-07-08 summary:We address the issue of edge detection in Synthetic Aperture Radar imagery.In particular, we propose nonparametric methods for edge detection, andnumerically compare them to an alternative method that has been recentlyproposed in the literature. Our results show that some of the proposed methodsdisplay superior results and are computationally simpler than the existingmethod. An application to real (not simulated) data is presented and discussed.
arxiv-900-140 | Keeping greed good: sparse regression under design uncertainty with application to biomass characterization | http://arxiv.org/pdf/1207.1888v1.pdf | author:David J. Biagioni, Ryan Elmore, Wesley Jones category:stat.AP stat.CO stat.ME stat.ML published:2012-07-08 summary:In this paper, we consider the classic measurement error regression scenarioin which our independent, or design, variables are observed with severalsources of additive noise. We will show that our motivating example'sreplicated measurements on both the design and dependent variables may beleveraged to enhance a sparse regression algorithm. Specifically, we estimatethe variance and use it to scale our design variables. We demonstrate theefficacy of scaling from several points of view and validate it empiricallywith a biomass characterization data set using two of the most widely usedsparse algorithms: least angle regression (LARS) and the Dantzig selector (DS).
arxiv-900-141 | Finding Structure in Text, Genome and Other Symbolic Sequences | http://arxiv.org/pdf/1207.1847v1.pdf | author:Ted Dunning category:cs.CL cs.IR published:2012-07-08 summary:The statistical methods derived and described in this thesis provide new waysto elucidate the structural properties of text and other symbolic sequences.Generically, these methods allow detection of a difference in the frequency ofa single feature, the detection of a difference between the frequencies of anensemble of features and the attribution of the source of a text. These threeabstract tasks suffice to solve problems in a wide variety of settings.Furthermore, the techniques described in this thesis can be extended to providea wide range of additional tests beyond the ones described here. A variety of applications for these methods are examined in detail. Theseapplications are drawn from the area of text analysis and genetic sequenceanalysis. The textually oriented tasks include finding interesting collocationsand cooccurent phrases, language identification, and information retrieval. Thebiologically oriented tasks include species identification and the discovery ofpreviously unreported long range structure in genes. In the applicationsreported here where direct comparison is possible, the performance of these newmethods substantially exceeds the state of the art. Overall, the methods described here provide new and effective ways to analysetext and other symbolic sequences. Their particular strength is that they dealwell with situations where relatively little data are available. Since thesemethods are abstract in nature, they can be applied in novel situations withrelative ease.
arxiv-900-142 | Object Recognition with Multi-Scale Pyramidal Pooling Networks | http://arxiv.org/pdf/1207.1765v1.pdf | author:Jonathan Masci, Ueli Meier, Gabriel Fricout, Jürgen Schmidhuber category:cs.CV cs.NE published:2012-07-07 summary:We present a Multi-Scale Pyramidal Pooling Network, featuring a novelpyramidal pooling layer at multiple scales and a novel encoding layer. Thanksto the former the network does not require all images of a given classificationtask to be of equal size. The encoding layer improves generalisationperformance in comparison to similar neural network architectures, especiallywhen training data is scarce. We evaluate and compare our system toconvolutional neural networks and state-of-the-art computer vision methods onvarious benchmark datasets. We also present results on industrial steel defectclassification, where existing architectures are not applicable because of theconstraint on equally sized input images. The proposed architecture can be seenas a fully supervised hierarchical bag-of-features extension that is trainedonline and can be fine-tuned for any given task.
arxiv-900-143 | A Spectral Algorithm for Learning Hidden Markov Models | http://arxiv.org/pdf/0811.4413v6.pdf | author:Daniel Hsu, Sham M. Kakade, Tong Zhang category:cs.LG cs.AI published:2008-11-26 summary:Hidden Markov Models (HMMs) are one of the most fundamental and widely usedstatistical tools for modeling discrete time series. In general, learning HMMsfrom data is computationally hard (under cryptographic assumptions), andpractitioners typically resort to search heuristics which suffer from the usuallocal optima issues. We prove that under a natural separation condition (boundson the smallest singular value of the HMM parameters), there is an efficientand provably correct algorithm for learning HMMs. The sample complexity of thealgorithm does not explicitly depend on the number of distinct (discrete)observations---it implicitly depends on this quantity through spectralproperties of the underlying HMM. This makes the algorithm particularlyapplicable to settings with a large number of observations, such as those innatural language processing where the space of observation is sometimes thewords in a language. The algorithm is also simple, employing only a singularvalue decomposition and matrix multiplications.
arxiv-900-144 | Safe Exploration in Markov Decision Processes | http://arxiv.org/pdf/1205.4810v3.pdf | author:Teodor Mihai Moldovan, Pieter Abbeel category:cs.LG published:2012-05-22 summary:In environments with uncertain dynamics exploration is necessary to learn howto perform well. Existing reinforcement learning algorithms provide strongexploration guarantees, but they tend to rely on an ergodicity assumption. Theessence of ergodicity is that any state is eventually reachable from any otherstate by following a suitable policy. This assumption allows for explorationalgorithms that operate by simply favoring states that have rarely been visitedbefore. For most physical systems this assumption is impractical as the systemswould break before any reasonable exploration has taken place, i.e., mostphysical systems don't satisfy the ergodicity assumption. In this paper weaddress the need for safe exploration methods in Markov decision processes. Wefirst propose a general formulation of safety through ergodicity. We show thatimposing safety by restricting attention to the resulting set of guaranteedsafe policies is NP-hard. We then present an efficient algorithm for guaranteedsafe, but potentially suboptimal, exploration. At the core is an optimizationformulation in which the constraints restrict attention to a subset of theguaranteed safe policies and the objective favors exploration policies. Ourframework is compatible with the majority of previously proposed explorationmethods, which rely on an exploration bonus. Our experiments, which include aMartian terrain exploration problem, show that our method is able to explorebetter than classical exploration methods.
arxiv-900-145 | Sequential detection of multiple change points in networks: a graphical model approach | http://arxiv.org/pdf/1207.1687v1.pdf | author:Arash Ali Amini, XuanLong Nguyen category:math.ST stat.ML stat.TH published:2012-07-06 summary:We propose a probabilistic formulation that enables sequential detection ofmultiple change points in a network setting. We present a class of sequentialdetection rules for certain functionals of change points (minimum among asubset), and prove their asymptotic optimality properties in terms of expecteddetection delay time. Drawing from graphical model formalism, the sequentialdetection rules can be implemented by a computationally efficientmessage-passing protocol which may scale up linearly in network size and inwaiting time. The effectiveness of our inference algorithm is demonstrated bysimulations.
arxiv-900-146 | The Complexity of Learning Principles and Parameters Grammars | http://arxiv.org/pdf/1207.0052v3.pdf | author:Jacob Andreas category:cs.FL cs.CL published:2012-06-30 summary:We investigate models for learning the class of context-free andcontext-sensitive languages (CFLs and CSLs). We begin with a brief discussionof some early hardness results which show that unrestricted language learningis impossible, and unrestricted CFL learning is computationally infeasible; wethen briefly survey the literature on algorithms for learning restrictedsubclasses of the CFLs. Finally, we introduce a new family of subclasses, theprincipled parametric context-free grammars (and a corresponding family ofprincipled parametric context-sensitive grammars), which roughly model the"Principles and Parameters" framework in psycholinguistics. We present threehardness results: first, that the PPCFGs are not efficiently learnable givenequivalence and membership oracles, second, that the PPCFGs are not efficientlylearnable from positive presentations unless P = NP, and third, that the PPCSGsare not efficiently learnable from positive presentations unless integerfactorization is in P.
arxiv-900-147 | Analysis of Multi-Scale Fractal Dimension to Classify Human Motion | http://arxiv.org/pdf/1207.1649v1.pdf | author:Núbia Rosa da Silva, Odemir Martinez Bruno category:cs.CV published:2012-07-06 summary:In recent years there has been considerable interest in human actionrecognition. Several approaches have been developed in order to enhance theautomatic video analysis. Although some developments have been achieved by thecomputer vision community, the properly classification of human motion is stilla hard and challenging task. The objective of this study is to investigate theuse of 3D multi-scale fractal dimension to recognize motion patterns in videos.In order to develop a robust strategy for human motion classification, weproposed a method where the Fourier transform is used to calculate thederivative in which all data points are deemed. Our results shown thatdifferent accuracy rates can be found for different databases. We believe thatin specific applications our results are the first step to develop an automaticmonitoring system, which can be applied in security systems, trafficmonitoring, biology, physical therapy, cardiovascular disease among manyothers.
arxiv-900-148 | An Innovative Skin Detection Approach Using Color Based Image Retrieval Technique | http://arxiv.org/pdf/1207.1551v1.pdf | author:Shervan Fekri-Ershad, Mohammad Saberi, Farshad Tajeripour category:cs.CV published:2012-07-06 summary:From The late 90th, "Skin Detection" becomes one of the major problems inimage processing. If "Skin Detection" will be done in high accuracy, it can beused in many cases as face recognition, Human Tracking and etc. Until now somany methods were presented for solving this problem. In most of these methods,color space was used to extract feature vector for classifying pixels, but themost of them have not good accuracy in detecting types of skin. The proposedapproach in this paper is based on "Color based image retrieval" (CBIR)technique. In this method, first by means of CBIR method and image tiling andconsidering the relation between pixel and its neighbors, a feature vectorwould be defined and then with using a training step, detecting the skin in thetest stage. The result shows that the presenting approach, in addition to itshigh accuracy in detecting type of skin, has no sensitivity to illuminationintensity and moving face orientation.
arxiv-900-149 | Multimodal similarity-preserving hashing | http://arxiv.org/pdf/1207.1522v1.pdf | author:Jonathan Masci, Michael M. Bronstein, Alexander A. Bronstein, Jürgen Schmidhuber category:cs.CV cs.NE published:2012-07-06 summary:We introduce an efficient computational framework for hashing data belongingto multiple modalities into a single representation space where they becomemutually comparable. The proposed approach is based on a novel coupled siameseneural network architecture and allows unified treatment of intra- andinter-modality similarity learning. Unlike existing cross-modality similaritylearning approaches, our hashing functions are not limited to binarized linearprojections and can assume arbitrarily complex forms. We show experimentallythat our method significantly outperforms state-of-the-art hashing approacheson multimedia retrieval tasks.
arxiv-900-150 | An experimental study of exhaustive solutions for the Mastermind puzzle | http://arxiv.org/pdf/1207.1315v1.pdf | author:J. J. Merelo, Antonio M. Mora, Carlos Cotta, Thomas P. Runarsson category:cs.NE math.OC published:2012-07-05 summary:Mastermind is in essence a search problem in which a string of symbols thatis kept secret must be found by sequentially playing strings that use the samealphabet, and using the responses that indicate how close are those otherstrings to the secret one as hints. Although it is commercialized as a game, itis a combinatorial problem of high complexity, with applications on fields thatrange from computer security to genomics. As such a kind of problem, there areno exact solutions; even exhaustive search methods rely on heuristics tochoose, at every step, strings to get the best possible hint. These methodsmostly try to play the move that offers the best reduction in search space sizein the next step; this move is chosen according to an empirical score. However,in this paper we will examine several state of the art exhaustive searchmethods and show that another factor, the presence of the actual solution amongthe candidate moves, or, in other words, the fact that the actual solution hasthe highest score, plays also a very important role. Using that, we willpropose new exhaustive search approaches that obtain results which arecomparable to the classic ones, and besides, are better suited as a basis fornon-exhaustive search strategies such as evolutionary algorithms, since theirbehavior in a series of key indicators is better than the classical algorithms.
arxiv-900-151 | Training Restricted Boltzmann Machines on Word Observations | http://arxiv.org/pdf/1202.5695v2.pdf | author:George E. Dahl, Ryan P. Adams, Hugo Larochelle category:cs.LG stat.ML published:2012-02-25 summary:The restricted Boltzmann machine (RBM) is a flexible tool for modelingcomplex data, however there have been significant computational difficulties inusing RBMs to model high-dimensional multinomial observations. In naturallanguage processing applications, words are naturally modeled by K-ary discretedistributions, where K is determined by the vocabulary size and can easily bein the hundreds of thousands. The conventional approach to training RBMs onword observations is limited because it requires sampling the states of K-waysoftmax visible units during block Gibbs updates, an operation that takes timelinear in K. In this work, we address this issue by employing a more generalclass of Markov chain Monte Carlo operators on the visible units, yieldingupdates with computational complexity independent of K. We demonstrate thesuccess of our approach by training RBMs on hundreds of millions of wordn-grams using larger vocabularies than previously feasible and using thelearned features to improve performance on chunking and sentimentclassification tasks, achieving state-of-the-art results on the latter.
arxiv-900-152 | On unified view of nullspace-type conditions for recoveries associated with general sparsity structures | http://arxiv.org/pdf/1207.1119v1.pdf | author:Anatoli Juditsky, Fatma Kilinc Karzan, Arkadi Nemirovski category:math.OC cs.IT math.IT stat.ML published:2012-07-04 summary:We discuss a general notion of "sparsity structure" and associated recoveriesof a sparse signal from its linear image of reduced dimension possiblycorrupted with noise. Our approach allows for uni?ed treatment of (a) the"usual sparsity" and "usual $\ell_1$ recovery," (b) block-sparsity withpossibly overlapping blocks and associated block-$\ell_1$ recovery, and (c)low-rank-oriented recovery by nuclear norm minimization. The proposed recoveryroutines are natural extensions of the usual $\ell_1$ minimization used inCompressed Sensing. Specifically we present nullspace-type sufficientconditions for the recovery to be precise on sparse signals in the noiselesscase. Then we derive error bounds for imperfect (nearly sparse signal, presenceof observation noise, etc.) recovery under these conditions. In all of thesecases, we present efficiently verifiable sufficient conditions for the validityof the associated nullspace properties.
arxiv-900-153 | Ordering-Based Search: A Simple and Effective Algorithm for Learning Bayesian Networks | http://arxiv.org/pdf/1207.1429v1.pdf | author:Marc Teyssier, Daphne Koller category:cs.LG cs.AI stat.ML published:2012-07-04 summary:One of the basic tasks for Bayesian networks (BNs) is that of learning anetwork structure from data. The BN-learning problem is NP-hard, so thestandard solution is heuristic search. Many approaches have been proposed forthis task, but only a very small number outperform the baseline of greedyhill-climbing with tabu lists; moreover, many of the proposed algorithms arequite complex and hard to implement. In this paper, we propose a very simpleand easy-to-implement method for addressing this task. Our approach is based onthe well-known fact that the best network (of bounded in-degree) consistentwith a given node ordering can be found very efficiently. We therefore proposea search not over the space of structures, but over the space of orderings,selecting for each ordering the best network consistent with it. This searchspace is much smaller, makes more global search steps, has a lower branchingfactor, and avoids costly acyclicity checks. We present results for thisalgorithm on both synthetic and real data sets, evaluating both the score ofthe network found and in the running time. We show that ordering-based searchoutperforms the standard baseline, and is competitive with recent algorithmsthat are much harder to implement.
arxiv-900-154 | Mining Associated Text and Images with Dual-Wing Harmoniums | http://arxiv.org/pdf/1207.1423v1.pdf | author:Eric P. Xing, Rong Yan, Alexander G. Hauptmann category:cs.LG cs.DB stat.ML published:2012-07-04 summary:We propose a multi-wing harmonium model for mining multimedia data thatextends and improves on earlier models based on two-layer random fields, whichcapture bidirectional dependencies between hidden topic aspects and observedinputs. This model can be viewed as an undirected counterpart of the two-layerdirected models such as LDA for similar tasks, but bears significant differencein inference/learning cost tradeoffs, latent topic representations, and topicmixing mechanisms. In particular, our model facilitates efficient inference androbust topic mixing, and potentially provides high flexibilities in modelingthe latent topic spaces. A contrastive divergence and a variational algorithmare derived for learning. We specialized our model to a dual-wing harmonium forcaptioned images, incorporating a multivariate Poisson for word-counts and amultivariate Gaussian for color histogram. We present empirical results on theapplications of this model to classification, retrieval and image annotation onnews video collections, and we report an extensive comparison with variousextant models.
arxiv-900-155 | A Function Approximation Approach to Estimation of Policy Gradient for POMDP with Structured Policies | http://arxiv.org/pdf/1207.1421v1.pdf | author:Huizhen Yu category:cs.LG stat.ML published:2012-07-04 summary:We consider the estimation of the policy gradient in partially observableMarkov decision processes (POMDP) with a special class of structured policiesthat are finite-state controllers. We show that the gradient estimation can bedone in the Actor-Critic framework, by making the critic compute a "value"function that does not depend on the states of POMDP. This function is theconditional mean of the true value function that depends on the states. We showthat the critic can be implemented using temporal difference (TD) methods withlinear function approximations, and the analytical results on TD andActor-Critic can be transfered to this case. Although Actor-Critic algorithmshave been used extensively in Markov decision processes (MDP), up to now theyhave not been proposed for POMDP as an alternative to the earlier proposalGPOMDP algorithm, an actor-only method. Furthermore, we show that the same ideaapplies to semi-Markov problems with a subset of finite-state controllers.
arxiv-900-156 | Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars | http://arxiv.org/pdf/1207.1420v1.pdf | author:Luke S. Zettlemoyer, Michael Collins category:cs.CL published:2012-07-04 summary:This paper addresses the problem of mapping natural language sentences tolambda-calculus encodings of their meaning. We describe a learning algorithmthat takes as input a training set of sentences labeled with expressions in thelambda calculus. The algorithm induces a grammar for the problem, along with alog-linear model that represents a distribution over syntactic and semanticanalyses conditioned on the input sentence. We apply the method to the task oflearning natural language interfaces to databases and show that the learnedparsers outperform previous methods in two benchmark database domains.
arxiv-900-157 | The DLR Hierarchy of Approximate Inference | http://arxiv.org/pdf/1207.1417v1.pdf | author:Michal Rosen-Zvi, Michael I. Jordan, Alan Yuille category:cs.LG stat.ML published:2012-07-04 summary:We propose a hierarchy for approximate inference based on the Dobrushin,Lanford, Ruelle (DLR) equations. This hierarchy includes existing algorithms,such as belief propagation, and also motivates novel algorithms such asfactorized neighbors (FN) algorithms and variants of mean field (MF)algorithms. In particular, we show that extrema of the Bethe free energycorrespond to approximate solutions of the DLR equations. In addition, wedemonstrate a close connection between these approximate algorithms and Gibbssampling. Finally, we compare and contrast various of the algorithms in the DLRhierarchy on spin-glass problems. The experiments show that algorithms higherup in the hierarchy give more accurate results when they converge but tend tobe less stable.
arxiv-900-158 | Two-Way Latent Grouping Model for User Preference Prediction | http://arxiv.org/pdf/1207.1414v1.pdf | author:Eerika Savia, Kai Puolamaki, Janne Sinkkonen, Samuel Kaski category:cs.IR cs.LG stat.ML published:2012-07-04 summary:We introduce a novel latent grouping model for predicting the relevance of anew document to a user. The model assumes a latent group structure for bothusers and documents. We compared the model against a state-of-the-art method,the User Rating Profile model, where only users have a latent group structure.We estimate both models by Gibbs sampling. The new method predicts relevancemore accurately for new documents that have few known ratings. The reason isthat generalization over documents then becomes necessary and hence the twowaygrouping is profitable.
arxiv-900-159 | Discovery of non-gaussian linear causal models using ICA | http://arxiv.org/pdf/1207.1413v1.pdf | author:Shohei Shimizu, Aapo Hyvarinen, Yutaka Kano, Patrik O. Hoyer category:cs.LG cs.MS stat.ML published:2012-07-04 summary:In recent years, several methods have been proposed for the discovery ofcausal structure from non-experimental data (Spirtes et al. 2000; Pearl 2000).Such methods make various assumptions on the data generating process tofacilitate its identification from purely observational data. Continuing thisline of research, we show how to discover the complete causal structure ofcontinuous-valued data, under the assumptions that (a) the data generatingprocess is linear, (b) there are no unobserved confounders, and (c) disturbancevariables have non-gaussian distributions of non-zero variances. The solutionrelies on the use of the statistical method known as independent componentanalysis (ICA), and does not require any pre-specified time-ordering of thevariables. We provide a complete Matlab package for performing this LiNGAManalysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate theeffectiveness of the method using artificially generated data.
arxiv-900-160 | Piecewise Training for Undirected Models | http://arxiv.org/pdf/1207.1409v1.pdf | author:Charles Sutton, Andrew McCallum category:cs.LG stat.ML published:2012-07-04 summary:For many large undirected models that arise in real-world applications, exactmaximumlikelihood training is intractable, because it requires computingmarginal distributions of the model. Conditional training is even moredifficult, because the partition function depends not only on the parameters,but also on the observed input, requiring repeated inference over each trainingexample. An appealing idea for such models is to independently train a localundirected classifier over each clique, afterwards combining the learnedweights into a single global model. In this paper, we show that this piecewisemethod can be justified as minimizing a new family of upper bounds on the logpartition function. On three natural-language data sets, piecewise training ismore accurate than pseudolikelihood, and often performs comparably to globaltraining using belief propagation.
arxiv-900-161 | A Conditional Random Field for Discriminatively-trained Finite-state String Edit Distance | http://arxiv.org/pdf/1207.1406v1.pdf | author:Andrew McCallum, Kedar Bellare, Fernando Pereira category:cs.LG cs.AI published:2012-07-04 summary:The need to measure sequence similarity arises in information extraction,object identity, data mining, biological sequence analysis, and other domains.This paper presents discriminative string-edit CRFs, a finitestate conditionalrandom field model for edit sequences between strings. Conditional randomfields have advantages over generative approaches to this problem, such as pairHMMs or the work of Ristad and Yianilos, because as conditionally-trainedmethods, they enable the use of complex, arbitrary actions and features of theinput strings. As in generative models, the training data does not have tospecify the edit sequences between the given string pairs. Unlike generativemodels, however, our model is trained on both positive and negative instancesof string pairs. We present positive experimental results on several data sets.
arxiv-900-162 | A submodular-supermodular procedure with applications to discriminative structure learning | http://arxiv.org/pdf/1207.1404v1.pdf | author:Mukund Narasimhan, Jeff A. Bilmes category:cs.LG cs.DS stat.ML published:2012-07-04 summary:In this paper, we present an algorithm for minimizing the difference betweentwo submodular functions using a variational framework which is based on (anextension of) the concave-convex procedure [17]. Because several commonly usedmetrics in machine learning, like mutual information and conditional mutualinformation, are submodular, the problem of minimizing the difference of twosubmodular problems arises naturally in many machine learning applications. Twosuch applications are learning discriminatively structured graphical models andfeature selection under computational complexity constraints. A commonly usedmetric for measuring discriminative capacity is the EAR measure which is thedifference between two conditional mutual information terms. Feature selectiontaking complexity considerations into account also fall into this frameworkbecause both the information that a set of features provide and the cost ofcomputing and using the features can be modeled as submodular functions. Thisproblem is NP-hard, and we give a polynomial time heuristic for it. We alsopresent results on synthetic data to show that classifiers based ondiscriminative graphical models using this algorithm can significantlyoutperform classifiers based on generative graphical models.
arxiv-900-163 | Obtaining Calibrated Probabilities from Boosting | http://arxiv.org/pdf/1207.1403v1.pdf | author:Alexandru Niculescu-Mizil, Richard A. Caruana category:cs.LG stat.ML published:2012-07-04 summary:Boosted decision trees typically yield good accuracy, precision, and ROCarea. However, because the outputs from boosting are not well calibratedposterior probabilities, boosting yields poor squared error and cross-entropy.We empirically demonstrate why AdaBoost predicts distorted probabilities andexamine three calibration methods for correcting this distortion: PlattScaling, Isotonic Regression, and Logistic Correction. We also experiment withboosting using log-loss instead of the usual exponential loss. Experiments showthat Logistic Correction and boosting with log-loss work well when boostingweak models such as decision stumps, but yield poor performance when boostingmore complex models such as full decision trees. Platt Scaling and IsotonicRegression, however, significantly improve the probabilities predicted by
arxiv-900-164 | Toward Practical N2 Monte Carlo: the Marginal Particle Filter | http://arxiv.org/pdf/1207.1396v1.pdf | author:Mike Klaas, Nando de Freitas, Arnaud Doucet category:stat.CO cs.LG stat.ML published:2012-07-04 summary:Sequential Monte Carlo techniques are useful for state estimation innon-linear, non-Gaussian dynamic models. These methods allow us to approximatethe joint posterior distribution using sequential importance sampling. In thisframework, the dimension of the target distribution grows with each time step,thus it is necessary to introduce some resampling steps to ensure that theestimates provided by the algorithm have a reasonable variance. In manyapplications, we are only interested in the marginal filtering distributionwhich is defined on a space of fixed dimension. We present a Sequential MonteCarlo algorithm called the Marginal Particle Filter which operates directly onthe marginal distribution, hence avoiding having to perform importance samplingon a space of growing dimension. Using this idea, we also derive an improvedversion of the auxiliary particle filter. We show theoretic and empiricalresults which demonstrate a reduction in variance over conventional particlefiltering, and present techniques for reducing the cost of the marginalparticle filter with N particles from O(N2) to O(N logN).
arxiv-900-165 | Learning about individuals from group statistics | http://arxiv.org/pdf/1207.1393v1.pdf | author:Hendrik Kuck, Nando de Freitas category:cs.LG stat.ML published:2012-07-04 summary:We propose a new problem formulation which is similar to, but moreinformative than, the binary multiple-instance learning problem. In thissetting, we are given groups of instances (described by feature vectors) alongwith estimates of the fraction of positively-labeled instances per group. Thetask is to learn an instance level classifier from this information. That is,we are trying to estimate the unknown binary labels of individuals fromknowledge of group statistics. We propose a principled probabilistic model tosolve this problem that accounts for uncertainty in the parameters and in theunknown individual labels. This model is trained with an efficient MCMCalgorithm. Its performance is demonstrated on both synthetic and real-worlddata arising in general object recognition.
arxiv-900-166 | Learning Bayesian Network Parameters with Prior Knowledge about Context-Specific Qualitative Influences | http://arxiv.org/pdf/1207.1387v1.pdf | author:Ad Feelders, Linda C. van der Gaag category:cs.AI cs.LG stat.ML published:2012-07-04 summary:We present a method for learning the parameters of a Bayesian network withprior knowledge about the signs of influences between variables. Our methodaccommodates not just the standard signs, but provides for context-specificsigns as well. We show how the various signs translate into order constraintson the network parameters and how isotonic regression can be used to computeorder-constrained estimates from the available data. Our experimental resultsshow that taking prior knowledge about the signs of influences into accountleads to an improved fit of the true distribution, especially when only a smallsample of data is available. Moreover, the computed estimates are guaranteed tobe consistent with the specified signs, thereby resulting in a network that ismore likely to be accepted by experts in its domain of application.
arxiv-900-167 | Maximum Margin Bayesian Networks | http://arxiv.org/pdf/1207.1382v1.pdf | author:Yuhong Guo, Dana Wilkinson, Dale Schuurmans category:cs.LG stat.ML published:2012-07-04 summary:We consider the problem of learning Bayesian network classifiers thatmaximize the marginover a set of classification variables. We find that thisproblem is harder for Bayesian networks than for undirected graphical modelslike maximum margin Markov networks. The main difficulty is that the parametersin a Bayesian network must satisfy additional normalization constraints that anundirected graphical model need not respect. These additional constraintscomplicate the optimization task. Nevertheless, we derive an effective trainingalgorithm that solves the maximum margin training problem for a range ofBayesian network topologies, and converges to an approximate solution forarbitrary network topologies. Experimental results show that the method candemonstrate improved generalization performance over Markov networks when thedirected graphical structure encodes relevant knowledge. In practice, thetraining technique allows one to combine prior knowledge expressed as adirected (causal) model with state of the art discriminative learning methods.
arxiv-900-168 | Bayes Blocks: An Implementation of the Variational Bayesian Building Blocks Framework | http://arxiv.org/pdf/1207.1380v1.pdf | author:Markus Harva, Tapani Raiko, Antti Honkela, Harri Valpola, Juha Karhunen category:cs.MS cs.LG stat.ML published:2012-07-04 summary:A software library for constructing and learning probabilistic models ispresented. The library offers a set of building blocks from which a largevariety of static and dynamic models can be built. These include hierarchicalmodels for variances of other variables and many nonlinear models. Theunderlying variational Bayesian machinery, providing for fast and robustestimation but being mathematically rather involved, is almost completelyhidden from the user thus making it very easy to use the library. The buildingblocks include Gaussian, rectified Gaussian and mixture-of-Gaussians variablesand computational nodes which can be combined rather freely.
arxiv-900-169 | On the Detection of Concept Changes in Time-Varying Data Stream by Testing Exchangeability | http://arxiv.org/pdf/1207.1379v1.pdf | author:Shen-Shyang Ho, Harry Wechsler category:cs.LG stat.ML published:2012-07-04 summary:A martingale framework for concept change detection based on testing dataexchangeability was recently proposed (Ho, 2005). In this paper, we describethe proposed change-detection test based on the Doob's Maximal Inequality andshow that it is an approximation of the sequential probability ratio test(SPRT). The relationship between the threshold value used in the proposed testand its size and power is deduced from the approximation. The mean delay timebefore a change is detected is estimated using the average sample number of aSPRT. The performance of the test using various threshold values is examined onfive different data stream scenarios simulated using two synthetic data sets.Finally, experimental results show that the test is effective in detectingchanges in time-varying data streams simulated using three benchmark data sets.
arxiv-900-170 | Belief Updating and Learning in Semi-Qualitative Probabilistic Networks | http://arxiv.org/pdf/1207.1367v1.pdf | author:Cassio Polpo de Campos, Fabio Gagliardi Cozman category:cs.AI stat.ML published:2012-07-04 summary:This paper explores semi-qualitative probabilistic networks (SQPNs) thatcombine numeric and qualitative information. We first show that exactinferences with SQPNs are NPPP-Complete. We then show that existing qualitativerelations in SQPNs (plus probabilistic logic and imprecise assessments) can bedealt effectively through multilinear programming. We then discuss learning: weconsider a maximum likelihood method that generates point estimates given aSQPN and empirical data, and we describe a Bayesian-minded method that employsthe Imprecise Dirichlet Model to generate set-valued estimates.
arxiv-900-171 | Learning Factor Graphs in Polynomial Time & Sample Complexity | http://arxiv.org/pdf/1207.1366v1.pdf | author:Pieter Abbeel, Daphne Koller, Andrew Y. Ng category:cs.LG stat.ML published:2012-07-04 summary:We study computational and sample complexity of parameter and structurelearning in graphical models. Our main result shows that the class of factorgraphs with bounded factor size and bounded connectivity can be learned inpolynomial time and polynomial number of samples, assuming that the data isgenerated by a network in this class. This result covers both parameterestimation for a known network structure and structure learning. It implies asa corollary that we can learn factor graphs for both Bayesian networks andMarkov networks of bounded degree, in polynomial time and sample complexity.Unlike maximum likelihood estimation, our method does not require inference inthe underlying network, and so applies to networks where inference isintractable. We also show that the error of our learned model degradesgracefully when the generating distribution is not a member of the target classof networks.
arxiv-900-172 | Learning from Sparse Data by Exploiting Monotonicity Constraints | http://arxiv.org/pdf/1207.1364v1.pdf | author:Eric E. Altendorf, Angelo C. Restificar, Thomas G. Dietterich category:cs.LG stat.ML published:2012-07-04 summary:When training data is sparse, more domain knowledge must be incorporated intothe learning algorithm in order to reduce the effective size of the hypothesisspace. This paper builds on previous work in which knowledge about qualitativemonotonicities was formally represented and incorporated into learningalgorithms (e.g., Clark & Matwin's work with the CN2 rule learning algorithm).We show how to interpret knowledge of qualitative influences, and in particularof monotonicities, as constraints on probability distributions, and toincorporate this knowledge into Bayesian network learning algorithms. We showthat this yields improved accuracy, particularly with very small training sets(e.g. less than 10 examples).
arxiv-900-173 | PAC-Bayesian Majority Vote for Late Classifier Fusion | http://arxiv.org/pdf/1207.1019v1.pdf | author:Emilie Morvant, Amaury Habrard, Stéphane Ayache category:stat.ML cs.CV cs.LG cs.MM published:2012-07-04 summary:A lot of attention has been devoted to multimedia indexing over the past fewyears. In the literature, we often consider two kinds of fusion schemes: Theearly fusion and the late fusion. In this paper we focus on late classifierfusion, where one combines the scores of each modality at the decision level.To tackle this problem, we investigate a recent and elegant well-foundedquadratic program named MinCq coming from the Machine Learning PAC-Bayestheory. MinCq looks for the weighted combination, over a set of real-valuedfunctions seen as voters, leading to the lowest misclassification rate, whilemaking use of the voters' diversity. We provide evidence that this method isnaturally adapted to late fusion procedure. We propose an extension of MinCq byadding an order- preserving pairwise loss for ranking, helping to improve MeanAveraged Precision measure. We confirm the good behavior of the MinCq-basedfusion approaches with experiments on a real image benchmark.
arxiv-900-174 | Unsupervised spectral learning | http://arxiv.org/pdf/1207.1358v1.pdf | author:Susan Shortreed, Marina Meila category:cs.LG stat.ML published:2012-07-04 summary:In spectral clustering and spectral image segmentation, the data is partionedstarting from a given matrix of pairwise similarities S. the matrix S isconstructed by hand, or learned on a separate training set. In this paper weshow how to achieve spectral clustering in unsupervised mode. Our algorithmstarts with a set of observed pairwise features, which are possible componentsof an unknown, parametric similarity function. This function is learnediteratively, at the same time as the clustering of the data. The algorithmshows promosing results on synthetic and real data.
arxiv-900-175 | AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem | http://arxiv.org/pdf/1110.3907v3.pdf | author:Peng Sun, Mark D. Reid, Jie Zhou category:stat.ML cs.AI cs.CV published:2011-10-18 summary:This paper presents an improvement to model learning when using multi-classLogitBoost for classification. Motivated by the statistical view, LogitBoostcan be seen as additive tree regression. Two important factors in this settingare: 1) coupled classifier output due to a sum-to-zero constraint, and 2) thedense Hessian matrices that arise when computing tree node split gain and nodevalue fittings. In general, this setting is too complicated for a tractablemodel learning algorithm. However, too aggressive simplification of the settingmay lead to degraded performance. For example, the original LogitBoost isoutperformed by ABC-LogitBoost due to the latter's more careful treatment ofthe above two factors. In this paper we propose techniques to address the two main difficulties ofthe LogitBoost setting: 1) we adopt a vector tree (i.e. each node value isvector) that enforces a sum-to-zero constraint, and 2) we use an adaptive blockcoordinate descent that exploits the dense Hessian when computing tree splitgain and node values. Higher classification accuracy and faster convergencerates are observed for a range of public data sets when compared to both theoriginal and the ABC-LogitBoost implementations.
arxiv-900-176 | Relational Data Mining Through Extraction of Representative Exemplars | http://arxiv.org/pdf/1207.0833v1.pdf | author:Frédéric Blanchard, Michel Herbin category:cs.AI cs.IR stat.ML published:2012-07-03 summary:With the growing interest on Network Analysis, Relational Data Mining isbecoming an emphasized domain of Data Mining. This paper addresses the problemof extracting representative elements from a relational dataset. After definingthe notion of degree of representativeness, computed using the Bordaaggregation procedure, we present the extraction of exemplars which are therepresentative elements of the dataset. We use these concepts to build anetwork on the dataset. We expose the main properties of these notions and wepropose two typical applications of our framework. The first applicationconsists in resuming and structuring a set of binary images and the second inmining co-authoring relation in a research team.
arxiv-900-177 | Web-Based Benchmark for Keystroke Dynamics Biometric Systems: A Statistical Analysis | http://arxiv.org/pdf/1207.0784v1.pdf | author:Romain Giot, Mohamad El-Abed, Christophe Rosenberger category:cs.LG published:2012-07-03 summary:Most keystroke dynamics studies have been evaluated using a specific kind ofdataset in which users type an imposed login and password. Moreover, thesestudies are optimistics since most of them use different acquisition protocols,private datasets, controlled environment, etc. In order to enhance the accuracyof keystroke dynamics' performance, the main contribution of this paper istwofold. First, we provide a new kind of dataset in which users have typed bothan imposed and a chosen pairs of logins and passwords. In addition, thekeystroke dynamics samples are collected in a web-based uncontrolledenvironment (OS, keyboards, browser, etc.). Such kind of dataset is importantsince it provides us more realistic results of keystroke dynamics' performancein comparison to the literature (controlled environment, etc.). Second, wepresent a statistical analysis of well known assertions such as therelationship between performance and password size, impact of fusion schemes onsystem overall performance, and others such as the relationship betweenperformance and entropy. We put into obviousness in this paper some new resultson keystroke dynamics in realistic conditions.
arxiv-900-178 | Hybrid Template Update System for Unimodal Biometric Systems | http://arxiv.org/pdf/1207.0783v1.pdf | author:Romain Giot, Christophe Rosenberger, Bernadette Dorizzi category:cs.LG published:2012-07-03 summary:Semi-supervised template update systems allow to automatically take intoaccount the intra-class variability of the biometric data over time. Suchsystems can be inefficient by including too many impostor's samples or skippingtoo many genuine's samples. In the first case, the biometric reference driftsfrom the real biometric data and attracts more often impostors. In the secondcase, the biometric reference does not evolve quickly enough and alsoprogressively drifts from the real biometric data. We propose a hybrid systemusing several biometric sub-references in order to increase per- formance ofself-update systems by reducing the previously cited errors. The proposition isvalidated for a keystroke- dynamics authentication system (this modalitysuffers of high variability over time) on two consequent datasets from thestate of the art.
arxiv-900-179 | Polarimetric SAR Image Smoothing with Stochastic Distances | http://arxiv.org/pdf/1207.0771v1.pdf | author:Leonardo Torres, Antonio C. Medeiros, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML published:2012-07-03 summary:Polarimetric Synthetic Aperture Radar (PolSAR) images are establishing as animportant source of information in remote sensing applications. The mostcomplete format this type of imaging produces consists of complex-valuedHermitian matrices in every image coordinate and, as such, their visualizationis challenging. They also suffer from speckle noise which reduces thesignal-to-noise ratio. Smoothing techniques have been proposed in theliterature aiming at preserving different features and, analogously,projections from the cone of Hermitian positive matrices to different colorrepresentation spaces are used for enhancing certain characteristics. In thiswork we propose the use of stochastic distances between models that describethis type of data in a Nagao-Matsuyama-type of smoothing technique. Theresulting images are shown to present good visualization properties (noisereduction with preservation of fine details) in all the consideredvisualization spaces.
arxiv-900-180 | Generalized Statistical Complexity of SAR Imagery | http://arxiv.org/pdf/1207.0757v1.pdf | author:Eliana S. de Almeida, Antonio Carlos de Medeiros, Osvaldo A. Rosso, Alejandro C. Frery category:cs.IT cs.GR math.IT stat.AP stat.ML published:2012-07-03 summary:A new generalized Statistical Complexity Measure (SCM) was proposed by Rossoet al in 2010. It is a functional that captures the notions of order/disorderand of distance to an equilibrium distribution. The former is computed by ameasure of entropy, while the latter depends on the definition of a stochasticdivergence. When the scene is illuminated by coherent radiation, image data iscorrupted by speckle noise, as is the case of ultrasound-B, sonar, laser andSynthetic Aperture Radar (SAR) sensors. In the amplitude and intensity formats,this noise is multiplicative and non-Gaussian requiring, thus, specializedtechniques for image processing and understanding. One of the most successfulfamily of models for describing these images is the Multiplicative Model whichleads, among other probability distributions, to the G0 law. This distributionhas been validated in the literature as an expressive and tractable model,deserving the "universal" denomination for its ability to describe most typesof targets. In order to compute the statistical complexity of a site in animage corrupted by speckle noise, we assume that the equilibrium distributionis that of fully developed speckle, namely the Gamma law in intensity format,which appears in areas with little or no texture. We use the Shannon entropyalong with the Hellinger distance to measure the statistical complexity ofintensity SAR images, and we show that it is an expressive feature capable ofidentifying many types of targets.
arxiv-900-181 | The OS* Algorithm: a Joint Approach to Exact Optimization and Sampling | http://arxiv.org/pdf/1207.0742v1.pdf | author:Marc Dymetman, Guillaume Bouchard, Simon Carter category:cs.AI cs.CL cs.LG published:2012-07-03 summary:Most current sampling algorithms for high-dimensional distributions are basedon MCMC techniques and are approximate in the sense that they are valid onlyasymptotically. Rejection sampling, on the other hand, produces valid samples,but is unrealistically slow in high-dimension spaces. The OS* algorithm that wepropose is a unified approach to exact optimization and sampling, based onincremental refinements of a functional upper bound, which combines ideas ofadaptive rejection sampling and of A* optimization search. We show that thechoice of the refinement can be done in a way that ensures tractability inhigh-dimension spaces, and we present first experiments in two differentsettings: inference in high-order HMMs and in large discrete graphical models.
arxiv-900-182 | Inferring land use from mobile phone activity | http://arxiv.org/pdf/1207.1115v1.pdf | author:Jameson L. Toole, Michael Ulm, Dietmar Bauer, Marta C. Gonzalez category:stat.ML cs.LG physics.soc-ph H.2.8 published:2012-07-03 summary:Understanding the spatiotemporal distribution of people within a city iscrucial to many planning applications. Obtaining data to create requiredknowledge, currently involves costly survey methods. At the same timeubiquitous mobile sensors from personal GPS devices to mobile phones arecollecting massive amounts of data on urban systems. The locations,communications, and activities of millions of people are recorded and stored bynew information technologies. This work utilizes novel dynamic data, generatedby mobile phone users, to measure spatiotemporal changes in population. In theprocess, we identify the relationship between land use and dynamic populationover the course of a typical week. A machine learning classification algorithmis used to identify clusters of locations with similar zoned uses and mobilephone activity patterns. It is shown that the mobile phone data is capable ofdelivering useful information on actual land use that supplements zoningregulations.
arxiv-900-183 | Speckle Reduction using Stochastic Distances | http://arxiv.org/pdf/1207.0704v1.pdf | author:Leonardo Torres, Tamer Cavalcante, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML published:2012-07-03 summary:This paper presents a new approach for filter design based on stochasticdistances and tests between distributions. A window is defined around eachpixel, samples are compared and only those which pass a goodness-of-fit testare used to compute the filtered value. The technique is applied to intensitySynthetic Aperture Radar (SAR) data, using the Gamma model with varying numberof looks allowing, thus, changes in heterogeneity. Modified Nagao-Matsuyamawindows are used to define the samples. The proposal is compared with the Lee'sfilter which is considered a standard, using a protocol based on simulation.Among the criteria used to quantify the quality of filters, we employ theequivalent number of looks (related to the signal-to-noise ratio), linecontrast, and edge preservation. Moreover, we also assessed the filters by theUniversal Image Quality Index and the Pearson's correlation between edges.
arxiv-900-184 | Meme as Building Block for Evolutionary Optimization of Problem Instances | http://arxiv.org/pdf/1207.0702v1.pdf | author:Liang Feng, Yew Soon Ong, Ah Hwee Tan, Ivor Wai-Hung Tsang category:cs.NE published:2012-07-03 summary:A significantly under-explored area of evolutionary optimization in theliterature is the study of optimization methodologies that can evolve alongwith the problems solved. Particularly, present evolutionary optimizationapproaches generally start their search from scratch or the ground-zero stateof knowledge, independent of how similar the given new problem of interest isto those optimized previously. There has thus been the apparent lack ofautomated knowledge transfers and reuse across problems. Taking the cue, thispaper introduces a novel Memetic Computational Paradigm for search, one thatmodels after how human solves problems, and embarks on a study towardsintelligent evolutionary optimization of problems through the transfers ofstructured knowledge in the form of memes learned from previous problem-solvingexperiences, to enhance future evolutionary searches. In particular, theproposed memetic search paradigm is composed of four culture-inspiredoperators, namely, Meme Learning, Meme Selection, Meme Variation and MemeImitation. The learning operator mines for memes in the form of latentstructures derived from past experiences of problem-solving. The selectionoperator identifies the fit memes that replicate and transmit across problems,while the variation operator introduces innovations into the memes. Theimitation operator, on the other hand, defines how fit memes assimilate intothe search process of newly encountered problems, thus gearing towardsefficient and effective evolutionary optimization. Finally, comprehensivestudies on two widely studied challenging well established NP-hard routingproblem domains, particularly, the capacitated vehicle routing (CVR) andcapacitated arc routing (CAR), confirm the high efficacy of the proposedmemetic computational search paradigm for intelligent evolutionary optimizationof problems.
arxiv-900-185 | Local Water Diffusion Phenomenon Clustering From High Angular Resolution Diffusion Imaging (HARDI) | http://arxiv.org/pdf/1207.0677v1.pdf | author:Romain Giot, Christophe Charrier, Maxime Descoteaux category:cs.LG cs.CV published:2012-07-03 summary:The understanding of neurodegenerative diseases undoubtedly passes throughthe study of human brain white matter fiber tracts. To date, diffusion magneticresonance imaging (dMRI) is the unique technique to obtain information aboutthe neural architecture of the human brain, thus permitting the study of whitematter connections and their integrity. However, a remaining challenge of thedMRI community is to better characterize complex fiber crossing configurations,where diffusion tensor imaging (DTI) is limited but high angular resolutiondiffusion imaging (HARDI) now brings solutions. This paper investigates thedevelopment of both identification and classification process of the localwater diffusion phenomenon based on HARDI data to automatically detect imagingvoxels where there are single and crossing fiber bundle populations. Thetechnique is based on knowledge extraction processes and is validated on a dMRIphantom dataset with ground truth.
arxiv-900-186 | Agnostic System Identification for Model-Based Reinforcement Learning | http://arxiv.org/pdf/1203.1007v2.pdf | author:Stephane Ross, J. Andrew Bagnell category:cs.LG cs.AI cs.SY stat.ML published:2012-03-05 summary:A fundamental problem in control is to learn a model of a system fromobservations that is useful for controller synthesis. To provide goodperformance guarantees, existing methods must assume that the real system is inthe class of models considered during learning. We present an iterative methodwith strong guarantees even in the agnostic case where the system is not in theclass. In particular, we show that any no-regret online learning algorithm canbe used to obtain a near-optimal policy, provided some model achieves lowtraining error and access to a good exploration distribution. Our approachapplies to both discrete and continuous domains. We demonstrate its efficacyand scalability on a challenging helicopter domain from the literature.
arxiv-900-187 | On the origin of long-range correlations in texts | http://arxiv.org/pdf/1207.0658v1.pdf | author:Eduardo G. Altmann, Giampaolo Cristadoro, Mirko Degli Esposti category:cs.CL physics.soc-ph published:2012-07-03 summary:The complexity of human interactions with social and natural phenomena ismirrored in the way we describe our experiences through natural language. Inorder to retain and convey such a high dimensional information, the statisticalproperties of our linguistic output has to be highly correlated in time. Anexample are the robust observations, still largely not understood, ofcorrelations on arbitrary long scales in literary texts. In this paper weexplain how long-range correlations flow from highly structured linguisticlevels down to the building blocks of a text (words, letters, etc..). Bycombining calculations and data analysis we show that correlations take form ofa bursty sequence of events once we approach the semantically relevant topicsof the text. The mechanisms we identify are fairly general and can be equallyapplied to other hierarchical settings.
arxiv-900-188 | Improving neural networks by preventing co-adaptation of feature detectors | http://arxiv.org/pdf/1207.0580v1.pdf | author:Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov category:cs.NE cs.CV cs.LG published:2012-07-03 summary:When a large feedforward neural network is trained on a small training set,it typically performs poorly on held-out test data. This "overfitting" isgreatly reduced by randomly omitting half of the feature detectors on eachtraining case. This prevents complex co-adaptations in which a feature detectoris only helpful in the context of several other specific feature detectors.Instead, each neuron learns to detect a feature that is generally helpful forproducing the correct answer given the combinatorially large variety ofinternal contexts in which it must operate. Random "dropout" gives bigimprovements on many benchmark tasks and sets new records for speech and objectrecognition.
arxiv-900-189 | Time Series Classification by Class-Specific Mahalanobis Distance Measures | http://arxiv.org/pdf/1010.1526v6.pdf | author:Zoltán Prekopcsák, Daniel Lemire category:cs.LG published:2010-10-07 summary:To classify time series by nearest neighbors, we need to specify or learn oneor several distance measures. We consider variations of the Mahalanobisdistance measures which rely on the inverse covariance matrix of the data.Unfortunately --- for time series data --- the covariance matrix has often lowrank. To alleviate this problem we can either use a pseudoinverse, covarianceshrinking or limit the matrix to its diagonal. We review these alternatives andbenchmark them against competitive methods such as the related Large MarginNearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW)distance. As we expected, we find that the DTW is superior, but the Mahalanobisdistance measures are one to two orders of magnitude faster. To get bestresults with Mahalanobis distance measures, we recommend learning one distancemeasure per class using either covariance shrinking or the diagonal approach.
arxiv-900-190 | Applying Deep Belief Networks to Word Sense Disambiguation | http://arxiv.org/pdf/1207.0396v1.pdf | author:Peratham Wiriyathammabhum, Boonserm Kijsirikul, Hiroya Takamura, Manabu Okumura category:cs.CL cs.LG published:2012-07-02 summary:In this paper, we applied a novel learning algorithm, namely, Deep BeliefNetworks (DBN) to word sense disambiguation (WSD). DBN is a probabilisticgenerative model composed of multiple layers of hidden units. DBN usesRestricted Boltzmann Machine (RBM) to greedily train layer by layer as apretraining. Then, a separate fine tuning step is employed to improve thediscriminative power. We compared DBN with various state-of-the-art supervisedlearning algorithms in WSD such as Support Vector Machine (SVM), MaximumEntropy model (MaxEnt), Naive Bayes classifier (NB) and Kernel PrincipalComponent Analysis (KPCA). We used all words in the given paragraph,surrounding context words and part-of-speech of surrounding words as ourknowledge sources. We conducted our experiment on the SENSEVAL-2 data set. Weobserved that DBN outperformed all other learning algorithms.
arxiv-900-191 | More Effective Crossover Operators for the All-Pairs Shortest Path Problem | http://arxiv.org/pdf/1207.0369v1.pdf | author:Benjamin Doerr, Daniel Johannsen, Timo Kötzing, Frank Neumann, Madeleine Theile category:cs.NE published:2012-07-02 summary:The all-pairs shortest path problem is the first non-artificial problem forwhich it was shown that adding crossover can significantly speed up amutation-only evolutionary algorithm. Recently, the analysis of this algorithmwas refined and it was shown to have an expected optimization time (w.r.t. thenumber of fitness evaluations) of $\Theta(n^{3.25}(\log n)^{0.25})$. In contrast to this simple algorithm, evolutionary algorithms used inpractice usually employ refined recombination strategies in order to avoid thecreation of infeasible offspring. We study extensions of the basic algorithm bytwo such concepts which are central in recombination, namely \emph{repairmechanisms} and \emph{parent selection}. We show that repairing infeasibleoffspring leads to an improved expected optimization time of$\mathord{O}(n^{3.2}(\log n)^{0.2})$. As a second part of our study we provethat choosing parents that guarantee feasible offspring results in an evenbetter optimization time of $\mathord{O}(n^{3}\log n)$. Both results show that already simple adjustments of the recombinationoperator can asymptotically improve the runtime of evolutionary algorithms.
arxiv-900-192 | Autonomous Cleaning of Corrupted Scanned Documents - A Generative Modeling Approach | http://arxiv.org/pdf/1201.2605v2.pdf | author:Zhenwen Dai, Jörg Lücke category:cs.CV cs.LG published:2012-01-12 summary:We study the task of cleaning scanned text documents that are stronglycorrupted by dirt such as manual line strokes, spilled ink etc. We aim atautonomously removing dirt from a single letter-size page based only on theinformation the page contains. Our approach, therefore, has to learn characterrepresentations without supervision and requires a mechanism to distinguishlearned representations from irregular patterns. To learn characterrepresentations, we use a probabilistic generative model parameterizing patternfeatures, feature variances, the features' planar arrangements, and patternfrequencies. The latent variables of the model describe pattern class, patternposition, and the presence or absence of individual pattern features. The modelparameters are optimized using a novel variational EM approximation. Afterlearning, the parameters represent, independently of their absolute position,planar feature arrangements and their variances. A quality measure definedbased on the learned representation then allows for an autonomousdiscrimination between regular character patterns and the irregular patternsmaking up the dirt. The irregular patterns can thus be removed to clean thedocument. For a full Latin alphabet we found that a single page does notcontain sufficiently many character examples. However, even if heavilycorrupted by dirt, we show that a page containing a lower number of charactertypes can efficiently and autonomously be cleaned solely based on thestructural regularity of the characters it contains. In different examplesusing characters from different alphabets, we demonstrate generality of theapproach and discuss its implications for future developments.
arxiv-900-193 | Readouts for Echo-state Networks Built using Locally Regularized Orthogonal Forward Regression | http://arxiv.org/pdf/1110.4304v3.pdf | author:Ján Dolinský, Kei Hirose, Sadanori Konishi category:stat.ML stat.ME published:2011-10-19 summary:Echo state network (ESN) is viewed as a temporal non-orthogonal expansionwith pseudo-random parameters. Such expansions naturally give rise toregressors of various relevance to a teacher output. We illustrate that oftenonly a certain amount of the generated echo-regressors effectively explain thevariance of the teacher output and also that sole local regularization is notable to provide in-depth information concerning the importance of the generatedregressors. The importance is therefore determined by a joint calculation ofthe individual variance contributions and Bayesian relevance using locallyregularized orthogonal forward regression (LROFR) algorithm. This informationcan be advantageously used in a variety of ways for an in-depth analysis of anESN structure and its state-space parameters in relation to the unknowndynamics of the underlying problem. We present locally regularized linearreadout built using LROFR. The readout may have a different dimensionality thanan ESN model itself, and besides improving robustness and accuracy of an ESN itrelates the echo-regressors to different features of the training data and maydetermine what type of an additional readout is suitable for a task at hand.Moreover, as flexibility of the linear readout has limitations and mightsometimes be insufficient for certain tasks, we also present a radial basisfunction (RBF) readout built using LROFR. It is a flexible and parsimoniousreadout with excellent generalization abilities and is a viable alternative toreadouts based on a feed-forward neural network (FFNN) or an RBF net builtusing relevance vector machine (RVM).
arxiv-900-194 | Automated languages phylogeny from Levenshtein distance | http://arxiv.org/pdf/0911.3280v7.pdf | author:Maurizio Serva category:cs.CL q-bio.PE q-bio.QM published:2009-11-17 summary:Languages evolve over time in a process in which reproduction, mutation andextinction are all possible, similar to what happens to living organisms. Usingthis similarity it is possible, in principle, to build family trees which showthe degree of relatedness between languages. The method used by modern glottochronology, developed by Swadesh in the1950s, measures distances from the percentage of words with a common historicalorigin. The weak point of this method is that subjective judgment plays arelevant role. Recently we proposed an automated method that avoids the subjectivity, whoseresults can be replicated by studies that use the same database and thatdoesn't require a specific linguistic knowledge. Moreover, the method allows aquick comparison of a large number of languages. We applied our method to the Indo-European and Austronesian families,considering in both cases, fifty different languages. The resulting trees aresimilar to those of previous studies, but with some important differences inthe position of few languages and subgroups. We believe that these differencescarry new information on the structure of the tree and on the phylogeneticrelationships within families.
arxiv-900-195 | Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses | http://arxiv.org/pdf/1207.0268v1.pdf | author:Shivani Agarwal category:cs.LG stat.ML I.2.6 published:2012-07-02 summary:The problem of bipartite ranking, where instances are labeled positive ornegative and the goal is to learn a scoring function that minimizes theprobability of mis-ranking a pair of positive and negative instances (orequivalently, that maximizes the area under the ROC curve), has been widelystudied in recent years. A dominant theoretical and algorithmic framework forthe problem has been to reduce bipartite ranking to pairwise classification; inparticular, it is well known that the bipartite ranking regret can beformulated as a pairwise classification regret, which in turn can be upperbounded using usual regret bounds for classification problems. Recently,Kotlowski et al. (2011) showed regret bounds for bipartite ranking in terms ofthe regret associated with balanced versions of the standard (non-pairwise)logistic and exponential losses. In this paper, we show that such(non-pairwise) surrogate regret bounds for bipartite ranking can be obtained interms of a broad class of proper (composite) losses that we term as stronglyproper. Our proof technique is much simpler than that of Kotlowski et al.(2011), and relies on properties of proper (composite) losses as elucidatedrecently by Reid and Williamson (2010, 2011) and others. Our result yieldsexplicit surrogate bounds (with no hidden balancing terms) in terms of avariety of strongly proper losses, including for example logistic, exponential,squared and squared hinge losses as special cases. We also obtain tightersurrogate bounds under certain low-noise conditions via a recent result ofClemencon and Robbiano (2011).
arxiv-900-196 | Sequential Design for Computer Experiments with a Flexible Bayesian Additive Model | http://arxiv.org/pdf/1203.1078v2.pdf | author:Hugh Chipman, Pritam Ranjan, Weiwei Wang category:stat.ME stat.ML published:2012-03-06 summary:In computer experiments, a mathematical model implemented on a computer isused to represent complex physical phenomena. These models, known as computersimulators, enable experimental study of a virtual representation of thecomplex phenomena. Simulators can be thought of as complex functions that takemany inputs and provide an output. Often these simulators are themselvesexpensive to compute, and may be approximated by "surrogate models" such asstatistical regression models. In this paper we consider a new kind ofsurrogate model, a Bayesian ensemble of trees (Chipman et al. 2010), with thespecific goal of learning enough about the simulator that a particular featureof the simulator can be estimated. We focus on identifying the simulator'sglobal minimum. Utilizing the Bayesian version of the Expected Improvementcriterion (Jones et al. 1998), we show that this ensemble is particularlyeffective when the simulator is ill-behaved, exhibiting nonstationarity orabrupt changes in the response. A number of illustrations of the approach aregiven, including a tidal power application.
arxiv-900-197 | Single parameter galaxy classification: The Principal Curve through the multi-dimensional space of galaxy properties | http://arxiv.org/pdf/1207.0170v1.pdf | author:M. Taghizadeh-Popp, S. Heinis, A. S. Szalay category:astro-ph.CO cs.CV stat.ML published:2012-07-01 summary:We propose to describe the variety of galaxies from SDSS by using only oneaffine parameter. To this aim, we build the Principal Curve (P-curve) passingthrough the spine of the data point cloud, considering the eigenspace derivedfrom Principal Component Analysis of morphological, physical and photometricgalaxy properties. Thus, galaxies can be labeled, ranked and classified by asingle arc length value of the curve, measured at the unique closest projectionof the data points on the P-curve. We find that the P-curve has a "W" lettershape with 3 turning points, defining 4 branches that represent distinct galaxypopulations. This behavior is controlled mainly by 2 properties, namely u-r andSFR. We further present the variations of several galaxy properties as afunction of arc length. Luminosity functions variate from steep Schechter fitsat low arc length, to double power law and ending in Log-normal fits at higharc length. Galaxy clustering shows increasing autocorrelation power at largescales as arc length increases. PCA analysis allowed to find peculiar galaxypopulations located apart from the main cloud of data points, such as small redgalaxies dominated by a disk, of relatively high stellar mass-to-light ratioand surface mass density. The P-curve allows not only dimensionality reduction,but also provides supporting evidence for relevant physical models andscenarios in extragalactic astronomy: 1) Evidence for the hierarchical mergingscenario in the formation of a selected group of red massive galaxies. Thesegalaxies present a log-normal r-band luminosity function, which might arisefrom multiplicative processes involved in this scenario. 2) Connection betweenthe onset of AGN activity and star formation quenching, which appears in greengalaxies when transitioning from blue to red populations. (Full abstract indownloadable version)
arxiv-900-198 | Differentiable Pooling for Hierarchical Feature Learning | http://arxiv.org/pdf/1207.0151v1.pdf | author:Matthew D. Zeiler, Rob Fergus category:cs.CV cs.LG published:2012-06-30 summary:We introduce a parametric form of pooling, based on a Gaussian, which can beoptimized alongside the features in a single global objective function. Bycontrast, existing pooling schemes are based on heuristics (e.g. local maximum)and have no clear link to the cost function of the model. Furthermore, thevariables of the Gaussian explicitly store location information, distinct fromthe appearance captured by the features, thus providing a what/wheredecomposition of the input signal. Although the differentiable pooling schemecan be incorporated in a wide range of hierarchical models, we demonstrate itin the context of a Deconvolutional Network model (Zeiler et al. ICCV 2011). Wealso explore a number of secondary issues within this model and presentdetailed experiments on MNIST digits.
arxiv-900-199 | Distributed Robust Power System State Estimation | http://arxiv.org/pdf/1204.0991v2.pdf | author:Vassilis Kekatos, Georgios B. Giannakis category:stat.ML math.OC published:2012-04-04 summary:Deregulation of energy markets, penetration of renewables, advanced meteringcapabilities, and the urge for situational awareness, all call for system-widepower system state estimation (PSSE). Implementing a centralized estimatorthough is practically infeasible due to the complexity scale of aninterconnection, the communication bottleneck in real-time monitoring, regionaldisclosure policies, and reliability issues. In this context, distributed PSSEmethods are treated here under a unified and systematic framework. A novelalgorithm is developed based on the alternating direction method ofmultipliers. It leverages existing PSSE solvers, respects privacy policies,exhibits low communication load, and its convergence to the centralizedestimates is guaranteed even in the absence of local observability. Beyond theconventional least-squares based PSSE, the decentralized framework accommodatesa robust state estimator. By exploiting interesting links to the compressivesampling advances, the latter jointly estimates the state and identifiescorrupted measurements. The novel algorithms are numerically evaluated usingthe IEEE 14-, 118-bus, and a 4,200-bus benchmarks. Simulations demonstrate thatthe attainable accuracy can be reached within a few inter-area exchanges, whilelargest residual tests are outperformed.
arxiv-900-200 | Learning High-Dimensional Mixtures of Graphical Models | http://arxiv.org/pdf/1203.0697v2.pdf | author:A. Anandkumar, D. Hsu, F. Huang, S. M. Kakade category:stat.ML cs.AI cs.LG published:2012-03-04 summary:We consider unsupervised estimation of mixtures of discrete graphical models,where the class variable corresponding to the mixture components is hidden andeach mixture component over the observed variables can have a potentiallydifferent Markov graph structure and parameters. We propose a novel approachfor estimating the mixture components, and our output is a tree-mixture modelwhich serves as a good approximation to the underlying graphical model mixture.Our method is efficient when the union graph, which is the union of the Markovgraphs of the mixture components, has sparse vertex separators between any pairof observed variables. This includes tree mixtures and mixtures of boundeddegree graphs. For such models, we prove that our method correctly recovers theunion graph structure and the tree structures corresponding tomaximum-likelihood tree approximations of the mixture components. The sampleand computational complexities of our method scale as $\poly(p, r)$, for an$r$-component mixture of $p$-variate graphical models. We further extend ourresults to the case when the union graph has sparse local separators betweenany pair of observed variables, such as mixtures of locally tree-like graphs,and the mixture components are in the regime of correlation decay.
arxiv-900-201 | Optimally-Weighted Herding is Bayesian Quadrature | http://arxiv.org/pdf/1204.1664v2.pdf | author:Ferenc Huszár, David Duvenaud category:stat.ML math.NA G.1.4 published:2012-04-07 summary:Herding and kernel herding are deterministic methods of choosing sampleswhich summarise a probability distribution. A related task is choosing samplesfor estimating integrals using Bayesian quadrature. We show that the criterionminimised when selecting samples in kernel herding is equivalent to theposterior variance in Bayesian quadrature. We then show that sequentialBayesian quadrature can be viewed as a weighted version of kernel herding whichachieves performance superior to any other weighted herding method. Wedemonstrate empirically a rate of convergence faster than O(1/N). Our resultsalso imply an upper bound on the empirical error of the Bayesian quadratureestimate.
arxiv-900-202 | Density-Difference Estimation | http://arxiv.org/pdf/1207.0099v1.pdf | author:Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Marthinus Christoffel du Plessis, Song Liu, Ichiro Takeuchi category:cs.LG stat.ML published:2012-06-30 summary:We address the problem of estimating the difference between two probabilitydensities. A naive approach is a two-step procedure of first estimating twodensities separately and then computing their difference. However, such atwo-step procedure does not necessarily work well because the first step isperformed without regard to the second step and thus a small error incurred inthe first stage can cause a big error in the second stage. In this paper, wepropose a single-shot procedure for directly estimating the density differencewithout separately estimating two densities. We derive a non-parametricfinite-sample error bound for the proposed single-shot density-differenceestimator and show that it achieves the optimal convergence rate. Theusefulness of the proposed method is also demonstrated experimentally.
arxiv-900-203 | Decoupling Exploration and Exploitation in Multi-Armed Bandits | http://arxiv.org/pdf/1205.2874v3.pdf | author:Orly Avner, Shie Mannor, Ohad Shamir category:cs.LG published:2012-05-13 summary:We consider a multi-armed bandit problem where the decision maker can exploreand exploit different arms at every round. The exploited arm adds to thedecision maker's cumulative reward (without necessarily observing the reward)while the explored arm reveals its value. We devise algorithms for this setupand show that the dependence on the number of arms, k, can be much better thanthe standard square root of k dependence, depending on the behavior of thearms' reward sequences. For the important case of piecewise stationarystochastic bandits, we show a significant improvement over existing algorithms.Our algorithms are based on a non-uniform sampling policy, which we show isessential to the success of any algorithm in the adversarial setup. Finally, weshow some simulation results on an ultra-wide band channel selection inspiredsetting indicating the applicability of our algorithms.
arxiv-900-204 | Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders | http://arxiv.org/pdf/1207.0057v1.pdf | author:Yoshua Bengio, Guillaume Alain, Salah Rifai category:cs.LG stat.ML published:2012-06-30 summary:Recent work suggests that some auto-encoder variants do a good job ofcapturing the local manifold structure of the unknown data generating density.This paper contributes to the mathematical understanding of this phenomenon andhelps define better justified sampling algorithms for deep learning based onauto-encoder variants. We consider an MCMC where each step samples from aGaussian whose mean and covariance matrix depend on the previous state, definesthrough its asymptotic distribution a target density. First, we show that goodchoices (in the sense of consistency) for these mean and covariance functionsare the local expected value and local covariance under that target density.Then we show that an auto-encoder with a contractive penalty capturesestimators of these local moments in its reconstruction function and itsJacobian. A contribution of this work is thus a novel alternative tomaximum-likelihood density estimation, which we call local moment matching. Italso justifies a recently proposed sampling algorithm for the ContractiveAuto-Encoder and extends it to the Denoising Auto-Encoder.
arxiv-900-205 | A Hybrid Method for Distance Metric Learning | http://arxiv.org/pdf/1206.7112v1.pdf | author:Yi-Hao Kao, Benjamin Van Roy, Daniel Rubin, Jiajing Xu, Jessica Faruque, Sandy Napel category:cs.LG cs.IR stat.ML published:2012-06-29 summary:We consider the problem of learning a measure of distance among vectors in afeature space and propose a hybrid method that simultaneously learns fromsimilarity ratings assigned to pairs of vectors and class labels assigned toindividual vectors. Our method is based on a generative model in which classlabels can provide information that is not encoded in feature vectors but yetrelates to perceived similarity between objects. Experiments with syntheticdata as well as a real medical image retrieval problem demonstrate thatleveraging class labels through use of our method improves retrievalperformance significantly.
arxiv-900-206 | Cumulative Step-size Adaptation on Linear Functions: Technical Report | http://arxiv.org/pdf/1206.1208v2.pdf | author:Alexandre Adrien Chotard, Anne Auger, Nikolaus Hansen category:cs.LG published:2012-06-06 summary:The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation,where the step size is adapted measuring the length of a so-called cumulativepath. The cumulative path is a combination of the previous steps realized bythe algorithm, where the importance of each step decreases with time. Thisarticle studies the CSA-ES on composites of strictly increasing with affinelinear functions through the investigation of its underlying Markov chains.Rigorous results on the change and the variation of the step size are derivedwith and without cumulation. The step-size diverges geometrically fast in mostcases. Furthermore, the influence of the cumulation parameter is studied.
arxiv-900-207 | Visual Vocabulary Learning and Its Application to 3D and Mobile Visual Search | http://arxiv.org/pdf/1207.7244v1.pdf | author:Liujuan Cao category:cs.CV published:2012-06-29 summary:In this technical report, we review related works and recent trends in visualvocabulary based web image search, object recognition, mobile visual search,and 3D object retrieval. Especial focuses would be also given for the recenttrends in supervised/unsupervised vocabulary optimization, compact descriptorfor visual search, as well as in multi-view based 3D object representation.
arxiv-900-208 | Preliminary Design of Debris Removal Missions by Means of Simplified Models for Low-Thrust, Many-Revolution Transfers | http://arxiv.org/pdf/1207.3749v1.pdf | author:Federico Zuiani, Massimiliano Vasile category:math.OC cs.NE published:2012-06-29 summary:This paper presents a novel approach for the preliminary design ofLow-Thrust, many-revolution transfers. The main feature of the novel approachis a considerable reduction in the control parameters and a consequent gain incomputational speed. Each spiral is built by using a predefined pattern forthrust direction and switching structure. The pattern is then optimised tominimise propellant consumption and transfer time. The variation of the orbitalelements due to the thrust is computed analytically from a first-order solutionof the perturbed Keplerian motion. The proposed approach allows for a realisticestimation of {\Delta}V and time of flight required to transfer a spacecraftbetween two arbitrary orbits. Eccentricity and plane changes are both accountedfor. The novel approach is applied here to the design of missions for theremoval of space debris by means of an Ion Beam Shepherd Spacecraft. Inparticular, two slightly different variants of the proposed low-thrust controlmodel are used for the different phases of the mission. Thanks to their lowcomputational cost they can be included in a multiobjective optimisationproblem in which the sequence and timing of the removal of five pieces ofdebris are optimised to minimise propellant consumption and mission duration.
arxiv-900-209 | Smoothing proximal gradient method for general structured sparse regression | http://arxiv.org/pdf/1005.4717v4.pdf | author:Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Carbonell, Eric P. Xing category:stat.ML cs.LG math.OC stat.AP stat.CO published:2010-05-26 summary:We study the problem of estimating high-dimensional regression modelsregularized by a structured sparsity-inducing penalty that encodes priorstructural information on either the input or output variables. We consider twowidely adopted types of penalties of this kind as motivating examples: (1) thegeneral overlapping-group-lasso penalty, generalized from the group-lassopenalty; and (2) the graph-guided-fused-lasso penalty, generalized from thefused-lasso penalty. For both types of penalties, due to their nonseparabilityand nonsmoothness, developing an efficient optimization method remains achallenging problem. In this paper we propose a general optimization approach,the smoothing proximal gradient (SPG) method, which can solve structured sparseregression problems with any smooth convex loss under a wide spectrum ofstructured sparsity-inducing penalties. Our approach combines a smoothingtechnique with an effective proximal gradient method. It achieves a convergencerate significantly faster than the standard first-order methods, subgradientmethods, and is much more scalable than the most widely used interior-pointmethods. The efficiency and scalability of our method are demonstrated on bothsimulation experiments and real genetic data sets.
arxiv-900-210 | Learning Neighborhoods for Metric Learning | http://arxiv.org/pdf/1206.6883v1.pdf | author:Jun Wang, Adam Woznica, Alexandros Kalousis category:cs.LG published:2012-06-28 summary:Metric learning methods have been shown to perform well on different learningtasks. Many of them rely on target neighborhood relationships that are computedin the original feature space and remain fixed throughout learning. As aresult, the learned metric reflects the original neighborhood relations. Wepropose a novel formulation of the metric learning problem in which, inaddition to the metric, the target neighborhood relations are also learned in atwo-step iterative approach. The new formulation can be seen as ageneralization of many existing metric learning methods. The formulationincludes a target neighbor assignment rule that assigns different numbers ofneighbors to instances according to their quality; `high quality' instances getmore neighbors. We experiment with two of its instantiations that correspond tothe metric learning algorithms LMNN and MCML and compare it to other metriclearning methods on a number of datasets. The experimental results showstate-of-the-art performance and provide evidence that learning theneighborhood relations does improve predictive performance.
arxiv-900-211 | Elimination of Spurious Ambiguity in Transition-Based Dependency Parsing | http://arxiv.org/pdf/1206.6735v1.pdf | author:Shay B. Cohen, Carlos Gómez-Rodríguez, Giorgio Satta category:cs.CL cs.AI published:2012-06-28 summary:We present a novel technique to remove spurious ambiguity from transitionsystems for dependency parsing. Our technique chooses a canonical sequence oftransition operations (computation) for a given dependency tree. Our techniquecan be applied to a large class of bottom-up transition systems, including forinstance Nivre (2004) and Attardi (2006).
arxiv-900-212 | Piecewise Linear Topology, Evolutionary Algorithms, and Optimization Problems | http://arxiv.org/pdf/1206.6722v1.pdf | author:Andrew Clark category:cs.NE math.GN math.OC I.6.1 published:2012-06-28 summary:Schemata theory, Markov chains, and statistical mechanics have been used toexplain how evolutionary algorithms (EAs) work. Incremental success has beenachieved with all of these methods, but each has been stymied by limitationsrelated to its less-than-global view. We show that moving the investigationinto topological space improves our understanding of why EAs work.
arxiv-900-213 | Merging Belief Propagation and the Mean Field Approximation: A Free Energy Approach | http://arxiv.org/pdf/1112.0467v3.pdf | author:Erwin Riegler, Gunvor Elisabeth Kirkelund, Carles Navarro Manchón, Mihai-Alin Badiu, Bernard Henry Fleury category:cs.IT math.IT stat.ML published:2011-12-02 summary:We present a joint message passing approach that combines belief propagationand the mean field approximation. Our analysis is based on the region-basedfree energy approximation method proposed by Yedidia et al. We show that themessage passing fixed-point equations obtained with this combination correspondto stationary points of a constrained region-based free energy approximation.Moreover, we present a convergent implementation of these message passingfixedpoint equations provided that the underlying factor graph fulfills certaintechnical conditions. In addition, we show how to include hard constraints inthe part of the factor graph corresponding to belief propagation. Finally, wedemonstrate an application of our method to iterative channel estimation anddecoding in an orthogonal frequency division multiplexing (OFDM) system.
arxiv-900-214 | Plug-in martingales for testing exchangeability on-line | http://arxiv.org/pdf/1204.3251v2.pdf | author:Valentina Fedorova, Alex Gammerman, Ilia Nouretdinov, Vladimir Vovk category:cs.LG stat.ME 62G10 I.2.6 published:2012-04-15 summary:A standard assumption in machine learning is the exchangeability of data,which is equivalent to assuming that the examples are generated from the sameprobability distribution independently. This paper is devoted to testing theassumption of exchangeability on-line: the examples arrive one by one, andafter receiving each example we would like to have a valid measure of thedegree to which the assumption of exchangeability has been falsified. Suchmeasures are provided by exchangeability martingales. We extend knowntechniques for constructing exchangeability martingales and show that our newmethod is competitive with the martingales introduced before. Finally weinvestigate the performance of our testing method on two benchmark datasets,USPS and Statlog Satellite data; for the former, the known techniques givesatisfactory results, but for the latter our new more flexible method becomesnecessary.
arxiv-900-215 | Decentralized Data Fusion and Active Sensing with Mobile Sensors for Modeling and Predicting Spatiotemporal Traffic Phenomena | http://arxiv.org/pdf/1206.6230v2.pdf | author:Jie Chen, Kian Hsiang Low, Colin Keng-Yan Tan, Ali Oran, Patrick Jaillet, John M. Dolan, Gaurav S. Sukhatme category:cs.LG cs.AI cs.DC cs.MA cs.RO published:2012-06-27 summary:The problem of modeling and predicting spatiotemporal traffic phenomena overan urban road network is important to many traffic applications such asdetecting and forecasting congestion hotspots. This paper presents adecentralized data fusion and active sensing (D2FAS) algorithm for mobilesensors to actively explore the road network to gather and assimilate the mostinformative data for predicting the traffic phenomenon. We analyze the time andcommunication complexity of D2FAS and demonstrate that it can scale well with alarge number of observations and sensors. We provide a theoretical guarantee onits predictive performance to be equivalent to that of a sophisticatedcentralized sparse approximation for the Gaussian process (GP) model: Thecomputation of such a sparse approximate GP model can thus be parallelized anddistributed among the mobile sensors (in a Google-like MapReduce paradigm),thereby achieving efficient and scalable prediction. We also theoreticallyguarantee its active sensing performance that improves under various practicalenvironmental conditions. Empirical evaluation on real-world urban road networkdata shows that our D2FAS algorithm is significantly more time-efficient andscalable than state-of-the-art centralized algorithms while achievingcomparable predictive performance.
arxiv-900-216 | A Scalable Bootstrap for Massive Data | http://arxiv.org/pdf/1112.5016v2.pdf | author:Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael I. Jordan category:stat.ME stat.CO stat.ML published:2011-12-21 summary:The bootstrap provides a simple and powerful means of assessing the qualityof estimators. However, in settings involving large datasets---which areincreasingly prevalent---the computation of bootstrap-based quantities can beprohibitively demanding computationally. While variants such as subsampling andthe $m$ out of $n$ bootstrap can be used in principle to reduce the cost ofbootstrap computations, we find that these methods are generally not robust tospecification of hyperparameters (such as the number of subsampled datapoints), and they often require use of more prior information (such as rates ofconvergence of estimators) than the bootstrap. As an alternative, we introducethe Bag of Little Bootstraps (BLB), a new procedure which incorporates featuresof both the bootstrap and subsampling to yield a robust, computationallyefficient means of assessing the quality of estimators. BLB is well suited tomodern parallel and distributed computing architectures and furthermore retainsthe generic applicability and statistical efficiency of the bootstrap. Wedemonstrate BLB's favorable statistical performance via a theoretical analysiselucidating the procedure's properties, as well as a simulation study comparingBLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. Inaddition, we present results from a large-scale distributed implementation ofBLB demonstrating its computational superiority on massive data, a method foradaptively selecting BLB's hyperparameters, an empirical study applying BLB toseveral real datasets, and an extension of BLB to time series data.
arxiv-900-217 | A Permutation Approach to Testing Interactions in Many Dimensions | http://arxiv.org/pdf/1206.6519v1.pdf | author:Noah Simon, Robert Tibshirani category:stat.ML stat.CO stat.ME published:2012-06-27 summary:To date, testing interactions in high dimensions has been a challenging task.Existing methods often have issues with sensitivity to modeling assumptions andheavily asymptotic nominal p-values. To help alleviate these issues, we proposea permutation-based method for testing marginal interactions with a binaryresponse. Our method searches for pairwise correlations which differ betweenclasses. In this manuscript, we compare our method on real and simulated datato the standard approach of running many pairwise logistic models. On simulateddata our method finds more significant interactions at a lower false discoveryrate (especially in the presence of main effects). On real genomic data,although there is no gold standard, our method finds apparent signal and tellsa believable story, while logistic regression does not. We also give asymptoticconsistency results under not too restrictive assumptions.
arxiv-900-218 | Investigation of Color Constancy for Ubiquitous Wireless LAN/Camera Positioning: An Initial Outcome | http://arxiv.org/pdf/1206.6514v1.pdf | author:Wan Mohd Yaakob Wan Bejuri, Mohd Murtadha Mohamad, Maimunah Sapri, Mohd Adly Rosly category:cs.CV cs.HC published:2012-06-27 summary:This paper present our color constancy investigation in the hybridization ofWireless LAN and Camera positioning in the mobile phone. Five typical colorconstancy schemes are analyzed in different location environment. The resultscan be used to combine with RF signals from Wireless LAN positioning by usingmodel fitting approach in order to establish absolute positioning output. Thereis no conventional searching algorithm required, thus it is expected to reducethe complexity of computation. Finally we present our preliminary results toillustrate the indoor positioning algorithm performance evaluation for anindoor environment set-up.
arxiv-900-219 | Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring | http://arxiv.org/pdf/1206.6380v1.pdf | author:Sungjin Ahn, Anoop Korattikara, Max Welling category:cs.LG stat.CO stat.ML published:2012-06-27 summary:In this paper we address the following question: Can we approximately samplefrom a Bayesian posterior distribution if we are only allowed to touch a smallmini-batch of data-items for every sample we generate?. An algorithm based onthe Langevin equation with stochastic gradients (SGLD) was previously proposedto solve this, but its mixing rate was slow. By leveraging the Bayesian CentralLimit Theorem, we extend the SGLD algorithm so that at high mixing rates itwill sample from a normal approximation of the posterior, while for slow mixingrates it will mimic the behavior of SGLD with a pre-conditioner matrix. As abonus, the proposed algorithm is reminiscent of Fisher scoring (with stochasticgradients) and as such an efficient optimizer during burn-in.
arxiv-900-220 | High-Dimensional Covariance Decomposition into Sparse Markov and Independence Domains | http://arxiv.org/pdf/1206.6382v1.pdf | author:Majid Janzamin, Animashree Anandkumar category:cs.LG stat.ML published:2012-06-27 summary:In this paper, we present a novel framework incorporating a combination ofsparse models in different domains. We posit the observed data as generatedfrom a linear combination of a sparse Gaussian Markov model (with a sparseprecision matrix) and a sparse Gaussian independence model (with a sparsecovariance matrix). We provide efficient methods for decomposition of the datainto two domains, \viz Markov and independence domains. We characterize a setof sufficient conditions for identifiability and model consistency. Ourdecomposition method is based on a simple modification of the popular$\ell_1$-penalized maximum-likelihood estimator ($\ell_1$-MLE). We establishthat our estimator is consistent in both the domains, i.e., it successfullyrecovers the supports of both Markov and independence models, when the numberof samples $n$ scales as $n = \Omega(d^2 \log p)$, where $p$ is the number ofvariables and $d$ is the maximum node degree in the Markov model. Ourconditions for recovery are comparable to those of $\ell_1$-MLE for consistentestimation of a sparse Markov model, and thus, we guarantee successfulhigh-dimensional estimation of a richer class of models under comparableconditions. Our experiments validate these results and also demonstrate thatour models have better inference accuracy under simple algorithms such as loopybelief propagation.
arxiv-900-221 | Feature Selection via Probabilistic Outputs | http://arxiv.org/pdf/1206.6383v1.pdf | author:Andrea Danyluk, Nicholas Arnosti category:cs.LG stat.ML published:2012-06-27 summary:This paper investigates two feature-scoring criteria that make use ofestimated class probabilities: one method proposed by \citet{shen} and acomplementary approach proposed below. We develop a theoretical framework toanalyze each criterion and show that both estimate the spread (across allvalues of a given feature) of the probability that an example belongs to thepositive class. Based on our analysis, we predict when each scoring techniquewill be advantageous over the other and give empirical results validating ourpredictions.
arxiv-900-222 | Efficient and Practical Stochastic Subgradient Descent for Nuclear Norm Regularization | http://arxiv.org/pdf/1206.6384v1.pdf | author:Haim Avron, Satyen Kale, Shiva Kasiviswanathan, Vikas Sindhwani category:cs.LG stat.ML published:2012-06-27 summary:We describe novel subgradient methods for a broad class of matrixoptimization problems involving nuclear norm regularization. Unlike existingapproaches, our method executes very cheap iterations by combining low-rankstochastic subgradients with efficient incremental SVD updates, made possibleby highly optimized and parallelizable dense linear algebra operations on smallmatrices. Our practical algorithms always maintain a low-rank factorization ofiterates that can be conveniently held in memory and efficiently multiplied togenerate predictions in matrix completion settings. Empirical comparisonsconfirm that our approach is highly competitive with several recently proposedstate-of-the-art solvers for such problems.
arxiv-900-223 | Improved Estimation in Time Varying Models | http://arxiv.org/pdf/1206.6385v1.pdf | author:Doina Precup, Philip Bachman category:cs.LG stat.ME stat.ML published:2012-06-27 summary:Locally adapted parameterizations of a model (such as locally weightedregression) are expressive but often suffer from high variance. We describe anapproach for reducing the variance, based on the idea of estimatingsimultaneously a transformed space for the model, as well as locally adaptedparameterizations in this new space. We present a new problem formulation thatcaptures this idea and illustrate it in the important context of time varyingmodels. We develop an algorithm for learning a set of bases for approximating atime varying sparse network; each learned basis constitutes an archetypalsparse network structure. We also provide an extension for learning task-drivenbases. We present empirical results on synthetic data sets, as well as on a BCIEEG classification task.
arxiv-900-224 | How To Grade a Test Without Knowing the Answers --- A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing | http://arxiv.org/pdf/1206.6386v1.pdf | author:Yoram Bachrach, Thore Graepel, Tom Minka, John Guiver category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We propose a new probabilistic graphical model that jointly models thedifficulties of questions, the abilities of participants and the correctanswers to questions in aptitude testing and crowdsourcing settings. We devisean active learning/adaptive testing scheme based on a greedy minimization ofexpected model entropy, which allows a more efficient resource allocation bydynamically choosing the next question to be asked based on the previousresponses. We present experimental results that confirm the ability of ourmodel to infer the required parameters and demonstrate that the adaptivetesting scheme requires fewer questions to obtain the same accuracy as a statictest scenario.
arxiv-900-225 | Fast classification using sparse decision DAGs | http://arxiv.org/pdf/1206.6387v1.pdf | author:Djalel Benbouzid, Robert Busa-Fekete, Balazs Kegl category:cs.LG stat.ML published:2012-06-27 summary:In this paper we propose an algorithm that builds sparse decision DAGs(directed acyclic graphs) from a list of base classifiers provided by anexternal learning method such as AdaBoost. The basic idea is to cast the DAGdesign task as a Markov decision process. Each instance can decide to use or toskip each base classifier, based on the current state of the classifier beingbuilt. The result is a sparse decision DAG where the base classifiers areselected in a data-dependent way. The method has a single hyperparameter with aclear semantics of controlling the accuracy/speed trade-off. The algorithm iscompetitive with state-of-the-art cascade detectors on three object-detectionbenchmarks, and it clearly outperforms them when there is a small number ofbase classifiers. Unlike cascades, it is also readily applicable formulti-class classification. Using the multi-class setup, we show on a benchmarkweb page ranking data set that we can significantly improve the decision speedwithout harming the performance of the ranker.
arxiv-900-226 | Canonical Trends: Detecting Trend Setters in Web Data | http://arxiv.org/pdf/1206.6388v1.pdf | author:Felix Biessmann, Jens-Michalis Papaioannou, Mikio Braun, Andreas Harth category:cs.LG cs.SI stat.ML published:2012-06-27 summary:Much information available on the web is copied, reused or rephrased. Thephenomenon that multiple web sources pick up certain information is oftencalled trend. A central problem in the context of web data mining is to detectthose web sources that are first to publish information which will give rise toa trend. We present a simple and efficient method for finding trends dominatinga pool of web sources and identifying those web sources that publish theinformation relevant to a trend before others. We validate our approach on realdata collected from influential technology news feeds.
arxiv-900-227 | Incorporating Causal Prior Knowledge as Path-Constraints in Bayesian Networks and Maximal Ancestral Graphs | http://arxiv.org/pdf/1206.6390v1.pdf | author:Giorgos Borboudakis, Ioannis Tsamardinos category:cs.AI cs.CE cs.LG published:2012-06-27 summary:We consider the incorporation of causal knowledge about the presence orabsence of (possibly indirect) causal relations into a causal model. Suchcausal relations correspond to directed paths in a causal model. This type ofknowledge naturally arises from experimental data, among others. Specifically,we consider the formalisms of Causal Bayesian Networks and Maximal AncestralGraphs and their Markov equivalence classes: Partially Directed Acyclic Graphsand Partially Oriented Ancestral Graphs. We introduce sound and completeprocedures which are able to incorporate causal prior knowledge in such models.In simulated experiments, we show that often considering even a few causalfacts leads to a significant number of new inferences. In a case study, we alsoshow how to use real experimental data to infer causal knowledge andincorporate it into a real biological causal network. The code is available atmensxmachina.org.
arxiv-900-228 | Gaussian Process Quantile Regression using Expectation Propagation | http://arxiv.org/pdf/1206.6391v1.pdf | author:Alexis Boukouvalas, Remi Barillec, Dan Cornford category:stat.ME cs.LG stat.AP published:2012-06-27 summary:Direct quantile regression involves estimating a given quantile of a responsevariable as a function of input variables. We present a new framework fordirect quantile regression where a Gaussian process model is learned,minimising the expected tilted loss function. The integration required inlearning is not analytically tractable so to speed up the learning we employthe Expectation Propagation algorithm. We describe how this work relates toother quantile regression methods and apply the method on both synthetic andreal data sets. The method is shown to be competitive with state of the artmethods whilst allowing for the leverage of the full Gaussian processprobabilistic framework.
arxiv-900-229 | Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription | http://arxiv.org/pdf/1206.6392v1.pdf | author:Nicolas Boulanger-Lewandowski, Yoshua Bengio, Pascal Vincent category:cs.LG cs.SD stat.ML published:2012-06-27 summary:We investigate the problem of modeling symbolic sequences of polyphonic musicin a completely general piano-roll representation. We introduce a probabilisticmodel based on distribution estimators conditioned on a recurrent neuralnetwork that is able to discover temporal dependencies in high-dimensionalsequences. Our approach outperforms many traditional models of polyphonic musicon a variety of realistic datasets. We show how our musical language model canserve as a symbolic prior to improve the accuracy of polyphonic transcription.
arxiv-900-230 | Local Loss Optimization in Operator Models: A New Insight into Spectral Learning | http://arxiv.org/pdf/1206.6393v1.pdf | author:Borja Balle, Ariadna Quattoni, Xavier Carreras category:cs.LG stat.ML published:2012-06-27 summary:This paper re-visits the spectral method for learning latent variable modelsdefined in terms of observable operators. We give a new perspective on themethod, showing that operators can be recovered by minimizing a loss defined ona finite subset of the domain. A non-convex optimization similar to thespectral method is derived. We also propose a regularized convex relaxation ofthis optimization. We show that in practice the availabilty of a continuousregularization parameter (in contrast with the discrete number of states in theoriginal method) allows a better trade-off between accuracy and modelcomplexity. We also prove that in general, a randomized strategy for choosingthe local loss will succeed with high probability.
arxiv-900-231 | Nonparametric Link Prediction in Dynamic Networks | http://arxiv.org/pdf/1206.6394v1.pdf | author:Purnamrita Sarkar, Deepayan Chakrabarti, Michael Jordan category:cs.LG cs.SI stat.ML published:2012-06-27 summary:We propose a non-parametric link prediction algorithm for a sequence of graphsnapshots over time. The model predicts links based on the features of itsendpoints, as well as those of the local neighborhood around the endpoints.This allows for different types of neighborhoods in a graph, each with its owndynamics (e.g, growing or shrinking communities). We prove the consistency ofour estimator, and give a fast implementation based on locality-sensitivehashing. Experiments with simulated as well as five real-world dynamic graphsshow that we outperform the state of the art, especially when sharpfluctuations or non-linearities are present.
arxiv-900-232 | Convergence Rates for Differentially Private Statistical Estimation | http://arxiv.org/pdf/1206.6395v1.pdf | author:Kamalika Chaudhuri, Daniel Hsu category:cs.LG cs.CR stat.ML published:2012-06-27 summary:Differential privacy is a cryptographically-motivated definition of privacywhich has gained significant attention over the past few years. Differentiallyprivate solutions enforce privacy by adding random noise to a function computedover the data, and the challenge in designing such algorithms is to control theadded noise in order to optimize the privacy-accuracy-sample size tradeoff. This work studies differentially-private statistical estimation, and showsupper and lower bounds on the convergence rates of differentially privateapproximations to statistical estimators. Our results reveal a formalconnection between differential privacy and the notion of Gross ErrorSensitivity (GES) in robust statistics, by showing that the convergence rate ofany differentially private approximation to an estimator that is accurate overa large class of distributions has to grow with the GES of the estimator. Wethen provide an upper bound on the convergence rate of a differentially privateapproximation to an estimator with bounded range and bounded GES. We show thatthe bounded range condition is necessary if we wish to ensure a strict form ofdifferential privacy.
arxiv-900-233 | Joint Optimization and Variable Selection of High-dimensional Gaussian Processes | http://arxiv.org/pdf/1206.6396v1.pdf | author:Bo Chen, Rui Castro, Andreas Krause category:cs.LG stat.ML published:2012-06-27 summary:Maximizing high-dimensional, non-convex functions through noisy observationsis a notoriously hard problem, but one that arises in many applications. Inthis paper, we tackle this challenge by modeling the unknown function as asample from a high-dimensional Gaussian process (GP) distribution. Assumingthat the unknown function only depends on few relevant variables, we show thatit is possible to perform joint variable selection and GP optimization. Weprovide strong performance guarantees for our algorithm, bounding the samplecomplexity of variable selection, and as well as providing cumulative regretbounds. We further provide empirical evidence on the effectiveness of ouralgorithm on several benchmark optimization problems.
arxiv-900-234 | Communications Inspired Linear Discriminant Analysis | http://arxiv.org/pdf/1206.6397v1.pdf | author:Minhua Chen, William Carson, Miguel Rodrigues, Robert Calderbank, Lawrence Carin category:cs.LG stat.ML published:2012-06-27 summary:We study the problem of supervised linear dimensionality reduction, taking aninformation-theoretic viewpoint. The linear projection matrix is designed bymaximizing the mutual information between the projected signal and the classlabel (based on a Shannon entropy measure). By harnessing a recent theoreticalresult on the gradient of mutual information, the above optimization problemcan be solved directly using gradient descent, without requiring simplificationof the objective function. Theoretical analysis and empirical comparison aremade between the proposed method and two closely related methods (LinearDiscriminant Analysis and Information Discriminant Analysis), and comparisonsare also made with a method in which Renyi entropy is used to define the mutualinformation (in this case the gradient may be computed simply, under a specialparameter setting). Relative to these alternative approaches, the proposedmethod achieves promising results on real datasets.
arxiv-900-235 | Demand-Driven Clustering in Relational Domains for Predicting Adverse Drug Events | http://arxiv.org/pdf/1206.6399v1.pdf | author:Jesse Davis, Vitor Santos Costa, Peggy Peissig, Michael Caldwell, Elizabeth Berg, David Page category:cs.LG cs.AI stat.ML published:2012-06-27 summary:Learning from electronic medical records (EMR) is challenging due to theirrelational nature and the uncertain dependence between a patient's past andfuture health status. Statistical relational learning is a natural fit foranalyzing EMRs but is less adept at handling their inherent latent structure,such as connections between related medications or diseases. One way to capturethe latent structure is via a relational clustering of objects. We propose anovel approach that, instead of pre-clustering the objects, performs ademand-driven clustering during learning. We evaluate our algorithm on threereal-world tasks where the goal is to use EMRs to predict whether a patientwill have an adverse reaction to a medication. We find that our approach ismore accurate than performing no clustering, pre-clustering, and usingexpert-constructed medical heterarchies.
arxiv-900-236 | Online Bandit Learning against an Adaptive Adversary: from Regret to Policy Regret | http://arxiv.org/pdf/1206.6400v1.pdf | author:Raman Arora, Ofer Dekel, Ambuj Tewari category:cs.LG stat.ML published:2012-06-27 summary:Online learning algorithms are designed to learn even when their input isgenerated by an adversary. The widely-accepted formal definition of an onlinealgorithm's ability to learn is the game-theoretic notion of regret. We arguethat the standard definition of regret becomes inadequate if the adversary isallowed to adapt to the online algorithm's actions. We define the alternativenotion of policy regret, which attempts to provide a more meaningful way tomeasure an online algorithm's performance against adaptive adversaries.Focusing on the online bandit setting, we show that no bandit algorithm canguarantee a sublinear policy regret against an adaptive adversary withunbounded memory. On the other hand, if the adversary's memory is bounded, wepresent a general technique that converts any bandit algorithm with a sublinearregret bound into an algorithm with a sublinear policy regret bound. We extendthis result to other variants of regret, such as switching regret, internalregret, and swap regret.
arxiv-900-237 | Consistent Multilabel Ranking through Univariate Losses | http://arxiv.org/pdf/1206.6401v1.pdf | author:Krzysztof Dembczynski, Wojciech Kotlowski, Eyke Huellermeier category:cs.LG stat.ML published:2012-06-27 summary:We consider the problem of rank loss minimization in the setting ofmultilabel classification, which is usually tackled by means of convexsurrogate losses defined on pairs of labels. Very recently, this approach wasput into question by a negative result showing that commonly used pairwisesurrogate losses, such as exponential and logistic losses, are inconsistent. Inthis paper, we show a positive result which is arguably surprising in light ofthe previous one: the simpler univariate variants of exponential and logisticsurrogates (i.e., defined on single labels) are consistent for rank lossminimization. Instead of directly proving convergence, we give a much strongerresult by deriving regret bounds and convergence rates. The proposed lossessuggest efficient and scalable algorithms, which are tested experimentally.
arxiv-900-238 | Parallelizing Exploration-Exploitation Tradeoffs with Gaussian Process Bandit Optimization | http://arxiv.org/pdf/1206.6402v1.pdf | author:Thomas Desautels, Andreas Krause, Joel Burdick category:cs.LG stat.ML published:2012-06-27 summary:Can one parallelize complex exploration exploitation tradeoffs? As anexample, consider the problem of optimal high-throughput experimental design,where we wish to sequentially design batches of experiments in order tosimultaneously learn a surrogate function mapping stimulus to response andidentify the maximum of the function. We formalize the task as a multi-armedbandit problem, where the unknown payoff function is sampled from a Gaussianprocess (GP), and instead of a single arm, in each round we pull a batch ofseveral arms in parallel. We develop GP-BUCB, a principled algorithm forchoosing batches, based on the GP-UCB algorithm for sequential GP optimization.We prove a surprising result; as compared to the sequential approach, thecumulative regret of the parallel algorithm only increases by a constant factorindependent of the batch size B. Our results provide rigorous theoreticalsupport for exploiting parallelism in Bayesian global optimization. Wedemonstrate the effectiveness of our approach on two real-world applications.
arxiv-900-239 | Two Step CCA: A new spectral method for estimating vector models of words | http://arxiv.org/pdf/1206.6403v1.pdf | author:Paramveer Dhillon, Jordan Rodu, Dean Foster, Lyle Ungar category:cs.CL cs.LG published:2012-06-27 summary:Unlabeled data is often used to learn representations which can be used tosupplement baseline features in a supervised learner. For example, for textapplications where the words lie in a very high dimensional space (the size ofthe vocabulary), one can learn a low rank "dictionary" by aneigen-decomposition of the word co-occurrence matrix (e.g. using PCA or CCA).In this paper, we present a new spectral method based on CCA to learn aneigenword dictionary. Our improved procedure computes two set of CCAs, thefirst one between the left and right contexts of the given word and the secondone between the projections resulting from this CCA and the word itself. Weprove theoretically that this two-step procedure has lower sample complexitythan the simple single step procedure and also illustrate the empiricalefficacy of our approach and the richness of representations learned by our TwoStep CCA (TSCCA) procedure on the tasks of POS tagging and sentimentclassification.
arxiv-900-240 | Policy Gradients with Variance Related Risk Criteria | http://arxiv.org/pdf/1206.6404v1.pdf | author:Dotan Di Castro, Aviv Tamar, Shie Mannor category:cs.LG cs.CY math.OC stat.ML published:2012-06-27 summary:Managing risk in dynamic decision problems is of cardinal importance in manyfields such as finance and process control. The most common approach todefining risk is through various variance related criteria such as the SharpeRatio or the standard deviation adjusted reward. It is known that optimizingmany of the variance related risk criteria is NP-hard. In this paper we devisea framework for local policy gradient style algorithms for reinforcementlearning for variance related criteria. Our starting point is a new formula forthe variance of the cost-to-go in episodic tasks. Using this formula we developpolicy gradient algorithms for criteria that involve both the expected cost andthe variance of the cost. We prove the convergence of these algorithms to localminima and demonstrate their applicability in a portfolio planning problem.
arxiv-900-241 | Bounded Planning in Passive POMDPs | http://arxiv.org/pdf/1206.6405v1.pdf | author:Roy Fox, Naftali Tishby category:cs.LG cs.AI stat.ML published:2012-06-27 summary:In Passive POMDPs actions do not affect the world state, but still incurcosts. When the agent is bounded by information-processing constraints, it canonly keep an approximation of the belief. We present a variational principlefor the problem of maintaining the information which is most useful forminimizing the cost, and introduce an efficient and simple algorithm forfinding an optimum.
arxiv-900-242 | Bayesian Optimal Active Search and Surveying | http://arxiv.org/pdf/1206.6406v1.pdf | author:Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong, Jeff Schneider, Richard Mann category:cs.LG stat.ML published:2012-06-27 summary:We consider two active binary-classification problems with atypicalobjectives. In the first, active search, our goal is to actively uncover asmany members of a given class as possible. In the second, active surveying, ourgoal is to actively query points to ultimately predict the proportion of agiven class. Numerous real-world problems can be framed in these terms, and ineither case typical model-based concerns such as generalization error are onlyof secondary importance. We approach these problems via Bayesian decision theory; after choosingnatural utility functions, we derive the optimal policies. We provide threecontributions. In addition to introducing the active surveying problem, weextend previous work on active search in two ways. First, we prove a noveltheoretical result, that less-myopic approximations to the optimal policy canoutperform more-myopic approximations by any arbitrary degree. We then derivebounds that for certain models allow us to reduce (in practice dramatically)the exponential search space required by a na?ve implementation of the optimalpolicy, enabling further lookahead while still ensuring that optimal decisionsare always made.
arxiv-900-243 | Large-Scale Feature Learning With Spike-and-Slab Sparse Coding | http://arxiv.org/pdf/1206.6407v1.pdf | author:Ian Goodfellow, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML published:2012-06-27 summary:We consider the problem of object recognition with a large number of classes.In order to overcome the low amount of labeled examples available in thissetting, we introduce a new feature learning and extraction procedure based ona factor model we call spike-and-slab sparse coding (S3C). Prior work on S3Chas not prioritized the ability to exploit parallel architectures and scale S3Cto the enormous problem sizes needed for object recognition. We present a novelinference procedure for appropriate for use with GPUs which allows us todramatically increase both the training set size and the amount of latentfactors that S3C may be trained with. We demonstrate that this approachimproves upon the supervised learning capabilities of both sparse coding andthe spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10dataset. We use the CIFAR-100 dataset to demonstrate that our method scales tolarge numbers of classes better than previous methods. Finally, we use ourmethod to win the NIPS 2011 Workshop on Challenges In Learning HierarchicalModels? Transfer Learning Challenge.
arxiv-900-244 | Sequential Nonparametric Regression | http://arxiv.org/pdf/1206.6408v1.pdf | author:Haijie Gu, John Lafferty category:stat.ME astro-ph.IM cs.LG published:2012-06-27 summary:We present algorithms for nonparametric regression in settings where the dataare obtained sequentially. While traditional estimators select bandwidths thatdepend upon the sample size, for sequential data the effective sample size isdynamically changing. We propose a linear time algorithm that adjusts thebandwidth for each new data point, and show that the estimator achieves theoptimal minimax rate of convergence. We also propose the use of online expertmixing algorithms to adapt to unknown smoothness of the regression function. Weprovide simulations that confirm the theoretical results, and demonstrate theeffectiveness of the methods.
arxiv-900-245 | Scaling Up Coordinate Descent Algorithms for Large $\ell_1$ Regularization Problems | http://arxiv.org/pdf/1206.6409v1.pdf | author:Chad Scherrer, Mahantesh Halappanavar, Ambuj Tewari, David Haglin category:cs.LG cs.DC stat.ML published:2012-06-27 summary:We present a generic framework for parallel coordinate descent (CD)algorithms that includes, as special cases, the original sequential algorithmsCyclic CD and Stochastic CD, as well as the recent parallel Shotgun algorithm.We introduce two novel parallel algorithms that are also specialcases---Thread-Greedy CD and Coloring-Based CD---and give performancemeasurements for an OpenMP implementation of these.
arxiv-900-246 | On the Partition Function and Random Maximum A-Posteriori Perturbations | http://arxiv.org/pdf/1206.6410v1.pdf | author:Tamir Hazan, Tommi Jaakkola category:cs.LG stat.ML published:2012-06-27 summary:In this paper we relate the partition function to the max-statistics ofrandom variables. In particular, we provide a novel framework for approximatingand bounding the partition function using MAP inference on randomly perturbedmodels. As a result, we can use efficient MAP solvers such as graph-cuts toevaluate the corresponding partition function. We show that our method excelsin the typical "high signal - high coupling" regime that results in raggedenergy landscapes difficult for alternative approaches.
arxiv-900-247 | On the Difficulty of Nearest Neighbor Search | http://arxiv.org/pdf/1206.6411v1.pdf | author:Junfeng He, Sanjiv Kumar, Shih-Fu Chang category:cs.LG cs.DB cs.IR stat.ML published:2012-06-27 summary:Fast approximate nearest neighbor (NN) search in large databases is becomingpopular. Several powerful learning-based formulations have been proposedrecently. However, not much attention has been paid to a more fundamentalquestion: how difficult is (approximate) nearest neighbor search in a givendata set? And which data properties affect the difficulty of nearest neighborsearch and how? This paper introduces the first concrete measure calledRelative Contrast that can be used to evaluate the influence of several crucialdata characteristics such as dimensionality, sparsity, and database sizesimultaneously in arbitrary normed metric spaces. Moreover, we present atheoretical analysis to prove how the difficulty measure (relative contrast)determines/affects the complexity of Local Sensitive Hashing, a popularapproximate NN search method. Relative contrast also provides an explanationfor a family of heuristic hashing algorithms with good practical performancebased on PCA. Finally, we show that most of the previous works in measuring NNsearch meaningfulness/difficulty can be derived as special asymptotic cases fordense vectors of the proposed measure.
arxiv-900-248 | A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound | http://arxiv.org/pdf/1206.6412v1.pdf | author:Ming Ji, Tianbao Yang, Binbin Lin, Rong Jin, Jiawei Han category:cs.LG stat.ML published:2012-06-27 summary:In this work, we develop a simple algorithm for semi-supervised regression.The key idea is to use the top eigenfunctions of integral operator derived fromboth labeled and unlabeled examples as the basis functions and learn theprediction function by a simple linear regression. We show that underappropriate assumptions about the integral operator, this approach is able toachieve an improved regression error bound better than existing bounds ofsupervised learning. We also verify the effectiveness of the proposed algorithmby an empirical study.
arxiv-900-249 | A Convex Relaxation for Weakly Supervised Classifiers | http://arxiv.org/pdf/1206.6413v1.pdf | author:Armand Joulin, Francis Bach category:cs.LG stat.ML published:2012-06-27 summary:This paper introduces a general multi-class approach to weakly supervisedclassification. Inferring the labels and learning the parameters of the modelis usually done jointly through a block-coordinate descent algorithm such asexpectation-maximization (EM), which may lead to local minima. To avoid thisproblem, we propose a cost function based on a convex relaxation of thesoft-max loss. We then propose an algorithm specifically designed toefficiently solve the corresponding semidefinite program (SDP). Empirically,our method compares favorably to standard ones on different datasets formultiple instance learning and semi-supervised learning as well as onclustering tasks.
arxiv-900-250 | The Nonparametric Metadata Dependent Relational Model | http://arxiv.org/pdf/1206.6414v1.pdf | author:Dae Il Kim, Michael Hughes, Erik Sudderth category:cs.LG cs.SI stat.ML published:2012-06-27 summary:We introduce the nonparametric metadata dependent relational (NMDR) model, aBayesian nonparametric stochastic block model for network data. The NMDR allowsthe entities associated with each node to have mixed membership in an unboundedcollection of latent communities. Learned regression models allow thesememberships to depend on, and be predicted from, arbitrary node metadata. Wedevelop efficient MCMC algorithms for learning NMDR models from partiallyobserved node relationships. Retrospective MCMC methods allow our sampler towork directly with the infinite stick-breaking representation of the NMDR,avoiding the need for finite truncations. Our results demonstrate recovery ofuseful latent communities from real-world social and ecological networks, andthe usefulness of metadata in link prediction tasks.
arxiv-900-251 | The Big Data Bootstrap | http://arxiv.org/pdf/1206.6415v1.pdf | author:Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael Jordan category:cs.LG stat.ML published:2012-06-27 summary:The bootstrap provides a simple and powerful means of assessing the qualityof estimators. However, in settings involving large datasets, the computationof bootstrap-based quantities can be prohibitively demanding. As analternative, we present the Bag of Little Bootstraps (BLB), a new procedurewhich incorporates features of both the bootstrap and subsampling to obtain arobust, computationally efficient means of assessing estimator quality. BLB iswell suited to modern parallel and distributed computing architectures andretains the generic applicability, statistical efficiency, and favorabletheoretical properties of the bootstrap. We provide the results of an extensiveempirical and theoretical investigation of BLB's behavior, including a study ofits statistical correctness, its large-scale implementation and performance,selection of hyperparameters, and performance on real data.
arxiv-900-252 | An Infinite Latent Attribute Model for Network Data | http://arxiv.org/pdf/1206.6416v1.pdf | author:Konstantina Palla, David Knowles, Zoubin Ghahramani category:cs.LG stat.ML published:2012-06-27 summary:Latent variable models for network data extract a summary of the relationalstructure underlying an observed network. The simplest possible modelssubdivide nodes of the network into clusters; the probability of a link betweenany two nodes then depends only on their cluster assignment. Currentlyavailable models can be classified by whether clusters are disjoint or areallowed to overlap. These models can explain a "flat" clustering structure.Hierarchical Bayesian models provide a natural approach to capture more complexdependencies. We propose a model in which objects are characterised by a latentfeature vector. Each feature is itself partitioned into disjoint groups(subclusters), corresponding to a second layer of hierarchy. In experimentalcomparisons, the model achieves significantly improved predictive performanceon social and biological link prediction tasks. The results indicate thatmodels with a single layer hierarchy over-simplify real networks.
arxiv-900-253 | Learning Task Grouping and Overlap in Multi-task Learning | http://arxiv.org/pdf/1206.6417v1.pdf | author:Abhishek Kumar, Hal Daume III category:cs.LG stat.ML published:2012-06-27 summary:In the paradigm of multi-task learning, mul- tiple related prediction tasksare learned jointly, sharing information across the tasks. We propose aframework for multi-task learn- ing that enables one to selectively share theinformation across the tasks. We assume that each task parameter vector is alinear combi- nation of a finite number of underlying basis tasks. Thecoefficients of the linear combina- tion are sparse in nature and the overlapin the sparsity patterns of two tasks controls the amount of sharing acrossthese. Our model is based on on the assumption that task pa- rameters within agroup lie in a low dimen- sional subspace but allows the tasks in differ- entgroups to overlap with each other in one or more bases. Experimental results onfour datasets show that our approach outperforms competing methods.
arxiv-900-254 | Learning Invariant Representations with Local Transformations | http://arxiv.org/pdf/1206.6418v1.pdf | author:Kihyuk Sohn, Honglak Lee category:cs.LG cs.CV stat.ML published:2012-06-27 summary:Learning invariant representations is an important problem in machinelearning and pattern recognition. In this paper, we present a novel frameworkof transformation-invariant feature learning by incorporating lineartransformations into the feature learning algorithms. For example, we presentthe transformation-invariant restricted Boltzmann machine that compactlyrepresents data by its weights and their transformations, which achievesinvariance of the feature representation via probabilistic max pooling. Inaddition, we show that our transformation-invariant feature learning frameworkcan also be extended to other unsupervised learning methods, such asautoencoders or sparse coding. We evaluate our method on several imageclassification benchmark datasets, such as MNIST variations, CIFAR-10, andSTL-10, and show competitive or superior classification performance whencompared to the state-of-the-art. Furthermore, our method achievesstate-of-the-art performance on phone classification tasks with the TIMITdataset, which demonstrates wide applicability of our proposed algorithms toother domains.
arxiv-900-255 | Cross-Domain Multitask Learning with Latent Probit Models | http://arxiv.org/pdf/1206.6419v1.pdf | author:Shaobo Han, Xuejun Liao, Lawrence Carin category:cs.LG stat.ML published:2012-06-27 summary:Learning multiple tasks across heterogeneous domains is a challenging problemsince the feature space may not be the same for different tasks. We assume thedata in multiple tasks are generated from a latent common domain via sparsedomain transforms and propose a latent probit model (LPM) to jointly learn thedomain transforms, and the shared probit classifier in the common domain. Tolearn meaningful task relatedness and avoid over-fitting in classification, weintroduce sparsity in the domain transforms matrices, as well as in the commonclassifier. We derive theoretical bounds for the estimation error of theclassifier in terms of the sparsity of domain transforms. Anexpectation-maximization algorithm is derived for learning the LPM. Theeffectiveness of the approach is demonstrated on several real datasets.
arxiv-900-256 | Distributed Parameter Estimation via Pseudo-likelihood | http://arxiv.org/pdf/1206.6420v1.pdf | author:Qiang Liu, Alexander Ihler category:cs.LG cs.DC stat.ML published:2012-06-27 summary:Estimating statistical models within sensor networks requires distributedalgorithms, in which both data and computation are distributed across the nodesof the network. We propose a general approach for distributed learning based oncombining local estimators defined by pseudo-likelihood components,encompassing a number of combination methods, and provide both theoretical andexperimental analysis. We show that simple linear combination or max-votingmethods, when combined with second-order information, are statisticallycompetitive with more advanced and costly joint optimization. Our algorithmshave many attractive properties including low communication and computationalcost and "any-time" behavior.
arxiv-900-257 | Structured Learning from Partial Annotations | http://arxiv.org/pdf/1206.6421v1.pdf | author:Xinghua Lou, Fred Hamprecht category:cs.LG stat.ML published:2012-06-27 summary:Structured learning is appropriate when predicting structured outputs such astrees, graphs, or sequences. Most prior work requires the training set toconsist of complete trees, graphs or sequences. Specifying such detailed groundtruth can be tedious or infeasible for large outputs. Our main contribution isa large margin formulation that makes structured learning from only partiallyannotated data possible. The resulting optimization problem is non-convex, yetcan be efficiently solve by concave-convex procedure (CCCP) with novel speedupstrategies. We apply our method to a challenging tracking-by-assignment problemof a variable number of divisible objects. On this benchmark, using only 25% ofa full annotation we achieve a performance comparable to a model learned with afull annotation. Finally, we offer a unifying perspective of previous workusing the hinge, ramp, or max loss for structured learning, followed by anempirical comparison on their practical performance.
arxiv-900-258 | An Online Boosting Algorithm with Theoretical Justifications | http://arxiv.org/pdf/1206.6422v1.pdf | author:Shang-Tse Chen, Hsuan-Tien Lin, Chi-Jen Lu category:cs.LG stat.ML published:2012-06-27 summary:We study the task of online boosting--combining online weak learners into anonline strong learner. While batch boosting has a sound theoretical foundation,online boosting deserves more study from the theoretical perspective. In thispaper, we carefully compare the differences between online and batch boosting,and propose a novel and reasonable assumption for the online weak learner.Based on the assumption, we design an online boosting algorithm with a strongtheoretical guarantee by adapting from the offline SmoothBoost algorithm thatmatches the assumption closely. We further tackle the task of deciding thenumber of weak learners using established theoretical results for online convexprogramming and predicting with expert advice. Experiments on real-world datasets demonstrate that the proposed algorithm compares favorably with existingonline boosting algorithms.
arxiv-900-259 | A Joint Model of Language and Perception for Grounded Attribute Learning | http://arxiv.org/pdf/1206.6423v1.pdf | author:Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, Dieter Fox category:cs.CL cs.LG cs.RO published:2012-06-27 summary:As robots become more ubiquitous and capable, it becomes ever more importantto enable untrained users to easily interact with them. Recently, this has ledto study of the language grounding problem, where the goal is to extractrepresentations of the meanings of natural language tied to perception andactuation in the physical world. In this paper, we present an approach forjoint learning of language and perception models for grounded attributeinduction. Our perception model includes attribute classifiers, for example todetect object color and shape, and the language model is based on aprobabilistic categorial grammar that enables the construction of rich,compositional meaning representations. The approach is evaluated on the task ofinterpreting sentences that describe sets of objects in a physical workspace.We demonstrate accurate task performance and effective latent-variable conceptinduction in physical grounded scenes.
arxiv-900-260 | Anytime Marginal MAP Inference | http://arxiv.org/pdf/1206.6424v1.pdf | author:Denis Maua, Cassio De Campos category:cs.AI stat.ML published:2012-06-27 summary:This paper presents a new anytime algorithm for the marginal MAP problem ingraphical models. The algorithm is described in detail, its complexity andconvergence rate are studied, and relations to previous theoretical results forthe problem are discussed. It is shown that the algorithm runs inpolynomial-time if the underlying graph of the model has bounded tree-width,and that it provides guarantees to the lower and upper bounds obtained within afixed amount of computational resources. Experiments with both real andsynthetic generated models highlight its main characteristics and show that itcompares favorably against Park and Darwiche's systematic search, particularlyin the case of problems with many MAP variables and moderate tree-width.
arxiv-900-261 | Sparse Stochastic Inference for Latent Dirichlet allocation | http://arxiv.org/pdf/1206.6425v1.pdf | author:David Mimno, Matt Hoffman, David Blei category:cs.LG stat.ML published:2012-06-27 summary:We present a hybrid algorithm for Bayesian topic models that combines theefficiency of sparse Gibbs sampling with the scalability of online stochasticinference. We used our algorithm to analyze a corpus of 1.2 million books (33billion words) with thousands of topics. Our approach reduces the bias ofvariational inference and generalizes to many Bayesian hidden-variable models.
arxiv-900-262 | A Fast and Simple Algorithm for Training Neural Probabilistic Language Models | http://arxiv.org/pdf/1206.6426v1.pdf | author:Andriy Mnih, Yee Whye Teh category:cs.CL cs.LG published:2012-06-27 summary:In spite of their superior performance, neural probabilistic language models(NPLMs) remain far less widely used than n-gram models due to their notoriouslylong training times, which are measured in weeks even for moderately-sizeddatasets. Training NPLMs is computationally expensive because they areexplicitly normalized, which leads to having to consider all words in thevocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based onnoise-contrastive estimation, a newly introduced procedure for estimatingunnormalized continuous distributions. We investigate the behaviour of thealgorithm on the Penn Treebank corpus and show that it reduces the trainingtimes by more than an order of magnitude without affecting the quality of theresulting models. The algorithm is also more efficient and much more stablethan importance sampling because it requires far fewer noise samples to performwell. We demonstrate the scalability of the proposed approach by training severalneural language models on a 47M-word corpus with a 80K-word vocabulary,obtaining state-of-the-art results on the Microsoft Research SentenceCompletion Challenge dataset.
arxiv-900-263 | Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients | http://arxiv.org/pdf/1206.6427v1.pdf | author:Iftekhar Naim, Daniel Gildea category:cs.LG stat.ML published:2012-06-27 summary:The speed of convergence of the Expectation Maximization (EM) algorithm forGaussian mixture model fitting is known to be dependent on the amount ofoverlap among the mixture components. In this paper, we study the impact ofmixing coefficients on the convergence of EM. We show that when the mixturecomponents exhibit some overlap, the convergence of EM becomes slower as thedynamic range among the mixing coefficients increases. We propose adeterministic anti-annealing algorithm, that significantly improves the speedof convergence of EM for such mixtures with unbalanced mixing coefficients. Theproposed algorithm is compared against other standard optimization techniqueslike BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, wepropose a similar deterministic anti-annealing based algorithm for theDirichlet process mixture model and demonstrate its advantages over theconventional variational Bayesian approach.
arxiv-900-264 | A Binary Classification Framework for Two-Stage Multiple Kernel Learning | http://arxiv.org/pdf/1206.6428v1.pdf | author:Abhishek Kumar, Alexandru Niculescu-Mizil, Koray Kavukcuoglu, Hal Daume III category:cs.LG stat.ML published:2012-06-27 summary:With the advent of kernel methods, automating the task of specifying asuitable kernel has become increasingly important. In this context, theMultiple Kernel Learning (MKL) problem of finding a combination ofpre-specified base kernels that is suitable for the task at hand has receivedsignificant attention from researchers. In this paper we show that MultipleKernel Learning can be framed as a standard binary classification problem withadditional constraints that ensure the positive definiteness of the learnedkernel. Framing MKL in this way has the distinct advantage that it makes iteasy to leverage the extensive research in binary classification to developbetter performing and more scalable MKL algorithms that are conceptuallysimpler, and, arguably, more accessible to practitioners. Experiments on ninedata sets from different domains show that, despite its simplicity, theproposed technique compares favorably with current leading MKL approaches.
arxiv-900-265 | Incorporating Domain Knowledge in Matching Problems via Harmonic Analysis | http://arxiv.org/pdf/1206.6429v1.pdf | author:Deepti Pachauri, Maxwell Collins, Vikas SIngh, Risi Kondor category:cs.LG cs.CV stat.ML published:2012-06-27 summary:Matching one set of objects to another is a ubiquitous task in machinelearning and computer vision that often reduces to some form of the quadraticassignment problem (QAP). The QAP is known to be notoriously hard, both intheory and in practice. Here, we investigate if this difficulty can bemitigated when some additional piece of information is available: (a) that allQAP instances of interest come from the same application, and (b) the correctsolution for a set of such QAP instances is given. We propose a new approach toaccelerate the solution of QAPs based on learning parameters for a modifiedobjective function from prior QAP instances. A key feature of our approach isthat it takes advantage of the algebraic structure of permutations, inconjunction with special methods for optimizing functions over the symmetricgroup Sn in Fourier space. Experiments show that in practical domains the newmethod can outperform existing approaches.
arxiv-900-266 | Variational Bayesian Inference with Stochastic Search | http://arxiv.org/pdf/1206.6430v1.pdf | author:John Paisley, David Blei, Michael Jordan category:cs.LG stat.CO stat.ML published:2012-06-27 summary:Mean-field variational inference is a method for approximate Bayesianposterior inference. It approximates a full posterior distribution with afactorized set of distributions by maximizing a lower bound on the marginallikelihood. This requires the ability to integrate a sum of terms in the logjoint likelihood using this factorized distribution. Often not all integralsare in closed form, which is typically handled by using a lower bound. Wepresent an alternative algorithm based on stochastic optimization that allowsfor direct optimization of the variational lower bound. This method usescontrol variates to reduce the variance of the stochastic search gradient, inwhich existing lower bounds can play an important role. We demonstrate theapproach on two non-conjugate models: logistic regression and an approximationto the HDP.
arxiv-900-267 | Exact Maximum Margin Structure Learning of Bayesian Networks | http://arxiv.org/pdf/1206.6431v1.pdf | author:Robert Peharz, Franz Pernkopf category:cs.LG stat.ML published:2012-06-27 summary:Recently, there has been much interest in finding globally optimal Bayesiannetwork structures. These techniques were developed for generative scores andcan not be directly extended to discriminative scores, as desired forclassification. In this paper, we propose an exact method for finding networkstructures maximizing the probabilistic soft margin, a successfully applieddiscriminative score. Our method is based on branch-and-bound techniques withina linear programming framework and maintains an any-time solution, togetherwith worst-case sub-optimality bounds. We apply a set of order constraints forenforcing the network structure to be acyclic, which allows a compact problemrepresentation and the use of general-purpose optimization techniques. Inclassification experiments, our methods clearly outperform generatively trainednetwork structures and compete with support vector machines.
arxiv-900-268 | Sparse Support Vector Infinite Push | http://arxiv.org/pdf/1206.6432v1.pdf | author:Alain Rakotomamonjy category:cs.LG cs.CE stat.ML published:2012-06-27 summary:In this paper, we address the problem of embedded feature selection forranking on top of the list problems. We pose this problem as a regularizedempirical risk minimization with $p$-norm push loss function ($p=\infty$) andsparsity inducing regularizers. We leverage the issues related to thischallenging optimization problem by considering an alternating direction methodof multipliers algorithm which is built upon proximal operators of the lossfunction and the regularizer. Our main technical contribution is thus toprovide a numerical scheme for computing the infinite push loss functionproximal operator. Experimental results on toy, DNA microarray and BCI problemsshow how our novel algorithm compares favorably to competitors for ranking ontop while using fewer variables in the scoring function.
arxiv-900-269 | Copula Mixture Model for Dependency-seeking Clustering | http://arxiv.org/pdf/1206.6433v1.pdf | author:Melanie Rey, Volker Roth category:stat.ME cs.LG stat.ML published:2012-06-27 summary:We introduce a copula mixture model to perform dependency-seeking clusteringwhen co-occurring samples from different data sources are available. The modeltakes advantage of the great flexibility offered by the copulas framework toextend mixtures of Canonical Correlation Analysis to multivariate data witharbitrary continuous marginal densities. We formulate our model as anon-parametric Bayesian mixture, while providing efficient MCMC inference.Experiments on synthetic and real data demonstrate that the increasedflexibility of the copula mixture significantly improves the clustering and theinterpretability of the results.
arxiv-900-270 | A Generative Process for Sampling Contractive Auto-Encoders | http://arxiv.org/pdf/1206.6434v1.pdf | author:Salah Rifai, Yoshua Bengio, Yann Dauphin, Pascal Vincent category:cs.LG stat.ML published:2012-06-27 summary:The contractive auto-encoder learns a representation of the input data thatcaptures the local manifold structure around each data point, through theleading singular vectors of the Jacobian of the transformation from input torepresentation. The corresponding singular values specify how much localvariation is plausible in directions associated with the corresponding singularvectors, while remaining in a high-density region of the input space. Thispaper proposes a procedure for generating samples that are consistent with thelocal structure captured by a contractive auto-encoder. The associatedstochastic process defines a distribution from which one can sample, and whichexperimentally appears to converge quickly and mix well between modes, comparedto Restricted Boltzmann Machines and Deep Belief Networks. The intuitionsbehind this procedure can also be used to train the second layer of contractionthat pools lower-level features and learns to be invariant to the localdirections of variation discovered in the first layer. We show that this canhelp learn and represent invariances present in the data and improveclassification error.
arxiv-900-271 | Rethinking Collapsed Variational Bayes Inference for LDA | http://arxiv.org/pdf/1206.6435v1.pdf | author:Issei Sato, Hiroshi Nakagawa category:cs.LG stat.ML published:2012-06-27 summary:We propose a novel interpretation of the collapsed variational Bayesinference with a zero-order Taylor expansion approximation, called CVB0inference, for latent Dirichlet allocation (LDA). We clarify the properties ofthe CVB0 inference by using the alpha-divergence. We show that the CVB0inference is composed of two different divergence projections: alpha=1 and -1.This interpretation will help shed light on CVB0 works.
arxiv-900-272 | Efficient Structured Prediction with Latent Variables for General Graphical Models | http://arxiv.org/pdf/1206.6436v1.pdf | author:Alexander Schwing, Tamir Hazan, Marc Pollefeys, Raquel Urtasun category:cs.LG stat.ML published:2012-06-27 summary:In this paper we propose a unified framework for structured prediction withlatent variables which includes hidden conditional random fields and latentstructured support vector machines as special cases. We describe a localentropy approximation for this general formulation using duality, and derive anefficient message passing algorithm that is guaranteed to converge. Wedemonstrate its effectiveness in the tasks of image segmentation as well as 3Dindoor scene understanding from single images, showing that our approach issuperior to latent structured support vector machines and hidden conditionalrandom fields.
arxiv-900-273 | Large Scale Variational Bayesian Inference for Structured Scale Mixture Models | http://arxiv.org/pdf/1206.6437v1.pdf | author:Young Jun Ko, Matthias Seeger category:cs.CV cs.LG stat.ML published:2012-06-27 summary:Natural image statistics exhibit hierarchical dependencies across multiplescales. Representing such prior knowledge in non-factorial latent tree modelscan boost performance of image denoising, inpainting, deconvolution orreconstruction substantially, beyond standard factorial "sparse" methodology.We derive a large scale approximate Bayesian inference algorithm for linearmodels with non-factorial (latent tree-structured) scale mixture priors.Experimental results on a range of denoising and inpainting problemsdemonstrate substantially improved performance compared to MAP estimation or toinference with factorial priors.
arxiv-900-274 | Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation | http://arxiv.org/pdf/1206.6438v1.pdf | author:Yuan Shi, Fei Sha category:cs.LG stat.ML published:2012-06-27 summary:We study the problem of unsupervised domain adaptation, which aims to adaptclassifiers trained on a labeled source domain to an unlabeled target domain.Many existing approaches first learn domain-invariant features and thenconstruct classifiers with them. We propose a novel approach that jointly learnthe both. Specifically, while the method identifies a feature space where datain the source and the target domains are similarly distributed, it also learnsthe feature space discriminatively, optimizing an information-theoretic metricas an proxy to the expected misclassification error on the target domain. Weshow how this optimization can be effectively carried out with simplegradient-based methods and how hyperparameters can be cross-validated withoutdemanding any labeled data from the target domain. Empirical studies onbenchmark tasks of object recognition and sentiment analysis validated ourmodeling assumptions and demonstrated significant improvement of our methodover competing ones in classification accuracies.
arxiv-900-275 | Gap Filling in the Plant Kingdom---Trait Prediction Using Hierarchical Probabilistic Matrix Factorization | http://arxiv.org/pdf/1206.6439v1.pdf | author:Hanhuai Shan, Jens Kattge, Peter Reich, Arindam Banerjee, Franziska Schrodt, Markus Reichstein category:cs.CE cs.LG stat.AP published:2012-06-27 summary:Plant traits are a key to understanding and predicting the adaptation ofecosystems to environmental changes, which motivates the TRY project aiming atconstructing a global database for plant traits and becoming a standardresource for the ecological community. Despite its unprecedented coverage, alarge percentage of missing data substantially constrains joint trait analysis.Meanwhile, the trait data is characterized by the hierarchical phylogeneticstructure of the plant kingdom. While factorization based matrix completiontechniques have been widely used to address the missing data problem,traditional matrix factorization methods are unable to leverage thephylogenetic structure. We propose hierarchical probabilistic matrixfactorization (HPMF), which effectively uses hierarchical phylogeneticinformation for trait prediction. We demonstrate HPMF's high accuracy,effectiveness of incorporating hierarchical structure and ability to capturetrait correlation through experiments.
arxiv-900-276 | Predicting Preference Flips in Commerce Search | http://arxiv.org/pdf/1206.6440v1.pdf | author:Or Sheffet, Nina Mishra, Samuel Ieong category:cs.LG stat.ML published:2012-06-27 summary:Traditional approaches to ranking in web search follow the paradigm ofrank-by-score: a learned function gives each query-URL combination an absolutescore and URLs are ranked according to this score. This paradigm ensures thatif the score of one URL is better than another then one will always be rankedhigher than the other. Scoring contradicts prior work in behavioral economicsthat showed that users' preferences between two items depend not only on theitems but also on the presented alternatives. Thus, for the same query, users'preference between items A and B depends on the presence/absence of item C. Wepropose a new model of ranking, the Random Shopper Model, that allows andexplains such behavior. In this model, each feature is viewed as a Markov chainover the items to be ranked, and the goal is to find a weighting of thefeatures that best reflects their importance. We show that our model can belearned under the empirical risk minimization framework, and give an efficientlearning algorithm. Experiments on commerce search logs demonstrate that ouralgorithm outperforms scoring-based approaches including regression andlistwise ranking.
arxiv-900-277 | A Topic Model for Melodic Sequences | http://arxiv.org/pdf/1206.6441v1.pdf | author:Athina Spiliopoulou, Amos Storkey category:cs.LG cs.IR stat.ML published:2012-06-27 summary:We examine the problem of learning a probabilistic model for melody directlyfrom musical sequences belonging to the same genre. This is a challenging taskas one needs to capture not only the rich temporal structure evident in music,but also the complex statistical dependencies among different music components.To address this problem we introduce the Variable-gram Topic Model, whichcouples the latent topic formalism with a systematic model for contextualinformation. We evaluate the model on next-step prediction. Additionally, wepresent a novel way of model evaluation, where we directly compare modelsamples with data sequences using the Maximum Mean Discrepancy of stringkernels, to assess how close is the model distribution to the datadistribution. We show that the model has the highest performance under bothevaluation measures when compared to LDA, the Topic Bigram and relatednon-topic models.
arxiv-900-278 | Minimizing The Misclassification Error Rate Using a Surrogate Convex Loss | http://arxiv.org/pdf/1206.6442v1.pdf | author:Shai Ben-David, David Loker, Nathan Srebro, Karthik Sridharan category:cs.LG stat.ML published:2012-06-27 summary:We carefully study how well minimizing convex surrogate loss functions,corresponds to minimizing the misclassification error rate for the problem ofbinary classification with linear predictors. In particular, we show thatamongst all convex surrogate losses, the hinge loss gives essentially the bestpossible bound, of all convex loss functions, for the misclassification errorrate of the resulting linear predictor in terms of the best possible marginerror rate. We also provide lower bounds for specific convex surrogates thatshow how different commonly used losses qualitatively differ from each other.
arxiv-900-279 | Statistical Linear Estimation with Penalized Estimators: an Application to Reinforcement Learning | http://arxiv.org/pdf/1206.6444v1.pdf | author:Bernardo Avila Pires, Csaba Szepesvari category:cs.LG stat.ML published:2012-06-27 summary:Motivated by value function estimation in reinforcement learning, we studystatistical linear inverse problems, i.e., problems where the coefficients of alinear system to be solved are observed in noise. We consider penalizedestimators, where performance is evaluated using a matrix-weighted two-norm ofthe defect of the estimator measured with respect to the true, unknowncoefficients. Two objective functions are considered depending whether theerror of the defect measured with respect to the noisy coefficients is squaredor unsquared. We propose simple, yet novel and theoretically well-foundeddata-dependent choices for the regularization parameters for both cases thatavoid data-splitting. A distinguishing feature of our analysis is that wederive deterministic error bounds in terms of the error of the coefficients,thus allowing the complete separation of the analysis of the stochasticproperties of these errors. We show that our results lead to new insights andbounds for linear value function estimation in reinforcement learning.
arxiv-900-280 | Deep Lambertian Networks | http://arxiv.org/pdf/1206.6445v1.pdf | author:Yichuan Tang, Ruslan Salakhutdinov, Geoffrey Hinton category:cs.CV cs.LG stat.ML published:2012-06-27 summary:Visual perception is a challenging problem in part due to illuminationvariations. A possible solution is to first estimate an illumination invariantrepresentation before using it for recognition. The object albedo and surfacenormals are examples of such representations. In this paper, we introduce amultilayer generative model where the latent variables include the albedo,surface normals, and the light source. Combining Deep Belief Nets with theLambertian reflectance assumption, our model can learn good priors over thealbedo from 2D images. Illumination variations can be explained by changingonly the lighting latent variable in our model. By transferring learnedknowledge from similar objects, albedo and surface normals estimation from asingle image is possible in our model. Experiments demonstrate that our modelis able to generalize as well as improve over standard baselines in one-shotface recognition.
arxiv-900-281 | Agglomerative Bregman Clustering | http://arxiv.org/pdf/1206.6446v1.pdf | author:Matus Telgarsky, Sanjoy Dasgupta category:cs.LG stat.ML published:2012-06-27 summary:This manuscript develops the theory of agglomerative clustering with Bregmandivergences. Geometric smoothing techniques are developed to deal withdegenerate clusters. To allow for cluster models based on exponential familieswith overcomplete representations, Bregman divergences are developed fornondifferentiable convex functions.
arxiv-900-282 | Small-sample Brain Mapping: Sparse Recovery on Spatially Correlated Designs with Randomization and Clustering | http://arxiv.org/pdf/1206.6447v1.pdf | author:Gael Varoquaux, Alexandre Gramfort, Bertrand Thirion category:cs.LG cs.CV stat.AP stat.ML published:2012-06-27 summary:Functional neuroimaging can measure the brain?s response to an externalstimulus. It is used to perform brain mapping: identifying from theseobservations the brain regions involved. This problem can be cast into a linearsupervised learning task where the neuroimaging data are used as predictors forthe stimulus. Brain mapping is then seen as a support recovery problem. Onfunctional MRI (fMRI) data, this problem is particularly challenging as i) thenumber of samples is small due to limited acquisition time and ii) thevariables are strongly correlated. We propose to overcome these difficultiesusing sparse regression models over new variables obtained by clustering of theoriginal variables. The use of randomization techniques, e.g. bootstrapsamples, and clustering of the variables improves the recovery properties ofsparse methods. We demonstrate the benefit of our approach on an extensivesimulation study as well as two fMRI datasets.
arxiv-900-283 | Online Alternating Direction Method | http://arxiv.org/pdf/1206.6448v1.pdf | author:Huahua Wang, Arindam Banerjee category:cs.LG stat.ML published:2012-06-27 summary:Online optimization has emerged as powerful tool in large scale optimization.In this paper, we introduce efficient online algorithms based on thealternating directions method (ADM). We introduce a new proof technique for ADMin the batch setting, which yields the O(1/T) convergence rate of ADM and formsthe basis of regret analysis in the online setting. We consider two scenariosin the online setting, based on whether the solution needs to lie in thefeasible set or not. In both settings, we establish regret bounds for both theobjective function as well as constraint violation for general and stronglyconvex functions. Preliminary results are presented to illustrate theperformance of the proposed algorithms.
arxiv-900-284 | Monte Carlo Bayesian Reinforcement Learning | http://arxiv.org/pdf/1206.6449v1.pdf | author:Yi Wang, Kok Sung Won, David Hsu, Wee Sun Lee category:cs.LG stat.ML published:2012-06-27 summary:Bayesian reinforcement learning (BRL) encodes prior knowledge of the world ina model and represents uncertainty in model parameters by maintaining aprobability distribution over them. This paper presents Monte Carlo BRL(MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori afinite set of hypotheses for the model parameter values and forms a discretepartially observable Markov decision process (POMDP) whose state space is across product of the state space for the reinforcement learning task and thesampled model parameter space. The POMDP does not require conjugatedistributions for belief representation, as earlier works do, and can be solvedrelatively easily with point-based approximation algorithms. MC-BRL naturallyhandles both fully and partially observable worlds. Theoretical andexperimental results show that the discrete POMDP approximates the underlyingBRL task well with guaranteed performance.
arxiv-900-285 | Conditional Sparse Coding and Grouped Multivariate Regression | http://arxiv.org/pdf/1206.6450v1.pdf | author:Min Xu, John Lafferty category:cs.LG stat.ML published:2012-06-27 summary:We study the problem of multivariate regression where the data are naturallygrouped, and a regression matrix is to be estimated for each group. We proposean approach in which a dictionary of low rank parameter matrices is estimatedacross groups, and a sparse linear combination of the dictionary elements isestimated to form a model within each group. We refer to the method asconditional sparse coding since it is a coding procedure for the responsevectors Y conditioned on the covariate vectors X. This approach captures theshared information across the groups while adapting to the structure withineach group. It exploits the same intuition behind sparse coding that has beensuccessfully developed in computer vision and computational neuroscience. Wepropose an algorithm for conditional sparse coding, analyze its theoreticalproperties in terms of predictive accuracy, and present the results ofsimulation and brain imaging experiments that compare the new technique toreduced rank regression.
arxiv-900-286 | The Greedy Miser: Learning under Test-time Budgets | http://arxiv.org/pdf/1206.6451v1.pdf | author:Zhixiang Xu, Kilian Weinberger, Olivier Chapelle category:cs.LG stat.ML published:2012-06-27 summary:As machine learning algorithms enter applications in industrial settings,there is increased interest in controlling their cpu-time during testing. Thecpu-time consists of the running time of the algorithm and the extraction timeof the features. The latter can vary drastically when the feature set isdiverse. In this paper, we propose an algorithm, the Greedy Miser, thatincorporates the feature extraction cost during training to explicitly minimizethe cpu-time during testing. The algorithm is a straightforward extension ofstage-wise regression and is equally suitable for regression or multi-classclassification. Compared to prior work, it is significantly more cost-effectiveand scales to larger data sets.
arxiv-900-287 | Smoothness and Structure Learning by Proxy | http://arxiv.org/pdf/1206.6452v1.pdf | author:Benjamin Yackley, Terran Lane category:cs.LG math.OC stat.ML published:2012-06-27 summary:As data sets grow in size, the ability of learning methods to find structurein them is increasingly hampered by the time needed to search the large spacesof possibilities and generate a score for each that takes all of the observeddata into account. For instance, Bayesian networks, the model chosen in thispaper, have a super-exponentially large search space for a fixed number ofvariables. One possible method to alleviate this problem is to use a proxy,such as a Gaussian Process regressor, in place of the true scoring function,training it on a selection of sampled networks. We prove here that the use ofsuch a proxy is well-founded, as we can bound the smoothness of a commonly-usedscoring function for Bayesian network structure learning. We show here that,compared to an identical search strategy using the network?s exact scores, ourproxy-based search is able to get equivalent or better scores on a number ofdata sets in a fraction of the time.
arxiv-900-288 | Adaptive Canonical Correlation Analysis Based On Matrix Manifolds | http://arxiv.org/pdf/1206.6453v1.pdf | author:Florian Yger, Maxime Berar, Gilles Gasso, Alain Rakotomamonjy category:cs.LG stat.ML published:2012-06-27 summary:In this paper, we formulate the Canonical Correlation Analysis (CCA) problemon matrix manifolds. This framework provides a natural way for dealing withmatrix constraints and tools for building efficient algorithms even in anadaptive setting. Finally, an adaptive CCA algorithm is proposed and applied toa change detection problem in EEG signals.
arxiv-900-289 | Hierarchical Exploration for Accelerating Contextual Bandits | http://arxiv.org/pdf/1206.6454v1.pdf | author:Yisong Yue, Sue Ann Hong, Carlos Guestrin category:cs.LG stat.ML published:2012-06-27 summary:Contextual bandit learning is an increasingly popular approach to optimizingrecommender systems via user feedback, but can be slow to converge in practicedue to the need for exploring a large feature space. In this paper, we proposea coarse-to-fine hierarchical approach for encoding prior knowledge thatdrastically reduces the amount of exploration required. Intuitively, userpreferences can be reasonably embedded in a coarse low-dimensional featurespace that can be explored efficiently, requiring exploration in thehigh-dimensional space only as necessary. We introduce a bandit algorithm thatexplores within this coarse-to-fine spectrum, and prove performance guaranteesthat depend on how well the coarse space captures the user's preferences. Wedemonstrate substantial improvement over conventional bandit algorithms throughextensive simulation as well as a live user study in the setting ofpersonalized news recommendation.
arxiv-900-290 | Regularizers versus Losses for Nonlinear Dimensionality Reduction: A Factored View with New Convex Relaxations | http://arxiv.org/pdf/1206.6455v1.pdf | author:Yaoliang Yu, James Neufeld, Ryan Kiros, Xinhua Zhang, Dale Schuurmans category:cs.LG stat.ML published:2012-06-27 summary:We demonstrate that almost all non-parametric dimensionality reductionmethods can be expressed by a simple procedure: regularized loss minimizationplus singular value truncation. By distinguishing the role of the loss andregularizer in such a process, we recover a factored perspective that revealssome gaps in the current literature. Beyond identifying a useful new loss formanifold unfolding, a key contribution is to derive new convex regularizersthat combine distance maximization with rank reduction. These regularizers canbe applied to any loss.
arxiv-900-291 | Lognormal and Gamma Mixed Negative Binomial Regression | http://arxiv.org/pdf/1206.6456v1.pdf | author:Mingyuan Zhou, Lingbo Li, David Dunson, Lawrence Carin category:stat.AP cs.LG stat.ME published:2012-06-27 summary:In regression analysis of counts, a lack of simple and efficient algorithmsfor posterior computation has made Bayesian approaches appear unattractive andthus underdeveloped. We propose a lognormal and gamma mixed negative binomial(NB) regression model for counts, and present efficient closed-form Bayesianinference; unlike conventional Poisson models, the proposed approach has twofree parameters to include two different kinds of random effects, and allowsthe incorporation of prior information, such as sparsity in the regressioncoefficients. By placing a gamma distribution prior on the NB dispersionparameter r, and connecting a lognormal distribution prior with the logit ofthe NB probability parameter p, efficient Gibbs sampling and variational Bayesinference are both developed. The closed-form updates are obtained byexploiting conditional conjugacy via both a compound Poisson representation anda Polya-Gamma distribution based data augmentation approach. The proposedBayesian inference can be implemented routinely, while being easilygeneralizable to more complex settings involving multivariate dependencestructures. The algorithms are illustrated using real examples.
arxiv-900-292 | Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations | http://arxiv.org/pdf/1206.6457v1.pdf | author:Nando de Freitas, Alex Smola, Masrour Zoghi category:cs.LG stat.ML published:2012-06-27 summary:This paper analyzes the problem of Gaussian process (GP) bandits withdeterministic observations. The analysis uses a branch and bound algorithm thatis related to the UCB algorithm of (Srinivas et al, 2010). For GPs withGaussian observation noise, with variance strictly greater than zero, Srinivaset al proved that the regret vanishes at the approximate rate of$O(1/\sqrt{t})$, where t is the number of observations. To complement theirresult, we attack the deterministic case and attain a much faster exponentialconvergence rate. Under some regularity assumptions, we show that the regretdecreases asymptotically according to $O(e^{-\frac{\tau t}{(\ln t)^{d/4}}})$with high probability. Here, d is the dimension of the search space and tau isa constant that depends on the behaviour of the objective function near itsglobal maximum.
arxiv-900-293 | Batch Active Learning via Coordinated Matching | http://arxiv.org/pdf/1206.6458v1.pdf | author:Javad Azimi, Alan Fern, Xiaoli Zhang-Fern, Glencora Borradaile, Brent Heeringa category:cs.LG stat.ML published:2012-06-27 summary:Most prior work on active learning of classifiers has focused on sequentiallyselecting one unlabeled example at a time to be labeled in order to reduce theoverall labeling effort. In many scenarios, however, it is desirable to labelan entire batch of examples at once, for example, when labels can be acquiredin parallel. This motivates us to study batch active learning, whichiteratively selects batches of $k>1$ examples to be labeled. We propose a novelbatch active learning method that leverages the availability of high-qualityand efficient sequential active-learning policies by attempting to approximatetheir behavior when applied for $k$ steps. Specifically, our algorithm firstuses Monte-Carlo simulation to estimate the distribution of unlabeled examplesselected by a sequential policy over $k$ step executions. The algorithm thenattempts to select a set of $k$ examples that best matches this distribution,leading to a combinatorial optimization problem that we term "boundedcoordinated matching". While we show this problem is NP-hard in general, wegive an efficient greedy solution, which inherits approximation bounds fromsupermodular minimization theory. Our experimental results on eight benchmarkdatasets show that the proposed approach is highly effective
arxiv-900-294 | Bayesian Conditional Cointegration | http://arxiv.org/pdf/1206.6459v1.pdf | author:Chris Bracegirdle, David Barber category:cs.CE cs.LG stat.ME published:2012-06-27 summary:Cointegration is an important topic for time-series, and describes arelationship between two series in which a linear combination is stationary.Classically, the test for cointegration is based on a two stage process inwhich first the linear relation between the series is estimated by OrdinaryLeast Squares. Subsequently a unit root test is performed on the residuals. Awell-known deficiency of this classical approach is that it can lead toerroneous conclusions about the presence of cointegration. As an alternative,we present a framework for estimating whether cointegration exists usingBayesian inference which is empirically superior to the classical approach.Finally, we apply our technique to model segmented cointegration in whichcointegration may exist only for limited time. In contrast to previousapproaches our model makes no restriction on the number of possiblecointegration segments.
arxiv-900-295 | Output Space Search for Structured Prediction | http://arxiv.org/pdf/1206.6460v1.pdf | author:Janardhan Rao Doppa, Alan Fern, Prasad Tadepalli category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We consider a framework for structured prediction based on search in thespace of complete structured outputs. Given a structured input, an output isproduced by running a time-bounded search procedure guided by a learned costfunction, and then returning the least cost output uncovered during the search.This framework can be instantiated for a wide range of search spaces and searchprocedures, and easily incorporates arbitrary structured-prediction lossfunctions. In this paper, we make two main technical contributions. First, wedefine the limited-discrepancy search space over structured outputs, which isable to leverage powerful classification learning algorithms to improve thesearch space quality. Second, we give a generic cost function learningapproach, where the key idea is to learn a cost function that attempts to mimicthe behavior of conducting searches guided by the true loss function. Ourexperiments on six benchmark domains demonstrate that using our framework withonly a small amount of search is sufficient for significantly improving onstate-of-the-art structured-prediction performance.
arxiv-900-296 | On the Sample Complexity of Reinforcement Learning with a Generative Model | http://arxiv.org/pdf/1206.6461v1.pdf | author:Mohammad Gheshlaghi Azar, Remi Munos, Bert Kappen category:cs.LG stat.ML published:2012-06-27 summary:We consider the problem of learning the optimal action-value function in thediscounted-reward Markov decision processes (MDPs). We prove a new PAC bound onthe sample-complexity of model-based value iteration algorithm in the presenceof the generative model, which indicates that for an MDP with N state-actionpairs and the discount factor \gamma\in[0,1) onlyO(N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) samples are required to find an\epsilon-optimal estimation of the action-value function with the probability1-\delta. We also prove a matching lower bound of \Theta(N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) on the sample complexity ofestimating the optimal action-value function by every RL algorithm. To the bestof our knowledge, this is the first matching result on the sample complexity ofestimating the optimal (action-) value function in which the upper boundmatches the lower bound of RL in terms of N, \epsilon, \delta and 1/(1-\gamma).Also, both our lower bound and our upper bound significantly improve on thestate-of-the-art in terms of 1/(1-\gamma).
arxiv-900-297 | Learning Object Arrangements in 3D Scenes using Human Context | http://arxiv.org/pdf/1206.6462v1.pdf | author:Yun Jiang, Marcus Lim, Ashutosh Saxena category:cs.LG cs.CV cs.RO stat.ML published:2012-06-27 summary:We consider the problem of learning object arrangements in a 3D scene. Thekey idea here is to learn how objects relate to human poses based on theiraffordances, ease of use and reachability. In contrast to modelingobject-object relationships, modeling human-object relationships scaleslinearly in the number of objects. We design appropriate density functionsbased on 3D spatial features to capture this. We learn the distribution ofhuman poses in a scene using a variant of the Dirichlet process mixture modelthat allows sharing of the density function parameters across the same objecttypes. Then we can reason about arrangements of the objects in the room basedon these meaningful human poses. In our extensive experiments on 20 differentrooms with a total of 47 objects, our algorithm predicted correct placementswith an average error of 1.6 meters from ground truth. In arranging five realscenes, it received a score of 4.3/5 compared to 3.7 for the best baselinemethod.
arxiv-900-298 | An Iterative Locally Linear Embedding Algorithm | http://arxiv.org/pdf/1206.6463v1.pdf | author:Deguang Kong, Chris H. Q. Ding, Heng Huang, Feiping Nie category:cs.LG stat.ML published:2012-06-27 summary:Local Linear embedding (LLE) is a popular dimension reduction method. In thispaper, we first show LLE with nonnegative constraint is equivalent to thewidely used Laplacian embedding. We further propose to iterate the two steps inLLE repeatedly to improve the results. Thirdly, we relax the kNN constraint ofLLE and present a sparse similarity learning algorithm. The final Iterative LLEcombines these three improvements. Extensive experiment results show thatiterative LLE algorithm significantly improve both classification andclustering results.
arxiv-900-299 | Bayesian Efficient Multiple Kernel Learning | http://arxiv.org/pdf/1206.6465v1.pdf | author:Mehmet Gonen category:cs.LG stat.ML published:2012-06-27 summary:Multiple kernel learning algorithms are proposed to combine kernels in orderto obtain a better similarity measure or to integrate feature representationscoming from different data sources. Most of the previous research on suchmethods is focused on the computational efficiency issue. However, it is stillnot feasible to combine many kernels using existing Bayesian approaches due totheir high time complexity. We propose a fully conjugate Bayesian formulationand derive a deterministic variational approximation, which allows us tocombine hundreds or thousands of kernels very efficiently. We briefly explainhow the proposed method can be extended for multiclass learning andsemi-supervised learning. Experiments with large numbers of kernels onbenchmark data sets show that our inference method is quite fast, requiringless than a minute. On one bioinformatics and three image recognition datasets, our method outperforms previously reported results with bettergeneralization performance.
arxiv-900-300 | Utilizing Static Analysis and Code Generation to Accelerate Neural Networks | http://arxiv.org/pdf/1206.6466v1.pdf | author:Lawrence McAfee, Kunle Olukotun category:cs.NE cs.MS cs.PL published:2012-06-27 summary:As datasets continue to grow, neural network (NN) applications are becomingincreasingly limited by both the amount of available computational power andthe ease of developing high-performance applications. Researchers often musthave expert systems knowledge to make their algorithms run efficiently.Although available computing power increases rapidly each year, algorithmefficiency is not able to keep pace due to the use of general purposecompilers, which are not able to fully optimize specialized applicationdomains. Within the domain of NNs, we have the added knowledge that networkarchitecture remains constant during training, meaning the architecture's datastructure can be statically optimized by a compiler. In this paper, we presentSONNC, a compiler for NNs that utilizes static analysis to generate optimizedparallel code. We show that SONNC's use of static optimizations make it able tooutperform hand-optimized C++ code by up to 7.8X, and MATLAB code by up to 24X.Additionally, we show that use of SONNC significantly reduces code complexitywhen using structurally sparse networks.
