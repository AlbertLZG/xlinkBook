arxiv-1800-1 | Notes on image annotation | http://arxiv.org/abs/1210.3448 | author:Adela Barriuso, Antonio Torralba category:cs.CV cs.HC published:2012-10-12 summary:We are under the illusion that seeing is effortless, but frequently thevisual system is lazy and makes us believe that we understand something when infact we don't. Labeling a picture forces us to become aware of the difficultiesunderlying scene understanding. Suddenly, the act of seeing is not effortlessanymore. We have to make an effort in order to understand parts of the picturethat we neglected at first glance. In this report, an expert image annotator relates her experience onsegmenting and labeling tens of thousands of images. During this process, thenotes she took try to highlight the difficulties encountered, the solutionsadopted, and the decisions made in order to get a consistent set ofannotations. Those annotations constitute the SUN database.
arxiv-1800-2 | Bayesian Analysis for miRNA and mRNA Interactions Using Expression Data | http://arxiv.org/abs/1210.3456 | author:Mingjun Zhong, Rong Liu, Bo Liu category:stat.AP cs.LG q-bio.GN q-bio.MN stat.ML published:2012-10-12 summary:MicroRNAs (miRNAs) are small RNA molecules composed of 19-22 nt, which playimportant regulatory roles in post-transcriptional gene regulation byinhibiting the translation of the mRNA into proteins or otherwise cleaving thetarget mRNA. Inferring miRNA targets provides useful information forunderstanding the roles of miRNA in biological processes that are potentiallyinvolved in complex diseases. Statistical methodologies for point estimation,such as the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm,have been proposed to identify the interactions of miRNA and mRNA based onsequence and expression data. In this paper, we propose using the BayesianLASSO (BLASSO) and the non-negative Bayesian LASSO (nBLASSO) to analyse theinteractions between miRNA and mRNA using expression data. The proposedBayesian methods explore the posterior distributions for those parametersrequired to model the miRNA-mRNA interactions. These approaches can be used toobserve the inferred effects of the miRNAs on the targets by plotting theposterior distributions of those parameters. For comparison purposes, the LeastSquares Regression (LSR), Ridge Regression (RR), LASSO, non-negative LASSO(nLASSO), and the proposed Bayesian approaches were applied to four publicdatasets. We concluded that nLASSO and nBLASSO perform best in terms ofsensitivity and specificity. Compared to the point estimate algorithms, whichonly provide single estimates for those parameters, the Bayesian methods aremore meaningful and provide credible intervals, which take into account theuncertainty of the inferred interactions of the miRNA and mRNA. Furthermore,Bayesian methods naturally provide statistical significance to selectconvincing inferred interactions, while point estimate algorithms require amanually chosen threshold, which is less meaningful, to choose the possibleinteractions.
arxiv-1800-3 | A Flexible Mixed Integer Programming framework for Nurse Scheduling | http://arxiv.org/abs/1210.3652 | author:Murphy Choy, Michelle Cheong category:cs.DS cs.NE published:2012-10-12 summary:In this paper, a nurse-scheduling model is developed using mixed integerprogramming model. It is deployed to a general care ward to replace andautomate the current manual approach for scheduling. The developed modeldiffers from other similar studies in that it optimizes both hospitalsrequirement as well as nurse preferences by allowing flexibility in thetransfer of nurses from different duties. The model also incorporatedadditional policies which are part of the hospitals requirement but not part ofthe legislations. Hospitals key primary mission is to ensure continuous wardcare service with appropriate number of nursing staffs and the right mix ofnursing skills. The planning and scheduling is done to avoid additional nonessential cost for hospital. Nurses preferences are taken into considerationssuch as the number of night shift and consecutive rest days. We will alsoreformulate problems from another paper which considers the penalty objectiveusing the model but without the flexible components. The models are built usingAIMMS which solves the problem in very short amount of time.
arxiv-1800-4 | Quick Summary | http://arxiv.org/abs/1210.3634 | author:Robert Wahlstedt category:cs.CL cs.AI published:2012-10-12 summary:Quick Summary is an innovate implementation of an automatic documentsummarizer that inputs a document in the English language and evaluates eachsentence. The scanner or evaluator determines criteria based on its grammaticalstructure and place in the paragraph. The program then asks the user to specifythe number of sentences the person wishes to highlight. For example should theuser ask to have three of the most important sentences, it would highlight thefirst and most important sentence in green. Commonly this is the sentencecontaining the conclusion. Then Quick Summary finds the second most importantsentence usually called a satellite and highlights it in yellow. This isusually the topic sentence. Then the program finds the third most importantsentence and highlights it in red. The implementations of this technology areuseful in a society of information overload when a person typically receives 42emails a day (Microsoft). The paper also is a candid look at difficulty thatmachine learning has in textural translating. However, it speaks on how toovercome the obstacles that historically prevented progress. This paperproposes mathematical meta-data criteria that justify the place of importanceof a sentence. Just as tools for the study of relational symmetry inbio-informatics, this tool seeks to classify words with greater clarity."Survey Finds Workers Average Only Three Productive Days per Week." MicrosoftNews Center. Microsoft. Web. 31 Mar. 2012.
arxiv-1800-5 | A Benchmark to Select Data Mining Based Classification Algorithms For Business Intelligence And Decision Support Systems | http://arxiv.org/abs/1210.3139 | author:Pardeep Kumar, Nitin, Vivek Kumar Sehgal, Durg Singh Chauhan category:cs.DB cs.LG published:2012-10-11 summary:DSS serve the management, operations, and planning levels of an organizationand help to make decisions, which may be rapidly changing and not easilyspecified in advance. Data mining has a vital role to extract importantinformation to help in decision making of a decision support system.Integration of data mining and decision support systems (DSS) can lead to theimproved performance and can enable the tackling of new types of problems.Artificial Intelligence methods are improving the quality of decision support,and have become embedded in many applications ranges from ant lockingautomobile brakes to these days interactive search engines. It provides variousmachine learning techniques to support data mining. The classification is oneof the main and valuable tasks of data mining. Several types of classificationalgorithms have been suggested, tested and compared to determine the futuretrends based on unseen data. There has been no single algorithm found to besuperior over all others for all data sets. The objective of this paper is tocompare various classification algorithms that have been frequently used indata mining for decision support systems. Three decision trees basedalgorithms, one artificial neural network, one statistical, one support vectormachines with and without ada boost and one clustering algorithm are tested andcompared on four data sets from different domains in terms of predictiveaccuracy, error rate, classification index, comprehensibility and trainingtime. Experimental results demonstrate that Genetic Algorithm (GA) and supportvector machines based algorithms are better in terms of predictive accuracy.SVM without adaboost shall be the first choice in context of speed andpredictive accuracy. Adaboost improves the accuracy of SVM but on the cost oflarge training time.
arxiv-1800-6 | Inferring clonal evolution of tumors from single nucleotide somatic mutations | http://arxiv.org/abs/1210.3384 | author:Wei Jiao, Shankar Vembu, Amit G. Deshwar, Lincoln Stein, Quaid Morris category:cs.LG q-bio.PE q-bio.QM stat.ML published:2012-10-11 summary:High-throughput sequencing allows the detection and quantification offrequencies of somatic single nucleotide variants (SNV) in heterogeneous tumorcell populations. In some cases, the evolutionary history and populationfrequency of the subclonal lineages of tumor cells present in the sample can bereconstructed from these SNV frequency measurements. However, automated methodsto do this reconstruction are not available and the conditions under whichreconstruction is possible have not been described. We describe the conditions under which the evolutionary history can beuniquely reconstructed from SNV frequencies from single or multiple samplesfrom the tumor population and we introduce a new statistical model, PhyloSub,that infers the phylogeny and genotype of the major subclonal lineagesrepresented in the population of cancer cells. It uses a Bayesian nonparametricprior over trees that groups SNVs into major subclonal lineages andautomatically estimates the number of lineages and their ancestry. We samplefrom the joint posterior distribution over trees to identify evolutionaryhistories and cell population frequencies that have the highest probability ofgenerating the observed SNV frequency data. When multiple phylogenies areconsistent with a given set of SNV frequencies, PhyloSub represents theuncertainty in the tumor phylogeny using a partial order plot. Experiments on asimulated dataset and two real datasets comprising tumor samples from acutemyeloid leukemia and chronic lymphocytic leukemia patients demonstrate thatPhyloSub can infer both linear (or chain) and branching lineages and itsinferences are in good agreement with ground truth, where it is available.
arxiv-1800-7 | Fitness Landscape-Based Characterisation of Nature-Inspired Algorithms | http://arxiv.org/abs/1210.3210 | author:Matthew Crossley, Andy Nisbet, Martyn Amos category:cs.NE published:2012-10-11 summary:A significant challenge in nature-inspired algorithmics is the identificationof specific characteristics of problems that make them harder (or easier) tosolve using specific methods. The hope is that, by identifying thesecharacteristics, we may more easily predict which algorithms are best-suited toproblems sharing certain features. Here, we approach this problem using fitnesslandscape analysis. Techniques already exist for measuring the "difficulty" ofspecific landscapes, but these are often designed solely with evolutionaryalgorithms in mind, and are generally specific to discrete optimisation. Inthis paper we develop an approach for comparing a wide range of continuousoptimisation algorithms. Using a fitness landscape generation technique, wecompare six different nature-inspired algorithms and identify which methodsperform best on landscapes exhibiting specific features.
arxiv-1800-8 | Improved Graph Clustering | http://arxiv.org/abs/1210.3335 | author:Yudong Chen, Sujay Sanghavi, Huan Xu category:stat.ML published:2012-10-11 summary:Graph clustering involves the task of dividing nodes into clusters, so thatthe edge density is higher within clusters as opposed to across clusters. Anatural, classic and popular statistical setting for evaluating solutions tothis problem is the stochastic block model, also referred to as the plantedpartition model. In this paper we present a new algorithm--a convexified version of MaximumLikelihood--for graph clustering. We show that, in the classic stochastic blockmodel setting, it outperforms existing methods by polynomial factors when thecluster size is allowed to have general scalings. In fact, it is withinlogarithmic factors of known lower bounds for spectral methods, and there isevidence suggesting that no polynomial time algorithm would do significantlybetter. We then show that this guarantee carries over to a more general extension ofthe stochastic block model. Our method can handle the settings of semi-randomgraphs, heterogeneous degree distributions, unequal cluster sizes, unaffiliatednodes, partially observed graphs and planted clique/coloring etc. Inparticular, our results provide the best exact recovery guarantees to date forthe planted partition, planted k-disjoint-cliques and planted noisy coloringmodels with general cluster sizes; in other settings, we match the bestexisting results up to logarithmic factors.
arxiv-1800-9 | Unsupervised Detection and Tracking of Arbitrary Objects with Dependent Dirichlet Process Mixtures | http://arxiv.org/abs/1210.3288 | author:Willie Neiswanger, Frank Wood category:stat.ML cs.CV cs.LG published:2012-10-11 summary:This paper proposes a technique for the unsupervised detection and trackingof arbitrary objects in videos. It is intended to reduce the need for detectionand localization methods tailored to specific object types and serve as ageneral framework applicable to videos with varied objects, backgrounds, andimage qualities. The technique uses a dependent Dirichlet process mixture(DDPM) known as the Generalized Polya Urn (GPUDDPM) to model image pixel datathat can be easily and efficiently extracted from the regions in a video thatrepresent objects. This paper describes a specific implementation of the modelusing spatial and color pixel data extracted via frame differencing and givestwo algorithms for performing inference in the model to accomplish detectionand tracking. This technique is demonstrated on multiple synthetic andbenchmark video datasets that illustrate its ability to, without modification,detect and track objects with diverse physical characteristics moving overnon-uniform backgrounds and through occlusion.
arxiv-1800-10 | Artex is AnotheR TEXt summarizer | http://arxiv.org/abs/1210.3312 | author:Juan-Manuel Torres-Moreno category:cs.IR cs.AI cs.CL published:2012-10-11 summary:This paper describes Artex, another algorithm for Automatic TextSummarization. In order to rank sentences, a simple inner product is calculatedbetween each sentence, a document vector (text topic) and a lexical vector(vocabulary used by a sentence). Summaries are then generated by assembling thehighest ranked sentences. No ruled-based linguistic post-processing isnecessary in order to obtain summaries. Tests over several datasets (comingfrom Document Understanding Conferences (DUC), Text Analysis Conferences (TAC),evaluation campaigns, etc.) in French, English and Spanish have shown thatsummarizer achieves interesting results.
arxiv-1800-11 | Near-optimal compressed sensing guarantees for total variation minimization | http://arxiv.org/abs/1210.3098 | author:Deanna Needell, Rachel Ward category:math.NA cs.CV cs.IT math.IT published:2012-10-11 summary:Consider the problem of reconstructing a multidimensional signal from anunderdetermined set of measurements, as in the setting of compressed sensing.Without any additional assumptions, this problem is ill-posed. However, forsignals such as natural images or movies, the minimal total variation estimateconsistent with the measurements often produces a good approximation to theunderlying signal, even if the number of measurements is far smaller than theambient dimensionality. This paper extends recent reconstruction guarantees fortwo-dimensional images to signals of arbitrary dimension d>1 and to isotropictotal variation problems. To be precise, we show that a multidimensional signalx can be reconstructed from O(sd*log(N^d)) linear measurements using totalvariation minimization to within a factor of the best s-term approximation ofits gradient. The reconstruction guarantees we provide are necessarily optimalup to polynomial factors in the spatial dimension d.
arxiv-1800-12 | Three dimensional tracking of gold nanoparticles using digital holographic microscopy | http://arxiv.org/abs/1210.3326 | author:Frédéric Verpillat, Fadwa Joud, Pierre Desbiolles, Michel Gross category:physics.optics cs.CV published:2012-10-11 summary:In this paper we present a digital holographic microscope to track goldcolloids in three dimensions. We report observations of 100nm gold particles inmotion in water. The expected signal and the chosen method of reconstructionare described. We also discuss about how to implement the numerical calculationto reach real-time 3D tracking.
arxiv-1800-13 | Enhanced Compressed Sensing Recovery with Level Set Normals | http://arxiv.org/abs/1210.3350 | author:Virginia Estellers, Jean-Philippe Thiran, Xavier Bresson category:cs.CV published:2012-10-11 summary:We propose a compressive sensing algorithm that exploits geometric propertiesof images to recover images of high quality from few measurements. The imagereconstruction is done by iterating the two following steps: 1) estimation ofnormal vectors of the image level curves and 2) reconstruction of an imagefitting the normal vectors, the compressed sensing measurements and thesparsity constraint. The proposed technique can naturally extend to non localoperators and graphs to exploit the repetitive nature of textured images inorder to recover fine detail structures. In both cases, the problem is reducedto a series of convex minimization problems that can be efficiently solved witha combination of variable splitting and augmented Lagrangian methods, leadingto fast and easy-to-code algorithms. Extended experiments show a clearimprovement over related state-of-the-art algorithms in the quality of thereconstructed images and the robustness of the proposed method to noise,different kind of images and reduced measurements.
arxiv-1800-14 | Computationally Efficient Implementation of Convolution-based Locally Adaptive Binarization Techniques | http://arxiv.org/abs/1210.3165 | author:Ayatullah Faruk Mollah, Subhadip Basu, Mita Nasipuri category:cs.CV published:2012-10-11 summary:One of the most important steps of document image processing is binarization.The computational requirements of locally adaptive binarization techniques makethem unsuitable for devices with limited computing facilities. In this paper,we have presented a computationally efficient implementation of convolutionbased locally adaptive binarization techniques keeping the performancecomparable to the original implementation. The computational complexity hasbeen reduced from O(W2N2) to O(WN2) where WxW is the window size and NxN is theimage size. Experiments over benchmark datasets show that the computation timehas been reduced by 5 to 15 times depending on the window size while memoryconsumption remains the same with respect to the state-of-the-art algorithmicimplementation.
arxiv-1800-15 | Learning Onto-Relational Rules with Inductive Logic Programming | http://arxiv.org/abs/1210.2984 | author:Francesca A. Lisi category:cs.AI cs.DB cs.LG cs.LO published:2012-10-10 summary:Rules complement and extend ontologies on the Semantic Web. We refer to theserules as onto-relational since they combine DL-based ontology languages andKnowledge Representation formalisms supporting the relational data model withinthe tradition of Logic Programming and Deductive Databases. Rule authoring is avery demanding Knowledge Engineering task which can be automated thoughpartially by applying Machine Learning algorithms. In this chapter we show howInductive Logic Programming (ILP), born at the intersection of Machine Learningand Logic Programming and considered as a major approach to RelationalLearning, can be adapted to Onto-Relational Learning. For the sake ofillustration, we provide details of a specific Onto-Relational Learningsolution to the problem of learning rule-based definitions of DL concepts androles with ILP.
arxiv-1800-16 | Sequential Convex Programming Methods for A Class of Structured Nonlinear Programming | http://arxiv.org/abs/1210.3039 | author:Zhaosong Lu category:math.OC cs.NA stat.CO stat.ML published:2012-10-10 summary:In this paper we study a broad class of structured nonlinear programming(SNLP) problems. In particular, we first establish the first-order optimalityconditions for them. Then we propose sequential convex programming (SCP)methods for solving them in which each iteration is obtained by solving aconvex programming problem exactly or inexactly. Under some suitableassumptions, we establish that any accumulation point of the sequence generatedby the methods is a KKT point of the SNLP problems. In addition, we propose avariant of the exact SCP method for SNLP in which nonmonotone scheme and"local" Lipschitz constants of the associated functions are used. And a similarconvergence result as mentioned above is established.
arxiv-1800-17 | An anisotropy preserving metric for DTI processing | http://arxiv.org/abs/1210.2826 | author:Anne Collard, Silvère Bonnabel, Christophe Phillips, Rodolphe Sepulchre category:cs.CV math.DG published:2012-10-10 summary:Statistical analysis of Diffusion Tensor Imaging (DTI) data requires acomputational framework that is both numerically tractable (to account for thehigh dimensional nature of the data) and geometric (to account for thenonlinear nature of diffusion tensors). Building upon earlier studies that haveshown that a Riemannian framework is appropriate to address these challenges,the present paper proposes a novel metric and an accompanying computationalframework for DTI data processing. The proposed metric retains the geometry andthe computational tractability of earlier methods grounded in the affineinvariant metric. In addition, and in contrast to earlier methods, it providesan interpolation method which preserves anisotropy, a central informationcarried by diffusion tensor data.
arxiv-1800-18 | Efficient Solution to the 3D Problem of Automatic Wall Paintings Reassembly | http://arxiv.org/abs/1210.2877 | author:Constantin Papaodysseus, Dimitris Arabadjis, Michalis Exarhos, Panayiotis Rousopoulos, Solomon Zannos, Michail Panagopoulos, Lena Papazoglou-Manioudaki category:cs.CV math.DG published:2012-10-10 summary:This paper introduces a new approach for the automated reconstruction -reassembly of fragmented objects having one surface near to plane, on the basisof the 3D representation of their constituent fragments. The whole processstarts by 3D scanning of the available fragments. The obtained representationsare properly processed so that they can be tested for possible matches. Next,four novel criteria are introduced, that lead to the determination of pairs ofmatching fragments. These criteria have been chosen so as the whole processimitates the instinctive reassembling method dedicated scholars apply. Thefirst criterion exploits the volume of the gap between two properly placedfragments. The second one considers the fragments' overlapping in each possiblematching position. Criteria 3,4 employ principles from calculus of variationsto obtain bounds for the area and the mean curvature of the contact surfacesand the length of contact curves, which must hold if the two fragments match.The method has been applied, with great success, both in the reconstruction ofobjects artificially broken by the authors and, most importantly, in thevirtual reassembling of parts of wall paintings belonging to the Mycenaiccivilization (c. 1300 B.C.), excavated in a highly fragmented condition inTyrins, Greece.
arxiv-1800-19 | Kinects and Human Kinetics: A New Approach for Studying Crowd Behavior | http://arxiv.org/abs/1210.2838 | author:Stefan Seer, Norbert Brändle, Carlo Ratti category:cs.CV physics.soc-ph published:2012-10-10 summary:Modeling crowd behavior relies on accurate data of pedestrian movements at ahigh level of detail. Imaging sensors such as cameras provide a good basis forcapturing such detailed pedestrian motion data. However, currently availablecomputer vision technologies, when applied to conventional video footage, stillcannot automatically unveil accurate motions of groups of people or crowds fromthe image sequences. We present a novel data collection approach for studyingcrowd behavior which uses the increasingly popular low-cost sensor MicrosoftKinect. The Kinect captures both standard camera data and a three-dimensionaldepth map. Our human detection and tracking algorithm is based on agglomerativeclustering of depth data captured from an elevated view - in contrast to thelateral view used for gesture recognition in Kinect gaming applications. Ourapproach transforms local Kinect 3D data to a common world coordinate system inorder to stitch together human trajectories from multiple Kinects, which allowsfor a scalable and flexible capturing area. At a testbed with real-worldpedestrian traffic we demonstrate that our approach can provide accuratetrajectories from three Kinects with a Pedestrian Detection Rate of up to 94%and a Multiple Object Tracking Precision of 4 cm. Using a comprehensive datasetof 2240 captured human trajectories we calibrate three variations of the SocialForce model. The results of our model validations indicate their particularability to reproduce the observed crowd behavior in microscopic simulations.
arxiv-1800-20 | Comparing several heuristics for a packing problem | http://arxiv.org/abs/1210.4502 | author:Camelia-M. Pintea, Cristian Pascan, Mara Hajdu-Macelaru category:cs.NE published:2012-10-10 summary:Packing problems are in general NP-hard, even for simple cases. Since nowthere are no highly efficient algorithms available for solving packingproblems. The two-dimensional bin packing problem is about packing all givenrectangular items, into a minimum size rectangular bin, without overlapping.The restriction is that the items cannot be rotated. The current paper iscomparing a greedy algorithm with a hybrid genetic algorithm in order to seewhich technique is better for the given problem. The algorithms are tested ondifferent sizes data.
arxiv-1800-21 | Cost-Sensitive Tree of Classifiers | http://arxiv.org/abs/1210.2771 | author:Zhixiang Xu, Matt J. Kusner, Kilian Q. Weinberger, Minmin Chen category:stat.ML cs.LG published:2012-10-09 summary:Recently, machine learning algorithms have successfully entered large-scalereal-world industrial applications (e.g. search engines and email spamfilters). Here, the CPU cost during test time must be budgeted and accountedfor. In this paper, we address the challenge of balancing the test-time costand the classifier accuracy in a principled fashion. The test-time cost of aclassifier is often dominated by the computation required for featureextraction-which can vary drastically across eatures. We decrease thisextraction time by constructing a tree of classifiers, through which testinputs traverse along individual paths. Each path extracts different featuresand is optimized for a specific sub-partition of the input space. By onlycomputing features for inputs that benefit from them the most, our costsensitive tree of classifiers can match the high accuracies of the currentstate-of-the-art at a small fraction of the computational cost.
arxiv-1800-22 | Deconvolving Images with Unknown Boundaries Using the Alternating Direction Method of Multipliers | http://arxiv.org/abs/1210.2687 | author:Mariana S. C. Almeida, Mário A. T. Figueiredo category:math.OC cs.CV 68U10 I.4.4 published:2012-10-09 summary:The alternating direction method of multipliers (ADMM) has recently sparkedinterest as a flexible and efficient optimization tool for imaging inverseproblems, namely deconvolution and reconstruction under non-smooth convexregularization. ADMM achieves state-of-the-art speed by adopting a divide andconquer strategy, wherein a hard problem is split into simpler, efficientlysolvable sub-problems (e.g., using fast Fourier or wavelet transforms, orsimple proximity operators). In deconvolution, one of these sub-problemsinvolves a matrix inversion (i.e., solving a linear system), which can be doneefficiently (in the discrete Fourier domain) if the observation operator iscirculant, i.e., under periodic boundary conditions. This paper extendsADMM-based image deconvolution to the more realistic scenario of unknownboundary, where the observation operator is modeled as the composition of aconvolution (with arbitrary boundary conditions) with a spatial mask that keepsonly pixels that do not depend on the unknown boundary. The proposed approachalso handles, at no extra cost, problems that combine the recovery of missingpixels (i.e., inpainting) with deconvolution. We show that the resultingalgorithms inherit the convergence guarantees of ADMM and illustrate itsperformance on non-periodic deblurring (with and without inpainting of interiorpixels) under total-variation and frame-based regularization.
arxiv-1800-23 | Level Set Estimation from Compressive Measurements using Box Constrained Total Variation Regularization | http://arxiv.org/abs/1210.2474 | author:Akshay Soni, Jarvis Haupt category:cs.CV stat.AP stat.ML published:2012-10-09 summary:Estimating the level set of a signal from measurements is a task that arisesin a variety of fields, including medical imaging, astronomy, and digitalelevation mapping. Motivated by scenarios where accurate and completemeasurements of the signal may not available, we examine here a simpleprocedure for estimating the level set of a signal from highly incompletemeasurements, which may additionally be corrupted by additive noise. Theproposed procedure is based on box-constrained Total Variation (TV)regularization. We demonstrate the performance of our approach, relative toexisting state-of-the-art techniques for level set estimation from compressivemeasurements, via several simulation examples.
arxiv-1800-24 | Quantifying Causal Coupling Strength: A Lag-specific Measure For Multivariate Time Series Related To Transfer Entropy | http://arxiv.org/abs/1210.2748 | author:Jakob Runge, Jobst Heitzig, Norbert Marwan, Jürgen Kurths category:cs.IT math.IT stat.ML published:2012-10-09 summary:While it is an important problem to identify the existence of causalassociations between two components of a multivariate time series, a topicaddressed in Runge et al. (2012), it is even more important to assess thestrength of their association in a meaningful way. In the present article wefocus on the problem of defining a meaningful coupling strength usinginformation theoretic measures and demonstrate the short-comings of thewell-known mutual information and transfer entropy. Instead, we propose acertain time-delayed conditional mutual information, the momentary informationtransfer (MIT), as a measure of association that is general, causal andlag-specific, reflects a well interpretable notion of coupling strength and ispractically computable. MIT is based on the fundamental concept of sourceentropy, which we utilize to yield a notion of coupling strength that is,compared to mutual information and transfer entropy, well interpretable, inthat for many cases it solely depends on the interaction of the two componentsat a certain lag. In particular, MIT is thus in many cases able to exclude themisleading influence of autodependency within a process in aninformation-theoretic way. We formalize and prove this idea analytically andnumerically for a general class of nonlinear stochastic processes andillustrate the potential of MIT on climatological data.
arxiv-1800-25 | Gaussian process modelling of multiple short time series | http://arxiv.org/abs/1210.2503 | author:Hande Topa, Antti Honkela category:stat.ML q-bio.QM stat.ME published:2012-10-09 summary:We present techniques for effective Gaussian process (GP) modelling ofmultiple short time series. These problems are common when applying GP modelsindependently to each gene in a gene expression time series data set. Such setstypically contain very few time points. Naive application of common GPmodelling techniques can lead to severe over-fitting or under-fitting in asignificant fraction of the fitted models, depending on the details of the dataset. We propose avoiding over-fitting by constraining the GP length-scale tovalues that focus most of the energy spectrum to frequencies below the Nyquistfrequency corresponding to the sampling frequency in the data set.Under-fitting can be avoided by more informative priors on observation noise.Combining these methods allows applying GP methods reliably automatically tolarge numbers of independent instances of short time series. This isillustrated with experiments with both synthetic data and real gene expressiondata.
arxiv-1800-26 | Measuring the Influence of Observations in HMMs through the Kullback-Leibler Distance | http://arxiv.org/abs/1210.2613 | author:Vittorio Perduca, Gregory Nuel category:cs.IT cs.LG math.IT math.PR published:2012-10-09 summary:We measure the influence of individual observations on the sequence of thehidden states of the Hidden Markov Model (HMM) by means of the Kullback-Leiblerdistance (KLD). Namely, we consider the KLD between the conditionaldistribution of the hidden states' chain given the complete sequence ofobservations and the conditional distribution of the hidden chain given all theobservations but the one under consideration. We introduce a linear complexityalgorithm for computing the influence of all the observations. As anillustration, we investigate the application of our algorithm to the problem ofdetecting outliers in HMM data series.
arxiv-1800-27 | A General Methodology for the Determination of 2D Bodies Elastic Deformation Invariants. Application to the Automatic Identification of Parasites | http://arxiv.org/abs/1210.2646 | author:Dimitris Arabadjis, Panayiotis Rousopoulos, Constantin Papaodysseus, Michalis Panagopoulos, Panayiota Loumou, Georgios Theodoropoulos category:cs.CV cs.AI published:2012-10-09 summary:A novel methodology is introduced here that exploits 2D images of arbitraryelastic body deformation instances, so as to quantify mechano-elasticcharacteristics that are deformation invariant. Determination of suchcharacteristics allows for developing methods offering an image of theundeformed body. General assumptions about the mechano-elastic properties ofthe bodies are stated, which lead to two different approaches for obtainingbodies' deformation invariants. One was developed to spot deformed body'sneutral line and its cross sections, while the other solves deformation PDEs byperforming a set of equivalent image operations on the deformed body images.Both these processes may furnish a body undeformed version from its deformedimage. This was confirmed by obtaining the undeformed shape of deformedparasites, cells (protozoa), fibers and human lips. In addition, the method hasbeen applied to the important problem of parasite automatic classification fromtheir microscopic images. To achieve this, we first apply the previous methodto straighten the highly deformed parasites and then we apply a dedicated curveclassification method to the straightened parasite contours. It is demonstratedthat essentially different deformations of the same parasite give rise topractically the same undeformed shape, thus confirming the consistency of theintroduced methodology. Finally, the developed pattern recognition methodclassifies the unwrapped parasites into 6 families, with an accuracy rate of97.6 %.
arxiv-1800-28 | Multi-view constrained clustering with an incomplete mapping between views | http://arxiv.org/abs/1210.2640 | author:Eric Eaton, Marie desJardins, Sara Jacob category:cs.LG cs.AI published:2012-10-09 summary:Multi-view learning algorithms typically assume a complete bipartite mappingbetween the different views in order to exchange information during thelearning process. However, many applications provide only a partial mappingbetween the views, creating a challenge for current methods. To address thisproblem, we propose a multi-view algorithm based on constrained clustering thatcan operate with an incomplete mapping. Given a set of pairwise constraints ineach view, our approach propagates these constraints using a local similaritymeasure to those instances that can be mapped to the other views, allowing thepropagated constraints to be transferred across views via the partial mapping.It uses co-EM to iteratively estimate the propagation within each view based onthe current clustering model, transfer the constraints across views, and thenupdate the clustering model. By alternating the learning process between views,this approach produces a unified clustering model that is consistent with allviews. We show that this approach significantly improves clustering performanceover several other methods for transferring constraints and allows multi-viewclustering to be reliably applied when given a limited mapping between theviews. Our evaluation reveals that the propagated constraints have highprecision with respect to the true clusters in the data, explaining theirbenefit to clustering performance in both single- and multi-view learningscenarios.
arxiv-1800-29 | Optimization in Differentiable Manifolds in Order to Determine the Method of Construction of Prehistoric Wall-Paintings | http://arxiv.org/abs/1210.2629 | author:Dimitris Arabadjis, Panayiotis Rousopoulos, Constantin Papaodysseus, Michalis Exarhos, Michalis Panagopoulos, Lena Papazoglou-Manioudaki category:cs.CV cs.AI cs.CG published:2012-10-09 summary:In this paper a general methodology is introduced for the determination ofpotential prototype curves used for the drawing of prehistoric wall-paintings.The approach includes a) preprocessing of the wall-paintings contours toproperly partition them, according to their curvature, b) choice of prototypecurves families, c) analysis and optimization in 4-manifold for a firstestimation of the form of these prototypes, d) clustering of the contour partsand the prototypes, to determine a minimal number of potential guides, e)further optimization in 4-manifold, applied to each cluster separately, inorder to determine the exact functional form of the potential guides, togetherwith the corresponding drawn contour parts. The introduced methodologysimultaneously deals with two problems: a) the arbitrariness in data-pointsorientation and b) the determination of one proper form for a prototype curvethat optimally fits the corresponding contour data. Arbitrariness inorientation has been dealt with a novel curvature based error, while the properforms of curve prototypes have been exhaustively determined by embeddingcurvature deformations of the prototypes into 4-manifolds. Application of thismethodology to celebrated wall-paintings excavated at Tyrins, Greece and theGreek island of Thera, manifests it is highly probable that thesewall-paintings had been drawn by means of geometric guides that correspond tolinear spirals and hyperbolae. These geometric forms fit the drawings' lineswith an exceptionally low average error, less than 0.39mm. Hence, the approachsuggests the existence of accurate realizations of complicated geometricentities, more than 1000 years before their axiomatic formulation in ClassicalAges.
arxiv-1800-30 | ET-LDA: Joint Topic Modeling For Aligning, Analyzing and Sensemaking of Public Events and Their Twitter Feeds | http://arxiv.org/abs/1210.2164 | author:Yuheng Hu, Ajita John, Fei Wang, Doree Duncan Seligmann, Subbarao Kambhampati category:cs.LG cs.AI cs.SI physics.soc-ph published:2012-10-08 summary:Social media channels such as Twitter have emerged as popular platforms forcrowds to respond to public events such as speeches, sports and debates. Whilethis promises tremendous opportunities to understand and make sense of thereception of an event from the social media, the promises come entwined withsignificant technical challenges. In particular, given an event and anassociated large scale collection of tweets, we need approaches to effectivelyalign tweets and the parts of the event they refer to. This in turn raisesquestions about how to segment the event into smaller yet meaningful parts, andhow to figure out whether a tweet is a general one about the entire event orspecific one aimed at a particular segment of the event. In this work, wepresent ET-LDA, an effective method for aligning an event and its tweetsthrough joint statistical modeling of topical influences from the events andtheir associated tweets. The model enables the automatic segmentation of theevents and the characterization of tweets into two categories: (1) episodictweets that respond specifically to the content in the segments of the events,and (2) steady tweets that respond generally about the events. We present anefficient inference method for this model, and a comprehensive evaluation ofits effectiveness over existing methods. In particular, through a user study,we demonstrate that users find the topics, the segments, the alignment, and theepisodic tweets discovered by ET-LDA to be of higher quality and moreinteresting as compared to the state-of-the-art, with improvements in the rangeof 18-41%.
arxiv-1800-31 | Fast Online EM for Big Topic Modeling | http://arxiv.org/abs/1210.2179 | author:Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao category:cs.LG published:2012-10-08 summary:The expectation-maximization (EM) algorithm can compute themaximum-likelihood (ML) or maximum a posterior (MAP) point estimate of themixture models or latent variable models such as latent Dirichlet allocation(LDA), which has been one of the most popular probabilistic topic modelingmethods in the past decade. However, batch EM has high time and spacecomplexities to learn big LDA models from big data streams. In this paper, wepresent a fast online EM (FOEM) algorithm that infers the topic distributionfrom the previously unseen documents incrementally with constant memoryrequirements. Within the stochastic approximation framework, we show that FOEMcan converge to the local stationary point of the LDA's likelihood function. Bydynamic scheduling for the fast speed and parameter streaming for the lowmemory usage, FOEM is more efficient for some lifelong topic modeling tasksthan the state-of-the-art online LDA algorithms to handle both big data and bigmodels (aka, big topic modeling) on just a PC.
arxiv-1800-32 | Blending Learning and Inference in Structured Prediction | http://arxiv.org/abs/1210.2346 | author:Tamir Hazan, Alexander Schwing, David McAllester, Raquel Urtasun category:cs.LG published:2012-10-08 summary:In this paper we derive an efficient algorithm to learn the parameters ofstructured predictors in general graphical models. This algorithm blends thelearning and inference tasks, which results in a significant speedup overtraditional approaches, such as conditional random fields and structuredsupport vector machines. For this purpose we utilize the structures of thepredictors to describe a low dimensional structured prediction task whichencourages local consistencies within the different structures while learningthe parameters of the model. Convexity of the learning task provides the meansto enforce the consistencies between the different parts. Theinference-learning blending algorithm that we propose is guaranteed to convergeto the optimum of the low dimensional primal and dual programs. Unlike many ofthe existing approaches, the inference-learning blending allows us to learnefficiently high-order graphical models, over regions of any size, and verylarge number of parameters. We demonstrate the effectiveness of our approach,while presenting state-of-the-art results in stereo estimation, semanticsegmentation, shape reconstruction, and indoor scene understanding.
arxiv-1800-33 | Modeling Weather Conditions Consequences on Road Trafficking Behaviors | http://arxiv.org/abs/1210.2294 | author:Guillaume Allain, Fabrice Gamboa, Philippe Goudal, Jean-Noël Kien, Jean-Michel Loubes category:stat.ME stat.AP stat.ML published:2012-10-08 summary:We provide a model to understand how adverse weather conditions modifytraffic flow dynamic. We first prove that the microscopic Free Flow Speed ofthe vehicles is changed and then provide a rule to model this change. For this,we consider a thresholded linear model, corresponding to an application of aMARS model to road trafficking. This model adapts itself locally to the wholeroad network and provides accurate unbiased forecasted speed using live orshort term forecasted weather data information.
arxiv-1800-34 | A Fast Distributed Proximal-Gradient Method | http://arxiv.org/abs/1210.2289 | author:Annie I. Chen, Asuman Ozdaglar category:cs.DC cs.LG stat.ML published:2012-10-08 summary:We present a distributed proximal-gradient method for optimizing the averageof convex functions, each of which is the private local objective of an agentin a network with time-varying topology. The local objectives have distinctdifferentiable components, but they share a common nondifferentiable component,which has a favorable structure suitable for effective computation of theproximal operator. In our method, each agent iteratively updates its estimateof the global minimum by optimizing its local objective function, andexchanging estimates with others via communication in the network. UsingNesterov-type acceleration techniques and multiple communication steps periteration, we show that this method converges at the rate 1/k (where k is thenumber of communication rounds between the agents), which is faster than theconvergence rate of the existing distributed methods for solving this problem.The superior convergence rate of our method is also verified by numericalexperiments.
arxiv-1800-35 | Stable and robust sampling strategies for compressive imaging | http://arxiv.org/abs/1210.2380 | author:Felix Krahmer, Rachel Ward category:cs.CV cs.IT math.IT math.NA published:2012-10-08 summary:In many signal processing applications, one wishes to acquire images that aresparse in transform domains such as spatial finite differences or waveletsusing frequency domain samples. For such applications, overwhelming empiricalevidence suggests that superior image reconstruction can be obtained throughvariable density sampling strategies that concentrate on lower frequencies. Thewavelet and Fourier transform domains are not incoherent because low-orderwavelets and low-order frequencies are correlated, so compressive sensingtheory does not immediately imply sampling strategies and reconstructionguarantees. In this paper we turn to a more refined notion of coherence -- theso-called local coherence -- measuring for each sensing vector separately howcorrelated it is to the sparsity basis. For Fourier measurements and Haarwavelet sparsity, the local coherence can be controlled and bounded explicitly,so for matrices comprised of frequencies sampled from a suitable inverse squarepower-law density, we can prove the restricted isometry property withnear-optimal embedding dimensions. Consequently, the variable-density samplingstrategy we provide allows for image reconstructions that are stable tosparsity defects and robust to measurement noise. Our results cover bothreconstruction by $\ell_1$-minimization and by total variation minimization.The local coherence framework developed in this paper should be of independentinterest in sparse recovery problems more generally, as it implies that foroptimal sparse recovery results, it suffices to have bounded \emph{average}coherence from sensing basis to sparsity basis -- as opposed to bounded maximalcoherence -- as long as the sampling strategy is adapted accordingly.
arxiv-1800-36 | The Power of Linear Reconstruction Attacks | http://arxiv.org/abs/1210.2381 | author:Shiva Prasad Kasiviswanathan, Mark Rudelson, Adam Smith category:cs.DS cs.CR cs.LG math.PR published:2012-10-08 summary:We consider the power of linear reconstruction attacks in statistical dataprivacy, showing that they can be applied to a much wider range of settingsthan previously understood. Linear attacks have been studied before (Dinur andNissim PODS'03, Dwork, McSherry and Talwar STOC'07, Kasiviswanathan, Rudelson,Smith and Ullman STOC'10, De TCC'12, Muthukrishnan and Nikolov STOC'12) buthave so far been applied only in settings with releases that are obviouslylinear. Consider a database curator who manages a database of sensitive informationbut wants to release statistics about how a sensitive attribute (say, disease)in the database relates to some nonsensitive attributes (e.g., postal code,age, gender, etc). We show one can mount linear reconstruction attacks based onany release that gives: a) the fraction of records that satisfy a givennon-degenerate boolean function. Such releases include contingency tables(previously studied by Kasiviswanathan et al., STOC'10) as well as more complexoutputs like the error rate of classifiers such as decision trees; b) any oneof a large class of M-estimators (that is, the output of empirical riskminimization algorithms), including the standard estimators for linear andlogistic regression. We make two contributions: first, we show how these types of releases can betransformed into a linear format, making them amenable to existingpolynomial-time reconstruction algorithms. This is already perhaps surprising,since many of the above releases (like M-estimators) are obtained by solvinghighly nonlinear formulations. Second, we show how to analyze the resultingattacks under various distributional assumptions on the data. Specifically, weconsider a setting in which the same statistic (either a) or b) above) isreleased about how the sensitive attribute relates to all subsets of size k(out of a total of d) nonsensitive boolean attributes.
arxiv-1800-37 | Video De-fencing | http://arxiv.org/abs/1210.2388 | author:Yadong Mu, Wei Liu, Shuicheng Yan category:cs.CV cs.MM published:2012-10-08 summary:This paper describes and provides an initial solution to a novel videoediting task, i.e., video de-fencing. It targets automatic restoration of thevideo clips that are corrupted by fence-like occlusions during capture. Our keyobservation lies in the visual parallax between fences and background scenes,which is caused by the fact that the former are typically closer to the camera.Unlike in traditional image inpainting, fence-occluded pixels in the videostend to appear later in the temporal dimension and are therefore recoverablevia optimized pixel selection from relevant frames. To eventually producefence-free videos, major challenges include cross-frame sub-pixel imagealignment under diverse scene depth, and "correct" pixel selection that isrobust to dominating fence pixels. Several novel tools are developed in thispaper, including soft fence detection, weighted truncated optical flow methodand robust temporal median filter. The proposed algorithm is validated onseveral real-world video clips with fences.
arxiv-1800-38 | A notion of continuity in discrete spaces and applications | http://arxiv.org/abs/1210.2352 | author:Valerio Capraro category:math.MG cs.CV math.CO math.GN published:2012-10-08 summary:We propose a notion of continuous path for locally finite metric spaces,taking inspiration from the recent development of A-theory for locally finiteconnected graphs. We use this notion of continuity to derive an analogue in Z^2of the Jordan curve theorem and to extend to a quite large class of locallyfinite metric spaces (containing all finite metric spaces) an inequality forthe \ell^p-distortion of a metric space that has been recently proved byPierre-Nicolas Jolissaint and Alain Valette for finite connected graphs.
arxiv-1800-39 | Epitome for Automatic Image Colorization | http://arxiv.org/abs/1210.4481 | author:Yingzhen Yang, Xinqi Chu, Tian-Tsong Ng, Alex Yong-Sang Chia, Shuicheng Yan, Thomas S. Huang category:cs.CV cs.LG cs.MM published:2012-10-08 summary:Image colorization adds color to grayscale images. It not only increases thevisual appeal of grayscale images, but also enriches the information containedin scientific images that lack color information. Most existing methods ofcolorization require laborious user interaction for scribbles or imagesegmentation. To eliminate the need for human labor, we develop an automaticimage colorization method using epitome. Built upon a generative graphicalmodel, epitome is a condensed image appearance and shape model which alsoproves to be an effective summary of color information for the colorizationtask. We train the epitome from the reference images and perform inference inthe epitome to colorize grayscale images, rendering better colorization resultsthan previous method in our experiments.
arxiv-1800-40 | Semisupervised Classifier Evaluation and Recalibration | http://arxiv.org/abs/1210.2162 | author:Peter Welinder, Max Welling, Pietro Perona category:cs.LG cs.CV published:2012-10-08 summary:How many labeled examples are needed to estimate a classifier's performanceon a new dataset? We study the case where data is plentiful, but labels areexpensive. We show that by making a few reasonable assumptions on the structureof the data, it is possible to estimate performance curves, with confidencebounds, using a small number of ground truth labels. Our approach, which wecall Semisupervised Performance Evaluation (SPE), is based on a generativemodel for the classifier's confidence scores. In addition to estimating theperformance of classifiers on new datasets, SPE can be used to recalibrate aclassifier by re-estimating the class-conditional confidence distributions.
arxiv-1800-41 | Group Model Selection Using Marginal Correlations: The Good, the Bad and the Ugly | http://arxiv.org/abs/1210.2440 | author:Waheed U. Bajwa, Dustin G. Mixon category:math.ST cs.IT math.IT stat.ML stat.TH published:2012-10-08 summary:Group model selection is the problem of determining a small subset of groupsof predictors (e.g., the expression data of genes) that are responsible formajority of the variation in a response variable (e.g., the malignancy of atumor). This paper focuses on group model selection in high-dimensional linearmodels, in which the number of predictors far exceeds the number of samples ofthe response variable. Existing works on high-dimensional group model selectioneither require the number of samples of the response variable to besignificantly larger than the total number of predictors contributing to theresponse or impose restrictive statistical priors on the predictors and/ornonzero regression coefficients. This paper provides comprehensiveunderstanding of a low-complexity approach to group model selection that avoidssome of these limitations. The proposed approach, termed Group Thresholding(GroTh), is based on thresholding of marginal correlations of groups ofpredictors with the response variable and is reminiscent of existingthresholding-based approaches in the literature. The most importantcontribution of the paper in this regard is relating the performance of GroThto a polynomial-time verifiable property of the predictors for the general caseof arbitrary (random or deterministic) predictors and arbitrary nonzeroregression coefficients.
arxiv-1800-42 | Mining Permission Request Patterns from Android and Facebook Applications (extended author version) | http://arxiv.org/abs/1210.2429 | author:Mario Frank, Ben Dong, Adrienne Porter Felt, Dawn Song category:cs.CR cs.AI stat.ML published:2012-10-08 summary:Android and Facebook provide third-party applications with access to users'private data and the ability to perform potentially sensitive operations (e.g.,post to a user's wall or place phone calls). As a security measure, theseplatforms restrict applications' privileges with permission systems: users mustapprove the permissions requested by applications before the applications canmake privacy- or security-relevant API calls. However, recent studies haveshown that users often do not understand permission requests and lack a notionof typicality of requests. As a first step towards simplifying permissionsystems, we cluster a corpus of 188,389 Android applications and 27,029Facebook applications to find patterns in permission requests. Using a methodfor Boolean matrix factorization for finding overlapping clusters, we find thatFacebook permission requests follow a clear structure that exhibits highstability when fitted with only five clusters, whereas Android applicationsdemonstrate more complex permission requests. We also find that low-reputationapplications often deviate from the permission request patterns that weidentified for high-reputation applications suggesting that permission requestpatterns are indicative for user satisfaction or application quality.
arxiv-1800-43 | Privacy Aware Learning | http://arxiv.org/abs/1210.2085 | author:John C. Duchi, Michael I. Jordan, Martin J. Wainwright category:stat.ML cs.IT cs.LG math.IT published:2012-10-07 summary:We study statistical risk minimization problems under a privacy model inwhich the data is kept confidential even from the learner. In this localprivacy framework, we establish sharp upper and lower bounds on the convergencerates of statistical estimation procedures. As a consequence, we exhibit aprecise tradeoff between the amount of privacy the data preserves and theutility, as measured by convergence rate, of any statistical estimator orlearning procedure.
arxiv-1800-44 | Locally adaptive factor processes for multivariate time series | http://arxiv.org/abs/1210.2022 | author:Daniele Durante, Bruno Scarpa, David B. Dunson category:stat.AP stat.ML published:2012-10-07 summary:In modeling multivariate time series, it is important to allow time-varyingsmoothness in the mean and covariance process. In particular, there may becertain time intervals exhibiting rapid changes and others in which changes areslow. If such time-varying smoothness is not accounted for, one can obtainmisleading inferences and predictions, with over-smoothing across erratic timeintervals and under-smoothing across times exhibiting slow variation. This canlead to mis-calibration of predictive intervals, which can be substantially toonarrow or wide depending on the time. We propose a locally adaptive factorprocess for characterizing multivariate mean-covariance changes in continuoustime, allowing locally varying smoothness in both the mean and covariancematrix. This process is constructed utilizing latent dictionary functionsevolving in time through nested Gaussian processes and linearly related to theobserved data with a sparse mapping. Using a differential equationrepresentation, we bypass usual computational bottlenecks in obtaining MCMC andonline algorithms for approximate Bayesian inference. The performance isassessed in simulations and illustrated in a financial application.
arxiv-1800-45 | Sparsity by Worst-Case Quadratic Penalties | http://arxiv.org/abs/1210.2077 | author:Yves Grandvalet, Julien Chiquet, Christophe Ambroise category:stat.ML stat.CO published:2012-10-07 summary:This paper proposes a new robust regression interpretation of sparsepenalties such as the elastic net and the group-lasso. Beyond providing a newviewpoint on these penalization schemes, our approach results in a unifiedoptimization strategy. Our evaluation experiments demonstrate that thisstrategy, implemented on the elastic net, is computationally extremelyefficient for small to medium size problems. Our accompanying software solvesproblems at machine precision in the time required to get a rough estimate withcompeting state-of-the-art algorithms.
arxiv-1800-46 | Anomalous Vacillatory Learning | http://arxiv.org/abs/1210.2051 | author:Achilles Beros category:math.LO cs.LG cs.LO 03D80, 68Q32 published:2012-10-07 summary:In 1986, Osherson, Stob and Weinstein asked whether two variants of anomalousvacillatory learning, TxtFex^*_* and TxtFext^*_*, could be distinguished. Inboth, a machine is permitted to vacillate between a finite number of hypothesesand to make a finite number of errors. TxtFext^*_*-learning requires thathypotheses output infinitely often must describe the same finite variant of thecorrect set, while TxtFex^*_*-learning permits the learner to vacillate betweenfinitely many different finite variants of the correct set. In this paper weshow that TxtFex^*_* \neq TxtFext^*_*, thereby answering the question posed byOsherson, \textit{et al}. We prove this in a strong way by exhibiting a familyin TxtFex^*_2 \setminus {TxtFext}^*_*.
arxiv-1800-47 | Information fusion in multi-task Gaussian processes | http://arxiv.org/abs/1210.1928 | author:Shrihari Vasudevan, Arman Melkumyan, Steven Scheding category:stat.ML cs.AI cs.LG published:2012-10-06 summary:This paper evaluates heterogeneous information fusion using multi-taskGaussian processes in the context of geological resource modeling.Specifically, it empirically demonstrates that information integration acrossheterogeneous information sources leads to superior estimates of all thequantities being modeled, compared to modeling them individually. Multi-taskGaussian processes provide a powerful approach for simultaneous modeling ofmultiple quantities of interest while taking correlations between thesequantities into consideration. Experiments are performed on large scale realsensor data.
arxiv-1800-48 | Reply to Comments on Neuroelectrodynamics: Where are the Real Conceptual Pitfalls? | http://arxiv.org/abs/1210.1983 | author:Dorian Aur category:cs.NE nlin.AO physics.bio-ph q-bio.NC published:2012-10-06 summary:The fundamental, powerful process of computation in the brain has been widelymisunderstood. The paper [1] associates the general failure to buildintelligent thinking machines with current reductionist principles of temporalcoding and advocates for a change in paradigm regarding the brain analogy.Since fragments of information are stored in proteins which can shift betweenseveral structures to perform their function, the biological substrate isactively involved in physical computation. The intrinsic nonlinear dynamics ofaction potentials and synaptic activities maintain physical interactions withinand between neurons in the brain. During these events the required informationis exchanged between molecular structures (proteins) which store fragments ofinformation and the generated electric flux which carries and integratesinformation in the brain. The entire process of physical interaction explainshow the brain actively creates or experiences meaning. This process ofinteraction during an action potential generation can be simply seen as themoment when the neuron solves a many-body problem. A neuroelectrodynamic theoryshows that the neuron solves equations rather than exclusively computesfunctions. With the main focus on temporal patterns, the spike timing dogma(STD) has neglected important forms of computation which do occur insideneurons. In addition, artificial neural models have missed the most importantpart since the real super-computing power of the brain has its origins incomputations that occur within neurons.
arxiv-1800-49 | Feature Selection via L1-Penalized Squared-Loss Mutual Information | http://arxiv.org/abs/1210.1960 | author:Wittawat Jitkrittum, Hirotaka Hachiya, Masashi Sugiyama category:stat.ML cs.LG published:2012-10-06 summary:Feature selection is a technique to screen out less important features. Manyexisting supervised feature selection algorithms use redundancy and relevancyas the main criteria to select features. However, feature interaction,potentially a key characteristic in real-world problems, has not received muchattention. As an attempt to take feature interaction into account, we proposeL1-LSMI, an L1-regularization based algorithm that maximizes a squared-lossvariant of mutual information between selected features and outputs. Numericalresults show that L1-LSMI performs well in handling redundancy, detectingnon-linear dependency, and considering feature interaction.
arxiv-1800-50 | A comparative study on face recognition techniques and neural network | http://arxiv.org/abs/1210.1916 | author:Meftah Ur Rahman category:cs.CV published:2012-10-06 summary:In modern times, face recognition has become one of the key aspects ofcomputer vision. There are at least two reasons for this trend; the first isthe commercial and law enforcement applications, and the second is theavailability of feasible technologies after years of research. Due to the verynature of the problem, computer scientists, neuro-scientists and psychologistsall share a keen interest in this field. In plain words, it is a computerapplication for automatically identifying a person from a still image or videoframe. One of the ways to accomplish this is by comparing selected featuresfrom the image and a facial database. There are hundreds if not thousandfactors associated with this. In this paper some of the most common techniquesavailable including applications of neural network in facial recognition arestudied and compared with respect to their performance.
arxiv-1800-51 | Bayesian Inference with Posterior Regularization and applications to Infinite Latent SVMs | http://arxiv.org/abs/1210.1766 | author:Jun Zhu, Ning Chen, Eric P. Xing category:cs.LG cs.AI stat.ME stat.ML published:2012-10-05 summary:Existing Bayesian models, especially nonparametric Bayesian methods, rely onspecially conceived priors to incorporate domain knowledge for discoveringimproved latent representations. While priors can affect posteriordistributions through Bayes' rule, imposing posterior regularization isarguably more direct and in some cases more natural and general. In this paper,we present regularized Bayesian inference (RegBayes), a novel computationalframework that performs posterior inference with a regularization term on thedesired post-data posterior distribution under an information theoreticalformulation. RegBayes is more flexible than the procedure that elicits expertknowledge via priors, and it covers both directed Bayesian networks andundirected Markov networks whose Bayesian formulation results in hybrid chaingraph models. When the regularization is induced from a linear operator on theposterior distributions, such as the expectation operator, we present a generalconvex-analysis theorem to characterize the solution of RegBayes. Furthermore,we present two concrete examples of RegBayes, infinite latent support vectormachines (iLSVM) and multi-task infinite latent support vector machines(MT-iLSVM), which explore the large-margin idea in combination with anonparametric Bayesian model for discovering predictive latent features forclassification and multi-task learning, respectively. We present efficientinference methods and report empirical studies on several benchmark datasets,which appear to demonstrate the merits inherited from both large-marginlearning and Bayesian nonparametrics. Such results were not available untilnow, and contribute to push forward the interface between these two importantsubfields, which have been largely treated as isolated in the community.
arxiv-1800-52 | Learning Heterogeneous Similarity Measures for Hybrid-Recommendations in Meta-Mining | http://arxiv.org/abs/1210.1317 | author:Phong Nguyen, Jun Wang, Melanie Hilario, Alexandros Kalousis category:cs.LG cs.AI published:2012-10-04 summary:The notion of meta-mining has appeared recently and extends the traditionalmeta-learning in two ways. First it does not learn meta-models that providesupport only for the learning algorithm selection task but ones that supportthe whole data-mining process. In addition it abandons the so called black-boxapproach to algorithm description followed in meta-learning. Now in addition tothe datasets, algorithms also have descriptors, workflows as well. For thelatter two these descriptions are semantic, describing properties of thealgorithms. With the availability of descriptors both for datasets and datamining workflows the traditional modelling techniques followed inmeta-learning, typically based on classification and regression algorithms, areno longer appropriate. Instead we are faced with a problem the nature of whichis much more similar to the problems that appear in recommendation systems. Themost important meta-mining requirements are that suggestions should use onlydatasets and workflows descriptors and the cold-start problem, e.g. providingworkflow suggestions for new datasets. In this paper we take a different view on the meta-mining modelling problemand treat it as a recommender problem. In order to account for the meta-miningspecificities we derive a novel metric-based-learning recommender approach. Ourmethod learns two homogeneous metrics, one in the dataset and one in theworkflow space, and a heterogeneous one in the dataset-workflow space. Alllearned metrics reflect similarities established from the dataset-workflowpreference matrix. We demonstrate our method on meta-mining over biological(microarray datasets) problems. The application of our method is not limited tothe meta-mining problem, its formulations is general enough so that it can beapplied on problems with similar requirements.
arxiv-1800-53 | A network of spiking neurons for computing sparse representations in an energy efficient way | http://arxiv.org/abs/1210.1530 | author:Tao Hu, Alexander Genkin, Dmitri B. Chklovskii category:cs.NE q-bio.NC published:2012-10-04 summary:Computing sparse redundant representations is an important problem both inapplied mathematics and neuroscience. In many applications, this problem mustbe solved in an energy efficient way. Here, we propose a hybrid distributedalgorithm (HDA), which solves this problem on a network of simple nodescommunicating via low-bandwidth channels. HDA nodes perform bothgradient-descent-like steps on analog internal variables andcoordinate-descent-like steps via quantized external variables communicated toeach other. Interestingly, such operation is equivalent to a network ofintegrate-and-fire neurons, suggesting that HDA may serve as a model of neuralcomputation. We show that the numerical performance of HDA is on par withexisting algorithms. In the asymptotic regime the representation error of HDAdecays with time, t, as 1/t. HDA is stable against time-varying noise,specifically, the representation error decays as 1/sqrt(t) for Gaussian whitenoise.
arxiv-1800-54 | Learning Human Activities and Object Affordances from RGB-D Videos | http://arxiv.org/abs/1210.1207 | author:Hema Swetha Koppula, Rudhir Gupta, Ashutosh Saxena category:cs.RO cs.AI cs.CV published:2012-10-04 summary:Understanding human activities and object affordances are two very importantskills, especially for personal robots which operate in human environments. Inthis work, we consider the problem of extracting a descriptive labeling of thesequence of sub-activities being performed by a human, and more importantly, oftheir interactions with the objects in the form of associated affordances.Given a RGB-D video, we jointly model the human activities and objectaffordances as a Markov random field where the nodes represent objects andsub-activities, and the edges represent the relationships between objectaffordances, their relations with sub-activities, and their evolution overtime. We formulate the learning problem using a structural support vectormachine (SSVM) approach, where labelings over various alternate temporalsegmentations are considered as latent variables. We tested our method on achallenging dataset comprising 120 activity videos collected from 4 subjects,and obtained an accuracy of 79.4% for affordance, 63.4% for sub-activity and75.0% for high-level activity labeling. We then demonstrate the use of suchdescriptive labeling in performing assistive tasks by a PR2 robot.
arxiv-1800-55 | A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound | http://arxiv.org/abs/1210.1461 | author:Shusen Wang, Zhihua Zhang, Jian Li category:cs.LG cs.DM stat.ML published:2012-10-04 summary:The CUR matrix decomposition is an important extension of Nystr\"{o}mapproximation to a general matrix. It approximates any data matrix in terms ofa small number of its columns and rows. In this paper we propose a novelrandomized CUR algorithm with an expected relative-error bound. The proposedalgorithm has the advantages over the existing relative-error CUR algorithmsthat it possesses tighter theoretical bound and lower time complexity, and thatit can avoid maintaining the whole data matrix in main memory. Finally,experiments on several real-world datasets demonstrate significant improvementover the existing relative-error algorithms.
arxiv-1800-56 | Learning Locality-Constrained Collaborative Representation for Face Recognition | http://arxiv.org/abs/1210.1316 | author:Xi Peng, Lei Zhang, Zhang Yi, Kok Kiong Tan category:cs.CV published:2012-10-04 summary:The model of low-dimensional manifold and sparse representation are twowell-known concise models that suggest each data can be described by a fewcharacteristics. Manifold learning is usually investigated for dimensionreduction by preserving some expected local geometric structures from theoriginal space to a low-dimensional one. The structures are generallydetermined by using pairwise distance, e.g., Euclidean distance. Alternatively,sparse representation denotes a data point as a linear combination of thepoints from the same subspace. In practical applications, however, the nearbypoints in terms of pairwise distance may not belong to the same subspace, andvice versa. Consequently, it is interesting and important to explore how to geta better representation by integrating these two models together. To this end,this paper proposes a novel coding algorithm, called Locality-ConstrainedCollaborative Representation (LCCR), which improves the robustness anddiscrimination of data representation by introducing a kind of localconsistency. The locality term derives from a biologic observation that thesimilar inputs have similar code. The objective function of LCCR has ananalytical solution, and it does not involve local minima. The empiricalstudies based on four public facial databases, ORL, AR, Extended Yale B, andMultiple PIE, show that LCCR is promising in recognizing human faces fromfrontal views with varying expression and illumination, as well as variouscorruptions and occlusions.
arxiv-1800-57 | Learning from Collective Intelligence in Groups | http://arxiv.org/abs/1210.0954 | author:Guo-Jun Qi, Charu Aggarwal, Pierre Moulin, Thomas Huang category:cs.SI cs.LG published:2012-10-03 summary:Collective intelligence, which aggregates the shared information from largecrowds, is often negatively impacted by unreliable information sources with thelow quality data. This becomes a barrier to the effective use of collectiveintelligence in a variety of applications. In order to address this issue, wepropose a probabilistic model to jointly assess the reliability of sources andfind the true data. We observe that different sources are often not independentof each other. Instead, sources are prone to be mutually influenced, whichmakes them dependent when sharing information with each other. High dependencybetween sources makes collective intelligence vulnerable to the overuse ofredundant (and possibly incorrect) information from the dependent sources.Thus, we reveal the latent group structure among dependent sources, andaggregate the information at the group level rather than from individualsources directly. This can prevent the collective intelligence from beinginappropriately dominated by dependent sources. We will also explicitly revealthe reliability of groups, and minimize the negative impacts of unreliablegroups. Experimental results on real-world data sets show the effectiveness ofthe proposed approach with respect to existing algorithms.
arxiv-1800-58 | Logical segmentation for article extraction in digitized old newspapers | http://arxiv.org/abs/1210.0999 | author:Thomas Palfray, David Hébert, Stéphane Nicolas, Pierrick Tranouez, Thierry Paquet category:cs.IR cs.CV cs.DL published:2012-10-03 summary:Newspapers are documents made of news item and informative articles. They arenot meant to be red iteratively: the reader can pick his items in any order hefancies. Ignoring this structural property, most digitized newspaper archivesonly offer access by issue or at best by page to their content. We have built adigitization workflow that automatically extracts newspaper articles fromimages, which allows indexing and retrieval of information at the articlelevel. Our back-end system extracts the logical structure of the page toproduce the informative units: the articles. Each image is labelled at thepixel level, through a machine learning based method, then the page logicalstructure is constructed up from there by the detection of structuring entitiessuch as horizontal and vertical separators, titles and text lines. This logicalstructure is stored in a METS wrapper associated to the ALTO file produced bythe system including the OCRed text. Our front-end system provides a web highdefinition visualisation of images, textual indexing and retrieval facilities,searching and reading at the article level. Articles transcriptions can becollaboratively corrected, which as a consequence allows for better indexing.We are currently testing our system on the archives of the Journal de Rouen,one of France eldest local newspaper. These 250 years of publication amount to300 000 pages of very variable image quality and layout complexity. Test year1808 can be consulted at plair.univ-rouen.fr.
arxiv-1800-59 | Blurred Image Classification based on Adaptive Dictionary | http://arxiv.org/abs/1210.1029 | author:Guangling Sun, Guoqing Li, Jie Yin category:cs.CV published:2012-10-03 summary:Two types of framework for blurred image classification based on adaptivedictionary are proposed. Given a blurred image, instead of image deblurring,the semantic category of the image is determined by blur insensitive sparsecoefficients calculated depending on an adaptive dictionary. The dictionary isadaptive to the Point Spread Function (PSF) estimated from input blurred image.The PSF is assumed to be space invariant and inferred separately in oneframework or updated combining with sparse coefficients calculation in analternative and iterative algorithm in the other framework. The experiment hasevaluated three types of blur, naming defocus blur, simple motion blur andcamera shake blur. The experiment results confirm the effectiveness of theproposed frameworks.
arxiv-1800-60 | Robust Degraded Face Recognition Using Enhanced Local Frequency Descriptor and Multi-scale Competition | http://arxiv.org/abs/1210.1033 | author:Guangling Sun, Guoqing Li, Xinpeng Zhang category:cs.CV published:2012-10-03 summary:Recognizing degraded faces from low resolution and blurred images are commonyet challenging task. Local Frequency Descriptor (LFD) has been proved to beeffective for this task yet it is extracted from a spatial neighborhood of apixel of a frequency plane independently regardless of correlations betweenfrequencies. In addition, it uses a fixed window size named single scale ofshort-term Frequency transform (STFT). To explore the frequency correlationsand preserve low resolution and blur insensitive simultaneously, we proposeEnhanced LFD in which information in space and frequency is jointly utilized soas to be more descriptive and discriminative than LFD. The multi-scalecompetition strategy that extracts multiple descriptors corresponding tomultiple window sizes of STFT and take one corresponding to maximum confidenceas the final recognition result. The experiments conducted on Yale and FERETdatabases demonstrate that promising results have been achieved by the proposedEnhanced LFD and multi-scale competition strategy.
arxiv-1800-61 | Predicting human preferences using the block structure of complex social networks | http://arxiv.org/abs/1210.1048 | author:Roger Guimera, Alejandro Llorente, Esteban Moro, Marta Sales-Pardo category:physics.soc-ph cs.SI stat.ML published:2012-10-03 summary:With ever-increasing available data, predicting individuals' preferences andhelping them locate the most relevant information has become a pressing need.Understanding and predicting preferences is also important from a fundamentalpoint of view, as part of what has been called a "new" computational socialscience. Here, we propose a novel approach based on stochastic block models,which have been developed by sociologists as plausible models of complexnetworks of social interactions. Our model is in the spirit of predictingindividuals' preferences based on the preferences of others but, rather thanfitting a particular model, we rely on a Bayesian approach that samples overthe ensemble of all possible models. We show that our approach is considerablymore accurate than leading recommender algorithms, with major relativeimprovements between 38% and 99% over industry-level algorithms. Besides, ourapproach sheds light on decision-making processes by identifying groups ofindividuals that have consistently similar preferences, and enabling theanalysis of the characteristics of those groups.
arxiv-1800-62 | Sensory Anticipation of Optical Flow in Mobile Robotics | http://arxiv.org/abs/1210.1104 | author:Arturo Ribes, Jesús Cerquides, Yiannis Demiris, Ramón López de Mántaras category:cs.RO cs.LG published:2012-10-03 summary:In order to anticipate dangerous events, like a collision, an agent needs tomake long-term predictions. However, those are challenging due to uncertaintiesin internal and external variables and environment dynamics. A sensorimotormodel is acquired online by the mobile robot using a state-of-the-art methodthat learns the optical flow distribution in images, both in space and time.The learnt model is used to anticipate the optical flow up to a given timehorizon and to predict an imminent collision by using reinforcement learning.We demonstrate that multi-modal predictions reduce to simpler distributionsonce actions are taken into account.
arxiv-1800-63 | Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations | http://arxiv.org/abs/1210.1121 | author:Krishnakumar Balasubramanian, Kai Yu, Guy Lebanon category:stat.ML cs.LG published:2012-10-03 summary:We propose and analyze a novel framework for learning sparse representations,based on two statistical techniques: kernel smoothing and marginal regression.The proposed approach provides a flexible framework for incorporating featuresimilarity or temporal information present in data sets, via non-parametrickernel smoothing. We provide generalization bounds for dictionary learningusing smooth sparse coding and show how the sample complexity depends on the L1norm of kernel function used. Furthermore, we propose using marginal regressionfor obtaining sparse codes, which significantly improves the speed and allowsone to scale to large dictionary sizes easily. We demonstrate the advantages ofthe proposed approach, both in terms of accuracy and speed by extensiveexperimentation on several real data sets. In addition, we demonstrate how theproposed approach could be used for improving semi-supervised sparse coding.
arxiv-1800-64 | Unfolding Latent Tree Structures using 4th Order Tensors | http://arxiv.org/abs/1210.1258 | author:Mariya Ishteva, Haesun Park, Le Song category:cs.LG stat.ML published:2012-10-03 summary:Discovering the latent structure from many observed variables is an importantyet challenging learning task. Existing approaches for discovering latentstructures often require the unknown number of hidden states as an input. Inthis paper, we propose a quartet based approach which is \emph{agnostic} tothis number. The key contribution is a novel rank characterization of thetensor associated with the marginal distribution of a quartet. Thischaracterization allows us to design a \emph{nuclear norm} based test forresolving quartet relations. We then use the quartet test as a subroutine in adivide-and-conquer algorithm for recovering the latent tree structure. Undermild conditions, the algorithm is consistent and its error probability decaysexponentially with increasing sample size. We demonstrate that the proposedapproach compares favorably to alternatives. In a real world stock dataset, italso discovers meaningful groupings of variables, and produces a model thatfits the data better.
arxiv-1800-65 | Evaluating Discussion Boards on BlackBoard as a Collaborative Learning Tool A Students Survey and Reflections | http://arxiv.org/abs/1210.1230 | author:AbdelHameed A. Badawy, Michelle M. Hugue category:cs.CV cs.CY published:2012-10-03 summary:In this paper, we investigate how the students think of their experience in ajunior level course that has a blackboard course presence where the studentsuse the discussion boards extensively. A survey is set up through blackboard asa voluntary quiz and the student who participated were given a freebie point.The results and the participation were very interesting in terms of thefeedback we got via open comments from the students as well as the statisticswe gathered from the answers to the questions. The students have shownunderstanding and willingness to participate in pedagogy-enhancing endeavors.
arxiv-1800-66 | Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization | http://arxiv.org/abs/1210.1190 | author:Abhishek Kumar, Vikas Sindhwani, Prabhanjan Kambadur category:stat.ML cs.LG published:2012-10-03 summary:The separability assumption (Donoho & Stodden, 2003; Arora et al., 2012)turns non-negative matrix factorization (NMF) into a tractable problem.Recently, a new class of provably-correct NMF algorithms have emerged underthis assumption. In this paper, we reformulate the separable NMF problem asthat of finding the extreme rays of the conical hull of a finite set ofvectors. From this geometric perspective, we derive new separable NMFalgorithms that are highly scalable and empirically noise robust, and haveseveral other favorable properties in relation to existing methods. A parallelimplementation of our algorithm demonstrates high scalability on shared- anddistributed-memory machines.
arxiv-1800-67 | Graph-Based Approaches to Clustering Network-Constrained Trajectory Data | http://arxiv.org/abs/1210.0762 | author:Mohamed Khalil El Mahrsi, Fabrice Rossi category:cs.LG stat.ML published:2012-10-02 summary:Even though clustering trajectory data attracted considerable attention inthe last few years, most of prior work assumed that moving objects can movefreely in an euclidean space and did not consider the eventual presence of anunderlying road network and its influence on evaluating the similarity betweentrajectories. In this paper, we present two approaches to clusteringnetwork-constrained trajectory data. The first approach discovers clusters oftrajectories that traveled along the same parts of the road network. The secondapproach is segment-oriented and aims to group together road segments based ontrajectories that they have in common. Both approaches use a graph model todepict the interactions between observations w.r.t. their similarity andcluster this similarity graph using a community detection algorithm. We alsopresent experimental results obtained on synthetic data to showcase ourpropositions.
arxiv-1800-68 | Local stability and robustness of sparse dictionary learning in the presence of noise | http://arxiv.org/abs/1210.0685 | author:Rodolphe Jenatton, Rémi Gribonval, Francis Bach category:stat.ML cs.LG published:2012-10-02 summary:A popular approach within the signal processing and machine learningcommunities consists in modelling signals as sparse linear combinations ofatoms selected from a learned dictionary. While this paradigm has led tonumerous empirical successes in various fields ranging from image to audioprocessing, there have only been a few theoretical arguments supporting theseevidences. In particular, sparse coding, or sparse dictionary learning, relieson a non-convex procedure whose local minima have not been fully analyzed yet.In this paper, we consider a probabilistic model of sparse signals, and showthat, with high probability, sparse coding admits a local minimum around thereference dictionary generating the signals. Our study takes into account thecase of over-complete dictionaries and noisy signals, thus extending previouswork limited to noiseless settings and/or under-complete dictionaries. Theanalysis we conduct is non-asymptotic and makes it possible to understand howthe key quantities of the problem, such as the coherence or the level of noise,can scale with respect to the dimension of the signals, the number of atoms,the sparsity and the number of observations.
arxiv-1800-69 | Nonparametric Unsupervised Classification | http://arxiv.org/abs/1210.0645 | author:Yingzhen Yang, Thomas S. Huang category:cs.LG stat.ML published:2012-10-02 summary:Unsupervised classification methods learn a discriminative classifier fromunlabeled data, which has been proven to be an effective way of simultaneouslyclustering the data and training a classifier from the data. Variousunsupervised classification methods obtain appealing results by the classifierslearned in an unsupervised manner. However, existing methods do not considerthe misclassification error of the unsupervised classifiers except unsupervisedSVM, so the performance of the unsupervised classifiers is not fully evaluated.In this work, we study the misclassification error of two popular classifiers,i.e. the nearest neighbor classifier (NN) and the plug-in classifier, in thesetting of unsupervised classification.
arxiv-1800-70 | Schrödinger Diffusion for Shape Analysis with Texture | http://arxiv.org/abs/1210.0880 | author:Jose A. Iglesias, Ron Kimmel category:cs.CV cs.CG cs.GR math.AP 68U05, 35K08 published:2012-10-02 summary:In recent years, quantities derived from the heat equation have becomepopular in shape processing and analysis of triangulated surfaces. Suchmeasures are often robust with respect to different kinds of perturbations,including near-isometries, topological noise and partialities. Here, we proposeto exploit the semigroup of a Schr\"{o}dinger operator in order to deal withtexture data, while maintaining the desirable properties of the heat kernel. Wedefine a family of Schr\"{o}dinger diffusion distances analogous to the onesassociated to the heat kernels, and show that they are continuous underperturbations of the data. As an application, we introduce a method forretrieval of textured shapes through comparison of Schr\"{o}dinger diffusiondistance histograms with the earth's mover distance, and present some numericalexperiments showing superior performance compared to an analogous method thatignores the texture.
arxiv-1800-71 | Revisiting the Training of Logic Models of Protein Signaling Networks with a Formal Approach based on Answer Set Programming | http://arxiv.org/abs/1210.0690 | author:Santiago Videla, Carito Guziolowski, Federica Eduati, Sven Thiele, Niels Grabe, Julio Saez-Rodriguez, Anne Siegel category:q-bio.QM cs.AI cs.CE cs.LG published:2012-10-02 summary:A fundamental question in systems biology is the construction and training todata of mathematical models. Logic formalisms have become very popular to modelsignaling networks because their simplicity allows us to model large systemsencompassing hundreds of proteins. An approach to train (Boolean) logic modelsto high-throughput phospho-proteomics data was recently introduced and solvedusing optimization heuristics based on stochastic methods. Here we demonstratehow this problem can be solved using Answer Set Programming (ASP), adeclarative problem solving paradigm, in which a problem is encoded as alogical program such that its answer sets represent solutions to the problem.ASP has significant improvements over heuristic methods in terms of efficiencyand scalability, it guarantees global optimality of solutions as well asprovides a complete set of solutions. We illustrate the application of ASP within silico cases based on realistic networks and data.
arxiv-1800-72 | Classification of Hepatic Lesions using the Matching Metric | http://arxiv.org/abs/1210.0866 | author:Aaron Adcock, Daniel Rubin, Gunnar Carlsson category:cs.CV cs.CG math.AT published:2012-10-02 summary:In this paper we present a methodology of classifying hepatic (liver) lesionsusing multidimensional persistent homology, the matching metric (also calledthe bottleneck distance), and a support vector machine. We present ourclassification results on a dataset of 132 lesions that have been outlined andannotated by radiologists. We find that topological features are useful in theclassification of hepatic lesions. We also find that two-dimensional persistenthomology outperforms one-dimensional persistent homology in this application.
arxiv-1800-73 | A fast compression-based similarity measure with applications to content-based image retrieval | http://arxiv.org/abs/1210.0758 | author:Daniele Cerra, Mihai Datcu category:stat.ML cs.IR cs.LG published:2012-10-02 summary:Compression-based similarity measures are effectively employed inapplications on diverse data types with a basically parameter-free approach.Nevertheless, there are problems in applying these techniques tomedium-to-large datasets which have been seldom addressed. This paper proposesa similarity measure based on compression with dictionaries, the FastCompression Distance (FCD), which reduces the complexity of these methods,without degradations in performance. On its basis a content-based color imageretrieval system is defined, which can be compared to state-of-the-art methodsbased on invariant color features. Through the FCD a better understanding ofcompression-based techniques is achieved, by performing experiments on datasetswhich are larger than the ones analyzed so far in literature.
arxiv-1800-74 | Learning mixtures of structured distributions over discrete domains | http://arxiv.org/abs/1210.0864 | author:Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun category:cs.LG cs.DS math.ST stat.TH published:2012-10-02 summary:Let $\mathfrak{C}$ be a class of probability distributions over the discretedomain $[n] = \{1,...,n\}.$ We show that if $\mathfrak{C}$ satisfies a rathergeneral condition -- essentially, that each distribution in $\mathfrak{C}$ canbe well-approximated by a variable-width histogram with few bins -- then thereis a highly efficient (both in terms of running time and sample complexity)algorithm that can learn any mixture of $k$ unknown distributions from$\mathfrak{C}.$ We analyze several natural types of distributions over $[n]$, includinglog-concave, monotone hazard rate and unimodal distributions, and show thatthey have the required structural property of being well-approximated by ahistogram with few bins. Applying our general algorithm, we obtainnear-optimally efficient algorithms for all these mixture learning problems.
arxiv-1800-75 | Detecting multiword phrases in mathematical text corpora | http://arxiv.org/abs/1210.0852 | author:Winfried Gödert category:cs.CL cs.IR 68P20 H.3.1; I.7.3 published:2012-10-02 summary:We present an approach for detecting multiword phrases in mathematical textcorpora. The method used is based on characteristic features of mathematicalterminology. It makes use of a software tool named Lingo which allows toidentify words by means of previously defined dictionaries for specific wordclasses as adjectives, personal names or nouns. The detection of multiwordgroups is done algorithmically. Possible advantages of the method for indexingand information retrieval and conclusions for applying dictionary-based methodsof automatic indexing instead of stemming procedures are discussed.
arxiv-1800-76 | Enhancing Twitter Data Analysis with Simple Semantic Filtering: Example in Tracking Influenza-Like Illnesses | http://arxiv.org/abs/1210.0848 | author:Son Doan, Lucila Ohno-Machado, Nigel Collier category:cs.SI cs.CL physics.soc-ph published:2012-10-02 summary:Systems that exploit publicly available user generated content such asTwitter messages have been successful in tracking seasonal influenza. Wedeveloped a novel filtering method for Influenza-Like-Illnesses (ILI)-relatedmessages using 587 million messages from Twitter micro-blogs. We first filteredmessages based on syndrome keywords from the BioCaster Ontology, an extantknowledge model of laymen's terms. We then filtered the messages according tosemantic features such as negation, hashtags, emoticons, humor and geography.The data covered 36 weeks for the US 2009 influenza season from 30th August2009 to 8th May 2010. Results showed that our system achieved the highestPearson correlation coefficient of 98.46% (p-value<2.2e-16), an improvement of3.98% over the previous state-of-the-art method. The results indicate thatsimple NLP-based enhancements to existing approaches to mine Twitter data canincrease the value of this inexpensive resource.
arxiv-1800-77 | A Survey of Multibiometric Systems | http://arxiv.org/abs/1210.0829 | author:Harbi AlMahafzah, Maen Zaid AlRwashdeh category:cs.CV published:2012-10-02 summary:Most biometric systems deployed in real-world applications are unimodal.Using unimodal biometric systems have to contend with a variety of problemssuch as: Noise in sensed data; Intra-class variations; Inter-classsimilarities; Non-universality; Spoof attacks. These problems have addressed byusing multibiometric systems, which expected to be more reliable due to thepresence of multiple, independent pieces of evidence.
arxiv-1800-78 | Distributed High Dimensional Information Theoretical Image Registration via Random Projections | http://arxiv.org/abs/1210.0824 | author:Zoltan Szabo, Andras Lorincz category:cs.IT cs.LG math.IT stat.ML published:2012-10-02 summary:Information theoretical measures, such as entropy, mutual information, andvarious divergences, exhibit robust characteristics in image registrationapplications. However, the estimation of these quantities is computationallyintensive in high dimensions. On the other hand, consistent estimation frompairwise distances of the sample points is possible, which suits randomprojection (RP) based low dimensional embeddings. We adapt the RP technique tothis task by means of a simple ensemble method. To the best of our knowledge,this is the first distributed, RP based information theoretical imageregistration approach. The efficiency of the method is demonstrated throughnumerical examples.
arxiv-1800-79 | Discrete geodesic calculus in the space of viscous fluidic objects | http://arxiv.org/abs/1210.0822 | author:Martin Rumpf, Benedikt Wirth category:math.NA cs.CV published:2012-10-02 summary:Based on a local approximation of the Riemannian distance on a manifold by acomputationally cheap dissimilarity measure, a time discrete geodesic calculusis developed, and applications to shape space are explored. The dissimilaritymeasure is derived from a deformation energy whose Hessian reproduces theunderlying Riemannian metric, and it is used to define length and energy ofdiscrete paths in shape space. The notion of discrete geodesics defined asenergy minimizing paths gives rise to a discrete logarithmic map, a variationaldefinition of a discrete exponential map, and a time discrete paralleltransport. This new concept is applied to a shape space in which shapes areconsidered as boundary contours of physical objects consisting of viscousmaterial. The flexibility and computational efficiency of the approach isdemonstrated for topology preserving shape morphing, the representation ofpaths in shape space via local shape variations as path generators, shapeextrapolation via discrete geodesic flow, and the transfer of geometricfeatures.
arxiv-1800-80 | Robust PCA and subspace tracking from incomplete observations using L0-surrogates | http://arxiv.org/abs/1210.0805 | author:Clemens Hage, Martin Kleinsteuber category:stat.ML published:2012-10-02 summary:Many applications in data analysis rely on the decomposition of a data matrixinto a low-rank and a sparse component. Existing methods that tackle this taskuse the nuclear norm and L1-cost functions as convex relaxations of the rankconstraint and the sparsity measure, respectively, or employ thresholdingtechniques. We propose a method that allows for reconstructing and tracking asubspace of upper-bounded dimension from incomplete and corrupted observations.It does not require any a priori information about the number of outliers. Thecore of our algorithm is an intrinsic Conjugate Gradient method on the set oforthogonal projection matrices, the so-called Grassmannian. Non-convex sparsitymeasures are used for outlier detection, which leads to improved performance interms of robustly recovering and tracking the low-rank matrix. In particular,our approach can cope with more outliers and with an underlying matrix ofhigher rank than other state-of-the-art methods.
arxiv-1800-81 | Multibiometric: Feature Level Fusion Using FKP Multi-Instance biometric | http://arxiv.org/abs/1210.0818 | author:Harbi AlMahafzah, Mohammad Imran, H. S. Sheshadri category:cs.CV published:2012-10-02 summary:This paper proposed the use of multi-instance feature level fusion as a meansto improve the performance of Finger Knuckle Print (FKP) verification. Alog-Gabor filter has been used to extract the image local orientationinformation, and represent the FKP features. Experiments are performed usingthe FKP database, which consists of 7,920 images. Results indicate that themulti-instance verification approach outperforms higher performance than usingany single instance. The influence on biometric performance using feature levelfusion under different fusion rules have been demonstrated in this paper.
arxiv-1800-82 | A Semantic Approach for Automatic Structuring and Analysis of Software Process Patterns | http://arxiv.org/abs/1210.0794 | author:Nahla Jlaiel, Khouloud Madhbouh, Mohamed Ben Ahmed category:cs.AI cs.CL published:2012-10-02 summary:The main contribution of this paper, is to propose a novel semantic approachbased on a Natural Language Processing technique in order to ensure a semanticunification of unstructured process patterns which are expressed not only indifferent formats but also, in different forms. This approach is implementedusing the GATE text engineering framework and then evaluated leading up tohigh-quality results motivating us to continue in this direction.
arxiv-1800-83 | TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data Classification | http://arxiv.org/abs/1210.0699 | author:Xavier Bresson, Ruiliang Zhang category:cs.LG published:2012-10-02 summary:We introduce semi-supervised data classification algorithms based on totalvariation (TV), Reproducing Kernel Hilbert Space (RKHS), support vector machine(SVM), Cheeger cut, labeled and unlabeled data points. We design binary andmulti-class semi-supervised classification algorithms. We compare the TV-basedclassification algorithms with the related Laplacian-based algorithms, and showthat TV classification perform significantly better when the number of labeleddata is small.
arxiv-1800-84 | Evaluation of linear classifiers on articles containing pharmacokinetic evidence of drug-drug interactions | http://arxiv.org/abs/1210.0734 | author:Artemy Kolchinsky, Anália Lourenço, Lang Li, Luis M. Rocha category:stat.ML cs.LG q-bio.QM published:2012-10-02 summary:Background. Drug-drug interaction (DDI) is a major cause of morbidity andmortality. [...] Biomedical literature mining can aid DDI research byextracting relevant DDI signals from either the published literature or largeclinical databases. However, though drug interaction is an ideal area fortranslational research, the inclusion of literature mining methodologies in DDIworkflows is still very preliminary. One area that can benefit from literaturemining is the automatic identification of a large number of potential DDIs,whose pharmacological mechanisms and clinical significance can then be studiedvia in vitro pharmacology and in populo pharmaco-epidemiology. Experiments. Weimplemented a set of classifiers for identifying published articles relevant toexperimental pharmacokinetic DDI evidence. These documents are important foridentifying causal mechanisms behind putative drug-drug interactions, animportant step in the extraction of large numbers of potential DDIs. Weevaluate performance of several linear classifiers on PubMed abstracts, underdifferent feature transformation and dimensionality reduction methods. Inaddition, we investigate the performance benefits of including variouspublicly-available named entity recognition features, as well as a set ofinternally-developed pharmacokinetic dictionaries. Results. We found thatseveral classifiers performed well in distinguishing relevant and irrelevantabstracts. We found that the combination of unigram and bigram textual featuresgave better performance than unigram features alone, and also thatnormalization transforms that adjusted for feature frequency and documentlength improved classification. For some classifiers, such as lineardiscriminant analysis (LDA), proper dimensionality reduction had a large impacton performance. Finally, the inclusion of NER features and dictionaries wasfound not to help classification.
arxiv-1800-85 | Invariance of visual operations at the level of receptive fields | http://arxiv.org/abs/1210.0754 | author:Tony Lindeberg category:q-bio.NC cs.CV published:2012-10-02 summary:Receptive field profiles registered by cell recordings have shown thatmammalian vision has developed receptive fields tuned to different sizes andorientations in the image domain as well as to different image velocities inspace-time. This article presents a theoretical model by which families ofidealized receptive field profiles can be derived mathematically from a smallset of basic assumptions that correspond to structural properties of theenvironment. The article also presents a theory for how basic invarianceproperties to variations in scale, viewing direction and relative motion can beobtained from the output of such receptive fields, using complementaryselection mechanisms that operate over the output of families of receptivefields tuned to different parameters. Thereby, the theory shows how basicinvariance properties of a visual system can be obtained already at the levelof receptive fields, and we can explain the different shapes of receptive fieldprofiles found in biological vision from a requirement that the visual systemshould be invariant to the natural types of image transformations that occur inits environment.
arxiv-1800-86 | Think Locally, Act Globally: Perfectly Balanced Graph Partitioning | http://arxiv.org/abs/1210.0477 | author:Peter Sanders, Christian Schulz category:cs.DS cs.DC cs.NE published:2012-10-01 summary:We present a novel local improvement scheme for the perfectly balanced graphpartitioning problem. This scheme encodes local searches that are notrestricted to a balance constraint into a model allowing us to findcombinations of these searches maintaining balance by applying a negative cycledetection algorithm. We combine this technique with an algorithm to balanceunbalanced solutions and integrate it into a parallel multi-level evolutionaryalgorithm, KaFFPaE, to tackle the problem. Overall, we obtain a system that isfast on the one hand and on the other hand is able to improve or reproduce mostof the best known perfectly balanced partitioning results ever reported in theliterature.
arxiv-1800-87 | Inference algorithms for pattern-based CRFs on sequence data | http://arxiv.org/abs/1210.0508 | author:Rustem Takhanov, Vladimir Kolmogorov category:cs.LG cs.DS published:2012-10-01 summary:We consider Conditional Random Fields (CRFs) with pattern-based potentialsdefined on a chain. In this model the energy of a string (labeling) $x_1...x_n$is the sum of terms over intervals $[i,j]$ where each term is non-zero only ifthe substring $x_i...x_j$ equals a prespecified pattern $\alpha$. Such CRFs canbe naturally applied to many sequence tagging problems. We present efficient algorithms for the three standard inference tasks in aCRF, namely computing (i) the partition function, (ii) marginals, and (iii)computing the MAP. Their complexities are respectively $O(n L)$, $O(n L\ell_{max})$ and $O(n L \min\{D,\log (\ell_{max}+1)\})$ where $L$ is thecombined length of input patterns, $\ell_{max}$ is the maximum length of apattern, and $D$ is the input alphabet. This improves on the previousalgorithms of (Ye et al., 2009) whose complexities are respectively $O(n LD)$, $O(n \Gamma L^2 \ell_{max}^2)$ and $O(n L D)$, where $\Gamma$ isthe number of input patterns. In addition, we give an efficient algorithm for sampling. Finally, weconsider the case of non-positive weights. (Komodakis & Paragios, 2009) gave an$O(n L)$ algorithm for computing the MAP. We present a modification that hasthe same worst-case complexity but can beat it in the best case.
arxiv-1800-88 | Super-resolution using Sparse Representations over Learned Dictionaries: Reconstruction of Brain Structure using Electron Microscopy | http://arxiv.org/abs/1210.0564 | author:Tao Hu, Juan Nunez-Iglesias, Shiv Vitaladevuni, Lou Scheffer, Shan Xu, Mehdi Bolorizadeh, Harald Hess, Richard Fetter, Dmitri Chklovskii category:cs.CV q-bio.NC stat.ML published:2012-10-01 summary:A central problem in neuroscience is reconstructing neuronal circuits on thesynapse level. Due to a wide range of scales in brain architecture suchreconstruction requires imaging that is both high-resolution andhigh-throughput. Existing electron microscopy (EM) techniques possess requiredresolution in the lateral plane and either high-throughput or high depthresolution but not both. Here, we exploit recent advances in unsupervisedlearning and signal processing to obtain high depth-resolution EM imagescomputationally without sacrificing throughput. First, we show that the braintissue can be represented as a sparse linear combination of localized basisfunctions that are learned using high-resolution datasets. We then developcompressive sensing-inspired techniques that can reconstruct the brain tissuefrom very few (typically 5) tomographic views of each section. This enablestracing of neuronal processes and, hence, high throughput reconstruction ofneural circuits on the level of individual synapses.
arxiv-1800-89 | Sparse LMS via Online Linearized Bregman Iteration | http://arxiv.org/abs/1210.0563 | author:Tao Hu, Dmitri B. Chklovskii category:cs.IT cs.LG math.IT stat.ML published:2012-10-01 summary:We propose a version of least-mean-square (LMS) algorithm for sparse systemidentification. Our algorithm called online linearized Bregman iteration (OLBI)is derived from minimizing the cumulative prediction error squared along withan l1-l2 norm regularizer. By systematically treating the non-differentiableregularizer we arrive at a simple two-step iteration. We demonstrate that OLBIis bias free and compare its operation with existing sparse LMS algorithms byrederiving them in the online convex optimization framework. We performconvergence analysis of OLBI for white input signals and derive theoreticalexpressions for both the steady state and instantaneous mean square deviations(MSD). We demonstrate numerically that OLBI improves the performance of LMStype algorithms for signals generated from sparse tap weights.
arxiv-1800-90 | Memory Constraint Online Multitask Classification | http://arxiv.org/abs/1210.0473 | author:Giovanni Cavallanti, Nicolò Cesa-Bianchi category:cs.LG published:2012-10-01 summary:We investigate online kernel algorithms which simultaneously process multipleclassification tasks while a fixed constraint is imposed on the size of theiractive sets. We focus in particular on the design of algorithms that canefficiently deal with problems where the number of tasks is extremely high andthe task data are large scale. Two new projection-based algorithms areintroduced to efficiently tackle those issues while presenting different tradeoffs on how the available memory is managed with respect to the priorinformation about the learning tasks. Theoretically sound budget algorithms aredevised by coupling the Randomized Budget Perceptron and the Forgetronalgorithms with the multitask kernel. We show how the two seemingly contrastingproperties of learning from multiple tasks and keeping a constant memoryfootprint can be balanced, and how the sharing of the available space amongdifferent tasks is automatically taken care of. We propose and discuss newinsights on the multitask kernel. Experiments show that online kernel multitaskalgorithms running on a budget can efficiently tackle real world learningproblems involving multiple tasks.
arxiv-1800-91 | Intra-Retinal Layer Segmentation of 3D Optical Coherence Tomography Using Coarse Grained Diffusion Map | http://arxiv.org/abs/1210.0310 | author:Raheleh Kafieh, Hossein Rabbani, Michael D. Abramoff, Milan Sonka category:cs.CV published:2012-10-01 summary:Optical coherence tomography (OCT) is a powerful and noninvasive method forretinal imaging. In this paper, we introduce a fast segmentation method basedon a new variant of spectral graph theory named diffusion maps. The research isperformed on spectral domain (SD) OCT images depicting macular and optic nervehead appearance. The presented approach does not require edge-based imageinformation and relies on regional image texture. Consequently, the proposedmethod demonstrates robustness in situations of low image contrast or poorlayer-to-layer image gradients. Diffusion mapping is applied to 2D and 3D OCTdatasets composed of two steps, one for partitioning the data into importantand less important sections, and another one for localization of internallayers.In the first step, the pixels/voxels are grouped in rectangular/cubicsets to form a graph node.The weights of a graph are calculated based ongeometric distances between pixels/voxels and differences of their meanintensity.The first diffusion map clusters the data into three parts, thesecond of which is the area of interest. The other two sections are eliminatedfrom the remaining calculations. In the second step, the remaining area issubjected to another diffusion map assessment and the internal layers arelocalized based on their textural similarities.The proposed method was testedon 23 datasets from two patient groups (glaucoma and normals). The meanunsigned border positioning errors(mean - SD) was 8.52 - 3.13 and 7.56 - 2.95micrometer for the 2D and 3D methods, respectively.
arxiv-1800-92 | Combined Descriptors in Spatial Pyramid Domain for Image Classification | http://arxiv.org/abs/1210.0386 | author:Junlin Hu, Ping Guo category:cs.CV I.4.9; I.5.4 published:2012-10-01 summary:Recently spatial pyramid matching (SPM) with scale invariant featuretransform (SIFT) descriptor has been successfully used in image classification.Unfortunately, the codebook generation and feature quantization proceduresusing SIFT feature have the high complexity both in time and space. To addressthis problem, in this paper, we propose an approach which combines local binarypatterns (LBP) and three-patch local binary patterns (TPLBP) in spatial pyramiddomain. The proposed method does not need to learn the codebook and featurequantization processing, hence it becomes very efficient. Experiments on twopopular benchmark datasets demonstrate that the proposed method alwayssignificantly outperforms the very popular SPM based SIFT descriptor methodboth in time and classification accuracy.
arxiv-1800-93 | Enhanced Techniques for PDF Image Segmentation and Text Extraction | http://arxiv.org/abs/1210.0347 | author:D. Sasirekha, E. Chandra category:cs.CV published:2012-10-01 summary:Extracting text objects from the PDF images is a challenging problem. Thetext data present in the PDF images contain certain useful information forautomatic annotation, indexing etc. However variations of the text due todifferences in text style, font, size, orientation, alignment as well ascomplex structure make the problem of automatic text extraction extremelydifficult and challenging job. This paper presents two techniques underblock-based classification. After a brief introduction of the classificationmethods, two methods were enhanced and results were evaluated. The performancemetrics for segmentation and time consumption are tested for both the models.
arxiv-1800-94 | On The Convergence of a Nash Seeking Algorithm with Stochastic State Dependent Payoff | http://arxiv.org/abs/1210.0193 | author:A. F. Hanif, H. Tembine, M. Assaad, D. Zeghlache category:math.OC math.DS math.NA stat.ML published:2012-09-30 summary:Distributed strategic learning has been getting attention in recent years. Assystems become distributed finding Nash equilibria in a distributed fashion isbecoming more important for various applications. In this paper, we develop adistributed strategic learning framework for seeking Nash equilibria understochastic state-dependent payoff functions. We extend the work of Krsticet.al. in [1] to the case of stochastic state dependent payoff functions. Wedevelop an iterative distributed algorithm for Nash seeking and examine itsconvergence to a limiting trajectory defined by an Ordinary DifferentialEquation (ODE). We show convergence of our proposed algorithm for vanishingstep size and provide an error bound for fixed step size. Finally, we conduct astability analysis and apply the proposed scheme in a generic wirelessnetworks. We also present numerical results which corroborate our claim.
arxiv-1800-95 | A Linguistic Model for Terminology Extraction based Conditional Random Fields | http://arxiv.org/abs/1210.0252 | author:Fethi Fkih, Mohamed Nazih Omri, Imen Toumia category:cs.CL cs.AI I.2.6; I.2.7 published:2012-09-30 summary:In this paper, we show the possibility of using a linear Conditional RandomFields (CRF) for terminology extraction from a specialized text corpus.
arxiv-1800-96 | Iterative Reweighted Minimization Methods for $l_p$ Regularized Unconstrained Nonlinear Programming | http://arxiv.org/abs/1210.0066 | author:Zhaosong Lu category:math.OC cs.LG stat.CO stat.ML published:2012-09-29 summary:In this paper we study general $l_p$ regularized unconstrained minimizationproblems. In particular, we derive lower bounds for nonzero entries of first-and second-order stationary points, and hence also of local minimizers of the$l_p$ minimization problems. We extend some existing iterative reweighted $l_1$(IRL1) and $l_2$ (IRL2) minimization methods to solve these problems andproposed new variants for them in which each subproblem has a closed formsolution. Also, we provide a unified convergence analysis for these methods. Inaddition, we propose a novel Lipschitz continuous $\epsilon$-approximation to$\x\^p_p$. Using this result, we develop new IRL1 methods for the $l_p$minimization problems and showed that any accumulation point of the sequencegenerated by these methods is a first-order stationary point, provided that theapproximation parameter $\epsilon$ is below a computable threshold value. Thisis a remarkable result since all existing iterative reweighted minimizationmethods require that $\epsilon$ be dynamically updated and approach zero. Ourcomputational results demonstrate that the new IRL1 method is generally morestable than the existing IRL1 methods [21,18] in terms of objective functionvalue and CPU time.
arxiv-1800-97 | Demosaicing and Superresolution for Color Filter Array via Residual Image Reconstruction and Sparse Representation | http://arxiv.org/abs/1210.0115 | author:Guangling Sun category:cs.CV published:2012-09-29 summary:A framework of demosaicing and superresolution for color filter array (CFA)via residual image reconstruction and sparse representation is presented.Giventhe intermediate image produced by certain demosaicing and interpolationtechnique, a residual image between the final reconstruction image and theintermediate image is reconstructed using sparse representation.The finalreconstruction image has richer edges and details than that of the intermediateimage. Specifically, a generic dictionary is learned from a large set ofcomposite training data composed of intermediate data and residual data. Thelearned dictionary implies a mapping between the two data. A specificdictionary adaptive to the input CFA is learned thereafter. Using the adaptivedictionary, the sparse coefficients of intermediate data are computed andtransformed to predict residual image. The residual image is added back intothe intermediate image to obtain the final reconstruction image. Experimentalresults demonstrate the state-of-the-art performance in terms of PSNR andsubjective visual perception.
arxiv-1800-98 | Self-Delimiting Neural Networks | http://arxiv.org/abs/1210.0118 | author:Juergen Schmidhuber category:cs.NE published:2012-09-29 summary:Self-delimiting (SLIM) programs are a central concept of theoretical computerscience, particularly algorithmic information & probability theory, andasymptotically optimal program search (AOPS). To apply AOPS to (possiblyrecurrent) neural networks (NNs), I introduce SLIM NNs. Neurons of a typicalSLIM NN have threshold activation functions. During a computational episode,activations are spreading from input neurons through the SLIM NN until thecomputation activates a special halt neuron. Weights of the NN's usedconnections define its program. Halting programs form a prefix code. The resetof the initial NN state does not cost more than the latest program execution.Since prefixes of SLIM programs influence their suffixes (weight changesoccurring early in an episode influence which weights are considered later),SLIM NN learning algorithms (LAs) should execute weight changes online duringactivation spreading. This can be achieved by applying AOPS to growing SLIMNNs. To efficiently teach a SLIM NN to solve many tasks, such as correctlyclassifying many different patterns, or solving many different robot controltasks, each connection keeps a list of tasks it is used for. The lists may beefficiently updated during training. To evaluate the overall effect ofcurrently tested weight changes, a SLIM NN LA needs to re-test performance onlyon the efficiently computable union of tasks potentially affected by thecurrent weight changes. Future SLIM NNs will be implemented on 3-dimensionalbrain-like multi-processor hardware. Their LAs will minimize task-specifictotal wire length of used connections, to encourage efficient solutions ofsubtasks by subsets of neurons that are physically close. The novel class ofSLIM NN LAs is currently being probed in ongoing experiments to be reported inseparate papers.
arxiv-1800-99 | A Low Cost Vision Based Hybrid Fiducial Mark Tracking Technique for Mobile Industrial Robots | http://arxiv.org/abs/1210.0153 | author:Mohammed Y Aalsalem, Wazir Zada Khan, Quratul Ain Arshad category:cs.CV cs.RO published:2012-09-29 summary:The field of robotic vision is developing rapidly. Robots can reactintelligently and provide assistance to user activities through sentientcomputing. Since industrial applications pose complex requirements that cannotbe handled by humans, an efficient low cost and robust technique is requiredfor the tracking of mobile industrial robots. The existing sensor basedtechniques for mobile robot tracking are expensive and complex to deploy,configure and maintain. Also some of them demand dedicated and often expensivehardware. This paper presents a low cost vision based technique called HybridFiducial Mark Tracking (HFMT) technique for tracking mobile industrial robot.HFMT technique requires off-the-shelf hardware (CCD cameras) and printable 2-Dcircular marks used as fiducials for tracking a mobile industrial robot on apre-defined path. This proposed technique allows the robot to track on apredefined path by using fiducials for the detection of Right and Left turns onthe path and White Strip for tracking the path. The HFMT technique isimplemented and tested on an indoor mobile robot at our laboratory.Experimental results from robot navigating in real environments have confirmedthat our approach is simple and robust and can be adopted in any hostileindustrial environment where humans are unable to work.
arxiv-1800-100 | Optimistic Agents are Asymptotically Optimal | http://arxiv.org/abs/1210.0077 | author:Peter Sunehag, Marcus Hutter category:cs.AI cs.LG published:2012-09-29 summary:We use optimism to introduce generic asymptotically optimal reinforcementlearning agents. They achieve, with an arbitrary finite or compact class ofenvironments, asymptotically optimal behavior. Furthermore, in the finitedeterministic case we provide finite error bounds.
arxiv-1800-101 | Gene selection with guided regularized random forest | http://arxiv.org/abs/1209.6425 | author:Houtao Deng, George Runger category:cs.LG cs.CE published:2012-09-28 summary:The regularized random forest (RRF) was recently proposed for featureselection by building only one ensemble. In RRF the features are evaluated on apart of the training data at each tree node. We derive an upper bound for thenumber of distinct Gini information gain values in a node, and show that manyfeatures can share the same information gain at a node with a small number ofinstances and a large number of features. Therefore, in a node with a smallnumber of instances, RRF is likely to select a feature not strongly relevant.Here an enhanced RRF, referred to as the guided RRF (GRRF), is proposed. InGRRF, the importance scores from an ordinary random forest (RF) are used toguide the feature selection process in RRF. Experiments on 10 gene data setsshow that the accuracy performance of GRRF is, in general, more robust than RRFwhen their parameters change. GRRF is computationally efficient, can selectcompact feature subsets, and has competitive accuracy performance, compared toRRF, varSelRF and LASSO logistic regression (with evaluations from an RFclassifier). Also, RF applied to the features selected by RRF with the minimalregularization outperforms RF applied to all the features for most of the datasets considered here. Therefore, if accuracy is considered more important thanthe size of the feature subset, RRF with the minimal regularization may beconsidered. We use the accuracy performance of RF, a strong classifier, toevaluate feature selection methods, and illustrate that weak classifiers areless capable of capturing the information contained in a feature subset. BothRRF and GRRF were implemented in the "RRF" R package available at CRAN, theofficial R package archive.
arxiv-1800-102 | Dimensionality Reduction and Classification feature using Mutual Information applied to Hyperspectral Images : A Filter strategy based algorithm | http://arxiv.org/abs/1210.0052 | author:ELkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV 68U10, 68R05 published:2012-09-28 summary:Hyperspectral images (HIS) classification is a high technical remote sensingtool. The goal is to reproduce a thematic map that will be compared with areference ground truth map (GT), constructed by expecting the region. The HIScontains more than a hundred bidirectional measures, called bands (or simplyimages), of the same region. They are taken at juxtaposed frequencies.Unfortunately, some bands contain redundant information, others are affected bythe noise, and the high dimensionality of features made the accuracy ofclassification lower. The problematic is how to find the good bands to classifythe pixels of regions. Some methods use Mutual Information (MI) and threshold,to select relevant bands, without treatment of redundancy. Others control andeliminate redundancy by selecting the band top ranking the MI, and if itsneighbors have sensibly the same MI with the GT, they will be consideredredundant and so discarded. This is the most inconvenient of this method,because this avoids the advantage of hyperspectral images: some preciousinformation can be discarded. In this paper we'll accept the useful redundancy.A band contains useful redundancy if it contributes to produce an estimatedreference map that has higher MI with the GT.nTo control redundancy, weintroduce a complementary threshold added to last value of MI. This process isa Filter strategy; it gets a better performance of classification accuracy andnot expensive, but less preferment than Wrapper strategy.
arxiv-1800-103 | A Deterministic Analysis of an Online Convex Mixture of Expert Algorithms | http://arxiv.org/abs/1209.6409 | author:Mehmet A. Donmez, Sait Tunc, Suleyman S. Kozat category:cs.LG published:2012-09-28 summary:We analyze an online learning algorithm that adaptively combines outputs oftwo constituent algorithms (or the experts) running in parallel to model anunknown desired signal. This online learning algorithm is shown to achieve (andin some cases outperform) the mean-square error (MSE) performance of the bestconstituent algorithm in the mixture in the steady-state. However, the MSEanalysis of this algorithm in the literature uses approximations and relies onstatistical models on the underlying signals and systems. Hence, such ananalysis may not be useful or valid for signals generated by various real lifesystems that show high degrees of nonstationarity, limit cycles and, in manycases, that are even chaotic. In this paper, we produce results in anindividual sequence manner. In particular, we relate the time-accumulatedsquared estimation error of this online algorithm at any time over any intervalto the time accumulated squared estimation error of the optimal convex mixtureof the constituent algorithms directly tuned to the underlying signal in adeterministic sense without any statistical assumptions. In this sense, ouranalysis provides the transient, steady-state and tracking behavior of thisalgorithm in a strong sense without any approximations in the derivations orstatistical assumptions on the underlying signals such that our results areguaranteed to hold. We illustrate the introduced results through examples.
arxiv-1800-104 | Scoring and Searching over Bayesian Networks with Causal and Associative Priors | http://arxiv.org/abs/1209.6561 | author:Giorgos Borboudakis, Ioannis Tsamardinos category:cs.AI cs.LG stat.ML published:2012-09-28 summary:A significant theoretical advantage of search-and-score methods for learningBayesian Networks is that they can accept informative prior beliefs for eachpossible network, thus complementing the data. In this paper, a method ispresented for assigning priors based on beliefs on the presence or absence ofcertain paths in the true network. Such beliefs correspond to knowledge aboutthe possible causal and associative relations between pairs of variables. Thistype of knowledge naturally arises from prior experimental and observationaldata, among others. In addition, a novel search-operator is proposed to takeadvantage of such prior knowledge. Experiments show that, using path beliefsimproves the learning of the skeleton, as well as the edge directions in thenetwork.
arxiv-1800-105 | Coupled quasi-harmonic bases | http://arxiv.org/abs/1210.0026 | author:A. Kovnatsky, M. M. Bronstein, A. M. Bronstein, K. Glashoff, R. Kimmel category:cs.CV cs.GR published:2012-09-28 summary:The use of Laplacian eigenbases has been shown to be fruitful in manycomputer graphics applications. Today, state-of-the-art approaches to shapeanalysis, synthesis, and correspondence rely on these natural harmonic basesthat allow using classical tools from harmonic analysis on manifolds. However,many applications involving multiple shapes are obstacled by the fact thatLaplacian eigenbases computed independently on different shapes are oftenincompatible with each other. In this paper, we propose the construction ofcommon approximate eigenbases for multiple shapes using approximate jointdiagonalization algorithms. We illustrate the benefits of the proposed approachon tasks from shape editing, pose transfer, correspondence, and similarity.
arxiv-1800-106 | Sparse Modeling of Intrinsic Correspondences | http://arxiv.org/abs/1209.6560 | author:J. Pokrass, A. M. Bronstein, M. M. Bronstein, P. Sprechmann, G. Sapiro category:cs.GR cs.CG cs.CV published:2012-09-28 summary:We present a novel sparse modeling approach to non-rigid shape matching usingonly the ability to detect repeatable regions. As the input to our algorithm,we are given only two sets of regions in two shapes; no descriptors areprovided so the correspondence between the regions is not know, nor we know howmany regions correspond in the two shapes. We show that even with such scarceinformation, it is possible to establish very accurate correspondence betweenthe shapes by using methods from the field of sparse modeling, being this, thefirst non-trivial use of sparse models in shape correspondence. We formulatethe problem of permuted sparse coding, in which we solve simultaneously for anunknown permutation ordering the regions on two shapes and for an unknowncorrespondence in functional representation. We also propose a robust variantcapable of handling incomplete matches. Numerically, the problem is solvedefficiently by alternating the solution of a linear assignment and a sparsecoding problem. The proposed methods are evaluated qualitatively andquantitatively on standard benchmarks containing both synthetic and scannedobjects.
arxiv-1800-107 | Band Selection and Classification of Hyperspectral Images using Mutual Information: An algorithm based on minimizing the error probability using the inequality of Fano | http://arxiv.org/abs/1210.0528 | author:Elkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV 68U10, 68R05 published:2012-09-28 summary:Hyperspectral image is a substitution of more than a hundred images, calledbands, of the same region. They are taken at juxtaposed frequencies. Thereference image of the region is called Ground Truth map (GT). the problematicis how to find the good bands to classify the pixels of regions; because thebands can be not only redundant, but a source of confusion, and decreasing sothe accuracy of classification. Some methods use Mutual Information (MI) andthreshold, to select relevant bands. Recently there's an algorithm selectionbased on mutual information, using bandwidth rejection and a threshold tocontrol and eliminate redundancy. The band top ranking the MI is selected, andif its neighbors have sensibly the same MI with the GT, they will be consideredredundant and so discarded. This is the most inconvenient of this method,because this avoids the advantage of hyperspectral images: some preciousinformation can be discarded. In this paper we'll make difference betweenuseful and useless redundancy. A band contains useful redundancy if itcontributes to decreasing error probability. According to this scheme, weintroduce new algorithm using also mutual information, but it retains only thebands minimizing the error probability of classification. To controlredundancy, we introduce a complementary threshold. So the good band candidatemust contribute to decrease the last error probability augmented by thethreshold. This process is a wrapper strategy; it gets high performance ofclassification accuracy but it is expensive than filter strategy.
arxiv-1800-108 | A Complete System for Candidate Polyps Detection in Virtual Colonoscopy | http://arxiv.org/abs/1209.6525 | author:Marcelo Fiori, Pablo Musé, Guillermo Sapiro category:cs.CV cs.LG published:2012-09-28 summary:Computer tomographic colonography, combined with computer-aided detection, isa promising emerging technique for colonic polyp analysis. We present acomplete pipeline for polyp detection, starting with a simple colonsegmentation technique that enhances polyps, followed by an adaptive-scalecandidate polyp delineation and classification based on new texture andgeometric features that consider both the information in the candidate polyplocation and its immediate surrounding area. The proposed system is tested withground truth data, including flat and small polyps which are hard to detecteven with optical colonoscopy. For polyps larger than 6mm in size we achieve100% sensitivity with just 0.9 false positives per case, and for polyps largerthan 3mm in size we achieve 93% sensitivity with 2.8 false positives per case.
arxiv-1800-109 | Partial Gaussian Graphical Model Estimation | http://arxiv.org/abs/1209.6419 | author:Xiao-Tong Yuan, Tong Zhang category:cs.LG cs.IT math.IT stat.ML published:2012-09-28 summary:This paper studies the partial estimation of Gaussian graphical models fromhigh-dimensional empirical observations. We derive a convex formulation forthis problem using $\ell_1$-regularized maximum-likelihood estimation, whichcan be solved via a block coordinate descent algorithm. Statistical estimationperformance can be established for our method. The proposed approach hascompetitive empirical performance compared to existing methods, as demonstratedby various experiments on synthetic and real datasets.
arxiv-1800-110 | Review of Statistical Shape Spaces for 3D Data with Comparative Analysis for Human Faces | http://arxiv.org/abs/1209.6491 | author:Alan Brunton, Augusto Salazar, Timo Bolkart, Stefanie Wuhrer category:cs.CV cs.GR published:2012-09-28 summary:With systems for acquiring 3D surface data being evermore commonplace, it hasbecome important to reliably extract specific shapes from the acquired data. Inthe presence of noise and occlusions, this can be done through the use ofstatistical shape models, which are learned from databases of clean examples ofthe shape in question. In this paper, we review, analyze and compare differentstatistical models: from those that analyze the variation in geometry globallyto those that analyze the variation in geometry locally. We first review howdifferent types of models have been used in the literature, then proceed todefine the models and analyze them theoretically, in terms of both theirstatistical and computational aspects. We then perform extensive experimentalcomparison on the task of model fitting, and give intuition about which type ofmodel is better for a few applications. Due to the wide availability ofdatabases of high-quality data, we use the human face as the specific shape wewish to extract from corrupted data.
arxiv-1800-111 | Face Alignment Using Active Shape Model And Support Vector Machine | http://arxiv.org/abs/1209.6151 | author:Thai Hoang Le, Truong Nhat Vo category:cs.CV published:2012-09-27 summary:The Active Shape Model (ASM) is one of the most popular local texture modelsfor face alignment. It applies in many fields such as locating facial featuresin the image, face synthesis, etc. However, the experimental results show thatthe accuracy of the classical ASM for some applications is not high. This papersuggests some improvements on the classical ASM to increase the performance ofthe model in the application: face alignment. Four of our major improvementsinclude: i) building a model combining Sobel filter and the 2-D profile insearching face in image; ii) applying Canny algorithm for the enhancement edgeon image; iii) Support Vector Machine (SVM) is used to classify landmarks onface, in order to determine exactly location of these landmarks support forASM; iv)automatically adjust 2-D profile in the multi-level model based on thesize of the input image. The experimental results on Caltech face database andTechnical University of Denmark database (imm_face) show that our proposedimprovement leads to far better performance.
arxiv-1800-112 | The Biometric Menagerie - A Fuzzy and Inconsistent Concept | http://arxiv.org/abs/1209.6189 | author:Nicolaie Popescu-Bodorin, Valentina E. Balas, Iulia M. Motoc category:cs.CV 68U10 I.5 published:2012-09-27 summary:This paper proves that in iris recognition, the concepts of sheep, goats,lambs and wolves - as proposed by Doddington and Yager in the so-calledBiometric Menagerie, are at most fuzzy and at least not quite well defined.They depend not only on the users or on their biometric templates, but also onthe parameters that calibrate the iris recognition system. This paper showsthat, in the case of iris recognition, the extensions of these concepts havevery unsharp and unstable (non-stationary) boundaries. The membership of a userto these categories is more often expressed as a degree (as a fuzzy value)rather than as a crisp value. Moreover, they are defined by fuzzy Sugeno rulesinstead of classical (crisp) definitions. For these reasons, we said that theBiometric Menagerie proposed by Doddington and Yager could be at most a fuzzyconcept of biometry, but even this status is conditioned by improving itsdefinition. All of these facts are confirmed experimentally in a series of 12exhaustive iris recognition tests undertaken for University of Bath Iris ImageDatabase while using three different iris code dimensions (256x16, 128x8 and64x4), two different iris texture encoders (Log-Gabor and Haar-Hilbert) and twodifferent types of safety models.
arxiv-1800-113 | Noise Influence on the Fuzzy-Linguistic Partitioning of Iris Code Space | http://arxiv.org/abs/1209.6190 | author:Iulia M. Motoc, Cristina M. Noaica, Robert Badea, Claudiu G. Ghica category:cs.CV 68U10 I.5 published:2012-09-27 summary:This paper analyses the set of iris codes stored or used in an irisrecognition system as an f-granular space. The f-granulation is given byidentifying in the iris code space the extensions of the fuzzy concepts wolves,goats, lambs and sheep (previously introduced by Doddington as 'animals' of thebiometric menagerie) - which together form a partitioning of the iris codespace. The main question here is how objective (stable / stationary) thispartitioning is when the iris segments are subject to noisy acquisition. Inorder to prove that the f-granulation of iris code space with respect to thefuzzy concepts that define the biometric menagerie is unstable in noisyconditions (is sensitive to noise), three types of noise (localvar, motionblur, salt and pepper) have been alternatively added to the iris segmentsextracted from University of Bath Iris Image Database. The results of 180exhaustive (all-to-all) iris recognition tests are presented and commentedhere.
arxiv-1800-114 | Learning Robust Low-Rank Representations | http://arxiv.org/abs/1209.6393 | author:Pablo Sprechmann, Alex M. Bronstein, Guillermo Sapiro category:cs.LG math.OC published:2012-09-27 summary:In this paper we present a comprehensive framework for learning robustlow-rank representations by combining and extending recent ideas for learningfast sparse coding regressors with structured non-convex optimizationtechniques. This approach connects robust principal component analysis (RPCA)with dictionary learning techniques and allows its approximation via trainableencoders. We propose an efficient feed-forward architecture derived from anoptimization algorithm designed to exactly solve robust low dimensionalprojections. This architecture, in combination with different trainingobjective functions, allows the regressors to be used as online approximants ofthe exact offline RPCA problem or as RPCA-based neural networks. Simplemodifications of these encoders can handle challenging extensions, such as theinclusion of geometric data transformations. We present several examples withreal data from image, audio, and video processing. When used to approximateRPCA, our basic implementation shows several orders of magnitude speedupcompared to the exact solvers with almost no performance degradation. We showthe strength of the inclusion of learning to the RPCA approach on a musicsource separation application, where the encoders outperform the exact RPCAalgorithms, which are already reported to produce state-of-the-art results on abenchmark database. Our preliminary implementation on an iPad showsfaster-than-real-time performance with minimal latency.
arxiv-1800-115 | Sparse Ising Models with Covariates | http://arxiv.org/abs/1209.6342 | author:Jie Cheng, Elizaveta Levina, Pei Wang, Ji Zhu category:stat.ML cs.LG published:2012-09-27 summary:There has been a lot of work fitting Ising models to multivariate binary datain order to understand the conditional dependency relationships between thevariables. However, additional covariates are frequently recorded together withthe binary data, and may influence the dependence relationships. Motivated bysuch a dataset on genomic instability collected from tumor samples of severaltypes, we propose a sparse covariate dependent Ising model to study both theconditional dependency within the binary data and its relationship with theadditional covariates. This results in subject-specific Ising models, where thesubject's covariates influence the strength of association between the genes.As in all exploratory data analysis, interpretability of results is important,and we use L1 penalties to induce sparsity in the fitted graphs and in thenumber of selected covariates. Two algorithms to fit the model are proposed andcompared on a set of simulated data, and asymptotic results are established.The results on the tumor dataset and their biological significance arediscussed in detail.
arxiv-1800-116 | More Is Better: Large Scale Partially-supervised Sentiment Classification - Appendix | http://arxiv.org/abs/1209.6329 | author:Yoav Haimovitch, Koby Crammer, Shie Mannor category:cs.LG published:2012-09-27 summary:We describe a bootstrapping algorithm to learn from partially labeled data,and the results of an empirical study for using it to improve performance ofsentiment classification using up to 15 million unlabeled Amazon productreviews. Our experiments cover semi-supervised learning, domain adaptation andweakly supervised learning. In some cases our methods were able to reduce testerror by more than half using such large amount of data. NOTICE: This is only the supplementary material.
arxiv-1800-117 | Reclassification formula that provides to surpass K-means method | http://arxiv.org/abs/1209.6204 | author:M. Kharinov category:cs.CV cs.DS published:2012-09-27 summary:The paper presents a formula for the reclassification of multidimensionaldata points (columns of real numbers, "objects", "vectors", etc.). This formuladescribes the change in the total squared error caused by reclassification ofdata points from one cluster into another and prompts the way to calculate thesequence of optimal partitions, which are characterized by a minimum value ofthe total squared error E (weighted sum of within-class variance,within-cluster sum of squares WCSS etc.), i.e. the sum of squared distancesfrom each data point to its cluster center. At that source data points aretreated with repetitions allowed, and resulting clusters from differentpartitions, in general case, overlap each other. The final partitions arecharacterized by "equilibrium" stability with respect to the reclassificationof the data points, where the term "stability" means that any prescribedreclassification of data points does not increase the total squared error E. Itis important that conventional K-means method, in general case, providesgeneration of instable partitions with overstated values of the total squarederror E. The proposed method, based on the formula of reclassification, is moreefficient than K-means method owing to converting of any partition into stableone, as well as involving into the process of reclassification of certain setsof data points, in contrast to the classification of individual data pointsaccording to K-means method.
arxiv-1800-118 | Refinability of splines from lattice Voronoi cells | http://arxiv.org/abs/1209.5826 | author:Jorg Peters category:math.NA cs.CV 41A15, 65D07 published:2012-09-26 summary:Splines can be constructed by convolving the indicator function of theVoronoi cell of a lattice. This paper presents simple criteria that imply thatonly a small subset of such spline families can be refined: essentially thewell-known box splines and tensor-product splines. Among the many non-refinableconstructions are hex-splines and their generalization to non-Cartesianlattices. An example shows how non-refinable splines can exhibit increasedapproximation error upon refinement of the lattice.
arxiv-1800-119 | The Issue-Adjusted Ideal Point Model | http://arxiv.org/abs/1209.6004 | author:Sean M. Gerrish, David M. Blei category:stat.ML cs.LG stat.AP published:2012-09-26 summary:We develop a model of issue-specific voting behavior. This model can be usedto explore lawmakers' personal voting patterns of voting by issue area,providing an exploratory window into how the language of the law is correlatedwith political support. We derive approximate posterior inference algorithmsbased on variational methods. Across 12 years of legislative data, wedemonstrate both improvement in heldout prediction performance and the model'sutility in interpreting an inherently multi-dimensional space.
arxiv-1800-120 | Reproduction of Images by Gamut Mapping and Creation of New Test Charts in Prepress Process | http://arxiv.org/abs/1209.6037 | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-09-26 summary:With the advent of digital images the problem of keeping picturevisualization uniformity arises because each printing or scanning device hasits own color chart. So, universal color profiles are made by ICC to bringuniformity in various types of devices. Keeping that color profile in mindvarious new color charts are created and calibrated with the help of standardIT8 test charts available in the market. The main objective to colorreproduction is to produce the identical picture at device output. For thatprinciples for gamut mapping has been designed
arxiv-1800-121 | Bayesian Mixture Models for Frequent Itemset Discovery | http://arxiv.org/abs/1209.6001 | author:Ruefei He, Jonathan Shapiro category:cs.LG cs.IR stat.ML published:2012-09-26 summary:In binary-transaction data-mining, traditional frequent itemset mining oftenproduces results which are not straightforward to interpret. To overcome thisproblem, probability models are often used to produce more compact andconclusive results, albeit with some loss of accuracy. Bayesian statistics havebeen widely used in the development of probability models in machine learningin recent years and these methods have many advantages, including theirabilities to avoid overfitting. In this paper, we develop two Bayesian mixturemodels with the Dirichlet distribution prior and the Dirichlet process (DP)prior to improve the previous non-Bayesian mixture model developed fortransaction dataset mining. We implement the inference of both mixture modelsusing two methods: a collapsed Gibbs sampling scheme and a variationalapproximation algorithm. Experiments in several benchmark problems have shownthat both mixture models achieve better performance than a non-Bayesian mixturemodel. The variational algorithm is the faster of the two approaches while theGibbs sampling method achieves a more accurate results. The Dirichlet processmixture model can automatically grow to a proper complexity for a betterapproximation. Once the model is built, it can be very fast to query and runanalysis on (typically 10 times faster than Eclat, as we will show in theexperiment section). However, these approaches also show that mixture modelsunderestimate the probabilities of frequent itemsets. Consequently, thesemodels have a higher sensitivity but a lower specificity.
arxiv-1800-122 | PlaceRaider: Virtual Theft in Physical Spaces with Smartphones | http://arxiv.org/abs/1209.5982 | author:Robert Templeman, Zahid Rahman, David Crandall, Apu Kapadia category:cs.CR cs.CV published:2012-09-26 summary:As smartphones become more pervasive, they are increasingly targeted bymalware. At the same time, each new generation of smartphone featuresincreasingly powerful onboard sensor suites. A new strain of sensor malware hasbeen developing that leverages these sensors to steal information from thephysical environment (e.g., researchers have recently demonstrated how malwarecan listen for spoken credit card numbers through the microphone, or feelkeystroke vibrations using the accelerometer). Yet the possibilities of whatmalware can see through a camera have been understudied. This paper introducesa novel visual malware called PlaceRaider, which allows remote attackers toengage in remote reconnaissance and what we call virtual theft. Throughcompletely opportunistic use of the camera on the phone and other sensors,PlaceRaider constructs rich, three dimensional models of indoor environments.Remote burglars can thus download the physical space, study the environmentcarefully, and steal virtual objects from the environment (such as financialdocuments, information on computer monitors, and personally identifiableinformation). Through two human subject studies we demonstrate theeffectiveness of using mobile devices as powerful surveillance and virtualtheft platforms, and we suggest several possible defenses against visualmalware.
arxiv-1800-123 | Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs | http://arxiv.org/abs/1209.5860 | author:Yangbo He, Jinzhu Jia, Bin Yu category:stat.ML cs.DM stat.ME published:2012-09-26 summary:Graphical models are popular statistical tools which are used to representdependent or causal complex systems. Statistically equivalent causal ordirected graphical models are said to belong to a Markov equivalent class. Itis of great interest to describe and understand the space of such classes.However, with currently known algorithms, sampling over such classes is onlyfeasible for graphs with fewer than approximately 20 vertices. In this paper,we design reversible irreducible Markov chains on the space of Markovequivalent classes by proposing a perfect set of operators that determine thetransitions of the Markov chain. The stationary distribution of a proposedMarkov chain has a closed form and can be computed easily. Specifically, weconstruct a concrete perfect set of operators on sparse Markov equivalenceclasses by introducing appropriate conditions on each possible operator.Algorithms and their accelerated versions are provided to efficiently generateMarkov chains and to explore properties of Markov equivalence classes of sparsedirected acyclic graphs (DAGs) with thousands of vertices. We findexperimentally that in most Markov equivalence classes of sparse DAGs, (1) mostedges are directed, (2) most undirected subgraphs are small and (3) the numberof these undirected subgraphs grows approximately linearly with the number ofvertices. The article contains supplement arXiv:1303.0632,http://dx.doi.org/10.1214/13-AOS1125SUPP
arxiv-1800-124 | Locality-Sensitive Hashing with Margin Based Feature Selection | http://arxiv.org/abs/1209.5833 | author:Makiko Konoshima, Yui Noma category:cs.LG cs.IR published:2012-09-26 summary:We propose a learning method with feature selection for Locality-SensitiveHashing. Locality-Sensitive Hashing converts feature vectors into bit arrays.These bit arrays can be used to perform similarity searches and personalauthentication. The proposed method uses bit arrays longer than those used inthe end for similarity and other searches and by learning selects the bits thatwill be used. We demonstrated this method can effectively perform optimizationfor cases such as fingerprint images with a large number of labels andextremely few data that share the same labels, as well as verifying that it isalso effective for natural images, handwritten digits, and speech features.
arxiv-1800-125 | Subset Selection for Gaussian Markov Random Fields | http://arxiv.org/abs/1209.5991 | author:Satyaki Mahalanabis, Daniel Stefankovic category:cs.LG stat.ML 68Q32 published:2012-09-26 summary:Given a Gaussian Markov random field, we consider the problem of selecting asubset of variables to observe which minimizes the total expected squaredprediction error of the unobserved variables. We first show that finding anexact solution is NP-hard even for a restricted class of Gaussian Markov randomfields, called Gaussian free fields, which arise in semi-supervised learningand computer vision. We then give a simple greedy approximation algorithm forGaussian free fields on arbitrary graphs. Finally, we give a message passingalgorithm for general Gaussian Markov random fields on bounded tree-widthgraphs.
arxiv-1800-126 | Movie Popularity Classification based on Inherent Movie Attributes using C4.5,PART and Correlation Coefficient | http://arxiv.org/abs/1209.6070 | author:Khalid Ibnal Asad, Tanvir Ahmed, Md. Saiedur Rahman category:cs.LG cs.DB cs.IR H.2.8 published:2012-09-26 summary:Abundance of movie data across the internet makes it an obvious candidate formachine learning and knowledge discovery. But most researches are directedtowards bi-polar classification of movie or generation of a movierecommendation system based on reviews given by viewers on various internetsites. Classification of movie popularity based solely on attributes of a moviei.e. actor, actress, director rating, language, country and budget etc. hasbeen less highlighted due to large number of attributes that are associatedwith each movie and their differences in dimensions. In this paper, we proposeclassification scheme of pre-release movie popularity based on inherentattributes using C4.5 and PART classifier algorithm and define the relationbetween attributes of post release movies using correlation coefficient.
arxiv-1800-127 | Segmentation of Breast Regions in Mammogram Based on Density: A Review | http://arxiv.org/abs/1209.5494 | author:Nafiza Saidin, Harsa Amylia Mat Sakim, Umi Kalthum Ngah, Ibrahim Lutfi Shuaib category:cs.CV published:2012-09-25 summary:The focus of this paper is to review approaches for segmentation of breastregions in mammograms according to breast density. Studies based on densityhave been undertaken because of the relationship between breast cancer anddensity. Breast cancer usually occurs in the fibroglandular area of breasttissue, which appears bright on mammograms and is described as breast density.Most of the studies are focused on the classification methods for glandulartissue detection. Others highlighted on the segmentation methods forfibroglandular tissue, while few researchers performed segmentation of thebreast anatomical regions based on density. There have also been works on thesegmentation of other specific parts of breast regions such as either detectionof nipple position, skin-air interface or pectoral muscles. The problems on theevaluation performance of the segmentation results in relation to ground truthare also discussed in this paper.
arxiv-1800-128 | Environmental Sounds Spectrogram Classification using Log-Gabor Filters and Multiclass Support Vector Machines | http://arxiv.org/abs/1209.5756 | author:Sameh Souli, Zied Lachiri category:cs.CV published:2012-09-25 summary:This paper presents novel approaches for efficient feature extraction usingenvironmental sound magnitude spectrogram. We propose approach based on thevisual domain. This approach included three methods. The first method is basedon extraction for each spectrogram a single log-Gabor filter followed by mutualinformation procedure. In the second method, the spectrogram is passed by thesame steps of the first method but with an averaged bank of 12 log-Gaborfilter. The third method consists of spectrogram segmentation into threepatches, and after that for each spectrogram patch we applied the secondmethod. The classification results prove that the second method is the mostefficient in our environmental sound classification system.
arxiv-1800-129 | Optimal Weighting of Multi-View Data with Low Dimensional Hidden States | http://arxiv.org/abs/1209.5477 | author:Yichao Lu, Dean P. Foster category:stat.ML cs.LG published:2012-09-25 summary:In Natural Language Processing (NLP) tasks, data often has the following twoproperties: First, data can be chopped into multi-views which has beensuccessfully used for dimension reduction purposes. For example, in topicclassification, every paper can be chopped into the title, the main text andthe references. However, it is common that some of the views are less noisierthan other views for supervised learning problems. Second, unlabeled data areeasy to obtain while labeled data are relatively rare. For example, articlesoccurred on New York Times in recent 10 years are easy to grab but having themclassified as 'Politics', 'Finance' or 'Sports' need human labor. Hence lessnoisy features are preferred before running supervised learning methods. Inthis paper we propose an unsupervised algorithm which optimally weightsfeatures from different views when these views are generated from a lowdimensional hidden state, which occurs in widely used models like MixtureGaussian Model, Hidden Markov Model (HMM) and Latent Dirichlet Allocation(LDA).
arxiv-1800-130 | Feature selection with test cost constraint | http://arxiv.org/abs/1209.5601 | author:Fan Min, Qinghua Hu, William Zhu category:cs.AI cs.LG published:2012-09-25 summary:Feature selection is an important preprocessing step in machine learning anddata mining. In real-world applications, costs, including money, time and otherresources, are required to acquire the features. In some cases, there is a testcost constraint due to limited resources. We shall deliberately select aninformative and cheap feature subset for classification. This paper proposesthe feature selection with test cost constraint problem for this issue. The newproblem has a simple form while described as a constraint satisfaction problem(CSP). Backtracking is a general algorithm for CSP, and it is efficient insolving the new problem on medium-sized data. As the backtracking algorithm isnot scalable to large datasets, a heuristic algorithm is also developed.Experimental results show that the heuristic algorithm can find the optimalsolution in most cases. We also redefine some existing feature selectionproblems in rough sets, especially in decision-theoretic rough sets, from theviewpoint of CSP. These new definitions provide insight to some new researchdirections.
arxiv-1800-131 | Towards a learning-theoretic analysis of spike-timing dependent plasticity | http://arxiv.org/abs/1209.5549 | author:David Balduzzi, Michel Besserve category:q-bio.NC cs.LG stat.ML published:2012-09-25 summary:This paper suggests a learning-theoretic perspective on how synapticplasticity benefits global brain functioning. We introduce a model, theselectron, that (i) arises as the fast time constant limit of leakyintegrate-and-fire neurons equipped with spiking timing dependent plasticity(STDP) and (ii) is amenable to theoretical analysis. We show that the selectronencodes reward estimates into spikes and that an error bound on spikes iscontrolled by a spiking margin and the sum of synaptic weights. Moreover, theefficacy of spikes (their usefulness to other reward maximizing selectrons)also depends on total synaptic strength. Finally, based on our analysis, wepropose a regularized version of STDP, and show the regularization improves therobustness of neuronal learning when faced with multiple stimuli.
arxiv-1800-132 | Supervised Blockmodelling | http://arxiv.org/abs/1209.5561 | author:Leto Peel category:cs.LG cs.SI stat.ML published:2012-09-25 summary:Collective classification models attempt to improve classificationperformance by taking into account the class labels of related instances.However, they tend not to learn patterns of interactions between classes and/ormake the assumption that instances of the same class link to each other(assortativity assumption). Blockmodels provide a solution to these issues,being capable of modelling assortative and disassortative interactions, andlearning the pattern of interactions in the form of a summary network. TheSupervised Blockmodel provides good classification performance using linkstructure alone, whilst simultaneously providing an interpretable summary ofnetwork interactions to allow a better understanding of the data. This workexplores three variants of supervised blockmodels of varying complexity andtests them on four structurally different real world networks.
arxiv-1800-133 | Natural Language Processing - A Survey | http://arxiv.org/abs/1209.6238 | author:Kevin Mote category:cs.CL published:2012-09-25 summary:The utility and power of Natural Language Processing (NLP) seems destined tochange our technological society in profound and fundamental ways. Howeverthere are, to date, few accessible descriptions of the science of NLP that havebeen written for a popular audience, or even for an audience of intelligent,but uninitiated scientists. This paper aims to provide just such an overview.In short, the objective of this article is to describe the purpose, proceduresand practical applications of NLP in a clear, balanced, and readable way. Wewill examine the most recent literature describing the methods and processes ofNLP, analyze some of the challenges that researchers are faced with, andbriefly survey some of the current and future applications of this science toIT research in general.
arxiv-1800-134 | Minimizing inter-subject variability in fNIRS based Brain Computer Interfaces via multiple-kernel support vector learning | http://arxiv.org/abs/1209.5467 | author:Berdakh Abibullaev, Jinung An, Seung-Hyun Lee, Sang-Hyeon Jin, Jeon-Il Moon category:stat.ML cs.LG published:2012-09-25 summary:Brain signal variability in the measurements obtained from different subjectsduring different sessions significantly deteriorates the accuracy of mostbrain-computer interface (BCI) systems. Moreover these variabilities, alsoknown as inter-subject or inter-session variabilities, require lengthycalibration sessions before the BCI system can be used. Furthermore, thecalibration session has to be repeated for each subject independently andbefore use of the BCI due to the inter-session variability. In this study, wepresent an algorithm in order to minimize the above-mentioned variabilities andto overcome the time-consuming and usually error-prone calibration time. Ouralgorithm is based on linear programming support-vector machines and theirextensions to a multiple kernel learning framework. We tackle the inter-subjector -session variability in the feature spaces of the classifiers. This is doneby incorporating each subject- or session-specific feature spaces into muchricher feature spaces with a set of optimal decision boundaries. Each decisionboundary represents the subject- or a session specific spatio-temporalvariabilities of neural signals. Consequently, a single classifier withmultiple feature spaces will generalize well to new unseen test patterns evenwithout the calibration steps. We demonstrate that classifiers maintain goodperformances even under the presence of a large degree of BCI variability. Thepresent study analyzes BCI variability related to oxy-hemoglobin neural signalsmeasured using a functional near-infrared spectroscopy.
arxiv-1800-135 | copulaedas: An R Package for Estimation of Distribution Algorithms Based on Copulas | http://arxiv.org/abs/1209.5429 | author:Yasser Gonzalez-Fernandez, Marta Soto category:cs.NE cs.MS published:2012-09-24 summary:The use of copula-based models in EDAs (estimation of distributionalgorithms) is currently an active area of research. In this context, thecopulaedas package for R provides a platform where EDAs based on copulas can beimplemented and studied. The package offers complete implementations of variousEDAs based on copulas and vines, a group of well-known optimization problems,and utility functions to study the performance of the algorithms. Newlydeveloped EDAs can be easily integrated into the package by extending an S4class with generic functions for their main components. This paper presentscopulaedas by providing an overview of EDAs based on copulas, a description ofthe implementation of the package, and an illustration of its use throughexamples. The examples include running the EDAs defined in the package,implementing new algorithms, and performing an empirical study to compare thebehavior of different algorithms on benchmark functions and a real-worldproblem.
arxiv-1800-136 | Spike Timing Dependent Competitive Learning in Recurrent Self Organizing Pulsed Neural Networks Case Study: Phoneme and Word Recognition | http://arxiv.org/abs/1209.5245 | author:Tarek Behi, Najet Arous, Noureddine Ellouze category:cs.CV cs.AI q-bio.NC published:2012-09-24 summary:Synaptic plasticity seems to be a capital aspect of the dynamics of neuralnetworks. It is about the physiological modifications of the synapse, whichhave like consequence a variation of the value of the synaptic weight. Theinformation encoding is based on the precise timing of single spike events thatis based on the relative timing of the pre- and post-synaptic spikes, localsynapse competitions within a single neuron and global competition via lateralconnections. In order to classify temporal sequences, we present in this paperhow to use a local hebbian learning, spike-timing dependent plasticity forunsupervised competitive learning, preserving self-organizing maps of spikingneurons. In fact we present three variants of self-organizing maps (SOM) withspike-timing dependent Hebbian learning rule, the Leaky Integrators Neurons(LIN), the Spiking_SOM and the recurrent Spiking_SOM (RSSOM) models. The casestudy of the proposed SOM variants is phoneme classification and wordrecognition in continuous speech and speaker independent.
arxiv-1800-137 | On Move Pattern Trends in a Large Go Games Corpus | http://arxiv.org/abs/1209.5251 | author:Petr Baudiš, Josef Moudřík category:cs.AI cs.LG published:2012-09-24 summary:We process a large corpus of game records of the board game of Go and proposea way of extracting summary information on played moves. We then apply severalbasic data-mining methods on the summary information to identify the mostdifferentiating features within the summary information, and discuss theircorrespondence with traditional Go knowledge. We show statistically significantmappings of the features to player attributes such as playing strength orinformally perceived "playing style" (e.g. territoriality or aggressivity),describe accurate classifiers for these attributes, and propose applicationsincluding seeding real-work ranks of internet players, aiding in Go study andtuning of Go-playing programs, or contribution to Go-theoretical discussion onthe scope of "playing style".
arxiv-1800-138 | Towards Large-scale and Ultrahigh Dimensional Feature Selection via Feature Generation | http://arxiv.org/abs/1209.5260 | author:Mingkui Tan, Ivor W. Tsang, Li Wang category:cs.LG published:2012-09-24 summary:In many real-world applications such as text mining, it is desirable toselect the most relevant features or variables to improve the generalizationability, or to provide a better interpretation of the prediction models. {Inthis paper, a novel adaptive feature scaling (AFS) scheme is proposed byintroducing a feature scaling {vector $\d \in [0, 1]^m$} to alleviate the biasproblem brought by the scaling bias of the diverse features.} By reformulatingthe resultant AFS model to semi-infinite programming problem, a novel featuregenerating method is presented to identify the most relevant features forclassification problems. In contrast to the traditional feature selectionmethods, the new formulation has the advantage of solving extremelyhigh-dimensional and large-scale problems. With an exact solution to theworst-case analysis in the identification of relevant features, the proposedfeature generating scheme converges globally. More importantly, the proposedscheme facilitates the group selection with or without special structures.Comprehensive experiments on a wide range of synthetic and real-world datasetsdemonstrate that the proposed method {achieves} better or competitiveperformance compared with the existing methods on (group) feature selection interms of generalization performance and training efficiency. The C++ and MATLABimplementations of our algorithm can be available at\emph{http://c2inet.sce.ntu.edu.sg/Mingkui/robust-FGM.rar}.
arxiv-1800-139 | Developing Improved Greedy Crossover to Solve Symmetric Traveling Salesman Problem | http://arxiv.org/abs/1209.5339 | author:Hassan Ismkhan, Kamran Zamanifar category:cs.NE published:2012-09-24 summary:The Traveling Salesman Problem (TSP) is one of the most famous optimizationproblems. Greedy crossover designed by Greffenstette et al, can be used whileSymmetric TSP (STSP) is resolved by Genetic Algorithm (GA). Researchers haveproposed several versions of greedy crossover. Here we propose improved versionof it. We compare our greedy crossover with some of recent crossovers, we useour greedy crossover and some recent crossovers in GA then compare crossoverson speed and accuracy.
arxiv-1800-140 | BPRS: Belief Propagation Based Iterative Recommender System | http://arxiv.org/abs/1209.5335 | author:Erman Ayday, Arash Einolghozati, Faramarz Fekri category:cs.LG published:2012-09-24 summary:In this paper we introduce the first application of the Belief Propagation(BP) algorithm in the design of recommender systems. We formulate therecommendation problem as an inference problem and aim to compute the marginalprobability distributions of the variables which represent the ratings to bepredicted. However, computing these marginal probability functions iscomputationally prohibitive for large-scale systems. Therefore, we utilize theBP algorithm to efficiently compute these functions. Recommendations for eachactive user are then iteratively computed by probabilistic message passing. Asopposed to the previous recommender algorithms, BPRS does not require solvingthe recommendation problem for all the users if it wishes to update therecommendations for only a single active. Further, BPRS computes therecommendations for each user with linear complexity and without requiring atraining period. Via computer simulations (using the 100K MovieLens dataset),we verify that BPRS iteratively reduces the error in the predicted ratings ofthe users until it converges. Finally, we confirm that BPRS is comparable tothe state of art methods such as Correlation-based neighborhood model (CorNgbr)and Singular Value Decomposition (SVD) in terms of rating and precisionaccuracy. Therefore, we believe that the BP-based recommendation algorithm is anew promising approach which offers a significant advantage on scalabilitywhile providing competitive accuracy for the recommender systems.
arxiv-1800-141 | A New Continuous-Time Equality-Constrained Optimization Method to Avoid Singularity | http://arxiv.org/abs/1209.5218 | author:Quan Quan, Kai-Yuan Cai category:cs.NE published:2012-09-24 summary:In equality-constrained optimization, a standard regularity assumption isoften associated with feasible point methods, namely the gradients ofconstraints are linearly independent. In practice, the regularity assumptionmay be violated. To avoid such a singularity, we propose a new projectionmatrix, based on which a feasible point method for the continuous-time,equality-constrained optimization problem is developed. First, the equalityconstraint is transformed into a continuous-time dynamical system withsolutions that always satisfy the equality constraint. Then, the singularity isexplained in detail and a new projection matrix is proposed to avoidsingularity. An update (or say a controller) is subsequently designed todecrease the objective function along the solutions of the transformed system.The invariance principle is applied to analyze the behavior of the solution. Wealso propose a modified approach for addressing cases in which solutions do notsatisfy the equality constraint. Finally, the proposed optimization approachesare applied to two examples to demonstrate its effectiveness.
arxiv-1800-142 | Improving accuracy and power with transfer learning using a meta-analytic database | http://arxiv.org/abs/1209.5375 | author:Yannick Schwartz, Gaël Varoquaux, Christophe Pallier, Philippe Pinel, Jean-Baptiste Poline, Bertrand Thirion category:stat.ML published:2012-09-24 summary:Typical cohorts in brain imaging studies are not large enough for systematictesting of all the information contained in the images. To build testableworking hypotheses, investigators thus rely on analysis of previous work,sometimes formalized in a so-called meta-analysis. In brain imaging, thisapproach underlies the specification of regions of interest (ROIs) that areusually selected on the basis of the coordinates of previously detectedeffects. In this paper, we propose to use a database of images, rather thancoordinates, and frame the problem as transfer learning: learning adiscriminant model on a reference task to apply it to a different but relatednew task. To facilitate statistical analysis of small cohorts, we use a sparsediscriminant model that selects predictive voxels on the reference task andthus provides a principled procedure to define ROIs. The benefits of ourapproach are twofold. First it uses the reference database for prediction, i.e.to provide potential biomarkers in a clinical setting. Second it increasesstatistical power on the new task. We demonstrate on a set of 18 pairs offunctional MRI experimental conditions that our approach gives good prediction.In addition, on a specific transfer situation involving different scanners atdifferent locations, we show that voxel selection based on transfer learningleads to higher detection power on small cohorts.
arxiv-1800-143 | Learning Topic Models and Latent Bayesian Networks Under Expansion Constraints | http://arxiv.org/abs/1209.5350 | author:Animashree Anandkumar, Daniel Hsu, Adel Javanmard, Sham M. Kakade category:stat.ML cs.LG stat.AP published:2012-09-24 summary:Unsupervised estimation of latent variable models is a fundamental problemcentral to numerous applications of machine learning and statistics. This workpresents a principled approach for estimating broad classes of such models,including probabilistic topic models and latent linear Bayesian networks, usingonly second-order observed moments. The sufficient conditions foridentifiability of these models are primarily based on weak expansionconstraints on the topic-word matrix, for topic models, and on the directedacyclic graph, for Bayesian networks. Because no assumptions are made on thedistribution among the latent variables, the approach can handle arbitrarycorrelations among the topics or latent factors. In addition, a tractablelearning method via $\ell_1$ optimization is proposed and studied in numericalexperiments.
arxiv-1800-144 | Model based neuro-fuzzy ASR on Texas processor | http://arxiv.org/abs/1209.5417 | author:Hesam Ekhtiyar, Mehdi Sheida, Somaye Sobati Moghadam category:cs.CV published:2012-09-24 summary:In this paper an algorithm for recognizing speech has been proposed. Therecognized speech is used to execute related commands which use the MFCC andtwo kind of classifiers, first one uses MLP and second one uses fuzzy inferencesystem as a classifier. The experimental results demonstrate the high gain andefficiency of the proposed algorithm. We have implemented this system based ongraphical design and tested on a fix point digital signal processor (DSP) of600 MHz, with reference DM6437-EVM of Texas instrument.
arxiv-1800-145 | Fast Randomized Model Generation for Shapelet-Based Time Series Classification | http://arxiv.org/abs/1209.5038 | author:Daniel Gordon, Danny Hendler, Lior Rokach category:cs.LG published:2012-09-23 summary:Time series classification is a field which has drawn much attention over thepast decade. A new approach for classification of time series usesclassification trees based on shapelets. A shapelet is a subsequence extractedfrom one of the time series in the dataset. A disadvantage of this approach isthe time required for building the shapelet-based classification tree. Thesearch for the best shapelet requires examining all subsequences of all lengthsfrom all time series in the training set. A key goal of this work was to find an evaluation order of the shapeletsspace which enables fast convergence to an accurate model. The comparativeanalysis we conducted clearly indicates that a random evaluation order yieldsthe best results. Our empirical analysis of the distribution of high-qualityshapelets within the shapelets space provides insights into why randomizedshapelets sampling is superior to alternative evaluation orders. We present an algorithm for randomized model generation for shapelet-basedclassification that converges extremely quickly to a model with surprisinglyhigh accuracy after evaluating only an exceedingly small fraction of theshapelets space.
arxiv-1800-146 | Creation of Digital Test Form for Prepress Department | http://arxiv.org/abs/1209.5039 | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-09-23 summary:The main problem in colour management in prepress department is lack ofavailability of literature on colour management and knowledge gap betweenprepress department and press department. So a digital test from has beencreated by Adobe Photoshop to analyse the ICC profile and to create a newprofile and this analysed data is used to study about various grey scale of RGBand CMYK images. That helps in conversion of image from RGB to CMYK in prepressdepartment.
arxiv-1800-147 | Image Classification and Optimized Image Reproduction | http://arxiv.org/abs/1209.5040 | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-09-23 summary:By taking into account the properties and limitations of the human visualsystem, images can be more efficiently compressed, colors more accuratelyreproduced, prints better rendered. To show all these advantages in this papernew adapted color charts have been created based on technical and visual imagecategory analysis. A number of tests have been carried out using extreme imageswith their key information strictly in dark and light areas. It was shown thatthe image categorization using the adapted color charts improves the analysisof relevant image information with regard to both the image gradation and thedetail reproduction. The images with key information in hi-key areas were alsotest printed using the adapted color charts.
arxiv-1800-148 | An Implementation of Computer Graphics as Prepress Image Enhancement Process | http://arxiv.org/abs/1209.5041 | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-09-23 summary:The production of a printed product involves three stages: prepress, theprinting process (press) itself, and finishing (post press). There are varioustypes of equipments (printers, scanners) and various qualities image arepresent in the market. These give different color rendering each time duringreproduction. So, a color key tool has been developed keeping Color ManagementScheme (CMS) in mind so that during reproduction no color rendering takes placeirrespective of use of any device and resolution level has also been improved.
arxiv-1800-149 | Gemini: Graph estimation with matrix variate normal instances | http://arxiv.org/abs/1209.5075 | author:Shuheng Zhou category:stat.ML math.ST stat.TH published:2012-09-23 summary:Undirected graphs can be used to describe matrix variate distributions. Inthis paper, we develop new methods for estimating the graphical structures andunderlying parameters, namely, the row and column covariance and inversecovariance matrices from the matrix variate data. Under sparsity conditions, weshow that one is able to recover the graphs and covariance matrices with asingle random matrix from the matrix variate normal distribution. Our methodextends, with suitable adaptation, to the general setting where replicates areavailable. We establish consistency and obtain the rates of convergence in theoperator and the Frobenius norm. We show that having replicates will allow oneto estimate more complicated graphical structures and achieve faster rates ofconvergence. We provide simulation evidence showing that we can recovergraphical structures as well as estimating the precision matrices, as predictedby theory.
arxiv-1800-150 | Making a Science of Model Search | http://arxiv.org/abs/1209.5111 | author:J. Bergstra, D. Yamins, D. D. Cox category:cs.CV cs.NE published:2012-09-23 summary:Many computer vision algorithms depend on a variety of parameter choices andsettings that are typically hand-tuned in the course of evaluating thealgorithm. While such parameter tuning is often presented as being incidentalto the algorithm, correctly setting these parameter choices is frequentlycritical to evaluating a method's full potential. Compounding matters, theseparameters often must be re-tuned when the algorithm is applied to a newproblem domain, and the tuning process itself often depends on personalexperience and intuition in ways that are hard to describe. Since theperformance of a given technique depends on both the fundamental quality of thealgorithm and the details of its tuning, it can be difficult to determinewhether a given technique is genuinely better, or simply better tuned. In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools to replacehand-tuning with a reproducible and unbiased optimization process. Our approachis to expose the underlying expression graph of how a performance metric (e.g.classification accuracy on validation examples) is computed from parametersthat govern not only how individual processing steps are applied, but evenwhich processing steps are included. A hyper parameter optimization algorithmtransforms this graph into a program for optimizing that performance metric.Our approach yields state of the art results on three disparate computer visionproblems: a face-matching verification task (LFW), a face identification task(PubFig83) and an object recognition task (CIFAR-10), using a single algorithm.More broadly, we argue that the formalization of a meta-model supports moreobjective, reproducible, and quantitative evaluation of computer visionalgorithms, and that it can serve as a valuable tool for guiding algorithmdevelopment.
arxiv-1800-151 | An efficient model-free estimation of multiclass conditional probability | http://arxiv.org/abs/1209.4951 | author:Tu Xu, Junhui Wang category:stat.ML cs.LG stat.ME published:2012-09-22 summary:Conventional multiclass conditional probability estimation methods, such asFisher's discriminate analysis and logistic regression, often requirerestrictive distributional model assumption. In this paper, a model-freeestimation method is proposed to estimate multiclass conditional probabilitythrough a series of conditional quantile regression functions. Specifically,the conditional class probability is formulated as difference of correspondingcumulative distribution functions, where the cumulative distribution functionscan be converted from the estimated conditional quantile regression functions.The proposed estimation method is also efficient as its computation cost doesnot increase exponentially with the number of classes. The theoretical andnumerical studies demonstrate that the proposed estimation method is highlycompetitive against the existing competitors, especially when the number ofclasses is relatively large.
arxiv-1800-152 | A Bayesian Nonparametric Approach to Image Super-resolution | http://arxiv.org/abs/1209.5019 | author:Gungor Polatkan, Mingyuan Zhou, Lawrence Carin, David Blei, Ingrid Daubechies category:cs.LG stat.ML published:2012-09-22 summary:Super-resolution methods form high-resolution images from low-resolutionimages. In this paper, we develop a new Bayesian nonparametric model forsuper-resolution. Our method uses a beta-Bernoulli process to learn a set ofrecurring visual patterns, called dictionary elements, from the data. Becauseit is nonparametric, the number of elements found is also determined from thedata. We test the results on both benchmark and natural images, comparing withseveral other models from the research literature. We perform large-scale humanevaluation experiments to assess the visual quality of the results. In a firstimplementation, we use Gibbs sampling to approximate the posterior. However,this algorithm is not feasible for large-scale data. To circumvent this, wethen develop an online variational Bayes (VB) algorithm. This algorithm findshigh quality dictionaries in a fraction of the time needed by the Gibbssampler.
arxiv-1800-153 | Regression trees for longitudinal and multiresponse data | http://arxiv.org/abs/1209.4690 | author:Wei-Yin Loh, Wei Zheng category:stat.ML stat.AP stat.ME published:2012-09-21 summary:Previous algorithms for constructing regression tree models for longitudinaland multiresponse data have mostly followed the CART approach. Consequently,they inherit the same selection biases and computational difficulties as CART.We propose an alternative, based on the GUIDE approach, that treats eachlongitudinal data series as a curve and uses chi-squared tests of the residualcurve patterns to select a variable to split each node of the tree. Besidesbeing unbiased, the method is applicable to data with fixed and random timepoints and with missing values in the response or predictor variables.Simulation results comparing its mean squared prediction error with that ofMVPART are given, as well as examples comparing it with standard linear mixedeffects and generalized estimating equation models. Conditions for asymptoticconsistency of regression tree function estimates are also given.
arxiv-1800-154 | Efficient Regularized Least-Squares Algorithms for Conditional Ranking on Relational Data | http://arxiv.org/abs/1209.4825 | author:Tapio Pahikkala, Antti Airola, Michiel Stock, Bernard De Baets, Willem Waegeman category:cs.LG stat.ML published:2012-09-21 summary:In domains like bioinformatics, information retrieval and social networkanalysis, one can find learning tasks where the goal consists of inferring aranking of objects, conditioned on a particular target object. We present ageneral kernel framework for learning conditional rankings from various typesof relational data, where rankings can be conditioned on unseen data objects.We propose efficient algorithms for conditional ranking by optimizing squaredregression and ranking loss functions. We show theoretically, that learningwith the ranking loss is likely to generalize better than with the regressionloss. Further, we prove that symmetry or reciprocity properties of relationscan be efficiently enforced in the learned models. Experiments on synthetic andreal-world data illustrate that the proposed methods deliver state-of-the-artperformance in terms of predictive power and computational efficiency.Moreover, we also show empirically that incorporating symmetry or reciprocityproperties can improve the generalization performance.
arxiv-1800-155 | A Note on the SPICE Method | http://arxiv.org/abs/1209.4887 | author:Cristian R. Rojas, Dimitrios Katselis, Håkan Hjalmarsson category:stat.ML cs.SY published:2012-09-21 summary:In this article, we analyze the SPICE method developed in [1], and establishits connections with other standard sparse estimation methods such as the Lassoand the LAD-Lasso. This result positions SPICE as a computationally efficienttechnique for the calculation of Lasso-type estimators. Conversely, thisconnection is very useful for establishing the asymptotic properties of SPICEunder several problem scenarios and for suggesting suitable modifications incases where the naive version of SPICE would not work.
arxiv-1800-156 | On the Sensitivity of Shape Fitting Problems | http://arxiv.org/abs/1209.4893 | author:Kasturi Varadarajan, Xin Xiao category:cs.CG cs.LG published:2012-09-21 summary:In this article, we study shape fitting problems, $\epsilon$-coresets, andtotal sensitivity. We focus on the $(j,k)$-projective clustering problems,including $k$-median/$k$-means, $k$-line clustering, $j$-subspaceapproximation, and the integer $(j,k)$-projective clustering problem. We deriveupper bounds of total sensitivities for these problems, and obtain$\epsilon$-coresets using these upper bounds. Using a dimension-reduction typeargument, we are able to greatly simplify earlier results on total sensitivityfor the $k$-median/$k$-means clustering problems, and obtainpositively-weighted $\epsilon$-coresets for several variants of the$(j,k)$-projective clustering problem. We also extend an earlier result on$\epsilon$-coresets for the integer $(j,k)$-projective clustering problem infixed dimension to the case of high dimension.
arxiv-1800-157 | The Pascal Triangle of a Discrete Image: Definition, Properties and Application to Shape Analysis | http://arxiv.org/abs/1209.4850 | author:Mireille Boutin, Shanshan Huang category:math-ph cs.CV math.MP published:2012-09-21 summary:We define the Pascal triangle of a discrete (gray scale) image as a pyramidalarrangement of complex-valued moments and we explore its geometricsignificance. In particular, we show that the entries of row k of this trianglecorrespond to the Fourier series coefficients of the moment of order k of theRadon transform of the image. Group actions on the plane can be naturallyprolonged onto the entries of the Pascal triangle. We study the prolongation ofsome common group actions, such as rotations and reflections, and we proposesimple tests for detecting equivalences and self-equivalences under these groupactions. The motivating application of this work is the problem ofcharacterizing the geometry of objects on images, for example by detectingapproximate symmetries.
arxiv-1800-158 | The Future of Neural Networks | http://arxiv.org/abs/1209.4855 | author:Sachin Lakra, T. V. Prasad, G. Ramakrishna category:cs.NE published:2012-09-20 summary:The paper describes some recent developments in neural networks and discussesthe applicability of neural networks in the development of a machine thatmimics the human brain. The paper mentions a new architecture, the pulsedneural network that is being considered as the next generation of neuralnetworks. The paper also explores the use of memristors in the development of abrain-like computer called the MoNETA. A new model, multi/infinite dimensionalneural networks, are a recent development in the area of advanced neuralnetworks. The paper concludes that the need of neural networks in thedevelopment of human-like technology is essential and may be non-expendable forit.
arxiv-1800-159 | A Neuro-Fuzzy Technique for Implementing the Half-Adder Circuit Using the CANFIS Model | http://arxiv.org/abs/1209.4895 | author:Sachin Lakra, T. V. Prasad, Deepak Sharma, Shree Harsh Atrey, Anubhav Sharma category:cs.NE published:2012-09-20 summary:A Neural Network, in general, is not considered to be a good solver ofmathematical and binary arithmetic problems. However, networks have beendeveloped for such problems as the XOR circuit. This paper presents a techniquefor the implementation of the Half-adder circuit using the CoActive Neuro-FuzzyInference System (CANFIS) Model and attempts to solve the problem using theNeuroSolutions 5 Simulator. The paper gives the experimental results along withthe interpretations and possible applications of the technique.
arxiv-1800-160 | Stemmer for Serbian language | http://arxiv.org/abs/1209.4471 | author:Nikola Milošević category:cs.CL cs.IR published:2012-09-20 summary:In linguistic morphology and information retrieval, stemming is the processfor reducing inflected (or sometimes derived) words to their stem, base or rootform; generally a written word form. In this work is presented suffix strippingstemmer for Serbian language, one of the highly inflectional languages.
arxiv-1800-161 | An Efficient Color Face Verification Based on 2-Directional 2-Dimensional Feature Extraction | http://arxiv.org/abs/1209.4420 | author:Lan-Ting LI category:cs.CV published:2012-09-20 summary:A novel and uniform framework for face verification is presented in thispaper. First of all, a 2-directional 2-dimensional feature extraction method isadopted to extract client-specific template - 2D discrimant projection matrix.Then the face skin color information is utilized as an additive feature toenhance decision making strategy that makes use of not only 2D grey feature butalso 2D skin color feature. A fusion decision of both is applied to experimentthe performance on the XM2VTS database according to Lausanne protocol.Experimental results show that the framework achieves high verificationaccuracy and verification speed.
arxiv-1800-162 | Probabilistic Auto-Associative Models and Semi-Linear PCA | http://arxiv.org/abs/1209.4551 | author:Serge Iovleff category:stat.AP stat.ML published:2012-09-20 summary:Auto-Associative models cover a large class of methods used in data analysis.In this paper, we describe the generals properties of these models when theprojection component is linear and we propose and test an easy to implementProbabilistic Semi-Linear Auto- Associative model in a Gaussian setting. Weshow it is a generalization of the PCA model to the semi-linear case. Numericalexperiments on simulated datasets and a real astronomical application highlightthe interest of this approach
arxiv-1800-163 | Head Frontal-View Identification Using Extended LLE | http://arxiv.org/abs/1209.4419 | author:Chao Wang category:cs.CV published:2012-09-20 summary:Automatic head frontal-view identification is challenging due to appearancevariations caused by pose changes, especially without any training samples. Inthis paper, we present an unsupervised algorithm for identifying frontal viewamong multiple facial images under various yaw poses (derived from the sameperson). Our approach is based on Locally Linear Embedding (LLE), with theassumption that with yaw pose being the only variable, the facial images shouldlie in a smooth and low dimensional manifold. We horizontally flip the facialimages and present two K-nearest neighbor protocols for the original images andthe flipped images, respectively. In the proposed extended LLE, for any facialimage (original or flipped one), we search (1) the Ko nearest neighbors amongthe original facial images and (2) the Kf nearest neighbors among the flippedfacial images to construct the same neighborhood graph. The extended LLEeliminates the differences (because of background, face position and scale inthe whole image and some asymmetry of left-right face) between the originalfacial image and the flipped facial image at the same yaw pose so that theflipped facial images can be used effectively. Our approach does not need anytraining samples as prior information. The experimental results show that thefrontal view of head can be identified reliably around the lowest point of thepose manifold for multiple facial images, especially the cropped facial images(little background and centered face).
arxiv-1800-164 | Image Super-Resolution via Sparse Bayesian Modeling of Natural Images | http://arxiv.org/abs/1209.4317 | author:Haichao Zhang, David Wipf, Yanning Zhang category:cs.CV published:2012-09-19 summary:Image super-resolution (SR) is one of the long-standing and active topics inimage processing community. A large body of works for image super resolutionformulate the problem with Bayesian modeling techniques and then obtain itsMaximum-A-Posteriori (MAP) solution, which actually boils down to a regularizedregression task over separable regularization term. Although straightforward,this approach cannot exploit the full potential offered by the probabilisticmodeling, as only the posterior mode is sought. Also, the separable property ofthe regularization term can not capture any correlations between the sparsecoefficients, which sacrifices much on its modeling accuracy. We propose aBayesian image SR algorithm via sparse modeling of natural images. The sparsityproperty of the latent high resolution image is exploited by introducing latentvariables into the high-order Markov Random Field (MRF) which capture thecontent adaptive variance by pixel-wise adaptation. The high-resolution imageis estimated via Empirical Bayesian estimation scheme, which is substantiallyfaster than our previous approach based on Markov Chain Monte Carlo sampling[1]. It is shown that the actual cost function for the proposed approachactually incorporates a non-factorial regularization term over the sparsecoefficients. Experimental results indicate that the proposed method cangenerate competitive or better results than \emph{state-of-the-art} SRalgorithms.
arxiv-1800-165 | Comunication-Efficient Algorithms for Statistical Optimization | http://arxiv.org/abs/1209.4129 | author:Yuchen Zhang, John C. Duchi, Martin Wainwright category:stat.ML cs.LG stat.CO published:2012-09-19 summary:We analyze two communication-efficient algorithms for distributed statisticaloptimization on large-scale data sets. The first algorithm is a standardaveraging method that distributes the $N$ data samples evenly to $\nummac$machines, performs separate minimization on each subset, and then averages theestimates. We provide a sharp analysis of this average mixture algorithm,showing that under a reasonable set of conditions, the combined parameterachieves mean-squared error that decays as $\order(N^{-1}+(N/m)^{-2})$.Whenever $m \le \sqrt{N}$, this guarantee matches the best possible rateachievable by a centralized algorithm having access to all $\totalnumobs$samples. The second algorithm is a novel method, based on an appropriate formof bootstrap subsampling. Requiring only a single round of communication, ithas mean-squared error that decays as $\order(N^{-1} + (N/m)^{-3})$, and so ismore robust to the amount of parallelization. In addition, we show that astochastic gradient-based method attains mean-squared error decaying as$O(N^{-1} + (N/ m)^{-3/2})$, easing computation at the expense of penalties inthe rate of convergence. We also provide experimental evaluation of ourmethods, investigating their performance both on simulated data and on alarge-scale regression problem from the internet search domain. In particular,we show that our methods can be used to efficiently solve an advertisementprediction problem from the Chinese SoSo Search Engine, which involves logisticregression with $N \approx 2.4 \times 10^8$ samples and $d \approx 740,000$covariates.
arxiv-1800-166 | Alpha/Beta Divergences and Tweedie Models | http://arxiv.org/abs/1209.4280 | author:Y. Kenan Yilmaz, A. Taylan Cemgil category:stat.ML cs.IT math.IT math.ST stat.TH published:2012-09-19 summary:We describe the underlying probabilistic interpretation of alpha and betadivergences. We first show that beta divergences are inherently tied to Tweediedistributions, a particular type of exponential family, known as exponentialdispersion models. Starting from the variance function of a Tweedie model, weoutline how to get alpha and beta divergences as special cases of Csisz\'ar's$f$ and Bregman divergences. This result directly generalizes the well-knownrelationship between the Gaussian distribution and least squares estimation toTweedie models and beta divergence minimization.
arxiv-1800-167 | Multi-Level Modeling of Quotation Families Morphogenesis | http://arxiv.org/abs/1209.4277 | author:Elisa Omodei, Thierry Poibeau, Jean-Philippe Cointet category:cs.CY cs.CL cs.SI physics.soc-ph published:2012-09-19 summary:This paper investigates cultural dynamics in social media by examining theproliferation and diversification of clearly-cut pieces of content: quotedtexts. In line with the pioneering work of Leskovec et al. and Simmons et al.on memes dynamics we investigate in deep the transformations that quotationspublished online undergo during their diffusion. We deliberately put aside thestructure of the social network as well as the dynamical patterns pertaining tothe diffusion process to focus on the way quotations are changed, how oftenthey are modified and how these changes shape more or less diverse families andsub-families of quotations. Following a biological metaphor, we try tounderstand in which way mutations can transform quotations at different scalesand how mutation rates depend on various properties of the quotations.
arxiv-1800-168 | Variational Inference in Nonconjugate Models | http://arxiv.org/abs/1209.4360 | author:Chong Wang, David M. Blei category:stat.ML published:2012-09-19 summary:Mean-field variational methods are widely used for approximate posteriorinference in many probabilistic models. In a typical application, mean-fieldmethods approximately compute the posterior with a coordinate-ascentoptimization algorithm. When the model is conditionally conjugate, thecoordinate updates are easily derived and in closed form. However, many modelsof interest---like the correlated topic model and Bayesian logisticregression---are nonconjuate. In these models, mean-field methods cannot bedirectly applied and practitioners have had to develop variational algorithmson a case-by-case basis. In this paper, we develop two generic methods fornonconjugate models, Laplace variational inference and delta method variationalinference. Our methods have several advantages: they allow for easily derivedvariational algorithms with a wide class of nonconjugate models; they extendand unify some of the existing algorithms that have been derived for specificmodels; and they work well on real-world datasets. We studied our methods onthe correlated topic model, Bayesian logistic regression, and hierarchicalBayesian logistic regression.
arxiv-1800-169 | Network Routing Optimization Using Swarm Intelligence | http://arxiv.org/abs/1209.3909 | author:Mohamed A. El Galil category:cs.NE cs.DM published:2012-09-18 summary:The aim of this paper is to highlight and explore a traditional problem,which is the minimum spanning tree, and finding the shortest-path in networkrouting, by using Swarm Intelligence. This work to be considered as aninvestigation topic with combination between operations research, discretemathematics, and evolutionary computing aiming to solve one of networkingproblems.
arxiv-1800-170 | Evolution and the structure of learning agents | http://arxiv.org/abs/1209.3818 | author:Alok Raj category:cs.AI cs.LG I.2; I.2.6 published:2012-09-18 summary:This paper presents the thesis that all learning agents of finite informationsize are limited by their informational structure in what goals they canefficiently learn to achieve in a complex environment. Evolutionary change iscritical for creating the required structure for all learning agents in anycomplex environment. The thesis implies that there is no efficient universallearning algorithm. An agent can go past the learning limits imposed by itsstructure only by slow evolutionary change or blind search which in a verycomplex environment can only give an agent an inefficient universal learningcapability that can work only in evolutionary timescales or improbable luck.
arxiv-1800-171 | Scaling Multidimensional Inference for Structured Gaussian Processes | http://arxiv.org/abs/1209.4120 | author:Elad Gilboa, Yunus Saatçi, John P. Cunningham category:stat.ML published:2012-09-18 summary:Exact Gaussian Process (GP) regression has O(N^3) runtime for data size N,making it intractable for large N. Many algorithms for improving GP scalingapproximate the covariance with lower rank matrices. Other work has exploitedstructure inherent in particular covariance functions, including GPs withimplied Markov structure, and equispaced inputs (both enable O(N) runtime).However, these GP advances have not been extended to the multidimensional inputsetting, despite the preponderance of multidimensional applications. This paperintroduces and tests novel extensions of structured GPs to multidimensionalinputs. We present new methods for additive GPs, showing a novel connectionbetween the classic backfitting method and the Bayesian framework. To achieveoptimal accuracy-complexity tradeoff, we extend this model with a novel variantof projection pursuit regression. Our primary result -- projection pursuitGaussian Process Regression -- shows orders of magnitude speedup whilepreserving high accuracy. The natural second and third steps includenon-Gaussian observations and higher dimensional equispaced grid methods. Weintroduce novel techniques to address both of these necessary directions. Wethoroughly illustrate the power of these three advances on several datasets,achieving close performance to the naive Full GP at orders of magnitude lesscost.
arxiv-1800-172 | Transferring Subspaces Between Subjects in Brain-Computer Interfacing | http://arxiv.org/abs/1209.4115 | author:Wojciech Samek, Frank C. Meinecke, Klaus-Robert Müller category:stat.ML cs.HC cs.LG published:2012-09-18 summary:Compensating changes between a subjects' training and testing session inBrain Computer Interfacing (BCI) is challenging but of great importance for arobust BCI operation. We show that such changes are very similar betweensubjects, thus can be reliably estimated using data from other users andutilized to construct an invariant feature space. This novel approach tolearning from other subjects aims to reduce the adverse effects of commonnon-stationarities, but does not transfer discriminative information. This isan important conceptual difference to standard multi-subject methods that e.g.improve the covariance matrix estimation by shrinking it towards the average ofother users or construct a global feature space. These methods do not reducesthe shift between training and test data and may produce poor results whensubjects have very different signal characteristics. In this paper we compareour approach to two state-of-the-art multi-subject methods on toy data and twodata sets of EEG recordings from subjects performing motor imagery. We showthat it can not only achieve a significant increase in performance, but alsothat the extracted change patterns allow for a neurophysiologically meaningfulinterpretation.
arxiv-1800-173 | Writing Reusable Digital Geometry Algorithms in a Generic Image Processing Framework | http://arxiv.org/abs/1209.4233 | author:Roland Levillain, Thierry Géraud, Laurent Najman category:cs.MS cs.CV published:2012-09-18 summary:Digital Geometry software should reflect the generality of the underlyingmathe- matics: mapping the latter to the former requires genericity. Bydesigning generic solutions, one can effectively reuse digital geometry datastructures and algorithms. We propose an image processing framework focused onthe Generic Programming paradigm in which an algorithm on the paper can beturned into a single code, written once and usable with various input types.This approach enables users to design and implement new methods at a lowercost, try cross-domain experiments and help generalize results
arxiv-1800-174 | Generalized Canonical Correlation Analysis for Disparate Data Fusion | http://arxiv.org/abs/1209.3761 | author:Ming Sun, Carey E. Priebe, Minh Tang category:stat.ML cs.LG published:2012-09-17 summary:Manifold matching works to identify embeddings of multiple disparate dataspaces into the same low-dimensional space, where joint inference can bepursued. It is an enabling methodology for fusion and inference from multipleand massive disparate data sources. In this paper we focus on a method calledCanonical Correlation Analysis (CCA) and its generalization GeneralizedCanonical Correlation Analysis (GCCA), which belong to the more general ReducedRank Regression (RRR) framework. We present an efficiency investigation of CCAand GCCA under different training conditions for a particular text documentclassification task.
arxiv-1800-175 | Active Learning for Crowd-Sourced Databases | http://arxiv.org/abs/1209.3686 | author:Barzan Mozafari, Purnamrita Sarkar, Michael J. Franklin, Michael I. Jordan, Samuel Madden category:cs.LG cs.DB published:2012-09-17 summary:Crowd-sourcing has become a popular means of acquiring labeled data for awide variety of tasks where humans are more accurate than computers, e.g.,labeling images, matching objects, or analyzing sentiment. However, relyingsolely on the crowd is often impractical even for data sets with thousands ofitems, due to time and cost constraints of acquiring human input (which costpennies and minutes per label). In this paper, we propose algorithms forintegrating machine learning into crowd-sourced databases, with the goal ofallowing crowd-sourcing applications to scale, i.e., to handle larger datasetsat lower costs. The key observation is that, in many of the above tasks, humansand machine learning algorithms can be complementary, as humans are often moreaccurate but slow and expensive, while algorithms are usually less accurate,but faster and cheaper. Based on this observation, we present two new active learning algorithms tocombine humans and algorithms together in a crowd-sourced database. Ouralgorithms are based on the theory of non-parametric bootstrap, which makes ourresults applicable to a broad class of machine learning models. Our results, onthree real-life datasets collected with Amazon's Mechanical Turk, and on 15well-known UCI data sets, show that our methods on average ask humans to labelone to two orders of magnitude fewer items to achieve the same accuracy as abaseline that labels random images, and two to eight times fewer questions thanprevious active learning schemes.
arxiv-1800-176 | Submodularity in Batch Active Learning and Survey Problems on Gaussian Random Fields | http://arxiv.org/abs/1209.3694 | author:Yifei Ma, Roman Garnett, Jeff Schneider category:cs.LG cs.AI cs.DS published:2012-09-17 summary:Many real-world datasets can be represented in the form of a graph whose edgeweights designate similarities between instances. A discrete Gaussian randomfield (GRF) model is a finite-dimensional Gaussian process (GP) whose priorcovariance is the inverse of a graph Laplacian. Minimizing the trace of thepredictive covariance Sigma (V-optimality) on GRFs has proven successful inbatch active learning classification problems with budget constraints. However,its worst-case bound has been missing. We show that the V-optimality on GRFs asa function of the batch query set is submodular and hence its greedy selectionalgorithm guarantees an (1-1/e) approximation ratio. Moreover, GRF models havethe absence-of-suppressor (AofS) condition. For active survey problems, wepropose a similar survey criterion which minimizes 1'(Sigma)1. In practice,V-optimality criterion performs better than GPs with mutual information gaincriteria and allows nonuniform costs for different nodes.
arxiv-1800-177 | A Bayesian method for the analysis of deterministic and stochastic time series | http://arxiv.org/abs/1209.3730 | author:C. A. L. Bailer-Jones category:astro-ph.IM astro-ph.SR stat.ML published:2012-09-17 summary:I introduce a general, Bayesian method for modelling univariate time seriesdata assumed to be drawn from a continuous, stochastic process. The methodaccommodates arbitrary temporal sampling, and takes into account measurementuncertainties for arbitrary error models (not just Gaussian) on both the timeand signal variables. Any model for the deterministic component of thevariation of the signal with time is supported, as is any model of thestochastic component on the signal and time variables. Models illustrated hereare constant and sinusoidal models for the signal mean combined with a Gaussianstochastic component, as well as a purely stochastic model, theOrnstein-Uhlenbeck process. The posterior probability distribution over modelparameters is determined via Monte Carlo sampling. Models are compared usingthe "cross-validation likelihood", in which the posterior-averaged likelihoodfor different partitions of the data are combined. In principle this is morerobust to changes in the prior than is the evidence (the prior-averagedlikelihood). The method is demonstrated by applying it to the light curves of11 ultra cool dwarf stars, claimed by a previous study to show statisticallysignificant variability. This is reassessed here by calculating thecross-validation likelihood for various time series models, including a nullhypothesis of no variability beyond the error bars. 10 of 11 light curves areconfirmed as being significantly variable, and one of these seems to beperiodic, with two plausible periods identified. Another object is bestdescribed by the Ornstein-Uhlenbeck process, a conclusion which is obviouslylimited to the set of models actually tested.
arxiv-1800-178 | Recovering Block-structured Activations Using Compressive Measurements | http://arxiv.org/abs/1209.3431 | author:Sivaraman Balakrishnan, Mladen Kolar, Alessandro Rinaldo, Aarti Singh category:stat.ML published:2012-09-15 summary:We consider the problems of detection and localization of a contiguous blockof weak activation in a large matrix, from a small number of noisy, possiblyadaptive, compressive (linear) measurements. This is closely related to theproblem of compressed sensing, where the task is to estimate a sparse vectorusing a small number of linear measurements. Contrary to results in compressedsensing, where it has been shown that neither adaptivity nor contiguousstructure help much, we show that for reliable localization the magnitude ofthe weakest signals is strongly influenced by both structure and the ability tochoose measurements adaptively while for detection neither adaptivity norstructure reduce the requirement on the magnitude of the signal. Wecharacterize the precise tradeoffs between the various problem parameters, thesignal strength and the number of measurements required to reliably detect andlocalize the block of activation. The sufficient conditions are complementedwith information theoretic lower bounds.
arxiv-1800-179 | A Hajj And Umrah Location Classification System For Video Crowded Scenes | http://arxiv.org/abs/1209.3433 | author:Hossam M. Zawbaa, Salah A. Aly, Adnan A. Gutub category:cs.CV cs.CY cs.LG published:2012-09-15 summary:In this paper, a new automatic system for classifying ritual locations indiverse Hajj and Umrah video scenes is investigated. This challenging subjecthas mostly been ignored in the past due to several problems one of which is thelack of realistic annotated video datasets. HUER Dataset is defined to modelsix different Hajj and Umrah ritual locations[26]. The proposed Hajj and Umrah ritual location classifying system consists offour main phases: Preprocessing, segmentation, feature extraction, and locationclassification phases. The shot boundary detection and background/foregroudsegmentation algorithms are applied to prepare the input video scenes into theKNN, ANN, and SVM classifiers. The system improves the state of art results onHajj and Umrah location classifications, and successfully recognizes the sixHajj rituals with more than 90% accuracy. The various demonstrated experimentsshow the promising results.
arxiv-1800-180 | Further Optimal Regret Bounds for Thompson Sampling | http://arxiv.org/abs/1209.3353 | author:Shipra Agrawal, Navin Goyal category:cs.LG cs.DS stat.ML 68W40, 68Q25 F.2.0 published:2012-09-15 summary:Thompson Sampling is one of the oldest heuristics for multi-armed banditproblems. It is a randomized algorithm based on Bayesian ideas, and hasrecently generated significant interest after several studies demonstrated itto have better empirical performance compared to the state of the art methods.In this paper, we provide a novel regret analysis for Thompson Sampling thatsimultaneously proves both the optimal problem-dependent bound of$(1+\epsilon)\sum_i \frac{\ln T}{\Delta_i}+O(\frac{N}{\epsilon^2})$ and thefirst near-optimal problem-independent bound of $O(\sqrt{NT\ln T})$ on theexpected regret of this algorithm. Our near-optimal problem-independent boundsolves a COLT 2012 open problem of Chapelle and Li. The optimalproblem-dependent regret bound for this problem was first proven recently byKaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques areconceptually simple, easily extend to distributions other than the Betadistribution, and also extend to the more general contextual bandits setting[Manuscript, Agrawal and Goyal, 2012].
arxiv-1800-181 | Negative Binomial Process Count and Mixture Modeling | http://arxiv.org/abs/1209.3442 | author:Mingyuan Zhou, Lawrence Carin category:stat.ME stat.ML published:2012-09-15 summary:The seemingly disjoint problems of count and mixture modeling are unitedunder the negative binomial (NB) process. A gamma process is employed to modelthe rate measure of a Poisson process, whose normalization provides a randomprobability measure for mixture modeling and whose marginalization leads to anNB process for count modeling. A draw from the NB process consists of a Poissondistributed finite number of distinct atoms, each of which is associated with alogarithmic distributed number of data samples. We reveal relationships betweenvarious count- and mixture-modeling distributions and construct aPoisson-logarithmic bivariate distribution that connects the NB and Chineserestaurant table distributions. Fundamental properties of the models aredeveloped, and we derive efficient Bayesian inference. It is shown that withaugmentation and normalization, the NB process and gamma-NB process can bereduced to the Dirichlet process and hierarchical Dirichlet process,respectively. These relationships highlight theoretical, structural andcomputational advantages of the NB process. A variety of NB processes,including the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB andzero-inflated-NB processes, with distinct sharing mechanisms, are alsoconstructed. These models are applied to topic modeling, with connections madeto existing algorithms under Poisson factor analysis. Example results show theimportance of inferring both the NB dispersion and probability parameters.
arxiv-1800-182 | Thompson Sampling for Contextual Bandits with Linear Payoffs | http://arxiv.org/abs/1209.3352 | author:Shipra Agrawal, Navin Goyal category:cs.LG cs.DS stat.ML 68W40, 68Q25 F.2.0 published:2012-09-15 summary:Thompson Sampling is one of the oldest heuristics for multi-armed banditproblems. It is a randomized algorithm based on Bayesian ideas, and hasrecently generated significant interest after several studies demonstrated itto have better empirical performance compared to the state-of-the-art methods.However, many questions regarding its theoretical performance remained open. Inthis paper, we design and analyze a generalization of Thompson Samplingalgorithm for the stochastic contextual multi-armed bandit problem with linearpayoff functions, when the contexts are provided by an adaptive adversary. Thisis among the most important and widely studied versions of the contextualbandits problem. We provide the first theoretical guarantees for the contextualversion of Thompson Sampling. We prove a high probability regret bound of$\tilde{O}(d^{3/2}\sqrt{T})$ (or $\tilde{O}(d\sqrt{T \log(N)})$), which is thebest regret bound achieved by any computationally efficient algorithm availablefor this problem in the current literature, and is within a factor of$\sqrt{d}$ (or $\sqrt{\log(N)}$) of the information-theoretic lower bound forthis problem.
arxiv-1800-183 | Analog readout for optical reservoir computers | http://arxiv.org/abs/1209.3129 | author:Anteo Smerieri, François Duport, Yvan Paquot, Benjamin Schrauwen, Marc Haelterman, Serge Massar category:cs.ET cs.LG cs.NE physics.optics published:2012-09-14 summary:Reservoir computing is a new, powerful and flexible machine learningtechnique that is easily implemented in hardware. Recently, by using atime-multiplexed architecture, hardware reservoir computers have reachedperformance comparable to digital implementations. Operating speeds allowingfor real time information operation have been reached using optoelectronicsystems. At present the main performance bottleneck is the readout layer whichuses slow, digital postprocessing. We have designed an analog readout suitablefor time-multiplexed optoelectronic reservoir computers, capable of working inreal time. The readout has been built and tested experimentally on a standardbenchmark task. Its performance is better than non-reservoir methods, withample room for further improvement. The present work thereby overcomes one ofthe major limitations for the future development of hardware reservoircomputers.
arxiv-1800-184 | Agent-based Exploration of Wirings of Biological Neural Networks: Position Paper | http://arxiv.org/abs/1209.3150 | author:Önder Gürcan, Oğuz Dikenelli, Kemal S. Türker category:cs.NE q-bio.NC published:2012-09-14 summary:The understanding of human central nervous system depends on knowledge of itswiring. However, there are still gaps in our understanding of its wiring due totechnical difficulties. While some information is coming out from humanexperiments, medical research is lacking of simulation models to put currentfindings together to obtain the global picture and to predict hypotheses tolead future experiments. Agent-based modeling and simulation (ABMS) is a strongcandidate for the simulation model. In this position paper, we discuss thecurrent status of "neural wiring" and "ABMS in biological systems". Inparticular, we discuss that the ABMS context provides features required forexploration of biological neural wiring.
arxiv-1800-185 | Beyond Stemming and Lemmatization: Ultra-stemming to Improve Automatic Text Summarization | http://arxiv.org/abs/1209.3126 | author:Juan-Manuel Torres-Moreno category:cs.IR cs.CL published:2012-09-14 summary:In Automatic Text Summarization, preprocessing is an important phase toreduce the space of textual representation. Classically, stemming andlemmatization have been widely used for normalizing words. However, even usingnormalization on large texts, the curse of dimensionality can disturb theperformance of summarizers. This paper describes a new method for normalizationof words to further reduce the space of representation. We propose to reduceeach word to its initial letters, as a form of Ultra-stemming. The results showthat Ultra-stemming not only preserve the content of summaries produced by thisrepresentation, but often the performances of the systems can be dramaticallyimproved. Summaries on trilingual corpora were evaluated automatically withFresa. Results confirm an increase in the performance, regardless of summarizersystem used.
arxiv-1800-186 | Detection and Classification of Viewer Age Range Smart Signs at TV Broadcast | http://arxiv.org/abs/1209.3113 | author:Baran Tander, Atilla Özmen, Murat Başkan category:cs.CV published:2012-09-14 summary:In this paper, the identification and classification of Viewer Age RangeSmart Signs, designed by the Radio and Television Supreme Council of Turkey, togive age range information for the TV viewers, are realized. Therefore, theautomatic detection at the broadcast will be possible, enabling themanufacturing of TV receivers which are sensible to these signs. The mostimportant step at this process is the pattern recognition. Since the symbolsthat must be identified are circular, various circle detection techniques canbe employed. In our study, first, two different circle segmentation methods forstill images are analyzed, their advantages and drawbacks are discussed. Apopular neural network structure called Multilayer Perceptron is employed forthe classification. Afterwards, the same procedures are carried out forstreaming video. All of the steps depicted above are realized on a standard PC.
arxiv-1800-187 | Link Prediction in Graphs with Autoregressive Features | http://arxiv.org/abs/1209.3230 | author:Emile Richard, Stephane Gaiffas, Nicolas Vayatis category:stat.ML published:2012-09-14 summary:In the paper, we consider the problem of link prediction in time-evolvinggraphs. We assume that certain graph features, such as the node degree, followa vector autoregressive (VAR) model and we propose to use this information toimprove the accuracy of prediction. Our strategy involves a joint optimizationprocedure over the space of adjacency matrices and VAR matrices which takesinto account both sparsity and low rank properties of the matrices. Oracleinequalities are derived and illustrate the trade-offs in the choice ofsmoothing parameters when modeling the joint effect of sparsity and low rankproperty. The estimate is computed efficiently using proximal methods through ageneralized forward-backward agorithm.
arxiv-1800-188 | Signal Recovery in Unions of Subspaces with Applications to Compressive Imaging | http://arxiv.org/abs/1209.3079 | author:Nikhil Rao, Benjamin Recht, Robert Nowak category:stat.ML math.OC published:2012-09-14 summary:In applications ranging from communications to genetics, signals can bemodeled as lying in a union of subspaces. Under this model, signal coefficientsthat lie in certain subspaces are active or inactive together. The potentialsubspaces are known in advance, but the particular set of subspaces that areactive (i.e., in the signal support) must be learned from measurements. We showthat exploiting knowledge of subspaces can further reduce the number ofmeasurements required for exact signal recovery, and derive universal boundsfor the number of measurements needed. The bound is universal in the sense thatit only depends on the number of subspaces under consideration, and theirorientation relative to each other. The particulars of the subspaces (e.g.,compositions, dimensions, extents, overlaps, etc.) does not affect the resultswe obtain. In the process, we derive sample complexity bounds for the specialcase of the group lasso with overlapping groups (the latent group lasso), whichis used in a variety of applications. Finally, we also show that wavelettransform coefficients of images can be modeled as lying in groups, and hencecan be efficiently recovered using group lasso methods.
arxiv-1800-189 | Hessian Schatten-Norm Regularization for Linear Inverse Problems | http://arxiv.org/abs/1209.3318 | author:Stamatios Lefkimmiatis, John Paul Ward, Michael Unser category:math.OC cs.CV cs.NA published:2012-09-14 summary:We introduce a novel family of invariant, convex, and non-quadraticfunctionals that we employ to derive regularized solutions of ill-posed linearinverse imaging problems. The proposed regularizers involve the Schatten normsof the Hessian matrix, computed at every pixel of the image. They can be viewedas second-order extensions of the popular total-variation (TV) semi-norm sincethey satisfy the same invariance properties. Meanwhile, by taking advantage ofsecond-order derivatives, they avoid the staircase effect, a common artifact ofTV-based reconstructions, and perform well for a wide range of applications. Tosolve the corresponding optimization problems, we propose an algorithm that isbased on a primal-dual formulation. A fundamental ingredient of this algorithmis the projection of matrices onto Schatten norm balls of arbitrary radius.This operation is performed efficiently based on a direct link we providebetween vector projections onto $\ell_q$ norm balls and matrix projections ontoSchatten norm balls. Finally, we demonstrate the effectiveness of the proposedmethods through experimental results on several inverse imaging problems withreal and simulated data.
arxiv-1800-190 | Predator confusion is sufficient to evolve swarming behavior | http://arxiv.org/abs/1209.3330 | author:Randal S. Olson, Arend Hintze, Fred C. Dyer, David B. Knoester, Christoph Adami category:q-bio.PE cs.NE nlin.AO q-bio.NC published:2012-09-14 summary:Swarming behaviors in animals have been extensively studied due to theirimplications for the evolution of cooperation, social cognition, andpredator-prey dynamics. An important goal of these studies is discerning whichevolutionary pressures favor the formation of swarms. One hypothesis is thatswarms arise because the presence of multiple moving prey in swarms causesconfusion for attacking predators, but it remains unclear how important thisselective force is. Using an evolutionary model of a predator-prey system, weshow that predator confusion provides a sufficient selection pressure to evolveswarming behavior in prey. Furthermore, we demonstrate that the evolutionaryeffect of predator confusion on prey could in turn exert pressure on thestructure of the predator's visual field, favoring the frontally oriented,high-resolution visual systems commonly observed in predators that feed onswarming animals. Finally, we provide evidence that when prey evolve swarmingin response to predator confusion, there is a change in the shape of thefunctional response curve describing the predator's consumption rate as preydensity increases. Thus, we show that a relatively simple perceptualconstraint--predator confusion--could have pervasive evolutionary effects onprey behavior, predator sensory mechanisms, and the ecological interactionsbetween predators and prey.
arxiv-1800-191 | Multi-track Map Matching | http://arxiv.org/abs/1209.2759 | author:Adel Javanmard, Maya Haridasan, Li Zhang category:cs.LG cs.DS stat.AP published:2012-09-13 summary:We study algorithms for matching user tracks, consisting of time-orderedlocation points, to paths in the road network. Previous work has focused on thescenario where the location data is linearly ordered and consists of fairlydense and regular samples. In this work, we consider the \emph{multi-track mapmatching}, where the location data comes from different trips on the sameroute, each with very sparse samples. This captures the realistic scenariowhere users repeatedly travel on regular routes and samples are sparselycollected, either due to energy consumption constraints or because samples areonly collected when the user actively uses a service. In the multi-trackproblem, the total set of combined locations is only partially ordered, ratherthan globally ordered as required by previous map-matching algorithms. Wepropose two methods, the iterative projection scheme and the graph Laplacianscheme, to solve the multi-track problem by using a single-track map-matchingsubroutine. We also propose a boosting technique which may be applied to eitherapproach to improve the accuracy of the estimated paths. In addition, in orderto deal with variable sampling rates in single-track map matching, we propose amethod based on a particular regularized cost function that can be adapted fordifferent sampling rates and measurement errors. We evaluate the effectivenessof our techniques for reconstructing tracks under several differentconfigurations of sampling error and sampling rate.
arxiv-1800-192 | Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL | http://arxiv.org/abs/1209.2784 | author:Nishant A. Mehta, Dongryeol Lee, Alexander G. Gray category:cs.LG stat.ML published:2012-09-13 summary:Since its inception, the modus operandi of multi-task learning (MTL) has beento minimize the task-wise mean of the empirical risks. We introduce ageneralized loss-compositional paradigm for MTL that includes a spectrum offormulations as a subfamily. One endpoint of this spectrum is minimax MTL: anew MTL formulation that minimizes the maximum of the tasks' empirical risks.Via a certain relaxation of minimax MTL, we obtain a continuum of MTLformulations spanning minimax MTL and classical MTL. The full paradigm itselfis loss-compositional, operating on the vector of empirical risks. Itincorporates minimax MTL, its relaxations, and many new MTL formulations asspecial cases. We show theoretically that minimax MTL tends to avoid worst caseoutcomes on newly drawn test tasks in the learning to learn (LTL) test setting.The results of several MTL formulations on synthetic and real problems in theMTL and LTL test settings are encouraging.
arxiv-1800-193 | Hirarchical Digital Image Inpainting Using Wavelets | http://arxiv.org/abs/1209.2816 | author:S. Padmavathi, B. Priyalakshmi. Dr. K. P. Soman category:cs.CV published:2012-09-13 summary:Inpainting is the technique of reconstructing unknown or damaged portions ofan image in a visually plausible way. Inpainting algorithm automatically fillsthe damaged region in an image using the information available in undamagedregion. Propagation of structure and texture information becomes a challenge asthe size of damaged area increases. In this paper, a hierarchical inpaintingalgorithm using wavelets is proposed. The hierarchical method tries to keep themask size smaller while wavelets help in handling the high pass structureinformation and low pass texture information separately. The performance of theproposed algorithm is tested using different factors. The results of ouralgorithm are compared with existing methods such as interpolation, diffusionand exemplar techniques.
arxiv-1800-194 | Improving Energy Efficiency in Femtocell Networks: A Hierarchical Reinforcement Learning Framework | http://arxiv.org/abs/1209.2790 | author:Xianfu Chen, Honggang Zhang, Tao Chen, Mika Lasanen category:cs.LG published:2012-09-13 summary:This paper investigates energy efficiency for two-tier femtocell networksthrough combining game theory and stochastic learning. With the Stackelberggame formulation, a hierarchical reinforcement learning framework is applied tostudy the joint average utility maximization of macrocells and femtocellssubject to the minimum signal-to-interference-plus-noise-ratio requirements.The macrocells behave as the leaders and the femtocells are followers duringthe learning procedure. At each time step, the leaders commit to dynamicstrategies based on the best responses of the followers, while the followerscompete against each other with no further information but the leaders'strategy information. In this paper, we propose two learning algorithms toschedule each cell's stochastic power levels, leading by the macrocells.Numerical experiments are presented to validate the proposed studies and showthat the two learning algorithms substantially improve the energy efficiency ofthe femtocell networks.
arxiv-1800-195 | A new class of metrics for spike trains | http://arxiv.org/abs/1209.2918 | author:Cătălin V. Rusu, Răzvan V. Florian category:cs.IT cs.NE math.IT q-bio.NC published:2012-09-13 summary:The distance between a pair of spike trains, quantifying the differencesbetween them, can be measured using various metrics. Here we introduce a newclass of spike train metrics, inspired by the Pompeiu-Hausdorff distance, andcompare them with existing metrics. Some of our new metrics (the modulus-metricand the max-metric) have characteristics that are qualitatively different thanthose of classical metrics like the van Rossum distance or the Victor & Purpuradistance. The modulus-metric and the max-metric are particularly suitable formeasuring distances between spike trains where information is encoded inbursts, but the number and the timing of spikes inside a burst does not carryinformation. The modulus-metric does not depend on any parameters and can becomputed using a fast algorithm, in a time that depends linearly on the numberof spikes in the two spike trains. We also introduce localized versions of thenew metrics, which could have the biologically-relevant interpretation ofmeasuring the differences between spike trains as they are perceived at aparticular moment in time by a neuron receiving these spike trains.
arxiv-1800-196 | Community Detection in the Labelled Stochastic Block Model | http://arxiv.org/abs/1209.2910 | author:Simon Heimlicher, Marc Lelarge, Laurent Massoulié category:cs.SI cs.LG math.PR physics.soc-ph published:2012-09-13 summary:We consider the problem of community detection from observed interactionsbetween individuals, in the context where multiple types of interaction arepossible. We use labelled stochastic block models to represent the observeddata, where labels correspond to interaction types. Focusing on a two-communityscenario, we conjecture a threshold for the problem of reconstructing thehidden communities in a way that is correlated with the true partition. Tosubstantiate the conjecture, we prove that the given threshold correctlyidentifies a transition on the behaviour of belief propagation from insensitiveto sensitive. We further prove that the same threshold corresponds to thetransition in a related inference problem on a tree model from infeasible tofeasible. Finally, numerical results using belief propagation for communitydetection give further support to the conjecture.
arxiv-1800-197 | Parametric Local Metric Learning for Nearest Neighbor Classification | http://arxiv.org/abs/1209.3056 | author:Jun Wang, Adam Woznica, Alexandros Kalousis category:cs.LG published:2012-09-13 summary:We study the problem of learning local metrics for nearest neighborclassification. Most previous works on local metric learning learn a number oflocal unrelated metrics. While this "independence" approach delivers anincreased flexibility its downside is the considerable risk of overfitting. Wepresent a new parametric local metric learning method in which we learn asmooth metric matrix function over the data manifold. Using an approximationerror bound of the metric matrix function we learn local metrics as linearcombinations of basis metrics defined on anchor points over different regionsof the instance space. We constrain the metric matrix function by imposing onthe linear combinations manifold regularization which makes the learned metricmatrix function vary smoothly along the geodesics of the data manifold. Ourmetric learning method has excellent performance both in terms of predictivepower and scalability. We experimented with several large-scale classificationproblems, tens of thousands of instances, and compared it with several state ofthe art metric learning methods, both global and local, as well as to SVM withautomatic kernel selection, all of which it outperforms in a significantmanner.
arxiv-1800-198 | A Novel Approach of Harris Corner Detection of Noisy Images using Adaptive Wavelet Thresholding Technique | http://arxiv.org/abs/1209.2903 | author:Nilanjan Dey, Pradipti Nandi, Nilanjana Barman category:cs.CV published:2012-09-13 summary:In this paper we propose a method of corner detection for obtaining featureswhich is required to track and recognize objects within a noisy image. Cornerdetection of noisy images is a challenging task in image processing. Naturalimages often get corrupted by noise during acquisition and transmission. ThoughCorner detection of these noisy images does not provide desired results, hencede-noising is required. Adaptive wavelet thresholding approach is applied forthe same.
arxiv-1800-199 | Training a Feed-forward Neural Network with Artificial Bee Colony Based Backpropagation Method | http://arxiv.org/abs/1209.2548 | author:Sudarshan Nandy, Partha Pratim Sarkar, Achintya Das category:cs.NE cs.AI published:2012-09-12 summary:Back-propagation algorithm is one of the most widely used and populartechniques to optimize the feed forward neural network training. Natureinspired meta-heuristic algorithms also provide derivative-free solution tooptimize complex problem. Artificial bee colony algorithm is a nature inspiredmeta-heuristic algorithm, mimicking the foraging or food source searchingbehaviour of bees in a bee colony and this algorithm is implemented in severalapplications for an improved optimized outcome. The proposed method in thispaper includes an improved artificial bee colony algorithm basedback-propagation neural network training method for fast and improvedconvergence rate of the hybrid neural network learning method. The result isanalysed with the genetic algorithm based back-propagation method, and it isanother hybridized procedure of its kind. Analysis is performed over standarddata sets, reflecting the light of efficiency of proposed method in terms ofconvergence speed and rate.
arxiv-1800-200 | Wavelet Based Image Coding Schemes : A Recent Survey | http://arxiv.org/abs/1209.2515 | author:V. J. Rehna, M. K. Jeya Kumar category:cs.CV published:2012-09-12 summary:A variety of new and powerful algorithms have been developed for imagecompression over the years. Among them the wavelet-based image compressionschemes have gained much popularity due to their overlapping nature whichreduces the blocking artifacts that are common phenomena in JPEG compressionand multiresolution character which leads to superior energy compaction withhigh quality reconstructed images. This paper provides a detailed survey onsome of the popular wavelet coding techniques such as the Embedded ZerotreeWavelet (EZW) coding, Set Partitioning in Hierarchical Tree (SPIHT) coding, theSet Partitioned Embedded Block (SPECK) Coder, and the Embedded Block Codingwith Optimized Truncation (EBCOT) algorithm. Other wavelet-based codingtechniques like the Wavelet Difference Reduction (WDR) and the Adaptive ScannedWavelet Difference Reduction (ASWDR) algorithms, the Space FrequencyQuantization (SFQ) algorithm, the Embedded Predictive Wavelet Image Coder(EPWIC), Compression with Reversible Embedded Wavelet (CREW), the Stack-Run(SR) coding and the recent Geometric Wavelet (GW) coding are also discussed.Based on the review, recommendations and discussions are presented foralgorithm development and implementation.
arxiv-1800-201 | Cultural Algorithm Toolkit for Multi-objective Rule Mining | http://arxiv.org/abs/1209.2948 | author:Sujatha Srinivasan, Sivakumar Ramakrishnan category:cs.NE cs.AI published:2012-09-12 summary:Cultural algorithm is a kind of evolutionary algorithm inspired from societalevolution and is composed of a belief space, a population space and a protocolthat enables exchange of knowledge between these sources. Knowledge created inthe population space is accepted into the belief space while this collectiveknowledge from these sources is combined to influence the decisions of theindividual agents in solving problems. Classification rules comes underdescriptive knowledge discovery in data mining and are the most sought out byusers since they represent highly comprehensible form of knowledge. The ruleshave certain properties which make them useful forms of actionable knowledge tousers. The rules are evaluated using these properties namely the rule metrics.In the current study a Cultural Algorithm Toolkit for Classification RuleMining (CAT-CRM) is proposed which allows the user to control three differentset of parameters namely the evolutionary parameters, the rule parameters aswell as agent parameters and hence can be used for experimenting with anevolutionary system, a rule mining system or an agent based social system.Results of experiments conducted to observe the effect of different number andtype of metrics on the performance of the algorithm on bench mark data sets isreported.
arxiv-1800-202 | Conditional validity of inductive conformal predictors | http://arxiv.org/abs/1209.2673 | author:Vladimir Vovk category:cs.LG 68T05, 62G15 published:2012-09-12 summary:Conformal predictors are set predictors that are automatically valid in thesense of having coverage probability equal to or exceeding a given confidencelevel. Inductive conformal predictors are a computationally efficient versionof conformal predictors satisfying the same property of validity. However,inductive conformal predictors have been only known to control unconditionalcoverage probability. This paper explores various versions of conditionalvalidity and various ways to achieve them using inductive conformal predictorsand their modifications.
arxiv-1800-203 | Probabilities on Sentences in an Expressive Logic | http://arxiv.org/abs/1209.2620 | author:Marcus Hutter, John W. Lloyd, Kee Siong Ng, William T. B. Uther category:cs.LO cs.AI cs.LG math.LO math.PR published:2012-09-12 summary:Automated reasoning about uncertain knowledge has many applications. Onedifficulty when developing such systems is the lack of a completelysatisfactory integration of logic and probability. We address this problemdirectly. Expressive languages like higher-order logic are ideally suited forrepresenting and reasoning about structured knowledge. Uncertain knowledge canbe modeled by using graded probabilities rather than binary truth-values. Themain technical problem studied in this paper is the following: Given a set ofsentences, each having some probability of being true, what probability shouldbe ascribed to other (query) sentences? A natural wish-list, among others, isthat the probability distribution (i) is consistent with the knowledge base,(ii) allows for a consistent inference procedure and in particular (iii)reduces to deductive logic in the limit of probabilities being 0 and 1, (iv)allows (Bayesian) inductive reasoning and (v) learning in the limit and inparticular (vi) allows confirmation of universally quantifiedhypotheses/sentences. We translate this wish-list into technical requirementsfor a prior probability and show that probabilities satisfying all our criteriaexist. We also give explicit constructions and several generalcharacterizations of probabilities that satisfy some or all of the criteria andvarious (counter) examples. We also derive necessary and sufficient conditionsfor extending beliefs about finitely many sentences to suitable probabilitiesover all sentences, and in particular least dogmatic or least biased ones. Weconclude with a brief outlook on how the developed theory might be used andapproximated in autonomous reasoning agents. Our theory is a step towards aglobally consistent and empirically satisfactory unification of probability andlogic.
arxiv-1800-204 | Performance Evaluation of Predictive Classifiers For Knowledge Discovery From Engineering Materials Data Sets | http://arxiv.org/abs/1209.2501 | author:Hemanth K. S Doreswamy category:cs.LG published:2012-09-12 summary:In this paper, naive Bayesian and C4.5 Decision Tree Classifiers(DTC) aresuccessively applied on materials informatics to classify the engineeringmaterials into different classes for the selection of materials that suit theinput design specifications. Here, the classifiers are analyzed individuallyand their performance evaluation is analyzed with confusion matrix predictiveparameters and standard measures, the classification results are analyzed ondifferent class of materials. Comparison of classifiers has found that naiveBayesian classifier is more accurate and better than the C4.5 DTC. Theknowledge discovered by the naive bayesian classifier can be employed fordecision making in materials selection in manufacturing industries.
arxiv-1800-205 | Positivity and Transportation | http://arxiv.org/abs/1209.2655 | author:Marco Cuturi category:stat.ML math.CO published:2012-09-12 summary:We prove in this paper that the weighted volume of the set of integraltransportation matrices between two integral histograms r and c of equal sum isa positive definite kernel of r and c when the set of considered weights formsa positive definite matrix. The computation of this quantity, despite being thesubject of a significant research effort in algebraic statistics, remains anintractable challenge for histograms of even modest dimensions. We propose analternative kernel which, rather than considering all matrices of thetransportation polytope, only focuses on a sub-sample of its vertices known asits Northwestern corner solutions. The resulting kernel is positive definiteand can be computed with a number of operations O(R^2d) that grows linearly inthe complexity of the dimension d, where R^2, the total amount of sampledvertices, is a parameter that controls the complexity of the kernel.
arxiv-1800-206 | Sparse Representation of Astronomical Images | http://arxiv.org/abs/1209.2657 | author:Laura Rebollo-Neira, James Bowley category:math-ph cs.CV math.MP published:2012-09-12 summary:Sparse representation of astronomical images is discussed. It is shown that asignificant gain in sparsity is achieved when particular mixed dictionaries areused for approximating these types of images with greedy selection strategies.Experiments are conducted to confirm: i)Effectiveness at producing sparserepresentations. ii)Competitiveness, with respect to the time required toprocess large images.The latter is a consequence of the suitability of theproposed dictionaries for approximating images in partitions of smallblocks.This feature makes it possible to apply the effective greedy selectiontechnique Orthogonal Matching Pursuit, up to some block size. For blocksexceeding that size a refinement of the original Matching Pursuit approach isconsidered. The resulting method is termed Self Projected Matching Pursuit,because is shown to be effective for implementing, via Matching Pursuit itself,the optional back-projection intermediate steps in that approach.
arxiv-1800-207 | WikiSent : Weakly Supervised Sentiment Analysis Through Extractive Summarization With Wikipedia | http://arxiv.org/abs/1209.2493 | author:Subhabrata Mukherjee, Pushpak Bhattacharyya category:cs.IR cs.CL published:2012-09-12 summary:This paper describes a weakly supervised system for sentiment analysis in themovie review domain. The objective is to classify a movie review into apolarity class, positive or negative, based on those sentences bearing opinionon the movie alone. The irrelevant text, not directly related to the revieweropinion on the movie, is left out of analysis. Wikipedia incorporates the worldknowledge of movie-specific features in the system which is used to obtain anextractive summary of the review, consisting of the reviewer's opinions aboutthe specific aspects of the movie. This filters out the concepts which areirrelevant or objective with respect to the given movie. The proposed system,WikiSent, does not require any labeled data for training. The only weaksupervision arises out of the usage of resources like WordNet, Part-of-SpeechTagger and Sentiment Lexicons by virtue of their construction. WikiSentachieves a considerable accuracy improvement over the baseline and has a betteror comparable accuracy to the existing semi-supervised and unsupervised systemsin the domain, on the same dataset. We also perform a general movie reviewtrend analysis using WikiSent to find the trend in movie-making and the publicacceptance in terms of movie genre, year of release and polarity.
arxiv-1800-208 | Regret Bounds for Restless Markov Bandits | http://arxiv.org/abs/1209.2693 | author:Ronald Ortner, Daniil Ryabko, Peter Auer, Rémi Munos category:cs.LG math.OC stat.ML published:2012-09-12 summary:We consider the restless Markov bandit problem, in which the state of eacharm evolves according to a Markov process independently of the learner'sactions. We suggest an algorithm that after $T$ steps achieves$\tilde{O}(\sqrt{T})$ regret with respect to the best policy that knows thedistributions of all arms. No assumptions on the Markov chains are made exceptthat they are irreducible. In addition, we show that index-based policies arenecessarily suboptimal for the considered problem.
arxiv-1800-209 | TwiSent: A Multistage System for Analyzing Sentiment in Twitter | http://arxiv.org/abs/1209.2495 | author:Subhabrata Mukherjee, Akshat Malu, A. R. Balamurali, Pushpak Bhattacharyya category:cs.IR cs.CL published:2012-09-12 summary:In this paper, we present TwiSent, a sentiment analysis system for Twitter.Based on the topic searched, TwiSent collects tweets pertaining to it andcategorizes them into the different polarity classes positive, negative andobjective. However, analyzing micro-blog posts have many inherent challengescompared to the other text genres. Through TwiSent, we address the problems of1) Spams pertaining to sentiment analysis in Twitter, 2) Structural anomaliesin the text in the form of incorrect spellings, nonstandard abbreviations,slangs etc., 3) Entity specificity in the context of the topic searched and 4)Pragmatics embedded in text. The system performance is evaluated on manuallyannotated gold standard data and on an automatically annotated tweet set basedon hashtags. It is a common practise to show the efficacy of a supervisedsystem on an automatically annotated dataset. However, we show that such asystem achieves lesser classification accurcy when tested on generic twitterdataset. We also show that our system performs much better than an existingsystem.
arxiv-1800-210 | Visual Tracking with Similarity Matching Ratio | http://arxiv.org/abs/1209.2696 | author:Aysegul Dundar, Jonghoon Jin, Eugenio Culurciello category:cs.CV cs.RO published:2012-09-12 summary:This paper presents a novel approach to visual tracking: Similarity MatchingRatio (SMR). The traditional approach of tracking is minimizing some measuresof the difference between the template and a patch from the frame. Thisapproach is vulnerable to outliers and drastic appearance changes and anextensive study is focusing on making the approach more tolerant to them.However, this often results in longer, corrective algo- rithms which do notsolve the original problem. This paper proposes a novel approach to thedefinition of the tracking problems, SMR, which turns the differences into aprobability measure. Only pixel differences below a threshold count towardsdeciding the match, the rest are ignored. This approach makes the SMR trackerrobust to outliers and points that dramaticaly change appearance. The SMRtracker is tested on challenging video sequences and achieved state-of-the-artperformance.
arxiv-1800-211 | Comparison Study for Clonal Selection Algorithm and Genetic Algorithm | http://arxiv.org/abs/1209.2717 | author:Ezgi Deniz Ulker, Sadik Ulker category:cs.NE published:2012-09-12 summary:Two metaheuristic algorithms namely Artificial Immune Systems (AIS) andGenetic Algorithms are classified as computational systems inspired bytheoretical immunology and genetics mechanisms. In this work we examine thecomparative performances of two algorithms. A special selection algorithm,Clonal Selection Algorithm (CLONALG), which is a subset of Artificial ImmuneSystems, and Genetic Algorithms are tested with certain benchmark functions. Itis shown that depending on type of a function Clonal Selection Algorithm andGenetic Algorithm have better performance over each other.
arxiv-1800-212 | Approximate evaluation of marginal association probabilities with belief propagation | http://arxiv.org/abs/1209.6299 | author:Jason L. Williams, Roslyn A. Lau category:cs.AI cs.CV published:2012-09-12 summary:Data association, the problem of reasoning over correspondence betweentargets and measurements, is a fundamental problem in tracking. This paperpresents a graphical model formulation of data association and applies anapproximate inference method, belief propagation (BP), to obtain estimates ofmarginal association probabilities. We prove that BP is guaranteed to converge,and bound the number of iterations necessary. Experiments reveal a favourablecomparison to prior methods in terms of accuracy and computational complexity.
arxiv-1800-213 | Likelihood Estimation with Incomplete Array Variate Observations | http://arxiv.org/abs/1209.2669 | author:Deniz Akdemir category:stat.ME math.ST stat.ML stat.TH published:2012-09-12 summary:Missing data is an important challenge when dealing with high dimensionaldata arranged in the form of an array. In this paper, we propose methods forestimation of the parameters of array variate normal probability model frompartially observed multiway data. The methods developed here are useful formissing data imputation, estimation of mean and covariance parameters formultiway data. A multiway semi-parametric mixed effects model that allowsseparation of multiway covariance effects is also defined and an efficientalgorithm for estimation is recommended. We provide simulation results alongwith real life data from genetics to demonstrate these methods.
arxiv-1800-214 | Leveraging Sentiment to Compute Word Similarity | http://arxiv.org/abs/1209.2341 | author:A. R. Balamurali, Subhabrata Mukherjee, Akshat Malu, Pushpak Bhattacharyya category:cs.IR cs.CL published:2012-09-11 summary:In this paper, we introduce a new WordNet based similarity metric, SenSim,which incorporates sentiment content (i.e., degree of positive or negativesentiment) of the words being compared to measure the similarity between them.The proposed metric is based on the hypothesis that knowing the sentiment isbeneficial in measuring the similarity. To verify this hypothesis, we measureand compare the annotator agreement for 2 annotation strategies: 1) sentimentinformation of a pair of words is considered while annotating and 2) sentimentinformation of a pair of words is not considered while annotating.Inter-annotator correlation scores show that the agreement is better when thetwo annotators consider sentiment information while assigning a similarityscore to a pair of words. We use this hypothesis to measure the similaritybetween a pair of words. Specifically, we represent each word as a vectorcontaining sentiment scores of all the content words in the WordNet gloss ofthe sense of that word. These sentiment scores are derived from a sentimentlexicon. We then measure the cosine similarity between the two vectors. Weperform both intrinsic and extrinsic evaluation of SenSim and compare theperformance with other widely usedWordNet similarity metrics.
arxiv-1800-215 | Cooperative learning in multi-agent systems from intermittent measurements | http://arxiv.org/abs/1209.2194 | author:Naomi Ehrich Leonard, Alex Olshevsky category:math.OC cs.LG cs.MA cs.SY published:2012-09-11 summary:Motivated by the problem of tracking a direction in a decentralized way, weconsider the general problem of cooperative learning in multi-agent systemswith time-varying connectivity and intermittent measurements. We propose adistributed learning protocol capable of learning an unknown vector $\mu$ fromnoisy measurements made independently by autonomous nodes. Our protocol iscompletely distributed and able to cope with the time-varying, unpredictable,and noisy nature of inter-agent communication, and intermittent noisymeasurements of $\mu$. Our main result bounds the learning speed of ourprotocol in terms of the size and combinatorial features of the (time-varying)networks connecting the nodes.
arxiv-1800-216 | Multimodal diffusion geometry by joint diagonalization of Laplacians | http://arxiv.org/abs/1209.2295 | author:Davide Eynard, Klaus Glashoff, Michael M. Bronstein, Alexander M. Bronstein category:cs.CV cs.AI published:2012-09-11 summary:We construct an extension of diffusion geometry to multiple modalitiesthrough joint approximate diagonalization of Laplacian matrices. This naturallyextends classical data analysis tools based on spectral geometry, such asdiffusion maps and spectral clustering. We provide several synthetic and realexamples of manifold learning, retrieval, and clustering demonstrating that thejoint diffusion geometry frequently better captures the inherent structure ofmulti-modal data. We also show that many previous attempts to constructmultimodal spectral clustering can be seen as particular cases of jointapproximate diagonalization of the Laplacians.
arxiv-1800-217 | Query Complexity of Derivative-Free Optimization | http://arxiv.org/abs/1209.2434 | author:Kevin G. Jamieson, Robert D. Nowak, Benjamin Recht category:stat.ML cs.LG published:2012-09-11 summary:This paper provides lower bounds on the convergence rate of Derivative FreeOptimization (DFO) with noisy function evaluations, exposing a fundamental andunavoidable gap between the performance of algorithms with access to gradientsand those with access to only function evaluations. However, there aresituations in which DFO is unavoidable, and for such situations we propose anew DFO algorithm that is proved to be near optimal for the class of stronglyconvex objective functions. A distinctive feature of the algorithm is that ituses only Boolean-valued function comparisons, rather than functionevaluations. This makes the algorithm useful in an even wider range ofapplications, such as optimization based on paired comparisons from humansubjects, for example. We also show that regardless of whether DFO is based onnoisy function evaluations or Boolean-valued function comparisons, theconvergence rate is the same.
arxiv-1800-218 | Identification of Fertile Translations in Medical Comparable Corpora: a Morpho-Compositional Approach | http://arxiv.org/abs/1209.2400 | author:Estelle Delpech, Béatrice Daille, Emmanuel Morin, Claire Lemaire category:cs.CL published:2012-09-11 summary:This paper defines a method for lexicon in the biomedical domain fromcomparable corpora. The method is based on compositional translation andexploits morpheme-level translation equivalences. It can generate translationsfor a large variety of morphologically constructed words and can also generate'fertile' translations. We show that fertile translations increase the overallquality of the extracted lexicon for English to French translation.
arxiv-1800-219 | Feature Specific Sentiment Analysis for Product Reviews | http://arxiv.org/abs/1209.2352 | author:Subhabrata Mukherjee, Pushpak Bhattacharyya category:cs.IR cs.CL published:2012-09-11 summary:In this paper, we present a novel approach to identify feature specificexpressions of opinion in product reviews with different features and mixedemotions. The objective is realized by identifying a set of potential featuresin the review and extracting opinion expressions about those features byexploiting their associations. Capitalizing on the view that more closelyassociated words come together to express an opinion about a certain feature,dependency parsing is used to identify relations between the opinionexpressions. The system learns the set of significant relations to be used bydependency parsing and a threshold parameter which allows us to merge closelyassociated opinion expressions. The data requirement is minimal as this is aone time learning of the domain independent parameters. The associations arerepresented in the form of a graph which is partitioned to finally retrieve theopinion expression describing the user specified feature. We show that thesystem achieves a high accuracy across all domains and performs at par withstate-of-the-art systems despite its data limitations.
arxiv-1800-220 | On the Complexity of Bandit and Derivative-Free Stochastic Convex Optimization | http://arxiv.org/abs/1209.2388 | author:Ohad Shamir category:cs.LG math.OC stat.ML published:2012-09-11 summary:The problem of stochastic convex optimization with bandit feedback (in thelearning community) or without knowledge of gradients (in the optimizationcommunity) has received much attention in recent years, in the form ofalgorithms and performance upper bounds. However, much less is known about theinherent complexity of these problems, and there are few lower bounds in theliterature, especially for nonlinear functions. In this paper, we investigatethe attainable error/regret in the bandit and derivative-free settings, as afunction of the dimension d and the available number of queries T. We provide aprecise characterization of the attainable performance for strongly-convex andsmooth functions, which also imply a non-trivial lower bound for more generalproblems. Moreover, we prove that in both the bandit and derivative-freesetting, the required number of queries must scale at least quadratically withthe dimension. Finally, we show that on the natural class of quadraticfunctions, it is possible to obtain a "fast" O(1/T) error rate in terms of T,under mild assumptions, even without having access to gradients. To the best ofour knowledge, this is the first such rate in a derivative-free stochasticsetting, and holds despite previous results which seem to imply the contrary.
arxiv-1800-221 | Counterfactual Reasoning and Learning Systems | http://arxiv.org/abs/1209.2355 | author:Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson category:cs.LG cs.AI cs.IR math.ST stat.TH published:2012-09-11 summary:This work shows how to leverage causal inference to understand the behaviorof complex learning systems interacting with their environment and predict theconsequences of changes to the system. Such predictions allow both humans andalgorithms to select changes that improve both the short-term and long-termperformance of such systems. This work is illustrated by experiments carriedout on the ad placement system associated with the Bing search engine.
arxiv-1800-222 | A Bayesian Boosting Model | http://arxiv.org/abs/1209.1996 | author:Alexander Lorbert, David M. Blei, Robert E. Schapire, Peter J. Ramadge category:stat.ML published:2012-09-10 summary:We offer a novel view of AdaBoost in a statistical setting. We propose aBayesian model for binary classification in which label noise is modeledhierarchically. Using variational inference to optimize a dynamic evidencelower bound, we derive a new boosting-like algorithm called VIBoost. We showits close connections to AdaBoost and give experimental results from fourdatasets.
arxiv-1800-223 | Modeling controversies in the press: the case of the abnormal bees' death | http://arxiv.org/abs/1209.2163 | author:Alexandre Delanoë, Serge Galam category:physics.soc-ph cs.CL published:2012-09-10 summary:The controversy about the cause(s) of abnormal death of bee colonies inFrance is investigated through an extensive analysis of the french speakingpress. A statistical analysis of textual data is first performed on the lexiconused by journalists to describe the facts and to present associatedinformations during the period 1998-2010. Three states are identified toexplain the phenomenon. The first state asserts a unique cause, the second onefocuses on multifactor causes and the third one states the absence of currentproof. Assigning each article to one of the three states, we are able to followthe associated opinion dynamics among the journalists over 13 years. Then, weapply the Galam sequential probabilistic model of opinion dynamic to thosedata. Assuming journalists are either open mind or inflexible about theirrespective opinions, the results are reproduced precisely provided we accountfor a series of annual changes in the proportions of respective inflexibles.The results shed a new counter intuitive light on the various pressure supposedto apply on the journalists by either chemical industries or beekeepers andexperts or politicians. The obtained dynamics of respective inflexibles showsthe possible effect of lobbying, the inertia of the debate and the netadvantage gained by the first whistleblowers.
arxiv-1800-224 | A Comparative Study of Efficient Initialization Methods for the K-Means Clustering Algorithm | http://arxiv.org/abs/1209.1960 | author:M. Emre Celebi, Hassan A. Kingravi, Patricio A. Vela category:cs.LG cs.CV I.5.3; H.2.8 published:2012-09-10 summary:K-means is undoubtedly the most widely used partitional clustering algorithm.Unfortunately, due to its gradient descent nature, this algorithm is highlysensitive to the initial placement of the cluster centers. Numerousinitialization methods have been proposed to address this problem. In thispaper, we first present an overview of these methods with an emphasis on theircomputational efficiency. We then compare eight commonly used linear timecomplexity initialization methods on a large and diverse collection of datasets using various performance criteria. Finally, we analyze the experimentalresults using non-parametric statistical tests and provide recommendations forpractitioners. We demonstrate that popular initialization methods often performpoorly and that there are in fact strong alternatives to these methods.
arxiv-1800-225 | Group descent algorithms for nonconvex penalized linear and logistic regression models with grouped predictors | http://arxiv.org/abs/1209.2160 | author:Patrick Breheny, Jian Huang category:stat.CO stat.ML published:2012-09-10 summary:Penalized regression is an attractive framework for variable selectionproblems. Often, variables possess a grouping structure, and the relevantselection problem is that of selecting groups, not individual variables. Thegroup lasso has been proposed as a way of extending the ideas of the lasso tothe problem of group selection. Nonconvex penalties such as SCAD and MCP havebeen proposed and shown to have several advantages over the lasso; thesepenalties may also be extended to the group selection problem, giving rise togroup SCAD and group MCP methods. Here, we describe algorithms for fittingthese models stably and efficiently. In addition, we present simulation resultsand real data examples comparing and contrasting the statistical properties ofthese methods.
arxiv-1800-226 | Blind Image Deblurring by Spectral Properties of Convolution Operators | http://arxiv.org/abs/1209.2082 | author:Guangcan Liu, Shiyu Chang, Yi Ma category:cs.CV published:2012-09-10 summary:In this paper, we study the problem of recovering a sharp version of a givenblurry image when the blur kernel is unknown. Previous methods often introducean image-independent regularizer (such as Gaussian or sparse priors) on thedesired blur kernel. We shall show that the blurry image itself encodes richinformation about the blur kernel. Such information can be found throughanalyzing and comparing how the spectrum of an image as a convolution operatorchanges before and after blurring. Our analysis leads to an effective convexregularizer on the blur kernel which depends only on the given blurry image. Weshow that the minimizer of this regularizer guarantees to give goodapproximation to the blur kernel if the original image is sharp enough. Bycombining this powerful regularizer with conventional image deblurringtechniques, we show how we could significantly improve the deblurring resultsthrough simulations and experiments on real images. In addition, our analysisand experiments help explaining a widely accepted doctrine; that is, the edgesare good features for deblurring.
arxiv-1800-227 | Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization | http://arxiv.org/abs/1209.1873 | author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG math.OC published:2012-09-10 summary:Stochastic Gradient Descent (SGD) has become popular for solving large scalesupervised machine learning optimization problems such as SVM, due to theirstrong theoretical guarantees. While the closely related Dual Coordinate Ascent(DCA) method has been implemented in various software packages, it has so farlacked good convergence analysis. This paper presents a new analysis ofStochastic Dual Coordinate Ascent (SDCA) showing that this class of methodsenjoy strong theoretical guarantees that are comparable or better than SGD.This analysis justifies the effectiveness of SDCA for practical applications.
arxiv-1800-228 | Fused Multiple Graphical Lasso | http://arxiv.org/abs/1209.2139 | author:Sen Yang, Zhaosong Lu, Xiaotong Shen, Peter Wonka, Jieping Ye category:cs.LG stat.ML published:2012-09-10 summary:In this paper, we consider the problem of estimating multiple graphicalmodels simultaneously using the fused lasso penalty, which encourages adjacentgraphs to share similar structures. A motivating example is the analysis ofbrain networks of Alzheimer's disease using neuroimaging data. Specifically, wemay wish to estimate a brain network for the normal controls (NC), a brainnetwork for the patients with mild cognitive impairment (MCI), and a brainnetwork for Alzheimer's patients (AD). We expect the two brain networks for NCand MCI to share common structures but not to be identical to each other;similarly for the two brain networks for MCI and AD. The proposed formulationcan be solved using a second-order method. Our key technical contribution is toestablish the necessary and sufficient condition for the graphs to bedecomposable. Based on this key property, a simple screening rule is presented,which decomposes the large graphs into small subgraphs and allows an efficientestimation of multiple independent (small) subgraphs, dramatically reducing thecomputational cost. We perform experiments on both synthetic and real data; ourresults demonstrate the effectiveness and efficiency of the proposed approach.
arxiv-1800-229 | Securing Your Transactions: Detecting Anomalous Patterns In XML Documents | http://arxiv.org/abs/1209.1797 | author:Eitan Menahem, Alon Schclar, Lior Rokach, Yuval Elovici category:cs.CR cs.LG published:2012-09-09 summary:XML transactions are used in many information systems to store data andinteract with other systems. Abnormal transactions, the result of either anon-going cyber attack or the actions of a benign user, can potentially harm theinteracting systems and therefore they are regarded as a threat. In this paperwe address the problem of anomaly detection and localization in XMLtransactions using machine learning techniques. We present a new XML anomalydetection framework, XML-AD. Within this framework, an automatic method forextracting features from XML transactions was developed as well as a practicalmethod for transforming XML features into vectors of fixed dimensionality. Withthese two methods in place, the XML-AD framework makes it possible to utilizegeneral learning algorithms for anomaly detection. Central to the functioningof the framework is a novel multi-univariate anomaly detection algorithm,ADIFA. The framework was evaluated on four XML transactions datasets, capturedfrom real information systems, in which it achieved over 89% true positivedetection rate with less than a 0.2% false positive rate.
arxiv-1800-230 | A spatio-spectral hybridization for edge preservation and noisy image restoration via local parametric mixtures and Lagrangian relaxation | http://arxiv.org/abs/1209.1826 | author:Kinjal Basu, Debapriya Sengupta category:stat.ME cs.CV stat.AP published:2012-09-09 summary:This paper investigates a fully unsupervised statistical method for edgepreserving image restoration and compression using a spatial decompositionscheme. Smoothed maximum likelihood is used for local estimation of edge pixelsfrom mixture parametric models of local templates. For the complementary smoothpart the traditional L2-variational problem is solved in the Fourier domainwith Thin Plate Spline (TPS) regularization. It is well known that naiveFourier compression of the whole image fails to restore a piece-wise smoothnoisy image satisfactorily due to Gibbs phenomenon. Images are interpreted asrelative frequency histograms of samples from bi-variate densities where thesample sizes might be unknown. The set of discontinuities is assumed to becompletely unsupervised Lebesgue-null, compact subset of the plane in thecontinuous formulation of the problem. Proposed spatial decomposition uses awidely used topological concept, partition of unity. The decision on edge pixelneighborhoods are made based on the multiple testing procedure of Holms.Statistical summary of the ?final output is decomposed into two layers ofinformation extraction, one for the subset of edge pixels and the other for thesmooth region. Robustness is also demonstrated by applying the technique onnoisy degradation of clean images.
arxiv-1800-231 | On the Use of Lee's Protocol for Speckle-Reducing Techniques | http://arxiv.org/abs/1209.1788 | author:Elsa E. Moschetti, M. Gabriela Palacio, Mery Picco, Oscar H. Bustos, Alejandro C. Frery category:cs.CV published:2012-09-09 summary:This paper presents two new MAP (Maximum a Posteriori) filters for specklenoise reduction and a Monte Carlo procedure for the assessment of theirperformance. In order to quantitatively evaluate the results obtained usingthese new filters, with respect to classical ones, a Monte Carlo extension ofLee's protocol is proposed. This extension of the protocol shows that itsoriginal version leads to inconsistencies that hamper its use as a generalprocedure for filter assessment. Some solutions for these inconsistencies areproposed, and a consistent comparison of speckle-reducing filters is provided.
arxiv-1800-232 | An Empirical Study of MAUC in Multi-class Problems with Uncertain Cost Matrices | http://arxiv.org/abs/1209.1800 | author:Rui Wang, Ke Tang category:cs.LG published:2012-09-09 summary:Cost-sensitive learning relies on the availability of a known and fixed costmatrix. However, in some scenarios, the cost matrix is uncertain duringtraining, and re-train a classifier after the cost matrix is specified wouldnot be an option. For binary classification, this issue can be successfullyaddressed by methods maximizing the Area Under the ROC Curve (AUC) metric.Since the AUC can measure performance of base classifiers independent of costduring training, and a larger AUC is more likely to lead to a smaller totalcost in testing using the threshold moving method. As an extension of AUC tomulti-class problems, MAUC has attracted lots of attentions and been widelyused. Although MAUC also measures performance of base classifiers independentof cost, it is unclear whether a larger MAUC of classifiers is more likely tolead to a smaller total cost. In fact, it is also unclear what kinds ofpost-processing methods should be used in multi-class problems to convert baseclassifiers into discrete classifiers such that the total cost is as small aspossible. In the paper, we empirically explore the relationship between MAUCand the total cost of classifiers by applying two categories of post-processingmethods. Our results suggest that a larger MAUC is also beneficial.Interestingly, simple calibration methods that convert the output matrix intoposterior probabilities perform better than existing sophisticated postre-optimization methods.
arxiv-1800-233 | Rank Centrality: Ranking from Pair-wise Comparisons | http://arxiv.org/abs/1209.1688 | author:Sahand Negahban, Sewoong Oh, Devavrat Shah category:cs.LG stat.ML published:2012-09-08 summary:The question of aggregating pair-wise comparisons to obtain a global rankingover a collection of objects has been of interest for a very long time: be itranking of online gamers (e.g. MSR's TrueSkill system) and chess players,aggregating social opinions, or deciding which product to sell based ontransactions. In most settings, in addition to obtaining a ranking, finding`scores' for each object (e.g. player's rating) is of interest forunderstanding the intensity of the preferences. In this paper, we propose Rank Centrality, an iterative rank aggregationalgorithm for discovering scores for objects (or items) from pair-wisecomparisons. The algorithm has a natural random walk interpretation over thegraph of objects with an edge present between a pair of objects if they arecompared; the score, which we call Rank Centrality, of an object turns out tobe its stationary probability under this random walk. To study the efficacy ofthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in whicheach object has an associated score which determines the probabilistic outcomesof pair-wise comparisons between objects. In terms of the pair-wise marginalprobabilities, which is the main subject of this paper, the MNL model and theBTL model are identical. We bound the finite sample error rates between thescores assumed by the BTL model and those estimated by our algorithm. Inparticular, the number of samples required to learn the score well with highprobability depends on the structure of the comparison graph. When theLaplacian of the comparison graph has a strictly positive spectral gap, e.g.each item is compared to a subset of randomly chosen items, this leads todependence on the number of samples that is nearly order-optimal.
arxiv-1800-234 | Load Distribution Composite Design Pattern for Genetic Algorithm-Based Autonomic Computing Systems | http://arxiv.org/abs/1209.1734 | author:Vishnuvardhan Mannava, T. Ramesh category:cs.SE cs.DC cs.NE published:2012-09-08 summary:Current autonomic computing systems are ad hoc solutions that are designedand implemented from the scratch. When designing software, in most cases two ormore patterns are to be composed to solve a bigger problem. A composite designpatterns shows a synergy that makes the composition more than just the sum ofits parts which leads to ready-made software architectures. As far as we know,there are no studies on composition of design patterns for autonomic computingdomain. In this paper we propose pattern-oriented software architecture forself-optimization in autonomic computing system using design patternscomposition and multi objective evolutionary algorithms that software designersand/or programmers can exploit to drive their work. Main objective of thesystem is to reduce the load in the server by distributing the population toclients. We used Case Based Reasoning, Database Access, and Master Slave designpatterns. We evaluate the effectiveness of our architecture with and withoutdesign patterns compositions. The use of composite design patterns in thearchitecture and quantitative measurements are presented. A simple UML classdiagram is used to describe the architecture.
arxiv-1800-235 | Difference of Normals as a Multi-Scale Operator in Unorganized Point Clouds | http://arxiv.org/abs/1209.1759 | author:Yani Ioannou, Babak Taati, Robin Harrap, Michael Greenspan category:cs.CV published:2012-09-08 summary:A novel multi-scale operator for unorganized 3D point clouds is introduced.The Difference of Normals (DoN) provides a computationally efficient,multi-scale approach to processing large unorganized 3D point clouds. Theapplication of DoN in the multi-scale filtering of two different real-worldoutdoor urban LIDAR scene datasets is quantitatively and qualitativelydemonstrated. In both datasets the DoN operator is shown to segment large 3Dpoint clouds into scale-salient clusters, such as cars, people, and lamp poststowards applications in semi-automatic annotation, and as a pre-processing stepin automatic object recognition. The application of the operator tosegmentation is evaluated on a large public dataset of outdoor LIDAR sceneswith ground truth annotations.
arxiv-1800-236 | Design of Spectrum Sensing Policy for Multi-user Multi-band Cognitive Radio Network | http://arxiv.org/abs/1209.1739 | author:Jan Oksanen, Jarmo Lundén, Visa Koivunen category:cs.LG cs.NI published:2012-09-08 summary:Finding an optimal sensing policy for a particular access policy and sensingscheme is a laborious combinatorial problem that requires the system modelparameters to be known. In practise the parameters or the model itself may notbe completely known making reinforcement learning methods appealing. In thispaper a non-parametric reinforcement learning-based method is developed forsensing and accessing multi-band radio spectrum in multi-user cognitive radionetworks. A suboptimal sensing policy search algorithm is proposed for aparticular multi-user multi-band access policy and the randomizedChair-Varshney rule. The randomized Chair-Varshney rule is used to reduce theprobability of false alarms under a constraint on the probability of detectionthat protects the primary user. The simulation results show that the proposedmethod achieves a sum profit (e.g. data rate) close to the optimal sensingpolicy while achieving the desired probability of detection.
arxiv-1800-237 | Information content versus word length in random typing | http://arxiv.org/abs/1209.1751 | author:Ramon Ferrer-i-Cancho, Fermín Moscoso del Prado Martín category:cs.CL published:2012-09-08 summary:Recently, it has been claimed that a linear relationship between a measure ofinformation content and word length is expected from word length optimizationand it has been shown that this linearity is supported by a strong correlationbetween information content and word length in many languages (Piantadosi etal. 2011, PNAS 108, 3825-3826). Here, we study in detail some connectionsbetween this measure and standard information theory. The relationship betweenthe measure and word length is studied for the popular random typing processwhere a text is constructed by pressing keys at random from a keyboardcontaining letters and a space behaving as a word delimiter. Although thisrandom process does not optimize word lengths according to information content,it exhibits a linear relationship between information content and word length.The exact slope and intercept are presented for three major variants of therandom typing process. A strong correlation between information content andword length can simply arise from the units making a word (e.g., letters) andnot necessarily from the interplay between a word and its context as proposedby Piantadosi et al. In itself, the linear relation does not entail the resultsof any optimization process.
arxiv-1800-238 | Bandits with heavy tail | http://arxiv.org/abs/1209.1727 | author:Sébastien Bubeck, Nicolò Cesa-Bianchi, Gábor Lugosi category:stat.ML cs.LG published:2012-09-08 summary:The stochastic multi-armed bandit problem is well understood when the rewarddistributions are sub-Gaussian. In this paper we examine the bandit problemunder the weaker assumption that the distributions have moments of order1+\epsilon, for some $\epsilon \in (0,1]$. Surprisingly, moments of order 2(i.e., finite variance) are sufficient to obtain regret bounds of the sameorder as under sub-Gaussian reward distributions. In order to achieve suchregret, we define sampling strategies based on refined estimators of the meansuch as the truncated empirical mean, Catoni's M-estimator, and themedian-of-means estimator. We also derive matching lower bounds that also showthat the best achievable regret deteriorates when \epsilon <1.
arxiv-1800-239 | Wavelet Based QRS Complex Detection of ECG Signal | http://arxiv.org/abs/1209.1563 | author:Sayantan Mukhopadhyay, Shouvik Biswas, Anamitra Bardhan Roy, Nilanjan Dey category:cs.CV published:2012-09-07 summary:The Electrocardiogram (ECG) is a sensitive diagnostic tool that is used todetect various cardiovascular diseases by measuring and recording theelectrical activity of the heart in exquisite detail. A wide range of heartcondition is determined by thorough examination of the features of the ECGreport. Automatic extraction of time plane features is important foridentification of vital cardiac diseases. This paper presents amulti-resolution wavelet transform based system for detection 'P', 'Q', 'R','S', 'T' peaks complex from original ECG signal. 'R-R' time lapse is animportant minutia of the ECG signal that corresponds to the heartbeat of theconcerned person. Abrupt increase in height of the 'R' wave or changes in themeasurement of the 'R-R' denote various anomalies of human heart. Similarly'P-P', 'Q-Q', 'S-S', 'T-T' also corresponds to different anomalies of heart andtheir peak amplitude also envisages other cardiac diseases. In this proposedmethod the 'PQRST' peaks are marked and stored over the entire signal and thetime interval between two consecutive 'R' peaks and other peaks interval aremeasured to detect anomalies in behavior of heart, if any. The peaks areachieved by the composition of Daubeheissub bands wavelet of original ECGsignal. The accuracy of the 'PQRST' complex detection and interval measurementis achieved up to 100% with high exactitude by processing and thresholding theoriginal ECG signal.
arxiv-1800-240 | A Comparative Study between Moravec and Harris Corner Detection of Noisy Images Using Adaptive Wavelet Thresholding Technique | http://arxiv.org/abs/1209.1558 | author:Nilanjan Dey, Pradipti Nandi, Nilanjana Barman, Debolina Das, Subhabrata Chakraborty category:cs.CV published:2012-09-07 summary:In this paper a comparative study between Moravec and Harris Corner Detectionhas been done for obtaining features required to track and recognize objectswithin a noisy image. Corner detection of noisy images is a challenging task inimage processing. Natural images often get corrupted by noise duringacquisition and transmission. As Corner detection of these noisy images doesnot provide desired results, hence de-noising is required. Adaptive waveletthresholding approach is applied for the same.
arxiv-1800-241 | On spatial selectivity and prediction across conditions with fMRI | http://arxiv.org/abs/1209.1450 | author:Yannick Schwartz, Gaël Varoquaux, Bertrand Thirion category:stat.ML cs.LG published:2012-09-07 summary:Researchers in functional neuroimaging mostly use activation coordinates toformulate their hypotheses. Instead, we propose to use the full statisticalimages to define regions of interest (ROIs). This paper presents two machinelearning approaches, transfer learning and selection transfer, that arecompared upon their ability to identify the common patterns between brainactivation maps related to two functional tasks. We provide some preliminaryquantification of these similarities, and show that selection transfer makes itpossible to set a spatial scale yielding ROIs that are more specific to thecontext of interest than with transfer learning. In particular, selectiontransfer outlines well known regions such as the Visual Word Form Area whendiscriminating between different visual tasks.
arxiv-1800-242 | Learning Model-Based Sparsity via Projected Gradient Descent | http://arxiv.org/abs/1209.1557 | author:Sohail Bahmani, Petros T. Boufounos, Bhiksha Raj category:stat.ML cs.LG math.OC 62FXX, 65KXX published:2012-09-07 summary:Several convex formulation methods have been proposed previously forstatistical estimation with structured sparsity as the prior. These methodsoften require a carefully tuned regularization parameter, often a cumbersome orheuristic exercise. Furthermore, the estimate that these methods produce mightnot belong to the desired sparsity model, albeit accurately approximating thetrue parameter. Therefore, greedy-type algorithms could often be more desirablein estimating structured-sparse parameters. So far, these greedy methods havemostly focused on linear statistical models. In this paper we study theprojected gradient descent with non-convex structured-sparse parameter model asthe constraint set. Should the cost function have a Stable Model-RestrictedHessian the algorithm produces an approximation for the desired minimizer. Asan example we elaborate on application of the main results to estimation inGeneralized Linear Model.
arxiv-1800-243 | Wavelet Based Normal and Abnormal Heart Sound Identification using Spectrogram Analysis | http://arxiv.org/abs/1209.1224 | author:Nilanjan Dey, Achintya Das, Sheli Sinha Chaudhuri category:cs.CV published:2012-09-06 summary:The present work proposes a computer-aided normal and abnormal heart soundidentification based on Discrete Wavelet Transform (DWT), it being useful fortele-diagnosis of heart diseases. Due to the presence of Cumulative Frequencycomponents in the spectrogram, DWT is applied on the spectro-gram up to n levelto extract the features from the individual approximation components. Onedimensional feature vector is obtained by evaluating the Row Mean of theapproximation components of these spectrograms. For this present approach, theset of spectrograms has been considered as the database, rather than raw soundsamples. Minimum Euclidean distance is computed between feature vector of thetest sample and the feature vectors of the stored samples to identify the heartsound. By applying this algorithm, almost 82% of accuracy was achieved.
arxiv-1800-244 | FCM Based Blood Vessel Segmentation Method for Retinal Images | http://arxiv.org/abs/1209.1181 | author:Nilanjan Dey, Anamitra Bardhan Roy, Moumita Pal, Achintya Das category:cs.CV published:2012-09-06 summary:Segmentation of blood vessels in retinal images provides early diagnosis ofdiseases like glaucoma, diabetic retinopathy and macular degeneration. Amongthese diseases occurrence of Glaucoma is most frequent and has serious ocularconsequences that can even lead to blindness, if it is not detected early. Theclinical criteria for the diagnosis of glaucoma include intraocular pressuremeasurement, optic nerve head evaluation, retinal nerve fiber layer and visualfield defects. This form of blood vessel segmentation helps in early detectionfor ophthalmic diseases, and potentially reduces the risk of blindness. Thelow-contrast images at the retina owing to narrow blood vessels of the retinaare difficult to extract. These low contrast images are, however useful inrevealing certain systemic diseases. Motivated by the goals of improvingdetection of such vessels, this present work proposes an algorithm forsegmentation of blood vessels and compares the results between expertophthalmologist hand-drawn ground-truths and segmented image(i.e. the output ofthe present work).Sensitivity, specificity, positive predictive value (PPV),positive likelihood ratio (PLR) and accuracy are used to evaluate overallperformance.It is found that this work segments blood vessels successfully withsensitivity, specificity, PPV, PLR and accuracy of 99.62%, 54.66%, 95.08%,219.72 and 95.03%, respectively.
arxiv-1800-245 | Solving Support Vector Machines in Reproducing Kernel Banach Spaces with Positive Definite Functions | http://arxiv.org/abs/1209.1171 | author:Gregory E. Fasshauer, Fred J. Hickernell, Qi Ye category:stat.ML math.NA math.OC published:2012-09-06 summary:In this paper we solve support vector machines in reproducing kernel Banachspaces with reproducing kernels defined on nonsymmetric domains instead of thetraditional methods in reproducing kernel Hilbert spaces. Using theorthogonality of semi-inner-products, we can obtain the explicitrepresentations of the dual (normalized-duality-mapping) elements of supportvector machine solutions. In addition, we can introduce the reproductionproperty in a generalized native space by Fourier transform techniques suchthat it becomes a reproducing kernel Banach space, which can be even embeddedinto Sobolev spaces, and its reproducing kernel is set up by the relatedpositive definite function. The representations of the optimal solutions ofsupport vector machines (regularized empirical risks) in these reproducingkernel Banach spaces are formulated explicitly in terms of positive definitefunctions, and their finite numbers of coefficients can be computed by fixedpoint iteration. We also give some typical examples of reproducing kernelBanach spaces induced by Mat\'ern functions (Sobolev splines) so that theirsupport vector machine solutions are well computable as the classicalalgorithms. Moreover, each of their reproducing bases includes information frommultiple training data points. The concept of reproducing kernel Banach spacesoffers us a new numerical tool for solving support vector machines.
arxiv-1800-246 | Multiclass Learning with Simplex Coding | http://arxiv.org/abs/1209.1360 | author:Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, Jean-Jacques Slotine category:stat.ML cs.LG published:2012-09-06 summary:In this paper we discuss a novel framework for multiclass learning, definedby a suitable coding/decoding strategy, namely the simplex coding, that allowsto generalize to multiple classes a relaxation approach commonly used in binaryclassification. In this framework, a relaxation error analysis can be developedavoiding constraints on the considered hypotheses class. Moreover, we show thatin this setting it is possible to derive the first provably consistentregularized method with training/tuning complexity which is independent to thenumber of classes. Tools from convex analysis are introduced that can be usedbeyond the scope of this paper.
arxiv-1800-247 | The Sample Complexity of Search over Multiple Populations | http://arxiv.org/abs/1209.1380 | author:Matthew L. Malloy, Gongguo Tang, Robert D. Nowak category:cs.IT math.IT stat.ML published:2012-09-06 summary:This paper studies the sample complexity of searching over multiplepopulations. We consider a large number of populations, each corresponding toeither distribution P0 or P1. The goal of the search problem studied here is tofind one population corresponding to distribution P1 with as few samples aspossible. The main contribution is to quantify the number of samples needed tocorrectly find one such population. We consider two general approaches:non-adaptive sampling methods, which sample each population a predeterminednumber of times until a population following P1 is found, and adaptive samplingmethods, which employ sequential sampling schemes for each population. We firstderive a lower bound on the number of samples required by any sampling scheme.We then consider an adaptive procedure consisting of a series of sequentialprobability ratio tests, and show it comes within a constant factor of thelower bound. We give explicit expressions for this constant when samples of thepopulations follow Gaussian and Bernoulli distributions. An alternativeadaptive scheme is discussed which does not require full knowledge of P1, andcomes within a constant factor of the optimal scheme. For comparison, a lowerbound on the sampling requirements of any non-adaptive scheme is presented.
arxiv-1800-248 | The Annealing Sparse Bayesian Learning Algorithm | http://arxiv.org/abs/1209.1033 | author:Benyuan Liu, Hongqi Fan, Zaiqi Lu, Qiang Fu category:cs.IT cs.LG math.IT published:2012-09-05 summary:In this paper we propose a two-level hierarchical Bayesian model and anannealing schedule to re-enable the noise variance learning capability of thefast marginalized Sparse Bayesian Learning Algorithms. The performance such asNMSE and F-measure can be greatly improved due to the annealing technique. Thisalgorithm tends to produce the most sparse solution under moderate SNRscenarios and can outperform most concurrent SBL algorithms while pertainssmall computational load.
arxiv-1800-249 | Constructing the L2-Graph for Robust Subspace Learning and Subspace Clustering | http://arxiv.org/abs/1209.0841 | author:Xi Peng, Zhiding Yu, Huajin Tang, Zhang Yi category:cs.CV cs.MM published:2012-09-05 summary:Under the framework of graph-based learning, the key to robust subspaceclustering and subspace learning is to obtain a good similarity graph thateliminates the effects of errors and retains only connections between the datapoints from the same subspace (i.e., intra-subspace data points). Recent worksachieve good performance by modeling errors into their objective functions toremove the errors from the inputs. However, these approaches face thelimitations that the structure of errors should be known prior and a complexconvex problem must be solved. In this paper, we present a novel method toeliminate the effects of the errors from the projection space (representation)rather than from the input space. We first prove that $\ell_1$-, $\ell_2$-,$\ell_{\infty}$-, and nuclear-norm based linear projection spaces share theproperty of Intra-subspace Projection Dominance (IPD), i.e., the coefficientsover intra-subspace data points are larger than those over inter-subspace datapoints. Based on this property, we introduce a method to construct a sparsesimilarity graph, called L2-Graph. The subspace clustering and subspacelearning algorithms are developed upon L2-Graph. Experiments show that L2-Graphalgorithms outperform the state-of-the-art methods for feature extraction,image clustering, and motion segmentation in terms of accuracy, robustness, andtime efficiency.
arxiv-1800-250 | Structuring Relevant Feature Sets with Multiple Model Learning | http://arxiv.org/abs/1209.0913 | author:Jun Wang, Alexandros Kalousis category:cs.LG published:2012-09-05 summary:Feature selection is one of the most prominent learning tasks, especially inhigh-dimensional datasets in which the goal is to understand the mechanismsthat underly the learning dataset. However most of them typically deliver justa flat set of relevant features and provide no further information on what kindof structures, e.g. feature groupings, might underly the set of relevantfeatures. In this paper we propose a new learning paradigm in which our goal isto uncover the structures that underly the set of relevant features for a givenlearning problem. We uncover two types of features sets, non-replaceablefeatures that contain important information about the target variable andcannot be replaced by other features, and functionally similar features setsthat can be used interchangeably in learned models, given the presence of thenon-replaceable features, with no change in the predictive performance. To doso we propose a new learning algorithm that learns a number of disjoint modelsusing a model disjointness regularization constraint together with a constrainton the predictive agreement of the disjoint models. We explore the behavior ofour approach on a number of high-dimensional datasets, and show that, asexpected by their construction, these satisfy a number of properties. Namely,model disjointness, a high predictive agreement, and a similar predictiveperformance to models learned on the full set of relevant features. The abilityto structure the set of relevant features in such a manner can become avaluable tool in different applications of scientific knowledge discovery.
arxiv-1800-251 | Video Data Visualization System: Semantic Classification And Personalization | http://arxiv.org/abs/1209.1125 | author:Jamel Slimi, Anis Ben Ammar, Adel M. Alimi category:cs.IR cs.CV cs.MM published:2012-09-05 summary:We present in this paper an intelligent video data visualization tool, basedon semantic classification, for retrieving and exploring a large scale corpusof videos. Our work is based on semantic classification resulting from semanticanalysis of video. The obtained classes will be projected in the visualizationspace. The graph is represented by nodes and edges, the nodes are the keyframesof video documents and the edges are the relation between documents and theclasses of documents. Finally, we construct the user's profile, based on theinteraction with the system, to render the system more adequate to itsreferences.
arxiv-1800-252 | Learning Manifolds with K-Means and K-Flats | http://arxiv.org/abs/1209.1121 | author:Guillermo D. Canas, Tomaso Poggio, Lorenzo Rosasco category:cs.LG stat.ML K.3.2 published:2012-09-05 summary:We study the problem of estimating a manifold from random samples. Inparticular, we consider piecewise constant and piecewise linear estimatorsinduced by k-means and k-flats, and analyze their performance. We extendprevious results for k-means in two separate directions. First, we provide newresults for k-means reconstruction on manifolds and, secondly, we provereconstruction bounds for higher-order approximation (k-flats), for which noknown results were previously available. While the results for k-means arenovel, some of the technical tools are well-established in the literature. Inthe case of k-flats, both the results and the mathematical tools are new.
arxiv-1800-253 | Visual Exploration of Simulated and Measured Blood Flow | http://arxiv.org/abs/1209.0999 | author:Anna Vilanova, Bernhard Preim, Roy van Pelt, Rocco Gasteiger, Mathias Neugebauer, Thomas Wischgoll category:cs.GR cs.CV published:2012-09-05 summary:Morphology of cardiovascular tissue is influenced by the unsteady behavior ofthe blood flow and vice versa. Therefore, the pathogenesis of severalcardiovascular diseases is directly affected by the blood-flow dynamics.Understanding flow behavior is of vital importance to understand thecardiovascular system and potentially harbors a considerable value for bothdiagnosis and risk assessment. The analysis of hemodynamic characteristicsinvolves qualitative and quantitative inspection of the blood-flow field.Visualization plays an important role in the qualitative exploration, as wellas the definition of relevant quantitative measures and its validation. Thereare two main approaches to obtain information about the blood flow: simulationby computational fluid dynamics, and in-vivo measurements. Although research onblood flow simulation has been performed for decades, many open problems remainconcerning accuracy and patient-specific solutions. Possibilities for realmeasurement of blood flow have recently increased considerably by newdevelopments in magnetic resonance imaging which enable the acquisition of 3Dquantitative measurements of blood-flow velocity fields. This chapter presentsthe visualization challenges for both simulation and real measurements ofunsteady blood-flow fields.
arxiv-1800-254 | Restricting exchangeable nonparametric distributions | http://arxiv.org/abs/1209.1145 | author:Sinead Williamson, Zoubin Ghahramani, Steven N. MacEachern, Eric P. Xing category:stat.ME stat.ML published:2012-09-05 summary:Distributions over exchangeable matrices with infinitely many columns, suchas the Indian buffet process, are useful in constructing nonparametric latentvariable models. However, the distribution implied by such models over thenumber of features exhibited by each data point may be poorly- suited for manymodeling tasks. In this paper, we propose a class of exchangeable nonparametricpriors obtained by restricting the domain of existing models. Such models allowus to specify the distribution over the number of features per data point, andcan achieve better performance on data sets where the number of features is notwell-modeled by the original distribution.
arxiv-1800-255 | A Max-Product EM Algorithm for Reconstructing Markov-tree Sparse Signals from Compressive Samples | http://arxiv.org/abs/1209.1064 | author:Zhao Song, Aleksandar Dogandzic category:stat.ML cs.IT math.IT published:2012-09-05 summary:We propose a Bayesian expectation-maximization (EM) algorithm forreconstructing Markov-tree sparse signals via belief propagation. Themeasurements follow an underdetermined linear model where theregression-coefficient vector is the sum of an unknown approximately sparsesignal and a zero-mean white Gaussian noise with an unknown variance. Thesignal is composed of large- and small-magnitude components identified bybinary state variables whose probabilistic dependence structure is described bya Markov tree. Gaussian priors are assigned to the signal coefficients giventheir state variables and the Jeffreys' noninformative prior is assigned to thenoise variance. Our signal reconstruction scheme is based on an EM iterationthat aims at maximizing the posterior distribution of the signal and its statevariables given the noise variance. We construct the missing data for the EMiteration so that the complete-data posterior distribution corresponds to ahidden Markov tree (HMT) probabilistic graphical model that contains no loopsand implement its maximization (M) step via a max-product algorithm. This EMalgorithm estimates the vector of state variables as well as solves iterativelya linear system of equations to obtain the corresponding signal estimate. Weselect the noise variance so that the corresponding estimated signal and statevariables obtained upon convergence of the EM iteration have the largestmarginal posterior distribution. We compare the proposed and existingstate-of-the-art reconstruction methods via signal and image reconstructionexperiments.
arxiv-1800-256 | Improving the K-means algorithm using improved downhill simplex search | http://arxiv.org/abs/1209.0853 | author:Ehsan Saboori, Shafigh Parsazad, Anoosheh Sadeghi category:cs.LG published:2012-09-05 summary:The k-means algorithm is one of the well-known and most popular clusteringalgorithms. K-means seeks an optimal partition of the data by minimizing thesum of squared error with an iterative optimization procedure, which belongs tothe category of hill climbing algorithms. As we know hill climbing searches arefamous for converging to local optimums. Since k-means can converge to a localoptimum, different initial points generally lead to different convergencecancroids, which makes it important to start with a reasonable initialpartition in order to achieve high quality clustering solutions. However, intheory, there exist no efficient and universal methods for determining suchinitial partitions. In this paper we tried to find an optimum initialpartitioning for k-means algorithm. To achieve this goal we proposed a newimproved version of downhill simplex search, and then we used it in order tofind an optimal result for clustering approach and then compare this algorithmwith Genetic Algorithm base (GA), Genetic K-Means (GKM), Improved GeneticK-Means (IGKM) and k-means algorithms.
arxiv-1800-257 | Learning Probability Measures with respect to Optimal Transport Metrics | http://arxiv.org/abs/1209.1077 | author:Guillermo D. Canas, Lorenzo Rosasco category:cs.LG stat.ML K.3.2 published:2012-09-05 summary:We study the problem of estimating, in the sense of optimal transportmetrics, a measure which is assumed supported on a manifold embedded in aHilbert space. By establishing a precise connection between optimal transportmetrics, optimal quantization, and learning theory, we derive new probabilisticbounds for the performance of a classic algorithm in unsupervised learning(k-means), when used to produce a probability measure derived from the data. Inthe course of the analysis, we arrive at new lower bounds, as well asprobabilistic upper bounds on the convergence rate of the empirical law oflarge numbers, which, unlike existing bounds, are applicable to a wide class ofmeasures.
arxiv-1800-258 | Multiresolution Gaussian Processes | http://arxiv.org/abs/1209.0833 | author:Emily B. Fox, David B. Dunson category:stat.ME stat.ML published:2012-09-05 summary:We propose a multiresolution Gaussian process to capture long-range,non-Markovian dependencies while allowing for abrupt changes. Themultiresolution GP hierarchically couples a collection of smooth GPs, eachdefined over an element of a random nested partition. Long-range dependenciesare captured by the top-level GP while the partition points define the abruptchanges. Due to the inherent conjugacy of the GPs, one can analyticallymarginalize the GPs and compute the conditional likelihood of the observationsgiven the partition tree. This property allows for efficient inference of thepartition itself, for which we employ graph-theoretic techniques. We apply themultiresolution GP to the analysis of Magnetoencephalography (MEG) recordingsof brain activity.
arxiv-1800-259 | Robustness and Generalization for Metric Learning | http://arxiv.org/abs/1209.1086 | author:Aurélien Bellet, Amaury Habrard category:cs.LG stat.ML published:2012-09-05 summary:Metric learning has attracted a lot of interest over the last decade, but thegeneralization ability of such methods has not been thoroughly studied. In thispaper, we introduce an adaptation of the notion of algorithmic robustness(previously introduced by Xu and Mannor) that can be used to derivegeneralization bounds for metric learning. We further show that a weak notionof robustness is in fact a necessary and sufficient condition for a metriclearning algorithm to generalize. To illustrate the applicability of theproposed framework, we derive generalization results for a large family ofexisting metric learning algorithms, including some sparse formulations thatare not covered by previous results.
arxiv-1800-260 | Augment-and-Conquer Negative Binomial Processes | http://arxiv.org/abs/1209.1119 | author:Mingyuan Zhou, Lawrence Carin category:stat.ML stat.ME published:2012-09-05 summary:By developing data augmentation methods unique to the negative binomial (NB)distribution, we unite seemingly disjoint count and mixture models under the NBprocess framework. We develop fundamental properties of the models and deriveefficient Gibbs sampling inference. We show that the gamma-NB process can bereduced to the hierarchical Dirichlet process with normalization, highlightingits unique theoretical, structural and computational advantages. A variety ofNB processes with distinct sharing mechanisms are constructed and applied totopic modeling, with connections to existing algorithms, showing the importanceof inferring both the NB dispersion and probability parameters.
arxiv-1800-261 | Compressive Optical Deflectometric Tomography: A Constrained Total-Variation Minimization Approach | http://arxiv.org/abs/1209.0654 | author:Adriana Gonzalez, Laurent Jacques, Christophe De Vleeschouwer, Philippe Antoine category:cs.CV math.OC published:2012-09-04 summary:Optical Deflectometric Tomography (ODT) provides an accurate characterizationof transparent materials whose complex surfaces present a real challenge formanufacture and control. In ODT, the refractive index map (RIM) of atransparent object is reconstructed by measuring light deflection undermultiple orientations. We show that this imaging modality can be made"compressive", i.e., a correct RIM reconstruction is achievable with far lessobservations than required by traditional Filtered Back Projection (FBP)methods. Assuming a cartoon-shape RIM model, this reconstruction is driven byminimizing the map Total-Variation under a fidelity constraint with theavailable observations. Moreover, two other realistic assumptions are added toimprove the stability of our approach: the map positivity and a frontiercondition. Numerically, our method relies on an accurate ODT sensing model andon a primal-dual minimization scheme, including easily the sensing operator andthe proposed RIM constraints. We conclude this paper by demonstrating the powerof our method on synthetic and experimental data under various compressivescenarios. In particular, the compressiveness of the stabilized ODT problem isdemonstrated by observing a typical gain of 20 dB compared to FBP at only 5% of360 incident light angles for moderately noisy sensing.
arxiv-1800-262 | Synthesis of Stochastic Flow Networks | http://arxiv.org/abs/1209.0724 | author:Hongchao Zhou, Ho-Lin Chen, Jehoshua Bruck category:cs.IT cs.NE math.IT math.PR published:2012-09-04 summary:A stochastic flow network is a directed graph with incoming edges (inputs)and outgoing edges (outputs), tokens enter through the input edges, travelstochastically in the network, and can exit the network through the outputedges. Each node in the network is a splitter, namely, a token can enter a nodethrough an incoming edge and exit on one of the output edges according to apredefined probability distribution. Stochastic flow networks can be easilyimplemented by DNA-based chemical reactions, with promising applications inmolecular computing and stochastic computing. In this paper, we address afundamental synthesis question: Given a finite set of possible splitters and anarbitrary rational probability distribution, design a stochastic flow network,such that every token that enters the input edge will exit the outputs with theprescribed probability distribution. The problem of probability transformation dates back to von Neumann's 1951work and was followed, among others, by Knuth and Yao in 1976. Most existingworks have been focusing on the "simulation" of target distributions. In thispaper, we design optimal-sized stochastic flow networks for "synthesizing"target distributions. It shows that when each splitter has two outgoing edgesand is unbiased, an arbitrary rational probability \frac{a}{b} with a\leq b\leq2^n can be realized by a stochastic flow network of size n that is optimal.Compared to the other stochastic systems, feedback (cycles in networks)strongly improves the expressibility of stochastic flow networks.
arxiv-1800-263 | Monotonicity of Fitness Landscapes and Mutation Rate Control | http://arxiv.org/abs/1209.0514 | author:Roman V. Belavkin, Alastair Channon, Elizabeth Aston, John Aston, Rok Krasovec, Christopher G. Knight category:q-bio.PE cs.IT cs.NE math.IT math.OC published:2012-09-04 summary:The typical view in evolutionary biology is that mutation rates areminimised. Contrary to that view, studies in combinatorial optimisation andsearch have shown a clear advantage of using variable mutation rates as acontrol parameter to optimise the performance of evolutionary algorithms.Ronald Fisher's work is the basis of much biological theory in this area. Heused Euclidean geometry of continuous, infinite phenotypic spaces to study therelation between mutation size and expected fitness of the offspring. Here wedevelop a general theory of optimal mutation rate control that is based on thealternative geometry of discrete and finite spaces of DNA sequences. We definethe monotonic properties of fitness landscapes, which allows us to relatefitness to the topology of genotypes and mutation size. First, we consider thecase of a perfectly monotonic fitness landscape, in which the optimal mutationrate control functions can be derived exactly or approximately depending onadditional constraints of the problem. Then we consider the general case ofnon-monotonic landscapes. We use the ideas of local and weak monotonicity toshow that optimal mutation rate control functions exist in any such landscapeand that they resemble control functions in a monotonic landscape at least insome neighbourhood of a fitness maximum. Generally, optimal mutation ratesincrease when fitness decreases, and the increase of mutation rate is morerapid in landscapes that are less monotonic (more rugged). We demonstrate theserelationships by obtaining and analysing approximately optimal mutation ratecontrol functions in 115 complete landscapes of binding scores between DNAsequences and transcription factors. We discuss the relevance of these findingsto living organisms, including the phenomenon of stress-induced mutagenesis.
arxiv-1800-264 | Sparse coding for multitask and transfer learning | http://arxiv.org/abs/1209.0738 | author:Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes category:cs.LG stat.ML published:2012-09-04 summary:We investigate the use of sparse coding and dictionary learning in thecontext of multitask and transfer learning. The central assumption of ourlearning method is that the tasks parameters are well approximated by sparselinear combinations of the atoms of a dictionary on a high or infinitedimensional space. This assumption, together with the large quantity ofavailable data in the multitask and transfer learning settings, allows aprincipled choice of the dictionary. We provide bounds on the generalizationerror of this approach, for both settings. Numerical experiments on onesynthetic and two real datasets show the advantage of our method over singletask learning, a previous method based on orthogonal and dense representationof the tasks and a related method learning task grouping.
arxiv-1800-265 | Efficient EM Training of Gaussian Mixtures with Missing Data | http://arxiv.org/abs/1209.0521 | author:Olivier Delalleau, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML published:2012-09-04 summary:In data-mining applications, we are frequently faced with a large fraction ofmissing entries in the data matrix, which is problematic for most discriminantmachine learning algorithms. A solution that we explore in this paper is theuse of a generative model (a mixture of Gaussians) to compute the conditionalexpectation of the missing variables given the observed variables. Sincetraining a Gaussian mixture with many different patterns of missing values canbe computationally very expensive, we introduce a spanning-tree based algorithmthat significantly speeds up training in these conditions. We also observe thatgood results can be obtained by using the generative model to fill-in themissing values for a separate discriminant learning algorithm.
arxiv-1800-266 | Seeded Graph Matching | http://arxiv.org/abs/1209.0367 | author:Donniell E. Fishkind, Sancar Adali, Carey E. Priebe category:stat.ML published:2012-09-03 summary:Graph inference is a burgeoning field in the applied and theoreticalstatistics communities, as well as throughout the wider world of science,engineering, business, etc. Given two graphs on the same number of vertices,the graph matching problem is to find a bijection between the two vertex setswhich minimizes the number of adjacency disagreements between the two graphs.The seeded graph matching problem is the graph matching problem with anadditional constraint that the bijection assigns some particular vertices ofone vertex set to respective particular vertices of the other vertex set.Solving the (seeded) graph matching problem will enable methodologies for manygraph inference tasks, but the problem is NP-hard. We modify thestate-of-the-art approximate graph matching algorithm of Vogelstein et al.(2012) to make it a fast approximate seeded graph matching algorithm. Wedemonstrate the effectiveness of our algorithm - and the potential for dramaticperformance improvement from incorporating just a few seeds - via simulationand real data experiments.
arxiv-1800-267 | Fixed-rank matrix factorizations and Riemannian low-rank optimization | http://arxiv.org/abs/1209.0430 | author:B. Mishra, G. Meyer, S. Bonnabel, R. Sepulchre category:cs.LG math.OC published:2012-09-03 summary:Motivated by the problem of learning a linear regression model whoseparameter is a large fixed-rank non-symmetric matrix, we consider theoptimization of a smooth cost function defined on the set of fixed-rankmatrices. We adopt the geometric framework of optimization on Riemannianquotient manifolds. We study the underlying geometries of several well-knownfixed-rank matrix factorizations and then exploit the Riemannian quotientgeometry of the search space in the design of a class of gradient descent andtrust-region algorithms. The proposed algorithms generalize our previousresults on fixed-rank symmetric positive semidefinite matrices, apply to abroad range of applications, scale to high-dimensional problems and confer ageometric basis to recent contributions on the learning of fixed-ranknon-symmetric matrices. We make connections with existing algorithms in thecontext of low-rank matrix completion and discuss relative usefulness of theproposed framework. Numerical experiments suggest that the proposed algorithmscompete with the state-of-the-art and that manifold optimization offers aneffective and versatile framework for the design of machine learning algorithmsthat learn a fixed-rank matrix.
arxiv-1800-268 | Robopinion: Opinion Mining Framework Inspired by Autonomous Robot Navigation | http://arxiv.org/abs/1209.0249 | author:M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny category:cs.CL cs.IR published:2012-09-03 summary:Data association methods are used by autonomous robots to find matchesbetween the current landmarks and the new set of observed features. We seek aframework for opinion mining to benefit from advancements in autonomous robotnavigation in both research and development
arxiv-1800-269 | Proximal methods for the latent group lasso penalty | http://arxiv.org/abs/1209.0368 | author:Silvia Villa, Lorenzo Rosasco, Sofia Mosci, Alessandro Verri category:math.OC cs.LG stat.ML 65K10, 90C25 published:2012-09-03 summary:We consider a regularized least squares problem, with regularization bystructured sparsity-inducing norms, which extend the usual $\ell_1$ and thegroup lasso penalty, by allowing the subsets to overlap. Such regularizationslead to nonsmooth problems that are difficult to optimize, and we propose inthis paper a suitable version of an accelerated proximal method to solve them.We prove convergence of a nested procedure, obtained composing an acceleratedproximal method with an inner algorithm for computing the proximity operator.By exploiting the geometrical properties of the penalty, we devise a new activeset strategy, thanks to which the inner iteration is relatively fast, thusguaranteeing good computational performances of the overall algorithm. Ourapproach allows to deal with high dimensional problems without pre-processingfor dimensionality reduction, leading to better computational and predictionperformances with respect to the state-of-the art methods, as shown empiricallyboth on toy and real data.
arxiv-1800-270 | Automatic ECG Beat Arrhythmia Detection | http://arxiv.org/abs/1209.0167 | author:M. Bazarghan, Y. Jaberi, R. Amandi, M. Abedi category:cs.NE published:2012-09-02 summary:Background: In recent years automated data analysis techniques have drawngreat attention and are used in almost every field of research includingbiomedical. Artificial Neural Networks (ANNs) are one of the Computer- Aided-Diagnosis tools which are used extensively by advances in computer hardwaretechnology. The application of these techniques for disease diagnosis has madegreat progress and is widely used by physicians. An Electrocardiogram carriesvital information about heart activity and physicians use this signal forcardiac disease diagnosis which was the great motivation towards our study.Methods: In this study we are using Probabilistic Neural Networks (PNN) as anautomatic technique for ECG signal analysis along with a Genetic Algorithm(GA). As every real signal recorded by the equipment can have differentartifacts, we need to do some preprocessing steps before feeding it to the ANN.Wavelet transform is used for extracting the morphological parameters andmedian filter for data reduction of the ECG signal. The subset of morphologicalparameters are chosen and optimized using GA. We had two approaches in ourinvestigation, the first one uses the whole signal with 289 normalized andde-noised data points as input to the ANN. In the second approach afterapplying all the preprocessing steps the signal is reduced to 29 data pointsand also their important parameters extracted to form the ANN input with 35data points. Results: The outcome of the two approaches for 8 types ofarrhythmia shows that the second approach is superior than the first one withan average accuracy of %99.42.
arxiv-1800-271 | Short-time homomorphic wavelet estimation | http://arxiv.org/abs/1209.0196 | author:Roberto H. Herrera, Mirko Van der Baan category:physics.geo-ph cs.CV published:2012-09-02 summary:Successful wavelet estimation is an essential step for seismic methods likeimpedance inversion, analysis of amplitude variations with offset and fullwaveform inversion. Homomorphic deconvolution has long intrigued as apotentially elegant solution to the wavelet estimation problem. Yet asuccessful implementation has proven difficult. Associated disadvantages likephase unwrapping and restrictions of sparsity in the reflectivity functionlimit its application. We explore short-time homomorphic wavelet estimation asa combination of the classical homomorphic analysis and log-spectral averaging.The introduced method of log-spectral averaging using a short-term Fouriertransform increases the number of sample points, thus reducing estimationvariances. We apply the developed method on synthetic and real data examplesand demonstrate good performance.
arxiv-1800-272 | A Session Based Blind Watermarking Technique within the NROI of Retinal Fundus Images for Authentication Using DWT, Spread Spectrum and Harris Corner Detection | http://arxiv.org/abs/1209.0053 | author:Nilanjan Dey, Moumita Pal, Achintya Das category:cs.CV cs.CY published:2012-09-01 summary:Digital Retinal Fundus Images helps to detect various ophthalmic diseases bydetecting morphological changes in optical cup, optical disc and macula.Present work proposes a method for the authentication of medical images basedon Discrete Wavelet Transformation (DWT) and Spread Spectrum. Proper selectionof the Non Region of Interest (NROI) for watermarking is crucial, as the areaunder concern has to be the least required portion conveying any medicalinformation. Proposed method discusses both the selection of least impact areaand the blind watermarking technique. Watermark is embedded within theHigh-High (HH) sub band. During embedding, watermarked image is dispersedwithin the band using a pseudo random sequence and a Session key. Watermarkedimage is extracted using the session key and the size of the image. In thisapproach the generated watermarked image having an acceptable level ofimperceptibility and distortion is compared to the Original retinal image basedon Peak Signal to Noise Ratio (PSNR) and correlation value.
arxiv-1800-273 | Learning implicitly in reasoning in PAC-Semantics | http://arxiv.org/abs/1209.0056 | author:Brendan Juba category:cs.AI cs.DS cs.LG cs.LO published:2012-09-01 summary:We consider the problem of answering queries about formulas of propositionallogic based on background knowledge partially represented explicitly as otherformulas, and partially represented as partially obscured examplesindependently drawn from a fixed probability distribution, where the queriesare answered with respect to a weaker semantics than usual -- PAC-Semantics,introduced by Valiant (2000) -- that is defined using the distribution ofexamples. We describe a fairly general, efficient reduction to limited versionsof the decision problem for a proof system (e.g., bounded space treelikeresolution, bounded degree polynomial calculus, etc.) from correspondingversions of the reasoning problem where some of the background knowledge is notexplicitly given as formulas, only learnable from the examples. Crucially, wedo not generate an explicit representation of the knowledge extracted from theexamples, and so the "learning" of the background knowledge is only doneimplicitly. As a consequence, this approach can utilize formulas as backgroundknowledge that are not perfectly valid over the distribution---essentially theanalogue of agnostic learning here.
arxiv-1800-274 | A History of Cluster Analysis Using the Classification Society's Bibliography Over Four Decades | http://arxiv.org/abs/1209.0125 | author:Fionn Murtagh, Michael J. Kurtz category:cs.DL cs.LG stat.ML 62H30 I.5.3; H.3.3 published:2012-09-01 summary:The Classification Literature Automated Search Service, an annualbibliography based on citation of one or more of a set of around 80 book orjournal publications, ran from 1972 to 2012. We analyze here the years 1994 to2011. The Classification Society's Service, as it was termed, has been producedby the Classification Society. In earlier decades it was distributed as adiskette or CD with the Journal of Classification. Among our findings are thefollowing: an enormous increase in scholarly production post approximately2000; a very major increase in quantity, coupled with work in differentdisciplines, from approximately 2004; and a major shift also from clusteranalysis in earlier times having mathematics and psychology as disciplines ofthe journals published in, and affiliations of authors, contrasted with, inmore recent times, a "centre of gravity" in management and engineering.
arxiv-1800-275 | Autoregressive short-term prediction of turning points using support vector regression | http://arxiv.org/abs/1209.0127 | author:Ran El-Yaniv, Alexandra Faynburd category:cs.LG cs.CE cs.NE published:2012-09-01 summary:This work is concerned with autoregressive prediction of turning points infinancial price sequences. Such turning points are critical local extremapoints along a series, which mark the start of new swings. Predicting thefuture time of such turning points or even their early or late identificationslightly before or after the fact has useful applications in economics andfinance. Building on recently proposed neural network model for turning pointprediction, we propose and study a new autoregressive model for predictingturning points of small swings. Our method relies on a known turning pointindicator, a Fourier enriched representation of price histories, and supportvector regression. We empirically examine the performance of the proposedmethod over a long history of the Dow Jones Industrial average. Our study showsthat the proposed method is superior to the previous neural network model, interms of trading performance of a simple trading application and also exhibitsa quantifiable advantage over the buy-and-hold benchmark.
arxiv-1800-276 | Estimating the historical and future probabilities of large terrorist events | http://arxiv.org/abs/1209.0089 | author:Aaron Clauset, Ryan Woodard category:cs.LG physics.soc-ph stat.AP stat.ME published:2012-09-01 summary:Quantities with right-skewed distributions are ubiquitous in complex socialsystems, including political conflict, economics and social networks, and thesesystems sometimes produce extremely large events. For instance, the 9/11terrorist events produced nearly 3000 fatalities, nearly six times more thanthe next largest event. But, was this enormous loss of life statisticallyunlikely given modern terrorism's historical record? Accurately estimating theprobability of such an event is complicated by the large fluctuations in theempirical distribution's upper tail. We present a generic statistical algorithmfor making such estimates, which combines semi-parametric models of tailbehavior and a nonparametric bootstrap. Applied to a global database ofterrorist events, we estimate the worldwide historical probability of observingat least one 9/11-sized or larger event since 1968 to be 11-35%. These resultsare robust to conditioning on global variations in economic development,domestic versus international events, the type of weapon used and a truncatedhistory that stops at 1998. We then use this procedure to make a data-drivenstatistical forecast of at least one similar event over the next decade.
arxiv-1800-277 | A two-stage denoising filter: the preprocessed Yaroslavsky filter | http://arxiv.org/abs/1208.6516 | author:Joseph Salmon, Rebecca Willett, Ery Arias-Castro category:cs.CV math.ST stat.TH published:2012-08-31 summary:This paper describes a simple image noise removal method which combines apreprocessing step with the Yaroslavsky filter for strong numerical, visual,and theoretical performance on a broad class of images. The framework developedis a two-stage approach. In the first stage the image is filtered with aclassical denoising method (e.g., wavelet or curvelet thresholding). In thesecond stage a modification of the Yaroslavsky filter is performed on theoriginal noisy image, where the weights of the filters are governed by pixelsimilarities in the denoised image from the first stage. Similar prefilteringideas have proved effective previously in the literature, and this paperprovides theoretical guarantees and important insight into why prefiltering canbe effective. Empirically, this simple approach achieves very good performancefor cartoon images, and can be computed much more quickly than currentpatch-based denoising algorithms.
arxiv-1800-278 | A Widely Applicable Bayesian Information Criterion | http://arxiv.org/abs/1208.6338 | author:Sumio Watanabe category:cs.LG stat.ML published:2012-08-31 summary:A statistical model or a learning machine is called regular if the map takinga parameter to a probability distribution is one-to-one and if its Fisherinformation matrix is always positive definite. If otherwise, it is calledsingular. In regular statistical models, the Bayes free energy, which isdefined by the minus logarithm of Bayes marginal likelihood, can beasymptotically approximated by the Schwarz Bayes information criterion (BIC),whereas in singular models such approximation does not hold. Recently, it was proved that the Bayes free energy of a singular model isasymptotically given by a generalized formula using a birational invariant, thereal log canonical threshold (RLCT), instead of half the number of parametersin BIC. Theoretical values of RLCTs in several statistical models are now beingdiscovered based on algebraic geometrical methodology. However, it has beendifficult to estimate the Bayes free energy using only training samples,because an RLCT depends on an unknown true distribution. In the present paper, we define a widely applicable Bayesian informationcriterion (WBIC) by the average log likelihood function over the posteriordistribution with the inverse temperature $1/\log n$, where $n$ is the numberof training samples. We mathematically prove that WBIC has the same asymptoticexpansion as the Bayes free energy, even if a statistical model is singular forand unrealizable by a statistical model. Since WBIC can be numericallycalculated without any information about a true distribution, it is ageneralized version of BIC onto singular statistical models.
arxiv-1800-279 | On the convergence of maximum variance unfolding | http://arxiv.org/abs/1209.0016 | author:Ery Arias-Castro, Bruno Pelletier category:stat.ML published:2012-08-31 summary:Maximum Variance Unfolding is one of the main methods for (nonlinear)dimensionality reduction. We study its large sample limit, providing specificrates of convergence under standard assumptions. We find that it is consistentwhen the underlying submanifold is isometric to a convex subset, and we providesome simple examples where it fails to be consistent.
arxiv-1800-280 | Combinatorial Gradient Fields for 2D Images with Empirically Convergent Separatrices | http://arxiv.org/abs/1208.6523 | author:Jan Reininghaus, David Günther, Ingrid Hotz, Tino Weinkauf, Hans Peter Seidel category:cs.CV cs.CG cs.DM 68U10 published:2012-08-31 summary:This paper proposes an efficient probabilistic method that computescombinatorial gradient fields for two dimensional image data. In contrast toexisting algorithms, this approach yields a geometric Morse-Smale complex thatconverges almost surely to its continuous counterpart when the image resolutionis increased. This approach is motivated using basic ideas from probabilitytheory and builds upon an algorithm from discrete Morse theory with a strongmathematical foundation. While a formal proof is only hinted at, we do providea thorough numerical evaluation of our method and compare it to establishedalgorithms.
arxiv-1800-281 | Statistically adaptive learning for a general class of cost functions (SA L-BFGS) | http://arxiv.org/abs/1209.0029 | author:Stephen Purpura, Dustin Hillard, Mark Hubenthal, Jim Walsh, Scott Golder, Scott Smith category:cs.LG stat.ML published:2012-08-31 summary:We present a system that enables rapid model experimentation for tera-scalemachine learning with trillions of non-zero features, billions of trainingexamples, and millions of parameters. Our contribution to the literature is anew method (SA L-BFGS) for changing batch L-BFGS to perform in near real-timeby using statistical tools to balance the contributions of previous weights,old training examples, and new training examples to achieve fast convergencewith few iterations. The result is, to our knowledge, the most scalable andflexible linear learning system reported in the literature, beating standardpractice with the current best system (Vowpal Wabbit and AllReduce). Using theKDD Cup 2012 data set from Tencent, Inc. we provide experimental results toverify the performance of this method.
arxiv-1800-282 | Comparative Study and Optimization of Feature-Extraction Techniques for Content based Image Retrieval | http://arxiv.org/abs/1208.6335 | author:Aman Chadha, Sushmit Mallik, Ravdeep Johar category:cs.CV published:2012-08-30 summary:The aim of a Content-Based Image Retrieval (CBIR) system, also known as Queryby Image Content (QBIC), is to help users to retrieve relevant images based ontheir contents. CBIR technologies provide a method to find images in largedatabases by using unique descriptors from a trained image. The imagedescriptors include texture, color, intensity and shape of the object inside animage. Several feature-extraction techniques viz., Average RGB, Color Moments,Co-occurrence, Local Color Histogram, Global Color Histogram and GeometricMoment have been critically compared in this paper. However, individually thesetechniques result in poor performance. So, combinations of these techniqueshave also been evaluated and results for the most efficient combination oftechniques have been presented and optimized for each class of image query. Wealso propose an improvement in image retrieval performance by introducing theidea of Query modification through image cropping. It enables the user toidentify a region of interest and modify the initial query to refine andpersonalize the image retrieval results.
arxiv-1800-283 | An Improved Bound for the Nystrom Method for Large Eigengap | http://arxiv.org/abs/1209.0001 | author:Mehrdad Mahdavi, Tianbao Yang, Rong Jin category:cs.LG cs.NA stat.ML published:2012-08-30 summary:We develop an improved bound for the approximation error of the Nystr\"{o}mmethod under the assumption that there is a large eigengap in the spectrum ofkernel matrix. This is based on the empirical observation that the eigengap hasa significant impact on the approximation error of the Nystr\"{o}m method. Ourapproach is based on the concentration inequality of integral operator and thetheory of matrix perturbation. Our analysis shows that when there is a largeeigengap, we can improve the approximation error of the Nystr\"{o}m method from$O(N/m^{1/4})$ to $O(N/m^{1/2})$ when measured in Frobenius norm, where $N$ isthe size of the kernel matrix, and $m$ is the number of sampled columns.
arxiv-1800-284 | Benchmarking recognition results on word image datasets | http://arxiv.org/abs/1208.6137 | author:Deepak Kumar, M N Anil Prasad, A G Ramakrishnan category:cs.CV published:2012-08-30 summary:We have benchmarked the maximum obtainable recognition accuracy on variousword image datasets using manual segmentation and a currently availablecommercial OCR. We have developed a Matlab program, with graphical userinterface, for semi-automated pixel level segmentation of word images. Wediscuss the advantages of pixel level annotation. We have covered fivedatabases adding up to over 3600 word images. These word images have beencropped from camera captured scene, born-digital and street view images. Werecognize the segmented word image using the trial version of Nuance OmnipageOCR. We also discuss, how the degradations introduced during acquisition orinaccuracies introduced during creation of word images affect the recognitionof the word present in the image. Word images for different kinds ofdegradations and correction for slant and curvy nature of words are alsodiscussed. The word recognition rates obtained on ICDAR 2003, Sign evaluation,Street view, Born-digital and ICDAR 2011 datasets are 83.9%, 89.3%, 79.6%,88.5% and 86.7% respectively.
arxiv-1800-285 | Link Prediction via Generalized Coupled Tensor Factorisation | http://arxiv.org/abs/1208.6231 | author:Beyza Ermiş, Evrim Acar, A. Taylan Cemgil category:cs.LG published:2012-08-30 summary:This study deals with the missing link prediction problem: the problem ofpredicting the existence of missing connections between entities of interest.We address link prediction using coupled analysis of relational datasetsrepresented as heterogeneous data, i.e., datasets in the form of matrices andhigher-order tensors. We propose to use an approach based on probabilisticinterpretation of tensor factorisation models, i.e., Generalised Coupled TensorFactorisation, which can simultaneously fit a large class of tensor models tohigher-order tensors/matrices with com- mon latent factors using different lossfunctions. Numerical experiments demonstrate that joint analysis of data frommultiple sources via coupled factorisation improves the link predictionperformance and the selection of right loss function and tensor model iscrucial for accurately predicting missing links.
arxiv-1800-286 | Average word length dynamics as indicator of cultural changes in society | http://arxiv.org/abs/1208.6109 | author:Vladimir V. Bochkarev, Anna V. Shevlyakova, Valery D. Solovyev category:cs.CL 91F20 J.5 published:2012-08-30 summary:Dynamics of average length of words in Russian and English is analysed in thearticle. Words belonging to the diachronic text corpus Google Books Ngram anddated back to the last two centuries are studied. It was found out that averageword length slightly increased in the 19th century, and then it was growingrapidly most of the 20th century and started decreasing over the period fromthe end of the 20th - to the beginning of the 21th century. Words whichcontributed mostly to increase or decrease of word average length wereidentified. At that, content words and functional words are analysedseparately. Long content words contribute mostly to word average length ofword. As it was shown, these words reflect the main tendencies of socialdevelopment and thus, are used frequently. Change of frequency of personalpronouns also contributes significantly to change of average word length. Theother parameters connected with average length of word were also analysed.
arxiv-1800-287 | Authorship Identification in Bengali Literature: a Comparative Analysis | http://arxiv.org/abs/1208.6268 | author:Tanmoy Chakraborty category:cs.CL cs.IR published:2012-08-30 summary:Stylometry is the study of the unique linguistic styles and writing behaviorsof individuals. It belongs to the core task of text categorization likeauthorship identification, plagiarism detection etc. Though reasonable numberof studies have been conducted in English language, no major work has been doneso far in Bengali. In this work, We will present a demonstration of authorshipidentification of the documents written in Bengali. We adopt a set offine-grained stylistic features for the analysis of the text and use them todevelop two different models: statistical similarity model consisting of threemeasures and their combination, and machine learning model with Decision Tree,Neural Network and SVM. Experimental results show that SVM outperforms otherstate-of-the-art methods after 10-fold cross validations. We also validate therelative importance of each stylistic feature to show that some of them remainconsistently significant in every model used in this experiment.
arxiv-1800-288 | Tenacious tagging of images via Mellin monomials | http://arxiv.org/abs/1208.5842 | author:Kieran G. Larkin, Peter A. Fletcher, Stephen J. Hardy category:cs.CV math.CA published:2012-08-29 summary:We describe a method for attaching persistent metadata to an image. Themethod can be interpreted as a template-based blind watermarking scheme, robustto common editing operations, namely: cropping, rotation, scaling, stretching,shearing, compression, printing, scanning, noise, and color removal. Robustnessis achieved through the reciprocity of the embedding and detection invariants.The embedded patterns are real onedimensional Mellin monomial patternsdistributed over two-dimensions. The embedded patterns are scale invariant andcan be directly embedded in an image by simple pixel addition. Detectionachieves rotation and general affine invariance by signal projection usingimplicit Radon transformation. Embedded signals contract to one-dimension inthe two-dimensional Fourier polar domain. The real signals are detected bycorrelation with complex Mellin monomial templates. Using a unique template of4 chirp patterns we detect the affine signature with exquisite sensitivity andmoderate security. The practical implementation achieves efficiencies throughfast Fourier transform (FFT) correspondences such as the projection-slicetheorem, the FFT correlation relation, and fast resampling via the chirp-ztransform. The overall method utilizes orthodox spread spectrum patterns forthe payload and performs well in terms of the classicrobustness-capacity-visibility performance triangle. Tags are entirelyimperceptible with a mean SSIM greater than 0.988 in all cases tested.Watermarked images survive almost all Stirmark attacks. The method is ideal forattaching metadata robustly to both digital and analogue images.
arxiv-1800-289 | Vector Field k-Means: Clustering Trajectories by Fitting Multiple Vector Fields | http://arxiv.org/abs/1208.5801 | author:Nivan Ferreira, James T. Klosowski, Carlos Scheidegger, Claudio Silva category:cs.LG published:2012-08-28 summary:Scientists study trajectory data to understand trends in movement patterns,such as human mobility for traffic analysis and urban planning. There is apressing need for scalable and efficient techniques for analyzing this data anddiscovering the underlying patterns. In this paper, we introduce a noveltechnique which we call vector-field $k$-means. The central idea of our approach is to use vector fields to induce asimilarity notion between trajectories. Other clustering algorithms seek arepresentative trajectory that best describes each cluster, much like $k$-meansidentifies a representative "center" for each cluster. Vector-field $k$-means,on the other hand, recognizes that in all but the simplest examples, no singletrajectory adequately describes a cluster. Our approach is based on the premisethat movement trends in trajectory data can be modeled as flows within multiplevector fields, and the vector field itself is what defines each of theclusters. We also show how vector-field $k$-means connects techniques forscalar field design on meshes and $k$-means clustering. We present an algorithm that finds a locally optimal clustering oftrajectories into vector fields, and demonstrate how vector-field $k$-means canbe used to mine patterns from trajectory data. We present experimental evidenceof its effectiveness and efficiency using several datasets, includinghistorical hurricane data, GPS tracks of people and vehicles, and anonymouscall records from a large phone company. We compare our results to previoustrajectory clustering techniques, and find that our algorithm performs fasterin practice than the current state-of-the-art in trajectory clustering, in someexamples by a large margin.
arxiv-1800-290 | Sensitive Ants in Solving the Generalized Vehicle Routing Problem | http://arxiv.org/abs/1208.5341 | author:Camelia-M. Pintea, Camelia Chira, D. Dumitrescu, Petrica C. Pop category:cs.AI cs.NE 68T20 published:2012-08-27 summary:The idea of sensitivity in ant colony systems has been exploited in hybridant-based models with promising results for many combinatorial optimizationproblems. Heterogeneity is induced in the ant population by endowing individualants with a certain level of sensitivity to the pheromone trail. The variablepheromone sensitivity within the same population of ants can potentiallyintensify the search while in the same time inducing diversity for theexploration of the environment. The performance of sensitive ant models isinvestigated for solving the generalized vehicle routing problem. Numericalresults and comparisons are discussed and analysed with a focus on emphasizingany particular aspects and potential benefits related to hybrid ant-basedmodels.
arxiv-1800-291 | A Missing and Found Recognition System for Hajj and Umrah | http://arxiv.org/abs/1208.5365 | author:Salah A. Aly category:cs.CV cs.CY published:2012-08-27 summary:This note describes an integrated recognition system for identifying missingand found objects as well as missing, dead, and found people during Hajj andUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom ofSaudi Arabia. It is assumed that the total estimated number of pilgrims willreach 20 millions during the next decade. The ultimate goal of this system isto integrate facial recognition and object identification solutions into theHajj and Umrah rituals. The missing and found computerized system is part ofthe CrowdSensing system for Hajj and Umrah crowd estimation, management andsafety.
arxiv-1800-292 | A hybrid ACO approach to the Matrix Bandwidth Minimization Problem | http://arxiv.org/abs/1208.5333 | author:Camelia-M. Pintea, Camelia Chira, Gloria-C. Crisan category:cs.AI cs.NE 68T20 published:2012-08-27 summary:The evolution of the human society raises more and more difficult endeavors.For some of the real-life problems, the computing time-restriction enhancestheir complexity. The Matrix Bandwidth Minimization Problem (MBMP) seeks for asimultaneous permutation of the rows and the columns of a square matrix inorder to keep its nonzero entries close to the main diagonal. The MBMP is ahighly investigated P-complete problem, as it has broad applications inindustry, logistics, artificial intelligence or information recovery. Thispaper describes a new attempt to use the Ant Colony Optimization framework intackling MBMP. The introduced model is based on the hybridization of the AntColony System technique with new local search mechanisms. Computationalexperiments confirm a good performance of the proposed algorithm for theconsidered set of MBMP instances.
arxiv-1800-293 | New results of ant algorithms for the Linear Ordering Problem | http://arxiv.org/abs/1208.5340 | author:Camelia-M. Pintea, Camelia Chira, D. Dumitrescu category:cs.AI cs.NE 68T20 published:2012-08-27 summary:Ant-based algorithms are successful tools for solving complex problems. Oneof these problems is the Linear Ordering Problem (LOP). The paper shows newresults on some LOP instances, using Ant Colony System (ACS) and the Step-BackSensitive Ant Model (SB-SAM).
arxiv-1800-294 | Are You Imitating Me? Unsupervised Sparse Modeling for Group Activity Analysis from a Single Video | http://arxiv.org/abs/1208.5451 | author:Zhongwei Tang, Alexey Castrodad, Mariano Tepper, Guillermo Sapiro category:cs.CV 94A08 published:2012-08-27 summary:A framework for unsupervised group activity analysis from a single video ishere presented. Our working hypothesis is that human actions lie on a union oflow-dimensional subspaces, and thus can be efficiently modeled as sparse linearcombinations of atoms from a learned dictionary representing the action'sprimitives. Contrary to prior art, and with the primary goal of spatio-temporalaction grouping, in this work only one single video segment is available forboth unsupervised learning and analysis without any prior training information.After extracting simple features at a single spatio-temporal scale, we learn adictionary for each individual in the video during each short time lapse. Thesedictionaries allow us to compare the individuals' actions by producing anaffinity matrix which contains sufficient discriminative information about theactions in the scene leading to grouping with simple and efficient tools. Withdiverse publicly available real videos, we demonstrate the effectiveness of theproposed framework and its robustness to cluttered backgrounds, changes ofhuman appearance, and action variability.
arxiv-1800-295 | Graph Degree Linkage: Agglomerative Clustering on a Directed Graph | http://arxiv.org/abs/1208.5092 | author:Wei Zhang, Xiaogang Wang, Deli Zhao, Xiaoou Tang category:cs.CV cs.SI stat.ML published:2012-08-25 summary:This paper proposes a simple but effective graph-based agglomerativealgorithm, for clustering high-dimensional data. We explore the different rolesof two fundamental concepts in graph theory, indegree and outdegree, in thecontext of clustering. The average indegree reflects the density near a sample,and the average outdegree characterizes the local geometry around a sample.Based on such insights, we define the affinity measure of clusters via theproduct of average indegree and average outdegree. The product-based affinitymakes our algorithm robust to noise. The algorithm has three main advantages:good performance, easy implementation, and high computational efficiency. Wetest the algorithm on two fundamental computer vision problems: imageclustering and object matching. Extensive experiments demonstrate that itoutperforms the state-of-the-arts in both applications.
arxiv-1800-296 | Identification of Probabilities of Languages | http://arxiv.org/abs/1208.5003 | author:Paul M. B. Vitanyi, Nick Chater category:cs.LG math.PR 68 published:2012-08-24 summary:We consider the problem of inferring the probability distribution associatedwith a language, given data consisting of an infinite sequence of elements ofthe languge. We do this under two assumptions on the algorithms concerned: (i)like a real-life algorothm it has round-off errors, and (ii) it has noround-off errors. Assuming (i) we (a) consider a probability mass function ofthe elements of the language if the data are drawn independent identicallydistributed (i.i.d.), provided the probability mass function is computable andhas a finite expectation. We give an effective procedure to almost surelyidentify in the limit the target probability mass function using the Strong Lawof Large Numbers. Second (b) we treat the case of possibly incomputableprobabilistic mass functions in the above setting. In this case we can onlypointswize converge to the target probability mass function almost surely.Third (c) we consider the case where the data are dependent assuming they aretypical for at least one computable measure and the language is finite. Thereis an effective procedure to identify by infinite recurrence a nonempty subsetof the computable measures according to which the data is typical. Here we usethe theory of Kolmogorov complexity. Assuming (ii) we obtain the weaker resultfor (a) that the target distribution is identified by infinite recurrencealmost surely; (b) stays the same as under assumption (i). We consider theassociated predictions.
arxiv-1800-297 | Changepoint detection for high-dimensional time series with missing data | http://arxiv.org/abs/1208.5062 | author:Yao Xie, Jiaji Huang, Rebecca Willett category:stat.ML cs.LG published:2012-08-24 summary:This paper describes a novel approach to change-point detection when theobserved high-dimensional data may have missing elements. The performance ofclassical methods for change-point detection typically scales poorly with thedimensionality of the data, so that a large number of observations arecollected after the true change-point before it can be reliably detected.Furthermore, missing components in the observed data handicap conventionalapproaches. The proposed method addresses these challenges by modeling thedynamic distribution underlying the data as lying close to a time-varyinglow-dimensional submanifold embedded within the ambient observation space.Specifically, streaming data is used to track a submanifold approximation,measure deviations from this approximation, and calculate a series ofstatistics of the deviations for detecting when the underlying manifold haschanged in a sharp or unexpected manner. The approach described in this paperleverages several recent results in the field of high-dimensional dataanalysis, including subspace tracking with missing data, multiscale analysistechniques for point clouds, online optimization, and change-point detectionperformance analysis. Simulations and experiments highlight the robustness andefficacy of the proposed approach in detecting an abrupt change in an otherwiseslowly varying low-dimensional manifold.
arxiv-1800-298 | WESD - Weighted Spectral Distance for Measuring Shape Dissimilarity | http://arxiv.org/abs/1208.5016 | author:Ender Konukoglu, Ben Glocker, Antonio Criminisi, Kilian M. Pohl category:cs.CV published:2012-08-24 summary:This article presents a new distance for measuring shape dissimilaritybetween objects. Recent publications introduced the use of eigenvalues of theLaplace operator as compact shape descriptors. Here, we revisit the eigenvaluesto define a proper distance, called Weighted Spectral Distance (WESD), forquantifying shape dissimilarity. The definition of WESD is derived throughanalysing the heat-trace. This analysis provides the proposed distance anintuitive meaning and mathematically links it to the intrinsic geometry ofobjects. We analyse the resulting distance definition, present and prove itsimportant theoretical properties. Some of these properties include: i) WESD isdefined over the entire sequence of eigenvalues yet it is guaranteed toconverge, ii) it is a pseudometric, iii) it is accurately approximated with afinite number of eigenvalues, and iv) it can be mapped to the [0,1) interval.Lastly, experiments conducted on synthetic and real objects are presented.These experiments highlight the practical benefits of WESD for applications invision and medical image analysis.
arxiv-1800-299 | Automatic Segmentation of Fluorescence Lifetime Microscopy Images of Cells Using Multi-Resolution Community Detection | http://arxiv.org/abs/1208.4662 | author:Dandan Hu, Pinaki Sarder, Peter Ronhovde, Sandra Orthaus, Samuel Achilefu, Zohar Nussinov category:physics.med-ph cs.CV published:2012-08-23 summary:We have developed an automatic method for segmenting fluorescence lifetime(FLT) imaging microscopy (FLIM) images of cells inspired by a multi-resolutioncommunity detection (MCD) based network segmentation method. The imageprocessing problem is framed as identifying segments with respective averageFLTs against a background in FLIM images. The proposed method segments a FLIMimage for a given resolution of the network composed using image pixels as thenodes and similarity between the pixels as the edges. In the resultingsegmentation, low network resolution leads to larger segments and high networkresolution leads to smaller segments. Further, the mean-square error (MSE) inestimating the FLT segments in a FLIM image using the proposed method was foundto be consistently decreasing with increasing resolution of the correspondingnetwork. The proposed MCD method outperformed a popular spectral clusteringbased method in performing FLIM image segmentation. The spectral segmentationmethod introduced noisy segments in its output at high resolution. It wasunable to offer a consistent decrease in MSE with increasing resolution.
arxiv-1800-300 | Optimized Look-Ahead Tree Policies: A Bridge Between Look-Ahead Tree Policies and Direct Policy Search | http://arxiv.org/abs/1208.4773 | author:Tobias Jung, Louis Wehenkel, Damien Ernst, Francis Maes category:cs.SY cs.AI cs.LG published:2012-08-23 summary:Direct policy search (DPS) and look-ahead tree (LT) policies are two widelyused classes of techniques to produce high performance policies for sequentialdecision-making problems. To make DPS approaches work well, one crucial issueis to select an appropriate space of parameterized policies with respect to thetargeted problem. A fundamental issue in LT approaches is that, to take gooddecisions, such policies must develop very large look-ahead trees which mayrequire excessive online computational resources. In this paper, we propose anew hybrid policy learning scheme that lies at the intersection of DPS and LT,in which the policy is an algorithm that develops a small look-ahead tree in adirected way, guided by a node scoring function that is learned through DPS.The LT-based representation is shown to be a versatile way of representingpolicies in a DPS scheme, while at the same time, DPS enables to significantlyreduce the size of the look-ahead trees that are required to take high-qualitydecisions. We experimentally compare our method with two other state-of-the-art DPStechniques and four common LT policies on four benchmark domains and show thatit combines the advantages of the two techniques from which it originates. Inparticular, we show that our method: (1) produces overall better performingpolicies than both pure DPS and pure LT policies, (2) requires a substantiallysmaller number of policy evaluations than other DPS techniques, (3) is easy totune and (4) results in policies that are quite robust with respect toperturbations of the initial conditions.
