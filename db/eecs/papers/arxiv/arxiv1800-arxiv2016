arxiv-1800-1 | A Practical Algorithm for Topic Modeling with Provable Guarantees | http://arxiv.org/pdf/1212.4777v1.pdf | author:Sanjeev Arora, Rong Ge, Yoni Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, Michael Zhu category:cs.LG cs.DS stat.ML published:2012-12-19 summary:Topic models provide a useful method for dimensionality reduction andexploratory data analysis in large text corpora. Most approaches to topic modelinference have been based on a maximum likelihood objective. Efficientalgorithms exist that approximate this objective, but they have no provableguarantees. Recently, algorithms have been introduced that provide provablebounds, but these algorithms are not practical because they are inefficient andnot robust to violations of model assumptions. In this paper we present analgorithm for topic model inference that is both provable and practical. Thealgorithm produces results comparable to the best MCMC implementations whilerunning orders of magnitude faster.
arxiv-1800-2 | Maximally Informative Observables and Categorical Perception | http://arxiv.org/pdf/1212.5091v1.pdf | author:Elaine Tsiang category:cs.LG cs.SD published:2012-12-19 summary:We formulate the problem of perception in the framework of informationtheory, and prove that categorical perception is equivalent to the existence ofan observable that has the maximum possible information on the target ofperception. We call such an observable maximally informative. Regardlesswhether categorical perception is real, maximally informative observables canform the basis of a theory of perception. We conclude with the implications ofsuch a theory for the problem of speech perception.
arxiv-1800-3 | Natural Language Understanding Based on Semantic Relations between Sentences | http://arxiv.org/pdf/1212.4674v1.pdf | author:Hyeok Kong category:cs.CL published:2012-12-19 summary:In this paper, we define event expression over sentences of natural languageand semantic relations between events. Based on this definition, we formallyconsider text understanding process having events as basic unit.
arxiv-1800-4 | Perceptually Motivated Shape Context Which Uses Shape Interiors | http://arxiv.org/pdf/1212.4608v1.pdf | author:Vittal Premachandran, Ramakrishna Kakarala category:cs.CV published:2012-12-19 summary:In this paper, we identify some of the limitations of current-day shapematching techniques. We provide examples of how contour-based shape matchingtechniques cannot provide a good match for certain visually similar shapes. Toovercome this limitation, we propose a perceptually motivated variant of thewell-known shape context descriptor. We identify that the interior propertiesof the shape play an important role in object recognition and develop adescriptor that captures these interior properties. We show that our method caneasily be augmented with any other shape matching algorithm. We also show fromour experiments that the use of our descriptor can significantly improve theretrieval rates.
arxiv-1800-5 | Simple Regret Optimization in Online Planning for Markov Decision Processes | http://arxiv.org/pdf/1206.3382v2.pdf | author:Zohar Feldman, Carmel Domshlak category:cs.AI cs.LG published:2012-06-15 summary:We consider online planning in Markov decision processes (MDPs). In onlineplanning, the agent focuses on its current state only, deliberates about theset of possible policies from that state onwards and, when interrupted, usesthe outcome of that exploratory deliberation to choose what action to performnext. The performance of algorithms for online planning is assessed in terms ofsimple regret, which is the agent's expected performance loss when the chosenaction, rather than an optimal one, is followed. To date, state-of-the-art algorithms for online planning in general MDPs areeither best effort, or guarantee only polynomial-rate reduction of simpleregret over time. Here we introduce a new Monte-Carlo tree search algorithm,BRUE, that guarantees exponential-rate reduction of simple regret and errorprobability. This algorithm is based on a simple yet non-standard state-spacesampling scheme, MCTS2e, in which different parts of each sample are dedicatedto different exploratory objectives. Our empirical evaluation shows that BRUEnot only provides superior performance guarantees, but is also very effectivein practice and favorably compares to state-of-the-art. We then extend BRUEwith a variant of "learning by forgetting." The resulting set of algorithms,BRUE(alpha), generalizes BRUE, improves the exponential factor in the upperbound on its reduction rate, and exhibits even more attractive empiricalperformance.
arxiv-1800-6 | A complexity analysis of statistical learning algorithms | http://arxiv.org/pdf/1212.4562v1.pdf | author:Mark A. Kon category:stat.ML published:2012-12-19 summary:We apply information-based complexity analysis to support vector machine(SVM) algorithms, with the goal of a comprehensive continuous algorithmicanalysis of such algorithms. This involves complexity measures in which somehigher order operations (e.g., certain optimizations) are considered primitivefor the purposes of measuring complexity. We consider classes of informationoperators and algorithms made up of scaled families, and investigate theutility of scaling the complexities to minimize error. We look at the divisionof statistical learning into information and algorithmic components, at thecomplexities of each, and at applications to support vector machine (SVM) andmore general machine learning algorithms. We give applications to SVMalgorithms graded into linear and higher order components, and give an examplein biomedical informatics.
arxiv-1800-7 | GMM-Based Hidden Markov Random Field for Color Image and 3D Volume Segmentation | http://arxiv.org/pdf/1212.4527v1.pdf | author:Quan Wang category:cs.CV published:2012-12-18 summary:In this project, we first study the Gaussian-based hidden Markov random field(HMRF) model and its expectation-maximization (EM) algorithm. Then wegeneralize it to Gaussian mixture model-based hidden Markov random field. Thealgorithm is implemented in MATLAB. We also apply this algorithm to color imagesegmentation problems and 3D volume segmentation problems.
arxiv-1800-8 | HMRF-EM-image: Implementation of the Hidden Markov Random Field Model and its Expectation-Maximization Algorithm | http://arxiv.org/pdf/1207.3510v2.pdf | author:Quan Wang category:cs.CV published:2012-07-15 summary:In this project, we study the hidden Markov random field (HMRF) model and itsexpectation-maximization (EM) algorithm. We implement a MATLAB toolbox namedHMRF-EM-image for 2D image segmentation using the HMRF-EM framework. Thistoolbox also implements edge-prior-preserving image segmentation, and can beeasily reconfigured for other problems, such as 3D image segmentation.
arxiv-1800-9 | Analysis of Large-scale Traffic Dynamics using Non-negative Tensor Factorization | http://arxiv.org/pdf/1212.4675v1.pdf | author:Yufei Han, Fabien Moutarde category:cs.LG published:2012-12-18 summary:In this paper, we present our work on clustering and prediction of temporaldynamics of global congestion configurations in large-scale road networks.Instead of looking into temporal traffic state variation of individual links,or of small areas, we focus on spatial congestion configurations of the wholenetwork. In our work, we aim at describing the typical temporal dynamicpatterns of this network-level traffic state and achieving long-term predictionof the large-scale traffic dynamics, in a unified data-mining framework. Tothis end, we formulate this joint task using Non-negative Tensor Factorization(NTF), which has been shown to be a useful decomposition tools for multivariatedata sequences. Clustering and prediction are performed based on the compacttensor factorization results. Experiments on large-scale simulated dataillustrate the interest of our method with promising results for long-termforecast of traffic evolution.
arxiv-1800-10 | Identification of Nonlinear Systems From the Knowledge Around Different Operating Conditions: A Feed-Forward Multi-Layer ANN Based Approach | http://arxiv.org/pdf/1212.3225v3.pdf | author:Sayan Saha, Saptarshi Das, Anish Acharya, Abhishek Kumar, Sumit Mukherjee, Indranil Pan, Amitava Gupta category:cs.SY cs.NE published:2012-12-13 summary:The paper investigates nonlinear system identification using system outputdata at various linearized operating points. A feed-forward multi-layerArtificial Neural Network (ANN) based approach is used for this purpose andtested for two target applications i.e. nuclear reactor power level monitoringand an AC servo position control system. Various configurations of ANN usingdifferent activation functions, number of hidden layers and neurons in eachlayer are trained and tested to find out the best configuration. The trainingis carried out multiple times to check for consistency and the mean andstandard deviation of the root mean square errors (RMSE) are reported for eachconfiguration.
arxiv-1800-11 | Measuring the Influence of Observations in HMMs through the Kullback-Leibler Distance | http://arxiv.org/pdf/1210.2613v2.pdf | author:Vittorio Perduca, Gregory Nuel category:cs.IT cs.LG math.IT math.PR published:2012-10-09 summary:We measure the influence of individual observations on the sequence of thehidden states of the Hidden Markov Model (HMM) by means of the Kullback-Leiblerdistance (KLD). Namely, we consider the KLD between the conditionaldistribution of the hidden states' chain given the complete sequence ofobservations and the conditional distribution of the hidden chain given all theobservations but the one under consideration. We introduce a linear complexityalgorithm for computing the influence of all the observations. As anillustration, we investigate the application of our algorithm to the problem ofdetecting outliers in HMM data series.
arxiv-1800-12 | Visual Objects Classification with Sliding Spatial Pyramid Matching | http://arxiv.org/pdf/1212.3767v2.pdf | author:Hao Wooi Lim, Yong Haur Tay category:cs.CV published:2012-12-16 summary:We present a method for visual object classification using only a singlefeature, transformed color SIFT with a variant of Spatial Pyramid Matching(SPM) that we called Sliding Spatial Pyramid Matching (SSPM), trained with anensemble of linear regression (provided by LINEAR) to obtained state of the artresult on Caltech-101 of 83.46%. SSPM is a special version of SPM where insteadof dividing an image into K number of regions, a subwindow of fixed size isslide around the image with a fixed step size. For each subwindow, a histogramof visual words is generated. To obtained the visual vocabulary, instead ofperforming K-means clustering, we randomly pick N exemplars from the trainingset and encode them with a soft non-linear mapping method. We then trained 15models, each with a different visual word size with linear regression. All 15models are then averaged together to form a single strong model.
arxiv-1800-13 | Bayesian Group Nonnegative Matrix Factorization for EEG Analysis | http://arxiv.org/pdf/1212.4347v1.pdf | author:Bonggun Shin, Alice Oh category:cs.LG stat.ML published:2012-12-18 summary:We propose a generative model of a group EEG analysis, based on appropriatekernel assumptions on EEG data. We derive the variational inference update ruleusing various approximation techniques. The proposed model outperforms thecurrent state-of-the-art algorithms in terms of common pattern extraction. Thevalidity of the proposed model is tested on the BCI competition dataset.
arxiv-1800-14 | Sketch-to-Design: Context-based Part Assembly | http://arxiv.org/pdf/1212.4490v1.pdf | author:Xiaohua Xie, Kai Xu, Niloy J. Mitra, Daniel Cohen-Or, Baoquan Chen category:cs.GR cs.CV published:2012-12-18 summary:Designing 3D objects from scratch is difficult, especially when the userintent is fuzzy without a clear target form. In the spirit ofmodeling-by-example, we facilitate design by providing reference andinspiration from existing model contexts. We rethink model design as navigatingthrough different possible combinations of part assemblies based on a largecollection of pre-segmented 3D models. We propose an interactivesketch-to-design system, where the user sketches prominent features of parts tocombine. The sketched strokes are analyzed individually and in context with theother parts to generate relevant shape suggestions via a design galleryinterface. As the session progresses and more parts get selected, contextualcues becomes increasingly dominant and the system quickly converges to a finaldesign. As a key enabler, we use pre-learned part-based contextual informationto allow the user to quickly explore different combinations of parts. Ourexperiments demonstrate the effectiveness of our approach for efficientlydesigning new variations from existing shapes.
arxiv-1800-15 | Assessing Sentiment Strength in Words Prior Polarities | http://arxiv.org/pdf/1212.4315v1.pdf | author:Lorenzo Gatti, Marco Guerini category:cs.CL published:2012-12-18 summary:Many approaches to sentiment analysis rely on lexica where words are taggedwith their prior polarity - i.e. if a word out of context evokes somethingpositive or something negative. In particular, broad-coverage resources likeSentiWordNet provide polarities for (almost) every word. Since words can havemultiple senses, we address the problem of how to compute the prior polarity ofa word starting from the polarity of each sense and returning its polaritystrength as an index between -1 and 1. We compare 14 such formulae that appearin the literature, and assess which one best approximates the human judgementof prior polarities, with both regression and classification models.
arxiv-1800-16 | A genetic algorithm applied to the validation of building thermal models | http://arxiv.org/pdf/1212.5250v1.pdf | author:Alfred Jean Philippe Lauret, Harry Boyer, Carine Riviere, Alain Bastide category:cs.NE published:2012-12-18 summary:This paper presents the coupling of a building thermal simulation code withgenetic algorithms (GAs). GAs are randomized search algorithms that are basedon the mechanisms of natural selection and genetics. We show that this couplingallows the location of defective sub-models of a building thermal model i.e.parts of model that are responsible for the disagreements between measurementsand model predictions. The method first of all is checked and validated on thebasis of a numerical model of a building taken as reference. It is then appliedto a real building case. The results show that the method could constitute anefficient tool when checking the model validity.
arxiv-1800-17 | Fast nonparametric classification based on data depth | http://arxiv.org/pdf/1207.4992v2.pdf | author:Tatjana Lange, Karl Mosler, Pavlo Mozharovskyi category:stat.ML cs.LG 62H30 published:2012-07-20 summary:A new procedure, called DDa-procedure, is developed to solve the problem ofclassifying d-dimensional objects into q >= 2 classes. The procedure iscompletely nonparametric; it uses q-dimensional depth plots and a veryefficient algorithm for discrimination analysis in the depth space [0,1]^q.Specifically, the depth is the zonoid depth, and the algorithm is thealpha-procedure. In case of more than two classes several binaryclassifications are performed and a majority rule is applied. Specialtreatments are discussed for 'outsiders', that is, data having zero depthvector. The DDa-classifier is applied to simulated as well as real data, andthe results are compared with those of similar procedures that have beenrecently proposed. In most cases the new procedure has comparable error rates,but is much faster than other classification approaches, including the SVM.
arxiv-1800-18 | Application of Symmetric Uncertainty and Mutual Information to Dimensionality Reduction and Classification of Hyperspectral Images | http://arxiv.org/pdf/1211.0613v2.pdf | author:ELkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV published:2012-11-03 summary:Remote sensing is a technology to acquire data for disatant substances,necessary to construct a model knowledge for applications as classification.Recently Hyperspectral Images (HSI) becomes a high technical tool that the maingoal is to classify the point of a region. The HIS is more than a hundredbidirectional measures, called bands (or simply images), of the same regioncalled Ground Truth Map (GT). But some bands are not relevant because they areaffected by different atmospheric effects; others contain redundantinformation; and high dimensionality of HSI features make the accuracy ofclassification lower. All these bands can be important for some applications;but for the classification a small subset of these is relevant. The problematicrelated to HSI is the dimensionality reduction. Many studies use mutualinformation (MI) to select the relevant bands. Others studies use the MInormalized forms, like Symmetric Uncertainty, in medical imagery applications.In this paper we introduce an algorithm based also on MI to select relevantbands and it apply the Symmetric Uncertainty coefficient to control redundancyand increase the accuracy of classification. This algorithm is featureselection tool and a Filter strategy. We establish this study on HSI AVIRIS92AV3C. This is an effectiveness, and fast scheme to control redundancy.
arxiv-1800-19 | Feature Clustering for Accelerating Parallel Coordinate Descent | http://arxiv.org/pdf/1212.4174v1.pdf | author:Chad Scherrer, Ambuj Tewari, Mahantesh Halappanavar, David Haglin category:stat.ML cs.DC cs.LG math.OC published:2012-12-17 summary:Large-scale L1-regularized loss minimization problems arise inhigh-dimensional applications such as compressed sensing and high-dimensionalsupervised learning, including classification and regression problems.High-performance algorithms and implementations are critical to efficientlysolving these problems. Building upon previous work on coordinate descentalgorithms for L1-regularized problems, we introduce a novel family ofalgorithms called block-greedy coordinate descent that includes, as specialcases, several existing algorithms such as SCD, Greedy CD, Shotgun, andThread-Greedy. We give a unified convergence analysis for the family ofblock-greedy algorithms. The analysis suggests that block-greedy coordinatedescent can better exploit parallelism if features are clustered so that themaximum inner product between features in different blocks is small. Ourtheoretical convergence analysis is supported with experimental re- sults usingdata from diverse real-world applications. We hope that algorithmic approachesand convergence analysis we provide will not only advance the field, but willalso encourage researchers to systematically explore the design space ofalgorithms for solving large-scale L1-regularization problems.
arxiv-1800-20 | Alternating Maximization: Unifying Framework for 8 Sparse PCA Formulations and Efficient Parallel Codes | http://arxiv.org/pdf/1212.4137v1.pdf | author:Peter Richtárik, Martin Takáč, Selin Damla Ahipaşaoğlu category:stat.ML cs.LG math.OC published:2012-12-17 summary:Given a multivariate data set, sparse principal component analysis (SPCA)aims to extract several linear combinations of the variables that togetherexplain the variance in the data as much as possible, while controlling thenumber of nonzero loadings in these combinations. In this paper we consider 8different optimization formulations for computing a single sparse loadingvector; these are obtained by combining the following factors: we employ twonorms for measuring variance (L2, L1) and two sparsity-inducing norms (L0, L1),which are used in two different ways (constraint, penalty). Three of ourformulations, notably the one with L0 constraint and L1 variance, have not beenconsidered in the literature. We give a unifying reformulation which we proposeto solve via a natural alternating maximization (AM) method. We show the the AMmethod is nontrivially equivalent to GPower (Journ\'{e}e et al; JMLR11:517--553, 2010) for all our formulations. Besides this, we provide 24efficient parallel SPCA implementations: 3 codes (multi-core, GPU and cluster)for each of the 8 problems. Parallelism in the methods is aimed at i) speedingup computations (our GPU code can be 100 times faster than an efficient serialcode written in C++), ii) obtaining solutions explaining more variance and iii)dealing with big data problems (our cluster code is able to solve a 357 GBproblem in about a minute).
arxiv-1800-21 | Minimal model of associative learning for cross-situational lexicon acquisition | http://arxiv.org/pdf/1204.1564v4.pdf | author:Paulo F. C. Tilles, Jose F. Fontanari category:q-bio.NC cs.LG published:2012-04-06 summary:An explanation for the acquisition of word-object mappings is the associativelearning in a cross-situational scenario. Here we present analytical results ofthe performance of a simple associative learning algorithm for acquiring aone-to-one mapping between $N$ objects and $N$ words based solely on theco-occurrence between objects and words. In particular, a learning trial in ourlearning scenario consists of the presentation of $C + 1 < N$ objects togetherwith a target word, which refers to one of the objects in the context. We findthat the learning times are distributed exponentially and the learning ratesare given by $\ln{[\frac{N(N-1)}{C + (N-1)^{2}}]}$ in the case the $N$ targetwords are sampled randomly and by $\frac{1}{N} \ln [\frac{N-1}{C}] $ in thecase they follow a deterministic presentation sequence. This learningperformance is much superior to those exhibited by humans and more realisticlearning algorithms in cross-situational experiments. We show that introductionof discrimination limitations using Weber's law and forgetting reduce theperformance of the associative algorithm to the human level.
arxiv-1800-22 | Sentence Compression in Spanish driven by Discourse Segmentation and Language Models | http://arxiv.org/pdf/1212.3493v2.pdf | author:Alejandro Molina, Juan-Manuel Torres-Moreno, Iria da Cunha, Eric SanJuan, Gerardo Sierra category:cs.CL cs.IR published:2012-12-14 summary:Previous works demonstrated that Automatic Text Summarization (ATS) bysentences extraction may be improved using sentence compression. In this workwe present a sentence compressions approach guided by level-sentence discoursesegmentation and probabilistic language models (LM). The results presented hereshow that the proposed solution is able to generate coherent summaries withgrammatical compressed sentences. The approach is simple enough to betransposed into other languages.
arxiv-1800-23 | Learning Markov Decision Processes for Model Checking | http://arxiv.org/pdf/1212.3873v1.pdf | author:Hua Mao, Yingke Chen, Manfred Jaeger, Thomas D. Nielsen, Kim G. Larsen, Brian Nielsen category:cs.LG cs.LO cs.SE published:2012-12-17 summary:Constructing an accurate system model for formal model verification can beboth resource demanding and time-consuming. To alleviate this shortcoming,algorithms have been proposed for automatically learning system models based onobserved system behaviors. In this paper we extend the algorithm on learningprobabilistic automata to reactive systems, where the observed system behavioris in the form of alternating sequences of inputs and outputs. We propose analgorithm for automatically learning a deterministic labeled Markov decisionprocess model from the observed behavior of a reactive system. The proposedlearning algorithm is adapted from algorithms for learning deterministicprobabilistic finite automata, and extended to include both probabilistic andnondeterministic transitions. The algorithm is empirically analyzed andevaluated by learning system models of slot machines. The evaluation isperformed by analyzing the probabilistic linear temporal logic properties ofthe system as well as by analyzing the schedulers, in particular the optimalschedulers, induced by the learned models.
arxiv-1800-24 | Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees | http://arxiv.org/pdf/1212.3850v1.pdf | author:Nima Noorshams, Martin J. Wainwright category:cs.IT cs.LG math.IT stat.ML published:2012-12-16 summary:The sum-product or belief propagation (BP) algorithm is a widely usedmessage-passing technique for computing approximate marginals in graphicalmodels. We introduce a new technique, called stochastic orthogonal seriesmessage-passing (SOSMP), for computing the BP fixed point in models withcontinuous random variables. It is based on a deterministic approximation ofthe messages via orthogonal series expansion, and a stochastic approximationvia Monte Carlo estimates of the integral updates of the basis coefficients. Weprove that the SOSMP iterates converge to a \delta-neighborhood of the uniqueBP fixed point for any tree-structured graph, and for any graphs with cycles inwhich the BP updates satisfy a contractivity condition. In addition, wedemonstrate how to choose the number of basis coefficients as a function of thedesired approximation accuracy \delta and smoothness of the compatibilityfunctions. We illustrate our theory with both simulated examples and inapplication to optical flow estimation.
arxiv-1800-25 | Biologically Inspired Spiking Neurons : Piecewise Linear Models and Digital Implementation | http://arxiv.org/pdf/1212.3765v1.pdf | author:Hamid Soleimani, Arash Ahmadi, Mohammad Bavandpour category:cs.LG cs.NE q-bio.NC published:2012-12-16 summary:There has been a strong push recently to examine biological scale simulationsof neuromorphic algorithms to achieve stronger inference capabilities. Thispaper presents a set of piecewise linear spiking neuron models, which canreproduce different behaviors, similar to the biological neuron, both for asingle neuron as well as a network of neurons. The proposed models areinvestigated, in terms of digital implementation feasibility and costs,targeting large scale hardware implementation. Hardware synthesis and physicalimplementations on FPGA show that the proposed models can produce preciseneural behaviors with higher performance and considerably lower implementationcosts compared with the original model. Accordingly, a compact structure of themodels which can be trained with supervised and unsupervised learningalgorithms has been developed. Using this structure and based on a spike ratecoding, a character recognition case study has been implemented and tested.
arxiv-1800-26 | A comparative study of root-based and stem-based approaches for measuring the similarity between arabic words for arabic text mining applications | http://arxiv.org/pdf/1212.3634v1.pdf | author:Hanane Froud, Abdelmonaim Lachkar, Said Alaoui Ouatik category:cs.CL cs.IR published:2012-12-14 summary:Representation of semantic information contained in the words is needed forany Arabic Text Mining applications. More precisely, the purpose is to bettertake into account the semantic dependencies between words expressed by theco-occurrence frequencies of these words. There have been many proposals tocompute similarities between words based on their distributions in contexts. Inthis paper, we compare and contrast the effect of two preprocessing techniquesapplied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming)approaches for measuring the similarity between Arabic words with the wellknown abstractive model -Latent Semantic Analysis (LSA)- with a wide variety ofdistance functions and similarity measures, such as the Euclidean Distance,Cosine Similarity, Jaccard Coefficient, and the Pearson CorrelationCoefficient. The obtained results show that, on the one hand, the variety ofthe corpus produces more accurate results; on the other hand, the Stem-basedapproach outperformed the Root-based one because this latter affects the wordsmeanings.
arxiv-1800-27 | Learning efficient sparse and low rank models | http://arxiv.org/pdf/1212.3631v1.pdf | author:Pablo Sprechmann, Alex M. Bronstein, Guillermo Sapiro category:cs.LG published:2012-12-14 summary:Parsimony, including sparsity and low rank, has been shown to successfullymodel data in numerous machine learning and signal processing tasks.Traditionally, such modeling approaches rely on an iterative algorithm thatminimizes an objective function with parsimony-promoting terms. The inherentlysequential structure and data-dependent complexity and latency of iterativeoptimization constitute a major limitation in many applications requiringreal-time performance or involving large-scale data. Another limitationencountered by these modeling techniques is the difficulty of their inclusionin discriminative learning scenarios. In this work, we propose to move theemphasis from the model to the pursuit algorithm, and develop a process-centricview of parsimonious modeling, in which a learned deterministicfixed-complexity pursuit process is used in lieu of iterative optimization. Weshow a principled way to construct learnable pursuit process architectures forstructured sparse and robust low rank models, derived from the iteration ofproximal descent algorithms. These architectures learn to approximate the exactparsimonious representation at a fraction of the complexity of the standardoptimization methods. We also show that appropriate training regimes allow tonaturally extend parsimonious models to discriminative settings.State-of-the-art results are demonstrated on several challenging problems inimage and audio processing with several orders of magnitude speedup compared tothe exact optimization algorithms.
arxiv-1800-28 | Equivalence of History and Generator Epsilon-Machines | http://arxiv.org/pdf/1111.4500v2.pdf | author:Nicholas F. Travers, James P. Crutchfield category:math.PR cs.IT math.IT nlin.CD stat.ML published:2011-11-18 summary:Epsilon-machines are minimal, unifilar presentations of stationary stochasticprocesses. They were originally defined in the history machine sense, as hiddenMarkov models whose states are the equivalence classes of infinite pasts withthe same probability distribution over futures. In analyzing synchronization,though, an alternative generator definition was given: unifilar, edge-emittinghidden Markov models with probabilistically distinct states. The key differenceis that history epsilon-machines are defined by a process, whereas generatorepsilon-machines define a process. We show here that these two definitions areequivalent in the finite-state case.
arxiv-1800-29 | Proceedings Quantities in Formal Methods | http://arxiv.org/pdf/1212.3454v1.pdf | author:Uli Fahrenberg, Axel Legay, Claus Thrane category:cs.LO cs.FL cs.LG cs.SE published:2012-12-14 summary:This volume contains the proceedings of the Workshop on Quantities in FormalMethods, QFM 2012, held in Paris, France on 28 August 2012. The workshop wasaffiliated with the 18th Symposium on Formal Methods, FM 2012. The focus of theworkshop was on quantities in modeling, verification, and synthesis. Modernapplications of formal methods require to reason formally on quantities such astime, resources, or probabilities. Standard formal methods and tools havegotten very good at modeling (and verifying) qualitative properties: whether ornot certain events will occur. During the last years, these methods and toolshave been extended to also cover quantitative aspects, notably leading to toolslike e.g. UPPAAL (for real-time systems), PRISM (for probabilistic systems),and PHAVer (for hybrid systems). A lot of work remains to be done howeverbefore these tools can be used in the industrial applications at which they areaiming.
arxiv-1800-30 | Evolution of Plastic Learning in Spiking Networks via Memristive Connections | http://arxiv.org/pdf/1212.3441v1.pdf | author:Gerard Howard, Ella Gale, Larry Bull, Ben de Lacy Costello, Andy Adamatzky category:cs.ET cs.NE published:2012-12-14 summary:This article presents a spiking neuroevolutionary system which implementsmemristors as plastic connections, i.e. whose weights can vary during a trial.The evolutionary design process exploits parameter self-adaptation and variabletopologies, allowing the number of neurons, connection weights, andinter-neural connectivity pattern to emerge. By comparing two phenomenologicalreal-world memristor implementations with networks comprised of (i) linearresistors (ii) constant-valued connections, we demonstrate that this approachallows the evolution of networks of appropriate complexity to emerge whilstexploiting the memristive properties of the connections to reduce learningtime. We extend this approach to allow for heterogeneous mixtures of memristorswithin the networks; our approach provides an in-depth analysis of networkstructure. Our networks are evaluated on simulated robotic navigation tasks;results demonstrate that memristive plasticity enables higher performance thanconstant-weighted connections in both static and dynamic reward scenarios, andthat mixtures of memristive elements provide performance advantages whencompared to homogeneous memristive networks.
arxiv-1800-31 | Know Your Personalization: Learning Topic level Personalization in Online Services | http://arxiv.org/pdf/1212.3390v1.pdf | author:Anirban Majumder, Nisheeth Shrivastava category:cs.LG cs.IR published:2012-12-14 summary:Online service platforms (OSPs), such as search engines, news-websites,ad-providers, etc., serve highly pe rsonalized content to the user, based onthe profile extracted from his history with the OSP. Although personalization(generally) leads to a better user experience, it also raises privacy concernsfor the user---he does not know what is present in his profile and moreimportantly, what is being used to per sonalize content for him. In this paper,we capture OSP's personalization for an user in a new data structure called theperson alization vector ($\eta$), which is a weighted vector over a set oftopics, and present techniques to compute it for users of an OSP. Our approachtreats OSPs as black-boxes, and extracts $\eta$ by mining only their output,specifical ly, the personalized (for an user) and vanilla (without any userinformation) contents served, and the differences in these content. Weformulate a new model called Latent Topic Personalization (LTP) that capturesthe personalization vector into a learning framework and present efficientinference algorithms for it. We do extensive experiments for search resultpersonalization using both data from real Google users and synthetic datasets.Our results show high accuracy (R-pre = 84%) of LTP in finding personalizedtopics. For Google data, our qualitative results show how LTP can alsoidentifies evidences---queries for results on a topic with high $\eta$ valuewere re-ranked. Finally, we show how our approach can be used to build a newPrivacy evaluation framework focused at end-user privacy on commercial OSPs.
arxiv-1800-32 | Advances in Optimizing Recurrent Networks | http://arxiv.org/pdf/1212.0901v2.pdf | author:Yoshua Bengio, Nicolas Boulanger-Lewandowski, Razvan Pascanu category:cs.LG published:2012-12-04 summary:After a more than decade-long period of relatively little research activityin the area of recurrent neural networks, several new developments will bereviewed here that have allowed substantial progress both in understanding andin technical solutions towards more efficient training of recurrent networks.These advances have been motivated by and related to the optimization issuessurrounding deep learning. Although recurrent networks are extremely powerfulin what they can in principle represent in terms of modelling sequences,theirtraining is plagued by two aspects of the same issue regarding the learning oflong-term dependencies. Experiments reported here evaluate the use of clippinggradients, spanning longer time ranges with leaky integration, advancedmomentum techniques, using more powerful output probability models, andencouraging sparser gradients to help symmetry breaking and credit assignment.The experiments are performed on text and music data and show off the combinedeffects of these techniques in generally improving both training and testerror.
arxiv-1800-33 | A Novel Directional Weighted Minimum Deviation (DWMD) Based Filter for Removal of Random Valued Impulse Noise | http://arxiv.org/pdf/1212.3373v1.pdf | author:J. K. Mandal, Somnath Mukhopadhyay category:cs.CV published:2012-12-14 summary:The most median-based de noising methods works fine for restoring the imagescorrupted by Randomn Valued Impulse Noise with low noise level but very poorwith highly corrupted images. In this paper a directional weighted minimumdeviation (DWMD) based filter has been proposed for removal of high randomvalued impulse noise (RVIN). The proposed approach based on Standard Deviation(SD) works in two phases. The first phase detects the contaminated pixels bydifferencing between the test pixel and its neighbor pixels aligned with fourmain directions. The second phase filters only those pixels keeping othersintact. The filtering scheme is based on minimum standard deviation of the fourdirectional pixels. Extensive simulations show that the proposed filter notonly provide better performance of de noising RVIN but can preserve moredetails features even thin lines or dots. This technique shows betterperformance in terms of PSNR, Image Fidelity and Computational Cost compared tothe existing filters.
arxiv-1800-34 | Multifractal analysis of sentence lengths in English literary texts | http://arxiv.org/pdf/1212.3171v1.pdf | author:Iwona Grabska-Gradzińska, Andrzej Kulig, Jarosław Kwapień, Paweł Oświęcimka, Stanisław Drożdż category:cs.CL physics.soc-ph published:2012-12-13 summary:This paper presents analysis of 30 literary texts written in English bydifferent authors. For each text, there were created time series representinglength of sentences in words and analyzed its fractal properties using twomethods of multifractal analysis: MFDFA and WTMM. Both methods showed thatthere are texts which can be considered multifractal in this representation buta majority of texts are not multifractal or even not fractal at all. Out of 30books, only a few have so-correlated lengths of consecutive sentences that theanalyzed signals can be interpreted as real multifractals. An interestingdirection for future investigations would be identifying what are the specificfeatures which cause certain texts to be multifractal and other to bemonofractal or even not fractal at all.
arxiv-1800-35 | Diachronic Variation in Grammatical Relations | http://arxiv.org/pdf/1212.3162v1.pdf | author:Aaron Gerow, Khurshid Ahmad category:cs.CL published:2012-12-13 summary:We present a method of finding and analyzing shifts in grammatical relationsfound in diachronic corpora. Inspired by the econometric technique of measuringreturn and volatility instead of relative frequencies, we propose them as a wayto better characterize changes in grammatical patterns like nominalization,modification and comparison. To exemplify the use of these techniques, weexamine a corpus of NIPS papers and report trends which manifest at the token,part-of-speech and grammatical levels. Building up from frequency observationsto a second-order analysis, we show that shifts in frequencies overlook deepertrends in language, even when part-of-speech information is included. Examiningtoken, POS and grammatical levels of variation enables a summary view ofdiachronic text as a whole. We conclude with a discussion about how thesemethods can inform intuitions about specialist domains as well as changes inlanguage use as a whole.
arxiv-1800-36 | Identifying Metaphor Hierarchies in a Corpus Analysis of Finance Articles | http://arxiv.org/pdf/1212.3138v1.pdf | author:Aaron Georw, Mark Keane category:cs.CL published:2012-12-13 summary:Using a corpus of over 17,000 financial news reports (involving over 10Mwords), we perform an analysis of the argument-distributions of the UP- andDOWN-verbs used to describe movements of indices, stocks, and shares. Usingmeasures of the overlap in the argument distributions of these verbs andk-means clustering of their distributions, we advance evidence for the proposalthat the metaphors referred to by these verbs are organised into hierarchicalstructures of superordinate and subordinate groups.
arxiv-1800-37 | Multi-target tracking algorithms in 3D | http://arxiv.org/pdf/1212.3034v1.pdf | author:Rastislav Telgarsky category:cs.CV cs.DM 65D18, 68W05 published:2012-12-13 summary:Ladars provide a unique capability for identification of objects and motionsin scenes with fixed 3D field of view (FOV). This paper describes algorithmsfor multi-target tracking in 3D scenes including the preprocessing(mathematical morphology and Parzen windows), labeling of connected components,sorting of targets by selectable attributes (size, length of track, velocity),and handling of target states (acquired, coasting, re-acquired and tracked) inorder to assemble the target trajectories. This paper is derived from workingalgorithms coded in Matlab, which were tested and reviewed by others, and doesnot speculate about usage of general formulas or frameworks.
arxiv-1800-38 | Keyword Extraction for Identifying Social Actors | http://arxiv.org/pdf/1212.3023v1.pdf | author:Mahyuddin K. M. Nasution, Shahrul Azman Mohd Noah category:cs.IR cs.CL published:2012-12-13 summary:Identifying the social actor has become one of tasks in ArtificialIntelligence, whereby extracting keyword from Web snippets depend on the use ofweb is steadily gaining ground in this research. We develop therefore anapproach based on overlap principle for utilizing a collection of features inweb snippets, where use of keyword will eliminate the un-relevant web pages.
arxiv-1800-39 | Accelerating Inference: towards a full Language, Compiler and Hardware stack | http://arxiv.org/pdf/1212.2991v1.pdf | author:Shawn Hershey, Jeff Bernstein, Bill Bradley, Andrew Schweitzer, Noah Stein, Theo Weber, Ben Vigoda category:cs.SE cs.AI stat.ML published:2012-12-12 summary:We introduce Dimple, a fully open-source API for probabilistic modeling.Dimple allows the user to specify probabilistic models in the form of graphicalmodels, Bayesian networks, or factor graphs, and performs inference (byautomatically deriving an inference engine from a variety of algorithms) on themodel. Dimple also serves as a compiler for GP5, a hardware accelerator forinference.
arxiv-1800-40 | Pituitary Adenoma Volumetry with 3D Slicer | http://arxiv.org/pdf/1212.2860v1.pdf | author:Jan Egger, Tina Kapur, Christopher Nimsky, Ron Kikinis category:cs.CV published:2012-12-12 summary:In this study, we present pituitary adenoma volumetry using the free and opensource medical image computing platform for biomedical research: (3D) Slicer.Volumetric changes in cerebral pathologies like pituitary adenomas are acritical factor in treatment decisions by physicians and in general the volumeis acquired manually. Therefore, manual slice-by-slice segmentations inmagnetic resonance imaging (MRI) data, which have been obtained at regularintervals, are performed. In contrast to this manual time consumingslice-by-slice segmentation process Slicer is an alternative which can besignificantly faster and less user intensive. In this contribution, we comparepure manual segmentations of ten pituitary adenomas with semi-automaticsegmentations under Slicer. Thus, physicians drew the boundaries completelymanually on a slice-by-slice basis and performed a Slicer-enhanced segmentationusing the competitive region-growing based module of Slicer named GrowCut.Results showed that the time and user effort required for GrowCut-basedsegmentations were on average about thirty percent less than the pure manualsegmentations. Furthermore, we calculated the Dice Similarity Coefficient (DSC)between the manual and the Slicer-based segmentations to proof that the two arecomparable yielding an average DSC of 81.97\pm3.39%.
arxiv-1800-41 | IPF for Discrete Chain Factor Graphs | http://arxiv.org/pdf/1301.0613v1.pdf | author:Wim Wiegerinck, Tom Heskes category:cs.LG cs.AI stat.ML published:2012-12-12 summary:Iterative Proportional Fitting (IPF), combined with EM, is commonly used asan algorithm for likelihood maximization in undirected graphical models. Inthis paper, we present two iterative algorithms that generalize upon IPF. Thefirst one is for likelihood maximization in discrete chain factor graphs, whichwe define as a wide class of discrete variable models including undirectedgraphical models and Bayesian networks, but also chain graphs and sigmoidbelief networks. The second one is for conditional likelihood maximization instandard undirected models and Bayesian networks. In both algorithms, theiteration steps are expressed in closed form. Numerical simulations show thatthe algorithms are competitive with state of the art methods.
arxiv-1800-42 | Adaptive Foreground and Shadow Detection inImage Sequences | http://arxiv.org/pdf/1301.0612v1.pdf | author:Yang Wang, Tele Tan category:cs.CV published:2012-12-12 summary:This paper presents a novel method of foreground segmentation thatdistinguishes moving objects from their moving cast shadows in monocular imagesequences. The models of background, edge information, and shadow are set upand adaptively updated. A Bayesian belief network is proposed to describe therelationships among the segmentation label, background, intensity, and edgeinformation. The notion of Markov random field is used to encourage the spatialconnectivity of the segmented regions. The solution is obtained by maximizingthe posterior possibility density of the segmentation field.
arxiv-1800-43 | A New Class of Upper Bounds on the Log Partition Function | http://arxiv.org/pdf/1301.0610v1.pdf | author:Martin Wainwright, Tommi S. Jaakkola, Alan Willsky category:cs.LG stat.ML published:2012-12-12 summary:Bounds on the log partition function are important in a variety of contexts,including approximate inference, model fitting, decision theory, and largedeviations analysis. We introduce a new class of upper bounds on the logpartition function, based on convex combinations of distributions in theexponential domain, that is applicable to an arbitrary undirected graphicalmodel. In the special case of convex combinations of tree-structureddistributions, we obtain a family of variational problems, similar to the Bethefree energy, but distinguished by the following desirable properties: i. theyare cnvex, and have a unique global minimum; and ii. the global minimum givesan upper bound on the log partition function. The global minimum is defined bystationary conditions very similar to those defining fixed points of beliefpropagation or tree-based reparameterization Wainwright et al., 2001. As withBP fixed points, the elements of the minimizing argument can be used asapproximations to the marginals of the original model. The analysis describedhere can be extended to structures of higher treewidth e.g., hypertrees,thereby making connections with more advanced approximations e.g., Kikuchi andvariants Yedidia et al., 2001; Minka, 2001.
arxiv-1800-44 | Discriminative Probabilistic Models for Relational Data | http://arxiv.org/pdf/1301.0604v1.pdf | author:Ben Taskar, Pieter Abbeel, Daphne Koller category:cs.LG cs.AI stat.ML published:2012-12-12 summary:In many supervised learning tasks, the entities to be labeled are related toeach other in complex ways and their labels are not independent. For example,in hypertext classification, the labels of linked pages are highly correlated.A standard approach is to classify each entity independently, ignoring thecorrelations between them. Recently, Probabilistic Relational Models, arelational version of Bayesian networks, were used to define a jointprobabilistic model for a collection of related entities. In this paper, wepresent an alternative framework that builds on (conditional) Markov networksand addresses two limitations of the previous approach. First, undirectedmodels do not impose the acyclicity constraint that hinders representation ofmany important relational dependencies in directed models. Second, undirectedmodels are well suited for discriminative training, where we optimize theconditional likelihood of the labels given the features, which generallyimproves classification accuracy. We show how to train these modelseffectively, and how to use approximate probabilistic inference over thelearned model for collective classification of multiple related entities. Weprovide experimental results on a webpage classification task, showing thataccuracy can be significantly improved by modeling relational dependencies.
arxiv-1800-45 | Unsupervised Active Learning in Large Domains | http://arxiv.org/pdf/1301.0602v1.pdf | author:Harald Steck, Tommi S. Jaakkola category:cs.LG stat.ML published:2012-12-12 summary:Active learning is a powerful approach to analyzing data effectively. We showthat the feasibility of active learning depends crucially on the choice ofmeasure with respect to which the query is being optimized. The standardinformation gain, for example, does not permit an accurate evaluation with asmall committee, a representative subset of the model space. We propose asurrogate measure requiring only a small committee and discuss the propertiesof this new measure. We devise, in addition, a bootstrap approach for committeeselection. The advantages of this approach are illustrated in the context ofrecovering (regulatory) network models.
arxiv-1800-46 | Reinforcement Learning with Partially Known World Dynamics | http://arxiv.org/pdf/1301.0601v1.pdf | author:Christian R. Shelton category:cs.LG stat.ML published:2012-12-12 summary:Reinforcement learning would enjoy better success on real-world problems ifdomain knowledge could be imparted to the algorithm by the modelers. Mostproblems have both hidden state and unknown dynamics. Partially observableMarkov decision processes (POMDPs) allow for the modeling of both.Unfortunately, they do not provide a natural framework in which to specifyknowledge about the domain dynamics. The designer must either admit to knowingnothing about the dynamics or completely specify the dynamics (thereby turningit into a planning problem). We propose a new framework called a partiallyknown Markov decision process (PKMDP) which allows the designer to specifyknown dynamics while still leaving portions of the environment s dynamicsunknown.The model represents NOT ONLY the environment dynamics but also theagents knowledge of the dynamics. We present a reinforcement learning algorithmfor this model based on importance sampling. The algorithm incorporatesplanning based on the known dynamics and learning about the unknown dynamics.Our results clearly demonstrate the ability to add domain knowledge and theresulting benefits for learning.
arxiv-1800-47 | Advances in Boosting (Invited Talk) | http://arxiv.org/pdf/1301.0599v1.pdf | author:Robert E. Schapire category:cs.LG stat.ML published:2012-12-12 summary:Boosting is a general method of generating many simple classification rulesand combining them into a single, highly accurate rule. In this talk, I willreview the AdaBoost boosting algorithm and some of its underlying theory, andthen look at how this theory has helped us to face some of the challenges ofapplying AdaBoost in two domains: In the first of these, we used boosting forpredicting and modeling the uncertainty of prices in complicated, interactingauctions. The second application was to the classification of caller utterancesin a telephone spoken-dialogue system where we faced two challenges: the needto incorporate prior knowledge to compensate for initially insufficient data;and a later need to filter the large stream of unlabeled examples beingcollected to select the ones whose labels are likely to be most informative.
arxiv-1800-48 | Asymptotic Model Selection for Naive Bayesian Networks | http://arxiv.org/pdf/1301.0598v1.pdf | author:Dmitry Rusakov, Dan Geiger category:cs.AI cs.LG published:2012-12-12 summary:We develop a closed form asymptotic formula to compute the marginallikelihood of data given a naive Bayesian network model with two hidden statesand binary features. This formula deviates from the standard BIC score. Ourwork provides a concrete example that the BIC score is generally not valid forstatistical models that belong to a stratified exponential family. This standsin contrast to linear and curved exponential families, where the BIC score hasbeen proven to provide a correct approximation for the marginal likelihood.
arxiv-1800-49 | Bayesian Network Classifiers in a High Dimensional Framework | http://arxiv.org/pdf/1301.0593v1.pdf | author:Tatjana Pavlenko, Dietrich von Rosen category:cs.LG stat.ML published:2012-12-12 summary:We present a growing dimension asymptotic formalism. The perspective in thispaper is classification theory and we show that it can accommodateprobabilistic networks classifiers, including naive Bayes model and itsaugmented version. When represented as a Bayesian network these classifiershave an important advantage: The corresponding discriminant function turns outto be a specialized case of a generalized additive model, which makes itpossible to get closed form expressions for the asymptotic misclassificationprobabilities used here as a measure of classification accuracy. Moreover, inthis paper we propose a new quantity for assessing the discriminative power ofa set of features which is then used to elaborate the augmented naive Bayesclassifier. The result is a weighted form of the augmented naive Bayes thatdistributes weights among the sets of features according to theirdiscriminative power. We derive the asymptotic distribution of the sample baseddiscriminative power and show that it is seriously overestimated in a highdimensional case. We then apply this result to find the optimal, in a sense ofminimum misclassification probability, type of weighting.
arxiv-1800-50 | Expectation-Propogation for the Generative Aspect Model | http://arxiv.org/pdf/1301.0588v1.pdf | author:Thomas P. Minka, John Lafferty category:cs.LG cs.IR stat.ML published:2012-12-12 summary:The generative aspect model is an extension of the multinomial model for textthat allows word probabilities to vary stochastically across documents.Previous results with aspect models have been promising, but hindered by thecomputational difficulty of carrying out inference and learning. This paperdemonstrates that the simple variational methods of Blei et al (2001) can leadto inaccurate inferences and biased learning for the generative aspect model.We develop an alternative approach that leads to higher accuracy at comparablecost. An extension of Expectation-Propagation is used for inference and thenembedded in an EM algorithm for learning. Experimental results are presentedfor both synthetic and real data sets.
arxiv-1800-51 | Optimal Time Bounds for Approximate Clustering | http://arxiv.org/pdf/1301.0587v1.pdf | author:Ramgopal Mettu, Greg Plaxton category:cs.DS cs.LG stat.ML published:2012-12-12 summary:Clustering is a fundamental problem in unsupervised learning, and has beenstudied widely both as a problem of learning mixture models and as anoptimization problem. In this paper, we study clustering with respect theemph{k-median} objective function, a natural formulation of clustering in whichwe attempt to minimize the average distance to cluster centers. One of the maincontributions of this paper is a simple but powerful sampling technique that wecall emph{successive sampling} that could be of independent interest. We showthat our sampling procedure can rapidly identify a small set of points (of sizejust O(klog{n/k})) that summarize the input points for the purpose ofclustering. Using successive sampling, we develop an algorithm for the k-medianproblem that runs in O(nk) time for a wide range of values of k and isguaranteed, with high probability, to return a solution with cost at most aconstant factor times optimal. We also establish a lower bound of Omega(nk) onany randomized constant-factor approximation algorithm for the k-median problemthat succeeds with even a negligible (say 1/100) probability. Thus we establisha tight time bound of Theta(nk) for the k-median problem for a wide range ofvalues of k. The best previous upper bound for the problem was O(nk), where theO-notation hides polylogarithmic factors in n and k. The best previous lowerbound of O(nk) applied only to deterministic k-median algorithms. While wefocus our presentation on the k-median objective, all our upper bounds arevalid for the k-means objective as well. In this context our algorithm comparesfavorably to the widely used k-means heuristic, which requires O(nk) time forjust one iteration and provides no useful approximation guarantees.
arxiv-1800-52 | Staged Mixture Modelling and Boosting | http://arxiv.org/pdf/1301.0586v1.pdf | author:Christopher Meek, Bo Thiesson, David Heckerman category:cs.LG stat.ML published:2012-12-12 summary:In this paper, we introduce and evaluate a data-driven staged mixturemodeling technique for building density, regression, and classification models.Our basic approach is to sequentially add components to a finite mixture modelusing the structural expectation maximization (SEM) algorithm. We show that ourtechnique is qualitatively similar to boosting. This correspondence is anatural byproduct of the fact that we use the SEM algorithm to sequentially fitthe mixture model. Finally, in our experimental evaluation, we demonstrate theeffectiveness of our approach on a variety of prediction and density estimationtasks using real-world data.
arxiv-1800-53 | Decayed MCMC Filtering | http://arxiv.org/pdf/1301.0584v1.pdf | author:Bhaskara Marthi, Hanna Pasula, Stuart Russell, Yuval Peres category:cs.AI cs.LG cs.SY published:2012-12-12 summary:Filtering---estimating the state of a partially observable Markov processfrom a sequence of observations---is one of the most widely studied problems incontrol theory, AI, and computational statistics. Exact computation of theposterior distribution is generally intractable for large discrete systems andfor nonlinear continuous systems, so a good deal of effort has gone intodeveloping robust approximation algorithms. This paper describes a simplestochastic approximation algorithm for filtering called {em decayed MCMC}. Thealgorithm applies Markov chain Monte Carlo sampling to the space of statetrajectories using a proposal distribution that favours flips of more recentstate variables. The formal analysis of the algorithm involves a generalizationof standard coupling arguments for MCMC convergence. We prove that for anyergodic underlying Markov process, the convergence time of decayed MCMC withinverse-polynomial decay remains bounded as the length of the observationsequence grows. We show experimentally that decayed MCMC is at leastcompetitive with other approximation algorithms such as particle filtering.
arxiv-1800-54 | Almost-everywhere algorithmic stability and generalization error | http://arxiv.org/pdf/1301.0579v1.pdf | author:Samuel Kutin, Partha Niyogi category:cs.LG stat.ML published:2012-12-12 summary:We explore in some detail the notion of algorithmic stability as a viableframework for analyzing the generalization error of learning algorithms. Weintroduce the new notion of training stability of a learning algorithm and showthat, in a general setting, it is sufficient for good bounds on generalizationerror. In the PAC setting, training stability is both necessary and sufficientfor learnability.\ The approach based on training stability makes no referenceto VC dimension or VC entropy. There is no need to prove uniform convergence,and generalization error is bounded directly via an extended McDiarmidinequality. As a result it potentially allows us to deal with a broader classof learning algorithms than Empirical Risk Minimization. \ We also explore therelationships among VC dimension, generalization error, and various notions ofstability. Several examples of learning algorithms are considered.
arxiv-1800-55 | Dimension Correction for Hierarchical Latent Class Models | http://arxiv.org/pdf/1301.0578v1.pdf | author:Tomas Kocka, Nevin Lianwen Zhang category:cs.LG stat.ML published:2012-12-12 summary:Model complexity is an important factor to consider when selecting amonggraphical models. When all variables are observed, the complexity of a modelcan be measured by its standard dimension, i.e. the number of independentparameters. When hidden variables are present, however, standard dimensionmight no longer be appropriate. One should instead use effective dimension(Geiger et al. 1996). This paper is concerned with the computation of effectivedimension. First we present an upper bound on the effective dimension of alatent class (LC) model. This bound is tight and its computation is easy. Wethen consider a generalization of LC models called hierarchical latent class(HLC) models (Zhang 2002). We show that the effective dimension of an HLC modelcan be obtained from the effective dimensions of some related LC models. Wealso demonstrate empirically that using effective dimension in place ofstandard dimension improves the quality of models learned from data.
arxiv-1800-56 | Reduction of Maximum Entropy Models to Hidden Markov Models | http://arxiv.org/pdf/1301.0570v1.pdf | author:Joshua Goodman category:cs.AI cs.CL published:2012-12-12 summary:We show that maximum entropy (maxent) models can be modeled with certainkinds of HMMs, allowing us to construct maxent models with hidden variables,hidden state sequences, or other characteristics. The models can be trainedusing the forward-backward algorithm. While the results are primarily oftheoretical interest, unifying apparently unrelated concepts, we also giveexperimental results for a maxent model with a hidden variable on a worddisambiguation task; the model outperforms standard techniques.
arxiv-1800-57 | The Thing That We Tried Didn't Work Very Well : Deictic Representation in Reinforcement Learning | http://arxiv.org/pdf/1301.0567v1.pdf | author:Sarah Finney, Natalia Gardiol, Leslie Pack Kaelbling, Tim Oates category:cs.LG cs.AI published:2012-12-12 summary:Most reinforcement learning methods operate on propositional representationsof the world state. Such representations are often intractably large andgeneralize poorly. Using a deictic representation is believed to be a viablealternative: they promise generalization while allowing the use of existingreinforcement-learning methods. Yet, there are few experiments on learning withdeictic representations reported in the literature. In this paper we explorethe effectiveness of two forms of deictic representation and a na\"{i}vepropositional representation in a simple blocks-world domain. We find,empirically, that the deictic representations actually worsen learningperformance. We conclude with a discussion of possible causes of these resultsand strategies for more effective learning in domains with objects.
arxiv-1800-58 | An Information-Theoretic External Cluster-Validity Measure | http://arxiv.org/pdf/1301.0565v1.pdf | author:Byron E Dom category:cs.LG stat.ML published:2012-12-12 summary:In this paper we propose a measure of clustering quality or accuracy that isappropriate in situations where it is desirable to evaluate a clusteringalgorithm by somehow comparing the clusters it produces with ``ground truth'consisting of classes assigned to the patterns by manual means or some othermeans in whose veracity there is confidence. Such measures are refered to as``external'. Our measure also has the characteristic of allowing clusteringswith different numbers of clusters to be compared in a quantitative andprincipled way. Our evaluation scheme quantitatively measures how useful thecluster labels of the patterns are as predictors of their class labels. Incases where all clusterings to be compared have the same number of clusters,the measure is equivalent to the mutual information between the cluster labelsand the class labels. In cases where the numbers of clusters are different,however, it computes the reduction in the number of bits that would be requiredto encode (compress) the class labels if both the encoder and decoder have freeacccess to the cluster labels. To achieve this encoding the estimatedconditional probabilities of the class labels given the cluster labels mustalso be encoded. These estimated probabilities can be seen as a model for theclass labels and their associated code length as a model cost.
arxiv-1800-59 | Interpolating Conditional Density Trees | http://arxiv.org/pdf/1301.0563v1.pdf | author:Scott Davies, Andrew Moore category:cs.LG cs.AI stat.ML published:2012-12-12 summary:Joint distributions over many variables are frequently modeled by decomposingthem into products of simpler, lower-dimensional conditional distributions,such as in sparsely connected Bayesian networks. However, automaticallylearning such models can be very computationally expensive when there are manydatapoints and many continuous variables with complex nonlinear relationships,particularly when no good ways of decomposing the joint distribution are knowna priori. In such situations, previous research has generally focused on theuse of discretization techniques in which each continuous variable has a singlediscretization that is used throughout the entire network. \ In this paper, wepresent and compare a wide variety of tree-based algorithms for learning andevaluating conditional density estimates over continuous variables. These treescan be thought of as discretizations that vary according to the particularinteractions being modeled; however, the density within a given leaf of thetree need not be assumed constant, and we show that such nonuniform leafdensities lead to more accurate density estimation. We have developed Bayesiannetwork structure-learning algorithms that employ these tree-based conditionaldensity representations, and we show that they can be used to practically learncomplex joint probability models over dozens of continuous variables fromthousands of datapoints. We focus on finding models that are simultaneouslyaccurate, fast to learn, and fast to evaluate once they are learned.
arxiv-1800-60 | Continuation Methods for Mixing Heterogenous Sources | http://arxiv.org/pdf/1301.0562v1.pdf | author:Adrian Corduneanu, Tommi S. Jaakkola category:cs.LG stat.ML published:2012-12-12 summary:A number of modern learning tasks involve estimation from heterogeneousinformation sources. This includes classification with labeled and unlabeleddata as well as other problems with analogous structure such as competitive(game theoretic) problems. The associated estimation problems can be typicallyreduced to solving a set of fixed point equations (consistency conditions). Weintroduce a general method for combining a preferred information source withanother in this setting by evolving continuous paths of fixed points atintermediate allocations. We explicitly identify critical points along theunique paths to either increase the stability of estimation or to ensure asignificant departure from the initial source. The homotopy continuationapproach is guaranteed to terminate at the second source, and involves nocombinatorial effort. We illustrate the power of these ideas both inclassification tasks with labeled and unlabeled data, as well as in the contextof a competitive (min-max) formulation of DNA sequence motif discovery.
arxiv-1800-61 | Learning with Scope, with Application to Information Extraction and Classification | http://arxiv.org/pdf/1301.0556v1.pdf | author:David Blei, J Andrew Bagnell, Andrew McCallum category:cs.LG cs.IR stat.ML published:2012-12-12 summary:In probabilistic approaches to classification and information extraction, onetypically builds a statistical model of words under the assumption that futuredata will exhibit the same regularities as the training data. In many datasets, however, there are scope-limited features whose predictive power is onlyapplicable to a certain subset of the data. For example, in informationextraction from web pages, word formatting may be indicative of extractioncategory in different ways on different web pages. The difficulty with usingsuch features is capturing and exploiting the new regularities encountered inpreviously unseen data. In this paper, we propose a hierarchical probabilisticmodel that uses both local/scope-limited features, such as word formatting, andglobal features, such as word content. The local regularities are modeled as anunobserved random parameter which is drawn once for each local data set. Thisrandom parameter is estimated during the inference process and then used toperform classification with both the local and global features--- a procedurewhich is akin to automatically retuning the classifier to the localregularities on each newly encountered web page. Exact inference is intractableand we present approximations via point estimates and variational methods.Empirical results on large collections of web data demonstrate that this methodsignificantly improves performance from traditional models of global featuresalone.
arxiv-1800-62 | Tree-dependent Component Analysis | http://arxiv.org/pdf/1301.0554v1.pdf | author:Francis R. Bach, Michael I. Jordan category:cs.LG stat.ML published:2012-12-12 summary:We present a generalization of independent component analysis (ICA), whereinstead of looking for a linear transform that makes the data componentsindependent, we look for a transform that makes the data components well fit bya tree-structured graphical model. Treating the problem as a semiparametricstatistical problem, we show that the optimal transform is found by minimizinga contrast function based on mutual information, a function that directlyextends the contrast function used for classical ICA. We provide twoapproximations of this contrast function, one using kernel density estimation,and another using kernel generalized variance. This tree-dependent componentanalysis framework leads naturally to an efficient general multivariate densityestimation technique where only bivariate density estimation needs to beperformed.
arxiv-1800-63 | Learning Hierarchical Object Maps Of Non-Stationary Environments with mobile robots | http://arxiv.org/pdf/1301.0551v1.pdf | author:Dragomir Anguelov, Rahul Biswas, Daphne Koller, Benson Limketkai, Sebastian Thrun category:cs.LG cs.RO stat.ML published:2012-12-12 summary:Building models, or maps, of robot environments is a highly active researcharea; however, most existing techniques construct unstructured maps and assumestatic environments. In this paper, we present an algorithm for learning objectmodels of non-stationary objects found in office-type environments. Ouralgorithm exploits the fact that many objects found in office environments lookalike (e.g., chairs, recycling bins). It does so through a two-levelhierarchical representation, which links individual objects with generic shapetemplates of object classes. We derive an approximate EM algorithm for learningshape parameters at both levels of the hierarchy, using local occupancy gridmaps for representing shape. Additionally, we develop a Bayesian modelselection algorithm that enables the robot to estimate the total number ofobjects and object templates in the environment. Experimental results using areal robot equipped with a laser range finder indicate that our approachperforms well at learning object-based maps of simple office environments. Theapproach outperforms a previously developed non-hierarchical algorithm thatmodels objects but lacks class templates.
arxiv-1800-64 | Tracking Revisited using RGBD Camera: Baseline and Benchmark | http://arxiv.org/pdf/1212.2823v1.pdf | author:Shuran Song, Jianxiong Xiao category:cs.CV published:2012-12-12 summary:Although there has been significant progress in the past decade,tracking isstill a very challenging computer vision task, due to problems such asocclusion and model drift.Recently, the increased popularity of depth sensorse.g. Microsoft Kinect has made it easy to obtain depth data at low cost.Thismay be a game changer for tracking, since depth information can be used toprevent model drift and handle occlusion.In this paper, we construct abenchmark dataset of 100 RGBD videos with high diversity, including deformableobjects, various occlusion conditions and moving cameras. We propose a verysimple but strong baseline model for RGBD tracking, and present a quantitativecomparison of several state-of-the-art tracking algorithms.Experimental resultsshow that including depth information and reasoning about occlusionsignificantly improves tracking performance. The datasets, evaluation details,source code for the baseline algorithm, and instructions for submitting newmodels will be made available online after acceptance.
arxiv-1800-65 | Clustering of functional boxplots for multiple streaming time series | http://arxiv.org/pdf/1212.2784v1.pdf | author:Elvira Romano, Antonio Balzanella category:stat.ME stat.ML published:2012-12-12 summary:In this paper we introduce a micro-clustering strategy for FunctionalBoxplots. The aim is to summarize a set of streaming time series splitted innon overlapping windows. It is a two step strategy which performs at first, anon-line summarization by means of functional data structures, named FunctionalBoxplot micro-clusters; then it reveals the final summarization by processing,off-line, the functional data structures. Our main contribute consists inproviding a new definition of micro-cluster based on Functional Boxplots and,in defining a proximity measure which allows to compare and update them. Thisallows to get a finer graphical summarization of the streaming time series byfive functional basic statistics of data. The obtained synthesis will be ableto keep track of the dynamic evolution of the multiple streams.
arxiv-1800-66 | Bayesian one-mode projection for dynamic bipartite graphs | http://arxiv.org/pdf/1212.2767v1.pdf | author:Ioannis Psorakis, Iead Rezek, Zach Frankel, Stephen J. Roberts category:stat.ML cs.LG published:2012-12-12 summary:We propose a Bayesian methodology for one-mode projecting a bipartite networkthat is being observed across a series of discrete time steps. The resultingone mode network captures the uncertainty over the presence/absence of eachlink and provides a probability distribution over its possible weight values.Additionally, the incorporation of prior knowledge over previous states makesthe resulting network less sensitive to noise and missing observations thatusually take place during the data collection process. The methodology consistsof computationally inexpensive update rules and is scalable to large problems,via an appropriate distributed implementation.
arxiv-1800-67 | Deviation optimal learning using greedy Q-aggregation | http://arxiv.org/pdf/1203.2507v2.pdf | author:Dong Dai, Philippe Rigollet, Tong Zhang category:math.ST cs.LG stat.ML stat.TH published:2012-03-12 summary:Given a finite family of functions, the goal of model selection aggregationis to construct a procedure that mimics the function from this family that isthe closest to an unknown regression function. More precisely, we consider ageneral regression model with fixed design and measure the distance betweenfunctions by the mean squared error at the design points. While proceduresbased on exponential weights are known to solve the problem of model selectionaggregation in expectation, they are, surprisingly, sub-optimal in deviation.We propose a new formulation called Q-aggregation that addresses thislimitation; namely, its solution leads to sharp oracle inequalities that areoptimal in a minimax sense. Moreover, based on the new formulation, we designgreedy Q-aggregation procedures that produce sparse aggregation modelsachieving the optimal rate. The convergence and performance of these greedyprocedures are illustrated and compared with other standard methods onsimulated examples.
arxiv-1800-68 | A Thermodynamical Approach for Probability Estimation | http://arxiv.org/pdf/1201.1384v2.pdf | author:Takashi Isozaki category:cs.LG stat.ME published:2012-01-06 summary:The issue of discrete probability estimation for samples of small size isaddressed in this study. The maximum likelihood method often suffersover-fitting when insufficient data is available. Although the Bayesianapproach can avoid over-fitting by using prior distributions, it still hasproblems with objective analysis. In response to these drawbacks, a newtheoretical framework based on thermodynamics, where energy and temperature areintroduced, was developed. Entropy and likelihood are placed at the center ofthis method. The key principle of inference for probability mass functions isthe minimum free energy, which is shown to unify the two principles of maximumlikelihood and maximum entropy. Our method can robustly estimate probabilityfunctions from small size data.
arxiv-1800-69 | Enhanced skin colour classifier using RGB Ratio model | http://arxiv.org/pdf/1212.2692v1.pdf | author:Ghazali Osman, Muhammad Suzuri Hitam, Mohd Nasir Ismail category:cs.CV 68T10 published:2012-12-12 summary:Skin colour detection is frequently been used for searching people, facedetection, pornographic filtering and hand tracking. The presence of skin ornon-skin in digital image can be determined by manipulating pixels colour orpixels texture. The main problem in skin colour detection is to represent theskin colour distribution model that is invariant or least sensitive to changesin illumination condition. Another problem comes from the fact that manyobjects in the real world may possess almost similar skin-tone colour such aswood, leather, skin-coloured clothing, hair and sand. Moreover, skin colour isdifferent between races and can be different from a person to another, evenwith people of the same ethnicity. Finally, skin colour will appear a littledifferent when different types of camera are used to capture the object orscene. The objective in this study is to develop a skin colour classifier basedon pixel-based using RGB ratio model. The RGB ratio model is a newly proposedmethod that belongs under the category of an explicitly defined skin regionmodel. This skin classifier was tested with SIdb dataset and two benchmarkdatasets; UChile and TDSD datasets to measure classifier performance. Theperformance of skin classifier was measured based on true positive (TF) andfalse positive (FP) indicator. This newly proposed model was compared withKovac, Saleh and Swift models. The experimental results showed that the RGBratio model outperformed all the other models in term of detection rate. TheRGB ratio model is able to reduce FP detection that caused by reddish objectscolour as well as be able to detect darkened skin and skin covered by shadow.
arxiv-1800-70 | Joint Training of Deep Boltzmann Machines | http://arxiv.org/pdf/1212.2686v1.pdf | author:Ian Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG published:2012-12-12 summary:We introduce a new method for training deep Boltzmann machines jointly. Priormethods require an initial learning pass that trains the deep Boltzmann machinegreedily, one layer at a time, or do not perform well on classifi- cationtasks.
arxiv-1800-71 | Mining the Web for the Voice of the Herd to Track Stock Market Bubbles | http://arxiv.org/pdf/1212.2676v1.pdf | author:Aaron Gerow, Mark Keane category:cs.CL cs.IR physics.soc-ph q-fin.GN published:2012-12-11 summary:We show that power-law analyses of financial commentaries from newspaperweb-sites can be used to identify stock market bubbles, supplementingtraditional volatility analyses. Using a four-year corpus of 17,713 online,finance-related articles (10M+ words) from the Financial Times, the New YorkTimes, and the BBC, we show that week-to-week changes in power-lawdistributions reflect market movements of the Dow Jones Industrial Average(DJI), the FTSE-100, and the NIKKEI-225. Notably, the statistical regularitiesin language track the 2007 stock market bubble, showing emerging structure inthe language of commentators, as progressively greater agreement arose in theirpositive perceptions of the market. Furthermore, during the bubble period, amarked divergence in positive language occurs as revealed by a Kullback-Leibleranalysis.
arxiv-1800-72 | Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on support vector machine classification of RT-QuIC data | http://arxiv.org/pdf/1212.2617v1.pdf | author:William Hulme, Peter Richtárik, Lynne McGuire, Alison Green category:q-bio.QM cs.LG stat.AP published:2012-12-11 summary:In this work we study numerical construction of optimal clinical diagnostictests for detecting sporadic Creutzfeldt-Jakob disease (sCJD). A cerebrospinalfluid sample (CSF) from a suspected sCJD patient is subjected to a processwhich initiates the aggregation of a protein present only in cases of sCJD.This aggregation is indirectly observed in real-time at regular intervals, sothat a longitudinal set of data is constructed that is then analysed forevidence of this aggregation. The best existing test is based solely on thefinal value of this set of data, which is compared against a threshold toconclude whether or not aggregation, and thus sCJD, is present. This testcriterion was decided upon by analysing data from a total of 108 sCJD andnon-sCJD samples, but this was done subjectively and there is no supportingmathematical analysis declaring this criterion to be exploiting the availabledata optimally. This paper addresses this deficiency, seeking to validate orimprove the test primarily via support vector machine (SVM) classification.Besides this, we address a number of additional issues such as i) earlystopping of the measurement process, ii) the possibility of detecting theparticular type of sCJD and iii) the incorporation of additional patient datasuch as age, sex, disease duration and timing of CSF sampling into theconstruction of the test.
arxiv-1800-73 | Languages cool as they expand: Allometric scaling and the decreasing need for new words | http://arxiv.org/pdf/1212.2616v1.pdf | author:Alexander M. Petersen, Joel N. Tenenbaum, Shlomo Havlin, H. Eugene Stanley, Matjaz Perc category:physics.soc-ph cs.CL stat.AP published:2012-12-11 summary:We analyze the occurrence frequencies of over 15 million words recorded inmillions of books published during the past two centuries in seven differentlanguages. For all languages and chronological subsets of the data we confirmthat two scaling regimes characterize the word frequency distributions, withonly the more common words obeying the classic Zipf law. Using corpora ofunprecedented size, we test the allometric scaling relation between the corpussize and the vocabulary size of growing languages to demonstrate a decreasingmarginal need for new words, a feature that is likely related to the underlyingcorrelations between words. We calculate the annual growth fluctuations of worduse which has a decreasing trend as the corpus size increases, indicating aslowdown in linguistic evolution following language expansion. This "coolingpattern" forms the basis of a third statistical regularity, which unlike theZipf and the Heaps law, is dynamical in nature.
arxiv-1800-74 | Language Without Words: A Pointillist Model for Natural Language Processing | http://arxiv.org/pdf/1212.3228v1.pdf | author:Peiyou Song, Anhei Shu, David Phipps, Dan Wallach, Mohit Tiwari, Jedidiah Crandall, George Luger category:cs.CL cs.IR cs.SI published:2012-12-11 summary:This paper explores two separate questions: Can we perform natural languageprocessing tasks without a lexicon?; and, Should we? Existing natural languageprocessing techniques are either based on words as units or use units such asgrams only for basic classification tasks. How close can a machine come toreasoning about the meanings of words and phrases in a corpus without using anylexicon, based only on grams? Our own motivation for posing this question is based on our efforts to findpopular trends in words and phrases from online Chinese social media. This formof written Chinese uses so many neologisms, creative character placements, andcombinations of writing systems that it has been dubbed the "Martian Language."Readers must often use visual queues, audible queues from reading out loud, andtheir knowledge and understanding of current events to understand a post. Foranalysis of popular trends, the specific problem is that it is difficult tobuild a lexicon when the invention of new ways to refer to a word or concept iseasy and common. For natural language processing in general, we argue in thispaper that new uses of language in social media will challenge machines'abilities to operate with words as the basic unit of understanding, not only inChinese but potentially in other languages.
arxiv-1800-75 | Convex Relaxations for Learning Bounded Treewidth Decomposable Graphs | http://arxiv.org/pdf/1212.2573v1.pdf | author:K. S. Sesh Kumar, Francis Bach category:cs.LG cs.DS stat.ML published:2012-12-11 summary:We consider the problem of learning the structure of undirected graphicalmodels with bounded treewidth, within the maximum likelihood framework. This isan NP-hard problem and most approaches consider local search techniques. Inthis paper, we pose it as a combinatorial optimization problem, which is thenrelaxed to a convex optimization problem that involves searching over theforest and hyperforest polytopes with special structures, independently. Asupergradient method is used to solve the dual problem, with a run-timecomplexity of $O(k^3 n^{k+2} \log n)$ for each iteration, where $n$ is thenumber of variables and $k$ is a bound on the treewidth. We compare ourapproach to state-of-the-art methods on synthetic datasets and classicalbenchmarks, showing the gains of the novel convex approach.
arxiv-1800-76 | A Learning Framework for Morphological Operators using Counter-Harmonic Mean | http://arxiv.org/pdf/1212.2546v1.pdf | author:Jonathan Masci, Jesús Angulo, Jürgen Schmidhuber category:cs.CV published:2012-12-11 summary:We present a novel framework for learning morphological operators usingcounter-harmonic mean. It combines concepts from morphology and convolutionalneural networks. A thorough experimental validation analyzes basicmorphological operators dilation and erosion, opening and closing, as well asthe much more complex top-hat transform, for which we report a real-worldapplication from the steel industry. Using online learning and stochasticgradient descent, our system learns both the structuring element and thecomposition of operators. It scales well to large datasets and online settings.
arxiv-1800-77 | On The Delays In Spiking Neural P Systems | http://arxiv.org/pdf/1212.2529v1.pdf | author:Francis George C. Cabarle, Kelvin C. Buño, Henry N. Adorna category:cs.NE cs.DC cs.ET 97P20 F.4.1 published:2012-12-11 summary:In this work we extend and improve the results done in a previous work onsimulating Spiking Neural P systems (SNP systems in short) with delays usingSNP systems without delays. We simulate the former with the latter oversequential, iteration, join, and split routing. Our results provideconstructions so that both systems halt at exactly the same time, start withonly one spike, and produce the same number of spikes to the environment afterhalting.
arxiv-1800-78 | Robust Face Recognition using Local Illumination Normalization and Discriminant Feature Point Selection | http://arxiv.org/pdf/1212.2415v1.pdf | author:Song Han, Jinsong Kim, Cholhun Kim, Jongchol Jo, Sunam Han category:cs.LG cs.CV published:2012-12-11 summary:Face recognition systems must be robust to the variation of various factorssuch as facial expression, illumination, head pose and aging. Especially, therobustness against illumination variation is one of the most important problemsto be solved for the practical use of face recognition systems. Gabor waveletis widely used in face detection and recognition because it gives thepossibility to simulate the function of human visual system. In this paper, wepropose a method for extracting Gabor wavelet features which is stable underthe variation of local illumination and show experiment results demonstratingits effectiveness.
arxiv-1800-79 | Mining Techniques in Network Security to Enhance Intrusion Detection Systems | http://arxiv.org/pdf/1212.2414v1.pdf | author:Maher Salem, Ulrich Buehler category:cs.CR cs.LG published:2012-12-11 summary:In intrusion detection systems, classifiers still suffer from severaldrawbacks such as data dimensionality and dominance, different network featuretypes, and data impact on the classification. In this paper two significantenhancements are presented to solve these drawbacks. The first enhancement isan improved feature selection using sequential backward search and informationgain. This, in turn, extracts valuable features that enhance positively thedetection rate and reduce the false positive rate. The second enhancement istransferring nominal network features to numeric ones by exploiting thediscrete random variable and the probability mass function to solve the problemof different feature types, the problem of data dominance, and data impact onthe classification. The latter is combined to known normalization methods toachieve a significant hybrid normalization approach. Finally, an intensive andcomparative study approves the efficiency of these enhancements and showsbetter performance comparing to other proposed methods.
arxiv-1800-80 | On the complexity of learning a language: An improvement of Block's algorithm | http://arxiv.org/pdf/1212.2390v1.pdf | author:Eric Werner category:cs.CL cs.LG published:2012-12-11 summary:Language learning is thought to be a highly complex process. One of thehurdles in learning a language is to learn the rules of syntax of the language.Rules of syntax are often ordered in that before one rule can applied one mustapply another. It has been thought that to learn the order of n rules one mustgo through all n! permutations. Thus to learn the order of 27 rules wouldrequire 27! steps or 1.08889x10^{28} steps. This number is much greater thanthe number of seconds since the beginning of the universe! In an insightfulanalysis the linguist Block ([Block 86], pp. 62-63, p.238) showed that with theassumption of transitivity this vast number of learning steps reduces to a mere377 steps. We present a mathematical analysis of the complexity of Block'salgorithm. The algorithm has a complexity of order n^2 given n rules. Inaddition, we improve Block's results exponentially, by introducing an algorithmthat has complexity of order less than n log n.
arxiv-1800-81 | PAC-Bayesian Learning and Domain Adaptation | http://arxiv.org/pdf/1212.2340v1.pdf | author:Pascal Germain, Amaury Habrard, François Laviolette, Emilie Morvant category:stat.ML cs.LG published:2012-12-11 summary:In machine learning, Domain Adaptation (DA) arises when the distribution gen-erating the test (target) data differs from the one generating the learning(source) data. It is well known that DA is an hard task even under strongassumptions, among which the covariate-shift where the source and targetdistributions diverge only in their marginals, i.e. they have the same labelingfunction. Another popular approach is to consider an hypothesis class thatmoves closer the two distributions while implying a low-error for both tasks.This is a VC-dim approach that restricts the complexity of an hypothesis classin order to get good generalization. Instead, we propose a PAC-Bayesianapproach that seeks for suitable weights to be given to each hypothesis inorder to build a majority vote. We prove a new DA bound in the PAC-Bayesiancontext. This leads us to design the first DA-PAC-Bayesian algorithm based onthe minimization of the proposed bound. Doing so, we seek for a \rho-weightedmajority vote that takes into account a trade-off between three quantities. Thefirst two quantities being, as usual in the PAC-Bayesian approach, (a) thecomplexity of the majority vote (measured by a Kullback-Leibler divergence) and(b) its empirical risk (measured by the \rho-average errors on the sourcesample). The third quantity is (c) the capacity of the majority vote todistinguish some structural difference between the source and target samples.
arxiv-1800-82 | Bag-of-Words Representation for Biomedical Time Series Classification | http://arxiv.org/pdf/1212.2262v1.pdf | author:Jin Wang, Ping Liu, Mary F. H. She, Saeid Nahavandi, and Abbas Kouzani category:cs.LG cs.AI published:2012-12-11 summary:Automatic analysis of biomedical time series such as electroencephalogram(EEG) and electrocardiographic (ECG) signals has attracted great interest inthe community of biomedical engineering due to its important applications inmedicine. In this work, a simple yet effective bag-of-words representation thatis able to capture both local and global structure similarity information isproposed for biomedical time series representation. In particular, similar tothe bag-of-words model used in text document domain, the proposed method treatsa time series as a text document and extracts local segments from the timeseries as words. The biomedical time series is then represented as a histogramof codewords, each entry of which is the count of a codeword appeared in thetime series. Although the temporal order of the local segments is ignored, thebag-of-words representation is able to capture high-level structuralinformation because both local and global structural information are wellutilized. The performance of the bag-of-words model is validated on threedatasets extracted from real EEG and ECG signals. The experimental resultsdemonstrate that the proposed method is not only insensitive to parameters ofthe bag-of-words model such as local segment length and codebook size, but alsorobust to noise.
arxiv-1800-83 | Fast and Robust Linear Motion Deblurring | http://arxiv.org/pdf/1212.2245v1.pdf | author:Martin Welk, Patrik Raudaschl, Thomas Schwarzbauer, Martin Erler, Martin Läuter category:cs.CV I.4.4; G.1.9 published:2012-12-10 summary:We investigate efficient algorithmic realisations for robust deconvolution ofgrey-value images with known space-invariant point-spread function, withemphasis on 1D motion blur scenarios. The goal is to make deconvolutionsuitable as preprocessing step in automated image processing environments withtight time constraints. Candidate deconvolution methods are selected for theirrestoration quality, robustness and efficiency. Evaluation of restorationquality and robustness on synthetic and real-world test images leads us tofocus on a combination of Wiener filtering with few iterations of robust andregularised Richardson-Lucy deconvolution. We discuss algorithmic optimisationsfor specific scenarios. In the case of uniform linear motion blur in coordinatedirection, it is possible to achieve real-time performance (less than 50 ms) insingle-threaded CPU computation on images of $256\times256$ pixels. For moregeneral space-invariant blur settings, still favourable computation times areobtained. Exemplary parallel implementations demonstrate that the proposedmethod also achieves real-time performance for general 1D motion blurs in amulti-threaded CPU setting, and for general 2D blurs on a GPU.
arxiv-1800-84 | A Scale-Space Theory for Text | http://arxiv.org/pdf/1212.2145v1.pdf | author:Shuang-Hong Yang category:cs.IR cs.CL published:2012-12-10 summary:Scale-space theory has been established primarily by the computer vision andsignal processing communities as a well-founded and promising framework formulti-scale processing of signals (e.g., images). By embedding an originalsignal into a family of gradually coarsen signals parameterized with acontinuous scale parameter, it provides a formal framework to capture thestructure of a signal at different scales in a consistent way. In this paper,we present a scale space theory for text by integrating semantic and spatialfilters, and demonstrate how natural language documents can be understood,processed and analyzed at multiple resolutions, and how this scale-spacerepresentation can be used to facilitate a variety of NLP and text analysistasks.
arxiv-1800-85 | Minerva and minepy: a C engine for the MINE suite and its R, Python and MATLAB wrappers | http://arxiv.org/pdf/1208.4271v2.pdf | author:Davide Albanese, Michele Filosi, Roberto Visintainer, Samantha Riccadonna, Giuseppe Jurman, Cesare Furlanello category:stat.ML q-bio.QM published:2012-08-21 summary:We introduce a novel implementation in ANSI C of the MINE family ofalgorithms for computing maximal information-based measures of dependencebetween two variables in large datasets, with the aim of a low memory footprintand ease of integration within bioinformatics pipelines. We provide thelibraries minerva (with the R interface) and minepy for Python, MATLAB, Octaveand C++. The C solution reduces the large memory requirement of the originalJava implementation, has good upscaling properties, and offers a nativeparallelization for the R interface. Low memory requirements are demonstratedon the MINE benchmarks as well as on large (n=1340) microarray and IlluminaGAII RNA-seq transcriptomics datasets. Availability and Implementation: Source code and binaries are freelyavailable for download under GPL3 licence at http://minepy.sourceforge.net forminepy and through the CRAN repository http://cran.r-project.org for the Rpackage minerva. All software is multiplatform (MS Windows, Linux and OSX).
arxiv-1800-86 | Local Component Analysis | http://arxiv.org/pdf/1109.0093v4.pdf | author:Nicolas Le Roux, Francis Bach category:cs.LG published:2011-09-01 summary:Kernel density estimation, a.k.a. Parzen windows, is a popular densityestimation method, which can be used for outlier detection or clustering. Withmultivariate data, its performance is heavily reliant on the metric used withinthe kernel. Most earlier work has focused on learning only the bandwidth of thekernel (i.e., a scalar multiplicative factor). In this paper, we propose tolearn a full Euclidean metric through an expectation-minimization (EM)procedure, which can be seen as an unsupervised counterpart to neighbourhoodcomponent analysis (NCA). In order to avoid overfitting with a fullynonparametric density estimator in high dimensions, we also consider asemi-parametric Gaussian-Parzen density model, where some of the variables aremodelled through a jointly Gaussian density, while others are modelled throughParzen windows. For these two models, EM leads to simple closed-form updatesbased on matrix inversions and eigenvalue decompositions. We show empiricallythat our method leads to density estimators with higher test-likelihoods thannatural competing methods, and that the metrics may be used within mostunsupervised learning techniques that rely on such metrics, such as spectralclustering or manifold learning methods. Finally, we present a stochasticapproximation scheme which allows for the use of this method in a large-scalesetting.
arxiv-1800-87 | High-dimensional sequence transduction | http://arxiv.org/pdf/1212.1936v1.pdf | author:Nicolas Boulanger-Lewandowski, Yoshua Bengio, Pascal Vincent category:cs.LG published:2012-12-09 summary:We investigate the problem of transforming an input sequence into ahigh-dimensional output sequence in order to transcribe polyphonic audio musicinto symbolic notation. We introduce a probabilistic model based on a recurrentneural network that is able to learn realistic output distributions given theinput and we devise an efficient algorithm to search for the global mode ofthat distribution. The resulting method produces musically plausibletranscriptions even under high levels of noise and drastically outperformsprevious state-of-the-art approaches on five datasets of synthesized sounds andreal recordings, approximately halving the test error rate.
arxiv-1800-88 | Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization | http://arxiv.org/pdf/1109.5647v7.pdf | author:Alexander Rakhlin, Ohad Shamir, Karthik Sridharan category:cs.LG math.OC published:2011-09-26 summary:Stochastic gradient descent (SGD) is a simple and popular method to solvestochastic optimization problems which arise in machine learning. For stronglyconvex problems, its convergence rate was known to be O(\log(T)/T), by runningSGD for T iterations and returning the average point. However, recent resultsshowed that using a different algorithm, one can get an optimal O(1/T) rate.This might lead one to believe that standard SGD is suboptimal, and maybeshould even be replaced as a method of choice. In this paper, we investigatethe optimality of SGD in a stochastic setting. We show that for smoothproblems, the algorithm attains the optimal O(1/T) rate. However, fornon-smooth problems, the convergence rate with averaging might really be\Omega(\log(T)/T), and this is not just an artifact of the analysis. On theflip side, we show that a simple modification of the averaging step suffices torecover the O(1/T) rate, and no other change of the algorithm is necessary. Wealso present experimental results which support our findings, and point outopen problems.
arxiv-1800-89 | Condensés de textes par des méthodes numériques | http://arxiv.org/pdf/1212.1918v1.pdf | author:Juan-Manuel Torres-Moreno, Patricia Velázquez-Morales, Jean-Guy Meunier category:cs.IR cs.CL published:2012-12-09 summary:Since information in electronic form is already a standard, and that thevariety and the quantity of information become increasingly large, the methodsof summarizing or automatic condensation of texts is a critical phase of theanalysis of texts. This article describes CORTEX a system based on numericalmethods, which allows obtaining a condensation of a text, which is independentof the topic and of the length of the text. The structure of the system enablesit to find the abstracts in French or Spanish in very short times.
arxiv-1800-90 | Self Authentication of image through Daubechies Transform technique (SADT) | http://arxiv.org/pdf/1212.1863v1.pdf | author:Madhumita Sengupta, J. K. Mandal category:cs.CR cs.CV published:2012-12-09 summary:In this paper a 4 x 4 Daubechies transform based authentication techniquetermed as SADT has been proposed to authenticate gray scale images. The coverimage is transformed into the frequency domain using 4 x 4 mask in a row majororder using Daubechies transform technique, resulting four frequency subbandsAF, HF, VF and DF. One byte of every band in a mask is embedding with two orfour bits of secret information. Experimental results are computed and comparedwith the existing authentication techniques like Li s method [5], SCDFT [6],Region-Based method [7] and other similar techniques based on Mean Square Error(MSE), Peak Signal to Noise Ratio (PSNR) and Image Fidelity (IF), which showsbetter performance in SADT.
arxiv-1800-91 | An Empirical Comparison of V-fold Penalisation and Cross Validation for Model Selection in Distribution-Free Regression | http://arxiv.org/pdf/1212.1780v1.pdf | author:Charanpal Dhanjal, Nicolas Baskiotis, Stéphan Clémençon, Nicolas Usunier category:stat.ML published:2012-12-08 summary:Model selection is a crucial issue in machine-learning and a wide variety ofpenalisation methods (with possibly data dependent complexity penalties) haverecently been introduced for this purpose. However their empirical performanceis generally not well documented in the literature. It is the goal of thispaper to investigate to which extent such recent techniques can be successfullyused for the tuning of both the regularisation and kernel parameters in supportvector regression (SVR) and the complexity measure in regression trees (CART).This task is traditionally solved via V-fold cross-validation (VFCV), whichgives efficient results for a reasonable computational cost. A disadvantagehowever of VFCV is that the procedure is known to provide an asymptoticallysuboptimal risk estimate as the number of examples tends to infinity. Recently,a penalisation procedure called V-fold penalisation has been proposed toimprove on VFCV, supported by theoretical arguments. Here we report on anextensive set of experiments comparing V-fold penalisation and VFCV forSVR/CART calibration on several benchmark datasets. We highlight cases in whichVFCV and V-fold penalisation provide poor estimates of the risk respectivelyand introduce a modified penalisation technique to reduce the estimation error.
arxiv-1800-92 | Hybrid Optimized Back propagation Learning Algorithm For Multi-layer Perceptron | http://arxiv.org/pdf/1212.1752v1.pdf | author:Mriganka Chakraborty, Arka Ghosh category:cs.NE published:2012-12-08 summary:Standard neural network based on general back propagation learning usingdelta method or gradient descent method has some great faults like pooroptimization of error-weight objective function, low learning rate, instability.This paper introduces a hybrid supervised back propagation learning algorithmwhich uses trust-region method of unconstrained optimization of the errorobjective function by using quasi-newton method .This optimization leads tomore accurate weight update system for minimizing the learning error duringlearning phase of multi-layer perceptron.[13][14][15] In this paper augmentedline search is used for finding points which satisfies Wolfe condition. In thispaper, This hybrid back propagation algorithm has strong global convergenceproperties & is robust & efficient in practice.
arxiv-1800-93 | Evolution of the most common English words and phrases over the centuries | http://arxiv.org/pdf/1212.1709v1.pdf | author:Matjaz Perc category:physics.soc-ph cs.CL cs.DL published:2012-12-07 summary:By determining which were the most common English words and phrases since thebeginning of the 16th century, we obtain a unique large-scale view of theevolution of written text. We find that the most common words and phrases inany given year had a much shorter popularity lifespan in the 16th than they hadin the 20th century. By measuring how their usage propagated across the years,we show that for the past two centuries the process has been governed by linearpreferential attachment. Along with the steady growth of the English lexicon,this provides an empirical explanation for the ubiquity of the Zipf's law inlanguage statistics and confirms that writing, although undoubtedly anexpression of art and skill, is not immune to the same influences ofself-organization that are known to regulate processes as diverse as the makingof new friends and World Wide Web growth.
arxiv-1800-94 | ANOVA kernels and RKHS of zero mean functions for model-based sensitivity analysis | http://arxiv.org/pdf/1106.3571v2.pdf | author:Nicolas Durrande, David Ginsbourger, Olivier Roustant, Laurent Carraro category:stat.ML published:2011-06-17 summary:Given a reproducing kernel Hilbert space H of real-valued functions and asuitable measure mu over the source space D (subset of R), we decompose H asthe sum of a subspace of centered functions for mu and its orthogonal in H.This decomposition leads to a special case of ANOVA kernels, for which thefunctional ANOVA representation of the best predictor can be elegantly derived,either in an interpolation or regularization framework. The proposed kernelsappear to be particularly convenient for analyzing the e ffect of each (groupof) variable(s) and computing sensitivity indices without recursivity.
arxiv-1800-95 | Changepoint detection for high-dimensional time series with missing data | http://arxiv.org/pdf/1208.5062v3.pdf | author:Yao Xie, Jiaji Huang, Rebecca Willett category:stat.ML cs.LG published:2012-08-24 summary:This paper describes a novel approach to change-point detection when theobserved high-dimensional data may have missing elements. The performance ofclassical methods for change-point detection typically scales poorly with thedimensionality of the data, so that a large number of observations arecollected after the true change-point before it can be reliably detected.Furthermore, missing components in the observed data handicap conventionalapproaches. The proposed method addresses these challenges by modeling thedynamic distribution underlying the data as lying close to a time-varyinglow-dimensional submanifold embedded within the ambient observation space.Specifically, streaming data is used to track a submanifold approximation,measure deviations from this approximation, and calculate a series ofstatistics of the deviations for detecting when the underlying manifold haschanged in a sharp or unexpected manner. The approach described in this paperleverages several recent results in the field of high-dimensional dataanalysis, including subspace tracking with missing data, multiscale analysistechniques for point clouds, online optimization, and change-point detectionperformance analysis. Simulations and experiments highlight the robustness andefficacy of the proposed approach in detecting an abrupt change in an otherwiseslowly varying low-dimensional manifold.
arxiv-1800-96 | Using external sources of bilingual information for on-the-fly word alignment | http://arxiv.org/pdf/1212.1192v2.pdf | author:Miquel Esplà-Gomis, Felipe Sánchez-Martínez, Mikel L. Forcada category:cs.CL I.2.7 published:2012-12-05 summary:In this paper we present a new and simple language-independent method forword-alignment based on the use of external sources of bilingual informationsuch as machine translation systems. We show that the few parameters of thealigner can be trained on a very small corpus, which leads to resultscomparable to those obtained by the state-of-the-art tool GIZA++ in terms ofprecision. Regarding other metrics, such as alignment error rate or F-measure,the parametric aligner, when trained on a very small gold-standard (450 pairsof sentences), provides results comparable to those produced by GIZA++ whentrained on an in-domain corpus of around 10,000 pairs of sentences.Furthermore, the results obtained indicate that the training isdomain-independent, which enables the use of the trained aligner 'on the fly'on any new pair of sentences.
arxiv-1800-97 | The Clustering of Author's Texts of English Fiction in the Vector Space of Semantic Fields | http://arxiv.org/pdf/1212.1478v1.pdf | author:Bohdan Pavlyshenko category:cs.CL cs.DL cs.IR published:2012-12-06 summary:The clustering of text documents in the vector space of semantic fields andin the semantic space with orthogonal basis has been analysed. It is shown thatusing the vector space model with the basis of semantic fields is effective inthe cluster analysis algorithms of author's texts in English fiction. Theanalysis of the author's texts distribution in cluster structure showed thepresence of the areas of semantic space that represent the author's ideolectsof individual authors. SVD factorization of the semantic fields matrix makes itpossible to reduce significantly the dimension of the semantic space in thecluster analysis of author's texts.
arxiv-1800-98 | Automatic Detection of Texture Defects Using Texture-Periodicity and Gabor Wavelets | http://arxiv.org/pdf/1212.1329v1.pdf | author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV I.0; I.2.10 published:2012-12-06 summary:In this paper, we propose a machine vision algorithm for automaticallydetecting defects in textures belonging to 16 out of 17 wallpaper groups usingtexture-periodicity and a family of Gabor wavelets. Input defective images aresubjected to Gabor wavelet transformation in multi-scales andmulti-orientations and a resultant image is obtained in L2 norm. The resultantimage is split into several periodic blocks and energy of each block is used asa feature space to automatically identify defective and defect-free blocksusing Ward's hierarchical clustering. Experiments on defective fabric images ofthree major wallpaper groups, namely, pmm, p2 and p4m, show that the proposedmethod is robust in finding fabric defects without human intervention and canbe used for automatic defect detection in fabric industries.
arxiv-1800-99 | Autonomous Navigation by Robust Scan Matching Technique | http://arxiv.org/pdf/1212.1313v1.pdf | author:Debajyoti Banerji, Ranjit Ray, Jhankar Basu, Indrajit Basak category:cs.CV cs.AI published:2012-12-06 summary:For effective autonomous navigation,estimation of the pose of the robot isessential at every sampling time. For computing an accurateestimation,odometric error needs to be reduced with the help of data fromexternal sensor. In this work, a technique has been developed for accurate poseestimation of mobile robot by using Laser Range data. The technique is robustto noisy data, which may contain considerable amount of outliers. A grey imageis formed from laser range data and the key points from this image areextracted by Harris corner detector. The matching of the key points fromconsecutive data sets have been done while outliers have been rejected byRANSAC method. Robot state is measured by the correspondence between the twosets of keypoints. Finally, optimal robot state is estimated by Extended KalmanFilter. The technique has been applied to an operational robot in thelaboratory environment to show the robustness of the technique in presence ofnoisy sensor data. The performance of this new technique has been compared withthat of conventional ICP method. Through this method, effective and accuratenavigation has been achieved even in presence of substantial noise in thesensor data at the cost of a small amount of additional computationalcomplexity.
arxiv-1800-100 | A Learning Theoretic Approach to Energy Harvesting Communication System Optimization | http://arxiv.org/pdf/1208.4290v2.pdf | author:Pol Blasco, Deniz Gündüz, Mischa Dohler category:cs.LG cs.NI published:2012-08-21 summary:A point-to-point wireless communication system in which the transmitter isequipped with an energy harvesting device and a rechargeable battery, isstudied. Both the energy and the data arrivals at the transmitter are modeledas Markov processes. Delay-limited communication is considered assuming thatthe underlying channel is block fading with memory, and the instantaneouschannel state information is available at both the transmitter and thereceiver. The expected total transmitted data during the transmitter'sactivation time is maximized under three different sets of assumptionsregarding the information available at the transmitter about the underlyingstochastic processes. A learning theoretic approach is introduced, which doesnot assume any a priori information on the Markov processes governing thecommunication system. In addition, online and offline optimization problems arestudied for the same setting. Full statistical knowledge and causal informationon the realizations of the underlying stochastic processes are assumed in theonline optimization problem, while the offline optimization problem assumesnon-causal knowledge of the realizations in advance. Comparing the optimalsolutions in all three frameworks, the performance loss due to the lack of thetransmitter's information regarding the behaviors of the underlying Markovprocesses is quantified.
arxiv-1800-101 | On the probabilistic continuous complexity conjecture | http://arxiv.org/pdf/1212.1263v1.pdf | author:Mark A. Kon category:stat.ML published:2012-12-06 summary:In this paper we prove the probabilistic continuous complexity conjecture. Incontinuous complexity theory, this states that the complexity of solving acontinuous problem with probability approaching 1 converges (in this limit) tothe complexity of solving the same problem in its worst case. We prove theconjecture holds if and only if space of problem elements is uniformly convex.The non-uniformly convex case has a striking counterexample in the problem ofidentifying a Brownian path in Wiener space, where it is shown thatprobabilistic complexity converges to only half of the worst case complexity inthis limit.
arxiv-1800-102 | On Some Integrated Approaches to Inference | http://arxiv.org/pdf/1212.1180v1.pdf | author:Mark A. Kon, Leszek Plaskota category:stat.ML cs.LG published:2012-12-05 summary:We present arguments for the formulation of unified approach to differentstandard continuous inference methods from partial information. It is claimedthat an explicit partition of information into a priori (prior knowledge) and aposteriori information (data) is an important way of standardizing inferenceapproaches so that they can be compared on a normative scale, and so thatnotions of optimal algorithms become farther-reaching. The inference methodsconsidered include neural network approaches, information-based complexity, andMonte Carlo, spline, and regularization methods. The model is an extension ofcurrently used continuous complexity models, with a class of algorithms in theform of optimization methods, in which an optimization functional (involvingthe data) is minimized. This extends the family of current approaches incontinuous complexity theory, which include the use of interpolatory algorithmsin worst and average case settings.
arxiv-1800-103 | Multiscale Markov Decision Problems: Compression, Solution, and Transfer Learning | http://arxiv.org/pdf/1212.1143v1.pdf | author:Jake Bouvrie, Mauro Maggioni category:cs.AI cs.SY math.OC stat.ML published:2012-12-05 summary:Many problems in sequential decision making and stochastic control often havenatural multiscale structure: sub-tasks are assembled together to accomplishcomplex goals. Systematically inferring and leveraging hierarchical structure,particularly beyond a single level of abstraction, has remained a longstandingchallenge. We describe a fast multiscale procedure for repeatedly compressing,or homogenizing, Markov decision processes (MDPs), wherein a hierarchy ofsub-problems at different scales is automatically determined. Coarsened MDPsare themselves independent, deterministic MDPs, and may be solved usingexisting algorithms. The multiscale representation delivered by this proceduredecouples sub-tasks from each other and can lead to substantial improvements inconvergence rates both locally within sub-problems and globally acrosssub-problems, yielding significant computational savings. A second fundamentalaspect of this work is that these multiscale decompositions yield new transferopportunities across different problems, where solutions of sub-tasks atdifferent levels of the hierarchy may be amenable to transfer to new problems.Localized transfer of policies and potential operators at arbitrary scales isemphasized. Finally, we demonstrate compression and transfer in a collection ofillustrative domains, including examples involving discrete and continuousstatespaces.
arxiv-1800-104 | Using Wikipedia to Boost SVD Recommender Systems | http://arxiv.org/pdf/1212.1131v1.pdf | author:Gilad Katz, Guy Shani, Bracha Shapira, Lior Rokach category:cs.LG cs.IR stat.ML published:2012-12-05 summary:Singular Value Decomposition (SVD) has been used successfully in recent yearsin the area of recommender systems. In this paper we present how this model canbe extended to consider both user ratings and information from Wikipedia. Bymapping items to Wikipedia pages and quantifying their similarity, we are ableto use this information in order to improve recommendation accuracy, especiallywhen the sparsity is high. Another advantage of the proposed approach is thefact that it can be easily integrated into any other SVD implementation,regardless of additional parameters that may have been added to it. Preliminaryexperimental results on the MovieLens dataset are encouraging.
arxiv-1800-105 | Making Early Predictions of the Accuracy of Machine Learning Applications | http://arxiv.org/pdf/1212.1100v1.pdf | author:J. E. Smith, P. Caleb-Solly, M. A. Tahir, D. Sannen, H. van-Brussel category:cs.LG cs.AI stat.ML I.2.6; I.5.2 published:2012-12-05 summary:The accuracy of machine learning systems is a widely studied research topic.Established techniques such as cross-validation predict the accuracy on unseendata of the classifier produced by applying a given learning method to a giventraining data set. However, they do not predict whether incurring the cost ofobtaining more data and undergoing further training will lead to higheraccuracy. In this paper we investigate techniques for making such earlypredictions. We note that when a machine learning algorithm is presented with atraining set the classifier produced, and hence its error, will depend on thecharacteristics of the algorithm, on training set's size, and also on itsspecific composition. In particular we hypothesise that if a number ofclassifiers are produced, and their observed error is decomposed into bias andvariance terms, then although these components may behave differently, theirbehaviour may be predictable. We test our hypothesis by building models that, given a measurement takenfrom the classifier created from a limited number of samples, predict thevalues that would be measured from the classifier produced when the full dataset is presented. We create separate models for bias, variance and total error.Our models are built from the results of applying ten different machinelearning algorithms to a range of data sets, and tested with "unseen"algorithms and datasets. We analyse the results for various numbers of initialtraining samples, and total dataset sizes. Results show that our predictionsare very highly correlated with the values observed after undertaking the extratraining. Finally we consider the more complex case where an ensemble ofheterogeneous classifiers is trained, and show how we can accurately estimatean upper bound on the accuracy achievable after further training.
arxiv-1800-106 | Kernels on Sample Sets via Nonparametric Divergence Estimates | http://arxiv.org/pdf/1202.0302v2.pdf | author:Dougal J. Sutherland, Liang Xiong, Barnabás Póczos, Jeff Schneider category:cs.LG stat.ML published:2012-02-01 summary:Most machine learning algorithms, such as classification or regression, treatthe individual data point as the object of interest. Here we consider extendingmachine learning algorithms to operate on groups of data points. We suggesttreating a group of data points as an i.i.d. sample set from an underlyingfeature distribution for that group. Our approach employs kernel machines witha kernel on i.i.d. sample sets of vectors. We define certain kernel functionson pairs of distributions, and then use a nonparametric estimator toconsistently estimate those functions based on sample sets. The projection ofthe estimated Gram matrix to the cone of symmetric positive semi-definitematrices enables us to use kernel machines for classification, regression,anomaly detection, and low-dimensional embedding in the space of distributions.We present several numerical experiments both on real and simulated datasets todemonstrate the advantages of our new approach.
arxiv-1800-107 | Compiling Relational Database Schemata into Probabilistic Graphical Models | http://arxiv.org/pdf/1212.0967v1.pdf | author:Sameer Singh, Thore Graepel category:cs.AI cs.DB cs.LG stat.ML published:2012-12-05 summary:Instead of requiring a domain expert to specify the probabilisticdependencies of the data, in this work we present an approach that uses therelational DB schema to automatically construct a Bayesian graphical model fora database. This resulting model contains customized distributions for columns,latent variables that cluster the data, and factors that reflect and representthe foreign key links. Experiments demonstrate the accuracy of the model andthe scalability of inference on synthetic and real-world data.
arxiv-1800-108 | Feature extraction in protein sequences classification : a new stability measure | http://arxiv.org/pdf/1206.4822v3.pdf | author:Rabie Saidi, Sabeur Aridhi, Mondher Maddouri, Engelbert Mephu Nguifo category:cs.LG cs.CE q-bio.QM published:2012-06-21 summary:Feature extraction is an unavoidable task, especially in the critical step ofpreprocessing biological sequences. This step consists for example intransforming the biological sequences into vectors of motifs where each motifis a subsequence that can be seen as a property (or attribute) characterizingthe sequence. Hence, we obtain an object-property table where objects aresequences and properties are motifs extracted from sequences. This output canbe used to apply standard machine learning tools to perform data mining taskssuch as classification. Several previous works have described featureextraction methods for bio-sequence classification, but none of them discussedthe robustness of these methods when perturbing the input data. In this work,we introduce the notion of stability of the generated motifs in order to studythe robustness of motif extraction methods. We express this robustness in termsof the ability of the method to reveal any change occurring in the input dataand also its ability to target the interesting motifs. We use these criteria toevaluate and experimentally compare four existing extraction methods forbiological sequences.
arxiv-1800-109 | Evaluating Classifiers Without Expert Labels | http://arxiv.org/pdf/1212.0960v1.pdf | author:Hyun Joon Jung, Matthew Lease category:cs.LG cs.IR stat.ML published:2012-12-05 summary:This paper considers the challenge of evaluating a set of classifiers, asdone in shared task evaluations like the KDD Cup or NIST TREC, without expertlabels. While expert labels provide the traditional cornerstone for evaluatingstatistical learners, limited or expensive access to experts represents apractical bottleneck. Instead, we seek methodology for estimating performanceof the classifiers which is more scalable than expert labeling yet preserveshigh correlation with evaluation based on expert labels. We consider both: 1)using only labels automatically generated by the classifiers (blindevaluation); and 2) using labels obtained via crowdsourcing. Whilecrowdsourcing methods are lauded for scalability, using such data forevaluation raises serious concerns given the prevalence of label noise. Inregard to blind evaluation, two broad strategies are investigated: combine &score and score & combine methods infer a single pseudo-gold label set byaggregating classifier labels; classifiers are then evaluated based on thissingle pseudo-gold label set. On the other hand, score & combine methods: 1)sample multiple label sets from classifier outputs, 2) evaluate classifiers oneach label set, and 3) average classifier performance across label sets. Whenadditional crowd labels are also collected, we investigate two alternativeavenues for exploiting them: 1) direct evaluation of classifiers; or 2)supervision of combine & score methods. To assess generality of our techniques,classifier performance is measured using four common classification metrics,with statistical significance tests. Finally, we measure both score and rankcorrelations between estimated classifier performance vs. actual performanceaccording to expert judgments. Rigorous evaluation of classifiers from the TREC2011 Crowdsourcing Track shows reliable evaluation can be achieved withoutreliance on expert labels.
arxiv-1800-110 | Multiclass Diffuse Interface Models for Semi-Supervised Learning on Graphs | http://arxiv.org/pdf/1212.0945v1.pdf | author:Cristina Garcia-Cardona, Arjuna Flenner, Allon G. Percus category:stat.ML cs.LG math.ST stat.TH I.5.3 published:2012-12-05 summary:We present a graph-based variational algorithm for multiclass classificationof high-dimensional data, motivated by total variation techniques. The energyfunctional is based on a diffuse interface model with a periodic potential. Weaugment the model by introducing an alternative measure of smoothness thatpreserves symmetry among the class labels. Through this modification of thestandard Laplacian, we construct an efficient multiclass method that allows forsharp transitions between classes. The experimental results demonstrate thatour approach is competitive with the state of the art among other graph-basedalgorithms.
arxiv-1800-111 | Sparse seismic imaging using variable projection | http://arxiv.org/pdf/1212.0912v1.pdf | author:Aleksandr Y. Aravkin, Tristan van Leeuwen, Ning Tu category:math.OC stat.ML published:2012-12-05 summary:We consider an important class of signal processing problems where the signalof interest is known to be sparse, and can be recovered from data givenauxiliary information about how the data was generated. For example, a sparseGreen's function may be recovered from seismic experimental data using sparsityoptimization when the source signature is known. Unfortunately, in practicethis information is often missing, and must be recovered from data along withthe signal using deconvolution techniques. In this paper, we present a novel methodology to simultaneously solve for thesparse signal and auxiliary parameters using a recently proposed variableprojection technique. Our main contribution is to combine variable projectionwith sparsity promoting optimization, obtaining an efficient algorithm forlarge-scale sparse deconvolution problems. We demonstrate the algorithm on aseismic imaging example.
arxiv-1800-112 | Fast approximation of matrix coherence and statistical leverage | http://arxiv.org/pdf/1109.3843v2.pdf | author:Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, David P. Woodruff category:cs.DS cs.DM cs.LG published:2011-09-18 summary:The statistical leverage scores of a matrix $A$ are the squared row-norms ofthe matrix containing its (top) left singular vectors and the coherence is thelargest leverage score. These quantities are of interest in recently-popularproblems such as matrix completion and Nystr\"{o}m-based low-rank matrixapproximation as well as in large-scale statistical data analysis applicationsmore generally; moreover, they are of interest since they define the keystructural nonuniformity that must be dealt with in developing fast randomizedmatrix algorithms. Our main result is a randomized algorithm that takes asinput an arbitrary $n \times d$ matrix $A$, with $n \gg d$, and that returns asoutput relative-error approximations to all $n$ of the statistical leveragescores. The proposed algorithm runs (under assumptions on the precise values of$n$ and $d$) in $O(n d \log n)$ time, as opposed to the $O(nd^2)$ time requiredby the na\"{i}ve algorithm that involves computing an orthogonal basis for therange of $A$. Our analysis may be viewed in terms of computing a relative-errorapproximation to an underconstrained least-squares approximation problem, or,relatedly, it may be viewed as an application of Johnson-Lindenstrauss typeideas. Several practically-important extensions of our basic result are alsodescribed, including the approximation of so-called cross-leverage scores, theextension of these ideas to matrices with $n \approx d$, and the extension tostreaming environments.
arxiv-1800-113 | Unmixing of Hyperspectral Data Using Robust Statistics-based NMF | http://arxiv.org/pdf/1212.0888v1.pdf | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2012-12-04 summary:Mixed pixels are presented in hyperspectral images due to low spatialresolution of hyperspectral sensors. Spectral unmixing decomposes mixed pixelsspectra into endmembers spectra and abundance fractions. In this paper using ofrobust statistics-based nonnegative matrix factorization (RNMF) for spectralunmixing of hyperspectral data is investigated. RNMF uses a robust costfunction and iterative updating procedure, so is not sensitive to outliers.This method has been applied to simulated data using USGS spectral library,AVIRIS and ROSIS datasets. Unmixing results are compared to traditional NMFmethod based on SAD and AAD measures. Results demonstrate that this method canbe used efficiently for hyperspectral unmixing purposes.
arxiv-1800-114 | Fast Variational Inference in the Conjugate Exponential Family | http://arxiv.org/pdf/1206.5162v2.pdf | author:James Hensman, Magnus Rattray, Neil D. Lawrence category:cs.LG stat.ML published:2012-06-22 summary:We present a general method for deriving collapsed variational inferencealgo- rithms for probabilistic models in the conjugate exponential family. Ourmethod unifies many existing approaches to collapsed variational inference. Ourcollapsed variational inference leads to a new lower bound on the marginallikelihood. We exploit the information geometry of the bound to derive muchfaster optimization methods based on conjugate gradients for these models. Ourapproach is very general and is easily applied to any model where the meanfield update equations have been derived. Empirically we show significantspeed-ups for probabilistic models optimized using our bound.
arxiv-1800-115 | A Topological Code for Plane Images | http://arxiv.org/pdf/1212.0819v1.pdf | author:Evgeny Shchepin category:cs.CV math.GT published:2012-12-04 summary:It is proposed a new code for contours of plane images. This code was appliedfor optical character recognition of printed and handwritten characters. Onecan apply it to recognition of any visual images.
arxiv-1800-116 | A simple non-parametric Topic Mixture for Authors and Documents | http://arxiv.org/pdf/1211.6248v2.pdf | author:Arnim Bleier category:cs.LG stat.ML published:2012-11-27 summary:This article reviews the Author-Topic Model and presents a new non-parametricextension based on the Hierarchical Dirichlet Process. The extension isespecially suitable when no prior information about the number of componentsnecessary is available. A blocked Gibbs sampler is described and focus put onstaying as close as possible to the original model with only the minimum oftheoretical and implementation overhead necessary.
arxiv-1800-117 | Training Support Vector Machines Using Frank-Wolfe Optimization Methods | http://arxiv.org/pdf/1212.0695v1.pdf | author:Emanuele Frandi, Ricardo Nanculef, Maria Grazia Gasparo, Stefano Lodi, Claudio Sartori category:cs.LG cs.CV math.OC stat.ML published:2012-12-04 summary:Training a Support Vector Machine (SVM) requires the solution of a quadraticprogramming problem (QP) whose computational complexity becomes prohibitivelyexpensive for large scale datasets. Traditional optimization methods cannot bedirectly applied in these cases, mainly due to memory restrictions. By adopting a slightly different objective function and under mild conditionson the kernel used within the model, efficient algorithms to train SVMs havebeen devised under the name of Core Vector Machines (CVMs). This frameworkexploits the equivalence of the resulting learning problem with the task ofbuilding a Minimal Enclosing Ball (MEB) problem in a feature space, where datais implicitly embedded by a kernel function. In this paper, we improve on the CVM approach by proposing two novel methodsto build SVMs based on the Frank-Wolfe algorithm, recently revisited as a fastmethod to approximate the solution of a MEB problem. In contrast to CVMs, ouralgorithms do not require to compute the solutions of a sequence ofincreasingly complex QPs and are defined by using only analytic optimizationsteps. Experiments on a large collection of datasets show that our methodsscale better than CVMs in most cases, sometimes at the price of a slightlylower accuracy. As CVMs, the proposed methods can be easily extended to machinelearning problems other than binary classification. However, effectiveclassifiers are also obtained using kernels which do not satisfy the conditionrequired by CVMs and can thus be used for a wider set of problems.
arxiv-1800-118 | Separate Training for Conditional Random Fields Using Co-occurrence Rate Factorization | http://arxiv.org/pdf/1008.1566v5.pdf | author:Zhemin Zhu, Djoerd Hiemstra, Peter Apers, Andreas Wombacher category:cs.LG cs.AI published:2010-08-09 summary:The standard training method of Conditional Random Fields (CRFs) is very slowfor large-scale applications. As an alternative, piecewise training divides thefull graph into pieces, trains them independently, and combines the learnedweights at test time. In this paper, we present \emph{separate} training forundirected models based on the novel Co-occurrence Rate Factorization (CR-F).Separate training is a local training method. In contrast to MEMMs, separatetraining is unaffected by the label bias problem. Experiments show thatseparate training (i) is unaffected by the label bias problem; (ii) reduces thetraining time from weeks to seconds; and (iii) obtains competitive results tothe standard and piecewise training on linear-chain CRFs.
arxiv-1800-119 | Evaluation of Particle Swarm Optimization Algorithms for Weighted Max-Sat Problem: Technical Report | http://arxiv.org/pdf/1212.0639v1.pdf | author:Osama Khalil category:cs.NE published:2012-12-04 summary:An experimental evaluation is conducted to asses the performance of 4different Particle Swarm Optimization neighborhood structures in solvingMax-Sat problem. The experiment has shown that none of the algorithms achievesstatistically significant performance over the others under confidence level of0.05.
arxiv-1800-120 | On best subset regression | http://arxiv.org/pdf/1112.0918v2.pdf | author:Shifeng Xiong category:stat.ME stat.CO stat.ML 62J07, 62F12 G.3 published:2011-12-05 summary:In this paper we discuss the variable selection method from \ell0-normconstrained regression, which is equivalent to the problem of finding the bestsubset of a fixed size. Our study focuses on two aspects, consistency andcomputation. We prove that the sparse estimator from such a method can retainall of the important variables asymptotically for even exponentially growingdimensionality under regularity conditions. This indicates that the best subsetregression method can efficiently shrink the full model down to a submodel of asize less than the sample size, which can be analyzed by well-developedregression techniques for such cases in a follow-up study. We provide aniterative algorithm, called orthogonalizing subset selection (OSS), to addresscomputational issues in best subset regression. OSS is an EM algorithm, andthus possesses the monotonicity property. For any sparse estimator, OSS canimprove its fit of the model by putting it as an initial point. After thisimprovement, the sparsity of the estimator is kept. Another appealing featureof OSS is that, similarly to an effective algorithm for a continuousoptimization problem, OSS can converge to the global solution to the \ell0-normconstrained regression problem if the initial point lies in a neighborhood ofthe global solution. An accelerating algorithm of OSS and its combination withforward stepwise selection are also investigated. Simulations and a realexample are presented to evaluate the performances of the proposed methods.
arxiv-1800-121 | Low-rank Matrix Completion using Alternating Minimization | http://arxiv.org/pdf/1212.0467v1.pdf | author:Prateek Jain, Praneeth Netrapalli, Sujay Sanghavi category:stat.ML cs.LG math.OC published:2012-12-03 summary:Alternating minimization represents a widely applicable and empiricallysuccessful approach for finding low-rank matrices that best fit the given data.For example, for the problem of low-rank matrix completion, this method isbelieved to be one of the most accurate and efficient, and formed a majorcomponent of the winning entry in the Netflix Challenge. In the alternating minimization approach, the low-rank target matrix iswritten in a bi-linear form, i.e. $X = UV^\dag$; the algorithm then alternatesbetween finding the best $U$ and the best $V$. Typically, each alternating stepin isolation is convex and tractable. However the overall problem becomesnon-convex and there has been almost no theoretical understanding of when thisapproach yields a good result. In this paper we present first theoretical analysis of the performance ofalternating minimization for matrix completion, and the related problem ofmatrix sensing. For both these problems, celebrated recent results have shownthat they become well-posed and tractable once certain (now standard)conditions are imposed on the problem. We show that alternating minimizationalso succeeds under similar conditions. Moreover, compared to existing results,our paper shows that alternating minimization guarantees faster (in particular,geometric) convergence to the true matrix, while allowing a simpler analysis.
arxiv-1800-122 | Time series forecasting: model evaluation and selection using nonparametric risk bounds | http://arxiv.org/pdf/1212.0463v1.pdf | author:Daniel J. McDonald, Cosma Rohilla Shalizi, Mark Schervish category:math.ST cs.LG stat.ML stat.TH published:2012-12-03 summary:We derive generalization error bounds --- bounds on the expected inaccuracyof the predictions --- for traditional time series forecasting models. Ourresults hold for many standard forecasting tools including autoregressivemodels, moving average models, and, more generally, linear state-space models.These bounds allow forecasters to select among competing models and toguarantee that with high probability, their chosen model will perform wellwithout making strong assumptions about the data generating process orappealing to asymptotic theory. We motivate our techniques with and apply themto standard economic and financial forecasting tools --- a GARCH model forpredicting equity volatility and a dynamic stochastic general equilibrium model(DSGE), the standard tool in macroeconomic forecasting. We demonstrate inparticular how our techniques can aid forecasters and policy makers in choosingmodels which behave well under uncertainty and mis-specification.
arxiv-1800-123 | Compressive Schlieren Deflectometry | http://arxiv.org/pdf/1212.0433v1.pdf | author:Prasad Sudhakar, Laurent Jacques, Xavier Dubois, Philippe Antoine, Luc Joannes category:cs.CV published:2012-12-03 summary:Schlieren deflectometry aims at characterizing the deflections undergone byrefracted incident light rays at any surface point of a transparent object. Forsmooth surfaces, each surface location is actually associated with a sparsedeflection map (or spectrum). This paper presents a novel method tocompressively acquire and reconstruct such spectra. This is achieved byaltering the way deflection information is captured in a common SchlierenDeflectometer, i.e., the deflection spectra are indirectly observed by theprinciple of spread spectrum compressed sensing. These observations arerealized optically using a 2-D Spatial Light Modulator (SLM) adjusted to thecorresponding sensing basis and whose modulations encode the light deviationsubsequently recorded by a CCD camera. The efficiency of this approach isdemonstrated experimentally on the observation of few test objects. Further,using a simple parametrization of the deflection spectra we show that relevantkey parameters can be directly computed using the measurements, avoiding fullreconstruction.
arxiv-1800-124 | UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild | http://arxiv.org/pdf/1212.0402v1.pdf | author:Khurram Soomro, Amir Roshan Zamir, Mubarak Shah category:cs.CV published:2012-12-03 summary:We introduce UCF101 which is currently the largest dataset of human actions.It consists of 101 action classes, over 13k clips and 27 hours of video data.The database consists of realistic user uploaded videos containing cameramotion and cluttered background. Additionally, we provide baseline actionrecognition results on this new dataset using standard bag of words approachwith overall performance of 44.5%. To the best of our knowledge, UCF101 iscurrently the most challenging dataset of actions due to its large number ofclasses, large number of clips and also unconstrained nature of such clips.
arxiv-1800-125 | Hypergraph and protein function prediction with gene expression data | http://arxiv.org/pdf/1212.0388v1.pdf | author:Loc Tran category:stat.ML cs.LG q-bio.QM G.2.2 published:2012-12-03 summary:Most network-based protein (or gene) function prediction methods are based onthe assumption that the labels of two adjacent proteins in the network arelikely to be the same. However, assuming the pairwise relationship betweenproteins or genes is not complete, the information a group of genes that showvery similar patterns of expression and tend to have similar functions (i.e.the functional modules) is missed. The natural way overcoming the informationloss of the above assumption is to represent the gene expression data as thehypergraph. Thus, in this paper, the three un-normalized, random walk, andsymmetric normalized hypergraph Laplacian based semi-supervised learningmethods applied to hypergraph constructed from the gene expression data inorder to predict the functions of yeast proteins are introduced. Experimentresults show that the average accuracy performance measures of these threehypergraph Laplacian based semi-supervised learning methods are the same.However, their average accuracy performance measures of these three methods aremuch greater than the average accuracy performance measures of un-normalizedgraph Laplacian based semi-supervised learning method (i.e. the baseline methodof this paper) applied to gene co-expression network created from the geneexpression data.
arxiv-1800-126 | GLCM-based chi-square histogram distance for automatic detection of defects on patterned textures | http://arxiv.org/pdf/1212.0383v1.pdf | author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV I.0; I.2.10 published:2012-12-03 summary:Chi-square histogram distance is one of the distance measures that can beused to find dissimilarity between two histograms. Motivated by the fact thattexture discrimination by human vision system is based on second-orderstatistics, we make use of histogram of gray-level co-occurrence matrix (GLCM)that is based on second-order statistics and propose a new machine visionalgorithm for automatic defect detection on patterned textures. Input defectiveimages are split into several periodic blocks and GLCMs are computed afterquantizing the gray levels from 0-255 to 0-63 to keep the size of GLCM compactand to reduce computation time. Dissimilarity matrix derived from chi-squaredistances of the GLCMs is subjected to hierarchical clustering to automaticallyidentify defective and defect-free blocks. Effectiveness of the proposed methodis demonstrated through experiments on defective real-fabric images of 2 majorwallpaper groups (pmm and p4m groups).
arxiv-1800-127 | Dynamic recommender system : using cluster-based biases to improve the accuracy of the predictions | http://arxiv.org/pdf/1212.0763v1.pdf | author:Modou Gueye, Talel Abdessalem, Hubert Naacke category:cs.LG cs.DB cs.IR H.2.8; H.3.3 published:2012-12-03 summary:It is today accepted that matrix factorization models allow a high quality ofrating prediction in recommender systems. However, a major drawback of matrixfactorization is its static nature that results in a progressive declining ofthe accuracy of the predictions after each factorization. This is due to thefact that the new obtained ratings are not taken into account until a newfactorization is computed, which can not be done very often because of the highcost of matrix factorization. In this paper, aiming at improving the accuracy of recommender systems, wepropose a cluster-based matrix factorization technique that enables onlineintegration of new ratings. Thus, we significantly enhance the obtainedpredictions between two matrix factorizations. We use finer-grained user biasesby clustering similar items into groups, and allocating in these groups a biasto each user. The experiments we did on large datasets demonstrated theefficiency of our approach.
arxiv-1800-128 | Comparison of Fuzzy and Neuro Fuzzy Image Fusion Techniques and its Applications | http://arxiv.org/pdf/1212.0318v1.pdf | author:D. Srinivasa Rao, M. Seetha, M. H. M. Krishna Prasad category:cs.CV published:2012-12-03 summary:Image fusion is the process of integrating multiple images of the same sceneinto a single fused image to reduce uncertainty and minimizing redundancy whileextracting all the useful information from the source images. Image fusionprocess is required for different applications like medical imaging, remotesensing, medical imaging, machine vision, biometrics and military applicationswhere quality and critical information is required. In this paper, image fusionusing fuzzy and neuro fuzzy logic approaches utilized to fuse images fromdifferent sensors, in order to enhance visualization. The proposed work furtherexplores comparison between fuzzy based image fusion and neuro fuzzy fusiontechnique along with quality evaluation indices for image fusion like imagequality index, mutual information measure, fusion factor, fusion symmetry,fusion index, root mean square error, peak signal to noise ratio, entropy,correlation coefficient and spatial frequency. Experimental results obtainedfrom fusion process prove that the use of the neuro fuzzy based image fusionapproach shows better performance in first two test cases while in the thirdtest case fuzzy based image fusion technique gives better results.
arxiv-1800-129 | An Image Based Technique for Enhancement of Underwater Images | http://arxiv.org/pdf/1212.0291v1.pdf | author:C. J. Prabhakar, P. U. Praveen Kumar category:cs.CV published:2012-12-03 summary:The underwater images usually suffers from non-uniform lighting, lowcontrast, blur and diminished colors. In this paper, we proposed an image basedpreprocessing technique to enhance the quality of the underwater images. Theproposed technique comprises a combination of four filters such as homomorphicfiltering, wavelet denoising, bilateral filter and contrast equalization. Thesefilters are applied sequentially on degraded underwater images. The literaturesurvey reveals that image based preprocessing algorithms uses standard filtertechniques with various combinations. For smoothing the image, the image basedpreprocessing algorithms uses the anisotropic filter. The main drawback of theanisotropic filter is that iterative in nature and computation time is highcompared to bilateral filter. In the proposed technique, in addition to otherthree filters, we employ a bilateral filter for smoothing the image. Theexperimentation is carried out in two stages. In the first stage, we haveconducted various experiments on captured images and estimated optimalparameters for bilateral filter. Similarly, optimal filter bank and optimalwavelet shrinkage function are estimated for wavelet denoising. In the secondstage, we conducted the experiments using estimated optimal parameters, optimalfilter bank and optimal wavelet shrinkage function for evaluating the proposedtechnique. We evaluated the technique using quantitative based criteria such asa gradient magnitude histogram and Peak Signal to Noise Ratio (PSNR). Further,the results are qualitatively evaluated based on edge detection results. Theproposed technique enhances the quality of the underwater images and can beemployed prior to apply computer vision techniques.
arxiv-1800-130 | Simplification and integration in computing and cognition: the SP theory and the multiple alignment concept | http://arxiv.org/pdf/1212.0229v1.pdf | author:James Gerard Wolff category:cs.AI cs.CL published:2012-12-02 summary:The main purpose of this article is to describe potential benefits andapplications of the SP theory, a unique attempt to simplify and integrate ideasacross artificial intelligence, mainstream computing and human cognition, withinformation compression as a unifying theme. The theory, including a concept ofmultiple alignment, combines conceptual simplicity with descriptive andexplanatory power in several areas including representation of knowledge,natural language processing, pattern recognition, several kinds of reasoning,the storage and retrieval of information, planning and problem solving,unsupervised learning, information compression, and human perception andcognition. In the SP machine -- an expression of the SP theory which iscurrently realised in the form of computer models -- there is potential for anoverall simplification of computing systems, including software. As a theorywith a broad base of support, the SP theory promises useful insights in manyareas and the integration of structures and functions, both within a given areaand amongst different areas. There are potential benefits in natural languageprocessing (with potential for the understanding and translation of naturallanguages), the need for a versatile intelligence in autonomous robots,computer vision, intelligent databases, maintaining multiple versions ofdocuments or web pages, software engineering, criminal investigations, themanagement of big data and gaining benefits from it, the semantic web, medicaldiagnosis, the detection of computer viruses, the economical transmission ofdata, and data fusion. Further development of these ideas would be facilitatedby the creation of a high-parallel, web-based, open-source version of the SPmachine, with a good user interface. This would provide a means for researchersto explore what can be done with the system and to refine it.
arxiv-1800-131 | Metaheuristic Optimization: Algorithm Analysis and Open Problems | http://arxiv.org/pdf/1212.0220v1.pdf | author:Xin-She Yang category:math.OC cs.NE 90C26 published:2012-12-02 summary:Metaheuristic algorithms are becoming an important part of modernoptimization. A wide range of metaheuristic algorithms have emerged over thelast two decades, and many metaheuristics such as particle swarm optimizationare becoming increasingly popular. Despite their popularity, mathematicalanalysis of these algorithms lacks behind. Convergence analysis still remainsunsolved for the majority of metaheuristic algorithms, while efficiencyanalysis is equally challenging. In this paper, we intend to provide anoverview of convergence and efficiency studies of metaheuristics, and try toprovide a framework for analyzing metaheuristics in terms of convergence andefficiency. This can form a basis for analyzing other algorithms. We alsooutline some open questions as further research topics.
arxiv-1800-132 | Artificial Neural Network for Performance Modeling and Optimization of CMOS Analog Circuits | http://arxiv.org/pdf/1212.0215v1.pdf | author:Mriganka Chakraborty category:cs.NE published:2012-12-02 summary:This paper presents an implementation of multilayer feed forward neuralnetworks (NN) to optimize CMOS analog circuits. For modeling and designrecently neural network computational modules have got acceptance as anunorthodox and useful tool. To achieve high performance of active or passivecircuit component neural network can be trained accordingly. A well trainedneural network can produce more accurate outcome depending on its learningcapability. Neural network model can replace empirical modeling solutionslimited by range and accuracy.[2] Neural network models are easy to obtain fornew circuits or devices which can replace analytical methods. Numericalmodeling methods can also be replaced by neural network model due to theircomputationally expansive behavior.[2][10][20]. The pro- posed implementationis aimed at reducing resource requirement, without much compromise on thespeed. The NN ensures proper functioning by assigning the appropriate inputs,weights, biases, and excitation function of the layer that is currently beingcomputed. The concept used is shown to be very effective in reducing resourcerequirements and enhancing speed.
arxiv-1800-133 | Message-Passing Algorithms for Quadratic Minimization | http://arxiv.org/pdf/1212.0171v1.pdf | author:Nicholas Ruozzi, Sekhar Tatikonda category:cs.IT cs.LG math.IT stat.ML published:2012-12-02 summary:Gaussian belief propagation (GaBP) is an iterative algorithm for computingthe mean of a multivariate Gaussian distribution, or equivalently, the minimumof a multivariate positive definite quadratic function. Sufficient conditions,such as walk-summability, that guarantee the convergence and correctness ofGaBP are known, but GaBP may fail to converge to the correct solution given anarbitrary positive definite quadratic function. As was observed in previouswork, the GaBP algorithm fails to converge if the computation trees produced bythe algorithm are not positive definite. In this work, we will show that thefailure modes of the GaBP algorithm can be understood via graph covers, and weprove that a parameterized generalization of the min-sum algorithm can be usedto ensure that the computation trees remain positive definite whenever theinput matrix is positive definite. We demonstrate that the resulting algorithmis closely related to other iterative schemes for quadratic minimization suchas the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically,that there always exists a choice of parameters such that the abovegeneralization of the GaBP algorithm converges.
arxiv-1800-134 | An Evolution Strategy Approach toward Rule-set Generation for Network Intrusion Detection Systems (IDS) | http://arxiv.org/pdf/1212.0170v1.pdf | author:Herve Kabamba Mbikayi category:cs.CR cs.NE published:2012-12-01 summary:With the increasing number of intrusions in system and networkinfrastructures, Intrusion Detection Systems (IDS) have become an active areaof research to develop reliable and effective solutions to detect and counterthem. The use of Evolutionary Algorithms in IDS has proved its maturity overthe times. Although most of the research works have been based on the use ofgenetic algorithms in IDS, this paper presents an approach toward thegeneration of rules for the identification of anomalous connections usingevolution strategies . The emphasis is given on how the problem can be modeledinto ES primitives and how the fitness of the population can be evaluated inorder to find the local optima, therefore resulting in optimal rules that canbe used for detecting intrusions in intrusion detection systems.
arxiv-1800-135 | Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning | http://arxiv.org/pdf/1207.3859v3.pdf | author:Ulugbek S. Kamilov, Sundeep Rangan, Alyson K. Fletcher, Michael Unser category:cs.IT cs.LG math.IT published:2012-07-17 summary:We consider the estimation of an i.i.d. (possibly non-Gaussian) vector $\xbf\in \R^n$ from measurements $\ybf \in \R^m$ obtained by a general cascade modelconsisting of a known linear transform followed by a probabilisticcomponentwise (possibly nonlinear) measurement channel. A novel method, calledadaptive generalized approximate message passing (Adaptive GAMP), that enablesjoint learning of the statistics of the prior and measurement channel alongwith estimation of the unknown vector $\xbf$ is presented. The proposedalgorithm is a generalization of a recently-developed EM-GAMP that usesexpectation-maximization (EM) iterations where the posteriors in the E-stepsare computed via approximate message passing. The methodology can be applied toa large class of learning problems including the learning of sparse priors incompressed sensing or identification of linear-nonlinear cascade models indynamical systems and neural spiking processes. We prove that for large i.i.d.Gaussian transform matrices the asymptotic componentwise behavior of theadaptive GAMP algorithm is predicted by a simple set of scalar state evolutionequations. In addition, we show that when a certain maximum-likelihoodestimation can be performed in each step, the adaptive GAMP method can yieldasymptotically consistent parameter estimates, which implies that the algorithmachieves a reconstruction quality equivalent to the oracle algorithm that knowsthe correct parameter values. Remarkably, this result applies to essentiallyarbitrary parametrizations of the unknown distributions, including ones thatare nonlinear and non-Gaussian. The adaptive GAMP methodology thus provides asystematic, general and computationally efficient method applicable to a largerange of complex linear-nonlinear models with provable guarantees.
arxiv-1800-136 | Cumulative Step-size Adaptation on Linear Functions | http://arxiv.org/pdf/1212.0139v1.pdf | author:Alexandre Chotard, Anne Auger, Nikolaus Hansen category:cs.LG stat.ML published:2012-12-01 summary:The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation,where the step size is adapted measuring the length of a so-called cumulativepath. The cumulative path is a combination of the previous steps realized bythe algorithm, where the importance of each step decreases with time. Thisarticle studies the CSA-ES on composites of strictly increasing functions withaffine linear functions through the investigation of its underlying Markovchains. Rigorous results on the change and the variation of the step size arederived with and without cumulation. The step-size diverges geometrically fastin most cases. Furthermore, the influence of the cumulation parameter isstudied.
arxiv-1800-137 | Fingertip Detection: A Fast Method with Natural Hand | http://arxiv.org/pdf/1212.0134v1.pdf | author:J. L. Raheja, Karen Das, Ankit Chaudhary category:cs.CV published:2012-12-01 summary:Many vision based applications have used fingertips to track or manipulategestures in their applications. Gesture identification is a natural way to passthe signals to the machine, as the human express its feelings most of the timewith hand expressions. Here a novel time efficient algorithm has been describedfor fingertip detection. This method is invariant to hand direction and inpreprocessing it cuts only hand part from the full image, hence furthercomputation would be much faster than processing full image. Binary silhouetteof the input image is generated using HSV color space based skin filter andhand cropping done based on intensity histogram of the hand image
arxiv-1800-138 | From the decoding of cortical activities to the control of a JACO robotic arm: a whole processing chain | http://arxiv.org/pdf/1212.0083v1.pdf | author:Laurent Bougrain, Olivier Rochel, Octave Boussaton, Lionel Havet category:cs.NE cs.HC cs.RO q-bio.NC published:2012-12-01 summary:This paper presents a complete processing chain for decoding intracranialdata recorded in the cortex of a monkey and replicates the associated movementson a JACO robotic arm by Kinova. We developed specific modules inside theOpenViBE platform in order to build a Brain-Machine Interface able to read thedata, compute the position of the robotic finger and send this position to therobotic arm. More pre- cisely, two client/server protocols have been tested totransfer the finger positions: VRPN and a light protocol based on TCP/IPsockets. According to the requested finger position, the server calls theassoci- ated functions of an API by Kinova to move the fin- gers properly.Finally, we monitor the gap between the requested and actual fingers positions.This chain can be generalized to any movement of the arm or wrist.
arxiv-1800-139 | Challenges in Kurdish Text Processing | http://arxiv.org/pdf/1212.0074v1.pdf | author:Kyumars Sheykh Esmaili category:cs.IR cs.CL published:2012-12-01 summary:Despite having a large number of speakers, the Kurdish language is among theless-resourced languages. In this work we highlight the challenges and problemsin providing the required tools and techniques for processing texts written inKurdish. From a high-level perspective, the main challenges are: the inherentdiversity of the language, standardization and segmentation issues, and thelack of language resources.
arxiv-1800-140 | Artificial Neural Network Fuzzy Inference System (ANFIS) For Brain Tumor Detection | http://arxiv.org/pdf/1212.0059v1.pdf | author:Minakshi Sharma category:cs.CV cs.AI published:2012-12-01 summary:Detection and segmentation of Brain tumor is very important because itprovides anatomical information of normal and abnormal tissues which helps intreatment planning and patient follow-up. There are number of techniques forimage segmentation. Proposed research work uses ANFIS (Artificial NeuralNetwork Fuzzy Inference System) for image classification and then compares theresults with FCM (Fuzzy C means) and K-NN (K-nearest neighbor). ANFIS includesbenefits of both ANN and the fuzzy logic systems. A comprehensive feature setand fuzzy rules are selected to classify an abnormal image to the correspondingtumor type. Experimental results illustrate promising results in terms ofclassification accuracy. A comparative analysis is performed with the FCM andK-NN to show the superior nature of ANFIS systems.
arxiv-1800-141 | Simulation-based optimal Bayesian experimental design for nonlinear systems | http://arxiv.org/pdf/1108.4146v3.pdf | author:Xun Huan, Youssef M. Marzouk category:stat.ML stat.CO stat.ME published:2011-08-20 summary:The optimal selection of experimental conditions is essential to maximizingthe value of data for inference and prediction, particularly in situationswhere experiments are time-consuming and expensive to conduct. We propose ageneral mathematical framework and an algorithmic approach for optimalexperimental design with nonlinear simulation-based models; in particular, wefocus on finding sets of experiments that provide the most information abouttargeted sets of parameters. Our framework employs a Bayesian statistical setting, which provides afoundation for inference from noisy, indirect, and incomplete data, and anatural mechanism for incorporating heterogeneous sources of information. Anobjective function is constructed from information theoretic measures,reflecting expected information gain from proposed combinations of experiments.Polynomial chaos approximations and a two-stage Monte Carlo sampling method areused to evaluate the expected information gain. Stochastic approximationalgorithms are then used to make optimization feasible in computationallyintensive and high-dimensional settings. These algorithms are demonstrated onmodel problems and on nonlinear parameter estimation problems arising indetailed combustion kinetics.
arxiv-1800-142 | Secure voice based authentication for mobile devices: Vaulted Voice Verification | http://arxiv.org/pdf/1212.0042v1.pdf | author:R. C. Johnson, Walter J. Scheirer, Terrance E. Boult category:cs.CR cs.CV published:2012-11-30 summary:As the use of biometrics becomes more wide-spread, the privacy concerns thatstem from the use of biometrics are becoming more apparent. As the usage ofmobile devices grows, so does the desire to implement biometric identificationinto such devices. A large majority of mobile devices being used are mobilephones. While work is being done to implement different types of biometricsinto mobile phones, such as photo based biometrics, voice is a more naturalchoice. The idea of voice as a biometric identifier has been around a longtime. One of the major concerns with using voice as an identifier is theinstability of voice. We have developed a protocol that addresses thoseinstabilities and preserves privacy. This paper describes a novel protocol thatallows a user to authenticate using voice on a mobile/remote device withoutcompromising their privacy. We first discuss the \vv protocol, which hasrecently been introduced in research literature, and then describe itslimitations. We then introduce a novel adaptation and extension of the vaultedverification protocol to voice, dubbed $V^3$. Following that we show aperformance evaluation and then conclude with a discussion of security andfuture work.
arxiv-1800-143 | Viewpoint Invariant Object Detector | http://arxiv.org/pdf/1212.0030v1.pdf | author:Osama Khalil, Andrew Habib category:cs.CV published:2012-11-30 summary:Object Detection is the task of identifying the existence of an object classinstance and locating it within an image. Difficulties in handling highintra-class variations constitute major obstacles to achieving high performanceon standard benchmark datasets (scale, viewpoint, lighting conditions andorientation variations provide good examples). Suggested model aims atproviding more robustness to detecting objects suffering severe distortion dueto < 60{\deg} viewpoint changes. In addition, several model computationalbottlenecks have been resolved leading to a significant increase in the modelperformance (speed and space) without compromising the resulting accuracy.Finally, we produced two illustrative applications showing the potential of theobject detection technology being deployed in real life applications; namelycontent-based image search and content-based video search.
arxiv-1800-144 | Approximate Rank-Detecting Factorization of Low-Rank Tensors | http://arxiv.org/pdf/1211.7369v1.pdf | author:Franz J. Király, Andreas Ziehe category:stat.ML cs.LG math.NA published:2012-11-30 summary:We present an algorithm, AROFAC2, which detects the (CP-)rank of a degree 3tensor and calculates its factorization into rank-one components. We providegenerative conditions for the algorithm to work and demonstrate on bothsynthetic and real world data that AROFAC2 is a potentially outperformingalternative to the gold standard PARAFAC over which it has the advantages thatit can intrinsically detect the true rank, avoids spurious components, and isstable with respect to outliers and non-Gaussian noise.
arxiv-1800-145 | A recursive divide-and-conquer approach for sparse principal component analysis | http://arxiv.org/pdf/1211.7219v1.pdf | author:Qian Zhao, Deyu Meng, Zongben Xu category:cs.CV cs.LG stat.ML 62H25, 68T10 I.5.0; I.5.1 published:2012-11-30 summary:In this paper, a new method is proposed for sparse PCA based on the recursivedivide-and-conquer methodology. The main idea is to separate the originalsparse PCA problem into a series of much simpler sub-problems, each having aclosed-form solution. By recursively solving these sub-problems in ananalytical way, an efficient algorithm is constructed to solve the sparse PCAproblem. The algorithm only involves simple computations and is thus easy toimplement. The proposed method can also be very easily extended to other sparsePCA problems with certain constraints, such as the nonnegative sparse PCAproblem. Furthermore, we have shown that the proposed algorithm converges to astationary point of the problem, and its computational complexity isapproximately linear in both data size and dimensionality. The effectiveness ofthe proposed method is substantiated by extensive experiments implemented on aseries of synthetic and real data in both reconstruction-error-minimization anddata-variance-maximization viewpoints.
arxiv-1800-146 | Using Differential Evolution for the Graph Coloring | http://arxiv.org/pdf/1211.7200v1.pdf | author:Iztok Fister, Janez Brest category:math.CO cs.NE published:2012-11-30 summary:Differential evolution was developed for reliable and versatile functionoptimization. It has also become interesting for other domains because of itsease to use. In this paper, we posed the question of whether differentialevolution can also be used by solving of the combinatorial optimizationproblems, and in particular, for the graph coloring problem. Therefore, ahybrid self-adaptive differential evolution algorithm for graph coloring wasproposed that is comparable with the best heuristics for graph coloring today,i.e. Tabucol of Hertz and de Werra and the hybrid evolutionary algorithm ofGalinier and Hao. We have focused on the graph 3-coloring. Therefore, theevolutionary algorithm with method SAW of Eiben et al., which achievedexcellent results for this kind of graphs, was also incorporated into thisstudy. The extensive experiments show that the differential evolution couldbecome a competitive tool for the solving of graph coloring problem in thefuture.
arxiv-1800-147 | Erratum: Simplified Drift Analysis for Proving Lower Bounds in Evolutionary Computation | http://arxiv.org/pdf/1211.7184v1.pdf | author:Pietro S. Oliveto, Carsten Witt category:cs.NE F.2.0 published:2012-11-30 summary:This erratum points out an error in the simplified drift theorem (SDT)[Algorithmica 59(3), 369-386, 2011]. It is also shown that a minor modificationof one of its conditions is sufficient to establish a valid result. In manyrespects, the new theorem is more general than before. We no longer assume aMarkov process nor a finite search space. Furthermore, the proof of the theoremis more compact than the previous ones. Finally, previous applications of theSDT are revisited. It turns out that all of these either meet the modifiedcondition directly or by means of few additional arguments.
arxiv-1800-148 | Multislice Modularity Optimization in Community Detection and Image Segmentation | http://arxiv.org/pdf/1211.7180v1.pdf | author:Huiyi Hu, Yves van Gennip, Blake Hunter, Mason A. Porter, Andrea L. Bertozzi category:cs.SI cs.CV physics.soc-ph published:2012-11-30 summary:Because networks can be used to represent many complex systems, they haveattracted considerable attention in physics, computer science, sociology, andmany other disciplines. One of the most important areas of network science isthe algorithmic detection of cohesive groups (i.e., "communities") of nodes. Inthis paper, we algorithmically detect communities in social networks and imagedata by optimizing multislice modularity. A key advantage of modularityoptimization is that it does not require prior knowledge of the number or sizesof communities, and it is capable of finding network partitions that arecomposed of communities of different sizes. By optimizing multislice modularityand subsequently calculating diagnostics on the resulting network partitions,it is thereby possible to obtain information about network structure acrossmultiple system scales. We illustrate this method on data from both socialnetworks and images, and we find that optimization of multislice modularityperforms well on these two tasks without the need for extensiveproblem-specific adaptation. However, improving the computational speed of thismethod remains a challenging open problem.
arxiv-1800-149 | A recursive procedure for density estimation on the binary hypercube | http://arxiv.org/pdf/1112.1450v2.pdf | author:Maxim Raginsky, Jorge Silva, Svetlana Lazebnik, Rebecca Willett category:math.ST stat.ML stat.TH published:2011-12-07 summary:This paper describes a recursive estimation procedure for multivariate binarydensities (probability distributions of vectors of Bernoulli random variables)using orthogonal expansions. For $d$ covariates, there are $2^d$ basiscoefficients to estimate, which renders conventional approaches computationallyprohibitive when $d$ is large. However, for a wide class of densities thatsatisfy a certain sparsity condition, our estimator runs in probabilisticpolynomial time and adapts to the unknown sparsity of the underlying density intwo key ways: (1) it attains near-minimax mean-squared error for moderatesample sizes, and (2) the computational complexity is lower for sparserdensities. Our method also allows for flexible control of the trade-off betweenmean-squared error and computational complexity.
arxiv-1800-150 | Exact and Efficient Parallel Inference for Nonparametric Mixture Models | http://arxiv.org/pdf/1211.7120v1.pdf | author:Sinead A. Williamson, Avinava Dubey, Eric P. Xing category:stat.ML published:2012-11-29 summary:Nonparametric mixture models based on the Dirichlet process are an elegantalternative to finite models when the number of underlying components isunknown, but inference in such models can be slow. Existing attempts toparallelize inference in such models have relied on introducing approximations,which can lead to inaccuracies in the posterior estimate. In this paper, wedescribe auxiliary variable representations for the Dirichlet process and thehierarchical Dirichlet process that allow us to sample from the true posteriorin a distributed manner. We show that our approach allows scalable inferencewithout the deterioration in estimate quality that accompanies existingmethods.
arxiv-1800-151 | SVD Based Image Processing Applications: State of The Art, Contributions and Research Challenges | http://arxiv.org/pdf/1211.7102v1.pdf | author:Rowayda A. Sadek category:cs.CV cs.MM published:2012-11-29 summary:Singular Value Decomposition (SVD) has recently emerged as a new paradigm forprocessing different types of images. SVD is an attractive algebraic transformfor image processing applications. The paper proposes an experimental surveyfor the SVD as an efficient transform in image processing applications. Despitethe well-known fact that SVD offers attractive properties in imaging, theexploring of using its properties in various image applications is currently atits infancy. Since the SVD has many attractive properties have not beenutilized, this paper contributes in using these generous properties in newlyimage applications and gives a highly recommendation for more researchchallenges. In this paper, the SVD properties for images are experimentallypresented to be utilized in developing new SVD-based image processingapplications. The paper offers survey on the developed SVD based imageapplications. The paper also proposes some new contributions that wereoriginated from SVD properties analysis in different image processing. The aimof this paper is to provide a better understanding of the SVD in imageprocessing and identify important various applications and open researchdirections in this increasingly important area; SVD based image processing inthe future research.
arxiv-1800-152 | Evolving Culture vs Local Minima | http://arxiv.org/pdf/1203.2990v2.pdf | author:Yoshua Bengio category:cs.LG cs.AI I.2.6 published:2012-03-14 summary:We propose a theory that relates difficulty of learning in deep architecturesto culture and language. It is articulated around the following hypotheses: (1)learning in an individual human brain is hampered by the presence of effectivelocal minima; (2) this optimization difficulty is particularly important whenit comes to learning higher-level abstractions, i.e., concepts that cover avast and highly-nonlinear span of sensory configurations; (3) such high-levelabstractions are best represented in brains by the composition of many levelsof representation, i.e., by deep architectures; (4) a human brain can learnsuch high-level abstractions if guided by the signals produced by other humans,which act as hints or indirect supervision for these high-level abstractions;and (5), language and the recombination and optimization of mental conceptsprovide an efficient evolutionary recombination operator, and this gives riseto rapid search in the space of communicable ideas that help humans build upbetter high-level internal representations of their world. These hypotheses puttogether imply that human culture and the evolution of ideas have been crucialto counter an optimization difficulty: this optimization difficulty wouldotherwise make it very difficult for human brains to capture high-levelknowledge of the world. The theory is grounded in experimental observations ofthe difficulties of training deep artificial neural networks. Plausibleconsequences of this theory for the efficiency of cultural evolutions aresketched.
arxiv-1800-153 | Quick HyperVolume | http://arxiv.org/pdf/1207.4598v2.pdf | author:Luís M. S. Russo, Alexandre P. Francisco category:cs.DS cs.DM cs.NE published:2012-07-19 summary:We present a new algorithm to calculate exact hypervolumes. Given a set of$d$-dimensional points, it computes the hypervolume of the dominated space.Determining this value is an important subroutine of MultiobjectiveEvolutionary Algorithms (MOEAs). We analyze the "Quick Hypervolume" (QHV)algorithm theoretically and experimentally. The theoretical results are asignificant contribution to the current state of the art. Moreover theexperimental performance is also very competitive, compared with existing exacthypervolume algorithms. A full description of the algorithm is currently submitted to IEEETransactions on Evolutionary Computation.
arxiv-1800-154 | A New Automatic Method to Adjust Parameters for Object Recognition | http://arxiv.org/pdf/1211.6971v1.pdf | author:Issam Qaffou, Mohamed Sadgal, Aziz Elfazziki category:cs.CV cs.AI published:2012-11-29 summary:To recognize an object in an image, the user must apply a combination ofoperators, where each operator has a set of parameters. These parameters mustbe well adjusted in order to reach good results. Usually, this adjustment ismade manually by the user. In this paper we propose a new method to automatethe process of parameter adjustment for an object recognition task. Our methodis based on reinforcement learning, we use two types of agents: User Agent thatgives the necessary information and Parameter Agent that adjusts the parametersof each operator. Due to the nature of reinforcement learning the results donot depend only on the system characteristics but also on the user favoritechoices.
arxiv-1800-155 | Dynamic Network Cartography | http://arxiv.org/pdf/1211.6950v1.pdf | author:Gonzalo Mateos, Ketan Rajawat category:cs.NI cs.IT cs.MA math.IT stat.ML published:2012-11-29 summary:Communication networks have evolved from specialized, research and tacticaltransmission systems to large-scale and highly complex interconnections ofintelligent devices, increasingly becoming more commercial, consumer-oriented,and heterogeneous. Propelled by emergent social networking services andhigh-definition streaming platforms, network traffic has grown explosivelythanks to the advances in processing speed and storage capacity ofstate-of-the-art communication technologies. As "netizens" demand a seamlessnetworking experience that entails not only higher speeds, but also resilienceand robustness to failures and malicious cyber-attacks, ample opportunities forsignal processing (SP) research arise. The vision is for ubiquitous smartnetwork devices to enable data-driven statistical learning algorithms fordistributed, robust, and online network operation and management, adaptable tothe dynamically-evolving network landscape with minimal need for humanintervention. The present paper aims at delineating the analytical backgroundand the relevance of SP tools to dynamic network monitoring, introducing the SPreadership to the concept of dynamic network cartography -- a framework toconstruct maps of the dynamic network state in an efficient and scalable mannertailored to large-scale heterogeneous networks.
arxiv-1800-156 | On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes | http://arxiv.org/pdf/1211.6898v1.pdf | author:Bruno Scherrer, Boris Lesner category:cs.LG cs.AI published:2012-11-29 summary:We consider infinite-horizon stationary $\gamma$-discounted Markov DecisionProcesses, for which it is known that there exists a stationary optimal policy.Using Value and Policy Iteration with some error $\epsilon$ at each iteration,it is well-known that one can compute stationary policies that are$\frac{2\gamma}{(1-\gamma)^2}\epsilon$-optimal. After arguing that thisguarantee is tight, we develop variations of Value and Policy Iteration forcomputing non-stationary policies that can be up to$\frac{2\gamma}{1-\gamma}\epsilon$-optimal, which constitutes a significantimprovement in the usual situation when $\gamma$ is close to 1. Surprisingly,this shows that the problem of "computing near-optimal non-stationary policies"is much simpler than that of "computing near-optimal stationary policies".
arxiv-1800-157 | Automating rule generation for grammar checkers | http://arxiv.org/pdf/1211.6887v1.pdf | author:Marcin Miłkowski category:cs.CL cs.LG published:2012-11-29 summary:In this paper, I describe several approaches to automatic or semi-automaticdevelopment of symbolic rules for grammar checkers from the informationcontained in corpora. The rules obtained this way are an important addition tomanually-created rules that seem to dominate in rule-based checkers. However,the manual process of creation of rules is costly, time-consuming anderror-prone. It seems therefore advisable to use machine-learning algorithms tocreate the rules automatically or semi-automatically. The results obtained seemto corroborate my initial hypothesis that symbolic machine learning algorithmscan be useful for acquiring new rules for grammar checking. It turns out,however, that for practical uses, error corpora cannot be the sole source ofinformation used in grammar checking. I suggest therefore that only by usingdifferent approaches, grammar-checkers, or more generally, computer-aidedproofreading tools, will be able to cover most frequent and severe mistakes andavoid false alarms that seem to distract users.
arxiv-1800-158 | Overlapping clustering based on kernel similarity metric | http://arxiv.org/pdf/1211.6859v1.pdf | author:Chiheb-Eddine Ben N'Cir, Nadia Essoussi, Patrice Bertrand category:stat.ML cs.LG stat.ME published:2012-11-29 summary:Producing overlapping schemes is a major issue in clustering. Recent proposedoverlapping methods relies on the search of an optimal covering and are basedon different metrics, such as Euclidean distance and I-Divergence, used tomeasure closeness between observations. In this paper, we propose the use ofanother measure for overlapping clustering based on a kernel similarity metric.We also estimate the number of overlapped clusters using the Gram matrix.Experiments on both Iris and EachMovie datasets show the correctness of theestimation of number of clusters and show that measure based on kernelsimilarity metric improves the precision, recall and f-measure in overlappingclustering.
arxiv-1800-159 | Classification Recouvrante Basée sur les Méthodes à Noyau | http://arxiv.org/pdf/1211.6851v1.pdf | author:Chiheb-Eddine Ben N'Cir, Nadia Essoussi category:cs.LG stat.CO stat.ME stat.ML published:2012-11-29 summary:Overlapping clustering problem is an important learning issue in whichclusters are not mutually exclusive and each object may belongs simultaneouslyto several clusters. This paper presents a kernel based method that producesoverlapping clusters on a high feature space using mercer kernel techniques toimprove separability of input patterns. The proposed method, calledOKM-K(Overlapping $k$-means based kernel method), extends OKM (Overlapping$k$-means) method to produce overlapping schemes. Experiments are performed onoverlapping dataset and empirical results obtained with OKM-K outperformresults obtained with OKM.
arxiv-1800-160 | Higher-Order Momentum Distributions and Locally Affine LDDMM Registration | http://arxiv.org/pdf/1112.3166v2.pdf | author:Stefan Sommer, Mads Nielsen, Sune Darkner, Xavier Pennec category:cs.CV cs.NA published:2011-12-14 summary:To achieve sparse parametrizations that allows intuitive analysis, we aim torepresent deformation with a basis containing interpretable elements, and wewish to use elements that have the description capacity to represent thedeformation compactly. To accomplish this, we introduce in this paperhigher-order momentum distributions in the LDDMM registration framework. Whilethe zeroth order moments previously used in LDDMM only describe localdisplacement, the first-order momenta that are proposed here represent a basisthat allows local description of affine transformations and subsequent compactdescription of non-translational movement in a globally non-rigid deformation.The resulting representation contains directly interpretable information fromboth mathematical and modeling perspectives. We develop the mathematicalconstruction of the registration framework with higher-order momenta, we showthe implications for sparse image registration and deformation description, andwe provide examples of how the parametrization enables registration with a verylow number of parameters. The capacity and interpretability of theparametrization using higher-order momenta lead to natural modeling ofarticulated movement, and the method promises to be useful for quantifyingventricle expansion and progressing atrophy during Alzheimer's disease.
arxiv-1800-161 | Letter counting: a stem cell for Cryptology, Quantitative Linguistics, and Statistics | http://arxiv.org/pdf/1211.6847v1.pdf | author:Bernard Ycart category:math.HO cs.CL cs.CR published:2012-11-29 summary:Counting letters in written texts is a very ancient practice. It hasaccompanied the development of Cryptology, Quantitative Linguistics, andStatistics. In Cryptology, counting frequencies of the different characters inan encrypted message is the basis of the so called frequency analysis method.In Quantitative Linguistics, the proportion of vowels to consonants indifferent languages was studied long before authorship attribution. InStatistics, the alternation vowel-consonants was the only example that Markovever gave of his theory of chained events. A short history of letter countingis presented. The three domains, Cryptology, Quantitative Linguistics, andStatistics, are then examined, focusing on the interactions with the other twofields through letter counting. As a conclusion, the eclectism of pastcenturies scholars, their background in humanities, and their familiarity withcryptograms, are identified as contributing factors to the mutual enrichmentprocess which is described here.
arxiv-1800-162 | On unbiased performance evaluation for protein inference | http://arxiv.org/pdf/1211.6834v1.pdf | author:Zengyou He, Ting Huang, Peijun Zhu category:stat.AP cs.LG q-bio.QM published:2012-11-29 summary:This letter is a response to the comments of Serang (2012) on Huang and He(2012) in Bioinformatics. Serang (2012) claimed that the parameters for theFido algorithm should be specified using the grid search method in Serang etal. (2010) so as to generate a deserved accuracy in performance comparison. Itseems that it is an argument on parameter tuning. However, it is indeed theissue of how to conduct an unbiased performance evaluation for comparingdifferent protein inference algorithms. In this letter, we would explain why wedon't use the grid search for parameter selection in Huang and He (2012) andshow that this procedure may result in an over-estimated performance that isunfair to competing algorithms. In fact, this issue has also been pointed outby Li and Radivojac (2012).
arxiv-1800-163 | Graph Laplacians on Singular Manifolds: Toward understanding complex spaces: graph Laplacians on manifolds with singularities and boundaries | http://arxiv.org/pdf/1211.6727v1.pdf | author:Mikhail Belkin, Qichao Que, Yusu Wang, Xueyuan Zhou category:cs.AI cs.CG cs.LG published:2012-11-28 summary:Recently, much of the existing work in manifold learning has been done underthe assumption that the data is sampled from a manifold without boundaries andsingularities or that the functions of interest are evaluated away from suchpoints. At the same time, it can be argued that singularities and boundariesare an important aspect of the geometry of realistic data. In this paper we consider the behavior of graph Laplacians at points at ornear boundaries and two main types of other singularities: intersections, wheredifferent manifolds come together and sharp "edges", where a manifold sharplychanges direction. We show that the behavior of graph Laplacian near thesesingularities is quite different from that in the interior of the manifolds. Infact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis ofFourier series, can be observed in the behavior of graph Laplacian near suchpoints. Unlike in the interior of the domain, where graph Laplacian convergesto the Laplace-Beltrami operator, near singularities graph Laplacian tends to afirst-order differential operator, which exhibits different scaling behavior asa function of the kernel width. One important implication is that while pointsnear the singularities occupy only a small part of the total volume, thedifference in scaling results in a disproportionately large contribution to thetotal behavior. Another significant finding is that while the scaling behaviorof the operator is the same near different types of singularities, they arevery distinct at a more refined level of analysis. We believe that a comprehensive understanding of these structures in additionto the standard case of a smooth manifold can take us a long way toward bettermethods for analysis of complex non-linear data and can lead to significantprogress in algorithm design.
arxiv-1800-164 | Nonlinear Dynamic Field Embedding: On Hyperspectral Scene Visualization | http://arxiv.org/pdf/1211.6675v1.pdf | author:Dalton Lunga 'and' Okan Ersoy category:cs.CV cs.CE stat.ML published:2012-11-28 summary:Graph embedding techniques are useful to characterize spectral signaturerelations for hyperspectral images. However, such images consists of disjointclasses due to spatial details that are often ignored by existing graphcomputing tools. Robust parameter estimation is a challenge for kernelfunctions that compute such graphs. Finding a corresponding high qualitycoordinate system to map signature relations remains an open research question.We answer positively on these challenges by first proposing a kernel functionof spatial and spectral information in computing neighborhood graphs. Secondly,the study exploits the force field interpretation from mechanics and devise aunifying nonlinear graph embedding framework. The generalized framework leadsto novel unsupervised multidimensional artificial field embedding techniquesthat rely on the simple additive assumption of pair-dependent attraction andrepulsion functions. The formulations capture long range and short rangedistance related effects often associated with living organisms and help toestablish algorithmic properties that mimic mutual behavior for the purpose ofdimensionality reduction. The main benefits from the proposed models includesthe ability to preserve the local topology of data and produce qualityvisualizations i.e. maintaining disjoint meaningful neighborhoods. As part ofevaluation, visualization, gradient field trajectories, and semisupervisedclassification experiments are conducted for image scenes acquired by multiplesensors at various spatial resolutions over different types of objects. Theresults demonstrate the superiority of the proposed embedding framework overvarious widely used methods.
arxiv-1800-165 | Nature-Inspired Mateheuristic Algorithms: Success and New Challenges | http://arxiv.org/pdf/1211.6658v1.pdf | author:Xin-She Yang category:math.OC cs.NE 90C26 published:2012-11-28 summary:Despite the increasing popularity of metaheuristics, many crucially importantquestions remain unanswered. There are two important issues: theoreticalframework and the gap between theory and applications. At the moment, thepractice of metaheuristics is like heuristic itself, to some extent, by trialand error. Mathematical analysis lags far behind, apart from a few, limited,studies on convergence analysis and stability, there is no theoreticalframework for analyzing metaheuristic algorithms. I believe mathematical andstatistical methods using Markov chains and dynamical systems can be veryuseful in the future work. There is no doubt that any theoretical progress willprovide potentially huge insightful into meteheuristic algorithms.
arxiv-1800-166 | Nonparametric Bayesian Mixed-effect Model: a Sparse Gaussian Process Approach | http://arxiv.org/pdf/1211.6653v1.pdf | author:Yuyang Wang, Roni Khardon category:cs.LG stat.ML published:2012-11-28 summary:Multi-task learning models using Gaussian processes (GP) have been developedand successfully applied in various applications. The main difficulty with thisapproach is the computational cost of inference using the union of examplesfrom all tasks. Therefore sparse solutions, that avoid using the entire datadirectly and instead use a set of informative "representatives" are desirable.The paper investigates this problem for the grouped mixed-effect GP model whereeach individual response is given by a fixed-effect, taken from one of a set ofunknown groups, plus a random individual effect function that capturesvariations among individuals. Such models have been widely used in previouswork but no sparse solutions have been developed. The paper presents the firstsparse solution for such problems, showing how the sparse approximation can beobtained by maximizing a variational lower bound on the marginal likelihood,generalizing ideas from single-task Gaussian processes to handle themixed-effect model as well as grouping. Experiments using artificial and realdata validate the approach showing that it can recover the performance ofinference with the full sample, that it outperforms baseline methods, and thatit outperforms state of the art sparse solutions for other multi-task GPformulations.
arxiv-1800-167 | Deep Attribute Networks | http://arxiv.org/pdf/1211.2881v3.pdf | author:Junyoung Chung, Donghoon Lee, Youngjoo Seo, Chang D. Yoo category:cs.CV cs.LG stat.ML published:2012-11-13 summary:Obtaining compact and discriminative features is one of the major challengesin many of the real-world image classification tasks such as face verificationand object recognition. One possible approach is to represent input image onthe basis of high-level features that carry semantic meaning which humans canunderstand. In this paper, a model coined deep attribute network (DAN) isproposed to address this issue. For an input image, the model outputs theattributes of the input image without performing any classification. Theefficacy of the proposed model is evaluated on unconstrained face verificationand real-world object recognition tasks using the LFW and the a-PASCALdatasets. We demonstrate the potential of deep learning for attribute-basedclassification by showing comparable results with existing state-of-the-artresults. Once properly trained, the DAN is fast and does away with calculatinglow-level features which are maybe unreliable and computationally expensive.
arxiv-1800-168 | A LASSO-Penalized BIC for Mixture Model Selection | http://arxiv.org/pdf/1211.6451v1.pdf | author:Sakyajit Bhattacharya, Paul D. McNicholas category:stat.ME math.ST stat.CO stat.ML stat.TH published:2012-11-27 summary:The efficacy of family-based approaches to mixture model-based clustering andclassification depends on the selection of parsimonious models. Current wisdomsuggests the Bayesian information criterion (BIC) for mixture model selection.However, the BIC has well-known limitations, including a tendency tooverestimate the number of components as well as a proclivity for, oftendrastically, underestimating the number of components in higher dimensions.While the former problem might be soluble through merging components, thelatter is impossible to mitigate in clustering and classification applications.In this paper, a LASSO-penalized BIC (LPBIC) is introduced to overcome thisproblem. This approach is illustrated based on applications of extensions ofmixtures of factor analyzers, where the LPBIC is used to select both the numberof components and the number of latent factors. The LPBIC is shown to match oroutperform the BIC in several situations.
arxiv-1800-169 | Optimal Algorithms for Ridge and Lasso Regression with Partially Observed Attributes | http://arxiv.org/pdf/1108.4559v2.pdf | author:Elad Hazan, Tomer Koren category:cs.LG published:2011-08-23 summary:We consider the most common variants of linear regression, including Ridge,Lasso and Support-vector regression, in a setting where the learner is allowedto observe only a fixed number of attributes of each example at training time.We present simple and efficient algorithms for these problems: for Lasso andRidge regression they need the same total number of attributes (up toconstants) as do full-information algorithms, for reaching a certain accuracy.For Support-vector regression, we require exponentially less attributescompared to the state of the art. By that, we resolve an open problem recentlyposed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds tobe justified by superior performance compared to the state of the art.
arxiv-1800-170 | Creating a Digital Ecosystem: Service-Oriented Architectures with Distributed Evolutionary Computing | http://arxiv.org/pdf/0712.4159v5.pdf | author:G Briscoe category:cs.NE published:2007-12-26 summary:We start with a discussion of the relevant literature, including NatureInspired Computing as a framework in which to understand this work, and theprocess of biomimicry to be used in mimicking the necessary biologicalprocesses to create Digital Ecosystems. We then consider the relevanttheoretical ecology in creating the digital counterpart of a biologicalecosystem, including the topological structure of ecosystems, and evolutionaryprocesses within distributed environments. This leads to a discussion of therelevant fields from computer science for the creation of Digital Ecosystems,including evolutionary computing, Multi-Agent Systems, and Service-OrientedArchitectures. We then define Ecosystem-Oriented Architectures for the creationof Digital Ecosystems, imbibed with the properties of self-organisation andscalability from biological ecosystems, including a novel form of distributedevolutionary computing.
arxiv-1800-171 | Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation | http://arxiv.org/pdf/1211.2532v3.pdf | author:Dominique Guillot, Bala Rajaratnam, Benjamin T. Rolfs, Arian Maleki, Ian Wong category:stat.CO cs.LG stat.ML published:2012-11-12 summary:The L1-regularized maximum likelihood estimation problem has recently becomea topic of great interest within the machine learning, statistics, andoptimization communities as a method for producing sparse inverse covarianceestimators. In this paper, a proximal gradient method (G-ISTA) for performingL1-regularized covariance matrix estimation is presented. Although numerousalgorithms have been proposed for solving this problem, this simple proximalgradient method is found to have attractive theoretical and numericalproperties. G-ISTA has a linear rate of convergence, resulting in an O(log e)iteration complexity to reach a tolerance of e. This paper gives eigenvaluebounds for the G-ISTA iterates, providing a closed-form linear convergencerate. The rate is shown to be closely related to the condition number of theoptimal point. Numerical convergence results and timing comparisons for theproposed method are presented. G-ISTA is shown to perform very well, especiallywhen the optimal point is well-conditioned.
arxiv-1800-172 | Neuro-Fuzzy Computing System with the Capacity of Implementation on Memristor-Crossbar and Optimization-Free Hardware Training | http://arxiv.org/pdf/1211.6205v1.pdf | author:Farnood Merrikh-Bayat, Farshad Merrikh-Bayat, Saeed Bagheri Shouraki category:cs.NE cs.AI published:2012-11-27 summary:In this paper, first we present a new explanation for the relation betweenlogical circuits and artificial neural networks, logical circuits and fuzzylogic, and artificial neural networks and fuzzy inference systems. Then, basedon these results, we propose a new neuro-fuzzy computing system which caneffectively be implemented on the memristor-crossbar structure. One importantfeature of the proposed system is that its hardware can directly be trainedusing the Hebbian learning rule and without the need to any optimization. Thesystem also has a very good capability to deal with huge number of input-outtraining data without facing problems like overtraining.
arxiv-1800-173 | The Interplay Between Stability and Regret in Online Learning | http://arxiv.org/pdf/1211.6158v1.pdf | author:Ankan Saha, Prateek Jain, Ambuj Tewari category:cs.LG stat.ML published:2012-11-26 summary:This paper considers the stability of online learning algorithms and itsimplications for learnability (bounded regret). We introduce a novel quantitycalled {\em forward regret} that intuitively measures how good an onlinelearning algorithm is if it is allowed a one-step look-ahead into the future.We show that given stability, bounded forward regret is equivalent to boundedregret. We also show that the existence of an algorithm with bounded regretimplies the existence of a stable algorithm with bounded regret and boundedforward regret. The equivalence results apply to general, possibly non-convexproblems. To the best of our knowledge, our analysis provides the first generalconnection between stability and regret in the online setting that is notrestricted to a particular class of algorithms. Our stability-regret connectionprovides a simple recipe for analyzing regret incurred by any online learningalgorithm. Using our framework, we analyze several existing online learningalgorithms as well as the "approximate" versions of algorithms like RDA thatsolve an optimization problem at each iteration. Our proofs are simpler thanexisting analysis for the respective algorithms, show a clear trade-off betweenstability and forward regret, and provide tighter regret bounds in some cases.Furthermore, using our recipe, we analyze "approximate" versions of severalalgorithms such as follow-the-regularized-leader (FTRL) that requires solvingan optimization problem at each step.
arxiv-1800-174 | An Evaluation of Popular Copy-Move Forgery Detection Approaches | http://arxiv.org/pdf/1208.3665v2.pdf | author:Vincent Christlein, Christian Riess, Johannes Jordan, Corinna Riess, Elli Angelopoulou category:cs.CV I.4.9 published:2012-08-17 summary:A copy-move forgery is created by copying and pasting content within the sameimage, and potentially post-processing it. In recent years, the detection ofcopy-move forgeries has become one of the most actively researched topics inblind image forensics. A considerable number of different algorithms have beenproposed focusing on different types of postprocessed copies. In this paper, weaim to answer which copy-move forgery detection algorithms and processing steps(e.g., matching, filtering, outlier detection, affine transformationestimation) perform best in various postprocessing scenarios. The focus of ouranalysis is to evaluate the performance of previously proposed feature sets. Weachieve this by casting existing algorithms in a common pipeline. In thispaper, we examined the 15 most prominent feature sets. We analyzed thedetection performance on a per-image basis and on a per-pixel basis. We createda challenging real-world copy-move dataset, and a software framework forsystematic image manipulation. Experiments show, that the keypoint-basedfeatures SIFT and SURF, as well as the block-based DCT, DWT, KPCA, PCA andZernike features perform very well. These feature sets exhibit the bestrobustness against various noise sources and downsampling, while reliablyidentifying the copied regions.
arxiv-1800-175 | Trace transform based method for color image domain identification | http://arxiv.org/pdf/1208.3901v2.pdf | author:Igor G. Olaizola, Marco Quartulli, Julian Florez, Basilio Sierra category:cs.CV published:2012-08-19 summary:Context categorization is a fundamental pre-requisite for multi-domainmultimedia content analysis applications in order to manage contextualinformation in an efficient manner. In this paper, we introduce a new colorimage context categorization method (DITEC) based on the trace transform. Theproblem of dimensionality reduction of the obtained trace transform signal isaddressed through statistical descriptors that keep the underlying information.These extracted features offer a highly discriminant behavior for contentcategorization. The theoretical properties of the method are analyzed andvalidated experimentally through two different datasets.
arxiv-1800-176 | Efficient algorithms for robust recovery of images from compressed data | http://arxiv.org/pdf/1211.7276v1.pdf | author:Duc Son Pham, Svetha Venkatesh category:cs.IT cs.LG math.IT stat.ML published:2012-11-26 summary:Compressed sensing (CS) is an important theory for sub-Nyquist sampling andrecovery of compressible data. Recently, it has been extended by Pham andVenkatesh to cope with the case where corruption to the CS data is modeled asimpulsive noise. The new formulation, termed as robust CS, combines robuststatistics and CS into a single framework to suppress outliers in the CSrecovery. To solve the newly formulated robust CS problem, Pham and Venkateshsuggested a scheme that iteratively solves a number of CS problems, thesolutions from which converge to the true robust compressed sensing solution.However, this scheme is rather inefficient as it has to use existing CS solversas a proxy. To overcome limitation with the original robust CS algorithm, wepropose to solve the robust CS problem directly in this paper and drive morecomputationally efficient algorithms by following latest advances inlarge-scale convex optimization for non-smooth regularization. Furthermore, wealso extend the robust CS formulation to various settings, including additionalaffine constraints, $\ell_1$-norm loss function, mixed-norm regularization, andmulti-tasking, so as to further improve robust CS. We also derive simple buteffective algorithms to solve these extensions. We demonstrate that the newalgorithms provide much better computational advantage over the original robustCS formulation, and effectively solve more sophisticated extensions where theoriginal methods simply cannot. We demonstrate the usefulness of the extensionson several CS imaging tasks.
arxiv-1800-177 | OpenCFU, a New Free and Open-Source Software to Count Cell Colonies and Other Circular Objects | http://arxiv.org/pdf/1210.5502v3.pdf | author:Quentin Geissmann category:q-bio.QM cs.CV published:2012-10-18 summary:Counting circular objects such as cell colonies is an important source ofinformation for biologists. Although this task is often time-consuming andsubjective, it is still predominantly performed manually. The aim of thepresent work is to provide a new tool to enumerate circular objects fromdigital pictures and video streams. Here, I demonstrate that the createdprogram, OpenCFU, is very robust, accurate and fast. In addition, it providescontrol over the processing parameters and is implemented in an in- tuitive andmodern interface. OpenCFU is a cross-platform and open-source software freelyavailable at http://opencfu.sourceforge.net.
arxiv-1800-178 | Bayesian learning of noisy Markov decision processes | http://arxiv.org/pdf/1211.5901v1.pdf | author:Sumeetpal S. Singh, Nicolas Chopin, Nick Whiteley category:stat.ML cs.LG stat.CO published:2012-11-26 summary:We consider the inverse reinforcement learning problem, that is, the problemof learning from, and then predicting or mimicking a controller based onstate/action data. We propose a statistical model for such data, derived fromthe structure of a Markov decision process. Adopting a Bayesian approach toinference, we show how latent variables of the model can be estimated, and howpredictions about actions can be made, in a unified framework. A new Markovchain Monte Carlo (MCMC) sampler is devised for simulation from the posteriordistribution. This step includes a parameter expansion step, which is shown tobe essential for good convergence properties of the MCMC sampler. As anillustration, the method is applied to learning a human controller.
arxiv-1800-179 | Optimistic Rates for Learning with a Smooth Loss | http://arxiv.org/pdf/1009.3896v2.pdf | author:Nathan Srebro, Karthik Sridharan, Ambuj Tewari category:cs.LG published:2010-09-20 summary:We establish an excess risk bound of O(H R_n^2 + R_n \sqrt{H L*}) forempirical risk minimization with an H-smooth loss function and a hypothesisclass with Rademacher complexity R_n, where L* is the best risk achievable bythe hypothesis class. For typical hypothesis classes where R_n = \sqrt{R/n},this translates to a learning rate of O(RH/n) in the separable (L*=0) case andO(RH/n + \sqrt{L^* RH/n}) more generally. We also provide similar guaranteesfor online and stochastic convex optimization with a smooth non-negativeobjective.
arxiv-1800-180 | An Automatic Algorithm for Object Recognition and Detection Based on ASIFT Keypoints | http://arxiv.org/pdf/1211.5829v1.pdf | author:Reza Oji category:cs.AI cs.CV published:2012-11-26 summary:Object recognition is an important task in image processing and computervision. This paper presents a perfect method for object recognition with fullboundary detection by combining affine scale invariant feature transform(ASIFT) and a region merging algorithm. ASIFT is a fully affine invariantalgorithm that means features are invariant to six affine parameters namelytranslation (2 parameters), zoom, rotation and two camera axis orientations.The features are very reliable and give us strong keypoints that can be usedfor matching between different images of an object. We trained an object inseveral images with different aspects for finding best keypoints of it. Then, arobust region merging algorithm is used to recognize and detect the object withfull boundary in the other images based on ASIFT keypoints and a similaritymeasure for merging regions in the image. Experimental results show that thepresented method is very efficient and powerful to recognize the object anddetect it with high accuracy.
arxiv-1800-181 | Detection of elliptical shapes via cross-entropy clustering | http://arxiv.org/pdf/1211.5712v1.pdf | author:Jacek Tabor, Krzysztof Misztal category:cs.CV published:2012-11-24 summary:The problem of finding elliptical shapes in an image will be considered. Wediscuss the solution which uses cross-entropy clustering. The proposed methodallows the search for ellipses with predefined sizes and position in the space.Moreover, it works well for search of ellipsoids in higher dimensions.
arxiv-1800-182 | Texture Modeling with Convolutional Spike-and-Slab RBMs and Deep Extensions | http://arxiv.org/pdf/1211.5687v1.pdf | author:Heng Luo, Pierre Luc Carrier, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML published:2012-11-24 summary:We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texturemodeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves orsurpasses the state-of-the-art on texture synthesis and inpainting byparametric models. We also develop a novel RBM model with a spike-and-slabvisible layer and binary variables in the hidden layer. This model is designedto be stacked on top of the TssRBM. We show the resulting deep belief network(DBN) is a powerful generative model that improves on single-layer models andis capable of modeling not only single high-resolution and challenging texturesbut also multiple textures.
arxiv-1800-183 | New Hoopoe Heuristic Optimization | http://arxiv.org/pdf/1211.6410v1.pdf | author:Mohammed El-Dosuky, Ahmed EL-Bassiouny, Taher Hamza, Magdy Rashad category:cs.NE cs.AI published:2012-11-24 summary:Most optimization problems in real life applications are often highlynonlinear. Local optimization algorithms do not give the desired performance.So, only global optimization algorithms should be used to obtain optimalsolutions. This paper introduces a new nature-inspired metaheuristicoptimization algorithm, called Hoopoe Heuristic (HH). In this paper, we willstudy HH and validate it against some test functions. Investigations show thatit is very promising and could be seen as an optimization of the powerfulalgorithm of cuckoo search. Finally, we discuss the features of HoopoeHeuristic and propose topics for further studies.
arxiv-1800-184 | Theano: new features and speed improvements | http://arxiv.org/pdf/1211.5590v1.pdf | author:Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio category:cs.SC cs.LG published:2012-11-23 summary:Theano is a linear algebra compiler that optimizes a user'ssymbolically-specified mathematical computations to produce efficient low-levelimplementations. In this paper, we present new features and efficiencyimprovements to Theano, and benchmarks demonstrating Theano's performancerelative to Torch7, a recently introduced machine learning library, and toRNNLM, a C++ library targeted at recurrent neural networks.
arxiv-1800-185 | Improving Perceptual Color Difference using Basic Color Terms | http://arxiv.org/pdf/1211.5556v1.pdf | author:Ofir Pele, Michael Werman category:cs.CV cs.GR published:2012-11-23 summary:We suggest a new color distance based on two observations. First, perceptualcolor differences were designed to be used to compare very similar colors. Theydo not capture human perception for medium and large color differences well.Thresholding was proposed to solve the problem for large color differences,i.e. two totally different colors are always the same distance apart. We showthat thresholding alone cannot improve medium color differences. We suggest toalleviate this problem using basic color terms. Second, when a color distanceis used for edge detection, many small distances around the just noticeabledifference may account for false edges. We suggest to reduce the effect ofsmall distances.
arxiv-1800-186 | Genetic Algorithm Modeling with GPU Parallel Computing Technology | http://arxiv.org/pdf/1211.5481v1.pdf | author:Stefano Cavuoti, Mauro Garofalo, Massimo Brescia, Antonio Pescapé, Giuseppe Longo, Giorgio Ventre category:astro-ph.IM cs.DC cs.NE published:2012-11-23 summary:We present a multi-purpose genetic algorithm, designed and implemented withGPGPU / CUDA parallel computing technology. The model was derived from amulti-core CPU serial implementation, named GAME, already scientificallysuccessfully tested and validated on astrophysical massive data classificationproblems, through a web application resource (DAMEWARE), specialized in datamining based on Machine Learning paradigms. Since genetic algorithms areinherently parallel, the GPGPU computing paradigm has provided an exploit ofthe internal training features of the model, permitting a strong optimizationin terms of processing performances and scalability.
arxiv-1800-187 | Analysis of a randomized approximation scheme for matrix multiplication | http://arxiv.org/pdf/1211.5414v1.pdf | author:Daniel Hsu, Sham M. Kakade, Tong Zhang category:cs.DS cs.LG cs.NA stat.ML published:2012-11-23 summary:This note gives a simple analysis of a randomized approximation scheme formatrix multiplication proposed by Sarlos (2006) based on a random rotationfollowed by uniform column sampling. The result follows from a matrix versionof Bernstein's inequality and a tail inequality for quadratic forms insubgaussian random vectors.
arxiv-1800-188 | Ecosystem-Oriented Distributed Evolutionary Computing | http://arxiv.org/pdf/1211.5400v1.pdf | author:Gerard Briscoe, Philippe De Wilde category:cs.NE published:2012-11-23 summary:We create a novel optimisation technique inspired by natural ecosystems,where the optimisation works at two levels: a first optimisation, migration ofgenes which are distributed in a peer-to-peer network, operating continuouslyin time; this process feeds a second optimisation based on evolutionarycomputing that operates locally on single peers and is aimed at findingsolutions to satisfy locally relevant constraints. We consider from the domainof computer science distributed evolutionary computing, with the relevanttheory from the domain of theoretical biology, including the fields ofevolutionary and ecological theory, the topological structure of ecosystems,and evolutionary processes within distributed environments. We then defineecosystem- oriented distributed evolutionary computing, imbibed with theproperties of self-organisation, scalability and sustainability from naturalecosystems, including a novel form of distributed evolu- tionary computing.Finally, we conclude with a discussion of the apparent compromises resultingfrom the hybrid model created, such as the network topology.
arxiv-1800-189 | Bottleneck of using single memristor as a synapse and its solution | http://arxiv.org/pdf/1008.3450v3.pdf | author:Farnood Merrikh-Bayat, Saeed Bagheri Shouraki, Iman Esmaili Paeen Afrakoti category:cs.NE published:2010-08-20 summary:It is now widely accepted that memristive devices are perfect candidates forthe emulation of biological synapses in neuromorphic systems. This is mainlybecause of the fact that like the strength of synapse, memristance of thememristive device can be tuned actively (e.g., by the application of volt- ageor current). In addition, it is also possible to fabricate very high density ofmemristive devices (comparable to the number of synapses in real biologicalsystem) through the nano-crossbar structures. However, in this paper we willshow that there are some problems associated with memristive synapses(memristive devices which are playing the role of biological synapses). Forexample, we show that the variation rate of the memristance of memristivedevice depends completely on the current memristance of the device andtherefore it can change significantly with time during the learning phase. Thisphenomenon can degrade the performance of learning methods like SpikeTiming-Dependent Plasticity (STDP) and cause the corresponding neuromorphicsystems to become unstable. Finally, at the end of this paper, we illustratethat using two serially connected memristive devices with different polaritiesas a synapse can somewhat fix the aforementioned problem.
arxiv-1800-190 | Cobb Angle Measurement of Scoliosis with Reduced Variability | http://arxiv.org/pdf/1211.5355v1.pdf | author:Raka Kundu, Amlan Chakrabarti, Prasanna K. Lenka category:cs.CV published:2012-11-22 summary:Cobb angle, which is a measure of spinal curvature is the standard method forquantifying the magnitude of Scoliosis related to spinal deformity inorthopedics. Determining the Cobb angle through manual process is subject tohuman errors. In this work, we propose a methodology to measure the magnitudeof Cobb angle, which appreciably reduces the variability related to itsmeasurement compared to the related works. The proposed methodology isfacilitated by using a suitable new improved version of Non-Local Means forimage denoisation and Otsus automatic threshold selection for Canny edgedetection. We have selected NLM for preprocessing of the image as it is one ofthe fine states of art for image denoisation and helps in retaining the imagequality. Trimmedmean, median are more robust to outliners than mean andfollowing this concept we observed that NLM denoising quality performance canbe enhanced by using Euclidean trimmed-mean replacing the mean. To prove thebetter performance of the Non-Local Euclidean Trimmed-mean denoising filter, wehave provided some comparative study results of the proposed denoisingtechnique with traditional NLM and NonLocal Euclidean Medians. The experimentalresults for Cobb angle measurement over intra observer and inter observerexperimental data reveals the better performance and superiority of theproposed approach compared to the related works. MATLAB2009b image processingtoolbox was used for the purpose of simulation and verification of the proposedmethodology.
arxiv-1800-191 | Service Composition Design Pattern for Autonomic Computing Systems using Association Rule based Learning and Service-Oriented Architecture | http://arxiv.org/pdf/1211.5227v1.pdf | author:Vishnuvardhan Mannava, T. Ramesh category:cs.SE cs.DC cs.LG published:2012-11-22 summary:In this paper we present a Service Injection and composition Design Patternfor Unstructured Peer-to-Peer networks, which is designed with Aspect-orienteddesign patterns, and amalgamation of the Strategy, Worker Object, andCheck-List Design Patterns used to design the Self-Adaptive Systems. It willapply self reconfiguration planes dynamically without the interruption orintervention of the administrator for handling service failures at the servers.When a client requests for a complex service, Service Composition should bedone to fulfil the request. If a service is not available in the memory, itwill be injected as Aspectual Feature Module code. We used Service OrientedArchitecture (SOA) with Web Services in Java to Implement the composite DesignPattern. As far as we know, there are no studies on composition of designpatterns for Peer-to-peer computing domain. The pattern is described using ajava-like notation for the classes and interfaces. A simple UML class andSequence diagrams are depicted.
arxiv-1800-192 | On pattern recovery of the fused Lasso | http://arxiv.org/pdf/1211.5194v1.pdf | author:Junyang Qian, Jinzhu Jia category:stat.ML math.ST stat.TH published:2012-11-22 summary:We study the property of the Fused Lasso Signal Approximator (FLSA) forestimating a blocky signal sequence with additive noise. We transform the FLSAto an ordinary Lasso problem. By studying the property of the design matrix inthe transformed Lasso problem, we find that the irrepresentable condition mightnot hold, in which case we show that the FLSA might not be able to recover thesignal pattern. We then apply the newly developed preconditioning method --Puffer Transformation [Jia and Rohe, 2012] on the transformed Lasso problem. Wecall the new method the preconditioned fused Lasso and we give non-asymptoticresults for this method. Results show that when the signal jump strength(signal difference between two neighboring groups) is big and the noise levelis small, our preconditioned fused Lasso estimator gives the correct patternwith high probability. Theoretical results give insight on what controls thesignal pattern recovery ability -- it is the noise level {instead of} thelength of the sequence. Simulations confirm our theorems and show significantimprovement of the preconditioned fused Lasso estimator over the vanilla FLSA.
arxiv-1800-193 | Learning in Hierarchical Social Networks | http://arxiv.org/pdf/1206.0652v4.pdf | author:Zhenliang Zhang, Edwin K. P. Chong, Ali Pezeshki, William Moran, Stephen D. Howard category:cs.SI cs.IT cs.LG math.IT published:2012-05-30 summary:We study a social network consisting of agents organized as a hierarchicalM-ary rooted tree, common in enterprise and military organizational structures.The goal is to aggregate information to solve a binary hypothesis testingproblem. Each agent at a leaf of the tree, and only such an agent, makes adirect measurement of the underlying true hypothesis. The leaf agent then makesa decision and sends it to its supervising agent, at the next level of thetree. Each supervising agent aggregates the decisions from the M members of itsgroup, produces a summary message, and sends it to its supervisor at the nextlevel, and so on. Ultimately, the agent at the root of the tree makes anoverall decision. We derive upper and lower bounds for the Type I and II errorprobabilities associated with this decision with respect to the number of leafagents, which in turn characterize the converge rates of the Type I, Type II,and total error probabilities. We also provide a message-passing schemeinvolving non-binary message alphabets and characterize the exponent of theerror probability with respect to the message alphabet size.
arxiv-1800-194 | Scaling Genetic Programming for Source Code Modification | http://arxiv.org/pdf/1211.5098v1.pdf | author:Brendan Cody-Kenny, Stephen Barrett category:cs.NE cs.SE published:2012-11-21 summary:In Search Based Software Engineering, Genetic Programming has been used forbug fixing, performance improvement and parallelisation of programs through themodification of source code. Where an evolutionary computation algorithm, suchas Genetic Programming, is to be applied to similar code manipulation tasks,the complexity and size of source code for real-world software poses ascalability problem. To address this, we intend to inspect how the SoftwareEngineering concepts of modularity, granularity and localisation of change canbe reformulated as additional mechanisms within a Genetic Programmingalgorithm.
arxiv-1800-195 | Quantifying Causal Coupling Strength: A Lag-specific Measure For Multivariate Time Series Related To Transfer Entropy | http://arxiv.org/pdf/1210.2748v2.pdf | author:Jakob Runge, Jobst Heitzig, Norbert Marwan, Jürgen Kurths category:cs.IT math.IT stat.ML published:2012-10-09 summary:While it is an important problem to identify the existence of causalassociations between two components of a multivariate time series, a topicaddressed in Runge et al. (2012), it is even more important to assess thestrength of their association in a meaningful way. In the present article wefocus on the problem of defining a meaningful coupling strength usinginformation theoretic measures and demonstrate the short-comings of thewell-known mutual information and transfer entropy. Instead, we propose acertain time-delayed conditional mutual information, the momentary informationtransfer (MIT), as a measure of association that is general, causal andlag-specific, reflects a well interpretable notion of coupling strength and ispractically computable. MIT is based on the fundamental concept of sourceentropy, which we utilize to yield a notion of coupling strength that is,compared to mutual information and transfer entropy, well interpretable, inthat for many cases it solely depends on the interaction of the two componentsat a certain lag. In particular, MIT is thus in many cases able to exclude themisleading influence of autodependency within a process in aninformation-theoretic way. We formalize and prove this idea analytically andnumerically for a general class of nonlinear stochastic processes andillustrate the potential of MIT on climatological data.
arxiv-1800-196 | Partition Tree Weighting | http://arxiv.org/pdf/1211.0587v2.pdf | author:Joel Veness, Martha White, Michael Bowling, András György category:cs.IT cs.LG math.IT stat.ML published:2012-11-03 summary:This paper introduces the Partition Tree Weighting technique, an efficientmeta-algorithm for piecewise stationary sources. The technique works byperforming Bayesian model averaging over a large class of possible partitionsof the data into locally stationary segments. It uses a prior, closely relatedto the Context Tree Weighting technique of Willems, that is well suited to datacompression applications. Our technique can be applied to any codingdistribution at an additional time and space cost only logarithmic in thesequence length. We provide a competitive analysis of the redundancy of ourmethod, and explore its application in a variety of settings. The order of theredundancy and the complexity of our algorithm matches those of the bestcompetitors available in the literature, and the new algorithm exhibits asuperior complexity-performance trade-off in our experiments.
arxiv-1800-197 | A Hybrid Bacterial Foraging Algorithm For Solving Job Shop Scheduling Problems | http://arxiv.org/pdf/1211.4971v1.pdf | author:S. Narendhar, T. Amudha category:cs.NE published:2012-11-21 summary:Bio-Inspired computing is the subset of Nature-Inspired computing. Job ShopScheduling Problem is categorized under popular scheduling problems. In thisresearch work, Bacterial Foraging Optimization was hybridized with Ant ColonyOptimization and a new technique Hybrid Bacterial Foraging Optimization forsolving Job Shop Scheduling Problem was proposed. The optimal solutionsobtained by proposed Hybrid Bacterial Foraging Optimization algorithms are muchbetter when compared with the solutions obtained by Bacterial ForagingOptimization algorithm for well-known test problems of different sizes. Fromthe implementation of this research work, it could be observed that theproposed Hybrid Bacterial Foraging Optimization was effective than BacterialForaging Optimization algorithm in solving Job Shop Scheduling Problems. HybridBacterial Foraging Optimization is used to implement real world Job ShopScheduling Problems.
arxiv-1800-198 | Summarizing Reviews with Variable-length Syntactic Patterns and Topic Models | http://arxiv.org/pdf/1211.4929v1.pdf | author:Trung V. Nguyen, Alice H. Oh category:cs.IR cs.CL published:2012-11-21 summary:We present a novel summarization framework for reviews of products andservices by selecting informative and concise text segments from the reviews.Our method consists of two major steps. First, we identify five frequentlyoccurring variable-length syntactic patterns and use them to extract candidatesegments. Then we use the output of a joint generative sentiment topic model tofilter out the non-informative segments. We verify the proposed method withquantitative and qualitative experiments. In a quantitative study, our approachoutperforms previous methods in producing informative segments and summariesthat capture aspects of products and services as expressed in theuser-generated pros and cons lists. Our user study with ninety users resonateswith this result: individual segments extracted and filtered by our method arerated as more useful by users compared to previous approaches by users.
arxiv-1800-199 | Localization from Incomplete Noisy Distance Measurements | http://arxiv.org/pdf/1103.1417v4.pdf | author:Adel Javanmard, Andrea Montanari category:math.ST cs.LG cs.SY math.OC math.PR stat.TH published:2011-03-08 summary:We consider the problem of positioning a cloud of points in the Euclideanspace $\mathbb{R}^d$, using noisy measurements of a subset of pairwisedistances. This task has applications in various areas, such as sensor networklocalization and reconstruction of protein conformations from NMR measurements.Also, it is closely related to dimensionality reduction problems and manifoldlearning, where the goal is to learn the underlying global geometry of a dataset using local (or partial) metric information. Here we propose areconstruction algorithm based on semidefinite programming. For a randomgeometric graph model and uniformly bounded noise, we provide a precisecharacterization of the algorithm's performance: In the noiseless case, we finda radius $r_0$ beyond which the algorithm reconstructs the exact positions (upto rigid transformations). In the presence of noise, we obtain upper and lowerbounds on the reconstruction error that match up to a factor that depends onlyon the dimension $d$, and the average degree of the nodes in the graph.
arxiv-1800-200 | A Traveling Salesman Learns Bayesian Networks | http://arxiv.org/pdf/1211.4888v1.pdf | author:Tuhin Sahai, Stefan Klus, Michael Dellnitz category:cs.LG stat.ML published:2012-11-20 summary:Structure learning of Bayesian networks is an important problem that arisesin numerous machine learning applications. In this work, we present a novelapproach for learning the structure of Bayesian networks using the solution ofan appropriately constructed traveling salesman problem. In our approach, onecomputes an optimal ordering (partially ordered set) of random variables usingmethods for the traveling salesman problem. This ordering significantly reducesthe search space for the subsequent greedy optimization that computes the finalstructure of the Bayesian network. We demonstrate our approach of learningBayesian networks on real world census and weather datasets. In both cases, wedemonstrate that the approach very accurately captures dependencies betweenrandom variables. We check the accuracy of the predictions based on independentstudies in both application domains.
arxiv-1800-201 | Domain Adaptations for Computer Vision Applications | http://arxiv.org/pdf/1211.4860v1.pdf | author:Oscar Beijbom category:cs.CV cs.LG stat.ML published:2012-11-20 summary:A basic assumption of statistical learning theory is that train and test dataare drawn from the same underlying distribution. Unfortunately, this assumptiondoesn't hold in many applications. Instead, ample labeled data might exist in aparticular `source' domain while inference is needed in another, `target'domain. Domain adaptation methods leverage labeled data from both domains toimprove classification on unseen data in the target domain. In this work wesurvey domain transfer learning methods for various application domains withfocus on recent work in Computer Vision.
arxiv-1800-202 | A survey of non-exchangeable priors for Bayesian nonparametric models | http://arxiv.org/pdf/1211.4798v1.pdf | author:Nicholas J. Foti, Sinead Williamson category:stat.ML cs.LG published:2012-11-20 summary:Dependent nonparametric processes extend distributions over measures, such asthe Dirichlet process and the beta process, to give distributions overcollections of measures, typically indexed by values in some covariate space.Such models are appropriate priors when exchangeability assumptions do nothold, and instead we want our model to vary fluidly with some set ofcovariates. Since the concept of dependent nonparametric processes wasformalized by MacEachern [1], there have been a number of models proposed andused in the statistics and machine learning literatures. Many of these modelsexhibit underlying similarities, an understanding of which, we hope, will helpin selecting an appropriate prior, developing new models, and leveraginginference techniques.
arxiv-1800-203 | Matching Through Features and Features Through Matching | http://arxiv.org/pdf/1211.4771v1.pdf | author:Ganesh Sundaramoorthi, Yanchao Yang category:cs.CV published:2012-11-20 summary:This paper addresses how to construct features for the problem of imagecorrespondence, in particular, the paper addresses how to construct features soas to maintain the right level of invariance versus discriminability. We showthat without additional prior knowledge of the 3D scene, the right tradeoffcannot be established in a pre-processing step of the images as is typicallydone in most feature-based matching methods. However, given knowledge of thesecond image to match, the tradeoff between invariance and discriminability offeatures in the first image is less ambiguous. This suggests to setup theproblem of feature extraction and matching as a joint estimation problem. Wedevelop a possible mathematical framework, a possible computational algorithm,and we give example demonstration on finding correspondence on images relatedby a scene that undergoes large 3D deformation of non-planar objects and cameraviewpoint change.
arxiv-1800-204 | A unifying representation for a class of dependent random measures | http://arxiv.org/pdf/1211.4753v1.pdf | author:Nicholas J. Foti, Joseph D. Futoma, Daniel N. Rockmore, Sinead Williamson category:stat.ML cs.LG published:2012-11-20 summary:We present a general construction for dependent random measures based onthinning Poisson processes on an augmented space. The framework is notrestricted to dependent versions of a specific nonparametric model, but can beapplied to all models that can be represented using completely random measures.Several existing dependent random measures can be seen as specific cases ofthis framework. Interesting properties of the resulting measures are derivedand the efficacy of the framework is demonstrated by constructing acovariate-dependent latent feature model and topic model that obtain superiorpredictive performance.
arxiv-1800-205 | Random Input Sampling for Complex Models Using Markov Chain Monte Carlo | http://arxiv.org/pdf/1211.4706v1.pdf | author:A. Gokcen Mahmutoglu, Alper T. Erdogan, Alper Demir category:stat.ML published:2012-11-20 summary:Many random processes can be simulated as the output of a deterministic modelaccepting random inputs. Such a model usually describes a complex mathematicalor physical stochastic system and the randomness is introduced in the inputvariables of the model. When the statistics of the output event are known,these input variables have to be chosen in a specific way for the output tohave the prescribed statistics. Because the probability distribution of theinput random variables is not directly known but dictated implicitly by thestatistics of the output random variables, this problem is usually intractablefor classical sampling methods. Based on Markov Chain Monte Carlo we propose anovel method to sample random inputs to such models by introducing amodification to the standard Metropolis-Hastings algorithm. As an example weconsider a system described by a stochastic differential equation (sde) anddemonstrate how sample paths of a random process satisfying this sde can begenerated with our technique.
arxiv-1800-206 | Content based video retrieval | http://arxiv.org/pdf/1211.4683v1.pdf | author:B. V. Patel, B. B. Meshram category:cs.MM cs.CV published:2012-11-20 summary:Content based video retrieval is an approach for facilitating the searchingand browsing of large image collections over World Wide Web. In this approach,video analysis is conducted on low level visual properties extracted from videoframe. We believed that in order to create an effective video retrieval system,visual perception must be taken into account. We conjectured that a techniquewhich employs multiple features for indexing and retrieval would be moreeffective in the discrimination and search tasks of videos. In order tovalidate this claim, content based indexing and retrieval systems wereimplemented using color histogram, various texture features and otherapproaches. Videos were stored in Oracle 9i Database and a user study measuredcorrectness of response.
arxiv-1800-207 | An Effective Method for Fingerprint Classification | http://arxiv.org/pdf/1211.4658v1.pdf | author:Monowar H. Bhuyan, Sarat Saharia, Dhruba Kr Bhattacharyya category:cs.CV cs.CR 68U35 I.5.3 published:2012-11-20 summary:This paper presents an effective method for fingerprint classification usingdata mining approach. Initially, it generates a numeric code sequence for eachfingerprint image based on the ridge flow patterns. Then for each class, a seedis selected by using a frequent itemsets generation technique. These seeds aresubsequently used for clustering the fingerprint images. The proposed methodwas tested and evaluated in terms of several real-life datasets and asignificant improvement in reducing the misclassification errors has beennoticed in comparison to its other counterparts.
arxiv-1800-208 | A Brief Review of Data Mining Application Involving Protein Sequence Classification | http://arxiv.org/pdf/1211.4866v1.pdf | author:Suprativ Saha, Rituparna Chaki category:cs.DB cs.NE published:2012-11-20 summary:Data mining techniques have been used by researchers for analyzing proteinsequences. In protein analysis, especially in protein sequence classification,selection of feature is most important. Popular protein sequence classificationtechniques involve extraction of specific features from the sequences.Researchers apply some well-known classification techniques like neuralnetworks, Genetic algorithm, Fuzzy ARTMAP, Rough Set Classifier etc foraccurate classification. This paper presents a review is with three differentclassification models such as neural network model, fuzzy ARTMAP model andRough set classifier model. A new technique for classifying protein sequenceshave been proposed in the end. The proposed technique tries to reduce thecomputational overheads encountered by earlier approaches and increase theaccuracy of classification.
arxiv-1800-209 | Five Modulus Method For Image Compression | http://arxiv.org/pdf/1211.4591v1.pdf | author:Firas A. Jassim, Hind E. Qassim category:cs.CV cs.MM published:2012-11-19 summary:Data is compressed by reducing its redundancy, but this also makes the dataless reliable, more prone to errors. In this paper a novel approach of imagecompression based on a new method that has been created for image compressionwhich is called Five Modulus Method (FMM). The new method consists ofconverting each pixel value in an 8-by-8 block into a multiple of 5 for each ofthe R, G and B arrays. After that, the new values could be divided by 5 to getnew values which are 6-bit length for each pixel and it is less in storagespace than the original value which is 8-bits. Also, a new protocol forcompression of the new values as a stream of bits has been presented that givesthe opportunity to store and transfer the new compressed image easily.
arxiv-1800-210 | Applying Dynamic Model for Multiple Manoeuvring Target Tracking Using Particle Filtering | http://arxiv.org/pdf/1211.4524v1.pdf | author:Mohammad Javad Parseh, Saeid Pashazadeh category:cs.CV cs.AI published:2012-11-19 summary:In this paper, we applied a dynamic model for manoeuvring targets in SIRparticle filter algorithm for improving tracking accuracy of multiplemanoeuvring targets. In our proposed approach, a color distribution model isused to detect changes of target's model . Our proposed approach controlsdeformation of target's model. If deformation of target's model is larger thana predetermined threshold, then the model will be updated. Global NearestNeighbor (GNN) algorithm is used as data association algorithm. We named ourproposed method as Deformation Detection Particle Filter (DDPF) . DDPF approachis compared with basic SIR-PF algorithm on real airshow videos. Comparisonsresults show that, the basic SIR-PF algorithm is not able to track themanoeuvring targets when the rotation or scaling is occurred in target' smodel. However, DDPF approach updates target's model when the rotation orscaling is occurred. Thus, the proposed approach is able to track themanoeuvring targets more efficiently and accurately.
arxiv-1800-211 | An Effective Fingerprint Classification and Search Method | http://arxiv.org/pdf/1211.4503v1.pdf | author:Monowar H. Bhuyan, D. K. Bhattacharyya category:cs.CV cs.CR 68U35 I.5.3 published:2012-11-19 summary:This paper presents an effective fingerprint classification method designedbased on a hierarchical agglomerative clustering technique. The performance ofthe technique was evaluated in terms of several real-life datasets and asignificant improvement in reducing the misclassification error has beennoticed. This paper also presents a query based faster fingerprint searchmethod over the clustered fingerprint databases. The retrieval accuracy of thesearch method has been found effective in light of several real-life databases.
arxiv-1800-212 | Rate-Distortion Analysis of Multiview Coding in a DIBR Framework | http://arxiv.org/pdf/1211.4499v1.pdf | author:Boshra Rajaei, Thomas Maugey, Hamid-Reza Pourreza, Pascal Frossard category:cs.CV published:2012-11-19 summary:Depth image based rendering techniques for multiview applications have beenrecently introduced for efficient view generation at arbitrary camerapositions. Encoding rate control has thus to consider both texture and depthdata. Due to different structures of depth and texture images and theirdifferent roles on the rendered views, distributing the available bit budgetbetween them however requires a careful analysis. Information loss due totexture coding affects the value of pixels in synthesized views while errors indepth information lead to shift in objects or unexpected patterns at theirboundaries. In this paper, we address the problem of efficient bit allocationbetween textures and depth data of multiview video sequences. We adopt arate-distortion framework based on a simplified model of depth and textureimages. Our model preserves the main features of depth and texture images.Unlike most recent solutions, our method permits to avoid rendering at encodingtime for distortion estimation so that the encoding complexity is notaugmented. In addition to this, our model is independent of the underlyinginpainting method that is used at decoder. Experiments confirm our theoreticalresults and the efficiency of our rate allocation strategy.
arxiv-1800-213 | A Rule-Based Approach For Aligning Japanese-Spanish Sentences From A Comparable Corpora | http://arxiv.org/pdf/1211.4488v1.pdf | author:Jessica C. Ramírez, Yuji Matsumoto category:cs.CL cs.AI published:2012-11-19 summary:The performance of a Statistical Machine Translation System (SMT) system isproportionally directed to the quality and length of the parallel corpus ituses. However for some pair of languages there is a considerable lack of them.The long term goal is to construct a Japanese-Spanish parallel corpus to beused for SMT, whereas, there are a lack of useful Japanese-Spanish parallelCorpus. To address this problem, In this study we proposed a method forextracting Japanese-Spanish Parallel Sentences from Wikipedia using POS taggingand Rule-Based approach. The main focus of this approach is the syntacticfeatures of both languages. Human evaluation was performed over a sample andshows promising results, in comparison with the baseline.
arxiv-1800-214 | Artificial Neural Network Based Optical Character Recognition | http://arxiv.org/pdf/1211.4385v1.pdf | author:Vivek Shrivastava, Navdeep Sharma category:cs.CV cs.NE published:2012-11-19 summary:Optical Character Recognition deals in recognition and classification ofcharacters from an image. For the recognition to be accurate, certaintopological and geometrical properties are calculated, based on which acharacter is classified and recognized. Also, the Human psychology perceivescharacters by its overall shape and features such as strokes, curves,protrusions, enclosures etc. These properties, also called Features areextracted from the image by means of spatial pixel-based calculation. Acollection of such features, called Vectors, help in defining a characteruniquely, by means of an Artificial Neural Network that uses these FeatureVectors.
arxiv-1800-215 | A Sensing Policy Based on Confidence Bounds and a Restless Multi-Armed Bandit Model | http://arxiv.org/pdf/1211.4384v1.pdf | author:Jan Oksanen, Visa Koivunen, H. Vincent Poor category:cs.IT cs.LG math.IT published:2012-11-19 summary:A sensing policy for the restless multi-armed bandit problem with stationarybut unknown reward distributions is proposed. The work is presented in thecontext of cognitive radios in which the bandit problem arises when decidingwhich parts of the spectrum to sense and exploit. It is shown that the proposedpolicy attains asymptotically logarithmic weak regret rate when the rewards arebounded independent and identically distributed or finite state Markovian.Simulation results verifying uniformly logarithmic weak regret are alsopresented. The proposed policy is a centrally coordinated index policy, inwhich the index of a frequency band is comprised of a sample mean term and aconfidence term. The sample mean term promotes spectrum exploitation whereasthe confidence term encourages exploration. The confidence term is designedsuch that the time interval between consecutive sensing instances of anysuboptimal band grows exponentially. This exponential growth between suboptimalsensing time instances leads to logarithmically growing weak regret. Simulationresults demonstrate that the proposed policy performs better than other similarmethods in the literature.
arxiv-1800-216 | Bayesian nonparametric models for ranked data | http://arxiv.org/pdf/1211.4321v1.pdf | author:Francois Caron, Yee Whye Teh category:stat.ML cs.LG stat.ME published:2012-11-19 summary:We develop a Bayesian nonparametric extension of the popular Plackett-Lucechoice model that can handle an infinite number of choice items. Our frameworkis based on the theory of random atomic measures, with the prior specified by agamma process. We derive a posterior characterization and a simple andeffective Gibbs sampler for posterior simulation. We develop a time-varyingextension of our model, and apply it to the New York Times lists of weeklybestselling books.
arxiv-1800-217 | Efficient Superimposition Recovering Algorithm | http://arxiv.org/pdf/1211.4307v1.pdf | author:Han Li, Kun Gai, Pinghua Gong, Changshui Zhang category:cs.CV published:2012-11-19 summary:In this article, we address the issue of recovering latent transparent layersfrom superimposition images. Here, we assume we have the estimatedtransformations and extracted gradients of latent layers. To rapidly recoverhigh-quality image layers, we propose an Efficient Superimposition RecoveringAlgorithm (ESRA) by extending the framework of accelerated gradient method. Inaddition, a key building block (in each iteration) in our proposed method isthe proximal operator calculating. Here we propose to employ a dual approachand present our Parallel Algorithm with Constrained Total Variation (PACTV)method. Our recovering method not only reconstructs high-quality layers withoutcolor-bias problem, but also theoretically guarantees good convergenceperformance.
arxiv-1800-218 | Non-Local Patch Regression: Robust Image Denoising in Patch Space | http://arxiv.org/pdf/1211.4264v1.pdf | author:Kunal N. Chaudhury, Amit Singer category:cs.CV published:2012-11-18 summary:It was recently demonstrated in [Chaudhury et al.,Non-Local EuclideanMedians,2012] that the denoising performance of Non-Local Means (NLM) can beimproved at large noise levels by replacing the mean by the robust Euclideanmedian. Numerical experiments on synthetic and natural images showed that thelatter consistently performed better than NLM beyond a certain noise level, andsignificantly so for images with sharp edges. The Euclidean mean and median canbe put into a common regression (on the patch space) framework, in which thel_2 norm of the residuals is considered in the former, while the l_1 norm isconsidered in the latter. The natural question then is what happens if weconsider l_p (0<p<1) regression? We investigate this possibility in this paper.
arxiv-1800-219 | PRISMA: PRoximal Iterative SMoothing Algorithm | http://arxiv.org/pdf/1206.2372v2.pdf | author:Francesco Orabona, Andreas Argyriou, Nathan Srebro category:math.OC cs.LG published:2012-06-11 summary:Motivated by learning problems including max-norm regularized matrixcompletion and clustering, robust PCA and sparse inverse covariance selection,we propose a novel optimization algorithm for minimizing a convex objectivewhich decomposes into three parts: a smooth part, a simple non-smooth Lipschitzpart, and a simple non-smooth non-Lipschitz part. We use a time variantsmoothing strategy that allows us to obtain a guarantee that does not depend onknowing in advance the total number of iterations nor a bound on the domain.
arxiv-1800-220 | Semantic Polarity of Adjectival Predicates in Online Reviews | http://arxiv.org/pdf/1211.4161v1.pdf | author:Ae-Lim Ahn, Éric Laporte, Jee-Sun Nam category:cs.CL published:2012-11-17 summary:Web users produce more and more documents expressing opinions. Because thesehave become important resources for customers and manufacturers, many havefocused on them. Opinions are often expressed through adjectives with positiveor negative semantic values. In extracting information from users' opinion inonline reviews, exact recognition of the semantic polarity of adjectives is oneof the most important requirements. Since adjectives have different semanticorientations according to contexts, it is not satisfying to extract opinioninformation without considering the semantic and lexical relations between theadjectives and the feature nouns appropriate to a given domain. In this paper,we present a classification of adjectives by polarity, and we analyzeadjectives that are undetermined in the absence of contexts. Our researchshould be useful for accurately predicting semantic orientations of opinionsentences, and should be taken into account before relying on an automaticmethods.
arxiv-1800-221 | Efficiently Learning from Revealed Preference | http://arxiv.org/pdf/1211.4150v1.pdf | author:Morteza Zadimoghaddam, Aaron Roth category:cs.GT cs.DS cs.LG published:2012-11-17 summary:In this paper, we consider the revealed preferences problem from a learningperspective. Every day, a price vector and a budget is drawn from an unknowndistribution, and a rational agent buys his most preferred bundle according tosome unknown utility function, subject to the given prices and budgetconstraint. We wish not only to find a utility function which rationalizes afinite set of observations, but to produce a hypothesis valuation functionwhich accurately predicts the behavior of the agent in the future. We giveefficient algorithms with polynomial sample-complexity for agents with linearvaluation functions, as well as for agents with linearly separable, concavevaluation functions with bounded second derivative.
arxiv-1800-222 | Data Clustering via Principal Direction Gap Partitioning | http://arxiv.org/pdf/1211.4142v1.pdf | author:Ralph Abbey, Jeremy Diepenbrock, Amy Langville, Carl Meyer, Shaina Race, Dexin Zhou category:stat.ML cs.LG published:2012-11-17 summary:We explore the geometrical interpretation of the PCA based clusteringalgorithm Principal Direction Divisive Partitioning (PDDP). We give severalexamples where this algorithm breaks down, and suggest a new method, gappartitioning, which takes into account natural gaps in the data betweenclusters. Geometric features of the PCA space are derived and illustrated andexperimental results are given which show our method is comparable on thedatasets used in the original paper on PDDP.
arxiv-1800-223 | On Calibrated Predictions for Auction Selection Mechanisms | http://arxiv.org/pdf/1211.3955v1.pdf | author:H. Brendan McMahan, Omkar Muralidharan category:cs.GT cs.LG published:2012-11-16 summary:Calibration is a basic property for prediction systems, and algorithms forachieving it are well-studied in both statistics and machine learning. In manyapplications, however, the predictions are used to make decisions that selectwhich observations are made. This makes calibration difficult, as adjustingpredictions to achieve calibration changes future data. We focus onclick-through-rate (CTR) prediction for search ad auctions. Here, CTRpredictions are used by an auction that determines which ads are shown, and wewant to maximize the value generated by the auction. We show that certain natural notions of calibration can be impossible toachieve, depending on the details of the auction. We also show that it can beimpossible to maximize auction efficiency while using calibrated predictions.Finally, we give conditions under which calibration is achievable andsimultaneously maximizes auction efficiency: roughly speaking, bids and queriesmust not contain information about CTRs that is not already captured by thepredictions.
arxiv-1800-224 | Visual Recognition of Isolated Swedish Sign Language Signs | http://arxiv.org/pdf/1211.3901v1.pdf | author:Saad Akram, Jonas Beskow, Hedvig Kjellstrom category:cs.CV published:2012-11-16 summary:We present a method for recognition of isolated Swedish Sign Language signs.The method will be used in a game intended to help children training signing athome, as a complement to training with a teacher. The target group is notprimarily deaf children, but children with language disorders. Using signlanguage as a support in conversation has been shown to greatly stimulate thespeech development of such children. The signer is captured with an RGB-D(Kinect) sensor, which has three advantages over a regular RGB camera. Firstly,it allows complex backgrounds to be removed easily. We segment the hands andface based on skin color and depth information. Secondly, it helps with theresolution of hand over face occlusion. Thirdly, signs take place in 3D; someaspects of the signs are defined by hand motion vertically to the image plane.This motion can be estimated if the depth is observable. The 3D motion of thehands relative to the torso are used as a cue together with the hand shape, andHMMs trained with this input are used for classification. To obtain higherrobustness towards differences across signers, Fisher Linear DiscriminantAnalysis is used to find the combinations of features that are most descriptivefor each sign, regardless of signer. Experiments show that the system candistinguish signs from a challenging 94 word vocabulary with a precision of upto 94% in the signer dependent case and up to 47% in the signer independentcase.
arxiv-1800-225 | A Bayesian Interpretation of the Particle Swarm Optimization and Its Kernel Extension | http://arxiv.org/pdf/1211.3845v1.pdf | author:Peter Andras category:cs.NE published:2012-11-16 summary:Particle swarm optimization is a popular method for solving difficultoptimization problems. There have been attempts to formulate the method informal probabilistic or stochastic terms (e.g. bare bones particle swarm) withthe aim to achieve more generality and explain the practical behavior of themethod. Here we present a Bayesian interpretation of the particle swarmoptimization. This interpretation provides a formal framework for incorporationof prior knowledge about the problem that is being solved. Furthermore, it alsoallows to extend the particle optimization method through the use of kernelfunctions that represent the intermediary transformation of the data into adifferent space where the optimization problem is expected to be easier to beresolved, such transformation can be seen as a form of prior knowledge aboutthe nature of the optimization problem. We derive from the general Bayesianformulation the commonly used particle swarm methods as particular cases.
arxiv-1800-226 | Clustering and Latent Semantic Indexing Aspects of the Singular Value Decomposition | http://arxiv.org/pdf/1011.4104v4.pdf | author:Andri Mirzal category:cs.LG cs.NA math.SP 15A18, 65F15 published:2010-11-17 summary:This paper discusses clustering and latent semantic indexing (LSI) aspects ofthe singular value decomposition (SVD). The purpose of this paper is twofold.The first is to give an explanation on how and why the singular vectors can beused in clustering. And the second is to show that the two seemingly unrelatedSVD aspects actually originate from the same source: related vertices tend tobe more clustered in the graph representation of lower rank approximate matrixusing the SVD than in the original semantic graph. Accordingly, the SVD canimprove retrieval performance of an information retrieval system since queriesmade to the approximate matrix can retrieve more relevant documents and filterout more irrelevant documents than the same queries made to the originalmatrix. By utilizing this fact, we will devise an LSI algorithm that mimicksSVD capability in clustering related vertices. Convergence analysis shows thatthe algorithm is convergent and produces a unique solution for each input.Experimental results using some standard datasets in LSI research show thatretrieval performances of the algorithm are comparable to the SVD's. Inaddition, the algorithm is more practical and easier to use because there is noneed to determine decomposition rank which is crucial in driving retrievalperformance of the SVD.
arxiv-1800-227 | A Principled Approach to Grammars for Controlled Natural Languages and Predictive Editors | http://arxiv.org/pdf/1211.3643v1.pdf | author:Tobias Kuhn category:cs.CL published:2012-11-15 summary:Controlled natural languages (CNL) with a direct mapping to formal logic havebeen proposed to improve the usability of knowledge representation systems,query interfaces, and formal specifications. Predictive editors are a popularapproach to solve the problem that CNLs are easy to read but hard to write.Such predictive editors need to be able to "look ahead" in order to show allpossible continuations of a given unfinished sentence. Such lookahead features,however, are difficult to implement in a satisfying way with existing grammarframeworks, especially if the CNL supports complex nonlocal structures such asanaphoric references. Here, methods and algorithms are presented for a newgrammar notation called Codeco, which is specifically designed for controllednatural languages and predictive editors. A parsing approach for Codeco basedon an extended chart parsing algorithm is presented. A large subset of AttemptoControlled English (ACE) has been represented in Codeco. Evaluation of thisgrammar and the parser implementation shows that the approach is practical,adequate and efficient.
arxiv-1800-228 | SERAPH: Semi-supervised Metric Learning Paradigm with Hyper Sparsity | http://arxiv.org/pdf/1105.0167v3.pdf | author:Gang Niu, Bo Dai, Makoto Yamada, Masashi Sugiyama category:stat.ML cs.AI published:2011-05-01 summary:We propose a general information-theoretic approach called Seraph(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metriclearning that does not rely upon the manifold assumption. Given the probabilityparameterized by a Mahalanobis distance, we maximize the entropy of thatprobability on labeled data and minimize it on unlabeled data following entropyregularization, which allows the supervised and unsupervised parts to beintegrated in a natural and meaningful way. Furthermore, Seraph is regularizedby encouraging a low-rank projection induced from the metric. The optimizationof Seraph is solved efficiently and stably by an EM-like scheme with theanalytical E-Step and convex M-Step. Experiments demonstrate that Seraphcompares favorably with many well-known global and local metric learningmethods.
arxiv-1800-229 | Ensemble Clustering with Logic Rules | http://arxiv.org/pdf/1207.3961v3.pdf | author:Deniz Akdemir category:stat.ML cs.LG published:2012-07-17 summary:In this article, the logic rule ensembles approach to supervised learning isapplied to the unsupervised or semi-supervised clustering. Logic rules whichwere obtained by combining simple conjunctive rules are used to partition theinput space and an ensemble of these rules is used to define a similaritymatrix. Similarity partitioning is used to partition the data in anhierarchical manner. We have used internal and external measures of clustervalidity to evaluate the quality of clusterings or to identify the number ofclusters.
arxiv-1800-230 | Memory Capacity of a Random Neural Network | http://arxiv.org/pdf/1211.3451v1.pdf | author:Matt Stowe category:cs.NE published:2012-11-14 summary:This paper considers the problem of information capacity of a random neuralnetwork. The network is represented by matrices that are square andsymmetrical. The matrices have a weight which determines the highest and lowestpossible value found in the matrix. The examined matrices are randomlygenerated and analyzed by a computer program. We find the surprising resultthat the capacity of the network is a maximum for the binary random neuralnetwork and it does not change as the number of quantization levels associatedwith the weights increases.
arxiv-1800-231 | Spectral Clustering: An empirical study of Approximation Algorithms and its Application to the Attrition Problem | http://arxiv.org/pdf/1211.3444v1.pdf | author:B. Cung, T. Jin, J. Ramirez, A. Thompson, C. Boutsidis, D. Needell category:cs.LG math.NA stat.ML published:2012-11-14 summary:Clustering is the problem of separating a set of objects into groups (calledclusters) so that objects within the same cluster are more similar to eachother than to those in different clusters. Spectral clustering is a nowwell-known method for clustering which utilizes the spectrum of the datasimilarity matrix to perform this separation. Since the method relies onsolving an eigenvector problem, it is computationally expensive for largedatasets. To overcome this constraint, approximation methods have beendeveloped which aim to reduce running time while maintaining accurateclassification. In this article, we summarize and experimentally evaluateseveral approximation methods for spectral clustering. From an applicationsstandpoint, we employ spectral clustering to solve the so-called attritionproblem, where one aims to identify from a set of employees those who arelikely to voluntarily leave the company from those who are not. Our study shedslight on the empirical performance of existing approximate spectral clusteringmethods and shows the applicability of these methods in an important businessoptimization related problem.
arxiv-1800-232 | Genetic Optimization of Keywords Subset in the Classification Analysis of Texts Authorship | http://arxiv.org/pdf/1211.3402v1.pdf | author:Bohdan Pavlyshenko category:cs.IR cs.CL published:2012-11-14 summary:The genetic selection of keywords set, the text frequencies of which areconsidered as attributes in text classification analysis, has been analyzed.The genetic optimization was performed on a set of words, which is the fractionof the frequency dictionary with given frequency limits. The frequencydictionary was formed on the basis of analyzed text array of texts of Englishfiction. As the fitness function which is minimized by the genetic algorithm,the error of nearest k neighbors classifier was used. The obtained results showhigh precision and recall of texts classification by authorship categories onthe basis of attributes of keywords set which were selected by the geneticalgorithm from the frequency dictionary.
arxiv-1800-233 | Sequence Transduction with Recurrent Neural Networks | http://arxiv.org/pdf/1211.3711v1.pdf | author:Alex Graves category:cs.NE cs.LG stat.ML published:2012-11-14 summary:Many machine learning tasks can be expressed as the transformation---or\emph{transduction}---of input sequences into output sequences: speechrecognition, machine translation, protein secondary structure prediction andtext-to-speech to name but a few. One of the key challenges in sequencetransduction is learning to represent both the input and output sequences in away that is invariant to sequential distortions such as shrinking, stretchingand translating. Recurrent neural networks (RNNs) are a powerful sequencelearning architecture that has proven capable of learning such representations.However RNNs traditionally require a pre-defined alignment between the inputand output sequences to perform transduction. This is a severe limitation since\emph{finding} the alignment is the most difficult aspect of many sequencetransduction problems. Indeed, even determining the length of the outputsequence is often challenging. This paper introduces an end-to-end,probabilistic sequence transduction system, based entirely on RNNs, that is inprinciple able to transform any input sequence into any finite, discrete outputsequence. Experimental results for phoneme recognition are provided on theTIMIT speech corpus.
arxiv-1800-234 | A Comparison of Meta-heuristic Search for Interactive Software Design | http://arxiv.org/pdf/1211.3371v1.pdf | author:C. L. Simons, J. E. Smith category:cs.AI cs.NE published:2012-11-14 summary:Advances in processing capacity, coupled with the desire to tackle problemswhere a human subjective judgment plays an important role in determining thevalue of a proposed solution, has led to a dramatic rise in the number ofapplications of Interactive Artificial Intelligence. Of particular note is thecoupling of meta-heuristic search engines with user-provided evaluation andrating of solutions, usually in the form of Interactive Evolutionary Algorithms(IEAs). These have a well-documented history of successes, but arguably thepreponderance of IEAs stems from this history, rather than as a consciousdesign choice of meta-heuristic based on the characteristics of the problem athand. This paper sets out to examine the basis for that assumption, taking as acase study the domain of interactive software design. We consider a range offactors that should affect the design choice including ease of use,scalability, and of course, performance, i.e. that ability to generate goodsolutions within the limited number of evaluations available in interactivework before humans lose focus. We then evaluate three methods, namely greedylocal search, an evolutionary algorithm and ant colony optimization, with avariety of representations for candidate solutions. Results show that aftersuitable parameter tuning, ant colony optimization is highly effective withininteractive search and out-performs evolutionary algorithms with respect toincreasing numbers of attributes and methods in the software design problem.However, when larger numbers of classes are present in the software design, anevolutionary algorithm using a naive grouping integer-based representationappears more scalable.
arxiv-1800-235 | Distributed Non-Stochastic Experts | http://arxiv.org/pdf/1211.3212v1.pdf | author:Varun Kanade, Zhenming Liu, Bozidar Radunovic category:cs.LG cs.AI published:2012-11-14 summary:We consider the online distributed non-stochastic experts problem, where thedistributed system consists of one coordinator node that is connected to $k$sites, and the sites are required to communicate with each other via thecoordinator. At each time-step $t$, one of the $k$ site nodes has to pick anexpert from the set ${1, ..., n}$, and the same site receives information aboutpayoffs of all experts for that round. The goal of the distributed system is tominimize regret at time horizon $T$, while simultaneously keeping communicationto a minimum. The two extreme solutions to this problem are: (i) Full communication: Thisessentially simulates the non-distributed setting to obtain the optimal$O(\sqrt{\log(n)T})$ regret bound at the cost of $T$ communication. (ii) Nocommunication: Each site runs an independent copy : the regret is$O(\sqrt{log(n)kT})$ and the communication is 0. This paper shows thedifficulty of simultaneously achieving regret asymptotically better than$\sqrt{kT}$ and communication better than $T$. We give a novel algorithm thatfor an oblivious adversary achieves a non-trivial trade-off: regret$O(\sqrt{k^{5(1+\epsilon)/6} T})$ and communication $O(T/k^{\epsilon})$, forany value of $\epsilon \in (0, 1/5)$. We also consider a variant of the model,where the coordinator picks the expert. In this model, we show that thelabel-efficient forecaster of Cesa-Bianchi et al. (2005) already gives usstrategy that is near optimal in regret vs communication trade-off.
arxiv-1800-236 | Network Sampling: From Static to Streaming Graphs | http://arxiv.org/pdf/1211.3412v1.pdf | author:Nesreen K. Ahmed, Jennifer Neville, Ramana Kompella category:cs.SI cs.DS cs.LG physics.soc-ph stat.ML published:2012-11-14 summary:Network sampling is integral to the analysis of social, information, andbiological networks. Since many real-world networks are massive in size,continuously evolving, and/or distributed in nature, the network structure isoften sampled in order to facilitate study. For these reasons, a more thoroughand complete understanding of network sampling is critical to support the fieldof network science. In this paper, we outline a framework for the generalproblem of network sampling, by highlighting the different objectives,population and units of interest, and classes of network sampling methods. Inaddition, we propose a spectrum of computational models for network samplingmethods, ranging from the traditionally studied model based on the assumptionof a static domain to a more challenging model that is appropriate forstreaming domains. We design a family of sampling methods based on the conceptof graph induction that generalize across the full spectrum of computationalmodels (from static to streaming) while efficiently preserving many of thetopological properties of the input graphs. Furthermore, we demonstrate howtraditional static sampling algorithms can be modified for graph streams foreach of the three main classes of sampling methods: node, edge, andtopology-based sampling. Our experimental results indicate that our proposedfamily of sampling methods more accurately preserves the underlying propertiesof the graph for both static and streaming graphs. Finally, we study the impactof network sampling algorithms on the parameter estimation and performanceevaluation of relational classification algorithms.
arxiv-1800-237 | On the Prior and Posterior Distributions Used in Graphical Modelling | http://arxiv.org/pdf/1201.4058v2.pdf | author:Marco Scutari category:math.ST stat.ML stat.TH published:2012-01-19 summary:Graphical model learning and inference are often performed using Bayesiantechniques. In particular, learning is usually performed in two separate steps.First, the graph structure is learned from the data; then the parameters of themodel are estimated conditional on that graph structure. While the probabilitydistributions involved in this second step have been studied in depth, the onesused in the first step have not been explored in as much detail. In this paper, we will study the prior and posterior distributions definedover the space of the graph structures for the purpose of learning thestructure of a graphical model. In particular, we will provide acharacterisation of the behaviour of those distributions as a function of thepossible edges of the graph. We will then use the properties resulting fromthis characterisation to define measures of structural variability for bothBayesian and Markov networks, and we will point out some of their possibleapplications.
arxiv-1800-238 | Time-series Scenario Forecasting | http://arxiv.org/pdf/1211.3010v1.pdf | author:Sriharsha Veeramachaneni category:stat.ML cs.LG stat.AP published:2012-11-13 summary:Many applications require the ability to judge uncertainty of time-seriesforecasts. Uncertainty is often specified as point-wise error bars around amean or median forecast. Due to temporal dependencies, such a method obscuressome information. We would ideally have a way to query the posteriorprobability of the entire time-series given the predictive variables, or at aminimum, be able to draw samples from this distribution. We use a Bayesiandictionary learning algorithm to statistically generate an ensemble offorecasts. We show that the algorithm performs as well as a physics-basedensemble method for temperature forecasts for Houston. We conclude that themethod shows promise for scenario forecasting where physics-based methods areabsent.
arxiv-1800-239 | Shattering-Extremal Systems | http://arxiv.org/pdf/1211.2980v1.pdf | author:Shay Moran category:math.CO cs.CG cs.DM cs.LG published:2012-11-13 summary:The Shatters relation and the VC dimension have been investigated since theearly seventies. These concepts have found numerous applications in statistics,combinatorics, learning theory and computational geometry. Shattering extremalsystems are set-systems with a very rich structure and many differentcharacterizations. The goal of this thesis is to elaborate on the structure ofthese systems.
arxiv-1800-240 | The application of a perceptron model to classify an individual's response to a proposed loading dose regimen of Warfarin | http://arxiv.org/pdf/1211.2945v1.pdf | author:Cen Wan, Irina V. Biktasheva, Steven Lane category:stat.AP cs.NE 68T05, 92C50 published:2012-11-13 summary:The dose regimen of Warfarin is separated into two phases. Firstly a loadingdose is given, which is designed to bring the International Normalisation Ratio(INR) to within therapeutic range. Then a stable maintenance dose is given tomaintain the INR within therapeutic range. In the United Kingdom (UK) theloading dose is usually given as three individual daily doses, the standardloading dose being 10mg on days one and two and 5mgs on day three, which can bevaried at the discretion of the clinician. However, due to the largeinter-individual variation in the response to Warfarin therapy, it is difficultto identify which patients will reach the narrow therapeutic window for targetINR, and which will be above or below the therapeutic window. The aim of thisresearch was to develop a methodology using a neural networks classificationalgorithm and data mining techniques to predict for a given loading dose andpatient characteristics if the patient is more likely to achieve target INR ormore likely to be above or below therapeutic range. Multilayer perceptron (MLP) and 10-fold stratified cross validationalgorithms were used to determine an artificial neural network to classifypatients' response to their initial Warfarin loading dose. The resulting neuralnetwork model correctly classifies an individual's response to their Warfarinloading dose over 80% of the time. As well as taking into account the initialloading dose, the final model also includes demographic, genetic and a numberof other potential confounding factors. With this model clinicians canpredetermine whether a given loading regimen, along with specific patientcharacteristics will achieve a therapeutic response for a particular patient.Thus tailoring the loading dose regimen to meet the individual needs of thepatient and reducing the risk of adverse drug reactions associated withWarfarin.
arxiv-1800-241 | Sure independence screening in generalized linear models with NP-dimensionality | http://arxiv.org/pdf/0903.5255v5.pdf | author:Jianqing Fan, Rui Song category:stat.ME math.ST stat.ML stat.TH published:2009-03-30 summary:Ultrahigh-dimensional variable selection plays an increasingly important rolein contemporary scientific discoveries and statistical research. Among others,Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] proposean independent screening framework by ranking the marginal correlations. Theyshowed that the correlation ranking procedure possesses a sure independencescreening property within the context of the linear model with Gaussiancovariates and responses. In this paper, we propose a more general version ofthe independent learning with ranking the maximum marginal likelihood estimatesor the maximum marginal likelihood itself in generalized linear models. We showthat the proposed methods, with Fan and Lv [J. R. Stat. Soc. Ser. B Stat.Methodol. 70 (2008) 849-911] as a very special case, also possess the surescreening property with vanishing false selection rate. The conditions underwhich the independence learning possesses a sure screening is surprisinglysimple. This justifies the applicability of such a simple method in a widespectrum. We quantify explicitly the extent to which the dimensionality can bereduced by independence screening, which depends on the interactions of thecovariance matrix of covariates and true parameters. Simulation studies areused to illustrate the utility of the proposed approaches. In addition, weestablish an exponential inequality for the quasi-maximum likelihood estimatorwhich is useful for high-dimensional statistical learning.
arxiv-1800-242 | Boosting Simple Collaborative Filtering Models Using Ensemble Methods | http://arxiv.org/pdf/1211.2891v1.pdf | author:Ariel Bar, Lior Rokach, Guy Shani, Bracha Shapira, Alon Schclar category:cs.IR cs.LG stat.ML published:2012-11-13 summary:In this paper we examine the effect of applying ensemble learning to theperformance of collaborative filtering methods. We present several systematicapproaches for generating an ensemble of collaborative filtering models basedon a single collaborative filtering algorithm (single-model or homogeneousensemble). We present an adaptation of several popular ensemble techniques inmachine learning for the collaborative filtering domain, including bagging,boosting, fusion and randomness injection. We evaluate the proposed approach onseveral types of collaborative filtering base models: k- NN, matrixfactorization and a neighborhood matrix factorization model. Empiricalevaluation shows a prediction improvement compared to all base CF algorithms.In particular, we show that the performance of an ensemble of simple (weak) CFmodels such as k-NN is competitive compared with a single strong CF model (suchas matrix factorization) while requiring an order of magnitude lesscomputational cost.
arxiv-1800-243 | Multi-Sensor Fusion via Reduction of Dimensionality | http://arxiv.org/pdf/1211.2863v1.pdf | author:Alon Schclar category:cs.CV published:2012-11-13 summary:Large high-dimensional datasets are becoming more and more popular in anincreasing number of research areas. Processing the high dimensional dataincurs a high computational cost and is inherently inefficient since many ofthe values that describe a data object are redundant due to noise and innercorrelations. Consequently, the dimensionality, i.e. the number of values thatare used to describe a data object, needs to be reduced prior to any otherprocessing of the data. The dimensionality reduction removes, in most cases,noise from the data and reduces substantially the computational cost ofalgorithms that are applied to the data. In this thesis, a novel coherent integrated methodology is introduced(theory, algorithm and applications) to reduce the dimensionality ofhigh-dimensional datasets. The method constructs a diffusion process among thedata coordinates via a random walk. The dimensionality reduction is obtainedbased on the eigen-decomposition of the Markov matrix that is associated withthe random walk. The proposed method is utilized for: (a) segmentation anddetection of anomalies in hyper-spectral images; (b) segmentation ofmulti-contrast MRI images; and (c) segmentation of video sequences. We also present algorithms for: (a) the characterization of materials usingtheir spectral signatures to enable their identification; (b) detection ofvehicles according to their acoustic signatures; and (c) classification ofvascular vessels recordings to detect hyper-tension and cardio-vasculardiseases. The proposed methodology and algorithms produce excellent results thatsuccessfully compete with current state-of-the-art algorithms.
arxiv-1800-244 | Sparse Distributed Learning Based on Diffusion Adaptation | http://arxiv.org/pdf/1206.3099v2.pdf | author:Paolo Di Lorenzo, Ali H. Sayed category:cs.LG cs.DC published:2012-06-14 summary:This article proposes diffusion LMS strategies for distributed estimationover adaptive networks that are able to exploit sparsity in the underlyingsystem model. The approach relies on convex regularization, common incompressive sensing, to enhance the detection of sparsity via a diffusiveprocess over the network. The resulting algorithms endow networks with learningabilities and allow them to learn the sparse structure from the incoming datain real-time, and also to track variations in the sparsity of the model. Weprovide convergence and mean-square performance analysis of the proposed methodand show under what conditions it outperforms the unregularized diffusionversion. We also show how to adaptively select the regularization parameter.Simulation results illustrate the advantage of the proposed filters for sparsedata recovery.
arxiv-1800-245 | Sketch Recognition using Domain Classification | http://arxiv.org/pdf/1211.2742v1.pdf | author:Vasudha Vashisht, Tanupriya Choudhury, T. V. Prasad category:cs.CV cs.HC published:2012-11-12 summary:Conceptualizing away the sketch processing details in a user interface willenable general users and domain experts to create more complex sketches. Thereare many domains for which sketch recognition systems are being developed. Butthey entail image-processing skill if they are to handle the details of eachdomain, and also they are lengthy to build. The implemented system goal is toenable user interface designers and domain experts who may not have proficiencyin sketch recognition to be able to construct these sketch systems. This sketchrecognition system takes in rough sketches from user drawn with the help ofmouse as its input. It then recognizes the sketch using segmentation and domainclassification, the properties of the user drawn sketch and segments aresearched heuristically in the domains and each figures of each domain, andfinally it shows its domain, the figure name and properties. It also draws thesketch smoothly. The work is resulted through extensive research and study ofmany existing image processing and pattern matching algorithms.
arxiv-1800-246 | A Hindi Speech Actuated Computer Interface for Web Search | http://arxiv.org/pdf/1211.2741v1.pdf | author:Kamlesh Sharma, S. V. A. V. Prasad, T. V. Prasad category:cs.CL cs.HC cs.IR published:2012-11-12 summary:Aiming at increasing system simplicity and flexibility, an audio evoked basedsystem was developed by integrating simplified headphone and user-friendlysoftware design. This paper describes a Hindi Speech Actuated ComputerInterface for Web search (HSACIWS), which accepts spoken queries in Hindilanguage and provides the search result on the screen. This system recognizesspoken queries by large vocabulary continuous speech recognition (LVCSR),retrieves relevant document by text retrieval, and provides the search resulton the Web by the integration of the Web and the voice systems. The LVCSR inthis system showed enough performance levels for speech with acoustic andlanguage models derived from a query corpus with target contents.
arxiv-1800-247 | Proximal Stochastic Dual Coordinate Ascent | http://arxiv.org/pdf/1211.2717v1.pdf | author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG math.OC published:2012-11-12 summary:We introduce a proximal version of dual coordinate ascent method. Wedemonstrate how the derived algorithmic framework can be used for numerousregularized loss minimization problems, including $\ell_1$ regularization andstructured output SVM. The convergence rates we obtain match, and sometimesimprove, state-of-the-art results.
arxiv-1800-248 | A Riemannian geometry for low-rank matrix completion | http://arxiv.org/pdf/1211.1550v2.pdf | author:B. Mishra, K. Adithya Apuroop, R. Sepulchre category:cs.LG cs.NA math.OC published:2012-11-07 summary:We propose a new Riemannian geometry for fixed-rank matrices that isspecifically tailored to the low-rank matrix completion problem. Exploiting thedegree of freedom of a quotient space, we tune the metric on our search spaceto the particular least square cost function. At one level, it illustrates in anovel way how to exploit the versatile framework of optimization on quotientmanifold. At another level, our algorithm can be considered as an improvedversion of LMaFit, the state-of-the-art Gauss-Seidel algorithm. We developnecessary tools needed to perform both first-order and second-orderoptimization. In particular, we propose gradient descent schemes (steepestdescent and conjugate gradient) and trust-region algorithms. We also show that,thanks to the simplicity of the cost function, it is numerically cheap toperform an exact linesearch given a search direction, which makes ouralgorithms competitive with the state-of-the-art on standard low-rank matrixcompletion instances.
arxiv-1800-249 | A Non-Blind Watermarking Scheme for Gray Scale Images in Discrete Wavelet Transform Domain using Two Subbands | http://arxiv.org/pdf/1211.2699v1.pdf | author:Abdur Shahid, Shahriar Badsha, Md. Rethwan Kabeer, Junaid Ahsan, Mufti Mahmud category:cs.MM cs.CV published:2012-11-12 summary:Digital watermarking is the process to hide digital pattern directly into adigital content. Digital watermarking techniques are used to address digitalrights management, protect information and conceal secrets. An invisiblenon-blind watermarking approach for gray scale images is proposed in thispaper. The host image is decomposed into 3-levels using Discrete WaveletTransform. Based on the parent-child relationship between the waveletcoefficients the Set Partitioning in Hierarchical Trees (SPIHT) compressionalgorithm is performed on the LH3, LH2, HL3 and HL2 subbands to find out thesignificant coefficients. The most significant coefficients of LH2 and HL2bands are selected to embed a binary watermark image. The selected significantcoefficients are modulated using Noise Visibility Function, which is consideredas the best strength to ensure better imperceptibility. The approach is testedagainst various image processing attacks such as addition of noise, filtering,cropping, JPEG compression, histogram equalization and contrast adjustment. Theexperimental results reveal the high effectiveness of the method.
arxiv-1800-250 | The Variational Garrote | http://arxiv.org/pdf/1109.0486v3.pdf | author:Hilbert J. Kappen, Vicenç Gómez category:stat.ME cs.LG published:2011-09-02 summary:In this paper, we present a new variational method for sparse regressionusing $L_0$ regularization. The variational parameters appear in theapproximate model in a way that is similar to Breiman's Garrote model. We referto this method as the variational Garrote (VG). We show that the combination ofthe variational approximation and $L_0$ regularization has the effect of makingthe problem effectively of maximal rank even when the number of samples issmall compared to the number of variables. The VG is compared numerically withthe Lasso method, ridge regression and the recently introduced paired meanfield method (PMF) (M. Titsias & M. L\'azaro-Gredilla., NIPS 2012). Numericalresults show that the VG and PMF yield more accurate predictions and moreaccurately reconstruct the true model than the other methods. It is shown thatthe VG finds correct solutions when the Lasso solution is inconsistent due tolarge input correlations. Globally, VG is significantly faster than PMF andtends to perform better as the problems become denser and in problems withstrongly correlated inputs. The naive implementation of the VG scales cubicwith the number of features. By introducing Lagrange multipliers we obtain adual formulation of the problem that scales cubic in the number of samples, butclose to linear in the number of features.
arxiv-1800-251 | A Comparative Study of Gaussian Mixture Model and Radial Basis Function for Voice Recognition | http://arxiv.org/pdf/1211.2556v1.pdf | author:Fatai Adesina Anifowose category:cs.LG cs.CV stat.ML published:2012-11-12 summary:A comparative study of the application of Gaussian Mixture Model (GMM) andRadial Basis Function (RBF) in biometric recognition of voice has been carriedout and presented. The application of machine learning techniques to biometricauthentication and recognition problems has gained a widespread acceptance. Inthis research, a GMM model was trained, using Expectation Maximization (EM)algorithm, on a dataset containing 10 classes of vowels and the model was usedto predict the appropriate classes using a validation dataset. For experimentalvalidity, the model was compared to the performance of two different versionsof RBF model using the same learning and validation datasets. The resultsshowed very close recognition accuracy between the GMM and the standard RBFmodel, but with GMM performing better than the standard RBF by less than 1% andthe two models outperformed similar models reported in literature. The DTREGversion of RBF outperformed the other two models by producing 94.8% recognitionaccuracy. In terms of recognition time, the standard RBF was found to be thefastest among the three models.
arxiv-1800-252 | New Edge Detection Technique based on the Shannon Entropy in Gray Level Images | http://arxiv.org/pdf/1211.2502v1.pdf | author:Mohamed A. El-Sayed, Tarek Abd-El Hafeez category:cs.CV published:2012-11-12 summary:Edge detection is an important field in image processing. Edges characterizeobject boundaries and are therefore useful for segmentation, registration,feature extraction, and identification of objects in a scene. In this paper, anapproach utilizing an improvement of Baljit and Amar method which uses Shannonentropy other than the evaluation of derivatives of the image in detectingedges in gray level images has been proposed. The proposed method can reducethe CPU time required for the edge detection process and the quality of theedge detector of the output images is robust. A standard test images, thereal-world and synthetic images are used to compare the results of the proposededge detector with the Baljit and Amar edge detector method. In order tovalidate the results, the run time of the proposed method and the perviousmethod are presented. It has been observed that the proposed edge detectorworks effectively for different gray scale digital images. The performanceevaluation of the proposed technique in terms of the measured CPU time and thequality of edge detector method are presented. Experimental results demonstratethat the proposed method achieve better result than the relevant classicmethod.
arxiv-1800-253 | A New Algorithm Based Entropic Threshold for Edge Detection in Images | http://arxiv.org/pdf/1211.2500v1.pdf | author:Mohamed A. El-Sayed category:cs.CV published:2012-11-12 summary:Edge detection is one of the most critical tasks in automatic image analysis.There exists no universal edge detection method which works well under allconditions. This paper shows the new approach based on the one of the mostefficient techniques for edge detection, which is entropy-based thresholding.The main advantages of the proposed method are its robustness and itsflexibility. We present experimental results for this method, and compareresults of the algorithm against several leading edge detection methods, suchas Canny, LOG, and Sobel. Experimental results demonstrate that the proposedmethod achieves better result than some classic methods and the quality of theedge detector of the output images is robust and decrease the computation time.
arxiv-1800-254 | Provable ICA with Unknown Gaussian Noise, and Implications for Gaussian Mixtures and Autoencoders | http://arxiv.org/pdf/1206.5349v2.pdf | author:Sanjeev Arora, Rong Ge, Ankur Moitra, Sushant Sachdeva category:cs.LG cs.DS published:2012-06-23 summary:We present a new algorithm for Independent Component Analysis (ICA) which hasprovable performance guarantees. In particular, suppose we are given samples ofthe form $y = Ax + \eta$ where $A$ is an unknown $n \times n$ matrix and $x$ isa random variable whose components are independent and have a fourth momentstrictly less than that of a standard Gaussian random variable and $\eta$ is an$n$-dimensional Gaussian random variable with unknown covariance $\Sigma$: Wegive an algorithm that provable recovers $A$ and $\Sigma$ up to an additive$\epsilon$ and whose running time and sample complexity are polynomial in $n$and $1 / \epsilon$. To accomplish this, we introduce a novel "quasi-whitening"step that may be useful in other contexts in which the covariance of Gaussiannoise is not known in advance. We also give a general framework for finding alllocal optima of a function (given an oracle for approximately finding just one)and this is a crucial step in our algorithm, one that has been overlooked inprevious attempts, and allows us to control the accumulation of error when wefind the columns of $A$ one by one via local search.
arxiv-1800-255 | Random Utility Theory for Social Choice | http://arxiv.org/pdf/1211.2476v1.pdf | author:Hossein Azari Soufiani, David C. Parkes, Lirong Xia category:cs.MA cs.LG stat.ML published:2012-11-11 summary:Random utility theory models an agent's preferences on alternatives bydrawing a real-valued score on each alternative (typically independently) froma parameterized distribution, and then ranking the alternatives according toscores. A special case that has received significant attention is thePlackett-Luce model, for which fast inference methods for maximum likelihoodestimators are available. This paper develops conditions on general randomutility models that enable fast inference within a Bayesian framework throughMC-EM, providing concave loglikelihood functions and bounded sets of globalmaxima solutions. Results on both real-world and simulated data provide supportfor the scalability of the approach and capability for model selection amonggeneral random utility models including Plackett-Luce.
arxiv-1800-256 | A Lightweight Stemmer for Gujarati | http://arxiv.org/pdf/1210.5486v2.pdf | author:Juhi Ameta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2012-10-19 summary:Gujarati is a resource poor language with almost no language processing toolsbeing available. In this paper we have shown an implementation of a rule basedstemmer of Gujarati. We have shown the creation of rules for stemming and therichness in morphology that Gujarati possesses. We have also evaluated ourresults by verifying it with a human expert.
arxiv-1800-257 | Hybrid methodology for hourly global radiation forecasting in Mediterranean area | http://arxiv.org/pdf/1211.2378v1.pdf | author:Cyril Voyant, Marc Muselli, Christophe Paoli, Marie Laure Nivet category:cs.NE cs.LG physics.ao-ph stat.AP published:2012-11-11 summary:The renewable energies prediction and particularly global radiationforecasting is a challenge studied by a growing number of research teams. Thispaper proposes an original technique to model the insolation time series basedon combining Artificial Neural Network (ANN) and Auto-Regressive and MovingAverage (ARMA) model. While ANN by its non-linear nature is effective topredict cloudy days, ARMA techniques are more dedicated to sunny days withoutcloud occurrences. Thus, three hybrids models are suggested: the first proposessimply to use ARMA for 6 months in spring and summer and to use an optimizedANN for the other part of the year; the second model is equivalent to the firstbut with a seasonal learning; the last model depends on the error occurred theprevious hour. These models were used to forecast the hourly global radiationfor five places in Mediterranean area. The forecasting performance was comparedamong several models: the 3 above mentioned models, the best ANN and ARMA foreach location. In the best configuration, the coupling of ANN and ARMA allowsan improvement of more than 1%, with a maximum in autumn (3.4%) and a minimumin winter (0.9%) where ANN alone is the best.
arxiv-1800-258 | A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function | http://arxiv.org/pdf/1206.1898v2.pdf | author:Pedro A. Ortega, Jordi Grau-Moya, Tim Genewein, David Balduzzi, Daniel A. Braun category:stat.ML cs.AI math.ST stat.TH published:2012-06-09 summary:We propose a novel Bayesian approach to solve stochastic optimizationproblems that involve finding extrema of noisy, nonlinear functions. Previouswork has focused on representing possible functions explicitly, which leads toa two-step procedure of first, doing inference over the function space andsecond, finding the extrema of these functions. Here we skip the representationstep and directly model the distribution over extrema. To this end, we devise anon-parametric conjugate prior based on a kernel regressor. The resultingposterior distribution directly captures the uncertainty over the maximum ofthe unknown function. We illustrate the effectiveness of our model byoptimizing a noisy, high-dimensional, non-convex objective function.
arxiv-1800-259 | Probabilistic Combination of Classifier and Cluster Ensembles for Non-transductive Learning | http://arxiv.org/pdf/1211.2304v1.pdf | author:Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh, Badrul Sarwar, Jean-David Ruvini category:cs.LG stat.ML published:2012-11-10 summary:Unsupervised models can provide supplementary soft constraints to helpclassify new target data under the assumption that similar objects in thetarget set are more likely to share the same class label. Such models can alsohelp detect possible differences between training and target distributions,which is useful in applications where concept drift may take place. This paperdescribes a Bayesian framework that takes as input class labels from existingclassifiers (designed based on labeled data from the source domain), as well ascluster labels from a cluster ensemble operating solely on the target data tobe classified, and yields a consensus labeling of the target data. Thisframework is particularly useful when the statistics of the target data driftor change from those of the training data. We also show that the proposedframework is privacy-aware and allows performing distributed learning whendata/models have sharing restrictions. Experiments show that our framework canyield superior results to those provided by applying classifier ensembles only.
arxiv-1800-260 | Dating Texts without Explicit Temporal Cues | http://arxiv.org/pdf/1211.2290v1.pdf | author:Abhimanu Kumar, Jason Baldridge, Matthew Lease, Joydeep Ghosh category:cs.CL cs.AI published:2012-11-10 summary:This paper tackles temporal resolution of documents, such as determining whena document is about or when it was written, based only on its text. We applytechniques from information retrieval that predict dates via language modelsover a discretized timeline. Unlike most previous works, we rely {\it solely}on temporal cues implicit in the text. We consider both document-likelihood anddivergence based techniques and several smoothing methods for both of them. Ourbest model predicts the mid-point of individuals' lives with a median of 22 andmean error of 36 years for Wikipedia biographies from 3800 B.C. to the presentday. We also show that this approach works well when training on suchbiographies and predicting dates both for non-biographical Wikipedia pagesabout specific years (500 B.C. to 2010 A.D.) and for publication dates of shortstories (1798 to 2008). Together, our work shows that, even in absence oftemporal extraction resources, it is possible to achieve remarkable temporallocality across a diverse set of texts.
arxiv-1800-261 | Calibrated Elastic Regularization in Matrix Completion | http://arxiv.org/pdf/1211.2264v1.pdf | author:Tingni Sun, Cun-Hui Zhang category:math.ST stat.ML stat.TH published:2012-11-09 summary:This paper concerns the problem of matrix completion, which is to estimate amatrix from observations in a small subset of indices. We propose a calibratedspectrum elastic net method with a sum of the nuclear and Frobenius penaltiesand develop an iterative algorithm to solve the convex minimization problem.The iterative algorithm alternates between imputing the missing entries in theincomplete matrix by the current guess and estimating the matrix by a scaledsoft-thresholding singular value decomposition of the imputed matrix until theresulting matrix converges. A calibration step follows to correct the biascaused by the Frobenius penalty. Under proper coherence conditions and forsuitable penalties levels, we prove that the proposed estimator achieves anerror bound of nearly optimal order and in proportion to the noise level. Thisprovides a unified analysis of the noisy and noiseless matrix completionproblems. Simulation results are presented to compare our proposal withprevious ones.
arxiv-1800-262 | No-Regret Algorithms for Unconstrained Online Convex Optimization | http://arxiv.org/pdf/1211.2260v1.pdf | author:Matthew Streeter, H. Brendan McMahan category:cs.LG published:2012-11-09 summary:Some of the most compelling applications of online convex optimization,including online prediction and classification, are unconstrained: the naturalfeasible set is R^n. Existing algorithms fail to achieve sub-linear regret inthis setting unless constraints on the comparator point x^* are known inadvance. We present algorithms that, without such prior knowledge, offernear-optimal regret bounds with respect to any choice of x^*. In particular,regret with respect to x^* = 0 is constant. We then prove lower bounds showingthat our guarantees are near-optimal in this setting.
arxiv-1800-263 | NF-SAVO: Neuro-Fuzzy system for Arabic Video OCR | http://arxiv.org/pdf/1211.2150v1.pdf | author:Mohamed Ben Halima, Hichem karray, Adel. M. Alimi, Ana Fernández Vila category:cs.CV published:2012-11-09 summary:In this paper we propose a robust approach for text extraction andrecognition from video clips which is called Neuro-Fuzzy system for ArabicVideo OCR. In Arabic video text recognition, a number of noise componentsprovide the text relatively more complicated to separate from the background.Further, the characters can be moving or presented in a diversity of colors,sizes and fonts that are not uniform. Added to this, is the fact that thebackground is usually moving making text extraction a more intricate process.Video include two kinds of text, scene text and artificial text. Scene text isusually text that becomes part of the scene itself as it is recorded at thetime of filming the scene. But artificial text is produced separately and awayfrom the scene and is laid over it at a later stage or during the postprocessing time. The emergence of artificial text is consequently vigilantlydirected. This type of text carries with it important information that helps invideo referencing, indexing and retrieval.
arxiv-1800-264 | Localisation of Numerical Date Field in an Indian Handwritten Document | http://arxiv.org/pdf/1211.2116v1.pdf | author:S Arunkumar, Pallab Kumar Sahu, Sudeep Gorai, Kalyan Ghosh category:cs.CV published:2012-11-09 summary:This paper describes a method to localise all those areas which mayconstitute the date field in an Indian handwritten document. Spatial patternsof the date field are studied from various handwritten documents and analgorithm is developed through statistical analysis to identify those sets ofconnected components which may constitute the date. Common date patternsfollowed in India are considered to classify the date formats in differentclasses. Reported results demonstrate promising performance of the proposedapproach
arxiv-1800-265 | Image denoising with multi-layer perceptrons, part 1: comparison with existing algorithms and with bounds | http://arxiv.org/pdf/1211.1544v3.pdf | author:Harold Christopher Burger, Christian J. Schuler, Stefan Harmeling category:cs.CV cs.LG published:2012-11-07 summary:Image denoising can be described as the problem of mapping from a noisy imageto a noise-free image. The best currently available denoising methodsapproximate this mapping with cleverly engineered algorithms. In this work weattempt to learn this mapping directly with plain multi layer perceptrons (MLP)applied to image patches. We will show that by training on large imagedatabases we are able to outperform the current state-of-the-art imagedenoising methods. In addition, our method achieves results that are superiorto one type of theoretical bound and goes a large way toward closing the gapwith a second type of theoretical bound. Our approach is easily adapted to lessextensively studied types of noise, such as mixed Poisson-Gaussian noise, JPEGartifacts, salt-and-pepper noise and noise resembling stripes, for which weachieve excellent results as well. We will show that combining a block-matchingprocedure with MLPs can further improve the results on certain images. In asecond paper, we detail the training trade-offs and the inner mechanisms of ourMLPs.
arxiv-1800-266 | An Approach of Improving Students Academic Performance by using k means clustering algorithm and Decision tree | http://arxiv.org/pdf/1211.6340v1.pdf | author:Md. Hedayetul Islam Shovon, Mahfuza Haque category:cs.LG published:2012-11-09 summary:Improving students academic performance is not an easy task for the academiccommunity of higher learning. The academic performance of engineering andscience students during their first year at university is a turning point intheir educational path and usually encroaches on their General PointAverage,GPA in a decisive manner. The students evaluation factors like classquizzes mid and final exam assignment lab work are studied. It is recommendedthat all these correlated information should be conveyed to the class teacherbefore the conduction of final exam. This study will help the teachers toreduce the drop out ratio to a significant level and improve the performance ofstudents. In this paper, we present a hybrid procedure based on Decision Treeof Data mining method and Data Clustering that enables academicians to predictstudents GPA and based on that instructor can take necessary step to improvestudent academic performance.
arxiv-1800-267 | 3D Surface Reconstruction of Underwater Objects | http://arxiv.org/pdf/1211.2082v1.pdf | author:C. J. Prabhakar, P. U. Praveen Kumar category:cs.CV published:2012-11-09 summary:In this paper, we propose a novel technique to reconstruct 3D surface of anunderwater object using stereo images. Reconstructing the 3D surface of anunderwater object is really a challenging task due to degraded quality ofunderwater images. There are various reason of quality degradation ofunderwater images i.e., non-uniform illumination of light on the surface ofobjects, scattering and absorption effects. Floating particles present inunderwater produces Gaussian noise on the captured underwater images whichdegrades the quality of images. The degraded underwater images are preprocessedby applying homomorphic, wavelet denoising and anisotropic filteringsequentially. The uncalibrated rectification technique is applied topreprocessed images to rectify the left and right images. The rectified leftand right image lies on a common plane. To find the correspondence points in aleft and right images, we have applied dense stereo matching technique i.e.,graph cut method. Finally, we estimate the depth of images using triangulationtechnique. The experimental result shows that the proposed method reconstruct3D surface of underwater objects accurately using captured underwater stereoimages.
arxiv-1800-268 | LAGE: A Java Framework to reconstruct Gene Regulatory Networks from Large-Scale Continues Expression Data | http://arxiv.org/pdf/1211.2073v1.pdf | author:Yang Lu, Mengying Wang, Kenny Q. Zhu, Bo Yuan category:cs.LG cs.CE q-bio.QM stat.ML published:2012-11-09 summary:LAGE is a systematic framework developed in Java. The motivation of LAGE isto provide a scalable and parallel solution to reconstruct Gene RegulatoryNetworks (GRNs) from continuous gene expression data for very large amount ofgenes. The basic idea of our framework is motivated by the philosophy ofdivideand-conquer. Specifically, LAGE recursively partitions genes intomultiple overlapping communities with much smaller sizes, learnsintra-community GRNs respectively before merge them altogether. Besides, thecomplete information of overlapping communities serves as the byproduct, whichcould be used to mine meaningful functional modules in biological networks.
arxiv-1800-269 | Time Complexity Analysis of Binary Space Partitioning Scheme for Image Compression | http://arxiv.org/pdf/1211.2037v1.pdf | author:Rehna V. J., M. K. Jeyakumar category:cs.CV published:2012-11-09 summary:Segmentation-based image coding methods provide high compression ratios whencompared with traditional image coding approaches like the transform and subband coding for low bit-rate compression applications. In this paper, asegmentation-based image coding method, namely the Binary Space Partitionscheme, that divides the desired image using a recursive procedure for codingis presented. The BSP approach partitions the desired image recursively byusing bisecting lines, selected from a collection of discrete optional lines,in a hierarchical manner. This partitioning procedure generates a binary tree,which is referred to as the BSP-tree representation of the desired image. Thealgorithm is extremely complex in computation and has high execution time. Thetime complexity of the BSP scheme is explored in this work.
arxiv-1800-270 | Multi-input Multi-output Beta Wavelet Network: Modeling of Acoustic Units for Speech Recognition | http://arxiv.org/pdf/1211.2007v1.pdf | author:Ridha Ejbali, Mourad Zaied, Chokri Ben Amar category:cs.CV published:2012-11-08 summary:In this paper, we propose a novel architecture of wavelet network calledMulti-input Multi-output Wavelet Network MIMOWN as a generalization of the oldarchitecture of wavelet network. This newel prototype was applied to speechrecognition application especially to model acoustic unit of speech. Theoriginality of our work is the proposal of MIMOWN to model acoustic unit ofspeech. This approach was proposed to overcome limitation of old waveletnetwork model. The use of the multi-input multi-output architecture will allowstraining wavelet network on various examples of acoustic units.
arxiv-1800-271 | Explosion prediction of oil gas using SVM and Logistic Regression | http://arxiv.org/pdf/1211.1526v2.pdf | author:Xiaofei Wang, Mingming Zhang, Liyong Shen, Suixiang Gao category:cs.CE cs.LG 62P30, 68T05 published:2012-11-07 summary:The prevention of dangerous chemical accidents is a primary problem ofindustrial manufacturing. In the accidents of dangerous chemicals, the oil gasexplosion plays an important role. The essential task of the explosionprevention is to estimate the better explosion limit of a given oil gas. Inthis paper, Support Vector Machines (SVM) and Logistic Regression (LR) are usedto predict the explosion of oil gas. LR can get the explicit probabilityformula of explosion, and the explosive range of the concentrations of oil gasaccording to the concentration of oxygen. Meanwhile, SVM gives higher accuracyof prediction. Furthermore, considering the practical requirements, the effectsof penalty parameter on the distribution of two types of errors are discussed.
arxiv-1800-272 | A Comparative study of Arabic handwritten characters invariant feature | http://arxiv.org/pdf/1211.1800v1.pdf | author:Hamdi Hassen, Maher khemakhem category:cs.CV published:2012-11-08 summary:This paper is practically interested in the unchangeable feature of Arabichandwritten character. It presents results of comparative study achieved oncertain features extraction techniques of handwritten character, based on Houghtransform, Fourier transform, Wavelet transform and Gabor Filter. Obtainedresults show that Hough Transform and Gabor filter are insensible to therotation and translation, Fourier Transform is sensible to the rotation butinsensible to the translation, in contrast to Hough Transform and Gabor filter,Wavelets Transform is sensitive to the rotation as well as to the translation.
arxiv-1800-273 | Algorithm for Missing Values Imputation in Categorical Data with Use of Association Rules | http://arxiv.org/pdf/1211.1799v1.pdf | author:Jiří Kaiser category:cs.LG published:2012-11-08 summary:This paper presents algorithm for missing values imputation in categoricaldata. The algorithm is based on using association rules and is presented inthree variants. Experimental shows better accuracy of missing values imputationusing the algorithm then using most common attribute value.
arxiv-1800-274 | Color Constancy based on Image Similarity via Bilayer Sparse Coding | http://arxiv.org/pdf/1207.3142v2.pdf | author:Bing Li, Weihua Xiong, Weiming Hu category:cs.CV published:2012-07-13 summary:Computational color constancy is a very important topic in computer visionand has attracted many researchers' attention. Recently, lots of research hasshown the effects of high level visual content information for illuminationestimation. However, all of these existing methods are essentiallycombinational strategies in which image's content analysis is only used toguide the combination or selection from a variety of individual illuminationestimation methods. In this paper, we propose a novel bilayer sparse codingmodel for illumination estimation that considers image similarity in terms ofboth low level color distribution and high level image scene contentsimultaneously. For the purpose, the image's scene content information isintegrated with its color distribution to obtain optimal illuminationestimation model. The experimental results on two real-world image sets showthat our algorithm is superior to other prevailing illumination estimationmethods, even better than combinational methods.
arxiv-1800-275 | 3D Scene Grammar for Parsing RGB-D Pointclouds | http://arxiv.org/pdf/1211.1752v1.pdf | author:Abhishek Anand, Sherwin Li category:cs.CV published:2012-11-08 summary:We pose 3D scene-understanding as a problem of parsing in a grammar. Agrammar helps us capture the compositional structure of real-word objects,e.g., a chair is composed of a seat, a back-rest and some legs. Having multiplerules for an object helps us capture structural variations in objects, e.g., achair can optionally also have arm-rests. Finally, having rules to capturecomposition at different levels helps us formulate the entire scene-processingpipeline as a single problem of finding most likely parse-tree---small segmentscombine to form parts of objects, parts to objects and objects to a scene. Weattach a generative probability model to our grammar by having afeature-dependent probability function for every rule. We evaluated it byextracting labels for every segment and comparing the results with thestate-of-the-art segment-labeling algorithm. Our algorithm was outperformed bythe state-or-the-art method. But, Our model can be trained very efficiently(within seconds), and it scales only linearly in with the number of rules inthe grammar. Also, we think that this is an important problem for the 3D visioncommunity. So, we are releasing our dataset and related code.
arxiv-1800-276 | Computer vision tools for the non-invasive assessment of autism-related behavioral markers | http://arxiv.org/pdf/1210.7014v2.pdf | author:Jordan Hashemi, Thiago Vallin Spina, Mariano Tepper, Amy Esler, Vassilios Morellas, Nikolaos Papanikolopoulos, Guillermo Sapiro category:cs.CV published:2012-10-25 summary:The early detection of developmental disorders is key to child outcome,allowing interventions to be initiated that promote development and improveprognosis. Research on autism spectrum disorder (ASD) suggests behavioralmarkers can be observed late in the first year of life. Many of these studiesinvolved extensive frame-by-frame video observation and analysis of a child'snatural behavior. Although non-intrusive, these methods are extremelytime-intensive and require a high level of observer training; thus, they areimpractical for clinical and large population research purposes. Diagnosticmeasures for ASD are available for infants but are only accurate when used byspecialists experienced in early diagnosis. This work is a first milestone in along-term multidisciplinary project that aims at helping clinicians and generalpractitioners accomplish this early detection/measurement task automatically.We focus on providing computer vision tools to measure and identify ASDbehavioral markers based on components of the Autism Observation Scale forInfants (AOSI). In particular, we develop algorithms to measure three criticalAOSI activities that assess visual attention. We augment these AOSI activitieswith an additional test that analyzes asymmetrical patterns in unsupportedgait. The first set of algorithms involves assessing head motion by trackingfacial features, while the gait analysis relies on joint foregroundsegmentation and 2D body pose estimation in video. We show results that provideinsightful knowledge to augment the clinician's behavioral observationsobtained from real in-clinic assessments.
arxiv-1800-277 | Linear Antenna Array with Suppressed Sidelobe and Sideband Levels using Time Modulation | http://arxiv.org/pdf/1211.1733v1.pdf | author:Swaprava Nath, Subrata Mitra category:cs.NE published:2012-11-08 summary:In this paper, the goal is to achieve an ultra low sidelobe level (SLL) andsideband levels (SBL) of a time modulated linear antenna array. The approachfollowed here is not to give fixed level of excitation to the elements of anarray, but to change it dynamically with time. The excitation levels of thedifferent array elements over time are varied to get the low sidelobe andsideband levels. The mathematics of getting the SLL and SBL furnished in detailand simulation is done using the mathematical results. The excitation patternover time is optimized using Genetic Algorithm (GA). Since, the amplitudes ofthe excitations of the elements are varied within a finite limit, results showit gives better sidelobe and sideband suppression compared to previous timemodulated arrays with uniform amplitude excitations.
arxiv-1800-278 | Inverse problems in approximate uniform generation | http://arxiv.org/pdf/1211.1722v1.pdf | author:Anindya De, Ilias Diakonikolas, Rocco A. Servedio category:cs.CC cs.DS cs.LG published:2012-11-07 summary:We initiate the study of \emph{inverse} problems in approximate uniformgeneration, focusing on uniform generation of satisfying assignments of varioustypes of Boolean functions. In such an inverse problem, the algorithm is givenuniform random satisfying assignments of an unknown function $f$ belonging to aclass $\C$ of Boolean functions, and the goal is to output a probabilitydistribution $D$ which is $\epsilon$-close, in total variation distance, to theuniform distribution over $f^{-1}(1)$. Positive results: We prove a general positive result establishing sufficientconditions for efficient inverse approximate uniform generation for a class$\C$. We define a new type of algorithm called a \emph{densifier} for $\C$, andshow (roughly speaking) how to combine (i) a densifier, (ii) an approximatecounting / uniform generation algorithm, and (iii) a Statistical Query learningalgorithm, to obtain an inverse approximate uniform generation algorithm. Weapply this general result to obtain a poly$(n,1/\eps)$-time algorithm for theclass of halfspaces; and a quasipoly$(n,1/\eps)$-time algorithm for the classof $\poly(n)$-size DNF formulas. Negative results: We prove a general negative result establishing that theexistence of certain types of signature schemes in cryptography implies thehardness of certain inverse approximate uniform generation problems. Thisimplies that there are no {subexponential}-time inverse approximate uniformgeneration algorithms for 3-CNF formulas; for intersections of two halfspaces;for degree-2 polynomial threshold functions; and for monotone 2-CNF formulas. Finally, we show that there is no general relationship between the complexityof the "forward" approximate uniform generation problem and the complexity ofthe inverse problem for a class $\C$ -- it is possible for either one to beeasy while the other is hard.
arxiv-1800-279 | Learning Monocular Reactive UAV Control in Cluttered Natural Environments | http://arxiv.org/pdf/1211.1690v1.pdf | author:Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey, J. Andrew Bagnell, Martial Hebert category:cs.RO cs.CV cs.LG cs.SY published:2012-11-07 summary:Autonomous navigation for large Unmanned Aerial Vehicles (UAVs) is fairlystraight-forward, as expensive sensors and monitoring devices can be employed.In contrast, obstacle avoidance remains a challenging task for Micro AerialVehicles (MAVs) which operate at low altitude in cluttered environments. Unlikelarge vehicles, MAVs can only carry very light sensors, such as cameras, makingautonomous navigation through obstacles much more challenging. In this paper,we describe a system that navigates a small quadrotor helicopter autonomouslyat low altitude through natural forest environments. Using only a single cheapcamera to perceive the environment, we are able to maintain a constant velocityof up to 1.5m/s. Given a small set of human pilot demonstrations, we use recentstate-of-the-art imitation learning techniques to train a controller that canavoid trees by adapting the MAVs heading. We demonstrate the performance of oursystem in a more controlled environment indoors, and in real natural forestenvironments outdoors.
arxiv-1800-280 | Discrete Energy Minimization, beyond Submodularity: Applications and Approximations | http://arxiv.org/pdf/1210.7362v2.pdf | author:Shai Bagon category:cs.CV cs.LG math.OC stat.ML published:2012-10-27 summary:In this thesis I explore challenging discrete energy minimization problemsthat arise mainly in the context of computer vision tasks. This work motivatesthe use of such "hard-to-optimize" non-submodular functionals, and proposesmethods and algorithms to cope with the NP-hardness of their optimization.Consequently, this thesis revolves around two axes: applications andapproximations. The applications axis motivates the use of such"hard-to-optimize" energies by introducing new tasks. As the energies becomeless constrained and structured one gains more expressive power for theobjective function achieving more accurate models. Results show howchallenging, hard-to-optimize, energies are more adequate for certain computervision applications. To overcome the resulting challenging optimization tasksthe second axis of this thesis proposes approximation algorithms to cope withthe NP-hardness of the optimization. Experiments show that these new methodsyield good results for representative challenging problems.
arxiv-1800-281 | James-Stein Type Center Pixel Weights for Non-Local Means Image Denoising | http://arxiv.org/pdf/1211.1656v1.pdf | author:Yue Wu, Brian Tracey, Joseph P. Noonan category:cs.CV published:2012-11-07 summary:Non-Local Means (NLM) and variants have been proven to be effective androbust in many image denoising tasks. In this letter, we study the parameterselection problem of center pixel weights (CPW) in NLM. Our key contributionsare: 1) we give a novel formulation of the CPW problem from the statisticalshrinkage perspective; 2) we introduce the James-Stein type CPWs for NLM; and3) we propose a new adaptive CPW that is locally tuned for each image pixel.Our experimental results showed that compared to existing CPW solutions, thenew proposed CPWs are more robust and effective under various noise levels. Inparticular, the NLM with the James-Stein type CPWs attain higher means withsmaller variances in terms of the peak signal and noise ratio, implying theyimprove the NLM robustness and make it less sensitive to parameter selection.
arxiv-1800-282 | A New Randomness Evaluation Method with Applications to Image Shuffling and Encryption | http://arxiv.org/pdf/1211.1654v1.pdf | author:Yue Wu, Sos Agaian, Joseph P. Noonan category:cs.CR cs.CV stat.AP published:2012-11-07 summary:This letter discusses the problem of testing the degree of randomness withinan image, particularly for a shuffled or encrypted image. Its key contributionsare: 1) a mathematical model of perfectly shuffled images; 2) the derivation ofthe theoretical distribution of pixel differences; 3) a new $Z$-test basedapproach to differentiate whether or not a test image is perfectly shuffled;and 4) a randomized algorithm to unbiasedly evaluate the degree of randomnesswithin a given image. Simulation results show that the proposed method isrobust and effective in evaluating the degree of randomness within an image,and may often be more suitable for image applications than commonly usedtesting schemes designed for binary data like NIST 800-22. The developed methodmay be also useful as a first step in determining whether or not a shuffling orencryption scheme is suitable for a particular cryptographic application.
arxiv-1800-283 | Different Operating Systems Compatible for Image Prepress Process in Color Management: Analysis and Performance Testing | http://arxiv.org/pdf/1211.1650v1.pdf | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-11-07 summary:Image computing has become a real catchphrase over the past few years and theinterpretations of the meaning of the term vary greatly. The Imagecomputingmarket is currently rapidly evolving with high growth prospects and almostdaily announcements of new devices and application platforms, which results inan increasing diversification of devices, operating system and developmentplatforms. Compared to more traditional information technology markets like theone of desktop computing, mobile computing is much less consolidated andneither standards nor even industry standards have yet been established. Thereare various platforms and interfaces which may be used to perform the desiredtasks through the device. We have tried to compare the various mobile operatingsystems and their trade-offs.
arxiv-1800-284 | Image denoising with multi-layer perceptrons, part 2: training trade-offs and analysis of their mechanisms | http://arxiv.org/pdf/1211.1552v1.pdf | author:Harold Christopher Burger, Christian J. Schuler, Stefan Harmeling category:cs.CV cs.LG published:2012-11-07 summary:Image denoising can be described as the problem of mapping from a noisy imageto a noise-free image. In another paper, we show that multi-layer perceptronscan achieve outstanding image denoising performance for various types of noise(additive white Gaussian noise, mixed Poisson-Gaussian noise, JPEG artifacts,salt-and-pepper noise and noise resembling stripes). In this work we discuss indetail which trade-offs have to be considered during the training procedure. Wewill show how to achieve good results and which pitfalls to avoid. By analysingthe activation patterns of the hidden units we are able to make observationsregarding the functioning principle of multi-layer perceptrons trained forimage denoising.
arxiv-1800-285 | Automatic ECG Beat Arrhythmia Detection | http://arxiv.org/pdf/1209.0167v3.pdf | author:M. Bazarghan, Y. Jaberi, R. Amandi, M. Abedi category:cs.NE published:2012-09-02 summary:Background: In recent years automated data analysis techniques have drawngreat attention and are used in almost every field of research includingbiomedical. Artificial Neural Networks (ANNs) are one of the Computer- Aided-Diagnosis tools which are used extensively by advances in computer hardwaretechnology. The application of these techniques for disease diagnosis has madegreat progress and is widely used by physicians. An Electrocardiogram carriesvital information about heart activity and physicians use this signal forcardiac disease diagnosis which was the great motivation towards our study.Methods: In this study we are using Probabilistic Neural Networks (PNN) as anautomatic technique for ECG signal analysis along with a Genetic Algorithm(GA). As every real signal recorded by the equipment can have differentartifacts, we need to do some preprocessing steps before feeding it to the ANN.Wavelet transform is used for extracting the morphological parameters andmedian filter for data reduction of the ECG signal. The subset of morphologicalparameters are chosen and optimized using GA. We had two approaches in ourinvestigation, the first one uses the whole signal with 289 normalized andde-noised data points as input to the ANN. In the second approach afterapplying all the preprocessing steps the signal is reduced to 29 data pointsand also their important parameters extracted to form the ANN input with 35data points. Results: The outcome of the two approaches for 8 types ofarrhythmia shows that the second approach is superior than the first one withan average accuracy of %99.42.
arxiv-1800-286 | Tangent-based manifold approximation with locally linear models | http://arxiv.org/pdf/1211.1893v1.pdf | author:Sofia Karygianni, Pascal Frossard category:cs.LG cs.CV published:2012-11-06 summary:In this paper, we consider the problem of manifold approximation with affinesubspaces. Our objective is to discover a set of low dimensional affinesubspaces that represents manifold data accurately while preserving themanifold's structure. For this purpose, we employ a greedy technique thatpartitions manifold samples into groups that can be each approximated by a lowdimensional subspace. We start by considering each manifold sample as adifferent group and we use the difference of tangents to determine appropriategroup mergings. We repeat this procedure until we reach the desired number ofsample groups. The best low dimensional affine subspaces corresponding to thefinal groups constitute our approximate manifold representation. Ourexperiments verify the effectiveness of the proposed scheme and show itssuperior performance compared to state-of-the-art methods for manifoldapproximation.
arxiv-1800-287 | Replica theory for learning curves for Gaussian processes on random graphs | http://arxiv.org/pdf/1202.5918v3.pdf | author:Matthew J. Urry, Peter Sollich category:stat.ML published:2012-02-27 summary:Statistical physics approaches can be used to derive accurate predictions forthe performance of inference methods learning from potentially noisy data, asquantified by the learning curve defined as the average error versus number oftraining examples. We analyse a challenging problem in the area ofnon-parametric inference where an effectively infinite number of parameters hasto be learned, specifically Gaussian process regression. When the inputs arevertices on a random graph and the outputs noisy function values, we show thatreplica techniques can be used to obtain exact performance predictions in thelimit of large graphs. The covariance of the Gaussian process prior is definedby a random walk kernel, the discrete analogue of squared exponential kernelson continuous spaces. Conventionally this kernel is normalised only globally,so that the prior variance can differ between vertices; as a more principledalternative we consider local normalisation, where the prior variance isuniform.
arxiv-1800-288 | From Bits to Images: Inversion of Local Binary Descriptors | http://arxiv.org/pdf/1211.1265v1.pdf | author:Emmanuel d'Angelo, Laurent jacques, Alexandre Alahi, Pierre Vandergheynst category:cs.CV cs.IT math.IT published:2012-11-06 summary:Local Binary Descriptors are becoming more and more popular for imagematching tasks, especially when going mobile. While they are extensivelystudied in this context, their ability to carry enough information in order toinfer the original image is seldom addressed. In this work, we leverage an inverse problem approach to show that it ispossible to directly reconstruct the image content from Local BinaryDescriptors. This process relies on very broad assumptions besides theknowledge of the pattern of the descriptor at hand. This generalizes previousresults that required either a prior learning database or non-binarizedfeatures. Furthermore, our reconstruction scheme reveals differences in the waydifferent Local Binary Descriptors capture and encode image information. Hence,the potential applications of our work are multiple, ranging from privacyissues caused by eavesdropping image keypoints streamed by mobile devices tothe design of better descriptors through the visualization and the analysis oftheir geometric content.
arxiv-1800-289 | Handwritten digit recognition by bio-inspired hierarchical networks | http://arxiv.org/pdf/1211.1255v1.pdf | author:Antonio G. Zippo, Giuliana Gelsomino, Sara Nencini, Gabriele E. M. Biella category:cs.LG cs.CV q-bio.NC published:2012-11-06 summary:The human brain processes information showing learning and predictionabilities but the underlying neuronal mechanisms still remain unknown.Recently, many studies prove that neuronal networks are able of bothgeneralizations and associations of sensory inputs. In this paper, following aset of neurophysiological evidences, we propose a learning framework with astrong biological plausibility that mimics prominent functions of corticalcircuitries. We developed the Inductive Conceptual Network (ICN), that is ahierarchical bio-inspired network, able to learn invariant patterns byVariable-order Markov Models implemented in its nodes. The outputs of thetop-most node of ICN hierarchy, representing the highest input generalization,allow for automatic classification of inputs. We found that the ICN clusterizedMNIST images with an error of 5.73% and USPS images with an error of 12.56%.
arxiv-1800-290 | Visual Transfer Learning: Informal Introduction and Literature Overview | http://arxiv.org/pdf/1211.1127v1.pdf | author:Erik Rodner category:cs.CV cs.LG published:2012-11-06 summary:Transfer learning techniques are important to handle small training sets andto allow for quick generalization even from only a few examples. The followingpaper is the introduction as well as the literature overview part of my thesisrelated to the topic of transfer learning for visual recognition problems.
arxiv-1800-291 | A Survey on Techniques of Improving Generalization Ability of Genetic Programming Solutions | http://arxiv.org/pdf/1211.1119v1.pdf | author:Vipul K. Dabhi, Sanjay Chaudhary category:cs.NE published:2012-11-06 summary:In the field of empirical modeling using Genetic Programming (GP), it isimportant to evolve solution with good generalization ability. Generalizationability of GP solutions get affected by two important issues: bloat andover-fitting. We surveyed and classified existing literature related todifferent techniques used by GP research community to deal with these issues.We also point out limitation of these techniques, if any. Moreover, theclassification of different bloat control approaches and measures for bloat andover-fitting are also discussed. We believe that this work will be useful to GPpractitioners in following ways: (i) to better understand concepts ofgeneralization in GP (ii) comparing existing bloat and over-fitting controltechniques and (iii) selecting appropriate approach to improve generalizationability of GP evolved solutions.
arxiv-1800-292 | Soft (Gaussian CDE) regression models and loss functions | http://arxiv.org/pdf/1211.1043v1.pdf | author:Jose Hernandez-Orallo category:cs.LG stat.ML published:2012-11-05 summary:Regression, unlike classification, has lacked a comprehensive and effectiveapproach to deal with cost-sensitive problems by the reuse (and not are-training) of general regression models. In this paper, a wide variety ofcost-sensitive problems in regression (such as bids, asymmetric losses andrejection rules) can be solved effectively by a lightweight but powerfulapproach, consisting of: (1) the conversion of any traditional one-parametercrisp regression model into a two-parameter soft regression model, seen as anormal conditional density estimator, by the use of newly-introduced enrichmentmethods; and (2) the reframing of an enriched soft regression model to newcontexts by an instance-dependent optimisation of the expected loss derivedfrom the conditional normal distribution.
arxiv-1800-293 | A Framework for Evaluating Approximation Methods for Gaussian Process Regression | http://arxiv.org/pdf/1205.6326v2.pdf | author:Krzysztof Chalupka, Christopher K. I. Williams, Iain Murray category:stat.ML cs.LG stat.CO published:2012-05-29 summary:Gaussian process (GP) predictors are an important component of many Bayesianapproaches to machine learning. However, even a straightforward implementationof Gaussian process regression (GPR) requires O(n^2) space and O(n^3) time fora dataset of n examples. Several approximation methods have been proposed, butthere is a lack of understanding of the relative merits of the differentapproximations, and in what situations they are most useful. We recommendassessing the quality of the predictions obtained as a function of the computetime taken, and comparing to standard baselines (e.g., Subset of Data andFITC). We empirically investigate four different approximation algorithms onfour different prediction problems, and make our code available to encouragefuture comparisons.
arxiv-1800-294 | Kernels and Submodels of Deep Belief Networks | http://arxiv.org/pdf/1211.0932v1.pdf | author:Guido F. Montufar, Jason Morton category:stat.ML published:2012-11-05 summary:We study the mixtures of factorizing probability distributions represented asvisible marginal distributions in stochastic layered networks. We take theperspective of kernel transitions of distributions, which gives a unifiedpicture of distributed representations arising from Deep Belief Networks (DBN)and other networks without lateral connections. We describe combinatorial andgeometric properties of the set of kernels and products of kernels realizableby DBNs as the network parameters vary. We describe explicit classes ofprobability distributions, including exponential families, that can be learnedby DBNs. We use these submodels to bound the maximal and the expectedKullback-Leibler approximation errors of DBNs from above depending on thenumber of hidden layers and units that they contain.
arxiv-1800-295 | Sparse recovery with unknown variance: a LASSO-type approach | http://arxiv.org/pdf/1101.0434v5.pdf | author:Stéphane Chrétien, Sébastien Darses category:math.ST stat.ML stat.TH published:2011-01-02 summary:We address the issue of estimating the regression vector $\beta$ in thegeneric $s$-sparse linear model $y = X\beta+z$, with $\beta\in\R^{p}$,$y\in\R^{n}$, $z\sim\mathcal N(0,\sg^2 I)$ and $p> n$ when the variance$\sg^{2}$ is unknown. We study two LASSO-type methods that jointly estimate$\beta$ and the variance. These estimators are minimizers of the $\ell_1$penalized least-squares functional, where the relaxation parameter is tunedaccording to two different strategies. In the first strategy, the relaxationparameter is of the order $\ch{\sigma} \sqrt{\log p}$, where $\ch{\sigma}^2$ isthe empirical variance. %The resulting optimization problem can be solved byrunning only a few successive LASSO instances with %recursive updating of therelaxation parameter. In the second strategy, the relaxation parameter ischosen so as to enforce a trade-off between the fidelity and the penalty termsat optimality. For both estimators, our assumptions are similar to the onesproposed by Cand\`es and Plan in {\it Ann. Stat. (2009)}, for the case where$\sg^{2}$ is known. We prove that our estimators ensure exact recovery of thesupport and sign pattern of $\beta$ with high probability. We presentsimulations results showing that the first estimator enjoys nearly the sameperformances in practice as the standard LASSO (known variance case) for a widerange of the signal to noise ratio. Our second estimator is shown to outperformboth in terms of false detection, when the signal to noise ratio is low.
arxiv-1800-296 | Analysis of Magnification in Depth from Defocus | http://arxiv.org/pdf/1203.6329v2.pdf | author:Arnav Bhavsar category:cs.CV published:2012-03-28 summary:In depth from defocus (DFD), when images are captured with different cameraparameters, a relative magnification is induced between them. Image warping isa simpler solution to account for magnification than seemingly more accurateoptical approaches. This work is an investigation into the effects ofmagnification on the accuracy of DFD. We comment on issues regarding scalingeffect on relative blur computation. We statistically analyze accountability ofscale factor, commenting on the bias and efficiency of the estimator that doesnot consider scale. We also discuss the effect of interpolation errors on blurestimation in a warping based solution to handle magnification and carry outexperimental analysis to comment on the blur estimation accuracy.
arxiv-1800-297 | Comparing K-Nearest Neighbors and Potential Energy Method in classification problem. A case study using KNN applet by E.M. Mirkes and real life benchmark data sets | http://arxiv.org/pdf/1211.0879v1.pdf | author:Yanshan Shi category:stat.ML cs.LG published:2012-11-05 summary:K-nearest neighbors (KNN) method is used in many supervised learningclassification problems. Potential Energy (PE) method is also developed forclassification problems based on its physical metaphor. The energy potentialused in the experiments are Yukawa potential and Gaussian Potential. In thispaper, I use both applet and MATLAB program with real life benchmark data toanalyze the performances of KNN and PE method in classification problems. Theresults show that in general, KNN and PE methods have similar performance. Inparticular, PE with Yukawa potential has worse performance than KNN when thedensity of the data is higher in the distribution of the database. When theGaussian potential is applied, the results from PE and KNN have similarbehavior. The indicators used are correlation coefficients and informationgain.
arxiv-1800-298 | Rejoinder: Latent variable graphical model selection via convex optimization | http://arxiv.org/pdf/1211.0835v1.pdf | author:Venkat Chandrasekaran, Pablo A. Parrilo, Alan S. Willsky category:math.ST cs.LG stat.ML stat.TH published:2012-11-05 summary:Rejoinder to "Latent variable graphical model selection via convexoptimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky[arXiv:1008.1290].
arxiv-1800-299 | Discussion: Latent variable graphical model selection via convex optimization | http://arxiv.org/pdf/1211.0817v1.pdf | author:Emmanuel J. Candés, Mahdi Soltanolkotabi category:math.ST cs.LG stat.ML stat.TH published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convexoptimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky[arXiv:1008.1290].
arxiv-1800-300 | Discussion: Latent variable graphical model selection via convex optimization | http://arxiv.org/pdf/1211.0808v1.pdf | author:Martin J. Wainwright category:math.ST cs.LG stat.ML stat.TH published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convexoptimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky[arXiv:1008.1290].
