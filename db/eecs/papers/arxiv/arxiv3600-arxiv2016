arxiv-3600-1 | Wavelet feature extraction and genetic algorithm for biomarker detection in colorectal cancer data | http://arxiv.org/abs/1305.7465 | author:Yihui Liu, Uwe Aickelin, Jan Feyereisl, Lindy G. Durrant category:cs.NE cs.CE published:2013-05-31 summary:Biomarkers which predict patient's survival can play an important role inmedical diagnosis and treatment. How to select the significant biomarkers fromhundreds of protein markers is a key step in survival analysis. In this paper anovel method is proposed to detect the prognostic biomarkers of survival incolorectal cancer patients using wavelet analysis, genetic algorithm, and Bayesclassifier. One dimensional discrete wavelet transform (DWT) is normally usedto reduce the dimensionality of biomedical data. In this study one dimensionalcontinuous wavelet transform (CWT) was proposed to extract the features ofcolorectal cancer data. One dimensional CWT has no ability to reducedimensionality of data, but captures the missing features of DWT, and iscomplementary part of DWT. Genetic algorithm was performed on extracted waveletcoefficients to select the optimized features, using Bayes classifier to buildits fitness function. The corresponding protein markers were located based onthe position of optimized features. Kaplan-Meier curve and Cox regression modelwere used to evaluate the performance of selected biomarkers. Experiments wereconducted on colorectal cancer dataset and several significant biomarkers weredetected. A new protein biomarker CD46 was found to significantly associatewith survival time.
arxiv-3600-2 | Privileged Information for Data Clustering | http://arxiv.org/abs/1305.7454 | author:Jan Feyereisl, Uwe Aickelin category:cs.LG stat.ML published:2013-05-31 summary:Many machine learning algorithms assume that all input samples areindependently and identically distributed from some common distribution oneither the input space X, in the case of unsupervised learning, or the inputand output space X x Y in the case of supervised and semi-supervised learning.In the last number of years the relaxation of this assumption has been exploredand the importance of incorporation of additional information within machinelearning algorithms became more apparent. Traditionally such fusion ofinformation was the domain of semi-supervised learning. More recently theinclusion of knowledge from separate hypothetical spaces has been proposed byVapnik as part of the supervised setting. In this work we are interested inexploring Vapnik's idea of master-class learning and the associated learningusing privileged information, however within the unsupervised setting. Adoptionof the advanced supervised learning paradigm for the unsupervised settinginstigates investigation into the difference between privileged and technicaldata. By means of our proposed aRi-MAX method stability of the KMeans algorithmis improved and identification of the best clustering solution is achieved onan artificial dataset. Subsequently an information theoretic dot product basedalgorithm called P-Dot is proposed. This method has the ability to utilize awide variety of clustering techniques, individually or in combination, whilefusing privileged and technical data for improved clustering. Application ofthe P-Dot method to the task of digit recognition confirms our findings in areal-world scenario.
arxiv-3600-3 | Motif Detection Inspired by Immune Memory (JORS) | http://arxiv.org/abs/1305.7434 | author:William Wilson, Phil Birkin, Uwe Aickelin category:cs.NE published:2013-05-31 summary:The search for patterns or motifs in data represents an area of key interestto many researchers. In this paper we present the Motif Tracking Algorithm, anovel immune inspired pattern identification tool that is able to identifyvariable length unknown motifs which repeat within time series data. Thealgorithm searches from a neutral perspective that is independent of the databeing analysed and the underlying motifs. In this paper we test the flexibilityof the motif tracking algorithm by applying it to the search for patterns intwo industrial data sets. The algorithm is able to identify a population ofmeaningful motifs in both cases, and the value of these motifs is discussed.
arxiv-3600-4 | Alternating Decision trees for early diagnosis of dengue fever | http://arxiv.org/abs/1305.7331 | author:M. Naresh Kumar category:cs.LG q-bio.QM stat.AP published:2013-05-31 summary:Dengue fever is a flu-like illness spread by the bite of an infected mosquitowhich is fast emerging as a major health problem. Timely and cost effectivediagnosis using clinical and laboratory features would reduce the mortalityrates besides providing better grounds for clinical management and diseasesurveillance. We wish to develop a robust and effective decision tree basedapproach for predicting dengue disease. Our analysis is based on the clinicalcharacteristics and laboratory measurements of the diseased individuals. Wehave developed and trained an alternating decision tree with boosting andcompared its performance with C4.5 algorithm for dengue disease diagnosis. Ofthe 65 patient records a diagnosis establishes that 53 individuals have beenconfirmed to have dengue fever. An alternating decision tree based algorithmwas able to differentiate the dengue fever using the clinical and laboratorydata with number of correctly classified instances as 89%, F-measure of 0.86and receiver operator characteristics (ROC) of 0.826 as compared to C4.5 havingcorrectly classified instances as 78%,h F-measure of 0.738 and ROC of 0.617respectively. Alternating decision tree based approach with boosting has beenable to predict dengue fever with a higher degree of accuracy than C4.5 baseddecision tree using simple clinical and laboratory features. Further analysison larger data sets is required to improve the sensitivity and specificity ofthe alternating decision trees.
arxiv-3600-5 | Theoretical formulation and analysis of the deterministic dendritic cell algorithm | http://arxiv.org/abs/1305.7476 | author:Feng Gu, Julie Greensmith, Uwe Aickelin category:cs.NE cs.DS published:2013-05-31 summary:As one of the emerging algorithms in the field of Artificial Immune Systems(AIS), the Dendritic Cell Algorithm (DCA) has been successfully applied to anumber of challenging real-world problems. However, one criticism is the lackof a formal definition, which could result in ambiguity for understanding thealgorithm. Moreover, previous investigations have mainly focused on itsempirical aspects. Therefore, it is necessary to provide a formal definition ofthe algorithm, as well as to perform runtime analyses to revealits theoreticalaspects. In this paper, we define the deterministic version of the DCA, namedthe dDCA, using set theory and mathematical functions. Runtime analyses of thestandard algorithm and the one with additional segmentation are performed. Ouranalysis suggests that the standard dDCA has a runtime complexity of O(n2) forthe worst-case scenario, where n is the number of input data instances. Theintroduction of segmentation changes the algorithm's worst case runtimecomplexity to O(max(nN; nz)), for DC population size N with size of eachsegment z. Finally, two runtime variables of the algorithm are formulated basedon the input data, to understand its runtime behaviour as guidelines forfurther development.
arxiv-3600-6 | The Dendritic Cell Algorithm for Intrusion Detection | http://arxiv.org/abs/1305.7416 | author:Feng Gu, Julie Greensmith, Uwe Aickelin category:cs.CR cs.NE published:2013-05-31 summary:As one of the solutions to intrusion detection problems, Artificial ImmuneSystems (AIS) have shown their advantages. Unlike genetic algorithms, there isno one archetypal AIS, instead there are four major paradigms. Among them, theDendritic Cell Algorithm (DCA) has produced promising results in variousapplications. The aim of this chapter is to demonstrate the potential for theDCA as a suitable candidate for intrusion detection problems. We review some ofthe commonly used AIS paradigms for intrusion detection problems anddemonstrate the advantages of one particular algorithm, the DCA. In order toclearly describe the algorithm, the background to its development and a formaldefinition are given. In addition, improvements to the original DCA arepresented and their implications are discussed, including previous work done onan online analysis component with segmentation and ongoing work on automateddata preprocessing. Based on preliminary results, both improvements appear tobe promising for online anomaly-based intrusion detection.
arxiv-3600-7 | Immune System Approaches to Intrusion Detection - A Review (ICARIS) | http://arxiv.org/abs/1305.7144 | author:Uwe Aickelin, Julie Greensmith, Jamie Twycross category:cs.CR cs.NE published:2013-05-30 summary:The use of artificial immune systems in intrusion detection is an appealingconcept for two reasons. Firstly, the human immune system provides the humanbody with a high level of protection from invading pathogens, in a robust,self-organised and distributed manner. Secondly, current techniques used incomputer security are not able to cope with the dynamic and increasinglycomplex nature of computer systems and their security. It is hoped thatbiologically inspired approaches in this area, including the use ofimmune-based systems will be able to meet this challenge. Here we collate thealgorithms used, the development of the systems and the outcome of theirimplementation. It provides an introduction and review of the key developmentswithin this field, in addition to making suggestions for future research.
arxiv-3600-8 | Tweets Miner for Stock Market Analysis | http://arxiv.org/abs/1305.7014 | author:Bohdan Pavlyshenko category:cs.IR cs.CL cs.SI published:2013-05-30 summary:In this paper, we present a software package for the data mining of Twittermicroblogs for the purpose of using them for the stock market analysis. Thepackage is written in R langauge using apropriate R packages. The model oftweets has been considered. We have also compared stock market charts withfrequent sets of keywords in Twitter microblogs messages.
arxiv-3600-9 | Structural and Functional Discovery in Dynamic Networks with Non-negative Matrix Factorization | http://arxiv.org/abs/1305.7169 | author:Shawn Mankad, George Michailidis category:cs.SI physics.soc-ph stat.ML published:2013-05-30 summary:Time series of graphs are increasingly prevalent in modern data and poseunique challenges to visual exploration and pattern extraction. This paperdescribes the development and application of matrix factorizations forexploration and time-varying community detection in time-evolving graphsequences. The matrix factorization model allows the user to home in on anddisplay interesting, underlying structure and its evolution over time. Themethods are scalable to weighted networks with a large number of time points ornodes, and can accommodate sudden changes to graph topology. Our techniques aredemonstrated with several dynamic graph series from both synthetic and realworld data, including citation and trade networks. These examples illustratehow users can steer the techniques and combine them with existing methods todiscover and display meaningful patterns in sizable graphs over many timepoints.
arxiv-3600-10 | Memory Implementations - Current Alternatives | http://arxiv.org/abs/1305.7130 | author:William Wilson, Uwe Aickelin category:cs.AI cs.NE published:2013-05-30 summary:Memory can be defined as the ability to retain and recall information in adiverse range of forms. It is a vital component of the way in which we as humanbeings operate on a day to day basis. Given a particular situation, decisionsare made and actions undertaken in response to that situation based on ourmemory of related prior events and experiences. By utilising our memory we cananticipate the outcome of our chosen actions to avoid unexpected or unwantedevents. In addition, as we subtly alter our actions and recognise alteredoutcomes we learn and create new memories, enabling us to improve theefficiency of our actions over time. However, as this process occurs sonaturally in the subconscious its importance is often overlooked.
arxiv-3600-11 | Non-linear dimensionality reduction: Riemannian metric estimation and the problem of geometric discovery | http://arxiv.org/abs/1305.7255 | author:Dominique Perraul-Joncas, Marina Meila category:stat.ML published:2013-05-30 summary:In recent years, manifold learning has become increasingly popular as a toolfor performing non-linear dimensionality reduction. This has led to thedevelopment of numerous algorithms of varying degrees of complexity that aim torecover man ifold geometry using either local or global features of the data. Building on the Laplacian Eigenmap and Diffusionmaps framework, we propose anew paradigm that offers a guarantee, under reasonable assumptions, that anymanifo ld learning algorithm will preserve the geometry of a data set. Ourapproach is based on augmenting the output of embedding algorithms withgeometric informatio n embodied in the Riemannian metric of the manifold. Weprovide an algorithm for estimating the Riemannian metric from data anddemonstrate possible application s of our approach in a variety of examples.
arxiv-3600-12 | Test cost and misclassification cost trade-off using reframing | http://arxiv.org/abs/1305.7111 | author:Celestine Periale Ma, José Hernández-Orallo category:cs.LG published:2013-05-30 summary:Many solutions to cost-sensitive classification (and regression) rely on someor all of the following assumptions: we have complete knowledge about the costcontext at training time, we can easily re-train whenever the cost contextchanges, and we have technique-specific methods (such as cost-sensitivedecision trees) that can take advantage of that information. In this paper weaddress the problem of selecting models and minimising joint cost (integratingboth misclassification cost and test costs) without any of the aboveassumptions. We introduce methods and plots (such as the so-called JROC plots)that can work with any off-the-shelf predictive technique, including ensembles,such that we reframe the model to use the appropriate subset of attributes (thefeature configuration) during deployment time. In other words, models aretrained with the available attributes (once and for all) and then deployed bysetting missing values on the attributes that are deemed ineffective forreducing the joint cost. As the number of feature configuration combinationsgrows exponentially with the number of features we introduce quadratic methodsthat are able to approximate the optimal configuration and model choices, asshown by the experimental results.
arxiv-3600-13 | Lensless Imaging by Compressive Sensing | http://arxiv.org/abs/1305.7181 | author:Gang Huang, Hong Jiang, Kim Matthews, Paul Wilford category:cs.CV published:2013-05-30 summary:In this paper, we propose a lensless compressive imaging architecture. Thearchitecture consists of two components, an aperture assembly and a sensor. Nolens is used. The aperture assembly consists of a two dimensional array ofaperture elements. The transmittance of each aperture element is independentlycontrollable. The sensor is a single detection element. A compressive sensingmatrix is implemented by adjusting the transmittance of the individual apertureelements according to the values of the sensing matrix. The proposedarchitecture is simple and reliable because no lens is used. The architecturecan be used for capturing images of visible and other spectra such as infrared,or millimeter waves, in surveillance applications for detecting anomalies orextracting features such as speed of moving objects. Multiple sensors may beused with a single aperture assembly to capture multi-view imagessimultaneously. A prototype was built by using a LCD panel and a photoelectricsensor for capturing images of visible spectrum.
arxiv-3600-14 | A Local Active Contour Model for Image Segmentation with Intensity Inhomogeneity | http://arxiv.org/abs/1305.7053 | author:Kaihua Zhang, Lei Zhang, Kin-Man Lam, David Zhang category:cs.CV published:2013-05-30 summary:A novel locally statistical active contour model (ACM) for image segmentationin the presence of intensity inhomogeneity is presented in this paper. Theinhomogeneous objects are modeled as Gaussian distributions of different meansand variances, and a moving window is used to map the original image intoanother domain, where the intensity distributions of inhomogeneous objects arestill Gaussian but are better separated. The means of the Gaussiandistributions in the transformed domain can be adaptively estimated bymultiplying a bias field with the original signal within the window. Astatistical energy functional is then defined for each local region, whichcombines the bias field, the level set function, and the constant approximatingthe true signal of the corresponding object. Experiments on both synthetic andreal images demonstrate the superiority of our proposed algorithm tostate-of-the-art and representative methods.
arxiv-3600-15 | Dienstplanerstellung in Krankenhaeusern mittels genetischer Algorithmen | http://arxiv.org/abs/1305.7056 | author:Uwe Aickelin category:cs.NE published:2013-05-30 summary:This thesis investigates the use of problem-specific knowledge to enhance agenetic algorithm approach to multiple-choice optimisation problems. It showsthat such information can significantly enhance performance, but that thechoice of information and the way it is included are important factors forsuccess.
arxiv-3600-16 | Predicting the Severity of Breast Masses with Data Mining Methods | http://arxiv.org/abs/1305.7057 | author:Sahar A. Mokhtar, Alaa. M. Elsayad category:cs.LG stat.ML published:2013-05-30 summary:Mammography is the most effective and available tool for breast cancerscreening. However, the low positive predictive value of breast biopsyresulting from mammogram interpretation leads to approximately 70% unnecessarybiopsies with benign outcomes. Data mining algorithms could be used to helpphysicians in their decisions to perform a breast biopsy on a suspicious lesionseen in a mammogram image or to perform a short term follow-up examinationinstead. In this research paper data mining classification algorithms; DecisionTree (DT), Artificial Neural Network (ANN), and Support Vector Machine (SVM)are analyzed on mammographic masses data set. The purpose of this study is toincrease the ability of physicians to determine the severity (benign ormalignant) of a mammographic mass lesion from BI-RADS attributes and thepatient,s age. The whole data set is divided for training the models and testthem by the ratio of 70:30% respectively and the performances of classificationalgorithms are compared through three statistical measures; sensitivity,specificity, and classification accuracy. Accuracy of DT, ANN and SVM are78.12%, 80.56% and 81.25% of test samples respectively. Our analysis shows thatout of these three classification models SVM predicts severity of breast cancerwith least error rate and highest accuracy.
arxiv-3600-17 | Optimal Rates of Convergence for Latent Generalized Correlation Matrix Estimation in Transelliptical Distribution | http://arxiv.org/abs/1305.6916 | author:Fang Han, Han Liu category:stat.ML published:2013-05-29 summary:Correlation matrix plays a key role in many multivariate methods (e.g.,graphical model estimation and factor analysis). The current state-of-the-artin estimating large correlation matrices focuses on the use of Pearson's samplecorrelation matrix. Although Pearson's sample correlation matrix enjoys variousgood properties under Gaussian models, its not an effective estimator whenfacing heavy-tail distributions with possible outliers. As a robustalternative, \cite{han2012transelliptical} advocated the use of a transformedversion of the Kendall's tau sample correlation matrix in estimating highdimensional latent generalized correlation matrix under the transellipticaldistribution family (or elliptical copula). The transelliptical family assumesthat after unspecified marginal monotone transformations, the data follow anelliptical distribution. In this paper, we study the theoretical properties ofthe Kendall's tau sample correlation matrix and its transformed versionproposed in \cite{han2012transelliptical} for estimating the populationKendall's tau correlation matrix and the latent Pearson's correlation matrixunder both spectral and restricted spectral norms. With regard to the spectralnorm, we highlight the role of "effective rank" in quantifying the rate ofconvergence. With regard to the restricted spectral norm, we for the first timepresent a "sign subgaussian condition" which is sufficient to guarantee thatthe rank-based correlation matrix estimator attains the optimal rate ofconvergence. In both cases, we do not need any moment condition.
arxiv-3600-18 | Generalized Denoising Auto-Encoders as Generative Models | http://arxiv.org/abs/1305.6663 | author:Yoshua Bengio, Li Yao, Guillaume Alain, Pascal Vincent category:cs.LG published:2013-05-29 summary:Recent work has shown how denoising and contractive autoencoders implicitlycapture the structure of the data-generating density, in the case where thecorruption noise is Gaussian, the reconstruction error is the squared error,and the data is continuous-valued. This has led to various proposals forsampling from this implicitly learned density function, using Langevin andMetropolis-Hastings MCMC. However, it remained unclear how to connect thetraining procedure of regularized auto-encoders to the implicit estimation ofthe underlying data-generating distribution when the data are discrete, orusing other forms of corruption process and reconstruction errors. Anotherissue is the mathematical justification which is only valid in the limit ofsmall corruption noise. We propose here a different attack on the problem,which deals with all these issues: arbitrary (but noisy enough) corruption,arbitrary reconstruction loss (seen as a log-likelihood), handling bothdiscrete and continuous-valued variables, and removing the bias due tonon-infinitesimal corruption noise (or non-infinitesimal contractive penalty).
arxiv-3600-19 | Video Human Segmentation using Fuzzy Object Models and its Application to Body Pose Estimation of Toddlers for Behavior Studies | http://arxiv.org/abs/1305.6918 | author:Thiago V. Spina, Mariano Tepper, Amy Esler, Vassilios Morellas, Nikolaos Papanikolopoulos, Alexandre X. Falcão, Guillermo Sapiro category:cs.CV published:2013-05-29 summary:Video object segmentation is a challenging problem due to the presence ofdeformable, connected, and articulated objects, intra- and inter-objectocclusions, object motion, and poor lighting. Some of these challenges call forobject models that can locate a desired object and separate it from itssurrounding background, even when both share similar colors and textures. Inthis work, we extend a fuzzy object model, named cloud system model (CSM), tohandle video segmentation, and evaluate it for body pose estimation of toddlersat risk of autism. CSM has been successfully used to model the parts of thebrain (cerebrum, left and right brain hemispheres, and cerebellum) in order toautomatically locate and separate them from each other, the connected brainstem, and the background in 3D MR-images. In our case, the objects arearticulated parts (2D projections) of the human body, which can deform, causeself-occlusions, and move along the video. The proposed CSM extension handlesarticulation by connecting the individual clouds, body parts, of the systemusing a 2D stickman model. The stickman representation naturally allows us toextract 2D body pose measures of arm asymmetry patterns during unsupported gaitof toddlers, a possible behavioral marker of autism. The results show that ourmethod can provide insightful knowledge to assist the specialist's observationsduring real in-clinic assessments.
arxiv-3600-20 | Rotation invariants of two dimensional curves based on iterated integrals | http://arxiv.org/abs/1305.6883 | author:Joscha Diehl category:cs.CV stat.ML published:2013-05-29 summary:We introduce a novel class of rotation invariants of two dimensional curvesbased on iterated integrals. The invariants we present are in some sensecomplete and we describe an algorithm to calculate them, giving explicitcomputations up to order six. We present an application to online(stroke-trajectory based) character recognition. This seems to be the firsttime in the literature that the use of iterated integrals of a curve isproposed for (invariant) feature extraction in machine learning applications.
arxiv-3600-21 | A Cooperative Coevolutionary Genetic Algorithm for Learning Bayesian Network Structures | http://arxiv.org/abs/1305.6537 | author:Arthur Carvalho category:cs.NE cs.AI published:2013-05-28 summary:We propose a cooperative coevolutionary genetic algorithm for learningBayesian network structures from fully observable data sets. Since this problemcan be decomposed into two dependent subproblems, that is to find an orderingof the nodes and an optimal connectivity matrix, our algorithm uses twosubpopulations, each one representing a subtask. We describe the empiricalresults obtained with simulations of the Alarm and Insurance networks. We showthat our algorithm outperforms the deterministic algorithm K2.
arxiv-3600-22 | Reinforcement Learning for the Soccer Dribbling Task | http://arxiv.org/abs/1305.6568 | author:Arthur Carvalho, Renato Oliveira category:cs.LG cs.RO stat.ML published:2013-05-28 summary:We propose a reinforcement learning solution to the \emph{soccer dribblingtask}, a scenario in which a soccer agent has to go from the beginning to theend of a region keeping possession of the ball, as an adversary attempts togain possession. While the adversary uses a stationary policy, the dribblerlearns the best action to take at each decision point. After definingmeaningful variables to represent the state space, and high-level macro-actionsto incorporate domain knowledge, we describe our application of thereinforcement learning algorithm \emph{Sarsa} with CMAC for functionapproximation. Our experiments show that, after the training period, thedribbler is able to accomplish its task against a strong adversary around 58%of the time.
arxiv-3600-23 | Matrices of forests, analysis of networks, and ranking problems | http://arxiv.org/abs/1305.6441 | author:Pavel Chebotarev, Rafig Agaev category:math.CO cs.CV cs.DM cs.NI published:2013-05-28 summary:The matrices of spanning rooted forests are studied as a tool for analysingthe structure of networks and measuring their properties. The problems ofrevealing the basic bicomponents, measuring vertex proximity, and ranking frompreference relations / sports competitions are considered. It is shown that thevertex accessibility measure based on spanning forests has a number ofdesirable properties. An interpretation for the stochastic matrix ofout-forests in terms of information dissemination is given.
arxiv-3600-24 | Normalized Online Learning | http://arxiv.org/abs/1305.6646 | author:Stephane Ross, Paul Mineiro, John Langford category:cs.LG stat.ML published:2013-05-28 summary:We introduce online learning algorithms which are independent of featurescales, proving regret bounds dependent on the ratio of scales existent in thedata rather than the absolute scale. This has several useful effects: there isno need to pre-normalize data, the test-time and test-space complexity arereduced, and the algorithms are more robust.
arxiv-3600-25 | Higher-order Segmentation via Multicuts | http://arxiv.org/abs/1305.6387 | author:Joerg Hendrik Kappes, Markus Speth, Gerhard Reinelt, Christoph Schnoerr category:cs.CV published:2013-05-28 summary:Multicuts enable to conveniently represent discrete graphical models forunsupervised and supervised image segmentation, in the case of local energyfunctions that exhibit symmetries. The basic Potts model and natural extensionsthereof to higher-order models provide a prominent class of such objectives,that cover a broad range of segmentation problems relevant to image analysisand computer vision. We exhibit a way to systematically take into account suchhigher-order terms for computational inference. Furthermore, we present resultsof a comprehensive and competitive numerical evaluation of a variety ofdedicated cutting-plane algorithms. Our approach enables the globally optimalevaluation of a significant subset of these models, without compromisingruntime. Polynomially solvable relaxations are studied as well, along withadvanced rounding schemes for post-processing.
arxiv-3600-26 | Active Sensing as Bayes-Optimal Sequential Decision Making | http://arxiv.org/abs/1305.6650 | author:Sheeraz Ahmad, Angela J. Yu category:cs.AI cs.CV published:2013-05-28 summary:Sensory inference under conditions of uncertainty is a major problem in bothmachine learning and computational neuroscience. An important but poorlyunderstood aspect of sensory processing is the role of active sensing. Here, wepresent a Bayes-optimal inference and control framework for active sensing,C-DAC (Context-Dependent Active Controller). Unlike previously proposedalgorithms that optimize abstract statistical objectives such as informationmaximization (Infomax) [Butko & Movellan, 2010] or one-step look-ahead accuracy[Najemnik & Geisler, 2005], our active sensing model directly minimizes acombination of behavioral costs, such as temporal delay, response error, andeffort. We simulate these algorithms on a simple visual search task toillustrate scenarios in which context-sensitivity is particularly beneficialand optimization with respect to generic statistical objectives particularlyinadequate. Motivated by the geometric properties of the C-DAC policy, wepresent both parametric and non-parametric approximations, which retaincontext-sensitivity while significantly reducing computational complexity.These approximations enable us to investigate the more complex probleminvolving peripheral vision, and we notice that the difference between C-DACand statistical policies becomes even more evident in this scenario.
arxiv-3600-27 | Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture | http://arxiv.org/abs/1305.6659 | author:Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin category:cs.LG stat.ML published:2013-05-28 summary:This paper presents a novel algorithm, based upon the dependent Dirichletprocess mixture model (DDPMM), for clustering batch-sequential data containingan unknown number of evolving clusters. The algorithm is derived via alow-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM,and provides a hard clustering with convergence guarantees similar to those ofthe k-means algorithm. Empirical results from a synthetic test with movingGaussian clusters and a test with real ADS-B aircraft trajectory datademonstrate that the algorithm requires orders of magnitude less computationaltime than contemporary probabilistic and hard clustering algorithms, whileproviding higher accuracy on the examined datasets.
arxiv-3600-28 | Adaptive estimation of the copula correlation matrix for semiparametric elliptical copulas | http://arxiv.org/abs/1305.6526 | author:Marten Wegkamp, Yue Zhao category:stat.ML published:2013-05-28 summary:We study the adaptive estimation of copula correlation matrix $\Sigma$ forthe semi-parametric elliptical copula model. In this context, the correlationsare connected to Kendall's tau through a sine function transformation. Hence, anatural estimate for $\Sigma$ is the plug-in estimator $\hat{\Sigma}$ withKendall's tau statistic. We first obtain a sharp bound on the operator norm of$\hat{\Sigma}-\Sigma$. Then we study a factor model of $\Sigma$, for which wepropose a refined estimator $\widetilde{\Sigma}$ by fitting a low-rank matrixplus a diagonal matrix to $\hat{\Sigma}$ using least squares with a nuclearnorm penalty on the low-rank matrix. The bound on the operator norm of$\hat{\Sigma}-\Sigma$ serves to scale the penalty term, and we obtain finitesample oracle inequalities for $\widetilde{\Sigma}$. We also consider anelementary factor copula model of $\Sigma$, for which we propose closed-formestimators. All of our estimation procedures are entirely data-driven.
arxiv-3600-29 | Information-Theoretic Approach to Efficient Adaptive Path Planning for Mobile Robotic Environmental Sensing | http://arxiv.org/abs/1305.6129 | author:Kian Hsiang Low, John M. Dolan, Pradeep Khosla category:cs.LG cs.AI cs.MA cs.RO published:2013-05-27 summary:Recent research in robot exploration and mapping has focused on samplingenvironmental hotspot fields. This exploration task is formalized by Low,Dolan, and Khosla (2008) in a sequential decision-theoretic planning underuncertainty framework called MASP. The time complexity of solving MASPapproximately depends on the map resolution, which limits its use inlarge-scale, high-resolution exploration and mapping. To alleviate thiscomputational difficulty, this paper presents an information-theoretic approachto MASP (iMASP) for efficient adaptive path planning; by reformulating thecost-minimizing iMASP as a reward-maximizing problem, its time complexitybecomes independent of map resolution and is less sensitive to increasing robotteam size as demonstrated both theoretically and empirically. Using thereward-maximizing dual, we derive a novel adaptive variant of maximum entropysampling, thus improving the induced exploration policy performance. It alsoallows us to establish theoretical bounds quantifying the performance advantageof optimal adaptive over non-adaptive policies and the performance quality ofapproximately optimal vs. optimal adaptive policies. We show analytically andempirically the superior performance of iMASP-based policies for sampling thelog-Gaussian process to that of policies for the widely-used Gaussian processin mapping the hotspot field. Lastly, we provide sufficient conditions that,when met, guarantee adaptivity has no benefit under an assumed environmentmodel.
arxiv-3600-30 | Optimal rates of convergence for persistence diagrams in Topological Data Analysis | http://arxiv.org/abs/1305.6239 | author:Frédéric Chazal, Marc Glisse, Catherine Labruère, Bertrand Michel category:math.ST cs.CG cs.LG math.GT stat.TH published:2013-05-27 summary:Computational topology has recently known an important development towarddata analysis, giving birth to the field of topological data analysis.Topological persistence, or persistent homology, appears as a fundamental toolin this field. In this paper, we study topological persistence in generalmetric spaces, with a statistical approach. We show that the use of persistenthomology can be naturally considered in general statistical frameworks andpersistence diagrams can be used as statistics with interesting convergenceproperties. Some numerical experiments are performed in various contexts toillustrate our results.
arxiv-3600-31 | Some results on a $χ$-divergence, an~extended~Fisher information and~generalized~Cramér-Rao inequalities | http://arxiv.org/abs/1305.6213 | author:Jean-François Bercher category:cs.IT math.IT stat.ML published:2013-05-27 summary:We propose a modified $\chi^{\beta}$-divergence, give some of its properties,and show that this leads to the definition of a generalized Fisher information.We give generalized Cram\'er-Rao inequalities, involving this Fisherinformation, an extension of the Fisher information matrix, and arbitrary normsand power of the estimation error. In the case of a location parameter, weobtain new characterizations of the generalized $q$-Gaussians, for instance asthe distribution with a given moment that minimizes the generalized Fisherinformation. Finally we indicate how the generalized Fisher information canlead to new uncertainty relations.
arxiv-3600-32 | On some interrelations of generalized $q$-entropies and a generalized Fisher information, including a Cramér-Rao inequality | http://arxiv.org/abs/1305.6215 | author:Jean-François Bercher category:cs.IT cond-mat.other math.IT stat.ML published:2013-05-27 summary:In this communication, we describe some interrelations between generalized$q$-entropies and a generalized version of Fisher information. In informationtheory, the de Bruijn identity links the Fisher information and the derivativeof the entropy. We show that this identity can be extended to generalizedversions of entropy and Fisher information. More precisely, a generalizedFisher information naturally pops up in the expression of the derivative of theTsallis entropy. This generalized Fisher information also appears as a specialcase of a generalized Fisher information for estimation problems. Indeed, wederive here a new Cram\'er-Rao inequality for the estimation of a parameter,which involves a generalized form of Fisher information. This generalizedFisher information reduces to the standard Fisher information as a particularcase. In the case of a translation parameter, the general Cram\'er-Raoinequality leads to an inequality for distributions which is saturated bygeneralized $q$-Gaussian distributions. These generalized $q$-Gaussians areimportant in several areas of physics and mathematics. They are known tomaximize the $q$-entropies subject to a moment constraint. The Cram\'er-Raoinequality shows that the generalized $q$-Gaussians also minimize thegeneralized Fisher information among distributions with a fixed moment.Similarly, the generalized $q$-Gaussians also minimize the generalized Fisherinformation among distributions with a given $q$-entropy.
arxiv-3600-33 | Extended Lambek calculi and first-order linear logic | http://arxiv.org/abs/1305.6238 | author:Richard Moot category:cs.CL cs.LO published:2013-05-27 summary:First-order multiplicative intuitionistic linear logic (MILL1) can be seen asan extension of the Lambek calculus. In addition to the fragment of MILL1 whichcorresponds to the Lambek calculus (of Moot & Piazza 2001), I will showfragments of MILL1 which generate the multiple context-free languages and whichcorrespond to the Displacement calculus of Morrilll e.a.
arxiv-3600-34 | Fast and accurate sentiment classification using an enhanced Naive Bayes model | http://arxiv.org/abs/1305.6143 | author:Vivek Narayanan, Ishan Arora, Arjun Bhatia category:cs.CL cs.IR cs.LG I.2.7 published:2013-05-27 summary:We have explored different methods of improving the accuracy of a Naive Bayesclassifier for sentiment analysis. We observed that a combination of methodslike negation handling, word n-grams and feature selection by mutualinformation results in a significant improvement in accuracy. This implies thata highly accurate and fast sentiment classifier can be built using a simpleNaive Bayes model that has linear training and testing time complexities. Weachieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.
arxiv-3600-35 | Supervised Feature Selection for Diagnosis of Coronary Artery Disease Based on Genetic Algorithm | http://arxiv.org/abs/1305.6046 | author:Sidahmed Mokeddem, Baghdad Atmani, Mostefa Mokaddem category:cs.LG cs.CE published:2013-05-26 summary:Feature Selection (FS) has become the focus of much research on decisionsupport systems areas for which data sets with tremendous number of variablesare analyzed. In this paper we present a new method for the diagnosis ofCoronary Artery Diseases (CAD) founded on Genetic Algorithm (GA) wrapped BayesNaive (BN) based FS. Basically, CAD dataset contains two classes defined with13 features. In GA BN algorithm, GA generates in each iteration a subset ofattributes that will be evaluated using the BN in the second step of theselection procedure. The final set of attribute contains the most relevantfeature model that increases the accuracy. The algorithm in this case produces85.50% classification accuracy in the diagnosis of CAD. Thus, the asset of theAlgorithm is then compared with the use of Support Vector Machine (SVM),MultiLayer Perceptron (MLP) and C4.5 decision tree Algorithm. The result ofclassification accuracy for those algorithms are respectively 83.5%, 83.16% and80.85%. Consequently, the GA wrapped BN Algorithm is correspondingly comparedwith other FS algorithms. The Obtained results have shown very promisingoutcomes for the diagnosis of CAD.
arxiv-3600-36 | ÖAGM/AAPR 2013 - The 37th Annual Workshop of the Austrian Association for Pattern Recognition | http://arxiv.org/abs/1305.5905 | author:Justus Piater, Antonio J. Rodríguez Sánchez category:cs.CV published:2013-05-25 summary:In this editorial, the organizers summarize facts and background about theevent.
arxiv-3600-37 | Reduce Meaningless Words for Joint Chinese Word Segmentation and Part-of-speech Tagging | http://arxiv.org/abs/1305.5918 | author:Kaixu Zhang, Maosong Sun category:cs.CL published:2013-05-25 summary:Conventional statistics-based methods for joint Chinese word segmentation andpart-of-speech tagging (S&T) have generalization ability to recognize new wordsthat do not appear in the training data. An undesirable side effect is that anumber of meaningless words will be incorrectly created. We propose aneffective and efficient framework for S&T that introduces features tosignificantly reduce meaningless words generation. A general lexicon, Wikepediaand a large-scale raw corpus of 200 billion characters are used to generateword-based features for the wordhood. The word-lattice based framework consistsof a character-based model and a word-based model in order to employ ourword-based features. Experiments on Penn Chinese treebank 5 show that thismethod has a 62.9% reduction of meaningless word generation in comparison withthe baseline. As a result, the F1 measure for segmentation is increased to0.984.
arxiv-3600-38 | Applications of Clifford's Geometric Algebra | http://arxiv.org/abs/1305.5663 | author:Eckhard Hitzer, Tohru Nitta, Yasuaki Kuroe category:math.RA cs.CV published:2013-05-24 summary:We survey the development of Clifford's geometric algebra and some of itsengineering applications during the last 15 years. Several recently developedapplications and their merits are discussed in some detail. We thus hope toclearly demonstrate the benefit of developing problem solutions in a unifiedframework for algebra and geometry with the widest possible scope: from quantumcomputing and electromagnetism to satellite navigation, from neural computingto camera geometry, image processing, robotics and beyond.
arxiv-3600-39 | A Symmetric Rank-one Quasi Newton Method for Non-negative Matrix Factorization | http://arxiv.org/abs/1305.5829 | author:Shu-Zhen Lai, Hou-Biao Li, Zu-Tao Zhang category:math.NA cs.LG cs.NA 15A18 published:2013-05-24 summary:As we all known, the nonnegative matrix factorization (NMF) is a dimensionreduction method that has been widely used in image processing, textcompressing and signal processing etc. In this paper, an algorithm fornonnegative matrix approximation is proposed. This method mainly bases on theactive set and the quasi-Newton type algorithm, by using the symmetric rank-oneand negative curvature direction technologies to approximate the Hessianmatrix. Our method improves the recent results of those methods in [PatternRecognition, 45(2012)3557-3565; SIAM J. Sci. Comput., 33(6)(2011)3261-3281;Neural Computation, 19(10)(2007)2756-2779, etc.]. Moreover, the object functiondecreases faster than many other NMF methods. In addition, some numericalexperiments are presented in the synthetic data, imaging processing and textclustering. By comparing with the other six nonnegative matrix approximationmethods, our experiments confirm to our analysis.
arxiv-3600-40 | An Inventory of Preposition Relations | http://arxiv.org/abs/1305.5785 | author:Vivek Srikumar, Dan Roth category:cs.CL published:2013-05-24 summary:We describe an inventory of semantic relations that are expressed byprepositions. We define these relations by building on the word sensedisambiguation task for prepositions and propose a mapping from prepositionsenses to the relation labels by collapsing semantically related senses acrossprepositions.
arxiv-3600-41 | Edge Detection in Radar Images Using Weibull Distribution | http://arxiv.org/abs/1305.5728 | author:Ali El-Zaart, Wafaa Kamel Al-Jibory category:cs.CV published:2013-05-24 summary:Radar images can reveal information about the shape of the surface terrain aswell as its physical and biophysical properties. Radar images have long beenused in geological studies to map structural features that are revealed by theshape of the landscape. Radar imagery also has applications in vegetation andcrop type mapping, landscape ecology, hydrology, and volcanology. Imageprocessing is using for detecting for objects in radar images. Edge detection;which is a method of determining the discontinuities in gray level images; is avery important initial step in Image processing. Many classical edge detectorshave been developed over time. Some of the well-known edge detection operatorsbased on the first derivative of the image are Roberts, Prewitt, Sobel which istraditionally implemented by convolving the image with masks. Also Gaussiandistribution has been used to build masks for the first and second derivative.However, this distribution has limit to only symmetric shape. This paper willuse to construct the masks, the Weibull distribution which was more generalthan Gaussian because it has symmetric and asymmetric shape. The constructedmasks are applied to images and we obtained good results.
arxiv-3600-42 | Flooding edge or node weighted graphs | http://arxiv.org/abs/1305.5756 | author:Fernand Meyer category:cs.CV published:2013-05-24 summary:Reconstruction closings have all properties of a physical flooding of atopographic surface. They are precious for simplifying gradient images or,filling unwanted catchment basins, on which a subsequent watershed transformextracts the targeted objects. Flooding a topographic surface may be modeled asflooding a node weighted graph (TG), with unweighted edges, the node weightsrepresenting the ground level. The progression of a flooding may also bemodeled on the region adjacency graph (RAG) of a topographic surface. On a RAGeach node represents a catchment basin and edges connect neighboring nodes. Theedges are weighted by the altitude of the pass point between both adjacentregions. The graph is flooded from sources placed at the marker positions andeach node is assigned to the source by which it has been flooded. The level ofthe flood is represented on the nodes on each type of graphs. The same floodingmay thus be modeled on a TG or on a RAG. We characterize all valid floodings onboth types of graphs, as they should verify the laws of hydrostatics. We thenshow that each flooding of a node weighted graph also is a flooding of an edgeweighted graph with appropriate edge weights. The highest flooding under aceiling function may be interpreted as the shortest distance to the root forthe ultrametric flooding distance in an augmented graph. The ultrametricdistance between two nodes is the minimal altitude of a flooding for which bothnodes are flooded. This remark permits to flood edge or node weighted graphs byusing shortest path algorithms. It appears that the collection of all lakes ofa RAG has the structure of a dendrogram, on which the highest flooding under aceiling function may be rapidly found.
arxiv-3600-43 | Characterizing A Database of Sequential Behaviors with Latent Dirichlet Hidden Markov Models | http://arxiv.org/abs/1305.5734 | author:Yin Song, Longbing Cao, Xuhui Fan, Wei Cao, Jian Zhang category:stat.ML cs.LG H.2.8; F.1.2 published:2013-05-24 summary:This paper proposes a generative model, the latent Dirichlet hidden Markovmodels (LDHMM), for characterizing a database of sequential behaviors(sequences). LDHMMs posit that each sequence is generated by an underlyingMarkov chain process, which are controlled by the corresponding parameters(i.e., the initial state vector, transition matrix and the emission matrix).These sequence-level latent parameters for each sequence are modeled as latentDirichlet random variables and parameterized by a set of deterministicdatabase-level hyper-parameters. Through this way, we expect to model thesequence in two levels: the database level by deterministic hyper-parametersand the sequence-level by latent parameters. To learn the deterministichyper-parameters and approximate posteriors of parameters in LDHMMs, we proposean iterative algorithm under the variational EM framework, which consists of Eand M steps. We examine two different schemes, the fully-factorized andpartially-factorized forms, for the framework, based on different assumptions.We present empirical results of behavior modeling and sequence classificationon three real-world data sets, and compare them to other related models. Theexperimental results prove that the proposed LDHMMs produce bettergeneralization performance in terms of log-likelihood and deliver competitiveresults on the sequence classification problem.
arxiv-3600-44 | Parallel Gaussian Process Regression with Low-Rank Covariance Matrix Approximations | http://arxiv.org/abs/1305.5826 | author:Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan Tan, Patrick Jaillet category:stat.ML cs.DC cs.LG published:2013-05-24 summary:Gaussian processes (GP) are Bayesian non-parametric models that are widelyused for probabilistic regression. Unfortunately, it cannot scale well withlarge data nor perform real-time predictions due to its cubic time cost in thedata size. This paper presents two parallel GP regression methods that exploitlow-rank covariance matrix approximations for distributing the computationalload among parallel machines to achieve time efficiency and scalability. Wetheoretically guarantee the predictive performances of our proposed parallelGPs to be equivalent to that of some centralized approximate GP regressionmethods: The computation of their centralized counterparts can be distributedamong parallel machines, hence achieving greater time efficiency andscalability. We analytically compare the properties of our parallel GPs such astime, space, and communication complexity. Empirical evaluation on tworeal-world datasets in a cluster of 20 computing nodes shows that our parallelGPs are significantly more time-efficient and scalable than their centralizedcounterparts and exact/full GP while achieving predictive performancescomparable to full GP.
arxiv-3600-45 | Development of a Hindi Lemmatizer | http://arxiv.org/abs/1305.6211 | author:Snigdha Paul, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-05-24 summary:We live in a translingual society, in order to communicate with people fromdifferent parts of the world we need to have an expertise in their respectivelanguages. Learning all these languages is not at all possible; therefore weneed a mechanism which can do this task for us. Machine translators haveemerged as a tool which can perform this task. In order to develop a machinetranslator we need to develop several different rules. The very first modulethat comes in machine translation pipeline is morphological analysis. Stemmingand lemmatization comes under morphological analysis. In this paper we havecreated a lemmatizer which generates rules for removing the affixes along withthe addition of rules for creating a proper root word.
arxiv-3600-46 | Adapting the Stochastic Block Model to Edge-Weighted Networks | http://arxiv.org/abs/1305.5782 | author:Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset category:stat.ML cs.LG cs.SI published:2013-05-24 summary:We generalize the stochastic block model to the important case in which edgesare annotated with weights drawn from an exponential family distribution. Thisgeneralization introduces several technical difficulties for model estimation,which we solve using a Bayesian approach. We introduce a variational algorithmthat efficiently approximates the model's posterior distribution for densegraphs. In specific numerical experiments on edge-weighted networks, thisweighted stochastic block model outperforms the common approach of firstapplying a single threshold to all weights and then applying the classicstochastic block model, which can obscure latent block structure in networks.This model will enable the recovery of latent structure in a broader range ofnetwork data than was previously possible.
arxiv-3600-47 | Compressive Sensing of Sparse Tensors | http://arxiv.org/abs/1305.5777 | author:Shmuel Friedland, Qun Li, Dan Schonfeld category:cs.CV cs.IT math.IT published:2013-05-24 summary:Compressive sensing (CS) has triggered enormous research activity since itsfirst appearance. CS exploits the signal's sparsity or compressibility in aparticular domain and integrates data compression and acquisition, thusallowing exact reconstruction through relatively few non-adaptive linearmeasurements. While conventional CS theory relies on data representation in theform of vectors, many data types in various applications such as color imaging,video sequences, and multi-sensor networks, are intrinsically represented byhigher-order tensors. Application of CS to higher-order data representation istypically performed by conversion of the data to very long vectors that must bemeasured using very large sampling matrices, thus imposing a huge computationaland memory burden. In this paper, we propose Generalized Tensor CompressiveSensing (GTCS)--a unified framework for compressive sensing of higher-ordertensors which preserves the intrinsic structure of tensor data with reducedcomputational complexity at reconstruction. GTCS offers an efficient means forrepresentation of multidimensional data by providing simultaneous acquisitionand compression from all tensor modes. In addition, we propound tworeconstruction procedures, a serial method (GTCS-S) and a parallelizable method(GTCS-P). We then compare the performance of the proposed method with Kroneckercompressive sensing (KCS) and multi way compressive sensing (MWCS). Wedemonstrate experimentally that GTCS outperforms KCS and MWCS in terms of bothreconstruction accuracy (within a range of compression ratios) and processingspeed. The major disadvantage of our methods (and of MWCS as well), is that thecompression ratios may be worse than that offered by KCS.
arxiv-3600-48 | A probabilistic framework for analysing the compositionality of conceptual combinations | http://arxiv.org/abs/1305.5753 | author:Peter D. Bruza, Kirsty Kitto, Brentyn J. Ramm, Laurianne Sitbon category:cs.CL published:2013-05-23 summary:Conceptual combination performs a fundamental role in creating the broadrange of compound phrases utilized in everyday language. This article providesa novel probabilistic framework for assessing whether the semantics ofconceptual combinations are compositional, and so can be considered as afunction of the semantics of the constituent concepts, or not. While thesystematicity and productivity of language provide a strong argument in favorof assuming compositionality, this very assumption is still regularlyquestioned in both cognitive science and philosophy. Additionally, theprinciple of semantic compositionality is underspecified, which means thatnotions of both "strong" and "weak" compositionality appear in the literature.Rather than adjudicating between different grades of compositionality, theframework presented here contributes formal methods for determining a cleardividing line between compositional and non-compositional semantics. Inaddition, we suggest that the distinction between these is contextuallysensitive. Utilizing formal frameworks developed for analyzing compositesystems in quantum theory, we present two methods that allow the semantics ofconceptual combinations to be classified as "compositional" or"non-compositional". Compositionality is first formalised by factorising thejoint probability distribution modeling the combination, where the terms in thefactorisation correspond to individual concepts. This leads to the necessaryand sufficient condition for the joint probability distribution to exist. Afailure to meet this condition implies that the underlying concepts cannot bemodeled in a single probability space when considering their combination, andthe combination is thus deemed "non-compositional". The formal analysis methodsare demonstrated by applying them to an empirical study of twenty-fournon-lexicalised conceptual combinations.
arxiv-3600-49 | A Primal Condition for Approachability with Partial Monitoring | http://arxiv.org/abs/1305.5399 | author:Shie Mannor, Vianney Perchet, Gilles Stoltz category:math.OC cs.GT cs.LG stat.ML published:2013-05-23 summary:In approachability with full monitoring there are two types of conditionsthat are known to be equivalent for convex sets: a primal and a dual condition.The primal one is of the form: a set C is approachable if and only allcontaining half-spaces are approachable in the one-shot game; while the dualone is of the form: a convex set C is approachable if and only if it intersectsall payoff sets of a certain form. We consider approachability in games withpartial monitoring. In previous works (Perchet 2011; Mannor et al. 2011) weprovided a dual characterization of approachable convex sets; we also exhibitedefficient strategies in the case where C is a polytope. In this paper weprovide primal conditions on a convex set to be approachable with partialmonitoring. They depend on a modified reward function and lead toapproachability strategies, based on modified payoff functions, that proceed byprojections similarly to Blackwell's (1956) strategy; this is in contrast withpreviously studied strategies in this context that relied mostly on thesignaling structure and aimed at estimating well the distributions of thesignals received. Our results generalize classical results by Kohlberg 1975(see also Mertens et al. 1994) and apply to games with arbitrary signalingstructure as well as to arbitrary convex sets.
arxiv-3600-50 | The most controversial topics in Wikipedia: A multilingual and geographical analysis | http://arxiv.org/abs/1305.5566 | author:Taha Yasseri, Anselm Spoerri, Mark Graham, János Kertész category:physics.soc-ph cs.CL cs.DL cs.SI published:2013-05-23 summary:We present, visualize and analyse the similarities and differences betweenthe controversial topics related to "edit wars" identified in 10 differentlanguage versions of Wikipedia. After a brief review of the related work wedescribe the methods developed to locate, measure, and categorize thecontroversial topics in the different languages. Visualizations of the degreeof overlap between the top 100 lists of most controversial articles indifferent languages and the content related to geographical locations will bepresented. We discuss what the presented analysis and visualizations can tellus about the multicultural aspects of Wikipedia and practices ofpeer-production. Our results indicate that Wikipedia is more than just anencyclopaedia; it is also a window into convergent and divergent social-spatialpriorities, interests and preferences.
arxiv-3600-51 | A Supervised Neural Autoregressive Topic Model for Simultaneous Image Classification and Annotation | http://arxiv.org/abs/1305.5306 | author:Yin Zheng, Yu-Jin Zhang, Hugo Larochelle category:cs.CV cs.LG stat.ML published:2013-05-23 summary:Topic modeling based on latent Dirichlet allocation (LDA) has been aframework of choice to perform scene recognition and annotation. Recently, anew type of topic model called the Document Neural Autoregressive DistributionEstimator (DocNADE) was proposed and demonstrated state-of-the-art performancefor document modeling. In this work, we show how to successfully apply andextend this model to the context of visual scene modeling. Specifically, wepropose SupDocNADE, a supervised extension of DocNADE, that increases thediscriminative power of the hidden topic features by incorporating labelinformation into the training objective of the model. We also describe how toleverage information about the spatial position of the visual words and how toembed additional image annotations, so as to simultaneously perform imageclassification and annotation. We test our model on the Scene15, LabelMe andUIUC-Sports datasets and show that it compares favorably to other topic modelssuch as the supervised variant of LDA.
arxiv-3600-52 | PAWL-Forced Simulated Tempering | http://arxiv.org/abs/1305.5017 | author:Luke Bornn category:stat.CO stat.ML published:2013-05-22 summary:In this short note, we show how the parallel adaptive Wang-Landau (PAWL)algorithm of Bornn et al. (2013) can be used to automate and improve simulatedtempering algorithms. While Wang-Landau and other stochastic approximationmethods have frequently been applied within the simulated tempering framework,this note demonstrates through a simple example the additional improvementsbrought about by parallelization, adaptive proposals and automated binsplitting.
arxiv-3600-53 | A novel automatic thresholding segmentation method with local adaptive thresholds | http://arxiv.org/abs/1305.5160 | author:Bo Xiao, Yuefeng Jing, Yonghong Guan category:cs.CV published:2013-05-22 summary:A novel method for segmenting bright objects from dark background forgrayscale image is proposed. The concept of this method can be stated simplyas: to pick out the local-thinnest bands on the grayscale grade-map. It turnsout to be a threshold-based method with local adaptive thresholds, where eachlocal threshold is determined by requiring the average normal-directiongradient on the object boundary to be local minimal. The method is highlyautomatic and the segmentation mimics a man's natural expectation even theobject boundaries are fuzzy.
arxiv-3600-54 | Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates | http://arxiv.org/abs/1305.5029 | author:Yuchen Zhang, John C. Duchi, Martin J. Wainwright category:math.ST cs.LG stat.ML stat.TH published:2013-05-22 summary:We establish optimal convergence rates for a decomposition-based scalableapproach to kernel ridge regression. The method is simple to describe: itrandomly partitions a dataset of size N into m subsets of equal size, computesan independent kernel ridge regression estimator for each subset, then averagesthe local solutions into a global predictor. This partitioning leads to asubstantial reduction in computation time versus the standard approach ofperforming kernel ridge regression on all N samples. Our two main theoremsestablish that despite the computational speed-up, statistical optimality isretained: as long as m is not too large, the partition-based estimator achievesthe statistical minimax rate over all estimators using the set of N samples. Asconcrete examples, our theory guarantees that the number of processors m maygrow nearly linearly for finite-rank kernels and Gaussian kernels andpolynomially in N for Sobolev spaces, which in turn allows for substantialreductions in computational cost. We conclude with experiments on bothsimulated data and a music-prediction task that complement our theoreticalresults, exhibiting the computational and statistical benefits of our approach.
arxiv-3600-55 | A Comparison of Random Forests and Ferns on Recognition of Instruments in Jazz Recordings | http://arxiv.org/abs/1305.5078 | author:Alicja A. Wieczorkowska, Miron B. Kursa category:cs.LG cs.IR cs.SD published:2013-05-22 summary:In this paper, we first apply random ferns for classification of real musicrecordings of a jazz band. No initial segmentation of audio data is assumed,i.e., no onset, offset, nor pitch data are needed. The notion of random fernsis described in the paper, to familiarize the reader with this classificationalgorithm, which was introduced quite recently and applied so far in imagerecognition tasks. The performance of random ferns is compared with randomforests for the same data. The results of experiments are presented in thepaper, and conclusions are drawn.
arxiv-3600-56 | Robust Logistic Regression using Shift Parameters (Long Version) | http://arxiv.org/abs/1305.4987 | author:Julie Tibshirani, Christopher D. Manning category:cs.AI cs.LG stat.ML published:2013-05-21 summary:Annotation errors can significantly hurt classifier performance, yet datasetsare only growing noisier with the increased use of Amazon Mechanical Turk andtechniques like distant supervision that automatically generate labels. In thispaper, we present a robust extension of logistic regression that incorporatesthe possibility of mislabelling directly into the objective. Our model can betrained through nearly the same means as logistic regression, and retains itsefficiency on high-dimensional datasets. Through named entity recognitionexperiments, we demonstrate that our approach can provide a significantimprovement over the standard model when annotation errors are present.
arxiv-3600-57 | Power to the Points: Validating Data Memberships in Clusterings | http://arxiv.org/abs/1305.4757 | author:Parasaran Raman, Suresh Venkatasubramanian category:cs.LG cs.CG published:2013-05-21 summary:A clustering is an implicit assignment of labels of points, based onproximity to other points. It is these labels that are then used for downstreamanalysis (either focusing on individual clusters, or identifyingrepresentatives of clusters and so on). Thus, in order to trust a clustering asa first step in exploratory data analysis, we must trust the labels assigned toindividual data. Without supervision, how can we validate this assignment? Inthis paper, we present a method to attach affinity scores to the implicitlabels of individual points in a clustering. The affinity scores capture theconfidence level of the cluster that claims to "own" the point. This method isvery general: it can be used with clusterings derived from Euclidean data,kernelized data, or even data derived from information spaces. It smoothlyincorporates importance functions on clusters, allowing us to eight differentclusters differently. It is also efficient: assigning an affinity score to apoint depends only polynomially on the number of clusters and is independent ofthe number of points in the data. The dimensionality of the underlying spaceonly appears in preprocessing. We demonstrate the value of our approach with anexperimental study that illustrates the use of these scores in different dataanalysis tasks, as well as the efficiency and flexibility of the method. Wealso demonstrate useful visualizations of these scores; these might proveuseful within an interactive analytics framework.
arxiv-3600-58 | Zero-sum repeated games: Counterexamples to the existence of the asymptotic value and the conjecture $\operatorname{maxmin}=\operatorname{lim}v_n$ | http://arxiv.org/abs/1305.4778 | author:Bruno Ziliotto category:math.OC cs.LG published:2013-05-21 summary:Mertens [In Proceedings of the International Congress of Mathematicians(Berkeley, Calif., 1986) (1987) 1528-1577 Amer. Math. Soc.] proposed twogeneral conjectures about repeated games: the first one is that, in anytwo-person zero-sum repeated game, the asymptotic value exists, and the secondone is that, when Player 1 is more informed than Player 2, in the long runPlayer 1 is able to guarantee the asymptotic value. We disprove these twolong-standing conjectures by providing an example of a zero-sum repeated gamewith public signals and perfect observation of the actions, where the value ofthe $\lambda$-discounted game does not converge when $\lambda$ goes to 0. Theaforementioned example involves seven states, two actions and two signals foreach player. Remarkably, players observe the payoffs, and play in turn.
arxiv-3600-59 | Improving NSGA-II with an Adaptive Mutation Operator | http://arxiv.org/abs/1305.4947 | author:Arthur Carvalho, Aluizio F. R. Araujo category:cs.NE published:2013-05-21 summary:The performance of a Multiobjective Evolutionary Algorithm (MOEA) iscrucially dependent on the parameter setting of the operators. The most desiredcontrol of such parameters presents the characteristic of adaptiveness, i.e.,the capacity of changing the value of the parameter, in distinct stages of theevolutionary process, using feedbacks from the search for determining thedirection and/or magnitude of changing. Given the great popularity of thealgorithm NSGA-II, the objective of this research is to create adaptivecontrols for each parameter existing in this MOEA. With these controls, weexpect to improve even more the performance of the algorithm. In this work, we propose an adaptive mutation operator that has an adaptivecontrol which uses information about the diversity of candidate solutions forcontrolling the magnitude of the mutation. A number of experiments consideringdifferent problems suggest that this mutation operator improves the ability ofthe NSGA-II for reaching the Pareto optimal Front and for getting a betterdiversity among the final solutions.
arxiv-3600-60 | A Data Mining Approach to Solve the Goal Scoring Problem | http://arxiv.org/abs/1305.4955 | author:Renato Oliveira, Paulo Adeodato, Arthur Carvalho, Icamaan Viegas, Christian Diego, Tsang Ing-Ren category:cs.AI cs.LG published:2013-05-21 summary:In soccer, scoring goals is a fundamental objective which depends on manyconditions and constraints. Considering the RoboCup soccer 2D-simulator, thispaper presents a data mining-based decision system to identify the best timeand direction to kick the ball towards the goal to maximize the overall chancesof scoring during a simulated soccer match. Following the CRISP-DM methodology,data for modeling were extracted from matches of major internationaltournaments (10691 kicks), knowledge about soccer was embedded viatransformation of variables and a Multilayer Perceptron was used to estimatethe scoring chance. Experimental performance assessment to compare thisapproach against previous LDA-based approach was conducted from 100 matches.Several statistical metrics were used to analyze the performance of the systemand the results showed an increase of 7.7% in the number of kicks, producing anoverall increase of 78% in the number of goals scored.
arxiv-3600-61 | Out-of-sample Extension for Latent Position Graphs | http://arxiv.org/abs/1305.4893 | author:Minh Tang, Youngser Park, Carey E. Priebe category:stat.ML published:2013-05-21 summary:We consider the problem of vertex classification for graphs constructed fromthe latent position model. It was shown previously that the approach ofembedding the graphs into some Euclidean space followed by classification inthat space can yields a universally consistent vertex classifier. However, amajor technical difficulty of the approach arises when classifying unlabeledout-of-sample vertices without including them in the embedding stage. In thispaper, we studied the out-of-sample extension for the graph embedding step andits impact on the subsequent inference tasks. We show that, under the latentposition graph model and for sufficiently large $n$, the mapping of theout-of-sample vertices is close to its true latent position. We thendemonstrate that successful inference for the out-of-sample vertices ispossible.
arxiv-3600-62 | On the Complexity Analysis of Randomized Block-Coordinate Descent Methods | http://arxiv.org/abs/1305.4723 | author:Zhaosong Lu, Lin Xiao category:math.OC cs.LG cs.NA math.NA stat.ML published:2013-05-21 summary:In this paper we analyze the randomized block-coordinate descent (RBCD)methods proposed in [8,11] for minimizing the sum of a smooth convex functionand a block-separable convex function. In particular, we extend Nesterov'stechnique developed in [8] for analyzing the RBCD method for minimizing asmooth convex function over a block-separable closed convex set to theaforementioned more general problem and obtain a sharper expected-value type ofconvergence rate than the one implied in [11]. Also, we obtain a betterhigh-probability type of iteration complexity, which improves upon the one in[11] by at least the amount $O(n/\epsilon)$, where $\epsilon$ is the targetsolution accuracy and $n$ is the number of problem blocks. In addition, forunconstrained smooth convex minimization, we develop a new technique called{\it randomized estimate sequence} to analyze the accelerated RBCD methodproposed by Nesterov [11] and establish a sharper expected-value type ofconvergence rate than the one given in [11].
arxiv-3600-63 | Random crossings in dependency trees | http://arxiv.org/abs/1305.4561 | author:Ramon Ferrer-i-Cancho category:cs.CL cs.DM cs.SI physics.soc-ph published:2013-05-20 summary:It has been hypothesized that the rather small number of crossings in realsyntactic dependency trees is a side-effect of pressure for dependency lengthminimization. Here we answer a related important research question: what wouldbe the expected number of crossings if the natural order of a sentence waslost? We show that this number depends only on the number of vertices of thedependency tree (the sentence length) and the second moment of vertex degrees.The expected number of crossings is minimum for a star tree (crossings areimpossible) and maximum for a linear tree (the number of crossings is of theorder of the square of the sequence length).
arxiv-3600-64 | Object Detection with Pixel Intensity Comparisons Organized in Decision Trees | http://arxiv.org/abs/1305.4537 | author:Nenad Markuš, Miroslav Frljak, Igor S. Pandžić, Jörgen Ahlberg, Robert Forchheimer category:cs.CV published:2013-05-20 summary:We describe a method for visual object detection based on an ensemble ofoptimized decision trees organized in a cascade of rejectors. The trees usepixel intensity comparisons in their internal nodes and this makes them able toprocess image regions very fast. Experimental analysis is provided through aface detection problem. The obtained results are encouraging and demonstratethat the method has practical value. Additionally, we analyse its sensitivityto noise and show how to perform fast rotation invariant object detection.Complete source code is provided at https://github.com/nenadmarkus/pico.
arxiv-3600-65 | Efficient Image Retargeting for High Dynamic Range Scenes | http://arxiv.org/abs/1305.4544 | author:Govind Salvi, Puneet Sharma, Shanmuganathan Raman category:cs.CV published:2013-05-20 summary:Most of the real world scenes have a very high dynamic range (HDR). Themobile phone cameras and the digital cameras available in markets are limitedin their capability in both the range and spatial resolution. Same argument canbe posed about the limited dynamic range display devices which also differ inthe spatial resolution and aspect ratios. In this paper, we address the problem of displaying the high contrast lowdynamic range (LDR) image of a HDR scene in a display device which hasdifferent spatial resolution compared to that of the capturing digital camera.The optimal solution proposed in this work can be employed with any camerawhich has the ability to shoot multiple differently exposed images of a scene.Further, the proposed solutions provide the flexibility in the depiction ofentire contrast of the HDR scene as a LDR image with an user specified spatialresolution. This task is achieved through an optimized content awareretargeting framework which preserves salient features along with the algorithmto combine multi-exposure images. We show the proposed approach performsexceedingly well in the generation of high contrast LDR image of varyingspatial resolution compared to an alternate approach.
arxiv-3600-66 | Robustness of Random Forest-based gene selection methods | http://arxiv.org/abs/1305.4525 | author:Miron B. Kursa category:cs.LG q-bio.QM published:2013-05-20 summary:Gene selection is an important part of microarray data analysis because itprovides information that can lead to a better mechanistic understanding of aninvestigated phenomenon. At the same time, gene selection is very difficultbecause of the noisy nature of microarray data. As a consequence, geneselection is often performed with machine learning methods. The Random Forestmethod is particularly well suited for this purpose. In this work, fourstate-of-the-art Random Forest-based feature selection methods were compared ina gene selection context. The analysis focused on the stability of selectionbecause, although it is necessary for determining the significance of results,it is often ignored in similar studies. The comparison of post-selection accuracy in the validation of Random Forestclassifiers revealed that all investigated methods were equivalent in thiscontext. However, the methods substantially differed with respect to the numberof selected genes and the stability of selection. Of the analysed methods, theBoruta algorithm predicted the most genes as potentially important. The post-selection classifier error rate, which is a frequently used measure,was found to be a potentially deceptive measure of gene selection quality. Whenthe number of consistently selected genes was considered, the Boruta algorithmwas clearly the best. Although it was also the most computationally intensivemethod, the Boruta algorithm's computational demands could be reduced to levelscomparable to those of other algorithms by replacing the Random Forestimportance with a comparable measure from Random Ferns (a similar butsimplified classifier). Despite their design assumptions, the minimal optimalselection methods, were found to select a high fraction of false positives.
arxiv-3600-67 | Meta Path-Based Collective Classification in Heterogeneous Information Networks | http://arxiv.org/abs/1305.4433 | author:Xiangnan Kong, Bokai Cao, Philip S. Yu, Ying Ding, David J. Wild category:cs.LG stat.ML published:2013-05-20 summary:Collective classification has been intensively studied due to its impact inmany important applications, such as web mining, bioinformatics and citationanalysis. Collective classification approaches exploit the dependencies of agroup of linked objects whose class labels are correlated and need to bepredicted simultaneously. In this paper, we focus on studying the collectiveclassification problem in heterogeneous networks, which involves multiple typesof data objects interconnected by multiple types of links. Intuitively, twoobjects are correlated if they are linked by many paths in the network.However, most existing approaches measure the dependencies among objectsthrough directly links or indirect links without considering the differentsemantic meanings behind different paths. In this paper, we study thecollective classification problem taht is defined among the same type ofobjects in heterogenous networks. Moreover, by considering different linkagepaths in the network, one can capture the subtlety of different types ofdependencies among objects. We introduce the concept of meta-path baseddependencies among objects, where a meta path is a path consisting a certainsequence of linke types. We show that the quality of collective classificationresults strongly depends upon the meta paths used. To accommodate the largenetwork size, a novel solution, called HCC (meta-path based HeterogenousCollective Classification), is developed to effectively assign labels to agroup of instances that are interconnected through different meta-paths. Theproposed HCC model can capture different types of dependencies among objectswith respect to different meta paths. Empirical studies on real-world networksdemonstrate that effectiveness of the proposed meta path-based collectiveclassification approach.
arxiv-3600-68 | Ensembles of Classifiers based on Dimensionality Reduction | http://arxiv.org/abs/1305.4345 | author:Alon Schclar, Lior Rokach, Amir Amit category:cs.LG published:2013-05-19 summary:We present a novel approach for the construction of ensemble classifiersbased on dimensionality reduction. Dimensionality reduction methods representdatasets using a small number of attributes while preserving the informationconveyed by the original dataset. The ensemble members are trained based ondimension-reduced versions of the training set. These versions are obtained byapplying dimensionality reduction to the original training set using differentvalues of the input parameters. This construction meets both the diversity andaccuracy criteria which are required to construct an ensemble classifier wherethe former criterion is obtained by the various input parameter values and thelatter is achieved due to the decorrelation and noise reduction properties ofdimensionality reduction. In order to classify a test sample, it is firstembedded into the dimension reduced space of each individual classifier byusing an out-of-sample extension algorithm. Each classifier is then applied tothe embedded sample and the classification is obtained via a voting scheme. Wepresent three variations of the proposed approach based on the RandomProjections, the Diffusion Maps and the Random Subspaces dimensionalityreduction algorithms. We also present a multi-strategy ensemble which combinesAdaBoost and Diffusion Maps. A comparison is made with the Bagging, AdaBoost,Rotation Forest ensemble classifiers and also with the base classifier whichdoes not incorporate dimensionality reduction. Our experiments used seventeenbenchmark datasets from the UCI repository. The results obtained by theproposed algorithms were superior in many cases to other algorithms.
arxiv-3600-69 | Horizon-Independent Optimal Prediction with Log-Loss in Exponential Families | http://arxiv.org/abs/1305.4324 | author:Peter Bartlett, Peter Grunwald, Peter Harremoes, Fares Hedayati, Wojciech Kotlowski category:cs.LG stat.ML published:2013-05-19 summary:We study online learning under logarithmic loss with regular parametricmodels. Hedayati and Bartlett (2012b) showed that a Bayesian predictionstrategy with Jeffreys prior and sequential normalized maximum likelihood(SNML) coincide and are optimal if and only if the latter is exchangeable, andif and only if the optimal strategy can be calculated without knowing the timehorizon in advance. They put forward the question what families haveexchangeable SNML strategies. This paper fully answers this open problem forone-dimensional exponential families. The exchangeability can happen only forthree classes of natural exponential family distributions, namely the Gaussian,Gamma, and the Tweedie exponential family of order 3/2. Keywords: SNMLExchangeability, Exponential Family, Online Learning, Logarithmic Loss,Bayesian Strategy, Jeffreys Prior, Fisher Information1
arxiv-3600-70 | Generalized Centroid Estimators in Bioinformatics | http://arxiv.org/abs/1305.4339 | author:Michiaki Hamada, Hisanori Kiryu, Wataru Iwasaki, Kiyoshi Asai category:q-bio.QM cs.LG published:2013-05-19 summary:In a number of estimation problems in bioinformatics, accuracy measures ofthe target problem are usually given, and it is important to design estimatorsthat are suitable to those accuracy measures. However, there is often adiscrepancy between an employed estimator and a given accuracy measure of theproblem. In this study, we introduce a general class of efficient estimatorsfor estimation problems on high-dimensional binary spaces, which representmanyfundamental problems in bioinformatics. Theoretical analysis reveals that theproposed estimators generally fit with commonly-used accuracy measures (e.g.sensitivity, PPV, MCC and F-score) as well as it can be computed efficiently inmany cases, and cover a wide range of problems in bioinformatics from theviewpoint of the principle of maximum expected accuracy (MEA). It is also shownthat some important algorithms in bioinformatics can be interpreted in aunified manner. Not only the concept presented in this paper gives a usefulframework to design MEA-based estimators but also it is highly extendable andsheds new light on many problems in bioinformatics.
arxiv-3600-71 | Quantum Annealing for Dirichlet Process Mixture Models with Applications to Network Clustering | http://arxiv.org/abs/1305.4325 | author:Issei Sato, Shu Tanaka, Kenichi Kurihara, Seiji Miyashita, Hiroshi Nakagawa category:quant-ph stat.ML published:2013-05-19 summary:We developed a new quantum annealing (QA) algorithm for Dirichlet processmixture (DPM) models based on the Chinese restaurant process (CRP). QA is aparallelized extension of simulated annealing (SA), i.e., it is a parallelstochastic optimization technique. Existing approaches [Kurihara et al.UAI2009, Sato et al. UAI2009] and cannot be applied to the CRP because their QAframework is formulated using a fixed number of mixture components. Theproposed QA algorithm can handle an unfixed number of classes in mixturemodels. We applied QA to a DPM model for clustering vertices in a network wherea CRP seating arrangement indicates a network partition. A multi core processorwas used for running QA in experiments, the results of which show that QA isbetter than SA, Markov chain Monte Carlo inference, and beam search at findinga maximum a posteriori estimation of a seating arrangement in the CRP. Sinceour QA algorithm is as easy as to implement the SA algorithm, it is suitablefor a wide range of applications.
arxiv-3600-72 | Blockwise SURE Shrinkage for Non-Local Means | http://arxiv.org/abs/1305.4298 | author:Yue Wu, Brian Tracey, Premkumar Natarajan, Joseph P. Noonan category:cs.CV published:2013-05-18 summary:In this letter, we investigate the shrinkage problem for the non-local means(NLM) image denoising. In particular, we derive the closed-form of the optimalblockwise shrinkage for NLM that minimizes the Stein's unbiased risk estimator(SURE). We also propose a constant complexity algorithm allowing fast blockwiseshrinkage. Simulation results show that the proposed blockwise shrinkage methodimproves NLM performance in attaining higher peak signal noise ratio (PSNR) andstructural similarity index (SSIM), and makes NLM more robust against parameterchanges. Similar ideas can be applicable to other patchwise image denoisingtechniques.
arxiv-3600-73 | Dynamic Covariance Models for Multivariate Financial Time Series | http://arxiv.org/abs/1305.4268 | author:Yue Wu, José Miguel Hernández-Lobato, Zoubin Ghahramani category:stat.ME stat.ML published:2013-05-18 summary:The accurate prediction of time-changing covariances is an important problemin the modeling of multivariate financial data. However, some of the mostpopular models suffer from a) overfitting problems and multiple local optima,b) failure to capture shifts in market conditions and c) large computationalcosts. To address these problems we introduce a novel dynamic model fortime-changing covariances. Over-fitting and local optima are avoided byfollowing a Bayesian approach instead of computing point estimates. Changes inmarket conditions are captured by assuming a diffusion process in parametervalues, and finally computationally efficient and scalable inference isperformed using particle filters. Experiments with financial data showexcellent performance of the proposed method with respect to current standardmodels.
arxiv-3600-74 | Embedding Riemannian Manifolds by the Heat Kernel of the Connection Laplacian | http://arxiv.org/abs/1305.4232 | author:Hau-tieng Wu category:math.DG math.SP math.ST stat.ML stat.TH published:2013-05-18 summary:Given a class of closed Riemannian manifolds with prescribed geometricconditions, we introduce an embedding of the manifolds into $\ell^2$ based onthe heat kernel of the Connection Laplacian associated with the Levi-Civitaconnection on the tangent bundle. As a result, we can construct a distance inthis class which leads to a pre-compactness theorem on the class underconsideration.
arxiv-3600-75 | Contractive De-noising Auto-encoder | http://arxiv.org/abs/1305.4076 | author:Fu-qiang Chen, Yan Wu, Guo-dong Zhao, Jun-ming Zhang, Ming Zhu, Jing Bai category:cs.LG published:2013-05-17 summary:Auto-encoder is a special kind of neural network based on reconstruction.De-noising auto-encoder (DAE) is an improved auto-encoder which is robust tothe input by corrupting the original data first and then reconstructing theoriginal input by minimizing the reconstruction error function. And contractiveauto-encoder (CAE) is another kind of improved auto-encoder to learn robustfeature by introducing the Frobenius norm of the Jacobean matrix of the learnedfeature with respect to the original input. In this paper, we combinede-noising auto-encoder and contractive auto- encoder, and propose anotherimproved auto-encoder, contractive de-noising auto- encoder (CDAE), which isrobust to both the original input and the learned feature. We stack CDAE toextract more abstract features and apply SVM for classification. The experimentresult on benchmark dataset MNIST shows that our proposed CDAE performed betterthan both DAE and CAE, proving the effective of our method.
arxiv-3600-76 | Factored expectation propagation for input-output FHMM models in systems biology | http://arxiv.org/abs/1305.4153 | author:Botond Cseke, Guido Sanguinetti category:stat.ML published:2013-05-17 summary:We consider the problem of joint modelling of metabolic signals and geneexpression in systems biology applications. We propose an approach based oninput-output factorial hidden Markov models and propose a structuredvariational inference approach to infer the structure and states of the model.We start from the classical free form structured variational mean fieldapproach and use a expectation propagation to approximate the expectationsneeded in the variational loop. We show that this corresponds to a factoredexpectation constrained approximate inference. We validate our model throughextensive simulations and demonstrate its applicability on a real worldbacterial data set.
arxiv-3600-77 | Machine learning on images using a string-distance | http://arxiv.org/abs/1305.4204 | author:Uzi Chester, Joel Ratsaby category:cs.LG cs.CV published:2013-05-17 summary:We present a new method for image feature-extraction which is based onrepresenting an image by a finite-dimensional vector of distances that measurehow different the image is from a set of image prototypes. We use the recentlyintroduced Universal Image Distance (UID) \cite{RatsabyChesterIEEE2012} tocompare the similarity between an image and a prototype image. The advantage inusing the UID is the fact that no domain knowledge nor any image analysis needto be done. Each image is represented by a finite dimensional feature vectorwhose components are the UID values between the image and a finite set of imageprototypes from each of the feature categories. The method is automatic sinceonce the user selects the prototype images, the feature vectors areautomatically calculated without the need to do any image analysis. Theprototype images can be of different size, in particular, different than theimage size. Based on a collection of such cases any supervised or unsupervisedlearning algorithm can be used to train and produce an image classifier orimage cluster analysis. In this paper we present the image feature-extractionmethod and use it on several supervised and unsupervised learning experimentsfor satellite image data.
arxiv-3600-78 | Indexing Medical Images based on Collaborative Experts Reports | http://arxiv.org/abs/1305.4077 | author:Abir Messaoudi, Riadh Bouslimi, Jalel Akaichi category:cs.CV cs.IR published:2013-05-17 summary:A patient is often willing to quickly get, from his physician, reliableanalysis and concise explanation according to provided linked medical images.The fact of making choices individually by the patient's physician may lead tomalpractices and consequently generates unforeseeable damages. The Institute ofMedicine of the National Sciences Academy(IMNAS) in USA published a studyestimating that up to 98,000 hospital deathseach year can be attributed tomedical malpractice [1]. Moreover, physician, in charge of medical imageanalysis, might be unavailable at the right time, which may complicate thepatient's state. The goal of this paper is to provide to physicians andpatients, a social network that permits to foster cooperation and to overcomethe problem of unavailability of doctors on site any time. Therefore, patientscan submit their medical images to be diagnosed and commented by severalexperts instantly. Consequently, the need to process opinions and to extractinformation automatically from the proposed social network became a necessitydue to the huge number of comments expressing specialist's reviews. For thisreason, we propose a kind of comments' summary keywords-based method whichextracts the major current terms and relevant words existing on physicians'annotations. The extracted keywords will present a new and robust method forimage indexation. In fact, significant extracted terms will be used later toindex images in order to facilitate their discovery for any appropriate use. Toovercome this challenge, we propose our Terminology Extraction of Annotation(TEA) mixed approach which focuses on algorithms mainly based on statisticalmethods and on external semantic resources.
arxiv-3600-79 | Sparse Approximate Inference for Spatio-Temporal Point Process Models | http://arxiv.org/abs/1305.4152 | author:Botond Cseke, Andrew Zammit Mangion, Tom Heskes, Guido Sanguinetti category:stat.ML published:2013-05-17 summary:Spatio-temporal point process models play a central role in the analysis ofspatially distributed systems in several disciplines. Yet, scalable inferenceremains computa- tionally challenging both due to the high resolution modellinggenerally required and the analytically intractable likelihood function. Here,we exploit the sparsity structure typical of (spatially) discretisedlog-Gaussian Cox process models by using approximate message-passingalgorithms. The proposed algorithms scale well with the state dimension and thelength of the temporal horizon with moderate loss in distributional accuracy.They hence provide a flexible and faster alternative to both non-linearfiltering-smoothing type algorithms and to approaches that implement theLaplace method or expectation propagation on (block) sparse latent Gaussianmodels. We infer the parameters of the latent Gaussian model using a structuredvariational Bayes approach. We demonstrate the proposed framework on simulationstudies with both Gaussian and point-process observations and use it toreconstruct the conflict intensity and dynamics in Afghanistan from theWikiLeaks Afghan War Diary.
arxiv-3600-80 | Flying Triangulation - towards the 3D movie camera | http://arxiv.org/abs/1305.4168 | author:Florian Willomitzer, Svenja Ettl, Christian Faber, Gerd Häusler category:cs.CV physics.optics published:2013-05-17 summary:Flying Triangulation sensors enable a free-hand and motion-robust 3D dataacquisition of complex shaped objects. The measurement principle is based on amulti-line light-sectioning approach and uses sophisticated algorithms forreal-time registration (S. Ettl et al., Appl. Opt. 51 (2012) 281-289). As"single-shot principle", light sectioning enables the option to get surfacedata from one single camera exposure. But there is a drawback: A pixel-densemeasurement is not possible because of fundamental information-theoreticalreasons. By "pixel-dense" we understand that each pixel displays individuallymeasured distance information, neither interpolated from its neighbour pixelsnor using lateral context information. Hence, for monomodal single-shotprinciples, the 3D data generated from one 2D raw image display a significantlylower space-bandwidth than the camera permits. This is the price one must payfor motion robustness. Currently, our sensors project about 10 lines (each with1000 pixels), reaching an considerable lower data efficiency than theoreticallypossible for a single-shot sensor. Our aim is to push Flying Triangulation toits information-theoretical limits. Therefore, the line density as well as themeasurement depth needs to be significantly increased. This causes seriousindexing ambiguities. On the road to a single-shot 3D movie camera, we areworking on solutions to overcome the problem of false line indexing byutilizing yet unexploited information. We will present several approaches andwill discuss profound information-theoretical questions about the informationefficiency of 3D sensors.
arxiv-3600-81 | Font Acknowledgment and Character Extraction of Digital and Scanned Images | http://arxiv.org/abs/1305.4064 | author:Syed Muhammad Arsalan Bashir category:cs.CV published:2013-05-17 summary:The font recognition and character extraction is of immense importance asthese are many scenarios where data are in such a form, which cannot beprocessed like in image form or as a hard copy. So the procedure developed inthis paper is basically related to identifying the font (Times New Roman, Arialand Comic Sans MS) and afterwards recovering the text using simple correlationbased method where the binary templates are correlated to the input image textcharacters. All of this extraction is done in the presence of a little noise asimages may have noisy patterns due to photocopying. The significance of thismethod exists in extraction of data from various monitoring (Surveillance)camera footages or even more. The method is developed on Matlab\c{opyright}which takes input image and recovers text and font information from it in atext file.
arxiv-3600-82 | Sparse Norm Filtering | http://arxiv.org/abs/1305.3971 | author:Chengxi Ye, Dacheng Tao, Mingli Song, David W. Jacobs, Min Wu category:cs.GR cs.CV cs.MM published:2013-05-17 summary:Optimization-based filtering smoothes an image by minimizing a fidelityfunction and simultaneously preserves edges by exploiting a sparse norm penaltyover gradients. It has obtained promising performance in practical problems,such as detail manipulation, HDR compression and deblurring, and thus hasreceived increasing attentions in fields of graphics, computer vision and imageprocessing. This paper derives a new type of image filter called sparse normfilter (SNF) from optimization-based filtering. SNF has a very simple form,introduces a general class of filtering techniques, and explains severalclassic filters as special implementations of SNF, e.g. the averaging filterand the median filter. It has advantages of being halo free, easy to implement,and low time and memory costs (comparable to those of the bilateral filter).Thus, it is more generic than a smoothing operator and can better adapt todifferent tasks. We validate the proposed SNF by a wide variety of applicationsincluding edge-preserving smoothing, outlier tolerant filtering, detailmanipulation, HDR compression, non-blind deconvolution, image segmentation, andcolorization.
arxiv-3600-83 | Binary Tree based Chinese Word Segmentation | http://arxiv.org/abs/1305.3981 | author:Kaixu Zhang, Can Wang, Maosong Sun category:cs.CL published:2013-05-17 summary:Chinese word segmentation is a fundamental task for Chinese languageprocessing. The granularity mismatch problem is the main cause of the errors.This paper showed that the binary tree representation can store outputs withdifferent granularity. A binary tree based framework is also designed toovercome the granularity mismatch problem. There are two steps in thisframework, namely tree building and tree pruning. The tree pruning step isspecially designed to focus on the granularity problem. Previous work forChinese word segmentation such as the sequence tagging can be easily employedin this framework. This framework can also provide quantitative error analysismethods. The experiments showed that after using a more sophisticated treepruning function for a state-of-the-art conditional random field basedbaseline, the error reduction can be up to 20%.
arxiv-3600-84 | Conditions for Convergence in Regularized Machine Learning Objectives | http://arxiv.org/abs/1305.4081 | author:Patrick Hop, Xinghao Pan category:cs.LG cs.NA math.OC published:2013-05-17 summary:Analysis of the convergence rates of modern convex optimization algorithmscan be achived through binary means: analysis of emperical convergence, oranalysis of theoretical convergence. These two pathways of capturinginformation diverge in efficacy when moving to the world of distributedcomputing, due to the introduction of non-intuitive, non-linear slowdownsassociated with broadcasting, and in some cases, gathering operations. Despitethese nuances in the rates of convergence, we can still show the existence ofconvergence, and lower bounds for the rates. This paper will serve as a helpfulcheat-sheet for machine learning practitioners encountering this problem classin the field.
arxiv-3600-85 | Evolutionary optimization of an experimental apparatus | http://arxiv.org/abs/1305.4094 | author:I. Geisel, K. Cordes, J. Mahnke, S. Jöllenbeck, J. Ostermann, J. Arlt, W. Ertmer, C. Klempt category:quant-ph cs.NE published:2013-05-17 summary:In recent decades, cold atom experiments have become increasingly complex.While computers control most parameters, optimization is mostly done manually.This is a time-consuming task for a high-dimensional parameter space withunknown correlations. Here we automate this process using a genetic algorithmbased on Differential Evolution. We demonstrate that this algorithm optimizes21 correlated parameters and that it is robust against local maxima andexperimental noise. The algorithm is flexible and easy to implement. Thus, thepresented scheme can be applied to a wide range of experimental optimizationtasks.
arxiv-3600-86 | Analysis Of Interest Points Of Curvelet Coefficients Contributions Of Microscopic Images And Improvement Of Edges | http://arxiv.org/abs/1305.3939 | author:A. Djimeli, D. Tchiotsop, R. Tchinda category:cs.CV published:2013-05-16 summary:This paper focuses on improved edge model based on Curvelet coefficientsanalysis. Curvelet transform is a powerful tool for multiresolutionrepresentation of object with anisotropic edge. Curvelet coefficientscontributions have been analyzed using Scale Invariant Feature Transform(SIFT), commonly used to study local structure in images. The permutation ofCurvelet coefficients from original image and edges image obtained fromgradient operator is used to improve original edges. Experimental results showthat this method brings out details on edges when the decomposition scaleincreases.
arxiv-3600-87 | Multi-View Learning for Web Spam Detection | http://arxiv.org/abs/1305.3814 | author:Ali Hadian, Behrouz Minaei-Bidgoli category:cs.IR cs.LG published:2013-05-16 summary:Spam pages are designed to maliciously appear among the top search results byexcessive usage of popular terms. Therefore, spam pages should be removed usingan effective and efficient spam detection system. Previous methods for web spamclassification used several features from various information sources (pagecontents, web graph, access logs, etc.) to detect web spam. In this paper, wefollow page-level classification approach to build fast and scalable spamfilters. We show that each web page can be classified with satisfiable accuracyusing only its own HTML content. In order to design a multi-view classificationsystem, we used state-of-the-art spam classification methods with distinctfeature sets (views) as the base classifiers. Then, a fusion model is learnedto combine the output of the base classifiers and make final prediction.Results show that multi-view learning significantly improves the classificationperformance, namely AUC by 22%, while providing linear speedup for parallelexecution.
arxiv-3600-88 | Evolution of Covariance Functions for Gaussian Process Regression using Genetic Programming | http://arxiv.org/abs/1305.3794 | author:Gabriel Kronberger, Michael Kommenda category:cs.NE cs.LG stat.ML published:2013-05-16 summary:In this contribution we describe an approach to evolve composite covariancefunctions for Gaussian processes using genetic programming. A critical aspectof Gaussian processes and similar kernel-based models such as SVM is, that thecovariance function should be adapted to the modeled data. Frequently, thesquared exponential covariance function is used as a default. However, this canlead to a misspecified model, which does not fit the data well. In the proposedapproach we use a grammar for the composition of covariance functions andgenetic programming to search over the space of sentences that can be derivedfrom the grammar. We tested the proposed approach on synthetic data fromtwo-dimensional test functions, and on the Mauna Loa CO2 time series. Theresults show, that our approach is feasible, finding covariance functions thatperform much better than a default covariance function. For the CO2 data set acomposite covariance function is found, that matches the performance of ahand-tuned covariance function.
arxiv-3600-89 | Inferring the Origin Locations of Tweets with Quantitative Confidence | http://arxiv.org/abs/1305.3932 | author:Reid Priedhorsky, Aron Culotta, Sara Y. Del Valle category:cs.SI cs.HC cs.LG published:2013-05-16 summary:Social Internet content plays an increasingly critical role in many domains,including public health, disaster management, and politics. However, itsutility is limited by missing geographic information; for example, fewer than1.6% of Twitter messages (tweets) contain a geotag. We propose a scalable,content-based approach to estimate the location of tweets using a novel yetsimple variant of gaussian mixture models. Further, because real-worldapplications depend on quantified uncertainty for such estimates, we proposenovel metrics of accuracy, precision, and calibration, and we evaluate ourapproach accordingly. Experiments on 13 million global, comprehensivelymulti-lingual tweets show that our approach yields reliable, well-calibratedresults competitive with previous computationally intensive methods. We alsoshow that a relatively small number of training data are required for goodestimates (roughly 30,000 tweets) and models are quite time-invariant(effective on tweets many weeks newer than the training set). Finally, we showthat toponyms and languages with small geographic footprint provide the mostuseful location signals.
arxiv-3600-90 | Rule-Based Semantic Tagging. An Application Undergoing Dictionary Glosses | http://arxiv.org/abs/1305.3882 | author:Daniel Christen category:cs.CL published:2013-05-16 summary:The project presented in this article aims to formalize criteria andprocedures in order to extract semantic information from parsed dictionaryglosses. The actual purpose of the project is the generation of a semanticnetwork (nearly an ontology) issued from a monolingual Italian dictionary,through unsupervised procedures. Since the project involves rule-based Parsing,Semantic Tagging and Word Sense Disambiguation techniques, its outcomes mayfind an interest also beyond this immediate intent. The cooperation of bothsyntactic and semantic features in meaning construction are investigated, andprocedures which allows a translation of syntactic dependencies in semanticrelations are discussed. The procedures that rise from this project can beapplied also to other text types than dictionary glosses, as they convert theoutput of a parsing process into a semantic representation. In addition somemechanism are sketched that may lead to a kind of procedural semantics, throughwhich multiple paraphrases of an given expression can be generated. Which meansthat these techniques may find an application also in 'query expansion'strategies, interesting Information Retrieval, Search Engines and QuestionAnswering Systems.
arxiv-3600-91 | Geometric primitive feature extraction - concepts, algorithms, and applications | http://arxiv.org/abs/1305.3885 | author:Dilip K. Prasad category:cs.CV cs.CG published:2013-05-16 summary:This thesis presents important insights and concepts related to the topic ofthe extraction of geometric primitives from the edge contours of digitalimages. Three specific problems related to this topic have been studied, viz.,polygonal approximation of digital curves, tangent estimation of digitalcurves, and ellipse fitting anddetection from digital curves. For the problemof polygonal approximation, two fundamental problems have been addressed.First, the nature of the performance evaluation metrics in relation to thelocal and global fitting characteristics has been studied. Second, an expliciterror bound of the error introduced by digitizing a continuous line segment hasbeen derived and used to propose a generic non-heuristic parameter independentframework which can be used in several dominant point detection methods. Forthe problem of tangent estimation for digital curves, a simple method oftangent estimation has been proposed. It is shown that the method has adefinite upper bound of the error for conic digital curves. It has been shownthat the method performs better than almost all (seventy two) existing tangentestimation methods for conic as well as several non-conic digital curves. Forthe problem of fitting ellipses on digital curves, a geometric distanceminimization model has been considered. An unconstrained, linear,non-iterative, and numerically stable ellipse fitting method has been proposedand it has been shown that the proposed method has better selectivity forelliptic digital curves (high true positive and low false positive) as comparedto several other ellipse fitting methods. For the problem of detecting ellipsesin a set of digital curves, several innovative and fast pre-processing,grouping, and hypotheses evaluation concepts applicable for digital curves havebeen proposed and combined to form an ellipse detection method.
arxiv-3600-92 | Transfer Learning for Content-Based Recommender Systems using Tree Matching | http://arxiv.org/abs/1305.3384 | author:Naseem Biadsy, Lior Rokach, Armin Shmilovici category:cs.LG cs.IR published:2013-05-15 summary:In this paper we present a new approach to content-based transfer learningfor solving the data sparsity problem in cases when the users' preferences inthe target domain are either scarce or unavailable, but the necessaryinformation on the preferences exists in another domain. We show that traininga system to use such information across domains can produce better performance.Specifically, we represent users' behavior patterns based on topological graphstructures. Each behavior pattern represents the behavior of a set of users,when the users' behavior is defined as the items they rated and the items'rating values. In the next step we find a correlation between behavior patternsin the source domain and behavior patterns in the target domain. This mappingis considered a bridge between the two domains. Based on the correlation andcontent-attributes of the items, we train a machine learning model to predictusers' ratings in the target domain. When we compare our approach to thepopularity approach and KNN-cross-domain on a real world dataset, the resultsshow that on an average of 83$%$ of the cases our approach outperforms bothmethods.
arxiv-3600-93 | Noisy Subspace Clustering via Thresholding | http://arxiv.org/abs/1305.3486 | author:Reinhard Heckel, Helmut Bölcskei category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2013-05-15 summary:We consider the problem of clustering noisy high-dimensional data points intoa union of low-dimensional subspaces and a set of outliers. The number ofsubspaces, their dimensions, and their orientations are unknown. Aprobabilistic performance analysis of the thresholding-based subspaceclustering (TSC) algorithm introduced recently in [1] shows that TSC succeedsin the noisy case, even when the subspaces intersect. Our results reveal anexplicit tradeoff between the allowed noise level and the affinity of thesubspaces. We furthermore find that the simple outlier detection schemeintroduced in [1] provably succeeds in the noisy case.
arxiv-3600-94 | Online Learning in a Contract Selection Problem | http://arxiv.org/abs/1305.3334 | author:Cem Tekin, Mingyan Liu category:cs.LG cs.GT math.OC stat.ML published:2013-05-15 summary:In an online contract selection problem there is a seller which offers a setof contracts to sequentially arriving buyers whose types are drawn from anunknown distribution. If there exists a profitable contract for the buyer inthe offered set, i.e., a contract with payoff higher than the payoff of notaccepting any contracts, the buyer chooses the contract that maximizes itspayoff. In this paper we consider the online contract selection problem tomaximize the sellers profit. Assuming that a structural property called orderedpreferences holds for the buyer's payoff function, we propose online learningalgorithms that have sub-linear regret with respect to the best set ofcontracts given the distribution over the buyer's type. This problem has manyapplications including spectrum contracts, wireless service provider data plansand recommendation systems.
arxiv-3600-95 | Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data | http://arxiv.org/abs/1305.3640 | author:Jan-Willem van de Meent, Jonathan E. Bronson, Frank Wood, Ruben L. Gonzalez Jr., Chris H. Wiggins category:stat.ML physics.bio-ph q-bio.QM published:2013-05-15 summary:We address the problem of analyzing sets of noisy time-varying signals thatall report on the same process but confound straightforward analyses due tocomplex inter-signal heterogeneities and measurement artifacts. In particularwe consider single-molecule experiments which indirectly measure the distinctsteps in a biomolecular process via observations of noisy time-dependentsignals such as a fluorescence intensity or bead position. Straightforwardhidden Markov model (HMM) analyses attempt to characterize such processes interms of a set of conformational states, the transitions that can occur betweenthese states, and the associated rates at which those transitions occur; butrequire ad-hoc post-processing steps to combine multiple signals. Here wedevelop a hierarchically coupled HMM that allows experimentalists to deal withinter-signal variability in a principled and automatic way. Our approach is ageneralized expectation maximization hyperparameter point estimation procedurewith variational Bayes at the level of individual time series that learns ansingle interpretable representation of the overall data generating process.
arxiv-3600-96 | Modeling Information Propagation with Survival Theory | http://arxiv.org/abs/1305.3616 | author:Manuel Gomez Rodriguez, Jure Leskovec, Bernhard Schoelkopf category:cs.SI cs.DS physics.soc-ph stat.ML published:2013-05-15 summary:Networks provide a skeleton for the spread of contagions, like, information,ideas, behaviors and diseases. Many times networks over which contagionsdiffuse are unobserved and need to be inferred. Here we apply survival theoryto develop general additive and multiplicative risk models under which thenetwork inference problems can be solved efficiently by exploiting theirconvexity. Our additive risk model generalizes several existing networkinference models. We show all these models are particular cases of our moregeneral model. Our multiplicative model allows for modeling scenarios in whicha node can either increase or decrease the risk of activation of another node,in contrast with previous approaches, which consider only positive riskincrements. We evaluate the performance of our network inference algorithms onlarge synthetic and real cascade datasets, and show that our models are able topredict the length and duration of cascades in real data.
arxiv-3600-97 | Classification for Big Dataset of Bioacoustic Signals Based on Human Scoring System and Artificial Neural Network | http://arxiv.org/abs/1305.3633 | author:Mohammad Pourhomayoun, Peter Dugan, Marian Popescu, Denise Risch, Hal Lewis, Christopher Clark category:cs.CV published:2013-05-15 summary:In this paper, we propose a method to improve sound classificationperformance by combining signal features, derived from the time-frequencyspectrogram, with human perception. The method presented herein exploits anartificial neural network (ANN) and learns the signal features based on thehuman perception knowledge. The proposed method is applied to a large acousticdataset containing 24 months of nearly continuous recordings. The results showa significant improvement in performance of the detection-classificationsystem; yielding as much as 20% improvement in true positive rate for a givenfalse positive rate.
arxiv-3600-98 | Bioacoustic Signal Classification Based on Continuous Region Processing, Grid Masking and Artificial Neural Network | http://arxiv.org/abs/1305.3635 | author:Mohammad Pourhomayoun, Peter Dugan, Marian Popescu, Christopher Clark category:cs.CV published:2013-05-15 summary:In this paper, we develop a novel method based on machine-learning and imageprocessing to identify North Atlantic right whale (NARW) up-calls in thepresence of high levels of ambient and interfering noise. We apply a continuousregion algorithm on the spectrogram to extract the regions of interest, andthen use grid masking techniques to generate a small feature set that is thenused in an artificial neural network classifier to identify the NARW up-calls.It is shown that the proposed technique is effective in detecting and capturingeven very faint up-calls, in the presence of ambient and interfering noises.The method is evaluated on a dataset recorded in Massachusetts Bay, UnitedStates. The dataset includes 20000 sound clips for training, and 10000 soundclips for testing. The results show that the proposed technique can achieve anerror rate of less than FPR = 4.5% for a 90% true positive rate.
arxiv-3600-99 | Qualitative detection of oil adulteration with machine learning approaches | http://arxiv.org/abs/1305.3149 | author:Xiao-Bo Jin, Qiang Lu, Feng Wang, Quan-gong Huo category:cs.CE cs.LG published:2013-05-14 summary:The study focused on the machine learning analysis approaches to identify theadulteration of 9 kinds of edible oil qualitatively and answered the followingthree questions: Is the oil sample adulterant? How does it constitute? What isthe main ingredient of the adulteration oil? After extracting thehigh-performance liquid chromatography (HPLC) data on triglyceride from 370 oilsamples, we applied the adaptive boosting with multi-class Hamming loss(AdaBoost.MH) to distinguish the oil adulteration in contrast with the supportvector machine (SVM). Further, we regarded the adulterant oil and the pure oilsamples as ones with multiple labels and with only one label, respectively.Then multi-label AdaBoost.MH and multi-label learning vector quantization(ML-LVQ) model were built to determine the ingredients and their relative ratioin the adulteration oil. The experimental results on six measures show thatML-LVQ achieves better performance than multi-label AdaBoost.MH.
arxiv-3600-100 | Scalable Audience Reach Estimation in Real-time Online Advertising | http://arxiv.org/abs/1305.3014 | author:Ali Jalali, Santanu Kolay, Peter Foldes, Ali Dasdan category:cs.LG cs.DB published:2013-05-14 summary:Online advertising has been introduced as one of the most efficient methodsof advertising throughout the recent years. Yet, advertisers are concernedabout the efficiency of their online advertising campaigns and consequently,would like to restrict their ad impressions to certain websites and/or certaingroups of audience. These restrictions, known as targeting criteria, limit thereachability for better performance. This trade-off between reachability andperformance illustrates a need for a forecasting system that can quicklypredict/estimate (with good accuracy) this trade-off. Designing such a systemis challenging due to (a) the huge amount of data to process, and, (b) the needfor fast and accurate estimates. In this paper, we propose a distributed faulttolerant system that can generate such estimates fast with good accuracy. Themain idea is to keep a small representative sample in memory across multiplemachines and formulate the forecasting problem as queries against the sample.The key challenge is to find the best strata across the past data, performmultivariate stratified sampling while ensuring fuzzy fall-back to cover thesmall minorities. Our results show a significant improvement over the uniformand simple stratified sampling strategies which are currently widely used inthe industry.
arxiv-3600-101 | Real Time Bid Optimization with Smooth Budget Delivery in Online Advertising | http://arxiv.org/abs/1305.3011 | author:Kuang-Chih Lee, Ali Jalali, Ali Dasdan category:cs.GT cs.LG published:2013-05-14 summary:Today, billions of display ad impressions are purchased on a daily basisthrough a public auction hosted by real time bidding (RTB) exchanges. Adecision has to be made for advertisers to submit a bid for each selected RTBad request in milliseconds. Restricted by the budget, the goal is to buy a setof ad impressions to reach as many targeted users as possible. A desired action(conversion), advertiser specific, includes purchasing a product, filling out aform, signing up for emails, etc. In addition, advertisers typically prefer tospend their budget smoothly over the time in order to reach a wider range ofaudience accessible throughout a day and have a sustainable impact. However,since the conversions occur rarely and the occurrence feedback is normallydelayed, it is very challenging to achieve both budget and performance goals atthe same time. In this paper, we present an online approach to the smoothbudget delivery while optimizing for the conversion performance. Our algorithmtries to select high quality impressions and adjust the bid price based on theprior performance distribution in an adaptive manner by distributing the budgetoptimally across time. Our experimental results from real advertising campaignsdemonstrate the effectiveness of our proposed approach.
arxiv-3600-102 | Estimating or Propagating Gradients Through Stochastic Neurons | http://arxiv.org/abs/1305.2982 | author:Yoshua Bengio category:cs.LG published:2013-05-14 summary:Stochastic neurons can be useful for a number of reasons in deep learningmodels, but in many cases they pose a challenging problem: how to estimate thegradient of a loss function with respect to the input of such stochasticneurons, i.e., can we "back-propagate" through these stochastic neurons? Weexamine this question, existing approaches, and present two novel families ofsolutions, applicable in different settings. In particular, it is demonstratedthat a simple biologically plausible formula gives rise to an an unbiased (butnoisy) estimator of the gradient with respect to a binary stochastic neuronfiring probability. Unlike other estimators which view the noise as a smallperturbation in order to estimate gradients by finite differences, thisestimator is unbiased even without assuming that the stochastic perturbation issmall. This estimator is also interesting because it can be applied in verygeneral settings which do not allow gradient back-propagation, including theestimation of the gradient with respect to future rewards, as required inreinforcement learning setups. We also propose an approach to approximatingthis unbiased but high-variance estimator by learning to predict it using abiased estimator. The second approach we propose assumes that an estimator ofthe gradient can be back-propagated and it provides an unbiased estimator ofthe gradient, but can only work with non-linearities unlike the hard threshold,but like the rectifier, that are not flat for all of their range. This issimilar to traditional sigmoidal units but has the advantage that for manyinputs, a hard decision (e.g., a 0 output) can be produced, which would beconvenient for conditional computation and achieving sparse representations andsparse gradients.
arxiv-3600-103 | Fast Linearized Alternating Direction Minimization Algorithm with Adaptive Parameter Selection for Multiplicative Noise Removal | http://arxiv.org/abs/1305.3006 | author:Dai-Qiang Chen, Li-Zhi Cheng category:cs.CV math.NA 68U10 published:2013-05-14 summary:Owing to the edge preserving ability and low computational cost of the totalvariation (TV), variational models with the TV regularization have been widelyinvestigated in the field of multiplicative noise removal. The key points ofthe successful application of these models lie in: the optimal selection of theregularization parameter which balances the data-fidelity term with the TVregularizer; the efficient algorithm to compute the solution. In this paper, wepropose two fast algorithms based on the linearized technique, which are ableto estimate the regularization parameter and recover the image simultaneously.In the iteration step of the proposed algorithms, the regularization parameteris adjusted by a special discrepancy function defined for multiplicative noise.The convergence properties of the proposed algorithms are proved under certainconditions, and numerical experiments demonstrate that the proposed algorithmsoverall outperform some state-of-the-art methods in the PSNR values andcomputational time.
arxiv-3600-104 | Optimization with First-Order Surrogate Functions | http://arxiv.org/abs/1305.3120 | author:Julien Mairal category:stat.ML cs.LG math.OC published:2013-05-14 summary:In this paper, we study optimization methods consisting of iterativelyminimizing surrogates of an objective function. By proposing severalalgorithmic variants and simple convergence analyses, we make two maincontributions. First, we provide a unified viewpoint for several first-orderoptimization techniques such as accelerated proximal gradient, block coordinatedescent, or Frank-Wolfe algorithms. Second, we introduce a new incrementalscheme that experimentally matches or outperforms state-of-the-art solvers forlarge-scale optimization problems typically arising in machine learning.
arxiv-3600-105 | Novel variational model for inpainting in the wavelet domain | http://arxiv.org/abs/1305.3013 | author:Dai-Qiang Chen, Li-Zhi Cheng category:cs.CV published:2013-05-14 summary:Wavelet domain inpainting refers to the process of recovering the missingcoefficients during the image compression or transmission stage. Recently, anefficient algorithm framework which is called Bregmanized operator splitting(BOS) was proposed for solving the classical variational model of waveletinpainting. However, it is still time-consuming to some extent due to the inneriteration. In this paper, a novel variational model is established to formulatethis reconstruction problem from the view of image decomposition. Then anefficient iterative algorithm based on the split-Bregman method is adopted tocalculate an optimal solution, and it is also proved to be convergent. Comparedwith the BOS algorithm the proposed algorithm avoids the inner iteration andhence is more simple. Numerical experiments demonstrate that the proposedmethod is very efficient and outperforms the current state-of-the-art methods,especially in the computational time.
arxiv-3600-106 | A Bag of Words Approach for Semantic Segmentation of Monitored Scenes | http://arxiv.org/abs/1305.3189 | author:Wassim Bouachir, Atousa Torabi, Guillaume-Alexandre Bilodeau, Pascal Blais category:cs.CV published:2013-05-14 summary:This paper proposes a semantic segmentation method for outdoor scenescaptured by a surveillance camera. Our algorithm classifies each perceptuallyhomogenous region as one of the predefined classes learned from a collection ofmanually labelled images. The proposed approach combines two different types ofinformation. First, color segmentation is performed to divide the scene intoperceptually similar regions. Then, the second step is based on SIFT keypointsand uses the bag of words representation of the regions for the classification.The prediction is done using a Na\"ive Bayesian Network as a generativeclassifier. Compared to existing techniques, our method provides more compactrepresentations of scene contents and the segmentation result is moreconsistent with human perception due to the combination of the colorinformation with the image keypoints. The experiments conducted on a publiclyavailable data set demonstrate the validity of the proposed method.
arxiv-3600-107 | I Wish I Didn't Say That! Analyzing and Predicting Deleted Messages in Twitter | http://arxiv.org/abs/1305.3107 | author:Sasa Petrovic, Miles Osborne, Victor Lavrenko category:cs.SI cs.CL published:2013-05-14 summary:Twitter has become a major source of data for social media researchers. Oneimportant aspect of Twitter not previously considered are {\em deletions} --removal of tweets from the stream. Deletions can be due to a multitude ofreasons such as privacy concerns, rashness or attempts to undo publicstatements. We show how deletions can be automatically predicted ahead of timeand analyse which tweets are likely to be deleted and how.
arxiv-3600-108 | Bioacoustical Periodic Pulse Train Signal Detection and Classification using Spectrogram Intensity Binarization and Energy Projection | http://arxiv.org/abs/1305.3250 | author:Marian Popescu, Peter J. Dugan, Mohammad Pourhomayoun, Denise Risch, Harold W. Lewis III, Christopher W. Clark category:cs.CV published:2013-05-14 summary:The following work outlines an approach for automatic detection andrecognition of periodic pulse train signals using a multi-stage process basedon spectrogram edge detection, energy projection and classification. The methodhas been implemented to automatically detect and recognize pulse train songs ofminke whales. While the long term goal of this work is to properly identify anddetect minke songs from large multi-year datasets, this effort was developedusing sounds off the coast of Massachusetts, in the Stellwagen Bank NationalMarine Sanctuary. The detection methodology is presented and evaluated on 232continuous hours of acoustic recordings and a qualitative analysis of machinelearning classifiers and their performance is described. The trained automaticdetection and classification system is applied to 120 continuous hours,comprised of various challenges such as broadband and narrowband noises, lowSNR, and other pulse train signatures. This automatic system achieves a TPR of63% for FPR of 0.6% (or 0.87 FP/h), at a Precision (PPV) of 84% and an F1 scoreof 71%.
arxiv-3600-109 | Efficient Density Estimation via Piecewise Polynomial Approximation | http://arxiv.org/abs/1305.3207 | author:Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun category:cs.LG cs.DS stat.ML published:2013-05-14 summary:We give a highly efficient "semi-agnostic" algorithm for learning univariateprobability distributions that are well approximated by piecewise polynomialdensity functions. Let $p$ be an arbitrary distribution over an interval $I$which is $\tau$-close (in total variation distance) to an unknown probabilitydistribution $q$ that is defined by an unknown partition of $I$ into $t$intervals and $t$ unknown degree-$d$ polynomials specifying $q$ over each ofthe intervals. We give an algorithm that draws $\tilde{O}(t\new{(d+1)}/\eps^2)$samples from $p$, runs in time $\poly(t,d,1/\eps)$, and with high probabilityoutputs a piecewise polynomial hypothesis distribution $h$ that is$(O(\tau)+\eps)$-close (in total variation distance) to $p$. This samplecomplexity is essentially optimal; we show that even for $\tau=0$, anyalgorithm that learns an unknown $t$-piecewise degree-$d$ probabilitydistribution over $I$ to accuracy $\eps$ must use $\Omega({\frac {t(d+1)}{\poly(1 + \log(d+1))}} \cdot {\frac 1 {\eps^2}})$ samples from thedistribution, regardless of its running time. Our algorithm combines tools fromapproximation theory, uniform convergence, linear programming, and dynamicprogramming. We apply this general algorithm to obtain a wide range of results for manynatural problems in density estimation over both continuous and discretedomains. These include state-of-the-art results for learning mixtures oflog-concave distributions; mixtures of $t$-modal distributions; mixtures ofMonotone Hazard Rate distributions; mixtures of Poisson Binomial Distributions;mixtures of Gaussians; and mixtures of $k$-monotone densities. Our generaltechnique yields computationally efficient algorithms for all these problems,in many cases with provably optimal sample complexities (up to logarithmicfactors) in all parameters.
arxiv-3600-110 | Automatic Parameter Adaptation for Multi-object Tracking | http://arxiv.org/abs/1305.2687 | author:Duc Phu Chau, Monique Thonnat, François Bremond category:cs.CV published:2013-05-13 summary:Object tracking quality usually depends on video context (e.g. objectocclusion level, object density). In order to decrease this dependency, thispaper presents a learning approach to adapt the tracker parameters to thecontext variations. In an offline phase, satisfactory tracking parameters arelearned for video context clusters. In the online control phase, once a contextchange is detected, the tracking parameters are tuned using the learned values.The experimental results show that the proposed approach outperforms the recenttrackers in state of the art. This paper brings two contributions: (1) aclassification method of video sequences to learn offline tracking parameters,(2) a new method to tune online tracking parameters using tracking context.
arxiv-3600-111 | Early Detection of Alzheimer's - A Crucial Requirement | http://arxiv.org/abs/1305.2713 | author:Ijaz Bukhari category:cs.CV physics.med-ph published:2013-05-13 summary:Alzheimer's, an old age disease of people over 65 years causes problems withmemory, thinking and behavior. This disease progresses very slow and itsidentification in early stages is very difficult. The symptoms of Alzheimer'sappear slowly and gradually will have worse effects. In its early stages, notonly the patients themselves but their loved ones are generally unable toaccept that the patient is suffering from disease. In this paper, we haveproposed a new algorithm to detect patients of Alzheimer's at early stages bycomparing the Magnetic Resonance Images (MRI) of the patients with normalpersons of their age. The progress of the disease can also be monitored byperiodic comparison of the previous and current MRI.
arxiv-3600-112 | HRF estimation improves sensitivity of fMRI encoding and decoding models | http://arxiv.org/abs/1305.2788 | author:Fabian Pedregosa, Michael Eickenberg, Bertrand Thirion, Alexandre Gramfort category:cs.LG stat.AP published:2013-05-13 summary:Extracting activation patterns from functional Magnetic Resonance Images(fMRI) datasets remains challenging in rapid-event designs due to the inherentdelay of blood oxygen level-dependent (BOLD) signal. The general linear model(GLM) allows to estimate the activation from a design matrix and a fixedhemodynamic response function (HRF). However, the HRF is known to varysubstantially between subjects and brain regions. In this paper, we propose amodel for jointly estimating the hemodynamic response function (HRF) and theactivation patterns via a low-rank representation of task effects.This model isbased on the linearity assumption behind the GLM and can be computed usingstandard gradient-based solvers. We use the activation patterns computed by ourmodel as input data for encoding and decoding studies and report performanceimprovement in both settings.
arxiv-3600-113 | Boosting with the Logistic Loss is Consistent | http://arxiv.org/abs/1305.2648 | author:Matus Telgarsky category:cs.LG stat.ML published:2013-05-13 summary:This manuscript provides optimization guarantees, generalization bounds, andstatistical consistency results for AdaBoost variants which replace theexponential loss with the logistic and similar losses (specifically, twicedifferentiable convex losses which are Lipschitz and tend to zero on one side). The heart of the analysis is to show that, in lieu of explicit regularizationand constraints, the structure of the problem is fairly rigidly controlled bythe source distribution itself. The first control of this type is in theseparable case, where a distribution-dependent relaxed weak learning rateinduces speedy convergence with high probability over any sample. Otherwise, inthe nonseparable case, the convex surrogate risk itself exhibitsdistribution-dependent levels of curvature, and consequently the algorithm'soutput has small norm with high probability.
arxiv-3600-114 | An efficient algorithm for learning with semi-bandit feedback | http://arxiv.org/abs/1305.2732 | author:Gergely Neu, Gábor Bartók category:cs.LG published:2013-05-13 summary:We consider the problem of online combinatorial optimization undersemi-bandit feedback. The goal of the learner is to sequentially select itsactions from a combinatorial decision set so as to minimize its cumulativeloss. We propose a learning algorithm for this problem based on combining theFollow-the-Perturbed-Leader (FPL) prediction method with a novel lossestimation procedure called Geometric Resampling (GR). Contrary to previoussolutions, the resulting algorithm can be efficiently implemented for anydecision set where efficient offline combinatorial optimization is possible atall. Assuming that the elements of the decision set can be described withd-dimensional binary vectors with at most m non-zero entries, we show that theexpected regret of our algorithm after T rounds is O(m sqrt(dT log d)). As aside result, we also improve the best known regret bounds for FPL in the fullinformation setting to O(m^(3/2) sqrt(T log d)), gaining a factor of sqrt(d/m)over previous bounds for this algorithm.
arxiv-3600-115 | Unsupervised ensemble of experts (EoE) framework for automatic binarization of document images | http://arxiv.org/abs/1305.2949 | author:Reza Farrahi Moghaddam, Fereydoun Farrahi Moghaddam, Mohamed Cheriet category:cs.CV published:2013-05-13 summary:In recent years, a large number of binarization methods have been developed,with varying performance generalization and strength against differentbenchmarks. In this work, to leverage on these methods, an ensemble of experts(EoE) framework is introduced, to efficiently combine the outputs of variousmethods. The proposed framework offers a new selection process of thebinarization methods, which are actually the experts in the ensemble, byintroducing three concepts: confidentness, endorsement and schools of experts.The framework, which is highly objective, is built based on two generalprinciples: (i) consolidation of saturated opinions and (ii) identification ofschools of experts. After building the endorsement graph of the ensemble for aninput document image based on the confidentness of the experts, the saturatedopinions are consolidated, and then the schools of experts are identified bythresholding the consolidated endorsement graph. A variation of the framework,in which no selection is made, is also introduced that combines the outputs ofall experts using endorsement-dependent weights. The EoE framework is evaluatedon the set of participating methods in the H-DIBCO'12 contest and also on anensemble generated from various instances of grid-based Sauvola method withpromising performance.
arxiv-3600-116 | Mean field variational Bayesian inference for support vector machine classification | http://arxiv.org/abs/1305.2667 | author:Jan Luts, John T. Ormerod category:stat.ME stat.ML published:2013-05-13 summary:A mean field variational Bayes approach to support vector machines (SVMs)using the latent variable representation on Polson & Scott (2012) is presented.This representation allows circumvention of many of the shortcomings associatedwith classical SVMs including automatic penalty parameter selection, theability to handle dependent samples, missing data and variable selection. Wedemonstrate on simulated and real datasets that our approach is easilyextendable to non-standard situations and outperforms the classical SVMapproach whilst remaining computationally efficient.
arxiv-3600-117 | A study for the effect of the Emphaticness and language and dialect for Voice Onset Time (VOT) in Modern Standard Arabic (MSA) | http://arxiv.org/abs/1305.2680 | author:Sulaiman S. AlDahri category:cs.CL cs.SD published:2013-05-13 summary:The signal sound contains many different features, including Voice Onset Time(VOT), which is a very important feature of stop sounds in many languages. Theonly application of VOT values is stopping phoneme subsets. This subset ofconsonant sounds is stop phonemes exist in the Arabic language, and in fact,all languages. The pronunciation of these sounds is hard and unique especiallyfor less-educated Arabs and non-native Arabic speakers. VOT can be utilized bythe human auditory system to distinguish between voiced and unvoiced stops suchas /p/ and /b/ in English.This search focuses on computing and analyzing VOT ofModern Standard Arabic (MSA), within the Arabic language, for all pairs ofnon-emphatic (namely, /d/ and /t/) and emphatic pairs (namely, /d?/ and /t?/)depending on carrier words. This research uses a database built by ourselves,and uses the carrier words syllable structure: CV-CV-CV. One of the mainoutcomes always found is the emphatic sounds (/d?/, /t?/) are less than 50% ofnon-emphatic (counter-part) sounds ( /d/, /t/).Also, VOT can be used toclassify or detect for a dialect ina language.
arxiv-3600-118 | Identifying Pairs in Simulated Bio-Medical Time-Series | http://arxiv.org/abs/1306.0541 | author:Uri Kartoun category:cs.LG cs.CE published:2013-05-12 summary:The paper presents a time-series-based classification approach to identifysimilarities in pairs of simulated human-generated patterns. An example for apattern is a time-series representing a heart rate during a specifictime-range, wherein the time-series is a sequence of data points that representthe changes in the heart rate values. A bio-medical simulator system wasdeveloped to acquire a collection of 7,871 price patterns of financialinstruments. The financial instruments traded in real-time on three Americanstock exchanges, NASDAQ, NYSE, and AMEX, simulate bio-medical measurements. Thesystem simulates a human in which each price pattern represents one bio-medicalsensor. Data provided during trading hours from the stock exchanges allowedreal-time classification. Classification is based on new machine learningtechniques: self-labeling, which allows the application of supervised learningmethods on unlabeled time-series and similarity ranking, which applied on adecision tree learning algorithm to classify time-series regardless of type andquantity.
arxiv-3600-119 | Accelerated Mini-Batch Stochastic Dual Coordinate Ascent | http://arxiv.org/abs/1305.2581 | author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG published:2013-05-12 summary:Stochastic dual coordinate ascent (SDCA) is an effective technique forsolving regularized loss minimization problems in machine learning. This paperconsiders an extension of SDCA under the mini-batch setting that is often usedin practice. Our main contribution is to introduce an accelerated mini-batchversion of SDCA and prove a fast convergence rate for this method. We discussan implementation of our method over a parallel computing system, and comparethe results to both the vanilla stochastic dual coordinate ascent and to theaccelerated deterministic gradient descent method of\cite{nesterov2007gradient}.
arxiv-3600-120 | On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions | http://arxiv.org/abs/1305.2505 | author:Purushottam Kar, Bharath K Sriperumbudur, Prateek Jain, Harish C Karnick category:cs.LG stat.ML published:2013-05-11 summary:In this paper, we study the generalization properties of online learningbased stochastic methods for supervised learning problems where the lossfunction is dependent on more than one training sample (e.g., metric learning,ranking). We present a generic decoupling technique that enables us to provideRademacher complexity-based generalization error bounds. Our bounds are ingeneral tighter than those obtained by Wang et al (COLT 2012) for the sameproblem. Using our decoupling technique, we are further able to obtain fastconvergence rates for strongly convex pairwise loss functions. We are also ableto analyze a class of memory efficient online learning algorithms for pairwiselearning problems that use only a bounded subset of past training samples toupdate the hypothesis at each step. Finally, in order to complement ourgeneralization bounds, we propose a novel memory efficient online learningalgorithm for higher order learning problems with bounded regret guarantees.
arxiv-3600-121 | Combining Drift Analysis and Generalized Schema Theory to Design Efficient Hybrid and/or Mixed Strategy EAs | http://arxiv.org/abs/1305.2490 | author:Boris Mitavskiy, Jun He category:cs.NE published:2013-05-11 summary:Hybrid and mixed strategy EAs have become rather popular for tackling variouscomplex and NP-hard optimization problems. While empirical evidence suggeststhat such algorithms are successful in practice, rather little theoreticalsupport for their success is available, not mentioning a solid mathematicalfoundation that would provide guidance towards an efficient design of this typeof EAs. In the current paper we develop a rigorous mathematical framework thatsuggests such designs based on generalized schema theory, fitness levels anddrift analysis. An example-application for tackling one of the classicalNP-hard problems, the "single-machine scheduling problem" is presented.
arxiv-3600-122 | Bandits with Knapsacks | http://arxiv.org/abs/1305.2545 | author:Ashwinkumar Badanidiyuru, Robert Kleinberg, Aleksandrs Slivkins category:cs.DS cs.LG published:2013-05-11 summary:Multi-armed bandit problems are the predominant theoretical model ofexploration-exploitation tradeoffs in learning, and they have countlessapplications ranging from medical trials, to communication networks, to Websearch and advertising. In many of these application domains the learner may beconstrained by one or more supply (or budget) limits, in addition to thecustomary limitation on the time horizon. The literature lacks a general modelencompassing these sorts of problems. We introduce such a model, called"bandits with knapsacks", that combines aspects of stochastic integerprogramming with online learning. A distinctive feature of our problem, incomparison to the existing regret-minimization literature, is that the optimalpolicy for a given latent distribution may significantly outperform the policythat plays the optimal fixed arm. Consequently, achieving sublinear regret inthe bandits-with-knapsacks problem is significantly more challenging than inconventional bandit problems. We present two algorithms whose reward is close to the information-theoreticoptimum: one is based on a novel "balanced exploration" paradigm, while theother is a primal-dual algorithm that uses multiplicative updates. Further, weprove that the regret achieved by both algorithms is optimal up topolylogarithmic factors. We illustrate the generality of the problem bypresenting applications in a number of different domains including electroniccommerce, routing, and scheduling. As one example of a concrete application, weconsider the problem of dynamic posted pricing with limited supply and obtainthe first algorithm whose regret, with respect to the optimal dynamic policy,is sublinear in the supply.
arxiv-3600-123 | Affine Invariant Divergences associated with Composite Scores and its Applications | http://arxiv.org/abs/1305.2473 | author:Takafumi Kanamori, Hironori Fujisawa category:math.ST stat.ML stat.TH published:2013-05-11 summary:In statistical analysis, measuring a score of predictive performance is animportant task. In many scientific fields, appropriate scores were tailored totackle the problems at hand. A proper score is a popular tool to obtainstatistically consistent forecasts. Furthermore, a mathematicalcharacterization of the proper score was studied. As a result, it was revealedthat the proper score corresponds to a Bregman divergence, which is anextension of the squared distance over the set of probability distributions. Inthe present paper, we introduce composite scores as an extension of the typicalscores in order to obtain a wider class of probabilistic forecasting. Then, wepropose a class of composite scores, named Holder scores, that induceequivariant estimators. The equivariant estimators have a favorable property,implying that the estimator is transformed in a consistent way, when the datais transformed. In particular, we deal with the affine transformation of thedata. By using the equivariant estimators under the affine transformation, onecan obtain estimators that do no essentially depend on the choice of the systemof units in the measurement. Conversely, we prove that the Holder score ischaracterized by the invariance property under the affine transformations.Furthermore, we investigate statistical properties of the estimators usingHolder scores for the statistical problems including estimation of regressionfunctions and robust parameter estimation, and illustrate the usefulness of thenewly introduced scores for statistical forecasting.
arxiv-3600-124 | Corrupted Sensing: Novel Guarantees for Separating Structured Signals | http://arxiv.org/abs/1305.2524 | author:Rina Foygel, Lester Mackey category:cs.IT math.IT math.OC stat.ML published:2013-05-11 summary:We study the problem of corrupted sensing, a generalization of compressedsensing in which one aims to recover a signal from a collection of corrupted orunreliable measurements. While an arbitrary signal cannot be recovered in theface of arbitrary corruption, tractable recovery is possible when both signaland corruption are suitably structured. We quantify the relationship betweensignal recovery and two geometric measures of structure, the Gaussiancomplexity of a tangent cone and the Gaussian distance to a subdifferential. Wetake a convex programming approach to disentangling signal and corruption,analyzing both penalized programs that trade off between signal and corruptioncomplexity, and constrained programs that bound the complexity of signal orcorruption when prior information is available. In each case, we provideconditions for exact signal recovery from structured corruption and stablesignal recovery from structured corruption with added unstructured noise. Oursimulations demonstrate close agreement between our theoretical recovery boundsand the sharp phase transitions observed in practice. In addition, we providenew interpretable bounds for the Gaussian complexity of sparse vectors,block-sparse vectors, and low-rank matrices, which lead to sharper guaranteesof recovery when combined with our results and those in the literature.
arxiv-3600-125 | Learning Policies for Contextual Submodular Prediction | http://arxiv.org/abs/1305.2532 | author:Stephane Ross, Jiaji Zhou, Yisong Yue, Debadeepta Dey, J. Andrew Bagnell category:cs.LG stat.ML published:2013-05-11 summary:Many prediction domains, such as ad placement, recommendation, trajectoryprediction, and document summarization, require predicting a set or list ofoptions. Such lists are often evaluated using submodular reward functions thatmeasure both quality and diversity. We propose a simple, efficient, andprovably near-optimal approach to optimizing such prediction problems based onno-regret learning. Our method leverages a surprising result from onlinesubmodular optimization: a single no-regret online learner can compete with anoptimal sequence of predictions. Compared to previous work, which either learna sequence of classifiers or rely on stronger assumptions such asrealizability, we ensure both data-efficiency as well as performance guaranteesin the fully agnostic setting. Experiments validate the efficiency andapplicability of the approach on a wide range of problems including manipulatortrajectory optimization, news recommendation and document summarization.
arxiv-3600-126 | Geiringer Theorems: From Population Genetics to Computational Intelligence, Memory Evolutive Systems and Hebbian Learning | http://arxiv.org/abs/1305.2504 | author:Boris Mitavskiy, Elio Tuci, Chris Cannings, Chris Cannings, Jonathan Rowe, Jun He category:cs.NE published:2013-05-11 summary:The classical Geiringer theorem addresses the limiting frequency ofoccurrence of various alleles after repeated application of crossover. It hasbeen adopted to the setting of evolutionary algorithms and, a lot morerecently, reinforcement learning and Monte-Carlo tree search methodology tocope with a rather challenging question of action evaluation at the chancenodes. The theorem motivates novel dynamic parallel algorithms that areexplicitly described in the current paper for the first time. The algorithmsinvolve independent agents traversing a dynamically constructed directed graphthat possibly has loops. A rather elegant and profound category-theoretic modelof cognition in biological neural networks developed by a well-known Frenchmathematician, professor Andree Ehresmann jointly with a neurosurgeon, Jan PaulVanbremeersch over the last thirty years provides a hint at the connectionbetween such algorithms and Hebbian learning.
arxiv-3600-127 | Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima | http://arxiv.org/abs/1305.2436 | author:Po-Ling Loh, Martin J. Wainwright category:math.ST cs.IT math.IT stat.ML stat.TH 62F12 published:2013-05-10 summary:We provide novel theoretical results regarding local optima of regularized$M$-estimators, allowing for nonconvexity in both loss and penalty functions.Under restricted strong convexity on the loss and suitable regularityconditions on the penalty, we prove that \emph{any stationary point} of thecomposite objective function will lie within statistical precision of theunderlying parameter vector. Our theory covers many nonconvex objectivefunctions of interest, including the corrected Lasso for errors-in-variableslinear models; regression for generalized linear models with nonconvexpenalties such as SCAD, MCP, and capped-$\ell_1$; and high-dimensionalgraphical model estimation. We quantify statistical accuracy by providingbounds on the $\ell_1$-, $\ell_2$-, and prediction error between stationarypoints and the population-level optimum. We also propose a simple modificationof composite gradient descent that may be used to obtain a near-global optimumwithin statistical precision $\epsilon$ in $\log(1/\epsilon)$ steps, which isthe fastest possible rate of any first-order method. We provide simulationstudies illustrating the sharpness of our theoretical results.
arxiv-3600-128 | Image Optimization and Prediction | http://arxiv.org/abs/1305.2828 | author:Shweta Jain, Urmila Shrawankar category:cs.CV published:2013-05-10 summary:Image Processing, Optimization and Prediction of an Image play a key role inComputer Science. Image processing provides a way to analyze and identify animage .Many areas like medical image processing, Satellite images, naturalimages and artificial images requires lots of analysis and research onoptimization. In Image Optimization and Prediction we are combining thefeatures of Query Optimization, Image Processing and Prediction . Imageoptimization is used in Pattern analysis, object recognition, in medical Imageprocessing to predict the type of diseases, in satellite images for predictingweather forecast, availability of water or mineral etc. Image Processing,Optimization and analysis is a wide open area for research .Lots of researchhas been conducted in the area of Image analysis and many techniques areavailable for image analysis but, a single technique is not yet identified forimage analysis and prediction .our research is focused on identifying a globaltechnique for image analysis and Prediction.
arxiv-3600-129 | Multivariate Regression with Calibration | http://arxiv.org/abs/1305.2238 | author:Han Liu, Lie Wang, Tuo Zhao category:stat.ML published:2013-05-10 summary:We propose a new method named calibrated multivariate regression (CMR) forfitting high dimensional multivariate regression models. Compared to existingmethods, CMR calibrates the regularization for each regression task withrespect to its noise level so that it is simultaneously tuning insensitive andachieves an improved finite sample performance. Computationally, we develop anefficient smoothed proximal gradient algorithm with a worst-case numerical rateof convergence $O(1/\epsilon)$, where $\epsilon$ is a pre-specified accuracy.Theoretically, we prove that CMR achieves the optimal rate of convergence inparameter estimation. We illustrate the usefulness of CMR by thorough numericalsimulations and show that CMR consistently outperforms existing multivariateregression methods. We also apply CMR on a brain activity prediction problemand find that CMR even outperforms the handcrafted models created by humanexperts.
arxiv-3600-130 | Multi-q Pattern Classification of Polarization Curves | http://arxiv.org/abs/1305.2876 | author:Ricardo Fabbri, Ivan N. Bastos, Francisco D. Moura Neto, Francisco J. P. Lopes, Wesley N. Goncalves, Odemir M. Bruno category:cs.CE cs.CV published:2013-05-10 summary:Several experimental measurements are expressed in the form ofone-dimensional profiles, for which there is a scarcity of methodologies ableto classify the pertinence of a given result to a specific group. Thepolarization curves that evaluate the corrosion kinetics of electrodes incorrosive media are an application where the behavior is chiefly analyzed fromprofiles. Polarization curves are indeed a classic method to determine theglobal kinetics of metallic electrodes, but the strong nonlinearity fromdifferent metals and alloys can overlap and the discrimination becomes achallenging problem. Moreover, even finding a typical curve from replicatedtests requires subjective judgement. In this paper we used the so-calledmulti-q approach based on the Tsallis statistics in a classification engine toseparate multiple polarization curve profiles of two stainless steels. Wecollected 48 experimental polarization curves in aqueous chloride medium of twostainless steel types, with different resistance against localized corrosion.Multi-q pattern analysis was then carried out on a wide potential range, fromcathodic up to anodic regions. An excellent classification rate was obtained,at a success rate of 90%, 80%, and 83% for low (cathodic), high (anodic), andboth potential ranges, respectively, using only 2% of the original profiledata. These results show the potential of the proposed approach towardsefficient, robust, systematic and automatic classification of highly non-linearprofile curves.
arxiv-3600-131 | Shape Reconstruction and Recognition with Isolated Non-directional Cues | http://arxiv.org/abs/1305.2395 | author:Toshiro Kubota, Jessica Ranck, Briley Acker, Herman De Haan category:cs.CV published:2013-05-10 summary:The paper investigates a hypothesis that our visual system groups visual cuesbased on how they form a surface, or more specifically triangulation derivedfrom the visual cues. To test our hypothesis, we compare shape recognition withthree different representations of visual cues: a set of isolated dotsdelineating the outline of the shape, a set of triangles obtained from Delaunaytriangulation of the set of dots, and a subset of Delaunay triangles excludingthose outside of the shape. Each participant was assigned to one particularrepresentation type and increased the number of dots (and consequentiallytriangles) until the underlying shape could be identified. We compare theaverage number of dots needed for identification among three types ofrepresentations. Our hypothesis predicts that the results from the threerepresentations will be similar. However, they show statistically significantdifferences. The paper also presents triangulation based algorithms forreconstruction and recognition of a shape from a set of isolated dots.Experiments showed that the algorithms were more effective and perceptuallyagreeable than similar contour based ones. From these experiments, we concludethat triangulation does affect our shape recognition. However, the surfacebased approach presents a number of computational advantages over the contourbased one and should be studied further.
arxiv-3600-132 | Revisiting Bayesian Blind Deconvolution | http://arxiv.org/abs/1305.2362 | author:David Wipf, Haichao Zhang category:cs.CV cs.LG stat.ML published:2013-05-10 summary:Blind deconvolution involves the estimation of a sharp signal or image givenonly a blurry observation. Because this problem is fundamentally ill-posed,strong priors on both the sharp image and blur kernel are required toregularize the solution space. While this naturally leads to a standard MAPestimation framework, performance is compromised by unknown trade-off parametersettings, optimization heuristics, and convergence issues stemming fromnon-convexity and/or poor prior selections. To mitigate some of these problems,a number of authors have recently proposed substituting a variational Bayesian(VB) strategy that marginalizes over the high-dimensional image space leadingto better estimates of the blur kernel. However, the underlying cost functionnow involves both integrals with no closed-form solution and complex,function-valued arguments, thus losing the transparency of MAP. Beyond standardBayesian-inspired intuitions, it thus remains unclear by exactly what mechanismthese methods are able to operate, rendering understanding, improvements andextensions more difficult. To elucidate these issues, we demonstrate that theVB methodology can be recast as an unconventional MAP problem with a veryparticular penalty/prior that couples the image, blur kernel, and noise levelin a principled way. This unique penalty has a number of useful characteristicspertaining to relative concavity, local minima avoidance, and scale-invariancethat allow us to rigorously explain the success of VB including its existingimplementational heuristics and approximations. It also provides strictcriteria for choosing the optimal image prior that, perhapscounter-intuitively, need not reflect the statistics of natural scenes. In sodoing we challenge the prevailing notion of why VB is successful for blinddeconvolution while providing a transparent platform for introducingenhancements.
arxiv-3600-133 | Performance Enhancement of Distributed Quasi Steady-State Genetic Algorithm | http://arxiv.org/abs/1305.2830 | author:Rahila Patel, Urmila Shrawankar, MM. Raghuwanshi, Anil N. Jaiswal category:cs.NE published:2013-05-10 summary:This paper proposes a new scheme for performance enhancement of distributedgenetic algorithm (DGA). Initial population is divided in two classes i.e.female and male. Simple distance based clustering is used for cluster formationaround females. For reclustering self-adaptive K-means is used, which produceswell distributed and well separated clusters. The self-adaptive K-means usedfor reclustering automatically locates initial position of centroids and numberof clusters. Four plans of co-evolution are applied on these clustersindependently. Clusters evolve separately. Merging of clusters takes placedepending on their performance. For experimentation unimodal and multimodaltest functions have been used. Test result show that the new scheme ofdistribution of population has given better performance.
arxiv-3600-134 | Human Mood Detection For Human Computer Interaction | http://arxiv.org/abs/1305.2827 | author:Preeti Badar, Urmila Shrawankar category:cs.CV published:2013-05-10 summary:In this paper we propose an easiest approach for facial expressionrecognition. Here we are using concept of SVM for Expression Classification.Main problem is sub divided in three main modules. First one is Face detectionin which we are using skin filter and Face segmentation. We are given morestress on feature Extraction. This method is effective enough for applicationwhere fast execution is required. Second, Facial Feature Extraction which isessential part for expression recognition. In this module we used EdgeProjection Analysis. Finally extracted features vector is passed towards SVMclassifier for Expression Recognition. We are considering six basic Expressions(Anger, Fear, Disgust, Joy, Sadness, and Surprise)
arxiv-3600-135 | Beyond Physical Connections: Tree Models in Human Pose Estimation | http://arxiv.org/abs/1305.2269 | author:Fang Wang, Yi Li category:cs.CV published:2013-05-10 summary:Simple tree models for articulated objects prevails in the last decade.However, it is also believed that these simple tree models are not capable ofcapturing large variations in many scenarios, such as human pose estimation.This paper attempts to address three questions: 1) are simple tree modelssufficient? more specifically, 2) how to use tree models effectively in humanpose estimation? and 3) how shall we use combined parts together with singleparts efficiently? Assuming we have a set of single parts and combined parts, and the goal is toestimate a joint distribution of their locations. We surprisingly find that nolatent variables are introduced in the Leeds Sport Dataset (LSP) duringlearning latent trees for deformable model, which aims at approximating thejoint distributions of body part locations using minimal tree structure. Thissuggests one can straightforwardly use a mixed representation of single andcombined parts to approximate their joint distribution in a simple tree model.As such, one only needs to build Visual Categories of the combined parts, andthen perform inference on the learned latent tree. Our method outperformed thestate of the art on the LSP, both in the scenarios when the training images arefrom the same dataset and from the PARSE dataset. Experiments on animal imagesfrom the VOC challenge further support our findings.
arxiv-3600-136 | Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation | http://arxiv.org/abs/1305.2452 | author:James Foulds, Levi Boyles, Christopher Dubois, Padhraic Smyth, Max Welling category:cs.LG published:2013-05-10 summary:In the internet era there has been an explosion in the amount of digital textinformation available, leading to difficulties of scale for traditionalinference algorithms for topic models. Recent advances in stochasticvariational inference algorithms for latent Dirichlet allocation (LDA) havemade it feasible to learn topic models on large-scale corpora, but thesemethods do not currently take full advantage of the collapsed representation ofthe model. We propose a stochastic algorithm for collapsed variational Bayesianinference for LDA, which is simpler and more efficient than the state of theart method. We show connections between collapsed variational Bayesianinference and MAP estimation for LDA, and leverage these connections to proveconvergence properties of the proposed algorithm. In experiments on large-scaletext corpora, the algorithm was found to converge faster and often to a bettersolution than the previous method. Human-subject experiments also demonstratedthat the method can learn coherent topics in seconds on small corpora,facilitating the use of topic models in interactive document analysis software.
arxiv-3600-137 | Inferring Team Strengths Using a Discrete Markov Random Field | http://arxiv.org/abs/1305.1998 | author:John Zech, Frank Wood category:stat.ML published:2013-05-09 summary:We propose an original model for inferring team strengths using a MarkovRandom Field, which can be used to generate historical estimates of theoffensive and defensive strengths of a team over time. This model was designedto be applied to sports such as soccer or hockey, in which contest outcomestake value in a limited discrete space. We perform inference using acombination of Expectation Maximization and Loopy Belief Propagation. Thechallenges of working with a non-convex optimization problem and ahigh-dimensional parameter space are discussed. The performance of the model isdemonstrated on professional soccer data from the English Premier League.
arxiv-3600-138 | An Adaptive Statistical Non-uniform Quantizer for Detail Wavelet Components in Lossy JPEG2000 Image Compression | http://arxiv.org/abs/1305.1986 | author:Madhur Srivastava, Satish K. Singh, Prasanta K. Panigrahi category:cs.MM cs.CV published:2013-05-09 summary:The paper presents a non-uniform quantization method for the Detailcomponents in the JPEG2000 standard. Incorporating the fact that thecoefficients lying towards the ends of the histogram plot of each Detailcomponent represent the structural information of an image, the quantizationstep sizes become smaller at they approach the ends of the histogram plot. Thevariable quantization step sizes are determined by the actual statistics of thewavelet coefficients. Mean and standard deviation are the two statisticalparameters used iteratively to obtain the variable step sizes. Moreover, themean of the coefficients lying within the step size is chosen as the quantizedvalue, contrary to the deadzone uniform quantizer which selects the midpoint ofthe quantization step size as the quantized value. The experimental results ofthe deadzone uniform quantizer and the proposed non-uniform quantizer areobjectively compared by using Mean-Squared Error (MSE) and Mean StructuralSimilarity Index Measure (MSSIM), to evaluate the quantization error andreconstructed image quality, respectively. Subjective analysis of thereconstructed images is also carried out. Through the objective and subjectiveassessments, it is shown that the non-uniform quantizer performs better thanthe deadzone uniform quantizer in the perceptual quality of the reconstructedimage, especially at low bitrates. More importantly, unlike the deadzoneuniform quantizer, the non-uniform quantizer accomplishes better visual qualitywith a few quantized values.
arxiv-3600-139 | Automatic Speech Recognition Using Template Model for Man-Machine Interface | http://arxiv.org/abs/1305.2959 | author:Neema Mishra, Urmila Shrawankar, V M Thakare category:cs.SD cs.CL published:2013-05-09 summary:Speech is a natural form of communication for human beings, and computerswith the ability to understand speech and speak with a human voice are expectedto contribute to the development of more natural man-machine interfaces.Computers with this kind of ability are gradually becoming a reality, throughthe evolution of speech recognition technologies. Speech is being an importantmode of interaction with computers. In this paper Feature extraction isimplemented using well-known Mel-Frequency Cepstral Coefficients (MFCC).Patternmatching is done using Dynamic time warping (DTW) algorithm.
arxiv-3600-140 | Stochastic gradient descent algorithms for strongly convex functions at O(1/T) convergence rates | http://arxiv.org/abs/1305.2218 | author:Shenghuo Zhu category:cs.LG cs.AI published:2013-05-09 summary:With a weighting scheme proportional to t, a traditional stochastic gradientdescent (SGD) algorithm achieves a high probability convergence rate ofO({\kappa}/T) for strongly convex functions, instead of O({\kappa} ln(T)/T). Wealso prove that an accelerated SGD algorithm also achieves a rate ofO({\kappa}/T).
arxiv-3600-141 | Speech Enhancement Using Pitch Detection Approach For Noisy Environment | http://arxiv.org/abs/1305.2352 | author:Rashmi Makhijani, Urmila Shrawankar, V M Thakare category:cs.SD cs.CL published:2013-05-09 summary:Acoustical mismatch among training and testing phases degrades outstandinglyspeech recognition results. This problem has limited the development ofreal-world nonspecific applications, as testing conditions are highly variantor even unpredictable during the training process. Therefore the backgroundnoise has to be removed from the noisy speech signal to increase the signalintelligibility and to reduce the listener fatigue. Enhancement techniquesapplied, as pre-processing stages; to the systems remarkably improverecognition results. In this paper, a novel approach is used to enhance theperceived quality of the speech signal when the additive noise cannot bedirectly controlled. Instead of controlling the background noise, we propose toreinforce the speech signal so that it can be heard more clearly in noisyenvironments. The subjective evaluation shows that the proposed method improvesperceptual quality of speech in various noisy environments. As in some casesspeaking may be more convenient than typing, even for rapid typists: manymathematical symbols are missing from the keyboard but can be easily spoken andrecognized. Therefore, the proposed system can be used in an applicationdesigned for mathematical symbol recognition (especially symbols not availableon the keyboard) in schools.
arxiv-3600-142 | Opportunities & Challenges In Automatic Speech Recognition | http://arxiv.org/abs/1305.2846 | author:Rashmi Makhijani, Urmila Shrawankar, V M Thakare category:cs.CL cs.SD published:2013-05-09 summary:Automatic speech recognition enables a wide range of current and emergingapplications such as automatic transcription, multimedia content analysis, andnatural human-computer interfaces. This paper provides a glimpse of theopportunities and challenges that parallelism provides for automatic speechrecognition and related application research from the point of view of speechresearchers. The increasing parallelism in computing platforms opens threemajor possibilities for speech recognition systems: improving recognitionaccuracy in non-ideal, everyday noisy environments; increasing recognitionthroughput in batch processing of speech data; and reducing recognition latencyin realtime usage scenarios. This paper describes technical challenges,approaches taken, and possible directions for future research to guide thedesign of efficient parallel software and hardware infrastructures.
arxiv-3600-143 | Repairing and Inpainting Damaged Images using Diffusion Tensor | http://arxiv.org/abs/1305.2221 | author:Faouzi Benzarti, Hamid Amiri category:cs.CV published:2013-05-09 summary:Removing or repairing the imperfections of a digital images or videos is avery active and attractive field of research belonging to the image inpaintingtechnique. This later has a wide range of applications, such as removingscratches in old photographic image, removing text and logos or creatingcartoon and artistic effects. In this paper, we propose an efficient method torepair a damaged image based on a non linear diffusion tensor. The idea is totrack perfectly the local geometry of the damaged image and allowing diffusiononly in the isophotes curves direction. To illustrate the effective performanceof our method, we present some experimental results on test and realphotographic color images
arxiv-3600-144 | An Overview of Hindi Speech Recognition | http://arxiv.org/abs/1305.2847 | author:Neema Mishra, Urmila Shrawankar, V M Thakare category:cs.CL cs.SD published:2013-05-09 summary:In this age of information technology, information access in a convenientmanner has gained importance. Since speech is a primary mode of communicationamong human beings, it is natural for people to expect to be able to carry outspoken dialogue with computer. Speech recognition system permits ordinarypeople to speak to the computer to retrieve information. It is desirable tohave a human computer dialogue in local language. Hindi being the most widelyspoken Language in India is the natural primary human language candidate forhuman machine interaction. There are five pairs of vowels in Hindi languages;one member is longer than the other one. This paper describes an overview ofspeech recognition system that includes how speech is produced and theproperties and characteristics of Hindi Phoneme.
arxiv-3600-145 | A Rank Minrelation - Majrelation Coefficient | http://arxiv.org/abs/1305.2038 | author:Patrick E. Meyer category:stat.ML cs.AI published:2013-05-09 summary:Improving the detection of relevant variables using a new bivariate measurecould importantly impact variable selection and large network inferencemethods. In this paper, we propose a new statistical coefficient that we callthe rank minrelation coefficient. We define a minrelation of X to Y (orequivalently a majrelation of Y to X) as a measure that estimate p(Y > X) whenX and Y are continuous random variables. The approach is similar to Lin'sconcordance coefficient that rather focuses on estimating p(X = Y). In otherwords, if a variable X exhibits a minrelation to Y then, as X increases, Y islikely to increases too. However, on the contrary to concordance orcorrelation, the minrelation is not symmetric. More explicitly, if X decreases,little can be said on Y values (except that the uncertainty on Y actuallyincreases). In this paper, we formally define this new kind of bivariatedependencies and propose a new statistical coefficient in order to detect thosedependencies. We show through several key examples that this new coefficienthas many interesting properties in order to select relevant variables, inparticular when compared to correlation.
arxiv-3600-146 | Joint Topic Modeling and Factor Analysis of Textual Information and Graded Response Data | http://arxiv.org/abs/1305.1956 | author:Andrew S. Lan, Christoph Studer, Andrew E. Waters, Richard G. Baraniuk category:stat.ML cs.LG published:2013-05-08 summary:Modern machine learning methods are critical to the development oflarge-scale personalized learning systems that cater directly to the needs ofindividual learners. The recently developed SPARse Factor Analysis (SPARFA)framework provides a new statistical model and algorithms for machinelearning-based learning analytics, which estimate a learner's knowledge of thelatent concepts underlying a domain, and content analytics, which estimate therelationships among a collection of questions and the latent concepts. SPARFAestimates these quantities given only the binary-valued graded responses to acollection of questions. In order to better interpret the estimated latentconcepts, SPARFA relies on a post-processing step that utilizes user-definedtags (e.g., topics or keywords) available for each question. In this paper, werelax the need for user-defined tags by extending SPARFA to jointly processboth graded learner responses and the text of each question and its associatedanswer(s) or other feedback. Our purely data-driven approach (i) enhances theinterpretability of the estimated latent concepts without the need ofexplicitly generating a set of tags or performing a post-processing step, (ii)improves the prediction performance of SPARFA, and (iii) scales to largetest/assessments where human annotation would prove burdensome. We demonstratethe efficacy of the proposed approach on two real educational datasets.
arxiv-3600-147 | The Dynamically Extended Mind -- A Minimal Modeling Case Study | http://arxiv.org/abs/1305.1958 | author:Tom Froese, Carlos Gershenson, David A. Rosenblueth category:cs.AI cs.NE nlin.CD I.2.0 published:2013-05-08 summary:The extended mind hypothesis has stimulated much interest in cognitivescience. However, its core claim, i.e. that the process of cognition can extendbeyond the brain via the body and into the environment, has been heavilycriticized. A prominent critique of this claim holds that when some part of theworld is coupled to a cognitive system this does not necessarily entail thatthe part is also constitutive of that cognitive system. This critique is knownas the "coupling-constitution fallacy". In this paper we respond to thisreductionist challenge by using an evolutionary robotics approach to create aminimal model of two acoustically coupled agents. We demonstrate how theinteraction process as a whole has properties that cannot be reduced to thecontributions of the isolated agents. We also show that the neural dynamics ofthe coupled agents has formal properties that are inherently impossible forthose neural networks in isolation. By keeping the complexity of the model toan absolute minimum, we are able to illustrate how the coupling-constitutionfallacy is in fact based on an inadequate understanding of the constitutiverole of nonlinear interactions in dynamical systems theory.
arxiv-3600-148 | Cover Tree Bayesian Reinforcement Learning | http://arxiv.org/abs/1305.1809 | author:Nikolaos Tziortziotis, Christos Dimitrakakis, Konstantinos Blekas category:stat.ML cs.LG published:2013-05-08 summary:This paper proposes an online tree-based Bayesian approach for reinforcementlearning. For inference, we employ a generalised context tree model. Thisdefines a distribution on multivariate Gaussian piecewise-linear models, whichcan be updated in closed form. The tree structure itself is constructed usingthe cover tree method, which remains efficient in high dimensional spaces. Wecombine the model with Thompson sampling and approximate dynamic programming toobtain effective exploration policies in unknown environments. The flexibilityand computational simplicity of the model render it suitable for manyreinforcement learning problems in continuous state spaces. We demonstrate thisin an experimental comparison with least squares policy iteration.
arxiv-3600-149 | Automated polyp detection in colon capsule endoscopy | http://arxiv.org/abs/1305.1912 | author:Alexander V. Mamonov, Isabel N. Figueiredo, Pedro N. Figueiredo, Yen-Hsi Richard Tsai category:cs.CV I.4.8 published:2013-05-08 summary:Colorectal polyps are important precursors to colon cancer, a major healthproblem. Colon capsule endoscopy (CCE) is a safe and minimally invasiveexamination procedure, in which the images of the intestine are obtained viadigital cameras on board of a small capsule ingested by a patient. The videosequence is then analyzed for the presence of polyps. We propose an algorithmthat relieves the labor of a human operator analyzing the frames in the videosequence. The algorithm acts as a binary classifier, which labels the frame aseither containing polyps or not, based on the geometrical analysis and thetexture content of the frame. The geometrical analysis is based on asegmentation of an image with the help of a mid-pass filter. The featuresextracted by the segmentation procedure are classified according to anassumption that the polyps are characterized as protrusions that are mostlyround in shape. Thus, we use a best fit ball radius as a decision parameter ofa binary classifier. We present a statistical study of the performance of ourapproach on a data set containing over 18,900 frames from the endoscopic videosequences of five adult patients. The algorithm demonstrates a solidperformance, achieving 47% sensitivity per frame and over 81% sensitivity perpolyp at a specificity level of 90%. On average, with a video sequence lengthof 3747 frames, only 367 false positive frames need to be inspected by a humanoperator.
arxiv-3600-150 | The Extended Parameter Filter | http://arxiv.org/abs/1305.1704 | author:Yusuf Erol, Lei Li, Bharath Ramsundar, Stuart J. Russell category:stat.ML cs.AI published:2013-05-08 summary:The parameters of temporal models, such as dynamic Bayesian networks, may bemodelled in a Bayesian context as static or atemporal variables that influencetransition probabilities at every time step. Particle filters fail for modelsthat include such variables, while methods that use Gibbs sampling of parametervariables may incur a per-sample cost that grows linearly with the length ofthe observation sequence. Storvik devised a method for incremental computationof exact sufficient statistics that, for some cases, reduces the per-samplecost to a constant. In this paper, we demonstrate a connection betweenStorvik's filter and a Kalman filter in parameter space and establish moregeneral conditions under which Storvik's filter works. Drawing on an analogy tothe extended Kalman filter, we develop and analyze, both theoretically andexperimentally, a Taylor approximation to the parameter posterior that allowsStorvik's method to be applied to a broader class of models. Our experiments onboth synthetic examples and real applications show improvement over existingmethods.
arxiv-3600-151 | Class Imbalance Problem in Data Mining Review | http://arxiv.org/abs/1305.1707 | author:Rushi Longadge, Snehalata Dongre category:cs.LG published:2013-05-08 summary:In last few years there are major changes and evolution has been done onclassification of data. As the application area of technology is increases thesize of data also increases. Classification of data becomes difficult becauseof unbounded size and imbalance nature of data. Class imbalance problem becomegreatest issue in data mining. Imbalance problem occur where one of the twoclasses having more sample than other classes. The most of algorithm are morefocusing on classification of major sample while ignoring or misclassifyingminority sample. The minority samples are those that rarely occur but veryimportant. There are different methods available for classification ofimbalance data set which is divided into three main categories, the algorithmicapproach, data-preprocessing approach and feature selection approach. Each ofthis technique has their own advantages and disadvantages. In this papersystematic study of each approach is define which gives the right direction forresearch in class imbalance problem.
arxiv-3600-152 | Speech: A Challenge to Digital Signal Processing Technology for Human-to-Computer Interaction | http://arxiv.org/abs/1305.1925 | author:Urmila Shrawankar, Anjali Mahajan category:cs.HC cs.CL published:2013-05-08 summary:This software project based paper is for a vision of the near future in whichcomputer interaction is characterized by natural face-to-face conversationswith lifelike characters that speak, emote, and gesture. The first step isspeech. The dream of a true virtual reality, a complete human-computerinteraction system will not come true unless we try to give some perception tomachine and make it perceive the outside world as humans communicate with eachother. This software project is under development for listening and replyingmachine (Computer) through speech. The Speech interface is developed to convertspeech input into some parametric form (Speech-to-Text) for further processingand the results, text output to speech synthesis (Text-to-Speech)
arxiv-3600-153 | GReTA - a novel Global and Recursive Tracking Algorithm in three dimensions | http://arxiv.org/abs/1305.1495 | author:Alessandro Attanasi, Andrea Cavagna, Lorenzo Del Castello, Irene Giardina, Asja Jelic, Stefania Melillo, Leonardo Parisi, Fabio Pellacini, Edward Shen, Edmondo Silvestri, Massimiliano Viale category:q-bio.QM cs.CV published:2013-05-07 summary:Tracking multiple moving targets allows quantitative measure of the dynamicbehavior in systems as diverse as animal groups in biology, turbulence in fluiddynamics and crowd and traffic control. In three dimensions, tracking severaltargets becomes increasingly hard since optical occlusions are very likely,i.e. two featureless targets frequently overlap for several frames. Occlusionsare particularly frequent in biological groups such as bird flocks, fishschools, and insect swarms, a fact that has severely limited collective animalbehavior field studies in the past. This paper presents a 3D tracking methodthat is robust in the case of severe occlusions. To ensure robustness, we adopta global optimization approach that works on all objects and frames at once. Toachieve practicality and scalability, we employ a divide and conquerformulation, thanks to which the computational complexity of the problem isreduced by orders of magnitude. We tested our algorithm with synthetic data,with experimental data of bird flocks and insect swarms and with publicbenchmark datasets, and show that our system yields high quality trajectoriesfor hundreds of moving targets with severe overlap. The results obtained onvery heterogeneous data show the potential applicability of our method to themost diverse experimental situations.
arxiv-3600-154 | High Level Pattern Classification via Tourist Walks in Networks | http://arxiv.org/abs/1305.1679 | author:Thiago Christiano Silva, Liang Zhao category:cs.AI cs.LG published:2013-05-07 summary:Complex networks refer to large-scale graphs with nontrivial connectionpatterns. The salient and interesting features that the complex network studyoffer in comparison to graph theory are the emphasis on the dynamicalproperties of the networks and the ability of inherently uncovering patternformation of the vertices. In this paper, we present a hybrid dataclassification technique combining a low level and a high level classifier. Thelow level term can be equipped with any traditional classification techniques,which realize the classification task considering only physical features (e.g.,geometrical or statistical features) of the input data. On the other hand, thehigh level term has the ability of detecting data patterns with semanticmeanings. In this way, the classification is realized by means of theextraction of the underlying network's features constructed from the inputdata. As a result, the high level classification process measures thecompliance of the test instances with the pattern formation of the trainingdata. Out of various high level perspectives that can be utilized to capturesemantic meaning, we utilize the dynamical features that are generated from atourist walker in a networked environment. Specifically, a weighted combinationof transient and cycle lengths generated by the tourist walk is employed forthat end. Interestingly, our study shows that the proposed technique is able tofurther improve the already optimized performance of traditional classificationtechniques.
arxiv-3600-155 | EURETILE 2010-2012 summary: first three years of activity of the European Reference Tiled Experiment | http://arxiv.org/abs/1305.1459 | author:Pier Stanislao Paolucci, Iuliana Bacivarov, Gert Goossens, Rainer Leupers, Frédéric Rousseau, Christoph Schumacher, Lothar Thiele, Piero Vicini category:cs.DC cs.AR cs.NE cs.OS cs.PL published:2013-05-07 summary:This is the summary of first three years of activity of the EURETILE FP7project 247846. EURETILE investigates and implements brain-inspired andfault-tolerant foundational innovations to the system architecture of massivelyparallel tiled computer architectures and the corresponding programmingparadigm. The execution targets are a many-tile HW platform, and a many-tilesimulator. A set of SW process - HW tile mapping candidates is generated by theholistic SW tool-chain using a combination of analytic and bio-inspiredmethods. The Hardware dependent Software is then generated, providing OSservices with maximum efficiency/minimal overhead. The many-tile simulatorcollects profiling data, closing the loop of the SW tool chain. Fine-grainparallelism inside processes is exploited by optimized intra-tile compilationtechniques, but the project focus is above the level of the elementary tile.The elementary HW tile is a multi-processor, which includes a fault tolerantDistributed Network Processor (for inter-tile communication) and ASIPaccelerators. Furthermore, EURETILE investigates and implements the innovationsfor equipping the elementary HW tile with high-bandwidth, low-latencybrain-like inter-tile communication emulating 3 levels of connection hierarchy,namely neural columns, cortical areas and cortex, and develops a dedicatedcortical simulation benchmark: DPSNN-STDP (Distributed Polychronous SpikingNeural Net with synaptic Spiking Time Dependent Plasticity). EURETILE leverageson the multi-tile HW paradigm and SW tool-chain developed by the FET-ACA SHAPESIntegrated Project (2006-2009).
arxiv-3600-156 | A Differential Equations Approach to Optimizing Regret Trade-offs | http://arxiv.org/abs/1305.1359 | author:Alexandr Andoni, Rina Panigrahy category:cs.LG published:2013-05-07 summary:We consider the classical question of predicting binary sequences and studythe {\em optimal} algorithms for obtaining the best possible regret and payofffunctions for this problem. The question turns out to be also equivalent to theproblem of optimal trade-offs between the regrets of two experts in an "expertsproblem", studied before by \cite{kearns-regret}. While, say, a regret of$\Theta(\sqrt{T})$ is known, we argue that it important to ask what is theprovably optimal algorithm for this problem --- both because it leads tonatural algorithms, as well as because regret is in fact often comparable inmagnitude to the final payoffs and hence is a non-negligible term. In the basic setting, the result essentially follows from a classical resultof Cover from '65. Here instead, we focus on another standard setting, oftime-discounted payoffs, where the final "stopping time" is not specified. Weexhibit an explicit characterization of the optimal regret for this setting. To obtain our main result, we show that the optimal payoff functions have tosatisfy the Hermite differential equation, and hence are given by the solutionsto this equation. It turns out that characterization of the payoff function isqualitatively different from the classical (non-discounted) setting, and,namely, there's essentially a unique optimal solution.
arxiv-3600-157 | A new framework for optimal classifier design | http://arxiv.org/abs/1305.1396 | author:Matías Di Martino, Guzman Hernández, Marcelo Fiori, Alicia Fernández category:cs.CV cs.LG stat.ML published:2013-05-07 summary:The use of alternative measures to evaluate classifier performance is gainingattention, specially for imbalanced problems. However, the use of thesemeasures in the classifier design process is still unsolved. In this work wepropose a classifier designed specifically to optimize one of these alternativemeasures, namely, the so-called F-measure. Nevertheless, the technique isgeneral, and it can be used to optimize other evaluation measures. An algorithmto train the novel classifier is proposed, and the numerical scheme is testedwith several databases, showing the optimality and robustness of the presentedclassifier.
arxiv-3600-158 | Standard Fingerprint Databases: Manual Minutiae Labeling and Matcher Performance Analyses | http://arxiv.org/abs/1305.1443 | author:Mehmet Kayaoglu, Berkay Topcu, Umut Uludag category:cs.CV published:2013-05-07 summary:Fingerprint verification and identification algorithms based on minutiaefeatures are used in many biometric systems today (e.g., governmental e-IDprograms, border control, AFIS, personal authentication for portable devices).Researchers in industry/academia are now able to utilize many publiclyavailable fingerprint databases (e.g., Fingerprint Verification Competition(FVC) & NIST databases) to compare/evaluate their feature extraction and/ormatching algorithm performances against those of others. The results from theseevaluations are typically utilized by decision makers responsible forimplementing the cited biometric systems, in selecting/tuning specific sensors,feature extractors and matchers. In this study, for a subset of the citedpublic fingerprint databases, we report fingerprint minutiae matching results,which are based on (i) minutiae extracted automatically from fingerprintimages, and (ii) minutiae extracted manually by human subjects. By doing so, weare able to (i) quantitatively judge the performance differences between thesetwo cases, (ii) elaborate on performance upper bounds of minutiae matching,utilizing what can be termed as "ground truth" minutiae features, (iii) analyzeminutiae matching performance, without coupling it with the minutiae extractionperformance beforehand. Further, as we will freely distribute the minutiaetemplates, originating from this manual labeling study, in a standard minutiaetemplate exchange format (ISO 19794-2), we believe that other researchers inthe biometrics community will be able to utilize the associated results &templates to create their own evaluations pertaining to their fingerprintminutiae extractors/matchers.
arxiv-3600-159 | A Method for Visuo-Spatial Classification of Freehand Shapes Freely Sketched | http://arxiv.org/abs/1305.1520 | author:Ney Renau-Ferrer, Céline Remi category:cs.CV published:2013-05-07 summary:We present the principle and the main steps of a new method for thevisuo-spatial analysis of geometrical sketches recorded online. Visuo-spatialanalysis is a necessary step for multi-level analysis. Multi-level analysissimultaneously allows classification, comparison or clustering of theconstituent parts of a pattern according to their visuo-spatial properties,their procedural strategies, their structural or temporal parameters, or anycombination of two or more of those parameters. The first results provided bythis method concern the comparison of sketches to some perfect patterns ofsimple geometrical figures and the measure of dissimilarity between realsketches. The mean rates of good decision higher than 95% obtained arepromising in both cases.
arxiv-3600-160 | Speech Enhancement Modeling Towards Robust Speech Recognition System | http://arxiv.org/abs/1305.1426 | author:Urmila Shrawankar, V. M. Thakare category:cs.SD cs.CL published:2013-05-07 summary:Form about four decades human beings have been dreaming of an intelligentmachine which can master the natural speech. In its simplest form, this machineshould consist of two subsystems, namely automatic speech recognition (ASR) andspeech understanding (SU). The goal of ASR is to transcribe natural speechwhile SU is to understand the meaning of the transcription. Recognizing andunderstanding a spoken sentence is obviously a knowledge-intensive process,which must take into account all variable information about the speechcommunication process, from acoustics to semantics and pragmatics. Whiledeveloping an Automatic Speech Recognition System, it is observed that someadverse conditions degrade the performance of the Speech Recognition System. Inthis contribution, speech enhancement system is introduced for enhancing speechsignals corrupted by additive noise and improving the performance of AutomaticSpeech Recognizers in noisy conditions. Automatic speech recognitionexperiments show that replacing noisy speech signals by the correspondingenhanced speech signals leads to an improvement in the recognition accuracies.The amount of improvement varies with the type of the corrupting noise.
arxiv-3600-161 | One-Pass AUC Optimization | http://arxiv.org/abs/1305.1363 | author:Wei Gao, Rong Jin, Shenghuo Zhu, Zhi-Hua Zhou category:cs.LG published:2013-05-07 summary:AUC is an important performance measure and many algorithms have been devotedto AUC optimization, mostly by minimizing a surrogate convex loss on a trainingdata set. In this work, we focus on one-pass AUC optimization that requiresonly going through the training data once without storing the entire trainingdataset, where conventional online learning algorithms cannot be applieddirectly because AUC is measured by a sum of losses defined over pairs ofinstances from different classes. We develop a regression-based algorithm whichonly needs to maintain the first and second order statistics of training datain memory, resulting a storage requirement independent from the size oftraining data. To efficiently handle high dimensional data, we develop arandomized algorithm that approximates the covariance matrices by low rankmatrices. We verify, both theoretically and empirically, the effectiveness ofthe proposed algorithm.
arxiv-3600-162 | Somoclu: An Efficient Parallel Library for Self-Organizing Maps | http://arxiv.org/abs/1305.1422 | author:Peter Wittek, Shi Chao Gao, Ik Soo Lim, Li Zhao category:cs.DC cs.MS cs.NE published:2013-05-07 summary:Somoclu is a massively parallel tool for training self-organizing maps onlarge data sets written in C++. It builds on OpenMP for multicore execution,and on MPI for distributing the workload across the nodes in a cluster. It isalso able to boost training by using CUDA if graphics processing units areavailable. A sparse kernel is included, which is useful for high-dimensionalbut sparse data, such as the vector spaces common in text mining workflows.Python, R and MATLAB interfaces facilitate interactive use. Apart from fastexecution, memory use is highly optimized, enabling training large emergentmaps even on a single computer.
arxiv-3600-163 | A Contrario Selection of Optimal Partitions for Image Segmentation | http://arxiv.org/abs/1305.1206 | author:Juan Cardelino, Vicent Caselles, Marcelo Bertalmio, Gregory Randall category:cs.CV published:2013-05-06 summary:We present a novel segmentation algorithm based on a hierarchicalrepresentation of images. The main contribution of this work is to explore thecapabilities of the A Contrario reasoning when applied to the segmentationproblem, and to overcome the limitations of current algorithms within thatframework. This exploratory approach has three main goals. Our first goal is to extend the search space of greedy merging algorithms tothe set of all partitions spanned by a certain hierarchy, and to cast thesegmentation as a selection problem within this space. In this way we increasethe number of tested partitions and thus we potentially improve thesegmentation results. In addition, this space is considerably smaller than thespace of all possible partitions, thus we still keep the complexity controlled. Our second goal aims to improve the locality of region merging algorithms,which usually merge pairs of neighboring regions. In this work, we overcomethis limitation by introducing a validation procedure for complete partitions,rather than for pairs of regions. The third goal is to perform an exhaustive experimental evaluationmethodology in order to provide reproducible results. Finally, we embed the selection process on a statistical A Contrarioframework which allows us to have only one free parameter related to thedesired scale.
arxiv-3600-164 | Gromov-Hausdorff Approximation of Metric Spaces with Linear Structure | http://arxiv.org/abs/1305.1172 | author:Frédéric Chazal, Jian Sun category:cs.CG cs.LG math.MG published:2013-05-06 summary:In many real-world applications data come as discrete metric spaces sampledaround 1-dimensional filamentary structures that can be seen as metric graphs.In this paper we address the metric reconstruction problem of such filamentarystructures from data sampled around them. We prove that they can beapproximated, with respect to the Gromov-Hausdorff distance by well-chosen Reebgraphs (and some of their variants) and we provide an efficient and easy toimplement algorithm to compute such approximations in almost linear time. Weillustrate the performances of our algorithm on a few synthetic and real datasets.
arxiv-3600-165 | New Alignment Methods for Discriminative Book Summarization | http://arxiv.org/abs/1305.1319 | author:David Bamman, Noah A. Smith category:cs.CL published:2013-05-06 summary:We consider the unsupervised alignment of the full text of a book with ahuman-written summary. This presents challenges not seen in other textalignment problems, including a disparity in length and, consequent to this, aviolation of the expectation that individual words and phrases should align,since large passages and chapters can be distilled into a single summaryphrase. We present two new methods, based on hidden Markov models, specificallytargeted to this problem, and demonstrate gains on an extractive booksummarization task. While there is still much room for improvement,unsupervised alignment holds intrinsic value in offering insight into whatfeatures of a book are deemed worthy of summarization.
arxiv-3600-166 | A Computer Vision System for Attention Mapping in SLAM based 3D Models | http://arxiv.org/abs/1305.1163 | author:Lucas Paletta, Katrin Santner, Gerald Fritz, Albert Hofmann, Gerald Lodron, Georg Thallinger, Heinz Mayer category:cs.CV published:2013-05-06 summary:The study of human factors in the frame of interaction studies has beenrelevant for usability engi-neering and ergonomics for decades. Today, with theadvent of wearable eye-tracking and Google glasses, monitoring of human factorswill soon become ubiquitous. This work describes a computer vision system thatenables pervasive mapping and monitoring of human attention. The keycontribu-tion is that our methodology enables full 3D recovery of the gazepointer, human view frustum and associated human centred measurements directlyinto an automatically computed 3D model in real-time. We apply RGB-D SLAM anddescriptor matching methodologies for the 3D modelling, locali-zation and fullyautomated annotation of ROIs (regions of interest) within the acquired 3Dmodel. This innovative methodology will open new avenues for attention studiesin real world environments, bringing new potential into automated processingfor human factors technologies.
arxiv-3600-167 | Techniques for Feature Extraction In Speech Recognition System : A Comparative Study | http://arxiv.org/abs/1305.1145 | author:Urmila Shrawankar, V M Thakare category:cs.SD cs.CL published:2013-05-06 summary:The time domain waveform of a speech signal carries all of the auditoryinformation. From the phonological point of view, it little can be said on thebasis of the waveform itself. However, past research in mathematics, acoustics,and speech technology have provided many methods for converting data that canbe considered as information if interpreted correctly. In order to find somestatistically relevant information from incoming data, it is important to havemechanisms for reducing the information of each segment in the audio signalinto a relatively small number of parameters, or features. These featuresshould describe each segment in such a characteristic way that other similarsegments can be grouped together by comparing their features. There areenormous interesting and exceptional ways to describe the speech signal interms of parameters. Though, they all have their strengths and weaknesses, wehave presented some of the most used methods with their importance.
arxiv-3600-168 | A Convex Functional for Image Denoising based on Patches with Constrained Overlaps and its vectorial application to Low Dose Differential Phase Tomography | http://arxiv.org/abs/1305.1256 | author:Alessandro Mirone, Emmanuel Brun, Paola Coan category:math.NA cs.CV published:2013-05-06 summary:We solve the image denoising problem with a dictionary learning technique bywriting a convex functional of a new form. This functional contains beside theusual sparsity inducing term and fidelity term, a new term which inducessimilarity between overlapping patches in the overlap regions. The functionaldepends on two free regularization parameters: a coefficient multiplying thesparsity-inducing $L_{1}$ norm of the patch basis functions coefficients, and acoefficient multiplying the $L_{2}$ norm of the differences between patches inthe overlapping regions. The solution is found by applying the iterativeproximal gradient descent method with FISTA acceleration. In the case oftomography reconstruction we calculate the gradient by applying projection ofthe solution and its error backprojection at each iterative step. We study thequality of the solution, as a function of the regularization parameters andnoise, on synthetic datas for which the solution is a-priori known. We applythe method on experimental data in the case of Differential Phase Tomography.For this case we use an original approach which consists in using vectorialpatches, each patch having two components: one per each gradient component. Theresulting algorithm, implemented in the ESRF tomography reconstruction codePyHST, results to be robust, efficient, and well adapted to strongly reduce therequired dose and the number of projections in medical tomography.
arxiv-3600-169 | Speckle Noise Reduction in Medical Ultrasound Images | http://arxiv.org/abs/1305.1344 | author:Faouzi Benzarti, Hamid Amiri category:cs.CV published:2013-05-06 summary:Ultrasound imaging is an incontestable vital tool for diagnosis, it providesin non-invasive manner the internal structure of the body to detect eventuallydiseases or abnormalities tissues. Unfortunately, the presence of speckle noisein these images affects edges and fine details which limit the contrastresolution and make diagnostic more difficult. In this paper, we propose adenoising approach which combines logarithmic transformation and a non lineardiffusion tensor. Since speckle noise is multiplicative and nonwhite process,the logarithmic transformation is a reasonable choice to convertsignaldependent or pure multiplicative noise to an additive one. The key ideafrom using diffusion tensor is to adapt the flow diffusion towards the localorientation by applying anisotropic diffusion along the coherent structuredirection of interesting features in the image. To illustrate the effectiveperformance of our algorithm, we present some experimental results onsynthetically and real echographic images.
arxiv-3600-170 | Towards an Author-Topic-Term-Model Visualization of 100 Years of German Sociological Society Proceedings | http://arxiv.org/abs/1305.1343 | author:Arnim Bleier, Andreas Strotmann category:cs.DL cs.CL cs.IR published:2013-05-06 summary:Author co-citation studies employ factor analysis to reduce high-dimensionalco-citation matrices to low-dimensional and possibly interpretable factors, butthese studies do not use any information from the text bodies of publications.We hypothesise that term frequencies may yield useful information forscientometric analysis. In our work we ask if word features in combination withBayesian analysis allow well-founded science mapping studies. This work goesback to the roots of Mosteller and Wallace's (1964) statistical text analysisusing word frequency features and a Bayesian inference approach, tough withdifferent goals. To answer our research question we (i) introduce a new dataset on which the experiments are carried out, (ii) describe the Bayesian modelemployed for inference and (iii) present first results of the analysis.
arxiv-3600-171 | How to find real-world applications for compressive sensing | http://arxiv.org/abs/1305.1199 | author:Leslie N. Smith category:cs.CV published:2013-05-06 summary:The potential of compressive sensing (CS) has spurred great interest in theresearch community and is a fast growing area of research. However, researchtranslating CS theory into practical hardware and demonstrating clear andsignificant benefits with this hardware over current, conventional imagingtechniques has been limited. This article helps researchers to find those nicheapplications where the CS approach provides substantial gain over conventionalapproaches by articulating lessons learned in finding one such application; seaskimming missile detection. As a proof of concept, it is demonstrated that asimplified CS missile detection architecture and algorithm provides comparableresults to the conventional imaging approach but using a smaller FPA. Theprimary message is that all of the excitement surrounding CS is necessary andappropriate for encouraging our creativity but we all must also take off our"rose colored glasses" and critically judge our ideas, methods and resultsrelative to conventional imaging approaches.
arxiv-3600-172 | On the Convergence and Consistency of the Blurring Mean-Shift Process | http://arxiv.org/abs/1305.1040 | author:Ting-Li Chen category:stat.ML cs.LG published:2013-05-05 summary:The mean-shift algorithm is a popular algorithm in computer vision and imageprocessing. It can also be cast as a minimum gamma-divergence estimation. Inthis paper we focus on the "blurring" mean shift algorithm, which is oneversion of the mean-shift process that successively blurs the dataset. Theanalysis of the blurring mean-shift is relatively more complicated compared tothe nonblurring version, yet the algorithm convergence and the estimationconsistency have not been well studied in the literature. In this paper weprove both the convergence and the consistency of the blurring mean-shift. Wealso perform simulation studies to compare the efficiency of the blurring andthe nonblurring versions of the mean-shift algorithms. Our results show thatthe blurring mean-shift has more efficiency.
arxiv-3600-173 | Hybridization of Otsu Method and Median Filter for Color Image Segmentation | http://arxiv.org/abs/1305.1052 | author:Firas Ajil Jassim, Fawzi H. Altaani category:cs.CV published:2013-05-05 summary:In this article a novel algorithm for color image segmentation has beendeveloped. The proposed algorithm based on combining two existing methods insuch a novel way to obtain a significant method to partition the color imageinto significant regions. On the first phase, the traditional Otsu method forgray channel image segmentation were applied for each of the R,G, and Bchannels separately to determine the suitable automatic threshold for eachchannel. After that, the new modified channels are integrated again toformulate a new color image. The resulted image suffers from some kind ofdistortion. To get rid of this distortion, the second phase is arise which isthe median filter to smooth the image and increase the segmented regions. Thisprocess looks very significant by the ocular eye. Experimental results werepresented on a variety of test images to support the proposed algorithm.
arxiv-3600-174 | Efficient Estimation of the number of neighbours in Probabilistic K Nearest Neighbour Classification | http://arxiv.org/abs/1305.1002 | author:Ji Won Yoon, Nial Friel category:cs.LG stat.ML published:2013-05-05 summary:Probabilistic k-nearest neighbour (PKNN) classification has been introducedto improve the performance of original k-nearest neighbour (KNN) classificationalgorithm by explicitly modelling uncertainty in the classification of eachfeature vector. However, an issue common to both KNN and PKNN is to select theoptimal number of neighbours, $k$. The contribution of this paper is toincorporate the uncertainty in $k$ into the decision making, and in so doinguse Bayesian model averaging to provide improved classification. Indeed theproblem of assessing the uncertainty in $k$ can be viewed as one of statisticalmodel selection which is one of the most important technical issues in thestatistics and machine learning domain. In this paper, a new functionalapproximation algorithm is proposed to reconstruct the density of the model(order) without relying on time consuming Monte Carlo simulations. In addition,this algorithm avoids cross validation by adopting Bayesian framework. Theperformance of this algorithm yielded very good performance on several realexperimental datasets.
arxiv-3600-175 | Regret Bounds for Reinforcement Learning with Policy Advice | http://arxiv.org/abs/1305.1027 | author:Mohammad Gheshlaghi Azar, Alessandro Lazaric, Emma Brunskill category:stat.ML cs.LG published:2013-05-05 summary:In some reinforcement learning problems an agent may be provided with a setof input policies, perhaps learned from prior experience or provided byadvisors. We present a reinforcement learning with policy advice (RLPA)algorithm which leverages this input set and learns to use the best policy inthe set for the reinforcement learning task at hand. We prove that RLPA has asub-linear regret of \tilde O(\sqrt{T}) relative to the best input policy, andthat both this regret and its computational complexity are independent of thesize of the state and action space. Our empirical simulations support ourtheoretical analysis. This suggests RLPA may offer significant advantages inlarge domains where some prior good policies are provided.
arxiv-3600-176 | Simple Deep Random Model Ensemble | http://arxiv.org/abs/1305.1019 | author:Xiao-Lei Zhang, Ji Wu category:cs.LG published:2013-05-05 summary:Representation learning and unsupervised learning are two central topics ofmachine learning and signal processing. Deep learning is one of the mosteffective unsupervised representation learning approach. The main contributionsof this paper to the topics are as follows. (i) We propose to view therepresentative deep learning approaches as special cases of the knowledge reuseframework of clustering ensemble. (ii) We propose to view sparse coding whenused as a feature encoder as the consensus function of clustering ensemble, andview dictionary learning as the training process of the base clusterings ofclustering ensemble. (ii) Based on the above two views, we propose a verysimple deep learning algorithm, named deep random model ensemble (DRME). It isa stack of random model ensembles. Each random model ensemble is a specialk-means ensemble that discards the expectation-maximization optimization ofeach base k-means but only preserves the default initialization method of thebase k-means. (iv) We propose to select the most powerful representation amongthe layers by applying DRME to clustering where the single-linkage is used asthe clustering algorithm. Moreover, the DRME based clustering can also detectthe number of the natural clusters accurately. Extensive experimentalcomparisons with 5 representation learning methods on 19 benchmark data setsdemonstrate the effectiveness of DRME.
arxiv-3600-177 | Dictionary learning based image enhancement for rarity detection | http://arxiv.org/abs/1305.0871 | author:Weifeng Liu, Xiaomeng Wang, Yanjiang Wang category:cs.CV published:2013-05-04 summary:Image enhancement is an important image processing technique that processesimages suitably for a specific application e.g. image editing. The conventionalsolutions of image enhancement are grouped into two categories which arespatial domain processing method and transform domain processing method such ascontrast manipulation, histogram equalization, homomorphic filtering. Thisletter proposes a new image enhance method based on dictionary learning.Particularly, the proposed method adjusts the image by manipulating the rarityof dictionary atoms. Firstly, learn the dictionary through sparse codingalgorithms on divided sub-image blocks. Secondly, compute the rarity ofdictionary atoms on statistics of the corresponding sparse coefficients.Thirdly, adjust the rarity according to specific application and form a newdictionary. Finally, reconstruct the image using the updated dictionary andsparse coefficients. Compared with the traditional techniques, the proposedmethod enhances image based on the image content not on distribution of pixelgrey value or frequency. The advantages of the proposed method lie in that itis in better correspondence with the response of the human visual system andmore suitable for salient objects extraction. The experimental resultsdemonstrate the effectiveness of the proposed image enhance method.
arxiv-3600-178 | On Comparison between Evolutionary Programming Network-based Learning and Novel Evolution Strategy Algorithm-based Learning | http://arxiv.org/abs/1305.0922 | author:M. A. Khayer Azad, Md. Shafiqul Islam, M. M. A. Hashem category:cs.NE cs.LG published:2013-05-04 summary:This paper presents two different evolutionary systems - EvolutionaryProgramming Network (EPNet) and Novel Evolutions Strategy (NES) Algorithm.EPNet does both training and architecture evolution simultaneously, whereas NESdoes a fixed network and only trains the network. Five mutation operatorsproposed in EPNet to reflect the emphasis on evolving ANNs behaviors. Closebehavioral links between parents and their offspring are maintained by variousmutations, such as partial training and node splitting. On the other hand, NESuses two new genetic operators - subpopulation-based max-mean arithmeticalcrossover and time-variant mutation. The above-mentioned two algorithms havebeen tested on a number of benchmark problems, such as the medical diagnosisproblems (breast cancer, diabetes, and heart disease). The results and thecomparison between them are also presented in this paper.
arxiv-3600-179 | CONATION: English Command Input/Output System for Computers | http://arxiv.org/abs/1305.0625 | author:Kamlesh Sharma, Dr. T. V. Prasad category:cs.HC cs.CL published:2013-05-03 summary:In this information technology age, a convenient and user friendly interfaceis required to operate the computer system on very fast rate. In the humanbeing, speech being a natural mode of communication has potential to being afast and convenient mode of interaction with computer. Speech recognition willplay an important role in taking technology to them. It is the need of this erato access the information within seconds. This paper describes the design anddevelopment of speaker independent and English command interpreted system forcomputers. HMM model is used to represent the phoneme like speech commands.Experiments have been done on real world data and system has been trained innormal condition for real world subject.
arxiv-3600-180 | Marginal AMP Chain Graphs | http://arxiv.org/abs/1305.0751 | author:Jose M. Peña category:stat.ML cs.AI published:2013-05-03 summary:We present a new family of models that is based on graphs that may haveundirected, directed and bidirected edges. We name these new models marginalAMP (MAMP) chain graphs because each of them is Markov equivalent to some AMPchain graph under marginalization of some of its nodes. However, MAMP chaingraphs do not only subsume AMP chain graphs but also multivariate regressionchain graphs. We describe global and pairwise Markov properties for MAMP chaingraphs and prove their equivalence for compositional graphoids. We alsocharacterize when two MAMP chain graphs are Markov equivalent. For Gaussian probability distributions, we also show that every MAMP chaingraph is Markov equivalent to some directed and acyclic graph withdeterministic nodes under marginalization and conditioning on some of itsnodes. This is important because it implies that the independence modelrepresented by a MAMP chain graph can be accounted for by some data generatingprocess that is partially observed and has selection bias. Finally, we modifyMAMP chain graphs so that they are closed under marginalization for Gaussianprobability distributions. This is a desirable feature because it guaranteesparsimonious models under marginalization.
arxiv-3600-181 | Spectral Classification Using Restricted Boltzmann Machine | http://arxiv.org/abs/1305.0665 | author:Fuqiang Chen, Yan Wu, Yude Bu, Guodong Zhao category:cs.LG published:2013-05-03 summary:In this study, a novel machine learning algorithm, restricted Boltzmannmachine (RBM), is introduced. The algorithm is applied for the spectralclassification in astronomy. RBM is a bipartite generative graphical model withtwo separate layers (one visible layer and one hidden layer), which can extracthigher level features to represent the original data. Despite generative, RBMcan be used for classification when modified with a free energy and a soft-maxfunction. Before spectral classification, the original data is binarizedaccording to some rule. Then we resort to the binary RBM to classifycataclysmic variables (CVs) and non-CVs (one half of all the given data fortraining and the other half for testing). The experiment result showsstate-of-the-art accuracy of 100%, which indicates the efficiency of the binaryRBM algorithm.
arxiv-3600-182 | Quantifying the Impact of Parameter Tuning on Nature-Inspired Algorithms | http://arxiv.org/abs/1305.0763 | author:Matthew Crossley, Andy Nisbet, Martyn Amos category:cs.NE published:2013-05-03 summary:The problem of parameterization is often central to the effective deploymentof nature-inspired algorithms. However, finding the optimal set of parametervalues for a combination of problem instance and solution method is highlychallenging, and few concrete guidelines exist on how and when such tuning maybe performed. Previous work tends to either focus on a specific algorithm oruse benchmark problems, and both of these restrictions limit the applicabilityof any findings. Here, we examine a number of different algorithms, and studythem in a "problem agnostic" fashion (i.e., one that is not tied to specificinstances) by considering their performance on fitness landscapes with varyingcharacteristics. Using this approach, we make a number of observations on whichalgorithms may (or may not) benefit from tuning, and in which specificcircumstances.
arxiv-3600-183 | Inference in Kingman's Coalescent with Particle Markov Chain Monte Carlo Method | http://arxiv.org/abs/1305.0855 | author:Yifei Chen, Xiaohui Xie category:stat.ML q-bio.PE published:2013-05-03 summary:We propose a new algorithm to do posterior sampling of Kingman's coalescent,based upon the Particle Markov Chain Monte Carlo methodology. Specifically, thealgorithm is an instantiation of the Particle Gibbs Sampling method, whichalternately samples coalescent times conditioned on coalescent tree structures,and tree structures conditioned on coalescent times via the conditionalSequential Monte Carlo procedure. We implement our algorithm as a C++ package,and demonstrate its utility via a parameter estimation task in populationgenetics on both single- and multiple-locus data. The experiment results showthat the proposed algorithm performs comparable to or better than severalwell-developed methods.
arxiv-3600-184 | An Improved EM algorithm | http://arxiv.org/abs/1305.0626 | author:Fuqiang Chen category:cs.LG cs.AI stat.ML published:2013-05-03 summary:In this paper, we firstly give a brief introduction of expectationmaximization (EM) algorithm, and then discuss the initial value sensitivity ofexpectation maximization algorithm. Subsequently, we give a short proof of EM'sconvergence. Then, we implement experiments with the expectation maximizationalgorithm (We implement all the experiments on Gaussion mixture model (GMM)).Our experiment with expectation maximization is performed in the followingthree cases: initialize randomly; initialize with result of K-means; initializewith result of K-medoids. The experiment result shows that expectationmaximization algorithm depend on its initial state or parameters. And we foundthat EM initialized with K-medoids performed better than both the oneinitialized with K-means and the one initialized randomly.
arxiv-3600-185 | Learning from Imprecise and Fuzzy Observations: Data Disambiguation through Generalized Loss Minimization | http://arxiv.org/abs/1305.0698 | author:Eyke Hüllermeier category:cs.LG published:2013-05-03 summary:Methods for analyzing or learning from "fuzzy data" have attracted increasingattention in recent years. In many cases, however, existing methods (forprecise, non-fuzzy data) are extended to the fuzzy case in an ad-hoc manner,and without carefully considering the interpretation of a fuzzy set when beingused for modeling data. Distinguishing between an ontic and an epistemicinterpretation of fuzzy set-valued data, and focusing on the latter, we arguethat a "fuzzification" of learning algorithms based on an application of thegeneric extension principle is not appropriate. In fact, the extensionprinciple fails to properly exploit the inductive bias underlying statisticaland machine learning methods, although this bias, at least in principle, offersa means for "disambiguating" the fuzzy data. Alternatively, we thereforepropose a method which is based on the generalization of loss functions inempirical risk minimization, and which performs model identification and datadisambiguation simultaneously. Elaborating on the fuzzification of specifictypes of losses, we establish connections to well-known loss functions inregression and classification. We compare our approach with related methods andillustrate its use in logistic regression for binary classification.
arxiv-3600-186 | Anisotropic oracle inequalities in noisy quantization | http://arxiv.org/abs/1305.0630 | author:Sébastien Loustau category:math.ST stat.ML stat.TH published:2013-05-03 summary:The effect of errors in variables in quantization is investigated. We provegeneral exact and non-exact oracle inequalities with fast rates for anempirical minimization based on a noisy sample$Z_i=X_i+\epsilon_i,i=1,\ldots,n$, where $X_i$ are i.i.d. with density $f$ and$\epsilon_i$ are i.i.d. with density $\eta$. These rates depend on the geometryof the density $f$ and the asymptotic behaviour of the characteristic functionof $\eta$. This general study can be applied to the problem of $k$-means clustering withnoisy data. For this purpose, we introduce a deconvolution $k$-means stochasticminimization which reaches fast rates of convergence under standard Pollard'sregularity assumptions.
arxiv-3600-187 | Feature Selection Based on Term Frequency and T-Test for Text Categorization | http://arxiv.org/abs/1305.0638 | author:Deqing Wang, Hui Zhang, Rui Liu, Weifeng Lv category:cs.LG cs.IR stat.ML published:2013-05-03 summary:Much work has been done on feature selection. Existing methods are based ondocument frequency, such as Chi-Square Statistic, Information Gain etc.However, these methods have two shortcomings: one is that they are not reliablefor low-frequency terms, and the other is that they only count whether one termoccurs in a document and ignore the term frequency. Actually, high-frequencyterms within a specific category are often regards as discriminators. This paper focuses on how to construct the feature selection function basedon term frequency, and proposes a new approach based on $t$-test, which is usedto measure the diversity of the distributions of a term between the specificcategory and the entire corpus. Extensive comparative experiments on two textcorpora using three classifiers show that our new approach is comparable to oror slightly better than the state-of-the-art feature selection methods (i.e.,$\chi^2$, and IG) in terms of macro-$F_1$ and micro-$F_1$.
arxiv-3600-188 | Learning Mixtures of Bernoulli Templates by Two-Round EM with Performance Guarantee | http://arxiv.org/abs/1305.0319 | author:Adrian Barbu, Tianfu Wu, Ying Nian Wu category:stat.ML published:2013-05-02 summary:Dasgupta and Shulman showed that a two-round variant of the EM algorithm canlearn mixture of Gaussian distributions with near optimal precision with highprobability if the Gaussian distributions are well separated and if thedimension is sufficiently high. In this paper, we generalize their theory tolearning mixture of high-dimensional Bernoulli templates. Each template is abinary vector, and a template generates examples by randomly switching itsbinary components independently with a certain probability. In computer visionapplications, a binary vector is a feature map of an image, where each binarycomponent indicates whether a local feature or structure is present or absentwithin a certain cell of the image domain. A Bernoulli template can beconsidered as a statistical model for images of objects (or parts of objects)from the same category. We show that the two-round EM algorithm can learnmixture of Bernoulli templates with near optimal precision with highprobability, if the Bernoulli templates are sufficiently different and if thenumber of features is sufficiently high. We illustrate the theoretical resultsby synthetic and real examples.
arxiv-3600-189 | Tensor Decompositions: A New Concept in Brain Data Analysis? | http://arxiv.org/abs/1305.0395 | author:Andrzej Cichocki category:cs.NA cs.LG q-bio.NC stat.ML published:2013-05-02 summary:Matrix factorizations and their extensions to tensor factorizations anddecompositions have become prominent techniques for linear and multilinearblind source separation (BSS), especially multiway Independent ComponentAnalysis (ICA), NonnegativeMatrix and Tensor Factorization (NMF/NTF), SmoothComponent Analysis (SmoCA) and Sparse Component Analysis (SCA). Moreover,tensor decompositions have many other potential applications beyond multilinearBSS, especially feature extraction, classification, dimensionality reductionand multiway clustering. In this paper, we briefly overview new and emergingmodels and approaches for tensor decompositions in applications to group andlinked multiway BSS/ICA, feature extraction, classification andMultiway PartialLeast Squares (MPLS) regression problems. Keywords: Multilinear BSS, linkedmultiway BSS/ICA, tensor factorizations and decompositions, constrained Tuckerand CP models, Penalized Tensor Decompositions (PTD), feature extraction,classification, multiway PLS and CCA.
arxiv-3600-190 | A quantum teleportation inspired algorithm produces sentence meaning from word meaning and grammatical structure | http://arxiv.org/abs/1305.0556 | author:Stephen Clark, Bob Coecke, Edward Grefenstette, Stephen Pulman, Mehrnoosh Sadrzadeh category:cs.CL quant-ph 68T50 I.2.7 published:2013-05-02 summary:We discuss an algorithm which produces the meaning of a sentence givenmeanings of its words, and its resemblance to quantum teleportation. In fact,this protocol was the main source of inspiration for this algorithm which hasmany applications in the area of Natural Language Processing.
arxiv-3600-191 | Testing Hypotheses by Regularized Maximum Mean Discrepancy | http://arxiv.org/abs/1305.0423 | author:Somayeh Danafar, Paola M. V. Rancoita, Tobias Glasmachers, Kevin Whittingstall, Juergen Schmidhuber category:cs.LG cs.AI stat.ML published:2013-05-02 summary:Do two data samples come from different distributions? Recent studies of thisfundamental problem focused on embedding probability distributions intosufficiently rich characteristic Reproducing Kernel Hilbert Spaces (RKHSs), tocompare distributions by the distance between their embeddings. We show thatRegularized Maximum Mean Discrepancy (RMMD), our novel measure for kernel-basedhypothesis testing, yields substantial improvements even when sample sizes aresmall, and excels at hypothesis tests involving multiple comparisons with powercontrol. We derive asymptotic distributions under the null and alternativehypotheses, and assess power control. Outstanding results are obtained on:challenging EEG data, MNIST, the Berkley Covertype, and the Flare-Solardataset.
arxiv-3600-192 | Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition | http://arxiv.org/abs/1305.0355 | author:Adel Javanmard, Andrea Montanari category:math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH published:2013-05-02 summary:In the high-dimensional regression model a response variable is linearlyrelated to $p$ covariates, but the sample size $n$ is smaller than $p$. Weassume that only a small subset of covariates is `active' (i.e., thecorresponding coefficients are non-zero), and consider the model-selectionproblem of identifying the active covariates. A popular approach is to estimatethe regression coefficients through the Lasso ($\ell_1$-regularized leastsquares). This is known to correctly identify the active set only if theirrelevant covariates are roughly orthogonal to the relevant ones, asquantified through the so called `irrepresentability' condition. In this paperwe study the `Gauss-Lasso' selector, a simple two-stage method that firstsolves the Lasso, and then performs ordinary least squares restricted to theLasso active set. We formulate `generalized irrepresentability condition'(GIC), an assumption that is substantially weaker than irrepresentability. Weprove that, under GIC, the Gauss-Lasso correctly recovers the active set.
arxiv-3600-193 | Deep Learning of Representations: Looking Forward | http://arxiv.org/abs/1305.0445 | author:Yoshua Bengio category:cs.LG published:2013-05-02 summary:Deep learning research aims at discovering learning algorithms that discovermultiple levels of distributed representations, with higher levels representingmore abstract concepts. Although the study of deep learning has already led toimpressive theoretical results, learning algorithms and breakthroughexperiments, several challenges lie ahead. This paper proposes to examine someof these challenges, centering on the questions of scaling deep learningalgorithms to much larger models and datasets, reducing optimizationdifficulties due to ill-conditioning or local minima, designing more efficientand powerful inference and sampling procedures, and learning to disentangle thefactors of variation underlying the observed data. It also proposes a fewforward-looking research directions aimed at overcoming these challenges.
arxiv-3600-194 | Quantile Regression for Large-scale Applications | http://arxiv.org/abs/1305.0087 | author:Jiyan Yang, Xiangrui Meng, Michael W. Mahoney category:cs.DS cs.DC cs.NA stat.ML published:2013-05-01 summary:Quantile regression is a method to estimate the quantiles of the conditionaldistribution of a response variable, and as such it permits a much moreaccurate portrayal of the relationship between the response variable andobserved covariates than methods such as Least-squares or Least AbsoluteDeviations regression. It can be expressed as a linear program, and, withappropriate preprocessing, interior-point methods can be used to find asolution for moderately large problems. Dealing with very large problems,\emph(e.g.), involving data up to and beyond the terabyte regime, remains achallenge. Here, we present a randomized algorithm that runs in nearly lineartime in the size of the input and that, with constant probability, computes a$(1+\epsilon)$ approximate solution to an arbitrary quantile regressionproblem. As a key step, our algorithm computes a low-distortionsubspace-preserving embedding with respect to the loss function of quantileregression. Our empirical evaluation illustrates that our algorithm iscompetitive with the best previous work on small to medium-sized problems, andthat in addition it can be implemented in MapReduce-like environments andapplied to terabyte-sized problems.
arxiv-3600-195 | An Adaptive Descriptor Design for Object Recognition in the Wild | http://arxiv.org/abs/1305.0311 | author:Zhenyu Guo, Z. Jane Wang category:cs.CV published:2013-05-01 summary:Digital images nowadays have various styles of appearance, in the aspects ofcolor tones, contrast, vignetting, and etc. These 'picture styles' are directlyrelated to the scene radiance, image pipeline of the camera, and postprocessing functions. Due to the complexity and nonlinearity of these causes,popular gradient-based image descriptors won't be invariant to differentpicture styles, which will decline the performance of object recognition. Giventhat images shared online or created by individual users are taken with a widerange of devices and may be processed by various post processing functions, tofind a robust object recognition system is useful and challenging. In thispaper, we present the first study on the influence of picture styles for objectrecognition, and propose an adaptive approach based on the kernel view ofgradient descriptors and multiple kernel learning, without estimating orspecifying the styles of images used in training and testing. We conductexperiments on Domain Adaptation data set and Oxford Flower data set. Theexperiments also include several variants of the flower data set by processingthe images with popular photo effects. The results demonstrate that ourproposed method improve from standard descriptors in all cases.
arxiv-3600-196 | Clustering Unclustered Data: Unsupervised Binary Labeling of Two Datasets Having Different Class Balances | http://arxiv.org/abs/1305.0103 | author:Marthinus Christoffel du Plessis, Masashi Sugiyama category:cs.LG published:2013-05-01 summary:We consider the unsupervised learning problem of assigning labels tounlabeled data. A naive approach is to use clustering methods, but this workswell only when data is properly clustered and each cluster corresponds to anunderlying class. In this paper, we first show that this unsupervised labelingproblem in balanced binary cases can be solved if two unlabeled datasets havingdifferent class balances are available. More specifically, estimation of thesign of the difference between probability densities of two unlabeled datasetsgives the solution. We then introduce a new method to directly estimate thesign of the density difference without density estimation. Finally, wedemonstrate the usefulness of the proposed method against several clusteringmethods on various toy problems and real-world datasets.
arxiv-3600-197 | Recovering Graph-Structured Activations using Adaptive Compressive Measurements | http://arxiv.org/abs/1305.0213 | author:Akshay Krishnamurthy, James Sharpnack, Aarti Singh category:stat.ML cs.IT math.IT published:2013-05-01 summary:We study the localization of a cluster of activated vertices in a graph, fromadaptively designed compressive measurements. We propose a hierarchicalpartitioning of the graph that groups the activated vertices into fewpartitions, so that a top-down sensing procedure can identify these partitions,and hence the activations, using few measurements. By exploiting the clusterstructure, we are able to provide localization guarantees at weaker signal tonoise ratios than in the unstructured setting. We complement this performanceguarantee with an information theoretic lower bound, providing a necessarysignal-to-noise ratio for any algorithm to successfully localize the cluster.We verify our analysis with some simulations, demonstrating the practicality ofour algorithm.
arxiv-3600-198 | MATAWS: A Multimodal Approach for Automatic WS Semantic Annotation | http://arxiv.org/abs/1305.0194 | author:Cihan Aksoy, Vincent Labatut, Chantal Cherifi, Jean-François Santucci category:cs.SE cs.CL cs.IR published:2013-05-01 summary:Many recent works aim at developing methods and tools for the processing ofsemantic Web services. In order to be properly tested, these tools must beapplied to an appropriate benchmark, taking the form of a collection ofsemantic WS descriptions. However, all of the existing publicly availablecollections are limited by their size or their realism (use of randomlygenerated or resampled descriptions). Larger and realistic syntactic (WSDL)collections exist, but their semantic annotation requires a certain level ofautomation, due to the number of operations to be processed. In this article,we propose a fully automatic method to semantically annotate such large WScollections. Our approach is multimodal, in the sense it takes advantage of thelatent semantics present not only in the parameter names, but also in the typenames and structures. Concept-to-word association is performed by using Sigma,a mapping of WordNet to the SUMO ontology. After having described in detailsour annotation method, we apply it to the larger collection of real-worldsyntactic WS descriptions we could find, and assess its efficiency.
arxiv-3600-199 | Video Segmentation via Diffusion Bases | http://arxiv.org/abs/1305.0218 | author:Dina Dushnik, Alon Schclar, Amir Averbuch category:cs.CV cs.MM published:2013-05-01 summary:Identifying moving objects in a video sequence, which is produced by a staticcamera, is a fundamental and critical task in many computer-visionapplications. A common approach performs background subtraction, whichidentifies moving objects as the portion of a video frame that differssignificantly from a background model. A good background subtraction algorithmhas to be robust to changes in the illumination and it should avoid detectingnon-stationary background objects such as moving leaves, rain, snow, andshadows. In addition, the internal background model should quickly respond tochanges in background such as objects that start to move or stop. We present anew algorithm for video segmentation that processes the input video sequence asa 3D matrix where the third axis is the time domain. Our approach identifiesthe background by reducing the input dimension using the \emph{diffusion bases}methodology. Furthermore, we describe an iterative method for extracting anddeleting the background. The algorithm has two versions and thus covers thecomplete range of backgrounds: one for scenes with static backgrounds and theother for scenes with dynamic (moving) backgrounds.
arxiv-3600-200 | Inverting Nonlinear Dimensionality Reduction with Scale-Free Radial Basis Function Interpolation | http://arxiv.org/abs/1305.0258 | author:Nathan D. Monnig, Bengt Fornberg, Francois G. Meyer category:math.NA cs.NA stat.ML published:2013-05-01 summary:Nonlinear dimensionality reduction embeddings computed from datasets do notprovide a mechanism to compute the inverse map. In this paper, we address theproblem of computing a stable inverse map to such a general bi-Lipschitz map.Our approach relies on radial basis functions (RBFs) to interpolate the inversemap everywhere on the low-dimensional image of the forward map. We demonstratethat the scale-free cubic RBF kernel performs better than the Gaussian kernel:it does not suffer from ill-conditioning, and does not require the choice of ascale. The proposed construction is shown to be similar to the Nystr\"omextension of the eigenvectors of the symmetric normalized graph Laplacianmatrix. Based on this observation, we provide a new interpretation of theNystr\"om extension with suggestions for improvement.
arxiv-3600-201 | Perceptron Mistake Bounds | http://arxiv.org/abs/1305.0208 | author:Mehryar Mohri, Afshin Rostamizadeh category:cs.LG published:2013-05-01 summary:We present a brief survey of existing mistake bounds and introduce novelbounds for the Perceptron or the kernel Perceptron algorithm. Our novel boundsgeneralize beyond standard margin-loss type bounds, allow for any convex andLipschitz loss function, and admit a very simple proof.
arxiv-3600-202 | Uniqueness of Tensor Decompositions with Applications to Polynomial Identifiability | http://arxiv.org/abs/1304.8087 | author:Aditya Bhaskara, Moses Charikar, Aravindan Vijayaraghavan category:cs.DS cs.LG math.ST stat.TH published:2013-04-30 summary:We give a robust version of the celebrated result of Kruskal on theuniqueness of tensor decompositions: we prove that given a tensor whosedecomposition satisfies a robust form of Kruskal's rank condition, it ispossible to approximately recover the decomposition if the tensor is known upto a sufficiently small (inverse polynomial) error. Kruskal's theorem has found many applications in proving the identifiabilityof parameters for various latent variable models and mixture models such asHidden Markov models, topic models etc. Our robust version immediately impliesidentifiability using only polynomially many samples in many of these settings.This polynomial identifiability is an essential first step towards efficientlearning algorithms for these models. Recently, algorithms based on tensor decompositions have been used toestimate the parameters of various hidden variable models efficiently inspecial cases as long as they satisfy certain "non-degeneracy" properties. Ourmethods give a way to go beyond this non-degeneracy barrier, and establishpolynomial identifiability of the parameters under much milder conditions.Given the importance of Kruskal's theorem in the tensor literature, we expectthat this robust version will have several applications beyond the settings weexplore in this work.
arxiv-3600-203 | Robust Spectral Compressed Sensing via Structured Matrix Completion | http://arxiv.org/abs/1304.8126 | author:Yuxin Chen, Yuejie Chi category:cs.IT cs.SY math.IT math.NA stat.ML published:2013-04-30 summary:The paper explores the problem of \emph{spectral compressed sensing}, whichaims to recover a spectrally sparse signal from a small random subset of its$n$ time domain samples. The signal of interest is assumed to be asuperposition of $r$ multi-dimensional complex sinusoids, while the underlyingfrequencies can assume any \emph{continuous} values in the normalized frequencydomain. Conventional compressed sensing paradigms suffer from the basismismatch issue when imposing a discrete dictionary on the Fourierrepresentation. To address this issue, we develop a novel algorithm, called\emph{Enhanced Matrix Completion (EMaC)}, based on structured matrix completionthat does not require prior knowledge of the model order. The algorithm startsby arranging the data into a low-rank enhanced form exhibiting multi-foldHankel structure, and then attempts recovery via nuclear norm minimization.Under mild incoherence conditions, EMaC allows perfect recovery as soon as thenumber of samples exceeds the order of $r\log^{4}n$, and is stable againstbounded noise. Even if a constant portion of samples are corrupted witharbitrary magnitude, EMaC still allows exact recovery, provided that the samplecomplexity exceeds the order of $r^{2}\log^{3}n$. Along the way, our resultsdemonstrate the power of convex relaxation in completing a low-rank multi-foldHankel or Toeplitz matrix from minimal observed entries. The performance of ouralgorithm and its applicability to super resolution are further validated bynumerical experiments.
arxiv-3600-204 | North Atlantic Right Whale Contact Call Detection | http://arxiv.org/abs/1304.7851 | author:Rami Abousleiman, Guangzhi Qu, Osamah Rawashdeh category:cs.LG cs.SD published:2013-04-30 summary:The North Atlantic right whale (Eubalaena glacialis) is an endangeredspecies. These whales continuously suffer from deadly vessel impacts alongsidethe eastern coast of North America. There have been countless efforts to savethe remaining 350 - 400 of them. One of the most prominent works is done byMarinexplore and Cornell University. A system of hydrophones linked tosatellite connected-buoys has been deployed in the whales habitat. Thesehydrophones record and transmit live sounds to a base station. These recordingmight contain the right whale contact call as well as many other noises. Thenoise rate increases rapidly in vessel-busy areas such as by the Boston harbor.This paper presents and studies the problem of detecting the North Atlanticright whale contact call with the presence of noise and other marine lifesounds. A novel algorithm was developed to preprocess the sound waves before atree based hierarchical classifier is used to classify the data and provide ascore. The developed model was trained with 30,000 data points made availablethrough the Cornell University Whale Detection Challenge program. Resultsshowed that the developed algorithm had close to 85% success rate in detectingthe presence of the North Atlantic right whale.
arxiv-3600-205 | Local Graph Clustering Beyond Cheeger's Inequality | http://arxiv.org/abs/1304.8132 | author:Zeyuan Allen Zhu, Silvio Lattanzi, Vahab Mirrokni category:cs.DS cs.LG stat.ML published:2013-04-30 summary:Motivated by applications of large-scale graph clustering, we studyrandom-walk-based LOCAL algorithms whose running times depend only on the sizeof the output cluster, rather than the entire graph. All previously known suchalgorithms guarantee an output conductance of $\tilde{O}(\sqrt{\phi(A)})$ whenthe target set $A$ has conductance $\phi(A)\in[0,1]$. In this paper, we improveit to $$\tilde{O}\bigg( \min\Big\{\sqrt{\phi(A)},\frac{\phi(A)}{\sqrt{\mathsf{Conn}(A)}} \Big\} \bigg)\enspace, $$ where theinternal connectivity parameter $\mathsf{Conn}(A) \in [0,1]$ is defined as thereciprocal of the mixing time of the random walk over the induced subgraph on$A$. For instance, using $\mathsf{Conn}(A) = \Omega(\lambda(A) / \log n)$ where$\lambda$ is the second eigenvalue of the Laplacian of the induced subgraph on$A$, our conductance guarantee can be as good as$\tilde{O}(\phi(A)/\sqrt{\lambda(A)})$. This builds an interesting connectionto the recent advance of the so-called improved Cheeger's Inequality [KKL+13],which says that global spectral algorithms can provide a conductance guaranteeof $O(\phi_{\mathsf{opt}}/\sqrt{\lambda_3})$ instead of$O(\sqrt{\phi_{\mathsf{opt}}})$. In addition, we provide theoretical guarantee on the clustering accuracy (interms of precision and recall) of the output set. We also prove that ouranalysis is tight, and perform empirical evaluation to support our theory onboth synthetic and real data. It is worth noting that, our analysis outperforms prior work when the clusteris well-connected. In fact, the better it is well-connected inside, the moresignificant improvement (both in terms of conductance and accuracy) we canobtain. Our results shed light on why in practice some random-walk-basedalgorithms perform better than its previous theory, and help guide futureresearch about local clustering.
arxiv-3600-206 | Digenes: genetic algorithms to discover conjectures about directed and undirected graphs | http://arxiv.org/abs/1304.7993 | author:Romain Absil, Hadrien Mélot category:cs.DM cs.NE published:2013-04-30 summary:We present Digenes, a new discovery system that aims to help researchers ingraph theory. While its main task is to find extremal graphs for a given(function of) invariants, it also provides some basic support in proofconception. This has already been proved to be very useful to find newconjectures since the AutoGraphiX system of Caporossi and Hansen (DiscreteMath. 212-2000). However, unlike existing systems, Digenes can be used bothwith directed or undirected graphs. In this paper, we present the principlesand functionality of Digenes, describe the genetic algorithms that have beendesigned to achieve them, and give some computational results and openquestions. This do arise some interesting questions regarding geneticalgorithms design particular to this field, such as crossover definition.
arxiv-3600-207 | Convolutional Neural Networks learn compact local image descriptors | http://arxiv.org/abs/1304.7948 | author:Christian Osendorfer, Justin Bayer, Patrick van der Smagt category:cs.CV published:2013-04-30 summary:A standard deep convolutional neural network paired with a suitable lossfunction learns compact local image descriptors that perform comparably tostate-of-the art approaches.
arxiv-3600-208 | Fractal-Based Detection of Microcalcification Clusters in Digital Mammograms | http://arxiv.org/abs/1304.8092 | author:P. Shanmugavadivu, V. Sivakumar category:cs.CV published:2013-04-30 summary:In this paper, a novel method for edge detection of microcalcificationclusters in mammogram images is presented using the concept of FractalDimension and Hurst co-efficient that enables to locate the microcalcificationsin the mammograms. This technique detects the edges accurately than the onesobtained by the conventional Sobel method. Generally, Sobel method detects theedges of the regions/objects in an image using the Fudge factor that assumesits value as 0.5, by default. In this proposed technique, the Fudge factor issuitably replaced with Hurst Co-efficient, which is computed as the differenceof Fractal dimension and the topological dimension of a given input image.These two dimensions are image-dependent, and hence the respective Hurstco-efficient too varies with respect to images. Hence, the image-dependentHurst co-efficient based Sobel method is proved to produce better results thanthe Fudge factor based Sobel method. The results of the proposed methodsubstantiate the merit of the proposed technique.
arxiv-3600-209 | Dictionary LASSO: Guaranteed Sparse Recovery under Linear Transformation | http://arxiv.org/abs/1305.0047 | author:Ji Liu, Lei Yuan, Jieping Ye category:stat.ML published:2013-04-30 summary:We consider the following signal recovery problem: given a measurement matrix$\Phi\in \mathbb{R}^{n\times p}$ and a noisy observation vector $c\in\mathbb{R}^{n}$ constructed from $c = \Phi\theta^* + \epsilon$ where$\epsilon\in \mathbb{R}^{n}$ is the noise vector whose entries follow i.i.d.centered sub-Gaussian distribution, how to recover the signal $\theta^*$ if$D\theta^*$ is sparse {\rca under a linear transformation}$D\in\mathbb{R}^{m\times p}$? One natural method using convex optimization isto solve the following problem: $$\min_{\theta} {1\over 2}\\Phi\theta - c\^2+ \lambda\D\theta\_1.$$ This paper provides an upper bound of the estimateerror and shows the consistency property of this method by assuming that thedesign matrix $\Phi$ is a Gaussian random matrix. Specifically, we show 1) inthe noiseless case, if the condition number of $D$ is bounded and themeasurement number $n\geq \Omega(s\log(p))$ where $s$ is the sparsity number,then the true solution can be recovered with high probability; and 2) in thenoisy case, if the condition number of $D$ is bounded and the measurementincreases faster than $s\log(p)$, that is, $s\log(p)=o(n)$, the estimate errorconverges to zero with probability 1 when $p$ and $s$ go to infinity. Ourresults are consistent with those for the special case $D=\bold{I}_{p\times p}$(equivalently LASSO) and improve the existing analysis. The condition number of$D$ plays a critical role in our analysis. We consider the condition numbers intwo cases including the fused LASSO and the random graph: the condition numberin the fused LASSO case is bounded by a constant, while the condition number inthe random graph case is bounded with high probability if $m\over p$ (i.e.,$#text{edge}\over #text{vertex}$) is larger than a certain constant. Numericalsimulations are consistent with our theoretical results.
arxiv-3600-210 | A least-squares method for sparse low rank approximation of multivariate functions | http://arxiv.org/abs/1305.0030 | author:Mathilde Chevreuil, Régis Lebrun, Anthony Nouy, Prashant Rai category:math.NA stat.ML published:2013-04-30 summary:In this paper, we propose a low-rank approximation method based on discreteleast-squares for the approximation of a multivariate function from random,noisy-free observations. Sparsity inducing regularization techniques are usedwithin classical algorithms for low-rank approximation in order to exploit thepossible sparsity of low-rank approximations. Sparse low-rank approximationsare constructed with a robust updated greedy algorithm which includes anoptimal selection of regularization parameters and approximation ranks usingcross validation techniques. Numerical examples demonstrate the capability ofapproximating functions of many variables even when very few functionevaluations are available, thus proving the interest of the proposed algorithmfor the propagation of uncertainties through complex computational models.
arxiv-3600-211 | ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge | http://arxiv.org/abs/1304.7942 | author:Michele Filannino, Gavin Brown, Goran Nenadic category:cs.CL published:2013-04-30 summary:This paper describes a temporal expression identification and normalizationsystem, ManTIME, developed for the TempEval-3 challenge. The identificationphase combines the use of conditional random fields along with apost-processing identification pipeline, whereas the normalization phase iscarried out using NorMA, an open-source rule-based temporal normalizer. Weinvestigate the performance variation with respect to different feature types.Specifically, we show that the use of WordNet-based features in theidentification task negatively affects the overall performance, and that thereis no statistically significant difference in using gazetteers, shallow parsingand propositional noun phrases labels on top of the morphological features. Onthe test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in theidentification phase. Normalization accuracies are 0.84 (type attribute) and0.77 (value attribute). Surprisingly, the use of the silver data (alone or inaddition to the gold annotated ones) does not improve the performance.
arxiv-3600-212 | Generalized Canonical Correlation Analysis for Classification | http://arxiv.org/abs/1304.7981 | author:Cencheng Shen, Ming Sun, Minh Tang, Carey E. Priebe category:stat.ML published:2013-04-30 summary:For multiple multivariate data sets, we derive conditions under whichGeneralized Canonical Correlation Analysis (GCCA) improves classificationperformance of the projected datasets, compared to standard CanonicalCorrelation Analysis (CCA) using only two data sets. We illustrate ourtheoretical results with simulations and a real data experiment.
arxiv-3600-213 | Revealing social networks of spammers through spectral clustering | http://arxiv.org/abs/1305.0051 | author:Kevin S. Xu, Mark Kliger, Yilun Chen, Peter J. Woolf, Alfred O. Hero III category:cs.SI cs.LG physics.soc-ph stat.ML published:2013-04-30 summary:To date, most studies on spam have focused only on the spamming phase of thespam cycle and have ignored the harvesting phase, which consists of the massacquisition of email addresses. It has been observed that spammers concealtheir identity to a lesser degree in the harvesting phase, so it may bepossible to gain new insights into spammers' behavior by studying the behaviorof harvesters, which are individuals or bots that collect email addresses. Inthis paper, we reveal social networks of spammers by identifying communities ofharvesters with high behavioral similarity using spectral clustering. The dataanalyzed was collected through Project Honey Pot, a distributed system formonitoring harvesting and spamming. Our main findings are (1) that mostspammers either send only phishing emails or no phishing emails at all, (2)that most communities of spammers also send only phishing emails or no phishingemails at all, and (3) that several groups of spammers within communitiesexhibit coherent temporal behavior and have similar IP addresses. Our findingsreveal some previously unknown behavior of spammers and suggest that there isindeed social structure between spammers to be discovered.
arxiv-3600-214 | Image Compression By Embedding Five Modulus Method Into JPEG | http://arxiv.org/abs/1305.0020 | author:Firas A. Jassim category:cs.CV cs.MM published:2013-04-30 summary:The standard JPEG format is almost the optimum format in image compression.The compression ratio in JPEG sometimes reaches 30:1. The compression ratio ofJPEG could be increased by embedding the Five Modulus Method (FMM) into theJPEG algorithm. The novel algorithm gives twice the time as the standard JPEGalgorithm or more. The novel algorithm was called FJPEG (Five-JPEG). Thequality of the reconstructed image after compression is approximatelyapproaches the JPEG. Standard test images have been used to support andimplement the suggested idea in this paper and the error metrics have beencomputed and compared with JPEG.
arxiv-3600-215 | Inferring ground truth from multi-annotator ordinal data: a probabilistic approach | http://arxiv.org/abs/1305.0015 | author:Balaji Lakshminarayanan, Yee Whye Teh category:stat.ML cs.LG published:2013-04-30 summary:A popular approach for large scale data annotation tasks is crowdsourcing,wherein each data point is labeled by multiple noisy annotators. We considerthe problem of inferring ground truth from noisy ordinal labels obtained frommultiple annotators of varying and unknown expertise levels. Annotation modelsfor ordinal data have been proposed mostly as extensions of theirbinary/categorical counterparts and have received little attention in thecrowdsourcing literature. We propose a new model for crowdsourced ordinal datathat accounts for instance difficulty as well as annotator expertise, andderive a variational Bayesian inference algorithm for parameter estimation. Weanalyze the ordinal extensions of several state-of-the-art annotator models forbinary/categorical labels and evaluate the performance of all the models on tworeal world datasets containing ordinal query-URL relevance scores, collectedthrough Amazon's Mechanical Turk. Our results indicate that the proposed modelperforms better or as well as existing state-of-the-art methods and is moreresistant to `spammy' annotators (i.e., annotators who assign labels randomlywithout actually looking at the instance) than popular baselines such as mean,median, and majority vote which do not account for annotator expertise.
arxiv-3600-216 | Semi-Supervised Information-Maximization Clustering | http://arxiv.org/abs/1304.8020 | author:Daniele Calandriello, Gang Niu, Masashi Sugiyama category:cs.LG stat.ML published:2013-04-30 summary:Semi-supervised clustering aims to introduce prior knowledge in the decisionprocess of a clustering algorithm. In this paper, we propose a novelsemi-supervised clustering algorithm based on the information-maximizationprinciple. The proposed method is an extension of a previous unsupervisedinformation-maximization clustering algorithm based on squared-loss mutualinformation to effectively incorporate must-links and cannot-links. Theproposed method is computationally efficient because the clustering solutioncan be obtained analytically via eigendecomposition. Furthermore, the proposedmethod allows systematic optimization of tuning parameters such as the kernelwidth, given the degree of belief in the must-links and cannot-links. Theusefulness of the proposed method is demonstrated through experiments.
arxiv-3600-217 | The Randomized Dependence Coefficient | http://arxiv.org/abs/1304.7717 | author:David Lopez-Paz, Philipp Hennig, Bernhard Schölkopf category:stat.ML published:2013-04-29 summary:We introduce the Randomized Dependence Coefficient (RDC), a measure ofnon-linear dependence between random variables of arbitrary dimension based onthe Hirschfeld-Gebelein-R\'enyi Maximum Correlation Coefficient. RDC is definedin terms of correlation of random non-linear copula projections; it isinvariant with respect to marginal distribution transformations, has lowcomputational cost and is easy to implement: just five lines of R code,included at the end of the paper.
arxiv-3600-218 | Fractal structures in Adversarial Prediction | http://arxiv.org/abs/1304.7576 | author:Rina Panigrahy, Preyas Popat category:cs.LG published:2013-04-29 summary:Fractals are self-similar recursive structures that have been used inmodeling several real world processes. In this work we study how "fractal-like"processes arise in a prediction game where an adversary is generating asequence of bits and an algorithm is trying to predict them. We will see thatunder a certain formalization of the predictive payoff for the algorithm it ismost optimal for the adversary to produce a fractal-like sequence to minimizethe algorithm's ability to predict. Indeed it has been suggested before thatfinancial markets exhibit a fractal-like behavior. We prove that a fractal-likedistribution arises naturally out of an optimization from the adversary'sperspective. In addition, we give optimal trade-offs between predictability and expecteddeviation (i.e. sum of bits) for our formalization of predictive payoff. Thisresult is motivated by the observation that several time series data exhibithigher deviations than expected for a completely random walk.
arxiv-3600-219 | A Discrete State Transition Algorithm for Generalized Traveling Salesman Problem | http://arxiv.org/abs/1304.7607 | author:Xiaolin Tang, Chunhua Yang, Xiaojun Zhou, Weihua Gui category:math.OC cs.AI cs.NE published:2013-04-29 summary:Generalized traveling salesman problem (GTSP) is an extension of classicaltraveling salesman problem (TSP), which is a combinatorial optimization problemand an NP-hard problem. In this paper, an efficient discrete state transitionalgorithm (DSTA) for GTSP is proposed, where a new local search operator named\textit{K-circle}, directed by neighborhood information in space, has beenintroduced to DSTA to shrink search space and strengthen search ability. Anovel robust update mechanism, restore in probability and risk in probability(Double R-Probability), is used in our work to escape from local minima. Theproposed algorithm is tested on a set of GTSP instances. Compared with otherheuristics, experimental results have demonstrated the effectiveness and strongadaptability of DSTA and also show that DSTA has better search ability than itscompetitors.
arxiv-3600-220 | Optimal amortized regret in every interval | http://arxiv.org/abs/1304.7577 | author:Rina Panigrahy, Preyas Popat category:cs.LG cs.DS stat.ML published:2013-04-29 summary:Consider the classical problem of predicting the next bit in a sequence ofbits. A standard performance measure is {\em regret} (loss in payoff) withrespect to a set of experts. For example if we measure performance with respectto two constant experts one that always predicts 0's and another that alwayspredicts 1's it is well known that one can get regret $O(\sqrt T)$ with respectto the best expert by using, say, the weighted majority algorithm. But thisalgorithm does not provide performance guarantee in any interval. There areother algorithms that ensure regret $O(\sqrt {x \log T})$ in any interval oflength $x$. In this paper we show a randomized algorithm that in an amortizedsense gets a regret of $O(\sqrt x)$ for any interval when the sequence ispartitioned into intervals arbitrarily. We empirically estimated the constantin the $O()$ for $T$ upto 2000 and found it to be small -- around 2.1. We alsoexperimentally evaluate the efficacy of this algorithm in predicting highfrequency stock data.
arxiv-3600-221 | Markovian models for one dimensional structure estimation on heavily noisy imagery | http://arxiv.org/abs/1304.7713 | author:Ana Georgina Flesia, Javier Gimenez, Elena Rufeil Fiori category:cs.CV stat.AP published:2013-04-29 summary:Radar (SAR) images often exhibit profound appearance variations due to avariety of factors including clutter noise produced by the coherent nature ofthe illumination. Ultrasound images and infrared images have similar clutteredappearance, that make 1 dimensional structures, as edges and object boundariesdifficult to locate. Structure information is usually extracted in two steps:first, building and edge strength mask classifying pixels as edge points byhypothesis testing, and secondly estimating from that mask, pixel wideconnected edges. With constant false alarm rate (CFAR) edge strength detectorsfor speckle clutter, the image needs to be scanned by a sliding window composedof several differently oriented splitting sub-windows. The accuracy of edgelocation for these ratio detectors depends strongly on the orientation of thesub-windows. In this work we propose to transform the edge strength detectionproblem into a binary segmentation problem in the undecimated wavelet domain,solvable using parallel 1d Hidden Markov Models. For general dependency models,exact estimation of the state map becomes computationally complex, but in ourmodel, exact MAP is feasible. The effectiveness of our approach is demonstratedon simulated noisy real-life natural images with available ground truth, whilethe strength of our output edge map is measured with Pratt's, Baddeley an Kappaproficiency measures. Finally, analysis and experiments on three differenttypes of SAR images, with different polarizations, resolutions and textures,illustrate that the proposed method can detect structure on SAR imageseffectively, providing a very good start point for active contour methods.
arxiv-3600-222 | Learning Geo-Temporal Non-Stationary Failure and Recovery of Power Distribution | http://arxiv.org/abs/1304.7710 | author:Yun Wei, Chuanyi Ji, Floyd Galvan, Stephen Couvillon, George Orellana, James Momoh category:cs.SY cs.LG physics.soc-ph published:2013-04-29 summary:Smart energy grid is an emerging area for new applications of machinelearning in a non-stationary environment. Such a non-stationary environmentemerges when large-scale failures occur at power distribution networks due toexternal disturbances such as hurricanes and severe storms. Power distributionnetworks lie at the edge of the grid, and are especially vulnerable to externaldisruptions. Quantifiable approaches are lacking and needed to learnnon-stationary behaviors of large-scale failure and recovery of powerdistribution. This work studies such non-stationary behaviors in three aspects.First, a novel formulation is derived for an entire life cycle of large-scalefailure and recovery of power distribution. Second, spatial-temporal models offailure and recovery of power distribution are developed as geo-location basedmultivariate non-stationary GI(t)/G(t)/Infinity queues. Third, thenon-stationary spatial-temporal models identify a small number of parameters tobe learned. Learning is applied to two real-life examples of large-scaledisruptions. One is from Hurricane Ike, where data from an operational networkis exact on failures and recoveries. The other is from Hurricane Sandy, whereaggregated data is used for inferring failure and recovery processes at one ofthe impacted areas. Model parameters are learned using real data. Two findingsemerge as results of learning: (a) Failure rates behave similarly at the twodifferent provider networks for two different hurricanes but differently at thegeographical regions. (b) Both rapid- and slow-recovery are present forHurricane Ike but only slow recovery is shown for a regional distributionnetwork from Hurricane Sandy.
arxiv-3600-223 | Machine Translation Systems in India | http://arxiv.org/abs/1304.7728 | author:Sugata Sanyal, Rajdeep Borgohain category:cs.CL cs.CY published:2013-04-29 summary:Machine Translation is the translation of one natural language into anotherusing automated and computerized means. For a multilingual country like India,with the huge amount of information exchanged between various regions and indifferent languages in digitized format, it has become necessary to find anautomated process from one language to another. In this paper, we take a lookat the various Machine Translation System in India which is specifically builtfor the purpose of translation between the Indian languages. We discuss thevarious approaches taken for building the machine translation system and thendiscuss some of the Machine Translation Systems in India along with theirfeatures.
arxiv-3600-224 | Deterministic Initialization of the K-Means Algorithm Using Hierarchical Clustering | http://arxiv.org/abs/1304.7465 | author:M. Emre Celebi, Hassan A. Kingravi category:cs.LG cs.CV I.5.3; H.2.8 published:2013-04-28 summary:K-means is undoubtedly the most widely used partitional clustering algorithm.Unfortunately, due to its gradient descent nature, this algorithm is highlysensitive to the initial placement of the cluster centers. Numerousinitialization methods have been proposed to address this problem. Many ofthese methods, however, have superlinear complexity in the number of datapoints, making them impractical for large data sets. On the other hand, linearmethods are often random and/or order-sensitive, which renders their resultsunrepeatable. Recently, Su and Dy proposed two highly successful hierarchicalinitialization methods named Var-Part and PCA-Part that are not only linear,but also deterministic (non-random) and order-invariant. In this paper, wepropose a discriminant analysis based approach that addresses a commondeficiency of these two methods. Experiments on a large and diverse collectionof data sets from the UCI Machine Learning Repository demonstrate that Var-Partand PCA-Part are highly competitive with one of the best random initializationmethods to date, i.e., k-means++, and that the proposed approach significantlyimproves the performance of both hierarchical methods.
arxiv-3600-225 | Semi-supervised Eigenvectors for Large-scale Locally-biased Learning | http://arxiv.org/abs/1304.7528 | author:Toke J. Hansen, Michael W. Mahoney category:cs.LG math.SP stat.ML published:2013-04-28 summary:In many applications, one has side information, e.g., labels that areprovided in a semi-supervised manner, about a specific target region of a largedata set, and one wants to perform machine learning and data analysis tasks"nearby" that prespecified target region. For example, one might be interestedin the clustering structure of a data graph near a prespecified "seed set" ofnodes, or one might be interested in finding partitions in an image that arenear a prespecified "ground truth" set of pixels. Locally-biased problems ofthis sort are particularly challenging for popular eigenvector-based machinelearning and data analysis tools. At root, the reason is that eigenvectors areinherently global quantities, thus limiting the applicability ofeigenvector-based methods in situations where one is interested in very localproperties of the data. In this paper, we address this issue by providing a methodology to constructsemi-supervised eigenvectors of a graph Laplacian, and we illustrate how theselocally-biased eigenvectors can be used to perform locally-biased machinelearning. These semi-supervised eigenvectors capturesuccessively-orthogonalized directions of maximum variance, conditioned onbeing well-correlated with an input seed set of nodes that is assumed to beprovided in a semi-supervised manner. We show that these semi-supervisedeigenvectors can be computed quickly as the solution to a system of linearequations; and we also describe several variants of our basic method that haveimproved scaling properties. We provide several empirical examplesdemonstrating how these semi-supervised eigenvectors can be used to performlocally-biased learning; and we discuss the relationship between our resultsand recent machine learning algorithms that use global eigenvectors of thegraph Laplacian.
arxiv-3600-226 | On Integrating Fuzzy Knowledge Using a Novel Evolutionary Algorithm | http://arxiv.org/abs/1304.7423 | author:Nafisa Afrin Chowdhury, Murshida Khatun, M. M. A. Hashem category:cs.NE cs.AI published:2013-04-28 summary:Fuzzy systems may be considered as knowledge-based systems that incorporateshuman knowledge into their knowledge base through fuzzy rules and fuzzymembership functions. The intent of this study is to present a fuzzy knowledgeintegration framework using a Novel Evolutionary Strategy (NES), which cansimultaneously integrate multiple fuzzy rule sets and their membership functionsets. The proposed approach consists of two phases: fuzzy knowledge encodingand fuzzy knowledge integration. Four application domains, the hepatitisdiagnosis, the sugarcane breeding prediction, Iris plants classification, andTic-tac-toe endgame were used to show the performance ofthe proposed knowledgeapproach. Results show that the fuzzy knowledge base derived using our approachperforms better than Genetic Algorithm based approach.
arxiv-3600-227 | Measuring Cultural Relativity of Emotional Valence and Arousal using Semantic Clustering and Twitter | http://arxiv.org/abs/1304.7507 | author:Eugene Yuta Bann, Joanna J. Bryson category:cs.CL cs.AI published:2013-04-28 summary:Researchers since at least Darwin have debated whether and to what extentemotions are universal or culture-dependent. However, previous studies haveprimarily focused on facial expressions and on a limited set of emotions. Giventhat emotions have a substantial impact on human lives, evidence for culturalemotional relativity might be derived by applying distributional semanticstechniques to a text corpus of self-reported behaviour. Here, we explore thisidea by measuring the valence and arousal of the twelve most popular emotionkeywords expressed on the micro-blogging site Twitter. We do this in threegeographical regions: Europe, Asia and North America. We demonstrate that inour sample, the valence and arousal levels of the same emotion keywords differsignificantly with respect to these geographical regions --- Europeans are, orat least present themselves as more positive and aroused, North Americans aremore negative and Asians appear to be more positive but less aroused whencompared to global valence and arousal levels of the same emotion keywords. Ourwork is the first in kind to programatically map large text corpora to adimensional model of affect.
arxiv-3600-228 | Constant conditional entropy and related hypotheses | http://arxiv.org/abs/1304.7359 | author:Ramon Ferrer-i-Cancho, Łukasz Dębowski, Fermín Moscoso del Prado Martín category:cs.CL cs.IT math.IT published:2013-04-27 summary:Constant entropy rate (conditional entropies must remain constant as thesequence length increases) and uniform information density (conditionalprobabilities must remain constant as the sequence length increases) are twoinformation theoretic principles that are argued to underlie a wide range oflinguistic phenomena. Here we revise the predictions of these principles to thelight of Hilberg's law on the scaling of conditional entropy in language andrelated laws. We show that constant entropy rate (CER) and two interpretationsfor uniform information density (UID), full UID and strong UID, areinconsistent with these laws. Strong UID implies CER but the reverse is nottrue. Full UID, a particular case of UID, leads to costly uncorrelatedsequences that are totally unrealistic. We conclude that CER and its particularcases are incomplete hypotheses about the scaling of conditional entropies.
arxiv-3600-229 | Bingham Procrustean Alignment for Object Detection in Clutter | http://arxiv.org/abs/1304.7399 | author:Jared Glover, Sanja Popovic category:cs.CV cs.RO stat.AP published:2013-04-27 summary:A new system for object detection in cluttered RGB-D images is presented. Ourmain contribution is a new method called Bingham Procrustean Alignment (BPA) toalign models with the scene. BPA uses point correspondences between orientedfeatures to derive a probability distribution over possible model poses. Theorientation component of this distribution, conditioned on the position, isshown to be a Bingham distribution. This result also applies to the classicproblem of least-squares alignment of point sets, when point features areorientation-less, and gives a principled, probabilistic way to measure poseuncertainty in the rigid alignment problem. Our detection system leverages BPAto achieve more reliable object detections in clutter.
arxiv-3600-230 | Reading Ancient Coin Legends: Object Recognition vs. OCR | http://arxiv.org/abs/1304.7184 | author:Albert Kavelar, Sebastian Zambanini, Martin Kampel category:cs.CV published:2013-04-26 summary:Standard OCR is a well-researched topic of computer vision and can beconsidered solved for machine-printed text. However, when applied tounconstrained images, the recognition rates drop drastically. Therefore, theemployment of object recognition-based techniques has become state of the artin scene text recognition applications. This paper presents a scene textrecognition method tailored to ancient coin legends and compares the resultsachieved in character and word recognition experiments to a standard OCRengine. The conducted experiments show that the proposed method outperforms thestandard OCR engine on a set of 180 cropped coin legend words.
arxiv-3600-231 | TimeML-strict: clarifying temporal annotation | http://arxiv.org/abs/1304.7289 | author:Leon Derczynski, Hector Llorens, Naushad UzZaman category:cs.CL I.2.7 published:2013-04-26 summary:TimeML is an XML-based schema for annotating temporal information overdiscourse. The standard has been used to annotate a variety of resources and isfollowed by a number of tools, the creation of which constitute hundreds ofthousands of man-hours of research work. However, the current state ofresources is such that many are not valid, or do not produce valid output, orcontain ambiguous or custom additions and removals. Difficulties arising fromthese variances were highlighted in the TempEval-3 exercise, which included itsown extra stipulations over conventional TimeML as a response. To unify the state of current resources, and to make progress toward easyadoption of its current incarnation ISO-TimeML, this paper introducesTimeML-strict: a valid, unambiguous, and easy-to-process subset of TimeML. Wealso introduce three resources -- a schema for TimeML-strict; a validator toolfor TimeML-strict, so that one may ensure documents are in the correct form;and a repair tool that corrects common invalidating errors and addsdisambiguating markup in order to convert documents from the laxer TimeMLstandard to TimeML-strict.
arxiv-3600-232 | Irreflexive and Hierarchical Relations as Translations | http://arxiv.org/abs/1304.7158 | author:Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko category:cs.LG published:2013-04-26 summary:We consider the problem of embedding entities and relations of knowledgebases in low-dimensional vector spaces. Unlike most existing approaches, whichare primarily efficient for modeling equivalence relations, our approach isdesigned to explicitly model irreflexive relations, such as hierarchies, byinterpreting them as translations operating on the low-dimensional embeddingsof the entities. Preliminary experiments show that, despite its simplicity anda smaller number of parameters than previous approaches, our approach achievesstate-of-the-art performance according to standard evaluation protocols on datafrom WordNet and Freebase.
arxiv-3600-233 | A Convex Approach for Image Hallucination | http://arxiv.org/abs/1304.7153 | author:Peter Innerhofer, Thomas Pock category:cs.CV published:2013-04-26 summary:In this paper we propose a global convex approach for image hallucination.Altering the idea of classical multi image super resolution (SU) systems tosingle image SU, we incorporate aligned images to hallucinate the output. Ourwork is based on the paper of Tappen et al. where they use a non-convex modelfor image hallucination. In comparison we formulate a convex primaloptimization problem and derive a fast converging primal-dual algorithm with aglobal optimal solution. We use a database with face images to incorporatehigh-frequency details to the high-resolution output. We show that we canachieve state-of-the-art results by using a convex approach.
arxiv-3600-234 | Synthesis of neural networks for spatio-temporal spike pattern recognition and processing | http://arxiv.org/abs/1304.7118 | author:J. Tapson, G. Cohen, S. Afshar, K. Stiefel, Y. Buskila, R. Wang, T. J. Hamilton, A. van Schaik category:cs.NE q-bio.NC published:2013-04-26 summary:The advent of large scale neural computational platforms has highlighted thelack of algorithms for synthesis of neural structures to perform predefinedcognitive tasks. The Neural Engineering Framework offers one such synthesis,but it is most effective for a spike rate representation of neural information,and it requires a large number of neurons to implement simple functions. Wedescribe a neural network synthesis method that generates synaptic connectivityfor neurons which process time-encoded neural signals, and which makes verysparse use of neurons. The method allows the user to specify, arbitrarily,neuronal characteristics such as axonal and dendritic delays, and synaptictransfer functions, and then solves for the optimal input-output relationshipusing computed dendritic weights. The method may be used for batch or onlinelearning and has an extremely fast optimization process. We demonstrate its usein generating a network to recognize speech which is sparsely encoded as spiketimes.
arxiv-3600-235 | In the sight of my wearable camera: Classifying my visual experience | http://arxiv.org/abs/1304.7236 | author:Alessandro Perina, Nebojsa Jojic category:cs.CV published:2013-04-26 summary:We introduce and we analyze a new dataset which resembles the input tobiological vision systems much more than most previously published ones. Ouranalysis leaded to several important conclusions. First, it is possible todisambiguate over dozens of visual scenes (locations) encountered over thecourse of several weeks of a human life with accuracy of over 80%, and thisopens up possibility for numerous novel vision applications, from earlydetection of dementia to everyday use of wearable camera streams for automaticreminders, and visual stream exchange. Second, our experimental resultsindicate that, generative models such as Latent Dirichlet Allocation orCounting Grids, are more suitable to such types of data, as they are morerobust to overtraining and comfortable with images at low resolution, blurredand characterized by relatively random clutter and a mix of objects.
arxiv-3600-236 | Algorithmic Optimisations for Iterative Deconvolution Methods | http://arxiv.org/abs/1304.7211 | author:Martin Welk, Martin Erler category:cs.CV I.4.4; F.2.1 published:2013-04-26 summary:We investigate possibilities to speed up iterative algorithms for non-blindimage deconvolution. We focus on algorithms in which convolution with thepoint-spread function to be deconvolved is used in each iteration, and aim ataccelerating these convolution operations as they are typically the mostexpensive part of the computation. We follow two approaches: First, for somepractically important specific point-spread functions, algorithmicallyefficient sliding window or list processing techniques can be used. In someconstellations this allows faster computation than via the Fourier domain.Second, as iterations progress, computation of convolutions can be restrictedto subsets of pixels. For moderate thinning rates this can be done with almostno impact on the reconstruction quality. Both approaches are demonstrated inthe context of Richardson-Lucy deconvolution but are not restricted to thismethod.
arxiv-3600-237 | Learning Densities Conditional on Many Interacting Features | http://arxiv.org/abs/1304.7230 | author:David C. Kessler, Jack Taylor, David B. Dunson category:stat.ML cs.LG published:2013-04-26 summary:Learning a distribution conditional on a set of discrete-valued features is acommonly encountered task. This becomes more challenging with ahigh-dimensional feature set when there is the possibility of interactionbetween the features. In addition, many frequently applied techniques consideronly prediction of the mean, but the complete conditional density is needed toanswer more complex questions. We demonstrate a novel nonparametric Bayesmethod based upon a tensor factorization of feature-dependent weights forGaussian kernels. The method makes use of multistage feature selection fordimension reduction. The resulting conditional density morphs flexibly with theselected features.
arxiv-3600-238 | Question Answering Against Very-Large Text Collections | http://arxiv.org/abs/1304.7157 | author:Leon Derczynski, Richard Shaw, Ben Solway, Jun Wang category:cs.CL cs.IR published:2013-04-26 summary:Question answering involves developing methods to extract useful informationfrom large collections of documents. This is done with specialised searchengines such as Answer Finder. The aim of Answer Finder is to provide an answerto a question rather than a page listing related documents that may contain thecorrect answer. So, a question such as "How tall is the Eiffel Tower" wouldsimply return "325m" or "1,063ft". Our task was to build on the current versionof Answer Finder by improving information retrieval, and also improving thepre-processing involved in question series analysis.
arxiv-3600-239 | An Algorithm for Training Polynomial Networks | http://arxiv.org/abs/1304.7045 | author:Roi Livni, Shai Shalev-Shwartz, Ohad Shamir category:cs.LG cs.AI stat.ML published:2013-04-26 summary:We consider deep neural networks, in which the output of each node is aquadratic function of its inputs. Similar to other deep architectures, thesenetworks can compactly represent any function on a finite training set. Themain goal of this paper is the derivation of an efficient layer-by-layeralgorithm for training such networks, which we denote as the \emph{BasisLearner}. The algorithm is a universal learner in the sense that the trainingerror is guaranteed to decrease at every iteration, and can eventually reachzero under mild conditions. We present practical implementations of thisalgorithm, as well as preliminary experimental results. We also compare ourdeep architecture to other shallow architectures for learning polynomials, inparticular kernel learning.
arxiv-3600-240 | Pulmonary Vascular Tree Segmentation from Contrast-Enhanced CT Images | http://arxiv.org/abs/1304.7140 | author:M. Helmberger, M. Urschler, M. Pienn, Z. Balint, A. Olschewski, H. Bischof category:cs.CV physics.med-ph published:2013-04-26 summary:We present a pulmonary vessel segmentation algorithm, which is fast, fullyautomatic and robust. It uses a coarse segmentation of the airway tree and aleft and right lung labeled volume to restrict a vessel enhancement filter,based on an offset medialness function, to the lungs. We show the applicationof our algorithm on contrast-enhanced CT images, where we derive a clinicalparameter to detect pulmonary hypertension (PH) in patients. Results on adataset of 24 patients show that quantitative indices derived from thesegmentation are applicable to distinguish patients with and without PH.Further work-in-progress results are shown on the VESSEL12 challenge dataset,which is composed of non-contrast-enhanced scans, where we range in themidfield of participating contestants.
arxiv-3600-241 | Filament and Flare Detection in Hα image sequences | http://arxiv.org/abs/1304.7132 | author:Gernot Riegler, Thomas Pock, Werner Pötzi, Astrid Veronig category:cs.CV astro-ph.IM published:2013-04-26 summary:Solar storms can have a major impact on the infrastructure of the earth. Someof the causing events are observable from ground in the H{\alpha} spectralline. In this paper we propose a new method for the simultaneous detection offlares and filaments in H{\alpha} image sequences. Therefore we perform severalpreprocessing steps to enhance and normalize the images. Based on the intensityvalues we segment the image by a variational approach. In a finalpostprecessing step we derive essential properties to classify the events andfurther demonstrate the performance by comparing our obtained results to thedata annotated by an expert. The information produced by our method can be usedfor near real-time alerts and the statistical analysis of existing data bysolar physicists.
arxiv-3600-242 | Supervised Heterogeneous Multiview Learning for Joint Association Study and Disease Diagnosis | http://arxiv.org/abs/1304.7284 | author:Shandian Zhe, Zenglin Xu, Yuan Qi category:cs.LG cs.CE stat.ML published:2013-04-26 summary:Given genetic variations and various phenotypical traits, such as MagneticResonance Imaging (MRI) features, we consider two important and related tasksin biomedical research: i)to select genetic and phenotypical markers fordisease diagnosis and ii) to identify associations between genetic andphenotypical data. These two tasks are tightly coupled because underlyingassociations between genetic variations and phenotypical features contain thebiological basis for a disease. While a variety of sparse models have beenapplied for disease diagnosis and canonical correlation analysis and itsextensions have bee widely used in association studies (e.g., eQTL analysis),these two tasks have been treated separately. To unify these two tasks, wepresent a new sparse Bayesian approach for joint association study and diseasediagnosis. In this approach, common latent features are extracted fromdifferent data sources based on sparse projection matrices and used to predictmultiple disease severity levels based on Gaussian process ordinal regression;in return, the disease status is used to guide the discovery of relationshipsbetween the data sources. The sparse projection matrices not only revealinteractions between data sources but also select groups of biomarkers relatedto the disease. To learn the model from data, we develop an efficientvariational expectation maximization algorithm. Simulation results demonstratethat our approach achieves higher accuracy in both predicting ordinal labelsand discovering associations between data sources than alternative methods. Weapply our approach to an imaging genetics dataset for the study of Alzheimer'sDisease (AD). Our method identifies biologically meaningful relationshipsbetween genetic variations, MRI features, and AD status, and achievessignificantly higher accuracy for predicting ordinal AD stages than thecompeting methods.
arxiv-3600-243 | An implementation of the relational k-means algorithm | http://arxiv.org/abs/1304.6899 | author:Balázs Szalkai category:cs.LG cs.CV cs.MS published:2013-04-25 summary:A C# implementation of a generalized k-means variant called relationalk-means is described here. Relational k-means is a generalization of thewell-known k-means clustering method which works for non-Euclidean scenarios aswell. The input is an arbitrary distance matrix, as opposed to the traditionalk-means method, where the clustered objects need to be identified with vectors.
arxiv-3600-244 | An Improved Approach for Word Ambiguity Removal | http://arxiv.org/abs/1304.7282 | author:Priti Saktel, Urmila Shrawankar category:cs.CL published:2013-04-25 summary:Word ambiguity removal is a task of removing ambiguity from a word, i.e.correct sense of word is identified from ambiguous sentences. This paperdescribes a model that uses Part of Speech tagger and three categories for wordsense disambiguation (WSD). Human Computer Interaction is very needful toimprove interactions between users and computers. For this, the Supervised andUnsupervised methods are combined. The WSD algorithm is used to find theefficient and accurate sense of a word based on domain information. Theaccuracy of this work is evaluated with the aim of finding best suitable domainof word.
arxiv-3600-245 | Euclidean Upgrade from a Minimal Number of Segments | http://arxiv.org/abs/1304.6990 | author:Tanja Schilling, Tomas Pajdla category:cs.CV published:2013-04-25 summary:In this paper, we propose an algebraic approach to upgrade a projectivereconstruction to a Euclidean one, and aim at computing the rectifyinghomography from a minimal number of 9 segments of known length. Constraints arederived from these segments which yield a set of polynomial equations that wesolve by means of Gr\"obner bases. We explain how a solver for such a system ofequations can be constructed from simplified template data. Moreover, wepresent experiments that demonstrate that the given problem can be solved inthis way.
arxiv-3600-246 | Direct Learning of Sparse Changes in Markov Networks by Density Ratio Estimation | http://arxiv.org/abs/1304.6803 | author:Song Liu, John A. Quinn, Michael U. Gutmann, Taiji Suzuki, Masashi Sugiyama category:stat.ML published:2013-04-25 summary:We propose a new method for detecting changes in Markov network structurebetween two sets of samples. Instead of naively fitting two Markov networkmodels separately to the two data sets and figuring out their difference, we\emph{directly} learn the network structure change by estimating the ratio ofMarkov network models. This density-ratio formulation naturally allows us tointroduce sparsity in the network structure change, which highly contributes toenhancing interpretability. Furthermore, computation of the normalization term,which is a critical bottleneck of the naive approach, can be remarkablymitigated. We also give the dual formulation of the optimization problem, whichfurther reduces the computation cost for large-scale Markov networks. Throughexperiments, we demonstrate the usefulness of our method.
arxiv-3600-247 | Digit Recognition in Handwritten Weather Records | http://arxiv.org/abs/1304.6933 | author:Manuel Keglevic, Robert Sablatnig category:cs.CV published:2013-04-25 summary:This paper addresses the automatic recognition of handwritten temperaturevalues in weather records. The localization of table cells is based on linedetection using projection profiles. Further, a stroke-preserving line removalmethod which is based on gradient images is proposed. The presented digitrecognition utilizes features which are extracted using a set of filters and aSupport Vector Machine classifier. It was evaluated on the MNIST and the USPSdataset and our own database with about 17,000 RGB digit images. An accuracy of99.36% per digit is achieved for the entire system using a set of 84 weatherrecords.
arxiv-3600-248 | Inference and learning in probabilistic logic programs using weighted Boolean formulas | http://arxiv.org/abs/1304.6810 | author:Daan Fierens, Guy Van den Broeck, Joris Renkens, Dimitar Shterionov, Bernd Gutmann, Ingo Thon, Gerda Janssens, Luc De Raedt category:cs.AI cs.LG cs.LO published:2013-04-25 summary:Probabilistic logic programs are logic programs in which some of the factsare annotated with probabilities. This paper investigates how classicalinference and learning tasks known from the graphical model community can betackled for probabilistic logic programs. Several such tasks such as computingthe marginals given evidence and learning from (partial) interpretations havenot really been addressed for probabilistic logic programs before. The first contribution of this paper is a suite of efficient algorithms forvarious inference tasks. It is based on a conversion of the program and thequeries and evidence to a weighted Boolean formula. This allows us to reducethe inference tasks to well-studied tasks such as weighted model counting,which can be solved using state-of-the-art methods known from the graphicalmodel and knowledge compilation literature. The second contribution is analgorithm for parameter estimation in the learning from interpretationssetting. The algorithm employs Expectation Maximization, and is built on top ofthe developed inference algorithms. The proposed approach is experimentally evaluated. The results show that theinference algorithms improve upon the state-of-the-art in probabilistic logicprogramming and that it is indeed possible to learn the parameters of aprobabilistic logic program from interpretations.
arxiv-3600-249 | The K-modes algorithm for clustering | http://arxiv.org/abs/1304.6478 | author:Miguel Á. Carreira-Perpiñán, Weiran Wang category:cs.LG stat.ME stat.ML published:2013-04-24 summary:Many clustering algorithms exist that estimate a cluster centroid, such asK-means, K-medoids or mean-shift, but no algorithm seems to exist that clustersdata by returning exactly K meaningful modes. We propose a natural definitionof a K-modes objective function by combining the notions of density and clusterassignment. The algorithm becomes K-means and K-medoids in the limit of verylarge and very small scales. Computationally, it is slightly slower thanK-means but much faster than mean-shift or K-medoids. Unlike K-means, it isable to find centroids that are valid patterns, truly representative of acluster, even with nonconvex clusters, and appears robust to outliers andmisspecification of the scale and number of clusters.
arxiv-3600-250 | k-Modulus Method for Image Transformation | http://arxiv.org/abs/1304.6759 | author:Firas A. Jassim category:cs.CV published:2013-04-24 summary:In this paper, we propose a new algorithm to make a novel spatial imagetransformation. The proposed approach aims to reduce the bit depth used forimage storage. The basic technique for the proposed transformation is based ofthe modulus operator. The goal is to transform the whole image into multiplesof predefined integer. The division of the whole image by that integer willguarantee that the new image surely less in size from the original image. Thek-Modulus Method could not be used as a stand alone transform for imagecompression because of its high compression ratio. It could be used as a schemeembedded in other image processing fields especially compression. According toits high PSNR value, it could be amalgamated with other methods to facilitatethe redundancy criterion.
arxiv-3600-251 | Comparison of several reweighted l1-algorithms for solving cardinality minimization problems | http://arxiv.org/abs/1304.6655 | author:Mohammad Javad Abdi category:math.OC stat.ML published:2013-04-24 summary:Reweighted l1-algorithms have attracted a lot of attention in the field ofapplied mathematics. A unified framework of such algorithms has been recentlyproposed by Zhao and Li. In this paper we construct a few new examples ofreweighted l1-methods. These functions are certain concave approximations ofthe l0-norm function. We focus on the numerical comparison between some new andexisting reweighted l1-algorithms. We show how the change of parameters inreweighted algorithms may affect the performance of the algorithms for findingthe solution of the cardinality minimization problem. In our experiments, theproblem data were generated according to different statistical distributions,and we test the algorithms on different sparsity level of the solution of theproblem. Our numerical results demonstrate that the reweighted l1-method is oneof the efficient methods for locating the solution of the cardinalityminimization problem.
arxiv-3600-252 | Low-rank optimization for distance matrix completion | http://arxiv.org/abs/1304.6663 | author:B. Mishra, G. Meyer, R. Sepulchre category:math.OC cs.LG stat.ML published:2013-04-24 summary:This paper addresses the problem of low-rank distance matrix completion. Thisproblem amounts to recover the missing entries of a distance matrix when thedimension of the data embedding space is possibly unknown but small compared tothe number of considered data points. The focus is on high-dimensionalproblems. We recast the considered problem into an optimization problem overthe set of low-rank positive semidefinite matrices and propose two efficientalgorithms for low-rank distance matrix completion. In addition, we propose astrategy to determine the dimension of the embedding space. The resultingalgorithms scale to high-dimensional problems and monotonically converge to aglobal solution of the problem. Finally, numerical experiments illustrate thegood performance of the proposed algorithms on benchmarks.
arxiv-3600-253 | A Theoretical Analysis of NDCG Type Ranking Measures | http://arxiv.org/abs/1304.6480 | author:Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Tie-Yan Liu, Wei Chen category:cs.LG cs.IR stat.ML published:2013-04-24 summary:A central problem in ranking is to design a ranking measure for evaluation ofranking functions. In this paper we study, from a theoretical perspective, thewidely used Normalized Discounted Cumulative Gain (NDCG)-type ranking measures.Although there are extensive empirical studies of NDCG, little is known aboutits theoretical properties. We first show that, whatever the ranking functionis, the standard NDCG which adopts a logarithmic discount, converges to 1 asthe number of items to rank goes to infinity. On the first sight, this resultis very surprising. It seems to imply that NDCG cannot differentiate good andbad ranking functions, contradicting to the empirical success of NDCG in manyapplications. In order to have a deeper understanding of ranking measures ingeneral, we propose a notion referred to as consistent distinguishability. Thisnotion captures the intuition that a ranking measure should have such aproperty: For every pair of substantially different ranking functions, theranking measure can decide which one is better in a consistent manner on almostall datasets. We show that NDCG with logarithmic discount has consistentdistinguishability although it converges to the same limit for all rankingfunctions. We next characterize the set of all feasible discount functions forNDCG according to the concept of consistent distinguishability. Specifically weshow that whether NDCG has consistent distinguishability depends on how fastthe discount decays, and 1/r is a critical point. We then turn to the cut-offversion of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k forvarious choices of k and the discount functions. Experimental results on realWeb search datasets agree well with the theory.
arxiv-3600-254 | Locally linear representation for subspace learning and clustering | http://arxiv.org/abs/1304.6487 | author:Liangli Zhen, Zhang Yi, Xi Peng, Dezhong Peng category:cs.LG stat.ML published:2013-04-24 summary:It is a key to construct a similarity graph in graph-oriented subspacelearning and clustering. In a similarity graph, each vertex denotes a datapoint and the edge weight represents the similarity between two points. Thereare two popular schemes to construct a similarity graph, i.e., pairwisedistance based scheme and linear representation based scheme. Most existingworks have only involved one of the above schemes and suffered from somelimitations. Specifically, pairwise distance based methods are sensitive to thenoises and outliers compared with linear representation based methods. On theother hand, there is the possibility that linear representation basedalgorithms wrongly select inter-subspaces points to represent a point, whichwill degrade the performance. In this paper, we propose an algorithm, calledLocally Linear Representation (LLR), which integrates pairwise distance withlinear representation together to address the problems. The proposed algorithmcan automatically encode each data point over a set of points that not onlycould denote the objective point with less residual error, but also are closeto the point in Euclidean space. The experimental results show that ourapproach is promising in subspace learning and subspace clustering.
arxiv-3600-255 | A Bag of Visual Words Approach for Symbols-Based Coarse-Grained Ancient Coin Classification | http://arxiv.org/abs/1304.6192 | author:Hafeez Anwar, Sebastian Zambanini, Martin Kampel category:cs.CV published:2013-04-23 summary:The field of Numismatics provides the names and descriptions of the symbolsminted on the ancient coins. Classification of the ancient coins aims atassigning a given coin to its issuer. Various issuers used various symbols fortheir coins. We propose to use these symbols for a framework that will coarselyclassify the ancient coins. Bag of visual words (BoVWs) is a well establishedvisual recognition technique applied to various problems in computer visionlike object and scene recognition. Improvements have been made by incorporatingthe spatial information to this technique. We apply the BoVWs technique to ourproblem and use three symbols for coarse-grained classification. We userectangular tiling, log-polar tiling and circular tiling to incorporate spatialinformation to BoVWs. Experimental results show that the circular tiling provessuperior to the rest of the methods for our problem.
arxiv-3600-256 | Learning Visual Symbols for Parsing Human Poses in Images | http://arxiv.org/abs/1304.6291 | author:Fang Wang, Yi Li category:cs.CV published:2013-04-23 summary:Parsing human poses in images is fundamental in extracting critical visualinformation for artificial intelligent agents. Our goal is to learnself-contained body part representations from images, which we call visualsymbols, and their symbol-wise geometric contexts in this parsing process. Eachsymbol is individually learned by categorizing visual features leveraged bygeometric information. In the categorization, we use Latent Support VectorMachine followed by an efficient cross validation procedure to learn visualsymbols. Then, these symbols naturally define geometric contexts of body partsin a fine granularity. When the structure of the compositional parts is a tree,we derive an efficient approach to estimating human poses in images.Experiments on two large datasets suggest our approach outperforms state of theart methods.
arxiv-3600-257 | Semi-Optimal Edge Detector based on Simple Standard Deviation with Adjusted Thresholding | http://arxiv.org/abs/1304.6379 | author:Firas A. Jassim category:cs.CV published:2013-04-23 summary:This paper proposes a novel method which combines both median filter andsimple standard deviation to accomplish an excellent edge detector for imageprocessing. First of all, a denoising process must be applied on the grey scaleimage using median filter to identify pixels which are likely to becontaminated by noise. The benefit of this step is to smooth the image and getrid of the noisy pixels. After that, the simple statistical standard deviationcould be computed for each 2X2 window size. If the value of the standarddeviation inside the 2X2 window size is greater than a predefined threshold,then the upper left pixel in the 2?2 window represents an edge. The visualdifferences between the proposed edge detector and the standard known edgedetectors have been shown to support the contribution in this paper.
arxiv-3600-258 | On Semantic Word Cloud Representation | http://arxiv.org/abs/1304.8016 | author:Lukas Barth, Stephen Kobourov, Sergey Pupyrev, Torsten Ueckerdt category:cs.DS cs.CL published:2013-04-23 summary:We study the problem of computing semantic-preserving word clouds in whichsemantically related words are close to each other. While several heuristicapproaches have been described in the literature, we formalize the underlyinggeometric algorithm problem: Word Rectangle Adjacency Contact (WRAC). In thismodel each word is associated with rectangle with fixed dimensions, and thegoal is to represent semantically related words by ensuring that the twocorresponding rectangles touch. We design and analyze efficient polynomial-timealgorithms for some variants of the WRAC problem, show that several generalvariants are NP-hard, and describe a number of approximation algorithms.Finally, we experimentally demonstrate that our theoretically-sound algorithmsoutperform the early heuristics.
arxiv-3600-259 | Counting people from above: Airborne video based crowd analysis | http://arxiv.org/abs/1304.6213 | author:Roland Perko, Thomas Schnabel, Gerald Fritz, Alexander Almer, Lucas Paletta category:cs.CV published:2013-04-23 summary:Crowd monitoring and analysis in mass events are highly importanttechnologies to support the security of attending persons. Proposed methodsbased on terrestrial or airborne image/video data often fail in achievingsufficiently accurate results to guarantee a robust service. We present a novelframework for estimating human count, density and motion from video data basedon custom tailored object detection techniques, a regression based densityestimate and a total variation based optical flow extraction. From the gatheredfeatures we present a detailed accuracy analysis versus ground truthmeasurements. In addition, all information is projected into world coordinatesto enable a direct integration with existing geo-information systems. Theresulting human counts demonstrate a mean error of 4% to 9% and thus representa most efficient measure that can be robustly applied in security criticalservices.
arxiv-3600-260 | The Stochastic Gradient Descent for the Primal L1-SVM Optimization Revisited | http://arxiv.org/abs/1304.6383 | author:Constantinos Panagiotakopoulos, Petroula Tsampouka category:cs.LG cs.AI published:2013-04-23 summary:We reconsider the stochastic (sub)gradient approach to the unconstrainedprimal L1-SVM optimization. We observe that if the learning rate is inverselyproportional to the number of steps, i.e., the number of times any trainingpattern is presented to the algorithm, the update rule may be transformed intothe one of the classical perceptron with margin in which the margin thresholdincreases linearly with the number of steps. Moreover, if we cycle repeatedlythrough the possibly randomly permuted training set the dual variables definednaturally via the expansion of the weight vector as a linear combination of thepatterns on which margin errors were made are shown to obey at the end of eachcomplete cycle automatically the box constraints arising in dual optimization.This renders the dual Lagrangian a running lower bound on the primal objectivetending to it at the optimum and makes available an upper bound on the relativeaccuracy achieved which provides a meaningful stopping criterion. In addition,we propose a mechanism of presenting the same pattern repeatedly to thealgorithm which maintains the above properties. Finally, we give experimentalevidence that algorithms constructed along these lines exhibit a considerablyimproved performance.
arxiv-3600-261 | A Counterexample for the Validity of Using Nuclear Norm as a Convex Surrogate of Rank | http://arxiv.org/abs/1304.6233 | author:Hongyang Zhang, Zhouchen Lin, Chao Zhang category:stat.ML math.OC published:2013-04-23 summary:Rank minimization has attracted a lot of attention due to its robustness indata recovery. To overcome the computational difficulty, rank is often replacedwith nuclear norm. For several rank minimization problems, such a replacementhas been theoretically proven to be valid, i.e., the solution to nuclear normminimization problem is also the solution to rank minimization problem.Although it is easy to believe that such a replacement may not always be valid,no concrete example has ever been found. We argue that such a validity checkingcannot be done by numerical computation and show, by analyzing the noiselesslatent low rank representation (LatLRR) model, that even for very simple rankminimization problems the validity may still break down. As a by-product, wefind that the solution to the nuclear norm minimization formulation of LatLRRis non-unique. Hence the results of LatLRR reported in the literature may bequestionable.
arxiv-3600-262 | Dealing with natural language interfaces in a geolocation context | http://arxiv.org/abs/1304.5880 | author:M. -A. Abchir, Isis Truck, Anna Pappa category:cs.CL published:2013-04-22 summary:In the geolocation field where high-level programs and low-level devicescoexist, it is often difficult to find a friendly user inter- face to configureall the parameters. The challenge addressed in this paper is to proposeintuitive and simple, thus natural lan- guage interfaces to interact withlow-level devices. Such inter- faces contain natural language processing andfuzzy represen- tations of words that facilitate the elicitation ofbusiness-level objectives in our context.
arxiv-3600-263 | Dynamic stochastic blockmodels: Statistical models for time-evolving networks | http://arxiv.org/abs/1304.5974 | author:Kevin S. Xu, Alfred O. Hero III category:cs.SI cs.LG physics.soc-ph stat.ME G.3; G.2.2 published:2013-04-22 summary:Significant efforts have gone into the development of statistical models foranalyzing data in the form of networks, such as social networks. Most existingwork has focused on modeling static networks, which represent either a singletime snapshot or an aggregate view over time. There has been recent interest instatistical modeling of dynamic networks, which are observed at multiple pointsin time and offer a richer representation of many complex phenomena. In thispaper, we propose a state-space model for dynamic networks that extends thewell-known stochastic blockmodel for static networks to the dynamic setting. Wethen propose a procedure to fit the model using a modification of the extendedKalman filter augmented with a local search. We apply the procedure to analyzea dynamic social network of email communication.
arxiv-3600-264 | Bayesian crack detection in ultra high resolution multimodal images of paintings | http://arxiv.org/abs/1304.5894 | author:Bruno Cornelis, Yun Yang, Joshua T. Vogelstein, Ann Dooms, Ingrid Daubechies, David Dunson category:cs.CV cs.LG published:2013-04-22 summary:The preservation of our cultural heritage is of paramount importance. Thanksto recent developments in digital acquisition techniques, powerful imageanalysis algorithms are developed which can be useful non-invasive tools toassist in the restoration and preservation of art. In this paper we propose asemi-supervised crack detection method that can be used for high-dimensionalacquisitions of paintings coming from different modalities. Our datasetconsists of a recently acquired collection of images of the Ghent Altarpiece(1432), one of Northern Europe's most important art masterpieces. Our goal isto build a classifier that is able to discern crack pixels from the backgroundconsisting of non-crack pixels, making optimal use of the information that isprovided by each modality. To accomplish this we employ a recently developednon-parametric Bayesian classifier, that uses tensor factorizations tocharacterize any conditional probability. A prior is placed on the parametersof the factorization such that every possible interaction between predictors isallowed while still identifying a sparse subset among these predictors. Theproposed Bayesian classifier, which we will refer to as conditional Bayesiantensor factorization or CBTF, is assessed by visually comparing classificationresults with the Random Forest (RF) algorithm.
arxiv-3600-265 | Multi-Label Classifier Chains for Bird Sound | http://arxiv.org/abs/1304.5862 | author:Forrest Briggs, Xiaoli Z. Fern, Jed Irvine category:cs.LG cs.SD stat.ML published:2013-04-22 summary:Bird sound data collected with unattended microphones for automatic surveys,or mobile devices for citizen science, typically contain multiplesimultaneously vocalizing birds of different species. However, few works haveconsidered the multi-label structure in birdsong. We propose to use an ensembleof classifier chains combined with a histogram-of-segments representation formulti-label classification of birdsong. The proposed method is compared withbinary relevance and three multi-instance multi-label learning (MIML)algorithms from prior work (which focus more on structure in the sound, andless on structure in the label sets). Experiments are conducted on tworeal-world birdsong datasets, and show that the proposed method usuallyoutperforms binary relevance (using the same features and base-classifier), andis better in some cases and worse in others compared to the MIML algorithms.
arxiv-3600-266 | The varifold representation of non-oriented shapes for diffeomorphic registration | http://arxiv.org/abs/1304.6108 | author:Nicolas Charon, Alain Trouvé category:cs.CG cs.CV math.DG published:2013-04-22 summary:In this paper, we address the problem of orientation that naturally ariseswhen representing shapes like curves or surfaces as currents. In the field ofcomputational anatomy, the framework of currents has indeed proved veryefficient to model a wide variety of shapes. However, in such approaches,orientation of shapes is a fundamental issue that can lead to several drawbacksin treating certain kind of datasets. More specifically, problems occur withstructures like acute pikes because of canceling effects of currents or withdata that consists in many disconnected pieces like fiber bundles for whichcurrents require a consistent orientation of all pieces. As a promisingalternative to currents, varifolds, introduced in the context of geometricmeasure theory by F. Almgren, allow the representation of any non-orientedmanifold (more generally any non-oriented rectifiable set). In particular, weexplain how varifolds can encode numerically non-oriented objects both from thediscrete and continuous point of view. We show various ways to build a Hilbertspace structure on the set of varifolds based on the theory of reproducingkernels. We show that, unlike the currents' setting, these metrics areconsistent with shape volume (theorem 4.1) and we derive a formula for thevariation of metric with respect to the shape (theorem 4.2). Finally, wepropose a generalization to non-oriented shapes of registration algorithms inthe context of Large Deformations Metric Mapping (LDDMM), which we detail witha few examples in the last part of the paper.
arxiv-3600-267 | Towards a Formal Distributional Semantics: Simulating Logical Calculi with Tensors | http://arxiv.org/abs/1304.5823 | author:Edward Grefenstette category:math.LO cs.CL cs.LO 68T50, 03B10 F.4.1; I.2.7 published:2013-04-22 summary:The development of compositional distributional models of semanticsreconciling the empirical aspects of distributional semantics with thecompositional aspects of formal semantics is a popular topic in thecontemporary literature. This paper seeks to bring this reconciliation one stepfurther by showing how the mathematical constructs commonly used incompositional distributional models, such as tensors and matrices, can be usedto simulate different aspects of predicate logic. This paper discusses how the canonical isomorphism between tensors andmultilinear maps can be exploited to simulate a full-blown quantifier-freepredicate calculus using tensors. It provides tensor interpretations of the setof logical connectives required to model propositional calculi. It suggests avariant of these tensor calculi capable of modelling quantifiers, using fewnon-linear operations. It finally discusses the relation between thesevariants, and how this relation should constitute the subject of future work.
arxiv-3600-268 | Continuum armed bandit problem of few variables in high dimensions | http://arxiv.org/abs/1304.5793 | author:Hemant Tyagi, Bernd Gärtner category:cs.LG published:2013-04-21 summary:We consider the stochastic and adversarial settings of continuum armedbandits where the arms are indexed by [0,1]^d. The reward functions r:[0,1]^d-> R are assumed to intrinsically depend on at most k coordinate variablesimplying r(x_1,..,x_d) = g(x_{i_1},..,x_{i_k}) for distinct and unknowni_1,..,i_k from {1,..,d} and some locally Holder continuous g:[0,1]^k -> R withexponent 0 < alpha <= 1. Firstly, assuming (i_1,..,i_k) to be fixed acrosstime, we propose a simple modification of the CAB1 algorithm where we constructthe discrete set of sampling points to obtain a bound ofO(n^((alpha+k)/(2*alpha+k)) (log n)^((alpha)/(2*alpha+k)) C(k,d)) on theregret, with C(k,d) depending at most polynomially in k and sub-logarithmicallyin d. The construction is based on creating partitions of {1,..,d} into kdisjoint subsets and is probabilistic, hence our result holds with highprobability. Secondly we extend our results to also handle the more generalcase where (i_1,...,i_k) can change over time and derive regret bounds for thesame.
arxiv-3600-269 | Prior-free and prior-dependent regret bounds for Thompson Sampling | http://arxiv.org/abs/1304.5758 | author:Sébastien Bubeck, Che-Yu Liu category:stat.ML cs.LG published:2013-04-21 summary:We consider the stochastic multi-armed bandit problem with a priordistribution on the reward distributions. We are interested in studyingprior-free and prior-dependent regret bounds, very much in the same spirit asthe usual distribution-free and distribution-dependent bounds for thenon-Bayesian stochastic bandit. Building on the techniques of Audibert andBubeck [2009] and Russo and Roy [2013] we first show that Thompson Samplingattains an optimal prior-free bound in the sense that for any priordistribution its Bayesian regret is bounded from above by $14 \sqrt{n K}$. Thisresult is unimprovable in the sense that there exists a prior distribution suchthat any algorithm has a Bayesian regret bounded from below by $\frac{1}{20}\sqrt{n K}$. We also study the case of priors for the setting of Bubeck et al.[2013] (where the optimal mean is known as well as a lower bound on thesmallest gap) and we show that in this case the regret of Thompson Sampling isin fact uniformly bounded over time, thus showing that Thompson Sampling cangreatly take advantage of the nice properties of these priors.
arxiv-3600-270 | Color image denoising by chromatic edges based vector valued diffusion | http://arxiv.org/abs/1304.5587 | author:V. B. Surya Prasath, Juan C. Moreno, K. Palaniappan category:cs.CV 68U10 I.4.3 published:2013-04-20 summary:In this letter we propose to denoise digital color images via an improvedgeometric diffusion scheme. By introducing edges detected from all three colorchannels into the diffusion the proposed scheme avoids color smearingartifacts. Vector valued diffusion is used to control the smoothing and thegeometry of color images are taken into consideration. Color edge strengthfunction computed from different planes is introduced and it stops thediffusion spread across chromatic edges. Experimental results indicate that thescheme achieves good denoising with edge preservation when compared to otherrelated schemes.
arxiv-3600-271 | Distributed Low-rank Subspace Segmentation | http://arxiv.org/abs/1304.5583 | author:Ameet Talwalkar, Lester Mackey, Yadong Mu, Shih-Fu Chang, Michael I. Jordan category:cs.CV cs.DC cs.LG stat.ML published:2013-04-20 summary:Vision problems ranging from image clustering to motion segmentation tosemi-supervised learning can naturally be framed as subspace segmentationproblems, in which one aims to recover multiple low-dimensional subspaces fromnoisy and corrupted input data. Low-Rank Representation (LRR), a convexformulation of the subspace segmentation problem, is provably and empiricallyaccurate on small problems but does not scale to the massive sizes of modernvision datasets. Moreover, past work aimed at scaling up low-rank matrixfactorization is not applicable to LRR given its non-decomposable constraints.In this work, we propose a novel divide-and-conquer algorithm for large-scalesubspace segmentation that can cope with LRR's non-decomposable constraints andmaintains LRR's strong recovery guarantees. This has immediate implications forthe scalability of subspace segmentation, which we demonstrate on a benchmarkface recognition dataset and in simulations. We then introduce novelapplications of LRR-based subspace segmentation to large-scale semi-supervisedlearning for multimedia event detection, concept detection, and image tagging.In each case, we obtain state-of-the-art results and order-of-magnitude speedups.
arxiv-3600-272 | Inverse Density as an Inverse Problem: The Fredholm Equation Approach | http://arxiv.org/abs/1304.5575 | author:Qichao Que, Mikhail Belkin category:cs.LG stat.ML published:2013-04-20 summary:In this paper we address the problem of estimating the ratio $\frac{q}{p}$where $p$ is a density function and $q$ is another density, or, more generallyan arbitrary function. Knowing or approximating this ratio is needed in variousproblems of inference and integration, in particular, when one needs to averagea function with respect to one probability distribution, given a sample fromanother. It is often referred as {\it importance sampling} in statisticalinference and is also closely related to the problem of {\it covariate shift}in transfer learning as well as to various MCMC methods. It may also be usefulfor separating the underlying geometry of a space, say a manifold, from thedensity function defined on it. Our approach is based on reformulating the problem of estimating$\frac{q}{p}$ as an inverse problem in terms of an integral operatorcorresponding to a kernel, and thus reducing it to an integral equation, knownas the Fredholm problem of the first kind. This formulation, combined with thetechniques of regularization and kernel methods, leads to a principledkernel-based framework for constructing algorithms and for analyzing themtheoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse RegularizedEstimator) is flexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds andconvergence rates for the Gaussian kernel in the case of densities defined on$\R^d$, compact domains in $\R^d$ and smooth $d$-dimensional sub-manifolds ofthe Euclidean space. We also show experimental results including applications to classificationand semi-supervised learning within the covariate shift framework anddemonstrate some encouraging experimental comparisons. We also show how theparameters of our algorithms can be chosen in a completely unsupervised manner.
arxiv-3600-273 | Analytic Feature Selection for Support Vector Machines | http://arxiv.org/abs/1304.5678 | author:Carly Stambaugh, Hui Yang, Felix Breuer category:cs.LG stat.ML I.2.6; I.2.7 published:2013-04-20 summary:Support vector machines (SVMs) rely on the inherent geometry of a data set toclassify training data. Because of this, we believe SVMs are an excellentcandidate to guide the development of an analytic feature selection algorithm,as opposed to the more commonly used heuristic methods. We propose afilter-based feature selection algorithm based on the inherent geometry of afeature set. Through observation, we identified six geometric properties thatdiffer between optimal and suboptimal feature sets, and have statisticallysignificant correlations to classifier performance. Our algorithm is based onlogistic and linear regression models using these six geometric properties aspredictor variables. The proposed algorithm achieves excellent results on highdimensional text data sets, with features that can be organized into a handfulof feature types; for example, unigrams, bigrams or semantic structuralfeatures. We believe this algorithm is a novel and effective approach tosolving the feature selection problem for linear SVMs.
arxiv-3600-274 | Dew Point modelling using GEP based multi objective optimization | http://arxiv.org/abs/1304.5594 | author:Siddharth Shroff, Vipul Dabhi category:cs.NE published:2013-04-20 summary:Different techniques are used to model the relationship between temperatures,dew point and relative humidity. Gene expression programming is capable ofmodelling complex realities with great accuracy, allowing at the same time, theextraction of knowledge from the evolved models compared to other learningalgorithms. We aim to use Gene Expression Programming for modelling of dewpoint. Generally, accuracy of the model is the only objective used by selectionmechanism of GEP. This will evolve large size models with low training error.To avoid this situation, use of multiple objectives, like accuracy and size ofthe model are preferred by Genetic Programming practitioners. Solution to amulti-objective problem is a set of solutions which satisfies the objectivesgiven by decision maker. Multi objective based GEP will be used to evolvesimple models. Various algorithms widely used for multi objective optimization,like NSGA II and SPEA 2, are tested on different test problems. The resultsobtained thereafter gives idea that SPEA 2 is better than NSGA II based on thefeatures like execution time, number of solutions obtained and convergencerate. We selected SPEA 2 for dew point prediction. The multi-objective base GEPproduces accurate and simpler (smaller) solutions compared to solutionsproduced by plain GEP for dew point predictions. Thus multi objective base GEPproduces better solutions by considering the dual objectives of fitness andsize of the solution. These simple models can be used to predict future valuesof dew point.
arxiv-3600-275 | A Survey on Multi-view Learning | http://arxiv.org/abs/1304.5634 | author:Chang Xu, Dacheng Tao, Chao Xu category:cs.LG published:2013-04-20 summary:In recent years, a great many methods of learning from multi-view data byconsidering the diversity of different views have been proposed. These viewsmay be obtained from multiple sources or different feature subsets. In tryingto organize and highlight similarities and differences between the variety ofmulti-view learning approaches, we review a number of representative multi-viewlearning algorithms in different areas and classify them into three groups: 1)co-training, 2) multiple kernel learning, and 3) subspace learning. Notably,co-training style algorithms train alternately to maximize the mutual agreementon two distinct views of the data; multiple kernel learning algorithms exploitkernels that naturally correspond to different views and combine kernels eitherlinearly or non-linearly to improve learning performance; and subspace learningalgorithms aim to obtain a latent subspace shared by multiple views by assumingthat the input views are generated from this latent subspace. Though there issignificant variance in the approaches to integrating multiple views to improvelearning performance, they mainly exploit either the consensus principle or thecomplementary principle to ensure the success of multi-view learning. Sinceaccessing multiple views is the fundament of multi-view learning, with theexception of study on learning a model from multiple views, it is also valuableto study how to construct multiple views and how to evaluate these views.Overall, by exploring the consistency and complementary properties of differentviews, multi-view learning is rendered more effective, more promising, and hasbetter generalization ability than single-view learning.
arxiv-3600-276 | Personalized Academic Research Paper Recommendation System | http://arxiv.org/abs/1304.5457 | author:Joonseok Lee, Kisung Lee, Jennifer G. Kim category:cs.IR cs.DL cs.LG published:2013-04-19 summary:A huge number of academic papers are coming out from a lot of conferences andjournals these days. In these circumstances, most researchers rely on key-basedsearch or browsing through proceedings of top conferences and journals to findtheir related work. To ease this difficulty, we propose a Personalized AcademicResearch Paper Recommendation System, which recommends related articles, foreach researcher, that may be interesting to her/him. In this paper, we firstintroduce our web crawler to retrieve research papers from the web. Then, wedefine similarity between two research papers based on the text similaritybetween them. Finally, we propose our recommender system developed usingcollaborative filtering methods. Our evaluation results demonstrate that oursystem recommends good quality research papers.
arxiv-3600-277 | A Joint Intensity and Depth Co-Sparse Analysis Model for Depth Map Super-Resolution | http://arxiv.org/abs/1304.5319 | author:Martin Kiechle, Simon Hawe, Martin Kleinsteuber category:cs.CV published:2013-04-19 summary:High-resolution depth maps can be inferred from low-resolution depthmeasurements and an additional high-resolution intensity image of the samescene. To that end, we introduce a bimodal co-sparse analysis model, which isable to capture the interdependency of registered intensity and depthinformation. This model is based on the assumption that the co-supports ofcorresponding bimodal image structures are aligned when computed by a suitablepair of analysis operators. No analytic form of such operators exist and wepropose a method for learning them from a set of registered training signals.This learning process is done offline and returns a bimodal analysis operatorthat is universally applicable to natural scenes. We use this to exploit thebimodal co-sparse analysis model as a prior for solving inverse problems, whichleads to an efficient algorithm for depth map super-resolution.
arxiv-3600-278 | Efficient Stochastic Gradient Descent for Strongly Convex Optimization | http://arxiv.org/abs/1304.5504 | author:Tianbao Yang, Lijun Zhang category:cs.LG stat.ML published:2013-04-19 summary:We motivate this study from a recent work on a stochastic gradient descent(SGD) method with only one projection \citep{DBLP:conf/nips/MahdaviYJZY12},which aims at alleviating the computational bottleneck of the standard SGDmethod in performing the projection at each iteration, and enjoys an $O(\logT/T)$ convergence rate for strongly convex optimization. In this paper, we makefurther contributions along the line. First, we develop an epoch-projection SGDmethod that only makes a constant number of projections less than $\log_2T$ butachieves an optimal convergence rate $O(1/T)$ for {\it strongly convexoptimization}. Second, we present a proximal extension to utilize the structureof the objective function that could further speed-up the computation andconvergence for sparse regularized loss minimization problems. Finally, weconsider an application of the proposed techniques to solving the highdimensional large margin nearest neighbor classification problem, yielding aspeed-up of orders of magnitude.
arxiv-3600-279 | Separating the Real from the Synthetic: Minutiae Histograms as Fingerprints of Fingerprints | http://arxiv.org/abs/1304.5409 | author:Carsten Gottschlich, Stephan Huckemann category:cs.CV cs.AI cs.DB published:2013-04-19 summary:In this study we show that by the current state-of-the-art syntheticallygenerated fingerprints can easily be discriminated from real fingerprints. Wepropose a method based on second order extended minutiae histograms (MHs) whichcan distinguish between real and synthetic prints with very high accuracy. MHsprovide a fixed-length feature vector for a fingerprint which are invariantunder rotation and translation. This 'test of realness' can be applied tosynthetic fingerprints produced by any method. In this work, tests areconducted on the 12 publicly available databases of FVC2000, FVC2002 andFVC2004 which are well established benchmarks for evaluating the performance offingerprint recognition algorithms; 3 of these 12 databases consist ofartificial fingerprints generated by the SFinGe software. Additionally, weevaluate the discriminative performance on a database of synthetic fingerprintsgenerated by the software of Bicz versus real fingerprint images. We concludewith suggestions for the improvement of synthetic fingerprint generation.
arxiv-3600-280 | Analytic Expressions for Stochastic Distances Between Relaxed Complex Wishart Distributions | http://arxiv.org/abs/1304.5417 | author:Alejandro C. Frery, Abraão D. C. Nascimento, Renato J. Cintra category:stat.ML published:2013-04-19 summary:The scaled complex Wishart distribution is a widely used model for multilookfull polarimetric SAR data whose adequacy has been attested in the literature.Classification, segmentation, and image analysis techniques which depend onthis model have been devised, and many of them employ some type ofdissimilarity measure. In this paper we derive analytic expressions for fourstochastic distances between relaxed scaled complex Wishart distributions intheir most general form and in important particular cases. Using thesedistances, inequalities are obtained which lead to new ways of deriving theBartlett and revised Wishart distances. The expressiveness of the four analyticdistances is assessed with respect to the variation of parameters. Suchdistances are then used for deriving new tests statistics, which are proved tohave asymptotic chi-square distribution. Adopting the test size as a comparisoncriterion, a sensitivity study is performed by means of Monte Carlo experimentssuggesting that the Bhattacharyya statistic outperforms all the others. Thepower of the tests is also assessed. Applications to actual data illustrate thediscrimination and homogeneity identification capabilities of these distances.
arxiv-3600-281 | Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget | http://arxiv.org/abs/1304.5299 | author:Anoop Korattikara, Yutian Chen, Max Welling category:cs.LG stat.ML published:2013-04-19 summary:Can we make Bayesian posterior MCMC sampling more efficient when faced withvery large datasets? We argue that computing the likelihood for N datapoints inthe Metropolis-Hastings (MH) test to reach a single binary decision iscomputationally inefficient. We introduce an approximate MH rule based on asequential hypothesis test that allows us to accept or reject samples with highconfidence using only a fraction of the data required for the exact MH rule.While this method introduces an asymptotic bias, we show that this bias can becontrolled and is more than offset by a decrease in variance due to our abilityto draw more samples per unit of time.
arxiv-3600-282 | Inexact Coordinate Descent: Complexity and Preconditioning | http://arxiv.org/abs/1304.5530 | author:Rachael Tappenden, Peter Richtárik, Jacek Gondzio category:math.OC cs.AI stat.ML published:2013-04-19 summary:In this paper we consider the problem of minimizing a convex function using arandomized block coordinate descent method. One of the key steps at eachiteration of the algorithm is determining the update to a block of variables.Existing algorithms assume that in order to compute the update, a particularsubproblem is solved exactly. In his work we relax this requirement, and allowfor the subproblem to be solved inexactly, leading to an inexact blockcoordinate descent method. Our approach incorporates the best known results forexact updates as a special case. Moreover, these theoretical guarantees arecomplemented by practical considerations: the use of iterative techniques todetermine the update as well as the use of preconditioning for furtheracceleration.
arxiv-3600-283 | Parallel Gaussian Process Optimization with Upper Confidence Bound and Pure Exploration | http://arxiv.org/abs/1304.5350 | author:Emile Contal, David Buffoni, Alexandre Robicquet, Nicolas Vayatis category:cs.LG stat.ML published:2013-04-19 summary:In this paper, we consider the challenge of maximizing an unknown function ffor which evaluations are noisy and are acquired with high cost. An iterativeprocedure uses the previous measures to actively select the next estimation off which is predicted to be the most useful. We focus on the case where thefunction can be evaluated in parallel with batches of fixed size and analyzethe benefit compared to the purely sequential procedure in terms of cumulativeregret. We introduce the Gaussian Process Upper Confidence Bound and PureExploration algorithm (GP-UCB-PE) which combines the UCB strategy and PureExploration in the same batch of evaluations along the parallel iterations. Weprove theoretical upper bounds on the regret with batches of size K for thisprocedure which show the improvement of the order of sqrt{K} for fixediteration cost over purely sequential versions. Moreover, the multiplicativeconstants involved have the property of being dimension-free. We also confirmempirically the efficiency of GP-UCB-PE on real and synthetic problems comparedto state-of-the-art competitors.
arxiv-3600-284 | Object Tracking in Videos: Approaches and Issues | http://arxiv.org/abs/1304.5212 | author:Duc Phu Chau, François Bremond, Monique Thonnat category:cs.CV published:2013-04-18 summary:Mobile object tracking has an important role in the computer visionapplications. In this paper, we use a tracked target-based taxonomy to presentthe object tracking algorithms. The tracked targets are divided into threecategories: points of interest, appearance and silhouette of mobile objects.Advantages and limitations of the tracking approaches are also analyzed to findthe future directions in the object tracking domain.
arxiv-3600-285 | Image Retrieval based on Bag-of-Words model | http://arxiv.org/abs/1304.5168 | author:Jialu Liu category:cs.IR cs.LG published:2013-04-18 summary:This article gives a survey for bag-of-words (BoW) or bag-of-features modelin image retrieval system. In recent years, large-scale image retrieval showssignificant potential in both industry applications and research problems. Aslocal descriptors like SIFT demonstrate great discriminative power in solvingvision problems like object recognition, image classification and annotation,more and more state-of-the-art large scale image retrieval systems are tryingto rely on them. A common way to achieve this is first quantizing localdescriptors into visual words, and then applying scalable textual indexing andretrieval schemes. We call this model as bag-of-words or bag-of-features model.The goal of this survey is to give an overview of this model and introducedifferent strategies when building the system based on this model.
arxiv-3600-286 | Polygon Matching and Indexing Under Affine Transformations | http://arxiv.org/abs/1304.4994 | author:Edgar Chávez, Ana C. Chávez-Cáliz, Jorge L. López-López category:cs.CV 51N10 published:2013-04-18 summary:Given a collection $\{Z_1,Z_2,\ldots,Z_m\}$ of $n$-sided polygons in theplane and a query polygon $W$ we give algorithms to find all $Z_\ell$ such that$W=f(Z_\ell)$ with $f$ an unknown similarity transformation in time independentof the size of the collection. If $f$ is a known affine transformation, we showhow to find all $Z_\ell$ such that $W=f(Z_\ell)$ in $O(n+\log(m))$ time. For a pair $W,W^\prime$ of polygons we can find all the pairs$Z_\ell,Z_{\ell^\prime}$ such that $W=f(Z_\ell)$ and$W^\prime=f(Z_{\ell^\prime})$ for an unknown affine transformation $f$ in$O(m+n)$ time. For the case of triangles we also give bounds for the problem of matchingtriangles with variable vertices, which is equivalent to affine matchingtriangles in noisy conditions.
arxiv-3600-287 | Feature Elimination in Kernel Machines in moderately high dimensions | http://arxiv.org/abs/1304.5245 | author:Sayan Dasgupta, Yair Goldberg, Michael Kosorok category:stat.ML 68T05, 62G08 published:2013-04-18 summary:We develop an approach for feature elimination in statistical learning withkernel machines, based on recursive elimination of features.We presenttheoretical properties of this method and show that it is uniformly consistentin finding the correct feature space under certain generalized assumptions.Wepresent four case studies to show that the assumptions are met in mostpractical situations and present simulation results to demonstrate performanceof the proposed approach.
arxiv-3600-288 | Combinaison d'information visuelle, conceptuelle, et contextuelle pour la construction automatique de hierarchies semantiques adaptees a l'annotation d'images | http://arxiv.org/abs/1304.5063 | author:Hichem Bannour, Céline Hudelot category:cs.CV cs.LG cs.MM 68T45 I.4.10 published:2013-04-18 summary:This paper proposes a new methodology to automatically build semantichierarchies suitable for image annotation and classification. The building ofthe hierarchy is based on a new measure of semantic similarity. The proposedmeasure incorporates several sources of information: visual, conceptual andcontextual as we defined in this paper. The aim is to provide a measure thatbest represents image semantics. We then propose rules based on this measure,for the building of the final hierarchy, and which explicitly encodehierarchical relationships between different concepts. Therefore, the builthierarchy is used in a semantic hierarchical classification framework for imageannotation. Our experiments and results show that the hierarchy built improvesclassification results. Ce papier propose une nouvelle methode pour la construction automatique dehierarchies semantiques adaptees a la classification et a l'annotationd'images. La construction de la hierarchie est basee sur une nouvelle mesure desimilarite semantique qui integre plusieurs sources d'informations: visuelle,conceptuelle et contextuelle que nous definissons dans ce papier. L'objectifest de fournir une mesure qui est plus proche de la semantique des images. Nousproposons ensuite des regles, basees sur cette mesure, pour la construction dela hierarchie finale qui encode explicitement les relations hierarchiques entreles differents concepts. La hierarchie construite est ensuite utilisee dans uncadre de classification semantique hierarchique d'images en concepts visuels.Nos experiences et resultats montrent que la hierarchie construite permetd'ameliorer les resultats de la classification.
arxiv-3600-289 | Unsupervised model-free representation learning | http://arxiv.org/abs/1304.4806 | author:Daniil Ryabko category:cs.LG stat.ML published:2013-04-17 summary:Numerous control and learning problems face the situation where sequences ofhigh-dimensional highly dependent data are available, but no or little feedbackis provided to the learner. To address this issue, we formulate the followingproblem. Given a series of observations X_0,...,X_n coming from a large(high-dimensional) space X, find a representation function f mapping X to afinite space Y such that the series f(X_0),...,f(X_n) preserve as muchinformation as possible about the original time-series dependence inX_0,...,X_n. We show that, for stationary time series, the function f can beselected as the one maximizing the time-series information h_0(f(X))- h_\infty(f(X)) where h_0(f(X)) is the Shannon entropy of f(X_0) and h_\infty (f(X)) isthe entropy rate of the time series f(X_0),...,f(X_n),... Implications for theproblem of optimal control are presented.
arxiv-3600-290 | Low-Rank Matrix and Tensor Completion via Adaptive Sampling | http://arxiv.org/abs/1304.4672 | author:Akshay Krishnamurthy, Aarti Singh category:stat.ML published:2013-04-17 summary:We study low rank matrix and tensor completion and propose novel algorithmsthat employ adaptive sampling schemes to obtain strong performance guarantees.Our algorithms exploit adaptivity to identify entries that are highlyinformative for learning the column space of the matrix (tensor) andconsequently, our results hold even when the row space is highly coherent, incontrast with previous analyses. In the absence of noise, we show that one canexactly recover a $n \times n$ matrix of rank $r$ from merely $\Omega(nr^{3/2}\log(r))$ matrix entries. We also show that one can recover an order $T$tensor using $\Omega(n r^{T-1/2}T^2 \log(r))$ entries. For noisy recovery, ouralgorithm consistently estimates a low rank matrix corrupted with noise using$\Omega(n r^{3/2} \textrm{polylog}(n))$ entries. We complement our study withsimulations that verify our theory and demonstrate the scalability of ouralgorithms.
arxiv-3600-291 | Hands-free Evolution of 3D-printable Objects via Eye Tracking | http://arxiv.org/abs/1304.4889 | author:Nick Cheney, Jeff Clune, Jason Yosinski, Hod Lipson category:cs.NE cs.HC published:2013-04-17 summary:Interactive evolution has shown the potential to create amazing and complexforms in both 2-D and 3-D settings. However, the algorithm is slow and usersquickly become fatigued. We propose that the use of eye tracking forinteractive evolution systems will both reduce user fatigue and improveevolutionary success. We describe a systematic method for testing thehypothesis that eye tracking driven interactive evolution will be a moresuccessful and easier-to-use design method than traditional interactiveevolution methods driven by mouse clicks. We provide preliminary results thatsupport the possibility of this proposal, and lay out future work toinvestigate these advantages in extensive clinical trials.
arxiv-3600-292 | A Health Monitoring System for Elder and Sick Persons | http://arxiv.org/abs/1304.4652 | author:Ankit Chaudhary, Jagdish L. Raheja category:cs.CV cs.HC published:2013-04-17 summary:This paper discusses a vision based health monitoring system which would bevery easy in use and deployment. Elder and sick people who are not able to talkor walk they are dependent on other human beings for their daily needs and needcontinuous monitoring. The developed system provides facility to the sick orelder person to describe his or her need to their caretaker in lingualdescription by showing particular hand gesture with the developed system. Thissystem uses fingertip detection technique for gesture extraction and artificialneural network for gesture classification and recognition. The system is ableto work in different light conditions and can be connected to different devicesto announce users need on a distant location.
arxiv-3600-293 | Tracking of Fingertips and Centres of Palm using KINECT | http://arxiv.org/abs/1304.4662 | author:J. L. Raheja, A. Chaudhary, K Singal category:cs.CV published:2013-04-17 summary:Hand Gesture is a popular way to interact or control machines and it has beenimplemented in many applications. The geometry of hand is such that it is hardto construct in virtual environment and control the joints but thefunctionality and DOF encourage researchers to make a hand like instrument.This paper presents a novel method for fingertips detection and centres ofpalms detection distinctly for both hands using MS KINECT in 3D from the inputimage. KINECT facilitates us by providing the depth information of foregroundobjects. The hands were segmented using the depth vector and centres of palmswere detected using distance transformation on inverse image. This result wouldbe used to feed the inputs to the robotic hands to emulate human handsoperation.
arxiv-3600-294 | Automated Switching System for Skin Pixel Segmentation in Varied Lighting | http://arxiv.org/abs/1304.4711 | author:Ankit Chaudhary, Ankur Gupta category:cs.CV published:2013-04-17 summary:In Computer Vision, colour-based spatial techniquesoften assume a static skincolour model. However, skin colour perceived by a camera can change whenlighting changes. In common real environment multiple light sources impinge onthe skin. Moreover, detection techniques may vary when the image under study istaken under different lighting condition than the one that was earlier underconsideration. Therefore, for robust skin pixel detection, a dynamic skincolour model that can cope with the changes must be employed. This paper showsthat skin pixel detection in a digital colour image can be significantlyimproved by employing automated colour space switching methods. In the root ofthe switching technique which is employed in this study, lies the statisticalmean of value of the skin pixels in the image which in turn has been derivedfrom the Value, measures as a third component of the HSV. The study is based onexperimentations on a set of images where capture time conditions varying fromhighly illuminated to almost dark.
arxiv-3600-295 | The Mahalanobis distance for functional data with applications to classification | http://arxiv.org/abs/1304.4786 | author:Esdras Joseph, Pedro Galeano, Rosa E. Lillo category:math.ST stat.CO stat.ME stat.ML stat.TH published:2013-04-17 summary:This paper presents a general notion of Mahalanobis distance for functionaldata that extends the classical multivariate concept to situations where theobserved data are points belonging to curves generated by a stochastic process.More precisely, a new semi-distance for functional observations that generalizethe usual Mahalanobis distance for multivariate datasets is introduced. Forthat, the development uses a regularized square root inverse operator inHilbert spaces. Some of the main characteristics of the functional Mahalanobissemi-distance are shown. Afterwards, new versions of several well knownfunctional classification procedures are developed using the Mahalanobisdistance for functional data as a measure of proximity between functionalobservations. The performance of several well known functional classificationprocedures are compared with those methods used in conjunction with theMahalanobis distance for functional data, with positive results, through aMonte Carlo study and the analysis of two real data examples.
arxiv-3600-296 | A Junction Tree Framework for Undirected Graphical Model Selection | http://arxiv.org/abs/1304.4910 | author:Divyanshu Vats, Robert Nowak category:stat.ML cs.AI cs.IT math.IT published:2013-04-17 summary:An undirected graphical model is a joint probability distribution defined onan undirected graph G*, where the vertices in the graph index a collection ofrandom variables and the edges encode conditional independence relationshipsamong random variables. The undirected graphical model selection (UGMS) problemis to estimate the graph G* given observations drawn from the undirectedgraphical model. This paper proposes a framework for decomposing the UGMSproblem into multiple subproblems over clusters and subsets of the separatorsin a junction tree. The junction tree is constructed using a graph thatcontains a superset of the edges in G*. We highlight three main properties ofusing junction trees for UGMS. First, different regularization parameters ordifferent UGMS algorithms can be used to learn different parts of the graph.This is possible since the subproblems we identify can be solved independentlyof each other. Second, under certain conditions, a junction tree based UGMSalgorithm can produce consistent results with fewer observations than the usualrequirements of existing algorithms. Third, both our theoretical andexperimental results show that the junction tree framework does a significantlybetter job at finding the weakest edges in a graph than existing methods. Thisproperty is a consequence of both the first and second properties. Finally, wenote that our framework is independent of the choice of the UGMS algorithm andcan be used as a wrapper around standard UGMS algorithms for more accurategraph estimation.
arxiv-3600-297 | Robust Noise Filtering in Image Sequences | http://arxiv.org/abs/1304.4765 | author:Soumaya Hichri, Faouzi Benzarti, Hamid Amiri category:cs.CV published:2013-04-17 summary:Image sequences filtering have recently become a very important technicalproblem especially with the advent of new technology in multimedia and videosystems applications. Often image sequences are corrupted by some amount ofnoise introduced by the image sensor and therefore inherently present in theimaging process. The main problem in the image sequences is how to deal withspatio-temporal and non stationary signals. In this paper, we propose a robustmethod for noise removal of image sequence based on coupled spatial andtemporal anisotropic diffusion. The idea is to achieve an adaptive smoothing inboth spatial and temporal directions, by solving a nonlinear diffusionequation. This allows removing noise while preserving all spatial and temporaldiscontinuities
arxiv-3600-298 | Spectral Compressed Sensing via Structured Matrix Completion | http://arxiv.org/abs/1304.4610 | author:Yuxin Chen, Yuejie Chi category:cs.IT cs.LG math.IT math.NA stat.ML published:2013-04-16 summary:The paper studies the problem of recovering a spectrally sparse object from asmall number of time domain samples. Specifically, the object of interest withambient dimension $n$ is assumed to be a mixture of $r$ complexmulti-dimensional sinusoids, while the underlying frequencies can assume anyvalue in the unit disk. Conventional compressed sensing paradigms suffer fromthe {\em basis mismatch} issue when imposing a discrete dictionary on theFourier representation. To address this problem, we develop a novelnonparametric algorithm, called enhanced matrix completion (EMaC), based onstructured matrix completion. The algorithm starts by arranging the data into alow-rank enhanced form with multi-fold Hankel structure, then attempts recoveryvia nuclear norm minimization. Under mild incoherence conditions, EMaC allowsperfect recovery as soon as the number of samples exceeds the order of$\mathcal{O}(r\log^{2} n)$. We also show that, in many instances, accuratecompletion of a low-rank multi-fold Hankel matrix is possible when the numberof observed entries is proportional to the information theoretical limits(except for a logarithmic gap). The robustness of EMaC against bounded noiseand its applicability to super resolution are further demonstrated by numericalexperiments.
arxiv-3600-299 | Sparse Coding and Dictionary Learning for Symmetric Positive Definite Matrices: A Kernel Approach | http://arxiv.org/abs/1304.4344 | author:Mehrtash T. Harandi, Conrad Sanderson, Richard Hartley, Brian C. Lovell category:cs.LG cs.CV stat.ML published:2013-04-16 summary:Recent advances suggest that a wide range of computer vision problems can beaddressed more appropriately by considering non-Euclidean geometry. This papertackles the problem of sparse coding and dictionary learning in the space ofsymmetric positive definite matrices, which form a Riemannian manifold. Withthe aid of the recently introduced Stein kernel (related to a symmetric versionof Bregman matrix divergence), we propose to perform sparse coding by embeddingRiemannian manifolds into reproducing kernel Hilbert spaces. This leads to aconvex and kernel version of the Lasso problem, which can be solvedefficiently. We furthermore propose an algorithm for learning a Riemanniandictionary (used for sparse coding), closely tied to the Stein kernel.Experiments on several classification tasks (face recognition, textureclassification, person re-identification) show that the proposed sparse codingapproach achieves notable improvements in discrimination accuracy, incomparison to state-of-the-art methods such as tensor sparse coding, Riemannianlocality preserving projection, and symmetry-driven accumulation of localfeatures.
arxiv-3600-300 | Sentiment Analysis : A Literature Survey | http://arxiv.org/abs/1304.4520 | author:Subhabrata Mukherjee, Pushpak Bhattacharyya category:cs.CL published:2013-04-16 summary:Our day-to-day life has always been influenced by what people think. Ideasand opinions of others have always affected our own opinions. The explosion ofWeb 2.0 has led to increased activity in Podcasting, Blogging, Tagging,Contributing to RSS, Social Bookmarking, and Social Networking. As a resultthere has been an eruption of interest in people to mine these vast resourcesof data for opinions. Sentiment Analysis or Opinion Mining is the computationaltreatment of opinions, sentiments and subjectivity of text. In this report, wetake a look at the various challenges and applications of Sentiment Analysis.We will discuss in details various approaches to perform a computationaltreatment of sentiments and opinions. Various supervised or data-driventechniques to SA like Na\"ive Byes, Maximum Entropy, SVM, and Voted Perceptronswill be discussed and their strengths and drawbacks will be touched upon. Wewill also see a new dimension of analyzing sentiments by Cognitive Psychologymainly through the work of Janyce Wiebe, where we will see ways to detectsubjectivity, perspective in narrative and understanding the discoursestructure. We will also study some specific topics in Sentiment Analysis andthe contemporary works in those areas.
