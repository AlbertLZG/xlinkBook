arxiv-3600-1 | Top-down particle filtering for Bayesian decision trees | http://arxiv.org/pdf/1303.0561v2.pdf | author:Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh category:stat.ML cs.LG published:2013-03-03 summary:Decision tree learning is a popular approach for classification andregression in machine learning and statistics, and Bayesianformulations---which introduce a prior distribution over decision trees, andformulate learning as posterior inference given data---have been shown toproduce competitive performance. Unlike classic decision tree learningalgorithms like ID3, C4.5 and CART, which work in a top-down manner, existingBayesian algorithms produce an approximation to the posterior distribution byevolving a complete tree (or collection thereof) iteratively via local MonteCarlo modifications to the structure of the tree, e.g., using Markov chainMonte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm thatinstead works in a top-down manner, mimicking the behavior and speed of classicalgorithms. We demonstrate empirically that our approach delivers accuracycomparable to the most popular MCMC method, but operates more than an order ofmagnitude faster, and thus represents a better computation-accuracy tradeoff.
arxiv-3600-2 | Sentiment in New York City: A High Resolution Spatial and Temporal View | http://arxiv.org/pdf/1308.5010v1.pdf | author:Karla Z. Bertrand, Maya Bialik, Kawandeep Virdee, Andreas Gros, Yaneer Bar-Yam category:physics.soc-ph cs.CL cs.CY published:2013-08-22 summary:Measuring public sentiment is a key task for researchers and policymakersalike. The explosion of available social media data allows for a moretime-sensitive and geographically specific analysis than ever before. In thispaper we analyze data from the micro-blogging site Twitter and generate asentiment map of New York City. We develop a classifier specifically tuned for140-character Twitter messages, or tweets, using key words, phrases andemoticons to determine the mood of each tweet. This method, combined withgeotagging provided by users, enables us to gauge public sentiment on extremelyfine-grained spatial and temporal scales. We find that public mood is generallyhighest in public parks and lowest at transportation hubs, and locate otherareas of strong sentiment such as cemeteries, medical centers, a jail, and asewage facility. Sentiment progressively improves with proximity to TimesSquare. Periodic patterns of sentiment fluctuate on both a daily and a weeklyscale: more positive tweets are posted on weekends than on weekdays, with adaily peak in sentiment around midnight and a nadir between 9:00 a.m. and noon.
arxiv-3600-3 | Reinforcement learning for port-Hamiltonian systems | http://arxiv.org/pdf/1212.5524v2.pdf | author:Olivier Sprangers, Gabriel A. D. Lopes, Robert Babuska category:cs.SY cs.LG published:2012-12-21 summary:Passivity-based control (PBC) for port-Hamiltonian systems provides anintuitive way of achieving stabilization by rendering a system passive withrespect to a desired storage function. However, in most instances the controllaw is obtained without any performance considerations and it has to becalculated by solving a complex partial differential equation (PDE). In orderto address these issues we introduce a reinforcement learning approach into theenergy-balancing passivity-based control (EB-PBC) method, which is a form ofPBC in which the closed-loop energy is equal to the difference between thestored and supplied energies. We propose a technique to parameterize EB-PBCthat preserves the systems's PDE matching conditions, does not require thespecification of a global desired Hamiltonian, includes performance criteria,and is robust to extra non-linearities such as control input saturation. Theparameters of the control law are found using actor-critic reinforcementlearning, enabling learning near-optimal control policies satisfying a desiredclosed-loop energy landscape. The advantages are that near-optimal controllerscan be generated using standard energy shaping techniques and that thesolutions learned can be interpreted in terms of energy shaping and dampinginjection, which makes it possible to numerically assess stability usingpassivity theory. From the reinforcement learning perspective, our proposalallows for the class of port-Hamiltonian systems to be incorporated in theactor-critic framework, speeding up the learning thanks to the resultingparameterization of the policy. The method has been successfully applied to thependulum swing-up problem in simulations and real-life experiments.
arxiv-3600-4 | A Unified Framework for Multi-Sensor HDR Video Reconstruction | http://arxiv.org/pdf/1308.4908v1.pdf | author:Joel Kronander, Stefan Gustavson, Gerhard Bonnet, Anders Ynnerman, Jonas Unger category:cs.CV cs.GR cs.MM published:2013-08-22 summary:One of the most successful approaches to modern high quality HDR-videocapture is to use camera setups with multiple sensors imaging the scene througha common optical system. However, such systems pose several challenges for HDRreconstruction algorithms. Previous reconstruction techniques have considereddebayering, denoising, resampling (align- ment) and exposure fusion as separateproblems. In contrast, in this paper we present a unifying approach, performingHDR assembly directly from raw sensor data. Our framework includes a cameranoise model adapted to HDR video and an algorithm for spatially adaptive HDRreconstruction based on fitting of local polynomial approximations to observedsensor data. The method is easy to implement and allows reconstruction to anarbitrary resolution and output mapping. We present an implementation in CUDAand show real-time performance for an experimental 4 Mpixel multi-sensor HDRvideo system. We further show that our algorithm has clear advantages overexisting methods, both in terms of flexibility and reconstruction quality.
arxiv-3600-5 | Validation of Soft Classification Models using Partial Class Memberships: An Extended Concept of Sensitivity & Co. applied to the Grading of Astrocytoma Tissues | http://arxiv.org/pdf/1301.0264v2.pdf | author:Claudia Beleites, Reiner Salzer, Valter Sergo category:stat.ML stat.AP published:2013-01-02 summary:We use partial class memberships in soft classification to model uncertainlabelling and mixtures of classes. Partial class memberships are not restrictedto predictions, but may also occur in reference labels (ground truth, goldstandard diagnosis) for training and validation data. Classifier performance is usually expressed as fractions of the confusionmatrix, such as sensitivity, specificity, negative and positive predictivevalues. We extend this concept to soft classification and discuss the bias andvariance properties of the extended performance measures. Ambiguity inreference labels translates to differences between best-case, expected andworst-case performance. We show a second set of measures comparing expected andideal performance which is closely related to regression performance, namelythe root mean squared error RMSE and the mean absolute error MAE. All calculations apply to classical crisp classification as well as to softclassification (partial class memberships and/or one-class classifiers). Theproposed performance measures allow to test classifiers with actual borderlinecases. In addition, hardening of e.g. posterior probabilities into class labelsis not necessary, avoiding the corresponding information loss and increase invariance. We implement the proposed performance measures in the R package"softclassval", which is available from CRAN and athttp://softclassval.r-forge.r-project.org. Our reasoning as well as the importance of partial memberships forchemometric classification is illustrated by a real-word application:astrocytoma brain tumor tissue grading (80 patients, 37000 spectra) for findingsurgical excision borders. As borderline cases are the actual target of theanalytical technique, samples which are diagnosed to be borderline cases mustbe included in the validation.
arxiv-3600-6 | A review on handwritten character and numeral recognition for Roman, Arabic, Chinese and Indian scripts | http://arxiv.org/pdf/1308.4902v1.pdf | author:Aini Najwa Azmi, Dewi Nasien, Siti Mariyam Shamsuddin category:cs.CV published:2013-08-22 summary:There are a lot of intensive researches on handwritten character recognition(HCR) for almost past four decades. The research has been done on some ofpopular scripts such as Roman, Arabic, Chinese and Indian. In this paper wepresent a review on HCR work on the four popular scripts. We have summarizedmost of the published paper from 2005 to recent and also analyzed the variousmethods in creating a robust HCR system. We also added some future direction ofresearch on HCR.
arxiv-3600-7 | Learning Sequence Neighbourhood Metrics | http://arxiv.org/pdf/1109.2034v2.pdf | author:Justin Bayer, Christian Osendorfer, Patrick van der Smagt category:cs.NE cs.LG published:2011-09-09 summary:Recurrent neural networks (RNNs) in combination with a pooling operator andthe neighbourhood components analysis (NCA) objective function are able todetect the characterizing dynamics of sequences and embed them into afixed-length vector space of arbitrary dimensionality. Subsequently, theresulting features are meaningful and can be used for visualization or nearestneighbour classification in linear time. This kind of metric learning forsequential data enables the use of algorithms tailored towards fixed lengthvector spaces such as R^n.
arxiv-3600-8 | The Sample-Complexity of General Reinforcement Learning | http://arxiv.org/pdf/1308.4828v1.pdf | author:Tor Lattimore, Marcus Hutter, Peter Sunehag category:cs.LG published:2013-08-22 summary:We present a new algorithm for general reinforcement learning where the trueenvironment is known to belong to a finite class of N arbitrary models. Thealgorithm is shown to be near-optimal for all but O(N log^2 N) time-steps withhigh probability. Infinite classes are also considered where we show thatcompactness is a key criterion for determining the existence of uniformsample-complexity bounds. A matching lower bound is given for the finite case.
arxiv-3600-9 | Convex and Scalable Weakly Labeled SVMs | http://arxiv.org/pdf/1303.1271v5.pdf | author:Yu-Feng Li, Ivor W. Tsang, James T. Kwok, Zhi-Hua Zhou category:cs.LG published:2013-03-06 summary:In this paper, we study the problem of learning from weakly labeled data,where labels of the training examples are incomplete. This includes, forexample, (i) semi-supervised learning where labels are partially known; (ii)multi-instance learning where labels are implicitly known; and (iii) clusteringwhere labels are completely unknown. Unlike supervised learning, learning withweak labels involves a difficult Mixed-Integer Programming (MIP) problem.Therefore, it can suffer from poor scalability and may also get stuck in localminimum. In this paper, we focus on SVMs and propose the WellSVM via a novellabel generation strategy. This leads to a convex relaxation of the originalMIP, which is at least as tight as existing convex Semi-Definite Programming(SDP) relaxations. Moreover, the WellSVM can be solved via a sequence of SVMsubproblems that are much more scalable than previous convex SDP relaxations.Experiments on three weakly labeled learning tasks, namely, (i) semi-supervisedlearning; (ii) multi-instance learning for locating regions of interest incontent-based information retrieval; and (iii) clustering, clearly demonstrateimproved performance, and WellSVM is also readily applicable on large datasets.
arxiv-3600-10 | Invertibility and Robustness of Phaseless Reconstruction | http://arxiv.org/pdf/1308.4718v1.pdf | author:Radu Balan, Yang Wang category:math.FA cs.CV stat.ML published:2013-08-21 summary:This paper is concerned with the question of reconstructing a vector in afinite-dimensional real Hilbert space when only the magnitudes of thecoefficients of the vector under a redundant linear map are known. We analyzevarious Lipschitz bounds of the nonlinear analysis map and we establishtheoretical performance bounds of any reconstruction algorithm. We show thatrobust and stable reconstruction requires additional redundancy than thecritical threshold.
arxiv-3600-11 | A proposal for a Chinese keyboard for cellphones, smartphones, ipads and tablets | http://arxiv.org/pdf/1308.4965v1.pdf | author:Maurice Margenstern, Lan Wu category:cs.HC cs.CL 69U99, 94A99 H.1.2; H.5.2 published:2013-08-21 summary:In this paper, we investigate the possibility to use two tilings of thehyperbolic plane as basic frame for devising a way to input texts in Chinesecharacters into messages of cellphones, smartphones, ipads and tablets.
arxiv-3600-12 | Can inferred provenance and its visualisation be used to detect erroneous annotation? A case study using UniProtKB | http://arxiv.org/pdf/1308.4618v1.pdf | author:Michael J. Bell, Matthew Collison, Phillip Lord category:cs.CL cs.CE cs.DL q-bio.QM published:2013-08-21 summary:A constant influx of new data poses a challenge in keeping the annotation inbiological databases current. Most biological databases contain significantquantities of textual annotation, which often contains the richest source ofknowledge. Many databases reuse existing knowledge, during the curation processannotations are often propagated between entries. However, this is often notmade explicit. Therefore, it can be hard, potentially impossible, for a readerto identify where an annotation originated from. Within this work we attempt toidentify annotation provenance and track its subsequent propagation.Specifically, we exploit annotation reuse within the UniProt Knowledgebase(UniProtKB), at the level of individual sentences. We describe a visualisationapproach for the provenance and propagation of sentences in UniProtKB whichenables a large-scale statistical analysis. Initially levels of sentence reusewithin UniProtKB were analysed, showing that reuse is heavily prevalent, whichenables the tracking of provenance and propagation. By analysing sentencesthroughout UniProtKB, a number of interesting propagation patterns wereidentified, covering over 100, 000 sentences. Over 8000 sentences remain in thedatabase after they have been removed from the entries where they originallyoccurred. Analysing a subset of these sentences suggest that approximately 30%are erroneous, whilst 35% appear to be inconsistent. These results suggest thatbeing able to visualise sentence propagation and provenance can aid in thedetermination of the accuracy and quality of textual annotation. Source codeand supplementary data are available from the authors website.
arxiv-3600-13 | A study of retrieval algorithms of sparse messages in networks of neural cliques | http://arxiv.org/pdf/1308.4506v1.pdf | author:Ala Aboudib, Vincent Gripon, Xiaoran Jiang category:cs.NE published:2013-08-21 summary:Associative memories are data structures addressed using part of the contentrather than an index. They offer good fault reliability and biologicalplausibility. Among different families of associative memories, sparse ones areknown to offer the best efficiency (ratio of the amount of bits stored to thatof bits used by the network itself). Their retrieval process performance hasbeen shown to benefit from the use of iterations. However classical algorithmsrequire having prior knowledge about the data to retrieve such as the number ofnonzero symbols. We introduce several families of algorithms to enhance theretrieval process performance in recently proposed sparse associative memoriesbased on binary neural networks. We show that these algorithms provide betterperformance, along with better plausibility than existing techniques. We alsoanalyze the required number of iterations and derive corresponding curves.
arxiv-3600-14 | High-Dimensional Feature Selection by Feature-Wise Non-Linear Lasso | http://arxiv.org/pdf/1202.0515v3.pdf | author:Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P. Xing, Masashi Sugiyama category:stat.ML cs.AI stat.ME published:2012-02-02 summary:The goal of supervised feature selection is to find a subset of inputfeatures that are responsible for predicting output values. The least absoluteshrinkage and selection operator (Lasso) allows computationally efficientfeature selection based on linear dependency between input features and outputvalues. In this paper, we consider a feature-wise kernelized Lasso forcapturing non-linear input-output dependency. We first show that, withparticular choices of kernel functions, non-redundant features with strongstatistical dependence on output values can be found in terms of kernel-basedindependence measures. We then show that the globally optimal solution can beefficiently computed; this makes the approach scalable to high-dimensionalproblems. The effectiveness of the proposed method is demonstrated throughfeature selection experiments with thousands of features.
arxiv-3600-15 | An Investigation of the Sampling-Based Alignment Method and Its Contributions | http://arxiv.org/pdf/1308.4479v1.pdf | author:Juan Luo, Yves Lepage category:cs.CL published:2013-08-21 summary:By investigating the distribution of phrase pairs in phrase translationtables, the work in this paper describes an approach to increase the number ofn-gram alignments in phrase translation tables output by a sampling-basedalignment method. This approach consists in enforcing the alignment of n-gramsin distinct translation subtables so as to increase the number of n-grams.Standard normal distribution is used to allot alignment time among translationsubtables, which results in adjustment of the distribution of n- grams. Thisleads to better evaluation results on statistical machine translation tasksthan the original sampling-based alignment approach. Furthermore, thetranslation quality obtained by merging phrase translation tables computed fromthe sampling-based alignment method and from MGIZA++ is examined.
arxiv-3600-16 | Influences Combination of Multi-Sensor Images on Classification Accuracy | http://arxiv.org/pdf/1308.4440v1.pdf | author:AL-Wassai Firouz, N. V. Kalyankar category:cs.CV published:2013-08-20 summary:This paper focuses on two main issues; first one is the impact of combinationof multi-sensor images on the supervised learning classification accuracy usingsegment Fusion (SF). The second issue attempts to undertake the study ofsupervised machine learning classification technique of remote sensing imagesby using four classifiers like Parallelepiped (Pp), Mahalanobis Distance (MD),Maximum-Likelihood (ML) and Euclidean Distance(ED) classifiers, and theiraccuracies have been evaluated on their respected classification to choose thebest technique for classification of remote sensing images. QuickBirdmultispectral data (MS) and panchromatic data (PAN) have been used in thisstudy to demonstrate the enhancement and accuracy assessment of fused imageover the original images using ALwassaiProcess software. According toexperimental result of this study, is that the test results indicate thesupervised classification results of fusion image, which generated better thanthe MS did. As well as the result with Euclidean classifier is robust andprovides better results than the other classifiers do, despite of the popularbelief that the maximum-likelihood classifier is the most accurate classifier.
arxiv-3600-17 | SAR Image Despeckling Algorithms using Stochastic Distances and Nonlocal Means | http://arxiv.org/pdf/1308.4338v1.pdf | author:Leonardo Torres, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML published:2013-08-20 summary:This paper presents two approaches for filter design based on stochasticdistances for intensity speckle reduction. A window is defined around eachpixel, overlapping samples are compared and only those which pass agoodness-of-fit test are used to compute the filtered value. The tests stemfrom stochastic divergences within the Information Theory framework. Thetechnique is applied to intensity Synthetic Aperture Radar (SAR) data withhomogeneous regions using the Gamma model. The first approach uses aNagao-Matsuyama-type procedure for setting the overlapping samples, and thesecond uses the nonlocal method. The proposals are compared with the ImprovedSigma filter and with anisotropic diffusion for speckled data (SRAD) using aprotocol based on Monte Carlo simulation. Among the criteria used to quantifythe quality of filters, we employ the equivalent number of looks, and line andedge preservation. Moreover, we also assessed the filters by the UniversalImage Quality Index and by the Pearson correlation between edges. Applicationsto real images are also discussed. The proposed methods show good results.
arxiv-3600-18 | Support Recovery for the Drift Coefficient of High-Dimensional Diffusions | http://arxiv.org/pdf/1308.4077v2.pdf | author:Jose Bento, Morteza Ibrahimi category:cs.IT cs.LG math.IT math.PR math.ST stat.TH published:2013-08-19 summary:Consider the problem of learning the drift coefficient of a $p$-dimensionalstochastic differential equation from a sample path of length $T$. We assumethat the drift is parametrized by a high-dimensional vector, and study thesupport recovery problem when both $p$ and $T$ can tend to infinity. Inparticular, we prove a general lower bound on the sample-complexity $T$ byusing a characterization of mutual information as a time integral ofconditional variance, due to Kadota, Zakai, and Ziv. For linear stochasticdifferential equations, the drift coefficient is parametrized by a $p\times p$matrix which describes which degrees of freedom interact under the dynamics. Inthis case, we analyze a $\ell_1$-regularized least squares estimator and provean upper bound on $T$ that nearly matches the lower bound on specific classesof sparse matrices.
arxiv-3600-19 | Pylearn2: a machine learning research library | http://arxiv.org/pdf/1308.4214v1.pdf | author:Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Frédéric Bastien, Yoshua Bengio category:stat.ML cs.LG cs.MS published:2013-08-20 summary:Pylearn2 is a machine learning research library. This does not just mean thatit is a collection of machine learning algorithms that share a common API; itmeans that it has been designed for flexibility and extensibility in order tofacilitate research projects that involve new or unusual use cases. In thispaper we give a brief history of the library, an overview of its basicphilosophy, a summary of the library's architecture, and a description of howthe Pylearn2 community functions socially.
arxiv-3600-20 | Scalable Convex Methods for Flexible Low-Rank Matrix Modeling | http://arxiv.org/pdf/1308.4211v1.pdf | author:William Fithian, Rahul Mazumder category:stat.ML published:2013-08-20 summary:We propose a general framework for reduced-rank modeling of matrix-valueddata. By applying a generalized nuclear norm penalty we directly modellow-dimensional latent variables associated with rows and columns. Ourframework flexibly incorporates row and column features, smoothing kernels, andother sources of side information by penalizing deviations from the row andcolumn models. Under general con- ditions these models can be estimatedscalably using convex optimization. The computational bottleneck - one singularvalue decomposition per iteration of a large but easy-to-apply matrix - can bescaled to large problems by representing the matrix implicitly and using warmstarts [Mazumder and Hastie, 2013]. Our framework generalizes traditionalconvex matrix completion and multi-task learning methods as well as MAPestimation under a large class of popular hierarchical Bayesian models.
arxiv-3600-21 | Towards Adapting ImageNet to Reality: Scalable Domain Adaptation with Implicit Low-rank Transformations | http://arxiv.org/pdf/1308.4200v1.pdf | author:Erik Rodner, Judy Hoffman, Jeff Donahue, Trevor Darrell, Kate Saenko category:cs.CV cs.LG stat.ML published:2013-08-20 summary:Images seen during test time are often not from the same distribution asimages used for learning. This problem, known as domain shift, occurs whentraining classifiers from object-centric internet image databases and trying toapply them directly to scene understanding tasks. The consequence is oftensevere performance degradation and is one of the major barriers for theapplication of classifiers in real-world systems. In this paper, we show how tolearn transform-based domain adaptation classifiers in a scalable manner. Thekey idea is to exploit an implicit rank constraint, originated from amax-margin domain adaptation formulation, to make optimization tractable.Experiments show that the transformation between domains can be veryefficiently learned from data and easily applied to new categories. This beginsto bridge the gap between large-scale internet image collections and objectimages captured in everyday life environments.
arxiv-3600-22 | Predicting protein contact map using evolutionary and physical constraints by integer programming (extended version) | http://arxiv.org/pdf/1308.1975v2.pdf | author:Zhiyong Wang, Jinbo Xu category:q-bio.QM cs.CE cs.LG math.OC q-bio.BM stat.ML published:2013-08-08 summary:Motivation. Protein contact map describes the pairwise spatial and functionalrelationship of residues in a protein and contains key information for protein3D structure prediction. Although studied extensively, it remains verychallenging to predict contact map using only sequence information. Mostexisting methods predict the contact map matrix element-by-element, ignoringcorrelation among contacts and physical feasibility of the whole contact map. Acouple of recent methods predict contact map based upon residue co-evolution,taking into consideration contact correlation and enforcing a sparsityrestraint, but these methods require a very large number of sequence homologsfor the protein under consideration and the resultant contact map may be stillphysically unfavorable. Results. This paper presents a novel method PhyCMAP for contact mapprediction, integrating both evolutionary and physical restraints by machinelearning and integer linear programming (ILP). The evolutionary restraintsinclude sequence profile, residue co-evolution and context-specific statisticalpotential. The physical restraints specify more concrete relationship amongcontacts than the sparsity restraint. As such, our method greatly reduces thesolution space of the contact map matrix and thus, significantly improvesprediction accuracy. Experimental results confirm that PhyCMAP outperformscurrently popular methods no matter how many sequence homologs are availablefor the protein under consideration. PhyCMAP can predict contacts withinminutes after PSIBLAST search for sequence homologs is done, much faster thanthe two recent methods PSICOV and EvFold. See http://raptorx.uchicago.edu for the web server.
arxiv-3600-23 | A balanced k-means algorithm for weighted point sets | http://arxiv.org/pdf/1308.4004v1.pdf | author:Steffen Borgwardt, Andreas Brieden, Peter Gritzmann category:math.OC cs.LG stat.ML published:2013-08-19 summary:The classical k-means algorithm for paritioning n points in R^d into ksubsets is one of the most popular and widely spread clustering methods inscientific and business applications. The present paper gives a generalizationthat is capable of handling weighted point sets and prescribed lower and upperbounds on the cluster sizes. The new algorithm replaces the assignment step ofk-means by the computation of a weight-balanced least-squares assignment. Thisis modelled as a linear program over a weight-balanced partition polytope whoseoptimal vertices correspond to clusterings that allow strongly feasible powerdiagrams. We use this correspondence to derive a worst-case upper bound n^O(dk)for the number of operations. This is similar to the known upper bound fork-means, polynomial for fixed k and d, and in view of the known complexityresults for k-means, essentially the best one can expect. Further, we show thekernelizability of our approach.
arxiv-3600-24 | Clustering of Complex Networks and Community Detection Using Group Search Optimization | http://arxiv.org/pdf/1307.1372v2.pdf | author:G. Kishore Kumar, V. K. Jayaraman category:cs.NE cs.DS published:2013-07-04 summary:Group Search Optimizer(GSO) is one of the best algorithms, is very new in thefield of Evolutionary Computing. It is very robust and efficient algorithm,which is inspired by animal searching behaviour. The paper describes anapplication of GSO to clustering of networks. We have tested GSO against fivestandard benchmark datasets, GSO algorithm is proved very competitive in termsof accuracy and convergence speed.
arxiv-3600-25 | Optimal Algorithms for Testing Closeness of Discrete Distributions | http://arxiv.org/pdf/1308.3946v1.pdf | author:Siu-On Chan, Ilias Diakonikolas, Gregory Valiant, Paul Valiant category:cs.DS cs.IT cs.LG math.IT published:2013-08-19 summary:We study the question of closeness testing for two discrete distributions.More precisely, given samples from two distributions $p$ and $q$ over an$n$-element set, we wish to distinguish whether $p=q$ versus $p$ is at least$\eps$-far from $q$, in either $\ell_1$ or $\ell_2$ distance. Batu et al. gavethe first sub-linear time algorithms for these problems, which matched thelower bounds of Valiant up to a logarithmic factor in $n$, and a polynomialfactor of $\eps.$ In this work, we present simple (and new) testers for both the $\ell_1$ and$\ell_2$ settings, with sample complexity that is information-theoreticallyoptimal, to constant factors, both in the dependence on $n$, and the dependenceon $\eps$; for the $\ell_1$ testing problem we establish that the samplecomplexity is $\Theta(\max\{n^{2/3}/\eps^{4/3}, n^{1/2}/\eps^2 \}).$
arxiv-3600-26 | A Likelihood Ratio Approach for Probabilistic Inequalities | http://arxiv.org/pdf/1308.4123v1.pdf | author:Xinjia Chen category:math.PR cs.LG math.ST stat.TH published:2013-08-18 summary:We propose a new approach for deriving probabilistic inequalities based onbounding likelihood ratios. We demonstrate that this approach is more generaland powerful than the classical method frequently used for derivingconcentration inequalities such as Chernoff bounds. We discover that theproposed approach is inherently related to statistical concepts such asmonotone likelihood ratio, maximum likelihood, and the method of moments forparameter estimation. A connection between the proposed approach and the largedeviation theory is also established. We show that, without using momentgenerating functions, tightest possible concentration inequalities may bereadily derived by the proposed approach. We have derived new concentrationinequalities using the proposed approach, which cannot be obtained by theclassical approach based on moment generating functions.
arxiv-3600-27 | Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary Independent Stochastic Neurons | http://arxiv.org/pdf/1306.2801v4.pdf | author:Kyunghyun Cho category:cs.NE cs.LG stat.ML published:2013-06-12 summary:In this paper, a simple, general method of adding auxiliary stochasticneurons to a multi-layer perceptron is proposed. It is shown that the proposedmethod is a generalization of recently successful methods of dropout (Hinton etal., 2012), explicit noise injection (Vincent et al., 2010; Bishop, 1995) andsemantic hashing (Salakhutdinov & Hinton, 2009). Under the proposed framework,an extension of dropout which allows using separate dropping probabilities fordifferent hidden neurons, or layers, is found to be available. The use ofdifferent dropping probabilities for hidden layers separately is empiricallyinvestigated.
arxiv-3600-28 | KL-based Control of the Learning Schedule for Surrogate Black-Box Optimization | http://arxiv.org/pdf/1308.2655v2.pdf | author:Ilya Loshchilov, Marc Schoenauer, Michèle Sebag category:cs.LG cs.AI stat.ML published:2013-08-12 summary:This paper investigates the control of an ML component within the CovarianceMatrix Adaptation Evolution Strategy (CMA-ES) devoted to black-boxoptimization. The known CMA-ES weakness is its sample complexity, the number ofevaluations of the objective function needed to approximate the global optimum.This weakness is commonly addressed through surrogate optimization, learning anestimate of the objective function a.k.a. surrogate model, and replacing mostevaluations of the true objective function with the (inexpensive) evaluation ofthe surrogate model. This paper presents a principled control of the learningschedule (when to relearn the surrogate model), based on the Kullback-Leiblerdivergence of the current search distribution and the training distribution ofthe former surrogate model. The experimental validation of the proposedapproach shows significant performance gains on a comprehensive set ofill-conditioned benchmark problems, compared to the best state of the artincluding the quasi-Newton high-precision BFGS method.
arxiv-3600-29 | Natural Language Web Interface for Database (NLWIDB) | http://arxiv.org/pdf/1308.3830v1.pdf | author:Rukshan Alexander, Prashanthi Rukshan, Sinnathamby Mahesan category:cs.CL cs.DB cs.HC published:2013-08-18 summary:It is a long term desire of the computer users to minimize the communicationgap between the computer and a human. On the other hand, almost all ICTapplications store information in to databases and retrieve from them.Retrieving information from the database requires knowledge of technicallanguages such as Structured Query Language. However majority of the computerusers who interact with the databases do not have a technical background andare intimidated by the idea of using languages such as SQL. For above reasons,a Natural Language Web Interface for Database (NLWIDB) has been developed. TheNLWIDB allows the user to query the database in a language more like English,through a convenient interface over the Internet.
arxiv-3600-30 | Reference Distance Estimator | http://arxiv.org/pdf/1308.3818v1.pdf | author:Yanpeng Li category:cs.LG stat.ML published:2013-08-18 summary:A theoretical study is presented for a simple linear classifier calledreference distance estimator (RDE), which assigns the weight of each feature jas P(rj)-P(r), where r is a reference feature relevant to the target class y.The analysis shows that if r performs better than random guess in predicting yand is conditionally independent with each feature j, the RDE will have thesame classification performance as that from P(yj)-P(y), a classifier trainedwith the gold standard y. Since the estimation of P(rj)-P(r) does not requirelabeled data, under the assumption above, RDE trained with a large number ofunlabeled examples would be close to that trained with infinite labeledexamples. For the case the assumption does not hold, we theoretically analyzethe factors that influence the closeness of the RDE to the perfect one underthe assumption, and present an algorithm to select reference features andcombine multiple RDEs from different reference features using both labeled andunlabeled data. The experimental results on 10 text classification tasks showthat the semi-supervised learning method improves supervised methods using5,000 labeled examples and 13 million unlabeled ones, and in many tasks, itsperformance is even close to a classifier trained with 13 million labeledexamples. In addition, the bounds in the theorems provide good estimation ofthe classification performance and can be useful for new algorithm design.
arxiv-3600-31 | Implementation Of Back-Propagation Neural Network For Isolated Bangla Speech Recognition | http://arxiv.org/pdf/1308.3785v1.pdf | author:Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan category:cs.CL cs.NE published:2013-08-17 summary:This paper is concerned with the development of Back-propagation NeuralNetwork for Bangla Speech Recognition. In this paper, ten bangla digits wererecorded from ten speakers and have been recognized. The features of thesespeech digits were extracted by the method of Mel Frequency CepstralCoefficient (MFCC) analysis. The mfcc features of five speakers were used totrain the network with Back propagation algorithm. The mfcc features of tenbangla digit speeches, from 0 to 9, of another five speakers were used to testthe system. All the methods and algorithms used in this research wereimplemented using the features of Turbo C and C++ languages. From ourinvestigation it is seen that the developed system can successfully encode andanalyze the mfcc features of the speech signal to recognition. The developedsystem achieved recognition rate about 96.332% for known speakers (i.e.,speaker dependent) and 92% for unknown speakers (i.e., speaker independent).
arxiv-3600-32 | Graph Colouring Problem Based on Discrete Imperialist Competitive Algorithm | http://arxiv.org/pdf/1308.3784v1.pdf | author:Hojjat Emami, Shahriar Lotfi category:cs.AI cs.NE published:2013-08-17 summary:In graph theory, Graph Colouring Problem (GCP) is an assignment of colours tovertices of any given graph such that the colours on adjacent vertices aredifferent. The GCP is known to be an optimization and NP-hard problem.Imperialist Competitive Algorithm (ICA) is a meta-heuristic optimization andstochastic search strategy which is inspired from socio-political phenomenon ofimperialistic competition. The ICA contains two main operators: theassimilation and the imperialistic competition. The ICA has excellentcapabilities such as high convergence rate and better global optimumachievement. In this research, a discrete version of ICA is proposed to dealwith the solution of GCP. We call this algorithm as the DICA. The performanceof the proposed method is compared with Genetic Algorithm (GA) on sevenwell-known graph colouring benchmarks. Experimental results demonstrate thesuperiority of the DICA for the benchmarks. This means DICA can produce optimaland valid solutions for different GCP instances.
arxiv-3600-33 | Comment on "robustness and regularization of support vector machines" by H. Xu, et al., (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009, arXiv:0803.3490) | http://arxiv.org/pdf/1308.3750v1.pdf | author:Yahya Forghani, Hadi Sadoghi Yazdi category:cs.LG published:2013-08-17 summary:This paper comments on the published work dealing with robustness andregularization of support vector machines (Journal of Machine LearningResearch, vol. 10, pp. 1485-1510, 2009) [arXiv:0803.3490] by H. Xu, etc. Theyproposed a theorem to show that it is possible to relate robustness in thefeature space and robustness in the sample space directly. In this paper, wepropose a counter example that rejects their theorem.
arxiv-3600-34 | Development of Comprehensive Devnagari Numeral and Character Database for Offline Handwritten Character Recognition | http://arxiv.org/pdf/1309.5357v1.pdf | author:Vikas J. Dongre, Vijay H. Mankar category:cs.CV published:2013-08-17 summary:In handwritten character recognition, benchmark database plays an importantrole in evaluating the performance of various algorithms and the resultsobtained by various researchers. In Devnagari script, there is lack of suchofficial benchmark. This paper focuses on the generation of offline benchmarkdatabase for Devnagari handwritten numerals and characters. The present workgenerated 5137 and 20305 isolated samples for numeral and character database,respectively, from 750 writers of all ages, sex, education, and profession. Theoffline sample images are stored in TIFF image format as it occupies lessmemory. Also, the data is presented in binary level so that memory requirementis further reduced. It will facilitate research on handwriting recognition ofDevnagari script through free access to the researchers.
arxiv-3600-35 | A History of Cluster Analysis Using the Classification Society's Bibliography Over Four Decades | http://arxiv.org/pdf/1209.0125v2.pdf | author:Fionn Murtagh, Michael J. Kurtz category:cs.DL cs.LG stat.ML 62H30 I.5.3; H.3.3 published:2012-09-01 summary:The Classification Literature Automated Search Service, an annualbibliography based on citation of one or more of a set of around 80 book orjournal publications, ran from 1972 to 2012. We analyze here the years 1994 to2011. The Classification Society's Service, as it was termed, has been producedby the Classification Society. In earlier decades it was distributed as adiskette or CD with the Journal of Classification. Among our findings are thefollowing: an enormous increase in scholarly production post approximately2000; a very major increase in quantity, coupled with work in differentdisciplines, from approximately 2004; and a major shift also from clusteranalysis in earlier times having mathematics and psychology as disciplines ofthe journals published in, and affiliations of authors, contrasted with, inmore recent times, a "centre of gravity" in management and engineering.
arxiv-3600-36 | Standardizing Interestingness Measures for Association Rules | http://arxiv.org/pdf/1308.3740v1.pdf | author:Mateen Shaikh, Paul D. McNicholas, M. Luiza Antonie, T. Brendan Murphy category:stat.AP cs.LG stat.ML published:2013-08-16 summary:Interestingness measures provide information that can be used to prune orselect association rules. A given value of an interestingness measure is ofteninterpreted relative to the overall range of the values that theinterestingness measure can take. However, properties of individual associationrules restrict the values an interestingness measure can achieve. Aninteresting measure can be standardized to take this into account, but this hasonly been done for one interestingness measure to date, i.e., the lift.Standardization provides greater insight than the raw value and may even alterresearchers' perception of the data. We derive standardized analogues of threeinterestingness measures and use real and simulated data to compare them totheir raw versions, each other, and the standardized lift.
arxiv-3600-37 | The Royal Birth of 2013: Analysing and Visualising Public Sentiment in the UK Using Twitter | http://arxiv.org/pdf/1308.1847v2.pdf | author:Vu Dung Nguyen, Blesson Varghese, Adam Barker category:cs.CL cs.IR cs.SI physics.soc-ph published:2013-08-08 summary:Analysis of information retrieved from microblogging services such as Twittercan provide valuable insight into public sentiment in a geographic region. Thisinsight can be enriched by visualising information in its geographic context.Two underlying approaches for sentiment analysis are dictionary-based andmachine learning. The former is popular for public sentiment analysis, and thelatter has found limited use for aggregating public sentiment from Twitterdata. The research presented in this paper aims to extend the machine learningapproach for aggregating public sentiment. To this end, a framework foranalysing and visualising public sentiment from a Twitter corpus is developed.A dictionary-based approach and a machine learning approach are implementedwithin the framework and compared using one UK case study, namely the royalbirth of 2013. The case study validates the feasibility of the framework foranalysis and rapid visualisation. One observation is that there is goodcorrelation between the results produced by the popular dictionary-basedapproach and the machine learning approach when large volumes of tweets areanalysed. However, for rapid analysis to be possible faster methods need to bedeveloped using big data techniques and parallel methods.
arxiv-3600-38 | Fast Stochastic Alternating Direction Method of Multipliers | http://arxiv.org/pdf/1308.3558v1.pdf | author:Leon Wenliang Zhong, James T. Kwok category:cs.LG cs.NA published:2013-08-16 summary:In this paper, we propose a new stochastic alternating direction method ofmultipliers (ADMM) algorithm, which incrementally approximates the fullgradient in the linearized ADMM formulation. Besides having a low per-iterationcomplexity as existing stochastic ADMM algorithms, the proposed algorithmimproves the convergence rate on convex problems from $O(\frac 1 {\sqrt{T}})$to $O(\frac 1 T)$, where $T$ is the number of iterations. This matches theconvergence rate of the batch ADMM algorithm, but without the need to visit allthe samples in each iteration. Experiments on the graph-guided fused lassodemonstrate that the new algorithm is significantly faster thanstate-of-the-art stochastic and batch ADMM algorithms.
arxiv-3600-39 | Genetic Algorithm for Solving Simple Mathematical Equality Problem | http://arxiv.org/pdf/1308.4675v1.pdf | author:Denny Hermawanto category:cs.NE published:2013-08-16 summary:This paper explains genetic algorithm for novice in this field. Basicphilosophy of genetic algorithm and its flowchart are described. Step by stepnumerical computation of genetic algorithm for solving simple mathematicalequality problem will be briefly explained
arxiv-3600-40 | Innovative Second-Generation Wavelets Construction With Recurrent Neural Networks for Solar Radiation Forecasting | http://arxiv.org/pdf/1308.3524v1.pdf | author:Giacomo Capizzi, Christian Napoli, Francesco Bonanno category:cs.NE published:2013-08-15 summary:Solar radiation prediction is an important challenge for the electricalengineer because it is used to estimate the power developed by commercialphotovoltaic modules. This paper deals with the problem of solar radiationprediction based on observed meteorological data. A 2-day forecast is obtainedby using novel wavelet recurrent neural networks (WRNNs). In fact, these WRNNSare used to exploit the correlation between solar radiation andtimescale-related variations of wind speed, humidity, and temperature. Theinput to the selected WRNN is provided by timescale-related bands of waveletcoefficients obtained from meteorological time series. The experimental setupavailable at the University of Catania, Italy, provided this information. Thenovelty of this approach is that the proposed WRNN performs the prediction inthe wavelet domain and, in addition, also performs the inverse wavelettransform, giving the predicted signal as output. The obtained simulationresults show a very low root-mean-square error compared to the results of thesolar radiation prediction approaches obtained by hybrid neural networksreported in the recent literature.
arxiv-3600-41 | Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations | http://arxiv.org/pdf/1308.3513v1.pdf | author:Finale Doshi-Velez, George Konidaris category:cs.LG cs.AI published:2013-08-15 summary:Control applications often feature tasks with similar, but not identical,dynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP),a framework that parametrizes a family of related dynamical systems with alow-dimensional set of latent factors, and introduce a semiparametricregression approach for learning its structure from data. In the controlsetting, we show that a learned HiP-MDP rapidly identifies the dynamics of anew task instance, allowing an agent to flexibly adapt to task variations.
arxiv-3600-42 | Stochastic Optimization for Machine Learning | http://arxiv.org/pdf/1308.3509v1.pdf | author:Andrew Cotter category:cs.LG published:2013-08-15 summary:It has been found that stochastic algorithms often find good solutions muchmore rapidly than inherently-batch approaches. Indeed, a very useful rule ofthumb is that often, when solving a machine learning problem, an iterativetechnique which relies on performing a very large number ofrelatively-inexpensive updates will often outperform one which performs asmaller number of much "smarter" but computationally-expensive updates. In this thesis, we will consider the application of stochastic algorithms totwo of the most important machine learning problems. Part i is concerned withthe supervised problem of binary classification using kernelized linearclassifiers, for which the data have labels belonging to exactly two classes(e.g. "has cancer" or "doesn't have cancer"), and the learning problem is tofind a linear classifier which is best at predicting the label. In Part ii, wewill consider the unsupervised problem of Principal Component Analysis, forwhich the learning task is to find the directions which contain most of thevariance of the data distribution. Our goal is to present stochastic algorithms for both problems which are,above all, practical--they work well on real-world data, in some cases betterthan all known competing algorithms. A secondary, but still very important,goal is to derive theoretical bounds on the performance of these algorithmswhich are at least competitive with, and often better than, those known forother approaches.
arxiv-3600-43 | Computational Rationalization: The Inverse Equilibrium Problem | http://arxiv.org/pdf/1308.3506v1.pdf | author:Kevin Waugh, Brian D. Ziebart, J. Andrew Bagnell category:cs.GT cs.LG stat.ML published:2013-08-15 summary:Modeling the purposeful behavior of imperfect agents from a small number ofobservations is a challenging task. When restricted to the single-agentdecision-theoretic setting, inverse optimal control techniques assume thatobserved behavior is an approximately optimal solution to an unknown decisionproblem. These techniques learn a utility function that explains the examplebehavior and can then be used to accurately predict or imitate future behaviorin similar observed or unobserved situations. In this work, we consider similar tasks in competitive and cooperativemulti-agent domains. Here, unlike single-agent settings, a player cannotmyopically maximize its reward; it must speculate on how the other agents mayact to influence the game's outcome. Employing the game-theoretic notion ofregret and the principle of maximum entropy, we introduce a technique forpredicting and generalizing behavior.
arxiv-3600-44 | Using Incomplete Information for Complete Weight Annotation of Road Networks -- Extended Version | http://arxiv.org/pdf/1308.0484v2.pdf | author:Bin Yang, Manohar Kaul, Christian S. Jensen category:cs.LG cs.DB published:2013-08-02 summary:We are witnessing increasing interests in the effective use of road networks.For example, to enable effective vehicle routing, weighted-graph models oftransportation networks are used, where the weight of an edge captures somecost associated with traversing the edge, e.g., greenhouse gas (GHG) emissionsor travel time. It is a precondition to using a graph model for routing thatall edges have weights. Weights that capture travel times and GHG emissions canbe extracted from GPS trajectory data collected from the network. However, GPStrajectory data typically lack the coverage needed to assign weights to alledges. This paper formulates and addresses the problem of annotating all edgesin a road network with travel cost based weights from a set of trips in thenetwork that cover only a small fraction of the edges, each with an associatedground-truth travel cost. A general framework is proposed to solve the problem.Specifically, the problem is modeled as a regression problem and solved byminimizing a judiciously designed objective function that takes into accountthe topology of the road network. In particular, the use of weighted PageRankvalues of edges is explored for assigning appropriate weights to all edges, andthe property of directional adjacency of edges is also taken into account toassign weights. Empirical studies with weights capturing travel time and GHGemissions on two road networks (Skagen, Denmark, and North Jutland, Denmark)offer insight into the design properties of the proposed techniques and offerevidence that the techniques are effective.
arxiv-3600-45 | Quantum Entanglement in Concept Combinations | http://arxiv.org/pdf/1302.3831v2.pdf | author:Diederik Aerts, Sandro Sozzo category:cs.AI cs.CL quant-ph published:2013-02-15 summary:Research in the application of quantum structures to cognitive scienceconfirms that these structures quite systematically appear in the dynamics ofconcepts and their combinations and quantum-based models faithfully representexperimental data of situations where classical approaches are problematical.In this paper, we analyze the data we collected in an experiment on a specificconceptual combination, showing that Bell's inequalities are violated in theexperiment. We present a new refined entanglement scheme to model these datawithin standard quantum theory rules, where 'entangled measurements andentangled evolutions' occur, in addition to the expected 'entangled states',and present a full quantum representation in complex Hilbert space of the data.This stronger form of entanglement in measurements and evolutions might haverelevant applications in the foundations of quantum theory, as well as in theinterpretation of nonlocality tests. It could indeed explain somenon-negligible 'anomalies' identified in EPR-Bell experiments.
arxiv-3600-46 | Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation | http://arxiv.org/pdf/1308.3432v1.pdf | author:Yoshua Bengio, Nicholas Léonard, Aaron Courville category:cs.LG published:2013-08-15 summary:Stochastic neurons and hard non-linearities can be useful for a number ofreasons in deep learning models, but in many cases they pose a challengingproblem: how to estimate the gradient of a loss function with respect to theinput of such stochastic or non-smooth neurons? I.e., can we "back-propagate"through these stochastic neurons? We examine this question, existingapproaches, and compare four families of solutions, applicable in differentsettings. One of them is the minimum variance unbiased gradient estimator forstochatic binary neurons (a special case of the REINFORCE algorithm). A secondapproach, introduced here, decomposes the operation of a binary stochasticneuron into a stochastic binary part and a smooth differentiable part, whichapproximates the expected effect of the pure stochatic binary neuron to firstorder. A third approach involves the injection of additive or multiplicativenoise in a computational graph that is otherwise differentiable. A fourthapproach heuristically copies the gradient with respect to the stochasticoutput directly as an estimator of the gradient with respect to the sigmoidargument (we call this the straight-through estimator). To explore a contextwhere these estimators are useful, we consider a small-scale version of {\emconditional computation}, where sparse stochastic units form a distributedrepresentation of gaters that can turn off in combinatorially many ways largechunks of the computation performed in the rest of the neural network. In thiscase, it is important that the gating units produce an actual 0 most of thetime. The resulting sparsity can be potentially be exploited to greatly reducethe computational cost of large deep networks for which conditional computationwould be useful.
arxiv-3600-47 | Learning ambiguous functions by neural networks | http://arxiv.org/pdf/1310.1250v1.pdf | author:Rui Ligeiro, R. Vilela Mendes category:cs.NE cs.LG 68T37, 82C32 published:2013-08-15 summary:It is not, in general, possible to have access to all variables thatdetermine the behavior of a system. Having identified a number of variableswhose values can be accessed, there may still be hidden variables whichinfluence the dynamics of the system. The result is model ambiguity in thesense that, for the same (or very similar) input values, different objectiveoutputs should have been obtained. In addition, the degree of ambiguity mayvary widely across the whole range of input values. Thus, to evaluate theaccuracy of a model it is of utmost importance to create a method to obtain thedegree of reliability of each output result. In this paper we present such ascheme composed of two coupled artificial neural networks: the first one beingresponsible for outputting the predicted value, whereas the other evaluates thereliability of the output, which is learned from the error values of the firstone. As an illustration, the scheme is applied to a model for tracking slopesin a straw chamber and to a credit scoring model.
arxiv-3600-48 | The algorithm of noisy k-means | http://arxiv.org/pdf/1308.3314v1.pdf | author:Camille Brunet, Sébastien Loustau category:stat.ML cs.LG published:2013-08-15 summary:In this note, we introduce a new algorithm to deal with finite dimensionalclustering with errors in variables. The design of this algorithm is based onrecent theoretical advances (see Loustau (2013a,b)) in statistical learningwith errors in variables. As the previous mentioned papers, the algorithm mixesdifferent tools from the inverse problem literature and the machine learningcommunity. Coarsely, it is based on a two-step procedure: (1) a deconvolutionstep to deal with noisy inputs and (2) Newton's iterations as the populark-means.
arxiv-3600-49 | Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery | http://arxiv.org/pdf/1307.5870v2.pdf | author:Cun Mu, Bo Huang, John Wright, Donald Goldfarb category:stat.ML cs.LG published:2013-07-22 summary:Recovering a low-rank tensor from incomplete information is a recurringproblem in signal processing and machine learning. The most popular convexrelaxation of this problem minimizes the sum of the nuclear norms of theunfoldings of the tensor. We show that this approach can be substantiallysuboptimal: reliably recovering a $K$-way tensor of length $n$ and Tucker rank$r$ from Gaussian measurements requires $\Omega(r n^{K-1})$ observations. Incontrast, a certain (intractable) nonconvex formulation needs only $O(r^K +nrK)$ observations. We introduce a very simple, new convex relaxation, whichpartially bridges this gap. Our new formulation succeeds with $O(r^{\lfloor K/2\rfloor}n^{\lceil K/2 \rceil})$ observations. While these results pertain toGaussian measurements, simulations strongly suggest that the new norm alsooutperforms the sum of nuclear norms for tensor completion from a random subsetof entries. Our lower bound for the sum-of-nuclear-norms model follows from a new resulton recovering signals with multiple sparse structures (e.g. sparse, low rank),which perhaps surprisingly demonstrates the significant suboptimality of thecommonly used recovery approach via minimizing the sum of individual sparsityinducing norms (e.g. $l_1$, nuclear norm). Our new formulation for low-ranktensor recovery however opens the possibility in reducing the sample complexityby exploiting several structures jointly.
arxiv-3600-50 | A Secure and Comparable Text Encryption Algorithm | http://arxiv.org/pdf/1308.3294v1.pdf | author:Nicholas Kersting category:cs.CR cs.CL cs.CY cs.SI published:2013-08-15 summary:This paper discloses a simple algorithm for encrypting text messages, basedon the NP-completeness of the subset sum problem, such that the similaritybetween encryptions is roughly proportional to the semantic similarity betweentheir generating messages. This allows parties to compare encrypted messagesfor semantic overlap without trusting an intermediary and might be applied, forexample, as a means of finding scientific collaborators over the Internet.
arxiv-3600-51 | Impulse Noise Removal In Speech Using Wavelets | http://arxiv.org/pdf/1310.7447v1.pdf | author:R. C. Nongpiur category:cs.CV published:2013-08-14 summary:A new method for removing impulse noise from speech in the wavelet transformdomain is proposed. The method utilizes the multiresolution property of thewavelet transform, which provides finer time resolution at the higherfrequencies than the short-time Fourier transform (STFT), to effectivelyidentify and remove impulse noise. It uses two features of speech todiscriminate speech from impulse noise: one is the slow time-varying nature ofspeech and the other is the Lipschitz regularity of the speech components. Onthe basis of these features, an algorithm has been developed to identify andsuppress wavelet coefficients that correspond to impulse noise. Experimentresults show that the new method is able to significantly reduce impulse noisewithout degrading the quality of the speech signal or introducing any audibleartifacts.
arxiv-3600-52 | Equitability Analysis of the Maximal Information Coefficient, with Comparisons | http://arxiv.org/pdf/1301.6314v2.pdf | author:David Reshef, Yakir Reshef, Michael Mitzenmacher, Pardis Sabeti category:cs.LG q-bio.QM stat.ML published:2013-01-27 summary:A measure of dependence is said to be equitable if it gives similar scores toequally noisy relationships of different types. Equitability is important indata exploration when the goal is to identify a relatively small set ofstrongest associations within a dataset as opposed to finding as many non-zeroassociations as possible, which often are too many to sift through. Thus anequitable statistic, such as the maximal information coefficient (MIC), can beuseful for analyzing high-dimensional data sets. Here, we explore bothequitability and the properties of MIC, and discuss several aspects of thetheory and practice of MIC. We begin by presenting an intuition behind theequitability of MIC through the exploration of the maximization andnormalization steps in its definition. We then examine the speed and optimalityof the approximation algorithm used to compute MIC, and suggest some directionsfor improving both. Finally, we demonstrate in a range of noise models andsample sizes that MIC is more equitable than natural alternatives, such asmutual information estimation and distance correlation.
arxiv-3600-53 | Arabic Text Recognition in Video Sequences | http://arxiv.org/pdf/1308.3243v1.pdf | author:M. Ben Halima, H. Karray, A. M. Alimi category:cs.MM cs.CL cs.CV published:2013-08-14 summary:In this paper, we propose a robust approach for text extraction andrecognition from Arabic news video sequence. The text included in videosequences is an important needful for indexing and searching system. However,this text is difficult to detect and recognize because of the variability ofits size, their low resolution characters and the complexity of thebackgrounds. To solve these problems, we propose a system performing in twomain tasks: extraction and recognition of text. Our system is tested on avaried database composed of different Arabic news programs and the obtainedresults are encouraging and show the merits of our approach.
arxiv-3600-54 | An interactive engine for multilingual video browsing using semantic content | http://arxiv.org/pdf/1308.3225v1.pdf | author:M. Ben Halima, M. Hamroun, S. Ben Moussa, A. M. Alimi category:cs.MM cs.CV cs.IR published:2013-08-14 summary:The amount of audio-visual information has increased dramatically with theadvent of High Speed Internet. Furthermore, technological advances in recentyears in the field of information technology, have simplified the use of videodata in various fields by the general public. This made it possible to storelarge collections of video documents into computer systems. To enable efficientuse of these collections, it is necessary to develop tools to facilitate accessto these documents and handling them. In this paper we propose a method forindexing and retrieval of video sequences in a video database of largedimension, based on a weighting technique to calculate the degree of membershipof a concept in a video also a structuring of the data of the audio-visual(context / concept / video) and a relevance feedback mechanism.
arxiv-3600-55 | Guiding Designs of Self-Organizing Swarms: Interactive and Automated Approaches | http://arxiv.org/pdf/1308.3400v1.pdf | author:Hiroki Sayama category:cs.NE nlin.AO published:2013-08-14 summary:Self-organization of heterogeneous particle swarms is rich in its dynamicsbut hard to design in a traditional top-down manner, especially when many typesof kinetically distinct particles are involved. In this chapter, we discuss howwe have been addressing this problem by (1) utilizing and enhancing interactiveevolutionary design methods and (2) realizing spontaneous evolution of selforganizing swarms within an artificial ecosystem.
arxiv-3600-56 | Normalized Google Distance of Multisets with Applications | http://arxiv.org/pdf/1308.3177v1.pdf | author:Andrew R. Cohen, P. M. B. Vitanyi category:cs.IR cs.LG published:2013-08-14 summary:Normalized Google distance (NGD) is a relative semantic distance based on theWorld Wide Web (or any other large electronic database, for instance Wikipedia)and a search engine that returns aggregate page counts. The earlier NGD betweenpairs of search terms (including phrases) is not sufficient for allapplications. We propose an NGD of finite multisets of search terms that isbetter for many applications. This gives a relative semantics shared by amultiset of search terms. We give applications and compare the results withthose obtained using the pairwise NGD. The derivation of NGD method is based onKolmogorov complexity.
arxiv-3600-57 | Locally epistatic genomic relationship matrices for genomic association, prediction and selection | http://arxiv.org/pdf/1302.3463v6.pdf | author:Deniz Akdemir category:stat.AP stat.ML published:2013-02-14 summary:As the amount and complexity of genetic information increases it is necessarythat we explore some efficient ways of handling these data. This study takesthe "divide and conquer" approach for analyzing high dimensional genomic data.Our aims include reducing the dimensionality of the problem that has to bedealt one at a time, improving the performance and interpretability of themodels. We propose using the inherent structures in the genome; to divide thebigger problem into manageable parts. In plant and animal breeding studies adistinction is made between the commercial value (additive + epistatic geneticeffects) and the breeding value (additive genetic effects) of an individualsince it is expected that some of the epistatic genetic effects will be lostdue to recombination. In this paper, we argue that the breeder can takeadvantage of some of the epistatic marker effects in regions of lowrecombination. The models introduced here aim to estimate local epistatic lineheritability by using the genetic map information and combine the localadditive and epistatic effects. To this end, we have used semi-parametric mixedmodels with multiple local genomic relationship matrices with hierarchicaltesting designs and lasso post-processing for sparsity in the final model andspeed. Our models produce good predictive performance along with geneticassociation information.
arxiv-3600-58 | System and Methods for Converting Speech to SQL | http://arxiv.org/pdf/1308.3106v1.pdf | author:Sachin Kumar, Ashish Kumar, Pinaki Mitra, Girish Sundaram category:cs.CL cs.DB published:2013-08-14 summary:This paper concerns with the conversion of a Spoken English Language Queryinto SQL for retrieving data from RDBMS. A User submits a query as speechsignal through the user interface and gets the result of the query in the textformat. We have developed the acoustic and language models using which a speechutterance can be converted into English text query and thus natural languageprocessing techniques can be applied on this English text query to generate anequivalent SQL query. For conversion of speech into English text HTK and Juliustools have been used and for conversion of English text query into SQL query wehave implemented a System which uses rule based translation to translateEnglish Language Query into SQL Query. The translation uses lexical analyzer,parser and syntax directed translation techniques like in compilers. JFLex andBYACC tools have been used to build lexical analyzer and parser respectively.System is domain independent i.e. system can run on different database as itgenerates lex files from the underlying database.
arxiv-3600-59 | Compact Relaxations for MAP Inference in Pairwise MRFs with Piecewise Linear Priors | http://arxiv.org/pdf/1308.3101v1.pdf | author:Christopher Zach, Christian Haene category:cs.CV cs.LG stat.ML published:2013-08-14 summary:Label assignment problems with large state spaces are important tasksespecially in computer vision. Often the pairwise interaction (or smoothnessprior) between labels assigned at adjacent nodes (or pixels) can be describedas a function of the label difference. Exact inference in such labeling tasksis still difficult, and therefore approximate inference methods based on alinear programming (LP) relaxation are commonly used in practice. In this workwe study how compact linear programs can be constructed for general piecwiselinear smoothness priors. The number of unknowns is O(LK) per pairwise cliquein terms of the state space size $L$ and the number of linear segments K. Thiscompares to an O(L^2) size complexity of the standard LP relaxation if thepiecewise linear structure is ignored. Our compact construction and thestandard LP relaxation are equivalent and lead to the same (approximate) labelassignment.
arxiv-3600-60 | When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity | http://arxiv.org/pdf/1308.2853v1.pdf | author:Animashree Anandkumar, Daniel Hsu, Majid Janzamin, Sham Kakade category:cs.LG cs.IR math.NA math.ST stat.ML stat.TH published:2013-08-13 summary:Overcomplete latent representations have been very popular for unsupervisedfeature learning in recent years. In this paper, we specify which overcompletemodels can be identified given observable moments of a certain order. Weconsider probabilistic admixture or topic models in the overcomplete regime,where the number of latent topics can greatly exceed the size of the observedword vocabulary. While general overcomplete topic models are not identifiable,we establish generic identifiability under a constraint, referred to as topicpersistence. Our sufficient conditions for identifiability involve a novel setof "higher order" expansion conditions on the topic-word matrix or thepopulation structure of the model. This set of higher-order expansionconditions allow for overcomplete models, and require the existence of aperfect matching from latent topics to higher order observed words. Weestablish that random structured topic models are identifiable w.h.p. in theovercomplete regime. Our identifiability results allows for general(non-degenerate) distributions for modeling the topic proportions, and thus, wecan handle arbitrarily correlated topics in our framework. Our identifiabilityresults imply uniqueness of a class of tensor decompositions with structuredsparsity which is contained in the class of Tucker decompositions, but is moregeneral than the Candecomp/Parafac (CP) decomposition.
arxiv-3600-61 | Universally consistent vertex classification for latent positions graphs | http://arxiv.org/pdf/1212.1182v3.pdf | author:Minh Tang, Daniel L. Sussman, Carey E. Priebe category:stat.ML math.ST stat.TH published:2012-12-05 summary:In this work we show that, using the eigen-decomposition of the adjacencymatrix, we can consistently estimate feature maps for latent position graphswith positive definite link function $\kappa$, provided that the latentpositions are i.i.d. from some distribution F. We then consider theexploitation task of vertex classification where the link function $\kappa$belongs to the class of universal kernels and class labels are observed for anumber of vertices tending to infinity and that the remaining vertices are tobe classified. We show that minimization of the empirical $\varphi$-risk forsome convex surrogate $\varphi$ of 0-1 loss over a class of linear classifierswith increasing complexities yields a universally consistent classifier, thatis, a classification rule with error converging to Bayes optimal for anydistribution F.
arxiv-3600-62 | An efficient ant based qos aware intelligent temporally ordered routing algorithm for manets | http://arxiv.org/pdf/1308.2762v1.pdf | author:Debajit Sensarma, Koushik Majumder category:cs.NI cs.NE published:2013-08-13 summary:A Mobile Ad hoc network (MANET) is a self configurable network connected bywireless links. This type of network is only suitable for temporarycommunication links as it is infrastructure-less and there is no centralisedcontrol. Providing QoS aware routing is a challenging task in this type ofnetwork due to dynamic topology and limited resources. The main purpose of QoSaware routing is to find a feasible path from source to destination which willsatisfy two or more end to end QoS constrains. Therefore, the task of designingan efficient routing algorithm which will satisfy all the quality of servicerequirements and be robust and adaptive is considered as a highly challengingproblem. In this work we have designed a new efficient and energy awaremultipath routing algorithm based on ACO framework, inspired by the behavioursof biological ants. Basically by considering QoS constraints and artificialants we have designed an intelligent version of classical Temporally OrderedRouting Algorithm (TORA) which will increase network lifetime and decreasepacket loss and average end to end delay that makes this algorithm suitable forreal time and multimedia applications.
arxiv-3600-63 | A New Fuzzy Stacked Generalization Technique and Analysis of its Performance | http://arxiv.org/pdf/1204.0171v5.pdf | author:Mete Ozay, Fatos T. Yarman Vural category:cs.LG cs.CV published:2012-04-01 summary:In this study, a new Stacked Generalization technique called Fuzzy StackedGeneralization (FSG) is proposed to minimize the difference between N -sampleand large-sample classification error of the Nearest Neighbor classifier. Theproposed FSG employs a new hierarchical distance learning strategy to minimizethe error difference. For this purpose, we first construct an ensemble ofbase-layer fuzzy k- Nearest Neighbor (k-NN) classifiers, each of which receivesa different feature set extracted from the same sample set. The fuzzymembership values computed at the decision space of each fuzzy k-NN classifierare concatenated to form the feature vectors of a fusion space. Finally, thefeature vectors are fed to a meta-layer classifier to learn the degree ofaccuracy of the decisions of the base-layer classifiers for meta-layerclassification. Rather than the power of the individual base layer-classifiers,diversity and cooperation of the classifiers become an important issue toimprove the overall performance of the proposed FSG. A weak base-layerclassifier may boost the overall performance more than a strong classifier, ifit is capable of recognizing the samples, which are not recognized by the restof the classifiers, in its own feature space. The experiments explore the typeof the collaboration among the individual classifiers required for an improvedperformance of the suggested architecture. Experiments on multiple featurereal-world datasets show that the proposed FSG performs better than the stateof the art ensemble learning algorithms such as Adaboost, Random Subspace andRotation Forest. On the other hand, compatible performances are observed in theexperiments on single feature multi-attribute datasets.
arxiv-3600-64 | B(eo)W(u)LF: Facilitating recurrence analysis on multi-level language | http://arxiv.org/pdf/1308.2696v1.pdf | author:A. Paxton, R. Dale category:cs.CL published:2013-08-12 summary:Discourse analysis may seek to characterize not only the overall compositionof a given text but also the dynamic patterns within the data. This technicalreport introduces a data format intended to facilitate multi-levelinvestigations, which we call the by-word long-form or B(eo)W(u)LF. Inspired bythe long-form data format required for mixed-effects modeling, B(eo)W(u)LFstructures linguistic data into an expanded matrix encoding any number ofresearchers-specified markers, making it ideal for recurrence-based analyses.While we do not necessarily claim to be the first to use methods along theselines, we have created a series of tools utilizing Python and MATLAB to enablesuch discourse analyses and demonstrate them using 319 lines of the Old Englishepic poem, Beowulf, translated into modern English.
arxiv-3600-65 | Local image registration a comparison for bilateral registration mammography | http://arxiv.org/pdf/1308.2654v1.pdf | author:José M. Celaya-Padilla, Juan Rodriguez-Rojas, Victor Trevino, José G. Gerardo Tamez-Pena category:cs.CV published:2013-08-12 summary:Early tumor detection is key in reducing the number of breast cancer deathand screening mammography is one of the most widely available and reliablemethod for early detection. However, it is difficult for the radiologist toprocess with the same attention each case, due the large amount of images to beread. Computer aided detection (CADe) systems improve tumor detection rate; butthe current efficiency of these systems is not yet adequate and the correctinterpretation of CADe outputs requires expert human intervention. Computeraided diagnosis systems (CADx) are being designed to improve cancer diagnosisaccuracy, but they have not been efficiently applied in breast cancer. CADxefficiency can be enhanced by considering the natural mirror symmetry betweenthe right and left breast. The objective of this work is to evaluateco-registration algorithms for the accurate alignment of the left to rightbreast for CADx enhancement. A set of mammograms were artificially altered tocreate a ground truth set to evaluate the registration efficiency of DEMONs,and SPLINE deformable registration algorithms. The registration accuracy wasevaluated using mean square errors, mutual information and correlation. Theresults on the 132 images proved that the SPLINE deformable registrationover-perform the DEMONS on mammography images.
arxiv-3600-66 | Faster gradient descent and the efficient recovery of images | http://arxiv.org/pdf/1308.2464v1.pdf | author:Hui Huang, Uri Ascher category:cs.CV cs.NA math.NA published:2013-08-12 summary:Much recent attention has been devoted to gradient descent algorithms wherethe steepest descent step size is replaced by a similar one from a previousiteration or gets updated only once every second step, thus forming a {\emfaster gradient descent method}. For unconstrained convex quadraticoptimization these methods can converge much faster than steepest descent. Butthe context of interest here is application to certain ill-posed inverseproblems, where the steepest descent method is known to have a smoothing,regularizing effect, and where a strict optimization solution is not necessary. Specifically, in this paper we examine the effect of replacing steepestdescent by a faster gradient descent algorithm in the practical context ofimage deblurring and denoising tasks. We also propose several highly efficientschemes for carrying out these tasks independently of the step size selection,as well as a scheme for the case where both blur and significant noise arepresent. In the above context there are situations where many steepest descent stepsare required, thus building slowness into the solution procedure. Our generalconclusion regarding gradient descent methods is that in such cases the fastergradient descent methods offer substantial advantages. In other situationswhere no such slowness buildup arises the steepest descent method can still bevery effective.
arxiv-3600-67 | Collective Mind: cleaning up the research and experimentation mess in computer engineering using crowdsourcing, big data and machine learning | http://arxiv.org/pdf/1308.2410v1.pdf | author:Grigori Fursin category:cs.SE cs.HC stat.ML published:2013-08-11 summary:Software and hardware co-design and optimization of HPC systems has becomeintolerably complex, ad-hoc, time consuming and error prone due to enormousnumber of available design and optimization choices, complex interactionsbetween all software and hardware components, and multiple strict requirementsplaced on performance, power consumption, size, reliability and cost. Wepresent our novel long-term holistic and practical solution to this problembased on customizable, plugin-based, schema-free, heterogeneous, open-sourceCollective Mind repository and infrastructure with unified web interfaces andon-line advise system. This collaborative framework distributes analysis andmulti-objective off-line and on-line auto-tuning of computer systems among manyparticipants while utilizing any available smart phone, tablet, laptop, clusteror data center, and continuously observing, classifying and modeling theirrealistic behavior. Any unexpected behavior is analyzed using shared datamining and predictive modeling plugins or exposed to the community atcTuning.org for collaborative explanation, top-down complexity reduction,incremental problem decomposition and detection of correlating program,architecture or run-time properties (features). Gradually increasingoptimization knowledge helps to continuously improve optimization heuristics ofany compiler, predict optimizations for new programs or suggest efficientrun-time (online) tuning and adaptation strategies depending on end-userrequirements. We decided to share all our past research artifacts includinghundreds of codelets, numerical applications, data sets, models, universalexperimental analysis and auto-tuning pipelines, self-tuning machine learningbased meta compiler, and unified statistical analysis and machine learningplugins in a public repository to initiate systematic, reproducible andcollaborative research, development and experimentation with a new publicationmodel where experiments and techniques are validated, ranked and improved bythe community.
arxiv-3600-68 | CDfdr: A Comparison Density Approach to Local False Discovery Rate Estimation | http://arxiv.org/pdf/1308.2403v1.pdf | author:Subhadeep Mukhopadhyay category:stat.ME math.ST stat.AP stat.ML stat.TH published:2013-08-11 summary:Efron et al. (2001) proposed empirical Bayes formulation of the frequentistBenjamini and Hochbergs False Discovery Rate method (Benjamini andHochberg,1995). This article attempts to unify the `two cultures' usingconcepts of comparison density and distribution function. We have also shownhow almost all of the existing local fdr methods can be viewed as proposingvarious model specification for comparison density - unifies the vastliterature of false discovery methods under one concept and notation.
arxiv-3600-69 | A radial basis function neural network based approach for the electrical characteristics estimation of a photovoltaic module | http://arxiv.org/pdf/1308.2375v1.pdf | author:Francesco Bonanno, Giacomo Capizzi, Christian Napoli, Giorgio Graditi, Giuseppe Marco Tina category:cs.NE published:2013-08-11 summary:The design process of photovoltaic (PV) modules can be greatly enhanced byusing advanced and accurate models in order to predict accurately theirelectrical output behavior. The main aim of this paper is to investigate theapplication of an advanced neural network based model of a module to improvethe accuracy of the predicted output I--V and P--V curves and to keep inaccount the change of all the parameters at different operating conditions.Radial basis function neural networks (RBFNN) are here utilized to predict theoutput characteristic of a commercial PV module, by reading only the data ofsolar irradiation and temperature. A lot of available experimental data wereused for the training of the RBFNN, and a backpropagation algorithm wasemployed. Simulation and experimental validation is reported.
arxiv-3600-70 | Exploratory Analysis of Highly Heterogeneous Document Collections | http://arxiv.org/pdf/1308.2359v1.pdf | author:Arun S. Maiya, John P. Thompson, Francisco Loaiza-Lemos, Robert M. Rolfe category:cs.CL cs.HC cs.IR published:2013-08-11 summary:We present an effective multifaceted system for exploratory analysis ofhighly heterogeneous document collections. Our system is based on intelligentlytagging individual documents in a purely automated fashion and exploiting thesetags in a powerful faceted browsing framework. Tagging strategies employedinclude both unsupervised and supervised approaches based on machine learningand natural language processing. As one of our key tagging strategies, weintroduce the KERA algorithm (Keyword Extraction for Reports and Articles).KERA extracts topic-representative terms from individual documents in a purelyunsupervised fashion and is revealed to be significantly more effective thanstate-of-the-art methods. Finally, we evaluate our system in its ability tohelp users locate documents pertaining to military critical technologies burieddeep in a large heterogeneous sea of information.
arxiv-3600-71 | Learning Features and their Transformations by Spatial and Temporal Spherical Clustering | http://arxiv.org/pdf/1308.2350v1.pdf | author:Jayanta K. Dutta, Bonny Banerjee category:cs.NE cs.CV q-bio.NC I.2; I.4; I.5 published:2013-08-10 summary:Learning features invariant to arbitrary transformations in the data is arequirement for any recognition system, biological or artificial. It is nowwidely accepted that simple cells in the primary visual cortex respond tofeatures while the complex cells respond to features invariant to differenttransformations. We present a novel two-layered feedforward neural model thatlearns features in the first layer by spatial spherical clustering andinvariance to transformations in the second layer by temporal sphericalclustering. Learning occurs in an online and unsupervised manner following theHebbian rule. When exposed to natural videos acquired by a camera mounted on acat's head, the first and second layer neurons in our model develop simple andcomplex cell-like receptive field properties. The model can predict by learninglateral connections among the first layer neurons. A topographic map to theirspatial features emerges by exponentially decaying the flow of activation withdistance from one neuron to another in the first layer that fire in closetemporal proximity, thereby minimizing the pooling length in an online mannersimultaneously with feature learning.
arxiv-3600-72 | Finite Element Model Updating Using Fish School Search Optimization Method | http://arxiv.org/pdf/1308.2307v1.pdf | author:I. Boulkabeit, L. Mthembu, T. Marwala, F. De Lima Neto category:cs.CE cs.NE published:2013-08-10 summary:A recent nature inspired optimization algorithm, Fish School Search (FSS) isapplied to the finite element model (FEM) updating problem. This method istested on a GARTEUR SM-AG19 aeroplane structure. The results of this algorithmare compared with two other metaheuristic algorithms; Genetic Algorithm (GA)and Particle Swarm Optimization (PSO). It is observed that on average, the FSSand PSO algorithms give more accurate results than the GA. A minor modificationto the FSS is proposed. This modification improves the performance of FSS onthe FEM updating problem which has a constrained search space.
arxiv-3600-73 | Second order scattering descriptors predict fMRI activity due to visual textures | http://arxiv.org/pdf/1310.1257v1.pdf | author:Michael Eickenberg, Fabian Pedregosa, Senoussi Mehdi, Alexandre Gramfort, Bertrand Thirion category:cs.CV published:2013-08-10 summary:Second layer scattering descriptors are known to provide good classificationperformance on natural quasi-stationary processes such as visual textures dueto their sensitivity to higher order moments and continuity with respect tosmall deformations. In a functional Magnetic Resonance Imaging (fMRI)experiment we present visual textures to subjects and evaluate the predictivepower of these descriptors with respect to the predictive power of simplecontour energy - the first scattering layer. We are able to conclude not onlythat invariant second layer scattering coefficients better encode voxelactivity, but also that well predicted voxels need not necessarily lie in knownretinotopic regions.
arxiv-3600-74 | Fast image segmentation and restoration using parametric curve evolution with junctions and topology changes | http://arxiv.org/pdf/1308.2292v1.pdf | author:Heike Benninghoff, Harald Garcke category:cs.CV math.AP math.NA published:2013-08-10 summary:Curve evolution schemes for image segmentation based on a region basedcontour model allowing for junctions, vector-valued images and topology changesare introduced. Together with an a posteriori denoising in the segmentedhomogeneous regions this leads to a fast and efficient method for imagesegmentation and restoration. An uneven spread of mesh points is avoided byusing the tangential degrees of freedom. Several numerical simulations onartificial test problems and on real images illustrate the performance of themethod.
arxiv-3600-75 | Supplement to "Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs" | http://arxiv.org/pdf/1303.0632v2.pdf | author:Yangbo He, Jinzhu Jia, Bin Yu category:stat.ML math.CO published:2013-03-04 summary:This supplementary material includes three parts: some preliminary results,four examples, an experiment, three new algorithms, and all proofs of theresults in the paper "Reversible MCMC on Markov equivalence classes of sparsedirected acyclic graphs".
arxiv-3600-76 | Precisely Verifying the Null Space Conditions in Compressed Sensing: A Sandwiching Algorithm | http://arxiv.org/pdf/1306.2665v3.pdf | author:Myung Cho, Weiyu Xu category:cs.IT cs.LG cs.SY math.IT math.OC stat.ML published:2013-06-11 summary:In this paper, we propose new efficient algorithms to verify the null spacecondition in compressed sensing (CS). Given an $(n-m) \times n$ ($m>0$) CSmatrix $A$ and a positive $k$, we are interested in computing $\displaystyle\alpha_k = \max_{\{z: Az=0,z\neq 0\}}\max_{\{K: K\leq k\}}$ ${\z_K\_{1}}{\z\_{1}}$, where $K$ represents subsets of $\{1,2,...,n\}$, and $K$is the cardinality of $K$. In particular, we are interested in finding themaximum $k$ such that $\alpha_k < {1}{2}$. However, computing $\alpha_k$ isknown to be extremely challenging. In this paper, we first propose a series ofnew polynomial-time algorithms to compute upper bounds on $\alpha_k$. Based onthese new polynomial-time algorithms, we further design a new sandwichingalgorithm, to compute the \emph{exact} $\alpha_k$ with greatly reducedcomplexity. When needed, this new sandwiching algorithm also achieves a smoothtradeoff between computational complexity and result accuracy. Empiricalresults show the performance improvements of our algorithm over existing knownmethods; and our algorithm outputs precise values of $\alpha_k$, with muchlower complexity than exhaustive search.
arxiv-3600-77 | Coding for Random Projections | http://arxiv.org/pdf/1308.2218v1.pdf | author:Ping Li, Michael Mitzenmacher, Anshumali Shrivastava category:cs.LG cs.DS cs.IT math.IT stat.CO published:2013-08-09 summary:The method of random projections has become very popular for large-scaleapplications in statistical learning, information retrieval, bio-informaticsand other applications. Using a well-designed coding scheme for the projecteddata, which determines the number of bits needed for each projected value andhow to allocate these bits, can significantly improve the effectiveness of thealgorithm, in storage cost as well as computational speed. In this paper, westudy a number of simple coding schemes, focusing on the task of similarityestimation and on an application to training linear classifiers. We demonstratethat uniform quantization outperforms the standard existing influential method(Datar et. al. 2004). Indeed, we argue that in many cases coding with just asmall number of bits suffices. Furthermore, we also develop a non-uniform 2-bitcoding scheme that generally performs well in practice, as confirmed by ourexperiments on training linear support vector machines (SVM).
arxiv-3600-78 | Entropy landscape of solutions in the binary perceptron problem | http://arxiv.org/pdf/1304.2850v2.pdf | author:Haiping Huang, K. Y. Michael Wong, Yoshiyuki Kabashima category:cs.LG published:2013-04-10 summary:The statistical picture of the solution space for a binary perceptron isstudied. The binary perceptron learns a random classification of input randompatterns by a set of binary synaptic weights. The learning of this network isdifficult especially when the pattern (constraint) density is close to thecapacity, which is supposed to be intimately related to the structure of thesolution space. The geometrical organization is elucidated by the entropylandscape from a reference configuration and of solution-pairs separated by agiven Hamming distance in the solution space. We evaluate the entropy at theannealed level as well as replica symmetric level and the mean field result isconfirmed by the numerical simulations on single instances using the proposedmessage passing algorithms. From the first landscape (a random configuration asa reference), we see clearly how the solution space shrinks as more constraintsare added. From the second landscape of solution-pairs, we deduce thecoexistence of clustering and freezing in the solution space.
arxiv-3600-79 | Time series modeling with pruned multi-layer perceptron and 2-stage damped least-squares method | http://arxiv.org/pdf/1308.1940v1.pdf | author:Cyril Voyant, Wani W. Tamas, Christophe Paoli, Aurélia Balu, Marc Muselli, Marie Laure Nivet, Gilles Notton category:cs.NE published:2013-08-08 summary:A Multi-Layer Perceptron (MLP) defines a family of artificial neural networksoften used in TS modeling and forecasting. Because of its "black box" aspect,many researchers refuse to use it. Moreover, the optimization (often based onthe exhaustive approach where "all" configurations are tested) and learningphases of this artificial intelligence tool (often based on theLevenberg-Marquardt algorithm; LMA) are weaknesses of this approach(exhaustively and local minima). These two tasks must be repeated depending onthe knowledge of each new problem studied, making the process, long, laboriousand not systematically robust. In this paper a pruning process is proposed.This method allows, during the training phase, to carry out an inputs selectingmethod activating (or not) inter-nodes connections in order to verify ifforecasting is improved. We propose to use iteratively the popular dampedleast-squares method to activate inputs and neurons. A first pass is applied to10% of the learning sample to determine weights significantly different from 0and delete other. Then a classical batch process based on LMA is used with thenew MLP. The validation is done using 25 measured meteorological TS andcross-comparing the prediction results of the classical LMA and the 2-stageLMA.
arxiv-3600-80 | Local Space-Time Smoothing for Version Controlled Documents | http://arxiv.org/pdf/1003.1410v2.pdf | author:Seungyeon Kim, Guy Lebanon category:cs.GR cs.CL cs.LG published:2010-03-06 summary:Unlike static documents, version controlled documents are continuously editedby one or more authors. Such collaborative revision process makes traditionalmodeling and visualization techniques inappropriate. In this paper we propose anew representation based on local space-time smoothing that captures importantrevision patterns. We demonstrate the applicability of our framework usingexperiments on synthetic and real-world data.
arxiv-3600-81 | Beyond Sentiment: The Manifold of Human Emotions | http://arxiv.org/pdf/1202.1568v2.pdf | author:Seungyeon Kim, Fuxin Li, Guy Lebanon, Irfan Essa category:cs.CL published:2012-02-08 summary:Sentiment analysis predicts the presence of positive or negative emotions ina text document. In this paper we consider higher dimensional extensions of thesentiment concept, which represent a richer set of human emotions. Our approachgoes beyond previous work in that our model contains a continuous manifoldrather than a finite set of human emotions. We investigate the resulting model,compare it to psychological observations, and explore its predictivecapabilities. Besides obtaining significant improvements over a baselinewithout manifold, we are also able to visualize different notions of positivesentiment in different domains.
arxiv-3600-82 | Particles Prefer Walking Along the Axes: Experimental Insights into the Behavior of a Particle Swarm | http://arxiv.org/pdf/1303.6145v2.pdf | author:Manuel Schmitt, Rolf Wanka category:cs.NE cs.AI I.2.8 published:2013-03-25 summary:Particle swarm optimization (PSO) is a widely used nature-inspiredmeta-heuristic for solving continuous optimization problems. However, whenrunning the PSO algorithm, one encounters the phenomenon of so-calledstagnation, that means in our context, the whole swarm starts to converge to asolution that is not (even a local) optimum. The goal of this work is to pointout possible reasons why the swarm stagnates at these non-optimal points. Toachieve our results, we use the newly defined potential of a swarm. The totalpotential has a portion for every dimension of the search space, and it dropswhen the swarm approaches the point of convergence. As it turns outexperimentally, the swarm is very likely to come sometimes into "unbalanced"states, i. e., almost all potential belongs to one axis. Therefore, the swarmbecomes blind for improvements still possible in any other direction. Finally,we show how in the light of the potential and these observations, a slightlyadapted PSO rebalances the potential and therefore increases the quality of thesolution.
arxiv-3600-83 | Satellite image classification methods and Landsat 5TM bands | http://arxiv.org/pdf/1308.1801v1.pdf | author:Jamshid Tamouk, Nasser Lotfi, Mina Farmanbar category:cs.CV astro-ph.IM published:2013-08-08 summary:This paper attempts to find the most accurate classification method amongparallelepiped, minimum distance and chain methods. Moreover, this study alsochallenges to find the suitable combination of bands, which can lead to betterresults in case combinations of bands occur. After comparing these threemethods, the chain method over perform the other methods with 79% overallaccuracy. Hence, it is more accurate than minimum distance with 67% andparallelepiped with 65%. On the other hand, based on bands features, and alsoby combining several researchers' findings, a table was created which includesthe main objects on the land and the suitable combination of the bands foraccurately detecting of landcover objects. During this process, it was observedthat band 4 (out of 7 bands of Landsat 5TM) is the band, which can be used forincreasing the accuracy of the combined bands in detecting objects on the land.
arxiv-3600-84 | OFF-Set: One-pass Factorization of Feature Sets for Online Recommendation in Persistent Cold Start Settings | http://arxiv.org/pdf/1308.1792v1.pdf | author:Michal Aharon, Natalie Aizenberg, Edward Bortnikov, Ronny Lempel, Roi Adadi, Tomer Benyamini, Liron Levin, Ran Roth, Ohad Serfaty category:cs.LG cs.IR H.4; D.2.8 published:2013-08-08 summary:One of the most challenging recommendation tasks is recommending to a new,previously unseen user. This is known as the 'user cold start' problem.Assuming certain features or attributes of users are known, one approach forhandling new users is to initially model them based on their features. Motivated by an ad targeting application, this paper describes an extremeonline recommendation setting where the cold start problem is perpetual. Everyuser is encountered by the system just once, receives a recommendation, andeither consumes or ignores it, registering a binary reward. We introduce One-pass Factorization of Feature Sets, OFF-Set, a novelrecommendation algorithm based on Latent Factor analysis, which models users bymapping their features to a latent space. Furthermore, OFF-Set is able to modelnon-linear interactions between pairs of features. OFF-Set is designed forpurely online recommendation, performing lightweight updates of its model pereach recommendation-reward observation. We evaluate OFF-Set against severalstate of the art baselines, and demonstrate its superiority on realad-targeting data.
arxiv-3600-85 | A Note on Topology Preservation in Classification, and the Construction of a Universal Neuron Grid | http://arxiv.org/pdf/1308.1603v2.pdf | author:Dietmar Volz category:cs.NE cs.AI nlin.AO stat.ML 92F99 published:2013-08-07 summary:It will be shown that according to theorems of K. Menger, every neuron gridif identified with a curve is able to preserve the adopted qualitativestructure of a data space. Furthermore, if this identification is made, theneuron grid structure can always be mapped to a subset of a universal neurongrid which is constructable in three space dimensions. Conclusions will bedrawn for established neuron grid types as well as neural fields.
arxiv-3600-86 | Near-Optimal Algorithms for Differentially-Private Principal Components | http://arxiv.org/pdf/1207.2812v3.pdf | author:Kamalika Chaudhuri, Anand D. Sarwate, Kaushik Sinha category:stat.ML cs.CR cs.LG published:2012-07-12 summary:Principal components analysis (PCA) is a standard tool for identifying goodlow-dimensional approximations to data in high dimension. Many data sets ofinterest contain private or sensitive information about individuals. Algorithmswhich operate on such data should be sensitive to the privacy risks inpublishing their outputs. Differential privacy is a framework for developingtradeoffs between privacy and the utility of these outputs. In this paper weinvestigate the theory and empirical performance of differentially privateapproximations to PCA and propose a new method which explicitly optimizes theutility of the output. We show that the sample complexity of the proposedmethod differs from the existing procedure in the scaling with the datadimension, and that our method is nearly optimal in terms of this scaling. Wefurthermore illustrate our results, showing that on real data there is a largeperformance gap between the existing method and our method.
arxiv-3600-87 | Logical analysis of natural language semantics to solve the problem of computer understanding | http://arxiv.org/pdf/1308.1507v1.pdf | author:Yuriy Ostapov category:cs.CL published:2013-08-07 summary:An object--oriented approach to create a natural language understandingsystem is considered. The understanding program is a formal system built on thebase of predicative calculus. Horn's clauses are used as well--formed formulas.An inference is based on the principle of resolution. Sentences of naturallanguage are represented in the view of typical predicate set. These predicatesdescribe physical objects and processes, abstract objects, categories andsemantic relations between objects. Predicates for concrete assertions aresaved in a database. To describe the semantics of classes for physical objects,abstract concepts and processes, a knowledge base is applied. The proposedrepresentation of natural language sentences is a semantic net. Nodes of suchnet are typical predicates. This approach is perspective as, firstly, suchtypification of nodes facilitates essentially forming of processing algorithmsand object descriptions, secondly, the effectiveness of algorithms is increased(particularly for the great number of nodes), thirdly, to describe thesemantics of words, encyclopedic knowledge is used, and this permitsessentially to extend the class of solved problems.
arxiv-3600-88 | A Multi-Swarm Cellular PSO based on Clonal Selection Algorithm in Dynamic Environments | http://arxiv.org/pdf/1308.1484v1.pdf | author:Somayeh Nabizadeh, Alireza Rezvanian, Mohammd Reza Meybodi category:cs.NE cs.AI published:2013-08-07 summary:Many real-world problems are dynamic optimization problems. In this case, theoptima in the environment change dynamically. Therefore, traditionaloptimization algorithms disable to track and find optima. In this paper, a newmulti-swarm cellular particle swarm optimization based on clonal selectionalgorithm (CPSOC) is proposed for dynamic environments. In the proposedalgorithm, the search space is partitioned into cells by a cellular automaton.Clustered particles in each cell, which make a sub-swarm, are evolved by theparticle swarm optimization and clonal selection algorithm. Experimentalresults on Moving Peaks Benchmark demonstrate the superiority of the CPSOC itspopular methods.
arxiv-3600-89 | Bayesian ensemble learning for image denoising | http://arxiv.org/pdf/1308.1374v1.pdf | author:Hyuntaek Oh category:cs.CV published:2013-08-06 summary:Natural images are often affected by random noise and image denoising haslong been a central topic in Computer Vision. Many algorithms have beenintroduced to remove the noise from the natural images, such as Gaussian,Wiener filtering and wavelet thresholding. However, many of these algorithmsremove the fine edges and make them blur. Recently, many promising denoisingalgorithms have been introduced such as Non-local Means, Fields of Experts, andBM3D. In this paper, we explore Bayesian method of ensemble learning for imagedenoising. Ensemble methods seek to combine multiple different algorithms toretain the strengths of all methods and the weaknesses of none. Bayesianensemble models are Non-local Means and Fields of Experts, the very successfulrecent algorithms. The Non-local Means presumes that the image contains anextensive amount of self-similarity. The approach of the Fields of Expertsmodel extends traditional Markov Random Field model by learning potentialfunctions over extended pixel neighborhoods. The two models are implemented andimage denoising is performed on natural images. The experimental resultsobtained are used to compare with the single algorithm and discuss the ensemblelearning and their approaches. Comparing to the results of Non-local Means andFields of Experts, Ensemble learning showed improvement nearly 1dB.
arxiv-3600-90 | Invariances of random fields paths, with applications in Gaussian Process Regression | http://arxiv.org/pdf/1308.1359v1.pdf | author:David Ginsbourger, Olivier Roustant, Nicolas Durrande category:math.ST math.PR stat.ME stat.ML stat.TH published:2013-08-06 summary:We study pathwise invariances of centred random fields that can be controlledthrough the covariance. A result involving composition operators is obtained insecond-order settings, and we show that various path properties includingadditivity boil down to invariances of the covariance kernel. These results areextended to a broader class of operators in the Gaussian case, via the Lo\`eveisometry. Several covariance-driven pathwise invariances are illustrated,including fields with symmetric paths, centred paths, harmonic paths, or sparsepaths. The proposed approach delivers a number of promising results andperspectives in Gaussian process regression.
arxiv-3600-91 | The evolution of representation in simple cognitive networks | http://arxiv.org/pdf/1206.5771v2.pdf | author:Lars Marstaller, Arend Hintze, Christoph Adami category:q-bio.NC cs.NE q-bio.PE published:2012-06-25 summary:Representations are internal models of the environment that can provideguidance to a behaving agent, even in the absence of sensory information. It isnot clear how representations are developed and whether or not they arenecessary or even essential for intelligent behavior. We argue here that theability to represent relevant features of the environment is the expectedconsequence of an adaptive process, give a formal definition of representationbased on information theory, and quantify it with a measure R. To measure how Rchanges over time, we evolve two types of networks---an artificial neuralnetwork and a network of hidden Markov gates---to solve a categorization taskusing a genetic algorithm. We find that the capacity to represent increasesduring evolutionary adaptation, and that agents form representations of theirenvironment during their lifetime. This ability allows the agents to act onsensorial inputs in the context of their acquired representations and enablescomplex and context-dependent behavior. We examine which concepts (features ofthe environment) our networks are representing, how the representations arelogically encoded in the networks, and how they form as an agent behaves tosolve a task. We conclude that R should be able to quantify the representationswithin any cognitive system, and should be predictive of an agent's long-termadaptive success.
arxiv-3600-92 | Science Fiction as a Worldwide Phenomenon: A Study of International Creation, Consumption and Dissemination | http://arxiv.org/pdf/1308.1292v1.pdf | author:Elysia Wells category:cs.DL cs.CL cs.SI physics.soc-ph published:2013-08-06 summary:This paper examines the international nature of science fiction. The focus ofthis research is to determine whether science fiction is primarily Englishspeaking and Western or global; being created and consumed by people innon-Western, non-English speaking countries? Science fiction's internationalpresence was found in three ways, by network analysis, by examining a onlineretailer and with a survey. Condor, a program developed by GalaxyAdvisors wasused to determine if science fiction is being talked about by non-Englishspeakers. An analysis of the international Amazon.com websites was done todiscover if it was being consumed worldwide. A survey was also conducted to seeif people had experience with science fiction. All three research methodsrevealed similar results. Science fiction was found to be international, withscience fiction creators originating in different countries and writing in ahost of different languages. English and non-English science fiction was beingcreated and consumed all over the world, not just in the English speaking West.
arxiv-3600-93 | The Group Lasso for Design of Experiments | http://arxiv.org/pdf/1308.1196v1.pdf | author:Kentaro Tanaka, Masami Miyakawa category:stat.ML 62K05 published:2013-08-06 summary:We introduce an application of the group lasso to design of exper- iments. Weshow that the problem of constructing an optimal design matrix can betransformed into a problem of the group lasso. We also give a numerical examplethat we can obtain several orthogonal arrays as the solutions of the grouplasso problems.
arxiv-3600-94 | Spatial-Aware Dictionary Learning for Hyperspectral Image Classification | http://arxiv.org/pdf/1308.1187v1.pdf | author:Ali Soltani-Farani, Hamid R. Rabiee, Seyyed Abbas Hosseini category:cs.CV cs.LG published:2013-08-06 summary:This paper presents a structured dictionary-based model for hyperspectraldata that incorporates both spectral and contextual characteristics of aspectral sample, with the goal of hyperspectral image classification. The ideais to partition the pixels of a hyperspectral image into a number of spatialneighborhoods called contextual groups and to model each pixel with a linearcombination of a few dictionary elements learned from the data. Since pixelsinside a contextual group are often made up of the same materials, their linearcombinations are constrained to use common elements from the dictionary. Tothis end, dictionary learning is carried out with a joint sparse regularizer toinduce a common sparsity pattern in the sparse coefficients of each contextualgroup. The sparse coefficients are then used for classification using a linearSVM. Experimental results on a number of real hyperspectral images confirm theeffectiveness of the proposed representation for hyperspectral imageclassification. Moreover, experiments with simulated multispectral data showthat the proposed model is capable of finding representations that mayeffectively be used for classification of multispectral-resolution samples.
arxiv-3600-95 | Multimodal Approach for Video Surveillance Indexing and Retrieval | http://arxiv.org/pdf/1308.1150v1.pdf | author:Ali Wali, Adel M. Alimi category:cs.MM cs.CV published:2013-08-06 summary:In this paper, we present an overview of a multimodal system to indexing andsearching video sequence by the content that has been developed within theREGIMVid project. A large part of our system has been developed as part ofTRECVideo evaluation. The MAVSIR platform provides High-level featureextraction from audio-visual content and concept/event-based video retrieval.We illustrate the architecture of the system as well as provide an overview ofthe descriptors supported to date. Then we demonstrate the usefulness of thetoolbox in the context of feature extraction, concepts/events learning andretrieval in large collections of video surveillance dataset. The results areencouraging as we are able to get good results on several event categories,while for all events we have gained valuable insights and experience.
arxiv-3600-96 | Empirical Entropy, Minimax Regret and Minimax Risk | http://arxiv.org/pdf/1308.1147v1.pdf | author:Alexander Rakhlin, Karthik Sridharan, Alexandre B. Tsybakov category:math.ST cs.LG stat.TH published:2013-08-06 summary:We consider the random design regression model with square loss. We propose amethod that aggregates empirical minimizers (ERM) over appropriately chosenrandom subsets and reduces to ERM in the extreme case, and we establish sharporacle inequalities for its risk. We show that, under the $\epsilon^{-p}$growth of the empirical $\epsilon$-entropy, the excess risk of the proposedmethod attains the rate $n^{-\frac{2}{2+p}}$ for $p\in(0,2]$ and $n^{-1/p}$ for$p> 2$ where $n$ is the sample size. Furthermore, for $p\in(0,2]$, the excessrisk rate matches the behavior of the minimax risk of function estimation inregression problems under the well-specified model. This yields a conclusionthat the rates of statistical estimation in well-specified models (minimaxrisk) and in misspecified models (minimax regret) are equivalent in the regime$p\in(0,2]$. In other words, for $p\in(0,2]$ the problem of statisticallearning enjoys the same minimax rate as the problem of statistical estimation.On the contrary, for $p>2$ we show that the rates of the minimax regret are, ingeneral, slower than for the minimax risk. Our oracle inequalities also implythe $v\log(n/v)/n$ rates for Vapnik-Chervonenkis type classes of dimension $v$without the usual convexity assumption on the class; we show that these ratesare optimal. Finally, for a slightly modified method, we derive a bound on theexcess risk of $s$-sparse convex aggregation improving that of (Lounici 07) andproviding the optimal rate.
arxiv-3600-97 | Image interpolation using Shearlet based iterative refinement | http://arxiv.org/pdf/1308.1126v1.pdf | author:H. Lakshman, W. -Q Lim, H. Schwarz, D. Marpe, G. Kutyniok, T. Wiegand category:cs.CV 94A08 65T60 published:2013-08-05 summary:This paper proposes an image interpolation algorithm exploiting sparserepresentation for natural images. It involves three main steps: (a) obtainingan initial estimate of the high resolution image using linear methods like FIRfiltering, (b) promoting sparsity in a selected dictionary through iterativethresholding, and (c) extracting high frequency information from theapproximation to refine the initial estimate. For the sparse modeling, ashearlet dictionary is chosen to yield a multiscale directional representation.The proposed algorithm is compared to several state-of-the-art methods toassess its objective as well as subjective performance. Compared to the cubicspline interpolation method, an average PSNR gain of around 0.8 dB is observedover a dataset of 200 images.
arxiv-3600-98 | Theoretical Issues for Global Cumulative Treatment Analysis (GCTA) | http://arxiv.org/pdf/1308.1066v1.pdf | author:Jeff Shrager category:stat.AP cs.LG published:2013-08-05 summary:Adaptive trials are now mainstream science. Recently, researchers have takenthe adaptive trial concept to its natural conclusion, proposing what we call"Global Cumulative Treatment Analysis" (GCTA). Similar to the adaptive trial,decision making and data collection and analysis in the GCTA are continuous andintegrated, and treatments are ranked in accord with the statistics of thisinformation, combined with what offers the most information gain. Where GCTAdiffers from an adaptive trial, or, for that matter, from any trial design, isthat all patients are implicitly participants in the GCTA process, regardlessof whether they are formally enrolled in a trial. This paper discusses some ofthe theoretical and practical issues that arise in the design of a GCTA, alongwith some preliminary thoughts on how they might be approached.
arxiv-3600-99 | Coevolutionary networks of reinforcement-learning agents | http://arxiv.org/pdf/1308.1049v1.pdf | author:Ardeshir Kianercy, Aram Galstyan category:cs.MA cs.LG nlin.AO published:2013-08-05 summary:This paper presents a model of network formation in repeated games where theplayers adapt their strategies and network ties simultaneously using a simplereinforcement-learning scheme. It is demonstrated that the coevolutionarydynamics of such systems can be described via coupled replicator equations. Weprovide a comprehensive analysis for three-player two-action games, which isthe minimum system size with nontrivial structural dynamics. In particular, wecharacterize the Nash equilibria (NE) in such games and examine the localstability of the rest points corresponding to those equilibria. We also studygeneral n-player networks via both simulations and analytical methods and findthat in the absence of exploration, the stable equilibria consist of starmotifs as the main building blocks of the network. Furthermore, in all stableequilibria the agents play pure strategies, even when the game allows mixed NE.Finally, we study the impact of exploration on learning outcomes, and observethat there is a critical exploration rate above which the symmetric anduniformly connected network topology becomes stable.
arxiv-3600-100 | Sign Stable Projections, Sign Cauchy Projections and Chi-Square Kernels | http://arxiv.org/pdf/1308.1009v1.pdf | author:Ping Li, Gennady Samorodnitsky, John Hopcroft category:cs.LG cs.DS cs.IR published:2013-08-05 summary:The method of stable random projections is popular for efficiently computingthe Lp distances in high dimension (where 0<p<=2), using small space. Becauseit adopts nonadaptive linear projections, this method is naturally suitablewhen the data are collected in a dynamic streaming fashion (i.e., turnstiledata streams). In this paper, we propose to use only the signs of the projecteddata and analyze the probability of collision (i.e., when the two signsdiffer). We derive a bound of the collision probability which is exact when p=2and becomes less sharp when p moves away from 2. Interestingly, when p=1 (i.e.,Cauchy random projections), we show that the probability of collision can beaccurately approximated as functions of the chi-square similarity. For example,when the (un-normalized) data are binary, the maximum approximation error ofthe collision probability is smaller than 0.0192. In text and visionapplications, the chi-square similarity is a popular measure for nonnegativedata when the features are generated from histograms. Our experiments confirmthat the proposed method is promising for large-scale learning applications.
arxiv-3600-101 | Fast Semidifferential-based Submodular Function Optimization | http://arxiv.org/pdf/1308.1006v1.pdf | author:Rishabh Iyer, Stefanie Jegelka, Jeff Bilmes category:cs.DS cs.DM cs.LG published:2013-08-05 summary:We present a practical and powerful new framework for both unconstrained andconstrained submodular function optimization based on discretesemidifferentials (sub- and super-differentials). The resulting algorithms,which repeatedly compute and then efficiently optimize submodularsemigradients, offer new and generalize many old methods for submodularoptimization. Our approach, moreover, takes steps towards providing a unifyingparadigm applicable to both submodular min- imization and maximization,problems that historically have been treated quite distinctly. The practicalityof our algorithms is important since interest in submodularity, owing to itsnatural and wide applicability, has recently been in ascendance within machinelearning. We analyze theoretical properties of our algorithms for minimizationand maximization, and show that many state-of-the-art maximization algorithmsare special cases. Lastly, we complement our theoretical analyses withsupporting empirical experiments.
arxiv-3600-102 | Context Specific Event Model For News Articles | http://arxiv.org/pdf/1308.0897v1.pdf | author:Kowcika A, Uma Maheswari, Geetha T V category:cs.CL cs.IR published:2013-08-05 summary:We present a new context based event indexing and event ranking model forNews Articles. The context event clusters formed from the UNL Graphs uses themodified scoring scheme for segmenting events which is followed by clusteringof events. From the context clusters obtained three models are developed-Identification of Main and Sub events; Event Indexing and Event Ranking. Basedon the properties considered from the UNL Graphs for the modified scoring mainevents and sub events associated with main-events are identified. The temporaldetails obtained from the context cluster are stored using hashmap datastructure. The temporal details are place-where the event took; person-whoinvolved in that event; time-when the event took place. Based on theinformation collected from the context clusters three indices are generated-Time index, Person index, and Place index. This index gives complete detailsabout every event obtained from context clusters. A new scoring scheme isintroduced for ranking the events. The scoring scheme for event ranking givesweight-age based on the priority level of the events. The priority levelincludes the occurrence of the event in the title of the document, eventfrequency, and inverse document frequency of the events.
arxiv-3600-103 | Head Gesture Recognition using Optical Flow based Classification with Reinforcement of GMM based Background Subtraction | http://arxiv.org/pdf/1308.0890v1.pdf | author:Parimita Saikia, Karen Das category:cs.CV published:2013-08-05 summary:This paper describes a technique of real time head gesture recognitionsystem. The method includes Gaussian mixture model (GMM) accompanied by opticalflow algorithm which provided us the required information regarding headmovement. The proposed model can be implemented in various control system. Weare also presenting the result and implementation of both mentioned method.
arxiv-3600-104 | MonoStream: A Minimal-Hardware High Accuracy Device-free WLAN Localization System | http://arxiv.org/pdf/1308.0768v1.pdf | author:Ibrahim Sabek, Moustafa Youssef category:cs.NI cs.LG published:2013-08-04 summary:Device-free (DF) localization is an emerging technology that allows thedetection and tracking of entities that do not carry any devices norparticipate actively in the localization process. Typically, DF systems requirea large number of transmitters and receivers to achieve acceptable accuracy,which is not available in many scenarios such as homes and small businesses. Inthis paper, we introduce MonoStream as an accurate single-stream DFlocalization system that leverages the rich Channel State Information (CSI) aswell as MIMO information from the physical layer to provide accurate DFlocalization with only one stream. To boost its accuracy and attain lowcomputational requirements, MonoStream models the DF localization problem as anobject recognition problem and uses a novel set of CSI-context features andtechniques with proven accuracy and efficiency. Experimental evaluation in twotypical testbeds, with a side-by-side comparison with the state-of-the-art,shows that MonoStream can achieve an accuracy of 0.95m with at least 26%enhancement in median distance error using a single stream only. Thisenhancement in accuracy comes with an efficient execution of less than 23ms perlocation update on a typical laptop. This highlights the potential ofMonoStream usage for real-time DF tracking applications.
arxiv-3600-105 | Ontology Enrichment by Extracting Hidden Assertional Knowledge from Text | http://arxiv.org/pdf/1308.0701v1.pdf | author:Meisam Booshehri, Abbas Malekpour, Peter Luksch, Kamran Zamanifar, Shahdad Shariatmadari category:cs.IR cs.CL 68Txx I.2.6 published:2013-08-03 summary:In this position paper we present a new approach for discovering some specialclasses of assertional knowledge in the text by using large RDF repositories,resulting in the extraction of new non-taxonomic ontological relations. Also weuse inductive reasoning beside our approach to make it outperform. Then, weprepare a case study by applying our approach on sample data and illustrate thesoundness of our proposed approach. Moreover in our point of view current LODcloud is not a suitable base for our proposal in all informational domains.Therefore we figure out some directions based on prior works to enrich datasetsof Linked Data by using web mining. The result of such enrichment can be reusedfor further relation extraction and ontology enrichment from unstructured freetext documents.
arxiv-3600-106 | A Comparison of Named Entity Recognition Tools Applied to Biographical Texts | http://arxiv.org/pdf/1308.0661v1.pdf | author:Samet Atdağ, Vincent Labatut category:cs.IR cs.CL published:2013-08-03 summary:Named entity recognition (NER) is a popular domain of natural languageprocessing. For this reason, many tools exist to perform this task. Amongstother points, they differ in the processing method they rely upon, the entitytypes they can detect, the nature of the text they can handle, and theirinput/output formats. This makes it difficult for a user to select anappropriate NER tool for a specific situation. In this article, we try toanswer this question in the context of biographic texts. For this matter, wefirst constitute a new corpus by annotating Wikipedia articles. We then selectpublicly available, well known and free for research NER tools for comparison:Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We applythem to our corpus, assess their performances and compare them. Whenconsidering overall performances, a clear hierarchy emerges: Stanford has thebest results, followed by LingPipe, Illionois and OpenCalais. However, a moredetailed evaluation performed relatively to entity types and article categorieshighlights the fact their performances are diversely influenced by thosefactors. This complementarity opens an interesting perspective regarding thecombination of these individual tools in order to improve performance.
arxiv-3600-107 | Compressive Shift Retrieval | http://arxiv.org/pdf/1303.4996v2.pdf | author:Henrik Ohlsson, Yonina C. Eldar, Allen Y. Yang, S. Shankar Sastry category:cs.SY cs.IT math.IT stat.ML published:2013-03-20 summary:The classical shift retrieval problem considers two signals in vector formthat are related by a shift. The problem is of great importance in manyapplications and is typically solved by maximizing the cross-correlationbetween the two signals. Inspired by compressive sensing, in this paper, weseek to estimate the shift directly from compressed signals. We show that undercertain conditions, the shift can be recovered using fewer samples and lesscomputation compared to the classical setup. Of particular interest is shiftestimation from Fourier coefficients. We show that under rather mild conditionsonly one Fourier coefficient suffices to recover the true shift.
arxiv-3600-108 | Exploring The Contribution of Unlabeled Data in Financial Sentiment Analysis | http://arxiv.org/pdf/1308.0658v1.pdf | author:Jimmy SJ. Ren, Wei Wang, Jiawei Wang, Stephen Shaoyi Liao category:cs.CL cs.LG published:2013-08-03 summary:With the proliferation of its applications in various industries, sentimentanalysis by using publicly available web data has become an active researcharea in text classification during these years. It is argued by researchersthat semi-supervised learning is an effective approach to this problem since itis capable to mitigate the manual labeling effort which is usually expensiveand time-consuming. However, there was a long-term debate on the effectivenessof unlabeled data in text classification. This was partially caused by the factthat many assumptions in theoretic analysis often do not hold in practice. Weargue that this problem may be further understood by adding an additionaldimension in the experiment. This allows us to address this problem in theperspective of bias and variance in a broader view. We show that the well-knownperformance degradation issue caused by unlabeled data can be reproduced as asubset of the whole scenario. We argue that if the bias-variance trade-off isto be better balanced by a more effective feature selection method unlabeleddata is very likely to boost the classification performance. We then propose afeature selection framework in which labeled and unlabeled training samples areboth considered. We discuss its potential in achieving such a balance. Besides,the application in financial sentiment analysis is chosen because it not onlyexemplifies an important application, the data possesses better illustrativepower as well. The implications of this study in text classification andfinancial sentiment analysis are both discussed.
arxiv-3600-109 | A Semi-automated Statistical Algorithm for Object Separation | http://arxiv.org/pdf/1301.0127v3.pdf | author:Madhur Srivastava, Satish K. Singh, Prasanta K. Panigrahi category:cs.CV published:2013-01-01 summary:We explicate a semi-automated statistical algorithm for object identificationand segregation in both gray scale and color images. The algorithm makesoptimal use of the observation that definite objects in an image are typicallyrepresented by pixel values having narrow Gaussian distributions aboutcharacteristic mean values. Furthermore, for visually distinct objects, thecorresponding Gaussian distributions have negligible overlap with each otherand hence the Mahalanobis distance between these distributions are large. Thesestatistical facts enable one to sub-divide images into multiple thresholds ofvariable sizes, each segregating similar objects. The procedure incorporatesthe sensitivity of human eye to the gray pixel values into the variablethreshold size, while mapping the Gaussian distributions into localized\delta-functions, for object separation. The effectiveness of this recursivestatistical algorithm is demonstrated using a wide variety of images.
arxiv-3600-110 | United Statistical Algorithm, Small and Big Data: Future OF Statistician | http://arxiv.org/pdf/1308.0641v1.pdf | author:Emanuel Parzen, Subhadeep Mukhopadhyay category:math.ST stat.ME stat.ML stat.TH published:2013-08-02 summary:This article provides the role of big idea statisticians in future of BigData Science. We describe the `United Statistical Algorithms' framework forcomprehensive uni?cation of traditional and novel statistical methods formodeling Small Data and Big Data, especially mixed data (discrete, continuous).
arxiv-3600-111 | Finite State Machine Synthesis for Evolutionary Hardware | http://arxiv.org/pdf/1307.6995v2.pdf | author:Andrey Bereza, Maksim Lyashov, Luis Blanco category:cs.NE cs.FL published:2013-07-26 summary:This article considers application of genetic algorithms for finite machinesynthesis. The resulting genetic finite state machines synthesis algorithmallows for creation of machines with less number of states and within shortertime. This makes it possible to use hardware-oriented genetic finite machinessynthesis algorithm in autonomous systems on reconfigurable platforms.
arxiv-3600-112 | Analysis of Descent-Based Image Registration | http://arxiv.org/pdf/1302.3785v2.pdf | author:Elif Vural, Pascal Frossard category:cs.CV published:2013-02-15 summary:We present a performance analysis for image registration with gradientdescent methods. We consider a typical multiscale registration setting wherethe global 2-D translation between a pair of images is estimated by smoothingthe images and minimizing the distance between them with gradient descent. Ourstudy particularly concentrates on the effect of noise and low-pass filteringon the alignment accuracy. We adopt an analytic representation for images andanalyze the well-behavedness of the image distance function by estimating theneighborhood of translations for which it is free of undesired local minima.This corresponds to the neighborhood of translation vectors that are correctlycomputable with a simple gradient descent minimization. We show that the areaof this neighborhood increases at least quadratically with the smoothing filtersize, which justifies the use of a smoothing step in image registration withlocal optimizers such as gradient descent. We then examine the effect of noiseon the alignment accuracy and derive an upper bound for the alignment error interms of the noise properties and filter size. Our main finding is that theerror increases at a rate that is at least linear with respect to the filtersize. Therefore, smoothing improves the well-behavedness of the distancefunction; however, this comes at the cost of amplifying the alignment error innoisy settings. Our results provide a mathematical insight about whyhierarchical techniques are effective in image registration, suggesting thatthe multiscale coarse-to-fine alignment strategy of these techniques is verysuitable from the perspective of the trade-off between the well-behavedness ofthe objective function and the registration accuracy. To the best of ourknowledge, this is the first such study for descent-based image registration.
arxiv-3600-113 | Bandit Market Makers | http://arxiv.org/pdf/1112.0076v4.pdf | author:Nicolas Della Penna, Mark D. Reid category:q-fin.TR cs.GT stat.ML published:2011-12-01 summary:We introduce a modular framework for market making. It combines cost-functionbased automated market makers with bandit algorithms. We obtain worst-caseprofits guarantee's relative to the best in hindsight within a class of natural"overround" cost functions . This combination allow us to havedistribution-free guarantees on the regret of profits while preserving thebounded worst-case losses and computational tractability over combinatorialspaces of the cost function based approach. We present simulation results tobetter understand the practical behaviour of market makers from the framework.
arxiv-3600-114 | Clustering-Based Matrix Factorization | http://arxiv.org/pdf/1301.6659v4.pdf | author:Nima Mirbakhsh, Charles X. Ling category:cs.LG published:2013-01-28 summary:Recommender systems are emerging technologies that nowadays can be found inmany applications such as Amazon, Netflix, and so on. These systems help usersto find relevant information, recommendations, and their preferred items.Slightly improvement of the accuracy of these recommenders can highly affectthe quality of recommendations. Matrix Factorization is a popular method inRecommendation Systems showing promising results in accuracy and complexity. Inthis paper we propose an extension of matrix factorization which adds generalneighborhood information on the recommendation model. Users and items areclustered into different categories to see how these categories sharepreferences. We then employ these shared interests of categories in a fusion byBiased Matrix Factorization to achieve more accurate recommendations. This is acomplement for the current neighborhood aware matrix factorization models whichrely on using direct neighborhood information of users and items. The proposedmodel is tested on two well-known recommendation system datasets: Movielens100kand Netflix. Our experiment shows applying the general latent features ofcategories into factorized recommender models improves the accuracy ofrecommendations. The current neighborhood-aware models need a great number ofneighbors to acheive good accuracies. To the best of our knowledge, theproposed model is better than or comparable with the current neighborhood-awaremodels when they consider fewer number of neighbors.
arxiv-3600-115 | Hybrid Focal Stereo Networks for Pattern Analysis in Homogeneous Scenes | http://arxiv.org/pdf/1308.0365v1.pdf | author:Emanuel Aldea, Khurom H. Kiyani category:cs.CV published:2013-08-01 summary:In this paper we address the problem of multiple camera calibration in thepresence of a homogeneous scene, and without the possibility of employingcalibration object based methods. The proposed solution exploits salientfeatures present in a larger field of view, but instead of employing activevision we replace the cameras with stereo rigs featuring a long focal analysiscamera, as well as a short focal registration camera. Thus, we are able topropose an accurate solution which does not require intrinsic variation modelsas in the case of zooming cameras. Moreover, the availability of the two viewssimultaneously in each rig allows for pose re-estimation between rigs as oftenas necessary. The algorithm has been successfully validated in an indoorsetting, as well as on a difficult scene featuring a highly dense pilgrim crowdin Makkah.
arxiv-3600-116 | Design and Development of an Expert System to Help Head of University Departments | http://arxiv.org/pdf/1308.0356v1.pdf | author:Shervan Fekri-Ershad, Hadi Tajalizadeh, Shahram Jafari category:cs.AI cs.LG published:2013-08-01 summary:One of the basic tasks which is responded for head of each universitydepartment, is employing lecturers based on some default factors such asexperience, evidences, qualifies and etc. In this respect, to help the heads,some automatic systems have been proposed until now using machine learningmethods, decision support systems (DSS) and etc. According to advantages anddisadvantages of the previous methods, a full automatic system is designed inthis paper using expert systems. The proposed system is included two mainsteps. In the first one, the human expert's knowledge is designed as decisiontrees. The second step is included an expert system which is evaluated usingextracted rules of these decision trees. Also, to improve the quality of theproposed system, a majority voting algorithm is proposed as post processingstep to choose the best lecturer which satisfied more expert's decision treesfor each course. The results are shown that the designed system averageaccuracy is 78.88. Low computational complexity, simplicity to program and aresome of other advantages of the proposed system.
arxiv-3600-117 | MAS for video objects segmentation and tracking based on active contours and SURF descriptor | http://arxiv.org/pdf/1308.0315v1.pdf | author:Mohamed Chakroun, Ali Wali, Adel M. Alimi category:cs.MM cs.CV published:2013-08-01 summary:In computer vision, video segmentation and tracking is an importantchallenging issue. In this paper, we describe a new video sequencessegmentation and tracking algorithm based on MAS "multi-agent systems" and SURF"Speeded Up Robust Features". Our approach consists in modelling a multi-agentsystem for segmenting the first image from a video sequence and trackingobjects in the video sequences. The used agents are supervisor and exploratoragents, they are communicating between them and they inspire in their behaviorfrom active contours approaches. The tracking of objects is based on SURFdescriptors "Speed Up Robust Features". We used the DIMA platform and "APIAteji PX" (an extension of the Java language to facilitate parallel programmingon heterogeneous architectures) to implement this algorithm. The experimentalresults indicate that the proposed algorithm is more robust and faster thanprevious approaches.
arxiv-3600-118 | Sparse Dictionary-based Attributes for Action Recognition and Summarization | http://arxiv.org/pdf/1308.0290v1.pdf | author:Qiang Qiu, Zhuolin Jiang, Rama Chellappa category:cs.CV published:2013-08-01 summary:We present an approach for dictionary learning of action attributes viainformation maximization. We unify the class distribution and appearanceinformation into an objective function for learning a sparse dictionary ofaction attributes. The objective function maximizes the mutual informationbetween what has been learned and what remains to be learned in terms ofappearance information and class distribution for each dictionary atom. Wepropose a Gaussian Process (GP) model for sparse representation to optimize thedictionary objective function. The sparse coding property allows a kernel withcompact support in GP to realize a very efficient dictionary learning process.Hence we can describe an action video by a set of compact and discriminativeaction attributes. More importantly, we can recognize modeled action categoriesin a sparse feature space, which can be generalized to unseen and unmodeledaction categories. Experimental results demonstrate the effectiveness of ourapproach in action recognition and summarization.
arxiv-3600-119 | An efficient model-free estimation of multiclass conditional probability | http://arxiv.org/pdf/1209.4951v3.pdf | author:Tu Xu, Junhui Wang category:stat.ML cs.LG stat.ME published:2012-09-22 summary:Conventional multiclass conditional probability estimation methods, such asFisher's discriminate analysis and logistic regression, often requirerestrictive distributional model assumption. In this paper, a model-freeestimation method is proposed to estimate multiclass conditional probabilitythrough a series of conditional quantile regression functions. Specifically,the conditional class probability is formulated as difference of correspondingcumulative distribution functions, where the cumulative distribution functionscan be converted from the estimated conditional quantile regression functions.The proposed estimation method is also efficient as its computation cost doesnot increase exponentially with the number of classes. The theoretical andnumerical studies demonstrate that the proposed estimation method is highlycompetitive against the existing competitors, especially when the number ofclasses is relatively large.
arxiv-3600-120 | Domain-invariant Face Recognition using Learned Low-rank Transformation | http://arxiv.org/pdf/1308.0275v1.pdf | author:Qiang Qiu, Guillermo Sapiro, Ching-Hui Chen category:cs.CV published:2013-08-01 summary:We present a low-rank transformation approach to compensate for facevariations due to changes in visual domains, such as pose and illumination. Thekey idea is to learn discriminative linear transformations for face imagesusing matrix rank as the optimization criteria. The learned lineartransformations restore a shared low-rank structure for faces from the samesubject, and, at the same time, force a high-rank structure for faces fromdifferent subjects. In this way, among the transformed faces, we reducevariations caused by domain changes within the classes, and increaseseparations between the classes for better face recognition across domains.Extensive experiments using public datasets are presented to demonstrate theeffectiveness of our approach for face recognition across domains. Thepotential of the approach for feature extraction in generic object recognitionand coded aperture design are discussed as well.
arxiv-3600-121 | Learning Robust Subspace Clustering | http://arxiv.org/pdf/1308.0273v1.pdf | author:Qiang Qiu, Guillermo Sapiro category:cs.CV published:2013-08-01 summary:We propose a low-rank transformation-learning framework to robustify subspaceclustering. Many high-dimensional data, such as face images and motionsequences, lie in a union of low-dimensional subspaces. The subspace clusteringproblem has been extensively studied in the literature to partition suchhigh-dimensional data into clusters corresponding to their underlyinglow-dimensional subspaces. However, low-dimensional intrinsic structures areoften violated for real-world observations, as they can be corrupted by errorsor deviate from ideal models. We propose to address this by learning a lineartransformation on subspaces using matrix rank, via its convex surrogate nuclearnorm, as the optimization criteria. The learned linear transformation restoresa low-rank structure for data from the same subspace, and, at the same time,forces a high-rank structure for data from different subspaces. In this way, wereduce variations within the subspaces, and increase separations between thesubspaces for more accurate subspace clustering. This proposed learned robustsubspace clustering framework significantly enhances the performance ofexisting subspace clustering methods. To exploit the low-rank structures of thetransformed subspaces, we further introduce a subspace clustering technique,called Robust Sparse Subspace Clustering, which efficiently combines robust PCAwith sparse modeling. We also discuss the online learning of thetransformation, and learning of the transformation while simultaneouslyreducing the data dimensionality. Extensive experiments using public datasetsare presented, showing that the proposed approach significantly outperformsstate-of-the-art subspace clustering methods.
arxiv-3600-122 | The blessing of transitivity in sparse and stochastic networks | http://arxiv.org/pdf/1307.2302v2.pdf | author:Karl Rohe, Tai Qin category:stat.ML published:2013-07-08 summary:The interaction between transitivity and sparsity, two common features inempirical networks, implies that there are local regions of large sparsenetworks that are dense. We call this the blessing of transitivity and it hasconsequences for both modeling and inference. Extant research suggests thatstatistical inference for the Stochastic Blockmodel is more difficult when theedges are sparse. However, this conclusion is confounded by the fact that theasymptotic limit in all of the previous studies is not merely sparse, but alsonon-transitive. To retain transitivity, the blocks cannot grow faster than theexpected degree. Thus, in sparse models, the blocks must remain asymptoticallysmall. \n Previous algorithmic research demonstrates that small "local"clusters are more amenable to computation, visualization, and interpretationwhen compared to "global" graph partitions. This paper provides the firststatistical results that demonstrate how these small transitive clusters arealso more amenable to statistical estimation. Theorem 2 shows that a "local"clustering algorithm can, with high probability, detect a transitive stochasticblock of a fixed size (e.g. 30 nodes) embedded in a large graph. The onlyconstraint on the ambient graph is that it is large and sparse--it could begenerated at random or by an adversary--suggesting a theoretical explanationfor the robust empirical performance of local clustering algorithms.
arxiv-3600-123 | A better Beta for the H measure of classification performance | http://arxiv.org/pdf/1202.2564v2.pdf | author:David J. Hand, Christoforos Anagnostopoulos category:stat.ME cs.CV stat.ML published:2012-02-12 summary:The area under the ROC curve is widely used as a measure of performance ofclassification rules. However, it has recently been shown that the measure isfundamentally incoherent, in the sense that it treats the relative severitiesof misclassifications differently when different classifiers are used. Toovercome this, Hand (2009) proposed the $H$ measure, which allows a givenresearcher to fix the distribution of relative severities to aclassifier-independent setting on a given problem. This note extends thediscussion, and proposes a modified standard distribution for the $H$ measure,which better matches the requirements of researchers, in particular those facedwith heavily unbalanced datasets, the $Beta(\pi_1+1,\pi_0+1)$ distribution.[Preprint submitted at Pattern Recognition Letters]
arxiv-3600-124 | Scoring and Searching over Bayesian Networks with Causal and Associative Priors | http://arxiv.org/pdf/1209.6561v2.pdf | author:Giorgos Borboudakis, Ioannis Tsamardinos category:cs.AI cs.LG stat.ML published:2012-09-28 summary:A significant theoretical advantage of search-and-score methods for learningBayesian Networks is that they can accept informative prior beliefs for eachpossible network, thus complementing the data. In this paper, a method ispresented for assigning priors based on beliefs on the presence or absence ofcertain paths in the true network. Such beliefs correspond to knowledge aboutthe possible causal and associative relations between pairs of variables. Thistype of knowledge naturally arises from prior experimental and observationaldata, among others. In addition, a novel search-operator is proposed to takeadvantage of such prior knowledge. Experiments show that, using path beliefsimproves the learning of the skeleton, as well as the edge directions in thenetwork.
arxiv-3600-125 | Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ) | http://arxiv.org/pdf/1307.8430v1.pdf | author:Bryan R. Conroy, Jennifer M. Walz, Brian Cheung, Paul Sajda category:cs.LG stat.ML published:2013-07-31 summary:We present an efficient algorithm for simultaneously training sparsegeneralized linear models across many related problems, which may arise frombootstrapping, cross-validation and nonparametric permutation testing. Ourapproach leverages the redundancies across problems to obtain significantcomputational improvements relative to solving the problems sequentially by aconventional algorithm. We demonstrate our fast simultaneous training ofgeneralized linear models (FaSTGLZ) algorithm on a number of real-worlddatasets, and we run otherwise computationally intensive bootstrapping andpermutation test analyses that are typically necessary for obtainingstatistically rigorous classification results and meaningful interpretation.Code is freely available at http://liinc.bme.columbia.edu/fastglz.
arxiv-3600-126 | Who and Where: People and Location Co-Clustering | http://arxiv.org/pdf/1307.8405v1.pdf | author:Zixuan Wang, Jinyun Yan category:cs.CV published:2013-07-31 summary:In this paper, we consider the clustering problem on images where each imagecontains patches in people and location domains. We exploit the correlationbetween people and location domains, and proposed a semi-supervisedco-clustering algorithm to cluster images. Our algorithm updates thecorrelation links at the runtime, and produces clustering in both domainssimultaneously. We conduct experiments in a manually collected dataset and aFlickr dataset. The result shows that the such correlation improves theclustering performance.
arxiv-3600-127 | The Planning-ahead SMO Algorithm | http://arxiv.org/pdf/1307.8305v1.pdf | author:Tobias Glasmachers category:cs.LG published:2013-07-31 summary:The sequential minimal optimization (SMO) algorithm and variants thereof arethe de facto standard method for solving large quadratic programs for supportvector machine (SVM) training. In this paper we propose a simple yet powerfulmodification. The main emphasis is on an algorithm improving the SMO step sizeby planning-ahead. The theoretical analysis ensures its convergence to theoptimum. Experiments involving a large number of datasets were carried out todemonstrate the superiority of the new algorithm.
arxiv-3600-128 | Tracking Extrema in Dynamic Environment using Multi-Swarm Cellular PSO with Local Search | http://arxiv.org/pdf/1307.8279v1.pdf | author:Somayeh Nabizadeh, Alireza Rezvanian, Mohammad Reza Meybodi category:cs.AI cs.NE published:2013-07-31 summary:Many real-world phenomena can be modelled as dynamic optimization problems.In such cases, the environment problem changes dynamically and therefore,conventional methods are not capable of dealing with such problems. In thispaper, a novel multi-swarm cellular particle swarm optimization algorithm isproposed by clustering and local search. In the proposed algorithm, the searchspace is partitioned into cells, while the particles identify changes in thesearch space and form clusters to create sub-swarms. Then a local search isapplied to improve the solutions in the each cell. Simulation results forstatic standard benchmarks and dynamic environments show superiority of theproposed method over other alternative approaches.
arxiv-3600-129 | A Prototyping Environment for Integrated Artificial Attention Systems | http://arxiv.org/pdf/1307.8233v1.pdf | author:Jan Tünnermann, Markus Hennig, Michael Silbernagel, Bärbel Mertsching category:cs.CV published:2013-07-31 summary:Artificial visual attention systems aim to support technical systems invisual tasks by applying the concepts of selective attention observed in humansand other animals. Such systems are typically evaluated against ground truthobtained from human gaze-data or manually annotated test images. When appliedto robotics, the systems are required to be adaptable to the target system.Here, we describe a flexible environment based on a robotic middleware layerallowing the development and testing of attention-guided vision systems. Insuch a framework, the systems can be tested with input from various sources,different attention algorithms at the core, and diverse subsequent tasks.
arxiv-3600-130 | A Novel Architecture for Relevant Blog Page Identifcation | http://arxiv.org/pdf/1307.8225v1.pdf | author:Deepti Kapri, Rosy Madaan, A. K Sharma, Ashutosh Dixit category:cs.IR cs.CL published:2013-07-31 summary:Blogs are undoubtedly the richest source of information available incyberspace. Blogs can be of various natures i.e. personal blogs which containposts on mixed issues or blogs can be domain specific which contains posts onparticular topics, this is the reason, they offer wide variety of relevantinformation which is often focused. A general search engine gives back a hugecollection of web pages which may or may not give correct answers, as web isthe repository of information of all kinds and a user has to go through variousdocuments before he gets what he was originally looking for, which is a verytime consuming process. So, the search can be made more focused and accurate ifit is limited to blogosphere instead of web pages. The reason being that theblogs are more focused in terms of information. So, User will only get relatedblogs in response to his query. These results will be then ranked according toour proposed method and are finally presented in front of user in descendingorder
arxiv-3600-131 | DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering | http://arxiv.org/pdf/1307.8136v1.pdf | author:Brian P. Kent, Alessandro Rinaldo, Timothy Verstynen category:stat.ME cs.LG stat.ML published:2013-07-30 summary:The level set tree approach of Hartigan (1975) provides a probabilisticallybased and highly interpretable encoding of the clustering behavior of adataset. By representing the hierarchy of data modes as a dendrogram of thelevel sets of a density estimator, this approach offers many advantages forexploratory analysis and clustering, especially for complex andhigh-dimensional data. Several R packages exist for level set tree estimation,but their practical usefulness is limited by computational inefficiency,absence of interactive graphical capabilities and, from a theoreticalperspective, reliance on asymptotic approximations. To make it easier forpractitioners to capture the advantages of level set trees, we have written thePython package DeBaCl for DEnsity-BAsed CLustering. In this article weillustrate how DeBaCl's level set tree estimates can be used for difficultclustering tasks and interactive graphical data analysis. The package isintended to promote the practical use of level set trees through improvementsin computational efficiency and a high degree of user customization. Inaddition, the flexible algorithms implemented in DeBaCl enjoy finite sampleaccuracy, as demonstrated in recent literature on density clustering. Finally,we show the level set tree framework can be easily extended to deal withfunctional data.
arxiv-3600-132 | Neural Network Capacity for Multilevel Inputs | http://arxiv.org/pdf/1307.8104v1.pdf | author:Matt Stowe, Subhash Kak category:cs.NE published:2013-07-30 summary:This paper examines the memory capacity of generalized neural networks.Hopfield networks trained with a variety of learning techniques areinvestigated for their capacity both for binary and non-binary alphabets. It isshown that the capacity can be much increased when multilevel inputs are used.New learning strategies are proposed to increase Hopfield network capacity, andthe scalability of these methods is also examined in respect to size of thenetwork. The ability to recall entire patterns from stimulation of a singleneuron is examined for the increased capacity networks.
arxiv-3600-133 | Extracting Information-rich Part of Texts using Text Denoising | http://arxiv.org/pdf/1307.8060v1.pdf | author:Rushdi Shams category:cs.IR cs.CL published:2013-07-30 summary:The aim of this paper is to report on a novel text reduction technique,called Text Denoising, that highlights information-rich content when processinga large volume of text data, especially from the biomedical domain. The corefeature of the technique, the text readability index, embodies the hypothesisthat complex text is more information-rich than the rest. When applied on taskslike biomedical relation bearing text extraction, keyphrase indexing andextracting sentences describing protein interactions, it is evident that thereduced set of text produced by text denoising is more information-rich thanthe rest.
arxiv-3600-134 | Extracting Connected Concepts from Biomedical Texts using Fog Index | http://arxiv.org/pdf/1307.8057v1.pdf | author:Rushdi Shams, Robert E. Mercer category:cs.CL cs.IR published:2013-07-30 summary:In this paper, we establish Fog Index (FI) as a text filter to locate thesentences in texts that contain connected biomedical concepts of interest. Todo so, we have used 24 random papers each containing four pairs of connectedconcepts. For each pair, we categorize sentences based on whether they containboth, any or none of the concepts. We then use FI to measure difficulty of thesentences of each category and find that sentences containing both of theconcepts have low readability. We rank sentences of a text according to theirFI and select 30 percent of the most difficult sentences. We use an associationmatrix to track the most frequent pairs of concepts in them. This matrixreports that the first filter produces some pairs that hold almost noconnections. To remove these unwanted pairs, we use the Equally WeightedHarmonic Mean of their Positive Predictive Value (PPV) and Sensitivity as asecond filter. Experimental results demonstrate the effectiveness of ourmethod.
arxiv-3600-135 | Optimistic Concurrency Control for Distributed Unsupervised Learning | http://arxiv.org/pdf/1307.8049v1.pdf | author:Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick, Michael I. Jordan category:cs.LG cs.AI cs.DC published:2013-07-30 summary:Research on distributed machine learning algorithms has focused primarily onone of two extremes - algorithms that obey strict concurrency constraints oralgorithms that obey few or no such constraints. We consider an intermediatealternative in which algorithms optimistically assume that conflicts areunlikely and if conflicts do arise a conflict-resolution protocol is invoked.We view this "optimistic concurrency control" paradigm as particularlyappropriate for large-scale machine learning algorithms, particularly in theunsupervised setting. We demonstrate our approach in three problem areas:clustering, feature learning and online facility location. We evaluate ourmethods via large-scale experiments in a cluster computing environment.
arxiv-3600-136 | A Study on Classification in Imbalanced and Partially-Labelled Data Streams | http://arxiv.org/pdf/1307.8012v1.pdf | author:R. J. Lyon, J. M. Brooke, J. D. Knowles, B. W. Stappers category:astro-ph.IM cs.LG published:2013-07-30 summary:The domain of radio astronomy is currently facing significant computationalchallenges, foremost amongst which are those posed by the development of theworld's largest radio telescope, the Square Kilometre Array (SKA). Preliminaryspecifications for this instrument suggest that the final design willincorporate between 2000 and 3000 individual 15 metre receiving dishes, whichtogether can be expected to produce a data rate of many TB/s. Given such a highdata rate, it becomes crucial to consider how this information will beprocessed and stored to maximise its scientific utility. In this paper, weconsider one possible data processing scenario for the SKA, for the purposes ofan all-sky pulsar survey. In particular we treat the selection of promisingsignals from the SKA processing pipeline as a data stream classificationproblem. We consider the feasibility of classifying signals that arrive via anunlabelled and heavily class imbalanced data stream, using currently availablealgorithms and frameworks. Our results indicate that existing stream learnersexhibit unacceptably low recall on real astronomical data when used in standardconfiguration; however, good false positive performance and comparable accuracyto static learners, suggests they have definite potential as an on-linesolution to this particular big data challenge.
arxiv-3600-137 | Sharp Threshold for Multivariate Multi-Response Linear Regression via Block Regularized Lasso | http://arxiv.org/pdf/1307.7993v1.pdf | author:Weiguang Wang, Yingbin Liang, Eric P. Xing category:cs.LG stat.ML published:2013-07-30 summary:In this paper, we investigate a multivariate multi-response (MVMR) linearregression problem, which contains multiple linear regression models withdifferently distributed design matrices, and different regression and outputvectors. The goal is to recover the support union of all regression vectorsusing $l_1/l_2$-regularized Lasso. We characterize sufficient and necessaryconditions on sample complexity \emph{as a sharp threshold} to guaranteesuccessful recovery of the support union. Namely, if the sample size is abovethe threshold, then $l_1/l_2$-regularized Lasso correctly recovers the supportunion; and if the sample size is below the threshold, $l_1/l_2$-regularizedLasso fails to recover the support union. In particular, the thresholdprecisely captures the impact of the sparsity of regression vectors and thestatistical properties of the design matrices on sample complexity. Therefore,the threshold function also captures the advantages of joint support unionrecovery using multi-task Lasso over individual support recovery usingsingle-task Lasso.
arxiv-3600-138 | Likelihood-ratio calibration using prior-weighted proper scoring rules | http://arxiv.org/pdf/1307.7981v1.pdf | author:Niko Brümmer, George Doddington category:stat.ML cs.LG published:2013-07-30 summary:Prior-weighted logistic regression has become a standard tool for calibrationin speaker recognition. Logistic regression is the optimization of the expectedvalue of the logarithmic scoring rule. We generalize this via a parametricfamily of proper scoring rules. Our theoretical analysis shows how differentmembers of this family induce different relative weightings over a spectrum ofapplications of which the decision thresholds range from low to high. Specialattention is given to the interaction between prior weighting and properscoring rule parameters. Experiments on NIST SRE'12 suggest that forapplications with low false-alarm rate requirements, scoring rules tailored toemphasize higher score thresholds may give better accuracy than logisticregression.
arxiv-3600-139 | Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction | http://arxiv.org/pdf/1307.7973v1.pdf | author:Jason Weston, Antoine Bordes, Oksana Yakhnenko, Nicolas Usunier category:cs.CL cs.IR cs.LG published:2013-07-30 summary:This paper proposes a novel approach for relation extraction from free textwhich is trained to jointly use information from the text and from existingknowledge. Our model is based on two scoring functions that operate by learninglow-dimensional embeddings of words and of entities and relationships from aknowledge base. We empirically show on New York Times articles aligned withFreebase relations that our approach is able to efficiently use the extrainformation provided by a large subset of Freebase data (4M entities, 23krelationships) to improve over existing methods that rely on text featuresalone.
arxiv-3600-140 | On the accuracy of the Viterbi alignment | http://arxiv.org/pdf/1307.7948v1.pdf | author:Kristi Kuljus, Jüri Lember category:stat.ME cs.LG stat.CO published:2013-07-30 summary:In a hidden Markov model, the underlying Markov chain is usually hidden.Often, the maximum likelihood alignment (Viterbi alignment) is used as itsestimate. Although having the biggest likelihood, the Viterbi alignment canbehave very untypically by passing states that are at most unexpected. To avoidsuch situations, the Viterbi alignment can be modified by forcing it not topass these states. In this article, an iterative procedure for improving theViterbi alignment is proposed and studied. The iterative approach is comparedwith a simple bunch approach where a number of states with low probability areall replaced at the same time. It can be seen that the iterative way ofadjusting the Viterbi alignment is more efficient and it has several advantagesover the bunch approach. The same iterative algorithm for improving the Viterbialignment can be used in the case of peeping, that is when it is possible toreveal hidden states. In addition, lower bounds for classificationprobabilities of the Viterbi alignment under different conditions on the modelparameters are studied.
arxiv-3600-141 | Energy Distribution of EEG Signals: EEG Signal Wavelet-Neural Network Classifier | http://arxiv.org/pdf/1307.7897v1.pdf | author:Ibrahim Omerhodzic, Samir Avdakovic, Amir Nuhanovic, Kemal Dizdarevic category:cs.NE q-bio.NC published:2013-07-30 summary:In this paper, a wavelet-based neural network (WNN) classifier forrecognizing EEG signals is implemented and tested under three sets EEG signals(healthy subjects, patients with epilepsy and patients with epileptic syndromeduring the seizure). First, the Discrete Wavelet Transform (DWT) with theMulti-Resolution Analysis (MRA) is applied to decompose EEG signal atresolution levels of the components of the EEG signal (delta, theta, alpha,beta and gamma) and the Parsevals theorem are employed to extract thepercentage distribution of energy features of the EEG signal at differentresolution levels. Second, the neural network (NN) classifies these extractedfeatures to identify the EEGs type according to the percentage distribution ofenergy features. The performance of the proposed algorithm has been evaluatedusing in total 300 EEG signals. The results showed that the proposed classifierhas the ability of recognizing and classifying EEG signals efficiently.
arxiv-3600-142 | 6th International Symposium on Attention in Cognitive Systems 2013 | http://arxiv.org/pdf/1307.6170v2.pdf | author:Lucas Paletta, Laurent Itti, Björn Schuller, Fang Fang category:cs.CV published:2013-07-22 summary:This volume contains the papers accepted at the 6th International Symposiumon Attention in Cognitive Systems (ISACS 2013), held in Beijing, August 5,2013. The aim of this symposium is to highlight the central role of attentionon various kinds of performance in cognitive systems processing. It bringstogether researchers and developers from both academia and industry, fromcomputer vision, robotics, perception psychology, psychophysics andneuroscience, in order to provide an interdisciplinary forum to present andcommunicate on computational models of attention, with the focus oninterdependencies with visual cognition. Furthermore, it intends to investigaterelevant objectives for performance comparison, to document and to investigatepromising application domains, and to discuss visual attention with referenceto other aspects of AI enabled systems.
arxiv-3600-143 | Scalable $k$-NN graph construction | http://arxiv.org/pdf/1307.7852v1.pdf | author:Jingdong Wang, Jing Wang, Gang Zeng, Zhuowen Tu, Rui Gan, Shipeng Li category:cs.CV cs.LG stat.ML published:2013-07-30 summary:The $k$-NN graph has played a central role in increasingly populardata-driven techniques for various learning and vision tasks; yet, finding anefficient and effective way to construct $k$-NN graphs remains a challenge,especially for large-scale high-dimensional data. In this paper, we propose anew approach to construct approximate $k$-NN graphs with emphasis in:efficiency and accuracy. We hierarchically and randomly divide the data pointsinto subsets and build an exact neighborhood graph over each subset, achievinga base approximate neighborhood graph; we then repeat this process for severaltimes to generate multiple neighborhood graphs, which are combined to yield amore accurate approximate neighborhood graph. Furthermore, we propose aneighborhood propagation scheme to further enhance the accuracy. We show boththeoretical and empirical accuracy and efficiency of our approach to $k$-NNgraph construction and demonstrate significant speed-up in dealing with largescale visual data.
arxiv-3600-144 | Hybrid Affinity Propagation | http://arxiv.org/pdf/1307.7851v1.pdf | author:Jingdong Wang, Hao Xu, Xian-Sheng Hua, Shipeng Li category:cs.CV published:2013-07-30 summary:In this paper, we address a problem of managing tagged images with hybridsummarization. We formulate this problem as finding a few image exemplars torepresent the image set semantically and visually, and solve it in a hybrid wayby exploiting both visual and textual information associated with images. Wepropose a novel approach, called homogeneous and heterogeneous messagepropagation ($\text{H}^\text{2}\text{MP}$). Similar to the affinity propagation(AP) approach, $\text{H}^\text{2}\text{MP}$ reduce the conventional\emph{vector} message propagation to \emph{scalar} message propagation to makethe algorithm more efficient. Beyond AP that can only handle homogeneous data,$\text{H}^\text{2}\text{MP}$ generalizes it to exploit extra heterogeneousrelations and the generalization is non-trivial as the reduction to scalarmessages from vector messages is more challenging. The main advantages of ourapproach lie in 1) that $\text{H}^\text{2}\text{MP}$ exploits visual similarityand in addition the useful information from the associated tags, including theassociations relation between images and tags and the relations within tags,and 2) that the summary is both visually and semantically satisfactory. Inaddition, our approach can also present a textual summary to a tagged imagecollection, which can be used to automatically generate a textual description.The experimental results demonstrate the effectiveness and efficiency of theroposed approach.
arxiv-3600-145 | An Integrated System for 3D Gaze Recovery and Semantic Analysis of Human Attention | http://arxiv.org/pdf/1307.7848v1.pdf | author:Lucas Paletta, Katrin Santner, Gerald Fritz category:cs.CV published:2013-07-30 summary:This work describes a computer vision system that enables pervasive mappingand monitoring of human attention. The key contribution is that our methodologyenables full 3D recovery of the gaze pointer, human view frustum and associatedhuman centered measurements directly into an automatically computed 3D model inreal-time. We apply RGB-D SLAM and descriptor matching methodologies for the 3Dmodeling, localization and fully automated annotation of ROIs (regions ofinterest) within the acquired 3D model. This innovative methodology will opennew avenues for attention studies in real world environments, bringing newpotential into automated processing for human factors technologies.
arxiv-3600-146 | Efficient Energy Minimization for Enforcing Statistics | http://arxiv.org/pdf/1307.7800v1.pdf | author:Yongsub Lim, Kyomin Jung, Pushmeet Kohli category:cs.CV published:2013-07-30 summary:Energy minimization algorithms, such as graph cuts, enable the computation ofthe MAP solution under certain probabilistic models such as Markov randomfields. However, for many computer vision problems, the MAP solution under themodel is not the ground truth solution. In many problem scenarios, the systemhas access to certain statistics of the ground truth. For instance, in imagesegmentation, the area and boundary length of the object may be known. In thesecases, we want to estimate the most probable solution that is consistent withsuch statistics, i.e., satisfies certain equality or inequality constraints. The above constrained energy minimization problem is NP-hard in general, andis usually solved using Linear Programming formulations, which relax theintegrality constraints. This paper proposes a novel method that finds thediscrete optimal solution of such problems by maximizing the correspondingLagrangian dual. This method can be applied to any constrained energyminimization problem whose unconstrained version is polynomial time solvable,and can handle multiple, equality or inequality, and linear or non-linearconstraints. We demonstrate the efficacy of our method on theforeground/background image segmentation problem, and show that it producesimpressive segmentation results with less error, and runs more than 20 timesfaster than the state-of-the-art LP relaxation based approaches.
arxiv-3600-147 | Protein (Multi-)Location Prediction: Using Location Inter-Dependencies in a Probabilistic Framework | http://arxiv.org/pdf/1307.7795v1.pdf | author:Ramanuja Simha, Hagit Shatkay category:q-bio.QM cs.CE cs.LG q-bio.GN published:2013-07-30 summary:Knowing the location of a protein within the cell is important forunderstanding its function, role in biological processes, and potential use asa drug target. Much progress has been made in developing computational methodsthat predict single locations for proteins, assuming that proteins localize toa single location. However, it has been shown that proteins localize tomultiple locations. While a few recent systems have attempted to predictmultiple locations of proteins, they typically treat locations as independentor capture inter-dependencies by treating each locations-combination present inthe training set as an individual location-class. We present a new method and apreliminary system we have developed that directly incorporatesinter-dependencies among locations into the multiple-location-predictionprocess, using a collection of Bayesian network classifiers. We evaluate oursystem on a dataset of single- and multi-localized proteins. Our results,obtained by incorporating inter-dependencies are significantly higher thanthose obtained by classifiers that do not use inter-dependencies. Theperformance of our system on multi-localized proteins is comparable to a topperforming system (YLoc+), without restricting predictions to be based only onlocation-combinations present in the training set.
arxiv-3600-148 | Multi-dimensional Parametric Mincuts for Constrained MAP Inference | http://arxiv.org/pdf/1307.7793v1.pdf | author:Yongsub Lim, Kyomin Jung, Pushmeet Kohli category:cs.LG cs.AI published:2013-07-30 summary:In this paper, we propose novel algorithms for inferring the Maximum aPosteriori (MAP) solution of discrete pairwise random field models undermultiple constraints. We show how this constrained discrete optimizationproblem can be formulated as a multi-dimensional parametric mincut problem viaits Lagrangian dual, and prove that our algorithm isolates all constraintinstances for which the problem can be solved exactly. These multiple solutionsenable us to even deal with `soft constraints' (higher order penaltyfunctions). Moreover, we propose two practical variants of our algorithm tosolve problems with hard constraints. We also show how our method can beapplied to solve various constrained discrete optimization problems such assubmodular minimization and shortest path computation. Experimental evaluationusing the foreground-background image segmentation problem with statisticconstraints reveals that our method is faster and its results are closer to theground truth labellings compared with the popular continuous relaxation basedmethods.
arxiv-3600-149 | Group Iterative Spectrum Thresholding for Super-Resolution Sparse Spectral Selection | http://arxiv.org/pdf/1207.6684v2.pdf | author:Yiyuan She, Huanghuang Li, Jiangping Wang, Dapeng Wu category:stat.ML published:2012-07-28 summary:Recently, sparsity-based algorithms are proposed for super-resolutionspectrum estimation. However, to achieve adequately high resolution inreal-world signal analysis, the dictionary atoms have to be close to each otherin frequency, thereby resulting in a coherent design. The popular convexcompressed sensing methods break down in presence of high coherence and largenoise. We propose a new regularization approach to handle model collinearityand obtain parsimonious frequency selection simultaneously. It takes advantageof the pairing structure of sine and cosine atoms in the frequency dictionary.A probabilistic spectrum screening is also developed for fast computation inhigh dimensions. A data-resampling version of high-dimensional BayesianInformation Criterion is used to determine the regularization parameters.Experiments show the efficacy and efficiency of the proposed algorithms inchallenging situations with small sample size, high frequency resolution, andlow signal-to-noise ratio.
arxiv-3600-150 | Sample Distortion for Compressed Imaging | http://arxiv.org/pdf/1303.5492v2.pdf | author:Chunli Guo, Mike E. Davies category:cs.CV cs.IT math.IT published:2013-03-22 summary:We propose the notion of a sample distortion (SD) function for independentand identically distributed (i.i.d) compressive distributions to fundamentallyquantify the achievable reconstruction performance of compressed sensing forcertain encoder-decoder pairs at a given sampling ratio. Two lower bounds onthe achievable performance and the intrinsic convexity property is derived. Azeroing procedure is then introduced to improve non convex SD functions. The SDframework is then applied to analyse compressed imaging with a multi-resolutionstatistical image model using both the generalized Gaussian distribution andthe two-state Gaussian mixture distribution. We subsequently focus on theGaussian encoder-Bayesian optimal approximate message passing (AMP) decoderpair, whose theoretical SD function is provided by the rigorous analysis of theAMP algorithm. Given the image statistics, analytic bandwise sample allocationfor bandwise independent model is derived as a reverse water-filling scheme.Som and Schniter's turbo message passing approach is further deployed tointegrate the bandwise sampling with the exploitation of the hidden Markov treestructure of wavelet coefficients. Natural image simulations confirm that withoracle image statistics, the SD function associated with the optimized sampleallocation can accurately predict the possible compressed sensing gains.Finally, a general sample allocation profile based on average image statisticsnot only illustrates preferable performance but also makes the schemepractical.
arxiv-3600-151 | Tight Lower Bounds for Homology Inference | http://arxiv.org/pdf/1307.7666v1.pdf | author:Sivaraman Balakrishnan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman category:stat.ML cs.CG math.ST stat.TH published:2013-07-29 summary:The homology groups of a manifold are important topological invariants thatprovide an algebraic summary of the manifold. These groups contain richtopological information, for instance, about the connected components, holes,tunnels and sometimes the dimension of the manifold. In earlier work, we haveconsidered the statistical problem of estimating the homology of a manifoldfrom noiseless samples and from noisy samples under several different noisemodels. We derived upper and lower bounds on the minimax risk for this problem.In this note we revisit the noiseless case. In previous work we used Le Cam'slemma to establish a lower bound that differed from the upper bound of Niyogi,Smale and Weinberger by a polynomial factor in the condition number. In this note we use a different construction based on the direct analysis ofthe likelihood ratio test to show that the upper bound of Niyogi, Smale andWeinberger is in fact tight, thus establishing rate optimal asymptotic minimaxbounds for the problem. The techniques we use here extend in a straightforwardway to the noisy settings considered in our earlier work.
arxiv-3600-152 | Automatic Mammogram image Breast Region Extraction and Removal of Pectoral Muscle | http://arxiv.org/pdf/1307.7474v1.pdf | author:R. Subash Chandra Boss, K. Thangavel, D. Arul Pon Daniel category:cs.CV published:2013-07-29 summary:Currently Mammography is a most effective imaging modality used byradiologists for the screening of breast cancer. Finding an accurate, robustand efficient breast region segmentation technique still remains a challengingproblem in digital mammography. Extraction of the breast profile region and theremoval of pectoral muscle are essential pre-processing steps in Computer AidedDiagnosis (CAD) system for the diagnosis of breast cancer. Primarily it allowsthe search for abnormalities to be limited to the region of the breast tissuewithout undue influence from the background of the mammogram. The presence ofpectoral muscle in mammograms biases detection procedures, which recommendsremoving the pectoral muscle during mammogram image pre-processing. Thepresence of pectoral muscle in mammograms may disturb or influence thedetection of breast cancer as the pectoral muscle and mammographic parenchymasappear similar. The goal of breast region extraction is reducing the image sizewithout losing anatomic information, it improve the accuracy of the overall CADsystem. The main objective of this study is to propose an automated method toidentify the pectoral muscle in Medio-Lateral Oblique (MLO) view mammograms. Inthis paper, we proposed histogram based 8-neighborhood connected componentlabelling method for breast region extraction and removal of pectoral muscle.The proposed method is evaluated by using the mean values of accuracy anderror. The comparative analysis shows that the proposed method identifies thebreast region more accurately.
arxiv-3600-153 | Integration of 3D Object Recognition and Planning for Robotic Manipulation: A Preliminary Report | http://arxiv.org/pdf/1307.7466v1.pdf | author:Damien Jade Duff, Esra Erdem, Volkan Patoglu category:cs.AI cs.CV cs.RO published:2013-07-29 summary:We investigate different approaches to integrating object recognition andplanning in a tabletop manipulation domain with the set of objects used in the2012 RoboCup@Work competition. Results of our preliminary experiments showthat, with some approaches, close integration of perception and planningimproves the quality of plans, as well as the computation times of feasibleplans.
arxiv-3600-154 | ParceLiNGAM: A causal ordering method robust against latent confounders | http://arxiv.org/pdf/1303.7410v2.pdf | author:Tatsuya Tashiro, Shohei Shimizu, Aapo Hyvarinen, Takashi Washio category:stat.ML published:2013-03-29 summary:We consider learning a causal ordering of variables in a linear non-Gaussianacyclic model called LiNGAM. Several existing methods have been shown toconsistently estimate a causal ordering assuming that all the model assumptionsare correct. But, the estimation results could be distorted if some assumptionsactually are violated. In this paper, we propose a new algorithm for learningcausal orders that is robust against one typical violation of the modelassumptions: latent confounders. The key idea is to detect latent confoundersby testing independence between estimated external influences and find subsets(parcels) that include variables that are not affected by latent confounders.We demonstrate the effectiveness of our method using artificial data andsimulated brain imaging data.
arxiv-3600-155 | Borel Isomorphic Dimensionality Reduction of Data and Supervised Learning | http://arxiv.org/pdf/1307.8333v1.pdf | author:Stan Hatko category:stat.ML published:2013-07-29 summary:In this project we further investigate the idea of reducing thedimensionality of datasets using a Borel isomorphism with the purpose ofsubsequently applying supervised learning algorithms, as originally suggestedby my supervisor V. Pestov (in 2011 Dagstuhl preprint). Any consistent learningalgorithm, for example kNN, retains universal consistency after a Borelisomorphism is applied. A series of concrete examples of Borel isomorphismsthat reduce the number of dimensions in a dataset is provided, based onmultiplying the data by orthogonal matrices before the dimensionality reducingBorel isomorphism is applied. We test the accuracy of the resulting classifierin a lower dimensional space with various data sets. Working with a phonemevoice recognition dataset, of dimension 256 with 5 classes (phonemes), we showthat a Borel isomorphic reduction to dimension 16 leads to a minimal drop inaccuracy. In conclusion, we discuss further prospects of the method.
arxiv-3600-156 | A new approach in dynamic traveling salesman problem: a hybrid of ant colony optimization and descending gradient | http://arxiv.org/pdf/1307.7435v1.pdf | author:Farhad Soleimanian Gharehchopogh, Isa Maleki, Seyyed Reza Khaze category:cs.NE published:2013-07-29 summary:Nowadays swarm intelligence-based algorithms are being used widely tooptimize the dynamic traveling salesman problem (DTSP). In this paper, we haveused mixed method of Ant Colony Optimization (AOC)and gradient descent tooptimize DTSP which differs with ACO algorithm in evaporation rate andinnovative data. This approach prevents premature convergence and scape fromlocal optimum spots and also makes it possible to find better solutions foralgorithm. In this paper, we are going to offer gradient descent and ACOalgorithm which in comparison to some former methods it shows that algorithmhas significantly improved routes optimization.
arxiv-3600-157 | Data mining application for cyber space users tendency in blog writing: a case study | http://arxiv.org/pdf/1307.7432v1.pdf | author:Farhad Soleimanian Gharehchopogh, Seyyed Reza Khaze category:cs.CY cs.LG published:2013-07-29 summary:Blogs are the recent emerging media which relies on information technologyand technological advance. Since the mass media in some less-developed anddeveloping countries are in government service and their policies are developedbased on governmental interests, so blogs are provided for ideas and exchangingopinions. In this paper, we highlighted performed simulations from obtainedinformation from 100 users and bloggers in Kohkiloye and Boyer Ahmad Provinceand using Weka 3.6 tool and c4.5 algorithm by applying decision tree with morethan %82 precision for getting future tendency anticipation of users toblogging and using in strategically areas.
arxiv-3600-158 | Participation anticipating in elections using data mining methods | http://arxiv.org/pdf/1307.7429v1.pdf | author:Amin Babazadeh Sangar, Seyyed Reza Khaze, Laya Ebrahimi category:cs.CY cs.LG published:2013-07-29 summary:Anticipating the political behavior of people will be considerable help forelection candidates to assess the possibility of their success and to beacknowledged about the public motivations to select them. In this paper, weprovide a general schematic of the architecture of participation anticipatingsystem in presidential election by using KNN, Classification Tree and Na\"iveBayes and tools orange based on crisp which had hopeful output. To test andassess the proposed model, we begin to use the case study by selecting 100qualified persons who attend in 11th presidential election of Islamic republicof Iran and anticipate their participation in Kohkiloye & Boyerahmad. Weindicate that KNN can perform anticipation and classification processes withhigh accuracy in compared with two other algorithms to anticipateparticipation.
arxiv-3600-159 | Accelerated Time-of-Flight Mass Spectrometry | http://arxiv.org/pdf/1212.4269v2.pdf | author:Morteza Ibrahimi, Andrea Montanari, George S Moore category:math.OC cs.CE stat.ML published:2012-12-18 summary:We study a simple modification to the conventional time of flight massspectrometry (TOFMS) where a \emph{variable} and (pseudo)-\emph{random} pulsingrate is used which allows for traces from different pulses to overlap. Thismodification requires little alteration to the currently employed hardware.However, it requires a reconstruction method to recover the spectrum fromhighly aliased traces. We propose and demonstrate an efficient algorithm thatcan process massive TOFMS data using computational resources that can beconsidered modest with today's standards. This approach can be used to improveduty cycle, speed, and mass resolving power of TOFMS at the same time. Weexpect this to extend the applicability of TOFMS to new domains.
arxiv-3600-160 | Learning Frames from Text with an Unsupervised Latent Variable Model | http://arxiv.org/pdf/1307.7382v1.pdf | author:Brendan O'Connor category:cs.CL published:2013-07-28 summary:We develop a probabilistic latent-variable model to discover semanticframes---types of events and their participants---from corpora. We present aDirichlet-multinomial model in which frames are latent categories that explainthe linking of verb-subject-object triples, given document-level sparsity. Weanalyze what the model learns, and compare it to FrameNet, noting it learnssome novel and interesting frames. This document also contains a discussion ofinference issues, including concentration parameter learning; and a small-scaleerror analysis of syntactic parsing accuracy.
arxiv-3600-161 | Learning to Understand by Evolving Theories | http://arxiv.org/pdf/1307.7303v1.pdf | author:Martin E. Mueller, Madhura D. Thosar category:cs.LG cs.AI published:2013-07-27 summary:In this paper, we describe an approach that enables an autonomous system toinfer the semantics of a command (i.e. a symbol sequence representing anaction) in terms of the relations between changes in the observations and theaction instances. We present a method of how to induce a theory (i.e. asemantic description) of the meaning of a command in terms of a minimal set ofbackground knowledge. The only thing we have is a sequence of observations fromwhich we extract what kinds of effects were caused by performing the command.This way, we yield a description of the semantics of the action and, hence, adefinition.
arxiv-3600-162 | Counterfactual Reasoning and Learning Systems | http://arxiv.org/pdf/1209.2355v5.pdf | author:Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson category:cs.LG cs.AI cs.IR math.ST stat.TH published:2012-09-11 summary:This work shows how to leverage causal inference to understand the behaviorof complex learning systems interacting with their environment and predict theconsequences of changes to the system. Such predictions allow both humans andalgorithms to select changes that improve both the short-term and long-termperformance of such systems. This work is illustrated by experiments carriedout on the ad placement system associated with the Bing search engine.
arxiv-3600-163 | A Review of Machine Learning based Anomaly Detection Techniques | http://arxiv.org/pdf/1307.7286v1.pdf | author:Harjinder Kaur, Gurpreet Singh, Jaspreet Minhas category:cs.LG cs.CR published:2013-07-27 summary:Intrusion detection is so much popular since the last two decades whereintrusion is attempted to break into or misuse the system. It is mainly of twotypes based on the intrusions, first is Misuse or signature based detection andthe other is Anomaly detection. In this paper Machine learning based methodswhich are one of the types of Anomaly detection techniques is discussed.
arxiv-3600-164 | Self-Learning for Player Localization in Sports Video | http://arxiv.org/pdf/1307.7198v1.pdf | author:Kenji Okuma, David G. Lowe, James J. Little category:cs.CV cs.AI published:2013-07-27 summary:This paper introduces a novel self-learning framework that automates thelabel acquisition process for improving models for detecting players inbroadcast footage of sports games. Unlike most previous self-learningapproaches for improving appearance-based object detectors from videos, weallow an unknown, unconstrained number of target objects in a more generalizedvideo sequence with non-static camera views. Our self-learning approach uses alatent SVM learning algorithm and deformable part models to represent the shapeand colour information of players, constraining their motions, and learns thecolour of the playing field by a gentle Adaboost algorithm. We combine thoseimage cues and discover additional labels automatically from unlabelled data.In our experiments, our approach exploits both labelled and unlabelled data insparsely labelled videos of sports games, providing a mean performanceimprovement of over 20% in the average precision for detecting sports playersand improved tracking, when videos contain very few labelled images.
arxiv-3600-165 | MixedGrad: An O(1/T) Convergence Rate Algorithm for Stochastic Smooth Optimization | http://arxiv.org/pdf/1307.7192v1.pdf | author:Mehrdad Mahdavi, Rong Jin category:cs.LG math.OC published:2013-07-26 summary:It is well known that the optimal convergence rate for stochasticoptimization of smooth functions is $O(1/\sqrt{T})$, which is same asstochastic optimization of Lipschitz continuous convex functions. This is incontrast to optimizing smooth functions using full gradients, which yields aconvergence rate of $O(1/T^2)$. In this work, we consider a new setup foroptimizing smooth functions, termed as {\bf Mixed Optimization}, which allowsto access both a stochastic oracle and a full gradient oracle. Our goal is tosignificantly improve the convergence rate of stochastic optimization of smoothfunctions by having an additional small number of accesses to the full gradientoracle. We show that, with an $O(\ln T)$ calls to the full gradient oracle andan $O(T)$ calls to the stochastic oracle, the proposed mixed optimizationalgorithm is able to achieve an optimization error of $O(1/T)$.
arxiv-3600-166 | A Comprehensive Evaluation of Machine Learning Techniques for Cancer Class Prediction Based on Microarray Data | http://arxiv.org/pdf/1307.7050v1.pdf | author:Khalid Raza, Atif N Hasan category:cs.LG cs.CE published:2013-07-26 summary:Prostate cancer is among the most common cancer in males and itsheterogeneity is well known. Its early detection helps making therapeuticdecision. There is no standard technique or procedure yet which is full-proofin predicting cancer class. The genomic level changes can be detected in geneexpression data and those changes may serve as standard model for any randomcancer data for class prediction. Various techniques were implied on prostatecancer data set in order to accurately predict cancer class including machinelearning techniques. Huge number of attributes and few number of sample inmicroarray data leads to poor machine learning, therefore the most challengingpart is attribute reduction or non significant gene reduction. In this work wehave compared several machine learning techniques for their accuracy inpredicting the cancer class. Machine learning is effective when number ofattributes (genes) are larger than the number of samples which is rarelypossible with gene expression data. Attribute reduction or gene filtering isabsolutely required in order to make the data more meaningful as most of thegenes do not participate in tumor development and are irrelevant for cancerprediction. Here we have applied combination of statistical techniques such asinter-quartile range and t-test, which has been effective in filteringsignificant genes and minimizing noise from data. Further we have done acomprehensive evaluation of ten state-of-the-art machine learning techniquesfor their accuracy in class prediction of prostate cancer. Out of thesetechniques, Bayes Network out performed with an accuracy of 94.11% followed byNavie Bayes with an accuracy of 91.17%. To cross validate our results, wemodified our training dataset in six different way and found that averagesensitivity, specificity, precision and accuracy of Bayes Network is highestamong all other techniques used.
arxiv-3600-167 | Infinite Mixtures of Multivariate Gaussian Processes | http://arxiv.org/pdf/1307.7028v1.pdf | author:Shiliang Sun category:cs.LG stat.ML published:2013-07-26 summary:This paper presents a new model called infinite mixtures of multivariateGaussian processes, which can be used to learn vector-valued functions andapplied to multitask learning. As an extension of the single multivariateGaussian process, the mixture model has the advantages of modeling multimodaldata and alleviating the computationally cubic complexity of the multivariateGaussian process. A Dirichlet process prior is adopted to allow the (possiblyinfinite) number of mixture components to be automatically inferred fromtraining data, and Markov chain Monte Carlo sampling techniques are used forparameter and latent variable inference. Preliminary experimental results onmultivariate regression show the feasibility of the proposed model.
arxiv-3600-168 | Multi-view Laplacian Support Vector Machines | http://arxiv.org/pdf/1307.7024v1.pdf | author:Shiliang Sun category:cs.LG stat.ML published:2013-07-26 summary:We propose a new approach, multi-view Laplacian support vector machines(SVMs), for semi-supervised learning under the multi-view scenario. Itintegrates manifold regularization and multi-view regularization into the usualformulation of SVMs and is a natural extension of SVMs from supervised learningto multi-view semi-supervised learning. The function optimization problem in areproducing kernel Hilbert space is converted to an optimization in afinite-dimensional Euclidean space. After providing a theoretical bound for thegeneralization performance of the proposed method, we further give aformulation of the empirical Rademacher complexity which affects the boundsignificantly. From this bound and the empirical Rademacher complexity, we cangain insights into the roles played by different regularization terms to thegeneralization performance. Experimental results on synthetic and real-worlddata sets are presented, which validate the effectiveness of the proposedmulti-view Laplacian SVMs approach.
arxiv-3600-169 | A Novel Architecture For Question Classification Based Indexing Scheme For Efficient Question Answering | http://arxiv.org/pdf/1307.6937v1.pdf | author:Renu Mudgal, Rosy Madaan, A. K. Sharma, Ashutosh Dixit category:cs.IR cs.CL published:2013-07-26 summary:Question answering system can be seen as the next step in informationretrieval, allowing users to pose question in natural language and receivecompact answers. For the Question answering system to be successful, researchhas shown that the correct classification of question with respect to theexpected answer type is requisite. We propose a novel architecture for questionclassification and searching in the index, maintained on the basis of expectedanswer types, for efficient question answering. The system uses the criteriafor Answer Relevance Score for finding the relevance of each answer returned bythe system. On analysis of the proposed system, it has been found that thesystem has shown promising results than the existing systems based on questionclassification.
arxiv-3600-170 | An Efficient Primal-Dual Prox Method for Non-Smooth Optimization | http://arxiv.org/pdf/1201.5283v5.pdf | author:Tianbao Yang, Mehrdad Mahdavi, Rong Jin, Shenghuo Zhu category:cs.LG published:2012-01-24 summary:We study the non-smooth optimization problems in machine learning, where boththe loss function and the regularizer are non-smooth functions. Previousstudies on efficient empirical loss minimization assume either a smooth lossfunction or a strongly convex regularizer, making them unsuitable fornon-smooth optimization. We develop a simple yet efficient method for a familyof non-smooth optimization problems where the dual form of the loss function isbilinear in primal and dual variables. We cast a non-smooth optimizationproblem into a minimax optimization problem, and develop a primal dual proxmethod that solves the minimax optimization problem at a rate of $O(1/T)${assuming that the proximal step can be efficiently solved}, significantlyfaster than a standard subgradient descent method that has an $O(1/\sqrt{T})$convergence rate. Our empirical study verifies the efficiency of the proposedmethod for various non-smooth optimization problems that arise ubiquitously inmachine learning by comparing it to the state-of-the-art first order methods.
arxiv-3600-171 | Memcapacitive neural networks | http://arxiv.org/pdf/1307.6921v1.pdf | author:Y. V. Pershin, M. Di Ventra category:cs.ET cs.NE q-bio.NC published:2013-07-26 summary:We show that memcapacitive (memory capacitive) systems can be used assynapses in artificial neural networks. As an example of our approach, wediscuss the architecture of an integrate-and-fire neural network based onmemcapacitive synapses. Moreover, we demonstrate that thespike-timing-dependent plasticity can be simply realized with some of thesedevices. Memcapacitive synapses are a low-energy alternative to memristivesynapses for neuromorphic computation.
arxiv-3600-172 | Sequential Transfer in Multi-armed Bandit with Finite Set of Models | http://arxiv.org/pdf/1307.6887v1.pdf | author:Mohammad Gheshlaghi Azar, Alessandro Lazaric, Emma Brunskill category:stat.ML cs.LG published:2013-07-25 summary:Learning from prior tasks and transferring that experience to improve futureperformance is critical for building lifelong learning agents. Although resultsin supervised and reinforcement learning show that transfer may significantlyimprove the learning performance, most of the literature on transfer is focusedon batch learning tasks. In this paper we study the problem of\textit{sequential transfer in online learning}, notably in the multi-armedbandit framework, where the objective is to minimize the cumulative regret overa sequence of tasks by incrementally transferring knowledge from prior tasks.We introduce a novel bandit algorithm based on a method-of-moments approach forthe estimation of the possible tasks and derive regret bounds for it.
arxiv-3600-173 | A Propound Method for the Improvement of Cluster Quality | http://arxiv.org/pdf/1307.6814v1.pdf | author:Shveta Kundra Bhatia, V. S. Dixit category:cs.LG published:2013-07-25 summary:In this paper Knockout Refinement Algorithm (KRA) is proposed to refineoriginal clusters obtained by applying SOM and K-Means clustering algorithms.KRA Algorithm is based on Contingency Table concepts. Metrics are computed forthe Original and Refined Clusters. Quality of Original and Refined Clusters arecompared in terms of metrics. The proposed algorithm (KRA) is tested in theeducational domain and results show that it generates better quality clustersin terms of improved metric values.
arxiv-3600-174 | Information content versus word length in natural language: A reply to Ferrer-i-Cancho and Moscoso del Prado Martin [arXiv:1209.1751] | http://arxiv.org/pdf/1307.6726v1.pdf | author:Steven T. Piantadosi, Harry Tily, Edward Gibson category:cs.CL math.PR published:2013-07-25 summary:Recently, Ferrer i Cancho and Moscoso del Prado Martin [arXiv:1209.1751]argued that an observed linear relationship between word length and averagesurprisal (Piantadosi, Tily, & Gibson, 2011) is not evidence for communicativeefficiency in human language. We discuss several shortcomings of their approachand critique: their model critically rests on inaccurate assumptions, isincapable of explaining key surprisal patterns in language, and is incompatiblewith recent behavioral results. More generally, we argue that statisticalmodels must not critically rely on assumptions that are incompatible with thereal system under study.
arxiv-3600-175 | Does generalization performance of $l^q$ regularization learning depend on $q$? A negative example | http://arxiv.org/pdf/1307.6616v1.pdf | author:Shaobo Lin, Chen Xu, Jingshan Zeng, Jian Fang category:cs.LG stat.ML published:2013-07-25 summary:$l^q$-regularization has been demonstrated to be an attractive technique inmachine learning and statistical modeling. It attempts to improve thegeneralization (prediction) capability of a machine (model) throughappropriately shrinking its coefficients. The shape of a $l^q$ estimatordiffers in varying choices of the regularization order $q$. In particular,$l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smoothridge regression. This makes the order $q$ a potential tuning parameter inapplications. To facilitate the use of $l^{q}$-regularization, we intend toseek for a modeling strategy where an elaborative selection on $q$ isavoidable. In this spirit, we place our investigation within a generalframework of $l^{q}$-regularized kernel learning under a sample dependenthypothesis space (SDHS). For a designated class of kernel functions, we showthat all $l^{q}$ estimators for $0< q < \infty$ attain similar generalizationerror bounds. These estimated bounds are almost optimal in the sense that up toa logarithmic factor, the upper and lower bounds are asymptotically identical.This finding tentatively reveals that, in some modeling contexts, the choice of$q$ might not have a strong impact in terms of the generalization capability.From this perspective, $q$ can be arbitrarily specified, or specified merely byother no generalization criteria like smoothness, computational complexity,sparsity, etc..
arxiv-3600-176 | Scaling the Indian Buffet Process via Submodular Maximization | http://arxiv.org/pdf/1304.3285v4.pdf | author:Colorado Reed, Zoubin Ghahramani category:stat.ML cs.LG published:2013-04-11 summary:Inference for latent feature models is inherently difficult as the inferencespace grows exponentially with the size of the input data and number of latentfeatures. In this work, we use Kurihara & Welling (2008)'smaximization-expectation framework to perform approximate MAP inference forlinear-Gaussian latent feature models with an Indian Buffet Process (IBP)prior. This formulation yields a submodular function of the features thatcorresponds to a lower bound on the model evidence. By adding a constant tothis function, we obtain a nonnegative submodular function that can bemaximized via a greedy algorithm that obtains at least a one-thirdapproximation to the optimal solution. Our inference method scales linearlywith the size of the input data, and we show the efficacy of our method on thelargest datasets currently analyzed using an IBP model.
arxiv-3600-177 | When is the majority-vote classifier beneficial? | http://arxiv.org/pdf/1307.6522v1.pdf | author:Mu Zhu category:math.ST stat.ML stat.TH published:2013-07-24 summary:In his seminal work, Schapire (1990) proved that weak classifiers could beimproved to achieve arbitrarily high accuracy, but he never implied that asimple majority-vote mechanism could always do the trick. By comparing theasymptotic misclassification error of the majority-vote classifier with theaverage individual error, we discover an interesting phase-transitionphenomenon. For binary classification with equal prior probabilities, ourresult implies that, for the majority-vote mechanism to work, the collection ofweak classifiers must meet the minimum requirement of having an average truepositive rate of at least 50% and an average false positive rate of at most50%.
arxiv-3600-178 | Cluster Trees on Manifolds | http://arxiv.org/pdf/1307.6515v1.pdf | author:Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman category:stat.ML cs.LG published:2013-07-24 summary:In this paper we investigate the problem of estimating the cluster tree for adensity $f$ supported on or near a smooth $d$-dimensional manifold $M$isometrically embedded in $\mathbb{R}^D$. We analyze a modified version of a$k$-nearest neighbor based algorithm recently proposed by Chaudhuri andDasgupta. The main results of this paper show that under mild assumptions on$f$ and $M$, we obtain rates of convergence that depend on $d$ only but not onthe ambient dimension $D$. We also show that similar (albeit non-algorithmic)results can be obtained for kernel density estimators. We sketch a constructionof a sample complexity lower bound instance for a natural class of manifoldoblivious clustering algorithms. We further briefly consider the known manifoldcase and show that in this case a spatially adaptive algorithm achieves betterrates.
arxiv-3600-179 | Human and Automatic Evaluation of English-Hindi Machine Translation | http://arxiv.org/pdf/1307.6163v2.pdf | author:Nisheeth Joshi, Hemant Darbari, Iti Mathur category:cs.CL published:2013-07-23 summary:For the past 60 years, Research in machine translation is going on. For thedevelopment in this field, a lot of new techniques are being developed eachday. As a result, we have witnessed development of many automatic machinetranslators. A manager of machine translation development project needs to knowthe performance increase/decrease, after changes have been done in his system.Due to this reason, a need for evaluation of machine translation systems wasfelt. In this article, we shall present the evaluation of some machinetranslators. This evaluation will be done by a human evaluator and by someautomatic evaluation metrics, which will be done at sentence, document andsystem level. In the end we shall also discuss the comparison between theevaluations.
arxiv-3600-180 | Generative, Fully Bayesian, Gaussian, Openset Pattern Classifier | http://arxiv.org/pdf/1307.6143v2.pdf | author:Niko Brummer category:stat.ML cs.LG published:2013-07-23 summary:This report works out the details of a closed-form, fully Bayesian,multiclass, openset, generative pattern classifier using multivariate Gaussianlikelihoods, with conjugate priors. The generative model has a commonwithin-class covariance, which is proportional to the between-class covariancein the conjugate prior. The scalar proportionality constant is the only pluginparameter. All other model parameters are intergated out in closed form. Anexpression is given for the model evidence, which can be used to make pluginestimates for the proportionality constant. Pattern recognition is done via thepredictive likeihoods of classes for which training data is available, as wellas a predicitve likelihood for any as yet unseen class.
arxiv-3600-181 | Storing non-uniformly distributed messages in networks of neural cliques | http://arxiv.org/pdf/1307.6410v1.pdf | author:Bartosz Boguslawski, Vincent Gripon, Fabrice Seguin, Frédéric Heitzmann category:cs.NE cs.SY published:2013-07-24 summary:Associative memories are data structures that allow retrieval of storedmessages from part of their content. They thus behave similarly to human brainthat is capable for instance of retrieving the end of a song given itsbeginning. Among different families of associative memories, sparse ones areknown to provide the best efficiency (ratio of the number of bits stored tothat of bits used). Nevertheless, it is well known that non-uniformity of thestored messages can lead to dramatic decrease in performance. We introduceseveral strategies to allow efficient storage of non-uniform messages inrecently introduced sparse associative memories. We analyse and discuss themethods introduced. We also present a practical application example.
arxiv-3600-182 | Matching-Constrained Active Contours | http://arxiv.org/pdf/1307.6303v1.pdf | author:Junyan Wang, Kap Luk Chan category:cs.CV published:2013-07-24 summary:In object segmentation by active contours, the initial contour is oftenrequired. Conventionally, the initial contour is provided by the user. Thispaper extends the conventional active contour model by incorporating featurematching in the formulation, which gives rise to a novel matching-constrainedactive contour. The numerical solution to the new optimization model providesan automated framework of object segmentation without user intervention. Themain idea is to incorporate feature point matching as a constraint in activecontour models. To this effect, we obtain a mathematical model of interiorpoints to boundary contour such that matching of interior feature points givescontour alignment, and we formulate the matching score as a constraint toactive contour model such that the feature matching of maximum score that givesthe contour alignment provides the initial feasible solution to the constrainedoptimization model of segmentation. The constraint also ensures that theoptimal contour does not deviate too much from the initial contour.Projected-gradient descent equations are derived to solve the constrainedoptimization. In the experiments, we show that our method is capable ofachieving the automatic object segmentation, and it outperforms the relatedmethods.
arxiv-3600-183 | Multi-View Learning for Web Spam Detection | http://arxiv.org/pdf/1305.3814v2.pdf | author:Ali Hadian, Behrouz Minaei-Bidgoli category:cs.IR cs.LG published:2013-05-16 summary:Spam pages are designed to maliciously appear among the top search results byexcessive usage of popular terms. Therefore, spam pages should be removed usingan effective and efficient spam detection system. Previous methods for web spamclassification used several features from various information sources (pagecontents, web graph, access logs, etc.) to detect web spam. In this paper, wefollow page-level classification approach to build fast and scalable spamfilters. We show that each web page can be classified with satisfiable accuracyusing only its own HTML content. In order to design a multi-view classificationsystem, we used state-of-the-art spam classification methods with distinctfeature sets (views) as the base classifiers. Then, a fusion model is learnedto combine the output of the base classifiers and make final prediction.Results show that multi-view learning significantly improves the classificationperformance, namely AUC by 22%, while providing linear speedup for parallelexecution.
arxiv-3600-184 | A Multi-objective Exploratory Procedure for Regression Model Selection | http://arxiv.org/pdf/1203.6276v3.pdf | author:Ankur Sinha, Pekka Malo, Timo Kuosmanen category:stat.CO cs.NE stat.AP G.3; G.1.6 published:2012-03-28 summary:Variable selection is recognized as one of the most critical steps instatistical modeling. The problems encountered in engineering and socialsciences are commonly characterized by over-abundance of explanatory variables,non-linearities and unknown interdependencies between the regressors. An addeddifficulty is that the analysts may have little or no prior knowledge on therelative importance of the variables. To provide a robust method for modelselection, this paper introduces the Multi-objective Genetic Algorithm forVariable Selection (MOGA-VS) that provides the user with an optimal set ofregression models for a given data-set. The algorithm considers the regressionproblem as a two objective task, and explores the Pareto-optimal (best subset)models by preferring those models over the other which have less number ofregression coefficients and better goodness of fit. The model exploration canbe performed based on in-sample or generalization error minimization. The modelselection is proposed to be performed in two steps. First, we generate thefrontier of Pareto-optimal regression models by eliminating the dominatedmodels without any user intervention. Second, a decision making process isexecuted which allows the user to choose the most preferred model usingvisualizations and simple metrics. The method has been evaluated on a recentlypublished real dataset on Communities and Crime within United States.
arxiv-3600-185 | Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts | http://arxiv.org/pdf/1307.5336v2.pdf | author:Pekka Malo, Ankur Sinha, Pyry Takala, Pekka Korhonen, Jyrki Wallenius category:cs.CL cs.IR q-fin.CP I.2.7 published:2013-07-19 summary:The use of robo-readers to analyze news texts is an emerging technology trendin computational finance. In recent research, a substantial effort has beeninvested to develop sophisticated financial polarity-lexicons that can be usedto investigate how financial sentiments relate to future company performance.However, based on experience from other fields, where sentiment analysis iscommonly applied, it is well-known that the overall semantic orientation of asentence may differ from the prior polarity of individual words. The objectiveof this article is to investigate how semantic orientations can be betterdetected in financial and economic news by accommodating the overallphrase-structure information and domain-specific use of language. Our threemain contributions are: (1) establishment of a human-annotated financephrase-bank, which can be used as benchmark for training and evaluatingalternative models; (2) presentation of a technique to enhance financiallexicons with attributes that help to identify expected direction of eventsthat affect overall sentiment; (3) development of a linearized phrase-structuremodel for detecting contextual semantic orientations in financial and economicnews texts. The relevance of the newly added lexicon features and the benefitof using the proposed learning-algorithm are demonstrated in a comparativestudy against previously used general sentiment models as well as the popularword frequency models used in recent financial studies. The proposed frameworkis parsimonious and avoids the explosion in feature-space caused by the use ofconventional n-gram features.
arxiv-3600-186 | Supervised Metric Learning with Generalization Guarantees | http://arxiv.org/pdf/1307.4514v2.pdf | author:Aurélien Bellet category:cs.LG stat.ML published:2013-07-17 summary:The crucial importance of metrics in machine learning algorithms has led toan increasing interest in optimizing distance and similarity functions, an areaof research known as metric learning. When data consist of feature vectors, alarge body of work has focused on learning a Mahalanobis distance. Less workhas been devoted to metric learning from structured objects (such as strings ortrees), most of it focusing on optimizing a notion of edit distance. Weidentify two important limitations of current metric learning approaches.First, they allow to improve the performance of local algorithms such ask-nearest neighbors, but metric learning for global algorithms (such as linearclassifiers) has not been studied so far. Second, the question of thegeneralization ability of metric learning methods has been largely ignored. Inthis thesis, we propose theoretical and algorithmic contributions that addressthese limitations. Our first contribution is the derivation of a new kernelfunction built from learned edit probabilities. Our second contribution is anovel framework for learning string and tree edit similarities inspired by therecent theory of (e,g,t)-good similarity functions. Using uniform stabilityarguments, we establish theoretical guarantees for the learned similarity thatgive a bound on the generalization error of a linear classifier built from thatsimilarity. In our third contribution, we extend these ideas to metric learningfrom feature vectors by proposing a bilinear similarity learning method thatefficiently optimizes the (e,g,t)-goodness. Generalization guarantees arederived for our approach, highlighting that our method minimizes a tighterbound on the generalization error of the classifier. Our last contribution is aframework for establishing generalization bounds for a large class of existingmetric learning algorithms based on a notion of algorithmic robustness.
arxiv-3600-187 | Machine learning of hierarchical clustering to segment 2D and 3D images | http://arxiv.org/pdf/1303.6163v3.pdf | author:Juan Nunez-Iglesias, Ryan Kennedy, Toufiq Parag, Jianbo Shi, Dmitri B. Chklovskii category:cs.CV cs.LG published:2013-03-25 summary:We aim to improve segmentation through the use of machine learning toolsduring region agglomeration. We propose an active learning approach forperforming hierarchical agglomerative segmentation from superpixels. Our methodcombines multiple features at all scales of the agglomerative process, worksfor data with an arbitrary number of dimensions, and scales to very largedatasets. We advocate the use of variation of information to measuresegmentation accuracy, particularly in 3D electron microscopy (EM) images ofneural tissue, and using this metric demonstrate an improvement over competingalgorithms in EM and natural images.
arxiv-3600-188 | Numerical Methods for Coupled Reconstruction and Registration in Digital Breast Tomosynthesis | http://arxiv.org/pdf/1307.6008v1.pdf | author:Guang Yang, John H. Hipwell, David J. Hawkes, Simon R. Arridge category:cs.CV physics.med-ph published:2013-07-23 summary:Digital Breast Tomosynthesis (DBT) provides an insight into the fine detailsof normal fibroglandular tissues and abnormal lesions by reconstructing apseudo-3D image of the breast. In this respect, DBT overcomes a majorlimitation of conventional X-ray mammography by reducing the confoundingeffects caused by the superposition of breast tissue. In a breast cancerscreening or diagnostic context, a radiologist is interested in detectingchange, which might be indicative of malignant disease. To help automate thistask image registration is required to establish spatial correspondence betweentime points. Typically, images, such as MRI or CT, are first reconstructed andthen registered. This approach can be effective if reconstructing using acomplete set of data. However, for ill-posed, limited-angle problems such asDBT, estimating the deformation is complicated by the significant artefactsassociated with the reconstruction, leading to severe inaccuracies in theregistration. This paper presents a mathematical framework, which couples thetwo tasks and jointly estimates both image intensities and the parameters of atransformation. We evaluate our methods using various computational digital phantoms,uncompressed breast MR images, and in-vivo DBT simulations. Firstly, we compareboth iterative and simultaneous methods to the conventional, sequential methodusing an affine transformation model. We show that jointly estimating imageintensities and parametric transformations gives superior results with respectto reconstruction fidelity and registration accuracy. Also, we incorporate anon-rigid B-spline transformation model into our simultaneous method. Theresults demonstrate a visually plausible recovery of the deformation withpreservation of the reconstruction fidelity.
arxiv-3600-189 | Perceptron Mistake Bounds | http://arxiv.org/pdf/1305.0208v2.pdf | author:Mehryar Mohri, Afshin Rostamizadeh category:cs.LG published:2013-05-01 summary:We present a brief survey of existing mistake bounds and introduce novelbounds for the Perceptron or the kernel Perceptron algorithm. Our novel boundsgeneralize beyond standard margin-loss type bounds, allow for any convex andLipschitz loss function, and admit a very simple proof.
arxiv-3600-190 | An Adaptive GMM Approach to Background Subtraction for Application in Real Time Surveillance | http://arxiv.org/pdf/1307.5800v1.pdf | author:Subra Mukherjee, Karen Das category:cs.CV published:2013-07-22 summary:Efficient security management has become an important parameter in todaysworld. As the problem is growing, there is an urgent need for the introductionof advanced technology and equipment to improve the state-of art ofsurveillance. In this paper we propose a model for real time backgroundsubtraction using AGMM. The proposed model is robust and adaptable to dynamicbackground, fast illumination changes, repetitive motion. Also we haveincorporated a method for detecting shadows using the Horpresert color model.The proposed model can be employed for monitoring areas where movement or entryis highly restricted. So on detection of any unexpected events in the scene analarm can be triggered and hence we can achieve real time surveillance even inthe absence of constant human monitoring.
arxiv-3600-191 | Bayesian inference for logistic models using Polya-Gamma latent variables | http://arxiv.org/pdf/1205.0310v3.pdf | author:Nicholas G. Polson, James G. Scott, Jesse Windle category:stat.ME stat.CO stat.ML published:2012-05-02 summary:We propose a new data-augmentation strategy for fully Bayesian inference inmodels with binomial likelihoods. The approach appeals to a new class ofPolya-Gamma distributions, which are constructed in detail. A variety ofexamples are presented to show the versatility of the method, includinglogistic regression, negative binomial regression, nonlinear mixed-effectsmodels, and spatial models for count data. In each case, our data-augmentationstrategy leads to simple, effective methods for posterior inference that: (1)circumvent the need for analytic approximations, numerical integration, orMetropolis-Hastings; and (2) outperform other known data-augmentationstrategies, both in ease of use and in computational efficiency. All methods,including an efficient sampler for the Polya-Gamma distribution, areimplemented in the R package BayesLogit. In the technical supplement appended to the end of the paper, we providefurther details regarding the generation of Polya-Gamma random variables; theempirical benchmarks reported in the main manuscript; and the extension of thebasic data-augmentation framework to contingency tables and multinomialoutcomes.
arxiv-3600-192 | Appearance Descriptors for Person Re-identification: a Comprehensive Review | http://arxiv.org/pdf/1307.5748v1.pdf | author:Riccardo Satta category:cs.CV published:2013-07-22 summary:In video-surveillance, person re-identification is the task of recognisingwhether an individual has already been observed over a network of cameras.Typically, this is achieved by exploiting the clothing appearance, as classicalbiometric traits like the face are impractical in real-world video surveillancescenarios. Clothing appearance is represented by means of low-level\textit{local} and/or \textit{global} features of the image, usually extractedaccording to some part-based body model to treat different body parts (e.g.torso and legs) independently. This paper provides a comprehensive review ofcurrent approaches to build appearance descriptors for personre-identification. The most relevant techniques are described in detail, andcategorised according to the body models and features used. The aim of thiswork is to provide a structured body of knowledge and a starting point forresearchers willing to conduct novel investigations on this challenging topic.
arxiv-3600-193 | A New Strategy of Cost-Free Learning in the Class Imbalance Problem | http://arxiv.org/pdf/1307.5730v1.pdf | author:Xiaowan Zhang, Bao-Gang Hu category:cs.LG published:2013-07-22 summary:In this work, we define cost-free learning (CFL) formally in comparison withcost-sensitive learning (CSL). The main difference between them is that a CFLapproach seeks optimal classification results without requiring any costinformation, even in the class imbalance problem. In fact, several CFLapproaches exist in the related studies, such as sampling and somecriteria-based pproaches. However, to our best knowledge, none of the existingCFL and CSL approaches are able to process the abstaining classificationsproperly when no information is given about errors and rejects. Based oninformation theory, we propose a novel CFL which seeks to maximize normalizedmutual information of the targets and the decision outputs of classifiers.Using the strategy, we can deal with binary/multi-class classificationswith/without abstaining. Significant features are observed from the newstrategy. While the degree of class imbalance is changing, the proposedstrategy is able to balance the errors and rejects accordingly andautomatically. Another advantage of the strategy is its ability of derivingoptimal rejection thresholds for abstaining classifications and the"equivalent" costs in binary classifications. The connection between rejectionthresholds and ROC curve is explored. Empirical investigation is made onseveral benchmark data sets in comparison with other existing approaches. Theclassification results demonstrate a promising perspective of the strategy inmachine learning.
arxiv-3600-194 | Top-down and Bottom-up Feature Combination for Multi-sensor Attentive Robots | http://arxiv.org/pdf/1307.5720v1.pdf | author:Esther L. Colombini, Alexandre S. Simões, Carlos H. C. Ribeiro category:cs.RO cs.CV published:2013-07-22 summary:The information available to robots in real tasks is widely distributed bothin time and space, requiring the agent to search for relevant data. In humans,that face the same problem when sounds, images and smells are presented totheir sensors in a daily scene, a natural system is applied: Attention. Asvision plays an important role in our routine, most research regardingattention has involved this sensorial system and the same has been replicatedto the robotics field. However,most of the robotics tasks nowadays do not relyonly in visual data, that are still costly. To allow the use of attentiveconcepts with other robotics sensors that are usually used in tasks such asnavigation, self-localization, searching and mapping, a generic attentionalmodel has been previously proposed. In this work, feature mapping functionswere designed to build feature maps to this attentive model from data fromrange scanner and sonar sensors. Experiments were performed in a high fidelitysimulated robotics environment and results have demonstrated the capability ofthe model on dealing with both salient stimuli and goal-driven attention overmultiple features extracted from multiple sensors.
arxiv-3600-195 | Understanding Humans' Strategies in Maze Solving | http://arxiv.org/pdf/1307.5713v1.pdf | author:Min Zhao, Andre G. Marquez category:cs.CV cs.AI q-bio.NC published:2013-07-22 summary:Navigating through a visual maze relies on the strategic use of eye movementsto select and identify the route. When navigating the maze, there aretrade-offs between exploring to the environment and relying on memory. Thisstudy examined strategies used to navigating through novel and familiar mazesthat were viewed from above and traversed by a mouse cursor. Eye and mousemovements revealed two modes that almost never occurred concurrently:exploration and guidance. Analyses showed that people learned mazes and wereable to devise and carry out complex, multi-faceted strategies that traded-offvisual exploration against active motor performance. These strategies took intoaccount available visual information, memory, confidence, the estimated cost intime for exploration, and idiosyncratic tolerance for error. Understanding thestrategies humans used for maze solving is valuable for applications incognitive neuroscience as well as in AI, robotics and human-robot interactions.
arxiv-3600-196 | Saliency-Guided Perceptual Grouping Using Motion Cues in Region-Based Artificial Visual Attention | http://arxiv.org/pdf/1307.5710v1.pdf | author:Jan Tünnermann, Dieter Enns, Bärbel Mertsching category:cs.CV published:2013-07-22 summary:Region-based artificial attention constitutes a framework for bio-inspiredattentional processes on an intermediate abstraction level for the use incomputer vision and mobile robotics. Segmentation algorithms produce regions ofcoherently colored pixels. These serve as proto-objects on which theattentional processes determine image portions of relevance. A singleregion---which not necessarily represents a full object---constitutes the focusof attention. For many post-attentional tasks, however, such as identifying ortracking objects, single segments are not sufficient. Here, we present asaliency-guided approach that groups regions that potentially belong to thesame object based on proximity and similarity of motion. We compare our resultsto object selection by thresholding saliency maps and a furtherattention-guided strategy.
arxiv-3600-197 | Is Bottom-Up Attention Useful for Scene Recognition? | http://arxiv.org/pdf/1307.5702v1.pdf | author:Samuel F. Dodge, Lina J. Karam category:cs.CV published:2013-07-22 summary:The human visual system employs a selective attention mechanism to understandthe visual world in an eficient manner. In this paper, we show howcomputational models of this mechanism can be exploited for the computer visionapplication of scene recognition. First, we consider saliency weighting andsaliency pruning, and provide a comparison of the performance of differentattention models in these approaches in terms of classification accuracy.Pruning can achieve a high degree of computational savings withoutsignificantly sacrificing classification accuracy. In saliency weighting,however, we found that classification performance does not improve. Inaddition, we present a new method to incorporate salient and non-salientregions for improved classification accuracy. We treat the salient andnon-salient regions separately and combine them using Multiple Kernel Learning.We evaluate our approach using the UIUC sports dataset and find that with asmall training size, our method improves upon the classification accuracy ofthe baseline bag of features approach.
arxiv-3600-198 | Visual saliency estimation by integrating features using multiple kernel learning | http://arxiv.org/pdf/1307.5693v1.pdf | author:Yasin Kavak, Erkut Erdem, Aykut Erdem category:cs.CV published:2013-07-22 summary:In the last few decades, significant achievements have been attained inpredicting where humans look at images through different computational models.However, how to determine contributions of different visual features to overallsaliency still remains an open problem. To overcome this issue, a recent classof models formulates saliency estimation as a supervised learning problem andaccordingly apply machine learning techniques. In this paper, we also addressthis challenging problem and propose to use multiple kernel learning (MKL) tocombine information coming from different feature dimensions and to performintegration at an intermediate level. Besides, we suggest to use responses of arecently proposed filterbank of object detectors, known as Object-Bank, asadditional semantic high-level features. Here we show that our MKL-basedframework together with the proposed object-specific features providestate-of-the-art performance as compared to SVM or AdaBoost-based saliencymodels.
arxiv-3600-199 | A study of parameters affecting visual saliency assessment | http://arxiv.org/pdf/1307.5691v1.pdf | author:Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit category:cs.CV published:2013-07-22 summary:Since the early 2000s, computational visual saliency has been a very activeresearch area. Each year, more and more new models are published in the maincomputer vision conferences. Nowadays, one of the big challenges is to find away to fairly evaluate all of these models. In this paper, a new framework isproposed to assess models of visual saliency. This evaluation is divided intothree experiments leading to the proposition of a new evaluation framework.Each experiment is based on a basic question: 1) there are two ground truthsfor saliency evaluation: what are the differences between eye fixations andmanually segmented salient regions?, 2) the properties of the salient regions:for example, do large, medium and small salient regions present differentdifficulties for saliency models? and 3) the metrics used to assess saliencymodels: what advantages would there be to mix them with PCA? Statisticalanalysis is used here to answer each of these three questions.
arxiv-3600-200 | Using a Dynamic Neural Field Model to Explore a Direct Collicular Inhibition Account of Inhibition of Return | http://arxiv.org/pdf/1307.5684v1.pdf | author:Jason Satel, Ross Story, Matthew D. Hilchey, Zhiguo Wang, Raymond M. Klein category:q-bio.NC cs.CV published:2013-07-22 summary:When the interval between a transient ash of light (a "cue") and a secondvisual response signal (a "target") exceeds at least 200ms, responding isslowest in the direction indicated by the first signal. This phenomenon iscommonly referred to as inhibition of return (IOR). The dynamic neural fieldmodel (DNF) has proven to have broad explanatory power for IOR, effectivelycapturing many empirical results. Previous work has used a short-termdepression (STD) implementation of IOR, but this approach fails to explain manybehavioral phenomena observed in the literature. Here, we explore a variantmodel of IOR involving a combination of STD and delayed direct collicularinhibition. We demonstrate that this hybrid model can better reproduceestablished behavioural results. We use the results of this model to proposeseveral experiments that would yield particularly valuable insight into thenature of the neurophysiological mechanisms underlying IOR.
arxiv-3600-201 | Sub- Diving Labeling Method for Optimization Problem by Genetic Algorithm | http://arxiv.org/pdf/1307.5840v1.pdf | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-22 summary:In many global Optimization Problems, it is required to evaluate a globalpoint (min or max) in large space that calculation effort is very high. In thispaper is presented new approach for optimization problem with subdivisionlabeling method (SLM) but in this method for higher dimensional has highcomputational. SLM Genetic Algorithm (SLMGA) in optimization problems is one ofthe solutions of this problem. In proposed algorithm the initial population iscrossing points and subdividing in each step is according to mutation. RSLMGAis compared with other well known algorithms: DE, PGA, Grefensstette andEshelman and numerical results show that RSLMGA achieve global optimal pointwith more decision by smaller generations.
arxiv-3600-202 | Sub-Dividing Genetic Method for Optimization Problems | http://arxiv.org/pdf/1307.5679v1.pdf | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-22 summary:Nowadays, optimization problem have more application in all major but theyhave problem in computation. Computation global point in continuous functionshave high calculation and this became clearer in large space .In this paper, weproposed Sub- Dividing Genetic Method(SGM) that have less computation thanother method for achieving global points . This method userotation mutation andcrossover based sub-division method that sub diving method is used for minimizesearch space and rotation mutation with crossover is used for finding globaloptimal points. In experimental, SGM algorithm is implemented on De Jongfunction. The numerical examples show that SGM is performed more optimal thanother methods such as Grefensstette, Random Value, and PNG.
arxiv-3600-203 | A New Approach for Finding the Global Optimal Point Using Subdividing Labeling Method (SLM) | http://arxiv.org/pdf/1307.5839v1.pdf | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-22 summary:In most global optimization problems, finding global optimal point inthemultidimensional and great search space needs high computations. In this paper,we present a new approach to find global optimal point with the low computationand few steps using subdividing labeling method (SLM) which can also be used inthe multi-dimensional and great search space. In this approach, in each step,crossing points will be labeled and complete label polytope search space ofselected polytope will be subdivided after being selected. SLM algorithm findsthe global point until h (subdivision function) turns into zero. SLM will beimplemented on five applications and compared with the latest techniques suchas random search, random search-walk and simulated annealing method. Theresults of the proposed method demonstrate that our new approach is faster andmore reliable and presents an optimal time complexity O (logn).
arxiv-3600-204 | Solving Traveling Salesman Problem by Marker Method | http://arxiv.org/pdf/1307.5674v1.pdf | author:Masoumeh Vali category:cs.NE cs.DS math.OC published:2013-07-22 summary:In this paper we use marker method and propose a new mutation operator thatselects the nearest neighbor among all near neighbors solving TravelingSalesman Problem.
arxiv-3600-205 | Rotational Mutation Genetic Algorithm on optimization Problems | http://arxiv.org/pdf/1307.5838v1.pdf | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-22 summary:Optimization problem, nowadays, have more application in all major but theyhave problem in computation. Calculation of the optimum point in the spaceswith the above dimensions is very time consuming. In this paper, there ispresented a new approach for the optimization of continuous functions withrotational mutation that is called RM. The proposed algorithm starts from thepoint which has best fitness value by elitism mechanism. Then, method ofrotational mutation is used to reach optimal point. In this paper, RM algorithmis implemented by GA(Briefly RMGA) and is compared with other well- knownalgorithms: DE, PGA, Grefensstette and Eshelman [15, 16] and numerical andsimulation results show that RMGA achieve global optimal point with moredecision by smaller generations.
arxiv-3600-206 | New Optimization Approach Using Clustering-Based Parallel Genetic Algorithm | http://arxiv.org/pdf/1307.5667v1.pdf | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-22 summary:In many global Optimization Problems, it is required to evaluate a globalpoint (min or max) in large space that calculation effort is very high. In thispaper is presented new approach for optimization problem with subdivisionlabeling method (SLM) but in this method for higher dimensional has highcalculation effort. Clustering-Based Parallel Genetic Algorithm (CBPGA) inoptimization problems is one of the solutions of this problem. That the initialpopulation is crossing points and subdividing in each step is according tomutation. After labeling all of crossing points, selecting is according topolytope that has complete label. In this method we propose an algorithm, basedon parallelization scheme using master-slave. SLM algorithm is implemented byCBPGA and compared the experimental results. The numerical examples andnumerical results show that SLMCBPGA is improved speed up and efficiency.
arxiv-3600-207 | Online Tracking Parameter Adaptation based on Evaluation | http://arxiv.org/pdf/1307.5653v1.pdf | author:Duc Phu Chau, Julien Badie, François Bremond, Monique Thonnat category:cs.CV published:2013-07-22 summary:Parameter tuning is a common issue for many tracking algorithms. In order tosolve this problem, this paper proposes an online parameter tuning to adapt atracking algorithm to various scene contexts. In an offline training phase,this approach learns how to tune the tracker parameters to cope with differentcontexts. In the online control phase, once the tracking quality is evaluatedas not good enough, the proposed approach computes the current context andtunes the tracking parameters using the learned values. The experimentalresults show that the proposed approach improves the performance of thetracking algorithm and outperforms recent state of the art trackers. This paperbrings two contributions: (1) an online tracking evaluation, and (2) a methodto adapt online tracking parameters to scene contexts.
arxiv-3600-208 | Unsupervised model-free representation learning | http://arxiv.org/pdf/1304.4806v3.pdf | author:Daniil Ryabko category:cs.LG stat.ML published:2013-04-17 summary:Numerous control and learning problems face the situation where sequences ofhigh-dimensional highly dependent data are available, but no or little feedbackis provided to the learner. To address this issue, we formulate the followingproblem. Given a series of observations X_0,...,X_n coming from a large(high-dimensional) space X, find a representation function f mapping X to afinite space Y such that the series f(X_0),...,f(X_n) preserve as muchinformation as possible about the original time-series dependence inX_0,...,X_n. We show that, for stationary time series, the function f can beselected as the one maximizing the time-series information h_0(f(X))- h_\infty(f(X)) where h_0(f(X)) is the Shannon entropy of f(X_0) and h_\infty (f(X)) isthe entropy rate of the time series f(X_0),...,f(X_n),... Implications for theproblem of optimal control are presented.
arxiv-3600-209 | Multi-horizon solar radiation forecasting for Mediterranean locations using time series models | http://arxiv.org/pdf/1307.6179v1.pdf | author:Cyril Voyant, Christophe Paoli, Marc Muselli, Marie Laure Nivet category:physics.ao-ph cs.NE published:2013-07-22 summary:Considering the grid manager's point of view, needs in terms of prediction ofintermittent energy like the photovoltaic resource can be distinguishedaccording to the considered horizon: following days (d+1, d+2 and d+3), nextday by hourly step (h+24), next hour (h+1) and next few minutes (m+5 e.g.).Through this work, we have identified methodologies using time series modelsfor the prediction horizon of global radiation and photovoltaic power. What wepresent here is a comparison of different predictors developed and tested topropose a hierarchy. For horizons d+1 and h+1, without advanced ad hoc timeseries pre-processing (stationarity) we find it is not easy to differentiatebetween autoregressive moving average (ARMA) and multilayer perceptron (MLP).However we observed that using exogenous variables improves significantly theresults for MLP . We have shown that the MLP were more adapted for horizonsh+24 and m+5. In summary, our results are complementary and improve theexisting prediction techniques with innovative tools: stationarity, numericalweather prediction combination, MLP and ARMA hybridization, multivariateanalysis, time index, etc.
arxiv-3600-210 | Performance comparison of State-of-the-art Missing Value Imputation Algorithms on Some Bench mark Datasets | http://arxiv.org/pdf/1307.5599v1.pdf | author:M. Naresh Kumar category:cs.LG stat.ML published:2013-07-22 summary:Decision making from data involves identifying a set of attributes thatcontribute to effective decision making through computational intelligence. Thepresence of missing values greatly influences the selection of right set ofattributes and this renders degradation in classification accuracies of theclassifiers. As missing values are quite common in data collection phase duringfield experiments or clinical trails appropriate handling would improve theclassifier performance. In this paper we present a review of recently developedmissing value imputation algorithms and compare their performance on some benchmark datasets.
arxiv-3600-211 | A Novel Equation based Classifier for Detecting Human in Images | http://arxiv.org/pdf/1307.5591v1.pdf | author:Subra Mukherjee, Karen Das category:cs.CV published:2013-07-22 summary:Shape based classification is one of the most challenging tasks in the fieldof computer vision. Shapes play a vital role in object recognition. The basicshapes in an image can occur in varying scale, position and orientation. Andspecially when detecting human, the task becomes more challenging owing to thelargely varying size, shape, posture and clothing of human. So, in our work wedetect human, based on the head-shoulder shape as it is the most unvarying partof human body. Here, firstly a new and a novel equation named as the OmegaEquation that describes the shape of human head-shoulder is developed and basedon this equation, a classifier is designed particularly for detecting humanpresence in a scene. The classifier detects human by analyzing some of thediscriminative features of the values of the parameters obtained from the Omegaequation. The proposed method has been tested on a variety of shape datasettaking into consideration the complexities of human head-shoulder shape. In allthe experiments the proposed method demonstrated satisfactory results.
arxiv-3600-212 | Regularized Discrete Optimal Transport | http://arxiv.org/pdf/1307.5551v1.pdf | author:Sira Ferradans, Nicolas Papadakis, Gabriel Peyré, Jean-François Aujol category:cs.CV cs.DM math.OC published:2013-07-21 summary:This article introduces a generalization of the discrete optimal transport,with applications to color image manipulations. This new formulation includes arelaxation of the mass conservation constraint and a regularization term. Thesetwo features are crucial for image processing tasks, which necessitate to takeinto account families of multimodal histograms, with large mass variationacross modes. The corresponding relaxed and regularized transportation problem is thesolution of a convex optimization problem. Depending on the regularizationused, this minimization can be solved using standard linear programming methodsor first order proximal splitting schemes. The resulting transportation plan can be used as a color transfer map, whichis robust to mass variation across images color palettes. Furthermore, theregularization of the transport plan helps to remove colorization artifacts dueto noise amplification. We also extend this framework to the computation of barycenters ofdistributions. The barycenter is the solution of an optimization problem, whichis separately convex with respect to the barycenter and the transportationplans, but not jointly convex. A block coordinate descent scheme converges to astationary point of the energy. We show that the resulting algorithm can beused for color normalization across several images. The relaxed and regularizedbarycenter defines a common color palette for those images. Applying colortransfer toward this average palette performs a color normalization of theinput images.
arxiv-3600-213 | A New Optimization Approach Based on Rotational Mutation and Crossover Operator | http://arxiv.org/pdf/1307.5534v1.pdf | author:Masoumeh Vali category:cs.NE math.OC published:2013-07-21 summary:Evaluating a global optimal point in many global optimization problems inlarge space is required to more calculations. In this paper, there is presenteda new approach for the continuous functions optimization with rotationalmutation and crossover operator. This proposed method (RMC) starts from thepoint which has best fitness value by elitism mechanism and after thatrotational mutation and crossover operator are used to reach optimal point. RMCmethod is implemented by GA (Briefly RMCGA) and is compared with otherwellknown algorithms such as: DE, PGA, Grefensstette and Eshelman[15,16] andnumerical and simulating results show that RMCGA achieve global optimal pointwith more decision by smaller generations.
arxiv-3600-214 | A Massively Parallel Associative Memory Based on Sparse Neural Networks | http://arxiv.org/pdf/1303.7032v2.pdf | author:Zhe Yao, Vincent Gripon, Michael G. Rabbat category:cs.AI cs.DC cs.NE published:2013-03-28 summary:Associative memories store content in such a way that the content can belater retrieved by presenting the memory with a small portion of the content,rather than presenting the memory with an address as in more traditionalmemories. Associative memories are used as building blocks for algorithmswithin database engines, anomaly detection systems, compression algorithms, andface recognition systems. A classical example of an associative memory is theHopfield neural network. Recently, Gripon and Berrou have introduced analternative construction which builds on ideas from the theory of errorcorrecting codes and which greatly outperforms the Hopfield network incapacity, diversity, and efficiency. In this paper we implement a variation ofthe Gripon-Berrou associative memory on a general purpose graphical processingunit (GPU). The work of Gripon and Berrou proposes two retrieval rules,sum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vectormultiplication and is easily implemented on the GPU. The sum-of-max rule ismuch less straightforward to implement because it involves non-linearoperations. However, the sum-of-max rule gives significantly better retrievalerror rates. We propose a hybrid rule tailored for implementation on a GPUwhich achieves a 880-fold speedup without sacrificing any accuracy.
arxiv-3600-215 | Combining One-Class Classifiers via Meta-Learning | http://arxiv.org/pdf/1112.5246v3.pdf | author:Eitan Menahem, Lior Rokach, Yuval Elovici category:cs.LG K.3.2 published:2011-12-22 summary:Selecting the best classifier among the available ones is a difficult task,especially when only instances of one class exist. In this work we examine thenotion of combining one-class classifiers as an alternative for selecting thebest classifier. In particular, we propose two new one-class classificationperformance measures to weigh classifiers and show that a simple ensemble thatimplements these measures can outperform the most popular one-class ensembles.Furthermore, we propose a new one-class ensemble scheme, TUPSO, which usesmeta-learning to combine one-class classifiers. Our experiments demonstrate thesuperiority of TUPSO over all other tested ensembles and show that the TUPSOperformance is statistically indistinguishable from that of the hypotheticalbest classifier.
arxiv-3600-216 | Optimal Recombination in Genetic Algorithms | http://arxiv.org/pdf/1307.5519v1.pdf | author:Anton V. Eremeev, Julia V. Kovalenko category:cs.NE cs.DS published:2013-07-21 summary:This paper surveys results on complexity of the optimal recombination problem(ORP), which consists in finding the best possible offspring as a result of arecombination operator in a genetic algorithm, given two parent solutions. Weconsider efficient reductions of the ORPs, allowing to establish polynomialsolvability or NP-hardness of the ORPs, as well as direct proofs of hardnessresults.
arxiv-3600-217 | A scalable stage-wise approach to large-margin multi-class loss based boosting | http://arxiv.org/pdf/1307.5497v1.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.LG published:2013-07-21 summary:We present a scalable and effective classification model to train multi-classboosting for multi-class classification problems. Shen and Hao introduced adirect formulation of multi- class boosting in the sense that it directlymaximizes the multi- class margin [C. Shen and Z. Hao, "A direct formulationfor totally-corrective multi- class boosting", in Proc. IEEE Conf. Comp. Vis.Patt. Recogn., 2011]. The major problem of their approach is its highcomputational complexity for training, which hampers its application onreal-world problems. In this work, we propose a scalable and simple stage-wisemulti-class boosting method, which also directly maximizes the multi-classmargin. Our approach of- fers a few advantages: 1) it is simple andcomputationally efficient to train. The approach can speed up the training timeby more than two orders of magnitude without sacrificing the classificationaccuracy. 2) Like traditional AdaBoost, it is less sensitive to the choice ofparameters and empirically demonstrates excellent generalization performance.Experimental results on challenging multi-class machine learning and visiontasks demonstrate that the proposed approach substantially improves theconvergence rate and accuracy of the final visual detector at no additionalcomputational cost compared to existing multi-class boosting.
arxiv-3600-218 | On GROUSE and Incremental SVD | http://arxiv.org/pdf/1307.5494v1.pdf | author:Laura Balzano, Stephen J. Wright category:cs.NA cs.LG stat.ML published:2013-07-21 summary:GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an incrementalalgorithm for identifying a subspace of Rn from a sequence of vectors in thissubspace, where only a subset of components of each vector is revealed at eachiteration. Recent analysis has shown that GROUSE converges locally at anexpected linear rate, under certain assumptions. GROUSE has a similar flavor tothe incremental singular value decomposition algorithm, which updates the SVDof a matrix following addition of a single column. In this paper, we modify theincremental SVD approach to handle missing data, and demonstrate that thismodified approach is equivalent to GROUSE, for a certain choice of analgorithmic parameter.
arxiv-3600-219 | Dictionary LASSO: Guaranteed Sparse Recovery under Linear Transformation | http://arxiv.org/pdf/1305.0047v2.pdf | author:Ji Liu, Lei Yuan, Jieping Ye category:stat.ML published:2013-04-30 summary:We consider the following signal recovery problem: given a measurement matrix$\Phi\in \mathbb{R}^{n\times p}$ and a noisy observation vector $c\in\mathbb{R}^{n}$ constructed from $c = \Phi\theta^* + \epsilon$ where$\epsilon\in \mathbb{R}^{n}$ is the noise vector whose entries follow i.i.d.centered sub-Gaussian distribution, how to recover the signal $\theta^*$ if$D\theta^*$ is sparse {\rca under a linear transformation}$D\in\mathbb{R}^{m\times p}$? One natural method using convex optimization isto solve the following problem: $$\min_{\theta} {1\over 2}\\Phi\theta - c\^2+ \lambda\D\theta\_1.$$ This paper provides an upper bound of the estimateerror and shows the consistency property of this method by assuming that thedesign matrix $\Phi$ is a Gaussian random matrix. Specifically, we show 1) inthe noiseless case, if the condition number of $D$ is bounded and themeasurement number $n\geq \Omega(s\log(p))$ where $s$ is the sparsity number,then the true solution can be recovered with high probability; and 2) in thenoisy case, if the condition number of $D$ is bounded and the measurementincreases faster than $s\log(p)$, that is, $s\log(p)=o(n)$, the estimate errorconverges to zero with probability 1 when $p$ and $s$ go to infinity. Ourresults are consistent with those for the special case $D=\bold{I}_{p\times p}$(equivalently LASSO) and improve the existing analysis. The condition number of$D$ plays a critical role in our analysis. We consider the condition numbers intwo cases including the fused LASSO and the random graph: the condition numberin the fused LASSO case is bounded by a constant, while the condition number inthe random graph case is bounded with high probability if $m\over p$ (i.e.,$#text{edge}\over #text{vertex}$) is larger than a certain constant. Numericalsimulations are consistent with our theoretical results.
arxiv-3600-220 | Clustering Algorithm for Gujarati Language | http://arxiv.org/pdf/1307.5393v1.pdf | author:Miral Patel, Prem Balani category:cs.CL published:2013-07-20 summary:Natural language processing area is still under research. But now a day it ison platform for worldwide researchers. Natural language processing includesanalyzing the language based on its structure and then tagging of each wordappropriately with its grammar base. Here we have 50,000 tagged words set andwe try to cluster those Gujarati words based on proposed algorithm, we havedefined our own algorithm for processing. Many clustering techniques areavailable Ex. Single linkage, complete, linkage,average linkage, Hear no ofclusters to be formed are not known, so it is all depends on the type of dataset provided . Clustering is preprocess for stemming . Stemming is the processwhere root is extracted from its word. Ex. cats= cat+S, meaning. Cat: Noun andplural form.
arxiv-3600-221 | Tensor-based formulation and nuclear norm regularization for multi-energy computed tomography | http://arxiv.org/pdf/1307.5348v1.pdf | author:Oguz Semerci, Ning Hao, Misha E. Kilmer, Eric L. Miller category:cs.CV physics.med-ph published:2013-07-19 summary:The development of energy selective, photon counting X-ray detectors allowsfor a wide range of new possibilities in the area of computed tomographic imageformation. Under the assumption of perfect energy resolution, here we propose atensor-based iterative algorithm that simultaneously reconstructs the X-rayattenuation distribution for each energy. We use a multi-linear image modelrather than a more standard "stacked vector" representation in order to developnovel tensor-based regularizers. Specifically, we model the multi-spectralunknown as a 3-way tensor where the first two dimensions are space and thethird dimension is energy. This approach allows for the design of tensornuclear norm regularizers, which like its two dimensional counterpart, is aconvex function of the multi-spectral unknown. The solution to the resultingconvex optimization problem is obtained using an alternating direction methodof multipliers (ADMM) approach. Simulation results shows that the generalizedtensor nuclear norm can be used as a stand alone regularization technique forthe energy selective (spectral) computed tomography (CT) problem and whencombined with total variation regularization it enhances the regularizationcapabilities especially at low energy images where the effects of noise aremost prominent.
arxiv-3600-222 | The Cluster Graphical Lasso for improved estimation of Gaussian graphical models | http://arxiv.org/pdf/1307.5339v1.pdf | author:Kean Ming Tan, Daniela Witten, Ali Shojaie category:stat.ML stat.ME published:2013-07-19 summary:We consider the task of estimating a Gaussian graphical model in thehigh-dimensional setting. The graphical lasso, which involves maximizing theGaussian log likelihood subject to an l1 penalty, is a well-studied approachfor this task. We begin by introducing a surprising connection between thegraphical lasso and hierarchical clustering: the graphical lasso in effectperforms a two-step procedure, in which (1) single linkage hierarchicalclustering is performed on the variables in order to identify connectedcomponents, and then (2) an l1-penalized log likelihood is maximized on thesubset of variables within each connected component. In other words, thegraphical lasso determines the connected components of the estimated networkvia single linkage clustering. Unfortunately, single linkage clustering isknown to perform poorly in certain settings. Therefore, we propose the clustergraphical lasso, which involves clustering the features using an alternative tosingle linkage clustering, and then performing the graphical lasso on thesubset of variables within each cluster. We establish model selectionconsistency for this technique, and demonstrate its improved performancerelative to the graphical lasso in a simulation study, as well as inapplications to an equities data set, a university webpage data set, and a geneexpression data set.
arxiv-3600-223 | Sparse Factor Analysis for Learning and Content Analytics | http://arxiv.org/pdf/1303.5685v2.pdf | author:Andrew S. Lan, Andrew E. Waters, Christoph Studer, Richard G. Baraniuk category:stat.ML cs.LG math.OC stat.AP published:2013-03-22 summary:We develop a new model and algorithms for machine learning-based learninganalytics, which estimate a learner's knowledge of the concepts underlying adomain, and content analytics, which estimate the relationships among acollection of questions and those concepts. Our model represents theprobability that a learner provides the correct response to a question in termsof three factors: their understanding of a set of underlying concepts, theconcepts involved in each question, and each question's intrinsic difficulty.We estimate these factors given the graded responses to a collection ofquestions. The underlying estimation problem is ill-posed in general,especially when only a subset of the questions are answered. The keyobservation that enables a well-posed solution is the fact that typicaleducational domains of interest involve only a small number of key concepts.Leveraging this observation, we develop both a bi-convex maximum-likelihood anda Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem.We also incorporate user-defined tags on questions to facilitate theinterpretability of the estimated factors. Experiments with synthetic andreal-world data demonstrate the efficacy of our approach. Finally, we make aconnection between SPARFA and noisy, binary-valued (1-bit) dictionary learningthat is of independent interest.
arxiv-3600-224 | Making Laplacians commute | http://arxiv.org/pdf/1307.6549v1.pdf | author:Michael M. Bronstein, Klaus Glashoff, Terry A. Loring category:cs.CV cs.GR math.SP published:2013-07-19 summary:In this paper, we construct multimodal spectral geometry by finding a pair ofclosest commuting operators (CCO) to a given pair of Laplacians. The CCOs arejointly diagonalizable and hence have the same eigenbasis. Our constructionnaturally extends classical data analysis tools based on spectral geometry,such as diffusion maps and spectral clustering. We provide several syntheticand real examples of applications in dimensionality reduction, shape analysis,and clustering, demonstrating that our method better captures the inherentstructure of multi-modal data.
arxiv-3600-225 | Speaker Independent Continuous Speech to Text Converter for Mobile Application | http://arxiv.org/pdf/1307.5736v1.pdf | author:R. Sandanalakshmi, P. Abinaya Viji, M. Kiruthiga, M. Manjari, M. Sharina category:cs.CL cs.NE cs.SD published:2013-07-19 summary:An efficient speech to text converter for mobile application is presented inthis work. The prime motive is to formulate a system which would give optimumperformance in terms of complexity, accuracy, delay and memory requirements formobile environment. The speech to text converter consists of two stages namelyfront-end analysis and pattern recognition. The front end analysis involvespreprocessing and feature extraction. The traditional voice activity detectionalgorithms which track only energy cannot successfully identify potentialspeech from input because the unwanted part of the speech also has some energyand appears to be speech. In the proposed system, VAD that calculates energy ofhigh frequency part separately as zero crossing rate to differentiate noisefrom speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used asfeature extraction method and Generalized Regression Neural Network is used asrecognizer. MFCC provides low word error rate and better feature extraction.Neural Network improves the accuracy. Thus a small database containing allpossible syllable pronunciation of the user is sufficient to give recognitionaccuracy closer to 100%. Thus the proposed technique entertains realization ofreal time speaker independent applications like mobile phones, PDAs etc.
arxiv-3600-226 | Model-Based Policy Gradients with Parameter-Based Exploration by Least-Squares Conditional Density Estimation | http://arxiv.org/pdf/1307.5118v1.pdf | author:Syogo Mori, Voot Tangkaratt, Tingting Zhao, Jun Morimoto, Masashi Sugiyama category:stat.ML cs.LG published:2013-07-19 summary:The goal of reinforcement learning (RL) is to let an agent learn an optimalcontrol policy in an unknown environment so that future expected rewards aremaximized. The model-free RL approach directly learns the policy based on datasamples. Although using many samples tends to improve the accuracy of policylearning, collecting a large number of samples is often expensive in practice.On the other hand, the model-based RL approach first estimates the transitionmodel of the environment and then learns the policy based on the estimatedtransition model. Thus, if the transition model is accurately learned from asmall amount of data, the model-based approach can perform better than themodel-free approach. In this paper, we propose a novel model-based RL method bycombining a recently proposed model-free policy search method called policygradients with parameter-based exploration and the state-of-the-art transitionmodel estimator called least-squares conditional density estimation. Throughexperiments, we demonstrate the practical usefulness of the proposed method.
arxiv-3600-227 | Automated Defect Localization via Low Rank Plus Outlier Modeling of Propagating Wavefield Data | http://arxiv.org/pdf/1307.5102v1.pdf | author:Stefano Gonella, Jarvis D. Haupt category:cs.CV published:2013-07-19 summary:This work proposes an agnostic inference strategy for material diagnostics,conceived within the context of laser-based non-destructive evaluation methods,which extract information about structural anomalies from the analysis ofacoustic wavefields measured on the structure's surface by means of a scanninglaser interferometer. The proposed approach couples spatiotemporal windowingwith low rank plus outlier modeling, to identify a priori unknown deviations inthe propagating wavefields caused by material inhomogeneities or defects, usingvirtually no knowledge of the structural and material properties of the medium.This characteristic makes the approach particularly suitable for diagnosticsscenarios where the mechanical and material models are complex, unknown, orunreliable. We demonstrate our approach in a simulated environment usingbenchmark point and line defect localization problems based on propagatingflexural waves in a thin plate.
arxiv-3600-228 | A Safe Screening Rule for Sparse Logistic Regression | http://arxiv.org/pdf/1307.4145v2.pdf | author:Jie Wang, Jiayu Zhou, Jun Liu, Peter Wonka, Jieping Ye category:cs.LG stat.ML published:2013-07-16 summary:The l1-regularized logistic regression (or sparse logistic regression) is awidely used method for simultaneous classification and feature selection.Although many recent efforts have been devoted to its efficient implementation,its application to high dimensional data still poses significant challenges. Inthis paper, we present a fast and effective sparse logistic regressionscreening rule (Slores) to identify the 0 components in the solution vector,which may lead to a substantial reduction in the number of features to beentered to the optimization. An appealing feature of Slores is that the dataset needs to be scanned only once to run the screening and its computationalcost is negligible compared to that of solving the sparse logistic regressionproblem. Moreover, Slores is independent of solvers for sparse logisticregression, thus Slores can be integrated with any existing solver to improvethe efficiency. We have evaluated Slores using high-dimensional data sets fromdifferent applications. Extensive experimental results demonstrate that Sloresoutperforms the existing state-of-the-art screening rules and the efficiency ofsolving sparse logistic regression is improved by one magnitude in general.
arxiv-3600-229 | On the Necessity of Mixed Models: Dynamical Frustrations in the Mind | http://arxiv.org/pdf/1307.4986v1.pdf | author:Diego Gabriel Krivochen category:nlin.CD cs.CL math.DS published:2013-07-18 summary:In the present work we will present and analyze some basic processes at thelocal and global level in linguistic derivations that seem to go beyond thelimits of Markovian or Turing-like computation, and require, in our opinion, aquantum processor. We will first present briefly the working hypothesis andthen focus on the empirical domain. At the same time, we will argue that amodel appealing to only one kind of computation (be it quantum or not) isnecessarily insufficient, and thus both linear and non-linear formal models areto be invoked in order to pursue a fuller understanding of mental computationswithin a unified framework.
arxiv-3600-230 | Nonlinear unmixing of hyperspectral images: models and algorithms | http://arxiv.org/pdf/1304.1875v2.pdf | author:Nicolas Dobigeon, Jean-Yves Tourneret, Cédric Richard, José C. M. Bermudez, Stephen McLaughlin, Alfred O. Hero category:stat.AP stat.ME stat.ML published:2013-04-06 summary:When considering the problem of unmixing hyperspectral images, most of theliterature in the geoscience and image processing areas relies on the widelyused linear mixing model (LMM). However, the LMM may be not valid and othernonlinear models need to be considered, for instance, when there aremulti-scattering effects or intimate interactions. Consequently, over the lastfew years, several significant contributions have been proposed to overcome thelimitations inherent in the LMM. In this paper, we present an overview ofrecent advances in nonlinear unmixing modeling.
arxiv-3600-231 | Noisy Subspace Clustering via Thresholding | http://arxiv.org/pdf/1305.3486v2.pdf | author:Reinhard Heckel, Helmut Bölcskei category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2013-05-15 summary:We consider the problem of clustering noisy high-dimensional data points intoa union of low-dimensional subspaces and a set of outliers. The number ofsubspaces, their dimensions, and their orientations are unknown. Aprobabilistic performance analysis of the thresholding-based subspaceclustering (TSC) algorithm introduced recently in [1] shows that TSC succeedsin the noisy case, even when the subspaces intersect. Our results reveal anexplicit tradeoff between the allowed noise level and the affinity of thesubspaces. We furthermore find that the simple outlier detection schemeintroduced in [1] provably succeeds in the noisy case.
arxiv-3600-232 | Variational Algorithms for Marginal MAP | http://arxiv.org/pdf/1302.6584v3.pdf | author:Qiang Liu, Alexander Ihler category:stat.ML cs.AI cs.IT cs.LG math.IT published:2013-02-26 summary:The marginal maximum a posteriori probability (MAP) estimation problem, whichcalculates the mode of the marginal posterior distribution of a subset ofvariables with the remaining variables marginalized, is an important inferenceproblem in many models, such as those with hidden variables or uncertainparameters. Unfortunately, marginal MAP can be NP-hard even on trees, and hasattracted less attention in the literature compared to the joint MAP(maximization) and marginalization problems. We derive a general dualrepresentation for marginal MAP that naturally integrates the marginalizationand maximization operations into a joint variational optimization problem,making it possible to easily extend most or all variational-based algorithms tomarginal MAP. In particular, we derive a set of "mixed-product" message passingalgorithms for marginal MAP, whose form is a hybrid of max-product, sum-productand a novel "argmax-product" message updates. We also derive a class ofconvergent algorithms based on proximal point methods, including one thattransforms the marginal MAP problem into a sequence of standard marginalizationproblems. Theoretically, we provide guarantees under which our algorithms giveglobally or locally optimal solutions, and provide novel upper bounds on theoptimal objectives. Empirically, we demonstrate that our algorithmssignificantly outperform the existing approaches, including a state-of-the-artalgorithm based on local search methods.
arxiv-3600-233 | Regret Bounds for Reinforcement Learning with Policy Advice | http://arxiv.org/pdf/1305.1027v2.pdf | author:Mohammad Gheshlaghi Azar, Alessandro Lazaric, Emma Brunskill category:stat.ML cs.LG published:2013-05-05 summary:In some reinforcement learning problems an agent may be provided with a setof input policies, perhaps learned from prior experience or provided byadvisors. We present a reinforcement learning with policy advice (RLPA)algorithm which leverages this input set and learns to use the best policy inthe set for the reinforcement learning task at hand. We prove that RLPA has asub-linear regret of \tilde O(\sqrt{T}) relative to the best input policy, andthat both this regret and its computational complexity are independent of thesize of the state and action space. Our empirical simulations support ourtheoretical analysis. This suggests RLPA may offer significant advantages inlarge domains where some prior good policies are provided.
arxiv-3600-234 | Approximation Algorithms for Bayesian Multi-Armed Bandit Problems | http://arxiv.org/pdf/1306.3525v2.pdf | author:Sudipto Guha, Kamesh Munagala category:cs.DS cs.LG published:2013-06-14 summary:In this paper, we consider several finite-horizon Bayesian multi-armed banditproblems with side constraints which are computationally intractable (NP-Hard)and for which no optimal (or near optimal) algorithms are known to exist withsub-exponential running time. All of these problems violate the standardexchange property, which assumes that the reward from the play of an arm is notcontingent upon when the arm is played. Not only are index policies suboptimalin these contexts, there has been little analysis of such policies in theseproblem settings. We show that if we consider near-optimal policies, in thesense of approximation algorithms, then there exists (near) index policies.Conceptually, if we can find policies that satisfy an approximate version ofthe exchange property, namely, that the reward from the play of an arm dependson when the arm is played to within a constant factor, then we have an avenuetowards solving these problems. However such an approximate version of theidling bandit property does not hold on a per-play basis and are shown to holdin a global sense. Clearly, such a property is not necessarily true ofarbitrary single arm policies and finding such single arm policies isnontrivial. We show that by restricting the state spaces of arms we can findsingle arm policies and that these single arm policies can be combined intoglobal (near) index policies where the approximate version of the exchangeproperty is true in expectation. The number of different bandit problems thatcan be addressed by this technique already demonstrate its wide applicability.
arxiv-3600-235 | Content Based Image Retrieval System using Feature Classification with Modified KNN Algorithm | http://arxiv.org/pdf/1307.4717v1.pdf | author:T. Dharani, I. Laurence Aroquiaraj category:cs.CV published:2013-07-17 summary:Feature means countenance, remote sensing scene objects with similarcharacteristics, associated to interesting scene elements in the imageformation process. They are classified into three types in image processing,that is low, middle and high. Low level features are color, texture and middlelevel feature is shape and high level feature is semantic gap of objects. Animage retrieval system is a computer system for browsing, searching andretrieving images from a large image database. Content Based Image Retrieval isa technique which uses visual features of image such as color, shape, textureto search user required image from large image database according to userrequests in the form of a query. MKNN is an enhancing method of KNN. Theproposed KNN classification is called MKNN. MKNN contains two parts forprocessing, they are validity of the train samples and applying weighted KNN.The validity of each point is computed according to its neighbors. In ourproposal, Modified K-Nearest Neighbor can be considered a kind of weighted KNNso that the query label is approximated by weighting the neighbors of thequery.
arxiv-3600-236 | The connection between Bayesian estimation of a Gaussian random field and RKHS | http://arxiv.org/pdf/1301.5288v3.pdf | author:Aleksandr Y. Aravkin, Bradley M. Bell, James V. Burke, Gianluigi Pillonetto category:stat.ML cs.LG math.ST stat.TH 47N30, 65K10 published:2013-01-22 summary:Reconstruction of a function from noisy data is often formulated as aregularized optimization problem over an infinite-dimensional reproducingkernel Hilbert space (RKHS). The solution describes the observed data and has asmall RKHS norm. When the data fit is measured using a quadratic loss, thisestimator has a known statistical interpretation. Given the noisy measurements,the RKHS estimate represents the posterior mean (minimum variance estimate) ofa Gaussian random field with covariance proportional to the kernel associatedwith the RKHS. In this paper, we provide a statistical interpretation when moregeneral losses are used, such as absolute value, Vapnik or Huber. Specifically,for any finite set of sampling locations (including where the data werecollected), the MAP estimate for the signal samples is given by the RKHSestimate evaluated at these locations.
arxiv-3600-237 | A New Convex Relaxation for Tensor Completion | http://arxiv.org/pdf/1307.4653v1.pdf | author:Bernardino Romera-Paredes, Massimiliano Pontil category:cs.LG math.OC stat.ML published:2013-07-17 summary:We study the problem of learning a tensor from a set of linear measurements.A prominent methodology for this problem is based on a generalization of tracenorm regularization, which has been used extensively for learning low rankmatrices, to the tensor setting. In this paper, we highlight some limitationsof this approach and propose an alternative convex relaxation on the Euclideanball. We then describe a technique to solve the associated regularizationproblem, which builds upon the alternating direction method of multipliers.Experiments on one synthetic dataset and two real datasets indicate that theproposed method improves significantly over tensor trace norm regularization interms of estimation error, while remaining computationally tractable.
arxiv-3600-238 | Convex Variational Image Restoration with Histogram Priors | http://arxiv.org/pdf/1301.3683v2.pdf | author:Paul Swoboda, Christoph Schnörr category:math.OC cs.CV G.1.6; I.4.4 published:2013-01-16 summary:We present a novel variational approach to image restoration (e.g.,denoising, inpainting, labeling) that enables to complement establishedvariational approaches with a histogram-based prior enforcing closeness of thesolution to some given empirical measure. By minimizing a single objectivefunction, the approach utilizes simultaneously two quite different sources ofinformation for restoration: spatial context in terms of some smoothness priorand non-spatial statistics in terms of the novel prior utilizing theWasserstein distance between probability measures. We study the combination ofthe functional lifting technique with two different relaxations of thehistogram prior and derive a jointly convex variational approach. Mathematicalequivalence of both relaxations is established and cases where optimality holdsare discussed. Additionally, we present an efficient algorithmic scheme for thenumerical treatment of the presented model. Experiments using the basictotal-variation based denoising approach as a case study demonstrate our novelregularization approach.
arxiv-3600-239 | Symmetries in LDDMM with higher order momentum distributions | http://arxiv.org/pdf/1306.3309v2.pdf | author:Henry Jacobs category:math.DS cs.CV published:2013-06-14 summary:In some implementations of the Large Deformation Diffeomorphic Metric Mappingformulation for image registration we consider the motion of particles whichlocally translate image data. We then lift the motion of the particles toobtain a motion on the entire image. However, it is certainly possible toconsider particles which do more than translate, and this is what will bedescribed in this paper. As the unreduced Lagrangian associated to EPDiffpossesses $\Diff(M)$ symmetry, it must also exhibit $G \subset \Diff(M)$symmetry, for any Lie subgroup. In this paper we will describe a tower of Liegroups $G^{(0)} \subseteq G^{(1)} \subseteq G^{(2)} \subseteq...$ whichcorrespond to preserving $k$-th order jet-data. The reduced configurationspaces $Q^{(k)} := \Diff(M) / G^{(k)}$ will be finite-dimensional (inparticular, $Q^{(0)}$ is the configuration manifold for $N$ particles in $M$).We will observe that $G^{(k)}$ is a normal subgroup of $G^{(0)}$ and so thequotient $G^{(0)} / G^{(k)}$ is itself a (finite dimensional) Lie group whichacts on $Q^{(k)}$. This makes $Q^{(k)}$ a principle bundle over $Q^{(0)}$ andthe reduced geodesic equations on $Q^{(k)}$ will possess $G^{(0)} /G^{(k)}$-symmetry. Noether's theorem implies the existence of conserved momentafor the reduced system on $T^{\ast}Q^{(k)}$.
arxiv-3600-240 | Processing stationary noise: model and parameter selection in variational methods | http://arxiv.org/pdf/1307.4592v1.pdf | author:Jérôme Fehrenbach, Pierre Weiss category:cs.CV math.OC stat.AP published:2013-07-17 summary:Additive or multiplicative stationary noise recently became an importantissue in applied fields such as microscopy or satellite imaging. Relatively fewworks address the design of dedicated denoising methods compared to the usualwhite noise setting. We recently proposed a variational algorithm to tacklethis issue. In this paper, we analyze this problem from a statistical point ofview and provide deterministic properties of the solutions of the associatedvariational problems. In the first part of this work, we demonstrate that inmany practical problems, the noise can be assimilated to a colored Gaussiannoise. We provide a quantitative measure of the distance between a stationaryprocess and the corresponding Gaussian process. In the second part, we focus onthe Gaussian setting and analyze denoising methods which consist of minimizingthe sum of a total variation term and an $l^2$ data fidelity term. While theconstrained formulation of this problem allows to easily tune the parameters,the Lagrangian formulation can be solved more efficiently since the problem isstrongly convex. Our second contribution consists in providing analyticalvalues of the regularization parameter in order to approximately satisfyMorozov's discrepancy principle.
arxiv-3600-241 | From Bandits to Experts: A Tale of Domination and Independence | http://arxiv.org/pdf/1307.4564v1.pdf | author:Noga Alon, Nicolò Cesa-Bianchi, Claudio Gentile, Yishay Mansour category:cs.LG stat.ML published:2013-07-17 summary:We consider the partial observability model for multi-armed bandits,introduced by Mannor and Shamir. Our main result is a characterization ofregret in the directed observability model in terms of the dominating andindependence numbers of the observability graph. We also show that in theundirected case, the learner can achieve optimal regret without even accessingthe observability graph before selecting an action. Both results are shownusing variants of the Exp3 algorithm operating on the observability graph in atime-efficient manner.
arxiv-3600-242 | Mammogram Edge Detection Using Hybrid Soft Computing Methods | http://arxiv.org/pdf/1307.4516v1.pdf | author:I. Laurence Aroquiaraj, K. Thangavel category:cs.CV published:2013-07-17 summary:Image segmentation is a crucial step in a wide range of method imageprocessing systems. It is useful in visualization of the different objectspresent in the image. In spite of the several methods available in theliterature, image segmentation still a challenging problem in most of imageprocessing applications. The challenge comes from the fuzziness of imageobjects and the overlapping of the different regions. Detection of edges in animage is a very important step towards understanding image features. There arelarge numbers of edge detection operators available, each designed to besensitive to certain types of edges. The Quality of edge detection can bemeasured from several criteria objectively. Some criteria are proposed in termsof mathematical measurement, some of them are based on application andimplementation requirements. Since edges often occur at image locationsrepresenting object boundaries, edge detection is extensively used in imagesegmentation when images are divided into areas corresponding to differentobjects. This can be used specifically for enhancing the tumor area inmammographic images. Different methods are available for edge detection likeRoberts, Sobel, Prewitt, Canny, Log edge operators. In this paper a novelalgorithms for edge detection has been proposed for mammographic images. Breastboundary, pectoral region and tumor location can be seen clearly by using thismethod. For comparison purpose Roberts, Sobel, Prewitt, Canny, Log edgeoperators are used and their results are displayed. Experimental resultsdemonstrate the effectiveness of the proposed approach.
arxiv-3600-243 | The performance of orthogonal multi-matching pursuit under RIP | http://arxiv.org/pdf/1210.5323v3.pdf | author:Zhiqiang Xu category:cs.IT cs.LG math.IT math.NA published:2012-10-19 summary:The orthogonal multi-matching pursuit (OMMP) is a natural extension oforthogonal matching pursuit (OMP). We denote the OMMP with the parameter $M$ asOMMP(M) where $M\geq 1$ is an integer. The main difference between OMP andOMMP(M) is that OMMP(M) selects $M$ atoms per iteration, while OMP only addsone atom to the optimal atom set. In this paper, we study the performance oforthogonal multi-matching pursuit (OMMP) under RIP. In particular, we showthat, when the measurement matrix A satisfies $(9s, 1/10)$-RIP, there exists anabsolutely constant $M_0\leq 8$ so that OMMP(M_0) can recover $s$-sparse signalwithin $s$ iterations. We furthermore prove that, for slowly-decaying$s$-sparse signal, OMMP(M) can recover s-sparse signal within $O(\frac{s}{M})$iterations for a large class of $M$. In particular, for $M=s^a$ with $a\in[0,1/2]$, OMMP(M) can recover slowly-decaying $s$-sparse signal within$O(s^{1-a})$ iterations. The result implies that OMMP can reduce thecomputational complexity heavily.
arxiv-3600-244 | Universally Elevating the Phase Transition Performance of Compressed Sensing: Non-Isometric Matrices are Not Necessarily Bad Matrices | http://arxiv.org/pdf/1307.4502v1.pdf | author:Weiyu Xu, Myung Cho category:cs.IT math.IT math.OC stat.ML published:2013-07-17 summary:In compressed sensing problems, $\ell_1$ minimization or Basis Pursuit wasknown to have the best provable phase transition performance of recoverablesparsity among polynomial-time algorithms. It is of great theoretical andpractical interest to find alternative polynomial-time algorithms which performbetter than $\ell_1$ minimization. \cite{Icassp reweighted l_1}, \cite{Isitreweighted l_1}, \cite{XuScaingLaw} and \cite{iterativereweightedjournal} haveshown that a two-stage re-weighted $\ell_1$ minimization algorithm can boostthe phase transition performance for signals whose nonzero elements follow anamplitude probability density function (pdf) $f(\cdot)$ whose $t$-th derivative$f^{t}(0) \neq 0$ for some integer $t \geq 0$. However, for signals whosenonzero elements are strictly suspended from zero in distribution (for example,constant-modulus, only taking values `$+d$' or `$-d$' for some nonzero realnumber $d$), no polynomial-time signal recovery algorithms were known toprovide better phase transition performance than plain $\ell_1$ minimization,especially for dense sensing matrices. In this paper, we show that apolynomial-time algorithm can universally elevate the phase-transitionperformance of compressed sensing, compared with $\ell_1$ minimization, evenfor signals with constant-modulus nonzero elements. Contrary to conventionalwisdoms that compressed sensing matrices are desired to be isometric, we showthat non-isometric matrices are not necessarily bad sensing matrices. In thispaper, we also provide a framework for recovering sparse signals when sensingmatrices are not isometric.
arxiv-3600-245 | Veni Vidi Vici, A Three-Phase Scenario For Parameter Space Analysis in Image Analysis and Visualization | http://arxiv.org/pdf/1307.6544v1.pdf | author:M. A. El-Dosuky category:cs.CV published:2013-07-17 summary:Automatic analysis of the enormous sets of images is a critical task in lifesciences. This faces many challenges such as: algorithms are highlyparameterized, significant human input is intertwined, and lacking a standardmeta-visualization approach. This paper proposes an alternative iterativeapproach for optimizing input parameters, saving time by minimizing the userinvolvement, and allowing for understanding the workflow of algorithms anddiscovering new ones. The main focus is on developing an interactivevisualization technique that enables users to analyze the relationships betweensampled input parameters and corresponding output. This technique isimplemented as a prototype called Veni Vidi Vici, or "I came, I saw, Iconquered." This strategy is inspired by the mathematical formulas of numberingcomputable functions and is developed atop ImageJ, a scientific imageprocessing program. A case study is presented to investigate the proposedframework. Finally, the paper explores some potential future issues in theapplication of the proposed approach in parameter space analysis invisualization.
arxiv-3600-246 | A Lipschitz Exploration-Exploitation Scheme for Bayesian Optimization | http://arxiv.org/pdf/1204.0047v2.pdf | author:Ali Jalali, Javad Azimi, Xiaoli Fern, Ruofei Zhang category:cs.LG stat.ML published:2012-03-30 summary:The problem of optimizing unknown costly-to-evaluate functions has beenstudied for a long time in the context of Bayesian Optimization. Algorithms inthis field aim to find the optimizer of the function by asking only a fewfunction evaluations at locations carefully selected based on a posteriormodel. In this paper, we assume the unknown function is Lipschitz continuous.Leveraging the Lipschitz property, we propose an algorithm with a distinctexploration phase followed by an exploitation phase. The exploration phase aimsto select samples that shrink the search space as much as possible. Theexploitation phase then focuses on the reduced search space and selects samplesclosest to the optimizer. Considering the Expected Improvement (EI) as abaseline, we empirically show that the proposed algorithm significantlyoutperforms EI.
arxiv-3600-247 | The Fitness Level Method with Tail Bounds | http://arxiv.org/pdf/1307.4274v1.pdf | author:Carsten Witt category:cs.NE published:2013-07-16 summary:The fitness-level method, also called the method of f-based partitions, is anintuitive and widely used technique for the running time analysis of randomizedsearch heuristics. It was originally defined to prove upper and lower bounds onthe expected running time. Recently, upper tail bounds were added to thetechnique; however, these tail bounds only apply to running times that are atleast twice as large as the expectation. We remove this restriction and supplement the fitness-level method with sharptail bounds, including lower tails. As an exemplary application, we prove thatthe running time of randomized local search on OneMax is sharply concentratedaround n ln n - 0.1159 n.
arxiv-3600-248 | A Brief Review of Nature-Inspired Algorithms for Optimization | http://arxiv.org/pdf/1307.4186v1.pdf | author:Iztok Fister Jr., Xin-She Yang, Iztok Fister, Janez Brest, Dušan Fister category:cs.NE published:2013-07-16 summary:Swarm intelligence and bio-inspired algorithms form a hot topic in thedevelopments of new algorithms inspired by nature. These nature-inspiredmetaheuristic algorithms can be based on swarm intelligence, biologicalsystems, physical and chemical systems. Therefore, these algorithms can becalled swarm-intelligence-based, bio-inspired, physics-based andchemistry-based, depending on the sources of inspiration. Though not all ofthem are efficient, a few algorithms have proved to be very efficient and thushave become popular tools for solving real-world problems. Some algorithms areinsufficiently studied. The purpose of this review is to present a relativelycomprehensive list of all the algorithms in the literature, so as to inspirefurther research.
arxiv-3600-249 | Efficient Mixed-Norm Regularization: Algorithms and Safe Screening Methods | http://arxiv.org/pdf/1307.4156v1.pdf | author:Jie Wang, Jun Liu, Jieping Ye category:cs.LG stat.ML published:2013-07-16 summary:Sparse learning has recently received increasing attention in many areasincluding machine learning, statistics, and applied mathematics. The mixed-normregularization based on the l1q norm with q>1 is attractive in manyapplications of regression and classification in that it facilitates groupsparsity in the model. The resulting optimization problem is, however,challenging to solve due to the inherent structure of the mixed-normregularization. Existing work deals with special cases with q=1, 2, infinity,and they cannot be easily extended to the general case. In this paper, wepropose an efficient algorithm based on the accelerated gradient method forsolving the general l1q-regularized problem. One key building block of theproposed algorithm is the l1q-regularized Euclidean projection (EP_1q). Ourtheoretical analysis reveals the key properties of EP_1q and illustrates whyEP_1q for the general q is significantly more challenging to solve than thespecial cases. Based on our theoretical analysis, we develop an efficientalgorithm for EP_1q by solving two zero finding problems. To further improvethe efficiency of solving large dimensional mixed-norm regularized problems, wepropose a screening method which is able to quickly identify the inactivegroups, i.e., groups that have 0 components in the solution. This may lead tosubstantial reduction in the number of groups to be entered to theoptimization. An appealing feature of our screening method is that the data setneeds to be scanned only once to run the screening. Compared to that of solvingthe mixed-norm regularized problems, the computational cost of our screeningtest is negligible. The key of the proposed screening method is an accuratesensitivity analysis of the dual optimal solution when the regularizationparameter varies. Experimental results demonstrate the efficiency of theproposed algorithm.
arxiv-3600-250 | Modified SPLICE and its Extension to Non-Stereo Data for Noise Robust Speech Recognition | http://arxiv.org/pdf/1307.4048v1.pdf | author:D. S. Pavan Kumar, N. Vishnu Prasad, Vikas Joshi, S. Umesh category:cs.LG cs.CV stat.ML published:2013-07-15 summary:In this paper, a modification to the training process of the popular SPLICEalgorithm has been proposed for noise robust speech recognition. Themodification is based on feature correlations, and enables this stereo-basedalgorithm to improve the performance in all noise conditions, especially inunseen cases. Further, the modified framework is extended to work fornon-stereo datasets where clean and noisy training utterances, but not stereocounterparts, are required. Finally, an MLLR-based computationally efficientrun-time noise adaptation method in SPLICE framework has been proposed. Themodified SPLICE shows 8.6% absolute improvement over SPLICE in Test C ofAurora-2 database, and 2.93% overall. Non-stereo method shows 10.37% and 6.93%absolute improvements over Aurora-2 and Aurora-4 baseline models respectively.Run-time adaptation shows 9.89% absolute improvement in modified framework ascompared to SPLICE for Test C, and 4.96% overall w.r.t. standard MLLRadaptation on HMMs.
arxiv-3600-251 | An alternative Gospel of structure: order, composition, processes | http://arxiv.org/pdf/1307.4038v1.pdf | author:Bob Coecke category:math.CT cs.CL quant-ph published:2013-07-15 summary:We survey some basic mathematical structures, which arguably are moreprimitive than the structures taught at school. These structures are orders,with or without composition, and (symmetric) monoidal categories. We listseveral `real life' incarnations of each of these. This paper also serves as anintroduction to these structures and their current and potentially future usesin linguistics, physics and knowledge representation.
arxiv-3600-252 | Development of a Hindi Lemmatizer | http://arxiv.org/pdf/1305.6211v2.pdf | author:Snigdha Paul, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-05-24 summary:We live in a translingual society, in order to communicate with people fromdifferent parts of the world we need to have an expertise in their respectivelanguages. Learning all these languages is not at all possible; therefore weneed a mechanism which can do this task for us. Machine translators haveemerged as a tool which can perform this task. In order to develop a machinetranslator we need to develop several different rules. The very first modulethat comes in machine translation pipeline is morphological analysis. Stemmingand lemmatization comes under morphological analysis. In this paper we havecreated a lemmatizer which generates rules for removing the affixes along withthe addition of rules for creating a proper root word.
arxiv-3600-253 | Part of Speech Tagging of Marathi Text Using Trigram Method | http://arxiv.org/pdf/1307.4299v1.pdf | author:Jyoti Singh, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-07-15 summary:In this paper we present a Marathi part of speech tagger. It is amorphologically rich language. It is spoken by the native people ofMaharashtra. The general approach used for development of tagger is statisticalusing trigram Method. The main concept of trigram is to explore the most likelyPOS for a token based on given information of previous two tags by calculatingprobabilities to determine which is the best sequence of a tag. In this paperwe show the development of the tagger. Moreover we have also shown theevaluation done.
arxiv-3600-254 | Rule Based Transliteration Scheme for English to Punjabi | http://arxiv.org/pdf/1307.4300v1.pdf | author:Deepti Bhalla, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-07-15 summary:Machine Transliteration has come out to be an emerging and a very importantresearch area in the field of machine translation. Transliteration basicallyaims to preserve the phonological structure of words. Proper transliteration ofname entities plays a very significant role in improving the quality of machinetranslation. In this paper we are doing machine transliteration forEnglish-Punjabi language pair using rule based approach. We have constructedsome rules for syllabification. Syllabification is the process to extract orseparate the syllable from the words. In this we are calculating theprobabilities for name entities (Proper names and location). For those wordswhich do not come under the category of name entities, separate probabilitiesare being calculated by using relative frequency through a statistical machinetranslation toolkit known as MOSES. Using these probabilities we aretransliterating our input text from English to Punjabi.
arxiv-3600-255 | Learning Markov networks with context-specific independences | http://arxiv.org/pdf/1307.3964v1.pdf | author:Alejandro Edera, Federico Schlüter, Facundo Bromberg category:cs.AI cs.LG stat.ML published:2013-07-15 summary:Learning the Markov network structure from data is a problem that hasreceived considerable attention in machine learning, and in many otherapplication fields. This work focuses on a particular approach for this purposecalled independence-based learning. Such approach guarantees the learning ofthe correct structure efficiently, whenever data is sufficient for representingthe underlying distribution. However, an important issue of such approach isthat the learned structures are encoded in an undirected graph. The problemwith graphs is that they cannot encode some types of independence relations,such as the context-specific independences. They are a particular case ofconditional independences that is true only for a certain assignment of itsconditioning set, in contrast to conditional independences that must hold forall its assignments. In this work we present CSPC, an independence-basedalgorithm for learning structures that encode context-specific independences,and encoding them in a log-linear model, instead of a graph. The central ideaof CSPC is combining the theoretical guarantees provided by theindependence-based approach with the benefits of representing complexstructures by using features in a log-linear model. We present experiments in asynthetic case, showing that CSPC is more accurate than the state-of-the-art IBalgorithms when the underlying distribution contains CSIs.
arxiv-3600-256 | Bayesian Structured Prediction Using Gaussian Processes | http://arxiv.org/pdf/1307.3846v1.pdf | author:Sebastien Bratieres, Novi Quadrianto, Zoubin Ghahramani category:stat.ML cs.LG published:2013-07-15 summary:We introduce a conceptually novel structured prediction model, GPstruct,which is kernelized, non-parametric and Bayesian, by design. We motivate themodel with respect to existing approaches, among others, conditional randomfields (CRFs), maximum margin Markov networks (M3N), and structured supportvector machines (SVMstruct), which embody only a subset of its properties. Wepresent an inference procedure based on Markov Chain Monte Carlo. The frameworkcan be instantiated for a wide range of structured objects such as linearchains, trees, grids, and other general graphs. As a proof of concept, themodel is benchmarked on several natural language processing tasks and a videogesture segmentation task involving a linear chain structure. We showprediction accuracies for GPstruct which are comparable to or exceeding thoseof CRFs and SVMstruct.
arxiv-3600-257 | The Fundamental Learning Problem that Genetic Algorithms with Uniform Crossover Solve Efficiently and Repeatedly As Evolution Proceeds | http://arxiv.org/pdf/1307.3824v1.pdf | author:Keki M. Burjorjee category:cs.NE cs.AI cs.CC cs.DM cs.LG published:2013-07-15 summary:This paper establishes theoretical bonafides for implicit concurrentmultivariate effect evaluation--implicit concurrency for short---a broad andversatile computational learning efficiency thought to underliegeneral-purpose, non-local, noise-tolerant optimization in genetic algorithmswith uniform crossover (UGAs). We demonstrate that implicit concurrency isindeed a form of efficient learning by showing that it can be used to obtainclose-to-optimal bounds on the time and queries required to approximatelycorrectly solve a constrained version (k=7, \eta=1/5) of a recognizablecomputational learning problem: learning parities with noisy membershipqueries. We argue that a UGA that treats the noisy membership query oracle as afitness function can be straightforwardly used to approximately correctly learnthe essential attributes in O(log^1.585 n) queries and O(n log^1.585 n) time,where n is the total number of attributes. Our proof relies on an accessiblesymmetry argument and the use of statistical hypothesis testing to reject aglobal null hypothesis at the 10^-100 level of significance. It is, to the bestof our knowledge, the first relatively rigorous identification of efficientcomputational learning in an evolutionary algorithm on a non-trivial learningproblem.
arxiv-3600-258 | Multiview Hessian Discriminative Sparse Coding for Image Annotation | http://arxiv.org/pdf/1307.3811v1.pdf | author:Weifeng Liu, Dacheng Tao, Jun Cheng, Yuanyan Tang category:cs.MM cs.CV cs.IT math.IT published:2013-07-15 summary:Sparse coding represents a signal sparsely by using an overcompletedictionary, and obtains promising performance in practical computer visionapplications, especially for signal restoration tasks such as image denoisingand image inpainting. In recent years, many discriminative sparse codingalgorithms have been developed for classification problems, but they cannotnaturally handle visual data represented by multiview features. In addition,existing sparse coding algorithms use graph Laplacian to model the localgeometry of the data distribution. It has been identified that Laplacianregularization biases the solution towards a constant function which possiblyleads to poor extrapolating power. In this paper, we present multiview Hessiandiscriminative sparse coding (mHDSC) which seamlessly integrates Hessianregularization with discriminative sparse coding for multiview learningproblems. In particular, mHDSC exploits Hessian regularization to steer thesolution which varies smoothly along geodesics in the manifold, and treats thelabel information as an additional view of feature for incorporating thediscriminative power for image annotation. We conduct extensive experiments onPASCAL VOC'07 dataset and demonstrate the effectiveness of mHDSC for imageannotation.
arxiv-3600-259 | Probabilistic inverse reinforcement learning in unknown environments | http://arxiv.org/pdf/1307.3785v1.pdf | author:Aristide C. Y. Tossou, Christos Dimitrakakis category:stat.ML cs.LG published:2013-07-14 summary:We consider the problem of learning by demonstration from agents acting inunknown stochastic Markov environments or games. Our aim is to estimate agentpreferences in order to construct improved policies for the same task that theagents are trying to solve. To do so, we extend previous probabilisticapproaches for inverse reinforcement learning in known MDPs to the case ofunknown dynamics or opponents. We do this by deriving two simplifiedprobabilistic models of the demonstrator's policy and utility. Fortractability, we use maximum a posteriori estimation rather than full Bayesianinference. Under a flat prior, this results in a convex optimisation problem.We find that the resulting algorithms are highly competitive against a varietyof other methods for inverse reinforcement learning that do have knowledge ofthe dynamics.
arxiv-3600-260 | A Minimal Six-Point Auto-Calibration Algorithm | http://arxiv.org/pdf/1307.3759v1.pdf | author:Evgeniy Martyushev category:cs.CV published:2013-07-14 summary:A non-iterative auto-calibration algorithm is presented. It deals with aminimal set of six scene points in three views taken by a camera with fixed butunknown intrinsic parameters. Calibration is based on the image correspondencesonly. The algorithm is implemented and validated on synthetic image data.
arxiv-3600-261 | Map of Life: Measuring and Visualizing Species' Relatedness with "Molecular Distance Maps" | http://arxiv.org/pdf/1307.3755v1.pdf | author:Lila Kari, Kathleen A. Hill, Abu Sadat Sayem, Nathaniel Bryans, Katelyn Davis, Nikesh S. Dattani category:q-bio.GN cs.CV q-bio.PE q-bio.QM 92, 68 published:2013-07-14 summary:We propose a novel combination of methods that (i) portrays quantitativecharacteristics of a DNA sequence as an image, (ii) computes distances betweenthese images, and (iii) uses these distances to output a map wherein eachsequence is a point in a common Euclidean space. In the resulting "MolecularDistance Map" each point signifies a DNA sequence, and the geometric distancebetween any two points reflects the degree of relatedness between thecorresponding sequences and species. Molecular Distance Maps present compelling visual representations ofrelationships between species and could be used for taxonomic clarifications,for species identification, and for studies of evolutionary history. One of theadvantages of this method is its general applicability since, as sequencealignment is not required, the DNA sequences chosen for comparison can becompletely different regions in different genomes. In fact, this method can beused to compare any two DNA sequences. For example, in our dataset of 3,176mitochondrial DNA sequences, it correctly finds the mtDNA sequences mostclosely related to that of the anatomically modern human (the Neanderthal, theDenisovan, and the chimp), and it finds that the sequence most different fromit belongs to a cucumber. Furthermore, our method can be used to compare realsequences to artificial, computer-generated, DNA sequences. For example, it isused to determine that the distances between a Homo sapiens sapiens mtDNA andartificial sequences of the same length and same trinucleotide frequencies canbe larger than the distance between the same human mtDNA and the mtDNA of afruit-fly. We demonstrate this method's promising potential for taxonomicalclarifications by applying it to a diverse variety of cases that have beenhistorically controversial, such as the genus Polypterus, the family Tarsiidae,and the vast (super)kingdom Protista.
arxiv-3600-262 | On Analyzing Estimation Errors due to Constrained Connections in Online Review Systems | http://arxiv.org/pdf/1307.3687v1.pdf | author:Junzhou Zhao category:cs.SI cs.LG published:2013-07-14 summary:Constrained connection is the phenomenon that a reviewer can only review asubset of products/services due to narrow range of interests or limitedattention capacity. In this work, we study how constrained connections canaffect estimation performance in online review systems (ORS). We find thatreviewers' constrained connections will cause poor estimation performance, bothfrom the measurements of estimation accuracy and Bayesian Cramer Rao lowerbound.
arxiv-3600-263 | Online Stochastic Optimization with Multiple Objectives | http://arxiv.org/pdf/1211.6013v2.pdf | author:Mehrdad Mahdavi, Tianbao Yang, Rong Jin category:cs.LG math.OC published:2012-11-26 summary:In this paper we propose a general framework to characterize and solve thestochastic optimization problems with multiple objectives underlying many realworld learning applications. We first propose a projection based algorithmwhich attains an $O(T^{-1/3})$ convergence rate. Then, by leveraging on thetheory of Lagrangian in constrained optimization, we devise a novel primal-dualstochastic approximation algorithm which attains the optimal convergence rateof $O(T^{-1/2})$ for general Lipschitz continuous objectives.
arxiv-3600-264 | Computational Model of Music Sight Reading: A Reinforcement Learning Approach | http://arxiv.org/pdf/1007.0546v4.pdf | author:Keyvan Yahya, Pouyan Rafiei Fard category:cs.AI cs.LG cs.NE math.OC published:2010-07-04 summary:Although the Music Sight Reading process has been studied from the cognitivepsychology view points, but the computational learning methods like theReinforcement Learning have not yet been used to modeling of such processes. Inthis paper, with regards to essential properties of our specific problem, weconsider the value function concept and will indicate that the optimum policycan be obtained by the method we offer without to be getting involved withcomputing of the complex value functions. Also, we will offer a normativebehavioral model for the interaction of the agent with the musical pitchenvironment and by using a slightly different version of Partially observableMarkov decision processes we will show that our method helps for fasterlearning of state-action pairs in our implemented agents.
arxiv-3600-265 | Minimum Error Rate Training and the Convex Hull Semiring | http://arxiv.org/pdf/1307.3675v1.pdf | author:Chris Dyer category:cs.LG I.2.6; I.2.7 published:2013-07-13 summary:We describe the line search used in the minimum error rate training algorithmMERT as the "inside score" of a weighted proof forest under a semiring definedin terms of well-understood operations from computational geometry. Thisconception leads to a straightforward complexity analysis of the dynamicprogramming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009)and practical approaches to implementation.
arxiv-3600-266 | A Data Management Approach for Dataset Selection Using Human Computation | http://arxiv.org/pdf/1307.3673v1.pdf | author:Alexandros Ntoulas, Omar Alonso, Vasilis Kandylas category:cs.LG cs.IR published:2013-07-13 summary:As the number of applications that use machine learning algorithms increases,the need for labeled data useful for training such algorithms intensifies. Getting labels typically involves employing humans to do the annotation,which directly translates to training and working costs. Crowdsourcingplatforms have made labeling cheaper and faster, but they still involvesignificant costs, especially for the cases where the potential set ofcandidate data to be labeled is large. In this paper we describe a methodologyand a prototype system aiming at addressing this challenge for Web-scaleproblems in an industrial setting. We discuss ideas on how to efficientlyselect the data to use for training of machine learning algorithms in anattempt to reduce cost. We show results achieving good performance with reducedcost by carefully selecting which instances to label. Our proposed algorithm ispresented as part of a framework for managing and generating training datasets,which includes, among other components, a human computation element.
arxiv-3600-267 | Knowledge Matters: Importance of Prior Information for Optimization | http://arxiv.org/pdf/1301.4083v6.pdf | author:Çağlar Gülçehre, Yoshua Bengio category:cs.LG cs.CV cs.NE stat.ML published:2013-01-17 summary:We explore the effect of introducing prior information into the intermediatelevel of neural networks for a learning task on which all the state-of-the-artmachine learning algorithms tested failed to learn. We motivate our work fromthe hypothesis that humans learn such intermediate concepts from otherindividuals via a form of supervision or guidance using a curriculum. Theexperiments we have conducted provide positive evidence in favor of thishypothesis. In our experiments, a two-tiered MLP architecture is trained on adataset with 64x64 binary inputs images, each image with three sprites. Thefinal task is to decide whether all the sprites are the same or one of them isdifferent. Sprites are pentomino tetris shapes and they are placed in an imagewith different locations using scaling and rotation transformations. The firstpart of the two-tiered MLP is pre-trained with intermediate-level targets beingthe presence of sprites at each location, while the second part takes theoutput of the first part as input and predicts the final task's target binaryevent. The two-tiered MLP architecture, with a few tens of thousand examples,was able to learn the task perfectly, whereas all other algorithms (includeunsupervised pre-training, but also traditional algorithms like SVMs, decisiontrees and boosting) all perform no better than chance. We hypothesize that theoptimization difficulty involved when the intermediate pre-training is notperformed is due to the {\em composition} of two highly non-linear tasks. Ourfindings are also consistent with hypotheses on cultural learning inspired bythe observations of optimization problems with deep learning, presumablybecause of effective local minima.
arxiv-3600-268 | On-line Bayesian parameter estimation in general non-linear state-space models: A tutorial and new results | http://arxiv.org/pdf/1307.3490v1.pdf | author:Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni, J. Fraser Forbes category:stat.CO stat.AP stat.ME stat.ML published:2013-07-12 summary:On-line estimation plays an important role in process control and monitoring.Obtaining a theoretical solution to the simultaneous state-parameter estimationproblem for non-linear stochastic systems involves solving complexmulti-dimensional integrals that are not amenable to analytical solution. Whilebasic sequential Monte-Carlo (SMC) or particle filtering (PF) algorithms forsimultaneous estimation exist, it is well recognized that there is a need formaking these on-line algorithms non-degenerate, fast and applicable toprocesses with missing measurements. To overcome the deficiencies intraditional algorithms, this work proposes a Bayesian approach to on-line stateand parameter estimation. Its extension to handle missing data in real-time isalso provided. The simultaneous estimation is performed by filtering anextended vector of states and parameters using an adaptivesequential-importance-resampling (SIR) filter with a kernel density estimationmethod. The approach uses an on-line optimization algorithm based onKullback-Leibler (KL) divergence to allow adaptation of the SIR filter forcombined state-parameter estimation. An optimal tuning rule to control thewidth of the kernel and the variance of the artificial noise added to theparameters is also proposed. The approach is illustrated through numericalexamples.
arxiv-3600-269 | Energy-aware adaptive bi-Lipschitz embeddings | http://arxiv.org/pdf/1307.3457v1.pdf | author:Bubacarr Bah, Ali Sadeghian, Volkan Cevher category:cs.LG cs.IT math.IT 68Q99 published:2013-07-12 summary:We propose a dimensionality reducing matrix design based on training datawith constraints on its Frobenius norm and number of rows. Our design criteriais aimed at preserving the distances between the data points in thedimensionality reduced space as much as possible relative to their distances inoriginal data space. This approach can be considered as a deterministicBi-Lipschitz embedding of the data points. We introduce a scalable learningalgorithm, dubbed AMUSE, and provide a rigorous estimation guarantee byleveraging game theoretic tools. We also provide a generalizationcharacterization of our matrix based on our sample data. We use compressivesensing problems as an example application of our problem, where the Frobeniusnorm design constraint translates into the sensing energy.
arxiv-3600-270 | Speedy Object Detection based on Shape | http://arxiv.org/pdf/1307.3439v1.pdf | author:Y. Jayanta Singh, Shalu Gupta category:cs.CV published:2013-07-12 summary:This study is a part of design of an audio system for in-house objectdetection system for visually impaired, low vision personnel by birth or by anaccident or due to old age. The input of the system will be scene and output asaudio. Alert facility is provided based on severity levels of the objects(snake, broke glass etc) and also during difficulties. The study proposedtechniques to provide speedy detection of objects based on shapes and itsscale. Features are extraction to have minimum spaces using dynamic scaling.From a scene, clusters of objects are formed based on the scale and shape.Searching is performed among the clusters initially based on the shape, scale,mean cluster value and index of object(s). The minimum operation to detect thepossible shape of the object is performed. In case the object does not have alikely matching shape, scale etc, then the several operations required for anobject detection will not perform; instead, it will declared as a new object.In such way, this study finds a speedy way of detecting objects.
arxiv-3600-271 | Thompson Sampling for 1-Dimensional Exponential Family Bandits | http://arxiv.org/pdf/1307.3400v1.pdf | author:Nathaniel Korda, Emilie Kaufmann, Remi Munos category:stat.ML published:2013-07-12 summary:Thompson Sampling has been demonstrated in many complex bandit models,however the theoretical guarantees available for the parametric multi-armedbandit are still limited to the Bernoulli case. Here we extend them by provingasymptotic optimality of the algorithm using the Jeffreys prior for1-dimensional exponential family bandits. Our proof builds on previous work,but also makes extensive use of closed forms for Kullback-Leibler divergenceand Fisher information (and thus Jeffreys prior) available in an exponentialfamily. This allow us to give a finite time exponential concentrationinequality for posterior distributions on exponential families that may be ofinterest in its own right. Moreover our analysis covers some distributions forwhich no optimistic algorithm has yet been proposed, including heavy-tailedexponential families.
arxiv-3600-272 | Performance Analysis of Clustering Algorithms for Gene Expression Data | http://arxiv.org/pdf/1307.3549v1.pdf | author:T. Chandrasekhar, K. Thangavel, E. Elayaraja category:cs.CE cs.LG published:2013-07-12 summary:Microarray technology is a process that allows thousands of genessimultaneously monitor to various experimental conditions. It is used toidentify the co-expressed genes in specific cells or tissues that are activelyused to make proteins, This method is used to analysis the gene expression, animportant task in bioinformatics research. Cluster analysis of gene expressiondata has proved to be a useful tool for identifying co-expressed genes,biologically relevant groupings of genes and samples. In this paper we analysedK-Means with Automatic Generations of Merge Factor for ISODATA- AGMFI, to groupthe microarray data sets on the basic of ISODATA. AGMFI is to generate initialvalues for merge and Spilt factor, maximum merge times instead of selectingefficient values as in ISODATA. The initial seeds for each cluster werenormally chosen either sequentially or randomly. The quality of the finalclusters was found to be influenced by these initial seeds. For the real lifeproblems, the suitable number of clusters cannot be predicted. To overcome theabove drawback the current research focused on developing the clusteringalgorithms without giving the initial number of clusters.
arxiv-3600-273 | Unsupervised Gene Expression Data using Enhanced Clustering Method | http://arxiv.org/pdf/1307.3337v1.pdf | author:T. Chandrasekhar, K. Thangavel, E. Elayaraja, E. N. Sathishkumar category:cs.CE cs.LG published:2013-07-12 summary:Microarrays are made it possible to simultaneously monitor the expressionprofiles of thousands of genes under various experimental conditions.Identification of co-expressed genes and coherent patterns is the central goalin microarray or gene expression data analysis and is an important task inbioinformatics research. Feature selection is a process to select featureswhich are more informative. It is one of the important steps in knowledgediscovery. The problem is that not all features are important. Some of thefeatures may be redundant, and others may be irrelevant and noisy. In this workthe unsupervised Gene selection method and Enhanced Center InitializationAlgorithm (ECIA) with K-Means algorithms have been applied for clustering ofGene Expression Data. This proposed clustering algorithm overcomes thedrawbacks in terms of specifying the optimal number of clusters andinitialization of good cluster centroids. Gene Expression Data show that couldidentify compact clusters with performs well in terms of the SilhouetteCoefficients cluster measure.
arxiv-3600-274 | Opinion Mining and Analysis: A survey | http://arxiv.org/pdf/1307.3336v1.pdf | author:Arti Buche, Dr. M. B. Chandak, Akshay Zadgaonkar category:cs.CL cs.IR published:2013-07-12 summary:The current research is focusing on the area of Opinion Mining also called assentiment analysis due to sheer volume of opinion rich web resources such asdiscussion forums, review sites and blogs are available in digital form. Oneimportant problem in sentiment analysis of product reviews is to producesummary of opinions based on product features. We have surveyed and analyzed inthis paper, various techniques that have been developed for the key tasks ofopinion mining. We have provided an overall picture of what is involved indeveloping a software system for opinion mining on the basis of our survey andanalysis.
arxiv-3600-275 | Hierarchical Approach for Total Variation Digital Image Inpainting | http://arxiv.org/pdf/1207.3576v2.pdf | author:S. Padmavathi, N. Archana, K. P. Soman category:cs.CV published:2012-07-16 summary:The art of recovering an image from damage in an undetectable form is knownas inpainting. The manual work of inpainting is most often a very timeconsuming process. Due to digitalization of this technique, it is automatic andfaster. In this paper, after the user selects the regions to be reconstructed,the algorithm automatically reconstruct the lost regions with the help of theinformation surrounding them. The existing methods perform very well when theregion to be reconstructed is very small, but fails in proper reconstruction asthe area increases. This paper describes a Hierarchical method by which thearea to be inpainted is reduced in multiple levels and Total Variation(TV)method is used to inpaint in each level. This algorithm gives betterperformance when compared with other existing algorithms such as nearestneighbor interpolation, Inpainting through Blurring and Sobolev Inpainting.
arxiv-3600-276 | A Reliable Effective Terascale Linear Learning System | http://arxiv.org/pdf/1110.4198v3.pdf | author:Alekh Agarwal, Olivier Chapelle, Miroslav Dudik, John Langford category:cs.LG stat.ML published:2011-10-19 summary:We present a system and a set of techniques for learning linear predictorswith convex losses on terascale datasets, with trillions of features, {Thenumber of features here refers to the number of non-zero entries in the datamatrix.} billions of training examples and millions of parameters in an hourusing a cluster of 1000 machines. Individually none of the component techniquesare new, but the careful synthesis required to obtain an efficientimplementation is. The result is, up to our knowledge, the most scalable andefficient linear learning system reported in the literature (as of 2011 whenour experiments were conducted). We describe and thoroughly evaluate thecomponents of the system, showing the importance of the various design choices.
arxiv-3600-277 | Improving the quality of Gujarati-Hindi Machine Translation through part-of-speech tagging and stemmer-assisted transliteration | http://arxiv.org/pdf/1307.3310v1.pdf | author:Juhi Ameta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-07-12 summary:Machine Translation for Indian languages is an emerging research area.Transliteration is one such module that we design while designing a translationsystem. Transliteration means mapping of source language text into the targetlanguage. Simple mapping decreases the efficiency of overall translationsystem. We propose the use of stemming and part-of-speech tagging fortransliteration. The effectiveness of translation can be improved if we usepart-of-speech tagging and stemming assisted transliteration.We have shown thatmuch of the content in Gujarati gets transliterated while being processed fortranslation to Hindi language.
arxiv-3600-278 | Fuzzy Fibers: Uncertainty in dMRI Tractography | http://arxiv.org/pdf/1307.3271v1.pdf | author:Thomas Schultz, Anna Vilanova, Ralph Brecheisen, Gordon Kindlmann category:cs.CV published:2013-07-11 summary:Fiber tracking based on diffusion weighted Magnetic Resonance Imaging (dMRI)allows for noninvasive reconstruction of fiber bundles in the human brain. Inthis chapter, we discuss sources of error and uncertainty in this technique,and review strategies that afford a more reliable interpretation of theresults. This includes methods for computing and rendering probabilistictractograms, which estimate precision in the face of measurement noise andartifacts. However, we also address aspects that have received less attentionso far, such as model selection, partial voluming, and the impact ofparameters, both in preprocessing and in fiber tracking itself. We conclude bygiving impulses for future research.
arxiv-3600-279 | Minimum Distance Estimation for Robust High-Dimensional Regression | http://arxiv.org/pdf/1307.3227v1.pdf | author:Aurélie C. Lozano, Nicolai Meinshausen category:stat.ME stat.ML published:2013-07-11 summary:We propose a minimum distance estimation method for robust regression insparse high-dimensional settings. The traditional likelihood-based estimatorslack resilience against outliers, a critical issue when dealing withhigh-dimensional noisy data. Our method, Minimum Distance Lasso (MD-Lasso),combines minimum distance functionals, customarily used in nonparametricestimation for their robustness, with l1-regularization for high-dimensionalregression. The geometry of MD-Lasso is key to its consistency and robustness.The estimator is governed by a scaling parameter that caps the influence ofoutliers: the loss per observation is locally convex and close to quadratic forsmall squared residuals, and flattens for squared residuals larger than thescaling parameter. As the parameter approaches infinity, the estimator becomesequivalent to least-squares Lasso. MD-Lasso enjoys fast convergence rates undermild conditions on the model error distribution, which hold for any of thesolutions in a convexity region around the true parameter and in certain casesfor every solution. Remarkably, a first-order optimization method is able toproduce iterates very close to the consistent solutions, with geometricconvergence and regardless of the initialization. A connection is establishedwith re-weighted least-squares that intuitively explains MD-Lasso robustness.The merits of our method are demonstrated through simulation and eQTL dataanalysis.
arxiv-3600-280 | Contrast Enhancement And Brightness Preservation Using Multi- Decomposition Histogram Equalization | http://arxiv.org/pdf/1307.3054v1.pdf | author:Sayali Nimkar, Sanal Varghese, Sucheta Shrivastava category:cs.CV published:2013-07-11 summary:Histogram Equalization (HE) has been an essential addition to the ImageEnhancement world. Enhancement techniques like Classical Histogram Equalization(CHE), Adaptive Histogram Equalization (ADHE), Bi-Histogram Equalization (BHE)and Recursive Mean Separate Histogram Equalization (RMSHE) methods enhancecontrast, however, brightness is not well preserved with these methods, whichgives an unpleasant look to the final image obtained. Thus, we introduce anovel technique Multi-Decomposition Histogram Equalization (MDHE) to eliminatethe drawbacks of the earlier methods. In MDHE, we have decomposed the inputsixty-four parts, applied CHE in each of the sub-images and then finallyinterpolated them in correct order. The final image after MDHE results incontrast enhanced and brightness preserved image compared to all othertechniques mentioned above. We have calculated the various parameters likePSNR, SNR, RMSE, MSE, etc. for every technique. Our results are well supportedby bar graphs, histograms and the parameter calculations at the end.
arxiv-3600-281 | Application of three graph Laplacian based semi-supervised learning methods to protein function prediction problem | http://arxiv.org/pdf/1211.4289v3.pdf | author:Loc Tran category:cs.LG cs.CE q-bio.QM stat.ML H.2.8 published:2012-11-19 summary:Protein function prediction is the important problem in modern biology. Inthis paper, the un-normalized, symmetric normalized, and random walk graphLaplacian based semi-supervised learning methods will be applied to theintegrated network combined from multiple networks to predict the functions ofall yeast proteins in these multiple networks. These multiple networks arenetwork created from Pfam domain structure, co-participation in a proteincomplex, protein-protein interaction network, genetic interaction network, andnetwork created from cell cycle gene expression measurements. Multiple networksare combined with fixed weights instead of using convex optimization todetermine the combination weights due to high time complexity of convexoptimization method. This simple combination method will not affect theaccuracy performance measures of the three semi-supervised learning methods.Experiment results show that the un-normalized and symmetric normalized graphLaplacian based methods perform slightly better than random walk graphLaplacian based method for integrated network. Moreover, the accuracyperformance measures of these three semi-supervised learning methods forintegrated network are much better than the best accuracy performance measuresof these three methods for the individual network.
arxiv-3600-282 | Genetic approach for arabic part of speech tagging | http://arxiv.org/pdf/1307.3489v1.pdf | author:Bilel Ben Ali, Fethi Jarray category:cs.CL cs.NE 68T50 published:2013-07-11 summary:With the growing number of textual resources available, the ability tounderstand them becomes critical. An essential first step in understandingthese sources is the ability to identify the part of speech in each sentence.Arabic is a morphologically rich language, wich presents a challenge for partof speech tagging. In this paper, our goal is to propose, improve and implementa part of speech tagger based on a genetic alorithm. The accuracy obtained withthis method is comparable to that of other probabilistic approaches.
arxiv-3600-283 | A New Approach to the Solution of Economic Dispatch Using Particle Swarm Optimization with Simulated Annealing | http://arxiv.org/pdf/1307.3014v1.pdf | author:V. Karthikeyan, S. Senthilkumar, V. J. Vijayalakshmi category:cs.CE cs.NE published:2013-07-11 summary:A new approach to the solution of Economic Dispatch using Particle SwarmOptimization is presented. It is the progression of allocating productionamongst the dedicated units such that the restriction forced are fulfilled andthe power needs are reduced. More just, the soft computing method has receivedsupplementary concentration and was used in a quantity of successful andsensible applications. Here, an attempt has been made to find out the minimumcost by using Particle Swarm Optimization Algorithm using the data of threegenerating units. In this work, data has been taken such as the losscoefficients with the max-min power limit and cost function. PSO and SimulatedAnnealing are functional to put out the least amount for dissimilar energyrequirements. When the outputs are compared with the conventional method, PSOseems to give an improved result with enhanced convergence feature. All themethods are executed in MATLAB environment. The effectiveness and feasibilityof the proposed method were demonstrated by three generating units case study.Output gives hopeful results, signifying that the projected method ofcalculation is competent of economically formative advanced eminence solutionsaddressing economic dispatch problems.
arxiv-3600-284 | Conversion of Braille to Text in English, Hindi and Tamil Languages | http://arxiv.org/pdf/1307.2997v1.pdf | author:S. Padmavathi, Manojna K. S. S, S. Sphoorthy Reddy, D. Meenakshy category:cs.CV published:2013-07-11 summary:The Braille system has been used by the visually impaired for reading andwriting. Due to limited availability of the Braille text books an efficientusage of the books becomes a necessity. This paper proposes a method to converta scanned Braille document to text which can be read out to many through thecomputer. The Braille documents are pre processed to enhance the dots andreduce the noise. The Braille cells are segmented and the dots from each cellis extracted and converted in to a number sequence. These are mapped to theappropriate alphabets of the language. The converted text is spoken out througha speech synthesizer. The paper also provides a mechanism to type the Braillecharacters through the number pad of the keyboard. The typed Braille characteris mapped to the alphabet and spoken out. The Braille cell has a standardrepresentation but the mapping differs for each language. In this paper mappingof English, Hindi and Tamil are considered.
arxiv-3600-285 | Accuracy of MAP segmentation with hidden Potts and Markov mesh prior models via Path Constrained Viterbi Training, Iterated Conditional Modes and Graph Cut based algorithms | http://arxiv.org/pdf/1307.2971v1.pdf | author:Ana Georgina Flesia, Josef Baumgartner, Javier Gimenez, Jorge Martinez category:cs.LG cs.CV stat.ML published:2013-07-11 summary:In this paper, we study statistical classification accuracy of two differentMarkov field environments for pixelwise image segmentation, considering thelabels of the image as hidden states and solving the estimation of such labelsas a solution of the MAP equation. The emission distribution is assumed thesame in all models, and the difference lays in the Markovian prior hypothesismade over the labeling random field. The a priori labeling knowledge will bemodeled with a) a second order anisotropic Markov Mesh and b) a classicalisotropic Potts model. Under such models, we will consider three differentsegmentation procedures, 2D Path Constrained Viterbi training for the HiddenMarkov Mesh, a Graph Cut based segmentation for the first order isotropic Pottsmodel, and ICM (Iterated Conditional Modes) for the second order isotropicPotts model. We provide a unified view of all three methods, and investigate goodness offit for classification, studying the influence of parameter estimation,computational gain, and extent of automation in the statistical measuresOverall Accuracy, Relative Improvement and Kappa coefficient, allowing robustand accurate statistical analysis on synthetic and real-life experimental datacoming from the field of Dental Diagnostic Radiography. All algorithms, usingthe learned parameters, generate good segmentations with little interactionwhen the images have a clear multimodal histogram. Suboptimal learning provesto be frail in the case of non-distinctive modes, which limits the complexityof usable models, and hence the achievable error rate as well. All Matlab code written is provided in a toolbox available for download fromour website, following the Reproducible Research Paradigm.
arxiv-3600-286 | Online Alternating Direction Method (longer version) | http://arxiv.org/pdf/1306.3721v2.pdf | author:Huahua Wang, Arindam Banerjee category:cs.LG math.OC published:2013-06-17 summary:Online optimization has emerged as powerful tool in large scale optimization.In this pa- per, we introduce efficient online optimization algorithms based onthe alternating direction method (ADM), which can solve online convexoptimization under linear constraints where the objective could be non-smooth.We introduce new proof techniques for ADM in the batch setting, which yields aO(1/T) convergence rate for ADM and forms the basis for regret anal- ysis inthe online setting. We consider two scenarios in the online setting, based onwhether an additional Bregman divergence is needed or not. In both settings, weestablish regret bounds for both the objective function as well as constraintsviolation for general and strongly convex functions. We also consider inexactADM updates where certain terms are linearized to yield efficient updates andshow the stochastic convergence rates. In addition, we briefly discuss thatonline ADM can be used as projection- free online learning algorithm in somescenarios. Preliminary results are presented to illustrate the performance ofthe proposed algorithms.
arxiv-3600-287 | A Polynomial Time Algorithm for Lossy Population Recovery | http://arxiv.org/pdf/1302.1515v2.pdf | author:Ankur Moitra, Michael Saks category:cs.DS cs.LG published:2013-02-06 summary:We give a polynomial time algorithm for the lossy population recoveryproblem. In this problem, the goal is to approximately learn an unknowndistribution on binary strings of length $n$ from lossy samples: for someparameter $\mu$ each coordinate of the sample is preserved with probability$\mu$ and otherwise is replaced by a `?'. The running time and number ofsamples needed for our algorithm is polynomial in $n$ and $1/\varepsilon$ foreach fixed $\mu>0$. This improves on algorithm of Wigderson and Yehudayoff thatruns in quasi-polynomial time for any $\mu > 0$ and the polynomial timealgorithm of Dvir et al which was shown to work for $\mu \gtrapprox 0.30$ byBatman et al. In fact, our algorithm also works in the more general frameworkof Batman et al. in which there is no a priori bound on the size of the supportof the distribution. The algorithm we analyze is implicit in previous work; ourmain contribution is to analyze the algorithm by showing (via linearprogramming duality and connections to complex analysis) that a certain matrixassociated with the problem has a robust local inverse even though itscondition number is exponentially small. A corollary of our result is the firstpolynomial time algorithm for learning DNFs in the restriction access model ofDvir et al.
arxiv-3600-288 | Anisotropic Diffusion for Details Enhancement in Multi-Exposure Image Fusion | http://arxiv.org/pdf/1307.2818v1.pdf | author:Harbinder Singh, Vinay Kumar, Sunil Bhooshan category:cs.MM cs.CV published:2013-07-10 summary:We develop a multiexposure image fusion method based on texture features,which exploits the edge preserving and intraregion smoothing property ofnonlinear diffusion filters based on partial differential equations (PDE). Withthe captured multiexposure image series, we first decompose images into baselayers and detail layers to extract sharp details and fine details,respectively. The magnitude of the gradient of the image intensity is utilizedto encourage smoothness at homogeneous regions in preference to inhomogeneousregions. Then, we have considered texture features of the base layer togenerate a mask (i.e., decision mask) that guides the fusion of base layers inmultiresolution fashion. Finally, well-exposed fused image is obtained thatcombines fused base layer and the detail layers at each scale across all theinput exposures. Proposed algorithm skipping complex High Dynamic Range Image(HDRI) generation and tone mapping steps to produce detail preserving image fordisplay on standard dynamic range display devices. Moreover, our technique iseffective for blending flash/no-flash image pair and multifocus images, thatis, images focused on different targets.
arxiv-3600-289 | Optimisation dans la détection de communautés recouvrantes et équilibre de Nash | http://arxiv.org/pdf/1307.2715v1.pdf | author:Michel Crampes, Michel Plantié, Marie Lopez category:stat.ML published:2013-07-10 summary:Community detection in graphs has been the subject of many algorithms. Recentmethods want to optimize a modularity function which shows a maximum ofrelationships within communities and found a minimum of inter-communityrelations. these algorithms are applied to unipartite, multipartite anddirected graphs. However, given the NP-completeness of the problem, thesealgorithms are heuristics that do not guarantee an optimum. In this paper weintroduce an algorithm which, based on an approximate solution obtained througha efficient detection algorithm, modifie it to achieve a local optimum based ona function. this reassignment function is a potential function and thereforethe computed optimum is a Nash equilibrium. We supplement our method with anoverlap function that allows to have simultaneously the two detection modes.Several experiments show the interest of our approach.
arxiv-3600-290 | Error Rate Bounds in Crowdsourcing Models | http://arxiv.org/pdf/1307.2674v1.pdf | author:Hongwei Li, Bin Yu, Dengyong Zhou category:stat.ML cs.LG stat.AP published:2013-07-10 summary:Crowdsourcing is an effective tool for human-powered computation on manytasks challenging for computers. In this paper, we provide finite-sampleexponential bounds on the error rate (in probability and in expectation) ofhyperplane binary labeling rules under the Dawid-Skene crowdsourcing model. Thebounds can be applied to analyze many common prediction methods, including themajority voting and weighted majority voting. These bound results could beuseful for controlling the error rate and designing better algorithms. We showthat the oracle Maximum A Posterior (MAP) rule approximately optimizes ourupper bound on the mean error rate for any hyperplane binary labeling rule, andpropose a simple data-driven weighted majority voting (WMV) rule (calledone-step WMV) that attempts to approximate the oracle MAP and has a provabletheoretical guarantee on the error rate. Moreover, we use simulated and realdata to demonstrate that the data-driven EM-MAP rule is a good approximation tothe oracle MAP rule, and to demonstrate that the mean error rate of thedata-driven EM-MAP rule is also bounded by the mean error rate bound of theoracle MAP rule with estimated parameters plugging into the bound.
arxiv-3600-291 | Selection Mammogram Texture Descriptors Based on Statistics Properties Backpropagation Structure | http://arxiv.org/pdf/1307.6542v1.pdf | author:Shofwatul 'Uyun, Sri Hartati, Agus Harjoko, Subanar category:cs.CV published:2013-07-10 summary:Computer Aided Diagnosis (CAD) system has been developed for the earlydetection of breast cancer, one of the most deadly cancer for women. The benignof mammogram has different texture from malignant. There are fifty mammogramimages used in this work which are divided for training and testing. Therefore,the selection of the right texture to determine the level of accuracy of CADsystem is important. The first and second order statistics are the texturefeature extraction methods which can be used on a mammogram. This workclassifies texture descriptor into nine groups where the extraction of featuresis classified using backpropagation learning with two types of multi-layerperceptron (MLP). The best texture descriptor as selected when the value ofregression 1 appears in both the MLP-1 and the MLP-2 with the number of epochesless than 1000. The results of testing show that the best selected texturedescriptor is the second order (combination) using all direction (0, 45, 90 and135) that have twenty four descriptors.
arxiv-3600-292 | Controlling the Precision-Recall Tradeoff in Differential Dependency Network Analysis | http://arxiv.org/pdf/1307.2611v1.pdf | author:Diane Oyen, Alexandru Niculescu-Mizil, Rachel Ostroff, Alex Stewart, Vincent P. Clark category:stat.ML cs.LG published:2013-07-09 summary:Graphical models have gained a lot of attention recently as a tool forlearning and representing dependencies among variables in multivariate data.Often, domain scientists are looking specifically for differences among thedependency networks of different conditions or populations (e.g. differencesbetween regulatory networks of different species, or differences betweendependency networks of diseased versus healthy populations). The standardmethod for finding these differences is to learn the dependency networks foreach condition independently and compare them. We show that this approach isprone to high false discovery rates (low precision) that can render theanalysis useless. We then show that by imposing a bias towards learning similardependency networks for each condition the false discovery rates can be reducedto acceptable levels, at the cost of finding a reduced number of differences.Algorithms developed in the transfer learning literature can be used to varythe strength of the imposed similarity bias and provide a natural mechanism tosmoothly adjust this differential precision-recall tradeoff to cater to therequirements of the analysis conducted. We present real case studies(oncological and neurological) where domain experts use the proposed techniqueto extract useful differential networks that shed light on the biologicalprocesses involved in cancer and brain function.
arxiv-3600-293 | Cups Products in Z2-Cohomology of 3D Polyhedral Complexes | http://arxiv.org/pdf/1207.2346v3.pdf | author:Rocio Gonalez-Diaz, Javier Lamar, Ronald Umble category:cs.CV 55-XX published:2012-07-10 summary:Let $I=(\mathbb{Z}^3,26,6,B)$ be a 3D digital image, let $Q(I)$ be theassociated cubical complex and let $\partial Q(I)$ be the subcomplex of $Q(I)$whose maximal cells are the quadrangles of $Q(I)$ shared by a voxel of $B$ inthe foreground -- the object under study -- and by a voxel of$\mathbb{Z}^3\smallsetminus B$ in the background -- the ambient space. We showhow to simplify the combinatorial structure of $\partial Q(I)$ and obtain a 3Dpolyhedral complex $P(I)$ homeomorphic to $\partial Q(I)$ but with fewer cells.We introduce an algorithm that computes cup products on$H^*(P(I);\mathbb{Z}_2)$ directly from the combinatorics. The computationalmethod introduced here can be effectively applied to any polyhedral complexembedded in $\mathbb{R}^3$.
arxiv-3600-294 | Tuned Models of Peer Assessment in MOOCs | http://arxiv.org/pdf/1307.2579v1.pdf | author:Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng, Daphne Koller category:cs.LG cs.AI cs.HC stat.AP stat.ML published:2013-07-09 summary:In massive open online courses (MOOCs), peer grading serves as a criticaltool for scaling the grading of complex, open-ended assignments to courses withtens or hundreds of thousands of students. But despite promising initialtrials, it does not always deliver accurate results compared to human experts.In this paper, we develop algorithms for estimating and correcting for graderbiases and reliabilities, showing significant improvement in peer gradingaccuracy on real data with 63,199 peer grades from Coursera's HCI courseofferings --- the largest peer grading networks analysed to date. We relategrader biases and reliabilities to other student factors such as studentengagement, performance as well as commenting style. We also show that ourmodel can lead to more intelligent assignment of graders to gradees.
arxiv-3600-295 | General Drift Analysis with Tail Bounds | http://arxiv.org/pdf/1307.2559v1.pdf | author:Per Kristian Lehre, Carsten Witt category:cs.NE published:2013-07-09 summary:Drift analysis is one of the state-of-the-art techniques for the runtimeanalysis of randomized search heuristics. In recent years, many different drifttheorems, including additive, multiplicative and variable drift, have beendeveloped, applied and partly generalized or adapted to particular processes. Acomprehensive overview article was missing. We provide not only such an overview but also present a universal drifttheorem that generalizes virtually all existing drift theorems found in theliterature. On the one hand, the new theorem bounds the expected first hittingtime of optimal states in the underlying stochastic process. On the other hand,it also allows for general upper and lower tail bounds on the hitting time,which were not known before except for the special case of upper bounds inmultiplicative drift scenarios. As a proof of concept, the new tail bounds areapplied to prove very precise sharp-concentration results on the running timeof the (1+1) EA on OneMax, general linear functions and LeadingOnes. Moreover,user-friendly specializations of the general drift theorem are given.
arxiv-3600-296 | Image Fusion Technologies In Commercial Remote Sensing Packages | http://arxiv.org/pdf/1307.2440v1.pdf | author:Firouz Abdullah Al-Wassai, N. V. Kalyankar category:cs.CV published:2013-07-09 summary:Several remote sensing software packages are used to the explicit purpose ofanalyzing and visualizing remotely sensed data, with the developing of remotesensing sensor technologies from last ten years. Accord-ing to literature, theremote sensing is still the lack of software tools for effective informationextraction from remote sensing data. So, this paper provides a state-of-art ofmulti-sensor image fusion technologies as well as review on the qualityevaluation of the single image or fused images in the commercial remote sensingpack-ages. It also introduces program (ALwassaiProcess) developed for imagefusion and classification.
arxiv-3600-297 | Major Limitations of Satellite images | http://arxiv.org/pdf/1307.2434v1.pdf | author:Firouz A. Al-Wassai, N. V. Kalyankar category:cs.CV published:2013-07-09 summary:Remote sensing has proven to be a powerful tool for the monitoring of theEarth surface to improve our perception of our surroundings has led tounprecedented developments in sensor and information technologies. However,technologies for effective use of the data and for extracting usefulinformation from the data of Remote sensing are still very limited since nosingle sensor combines the optimal spectral, spatial and temporal resolution.This paper briefly reviews the limitations of satellite remote sensing. Also,reviews on the problems of image fusion techniques. The conclusion of this,According to literature, the remote sensing is still the lack of software toolsfor effective information extraction from remote sensing data. The trade-off inspectral and spatial resolution will remain and new advanced data fusionapproaches are needed to make optimal use of remote sensors for extract themost useful information.
arxiv-3600-298 | The price of bandit information in multiclass online classification | http://arxiv.org/pdf/1302.1043v2.pdf | author:Amit Daniely, Tom Helbertal category:cs.LG published:2013-02-05 summary:We consider two scenarios of multiclass online learning of a hypothesis class$H\subseteq Y^X$. In the {\em full information} scenario, the learner isexposed to instances together with their labels. In the {\em bandit} scenario,the true label is not exposed, but rather an indication whether the learner'sprediction is correct or not. We show that the ratio between the error rates inthe two scenarios is at most $8\cdotY\cdot \log(Y)$ in the realizable case,and $\tilde{O}(\sqrt{Y})$ in the agnostic case. The results are tight up to alogarithmic factor and essentially answer an open question from (Daniely et.al. - Multiclass learnability and the erm principle). We apply these results to the class of $\gamma$-margin multiclass linearclassifiers in $\reals^d$. We show that the bandit error rate of this class is$\tilde{\Theta}(\frac{Y}{\gamma^2})$ in the realizable case and$\tilde{\Theta}(\frac{1}{\gamma}\sqrt{YT})$ in the agnostic case. Thisresolves an open question from (Kakade et. al. - Efficient bandit algorithmsfor online multiclass prediction).
arxiv-3600-299 | Approximate dynamic programming using fluid and diffusion approximations with applications to power management | http://arxiv.org/pdf/1307.1759v2.pdf | author:Wei Chen, Dayu Huang, Ankur A. Kulkarni, Jayakrishnan Unnikrishnan, Quanyan Zhu, Prashant Mehta, Sean Meyn, Adam Wierman category:cs.LG math.OC published:2013-07-06 summary:Neuro-dynamic programming is a class of powerful techniques for approximatingthe solution to dynamic programming equations. In their most computationallyattractive formulations, these techniques provide the approximate solution onlywithin a prescribed finite-dimensional function class. Thus, the question thatalways arises is how should the function class be chosen? The goal of thispaper is to propose an approach using the solutions to associated fluid anddiffusion approximations. In order to illustrate this approach, the paperfocuses on an application to dynamic speed scaling for power management incomputer processors.
arxiv-3600-300 | Bayesian Discovery of Multiple Bayesian Networks via Transfer Learning | http://arxiv.org/pdf/1307.2312v1.pdf | author:Diane Oyen, Terran Lane category:stat.ML cs.LG published:2013-07-09 summary:Bayesian network structure learning algorithms with limited data are beingused in domains such as systems biology and neuroscience to gain insight intothe underlying processes that produce observed data. Learning reliable networksfrom limited data is difficult, therefore transfer learning can improve therobustness of learned networks by leveraging data from related tasks. Existingtransfer learning algorithms for Bayesian network structure learning give asingle maximum a posteriori estimate of network models. Yet, many other modelsmay be equally likely, and so a more informative result is provided by Bayesianstructure discovery. Bayesian structure discovery algorithms estimate posteriorprobabilities of structural features, such as edges. We present transferlearning for Bayesian structure discovery which allows us to explore the sharedand unique structural features among related tasks. Efficient computationrequires that our transfer learning objective factors into local calculations,which we prove is given by a broad class of transfer biases. Theoretically, weshow the efficiency of our approach. Empirically, we show that compared tosingle task learning, transfer learning is better able to positively identifytrue edges. We apply the method to whole-brain neuroimaging data.
