arxiv-6900-1 | Bayesian Structure Learning for Markov Random Fields with a Spike and Slab Prior | http://arxiv.org/pdf/1408.2047v1.pdf | author:Yutian Chen, Max Welling category:cs.LG stat.ML published:2014-08-09 summary:In recent years a number of methods have been developed for automaticallylearning the (sparse) connectivity structure of Markov Random Fields. Thesemethods are mostly based on L1-regularized optimization which has a number ofdisadvantages such as the inability to assess model uncertainty and expensivecrossvalidation to find the optimal regularization parameter. Moreover, themodel's predictive performance may degrade dramatically with a suboptimal valueof the regularization parameter (which is sometimes desirable to inducesparseness). We propose a fully Bayesian approach based on a "spike and slab"prior (similar to L0 regularization) that does not suffer from theseshortcomings. We develop an approximate MCMC method combining Langevin dynamicsand reversible jump MCMC to conduct inference in this model. Experiments showthat the proposed model learns a good combination of the structure andparameter values without the need for separate hyper-parameter tuning.Moreover, the model's predictive performance is much more robust than L1-basedmethods with hyper-parameter settings that induce highly sparse modelstructures.
arxiv-6900-2 | Video In Sentences Out | http://arxiv.org/pdf/1408.6418v1.pdf | author:Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang category:cs.CV cs.CL cs.IR published:2014-08-09 summary:We present a system that produces sentential descriptions of video: who didwhat to whom, and where and how they did it. Action class is rendered as averb, participant objects as noun phrases, properties of those objects asadjectival modifiers in those noun phrases, spatial relations between thoseparticipants as prepositional phrases, and characteristics of the event asprepositional-phrase adjuncts and adverbial modifiers. Extracting theinformation needed to render these linguistic entities requires an approach toevent recognition that recovers object tracks, the trackto-role assignments,and changing body posture.
arxiv-6900-3 | Efficient Clustering with Limited Distance Information | http://arxiv.org/pdf/1408.2045v1.pdf | author:Konstantin Voevodski, Maria-Florina Balcan, Heiko Roglin, Shang-Hua Teng, Yu Xia category:cs.LG cs.AI published:2014-08-09 summary:Given a point set S and an unknown metric d on S, we study the problem ofefficiently partitioning S into k clusters while querying few distances betweenthe points. In our model we assume that we have access to one versus allqueries that given a point s 2 S return the distances between s and all otherpoints. We show that given a natural assumption about the structure of theinstance, we can efficiently find an accurate clustering using only O(k)distance queries. We use our algorithm to cluster proteins by sequencesimilarity. This setting nicely fits our model because we can use a fastsequence database search program to query a sequence against an entire dataset.We conduct an empirical study that shows that even though we query a smallfraction of the distances between the points, we produce clusterings that areclose to a desired clustering given by manual classification.
arxiv-6900-4 | Matrix Coherence and the Nystrom Method | http://arxiv.org/pdf/1408.2044v1.pdf | author:Ameet Talwalkar, Afshin Rostamizadeh category:cs.LG stat.ML published:2014-08-09 summary:The Nystrom method is an efficient technique used to speed up large-scalelearning applications by generating low-rank approximations. Crucial to theperformance of this technique is the assumption that a matrix can be wellapproximated by working exclusively with a subset of its columns. In this workwe relate this assumption to the concept of matrix coherence, connectingcoherence to the performance of the Nystrom method. Making use of related workin the compressed sensing and the matrix completion literature, we derive novelcoherence-based bounds for the Nystrom method in the low-rank setting. We thenpresent empirical results that corroborate these theoretical bounds. Finally,we present more general empirical results for the full-rank setting thatconvincingly demonstrate the ability of matrix coherence to measure the degreeto which information can be extracted from a subset of columns.
arxiv-6900-5 | Gaussian Process Structural Equation Models with Latent Variables | http://arxiv.org/pdf/1408.2042v1.pdf | author:Ricardo Silva, Robert B. Gramacy category:cs.LG stat.ML published:2014-08-09 summary:In a variety of disciplines such as social sciences, psychology, medicine andeconomics, the recorded data are considered to be noisy measurements of latentvariables connected by some causal structure. This corresponds to a family ofgraphical models known as the structural equation model with latent variables.While linear non-Gaussian variants have been well-studied, inference innonparametric structural equation models is still underdeveloped. We introducea sparse Gaussian process parameterization that defines a non-linear structureconnecting latent variables, unlike common formulations of Gaussian processlatent variable models. The sparse parameterization is given a full Bayesiantreatment without compromising Markov chain Monte Carlo efficiency. We comparethe stability of the sampling procedure and the predictive ability of the modelagainst the current practice.
arxiv-6900-6 | GraphLab: A New Framework For Parallel Machine Learning | http://arxiv.org/pdf/1408.2041v1.pdf | author:Yucheng Low, Joseph E. Gonzalez, Aapo Kyrola, Danny Bickson, Carlos E. Guestrin, Joseph Hellerstein category:cs.LG cs.DC published:2014-08-09 summary:Designing and implementing efficient, provably correct parallel machinelearning (ML) algorithms is challenging. Existing high-level parallelabstractions like MapReduce are insufficiently expressive while low-level toolslike MPI and Pthreads leave ML experts repeatedly solving the same designchallenges. By targeting common patterns in ML, we developed GraphLab, whichimproves upon abstractions like MapReduce by compactly expressing asynchronousiterative algorithms with sparse computational dependencies while ensuring dataconsistency and achieving a high degree of parallel performance. We demonstratethe expressiveness of the GraphLab framework by designing and implementingparallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso andCompressed Sensing. We show that using GraphLab we can achieve excellentparallel performance on large scale real-world problems.
arxiv-6900-7 | Prediction with Advice of Unknown Number of Experts | http://arxiv.org/pdf/1408.2040v1.pdf | author:Alexey Chernov, Vladimir Vovk category:cs.LG stat.ML published:2014-08-09 summary:In the framework of prediction with expert advice, we consider a recentlyintroduced kind of regret bounds: the bounds that depend on the effectiveinstead of nominal number of experts. In contrast to the Normal- Hedge bound,which mainly depends on the effective number of experts but also weakly dependson the nominal one, we obtain a bound that does not contain the nominal numberof experts at all. We use the defensive forecasting method and introduce anapplication of defensive forecasting to multivalued supermartingales.
arxiv-6900-8 | Incorporating Side Information in Probabilistic Matrix Factorization with Gaussian Processes | http://arxiv.org/pdf/1408.2039v1.pdf | author:Ryan Prescott Adams, George E. Dahl, Iain Murray category:cs.LG stat.ML published:2014-08-09 summary:Probabilistic matrix factorization (PMF) is a powerful method for modelingdata associ- ated with pairwise relationships, Finding use in collaborativeFiltering, computational bi- ology, and document analysis, among other areas.In many domains, there are additional covariates that can assist in prediction.For example, when modeling movie ratings, we might know when the ratingoccurred, where the user lives, or what actors appear in the movie. It isdifficult, however, to incorporate this side information into the PMF model. Wepropose a framework for incorporating side information by coupling togethermulti- ple PMF problems via Gaussian process priors. We replace scalar latentfeatures with func- tions that vary over the covariate space. The GP priors onthese functions require them to vary smoothly and share information. We applythis new method to predict the scores of professional basketball games, whereside information about the venue and date of the game are relevant for theoutcome.
arxiv-6900-9 | A direct method for estimating a causal ordering in a linear non-Gaussian acyclic model | http://arxiv.org/pdf/1408.2038v1.pdf | author:Shohei Shimizu, Aapo Hyvarinen, Yoshinobu Kawahara category:cs.LG stat.ML published:2014-08-09 summary:Structural equation models and Bayesian networks have been widely used toanalyze causal relations between continuous variables. In such frameworks,linear acyclic models are typically used to model the datagenerating process ofvariables. Recently, it was shown that use of non-Gaussianity identifies acausal ordering of variables in a linear acyclic model without using any priorknowledge on the network structure, which is not the case with conventionalmethods. However, existing estimation methods are based on iterative searchalgorithms and may not converge to a correct solution in a finite number ofsteps. In this paper, we propose a new direct method to estimate a causalordering based on non-Gaussianity. In contrast to the previous methods, ouralgorithm requires no algorithmic parameters and is guaranteed to converge tothe right solution within a small fixed number of steps if the data strictlyfollows the model.
arxiv-6900-10 | Quantum Annealing for Variational Bayes Inference | http://arxiv.org/pdf/1408.2037v1.pdf | author:Issei Sato, Kenichi Kurihara, Shu Tanaka, Hiroshi Nakagawa, Seiji Miyashita category:cs.LG stat.ML published:2014-08-09 summary:This paper presents studies on a deterministic annealing algorithm based onquantum annealing for variational Bayes (QAVB) inference, which can be seen asan extension of the simulated annealing for variational Bayes (SAVB) inference.QAVB is as easy as SAVB to implement. Experiments revealed QAVB finds a betterlocal optimum than SAVB in terms of the variational free energy in latentDirichlet allocation (LDA).
arxiv-6900-11 | Quantum Annealing for Clustering | http://arxiv.org/pdf/1408.2035v1.pdf | author:Kenichi Kurihara, Shu Tanaka, Seiji Miyashita category:cs.AI cs.LG published:2014-08-09 summary:This paper studies quantum annealing (QA) for clustering, which can be seenas an extension of simulated annealing (SA). We derive a QA algorithm forclustering and propose an annealing schedule, which is crucial in practice.Experiments show the proposed QA algorithm finds better clustering assignmentsthan SA. Furthermore, QA is as easy as SA to implement.
arxiv-6900-12 | Robust Graphical Modeling with t-Distributions | http://arxiv.org/pdf/1408.2033v1.pdf | author:Michael A. Finegold, Mathias Drton category:cs.LG stat.ML published:2014-08-09 summary:Graphical Gaussian models have proven to be useful tools for exploringnetwork structures based on multivariate data. Applications to studies of geneexpression have generated substantial interest in these models, and resultingrecent progress includes the development of fitting methodology involvingpenalization of the likelihood function. In this paper we advocate the use ofthe multivariate t and related distributions for more robust inference ofgraphs. In particular, we demonstrate that penalized likelihood inferencecombined with an application of the EM algorithm provides a simple andcomputationally efficient approach to model selection in the t-distributioncase.
arxiv-6900-13 | Bayesian Multitask Learning with Latent Hierarchies | http://arxiv.org/pdf/1408.2032v1.pdf | author:Hal Daume III category:cs.LG stat.ML published:2014-08-09 summary:We learn multiple hypotheses for related tasks under a latent hierarchicalrelationship between tasks. We exploit the intuition that for domainadaptation, we wish to share classifier structure, but for multitask learning,we wish to share covariance structure. Our hierarchical model is seen tosubsume several previously proposed multitask learning models and performs wellon three distinct real-world data sets.
arxiv-6900-14 | Conditional Probability Tree Estimation Analysis and Algorithms | http://arxiv.org/pdf/1408.2031v1.pdf | author:Alina Beygelzimer, John Langford, Yuri Lifshits, Gregory Sorkin, Alexander L. Strehl category:cs.LG stat.ML published:2014-08-09 summary:We consider the problem of estimating the conditional probability of a labelin time O(log n), where n is the number of possible labels. We analyze anatural reduction of this problem to a set of binary regression problemsorganized in a tree structure, proving a regret bound that scales with thedepth of the tree. Motivated by this analysis, we propose the first onlinealgorithm which provably constructs a logarithmic depth tree on the set oflabels to solve this problem. We test the algorithm empirically, showing thatit works succesfully on a dataset with roughly 106 labels.
arxiv-6900-15 | Blind Construction of Optimal Nonlinear Recursive Predictors for Discrete Sequences | http://arxiv.org/pdf/1408.2025v1.pdf | author:Cosma Shalizi, Kristina Lisa Klinkner category:cs.LG stat.ML published:2014-08-09 summary:We present a new method for nonlinear prediction of discrete random sequencesunder minimal structural assumptions. We give a mathematical construction foroptimal predictors of such processes, in the form of hidden Markov models. Wethen describe an algorithm, CSSR (Causal-State Splitting Reconstruction), whichapproximates the ideal predictor from data. We discuss the reliability of CSSR,its data requirements, and its performance in simulations. Finally, we compareour approach to existing methods using variablelength Markov models andcross-validated hidden Markov models, and show theoretically and experimentallythat our method delivers results superior to the former and at least comparableto the latter.
arxiv-6900-16 | Automatic Removal of Marginal Annotations in Printed Text Document | http://arxiv.org/pdf/1408.2015v1.pdf | author:Abdessamad Elboushaki, Rachida Hannane, P. Nagabhushan, Mohammed Javed category:cs.CV published:2014-08-09 summary:Recovering the original printed texts from a document with added handwrittenannotations in the marginal area is one of the challenging problems, especiallywhen the original document is not available. Therefore, this paper aims atsalvaging automatically the original document from the annotated document bydetecting and removing any handwritten annotations that appear in the marginalarea of the document without any loss of information. Here a two stagealgorithm is proposed, where in the first stage due to approximate marginalboundary detection with horizontal and vertical projection profiles, all of themarginal annotations along with some part of the original printed text that mayappear very close to the marginal boundary are removed. Therefore as a secondstage, using the connected components, a strategy is applied to bring back theprinted text components cropped during the first stage. The proposed method isvalidated using a dataset of 50 documents having complex handwrittenannotations, which gives an overall accuracy of 89.01% in removing the marginalannotations and 97.74% in case of retrieving the original printed textdocument.
arxiv-6900-17 | Gabor-like Image Filtering using a Neural Microcircuit | http://arxiv.org/pdf/1408.1986v1.pdf | author:C. Mayr, A. Heittmann, R. Schüffny category:cs.CV cs.ET q-bio.NC published:2014-08-08 summary:In this letter, we present an implementation of a neural microcircuit forimage processing employing Hebbian-adaptive learning. The neuronal circuitutilizes only excitatory synapses to correlate action potentials, extractingthe uncorrelated ones, which contain significant image information. Thiscircuit is capable of approximating Gabor-like image filtering and other imageprocessing functions
arxiv-6900-18 | A model of grassroots changes in linguistic systems | http://arxiv.org/pdf/1408.1985v1.pdf | author:Janet B. Pierrehumbert, Forrest Stonedahl, Robert Daland category:cs.CL nlin.AO physics.soc-ph published:2014-08-08 summary:Linguistic norms emerge in human communities because people imitate eachother. A shared linguistic system provides people with the benefits of sharedknowledge and coordinated planning. Once norms are in place, why would theyever change? This question, echoing broad questions in the theory of socialdynamics, has particular force in relation to language. By definition, aninnovator is in the minority when the innovation first occurs. In some areas ofsocial dynamics, important minorities can strongly influence the majoritythrough their power, fame, or use of broadcast media. But most linguisticchanges are grassroots developments that originate with ordinary people. Here,we develop a novel model of communicative behavior in communities, and identifya mechanism for arbitrary innovations by ordinary people to have a good chanceof being widely adopted. To imitate each other, people must form a mental representation of what otherpeople do. Each time they speak, they must also decide which form to producethemselves. We introduce a new decision function that enables us to smoothlyexplore the space between two types of behavior: probability matching (matchingthe probabilities of incoming experience) and regularization (producing someforms disproportionately often). Using Monte Carlo methods, we explore theinteractions amongst the degree of regularization, the distribution of biasesin a network, and the network position of the innovator. We identify tworegimes for the widespread adoption of arbritrary innovations, viewed asinformational cascades in the network. With moderate regularization ofexperienced input, average people (not well-connected people) are the mostlikely source of successful innovations. Our results shed light on a majoroutstanding puzzle in the theory of language change. The framework also holdspromise for understanding the dynamics of other social norms.
arxiv-6900-19 | Neighborhood Rank Order Coding for Robust Texture Analysis and Feature Extraction | http://arxiv.org/pdf/1408.1984v1.pdf | author:C. Mayr, R. Schüffny category:cs.CV published:2014-08-08 summary:Research into the visual cortex and general neural information processing hasled to various attempts to integrate pulse computation schemes in imageanalysis systems. Of interest is especially the robustness of representing ananalogue signal in the phase or duration of a pulsed, quasi-digital signal, aswell as the possibility of direct digital interaction, i.e. computation, amongthese signals. Such a computation can also achieve information compaction forsubsequent processing stages. By using a pulse order encoding scheme motivatedby dendritic pulse interaction, we will show that a powerful low-level featureand texture extraction operator, called Pulsed Local Orientation Coding (PLOC),can be implemented. Feature extraction results are being presented, and apossible VLSI implementation is detailed.
arxiv-6900-20 | Microtask crowdsourcing for disease mention annotation in PubMed abstracts | http://arxiv.org/pdf/1408.1928v1.pdf | author:Benjamin M Good, Max Nanis, Andrew I. Su category:cs.CL 9208 H.5.3; I.2.7 published:2014-08-08 summary:Identifying concepts and relationships in biomedical text enables knowledgeto be applied in computational analyses. Many biological natural languageprocess (BioNLP) projects attempt to address this challenge, but the state ofthe art in BioNLP still leaves much room for improvement. Progress in BioNLPresearch depends on large, annotated corpora for evaluating informationextraction systems and training machine learning models. Traditionally, suchcorpora are created by small numbers of expert annotators often working overextended periods of time. Recent studies have shown that workers on microtaskcrowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, inaggregate, generate high-quality annotations of biomedical text. Here, weinvestigated the use of the AMT in capturing disease mentions in PubMedabstracts. We used the NCBI Disease corpus as a gold standard for refining andbenchmarking our crowdsourcing protocol. After several iterations, we arrivedat a protocol that reproduced the annotations of the 593 documents in thetraining set of this gold standard with an overall F measure of 0.872(precision 0.862, recall 0.883). The output can also be tuned to optimize forprecision (max = 0.984 when recall = 0.269) or recall (max = 0.980 whenprecision = 0.436). Each document was examined by 15 workers, and theirannotations were merged based on a simple voting method. In total 145 workerscombined to complete all 593 documents in the span of 1 week at a cost of $.06per abstract per worker. The quality of the annotations, as judged with the Fmeasure, increases with the number of workers assigned to each task such thatthe system can be tuned to balance cost against quality. These resultsdemonstrate that microtask crowdsourcing can be a valuable tool for generatingwell-annotated corpora in BioNLP.
arxiv-6900-21 | Using Learned Predictions as Feedback to Improve Control and Communication with an Artificial Limb: Preliminary Findings | http://arxiv.org/pdf/1408.1913v1.pdf | author:Adam S. R. Parker, Ann L. Edwards, Patrick M. Pilarski category:cs.AI cs.HC cs.LG cs.RO published:2014-08-08 summary:Many people suffer from the loss of a limb. Learning to get by without an armor hand can be very challenging, and existing prostheses do not yet fulfil theneeds of individuals with amputations. One promising solution is to providegreater communication between a prosthesis and its user. Towards this end, wepresent a simple machine learning interface to supplement the control of arobotic limb with feedback to the user about what the limb will be experiencingin the near future. A real-time prediction learner was implemented to predictimpact-related electrical load experienced by a robot limb; the learningsystem's predictions were then communicated to the device's user to aid intheir interactions with a workspace. We tested this system with fiveable-bodied subjects. Each subject manipulated the robot arm while receivingdifferent forms of vibrotactile feedback regarding the arm's contact with itsworkspace. Our trials showed that communicable predictions could be learnedquickly during human control of the robot arm. Using these predictions as abasis for feedback led to a statistically significant improvement in taskperformance when compared to purely reactive feedback from the device. Ourstudy therefore contributes initial evidence that prediction learning andmachine intelligence can benefit not just control, but also feedback from anartificial limb. We expect that a greater level of acceptance and ownership canbe achieved if the prosthesis itself takes an active role in transmittinglearned knowledge about its state and its situation of use.
arxiv-6900-22 | Exploring the evolution of a trade-off between vigilance and foraging in group-living organisms | http://arxiv.org/pdf/1408.1906v1.pdf | author:Randal S. Olson, Patrick B. Haley, Fred C. Dyer, Christoph Adami category:q-bio.PE cs.GT cs.NE published:2014-08-08 summary:Despite the fact that grouping behavior has been actively studied for over acentury, the relative importance of the numerous proposed fitness benefits ofgrouping remain unclear. We use a digital model of evolving prey undersimulated predation to directly explore the evolution of gregarious foragingbehavior according to one such benefit, the "many eyes" hypothesis. Accordingto this hypothesis, collective vigilance allows prey in large groups to detectpredators more efficiently by making alarm signals or behavioral cues to eachother, thereby allowing individuals within the group to spend more timeforaging. Here, we find that collective vigilance is sufficient to select forgregarious foraging behavior as long there is not a direct cost for grouping(e.g., competition for limited food resources), even when controlling forconfounding factors such as the dilution effect. Further, we explore the roleof the genetic relatedness and reproductive strategy of the prey, and find thathighly related groups of prey with a semelparous reproductive strategy are themost likely to evolve gregarious foraging behavior mediated by the benefit ofvigilance. These findings, combined with earlier studies with evolving digitalorganisms, further sharpen our understanding of the factors favoring groupingbehavior.
arxiv-6900-23 | Rate Prediction and Selection in LTE systems using Modified Source Encoding Techniques | http://arxiv.org/pdf/1403.1412v5.pdf | author:K. P. Saishankar, Sheetal Kalyani, K. Narendran category:stat.AP cs.IT cs.LG math.IT published:2014-03-06 summary:In current wireless systems, the base-Station (eNodeB) tries to serve itsuser-equipment (UE) at the highest possible rate that the UE can reliablydecode. The eNodeB obtains this rate information as a quantized feedback fromthe UE at time n and uses this, for rate selection till the next feedback isreceived at time n + {\delta}. The feedback received at n can become outdatedbefore n + {\delta}, because of a) Doppler fading, and b) Change in the set ofactive interferers for a UE. Therefore rate prediction becomes essential.Since, the rates belong to a discrete set, we propose a discrete sequenceprediction approach, wherein, frequency trees for the discrete sequences arebuilt using source encoding algorithms like Prediction by Partial Match (PPM).Finding the optimal depth of the frequency tree used for prediction is cast asa model order selection problem. The rate sequence complexity is analysed toprovide an upper bound on model order. Information-theoretic criteria are thenused to solve the model order problem. Finally, two prediction algorithms areproposed, using the PPM with optimal model order and system level simulationsdemonstrate the improvement in packet loss and throughput due to thesealgorithms.
arxiv-6900-24 | Origin of the computational hardness for learning with binary synapses | http://arxiv.org/pdf/1408.1784v1.pdf | author:Haiping Huang, Yoshiyuki Kabashima category:cs.LG q-bio.NC published:2014-08-08 summary:Supervised learning in a binary perceptron is able to classify an extensivenumber of random patterns by a proper assignment of binary synaptic weights.However, to find such assignments in practice, is quite a nontrivial task. Therelation between the weight space structure and the algorithmic hardness hasnot yet been fully understood. To this end, we analytically derive theFranz-Parisi potential for the binary preceptron problem, by starting from anequilibrium solution of weights and exploring the weight space structure aroundit. Our result reveals the geometrical organization of the weightspace\textemdash the weight space is composed of isolated solutions, ratherthan clusters of exponentially many close-by solutions. The point-like clustersfar apart from each other in the weight space explain the previously observedglassy behavior of stochastic local search heuristics.
arxiv-6900-25 | A compact formula for the derivative of a 3-D rotation in exponential coordinates | http://arxiv.org/pdf/1312.0788v2.pdf | author:Guillermo Gallego, Anthony Yezzi category:cs.CV math.OC published:2013-12-03 summary:We present a compact formula for the derivative of a 3-D rotation matrix withrespect to its exponential coordinates. A geometric interpretation of theresulting expression is provided, as well as its agreement with otherless-compact but better-known formulas. To the best of our knowledge, thissimpler formula does not appear anywhere in the literature. We hope byproviding this more compact expression to alleviate the common pressure toreluctantly resort to alternative representations in various computationalapplications simply as a means to avoid the complexity of differential analysisin exponential coordinates.
arxiv-6900-26 | Beyond description. Comment on "Approaching human language with complex networks" by Cong & Liu | http://arxiv.org/pdf/1408.1774v1.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph published:2014-08-08 summary:Comment on "Approaching human language with complex networks" by Cong & Liu
arxiv-6900-27 | Real-Time and Robust Method for Hand Gesture Recognition System Based on Cross-Correlation Coefficient | http://arxiv.org/pdf/1408.1759v1.pdf | author:Reza Azad, Babak Azad, Iman Tavakoli Kazerooni category:cs.CV published:2014-08-08 summary:Hand gesture recognition possesses extensive applications in virtual reality,sign language recognition, and computer games. The direct interface of handgestures provides us a new way for communicating with the virtual environment.In this paper a novel and real-time approach for hand gesture recognitionsystem is presented. In the suggested method, first, the hand gesture isextracted from the main image by the image segmentation and morphologicaloperation and then is sent to feature extraction stage. In feature extractionstage the Cross-correlation coefficient is applied on the gesture to recognizeit. In the result part, the proposed approach is applied on American SignLanguage (ASL) database and the accuracy rate obtained 98.34%.
arxiv-6900-28 | Clustering under Perturbation Resilience | http://arxiv.org/pdf/1112.0826v4.pdf | author:Maria Florina Balcan, Yingyu Liang category:cs.LG cs.DS published:2011-12-05 summary:Motivated by the fact that distances between data points in many real-worldclustering instances are often based on heuristic measures, Bilu andLinial~\cite{BL} proposed analyzing objective based clustering problems underthe assumption that the optimum clustering to the objective is preserved undersmall multiplicative perturbations to distances between points. The hope isthat by exploiting the structure in such instances, one can overcome worst casehardness results. In this paper, we provide several strong results within this framework. Forcenter-based objectives, we present an algorithm that can optimally clusterinstances resilient to perturbations of factor $(1 + \sqrt{2})$, solving anopen problem of Awasthi et al.~\cite{ABS10}. For $k$-median, a center-basedobjective of special interest, we additionally give algorithms for a morerelaxed assumption in which we allow the optimal solution to change in a small$\epsilon$ fraction of the points after perturbation. We give the first boundsknown for $k$-median under this more realistic and more general assumption. Wealso provide positive results for min-sum clustering which is a generally muchharder objective than center-based objectives. Our algorithms are based on newlinkage criteria that may be of independent interest. Additionally, we give sublinear-time algorithms, showing algorithms that canreturn an implicit clustering from only access to a small random sample.
arxiv-6900-29 | Low-rank SIFT: An Affine Invariant Feature for Place Recognition | http://arxiv.org/pdf/1408.1688v1.pdf | author:Chao Yang, Shengnan Caih, Jingdong Wang, Long Quan category:cs.CV published:2014-08-07 summary:In this paper, we present a novel affine-invariant feature based on SIFT,leveraging the regular appearance of man-made objects. The feature achievesfull affine invariance without needing to simulate over affine parameter space.Low-rank SIFT, as we name the feature, is based on our observation that localtilt, which are caused by changes of camera axis orientation, could benormalized by converting local patches to standard low-rank forms. Rotation,translation and scaling invariance could be achieved in ways similar to SIFT.As an extension of SIFT, our method seeks to add prior to solve the ill-posedaffine parameter estimation problem and normalizes them directly, and isapplicable to objects with regular structures. Furthermore, owing to recentbreakthrough in convex optimization, such parameter could be computedefficiently. We will demonstrate its effectiveness in place recognition as ourmajor application. As extra contributions, we also describe our pipeline ofconstructing geotagged building database from the ground up, as well as anefficient scheme for automatic feature selection.
arxiv-6900-30 | Tracking Individual Targets in High Density Crowd Scenes Analysis of a Video Recording in Hajj 2009 | http://arxiv.org/pdf/1407.2044v2.pdf | author:Mohamed H. Dridi category:cs.CV physics.soc-ph published:2014-07-08 summary:In this paper we present a number of methods (manual, semi-automatic andautomatic) for tracking individual targets in high density crowd scenes wherethousand of people are gathered. The necessary data about the motion ofindividuals and a lot of other physical information can be extracted fromconsecutive image sequences in different ways, including optical flow and blockmotion estimation. One of the famous methods for tracking moving objects is theblock matching method. This way to estimate subject motion requires thespecification of a comparison window which determines the scale of theestimate. In this work we present a real-time method for pedestrian recognitionand tracking in sequences of high resolution images obtained by a stationary(high definition) camera located in different places on the Haram mosque inMecca. The objective is to estimate pedestrian velocities as a function of thelocal density.The resulting data of tracking moving pedestrians based on videosequences are presented in the following section. Through the evaluated systemthe spatio-temporal coordinates of each pedestrian during the Tawaf ritual areestablished. The pilgrim velocities as function of the local densities in theMataf area (Haram Mosque Mecca) are illustrated and very precisely documented.
arxiv-6900-31 | Real-Time Human-Computer Interaction Based on Face and Hand Gesture Recognition | http://arxiv.org/pdf/1408.1549v1.pdf | author:Reza Azad, Babak Azad, Nabil Belhaj Khalifa, Shahram Jamali category:cs.CV published:2014-08-07 summary:At the present time, hand gestures recognition system could be used as a moreexpected and useable approach for human computer interaction. Automatic handgesture recognition system provides us a new tactic for interactive with thevirtual environment. In this paper, a face and hand gesture recognition systemwhich is able to control computer media player is offered. Hand gesture andhuman face are the key element to interact with the smart system. We used theface recognition scheme for viewer verification and the hand gesturerecognition in mechanism of computer media player, for instance, volumedown/up, next music and etc. In the proposed technique, first, the hand gestureand face location is extracted from the main image by combination of skin andcascade detector and then is sent to recognition stage. In recognition stage,first, the threshold condition is inspected then the extracted face and gesturewill be recognized. In the result stage, the proposed technique is applied onthe video dataset and the high precision ratio acquired. Additional therecommended hand gesture recognition method is applied on static American SignLanguage (ASL) database and the correctness rate achieved nearby 99.40%. alsothe planned method could be used in gesture based computer games and virtualreality.
arxiv-6900-32 | Face Detection Using Radial Basis Functions Neural Networks With Fixed Spread | http://arxiv.org/pdf/1410.2173v1.pdf | author:K. A. A. Aziz, S. S. Abdullah category:cs.CV published:2014-08-07 summary:This paper presented a face detection system using Radial Basis FunctionNeural Networks With Fixed Spread Value. Face detection is the first step inface recognition system. The purpose is to localize and extract the face regionfrom the background that will be fed into the face recognition system foridentification. General preprocessing approach was used for normalizing theimage and Radial Basis Function (RBF) Neural Network was used to distinguishbetween face and non-face. RBF Neural Networks offer several advantagescompared to other neural network architecture such as they can be trained usingfast two stages training algorithm and the network possesses the property ofbest approximation. The output of the network can be optimized by settingsuitable value of center and spread of the RBF. In this paper, fixed spreadvalue will be used. The Radial Basis Function Neural Network (RBFNN) used todistinguish faces and non-faces and the evaluation of the system will be theperformance of detection, False Acceptance Rate (FAR), False Rejection Rate(FRR) and the discriminative properties.
arxiv-6900-33 | WebAL-1: Workshop on Artificial Life and the Web 2014 Proceedings | http://arxiv.org/pdf/1406.2507v4.pdf | author:Tim Taylor category:cs.NE cs.MA published:2014-06-10 summary:Proceedings of WebAL-1: Workshop on Artificial Life and the Web 2014, held atthe 14th International Conference on the Synthesis and Simulation of LivingSystems (ALIFE 14), New York, NY, 31 July 2014.
arxiv-6900-34 | Sparse and Low-Rank Covariance Matrices Estimation | http://arxiv.org/pdf/1407.4596v2.pdf | author:Shenglong Zhou, Naihua Xiu, Ziyan Luo, Lingchen Kong category:math.ST math.OC stat.ML stat.TH published:2014-07-17 summary:This paper aims at achieving a simultaneously sparse and low-rank estimatorfrom the semidefinite population covariance matrices. We first benefit from aconvex optimization which develops $l_1$-norm penalty to encourage the sparsityand nuclear norm to favor the low-rank property. For the proposed estimator, wethen prove that with large probability, the Frobenious norm of the estimationrate can be of order $O(\sqrt{s(\log{r})/n})$ under a mild case, where $s$ and$r$ denote the number of sparse entries and the rank of the populationcovariance respectively, $n$ notes the sample capacity. Finally an efficientalternating direction method of multipliers with global convergence is proposedto tackle this problem, and meantime merits of the approach are alsoillustrated by practicing numerical simulations.
arxiv-6900-35 | Preventing False Discovery in Interactive Data Analysis is Hard | http://arxiv.org/pdf/1408.1655v1.pdf | author:Moritz Hardt, Jonathan Ullman category:cs.LG cs.CC cs.DS published:2014-08-06 summary:We show that, under a standard hardness assumption, there is nocomputationally efficient algorithm that given $n$ samples from an unknowndistribution can give valid answers to $n^{3+o(1)}$ adaptively chosenstatistical queries. A statistical query asks for the expectation of apredicate over the underlying distribution, and an answer to a statisticalquery is valid if it is "close" to the correct expectation over thedistribution. Our result stands in stark contrast to the well known fact that exponentiallymany statistical queries can be answered validly and efficiently if the queriesare chosen non-adaptively (no query may depend on the answers to previousqueries). Moreover, a recent work by Dwork et al. shows how to accuratelyanswer exponentially many adaptively chosen statistical queries via acomputationally inefficient algorithm; and how to answer a quadratic number ofadaptive queries via a computationally efficient algorithm. The latter resultimplies that our result is tight up to a linear factor in $n.$ Conceptually, our result demonstrates that achieving statistical validityalone can be a source of computational intractability in adaptive settings. Forexample, in the modern large collaborative research environment, data analyststypically choose a particular approach based on previous findings. Falsediscovery occurs if a research finding is supported by the data but not by theunderlying distribution. While the study of preventing false discovery inStatistics is decades old, to the best of our knowledge our result is the firstto demonstrate a computational barrier. In particular, our result suggests thatthe perceived difficulty of preventing false discovery in today's collaborativeresearch environment may be inherent.
arxiv-6900-36 | When does Active Learning Work? | http://arxiv.org/pdf/1408.1319v1.pdf | author:Lewis Evans, Niall M. Adams, Christoforos Anagnostopoulos category:stat.ML cs.LG published:2014-08-06 summary:Active Learning (AL) methods seek to improve classifier performance whenlabels are expensive or scarce. We consider two central questions: Where doesAL work? How much does it help? To address these questions, a comprehensiveexperimental simulation study of Active Learning is presented. We consider avariety of tasks, classifiers and other AL factors, to present a broadexploration of AL performance in various settings. A precise way to quantifyperformance is needed in order to know when AL works. Thus we also present adetailed methodology for tackling the complexities of assessing AL performancein the context of this experimental study.
arxiv-6900-37 | New crossover operators for multiple subset selection tasks | http://arxiv.org/pdf/1408.1297v1.pdf | author:Arnab Roy, J. David Schaffer, Craig B. Laramee category:cs.NE 68 I.5.2, I.5.4 published:2014-08-06 summary:We have introduced two crossover operators, MMX-BLXexploit andMMX-BLXexplore, for simultaneously solving multiple feature/subset selectionproblems where the features may have numeric attributes and the subset sizesare not predefined. These operators differ on the level of exploration andexploitation they perform; one is designed to produce convergence controlledmutation and the other exhibits a quasi-constant mutation rate. We illustratethe characteristic of these operators by evolving pattern detectors todistinguish alcoholics from controls using their visually evoked responsepotentials (VERPs). This task encapsulates two groups of subset selectionproblems; choosing a subset of EEG leads along with the lead-weights (featureswith attributes) and the other that defines the temporal pattern thatcharacterizes the alcoholic VERPs. We observed better generalizationperformance from MMX-BLXexplore. Perhaps, MMX-BLXexploit was handicapped by nothaving a restart mechanism. These operators are novel and appears to holdpromise for solving simultaneous feature selection problems.
arxiv-6900-38 | Pseudo-Zernike Based Multi-Pass Automatic Target Recognition From Multi-Channel SAR | http://arxiv.org/pdf/1404.1682v3.pdf | author:Carmine Clemente, Luca Pallotta, Ian Proudler, Antonio De Maio, John J. Soraghan, Alfonso Farina category:cs.CV published:2014-04-07 summary:The capability to exploit multiple sources of information is of fundamentalimportance in a battlefield scenario. Information obtained from differentsources, and separated in space and time, provide the opportunity to exploitdiversities in order to mitigate uncertainty. For the specific challenge ofAutomatic Target Recognition (ATR) from radar platforms, both channel (e.g.polarization) and spatial diversity can provide useful information for such aspecific and critical task. In this paper the use of pseudo-Zernike momentsapplied to multi-channel multi-pass data is presented exploiting diversitiesand invariant properties leading to high confidence ATR, small computationalcomplexity and data transfer requirements. The effectiveness of the proposedapproach, in different configurations and data source availability isdemonstrated using real data.
arxiv-6900-39 | The functional mean-shift algorithm for mode hunting and clustering in infinite dimensions | http://arxiv.org/pdf/1408.1187v1.pdf | author:Mattia Ciollaro, Christopher Genovese, Jing Lei, Larry Wasserman category:stat.ME stat.AP stat.ML published:2014-08-06 summary:We introduce the functional mean-shift algorithm, an iterative algorithm forestimating the local modes of a surrogate density from functional data. We showthat the algorithm can be used for cluster analysis of functional data. Wepropose a test based on the bootstrap for the significance of the estimatedlocal modes of the surrogate density. We present two applications of ourmethodology. In the first application, we demonstrate how the functionalmean-shift algorithm can be used to perform spike sorting, i.e. cluster neuralactivity curves. In the second application, we use the functional mean-shiftalgorithm to distinguish between original and fake signatures.
arxiv-6900-40 | Computing With Contextual Numbers | http://arxiv.org/pdf/1408.0889v2.pdf | author:Vahid Moosavi category:cs.CE cs.CV cs.NE published:2014-08-05 summary:Self Organizing Map (SOM) has been applied into several classical modelingtasks including clustering, classification, function approximation andvisualization of high dimensional spaces. The final products of a trained SOMare a set of ordered (low dimensional) indices and their associated highdimensional weight vectors. While in the above-mentioned applications, thefinal high dimensional weight vectors play the primary role in thecomputational steps, from a certain perspective, one can interpret SOM as anonparametric encoder, in which the final low dimensional indices of thetrained SOM are pointer to the high dimensional space. We showed how using aone-dimensional SOM, which is not common in usual applications of SOM, one candevelop a nonparametric mapping from a high dimensional space to a continuousone-dimensional numerical field. These numerical values, called contextualnumbers, are ordered in a way that in a given context, similar numbers refer tosimilar high dimensional states. Further, as these numbers can be treatedsimilarly to usual continuous numbers, they can be replaced with theircorresponding high dimensional states within any data driven modeling problem.As a potential application, we showed how using contextual numbers could beused for the problem of high dimensional spatiotemporal dynamics.
arxiv-6900-41 | Boosted Markov Networks for Activity Recognition | http://arxiv.org/pdf/1408.1167v1.pdf | author:Truyen Tran, Hung Bui, Svetha Venkatesh category:cs.LG cs.CV stat.ML published:2014-08-06 summary:We explore a framework called boosted Markov networks to combine the learningcapacity of boosting and the rich modeling semantics of Markov networks andapplying the framework for video-based activity recognition. Importantly, weextend the framework to incorporate hidden variables. We show how the frameworkcan be applied for both model learning and feature selection. We demonstratethat boosted Markov networks with hidden variables perform comparably with thestandard maximum likelihood estimation. However, our framework is able to learnsparse models, and therefore can provide computational savings when the learnedmodels are used for classification.
arxiv-6900-42 | Human Activity Learning and Segmentation using Partially Hidden Discriminative Models | http://arxiv.org/pdf/1408.3081v1.pdf | author:Truyen Tran, Hung Bui, Svetha Venkatesh category:cs.LG cs.CV stat.ML published:2014-08-06 summary:Learning and understanding the typical patterns in the daily activities androutines of people from low-level sensory data is an important problem in manyapplication domains such as building smart environments, or providingintelligent assistance. Traditional approaches to this problem typically relyon supervised learning and generative models such as the hidden Markov modelsand its extensions. While activity data can be readily acquired from pervasivesensors, e.g. in smart environments, providing manual labels to supportsupervised training is often extremely expensive. In this paper, we propose anew approach based on semi-supervised training of partially hiddendiscriminative models such as the conditional random field (CRF) and themaximum entropy Markov model (MEMM). We show that these models allow us toincorporate both labeled and unlabeled data for learning, and at the same time,provide us with the flexibility and accuracy of the discriminative framework.Our experimental results in the video surveillance domain illustrate that thesemodels can perform better than their generative counterpart, the partiallyhidden Markov model, even when a substantial amount of labels are unavailable.
arxiv-6900-43 | MCMC for Hierarchical Semi-Markov Conditional Random Fields | http://arxiv.org/pdf/1408.1162v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh, Hung H. Bui category:stat.ML cs.LG stat.ME published:2014-08-06 summary:Deep architecture such as hierarchical semi-Markov models is an importantclass of models for nested sequential data. Current exact inference schemeseither cost cubic time in sequence length, or exponential time in model depth.These costs are prohibitive for large-scale problems with arbitrary length anddepth. In this contribution, we propose a new approximation technique that mayhave the potential to achieve sub-cubic time complexity in length and lineartime depth, at the cost of some loss of quality. The idea is based on twowell-known methods: Gibbs sampling and Rao-Blackwellisation. We provide somesimulation-based evaluation of the quality of the RGBS with respect to run timeand sequence length.
arxiv-6900-44 | Mixed-Variate Restricted Boltzmann Machines | http://arxiv.org/pdf/1408.1160v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG stat.ME published:2014-08-06 summary:Modern datasets are becoming heterogeneous. To this end, we present in thispaper Mixed-Variate Restricted Boltzmann Machines for simultaneously modellingvariables of multiple types and modalities, including binary and continuousresponses, categorical options, multicategorical choices, ordinal assessmentand category-ranked preferences. Dependency among variables is modeled usinglatent binary variables, each of which can be interpreted as a particularhidden aspect of the data. The proposed model, similar to the standard RBMs,allows fast evaluation of the posterior for the latent variables. Hence, it isnaturally suitable for many common tasks including, but not limited to, (a) asa pre-processing step to convert complex input data into a more convenientvectorial representation through the latent posteriors, thereby offering adimensionality reduction capacity, (b) as a classifier supporting binary,multiclass, multilabel, and label-ranking outputs, or a regression tool forcontinuous outputs and (c) as a data completion tool for multimodal andheterogeneous data. We evaluate the proposed model on a large-scale datasetusing the world opinion survey results on three tasks: feature extraction andvisualization, data completion and prediction.
arxiv-6900-45 | It is hard to see a needle in a haystack: Modeling contrast masking effect in a numerical observer | http://arxiv.org/pdf/1408.1135v1.pdf | author:Ali R. N. Avanaki, Kathryn S. Espig, Albert Xthona, Tom R. L. Kimpe, Predrag R. Bakic, Andrew D. A. Maidment category:cs.CV published:2014-08-05 summary:Within the framework of a virtual clinical trial for breast imaging, we aimto develop numerical observers that follow the same detection performancetrends as those of a typical human observer. In our prior work, we showed thatby including spatiotemporal contrast sensitivity function (stCSF) of humanvisual system (HVS) in a multi-slice channelized Hotelling observer (msCHO), wecan correctly predict trends of a typical human observer performance with theviewing parameters of browsing speed, viewing distance and contrast. In thiswork we further improve our numerical observer by modeling contrast masking.After stCSF, contrast masking is the second most prominent property of HVS andit refers to the fact that the presence of one signal affects the visibilitythreshold for another signal. Our results indicate that the improved numericalobserver better predicts changes in detection performance with backgroundcomplexity.
arxiv-6900-46 | The Case for a Mixed-Initiative Collaborative Neuroevolution Approach | http://arxiv.org/pdf/1408.0998v1.pdf | author:Sebastian Risi, Jinhong Zhang, Rasmus Taarnby, Peter Greve, Jan Piskur, Antonios Liapis, Julian Togelius category:cs.NE published:2014-08-05 summary:It is clear that the current attempts at using algorithms to createartificial neural networks have had mixed success at best when it comes tocreating large networks and/or complex behavior. This should not be unexpected,as creating an artificial brain is essentially a design problem. Human designingenuity still surpasses computational design for most tasks in most domains,including architecture, game design, and authoring literary fiction. This leadsus to ask which the best way is to combine human and machine design capacitieswhen it comes to designing artificial brains. Both of them have their strengthsand weaknesses; for example, humans are much too slow to manually specifythousands of neurons, let alone the billions of neurons that go into a humanbrain, but on the other hand they can rely on a vast repository of common-senseunderstanding and design heuristics that can help them perform a much betterguided search in design space than an algorithm. Therefore, in this paper weargue for a mixed-initiative approach for collaborative online brain buildingand present first results towards this goal.
arxiv-6900-47 | Confidence Sets Based on Thresholding Estimators in High-Dimensional Gaussian Regression Models | http://arxiv.org/pdf/1308.3201v2.pdf | author:Ulrike Schneider category:math.ST stat.ME stat.ML stat.TH published:2013-08-14 summary:We study confidence intervals based on hard-thresholding, soft-thresholding,and adaptive soft-thresholding in a linear regression model where the number ofregressors $k$ may depend on and diverge with sample size $n$. In addition tothe case of known error variance, we define and study versions of theestimators when the error variance is unknown. In the known variance case, weprovide an exact analysis of the coverage properties of such intervals infinite samples. We show that these intervals are always larger than thestandard interval based on the least-squares estimator. Asymptotically, theintervals based on the thresholding estimators are larger even by an order ofmagnitude when the estimators are tuned to perform consistent variableselection. For the unknown-variance case, we provide non-trivial lower boundsfor the coverage probabilities in finite samples and conduct an asymptoticanalysis where the results from the known-variance case can be shown to carryover asymptotically if the number of degrees of freedom $n-k$ tends to infinityfast enough in relation to the thresholding parameter.
arxiv-6900-48 | Speech earthquakes: scaling and universality in human voice | http://arxiv.org/pdf/1408.0985v1.pdf | author:Jordi Luque, Bartolo Luque, Lucas Lacasa category:physics.soc-ph cs.CL q-bio.NC published:2014-08-05 summary:Speech is a distinctive complex feature of human capabilities. In order tounderstand the physics underlying speech production, in this work weempirically analyse the statistics of large human speech datasets rangingseveral languages. We first show that during speech the energy is unevenlyreleased and power-law distributed, reporting a universal robustGutenberg-Richter-like law in speech. We further show that such earthquakes inspeech show temporal correlations, as the interevent statistics are againpower-law distributed. Since this feature takes place in the intra-phonemerange, we conjecture that the responsible for this complex phenomenon is notcognitive, but it resides on the physiological speech production mechanism.Moreover, we show that these waiting time distributions are scale invariantunder a renormalisation group transformation, suggesting that the process ofspeech generation is indeed operating close to a critical point. These resultsare put in contrast with current paradigms in speech processing, which pointtowards low dimensional deterministic chaos as the origin of nonlinear traitsin speech fluctuations. As these latter fluctuations are indeed the aspectsthat humanize synthetic speech, these findings may have an impact in futurespeech synthesis technologies. Results are robust and independent of thecommunication language or the number of speakers, pointing towards an universalpattern and yet another hint of complexity in human speech.
arxiv-6900-49 | A Flexible Iterative Framework for Consensus Clustering | http://arxiv.org/pdf/1408.0972v1.pdf | author:Shaina Race, Carl Meyer category:stat.ML cs.CV cs.LG published:2014-08-05 summary:A novel framework for consensus clustering is presented which has the abilityto determine both the number of clusters and a final solution using multiplealgorithms. A consensus similarity matrix is formed from an ensemble usingmultiple algorithms and several values for k. A variety of dimension reductiontechniques and clustering algorithms are considered for analysis. For noisy orhigh-dimensional data, an iterative technique is presented to refine thisconsensus matrix in way that encourages algorithms to agree upon a commonsolution. We utilize the theory of nearly uncoupled Markov chains to determinethe number, k , of clusters in a dataset by considering a random walk on thegraph defined by the consensus matrix. The eigenvalues of the associatedtransition probability matrix are used to determine the number of clusters.This method succeeds at determining the number of clusters in many datasetswhere previous methods fail. On every considered dataset, our consensus methodprovides a final result with accuracy well above the average of the individualalgorithms.
arxiv-6900-50 | Determining the Number of Clusters via Iterative Consensus Clustering | http://arxiv.org/pdf/1408.0967v1.pdf | author:Shaina Race, Carl Meyer, Kevin Valakuzhy category:stat.ML cs.CV cs.LG published:2014-08-05 summary:We use a cluster ensemble to determine the number of clusters, k, in a groupof data. A consensus similarity matrix is formed from the ensemble usingmultiple algorithms and several values for k. A random walk is induced on thegraph defined by the consensus matrix and the eigenvalues of the associatedtransition probability matrix are used to determine the number of clusters. Fornoisy or high-dimensional data, an iterative technique is presented to refinethis consensus matrix in way that encourages a block-diagonal form. It is shownthat the resulting consensus matrix is generally superior to existingsimilarity matrices for this type of spectral analysis.
arxiv-6900-51 | DuSK: A Dual Structure-preserving Kernel for Supervised Tensor Learning with Applications to Neuroimages | http://arxiv.org/pdf/1407.8289v2.pdf | author:Lifang He, Xiangnan Kong, Philip S. Yu, Ann B. Ragin, Zhifeng Hao, Xiaowei Yang category:cs.LG published:2014-07-31 summary:With advances in data collection technologies, tensor data is assumingincreasing prominence in many applications and the problem of supervised tensorlearning has emerged as a topic of critical significance in the data mining andmachine learning community. Conventional methods for supervised tensor learningmainly focus on learning kernels by flattening the tensor into vectors ormatrices, however structural information within the tensors will be lost. Inthis paper, we introduce a new scheme to design structure-preserving kernelsfor supervised tensor learning. Specifically, we demonstrate how to leveragethe naturally available structure within the tensorial representation to encodeprior knowledge in the kernel. We proposed a tensor kernel that can preservetensor structures based upon dual-tensorial mapping. The dual-tensorial mappingfunction can map each tensor instance in the input space to another tensor inthe feature space while preserving the tensorial structure. Theoretically, ourapproach is an extension of the conventional kernels in the vector space totensor space. We applied our novel kernel in conjunction with SVM to real-worldtensor classification problems including brain fMRI classification for threedifferent diseases (i.e., Alzheimer's disease, ADHD and brain damage by HIV).Extensive empirical studies demonstrate that our proposed approach caneffectively boost tensor classification performances, particularly with smallsample sizes.
arxiv-6900-52 | Real-Time Traffic Signal Control for Modern Roundabouts by Using Particle Swarm Optimization-Based Fuzzy Controller | http://arxiv.org/pdf/1408.0689v2.pdf | author:Yue-Jiao Gong, Jun Zhang category:cs.NE published:2014-08-04 summary:Due to that the existing traffic facilities can hardly be extended,developing traffic signal control methods is the most important way to improvethe traffic efficiency of modern roundabouts. This paper proposes a noveltraffic signal controller with two fuzzy layers for signalizing the roundabout.The outer layer of the controller computes urgency degrees of all the phasesubsets and then activates the most urgent subset. This mechanism helps toinstantly respond to the current traffic condition of the roundabout so as toimprove real-timeness. The inner layer of the controller computes extensiontime of the current phase. If the extension value is larger than a thresholdvalue, the current phase is maintained; otherwise the next phase in the runningphase subset (selected by the outer layer) is activated. The inner layer adoptswell-designed phase sequences, which helps to smooth the traffic flows and toavoid traffic jam. In general, the proposed traffic signal controller iscapable of improving real-timeness as well as reducing traffic congestion.Moreover, an offline particle swarm optimization (PSO) algorithm is developedto optimize the membership functions adopted in the proposed controller. Byusing optimal membership functions, the performance of the controller can befurther improved. Simulation results demonstrate that the proposed controlleroutperforms previous traffic signal controllers in terms of improving thetraffic efficiency of modern roundabouts.
arxiv-6900-53 | Estimating Maximally Probable Constrained Relations by Mathematical Programming | http://arxiv.org/pdf/1408.0838v1.pdf | author:Lizhen Qu, Bjoern Andres category:cs.LG cs.NA math.OC stat.ML published:2014-08-04 summary:Estimating a constrained relation is a fundamental problem in machinelearning. Special cases are classification (the problem of estimating a mapfrom a set of to-be-classified elements to a set of labels), clustering (theproblem of estimating an equivalence relation on a set) and ranking (theproblem of estimating a linear order on a set). We contribute a family ofprobability measures on the set of all relations between two finite, non-emptysets, which offers a joint abstraction of multi-label classification,correlation clustering and ranking by linear ordering. Estimating (learning) amaximally probable measure, given (a training set of) related and unrelatedpairs, is a convex optimization problem. Estimating (inferring) a maximallyprobable relation, given a measure, is a 01-linear program. It is solved inlinear time for maps. It is NP-hard for equivalence relations and linearorders. Practical solutions for all three cases are shown in experiments withreal data. Finally, estimating a maximally probable measure and relationjointly is posed as a mixed-integer nonlinear program. This formulationsuggests a mathematical programming approach to semi-supervised learning.
arxiv-6900-54 | Object Detection Through Exploration With A Foveated Visual Field | http://arxiv.org/pdf/1408.0814v1.pdf | author:Emre Akbas, Miguel P. Eckstein category:cs.CV published:2014-08-04 summary:We present a foveated object detector (FOD) as a biologically-inspiredalternative to the sliding window (SW) approach which is the dominant method ofsearch in computer vision object detection. Similar to the human visual system,the FOD has higher resolution at the fovea and lower resolution at the visualperiphery. Consequently, more computational resources are allocated at thefovea and relatively fewer at the periphery. The FOD processes the entirescene, uses retino-specific object detection classifiers to guide eyemovements, aligns its fovea with regions of interest in the input image andintegrates observations across multiple fixations. Our approach combines modernobject detectors from computer vision with a recent model of peripheral poolingregions found at the V1 layer of the human visual system. We assessed variouseye movement strategies on the PASCAL VOC 2007 dataset and show that the FODperforms on par with the SW detector while bringing significant computationalcost savings.
arxiv-6900-55 | Targetable Named Entity Recognition in Social Media | http://arxiv.org/pdf/1408.0782v1.pdf | author:Sandeep Ashwini, Jinho D. Choi category:cs.CL published:2014-08-04 summary:We present a novel approach for recognizing what we call targetable namedentities; that is, named entities in a targeted set (e.g, movies, books, TVshows). Unlike many other NER systems that need to retrain their statisticalmodels as new entities arrive, our approach does not require such retraining,which makes it more adaptable for types of entities that are frequentlyupdated. For this preliminary study, we focus on one entity type, movie title,using data collected from Twitter. Our system is tested on two evaluation sets,one including only entities corresponding to movies in our training set, andthe other excluding any of those entities. Our final model shows F1-scores of76.19% and 78.70% on these evaluation sets, which gives strong evidence thatour approach is completely unbiased to any par- ticular set of entities foundduring training.
arxiv-6900-56 | Multithreshold Entropy Linear Classifier | http://arxiv.org/pdf/1408.1054v1.pdf | author:Wojciech Marian Czarnecki, Jacek Tabor category:cs.LG stat.ML published:2014-08-04 summary:Linear classifiers separate the data with a hyperplane. In this paper wefocus on the novel method of construction of multithreshold linear classifier,which separates the data with multiple parallel hyperplanes. Proposed model isbased on the information theory concepts -- namely Renyi's quadratic entropyand Cauchy-Schwarz divergence. We begin with some general properties, including data scale invariance. Thenwe prove that our method is a multithreshold large margin classifier, whichshows the analogy to the SVM, while in the same time works with much broaderclass of hypotheses. What is also interesting, proposed method is aimed at themaximization of the balanced quality measure (such as Matthew's CorrelationCoefficient) as opposed to very common maximization of the accuracy. Thisfeature comes directly from the optimization problem statement and is furtherconfirmed by the experiments on the UCI datasets. It appears, that our Multithreshold Entropy Linear Classifier (MELC) obtainessimilar or higher scores than the ones given by SVM on both synthetic and realdata. We show how proposed approach can be benefitial for the cheminformaticsin the task of ligands activity prediction, where despite better classificationresults, MELC gives some additional insight into the data structure (classes ofunderrepresented chemical compunds).
arxiv-6900-57 | A Pattern Recognition System for Detecting Use of Mobile Phones While Driving | http://arxiv.org/pdf/1408.0680v1.pdf | author:Rafael A. Berri, Alexandre G. Silva, Rafael S. Parpinelli, Elaine Girardi, Rangel Arthur category:cs.CV published:2014-08-04 summary:It is estimated that 80% of crashes and 65% of near collisions involveddrivers inattentive to traffic for three seconds before the event. This paperdevelops an algorithm for extracting characteristics allowing the cell phonesidentification used during driving a vehicle. Experiments were performed onsets of images with 100 positive images (with phone) and the other 100 negativeimages (no phone), containing frontal images of the driver. Support VectorMachine (SVM) with Polynomial kernel is the most advantageous classificationsystem to the features provided by the algorithm, obtaining a success rate of91.57% for the vision system. Tests done on videos show that it is possible touse the image datasets for training classifiers in real situations. Periods of3 seconds were correctly classified at 87.43% of cases.
arxiv-6900-58 | A Multi-Stage Supply Chain Network Optimization Using Genetic Algorithms | http://arxiv.org/pdf/1408.0614v1.pdf | author:Nelson Christopher Dzupire, Yaw Nkansah-Gyekye category:math.OC cs.NE published:2014-08-04 summary:In today's global business market place, individual firms no longer competeas independent entities with unique brand names but as integral part of supplychain links. Key to success of any business is satisfying customer's demands ontime which may result in cost reductions and increase in service level. Insupply chain networks decisions are made with uncertainty about product'sdemands, costs, prices, lead times, quality in a competitive and collaborativeenvironment. If poor decisions are made, they may lead to excess inventoriesthat are costly or to insufficient inventory that cannot meet customer'sdemands. In this work we developed a bi-objective model that minimizes systemwide costs of the supply chain and delays on delivery of products todistribution centers for a three echelon supply chain. Picking a set of Paretofront for multi-objective optimization problems require robust and efficientmethods that can search an entire space. We used evolutionary algorithms tofind the set of Pareto fronts which have proved to be effective in finding theentire set of Pareto fronts.
arxiv-6900-59 | Adapted Approach for Fruit Disease Identification using Images | http://arxiv.org/pdf/1405.4930v5.pdf | author:Shiv Ram Dubey, Anand Singh Jalal category:cs.CV published:2014-05-20 summary:Diseases in fruit cause devastating problem in economic losses and productionin agricultural industry worldwide. In this paper, an adaptive approach for theidentification of fruit diseases is proposed and experimentally validated. Theimage processing based proposed approach is composed of the following mainsteps; in the first step K-Means clustering technique is used for the defectsegmentation, in the second step some state of the art features are extractedfrom the segmented image, and finally images are classified into one of theclasses by using a Multi-class Support Vector Machine. We have considereddiseases of apple as a test case and evaluated our approach for three types ofapple diseases namely apple scab, apple blotch and apple rot. Our experimentalresults express that the proposed solution can significantly support accuratedetection and automatic identification of fruit diseases. The classificationaccuracy for the proposed solution is achieved up to 93%.
arxiv-6900-60 | Implementation of a language driven Backpropagation algorithm | http://arxiv.org/pdf/1309.5676v3.pdf | author:I. V. Grossu, C. I. Ciuluvica category:cs.NE published:2013-09-23 summary:Inspired by the importance of both communication and feedback on errors inhuman learning, our main goal was to implement a similar mechanism insupervised learning of artificial neural networks. The starting point in ourstudy was the observation that words should accompany the input vectorsincluded in the training set, thus extending the ANN input space. This had asconsequence the necessity to take into consideration a modified sigmoidactivation function for neurons in the first hidden layer (in agreement with aspecific MLP apartment structure), and also a modified version of theBackpropagation algorithm, which allows using of unspecified (null) desiredoutput components. Following the belief that basic concepts should be tested onsimple examples, the previous mentioned mechanism was applied on both the XORproblem and a didactic color case study. In this context, we noticed theinteresting fact that the ANN was capable to categorize all desired inputvectors in the absence of their corresponding words, even though the trainingset included only word accompanied inputs, in both positive and negativeexamples. Further analysis along applying this approach to more complexscenarios is currently in progress, as we consider the proposed language-drivenalgorithm might contribute to a better understanding of learning in humans,opening as well the possibility to create a specific category of artificialneural networks, with abstraction capabilities.
arxiv-6900-61 | On the Computational Intractability of Exact and Approximate Dictionary Learning | http://arxiv.org/pdf/1405.6664v2.pdf | author:Andreas M. Tillmann category:cs.IT cs.LG math.IT published:2014-05-26 summary:The efficient sparse coding and reconstruction of signal vectors via linearobservations has received a tremendous amount of attention over the lastdecade. In this context, the automated learning of a suitable basis orovercomplete dictionary from training data sets of certain signal classes foruse in sparse representations has turned out to be of particular importanceregarding practical signal processing applications. Most popular dictionarylearning algorithms involve NP-hard sparse recovery problems in each iteration,which may give some indication about the complexity of dictionary learning butdoes not constitute an actual proof of computational intractability. In thistechnical note, we show that learning a dictionary with which a given set oftraining signals can be represented as sparsely as possible is indeed NP-hard.Moreover, we also establish hardness of approximating the solution to withinlarge factors of the optimal sparsity level. Furthermore, we give NP-hardnessand non-approximability results for a recent dictionary learning variationcalled the sensor permutation problem. Along the way, we also obtain a newnon-approximability result for the classical sparse recovery problem fromcompressed sensing.
arxiv-6900-62 | Adaptive Wavelet Based Identification and Extraction of PQRST Combination in Randomly Stretching ECG Sequence | http://arxiv.org/pdf/1408.0453v1.pdf | author:T. R. Gopalakrishnan Nair, A. P. Geetha, M. Asharani category:cs.CV published:2014-08-03 summary:Cardiovascular system study using ECG signals have evolved tremendously inthe domain of electronics and signal processing. However, there are certainfloating challenges unresolved in the analysis and detection of abnormalperformances of cardiovascular system. As the medical field is moving towardsmore automated and intelligent systems, wrong detection or wronginterpretations of ECG waveform of abnormal conditions can be quite fatal.Since the PQRST signals vary their positions randomly, the process of locating,identifying and classifying each feature can be cumbersome and it is prone toerrors. Here we present an automated scheme using adaptive wavelet to detectprominent R-peak with extreme accuracy and algorithmically tag and mark thecoexisting peaks P, Q, S, and T with almost same accuracy. The adaptive waveletapproach used in this scheme is capable of detecting R-peak in ECG with 99.99%accuracy along with the rest of the waveforms.
arxiv-6900-63 | Methodology For Detection of QRS Pattern Using Secondary Wavelets | http://arxiv.org/pdf/1408.0452v1.pdf | author:T. R. Gopalakrishnan Nair, A. P. Geetha, Asharani category:cs.CV published:2014-08-03 summary:Applications of wavelet transform to the field of health care signals havepaved the way for implementing revolutionary approaches in detecting thepresence of certain abnormalities in human health patterns. There wereextensive studies carried out using primary wavelets in various signals likeElectrocardiogram (ECG), sonogram etc. with a certain amount of success. On theother hand analysis using secondary wavelets which inherits the characteristicsof a set of variations available in signals like ECG can be a promise to detectdiseases with ease. Here a method to create a generalized adapted wavelet ispresented which contains the information of QRS pattern collected from ananomaly sample space. The method has been tested and found to be successful inlocating the position of R peak in noise embedded ECG signal.
arxiv-6900-64 | A Bayesian estimation approach to analyze non-Gaussian data-generating processes with latent classes | http://arxiv.org/pdf/1408.0337v1.pdf | author:Naoki Tanaka, Shohei Shimizu, Takashi Washio category:stat.ML published:2014-08-02 summary:A large amount of observational data has been accumulated in various fieldsin recent times, and there is a growing need to estimate the generatingprocesses of these data. A linear non-Gaussian acyclic model (LiNGAM) based onthe non-Gaussianity of external influences has been proposed to estimate thedata-generating processes of variables. However, the results of the estimationcan be biased if there are latent classes. In this paper, we first reviewLiNGAM, its extended model, as well as the estimation procedure for LiNGAM in aBayesian framework. We then propose a new Bayesian estimation procedure thatsolves the problem.
arxiv-6900-65 | Matrix Factorization with Explicit Trust and Distrust Relationships | http://arxiv.org/pdf/1408.0325v1.pdf | author:Rana Forsati, Mehrdad Mahdavi, Mehrnoush Shamsfard, Mohamed Sarwat category:cs.SI cs.IR cs.LG published:2014-08-02 summary:With the advent of online social networks, recommender systems have becamecrucial for the success of many online applications/services due to theirsignificance role in tailoring these applications to user-specific needs orpreferences. Despite their increasing popularity, in general recommendersystems suffer from the data sparsity and the cold-start problems. To alleviatethese issues, in recent years there has been an upsurge of interest inexploiting social information such as trust relations among users along withthe rating data to improve the performance of recommender systems. The mainmotivation for exploiting trust information in recommendation process stemsfrom the observation that the ideas we are exposed to and the choices we makeare significantly influenced by our social context. However, in large usercommunities, in addition to trust relations, the distrust relations also existbetween users. For instance, in Epinions the concepts of personal "web oftrust" and personal "block list" allow users to categorize their friends basedon the quality of reviews into trusted and distrusted friends, respectively. Inthis paper, we propose a matrix factorization based model for recommendation insocial rating networks that properly incorporates both trust and distrustrelationships aiming to improve the quality of recommendations and mitigate thedata sparsity and the cold-start users issues. Through experiments on theEpinions data set, we show that our new algorithm outperforms its standardtrust-enhanced or distrust-enhanced counterparts with respect to accuracy,thereby demonstrating the positive effect that incorporation of explicitdistrust information can have on recommender systems.
arxiv-6900-66 | Generalized Species Sampling Priors with Latent Beta reinforcements | http://arxiv.org/pdf/1012.0866v4.pdf | author:Edoardo M. Airoldi, Thiago Costa, Federico Bassetti, Fabrizio Leisen, Michele Guindani category:math.ST cs.LG stat.ME stat.TH published:2010-12-04 summary:Many popular Bayesian nonparametric priors can be characterized in terms ofexchangeable species sampling sequences. However, in some applications,exchangeability may not be appropriate. We introduce a {novel andprobabilistically coherent family of non-exchangeable species samplingsequences characterized by a tractable predictive probability function withweights driven by a sequence of independent Beta random variables. We comparetheir theoretical clustering properties with those of the Dirichlet Process andthe two parameters Poisson-Dirichlet process. The proposed constructionprovides a complete characterization of the joint process, differently fromexisting work. We then propose the use of such process as prior distribution ina hierarchical Bayes modeling framework, and we describe a Markov Chain MonteCarlo sampler for posterior inference. We evaluate the performance of the priorand the robustness of the resulting inference in a simulation study, providinga comparison with popular Dirichlet Processes mixtures and Hidden MarkovModels. Finally, we develop an application to the detection of chromosomalaberrations in breast cancer by leveraging array CGH data.
arxiv-6900-67 | Functional Principal Component Analysis and Randomized Sparse Clustering Algorithm for Medical Image Analysis | http://arxiv.org/pdf/1408.0204v1.pdf | author:Nan Lin, Junhai Jiang, Shicheng Guo, Momiao Xiong category:stat.ML cs.AI cs.CV cs.LG published:2014-08-01 summary:Due to advances in sensors, growing large and complex medical image data havethe ability to visualize the pathological change in the cellular or even themolecular level or anatomical changes in tissues and organs. As a consequence,the medical images have the potential to enhance diagnosis of disease,prediction of clinical outcomes, characterization of disease progression,management of health care and development of treatments, but also pose greatmethodological and computational challenges for representation and selection offeatures in image cluster analysis. To address these challenges, we firstextend one dimensional functional principal component analysis to the twodimensional functional principle component analyses (2DFPCA) to fully capturespace variation of image signals. Image signals contain a large number ofredundant and irrelevant features which provide no additional or no usefulinformation for cluster analysis. Widely used methods for removing redundantand irrelevant features are sparse clustering algorithms using a lasso-typepenalty to select the features. However, the accuracy of clustering using alasso-type penalty depends on how to select penalty parameters and a thresholdfor selecting features. In practice, they are difficult to determine. Recently,randomized algorithms have received a great deal of attention in big dataanalysis. This paper presents a randomized algorithm for accurate featureselection in image cluster analysis. The proposed method is applied to ovarianand kidney cancer histology image data from the TCGA database. The resultsdemonstrate that the randomized feature selection method coupled withfunctional principal component analysis substantially outperforms the currentsparse clustering algorithms in image cluster analysis.
arxiv-6900-68 | A RobustICA Based Algorithm for Blind Separation of Convolutive Mixtures | http://arxiv.org/pdf/1408.0193v1.pdf | author:Zaid Albataineh, Fathi M. Salem category:cs.LG cs.SD published:2014-08-01 summary:We propose a frequency domain method based on robust independent componentanalysis (RICA) to address the multichannel Blind Source Separation (BSS)problem of convolutive speech mixtures in highly reverberant environments. Weimpose regularization processes to tackle the ill-conditioning problem of thecovariance matrix and to mitigate the performance degradation in the frequencydomain. We apply an algorithm to separate the source signals in adverseconditions, i.e. high reverberation conditions when short observation signalsare available. Furthermore, we study the impact of several parameters on theperformance of separation, e.g. overlapping ratio and window type of thefrequency domain method. We also compare different techniques to solve thefrequency-domain permutation ambiguity. Through simulations and real worldexperiments, we verify the superiority of the presented convolutive algorithmamong other BSS algorithms, including recursive regularized ICA (RR ICA),independent vector analysis (IVA).
arxiv-6900-69 | OpenML: networked science in machine learning | http://arxiv.org/pdf/1407.7722v2.pdf | author:Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, Luis Torgo category:cs.LG cs.CY published:2014-07-29 summary:Many sciences have made significant breakthroughs by adopting online toolsthat help organize, structure and mine information that is too detailed to beprinted in journals. In this paper, we introduce OpenML, a place for machinelearning researchers to share and organize data in fine detail, so that theycan work more effectively, be more visible, and collaborate with others totackle harder problems. We discuss how OpenML relates to other examples ofnetworked science and what benefits it brings for machine learning research,individual scientists, as well as students and practitioners.
arxiv-6900-70 | Randomized Memetic Artificial Bee Colony Algorithm | http://arxiv.org/pdf/1408.0102v1.pdf | author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.NE published:2014-08-01 summary:Artificial Bee Colony (ABC) optimization algorithm is one of the recentpopulation based probabilistic approach developed for global optimization. ABCis simple and has been showed significant improvement over other NatureInspired Algorithms (NIAs) when tested over some standard benchmark functionsand for some complex real world optimization problems. Memetic Algorithms alsobecome one of the key methodologies to solve the very large and complexreal-world optimization problems. The solution search equation of Memetic ABCis based on Golden Section Search and an arbitrary value which tries to balanceexploration and exploitation of search space. But still there are some chancesto skip the exact solution due to its step size. In order to balance betweendiversification and intensification capability of the Memetic ABC, it israndomized the step size in Memetic ABC. The proposed algorithm is named asRandomized Memetic ABC (RMABC). In RMABC, new solutions are generated nearbythe best so far solution and it helps to increase the exploitation capabilityof Memetic ABC. The experiments on some test problems of different complexitiesand one well known engineering optimization application show that the proposedalgorithm outperforms over Memetic ABC (MeABC) and some other variant of ABCalgorithm(like Gbest guided ABC (GABC),Hooke Jeeves ABC (HJABC), Best-So-FarABC (BSFABC) and Modified ABC (MABC) in case of almost all the problems.
arxiv-6900-71 | Memetic Search in Differential Evolution Algorithm | http://arxiv.org/pdf/1408.0101v1.pdf | author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.NE published:2014-08-01 summary:Differential Evolution (DE) is a renowned optimization stratagem that caneasily solve nonlinear and comprehensive problems. DE is a well known anduncomplicated population based probabilistic approach for comprehensiveoptimization. It has apparently outperformed a number of EvolutionaryAlgorithms and further search heuristics in the vein of Particle SwarmOptimization at what time of testing over both yardstick and actual worldproblems. Nevertheless, DE, like other probabilistic optimization algorithms,from time to time exhibits precipitate convergence and stagnates at suboptimalposition. In order to stay away from stagnation behavior while maintaining anexcellent convergence speed, an innovative search strategy is introduced, namedmemetic search in DE. In the planned strategy, positions update equationcustomized as per a memetic search stratagem. In this strategy a bettersolution participates more times in the position modernize procedure. Theposition update equation is inspired from the memetic search in artificial beecolony algorithm. The proposed strategy is named as Memetic Search inDifferential Evolution (MSDE). To prove efficiency and efficacy of MSDE, it istested over 8 benchmark optimization problems and three real world optimizationproblems. A comparative analysis has also been carried out among proposed MSDEand original DE. Results show that the anticipated algorithm go one better thanthe basic DE and its recent deviations in a good number of the experiments.
arxiv-6900-72 | Conditional Restricted Boltzmann Machines for Cold Start Recommendations | http://arxiv.org/pdf/1408.0096v1.pdf | author:Jiankou Li, Wei Zhang category:cs.IR cs.LG stat.ML published:2014-08-01 summary:Restricted Boltzman Machines (RBMs) have been successfully used inrecommender systems. However, as with most of other collaborative filteringtechniques, it cannot solve cold start problems for there is no rating for anew item. In this paper, we first apply conditional RBM (CRBM) which could takeextra information into account and show that CRBM could solve cold startproblem very well, especially for rating prediction task. CRBM naturallycombine the content and collaborative data under a single framework which couldbe fitted effectively. Experiments show that CRBM can be compared favourablywith matrix factorization models, while hidden features learned from the formermodels are more easy to be interpreted.
arxiv-6900-73 | Bayesian nonparametric Plackett-Luce models for the analysis of preferences for college degree programmes | http://arxiv.org/pdf/1211.5037v3.pdf | author:François Caron, Yee Whye Teh, Thomas Brendan Murphy category:stat.ML cs.LG stat.ME published:2012-11-21 summary:In this paper we propose a Bayesian nonparametric model for clusteringpartial ranking data. We start by developing a Bayesian nonparametric extensionof the popular Plackett-Luce choice model that can handle an infinite number ofchoice items. Our framework is based on the theory of random atomic measures,with the prior specified by a completely random measure. We characterise theposterior distribution given data, and derive a simple and effective Gibbssampler for posterior simulation. We then develop a Dirichlet process mixtureextension of our model and apply it to investigate the clustering ofpreferences for college degree programmes amongst Irish secondary schoolgraduates. The existence of clusters of applicants who have similar preferencesfor degree programmes is established and we determine that subject matter andgeographical location of the third level institution characterise theseclusters.
arxiv-6900-74 | A Framework for learning multi-agent dynamic formation strategy in real-time applications | http://arxiv.org/pdf/1408.0058v1.pdf | author:Mehrab Norouzitallab, Valiallah Monajjemi, Saeed Shiry Ghidary, Mohammad Bagher Menhaj category:cs.RO cs.LG cs.MA published:2014-08-01 summary:Formation strategy is one of the most important parts of many multi-agentsystems with many applications in real world problems. In this paper, aframework for learning this task in a limited domain (restricted environment)is proposed. In this framework, agents learn either directly by observing anexpert behavior or indirectly by observing other agents or objects behavior.First, a group of algorithms for learning formation strategy based on limitedfeatures will be presented. Due to distributed and complex nature of manymulti-agent systems, it is impossible to include all features directly in thelearning process; thus, a modular scheme is proposed in order to reduce thenumber of features. In this method, some important features have indirectinfluence in learning instead of directly involving them as input features.This framework has the ability to dynamically assign a group of positions to agroup of agents to improve system performance. In addition, it can change theformation strategy when the context changes. Finally, this framework is able toautomatically produce many complex and flexible formation strategy algorithmswithout directly involving an expert to present and implement such complexalgorithms.
arxiv-6900-75 | Thurstonian Boltzmann Machines: Learning from Multiple Inequalities | http://arxiv.org/pdf/1408.0055v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG stat.ME published:2014-08-01 summary:We introduce Thurstonian Boltzmann Machines (TBM), a unified architecturethat can naturally incorporate a wide range of data inputs at the same time.Our motivation rests in the Thurstonian view that many discrete data types canbe considered as being generated from a subset of underlying latent continuousvariables, and in the observation that each realisation of a discrete typeimposes certain inequalities on those variables. Thus learning and inference inTBM reduce to making sense of a set of inequalities. Our proposed TBM naturallysupports the following types: Gaussian, intervals, censored, binary,categorical, muticategorical, ordinal, (in)-complete rank with and withoutties. We demonstrate the versatility and capacity of the proposed model onthree applications of very different natures; namely handwritten digitrecognition, collaborative filtering and complex social survey analysis.
arxiv-6900-76 | Cumulative Restricted Boltzmann Machines for Ordinal Matrix Data Analysis | http://arxiv.org/pdf/1408.0047v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.IR cs.LG stat.AP stat.ME published:2014-07-31 summary:Ordinal data is omnipresent in almost all multiuser-generated feedback -questionnaires, preferences etc. This paper investigates modelling of ordinaldata with Gaussian restricted Boltzmann machines (RBMs). In particular, wepresent the model architecture, learning and inference procedures for bothvector-variate and matrix-variate ordinal data. We show that our model is ableto capture latent opinion profile of citizens around the world, and iscompetitive against state-of-art collaborative filtering techniques onlarge-scale public datasets. The model thus has the potential to extendapplication of RBMs to diverse domains such as recommendation systems, productreviews and expert assessments.
arxiv-6900-77 | Learning From Ordered Sets and Applications in Collaborative Ranking | http://arxiv.org/pdf/1408.0043v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:cs.LG cs.IR stat.ML published:2014-07-31 summary:Ranking over sets arise when users choose between groups of items. Forexample, a group may be of those movies deemed $5$ stars to them, or acustomized tour package. It turns out, to model this data type properly, weneed to investigate the general combinatorics problem of partitioning a set andordering the subsets. Here we construct a probabilistic log-linear model over aset of ordered subsets. Inference in this combinatorial space is highlychallenging: The space size approaches $(N!/2)6.93145^{N+1}$ as $N$ approachesinfinity. We propose a \texttt{split-and-merge} Metropolis-Hastings procedurethat can explore the state-space efficiently. For discovering hidden aspects inthe data, we enrich the model with latent binary variables so that theposteriors can be efficiently evaluated. Finally, we evaluate the proposedmodel on large-scale collaborative filtering tasks and demonstrate that it iscompetitive against state-of-the-art methods.
arxiv-6900-78 | Learning Nash Equilibria in Congestion Games | http://arxiv.org/pdf/1408.0017v1.pdf | author:Walid Krichene, Benjamin Drighès, Alexandre M. Bayen category:cs.LG cs.GT published:2014-07-31 summary:We study the repeated congestion game, in which multiple populations ofplayers share resources, and make, at each iteration, a decentralized decisionon which resources to utilize. We investigate the following question: given amodel of how individual players update their strategies, does the resultingdynamics of strategy profiles converge to the set of Nash equilibria of theone-shot game? We consider in particular a model in which players update theirstrategies using algorithms with sublinear discounted regret. We show that theresulting sequence of strategy profiles converges to the set of Nash equilibriain the sense of Ces\`aro means. However, strong convergence is not guaranteedin general. We show that strong convergence can be guaranteed for a class ofalgorithms with a vanishing upper bound on discounted regret, and which satisfyan additional condition. We call such algorithms AREP algorithms, forApproximate REPlicator, as they can be interpreted as a discrete-timeapproximation of the replicator equation, which models the continuous-timeevolution of population strategies, and which is known to converge for theclass of congestion games. In particular, we show that the discounted Hedgealgorithm belongs to the AREP class, which guarantees its strong convergence.
arxiv-6900-79 | A Bottom-Up Approach for Automatic Pancreas Segmentation in Abdominal CT Scans | http://arxiv.org/pdf/1407.8497v1.pdf | author:Amal Farag, Le Lu, Evrim Turkbey, Jiamin Liu, Ronald M. Summers category:cs.CV published:2014-07-31 summary:Organ segmentation is a prerequisite for a computer-aided diagnosis (CAD)system to detect pathologies and perform quantitative analysis. Foranatomically high-variability abdominal organs such as the pancreas, previoussegmentation works report low accuracies when comparing to organs like theheart or liver. In this paper, a fully-automated bottom-up method is presentedfor pancreas segmentation, using abdominal computed tomography (CT) scans. Themethod is based on a hierarchical two-tiered information propagation byclassifying image patches. It labels superpixels as pancreas or not via poolingpatch-level confidences on 2D CT slices over-segmented by the Simple LinearIterative Clustering approach. A supervised random forest (RF) classifier istrained on the patch level and a two-level cascade of RFs is applied at thesuperpixel level, coupled with multi-channel feature extraction, respectively.On six-fold cross-validation using 80 patient CT volumes, we achieved 68.8%Dice coefficient and 57.2% Jaccard Index, comparable to or slightly better thanpublished state-of-the-art methods.
arxiv-6900-80 | Constrained Parametric Proposals and Pooling Methods for Semantic Segmentation in RGB-D Images | http://arxiv.org/pdf/1312.7715v2.pdf | author:Dan Banica, Cristian Sminchisescu category:cs.CV published:2013-12-30 summary:We focus on the problem of semantic segmentation based on RGB-D data, withemphasis on analyzing cluttered indoor scenes containing many instances frommany visual categories. Our approach is based on a parametric figure-groundintensity and depth-constrained proposal process that generates spatial layouthypotheses at multiple locations and scales in the image followed by asequential inference algorithm that integrates the proposals into a completescene estimate. Our contributions can be summarized as proposing the following:(1) a generalization of parametric max flow figure-ground proposal methodologyto take advantage of intensity and depth information, in order tosystematically and efficiently generate the breakpoints of an underlyingspatial model in polynomial time, (2) new region description methods based onsecond-order pooling over multiple features constructed using both intensityand depth channels, (3) an inference procedure that can resolve conflicts inoverlapping spatial partitions, and handles scenes with a large number ofobjects category instances, of very different scales, (4) extensive evaluationof the impact of depth, as well as the effectiveness of a large number ofdescriptors, both pre-designed and automatically obtained using deep learning,in a difficult RGB-D semantic segmentation problem with 92 classes. We reportstate of the art results in the challenging NYU Depth v2 dataset, extended forRMRC 2013 Indoor Segmentation Challenge, where currently the proposed modelranks first, with an average score of 24.61% and a number of 39 classes won.Moreover, we show that by combining second-order and deep learning features,over 15% relative accuracy improvements can be additionally achieved. In ascene classification benchmark, our methodology further improves the state ofthe art by 24%.
arxiv-6900-81 | A New Model of Array Grammar for generating Connected Patterns on an Image Neighborhood | http://arxiv.org/pdf/1407.8337v1.pdf | author:G. Vishnu Murthy, Pavan Kumar C., Vakulabharanam Vijaya Kumar category:cs.FL cs.CV published:2014-07-31 summary:Study of patterns on images is recognized as an important step incharacterization and classification of image. The ability to efficientlyanalyze and describe image patterns is thus of fundamental importance. Thestudy of syntactic methods of describing pictures has been of interest forresearchers. Array Grammars can be used to represent and recognize connectedpatterns. In any image the patterns are recognized using connected patterns. Itis very difficult to represent all connected patterns (CP) even on a small 3 x3 neighborhood in a pictorial way. The present paper proposes the model ofarray grammar capable of generating any kind of simple or complex pattern andderivation of connected pattern in an image neighborhood using the proposedgrammar is discussed.
arxiv-6900-82 | Learning Mixtures of Linear Classifiers | http://arxiv.org/pdf/1311.2547v4.pdf | author:Yuekai Sun, Stratis Ioannidis, Andrea Montanari category:cs.LG stat.ML published:2013-11-11 summary:We consider a discriminative learning (regression) problem, whereby theregression function is a convex combination of k linear classifiers. Existingapproaches are based on the EM algorithm, or similar techniques, withoutprovable guarantees. We develop a simple method based on spectral techniquesand a `mirroring' trick, that discovers the subspace spanned by theclassifiers' parameter vectors. Under a probabilistic assumption on the featurevector distribution, we prove that this approach has nearly optimal statisticalefficiency.
arxiv-6900-83 | Recommending with an Agenda: Active Learning of Private Attributes using Matrix Factorization | http://arxiv.org/pdf/1311.6802v2.pdf | author:Smriti Bhagat, Udi Weinsberg, Stratis Ioannidis, Nina Taft category:cs.LG cs.CY published:2013-11-26 summary:Recommender systems leverage user demographic information, such as age,gender, etc., to personalize recommendations and better place their targetedads. Oftentimes, users do not volunteer this information due to privacyconcerns, or due to a lack of initiative in filling out their online profiles.We illustrate a new threat in which a recommender learns private attributes ofusers who do not voluntarily disclose them. We design both passive and activeattacks that solicit ratings for strategically selected items, and could thusbe used by a recommender system to pursue this hidden agenda. Our methods arebased on a novel usage of Bayesian matrix factorization in an active learningsetting. Evaluations on multiple datasets illustrate that such attacks areindeed feasible and use significantly fewer rated items than static inferencemethods. Importantly, they succeed without sacrificing the quality ofrecommendations to users.
arxiv-6900-84 | Two-pass Discourse Segmentation with Pairing and Global Features | http://arxiv.org/pdf/1407.8215v1.pdf | author:Vanessa Wei Feng, Graeme Hirst category:cs.CL published:2014-07-30 summary:Previous attempts at RST-style discourse segmentation typically adoptfeatures centered on a single token to predict whether to insert a boundarybefore that token. In contrast, we develop a discourse segmenter utilizing aset of pairing features, which are centered on a pair of adjacent tokens in thesentence, by equally taking into account the information from both tokens.Moreover, we propose a novel set of global features, which encodecharacteristics of the segmentation as a whole, once we have an initialsegmentation. We show that both the pairing and global features are useful ontheir own, and their combination achieved an $F_1$ of 92.6% of identifyingin-sentence discourse boundaries, which is a 17.8% error-rate reduction overthe state-of-the-art performance, approaching 95% of human performance. Inaddition, similar improvement is observed across different classificationframeworks.
arxiv-6900-85 | Fast Bayesian Feature Selection for High Dimensional Linear Regression in Genomics via the Ising Approximation | http://arxiv.org/pdf/1407.8187v1.pdf | author:Charles K. Fisher, Pankaj Mehta category:q-bio.QM cs.LG stat.ML published:2014-07-30 summary:Feature selection, identifying a subset of variables that are relevant forpredicting a response, is an important and challenging component of manymethods in statistics and machine learning. Feature selection is especiallydifficult and computationally intensive when the number of variables approachesor exceeds the number of samples, as is often the case for many genomicdatasets. Here, we introduce a new approach -- the Bayesian Ising Approximation(BIA) -- to rapidly calculate posterior probabilities for feature relevance inL2 penalized linear regression. In the regime where the regression problem isstrongly regularized by the prior, we show that computing the marginalposterior probabilities for features is equivalent to computing themagnetizations of an Ising model. Using a mean field approximation, we show itis possible to rapidly compute the feature selection path described by theposterior probabilities as a function of the L2 penalty. We present simulationsand analytical results illustrating the accuracy of the BIA on some simpleregression problems. Finally, we demonstrate the applicability of the BIA tohigh dimensional regression by analyzing a gene expression dataset with nearly30,000 features.
arxiv-6900-86 | Merging and Shifting of Images with Prominence Coefficient for Predictive Analysis using Combined Image | http://arxiv.org/pdf/1407.8123v1.pdf | author:T. R. Gopalakrishnan Nair, Richa Sharma category:cs.CV published:2014-07-30 summary:Shifting of objects in an image and merging many images after appropriateshifting is being used in several engineering and scientific applications whichrequire complex perception development. A method has been presented here whichcould be used in precision engineering and biological applications where moreprecise prediction is required of a combined phenomenon with varying prominenceof each phenomenon. Accurate merging of intended pixels can be achieved in highquality using frequency domain techniques even though initial properties of theoriginal pixels are lost in this process. This paper introduces a technique toshift and merge various images with varying prominence of each image. Acoefficient named prominence coefficient has been introduced which is capableof making some of the images transparent and highlighting the rest as perrequirement of merging process which can be used as a simple but effectivetechnique for overlapped view of a set of images.
arxiv-6900-87 | Clustering Approach Towards Image Segmentation: An Analytical Study | http://arxiv.org/pdf/1407.8121v1.pdf | author:Dibya Jyoti Bora, Anil Kumar Gupta category:cs.CV published:2014-07-30 summary:Image processing is an important research area in computer vision. Imagesegmentation plays the vital rule in image processing research. There exist somany methods for image segmentation. Clustering is an unsupervised study.Clustering can also be used for image segmentation. In this paper, an in-depthstudy is done on different clustering techniques that can be used for imagesegmentation with their pros and cons. An experiment for color imagesegmentation based on clustering with K-Means algorithm is performed to observethe accuracy of clustering technique for the segmentation purpose.
arxiv-6900-88 | The Grow-Shrink strategy for learning Markov network structures constrained by context-specific independences | http://arxiv.org/pdf/1407.8088v1.pdf | author:Alejandro Edera, Yanela Strappa, Facundo Bromberg category:cs.LG cs.DS published:2014-07-30 summary:Markov networks are models for compactly representing complex probabilitydistributions. They are composed by a structure and a set of numerical weights.The structure qualitatively describes independences in the distribution, whichcan be exploited to factorize the distribution into a set of compact functions.A key application for learning structures from data is to automaticallydiscover knowledge. In practice, structure learning algorithms focused on"knowledge discovery" present a limitation: they use a coarse-grainedrepresentation of the structure. As a result, this representation cannotdescribe context-specific independences. Very recently, an algorithm calledCSPC was designed to overcome this limitation, but it has a high computationalcomplexity. This work tries to mitigate this downside presenting CSGS, analgorithm that uses the Grow-Shrink strategy for reducing unnecessarycomputations. On an empirical evaluation, the structures learned by CSGSachieve competitive accuracies and lower computational complexity with respectto those obtained by CSPC.
arxiv-6900-89 | Differentially-Private Logistic Regression for Detecting Multiple-SNP Association in GWAS Databases | http://arxiv.org/pdf/1407.8067v1.pdf | author:Fei Yu, Michal Rybar, Caroline Uhler, Stephen E. Fienberg category:stat.ML cs.LG stat.AP 62P10 published:2014-07-30 summary:Following the publication of an attack on genome-wide association studies(GWAS) data proposed by Homer et al., considerable attention has been given todeveloping methods for releasing GWAS data in a privacy-preserving way. Here,we develop an end-to-end differentially private method for solving regressionproblems with convex penalty functions and selecting the penalty parameters bycross-validation. In particular, we focus on penalized logistic regression withelastic-net regularization, a method widely used to in GWAS analyses toidentify disease-causing genes. We show how a differentially private procedurefor penalized logistic regression with elastic-net regularization can beapplied to the analysis of GWAS data and evaluate our method's performance.
arxiv-6900-90 | Targeting Optimal Active Learning via Example Quality | http://arxiv.org/pdf/1407.8042v1.pdf | author:Lewis P. G. Evans, Niall M. Adams, Christoforos Anagnostopoulos category:stat.ML cs.LG published:2014-07-30 summary:In many classification problems unlabelled data is abundant and a subset canbe chosen for labelling. This defines the context of active learning (AL),where methods systematically select that subset, to improve a classifier byretraining. Given a classification problem, and a classifier trained on a smallnumber of labelled examples, consider the selection of a single furtherexample. This example will be labelled by the oracle and then used to retrainthe classifier. This example selection raises a central question: given a fullyspecified stochastic description of the classification problem, which exampleis the optimal selection? If optimality is defined in terms of loss, thisdefinition directly produces expected loss reduction (ELR), a central quantitywhose maximum yields the optimal example selection. This work presents a newtheoretical approach to AL, example quality, which defines optimal AL behaviourin terms of ELR. Once optimal AL behaviour is defined mathematically, reasoningabout this abstraction provides insights into AL. In a theoretical context theoptimal selection is compared to existing AL methods, showing that heuristicscan make sub-optimal selections. Algorithms are constructed to estimate examplequality directly. A large-scale experimental study shows these algorithms to becompetitive with standard AL methods.
arxiv-6900-91 | Associative embeddings for large-scale knowledge transfer with self-assessment | http://arxiv.org/pdf/1312.3240v2.pdf | author:Alexander Vezhnevets, Vittorio Ferrari category:cs.CV published:2013-12-11 summary:We propose a method for knowledge transfer between semantically relatedclasses in ImageNet. By transferring knowledge from the images that havebounding-box annotations to the others, our method is capable of automaticallypopulating ImageNet with many more bounding-boxes and even pixel-levelsegmentations. The underlying assumption that objects from semantically relatedclasses look alike is formalized in our novel Associative Embedding (AE)representation. AE recovers the latent low-dimensional space of appearancevariations among image windows. The dimensions of AE space tend to correspondto aspects of window appearance (e.g. side view, close up, background). Wemodel the overlap of a window with an object using Gaussian Processes (GP)regression, which spreads annotation smoothly through AE space. Theprobabilistic nature of GP allows our method to perform self-assessment, i.e.assigning a quality estimate to its own output. It enables trading off theamount of returned annotations for their quality. A large scale experiment on219 classes and 0.5 million images demonstrates that our method outperformsstate-of-the-art methods and baselines for both object localization andsegmentation. Using self-assessment we can automatically return bounding-boxannotations for 30% of all images with high localization accuracy (i.e.~73%average overlap with ground-truth).
arxiv-6900-92 | Automated Machine Learning on Big Data using Stochastic Algorithm Tuning | http://arxiv.org/pdf/1407.7969v1.pdf | author:Thomas Nickson, Michael A Osborne, Steven Reece, Stephen J Roberts category:stat.ML published:2014-07-30 summary:We introduce a means of automating machine learning (ML) for big data tasks,by performing scalable stochastic Bayesian optimisation of ML algorithmparameters and hyper-parameters. More often than not, the critical tuning of MLalgorithm parameters has relied on domain expertise from experts, along withlaborious hand-tuning, brute search or lengthy sampling runs. Against thisbackground, Bayesian optimisation is finding increasing use in automatingparameter tuning, making ML algorithms accessible even to non-experts. However,the state of the art in Bayesian optimisation is incapable of scaling to thelarge number of evaluations of algorithm performance required to fit realisticmodels to complex, big data. We here describe a stochastic, sparse, Bayesianoptimisation strategy to solve this problem, using many thousands of noisyevaluations of algorithm performance on subsets of data in order to effectivelytrain algorithms for big data. We provide a comprehensive benchmarking ofpossible sparsification strategies for Bayesian optimisation, concluding that aNystrom approximation offers the best scaling and performance for real tasks.Our proposed algorithm demonstrates substantial improvement over the state ofthe art in tuning the parameters of a Gaussian Process time series predictiontask on real, big data.
arxiv-6900-93 | Accurate merging of images for predictive analysis using combined image | http://arxiv.org/pdf/1407.8176v1.pdf | author:T. R. Gopalakrishnan Nair, Richa Sharma category:cs.CV published:2014-07-30 summary:Several Scientific and engineering applications require merging of sampledimages for complex perception development. In most cases, for suchrequirements, images are merged at intensity level. Even though it gives fairlygood perception of combined scenario of objects and scenes, it is found thatthey are not sufficient enough to analyze certain engineering cases. The mainproblem is incoherent modulation of intensity arising out of phase propertiesbeing lost. In order to compensate these losses, combined phase and amplitudemerge is demanded. We present here a method which could be used in precisionengineering and biological applications where more precise prediction isrequired of a combined phenomenon. When pixels are added, its original propertyis lost but accurate merging of intended pixels can be achieved in high qualityusing frequency domain properties of an image. This paper introduces atechnique to merge various images which can be used as a simple but effectivetechnique for overlapped view of a set of images and producing reduced datasetfor review purposes.
arxiv-6900-94 | Learning Economic Parameters from Revealed Preferences | http://arxiv.org/pdf/1407.7937v1.pdf | author:Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, Vijay V. Vazirani category:cs.GT cs.LG published:2014-07-30 summary:A recent line of work, starting with Beigman and Vohra (2006) andZadimoghaddam and Roth (2012), has addressed the problem of {\em learning} autility function from revealed preference data. The goal here is to make use ofpast data describing the purchases of a utility maximizing agent when facedwith certain prices and budget constraints in order to produce a hypothesisfunction that can accurately forecast the {\em future} behavior of the agent. In this work we advance this line of work by providing sample complexityguarantees and efficient algorithms for a number of important classes. Bydrawing a connection to recent advances in multi-class learning, we provide acomputationally efficient algorithm with tight sample complexity guarantees($\Theta(d/\epsilon)$ for the case of $d$ goods) for learning linear utilityfunctions under a linear price model. This solves an open question inZadimoghaddam and Roth (2012). Our technique yields numerous generalizationsincluding the ability to learn other well-studied classes of utility functions,to deal with a misspecified model, and with non-linear prices.
arxiv-6900-95 | Sequential Design for Optimal Stopping Problems | http://arxiv.org/pdf/1309.3832v2.pdf | author:Robert B. Gramacy, Mike Ludkovski category:q-fin.CP q-fin.PR stat.ML published:2013-09-16 summary:We propose a new approach to solve optimal stopping problems via simulation.Working within the backward dynamic programming/Snell envelope framework, weaugment the methodology of Longstaff-Schwartz that focuses on approximating thestopping strategy. Namely, we introduce adaptive generation of the stochasticgrids anchoring the simulated sample paths of the underlying state process.This allows for active learning of the classifiers partitioning the state spaceinto the continuation and stopping regions. To this end, we examine sequentialdesign schemes that adaptively place new design points close to the stoppingboundaries. We then discuss dynamic regression algorithms that can implementsuch recursive estimation and local refinement of the classifiers. The newalgorithm is illustrated with a variety of numerical experiments, showing thatan order of magnitude savings in terms of design size can be achieved. We alsocompare with existing benchmarks in the context of pricing multi-dimensionalBermudan options.
arxiv-6900-96 | Bayesian Probabilistic Matrix Factorization: A User Frequency Analysis | http://arxiv.org/pdf/1407.7840v1.pdf | author:Cody Severinski, Ruslan Salakhutdinov category:stat.ML published:2014-07-29 summary:Matrix factorization (MF) has become a common approach to collaborativefiltering, due to ease of implementation and scalability to large data sets.Two existing drawbacks of the basic model is that it does not incorporate sideinformation on either users or items, and assumes a common variance for allusers. We extend the work of constrained probabilistic matrix factorization byderiving the Gibbs updates for the side feature vectors for items(Salakhutdinov and Minh, 2008). We show that this Bayesian treatment to theconstrained PMF model outperforms simple MAP estimation. We also considerextensions to heteroskedastic precision introduced in the literature(Lakshminarayanan, Bouchard, and Archambeau, 2011). We show that this tendsresult in overfitting for deterministic approximation algorithms (ex:Variational inference) when the observed entries in the user / item matrix aredistributed in an non-uniform manner. In light of this, we propose a truncatedprecision model. Our experimental results suggest that this model tends todelay overfitting.
arxiv-6900-97 | Sure Screening for Gaussian Graphical Models | http://arxiv.org/pdf/1407.7819v1.pdf | author:Shikai Luo, Rui Song, Daniela Witten category:stat.ML cs.LG published:2014-07-29 summary:We propose {graphical sure screening}, or GRASS, a very simple andcomputationally-efficient screening procedure for recovering the structure of aGaussian graphical model in the high-dimensional setting. The GRASS estimate ofthe conditional dependence graph is obtained by thresholding the elements ofthe sample covariance matrix. The proposed approach possesses the surescreening property: with very high probability, the GRASS estimated edge setcontains the true edge set. Furthermore, with high probability, the size of theestimated edge set is controlled. We provide a choice of threshold for GRASSthat can control the expected false positive rate. We illustrate theperformance of GRASS in a simulation study and on a gene expression data set,and show that in practice it performs quite competitively with more complex andcomputationally-demanding techniques for graph estimation.
arxiv-6900-98 | The Power of Online Learning in Stochastic Network Optimization | http://arxiv.org/pdf/1404.1592v2.pdf | author:Longbo Huang, Xin Liu, Xiaohong Hao category:math.OC cs.LG cs.SY published:2014-04-06 summary:In this paper, we investigate the power of online learning in stochasticnetwork optimization with unknown system statistics {\it a priori}. We areinterested in understanding how information and learning can be efficientlyincorporated into system control techniques, and what are the fundamentalbenefits of doing so. We propose two \emph{Online Learning-Aided Control}techniques, $\mathtt{OLAC}$ and $\mathtt{OLAC2}$, that explicitly utilize thepast system information in current system control via a learning procedurecalled \emph{dual learning}. We prove strong performance guarantees of theproposed algorithms: $\mathtt{OLAC}$ and $\mathtt{OLAC2}$ achieve thenear-optimal $[O(\epsilon), O([\log(1/\epsilon)]^2)]$ utility-delay tradeoffand $\mathtt{OLAC2}$ possesses an $O(\epsilon^{-2/3})$ convergence time.$\mathtt{OLAC}$ and $\mathtt{OLAC2}$ are probably the first algorithms thatsimultaneously possess explicit near-optimal delay guarantee and sub-linearconvergence time. Simulation results also confirm the superior performance ofthe proposed algorithms in practice. To the best of our knowledge, our attemptis the first to explicitly incorporate online learning into stochastic networkoptimization and to demonstrate its power in both theory and practice.
arxiv-6900-99 | A Hash-based Co-Clustering Algorithm for Categorical Data | http://arxiv.org/pdf/1407.7753v1.pdf | author:Fabricio Olivetti de França category:cs.LG published:2014-07-29 summary:Many real-life data are described by categorical attributes without apre-classification. A common data mining method used to extract informationfrom this type of data is clustering. This method group together the samplesfrom the data that are more similar than all other samples. But, categoricaldata pose a challenge when extracting information because: the calculation oftwo objects similarity is usually done by measuring the number of commonfeatures, but ignore a possible importance weighting; if the data may bedivided differently according to different subsets of the features, thealgorithm may find clusters with different meanings from each other,difficulting the post analysis. Data Co-Clustering of categorical data is thetechnique that tries to find subsets of samples that share a subset of featuresin common. By doing so, not only a sample may belong to more than one clusterbut, the feature selection of each cluster describe its own characteristics. Inthis paper a novel Co-Clustering technique for categorical data is proposed byusing Locality Sensitive Hashing technique in order to preprocess a list ofCo-Clusters seeds based on a previous research. Results indicate this techniqueis capable of finding high quality Co-Clusters in many different categoricaldata sets and scales linearly with the data set size.
arxiv-6900-100 | A CUDA-Based Real Parameter Optimization Benchmark | http://arxiv.org/pdf/1407.7737v1.pdf | author:Ke Ding, Ying Tan category:cs.NE published:2014-07-29 summary:Benchmarking is key for developing and comparing optimization algorithms. Inthis paper, a CUDA-based real parameter optimization benchmark (cuROB) isintroduced. Test functions of diverse properties are included within cuROB andimplemented efficiently with CUDA. Speedup of one order of magnitude can beachieved in comparison with CPU-based benchmark of CEC'14.
arxiv-6900-101 | A Latent Space Analysis of Editor Lifecycles in Wikipedia | http://arxiv.org/pdf/1407.7736v1.pdf | author:Xiangju Qin, Derek Greene, Pádraig Cunningham category:cs.SI cs.CL cs.CY physics.soc-ph published:2014-07-29 summary:Collaborations such as Wikipedia are a key part of the value of the modernInternet. At the same time there is concern that these collaborations arethreatened by high levels of member turnover. In this paper we borrow ideasfrom topic analysis to editor activity on Wikipedia over time into a latentspace that offers an insight into the evolving patterns of editor behavior.This latent space representation reveals a number of different categories ofeditor (e.g. content experts, social networkers) and we show that it doesprovide a signal that predicts an editor's departure from the community. Wealso show that long term editors gradually diversify their participation byshifting edit preference from one or two namespaces to multiple namespaces andexperience relatively soft evolution in their editor profiles, while short termeditors generally distribute their contribution randomly among the namespacesand experience considerably fluctuated evolution in their editor profiles.
arxiv-6900-102 | NMF with Sparse Regularizations in Transformed Domains | http://arxiv.org/pdf/1407.7691v1.pdf | author:Jérémy Rapin, Jérôme Bobin, Anthony Larue, Jean-Luc Starck category:stat.ML cs.LG 94A12 I.5.4 published:2014-07-29 summary:Non-negative blind source separation (non-negative BSS), which is alsoreferred to as non-negative matrix factorization (NMF), is a very active fieldin domains as different as astrophysics, audio processing or biomedical signalprocessing. In this context, the efficient retrieval of the sources requiresthe use of signal priors such as sparsity. If NMF has now been well studiedwith sparse constraints in the direct domain, only very few algorithms canencompass non-negativity together with sparsity in a transformed domain sincesimultaneously dealing with two priors in two different domains is challenging.In this article, we show how a sparse NMF algorithm coined non-negativegeneralized morphological component analysis (nGMCA) can be extended to imposenon-negativity in the direct domain along with sparsity in a transformeddomain, with both analysis and synthesis formulations. To our knowledge, thiswork presents the first comparison of analysis and synthesis priors ---as wellas their reweighted versions--- in the context of blind source separation.Comparisons with state-of-the-art NMF algorithms on realistic data show theefficiency as well as the robustness of the proposed algorithms.
arxiv-6900-103 | Hyperspectral Imaging and Analysis for Sparse Reconstruction and Recognition | http://arxiv.org/pdf/1407.7686v1.pdf | author:Zohaib Khan category:cs.CV published:2014-07-29 summary:This thesis proposes spatio-spectral techniques for hyperspectral imageanalysis. Adaptive spatio-spectral support and variable exposure hyperspectralimaging is demonstrated to improve spectral reflectance recovery fromhyperspectral images. Novel spectral dimensionality reduction techniques havebeen proposed from the perspective of spectral only and spatio-spectralinformation preservation. It was found that the joint sparse and joint groupsparse hyperspectral image models achieve lower reconstruction error and higherrecognition accuracy using only a small subset of bands. Hyperspectral imagedatabases have been developed and made publicly available for further researchin compressed hyperspectral imaging, forensic document analysis and spectralreflectance recovery.
arxiv-6900-104 | Level-based Analysis of Genetic Algorithms and other Search Processes | http://arxiv.org/pdf/1407.7663v1.pdf | author:Dogan Corus, Duc-Cuong Dang, Anton V. Eremeev, Per Kristian Lehre category:cs.NE q-bio.PE published:2014-07-29 summary:The fitness-level technique is a simple and old way to derive upper boundsfor the expected runtime of simple elitist evolutionary algorithms (EAs).Recently, the technique has been adapted to deduce the runtime of algorithmswith non-elitist populations and unary variation operators. In this paper, weshow that the restriction to unary variation operators can be removed. This gives rise to a much more general analytical tool which is applicable toa wide range of search processes. As introductory examples, we provide simpleruntime analyses of many variants of the Genetic Algorithm on well-knownbenchmark functions, such as OneMax, LeadingOnes, and the sorting problem.
arxiv-6900-105 | Chasing Ghosts: Competing with Stateful Policies | http://arxiv.org/pdf/1407.7635v1.pdf | author:Uriel Feige, Tomer Koren, Moshe Tennenholtz category:cs.LG published:2014-07-29 summary:We consider sequential decision making in a setting where regret is measuredwith respect to a set of stateful reference policies, and feedback is limitedto observing the rewards of the actions performed (the so called "bandit"setting). If either the reference policies are stateless rather than stateful,or the feedback includes the rewards of all actions (the so called "expert"setting), previous work shows that the optimal regret grows like$\Theta(\sqrt{T})$ in terms of the number of decision rounds $T$. The difficulty in our setting is that the decision maker unavoidably losestrack of the internal states of the reference policies, and thus cannotreliably attribute rewards observed in a certain round to any of the referencepolicies. In fact, in this setting it is impossible for the algorithm toestimate which policy gives the highest (or even approximately highest) totalreward. Nevertheless, we design an algorithm that achieves expected regret thatis sublinear in $T$, of the form $O( T/\log^{1/4}{T})$. Our algorithm is basedon a certain local repetition lemma that may be of independent interest. Wealso show that no algorithm can guarantee expected regret better than $O(T/\log^{3/2} T)$.
arxiv-6900-106 | A Survey on Two Dimensional Cellular Automata and Its Application in Image Processing | http://arxiv.org/pdf/1407.7626v1.pdf | author:Deepak Ranjan Nayak, Prashanta Kumar Patra, Amitav Mahapatra category:cs.CV published:2014-07-29 summary:Parallel algorithms for solving any image processing task is a highlydemanded approach in the modern world. Cellular Automata (CA) are the mostcommon and simple models of parallel computation. So, CA has been successfullyused in the domain of image processing for the last couple of years. This paperprovides a survey of available literatures of some methodologies employed bydifferent researchers to utilize the cellular automata for solving someimportant problems of image processing. The survey includes some importantimage processing tasks such as rotation, zooming, translation, segmentation,edge detection, compression and noise reduction of images. Finally, theexperimental results of some methodologies are presented.
arxiv-6900-107 | Learning Sparsely Used Overcomplete Dictionaries via Alternating Minimization | http://arxiv.org/pdf/1310.7991v2.pdf | author:Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli category:cs.LG math.OC stat.ML published:2013-10-30 summary:We consider the problem of sparse coding, where each sample consists of asparse linear combination of a set of dictionary atoms, and the task is tolearn both the dictionary elements and the mixing coefficients. Alternatingminimization is a popular heuristic for sparse coding, where the dictionary andthe coefficients are estimated in alternate steps, keeping the other fixed.Typically, the coefficients are estimated via $\ell_1$ minimization, keepingthe dictionary fixed, and the dictionary is estimated through least squares,keeping the coefficients fixed. In this paper, we establish local linearconvergence for this variant of alternating minimization and establish that thebasin of attraction for the global optimum (corresponding to the truedictionary and the coefficients) is $\order{1/s^2}$, where $s$ is the sparsitylevel in each sample and the dictionary satisfies RIP. Combined with the recentresults of approximate dictionary estimation, this yields provable guaranteesfor exact recovery of both the dictionary elements and the coefficients, whenthe dictionary elements are incoherent.
arxiv-6900-108 | Dynamic Feature Scaling for Online Learning of Binary Classifiers | http://arxiv.org/pdf/1407.7584v1.pdf | author:Danushka Bollegala category:cs.LG stat.ML published:2014-07-28 summary:Scaling feature values is an important step in numerous machine learningtasks. Different features can have different value ranges and some form of afeature scaling is often required in order to learn an accurate classifier.However, feature scaling is conducted as a preprocessing task prior tolearning. This is problematic in an online setting because of two reasons.First, it might not be possible to accurately determine the value range of afeature at the initial stages of learning when we have observed only a fewnumber of training instances. Second, the distribution of data can change overthe time, which render obsolete any feature scaling that we perform in apre-processing step. We propose a simple but an effective method to dynamicallyscale features at train time, thereby quickly adapting to any changes in thedata stream. We compare the proposed dynamic feature scaling method againstmore complex methods for estimating scaling parameters using several benchmarkdatasets for binary classification. Our proposed feature scaling methodconsistently outperforms more complex methods on all of the benchmark datasetsand improves classification accuracy of a state-of-the-art online binaryclassifier algorithm.
arxiv-6900-109 | Dependence versus Conditional Dependence in Local Causal Discovery from Gene Expression Data | http://arxiv.org/pdf/1407.7566v1.pdf | author:Eric V. Strobl, Shyam Visweswaran category:q-bio.QM cs.LG stat.ML published:2014-07-28 summary:Motivation: Algorithms that discover variables which are causally related toa target may inform the design of experiments. With observational geneexpression data, many methods discover causal variables by measuring eachvariable's degree of statistical dependence with the target using dependencemeasures (DMs). However, other methods measure each variable's ability toexplain the statistical dependence between the target and the remainingvariables in the data using conditional dependence measures (CDMs), since thisstrategy is guaranteed to find the target's direct causes, direct effects, anddirect causes of the direct effects in the infinite sample limit. In thispaper, we design a new algorithm in order to systematically compare therelative abilities of DMs and CDMs in discovering causal variables from geneexpression data. Results: The proposed algorithm using a CDM is sample efficient, since itconsistently outperforms other state-of-the-art local causal discoveryalgorithms when samples sizes are small. However, the proposed algorithm usinga CDM outperforms the proposed algorithm using a DM only when sample sizes areabove several hundred. These results suggest that accurate causal discoveryfrom gene expression data using current CDM-based algorithms requires datasetswith at least several hundred samples. Availability: The proposed algorithm is freely available athttps://github.com/ericstrobl/DvCD.
arxiv-6900-110 | Efficient Regularized Regression for Variable Selection with L0 Penalty | http://arxiv.org/pdf/1407.7508v1.pdf | author:Zhenqiu Liu, Gang Li category:cs.LG stat.ML published:2014-07-28 summary:Variable (feature, gene, model, which we use interchangeably) selections forregression with high-dimensional BIGDATA have found many applications inbioinformatics, computational biology, image processing, and engineering. Oneappealing approach is the L0 regularized regression which penalizes the numberof nonzero features in the model directly. L0 is known as the most essentialsparsity measure and has nice theoretical properties, while the popular L1regularization is only a best convex relaxation of L0. Therefore, it is naturalto expect that L0 regularized regression performs better than LASSO. However,it is well-known that L0 optimization is NP-hard and computationallychallenging. Instead of solving the L0 problems directly, most publications sofar have tried to solve an approximation problem that closely resembles L0regularization. In this paper, we propose an efficient EM algorithm (L0EM) that directlysolves the L0 optimization problem. $L_0$EM is efficient with high dimensionaldata. It also provides a natural solution to all Lp p in [0,2] problems. Theregularized parameter can be either determined through cross-validation or AICand BIC. Theoretical properties of the L0-regularized estimator are given undermild conditions that permit the number of variables to be much larger than thesample size. We demonstrate our methods through simulation and high-dimensionalgenomic data. The results indicate that L0 has better performance than LASSOand L0 with AIC or BIC has similar performance as computationally intensivecross-validation. The proposed algorithms are efficient in identifying thenon-zero variables with less-bias and selecting biologically important genesand pathways with high dimensional BIGDATA.
arxiv-6900-111 | A Fast Hierarchical Method for Multi-script and Arbitrary Oriented Scene Text Extraction | http://arxiv.org/pdf/1407.7504v1.pdf | author:Lluis Gomez, Dimosthenis Karatzas category:cs.CV published:2014-07-28 summary:Typography and layout lead to the hierarchical organisation of text in words,text lines, paragraphs. This inherent structure is a key property of text inany script and language, which has nonetheless been minimally leveraged byexisting text detection methods. This paper addresses the problem of textsegmentation in natural scenes from a hierarchical perspective. Contrary toexisting methods, we make explicit use of text structure, aiming directly tothe detection of region groupings corresponding to text within a hierarchyproduced by an agglomerative similarity clustering process over individualregions. We propose an optimal way to construct such an hierarchy introducing afeature space designed to produce text group hypotheses with high recall and anovel stopping rule combining a discriminative classifier and a probabilisticmeasure of group meaningfulness based in perceptual organization. Resultsobtained over four standard datasets, covering text in variable orientationsand different languages, demonstrate that our algorithm, while being trained ina single mixed dataset, outperforms state of the art methods in unconstrainedscenarios.
arxiv-6900-112 | Fine-grained Activity Recognition with Holistic and Pose based Features | http://arxiv.org/pdf/1406.1881v2.pdf | author:Leonid Pishchulin, Mykhaylo Andriluka, Bernt Schiele category:cs.CV published:2014-06-07 summary:Holistic methods based on dense trajectories are currently the de factostandard for recognition of human activities in video. Whether holisticrepresentations will sustain or will be superseded by higher level videoencoding in terms of body pose and motion is the subject of an ongoing debate.In this paper we aim to clarify the underlying factors responsible for goodperformance of holistic and pose-based representations. To that end we build onour recent dataset leveraging the existing taxonomy of human activities. Thisdataset includes 24,920 video snippets covering 410 human activities in total.Our analysis reveals that holistic and pose-based methods are highlycomplementary, and their performance varies significantly depending on theactivity. We find that holistic methods are mostly affected by the number andspeed of trajectories, whereas pose-based methods are mostly influenced byviewpoint of the person. We observe striking performance differences acrossactivities: for certain activities results with pose-based features are morethan twice as accurate compared to holistic features, and vice versa. The bestperforming approach in our comparison is based on the combination of holisticand pose-based approaches, which again underlines their complementarity.
arxiv-6900-113 | Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks | http://arxiv.org/pdf/1404.5772v3.pdf | author:Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin Wang, Tie-Yan Liu category:cs.IR cs.LG cs.NE published:2014-04-23 summary:Click prediction is one of the fundamental problems in sponsored search. Mostof existing studies took advantage of machine learning approaches to predict adclick for each event of ad view independently. However, as observed in thereal-world sponsored search system, user's behaviors on ads yield highdependency on how the user behaved along with the past time, especially interms of what queries she submitted, what ads she clicked or ignored, and howlong she spent on the landing pages of clicked ads, etc. Inspired by theseobservations, we introduce a novel framework based on Recurrent Neural Networks(RNN). Compared to traditional methods, this framework directly models thedependency on user's sequential behaviors into the click prediction processthrough the recurrent structure in RNN. Large scale evaluations on theclick-through logs from a commercial search engine demonstrate that ourapproach can significantly improve the click prediction accuracy, compared tosequence-independent approaches.
arxiv-6900-114 | 'Almost Sure' Chaotic Properties of Machine Learning Methods | http://arxiv.org/pdf/1407.7417v1.pdf | author:Nabarun Mondal, Partha P. Ghosh category:cs.LG cs.AI published:2014-07-28 summary:It has been demonstrated earlier that universal computation is 'almostsurely' chaotic. Machine learning is a form of computational fixed pointiteration, iterating over the computable function space. We showcase someproperties of this iteration, and establish in general that the iteration is'almost surely' of chaotic nature. This theory explains the observation in thecounter intuitive properties of deep learning methods. This paper demonstratesthat these properties are going to be universal to any learning method.
arxiv-6900-115 | kLog: A Language for Logical and Relational Learning with Kernels | http://arxiv.org/pdf/1205.3981v5.pdf | author:Paolo Frasconi, Fabrizio Costa, Luc De Raedt, Kurt De Grave category:cs.AI cs.LG cs.PL published:2012-05-17 summary:We introduce kLog, a novel approach to statistical relational learning.Unlike standard approaches, kLog does not represent a probability distributiondirectly. It is rather a language to perform kernel-based learning onexpressive logical and relational representations. kLog allows users to specifylearning problems declaratively. It builds on simple but powerful concepts:learning from interpretations, entity/relationship data modeling, logicprogramming, and deductive databases. Access by the kernel to the richrepresentation is mediated by a technique we call graphicalization: therelational representation is first transformed into a graph --- in particular,a grounded entity/relationship diagram. Subsequently, a choice of graph kerneldefines the feature space. kLog supports mixed numerical and symbolic data, aswell as background knowledge in the form of Prolog or Datalog programs as ininductive logic programming systems. The kLog framework can be applied totackle the same range of tasks that has made statistical relational learning sopopular, including classification, regression, multitask learning, andcollective classification. We also report about empirical comparisons, showingthat kLog can be either more accurate, or much faster at the same level ofaccuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available athttp://klog.dinfo.unifi.it along with tutorials.
arxiv-6900-116 | A Numerical Optimization Algorithm Inspired by the Strawberry Plant | http://arxiv.org/pdf/1407.7399v1.pdf | author:F. Merrikh-Bayat category:cs.NE published:2014-07-28 summary:This paper proposes a new numerical optimization algorithm inspired by thestrawberry plant for solving complicated engineering problems. Plants likestrawberry develop both runners and roots for propagation and search for waterresources and minerals. In these plants, runners and roots can be thought of astools for global and local searches, respectively. The proposed algorithm hasthree main differences with the trivial nature-inspired optimizationalgorithms: duplication-elimination of the computational agents at alliterations, subjecting all agents to both small and large movements from thebeginning to end, and the lack of communication (information exchange) betweenagents. Moreover, it has the advantage of using only three parameters to betuned by user. This algorithm is applied to standard test functions and theresults are compared with GA and PSO. The proposed algorithm is also used tosolve an open problem in the field of robust control theory. These simulationsshow that the proposed algorithm can very effectively solve complicatedoptimization problems.
arxiv-6900-117 | Non-parametric Image Registration of Airborne LiDAR, Hyperspectral and Photographic Imagery of Forests | http://arxiv.org/pdf/1410.0226v1.pdf | author:Juheon Lee, Xiaohao Cai, Carola-Bibiane Schonlieb, David Coomes category:cs.CV I.4.8 published:2014-07-28 summary:There is much current interest in using multi-sensor airborne remote sensingto monitor the structure and biodiversity of forests. This paper addresses theapplication of non-parametric image registration techniques to precisely alignimages obtained from multimodal imaging, which is critical for the successfulidentification of individual trees using object recognition approaches.Non-parametric image registration, in particular the technique of optimizingone objective function containing data fidelity and regularization terms,provides flexible algorithms for image registration. Using a survey ofwoodlands in southern Spain as an example, we show that non-parametric imageregistration can be successful at fusing datasets when there is little priorknowledge about how the datasets are interrelated (i.e. in the absence ofground control points). The validity of non-parametric registration methods inairborne remote sensing is demonstrated by a series of experiments. Precisedata fusion is a prerequisite to accurate recognition of objects withinairborne imagery, so non-parametric image registration could make a valuablecontribution to the analysis pipeline.
arxiv-6900-118 | Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression | http://arxiv.org/pdf/1206.6679v6.pdf | author:Tim Salimans, David A. Knowles category:stat.CO cs.CV stat.ML 62F15 published:2012-06-28 summary:We propose a general algorithm for approximating nonstandard Bayesianposterior distributions. The algorithm minimizes the Kullback-Leiblerdivergence of an approximating distribution to the intractable posteriordistribution. Our method can be used to approximate any posterior distribution,provided that it is given in closed form up to the proportionality constant.The approximation can be any distribution in the exponential family or anymixture of such distributions, which means that it can be made arbitrarilyprecise. Several examples illustrate the speed and accuracy of ourapproximation method in practice.
arxiv-6900-119 | Classification of Human Ventricular Arrhythmia in High Dimensional Representation Spaces | http://arxiv.org/pdf/1312.5354v2.pdf | author:Yaqub Alwan, Zoran Cvetkovic, Michael Curtis category:cs.CE cs.LG published:2013-12-18 summary:We studied classification of human ECGs labelled as normal sinus rhythm,ventricular fibrillation and ventricular tachycardia by means of support vectormachines in different representation spaces, using different observationlengths. ECG waveform segments of duration 0.5-4 s, their Fourier magnitudespectra, and lower dimensional projections of Fourier magnitude spectra wereused for classification. All considered representations were of much higherdimension than in published studies. Classification accuracy improved withsegment duration up to 2 s, with 4 s providing little improvement. We foundthat it is possible to discriminate between ventricular tachycardia andventricular fibrillation by the present approach with much shorter runs of ECG(2 s, minimum 86% sensitivity per class) than previously imagined. Ensembles ofclassifiers acting on 1 s segments taken over 5 s observation windows gave bestresults, with sensitivities of detection for all classes exceeding 93%.
arxiv-6900-120 | Beyond KernelBoost | http://arxiv.org/pdf/1407.8518v1.pdf | author:Roberto Rigamonti, Vincent Lepetit, Pascal Fua category:cs.CV cs.LG published:2014-07-28 summary:In this Technical Report we propose a set of improvements with respect to theKernelBoost classifier presented in [Becker et al., MICCAI 2013]. We start witha scheme inspired by Auto-Context, but that is suitable in situations where thelack of large training sets poses a potential problem of overfitting. The aimis to capture the interactions between neighboring image pixels to betterregularize the boundaries of segmented regions. As in Auto-Context [Tu et al.,PAMI 2009] the segmentation process is iterative and, at each iteration, thesegmentation results for the previous iterations are taken into account inconjunction with the image itself. However, unlike in [Tu et al., PAMI 2009],we organize our recursion so that the classifiers can progressively focus ondifficult-to-classify locations. This lets us exploit the power of thedecision-tree paradigm while avoiding over-fitting. In the context of thisarchitecture, KernelBoost represents a powerful building block due to itsability to learn on the score maps coming from previous iterations. We firstintroduce two important mechanisms to empower the KernelBoost classifier,namely pooling and the clustering of positive samples based on the appearanceof the corresponding ground-truth. These operations significantly contribute toincrease the effectiveness of the system on biomedical images, where textureplays a major role in the recognition of the different image components. Wethen present some other techniques that can be easily integrated in theKernelBoost framework to further improve the accuracy of the finalsegmentation. We show extensive results on different medical image datasets,including some multi-label tasks, on which our method is shown to outperformstate-of-the-art approaches. The resulting segmentations display high accuracy,neat contours, and reduced noise.
arxiv-6900-121 | Text Classification Using Association Rules, Dependency Pruning and Hyperonymization | http://arxiv.org/pdf/1407.7357v1.pdf | author:Yannis Haralambous, Philippe Lenca category:cs.IR cs.CL published:2014-07-28 summary:We present new methods for pruning and enhancing item- sets for textclassification via association rule mining. Pruning methods are based ondependency syntax and enhancing methods are based on replacing words by theirhyperonyms of various orders. We discuss the impact of these methods, comparedto pruning based on tfidf rank of words.
arxiv-6900-122 | Deep Networks with Internal Selective Attention through Feedback Connections | http://arxiv.org/pdf/1407.3068v2.pdf | author:Marijn Stollenga, Jonathan Masci, Faustino Gomez, Juergen Schmidhuber category:cs.CV cs.LG cs.NE 68T45 published:2014-07-11 summary:Traditional convolutional neural networks (CNN) are stationary andfeedforward. They neither change their parameters during evaluation nor usefeedback from higher to lower layers. Real brains, however, do. So does ourDeep Attention Selective Network (dasNet) architecture. DasNets feedbackstructure can dynamically alter its convolutional filter sensitivities duringclassification. It harnesses the power of sequential processing to improveclassification performance, by allowing the network to iteratively focus itsinternal attention on some of its convolutional filters. Feedback is trainedthrough direct policy search in a huge million-dimensional parameter space,through scalable natural evolution strategies (SNES). On the CIFAR-10 andCIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.
arxiv-6900-123 | Discovering Discriminative Cell Attributes for HEp-2 Specimen Image Classification | http://arxiv.org/pdf/1407.7330v1.pdf | author:Arnold Wiliem, Peter Hobson, Brian C. Lovell category:cs.CV cs.CE published:2014-07-28 summary:Recently, there has been a growing interest in developing Computer AidedDiagnostic (CAD) systems for improving the reliability and consistency ofpathology test results. This paper describes a novel CAD system for theAnti-Nuclear Antibody (ANA) test via Indirect Immunofluorescence protocol onHuman Epithelial Type 2 (HEp-2) cells. While prior works have primarily focusedon classifying cell images extracted from ANA specimen images, this work takesa further step by focussing on the specimen image classification problemitself. Our system is able to efficiently classify specimen images as well asproducing meaningful descriptions of ANA pattern class which helps physiciansto understand the differences between various ANA patterns. We achieve thisgoal by designing a specimen-level image descriptor that: (1) is highlydiscriminative; (2) has small descriptor length and (3) is semanticallymeaningful at the cell level. In our work, a specimen image descriptor isrepresented by its overall cell attribute descriptors. As such, we propose twomax-margin based learning schemes to discover cell attributes whilst stillmaintaining the discrimination of the specimen image descriptor. Our learningschemes differ from the existing discriminative attribute learning approachesas they primarily focus on discovering image-level attributes. Comparativeevaluations were undertaken to contrast the proposed approach to variousstate-of-the-art approaches on a novel HEp-2 cell dataset which wasspecifically proposed for the specimen-level classification. Finally, weshowcase the ability of the proposed approach to provide textual descriptionsto explain ANA patterns.
arxiv-6900-124 | A unified framework for thermal face recognition | http://arxiv.org/pdf/1407.7317v1.pdf | author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV published:2014-07-28 summary:The reduction of the cost of infrared (IR) cameras in recent years has madeIR imaging a highly viable modality for face recognition in practice. Aparticularly attractive advantage of IR-based over conventional, visiblespectrum-based face recognition stems from its invariance to visibleillumination. In this paper we argue that the main limitation of previous workon face recognition using IR lies in its ad hoc approach to treating differentnuisance factors which affect appearance, prohibiting a unified approach thatis capable of handling concurrent changes in multiple (or indeed all) majorextrinsic sources of variability, which is needed in practice. We describe thefirst approach that attempts to achieve this - the framework we proposeachieves outstanding recognition performance in the presence of variable (i)pose, (ii) facial expression, (iii) physiological state, (iv) partial occlusiondue to eye-wear, and (v) quasi-occlusion due to facial hair growth.
arxiv-6900-125 | A Poisson convolution model for characterizing topical content with word frequency and exclusivity | http://arxiv.org/pdf/1206.4631v3.pdf | author:Edoardo M Airoldi, Jonathan M Bischof category:cs.LG cs.CL cs.IR stat.ME stat.ML published:2012-06-18 summary:An ongoing challenge in the analysis of document collections is how tosummarize content in terms of a set of inferred themes that can be interpretedsubstantively in terms of topics. The current practice of parametrizing thethemes in terms of most frequent words limits interpretability by ignoring thedifferential use of words across topics. We argue that words that are bothcommon and exclusive to a theme are more effective at characterizing topicalcontent. We consider a setting where professional editors have annotateddocuments to a collection of topic categories, organized into a tree, in whichleaf-nodes correspond to the most specific topics. Each document is annotatedto multiple categories, at different levels of the tree. We introduce ahierarchical Poisson convolution model to analyze annotated documents in thissetting. The model leverages the structure among categories defined byprofessional editors to infer a clear semantic description for each topic interms of words that are both frequent and exclusive. We carry out a largerandomized experiment on Amazon Turk to demonstrate that topic summaries basedon the FREX score are more interpretable than currently established frequencybased summaries, and that the proposed model produces more efficient estimatesof exclusivity than with currently models. We also develop a parallelizedHamiltonian Monte Carlo sampler that allows the inference to scale to millionsof documents.
arxiv-6900-126 | Algorithms, Initializations, and Convergence for the Nonnegative Matrix Factorization | http://arxiv.org/pdf/1407.7299v1.pdf | author:Amy N. Langville, Carl D. Meyer, Russell Albright, James Cox, David Duling category:cs.NA cs.LG stat.ML 65F30 published:2014-07-28 summary:It is well known that good initializations can improve the speed and accuracyof the solutions of many nonnegative matrix factorization (NMF) algorithms.Many NMF algorithms are sensitive with respect to the initialization of W or Hor both. This is especially true of algorithms of the alternating least squares(ALS) type, including the two new ALS algorithms that we present in this paper.We compare the results of six initialization procedures (two standard and fournew) on our ALS algorithms. Lastly, we discuss the practical issue of choosingan appropriate convergence criterion.
arxiv-6900-127 | Leveraging user profile attributes for improving pedagogical accuracy of learning pathways | http://arxiv.org/pdf/1407.7260v1.pdf | author:Tanmay Sinha, Ankit Banka, Dae Ki Kang category:cs.CY cs.LG published:2014-07-27 summary:In recent years, with the enormous explosion of web based learning resources,personalization has become a critical factor for the success of services thatwish to leverage the power of Web 2.0. However, the relevance, significance andimpact of tailored content delivery in the learning domain is stillquestionable. Apart from considering only interaction based features likeratings and inferring learner preferences from them, if these services were toincorporate innate user profile attributes which affect learning activities,the quality of recommendations produced could be vastly improved. Recognizingthe crucial role of effective guidance in informal educational settings, weprovide a principled way of utilizing multiple sources of information from theuser profile itself for the recommendation task. We explore factors that affectthe choice of learning resources and explain in what way are they helpful toimprove the pedagogical accuracy of learning objects recommended. Through asystematical application of machine learning techniques, we further provide atechnological solution to convert these indirectly mapped learner specificattributes into a direct mapping with the learning resources. This mapping hasa distinct advantage of tagging learning resources to make their metadata moreinformative. The results of our empirical study depict the similarity ofnominal learning attributes with respect to each other. We further succeed incapturing the learner subset, whose preferences are most likely to be anindication of learning resource usage. Our novel system filters learner profileattributes to discover a tag that links them with learning resources.
arxiv-6900-128 | Fast Distributed Coordinate Descent for Non-Strongly Convex Losses | http://arxiv.org/pdf/1405.5300v2.pdf | author:Olivier Fercoq, Zheng Qu, Peter Richtárik, Martin Takáč category:math.OC cs.LG published:2014-05-21 summary:We propose an efficient distributed randomized coordinate descent method forminimizing regularized non-strongly convex loss functions. The method attainsthe optimal $O(1/k^2)$ convergence rate, where $k$ is the iteration counter.The core of the work is the theoretical study of stepsize parameters. We haveimplemented the method on Archer - the largest supercomputer in the UK - andshow that the method is capable of solving a (synthetic) LASSO optimizationproblem with 50 billion variables.
arxiv-6900-129 | An evolutionary solver for linear integer programming | http://arxiv.org/pdf/1407.7211v1.pdf | author:João Pedro Pedroso category:cs.NE cs.AI math.OC 80M50 G.1.6, I.2.8 published:2014-07-27 summary:In this paper we introduce an evolutionary algorithm for the solution oflinear integer programs. The strategy is based on the separation of thevariables into the integer subset and the continuous subset; the integervariables are fixed by the evolutionary system, and the continuous ones aredetermined in function of them, by a linear program solver. We report results obtained for some standard benchmark problems, and comparethem with those obtained by branch-and-bound. The performance of theevolutionary algorithm is promising. Good feasible solutions were generallyobtained, and in some of the difficult benchmark tests it outperformedbranch-and-bound.
arxiv-6900-130 | Principles and Parameters: a coding theory perspective | http://arxiv.org/pdf/1407.7169v1.pdf | author:Matilde Marcolli category:cs.CL cs.IT math.IT 91F20, 68P30 published:2014-07-26 summary:We propose an approach to Longobardi's parametric comparison method (PCM) viathe theory of error-correcting codes. One associates to a collection oflanguages to be analyzed with the PCM a binary (or ternary) code with one codewords for each language in the family and each word consisting of the binaryvalues of the syntactic parameters of the language, with the ternary caseallowing for an additional parameter state that takes into account phenomena ofentailment of parameters. The code parameters of the resulting code can becompared with some classical bounds in coding theory: the asymptotic bound, theGilbert-Varshamov bound, etc. The position of the code parameters with respectto some of these bounds provides quantitative information on the variability ofsyntactic parameters within and across historical-linguistic families. Whilecomputations carried out for languages belonging to the same family yield codesbelow the GV curve, comparisons across different historical families can giveexamples of isolated codes lying above the asymptotic bound.
arxiv-6900-131 | Mixture Model Averaging for Clustering | http://arxiv.org/pdf/1212.5760v3.pdf | author:Yuhong Wei, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2012-12-23 summary:In mixture model-based clustering applications, it is common to fit severalmodels from a family and report clustering results from only the `best' one. Insuch circumstances, selection of this best model is achieved using a modelselection criterion, most often the Bayesian information criterion. Rather thanthrow away all but the best model, we average multiple models that are in somesense close to the best one, thereby producing a weighted average of clusteringresults. Two (weighted) averaging approaches are considered: averaging thecomponent membership probabilities and averaging models. In both cases, Occam'swindow is used to determine closeness to the best model and weights arecomputed within a Bayesian model averaging paradigm. In some cases, we need tomerge components before averaging; we introduce a method for merging mixturecomponents based on the adjusted Rand index. The effectiveness of ourmodel-based clustering averaging approaches is illustrated using a family ofGaussian mixture models on real and simulated data.
arxiv-6900-132 | Pairwise Correlations in Layered Close-Packed Structures | http://arxiv.org/pdf/1407.7159v1.pdf | author:P. M. Riechers, D. P. Varn, J. P. Crutchfield category:cs.LG published:2014-07-26 summary:Given a description of the stacking statistics of layered close-packedstructures in the form of a hidden Markov model, we develop analyticalexpressions for the pairwise correlation functions between the layers. Thesemay be calculated analytically as explicit functions of model parameters or theexpressions may be used as a fast, accurate, and efficient way to obtainnumerical values. We present several examples, finding agreement with previouswork as well as deriving new relations.
arxiv-6900-133 | Randomized Block Coordinate Descent for Online and Stochastic Optimization | http://arxiv.org/pdf/1407.0107v3.pdf | author:Huahua Wang, Arindam Banerjee category:cs.LG published:2014-07-01 summary:Two types of low cost-per-iteration gradient descent methods have beenextensively studied in parallel. One is online or stochastic gradient descent(OGD/SGD), and the other is randomzied coordinate descent (RBCD). In thispaper, we combine the two types of methods together and propose onlinerandomized block coordinate descent (ORBCD). At each iteration, ORBCD onlycomputes the partial gradient of one block coordinate of one mini-batchsamples. ORBCD is well suited for the composite minimization problem where onefunction is the average of the losses of a large number of samples and theother is a simple regularizer defined on high dimensional variables. We showthat the iteration complexity of ORBCD has the same order as OGD or SGD. Forstrongly convex functions, by reducing the variance of stochastic gradients, weshow that ORBCD can converge at a geometric rate in expectation, matching theconvergence rate of SGD with variance reduction and RBCD.
arxiv-6900-134 | Decision Trees for Function Evaluation - Simultaneous Optimization of Worst and Expected Cost | http://arxiv.org/pdf/1309.2796v2.pdf | author:Ferdinando Cicalese, Eduardo Laber, Aline Medeiros Saettler category:cs.DS cs.AI cs.LG published:2013-09-11 summary:In several applications of automatic diagnosis and active learning a centralproblem is the evaluation of a discrete function by adaptively querying thevalues of its variables until the values read uniquely determine the value ofthe function. In general, the process of reading the value of a variable mightinvolve some cost, computational or even a fee to be paid for the experimentrequired for obtaining the value. This cost should be taken into account whendeciding the next variable to read. The goal is to design a strategy forevaluating the function incurring little cost (in the worst case or inexpectation according to a prior distribution on the possible variables'assignments). Our algorithm builds a strategy (decision tree) which attains alogarithmic approxima- tion simultaneously for the expected and worst costspent. This is best possible under the assumption that $P \neq NP.$
arxiv-6900-135 | Bayesian Nonparametric Dictionary Learning for Compressed Sensing MRI | http://arxiv.org/pdf/1302.2712v3.pdf | author:Yue Huang, John Paisley, Qin Lin, Xinghao Ding, Xueyang Fu, Xiao-ping Zhang category:cs.CV physics.med-ph stat.AP published:2013-02-12 summary:We develop a Bayesian nonparametric model for reconstructing magneticresonance images (MRI) from highly undersampled k-space data. We performdictionary learning as part of the image reconstruction process. To this end,we use the beta process as a nonparametric dictionary learning prior forrepresenting an image patch as a sparse combination of dictionary elements. Thesize of the dictionary and the patch-specific sparsity pattern are inferredfrom the data, in addition to other dictionary learning variables. Dictionarylearning is performed directly on the compressed image, and so is tailored tothe MRI being considered. In addition, we investigate a total variation penaltyterm in combination with the dictionary learning model, and show how thedenoising property of dictionary learning removes dependence on regularizationparameters in the noisy setting. We derive a stochastic optimization algorithmbased on Markov Chain Monte Carlo (MCMC) for the Bayesian model, and use thealternating direction method of multipliers (ADMM) for efficiently performingtotal variation minimization. We present empirical results on several MRI,which show that the proposed regularization framework can improvereconstruction accuracy over other methods.
arxiv-6900-136 | Crowdsourcing Dialect Characterization through Twitter | http://arxiv.org/pdf/1407.7094v1.pdf | author:Bruno Gonçalves, David Sánchez category:physics.soc-ph cs.CL cs.SI stat.ML published:2014-07-26 summary:We perform a large-scale analysis of language diatopic variation usinggeotagged microblogging datasets. By collecting all Twitter messages written inSpanish over more than two years, we build a corpus from which a carefullyselected list of concepts allows us to characterize Spanish varieties on aglobal scale. A cluster analysis proves the existence of well definedmacroregions sharing common lexical properties. Remarkably enough, we find thatSpanish language is split into two superdialects, namely, an urban speech usedacross major American and Spanish citites and a diverse form that encompassesrural areas and small towns. The latter can be further clustered into smallervarieties with a stronger regional character.
arxiv-6900-137 | Pushbroom Stereo for High-Speed Navigation in Cluttered Environments | http://arxiv.org/pdf/1407.7091v1.pdf | author:Andrew J. Barry, Russ Tedrake category:cs.RO cs.CV published:2014-07-26 summary:We present a novel stereo vision algorithm that is capable of obstacledetection on a mobile-CPU processor at 120 frames per second. Our systemperforms a subset of standard block-matching stereo processing, searching onlyfor obstacles at a single depth. By using an onboard IMU and state-estimator,we can recover the position of obstacles at all other depths, building andupdating a full depth-map at framerate. Here, we describe both the algorithm and our implementation on a high-speed,small UAV, flying at over 20 MPH (9 m/s) close to obstacles. The systemrequires no external sensing or computation and is, to the best of ourknowledge, the first high-framerate stereo detection system running onboard asmall UAV.
arxiv-6900-138 | Efficient Bayesian Nonparametric Modelling of Structured Point Processes | http://arxiv.org/pdf/1407.6949v1.pdf | author:Tom Gunter, Chris Lloyd, Michael A. Osborne, Stephen J. Roberts category:stat.ML published:2014-07-25 summary:This paper presents a Bayesian generative model for dependent Cox pointprocesses, alongside an efficient inference scheme which scales as if the pointprocesses were modelled independently. We can handle missing data naturally,infer latent structure, and cope with large numbers of observed processes. Afurther novel contribution enables the model to work effectively in higherdimensional spaces. Using this method, we achieve vastly improved predictiveperformance on both 2D and 1D real data, validating our structured approach.
arxiv-6900-139 | An Efficient Two-Stage Sparse Representation Method | http://arxiv.org/pdf/1404.1129v2.pdf | author:Chengyu Peng, Hong Cheng, Manchor Ko category:cs.CV G.1.6; I.4.10 published:2014-04-04 summary:There are a large number of methods for solving under-determined linearinverse problem. Many of them have very high time complexity for largedatasets. We propose a new method called Two-Stage Sparse Representation (TSSR)to tackle this problem. We decompose the representing space of signals into twoparts, the measurement dictionary and the sparsifying basis. The dictionary isdesigned to approximate a sub-Gaussian distribution to exploit itsconcentration property. We apply sparse coding to the signals on the dictionaryin the first stage, and obtain the training and testing coefficientsrespectively. Then we design the basis to approach an identity matrix in thesecond stage, to acquire the Restricted Isometry Property (RIP) anduniversality property. The testing coefficients are encoded over the basis andthe final representing coefficients are obtained. We verify that the projectionof testing coefficients onto the basis is a good approximation of the signalonto the representing space. Since the projection is conducted on a muchsparser space, the runtime is greatly reduced. For concrete realization, weprovide an instance for the proposed TSSR. Experiments on four biometricsdatabases show that TSSR is effective and efficient, comparing with severalclassical methods for solving linear inverse problem.
arxiv-6900-140 | Interpretable Low-Rank Document Representations with Label-Dependent Sparsity Patterns | http://arxiv.org/pdf/1407.6872v1.pdf | author:Ivan Ivek category:cs.CL cs.IR cs.LG published:2014-07-25 summary:In context of document classification, where in a corpus of documents theirlabel tags are readily known, an opportunity lies in utilizing labelinformation to learn document representation spaces with better discriminativeproperties. To this end, in this paper application of a Variational BayesianSupervised Nonnegative Matrix Factorization (supervised vbNMF) withlabel-driven sparsity structure of coefficients is proposed for learning ofdiscriminative nonsubtractive latent semantic components occuring in TF-IDFdocument representations. Constraints are such that the components pursued aremade to be frequently occuring in a small set of labels only, making itpossible to yield document representations with distinctive label-specificsparse activation patterns. A simple measure of quality of this kind ofsparsity structure, dubbed inter-label sparsity, is introduced andexperimentally brought into tight connection with classification performance.Representing a great practical convenience, inter-label sparsity is shown to beeasily controlled in supervised vbNMF by a single parameter.
arxiv-6900-141 | Substitute Based SCODE Word Embeddings in Supervised NLP Tasks | http://arxiv.org/pdf/1407.6853v1.pdf | author:Volkan Cirik, Deniz Yuret category:cs.CL published:2014-07-25 summary:We analyze a word embedding method in supervised tasks. It maps words on asphere such that words co-occurring in similar contexts lie closely. Thesimilarity of contexts is measured by the distribution of substitutes that canfill them. We compared word embeddings, including more recent representations,in Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examineour framework in multilingual dependency parsing as well. The results show thatthe proposed method achieves as good as or better results compared to the otherword embeddings in the tasks we investigate. It achieves state-of-the-artresults in multilingual dependency parsing. Word embeddings in 7 languages areavailable for public use.
arxiv-6900-142 | Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version | http://arxiv.org/pdf/1207.2940v4.pdf | author:Marc Peter Deisenroth, Shakir Mohamed category:stat.ML cs.LG cs.SY published:2012-07-12 summary:Rich and complex time-series data, such as those generated from engineeringsystems, financial markets, videos or neural recordings, are now a commonfeature of modern data analysis. Explaining the phenomena underlying thesediverse data sets requires flexible and accurate models. In this paper, wepromote Gaussian process dynamical systems (GPDS) as a rich model class that isappropriate for such analysis. In particular, we present a message passingalgorithm for approximate inference in GPDSs based on expectation propagation.By posing inference as a general message passing problem, we iterateforward-backward smoothing. Thus, we obtain more accurate posteriordistributions over latent structures, resulting in improved predictiveperformance compared to state-of-the-art GPDS smoothers, which are specialcases of our general message passing algorithm. Hence, we provide a unifyingapproach within which to contextualize message passing in GPDSs.
arxiv-6900-143 | Curved Gabor Filters for Fingerprint Image Enhancement | http://arxiv.org/pdf/1104.4298v2.pdf | author:Carsten Gottschlich category:cs.CV published:2011-04-21 summary:Gabor filters play an important role in many application areas for theenhancement of various types of images and the extraction of Gabor features.For the purpose of enhancing curved structures in noisy images, we introducecurved Gabor filters which locally adapt their shape to the direction of flow.These curved Gabor filters enable the choice of filter parameters whichincrease the smoothing power without creating artifacts in the enhanced image.In this paper, curved Gabor filters are applied to the curved ridge and valleystructure of low-quality fingerprint images. First, we combine two orientationfield estimation methods in order to obtain a more robust estimation for verynoisy images. Next, curved regions are constructed by following the respectivelocal orientation and they are used for estimating the local ridge frequency.Lastly, curved Gabor filters are defined based on curved regions and they areapplied for the enhancement of low-quality fingerprint images. Experimentalresults on the FVC2004 databases show improvements of this approach incomparison to state-of-the-art enhancement methods.
arxiv-6900-144 | Optimizing Auto-correlation for Fast Target Search in Large Search Space | http://arxiv.org/pdf/1407.3535v2.pdf | author:Arif Mahmood, Ajmal Mian, Robyn Owens category:cs.CV published:2014-07-14 summary:In remote sensing image-blurring is induced by many sources such asatmospheric scatter, optical aberration, spatial and temporal sensorintegration. The natural blurring can be exploited to speed up target search byfast template matching. In this paper, we synthetically induce additionalnon-uniform blurring to further increase the speed of the matching process. Toavoid loss of accuracy, the amount of synthetic blurring is varied spatiallyover the image according to the underlying content. We extend transitivealgorithm for fast template matching by incorporating controlled image blur. Tothis end we propose an Efficient Group Size (EGS) algorithm which minimizes thenumber of similarity computations for a particular search image. A largerefficient group size guarantees less computations and more speedup. EGSalgorithm is used as a component in our proposed Optimizing auto-correlation(OptA) algorithm. In OptA a search image is iteratively non-uniformly blurredwhile ensuring no accuracy degradation at any image location. In each iterationefficient group size and overall computations are estimated by using theproposed EGS algorithm. The OptA algorithm stops when the number ofcomputations cannot be further decreased without accuracy degradation. Theproposed algorithm is compared with six existing state of the art exhaustiveaccuracy techniques using correlation coefficient as the similarity measure.Experiments on satellite and aerial image datasets demonstrate theeffectiveness of the proposed algorithm.
arxiv-6900-145 | An Unsupervised Algorithm For Learning Lie Group Transformations | http://arxiv.org/pdf/1001.1027v4.pdf | author:Jascha Sohl-Dickstein, Ching Ming Wang, Bruno A. Olshausen category:cs.CV cs.LG published:2010-01-07 summary:We present several theoretical contributions which allow Lie groups to be fitto high dimensional datasets. Transformation operators are represented in theireigen-basis, reducing the computational complexity of parameter estimation tothat of training a linear transformation model. A transformation specific"blurring" operator is introduced that allows inference to escape local minimavia a smoothing of the transformation space. A penalty on traversed manifolddistance is added which encourages the discovery of sparse, minimal distance,transformations between states. Both learning and inference are demonstratedusing these methods for the full set of affine transformations on natural imagepatches. Transformation operators are then trained on natural video sequences.It is shown that the learned video transformations provide a better descriptionof inter-frame differences than the standard motion model based on rigidtranslation.
arxiv-6900-146 | Enhancing the Accuracy of Biometric Feature Extraction Fusion Using Gabor Filter and Mahalanobis Distance Algorithm | http://arxiv.org/pdf/1407.6748v1.pdf | author:Ayodeji S. Makinde, Yaw Nkansah-Gyekye, Loserian S. Laizer category:cs.CV published:2014-07-24 summary:Biometric recognition systems have advanced significantly in the last decadeand their use in specific applications will increase in the near future. Theability to conduct meaningful comparisons and assessments will be crucial tosuccessful deployment and increasing biometric adoption. The best modality usedas unimodal biometric systems are unable to fully address the problem of higherrecognition rate. Multimodal biometric systems are able to mitigate some of thelimitations encountered in unimodal biometric systems, such asnon-universality, distinctiveness, non-acceptability, noisy sensor data, spoofattacks, and performance. More reliable recognition accuracy and performanceare achievable as different modalities were being combined together anddifferent algorithms or techniques were being used. The work presented in thispaper focuses on a bimodal biometric system using face and fingerprint. Animage enhancement technique (histogram equalization) is used to enhance theface and fingerprint images. Salient features of the face and fingerprint wereextracted using the Gabor filter technique. A dimensionality reductiontechnique was carried out on both images extracted features using a principalcomponent analysis technique. A feature level fusion algorithm (Mahalanobisdistance technique) is used to combine each unimodal feature together. Theperformance of the proposed approach is validated and is effective.
arxiv-6900-147 | Trainable and Dynamic Computing: Error Backpropagation through Physical Media | http://arxiv.org/pdf/1407.6637v1.pdf | author:Michiel Hermans, Michaël Burm, Joni Dambre, Peter Bienstman category:cs.NE published:2014-07-24 summary:Machine learning algorithms, and more in particular neural networks, arguablyexperience a revolution in terms of performance. Currently, the best systems wehave for speech recognition, computer vision and similar problems are based onneural networks, trained using the half-century old backpropagation algorithm.Despite the fact that neural networks are a form of analog computers, they arestill implemented digitally for reasons of convenience and availability. Inthis paper we demonstrate how we can design physical linear dynamic systemswith non-linear feedback as a generic platform for dynamic, neuro-inspiredanalog computing. We show that a crucial advantage of this setup is that theerror backpropagation can be performed physically as well, which greatly speedsup the optimisation process. As we show in this paper, using one experimentallyvalidated and one conceptual example, such systems may be the key to providinga relatively straightforward mechanism for constructing highly scalable, fullydynamic analog computers.
arxiv-6900-148 | Ubic: Bridging the gap between digital cryptography and the physical world | http://arxiv.org/pdf/1403.1343v3.pdf | author:Mark Simkin, Dominique Schroeder, Andreas Bulling, Mario Fritz category:cs.CR cs.CV published:2014-03-06 summary:Advances in computing technology increasingly blur the boundary between thedigital domain and the physical world. Although the research community hasdeveloped a large number of cryptographic primitives and has demonstrated theirusability in all-digital communication, many of them have not yet made theirway into the real world due to usability aspects. We aim to make another steptowards a tighter integration of digital cryptography into real worldinteractions. We describe Ubic, a framework that allows users to bridge the gapbetween digital cryptography and the physical world. Ubic relies onhead-mounted displays, like Google Glass, resource-friendly computer visiontechniques as well as mathematically sound cryptographic primitives to provideusers with better security and privacy guarantees. The framework covers keycryptographic primitives, such as secure identification, document verificationusing a novel secure physical document format, as well as content hiding. Tomake a contribution of practical value, we focused on making Ubic as simple,easily deployable, and user friendly as possible.
arxiv-6900-149 | Convolutional Neural Associative Memories: Massive Capacity with Noise Tolerance | http://arxiv.org/pdf/1407.6513v1.pdf | author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi category:cs.NE cs.AI published:2014-07-24 summary:The task of a neural associative memory is to retrieve a set of previouslymemorized patterns from their noisy versions using a network of neurons. Anideal network should have the ability to 1) learn a set of patterns as theyarrive, 2) retrieve the correct patterns from noisy queries, and 3) maximizethe pattern retrieval capacity while maintaining the reliability in respondingto queries. The majority of work on neural associative memories has focused ondesigning networks capable of memorizing any set of randomly chosen patterns atthe expense of limiting the retrieval capacity. In this paper, we show that ifwe target memorizing only those patterns that have inherent redundancy (i.e.,belong to a subspace), we can obtain all the aforementioned properties. This isin sharp contrast with the previous work that could only improve one or twoaspects at the expense of the third. More specifically, we propose frameworkbased on a convolutional neural network along with an iterative algorithm thatlearns the redundancy among the patterns. The resulting network has a retrievalcapacity that is exponential in the size of the network. Moreover, theasymptotic error correction performance of our network is linear in the size ofthe patterns. We then ex- tend our approach to deal with patterns lieapproximately in a subspace. This extension allows us to memorize datasetscontaining natural patterns (e.g., images). Finally, we report experimentalresults on both synthetic and real datasets to support our claims.
arxiv-6900-150 | Nonlinear Quality of Life Index | http://arxiv.org/pdf/1008.4063v3.pdf | author:A. Zinovyev, A. N. Gorban category:cs.NE stat.AP published:2010-08-24 summary:We present details of the analysis of the nonlinear quality of life index for171 countries. This index is based on four indicators: GDP per capita byPurchasing Power Parities, Life expectancy at birth, Infant mortality rate, andTuberculosis incidence. We analyze the structure of the data in order to findthe optimal and independent on expert's opinion way to map several numericalindicators from a multidimensional space onto the one-dimensional space of thequality of life. In the 4D space we found a principal curve that goes "throughthe middle" of the dataset and project the data points on this curve. The orderalong this principal curve gives us the ranking of countries. Projection ontothe principal curve provides a solution to the classical problem ofunsupervised ranking of objects. It allows us to find the independent onexpert's opinion way to project several numerical indicators from amultidimensional space onto the one-dimensional space of the index values. Thisprojection is, in some sense, optimal and preserves as much information aspossible. For computation we used ViDaExpert, a tool for visualization andanalysis of multidimensional vectorial data (arXiv:1406.5550).
arxiv-6900-151 | New Method for Optimization of License Plate Recognition system with Use of Edge Detection and Connected Component | http://arxiv.org/pdf/1407.6510v1.pdf | author:Reza Azad, Hamid Reza Shayegh category:cs.CV published:2014-07-24 summary:License Plate recognition plays an important role on the traffic monitoringand parking management systems. In this paper, a fast and real time method hasbeen proposed which has an appropriate application to find tilt and poorquality plates. In the proposed method, at the beginning, the image isconverted into binary mode using adaptive threshold. Then, by using some edgedetection and morphology operations, plate number location has been specified.Finally, if the plat has tilt, its tilt is removed away. This method has beentested on another paper data set that has different images of the background,considering distance, and angel of view so that the correct extraction rate ofplate reached at 98.66%.
arxiv-6900-152 | Novel and Tuneable Method for Skin Detection Based on Hybrid Color Space and Color Statistical Features | http://arxiv.org/pdf/1407.6506v1.pdf | author:Reza Azad, Hamid Reza Shayegh category:cs.CV published:2014-07-24 summary:Skin detection is one of the most important and primary stages in some ofimage processing applications such as face detection and human tracking. Sofar, many approaches are proposed to done this case. Near all of these methodshave tried to find best match intensity distribution with skin pixels based onpopular color spaces such as RGB, CMYK or YCbCr. Results show these methodscannot provide an accurate approach for every kinds of skin. In this paper, anapproach is proposed to solve this problem using statistical featurestechnique. This approach is including two stages. In the first one, from pureskin statistical features were extracted and at the second stage, the skinpixels are detected using HSV and YCbCr color spaces. In the result part, theproposed approach is applied on FEI database and the accuracy rate reached99.25 + 0.2. Further proposed method is applied on complex background databaseand accuracy rate obtained 95.40+0.31%. The proposed approach can be used forall kinds of skin using train stage which is the main advantages of it. Lownoise sensitivity and low computational complexity are some of otheradvantages.
arxiv-6900-153 | Real-Time and Efficient Method for Accuracy Enhancement of Edge Based License Plate Recognition System | http://arxiv.org/pdf/1407.6498v1.pdf | author:Reza Azad, Babak Azad, Hamid Reza Shayegh category:cs.CV published:2014-07-24 summary:License Plate Recognition plays an important role on the traffic monitoringand parking management. Administration and restriction of those transportationtools for their better service becomes very essential. In this paper, a fastand real time method has an appropriate application to find plates that theplat has tilt and the picture quality is poor. In the proposed method, at thebeginning, the image is converted into binary mode with use of adaptivethreshold. And with use of edge detection and morphology operation, platenumber location has been specified and if the plat has tilt; its tilt isremoved away. Then its characters are distinguished using image processingtechniques. Finally, K Nearest Neighbour (KNN) classifier was used forcharacter recognition. This method has been tested on available data set thathas different images of the background, considering distance, and angel of viewso that the correct extraction rate of plate reached at 98% and characterrecognition rate achieved at 99.12%. Further we tested our characterrecognition stage on Persian vehicle data set and we achieved 99% correctrecognition rate.
arxiv-6900-154 | Sequential Changepoint Approach for Online Community Detection | http://arxiv.org/pdf/1407.5978v3.pdf | author:David Marangoni-Simonsen, Yao Xie category:stat.ML cs.LG cs.SI math.ST stat.TH published:2014-07-22 summary:We present new algorithms for detecting the emergence of a community in largenetworks from sequential observations. The networks are modeled usingErdos-Renyi random graphs with edges forming between nodes in the communitywith higher probability. Based on statistical changepoint detectionmethodology, we develop three algorithms: the Exhaustive Search (ES), themixture, and the Hierarchical Mixture (H-Mix) methods. Performance of thesemethods is evaluated by the average run length (ARL), which captures thefrequency of false alarms, and the detection delay. Numerical comparisons showthat the ES method performs the best; however, it is exponentially complex. Themixture method is polynomially complex by exploiting the fact that the size ofthe community is typically small in a large network. However, it may react to agroup of active edges that do not form a community. This issue is resolved bythe H-Mix method, which is based on a dendrogram decomposition of the network.We present an asymptotic analytical expression for ARL of the mixture methodwhen the threshold is large. Numerical simulation verifies that ourapproximation is accurate even in the non-asymptotic regime. Hence, it can beused to determine a desired threshold efficiently. Finally, numerical examplesshow that the mixture and the H-Mix methods can both detect a community quicklywith a lower complexity than the ES method.
arxiv-6900-155 | Learning Structured Outputs from Partial Labels using Forest Ensemble | http://arxiv.org/pdf/1407.6432v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.CV cs.LG published:2014-07-24 summary:Learning structured outputs with general structures is computationallychallenging, except for tree-structured models. Thus we propose an efficientboosting-based algorithm AdaBoost.MRF for this task. The idea is based on therealization that a graph is a superimposition of trees. Different from mostexisting work, our algorithm can handle partial labelling, and thus isparticularly attractive in practice where reliable labels are often sparselyobserved. In addition, our method works exclusively on trees and thus isguaranteed to converge. We apply the AdaBoost.MRF algorithm to an indoor videosurveillance scenario, where activities are modelled at multiple levels.
arxiv-6900-156 | Performance evaluation of wavelet scattering network in image texture classification in various color spaces | http://arxiv.org/pdf/1407.6423v1.pdf | author:Jiasong Wu, Longyu Jiang, Xu Han, Lotfi Senhadji, Huazhong Shu category:cs.CV published:2014-07-24 summary:Texture plays an important role in many image analysis applications. In thispaper, we give a performance evaluation of color texture classification byperforming wavelet scattering network in various color spaces. Experimentalresults on the KTH_TIPS_COL database show that opponent RGB based waveletscattering network outperforms other color spaces. Therefore, when dealing withthe problem of color texture classification, opponent RGB based waveletscattering network is recommended.
arxiv-6900-157 | Subspace Learning From Bits | http://arxiv.org/pdf/1407.6288v2.pdf | author:Yuejie Chi category:stat.ML cs.IT math.IT published:2014-07-23 summary:This paper proposes a simple sensing and estimation framework to faithfullyrecover the principal subspace of high-dimensional datasets or data streamsfrom a collection of one-bit measurements from distributed sensors based oncomparing accumulated energy projections of their data samples of dimension nover pairs of randomly selected directions. By leveraging low-dimensionalstructures, the top eigenvectors of a properly designed surrogate matrix isshown to recover the principal subspace of rank $r$ as soon as the number ofbit measurements exceeds the order of $nr^2 \log n$, which can be much smallerthan the ambient dimension of the covariance matrix. The sample complexity toobtain reliable comparison outcomes is also obtained. Furthermore, we develop alow-complexity online algorithm to track the principal subspace that allows newbit measurements arrive sequentially. Numerical examples are provided tovalidate the proposed approach.
arxiv-6900-158 | Parallel MCMC with Generalized Elliptical Slice Sampling | http://arxiv.org/pdf/1210.7477v2.pdf | author:Robert Nishihara, Iain Murray, Ryan P. Adams category:stat.CO stat.ML published:2012-10-28 summary:Probabilistic models are conceptually powerful tools for finding structure indata, but their practical effectiveness is often limited by our ability toperform inference in them. Exact inference is frequently intractable, soapproximate inference is often performed using Markov chain Monte Carlo (MCMC).To achieve the best possible results from MCMC, we want to efficiently simulatemany steps of a rapidly mixing Markov chain which leaves the targetdistribution invariant. Of particular interest in this regard is how to takeadvantage of multi-core computing to speed up MCMC-based inference, both toimprove mixing and to distribute the computational load. In this paper, wepresent a parallelizable Markov chain Monte Carlo algorithm for efficientlysampling from continuous probability distributions that can take advantage ofhundreds of cores. This method shares information between parallel Markovchains to build a scale-mixture of Gaussians approximation to the densityfunction of the target distribution. We combine this approximation with arecent method known as elliptical slice sampling to create a Markov chain withno step-size parameters that can mix rapidly without requiring gradient orcurvature computations.
arxiv-6900-159 | Clustering Partially Observed Graphs via Convex Optimization | http://arxiv.org/pdf/1104.4803v4.pdf | author:Yudong Chen, Ali Jalali, Sujay Sanghavi, Huan Xu category:cs.LG stat.ML published:2011-04-25 summary:This paper considers the problem of clustering a partially observedunweighted graph---i.e., one where for some node pairs we know there is an edgebetween them, for some others we know there is no edge, and for the remainingwe do not know whether or not there is an edge. We want to organize the nodesinto disjoint clusters so that there is relatively dense (observed)connectivity within clusters, and sparse across clusters. We take a novel yet natural approach to this problem, by focusing on findingthe clustering that minimizes the number of "disagreements"---i.e., the sum ofthe number of (observed) missing edges within clusters, and (observed) presentedges across clusters. Our algorithm uses convex optimization; its basis is areduction of disagreement minimization to the problem of recovering an(unknown) low-rank matrix and an (unknown) sparse matrix from their partiallyobserved sum. We evaluate the performance of our algorithm on the classicalPlanted Partition/Stochastic Block Model. Our main theorem provides sufficientconditions for the success of our algorithm as a function of the minimumcluster size, edge density and observation probability; in particular, theresults characterize the tradeoff between the observation probability and theedge density gap. When there are a constant number of clusters of equal size,our results are optimal up to logarithmic factors.
arxiv-6900-160 | Exact fit of simple finite mixture models | http://arxiv.org/pdf/1406.6038v2.pdf | author:Dirk Tasche category:stat.ML q-fin.RM 62P30, 62F10 published:2014-06-23 summary:How to forecast next year's portfolio-wide credit default rate based on lastyear's default observations and the current score distribution? A classicalapproach to this problem consists of fitting a mixture of the conditional scoredistributions observed last year to the current score distribution. This is aspecial (simple) case of a finite mixture model where the mixture componentsare fixed and only the weights of the components are estimated. The optimumweights provide a forecast of next year's portfolio-wide default rate. We pointout that the maximum-likelihood (ML) approach to fitting the mixturedistribution not only gives an optimum but even an exact fit if we allow themixture components to vary but keep their density ratio fix. From thisobservation we can conclude that the standard default rate forecast based onlast year's conditional default rates will always be located between lastyear's portfolio-wide default rate and the ML forecast for next year. As anapplication example, then cost quantification is discussed. We also discuss howthe mixture model based estimation methods can be used to forecast total loss.This involves the reinterpretation of an individual classification problem as acollective quantification problem.
arxiv-6900-161 | Novel and Automatic Parking Inventory System Based on Pattern Recognition and Directional Chain Code | http://arxiv.org/pdf/1407.6321v1.pdf | author:Reza Azad, Majid Nazari category:cs.CV published:2014-07-23 summary:The objective of this paper is to design an efficient vehicle license platerecognition System and to implement it for automatic parking inventory system.The system detects the vehicle first and then captures the image of the frontview of the vehicle. Vehicle license plate is localized and characters aresegmented. For finding the place of plate, a novel and real time method isexpressed. A new and robust technique based on directional chain code is usedfor character recognition. The resulting vehicle number is then compared withthe available database of all the vehicles so as to come up with informationabout the vehicle type and to charge entrance cost accordingly. The system isthen allowed to open parking barrier for the vehicle and generate entrance costreceipt. The vehicle information (such as entrance time, date, and cost amount)is also stored in the database to maintain the record. The hardware andsoftware integrated system is implemented and a working prototype model isdeveloped. Under the available database, the average accuracy of locatingvehicle license plate obtained 100%. Using 70% samples of character fortraining, we tested our scheme on whole samples and obtained 100% correctrecognition rate. Further we tested our character recognition stage on Persianvehicle data set and we achieved 99% correct recognition.
arxiv-6900-162 | A robust and adaptable method for face detection based on Color Probabilistic Estimation Technique | http://arxiv.org/pdf/1407.6318v1.pdf | author:Reza Azad, Fatemeh Davami category:cs.CV published:2014-07-23 summary:Human face perception is currently an active research area in the computervision community. Skin detection is one of the most important and primarystages for this purpose. So far, many approaches are proposed to done thiscase. Near all of these methods have tried to find best match intensitydistribution with skin pixels based on popular color spaces such as RGB, HSI orYCBCR. Results show that these methods cannot provide an accurate approach forevery kind of skin. In this paper, an approach is proposed to solve thisproblem using a color probabilistic estimation technique. This approach isincluding two stages. In the first one, the skin intensity distribution isestimated using some train photos of pure skin, and at the second stage, theskin pixels are detected using Gaussian model and optimal threshold tuning.Then from the skin region facial features have been extracted to get the facefrom the skin region. In the results section, the proposed approach is appliedon FEI database and the accuracy rate reached 99.25%. The proposed approach canbe used for all kinds of skin using train stage which is the main advantageamong the other advantages, such as Low noise sensitivity and low computationalcomplexity.
arxiv-6900-163 | Quadratically constrained quadratic programming for classification using particle swarms and applications | http://arxiv.org/pdf/1407.6315v1.pdf | author:Deepak Kumar, A G Ramakrishnan category:cs.AI cs.LG cs.NE math.OC published:2014-07-23 summary:Particle swarm optimization is used in several combinatorial optimizationproblems. In this work, particle swarms are used to solve quadratic programmingproblems with quadratic constraints. The approach of particle swarms is anexample for interior point methods in optimization as an iterative technique.This approach is novel and deals with classification problems without the useof a traditional classifier. Our method determines the optimal hyperplane orclassification boundary for a data set. In a binary classification problem, weconstrain each class as a cluster, which is enclosed by an ellipsoid. Theestimation of the optimal hyperplane between the two clusters is posed as aquadratically constrained quadratic problem. The optimization problem is solvedin distributed format using modified particle swarms. Our method has theadvantage of using the direction towards optimal solution rather than searchingthe entire feasible region. Our results on the Iris, Pima, Wine, and Thyroiddatasets show that the proposed method works better than a neural network andthe performance is close to that of SVM.
arxiv-6900-164 | scikit-image: Image processing in Python | http://arxiv.org/pdf/1407.6245v1.pdf | author:Stefan van der Walt, Johannes L. Schönberger, Juan Nunez-Iglesias, François Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu, the scikit-image contributors category:cs.MS cs.CV published:2014-07-23 summary:scikit-image is an image processing library that implements algorithms andutilities for use in research, education and industry applications. It isreleased under the liberal "Modified BSD" open source license, provides awell-documented API in the Python programming language, and is developed by anactive, international team of collaborators. In this paper we highlight theadvantages of open source to achieve the goals of the scikit-image library, andwe showcase several real-world image processing applications that usescikit-image.
arxiv-6900-165 | Towards Using Unlabeled Data in a Sparse-coding Framework for Human Activity Recognition | http://arxiv.org/pdf/1312.6995v3.pdf | author:Sourav Bhattacharya, Petteri Nurmi, Nils Hammerla, Thomas Plötz category:cs.LG cs.AI stat.ML published:2013-12-25 summary:We propose a sparse-coding framework for activity recognition in ubiquitousand mobile computing that alleviates two fundamental problems of currentsupervised learning approaches. (i) It automatically derives a compact, sparseand meaningful feature representation of sensor data that does not rely onprior expert knowledge and generalizes extremely well across domain boundaries.(ii) It exploits unlabeled sample data for bootstrapping effective activityrecognizers, i.e., substantially reduces the amount of ground truth annotationrequired for model estimation. Such unlabeled data is trivial to obtain, e.g.,through contemporary smartphones carried by users as they go about theireveryday activities. Based on the self-taught learning paradigm we automatically derive anover-complete set of basis vectors from unlabeled data that captures inherentpatterns present within activity data. Through projecting raw sensor data ontothe feature space defined by such over-complete sets of basis vectors effectivefeature extraction is pursued. Given these learned feature representations,classification backends are then trained using small amounts of labeledtraining data. We study the new approach in detail using two datasets which differ in termsof the recognition tasks and sensor modalities. Primarily we focus ontransportation mode analysis task, a popular task in mobile-phone basedsensing. The sparse-coding framework significantly outperforms thestate-of-the-art in supervised learning approaches. Furthermore, we demonstratethe great practical potential of the new approach by successfully evaluatingits generalization capabilities across both domain and sensor modalities byconsidering the popular Opportunity dataset. Our feature learning approachoutperforms state-of-the-art approaches to analyzing activities in dailyliving.
arxiv-6900-166 | Visual Word Selection without Re-Coding and Re-Pooling | http://arxiv.org/pdf/1407.6174v1.pdf | author:Fatih Cakir, Stan Sclaroff category:cs.CV published:2014-07-23 summary:The Bag-of-Words (BoW) representation is widely used in computer vision. Thesize of the codebook impacts the time and space complexity of the applicationsthat use BoW. Thus, given a training set for a particular computer vision task,a key problem is pruning a large codebook to select only a subset of visualwords. Evaluating possible selections of words to be included in the prunedcodebook can be computationally prohibitive; in a brute-force scheme,evaluating each pruned codebook requires re-coding of all features extractedfrom training images to words in the candidate codebook and then re-pooling thewords to obtain a representation of each image, e.g., histogram of visual wordfrequencies. In this paper, a method is proposed that selects and evaluates asubset of words from an initially large codebook, without the need forre-coding or re-pooling. Formulations are proposed for two commonly-usedschemes: hard and soft (kernel) coding of visual words with average-pooling.The effectiveness of these formulations is evaluated on the 15 Scenes andCaltech 10 benchmarks.
arxiv-6900-167 | Content-Level Selective Offloading in Heterogeneous Networks: Multi-armed Bandit Optimization and Regret Bounds | http://arxiv.org/pdf/1407.6154v1.pdf | author:Pol Blasco, Deniz Gündüz category:cs.IT cs.LG math.IT published:2014-07-23 summary:We consider content-level selective offloading of cellular downlink trafficto a wireless infostation terminal which stores high data-rate content in itscache memory. Cellular users in the vicinity of the infostation can directlydownload the stored content from the infostation through a broadband connection(e.g., WiFi), reducing the latency and load on the cellular network. The goalof the infostation cache controller (CC) is to store the most popular contentin the cache memory such that the maximum amount of traffic is offloaded to theinfostation. In practice, the popularity profile of the files is not known bythe CC, which observes only the instantaneous demands for those contents storedin the cache. Hence, the cache content placement is optimised based on thedemand history and on the cost associated to placing each content in the cache.By refreshing the cache content at regular time intervals, the CC graduallylearns the popularity profile, while at the same time exploiting the limitedcache capacity in the best way possible. This is formulated as a multi-armedbandit (MAB) problem with switching cost. Several algorithms are presented todecide on the cache content over time. The performance is measured in terms ofcache efficiency, defined as the amount of net traffic that is offloaded to theinfostation. In addition to theoretical regret bounds, the proposed algorithmsare analysed through numerical simulations. In particular, the impact of systemparameters, such as the number of files, number of users, cache size, andskewness of the popularity profile, on the performance is studied numerically.It is shown that the proposed algorithms learn the popularity profile quicklyfor a wide range of system parameters.
arxiv-6900-168 | A Fast Synchronization Clustering Algorithm | http://arxiv.org/pdf/1407.7449v1.pdf | author:Xinquan Chen category:cs.LG published:2014-07-23 summary:This paper presents a Fast Synchronization Clustering algorithm (FSynC),which is an improved version of SynC algorithm. In order to decrease the timecomplexity of the original SynC algorithm, we combine grid cell partitioningmethod and Red-Black tree to construct the near neighbor point set of everypoint. By simulated experiments of some artificial data sets and several realdata sets, we observe that FSynC algorithm can often get less time than SynCalgorithm for many kinds of data sets. At last, it gives some researchexpectations to popularize this algorithm.
arxiv-6900-169 | Permutation Models for Collaborative Ranking | http://arxiv.org/pdf/1407.6128v1.pdf | author:Truyen Tran, Svetha Venkatesh category:cs.IR cs.LG stat.ML published:2014-07-23 summary:We study the problem of collaborative filtering where ranking information isavailable. Focusing on the core of the collaborative ranking process, the userand their community, we propose new models for representation of the underlyingpermutations and prediction of ranks. The first approach is based on theassumption that the user makes successive choice of items in a stage-wisemanner. In particular, we extend the Plackett-Luce model in two ways -introducing parameter factoring to account for user-specific contribution, andmodelling the latent community in a generative setting. The second approachrelies on log-linear parameterisation, which relaxes the discrete-choiceassumption, but makes learning and inference much more involved. We proposeMCMC-based learning and inference methods and derive linear-time predictionalgorithms.
arxiv-6900-170 | Recognizing Image Style | http://arxiv.org/pdf/1311.3715v3.pdf | author:Sergey Karayev, Matthew Trentacoste, Helen Han, Aseem Agarwala, Trevor Darrell, Aaron Hertzmann, Holger Winnemoeller category:cs.CV published:2013-11-15 summary:The style of an image plays a significant role in how it is viewed, but stylehas received little attention in computer vision research. We describe anapproach to predicting style of images, and perform a thorough evaluation ofdifferent image features for these tasks. We find that features learned in amulti-layer network generally perform best -- even when trained with objectclass (not style) labels. Our large-scale learning methods results in the bestpublished performance on an existing dataset of aesthetic ratings andphotographic style annotations. We present two novel datasets: 80K Flickrphotographs annotated with 20 curated style labels, and 85K paintings annotatedwith 25 style/genre labels. Our approach shows excellent classificationperformance on both datasets. We use the learned classifiers to extendtraditional tag-based image search to consider stylistic constraints, anddemonstrate cross-dataset understanding of style.
arxiv-6900-171 | A Genetic Algorithm for Software Design Migration from Structured to Object Oriented Paradigm | http://arxiv.org/pdf/1407.6116v1.pdf | author:Md. Selim, Saeed Siddik, Alim Ul Gias, M. Abdullah-Al-Wadud, Shah Mostafa Khaled category:cs.SE cs.NE published:2014-07-23 summary:The potential benefit of migrating software design from Structured to ObjectOriented Paradigm is manifolded including modularity, manageability andextendability. This design migration should be automated as it will reduce thetime required in manual process. Our previous work has addressed this issue interms of optimal graph clustering problem formulated by a quadratic IntegerProgram (IP). However, it has been realized that solution to the IP iscomputationally hard and thus heuristic based methods are required to get anear optimal solution. This paper presents a Genetic Algorithm (GA) for optimalclustering with an objective of maximizing intra-cluster edges whereasminimizing the inter-cluster ones. The proposed algorithm relies on fitnessbased parent selection and cross-overing cluster elements to reach an optimalsolution step by step. The scheme was implemented and tested against a set ofreal and synthetic data. The experimental results show that GA outperforms ourprevious works based on Greedy and Monte Carlo approaches by 40% and 49.5%.
arxiv-6900-172 | Autonomous requirements specification processing using natural language processing | http://arxiv.org/pdf/1407.6099v1.pdf | author:S. G. Macdonell, K. Min, A. M. Connor category:cs.CL cs.SE published:2014-07-23 summary:We describe our ongoing research that centres on the application of naturallanguage processing (NLP) to software engineering and systems developmentactivities. In particular, this paper addresses the use of NLP in therequirements analysis and systems design processes. We have developed aprototype toolset that can assist the systems analyst or software engineer toselect and verify terms relevant to a project. In this paper we describe theprocesses employed by the system to extract and classify objects of interestfrom requirements documents. These processes are illustrated using a smallexample.
arxiv-6900-173 | Stabilizing Sparse Cox Model using Clinical Structures in Electronic Medical Records | http://arxiv.org/pdf/1407.6094v1.pdf | author:Shivapratap Gopakumar, Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG published:2014-07-23 summary:Stability in clinical prediction models is crucial for transferabilitybetween studies, yet has received little attention. The problem is paramount inhigh dimensional data which invites sparse models with feature selectioncapability. We introduce an effective method to stabilize sparse Cox model oftime-to-events using clinical structures inherent in Electronic MedicalRecords. Model estimation is stabilized using a feature graph derived from twotypes of EMR structures: temporal structure of disease and interventionrecurrences, and hierarchical structure of medical knowledge and practices. Wedemonstrate the efficacy of the method in predicting time-to-readmission ofheart failure patients. On two stability measures - the Jaccard index and theConsistency index - the use of clinical structures significantly increasedfeature stability without hurting discriminative power. Our model reported acompetitive AUC of 0.64 (95% CIs: [0.58,0.69]) for 6 months prediction.
arxiv-6900-174 | Joint Energy-based Detection and Classificationon of Multilingual Text Lines | http://arxiv.org/pdf/1407.6082v1.pdf | author:Igor Milevskiy, Yuri Boykov category:cs.CV published:2014-07-23 summary:This paper proposes a new hierarchical MDL-based model for a joint detectionand classi?cation of multilingual text lines in im- ages taken by hand-heldcameras. The majority of related text detec- tion methods assume alphabet-basedwriting in a single language, e.g. in Latin. They use simple clusteringheuristics speci?c to such texts: prox- imity between letters within one line,larger distance between separate lines, etc. We are interested in asignificantly more ambiguous problem where images combine alphabet andlogographic characters from multiple languages and typographic rules vary a lot(e.g. English, Korean, and Chinese). Complexity of detecting and classifyingtext lines in multiple languages calls for a more principled approach based oninformation- theoretic principles. Our new MDL model includes data costscombining geometric errors with classi?cation likelihoods and a hierarchicalsparsity term based on label costs. This energy model can be e?cientlyminimized by fusion moves. We demonstrate robustness of the proposed algorithmon a large new database of multilingual text images collected in the pub- lictransit system of Seoul.
arxiv-6900-175 | The U-curve optimization problem: improvements on the original algorithm and time complexity analysis | http://arxiv.org/pdf/1407.6067v1.pdf | author:Marcelo S. Reis, Carlos E. Ferreira, Junior Barrera category:cs.LG cs.CV 68T10 I.5.2 published:2014-07-22 summary:The U-curve optimization problem is characterized by a decomposable inU-shaped curves cost function over the chains of a Boolean lattice. Thisproblem can be applied to model the classical feature selection problem inMachine Learning. Recently, the U-Curve algorithm was proposed to give optimalsolutions to the U-curve problem. In this article, we point out that theU-Curve algorithm is in fact suboptimal, and introduce the U-Curve-Search (UCS)algorithm, which is actually optimal. We also present the results of optimaland suboptimal experiments, in which UCS is compared with the UBB optimalbranch-and-bound algorithm and the SFFS heuristic, respectively. We show that,in both experiments, $\proc{UCS}$ had a better performance than its competitor.Finally, we analyze the obtained results and point out improvements on UCS thatmight enhance the performance of this algorithm.
arxiv-6900-176 | Modeling languages from graph networks | http://arxiv.org/pdf/1407.6027v1.pdf | author:Alberto Besana, Cristina Martínez category:cs.CL math.CO published:2014-07-22 summary:We model and compute the probability distribution of the letters in randomgenerated words in a language by using the theory of set partitions, Youngtableaux and graph theoretical representation methods. This has been ofinterest for several application areas such as network systems, bioinformatics,internet search, data mining and computacional linguistics.
arxiv-6900-177 | Detection of Sclerotic Spine Metastases via Random Aggregation of Deep Convolutional Neural Network Classifications | http://arxiv.org/pdf/1407.5976v1.pdf | author:Holger R. Roth, Jianhua Yao, Le Lu, James Stieger, Joseph E. Burns, Ronald M. Summers category:cs.CV published:2014-07-22 summary:Automated detection of sclerotic metastases (bone lesions) in ComputedTomography (CT) images has potential to be an important tool in clinicalpractice and research. State-of-the-art methods show performance of 79%sensitivity or true-positive (TP) rate, at 10 false-positives (FP) per volume.We design a two-tiered coarse-to-fine cascade framework to first operate ahighly sensitive candidate generation system at a maximum sensitivity of ~92%but with high FP level (~50 per patient). Regions of interest (ROI) for lesioncandidates are generated in this step and function as input for the secondtier. In the second tier we generate N 2D views, via scale, randomtranslations, and rotations with respect to each ROI centroid coordinates.These random views are used to train a deep Convolutional Neural Network (CNN)classifier. In testing, the CNN is employed to assign individual probabilitiesfor a new set of N random views that are averaged at each ROI to compute afinal per-candidate classification probability. This second tier behaves as ahighly selective process to reject difficult false positives while preservinghigh sensitivities. We validate the approach on CT images of 59 patients (49with sclerotic metastases and 10 normal controls). The proposed method reducesthe number of FP/vol. from 4 to 1.2, 7 to 3, and 12 to 9.5 when comparing asensitivity rates of 60%, 70%, and 80% respectively in testing. TheArea-Under-the-Curve (AUC) is 0.834. The results show marked improvement uponprevious work.
arxiv-6900-178 | Learning to Optimize via Information-Directed Sampling | http://arxiv.org/pdf/1403.5556v4.pdf | author:Daniel Russo, Benjamin Van Roy category:cs.LG published:2014-03-21 summary:We propose information-directed sampling -- a new algorithm for onlineoptimization problems in which a decision-maker must balance betweenexploration and exploitation while learning from partial feedback. Each actionis sampled in a manner that minimizes the ratio between squared expectedsingle-period regret and a measure of information gain: the mutual informationbetween the optimal action and the next observation. We establish an expected regret bound for information-directed sampling thatapplies across a very general class of models and scales with the entropy ofthe optimal action distribution. For the widely studied Bernoulli, Gaussian,and linear bandit problems, we demonstrate simulation performance surpassingpopular approaches, including upper confidence bound algorithms, Thompsonsampling, and the knowledge gradient algorithm. Further, we present simpleanalytic examples illustrating that, due to the way it measures informationgain, information-directed sampling can dramatically outperform upperconfidence bound algorithms and Thompson sampling.
arxiv-6900-179 | Cube-Cut: Vertebral Body Segmentation in MRI-Data through Cubic-Shaped Divergences | http://arxiv.org/pdf/1404.4467v2.pdf | author:Robert Schwarzenberg, Bernd Freisleben, Christopher Nimsky, Jan Egger category:cs.CV published:2014-04-17 summary:In this article, we present a graph-based method using a cubic template forvolumetric segmentation of vertebrae in magnetic resonance imaging (MRI)acquisitions. The user can define the degree of deviation from a regular cubevia a smoothness value Delta. The Cube-Cut algorithm generates a directed graphwith two terminal nodes (s-t-network), where the nodes of the graph correspondto a cubic-shaped subset of the image's voxels. The weightings of the graph'sterminal edges, which connect every node with a virtual source s or a virtualsink t, represent the affinity of a voxel to the vertebra (source) and to thebackground (sink). Furthermore, a set of infinite weighted and non-terminaledges implements the smoothness term. After graph construction, a minimals-t-cut is calculated within polynomial computation time, which splits thenodes into two disjoint units. Subsequently, the segmentation result isdetermined out of the source-set. A quantitative evaluation of a C++implementation of the algorithm resulted in an average Dice SimilarityCoefficient (DSC) of 81.33% and a running time of less than a minute.
arxiv-6900-180 | Axioms for graph clustering quality functions | http://arxiv.org/pdf/1308.3383v2.pdf | author:Twan van Laarhoven, Elena Marchiori category:cs.CV cs.LG stat.ML published:2013-08-15 summary:We investigate properties that intuitively ought to be satisfied by graphclustering quality functions, that is, functions that assign a score to aclustering of a graph. Graph clustering, also known as network communitydetection, is often performed by optimizing such a function. Two axiomstailored for graph clustering quality functions are introduced, and the fouraxioms introduced in previous work on distance based clustering arereformulated and generalized for the graph setting. We show that modularity, astandard quality function for graph clustering, does not satisfy all of thesesix properties. This motivates the derivation of a new family of qualityfunctions, adaptive scale modularity, which does satisfy the proposed axioms.Adaptive scale modularity has two parameters, which give greater flexibility inthe kinds of clusterings that can be found. Standard graph clustering qualityfunctions, such as normalized cut and unnormalized cut, are obtained as specialcases of adaptive scale modularity. In general, the results of our investigation indicate that the consideredaxiomatic framework covers existing `good' quality functions for graphclustering, and can be used to derive an interesting new family of qualityfunctions.
arxiv-6900-181 | Resolution-limit-free and local Non-negative Matrix Factorization quality functions for graph clustering | http://arxiv.org/pdf/1407.5924v1.pdf | author:Twan van Laarhoven, Elena Marchiori category:stat.ML published:2014-07-22 summary:Many graph clustering quality functions suffer from a resolution limit, theinability to find small clusters in large graphs. So calledresolution-limit-free quality functions do not have this limit. This propertywas previously introduced for hard clustering, that is, graph partitioning. We investigate the resolution-limit-free property in the context ofNon-negative Matrix Factorization (NMF) for hard and soft graph clustering. Touse NMF in the hard clustering setting, a common approach is to assign eachnode to its highest membership cluster. We show that in this case symmetric NMFis not resolution-limit-free, but that it becomes so when hardness constraintsare used as part of the optimization. The resulting function is strongly linkedto the Constant Potts Model. In soft clustering, nodes can belong to more thanone cluster, with varying degrees of membership. In this settingresolution-limit-free turns out to be too strong a property. Therefore weintroduce locality, which roughly states that changing one part of the graphdoes not affect the clustering of other parts of the graph. We argue that thisis a desirable property, provide conditions under which NMF quality functionsare local, and propose a novel class of local probabilistic NMF qualityfunctions for soft graph clustering.
arxiv-6900-182 | Confidence-Based Dynamic Classifier Combination For Mean-Shift Tracking | http://arxiv.org/pdf/1107.5850v2.pdf | author:Ibrahim Saygin Topkaya, Hakan Erdogan category:cs.CV I.4.8 published:2011-07-29 summary:We introduce a novel tracking technique which uses dynamic confidence-basedfusion of two different information sources for robust and efficient trackingof visual objects. Mean-shift tracking is a popular and well known method usedin object tracking problems. Originally, the algorithm uses a similaritymeasure which is optimized by shifting a search area to the center of agenerated weight image to track objects. Recent improvements on the originalmean-shift algorithm involves using a classifier that differentiates the objectfrom its surroundings. We adopt this classifier-based approach and propose anapplication of a classifier fusion technique within this classifier-basedcontext in this work. We use two different classifiers, where one comes from abackground modeling method, to generate the weight image and we calculatecontributions of the classifiers dynamically using their confidences togenerate a final weight image to be used in tracking. The contributions of theclassifiers are calculated by using correlations between histograms of theirweight images and histogram of a defined ideal weight image in the previousframe. We show with experiments that our dynamic combination scheme selectsgood contributions for classifiers for different cases and improves trackingaccuracy significantly.
arxiv-6900-183 | How good are detection proposals, really? | http://arxiv.org/pdf/1406.6962v2.pdf | author:Jan Hosang, Rodrigo Benenson, Bernt Schiele category:cs.CV published:2014-06-26 summary:Current top performing Pascal VOC object detectors employ detection proposalsto guide the search for objects thereby avoiding exhaustive sliding windowsearch across images. Despite the popularity of detection proposals, it isunclear which trade-offs are made when using them during object detection. Weprovide an in depth analysis of ten object proposal methods along with fourbaselines regarding ground truth annotation recall (on Pascal VOC 2007 andImageNet 2013), repeatability, and impact on DPM detector performance. Ourfindings show common weaknesses of existing methods, and provide insights tochoose the most adequate method for different settings.
arxiv-6900-184 | Approximate Regularization Path for Nuclear Norm Based H2 Model Reduction | http://arxiv.org/pdf/1407.5820v1.pdf | author:Niclas Blomberg, Cristian R. Rojas, Bo Wahlberg category:cs.SY math.OC stat.ML published:2014-07-22 summary:This paper concerns model reduction of dynamical systems using the nuclearnorm of the Hankel matrix to make a trade-off between model fit and modelcomplexity. This results in a convex optimization problem where this trade-offis determined by one crucial design parameter. The main contribution is amethodology to approximately calculate all solutions up to a certain toleranceto the model reduction problem as a function of the design parameter. This iscalled the regularization path in sparse estimation and is a very importanttool in order to find the appropriate balance between fit and complexity. Weextend this to the more complicated nuclear norm case. The key idea is todetermine when to exactly calculate the optimal solution using an upper boundbased on the so-called duality gap. Hence, by solving a fixed number ofoptimization problems the whole regularization path up to a given tolerance canbe efficiently computed. We illustrate this approach on some numericalexamples.
arxiv-6900-185 | Multi-agents adaptive estimation and coverage control using Gaussian regression | http://arxiv.org/pdf/1407.5807v1.pdf | author:Andrea Carron, Marco Todescato, Ruggero Carli, Luca Schenato, Gianluigi Pillonetto category:cs.MA cs.SY stat.ML published:2014-07-22 summary:We consider a scenario where the aim of a group of agents is to perform theoptimal coverage of a region according to a sensory function. In particular,centroidal Voronoi partitions have to be computed. The difficulty of the taskis that the sensory function is unknown and has to be reconstructed on linefrom noisy measurements. Hence, estimation and coverage needs to be performedat the same time. We cast the problem in a Bayesian regression framework, wherethe sensory function is seen as a Gaussian random field. Then, we design a setof control inputs which try to well balance coverage and estimation, alsodiscussing convergence properties of the algorithm. Numerical experiments showthe effectivness of the new approach.
arxiv-6900-186 | Optimal designs for Lasso and Dantzig selector using Expander Codes | http://arxiv.org/pdf/1010.2457v6.pdf | author:Yohann de Castro category:math.ST cs.IT math.IT math.PR stat.ME stat.ML stat.TH published:2010-10-12 summary:We investigate the high-dimensional regression problem using adjacencymatrices of unbalanced expander graphs. In this frame, we prove that the$\ell_{2}$-prediction error and the $\ell_{1}$-risk of the lasso and theDantzig selector are optimal up to an explicit multiplicative constant. Thus wecan estimate a high-dimensional target vector with an error term similar to theone obtained in a situation where one knows the support of the largestcoordinates in advance. Moreover, we show that these design matrices have an explicit restrictedeigenvalue. Precisely, they satisfy the restricted eigenvalue assumption andthe compatibility condition with an explicit constant. Eventually, we capitalize on the recent construction of unbalanced expandergraphs due to Guruswami, Umans, and Vadhan, to provide a deterministicpolynomial time construction of these design matrices.
arxiv-6900-187 | Aggregation of local parametric candidates with exemplar-based occlusion handling for optical flow | http://arxiv.org/pdf/1407.5759v1.pdf | author:Denis Fortun, Patrick Bouthemy, Charles Kervrann category:cs.CV published:2014-07-22 summary:Handling all together large displacements, motion details and occlusionsremains an open issue for reliable computation of optical flow in a videosequence. We propose a two-step aggregation paradigm to address this problem.The idea is to supply local motion candidates at every pixel in a first step,and then to combine them to determine the global optical flow field in a secondstep. We exploit local parametric estimations combined with patchcorrespondences and we experimentally demonstrate that they are sufficient toproduce highly accurate motion candidates. The aggregation step is designed asthe discrete optimization of a global regularized energy. The occlusion map isestimated jointly with the flow field throughout the two steps. We propose ageneric exemplar-based approach for occlusion filling with motion vectors. Weachieve state-of-the-art results in computer vision benchmarks, withparticularly significant improvements in the case of large displacements andocclusions.
arxiv-6900-188 | Tree-based iterated local search for Markov random fields with applications in image analysis | http://arxiv.org/pdf/1407.5754v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:cs.AI cs.CV math.OC published:2014-07-22 summary:The \emph{maximum a posteriori} (MAP) assignment for general structure Markovrandom fields (MRFs) is computationally intractable. In this paper, we exploittree-based methods to efficiently address this problem. Our novel method, namedTree-based Iterated Local Search (T-ILS) takes advantage of the tractability oftree-structures embedded within MRFs to derive strong local search in an ILSframework. The method efficiently explores exponentially large neighborhood anddoes so with limited memory without any requirement on the cost functions. Weevaluate the T-ILS in a simulation of Ising model and two real-world problemsin computer vision: stereo matching, image denoising. Experimental resultsdemonstrate that our methods are competitive against state-of-the-art rivalswith a significant computational gain.
arxiv-6900-189 | Improved Onlooker Bee Phase in Artificial Bee Colony Algorithm | http://arxiv.org/pdf/1407.5753v1.pdf | author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.NE published:2014-07-22 summary:Artificial Bee Colony (ABC) is a distinguished optimization strategy that canresolve nonlinear and multifaceted problems. It is comparatively astraightforward and modern population based probabilistic approach forcomprehensive optimization. In the vein of the other population basedalgorithms, ABC is moreover computationally classy due to its slow nature ofsearch procedure. The solution exploration equation of ABC is extensivelyinfluenced by a arbitrary quantity which helps in exploration at the cost ofexploitation of the better search space. In the solution exploration equationof ABC due to the outsized step size the chance of skipping the factualsolution is high. Therefore, here this paper improve onlooker bee phase withhelp of a local search strategy inspired by memetic algorithm to balance thediversity and convergence capability of the ABC. The proposed algorithm isnamed as Improved Onlooker Bee Phase in ABC (IoABC). It is tested over 12 wellknown un-biased test problems of diverse complexities and two engineeringoptimization problems; results show that the anticipated algorithm go onebetter than the basic ABC and its recent deviations in a good number of theexperiments.
arxiv-6900-190 | Global optimization using Lévy flights | http://arxiv.org/pdf/1407.5739v1.pdf | author:Truyen Tran, Trung Thanh Nguyen, Hoang Linh Nguyen category:cs.NE published:2014-07-22 summary:This paper studies a class of enhanced diffusion processes in which randomwalkers perform L\'evy flights and apply it for global optimization. L\'evyflights offer controlled balance between exploitation and exploration. Wedevelop four optimization algorithms based on such properties. We compare newalgorithms with the well-known Simulated Annealing on hard test functions andthe results are very promising.
arxiv-6900-191 | Learning Rich Features from RGB-D Images for Object Detection and Segmentation | http://arxiv.org/pdf/1407.5736v1.pdf | author:Saurabh Gupta, Ross Girshick, Pablo Arbeláez, Jitendra Malik category:cs.CV cs.RO published:2014-07-22 summary:In this paper we study the problem of object detection for RGB-D images usingsemantically rich image and depth features. We propose a new geocentricembedding for depth images that encodes height above ground and angle withgravity for each pixel in addition to the horizontal disparity. We demonstratethat this geocentric embedding works better than using raw depth images forlearning feature representations with convolutional neural networks. Our finalobject detection system achieves an average precision of 37.3%, which is a 56%relative improvement over existing methods. We then focus on the task ofinstance segmentation where we label pixels belonging to object instances foundby our detector. For this task, we propose a decision forest approach thatclassifies pixels in the detection window as foreground or background using afamily of unary and binary tests that query shape and geocentric pose features.Finally, we use the output from our object detectors in an existing superpixelclassification framework for semantic scene segmentation and achieve a 24%relative improvement over current state-of-the-art for the object categoriesthat we study. We believe advances such as those represented in this paper willfacilitate the use of perception in fields like robotics.
arxiv-6900-192 | Artificial Life and the Web: WebAL Comes of Age | http://arxiv.org/pdf/1407.5719v1.pdf | author:Tim Taylor category:cs.NE cs.MA published:2014-07-22 summary:A brief survey is presented of the first 18 years of web-based ArtificialLife ("WebAL") research and applications, covering the period 1995-2013. Thesurvey is followed by a short discussion of common methodologies employed andcurrent technologies relevant to WebAL research. The paper concludes with aquick look at what the future may hold for work in this exciting area.
arxiv-6900-193 | Multichannel Compressive Sensing MRI Using Noiselet Encoding | http://arxiv.org/pdf/1407.5536v2.pdf | author:Kamlesh Pawar, Gary F. Egan, Jingxin Zhang category:physics.med-ph cs.CV published:2014-07-21 summary:The incoherence between measurement and sparsifying transform matrices andthe restricted isometry property (RIP) of measurement matrix are two of the keyfactors in determining the performance of compressive sensing (CS). In CS-MRI,the randomly under-sampled Fourier matrix is used as the measurement matrix andthe wavelet transform is usually used as sparsifying transform matrix. However,the incoherence between the randomly under-sampled Fourier matrix and thewavelet matrix is not optimal, which can deteriorate the performance of CS-MRI.Using the mathematical result that noiselets are maximally incoherent withwavelets, this paper introduces the noiselet unitary bases as the measurementmatrix to improve the incoherence and RIP in CS-MRI, and presents a method todesign the pulse sequence for the noiselet encoding. This novel encoding schemeis combined with the multichannel compressive sensing (MCS) framework to takethe advantage of multichannel data acquisition used in MRI scanners. Anempirical RIP analysis is presented to compare the multichannel noiselet andmultichannel Fourier measurement matrices in MCS. Simulations are presented inthe MCS framework to compare the performance of noiselet encodingreconstructions and Fourier encoding reconstructions at different accelerationfactors. The comparisons indicate that multichannel noiselet measurement matrixhas better RIP than that of its Fourier counterpart, and that noiselet encodedMCS-MRI outperforms Fourier encoded MCS-MRI in preserving image resolution andcan achieve higher acceleration factors. To demonstrate the feasibility of theproposed noiselet encoding scheme, two pulse sequences with tailored spatiallyselective RF excitation pulses was designed and implemented on a 3T scanner toacquire the data in the noiselet domain from a phantom and a human brain.
arxiv-6900-194 | Shape Primitive Histogram: A Novel Low-Level Face Representation for Face Recognition | http://arxiv.org/pdf/1312.7446v3.pdf | author:Sheng Huang, Dan Yang, Haopeng Zhang, Luwen Huangfu, Xiaohong Zhang category:cs.CV published:2013-12-28 summary:We further exploit the representational power of Haar wavelet and present anovel low-level face representation named Shape Primitives Histogram (SPH) forface recognition. Since human faces exist abundant shape features, we addressthe face representation issue from the perspective of the shape featureextraction. In our approach, we divide faces into a number of tiny shapefragments and reduce these shape fragments to several uniform atomic shapepatterns called Shape Primitives. A convolution with Haar Wavelet templates isapplied to each shape fragment to identify its belonging shape primitive. Afterthat, we do a histogram statistic of shape primitives in each spatial localimage patch for incorporating the spatial information. Finally, each face isrepresented as a feature vector via concatenating all the local histograms ofshape primitives. Four popular face databases, namely ORL, AR, Yale-B and LFW-adatabases, are employed to evaluate SPH and experimentally study the choices ofthe parameters. Extensive experimental results demonstrate that the proposedapproach outperform the state-of-the-arts.
arxiv-6900-195 | A metric for software vulnerabilities classification | http://arxiv.org/pdf/1212.3669v2.pdf | author:Gabriele Modena category:cs.SE cs.LG published:2012-12-15 summary:Vulnerability discovery and exploits detection are two wide areas of study insoftware engineering. This preliminary work tries to combine existing methodswith machine learning techniques to define a metric classification ofvulnerable computer programs. First a feature set has been defined and latertwo models have been tested against real world vulnerabilities. A relationbetween the classifier choice and the features has also been outlined.
arxiv-6900-196 | Predictive support recovery with TV-Elastic Net penalty and logistic regression: an application to structural MRI | http://arxiv.org/pdf/1407.5602v1.pdf | author:Mathieu Dubois, Fouad Hadj-Selem, Tommy Lofstedt, Matthieu Perrot, Clara Fischer, Vincent Frouin, Edouard Duchesnay category:stat.ML published:2014-07-21 summary:The use of machine-learning in neuroimaging offers new perspectives in earlydiagnosis and prognosis of brain diseases. Although such multivariate methodscan capture complex relationships in the data, traditional approaches provideirregular (l2 penalty) or scattered (l1 penalty) predictive pattern with a verylimited relevance. A penalty like Total Variation (TV) that exploits thenatural 3D structure of the images can increase the spatial coherence of theweight map. However, TV penalization leads to non-smooth optimization problemsthat are hard to minimize. We propose an optimization framework that minimizesany combination of l1, l2, and TV penalties while preserving the exact l1penalty. This algorithm uses Nesterov's smoothing technique to approximate theTV penalty with a smooth function such that the loss and the penalties areminimized with an exact accelerated proximal gradient algorithm. We propose anoriginal continuation algorithm that uses successively smaller values of thesmoothing parameter to reach a prescribed precision while achieving the bestpossible convergence rate. This algorithm can be used with other losses orpenalties. The algorithm is applied on a classification problem on the ADNIdataset. We observe that the TV penalty does not necessarily improve theprediction but provides a major breakthrough in terms of support recovery ofthe predictive brain regions.
arxiv-6900-197 | A Novel Hybrid Crossover based Artificial Bee Colony Algorithm for Optimization Problem | http://arxiv.org/pdf/1407.5574v1.pdf | author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.AI cs.NE published:2014-07-21 summary:Artificial bee colony (ABC) algorithm has proved its importance in solving anumber of problems including engineering optimization problems. ABC algorithmis one of the most popular and youngest member of the family of populationbased nature inspired meta-heuristic swarm intelligence method. ABC has beenproved its superiority over some other Nature Inspired Algorithms (NIA) whenapplied for both benchmark functions and real world problems. The performanceof search process of ABC depends on a random value which tries to balanceexploration and exploitation phase. In order to increase the performance it isrequired to balance the exploration of search space and exploitation of optimalsolution of the ABC. This paper outlines a new hybrid of ABC algorithm withGenetic Algorithm. The proposed method integrates crossover operation fromGenetic Algorithm (GA) with original ABC algorithm. The proposed method isnamed as Crossover based ABC (CbABC). The CbABC strengthens the exploitationphase of ABC as crossover enhances exploration of search space. The CbABCtested over four standard benchmark functions and a popular continuousoptimization problem.
arxiv-6900-198 | Statistical inference on errorfully observed graphs | http://arxiv.org/pdf/1211.3601v4.pdf | author:Carey E. Priebe, Daniel L. Sussman, Minh Tang, Joshua T. Vogelstein category:stat.ML published:2012-11-15 summary:Statistical inference on graphs is a burgeoning field in the applied andtheoretical statistics communities, as well as throughout the wider world ofscience, engineering, business, etc. In many applications, we are faced withthe reality of errorfully observed graphs. That is, the existence of an edgebetween two vertices is based on some imperfect assessment. In this paper, weconsider a graph $G = (V,E)$. We wish to perform an inference task -- theinference task considered here is "vertex classification". However, we do notobserve $G$; rather, for each potential edge $uv \in {{V}\choose{2}}$ weobserve an "edge-feature" which we use to classify $uv$ as edge/not-edge. Thuswe errorfully observe $G$ when we observe the graph $\widetilde{G} =(V,\widetilde{E})$ as the edges in $\widetilde{E}$ arise from theclassifications of the "edge-features", and are expected to be errorful.Moreover, we face a quantity/quality trade-off regarding the edge-features weobserve -- more informative edge-features are more expensive, and hence thenumber of potential edges that can be assessed decreases with the quality ofthe edge-features. We studied this problem by formulating a quantity/qualitytradeoff for a simple class of random graphs model, namely the stochasticblockmodel. We then consider a simple but optimal vertex classifier forclassifying $v$ and we derive the optimal quantity/quality operating point forsubsequent graph inference in the face of this trade-off. The optimal operatingpoints for the quantity/quality trade-off are surprising and illustrate theissue that methods for intermediate tasks should be chosen to maximizeperformance for the ultimate inference task. Finally, we investigate thequantity/quality tradeoff for errorful obesrvations of the {\it C.\ elegans}connectome graph.
arxiv-6900-199 | Completing Any Low-rank Matrix, Provably | http://arxiv.org/pdf/1306.2979v4.pdf | author:Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, Rachel Ward category:stat.ML cs.IT cs.LG math.IT published:2013-06-12 summary:Matrix completion, i.e., the exact and provable recovery of a low-rank matrixfrom a small subset of its elements, is currently only known to be possible ifthe matrix satisfies a restrictive structural constraint---known as {\emincoherence}---on its row and column spaces. In these cases, the subset ofelements is sampled uniformly at random. In this paper, we show that {\em any} rank-$ r $ $ n$-by-$ n $ matrix can beexactly recovered from as few as $O(nr \log^2 n)$ randomly chosen elements,provided this random choice is made according to a {\em specific biaseddistribution}: the probability of any element being sampled should beproportional to the sum of the leverage scores of the corresponding row, andcolumn. Perhaps equally important, we show that this specific form of samplingis nearly necessary, in a natural precise sense; this implies that otherperhaps more intuitive sampling schemes fail. We further establish three ways to use the above result for the setting whenleverage scores are not known \textit{a priori}: (a) a sampling strategy forthe case when only one of the row or column spaces are incoherent, (b) atwo-phase sampling procedure for general matrices that first samples toestimate leverage scores followed by sampling for exact recovery, and (c) ananalysis showing the advantages of weighted nuclear/trace-norm minimizationover the vanilla un-weighted formulation for the case of non-uniform sampling.
arxiv-6900-200 | Are There Good Mistakes? A Theoretical Analysis of CEGIS | http://arxiv.org/pdf/1407.5397v1.pdf | author:Susmit Jha, Sanjit A. Seshia category:cs.LO cs.AI cs.LG cs.PL published:2014-07-21 summary:Counterexample-guided inductive synthesis CEGIS is used to synthesizeprograms from a candidate space of programs. The technique is guaranteed toterminate and synthesize the correct program if the space of candidate programsis finite. But the technique may or may not terminate with the correct programif the candidate space of programs is infinite. In this paper, we perform atheoretical analysis of counterexample-guided inductive synthesis technique. Weinvestigate whether the set of candidate spaces for which the correct programcan be synthesized using CEGIS depends on the counterexamples used in inductivesynthesis, that is, whether there are good mistakes which would increase thesynthesis power. We investigate whether the use of minimal counterexamplesinstead of arbitrary counterexamples expands the set of candidate spaces ofprograms for which inductive synthesis can successfully synthesize a correctprogram. We consider two kinds of counterexamples: minimal counterexamples andhistory bounded counterexamples. The history bounded counterexample used in anyiteration of CEGIS is bounded by the examples used in previous iterations ofinductive synthesis. We examine the relative change in power of inductivesynthesis in both cases. We show that the synthesis technique using minimalcounterexamples MinCEGIS has the same synthesis power as CEGIS but thesynthesis technique using history bounded counterexamples HCEGIS has differentpower than that of CEGIS, but none dominates the other.
arxiv-6900-201 | Impact of regularization on Spectral Clustering | http://arxiv.org/pdf/1312.1733v2.pdf | author:Antony Joseph, Bin Yu category:stat.ML published:2013-12-05 summary:The performance of spectral clustering can be considerably improved viaregularization, as demonstrated empirically in Amini et. al (2012). Here, weprovide an attempt at quantifying this improvement through theoreticalanalysis. Under the stochastic block model (SBM), and its extensions, previousresults on spectral clustering relied on the minimum degree of the graph beingsufficiently large for its good performance. By examining the scenario wherethe regularization parameter $\tau$ is large we show that the minimum degreeassumption can potentially be removed. As a special case, for an SBM with twoblocks, the results require the maximum degree to be large (grow faster than$\log n$) as opposed to the minimum degree. More importantly, we show the usefulness of regularization in situationswhere not all nodes belong to well-defined clusters. Our results rely on a`bias-variance'-like trade-off that arises from understanding the concentrationof the sample Laplacian and the eigen gap as a function of the regularizationparameter. As a byproduct of our bounds, we propose a data-driven technique\textit{DKest} (standing for estimated Davis-Kahan bounds) for choosing theregularization parameter. This technique is shown to work well throughsimulations and on a real data set.
arxiv-6900-202 | Certifying the Existence of Epipolar Matrices | http://arxiv.org/pdf/1407.5367v1.pdf | author:Sameer Agarwal, Hon-leung Lee, Bernd Sturmfels, Rekha R. Thomas category:cs.CV math.AG published:2014-07-21 summary:Given a set of point correspondences in two images, the existence of afundamental matrix is a necessary condition for the points to be the images ofa 3-dimensional scene imaged with two pinhole cameras. If the cameracalibration is known then one requires the existence of an essential matrix. We present an efficient algorithm, using exact linear algebra, for testingthe existence of a fundamental matrix. The input is any number of pointcorrespondences. For essential matrices, we characterize the solvability of theDemazure polynomials. In both scenarios, we determine which linear subspacesintersect a fixed set defined by non-linear polynomials. The conditions wederive are polynomials stated purely in terms of image coordinates. Theyrepresent a new class of two-view invariants, free of fundamental(resp.~essential)~matrices.
arxiv-6900-203 | Practical Kernel-Based Reinforcement Learning | http://arxiv.org/pdf/1407.5358v1.pdf | author:André M. S. Barreto, Doina Precup, Joelle Pineau category:cs.LG cs.AI stat.ML published:2014-07-21 summary:Kernel-based reinforcement learning (KBRL) stands out among reinforcementlearning algorithms for its strong theoretical guarantees. By casting thelearning problem as a local kernel approximation, KBRL provides a way ofcomputing a decision policy which is statistically consistent and converges toa unique solution. Unfortunately, the model constructed by KBRL grows with thenumber of sample transitions, resulting in a computational cost that precludesits application to large-scale or on-line domains. In this paper we introducean algorithm that turns KBRL into a practical reinforcement learning tool.Kernel-based stochastic factorization (KBSF) builds on a simple idea: when atransition matrix is represented as the product of two stochastic matrices, onecan swap the factors of the multiplication to obtain another transition matrix,potentially much smaller, which retains some fundamental properties of itsprecursor. KBSF exploits such an insight to compress the information containedin KBRL's model into an approximator of fixed size. This makes it possible tobuild an approximation that takes into account both the difficulty of theproblem and the associated computational cost. KBSF's computational complexityis linear in the number of sample transitions, which is the best one can dowithout discarding data. Moreover, the algorithm's simple mechanics allow for afully incremental implementation that makes the amount of memory usedindependent of the number of sample transitions. The result is a kernel-basedreinforcement learning algorithm that can be applied to large-scale problems inboth off-line and on-line regimes. We derive upper bounds for the distancebetween the value functions computed by KBRL and KBSF using the same data. Wealso illustrate the potential of our algorithm in an extensive empirical studyin which KBSF is applied to difficult tasks based on real-world data.
arxiv-6900-204 | Robust Spectral Compressed Sensing via Structured Matrix Completion | http://arxiv.org/pdf/1304.8126v5.pdf | author:Yuxin Chen, Yuejie Chi category:cs.IT cs.SY math.IT math.NA stat.ML published:2013-04-30 summary:The paper explores the problem of \emph{spectral compressed sensing}, whichaims to recover a spectrally sparse signal from a small random subset of its$n$ time domain samples. The signal of interest is assumed to be asuperposition of $r$ multi-dimensional complex sinusoids, while the underlyingfrequencies can assume any \emph{continuous} values in the normalized frequencydomain. Conventional compressed sensing paradigms suffer from the basismismatch issue when imposing a discrete dictionary on the Fourierrepresentation. To address this issue, we develop a novel algorithm, called\emph{Enhanced Matrix Completion (EMaC)}, based on structured matrix completionthat does not require prior knowledge of the model order. The algorithm startsby arranging the data into a low-rank enhanced form exhibiting multi-foldHankel structure, and then attempts recovery via nuclear norm minimization.Under mild incoherence conditions, EMaC allows perfect recovery as soon as thenumber of samples exceeds the order of $r\log^{4}n$, and is stable againstbounded noise. Even if a constant portion of samples are corrupted witharbitrary magnitude, EMaC still allows exact recovery, provided that the samplecomplexity exceeds the order of $r^{2}\log^{3}n$. Along the way, our resultsdemonstrate the power of convex relaxation in completing a low-rank multi-foldHankel or Toeplitz matrix from minimal observed entries. The performance of ouralgorithm and its applicability to super resolution are further validated bynumerical experiments.
arxiv-6900-205 | Efficient Learning and Planning with Compressed Predictive States | http://arxiv.org/pdf/1312.0286v2.pdf | author:William L. Hamilton, Mahdi Milani Fard, Joelle Pineau category:cs.LG stat.ML published:2013-12-01 summary:Predictive state representations (PSRs) offer an expressive framework formodelling partially observable systems. By compactly representing systems asfunctions of observable quantities, the PSR learning approach avoids usinglocal-minima prone expectation-maximization and instead employs a globallyoptimal moment-based algorithm. Moreover, since PSRs do not require apredetermined latent state structure as an input, they offer an attractiveframework for model-based reinforcement learning when agents must plan withouta priori access to a system model. Unfortunately, the expressiveness of PSRscomes with significant computational cost, and this cost is a major factorinhibiting the use of PSRs in applications. In order to alleviate thisshortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSRlearning approach combines recent advancements in dimensionality reduction,incremental matrix decomposition, and compressed sensing. We show how thisapproach provides a principled avenue for learning accurate approximations ofPSRs, drastically reducing the computational costs associated with learningwhile also providing effective regularization. Going further, we propose aplanning framework which exploits these learned models. And we show that thisapproach facilitates model-learning and planning in large complex partiallyobservable domains, a task that is infeasible without the principled use ofcompression.
arxiv-6900-206 | A Comparative Analysis for Determining the Optimal Path using PSO and GA | http://arxiv.org/pdf/1407.5327v1.pdf | author:Kavitha Sooda, T. R. Gopalakrishnan Nair category:cs.NI cs.NE published:2014-07-20 summary:Significant research has been carried out recently to find the optimal pathin network routing. Among them, the evolutionary algorithm approach is an areawhere work is carried out extensively. We in this paper have used particleswarm optimization (PSO) and genetic algorithm (GA) for finding the optimalpath and the concept of region based network is introduced along with the useof indirect encoding. We demonstrate the advantage of fitness value and hopcount in both PSO and GA. A comparative study of PSO and genetic algorithm (GA)is carried out, and it was found that PSO converged to arrive at the optimalpath much faster than GA.
arxiv-6900-207 | Optimized Method for Iranian Road Signs Detection and recognition system | http://arxiv.org/pdf/1407.5324v1.pdf | author:Reza Azad, Babak Azad, Iman Tavakoli Kazerooni category:cs.CV published:2014-07-20 summary:Road sign recognition is one of the core technologies in IntelligentTransport Systems. In the current study, a robust and real-time method ispresented to identify and detect the roads speed signs in road image indifferent situations. In our proposed method, first, the connected componentsare created in the main image using the edge detection and mathematicalmorphology and the location of the road signs extracted by the geometric andcolor data; then the letters are segmented and recognized by Multiclass SupportVector Machine (SVMs) classifiers. Regarding that the geometric and colorfeatures ate properly used in detection the location of the road signs, so itis not sensitive to the distance and noise and has higher speed and efficiency.In the result part, the proposed approach is applied on Iranian road speed signdatabase and the detection and recognition accuracy rate achieved 98.66% and100% respectively.
arxiv-6900-208 | Object Proposal Generation using Two-Stage Cascade SVMs | http://arxiv.org/pdf/1407.5242v1.pdf | author:Ziming Zhang, Philip H. S. Torr category:cs.CV published:2014-07-20 summary:Object proposal algorithms have shown great promise as a first step forobject recognition and detection. Good object proposal generation algorithmsrequire high object recall rate as well as low computational cost, becausegenerating object proposals is usually utilized as a preprocessing step. Theproblem of how to accelerate the object proposal generation and evaluationprocess without decreasing recall is thus of great interest. In this paper, wepropose a new object proposal generation method using two-stage cascade SVMs,where in the first stage linear filters are learned for predefined quantizedscales/aspect-ratios independently, and in the second stage a global linearclassifier is learned across all the quantized scales/aspect-ratios forcalibration, so that all the proposals can be compared properly. The proposalswith highest scores are our final output. Specifically, we explain ourscale/aspect-ratio quantization scheme, and investigate the effects ofcombinations of $\ell_1$ and $\ell_2$ regularizers in cascade SVMs with/withoutranking constraints in learning. Comprehensive experiments on VOC2007 datasetare conducted, and our results achieve the state-of-the-art performance withhigh object recall rate and high computational efficiency. Besides, our methodhas been demonstrated to be suitable for not only class-specific but alsogeneric object proposal generation.
arxiv-6900-209 | A fast and robust algorithm to count topologically persistent holes in noisy clouds | http://arxiv.org/pdf/1312.1492v3.pdf | author:Vitaliy Kurlin category:cs.CG cs.CV math.AT published:2013-12-05 summary:Preprocessing a 2D image often produces a noisy cloud of interest points. Westudy the problem of counting holes in unorganized clouds in the plane. Theholes in a given cloud are quantified by the topological persistence of theirboundary contours when the cloud is analyzed at all possible scales. We designthe algorithm to count holes that are most persistent in the filtration ofoffsets (neighborhoods) around given points. The input is a cloud of $n$ pointsin the plane without any user-defined parameters. The algorithm has $O(n\logn)$ time and $O(n)$ space. The output is the array (number of holes, relativepersistence in the filtration). We prove theoretical guarantees when thealgorithm finds the correct number of holes (components in the complement) ofan unknown shape approximated by a cloud.
arxiv-6900-210 | Context Aware Dynamic Traffic Signal Optimization | http://arxiv.org/pdf/1407.5212v1.pdf | author:Kandarp Khandwala, Rudra Sharma, Snehal Rao category:cs.AI cs.NE published:2014-07-19 summary:Conventional urban traffic control systems have been based on historicaltraffic data. Later advancements made use of detectors, which enabled thegathering of real time traffic data, in order to reorganize and calibratetraffic signalization programs. Further evolvement provided the ability toforecast traffic conditions, in order to develop traffic signalization programsand strategies precomputed and applied at the most appropriate time frame forthe optimal control of the current traffic conditions. We, propose the nextgeneration of traffic control systems based on principles of ArtificialIntelligence and Context Awareness. Most of the existing algorithms use averagewaiting time or length of the queue to assess an algorithms performance.However, a low average waiting time may come at the cost of delaying othervehicles indefinitely. In our algorithm, besides the vehicle queue, we usefairness also as an important performance metric to assess an algorithmsperformance.
arxiv-6900-211 | A machine-compiled macroevolutionary history of Phanerozoic life | http://arxiv.org/pdf/1406.2963v2.pdf | author:Shanan E. Peters, Ce Zhang, Miron Livny, Christopher Ré category:cs.DB cs.CL cs.LG q-bio.PE published:2014-06-11 summary:Many aspects of macroevolutionary theory and our understanding of bioticresponses to global environmental change derive from literature-basedcompilations of palaeontological data. Existing manually assembled databasesare, however, incomplete and difficult to assess and enhance. Here, we developand validate the quality of a machine reading system, PaleoDeepDive, thatautomatically locates and extracts data from heterogeneous text, tables, andfigures in publications. PaleoDeepDive performs comparably to humans in complexdata extraction and inference tasks and generates congruent syntheticmacroevolutionary results. Unlike traditional databases, PaleoDeepDive producesa probabilistic database that systematically improves as information is added.We also show that the system can readily accommodate sophisticated data types,such as morphological data in biological illustrations and associated textualdescriptions. Our machine reading approach to scientific data integration andsynthesis brings within reach many questions that are currently underdeterminedand does so in ways that may stimulate entirely new modes of inquiry.
arxiv-6900-212 | Exploiting Smoothness in Statistical Learning, Sequential Prediction, and Stochastic Optimization | http://arxiv.org/pdf/1407.5908v1.pdf | author:Mehrdad Mahdavi category:cs.LG published:2014-07-19 summary:In the last several years, the intimate connection between convexoptimization and learning problems, in both statistical and sequentialframeworks, has shifted the focus of algorithmic machine learning to examinethis interplay. In particular, on one hand, this intertwinement brings forwardnew challenges in reassessment of the performance of learning algorithmsincluding generalization and regret bounds under the assumptions imposed byconvexity such as analytical properties of loss functions (e.g., Lipschitzness,strong convexity, and smoothness). On the other hand, emergence of datasets ofan unprecedented size, demands the development of novel and more efficientoptimization algorithms to tackle large-scale learning problems. The overarching goal of this thesis is to reassess the smoothness of lossfunctions in statistical learning, sequential prediction/online learning, andstochastic optimization and explicate its consequences. In particular weexamine how smoothness of loss function could be beneficial or detrimental inthese settings in terms of sample complexity, statistical consistency, regretanalysis, and convergence rate, and investigate how smoothness can be leveragedto devise more efficient learning algorithms.
arxiv-6900-213 | On the minimal teaching sets of two-dimensional threshold functions | http://arxiv.org/pdf/1307.1058v2.pdf | author:Max A. Alekseyev, Marina G. Basova, Nikolai Yu. Zolotykh category:math.CO cs.LG math.NT published:2013-07-03 summary:It is known that a minimal teaching set of any threshold function on thetwodimensional rectangular grid consists of 3 or 4 points. We derive exactformulae for the numbers of functions corresponding to these values and furtherrefine them in the case of a minimal teaching set of size 3. We also prove thatthe average cardinality of the minimal teaching sets of threshold functions isasymptotically 7/2. We further present corollaries of these results concerning some specialarrangements of lines in the plane.
arxiv-6900-214 | Pixels to Voxels: Modeling Visual Representation in the Human Brain | http://arxiv.org/pdf/1407.5104v1.pdf | author:Pulkit Agrawal, Dustin Stansbury, Jitendra Malik, Jack L. Gallant category:q-bio.NC cs.CV cs.NE published:2014-07-18 summary:The human brain is adept at solving difficult high-level visual processingproblems such as image interpretation and object recognition in natural scenes.Over the past few years neuroscientists have made remarkable progress inunderstanding how the human brain represents categories of objects and actionsin natural scenes. However, all current models of high-level human visionoperate on hand annotated images in which the objects and actions have beenassigned semantic tags by a human operator. No current models can account forhigh-level visual function directly in terms of low-level visual input (i.e.,pixels). To overcome this fundamental limitation we sought to develop a newclass of models that can predict human brain activity directly from low-levelvisual input (i.e., pixels). We explored two classes of models drawn fromcomputer vision and machine learning. The first class of models was based onFisher Vectors (FV) and the second was based on Convolutional Neural Networks(ConvNets). We find that both classes of models accurately predict brainactivity in high-level visual areas, directly from pixels and without the needfor any semantic tags or hand annotation of images. This is the first time thatsuch a mapping has been obtained. The fit models provide a new platform forexploring the functional principles of human vision, and they show that modernmethods of computer vision and machine learning provide important tools forcharacterizing brain function.
arxiv-6900-215 | Bayesian Nonparametric Crowdsourcing | http://arxiv.org/pdf/1407.5017v1.pdf | author:Pablo G. Moreno, Yee Whye Teh, Fernando Perez-Cruz, Antonio Artés-Rodríguez category:stat.ML stat.AP published:2014-07-18 summary:Crowdsourcing has been proven to be an effective and efficient tool toannotate large datasets. User annotations are often noisy, so methods tocombine the annotations to produce reliable estimates of the ground truth arenecessary. We claim that considering the existence of clusters of users in thiscombination step can improve the performance. This is especially important inearly stages of crowdsourcing implementations, where the number of annotationsis low. At this stage there is not enough information to accurately estimatethe bias introduced by each annotator separately, so we have to resort tomodels that consider the statistical links among them. In addition, findingthese clusters is interesting in itself as knowing the behavior of the pool ofannotators allows implementing efficient active learning strategies. Based onthis, we propose in this paper two new fully unsupervised models based on aChinese Restaurant Process (CRP) prior and a hierarchical structure that allowsinferring these groups jointly with the ground truth and the properties of theusers. Efficient inference algorithms based on Gibbs sampling with auxiliaryvariables are proposed. Finally, we perform experiments, both on synthetic andreal databases, to show the advantages of our models over state-of-the-artalgorithms.
arxiv-6900-216 | Motor Learning Mechanism on the Neuron Scale | http://arxiv.org/pdf/1407.7027v1.pdf | author:Peilei Liu, Ting Wang category:q-bio.NC cs.NE physics.bio-ph published:2014-07-18 summary:Based on existing data, we wish to put forward a biological model of motorsystem on the neuron scale. Then we indicate its implications in statistics andlearning. Specifically, neuron firing frequency and synaptic strength areprobability estimates in essence. And the lateral inhibition also hasstatistical implications. From the standpoint of learning, dendriticcompetition through retrograde messengers is the foundation of conditionalreflex and grandmother cell coding. And they are the kernel mechanisms of motorlearning and sensory motor integration respectively. Finally, we compare motorsystem with sensory system. In short, we would like to bridge the gap betweenmolecule evidences and computational models.
arxiv-6900-217 | Deep Metric Learning for Practical Person Re-Identification | http://arxiv.org/pdf/1407.4979v1.pdf | author:Dong Yi, Zhen Lei, Stan Z. Li category:cs.CV cs.LG cs.NE published:2014-07-18 summary:Various hand-crafted features and metric learning methods prevail in thefield of person re-identification. Compared to these methods, this paperproposes a more general way that can learn a similarity metric from imagepixels directly. By using a "siamese" deep neural network, the proposed methodcan jointly learn the color feature, texture feature and metric in a unifiedframework. The network has a symmetry structure with two sub-networks which areconnected by Cosine function. To deal with the big variations of person images,binomial deviance is used to evaluate the cost between similarities and labels,which is proved to be robust to outliers. Compared to existing researches, a more practical setting is studied in theexperiments that is training and test on different datasets (cross datasetperson re-identification). Both in "intra dataset" and "cross dataset"settings, the superiorities of the proposed method are illustrated on VIPeR andPRID.
arxiv-6900-218 | Hand Pointing Detection Using Live Histogram Template of Forehead Skin | http://arxiv.org/pdf/1407.4898v1.pdf | author:Ghassem Tofighi, Nasser Ali Afarin, Kamraan Raahemifar, Anastasios N. Venetsanopoulos category:cs.CV published:2014-07-18 summary:Hand pointing detection has multiple applications in many fields such asvirtual reality and control devices in smart homes. In this paper, we proposeda novel approach to detect pointing vector in 2D space of a room. Afterbackground subtraction, face and forehead is detected. In the second step,forehead skin H-S plane histograms in HSV space is calculated. By using thesehistogram templates of users skin, and back projection method, skin areas aredetected. The contours of hand are extracted using Freeman chain codealgorithm. Next step is finding fingertips. Points in hand contour which arecandidates for the fingertip can be found in convex defects of convex hull andcontour. We introduced a novel method for finding the fingertip based on thespecial points on the contour and their relationships. Our approach detectshand-pointing vectors in live video from a common webcam with 94%TP and 85%TN.
arxiv-6900-219 | Classification of Passes in Football Matches using Spatiotemporal Data | http://arxiv.org/pdf/1407.5093v1.pdf | author:Michael Horton, Joachim Gudmundsson, Sanjay Chawla, Joël Estephan category:cs.LG cs.CG I.5.2 published:2014-07-18 summary:A knowledgeable observer of a game of football (soccer) can make a subjectiveevaluation of the quality of passes made between players during the game. Weinvestigate the problem of producing an automated system to make the sameevaluation of passes. We present a model that constructs numerical predictorvariables from spatiotemporal match data using feature functions based onmethods from computational geometry, and then learns a classification functionfrom labelled examples of the predictor variables. Furthermore, the learnedclassifiers are analysed to determine if there is a relationship between thecomplexity of the algorithm that computed the predictor variable and theimportance of the variable to the classifier. Experimental results show that weare able to produce a classifier with 85.8% accuracy on classifying passes asGood, OK or Bad, and that the predictor variables computed using complexmethods from computational geometry are of moderate importance to the learnedclassifiers. Finally, we show that the inter-rater agreement on passclassification between the machine classifier and a human observer is ofsimilar magnitude to the agreement between two observers.
arxiv-6900-220 | Affine Subspace Representation for Feature Description | http://arxiv.org/pdf/1407.4874v1.pdf | author:Zhenhua Wang, Bin Fan, Fuchao Wu category:cs.CV published:2014-07-18 summary:This paper proposes a novel Affine Subspace Representation (ASR) descriptorto deal with affine distortions induced by viewpoint changes. Unlike thetraditional local descriptors such as SIFT, ASR inherently encodes localinformation of multi-view patches, making it robust to affine distortions whilemaintaining a high discriminative ability. To this end, PCA is used torepresent affine-warped patches as PCA-patch vectors for its compactness andefficiency. Then according to the subspace assumption, which implies that thePCA-patch vectors of various affine-warped patches of the same keypoint can berepresented by a low-dimensional linear subspace, the ASR descriptor isobtained by using a simple subspace-to-point mapping. Such a linear subspacerepresentation could accurately capture the underlying information of akeypoint (local structure) under multiple views without sacrificing itsdistinctiveness. To accelerate the computation of ASR descriptor, a fastapproximate algorithm is proposed by moving the most computational part (ie,warp patch under various affine transformations) to an offline training stage.Experimental results show that ASR is not only better than the state-of-the-artdescriptors under various image transformations, but also performs well withouta dedicated affine invariant detector when dealing with viewpoint changes.
arxiv-6900-221 | A Comparative Study of Meta-heuristic Algorithms for Solving Quadratic Assignment Problem | http://arxiv.org/pdf/1407.4863v1.pdf | author:Gamal Abd El-Nasser A. Said, Abeer M. Mahmoud, El-Sayed M. El-Horbaty category:cs.AI cs.NE published:2014-07-18 summary:Quadratic Assignment Problem (QAP) is an NP-hard combinatorial optimizationproblem, therefore, solving the QAP requires applying one or more of themeta-heuristic algorithms. This paper presents a comparative study betweenMeta-heuristic algorithms: Genetic Algorithm, Tabu Search, and Simulatedannealing for solving a real-life (QAP) and analyze their performance in termsof both runtime efficiency and solution quality. The results show that GeneticAlgorithm has a better solution quality while Tabu Search has a fasterexecution time in comparison with other Meta-heuristic algorithms for solvingQAP.
arxiv-6900-222 | An landcover fuzzy logic classification by maximumlikelihood | http://arxiv.org/pdf/1407.4739v1.pdf | author:T. Sarath, G. Nagalakshmi category:cs.CV cs.LG published:2014-07-17 summary:In present days remote sensing is most used application in many sectors. Thisremote sensing uses different images like multispectral, hyper spectral orultra spectral. The remote sensing image classification is one of thesignificant method to classify image. In this state we classify the maximumlikelihood classification with fuzzy logic. In this we experimenting fuzzylogic like spatial, spectral texture methods in that different sub methods tobe used for image classification.
arxiv-6900-223 | Sparse Partially Linear Additive Models | http://arxiv.org/pdf/1407.4729v1.pdf | author:Yin Lou, Jacob Bien, Rich Caruana, Johannes Gehrke category:stat.ME cs.LG stat.ML published:2014-07-17 summary:The generalized partially linear additive model (GPLAM) is a flexible andinterpretable approach to building predictive models. It combines features inan additive manner, allowing them to have either a linear or nonlinear effecton the response. However, the assignment of features to the linear andnonlinear groups is typically assumed known. Thus, to make a GPLAM a viableapproach in situations in which little is known $apriori$ about the features,one must overcome two primary model selection challenges: deciding whichfeatures to include in the model and determining which features to treatnonlinearly. We introduce sparse partially linear additive models (SPLAMs),which combine model fitting and $both$ of these model selection challenges intoa single convex optimization problem. SPLAM provides a bridge between the Lassoand sparse additive models. Through a statistical oracle inequality andthorough simulation, we demonstrate that SPLAM can outperform other methodsacross a broad spectrum of statistical regimes, including the high-dimensional($p\gg N$) setting. We develop efficient algorithms that are applied to realdata sets with half a million samples and over 45,000 features with excellentpredictive performance.
arxiv-6900-224 | Initial Comparison of Linguistic Networks Measures for Parallel Texts | http://arxiv.org/pdf/1405.1893v2.pdf | author:Kristina Ban, Ana Meštrović, Sanda Martinčić-Ipšić category:cs.CL cs.SI physics.soc-ph published:2014-05-08 summary:This paper presents preliminary results of Croatian syllable networksanalysis. Syllable network is a network in which nodes are syllables and linksbetween them are constructed according to their connections within words. Inthis paper we analyze networks of syllables generated from texts collected fromthe Croatian Wikipedia and Blogs. As a main tool we use complex networkanalysis methods which provide mechanisms that can reveal new patterns in alanguage structure. We aim to show that syllable networks have much higherclustering coefficient in comparison to Erd\"os-Renyi random networks. Theresults indicate that Croatian syllable networks exhibit certain properties ofa small world networks. Furthermore, we compared Croatian syllable networkswith Portuguese and Chinese syllable networks and we showed that they havesimilar properties.
arxiv-6900-225 | A preliminary study of Croatian Language Syllable Networks | http://arxiv.org/pdf/1405.4097v2.pdf | author:Kristina Ban, Ivan Ivakić, Ana Meštrović category:cs.CL published:2014-05-16 summary:This paper presents preliminary results of Croatian syllable networksanalysis. Syllable network is a network in which nodes are syllables and linksbetween them are constructed according to their connections within words. Inthis paper we analyze networks of syllables generated from texts collected fromthe Croatian Wikipedia and Blogs. As a main tool we use complex networkanalysis methods which provide mechanisms that can reveal new patterns in alanguage structure. We aim to show that syllable networks have much higherclustering coefficient in comparison to Erd\"os-Renyi random networks. Theresults indicate that Croatian syllable networks exhibit certain properties ofa small world networks. Furthermore, we compared Croatian syllable networkswith Portuguese and Chinese syllable networks and we showed that they havesimilar properties.
arxiv-6900-226 | Comparison of the language networks from literature and blogs | http://arxiv.org/pdf/1405.2702v2.pdf | author:Sabina Šišović, Sanda Martinčić-Ipšić, Ana Meštrović category:cs.CL cs.SI physics.soc-ph published:2014-05-12 summary:In this paper we present the comparison of the linguistic networks fromliterature and blog texts. The linguistic networks are constructed from textsas directed and weighted co-occurrence networks of words. Words are nodes andlinks are established between two nodes if they are directly co-occurringwithin the sentence. The comparison of the networks structure is performed atglobal level (network) in terms of: average node degree, average shortest pathlength, diameter, clustering coefficient, density and number of components.Furthermore, we perform analysis on the local level (node) by comparing therank plots of in and out degree, strength and selectivity. Theselectivity-based results point out that there are differences between thestructure of the networks constructed from literature and blogs.
arxiv-6900-227 | Toward Selectivity Based Keyword Extraction for Croatian News | http://arxiv.org/pdf/1407.4723v1.pdf | author:Slobodan Beliga, Ana Meštrović, Sanda Martinčcić-Ipšić category:cs.CL cs.IR cs.SI published:2014-07-17 summary:Preliminary report on network based keyword extraction for Croatian is anunsupervised method for keyword extraction from the complex network. We buildour approach with a new network measure the node selectivity, motivated by theresearch of the graph based centrality approaches. The node selectivity isdefined as the average weight distribution on the links of the single node. Weextract nodes (keyword candidates) based on the selectivity value. Furthermore,we expand extracted nodes to word-tuples ranked with the highest in/outselectivity values. Selectivity based extraction does not require linguisticknowledge while it is purely derived from statistical and structuralinformation en-compassed in the source text which is reflected into thestructure of the network. Obtained sets are evaluated on a manually annotatedkeywords: for the set of extracted keyword candidates average F1 score is24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidatesaverage F1 score is 25,9% and average F2 score is 24,47%.
arxiv-6900-228 | A feature construction framework based on outlier detection and discriminative pattern mining | http://arxiv.org/pdf/1407.4668v1.pdf | author:Albrecht Zimmermann category:cs.LG published:2014-07-17 summary:No matter the expressive power and sophistication of supervised learningalgorithms, their effectiveness is restricted by the features describing thedata. This is not a new insight in ML and many methods for feature selection,transformation, and construction have been developed. But while this ison-going for general techniques for feature selection and transformation, i.e.dimensionality reduction, work on feature construction, i.e. enriching thedata, is by now mainly the domain of image, particularly character,recognition, and NLP. In this work, we propose a new general framework for feature construction.The need for feature construction in a data set is indicated by class outliersand discriminative pattern mining used to derive features on theirk-neighborhoods. We instantiate the framework with LOF and C4.5-Rules, andevaluate the usefulness of the derived features on a diverse collection of UCIdata sets. The derived features are more often useful than ones derived byDC-Fringe, and our approach is much less likely to overfit. But while a weaklearner, Naive Bayes, benefits strongly from the feature construction, theeffect is less pronounced for C4.5, and almost vanishes for an SVM leaner. Keywords: feature construction, classification, outlier detection
arxiv-6900-229 | Optimization Under Uncertainty Using the Generalized Inverse Distribution Function | http://arxiv.org/pdf/1407.4636v1.pdf | author:Domenico Quagliarella, Giovanni Petrone, Gianluca Iaccarino category:math.OC cs.NE published:2014-07-17 summary:A framework for robust optimization under uncertainty based on the use of thegeneralized inverse distribution function (GIDF), also called quantilefunction, is here proposed. Compared to more classical approaches that rely onthe usage of statistical moments as deterministic attributes that define theobjectives of the optimization process, the inverse cumulative distributionfunction allows for the use of all the possible information available in theprobabilistic domain. Furthermore, the use of a quantile based approach leadsnaturally to a multi-objective methodology which allows an a-posterioriselection of the candidate design based on risk/opportunity criteria defined bythe designer. Finally, the error on the estimation of the objectives due to theresolution of the GIDF will be proven to be quantifiable
arxiv-6900-230 | Sparse Quadratic Discriminant Analysis and Community Bayes | http://arxiv.org/pdf/1407.4543v1.pdf | author:Ya Le, Trevor Hastie category:stat.ML stat.CO published:2014-07-17 summary:We develop a class of rules spanning the range between quadratic discriminantanalysis and naive Bayes, through a path of sparse graphical models. A grouplasso penalty is used to introduce shrinkage and encourage a similar pattern ofsparsity across precision matrices. It gives sparse estimates of interactionsand produces interpretable models. Inspired by the connected-componentsstructure of the estimated precision matrices, we propose the community Bayesmodel, which partitions features into several conditional independentcommunities and splits the classification problem into separate smaller ones.The community Bayes idea is quite general and can be applied to non-gaussiandata and likelihood-based classifiers.
arxiv-6900-231 | On the Complexity of Best Arm Identification in Multi-Armed Bandit Models | http://arxiv.org/pdf/1407.4443v1.pdf | author:Emilie Kaufmann, Olivier Cappé, Aurélien Garivier category:stat.ML cs.LG published:2014-07-16 summary:The stochastic multi-armed bandit model is a simple abstraction that hasproven useful in many different contexts in statistics and machine learning.Whereas the achievable limit in terms of regret minimization is now well known,our aim is to contribute to a better understanding of the performance in termsof identifying the m best arms. We introduce generic notions of complexity forthe two dominant frameworks considered in the literature: fixed-budget andfixed-confidence settings. In the fixed-confidence setting, we provide thefirst known distribution-dependent lower bound on the complexity that involvesinformation-theoretic quantities and holds when m is larger than 1 undergeneral assumptions. In the specific case of two armed-bandits, we deriverefined lower bounds in both the fixed-confidence and fixed-budget settings,along with matching algorithms for Gaussian and Bernoulli bandit models. Theseresults show in particular that the complexity of the fixed-budget setting maybe smaller than the complexity of the fixed-confidence setting, contradictingthe familiar behavior observed when testing fully specified alternatives. Inaddition, we also provide improved sequential stopping rules that haveguaranteed error probabilities and shorter average running times. The proofsrely on two technical results that are of independent interest : a deviationlemma for self-normalized sums (Lemma 19) and a novel change of measureinequality for bandit models (Lemma 1).
arxiv-6900-232 | Sequential Logistic Principal Component Analysis (SLPCA): Dimensional Reduction in Streaming Multivariate Binary-State System | http://arxiv.org/pdf/1407.4430v1.pdf | author:Zhaoyi Kang, Costas J. Spanos category:stat.ML cs.LG stat.AP published:2014-07-16 summary:Sequential or online dimensional reduction is of interests due to theexplosion of streaming data based applications and the requirement of adaptivestatistical modeling, in many emerging fields, such as the modeling of energyend-use profile. Principal Component Analysis (PCA), is the classical way ofdimensional reduction. However, traditional Singular Value Decomposition (SVD)based PCA fails to model data which largely deviates from Gaussiandistribution. The Bregman Divergence was recently introduced to achieve ageneralized PCA framework. If the random variable under dimensional reductionfollows Bernoulli distribution, which occurs in many emerging fields, thegeneralized PCA is called Logistic PCA (LPCA). In this paper, we extend thebatch LPCA to a sequential version (i.e. SLPCA), based on the sequential convexoptimization theory. The convergence property of this algorithm is discussedcompared to the batch version of LPCA (i.e. BLPCA), as well as its performancein reducing the dimension for multivariate binary-state systems. Itsapplication in building energy end-use profile modeling is also investigated.
arxiv-6900-233 | Subspace Restricted Boltzmann Machine | http://arxiv.org/pdf/1407.4422v1.pdf | author:Jakub M. Tomczak, Adam Gonczarek category:cs.LG published:2014-07-16 summary:The subspace Restricted Boltzmann Machine (subspaceRBM) is a third-orderBoltzmann machine where multiplicative interactions are between one visible andtwo hidden units. There are two kinds of hidden units, namely, gate units andsubspace units. The subspace units reflect variations of a pattern in data andthe gate unit is responsible for activating the subspace units. Additionally,the gate unit can be seen as a pooling feature. We evaluate the behavior ofsubspaceRBM through experiments with MNIST digit recognition task, measuringreconstruction error and classification error.
arxiv-6900-234 | In Defense of MinHash Over SimHash | http://arxiv.org/pdf/1407.4416v1.pdf | author:Anshumali Shrivastava, Ping Li category:stat.CO cs.DS cs.IR cs.LG stat.ML published:2014-07-16 summary:MinHash and SimHash are the two widely adopted Locality Sensitive Hashing(LSH) algorithms for large-scale data processing applications. Deciding whichLSH to use for a particular problem at hand is an important question, which hasno clear answer in the existing literature. In this study, we provide atheoretical answer (validated by experiments) that MinHash virtually alwaysoutperforms SimHash when the data are binary, as common in practice such assearch. The collision probability of MinHash is a function of resemblance similarity($\mathcal{R}$), while the collision probability of SimHash is a function ofcosine similarity ($\mathcal{S}$). To provide a common basis for comparison, weevaluate retrieval results in terms of $\mathcal{S}$ for both MinHash andSimHash. This evaluation is valid as we can prove that MinHash is a valid LSHwith respect to $\mathcal{S}$, by using a general inequality $\mathcal{S}^2\leq\mathcal{R}\leq \frac{\mathcal{S}}{2-\mathcal{S}}$. Our worst case analysis canshow that MinHash significantly outperforms SimHash in high similarity region. Interestingly, our intensive experiments reveal that MinHash is alsosubstantially better than SimHash even in datasets where most of the datapoints are not too similar to each other. This is partly because, in practicaldata, often $\mathcal{R}\geq \frac{\mathcal{S}}{z-\mathcal{S}}$ holds where $z$is only slightly larger than 2 (e.g., $z\leq 2.1$). Our restricted worst caseanalysis by assuming $\frac{\mathcal{S}}{z-\mathcal{S}}\leq \mathcal{R}\leq\frac{\mathcal{S}}{2-\mathcal{S}}$ shows that MinHash indeed significantlyoutperforms SimHash even in low similarity region. We believe the results in this paper will provide valuable guidelines forsearch in practice, especially when the data are sparse.
arxiv-6900-235 | Advancing Matrix Completion by Modeling Extra Structures beyond Low-Rankness | http://arxiv.org/pdf/1404.4646v2.pdf | author:Guangcan Liu, Ping Li category:stat.ME cs.IT cs.LG math.IT math.ST stat.TH published:2014-04-17 summary:A well-known method for completing low-rank matrices based on convexoptimization has been established by Cand{\`e}s and Recht. Althoughtheoretically complete, the method may not entirely solve the low-rank matrixcompletion problem. This is because the method captures only the low-ranknessproperty which gives merely a rough constraint that the data points locate onsome low-dimensional subspace, but generally ignores the extra structures whichspecify in more detail how the data points locate on the subspace. Whenever thegeometric distribution of the data points is not uniform, the coherenceparameters of data might be large and, accordingly, the method might fail evenif the latent matrix we want to recover is fairly low-rank. To better handlenon-uniform data, in this paper we propose a method termed Low-Rank FactorDecomposition (LRFD), which imposes an additional restriction that the datapoints must be represented as linear combinations of the bases in a dictionaryconstructed or learnt in advance. We show that LRFD can well handle non-uniformdata, provided that the dictionary is configured properly: We mathematicallyprove that if the dictionary itself is low-rank then LRFD is immune to thecoherence parameters which might be large on non-uniform data. This provides anelementary principle for learning the dictionary in LRFD and, naturally, leadsto a practical algorithm for advancing matrix completion. Extensive experimentson randomly generated matrices and motion datasets show encouraging results.
arxiv-6900-236 | Recovery of Coherent Data via Low-Rank Dictionary Pursuit | http://arxiv.org/pdf/1404.4032v2.pdf | author:Guangcan Liu, Ping Li category:stat.ME cs.IT cs.LG math.IT math.ST stat.TH published:2014-04-15 summary:The recently established RPCA method provides us a convenient way to restorelow-rank matrices from grossly corrupted observations. While elegant in theoryand powerful in reality, RPCA may be not an ultimate solution to the low-rankmatrix recovery problem. Indeed, its performance may not be perfect even whendata are strictly low-rank. This is because conventional RPCA ignores theclustering structures of the data which are ubiquitous in modern applications.As the number of cluster grows, the coherence of data keeps increasing, andaccordingly, the recovery performance of RPCA degrades. We show that thechallenges raised by coherent data (i.e., the data with high coherence) couldbe alleviated by Low-Rank Representation (LRR), provided that the dictionary inLRR is configured appropriately. More precisely, we mathematically prove thatif the dictionary itself is low-rank then LRR is immune to the coherenceparameter which increases with the underlying cluster number. This provides anelementary principle for dealing with coherent data. Subsequently, we devise apractical algorithm to obtain proper dictionaries in unsupervised environments.Our extensive experiments on randomly generated matrices verify our claims.
arxiv-6900-237 | Online Asynchronous Distributed Regression | http://arxiv.org/pdf/1407.4373v1.pdf | author:Gérard Biau, Ryad Zenine category:math.ST stat.ML stat.TH published:2014-07-16 summary:Distributed computing offers a high degree of flexibility to accommodatemodern learning constraints and the ever increasing size of datasets involvedin massive data issues. Drawing inspiration from the theory of distributedcomputation models developed in the context of gradient-type optimizationalgorithms, we present a consensus-based asynchronous distributed approach fornonparametric online regression and analyze some of its asymptotic properties.Substantial numerical evidence involving up to 28 parallel processors isprovided on synthetic datasets to assess the excellent performance of ourmethod, both in terms of computation time and prediction accuracy.
arxiv-6900-238 | Finding sparse solutions of systems of polynomial equations via group-sparsity optimization | http://arxiv.org/pdf/1311.5871v2.pdf | author:Fabien Lauer, Henrik Ohlsson category:cs.IT cs.LG math.IT math.OC stat.ML published:2013-11-22 summary:The paper deals with the problem of finding sparse solutions to systems ofpolynomial equations possibly perturbed by noise. In particular, we show howthese solutions can be recovered from group-sparse solutions of a derivedsystem of linear equations. Then, two approaches are considered to find thesegroup-sparse solutions. The first one is based on a convex relaxation resultingin a second-order cone programming formulation which can benefit from efficientreweighting techniques for sparsity enhancement. For this approach, sufficientconditions for the exact recovery of the sparsest solution to the polynomialsystem are derived in the noiseless setting, while stable recovery results areobtained for the noisy case. Though lacking a similar analysis, the secondapproach provides a more computationally efficient algorithm based on a greedystrategy adding the groups one-by-one. With respect to previous work, theproposed methods recover the sparsest solution in a very short computing timewhile remaining at least as accurate in terms of the probability of success.This probability is empirically analyzed to emphasize the relationship betweenthe ability of the methods to solve the polynomial system and the sparsity ofthe solution.
arxiv-6900-239 | Optimal CUR Matrix Decompositions | http://arxiv.org/pdf/1405.7910v2.pdf | author:Christos Boutsidis, David P. Woodruff category:cs.DS cs.LG math.NA published:2014-05-30 summary:The CUR decomposition of an $m \times n$ matrix $A$ finds an $m \times c$matrix $C$ with a subset of $c < n$ columns of $A,$ together with an $r \timesn$ matrix $R$ with a subset of $r < m$ rows of $A,$ as well as a $c \times r$low-rank matrix $U$ such that the matrix $C U R$ approximates the matrix $A,$that is, $ A - CUR _F^2 \le (1+\epsilon) A - A_k_F^2$, where$._F$ denotes the Frobenius norm and $A_k$ is the best $m \times n$ matrixof rank $k$ constructed via the SVD. We present input-sparsity-time anddeterministic algorithms for constructing such a CUR decomposition where$c=O(k/\epsilon)$ and $r=O(k/\epsilon)$ and rank$(U) = k$. Up to constantfactors, our algorithms are simultaneously optimal in $c, r,$ and rank$(U)$.
arxiv-6900-240 | Collaborative Filtering Ensemble for Personalized Name Recommendation | http://arxiv.org/pdf/1407.4832v1.pdf | author:Bernat Coma-Puig, Ernesto Diaz-Aviles, Wolfgang Nejdl category:cs.IR cs.AI cs.LG H.3.3; I.2.6 published:2014-07-16 summary:Out of thousands of names to choose from, picking the right one for yourchild is a daunting task. In this work, our objective is to help parents makingan informed decision while choosing a name for their baby. We follow arecommender system approach and combine, in an ensemble, the individualrankings produced by simple collaborative filtering algorithms in order toproduce a personalized list of names that meets the individual parents' taste.Our experiments were conducted using real-world data collected from the querylogs of 'nameling' (nameling.net), an online portal for searching and exploringnames, which corresponds to the dataset released in the context of the ECMLPKDD Discover Challenge 2013. Our approach is intuitive, easy to implement, andfeatures fast training and prediction steps.
arxiv-6900-241 | Mobile Camera Array Calibration for Light Field Acquisition | http://arxiv.org/pdf/1407.4206v1.pdf | author:Yichao Xu, Kazuki Maeno, Hajime Nagahara, Rin-ichiro Taniguchi category:cs.CV published:2014-07-16 summary:The light field camera is useful for computer graphics and visionapplications. Calibration is an essential step for these applications. Aftercalibration, we can rectify the captured image by using the calibrated cameraparameters. However, the large camera array calibration method, which assumesthat all cameras are on the same plane, ignores the orientation and intrinsicparameters. The multi-camera calibration technique usually assumes that theworking volume and viewpoints are fixed. In this paper, we describe acalibration algorithm suitable for a mobile camera array based light fieldacquisition system. The algorithm performs in Zhang's style by moving acheckerboard, and computes the initial parameters in closed form. Globaloptimization is then applied to refine all the parameters simultaneously. Ourimplementation is rather flexible in that users can assign the number ofviewpoints and refinement of intrinsic parameters is optional. Experiments onboth simulated data and real data acquired by a commercial product show thatour method yields good results. Digital refocusing application shows thecalibrated light field can well focus to the target object we desired.
arxiv-6900-242 | Automatic discovery of cell types and microcircuitry from neural connectomics | http://arxiv.org/pdf/1407.4137v1.pdf | author:Eric Jonas, Konrad Kording category:q-bio.NC stat.ML published:2014-07-15 summary:Neural connectomics has begun producing massive amounts of data,necessitating new analysis methods to discover the biological and computationalstructure. It has long been assumed that discovering neuron types and theirrelation to microcircuitry is crucial to understanding neural function. Here wedeveloped a nonparametric Bayesian technique that identifies neuron types andmicrocircuitry patterns in connectomics data. It combines the informationtraditionally used by biologists, including connectivity, cell body locationand the spatial distribution of synapses, in a principled andprobabilistically-coherent manner. We show that the approach recovers knownneuron types in the retina and enables predictions of connectivity, better thansimpler algorithms. It also can reveal interesting structure in the nervoussystem of C. elegans, and automatically discovers the structure of amicroprocessor. Our approach extracts structural meaning from connectomics,enabling new approaches of automatically deriving anatomical insights fromthese emerging datasets.
arxiv-6900-243 | Fast matrix completion without the condition number | http://arxiv.org/pdf/1407.4070v1.pdf | author:Moritz Hardt, Mary Wootters category:cs.LG cs.DS stat.ML published:2014-07-15 summary:We give the first algorithm for Matrix Completion whose running time andsample complexity is polynomial in the rank of the unknown target matrix,linear in the dimension of the matrix, and logarithmic in the condition numberof the matrix. To the best of our knowledge, all previous algorithms eitherincurred a quadratic dependence on the condition number of the unknown matrixor a quadratic dependence on the dimension of the matrix in the running time.Our algorithm is based on a novel extension of Alternating Minimization whichwe show has theoretical guarantees under standard assumptions even in thepresence of noise.
arxiv-6900-244 | Global disease monitoring and forecasting with Wikipedia | http://arxiv.org/pdf/1405.3612v2.pdf | author:Nicholas Generous, Geoffrey Fairchild, Alina Deshpande, Sara Y. Del Valle, Reid Priedhorsky category:cs.SI cs.LG physics.soc-ph published:2014-05-14 summary:Infectious disease is a leading threat to public health, economic stability,and other key social structures. Efforts to mitigate these impacts depend onaccurate and timely monitoring to measure the risk and progress of disease.Traditional, biologically-focused monitoring techniques are accurate but costlyand slow; in response, new techniques based on social internet data such associal media and search queries are emerging. These efforts are promising, butimportant challenges in the areas of scientific peer review, breadth ofdiseases and countries, and forecasting hamper their operational usefulness. We examine a freely available, open data source for this use: access logsfrom the online encyclopedia Wikipedia. Using linear models, language as aproxy for location, and a systematic yet simple article selection procedure, wetested 14 location-disease combinations and demonstrate that these datafeasibly support an approach that overcomes these challenges. Specifically, ourproof-of-concept yields models with $r^2$ up to 0.92, forecasting value up tothe 28 days tested, and several pairs of models similar enough to suggest thattransferring models from one location to another without re-training isfeasible. Based on these preliminary results, we close with a research agenda designedto overcome these challenges and produce a disease monitoring and forecastingsystem that is significantly more effective, robust, and globally comprehensivethan the current state of the art.
arxiv-6900-245 | Identification of Probabilities of Languages | http://arxiv.org/pdf/1208.5003v3.pdf | author:Paul M. B. Vitanyi, Nick Chater category:cs.LG math.PR 68 published:2012-08-24 summary:We consider the problem of inferring the probability distribution associatedwith a language, given data consisting of an infinite sequence of elements ofthe languge. We do this under two assumptions on the algorithms concerned: (i)like a real-life algorothm it has round-off errors, and (ii) it has noround-off errors. Assuming (i) we (a) consider a probability mass function ofthe elements of the language if the data are drawn independent identicallydistributed (i.i.d.), provided the probability mass function is computable andhas a finite expectation. We give an effective procedure to almost surelyidentify in the limit the target probability mass function using the Strong Lawof Large Numbers. Second (b) we treat the case of possibly incomputableprobabilistic mass functions in the above setting. In this case we can onlypointswize converge to the target probability mass function almost surely.Third (c) we consider the case where the data are dependent assuming they aretypical for at least one computable measure and the language is finite. Thereis an effective procedure to identify by infinite recurrence a nonempty subsetof the computable measures according to which the data is typical. Here we usethe theory of Kolmogorov complexity. Assuming (ii) we obtain the weaker resultfor (a) that the target distribution is identified by infinite recurrencealmost surely; (b) stays the same as under assumption (i). We consider theassociated predictions.
arxiv-6900-246 | An iterative approach to Hough transform without re-voting | http://arxiv.org/pdf/1407.3969v1.pdf | author:Giorgio Ricca, Mauro C. Beltrametti, Anna Maria Massone category:cs.CV 68T45, 68U10 published:2014-07-15 summary:Many bone shapes in the human skeleton are characterized by profiles that canbe associated to equations of algebraic curves. Fixing the parameters in thecurve equation, by means of a classical pattern recognition procedure like theHough transform technique, it is then possible to associate an equation to aspecific bone profile. However, most skeleton districts are more accuratelydescribed by piecewise defined curves. This paper utilizes an iterativeapproach of the Hough transform without re-voting, to provide an efficientprocedure for describing the profile of a bone in the human skeleton as acollection of different but continuously attached curves.
arxiv-6900-247 | Analysis of purely random forests bias | http://arxiv.org/pdf/1407.3939v1.pdf | author:Sylvain Arlot, Robin Genuer category:math.ST cs.LG stat.ME stat.TH published:2014-07-15 summary:Random forests are a very effective and commonly used statistical method, buttheir full theoretical analysis is still an open problem. As a first step,simplified models such as purely random forests have been introduced, in orderto shed light on the good performance of random forests. In this paper, westudy the approximation error (the bias) of some purely random forest models ina regression framework, focusing in particular on the influence of the numberof trees in the forest. Under some regularity assumptions on the regressionfunction, we show that the bias of an infinite forest decreases at a fasterrate (with respect to the size of each tree) than a single tree. As aconsequence, infinite forests attain a strictly better risk rate (with respectto the sample size) than single trees. Furthermore, our results allow to derivea minimum number of trees sufficient to reach the same rate as an infiniteforest. As a by-product of our analysis, we also show a link between the biasof purely random forests and the bias of some kernel estimators.
arxiv-6900-248 | Complex Support Vector Machines for Regression and Quaternary Classification | http://arxiv.org/pdf/1303.2184v3.pdf | author:Pantelis Bouboulis, Sergios Theodoridis, Charalampos Mavroforakis, Leoni Dalla category:cs.LG stat.ML published:2013-03-09 summary:The paper presents a new framework for complex Support Vector Regression aswell as Support Vector Machines for quaternary classification. The methodexploits the notion of widely linear estimation to model the input-out relationfor complex-valued data and considers two cases: a) the complex data are splitinto their real and imaginary parts and a typical real kernel is employed tomap the complex data to a complexified feature space and b) a pure complexkernel is used to directly map the data to the induced complex feature space.The recently developed Wirtinger's calculus on complex reproducing kernelHilbert spaces (RKHS) is employed in order to compute the Lagrangian and derivethe dual optimization problem. As one of our major results, we prove that anycomplex SVM/SVR task is equivalent with solving two real SVM/SVR tasksexploiting a specific real kernel which is generated by the chosen complexkernel. In particular, the case of pure complex kernels leads to the generationof new kernels, which have not been considered before. In the classificationcase, the proposed framework inherently splits the complex space into fourparts. This leads naturally in solving the four class-task (quaternaryclassification), instead of the typical two classes of the real SVM. In turn,this rationale can be used in a multiclass problem as a split-class scenariobased on four classes, as opposed to the one-versus-all method; this can leadto significant computational savings. Experiments demonstrate the effectivenessof the proposed framework for regression and classification tasks that involvecomplex data.
arxiv-6900-249 | Growing Regression Forests by Classification: Applications to Object Pose Estimation | http://arxiv.org/pdf/1312.6430v2.pdf | author:Kota Hara, Rama Chellappa category:cs.CV cs.LG stat.ML published:2013-12-22 summary:In this work, we propose a novel node splitting method for regression treesand incorporate it into the regression forest framework. Unlike traditionalbinary splitting, where the splitting rule is selected from a predefined set ofbinary splitting rules via trial-and-error, the proposed node splitting methodfirst finds clusters of the training data which at least locally minimize theempirical loss without considering the input space. Then splitting rules whichpreserve the found clusters as much as possible are determined by casting theproblem into a classification problem. Consequently, our new node splittingmethod enjoys more freedom in choosing the splitting rules, resulting in moreefficient tree structures. In addition to the Euclidean target space, wepresent a variant which can naturally deal with a circular target space by theproper use of circular statistics. We apply the regression forest employing ournode splitting to head pose estimation (Euclidean target space) and cardirection estimation (circular target space) and demonstrate that the proposedmethod significantly outperforms state-of-the-art methods (38.5% and 22.5%error reduction respectively).
arxiv-6900-250 | Part-based R-CNNs for Fine-grained Category Detection | http://arxiv.org/pdf/1407.3867v1.pdf | author:Ning Zhang, Jeff Donahue, Ross Girshick, Trevor Darrell category:cs.CV published:2014-07-15 summary:Semantic part localization can facilitate fine-grained categorization byexplicitly isolating subtle appearance differences associated with specificobject parts. Methods for pose-normalized representations have been proposed,but generally presume bounding box annotations at test time due to thedifficulty of object detection. We propose a model for fine-grainedcategorization that overcomes these limitations by leveraging deepconvolutional features computed on bottom-up region proposals. Our methodlearns whole-object and part detectors, enforces learned geometric constraintsbetween them, and predicts a fine-grained category from a pose-normalizedrepresentation. Experiments on the Caltech-UCSD bird dataset confirm that ourmethod outperforms state-of-the-art fine-grained categorization methods in anend-to-end evaluation without requiring a bounding box at test time.
arxiv-6900-251 | Controlled Natural Language Processing as Answer Set Programming: an Experiment | http://arxiv.org/pdf/1408.2466v1.pdf | author:Rolf Schwitter category:cs.CL cs.AI published:2014-07-15 summary:Most controlled natural languages (CNLs) are processed with the help of apipeline architecture that relies on different software components. Weinvestigate in this paper in an experimental way how well answer setprogramming (ASP) is suited as a unifying framework for parsing a CNL, derivinga formal representation for the resulting syntax trees, and for reasoning withthat representation. We start from a list of input tokens in ASP notation andshow how this input can be transformed into a syntax tree using an ASP grammarand then into reified ASP rules in form of a set of facts. These facts are thenprocessed by an ASP meta-interpreter that allows us to infer new knowledge.
arxiv-6900-252 | A semantic network-based evolutionary algorithm for computational creativity | http://arxiv.org/pdf/1404.7765v2.pdf | author:Atilim Gunes Baydin, Ramon Lopez de Mantaras, Santiago Ontanon category:cs.NE published:2014-04-30 summary:We introduce a novel evolutionary algorithm (EA) with a semanticnetwork-based representation. For enabling this, we establish new formulationsof EA variation operators, crossover and mutation, that we adapt to work onsemantic networks. The algorithm employs commonsense reasoning to ensure alloperations preserve the meaningfulness of the networks, using ConceptNet andWordNet knowledge bases. The algorithm can be interpreted as a novel memeticalgorithm (MA), given that (1) individuals represent pieces of information thatundergo evolution, as in the original sense of memetics as it was introduced byDawkins; and (2) this is different from existing MA, where the word "memetic"has been used as a synonym for local refinement after global optimization. Forevaluating the approach, we introduce an analogical similarity-based fitnessmeasure that is computed through structure mapping. This setup enables theopen-ended generation of networks analogous to a given base network.
arxiv-6900-253 | A Framework for Exploring Non-Linear Functional Connectivity and Causality in the Human Brain: Mutual Connectivity Analysis (MCA) of Resting-State Functional MRI with Convergent Cross-Mapping and Non-Metric Clustering | http://arxiv.org/pdf/1407.3809v1.pdf | author:Axel Wismüller, Xixi Wang, Adora M. DSouza, Mahesh B. Nagarajan category:cs.NE q-bio.NC published:2014-07-14 summary:We present a computational framework for analysis and visualization ofnon-linear functional connectivity in the human brain from resting statefunctional MRI (fMRI) data for purposes of recovering the underlying networkcommunity structure and exploring causality between network components. Ourproposed methodology of non-linear mutual connectivity analysis (MCA) involvestwo computational steps. First, the pair-wise cross-prediction performancebetween resting state fMRI pixel time series within the brain is evaluated. Theunderlying network structure is subsequently recovered from the affinity matrixconstructed through MCA using non-metric network partitioning/clustering withthe so-called Louvain method. We demonstrate our methodology in the task ofidentifying regions of the motor cortex associated with hand movement onresting state fMRI data acquired from eight slice locations in four subjects.For comparison, we also localized regions of the motor cortex through atask-based fMRI sequence involving a finger-tapping stimulus paradigm. Finally,we integrate convergent cross mapping (CCM) into the first step of MCA forinvestigating causality between regions of the motor cortex. Results regardingcausation between regions of the motor cortex revealed a significantdirectional variability and were not readily interpretable in a consistentmanner across all subjects. However, our results on whole-slice fMRI analysisdemonstrate that MCA-based model-free recovery of regions associated with theprimary motor cortex and supplementary motor area are in close agreement withlocalization of similar regions achieved with a task-based fMRI acquisition.Thus, we conclude that our computational framework MCA can extract andvisualize valuable information concerning the underlying network structure andcausation between different regions of the brain in resting state fMRI.
arxiv-6900-254 | Benchmarking Named Entity Disambiguation approaches for Streaming Graphs | http://arxiv.org/pdf/1407.3751v1.pdf | author:Sutanay Choudhury, Chase Dowling category:cs.CL cs.IR published:2014-07-14 summary:Named Entity Disambiaguation (NED) is a central task for applications dealingwith natural language text. Assume that we have a graph based knowledge base(subsequently referred as Knowledge Graph) where nodes represent various realworld entities such as people, location, organization and concepts. Given datasources such as social media streams and web pages Entity Linking is the taskof mapping named entities that are extracted from the data to those present inthe Knowledge Graph. This is an inherently difficult task due to severalreasons. Almost all these data sources are generated without any formalontology; the unstructured nature of the input, limited context and theambiguity involved when multiple entities are mapped to the same name make thisa hard task. This report looks at two state of the art systems employing twodistinctive approaches: graph based Accurate Online Disambiguation of Entities(AIDA) and Mined Evidence Named Entity Disambiguation (MENED), which employs astatistical inference approach. We compare both approaches using the data setand queries provided by the Knowledge Base Population (KBP) track at 2011 NISTText Analytics Conference (TAC). This report begins with an overview of therespective approaches, followed by detailed description of the experimentalsetup. It concludes with our findings from the benchmarking exercise.
arxiv-6900-255 | Finding representative sets of optimizations for adaptive multiversioning applications | http://arxiv.org/pdf/1407.4075v1.pdf | author:Lianjie Luo, Yang Chen, Chengyong Wu, Shun Long, Grigori Fursin category:cs.PL cs.LG published:2014-07-14 summary:Iterative compilation is a widely adopted technique to optimize programs fordifferent constraints such as performance, code size and power consumption inrapidly evolving hardware and software environments. However, in case ofstatically compiled programs, it is often restricted to optimizations for aspecific dataset and may not be applicable to applications that exhibitdifferent run-time behavior across program phases, multiple datasets or whenexecuted in heterogeneous, reconfigurable and virtual environments. Severalframeworks have been recently introduced to tackle these problems and enablerun-time optimization and adaptation for statically compiled programs based onstatic function multiversioning and monitoring of online program behavior. Inthis article, we present a novel technique to select a minimal set ofrepresentative optimization variants (function versions) for such frameworkswhile avoiding performance loss across available datasets and code-sizeexplosion. We developed a novel mapping mechanism using popular decision treeor rule induction based machine learning techniques to rapidly select best codeversions at run-time based on dataset features and minimize selection overhead.These techniques enable creation of self-tuning static binaries or librariesadaptable to changing behavior and environments at run-time using stagedcompilation that do not require complex recompilation frameworks whileeffectively outperforming traditional single-version non-adaptable code.
arxiv-6900-256 | Spatiotemporal Stacked Sequential Learning for Pedestrian Detection | http://arxiv.org/pdf/1407.3686v1.pdf | author:Alejandro González, Sebastian Ramos, David Vázquez, Antonio M. López, Jaume Amores category:cs.CV published:2014-07-14 summary:Pedestrian classifiers decide which image windows contain a pedestrian. Inpractice, such classifiers provide a relatively high response at neighborwindows overlapping a pedestrian, while the responses around potential falsepositives are expected to be lower. An analogous reasoning applies for imagesequences. If there is a pedestrian located within a frame, the same pedestrianis expected to appear close to the same location in neighbor frames. Therefore,such a location has chances of receiving high classification scores duringseveral frames, while false positives are expected to be more spurious. In thispaper we propose to exploit such correlations for improving the accuracy ofbase pedestrian classifiers. In particular, we propose to use two-stageclassifiers which not only rely on the image descriptors required by the baseclassifiers but also on the response of such base classifiers in a givenspatiotemporal neighborhood. More specifically, we train pedestrian classifiersusing a stacked sequential learning (SSL) paradigm. We use a new pedestriandataset we have acquired from a car to evaluate our proposal at different framerates. We also test on a well known dataset: Caltech. The obtained results showthat our SSL proposal boosts detection accuracy significantly with a minimalimpact on the computational cost. Interestingly, SSL improves more the accuracyat the most dangerous situations, i.e. when a pedestrian is close to thecamera.
arxiv-6900-257 | Finding Motif Sets in Time Series | http://arxiv.org/pdf/1407.3685v1.pdf | author:Anthony Bagnall, Jon Hills, Jason Lines category:cs.LG cs.DB published:2014-07-14 summary:Time-series motifs are representative subsequences that occur frequently in atime series; a motif set is the set of subsequences deemed to be instances of agiven motif. We focus on finding motif sets. Our motivation is to detect motifsets in household electricity-usage profiles, representing repeated patterns ofhousehold usage. We propose three algorithms for finding motif sets. Two are greedy algorithmsbased on pairwise comparison, and the third uses a heuristic measure of setquality to find the motif set directly. We compare these algorithms onsimulated datasets and on electricity-usage data. We show that Scan MK, thesimplest way of using the best-matching pair to find motif sets, is lessaccurate on our synthetic data than Set Finder and Cluster MK, although thelatter is very sensitive to parameter settings. We qualitatively analyse theoutputs for the electricity-usage data and demonstrate that both Scan MK andSet Finder can discover useful motif sets in such data.
arxiv-6900-258 | Memristor models for machine learning | http://arxiv.org/pdf/1406.2210v2.pdf | author:Juan Pablo Carbajal, Joni Dambre, Michiel Hermans, Benjamin Schrauwen category:cs.LG published:2014-06-09 summary:In the quest for alternatives to traditional CMOS, it is being suggested thatdigital computing efficiency and power can be improved by matching theprecision to the application. Many applications do not need the high precisionthat is being used today. In particular, large gains in area- and powerefficiency could be achieved by dedicated analog realizations of approximatecomputing engines. In this work, we explore the use of memristor networks foranalog approximate computation, based on a machine learning framework calledreservoir computing. Most experimental investigations on the dynamics ofmemristors focus on their nonvolatile behavior. Hence, the volatility that ispresent in the developed technologies is usually unwanted and it is notincluded in simulation models. In contrast, in reservoir computing, volatilityis not only desirable but necessary. Therefore, in this work, we propose twodifferent ways to incorporate it into memristor simulation models. The first isan extension of Strukov's model and the second is an equivalent Wiener modelapproximation. We analyze and compare the dynamical properties of these modelsand discuss their implications for the memory and the nonlinear processingcapacity of memristor networks. Our results indicate that device variability,increasingly causing problems in traditional computer design, is an asset inthe context of reservoir computing. We conclude that, although both modelscould lead to useful memristor based reservoir computing systems, theircomputational performance will differ. Therefore, experimental modelingresearch is required for the development of accurate volatile memristor models.
arxiv-6900-259 | An Enhancement Neighborhood connected Segmentation for 2D-Cellular Image | http://arxiv.org/pdf/1407.3664v1.pdf | author:Mohammed M. Abdelsamea category:cs.CV published:2014-07-14 summary:A good segmentation result depends on a set of "correct" choice for theseeds. When the input images are noisy, the seeds may fall on atypical pixelsthat are not representative of the region statistics. This can lead toerroneous segmentation results. In this paper, an automatic seeded regiongrowing algorithm is proposed for cellular image segmentation. First, theregions of interest (ROIs) extracted from the preprocessed image. Second, theinitial seeds are automatically selected based on ROIs extracted from theimage. Third, the most reprehensive seeds are selected using a machine learningalgorithm. Finally, the cellular image is segmented into regions where eachregion corresponds to a seed. The aim of the proposed is to automaticallyextract the Region of Interests (ROI) from in the cellular images in terms ofovercoming the explosion, under segmentation and over segmentation problems.Experimental results show that the proposed algorithm can improve the segmentedimage and the segmented results are less noisy as compared to some existingalgorithms.
arxiv-6900-260 | Self Organization Map based Texture Feature Extraction for Efficient Medical Image Categorization | http://arxiv.org/pdf/1408.4143v1.pdf | author:Marghny H. Mohamed, Mohammed M. Abdelsamea category:cs.CV cs.NE published:2014-07-14 summary:Texture is one of the most important properties of visual surface that helpsin discriminating one object from another or an object from background. Theself-organizing map (SOM) is an excellent tool in exploratory phase of datamining. It projects its input space on prototypes of a low-dimensional regulargrid that can be effectively utilized to visualize and explore properties ofthe data. This paper proposes an enhancement extraction method for accurateextracting features for efficient image representation it based on SOM neuralnetwork. In this approach, we apply three different partitioning approaches asa region of interested (ROI) selection methods for extracting differentaccurate textural features from medical image as a primary step of ourextraction method. Fisherfaces feature selection is used, for selectingdiscriminated features form extracted textural features. Experimental resultshowed the high accuracy of medical image categorization with our proposedextraction method. Experiments held on Mammographic Image Analysis Society(MIAS) dataset.
arxiv-6900-261 | Toward Network-based Keyword Extraction from Multitopic Web Documents | http://arxiv.org/pdf/1407.3636v1.pdf | author:Sabina Šišović, Sanda Martinčić-Ipšić, Ana Meštrović category:cs.CL cs.IR published:2014-07-14 summary:In this paper we analyse the selectivity measure calculated from the complexnetwork in the task of the automatic keyword extraction. Texts, collected fromdifferent web sources (portals, forums), are represented as directed andweighted co-occurrence complex networks of words. Words are nodes and links areestablished between two nodes if they are directly co-occurring within thesentence. We test different centrality measures for ranking nodes - keywordcandidates. The promising results are achieved using the selectivity measure.Then we propose an approach which enables extracting word pairs according tothe values of the in/out selectivity and weight measures combined withfiltering.
arxiv-6900-262 | On the Power of Adaptivity in Matrix Completion and Approximation | http://arxiv.org/pdf/1407.3619v1.pdf | author:Akshay Krishnamurthy, Aarti Singh category:stat.ML cs.LG published:2014-07-14 summary:We consider the related tasks of matrix completion and matrix approximationfrom missing data and propose adaptive sampling procedures for both problems.We show that adaptive sampling allows one to eliminate standard incoherenceassumptions on the matrix row space that are necessary for passive samplingprocedures. For exact recovery of a low-rank matrix, our algorithm judiciouslyselects a few columns to observe in full and, with few additional measurements,projects the remaining columns onto their span. This algorithm exactly recoversan $n \times n$ rank $r$ matrix using $O(nr\mu_0 \log^2(r))$ observations,where $\mu_0$ is a coherence parameter on the column space of the matrix. Inaddition to completely eliminating any row space assumptions that have pervadedthe literature, this algorithm enjoys a better sample complexity than anyexisting matrix completion algorithm. To certify that this improvement is dueto adaptive sampling, we establish that row space coherence is necessary forpassive sampling algorithms to achieve non-trivial sample complexity bounds. For constructing a low-rank approximation to a high-rank input matrix, wepropose a simple algorithm that thresholds the singular values of a zero-filledversion of the input matrix. The algorithm computes an approximation that isnearly as good as the best rank-$r$ approximation using $O(nr\mu \log^2(n))$samples, where $\mu$ is a slightly different coherence parameter on the matrixcolumns. Again we eliminate assumptions on the row space.
arxiv-6900-263 | Measuring Atmospheric Scattering from Digital Images of Urban Scenery using Temporal Polarization-Based Vision | http://arxiv.org/pdf/1407.3540v1.pdf | author:Tarek El-Gaaly, Joshua Gluckman category:cs.CV published:2014-07-14 summary:Particulate Matter (PM) is a form of air pollution that visually degradesurban scenery and is hazardous to human health and the environment. Currentmonitoring devices are limited in measuring average PM over large areas.Quantifying the visual effects of haze in digital images of urban scenery andcorrelating these effects to PM levels is a vital step in more practicallymonitoring our environment. Current image haze extraction algorithms removehaze from the scene for the sole purpose of enhancing vision. We present twoalgorithms which bridge the gap between image haze extraction and environmentalmonitoring. We provide a means of measuring atmospheric scattering from imagesof urban scenery by incorporating temporal knowledge. In doing so, we alsopresent a method of recovering an accurate depthmap of the scene and recoveringthe scene without the visual effects of haze. We compare our algorithm to threeknown haze removal methods. The algorithms are composed of an optimization overa model of haze formation in images and an optimization using a constraint ofconstant depth over a sequence of images taken over time. These algorithms notonly measure atmospheric scattering, but also recover a more accurate depthmapand dehazed image. The measurements of atmospheric scattering this researchproduces, can be directly correlated to PM levels and therefore pave the way tomonitoring the health of the environment by visual means. Accurate atmosphericsensing from digital images is a challenging and under-researched problem. Thiswork provides an important step towards a more practical and accurate visualmeans of measuring PM from digital images.
arxiv-6900-264 | Modeling the Complex Dynamics and Changing Correlations of Epileptic Events | http://arxiv.org/pdf/1402.6951v2.pdf | author:Drausin F. Wulsin, Emily B. Fox, Brian Litt category:stat.ML q-bio.NC stat.AP published:2014-02-27 summary:Patients with epilepsy can manifest short, sub-clinical epileptic "bursts" inaddition to full-blown clinical seizures. We believe the relationship betweenthese two classes of events---something not previously studiedquantitatively---could yield important insights into the nature and intrinsicdynamics of seizures. A goal of our work is to parse these complex epilepticevents into distinct dynamic regimes. A challenge posed by the intracranial EEG(iEEG) data we study is the fact that the number and placement of electrodescan vary between patients. We develop a Bayesian nonparametric Markov switchingprocess that allows for (i) shared dynamic regimes between a variable number ofchannels, (ii) asynchronous regime-switching, and (iii) an unknown dictionaryof dynamic regimes. We encode a sparse and changing set of dependencies betweenthe channels using a Markov-switching Gaussian graphical model for theinnovations process driving the channel dynamics and demonstrate the importanceof this model in parsing and out-of-sample predictions of iEEG data. We showthat our model produces intuitive state assignments that can help automateclinical analysis of seizures and enable the comparison of sub-clinical burstsand full clinical seizures.
arxiv-6900-265 | A Generalized Online Mirror Descent with Applications to Classification and Regression | http://arxiv.org/pdf/1304.2994v3.pdf | author:Francesco Orabona, Koby Crammer, Nicolò Cesa-Bianchi category:cs.LG published:2013-04-10 summary:Online learning algorithms are fast, memory-efficient, easy to implement, andapplicable to many prediction problems, including classification, regression,and ranking. Several online algorithms were proposed in the past few decades,some based on additive updates, like the Perceptron, and some on multiplicativeupdates, like Winnow. A unifying perspective on the design and the analysis ofonline algorithms is provided by online mirror descent, a general predictionstrategy from which most first-order algorithms can be obtained as specialcases. We generalize online mirror descent to time-varying regularizers withgeneric updates. Unlike standard mirror descent, our more general formulationalso captures second order algorithms, algorithms for composite losses andalgorithms for adaptive filtering. Moreover, we recover, and sometimes improve,known regret bounds as special cases of our analysis using specificregularizers. Finally, we show the power of our approach by deriving a newsecond order algorithm with a regret bound invariant with respect to arbitraryrescalings of individual features.
arxiv-6900-266 | A New Optimal Stepsize For Approximate Dynamic Programming | http://arxiv.org/pdf/1407.2676v2.pdf | author:Ilya O. Ryzhov, Peter I. Frazier, Warren B. Powell category:math.OC cs.AI cs.LG cs.SY stat.ML published:2014-07-10 summary:Approximate dynamic programming (ADP) has proven itself in a wide range ofapplications spanning large-scale transportation problems, health care, revenuemanagement, and energy systems. The design of effective ADP algorithms has manydimensions, but one crucial factor is the stepsize rule used to update a valuefunction approximation. Many operations research applications arecomputationally intensive, and it is important to obtain good results quickly.Furthermore, the most popular stepsize formulas use tunable parameters and canproduce very poor results if tuned improperly. We derive a new stepsize rulethat optimizes the prediction error in order to improve the short-termperformance of an ADP algorithm. With only one, relatively insensitive tunableparameter, the new rule adapts to the level of noise in the problem andproduces faster convergence in numerical experiments.
arxiv-6900-267 | Brain-like associative learning using a nanoscale non-volatile phase change synaptic device array | http://arxiv.org/pdf/1406.4951v4.pdf | author:Sukru Burc Eryilmaz, Duygu Kuzum, Rakesh Jeyasingh, SangBum Kim, Matthew BrightSky, Chung Lam, H. -S. Philip Wong category:cs.NE cs.LG published:2014-06-19 summary:Recent advances in neuroscience together with nanoscale electronic devicetechnology have resulted in huge interests in realizing brain-like computinghardwares using emerging nanoscale memory devices as synaptic elements.Although there has been experimental work that demonstrated the operation ofnanoscale synaptic element at the single device level, network level studieshave been limited to simulations. In this work, we demonstrate, usingexperiments, array level associative learning using phase change synapticdevices connected in a grid like configuration similar to the organization ofthe biological brain. Implementing Hebbian learning with phase change memorycells, the synaptic grid was able to store presented patterns and recallmissing patterns in an associative brain-like fashion. We found that the systemis robust to device variations, and large variations in cell resistance statescan be accommodated by increasing the number of training epochs. We illustratedthe tradeoff between variation tolerance of the network and the overall energyconsumption, and found that energy consumption is decreased significantly forlower variation tolerance.
arxiv-6900-268 | Monte Carlo Simulation for Lasso-Type Problems by Estimator Augmentation | http://arxiv.org/pdf/1401.4425v2.pdf | author:Qing Zhou category:stat.ME stat.ML published:2014-01-17 summary:Regularized linear regression under the $\ell_1$ penalty, such as the Lasso,has been shown to be effective in variable selection and sparse modeling. Thesampling distribution of an $\ell_1$-penalized estimator $\hat{\beta}$ is hardto determine as the estimator is defined by an optimization problem that ingeneral can only be solved numerically and many of its components may beexactly zero. Let $S$ be the subgradient of the $\ell_1$ norm of thecoefficient vector $\beta$ evaluated at $\hat{\beta}$. We find that the jointsampling distribution of $\hat{\beta}$ and $S$, together called an augmentedestimator, is much more tractable and has a closed-form density under a normalerror distribution in both low-dimensional ($p\leq n$) and high-dimensional($p>n$) settings. Given $\beta$ and the error variance $\sigma^2$, one mayemploy standard Monte Carlo methods, such as Markov chain Monte Carlo andimportance sampling, to draw samples from the distribution of the augmentedestimator and calculate expectations with respect to the sampling distributionof $\hat{\beta}$. We develop a few concrete Monte Carlo algorithms anddemonstrate with numerical examples that our approach may offer huge advantagesand great flexibility in studying sampling distributions in $\ell_1$-penalizedlinear regression. We also establish nonasymptotic bounds on the differencebetween the true sampling distribution of $\hat{\beta}$ and its estimatorobtained by plugging in estimated parameters, which justifies the validity ofMonte Carlo simulation from an estimated sampling distribution even when $p\ggn\to \infty$.
arxiv-6900-269 | Regression Trees for Longitudinal Data | http://arxiv.org/pdf/1309.7733v3.pdf | author:Madan Gopal Kundu, Jaroslaw Harezlak category:stat.ME stat.ML published:2013-09-30 summary:While studying response trajectory, often the population of interest may bediverse enough to exist distinct subgroups within it and the longitudinalchange in response may not be uniform in these subgroups. That is, thetimeslope and/or influence of covariates in longitudinal profile may vary amongthese different subgroups. For example, Raudenbush (2001) used depression as anexample to argue that it is incorrect to assume that all the people in a givenpopulation would be experiencing either increasing or decreasing levels ofdepression. In such cases, traditional linear mixed effects model (assumingcommon parametric form for covariates and time) is not directly applicable forthe entire population as a group-averaged trajectory can mask importantsubgroup differences. Our aim is to identify and characterize longitudinallyhomogeneous subgroups based on the combination of baseline covariates in themost parsimonious way. This goal can be achieved via constructing regressiontree for longitudinal data using baseline covariates as partitioning variables.We have proposed LongCART algorithm to construct regression tree for thelongitudinal data. In each node, the proposed LongCART algorithm determines theneed for further splitting (i.e. whether parameter(s) of longitudinal profileis influenced by any baseline attributes) via parameter instability tests andthus the decision of further splitting is type-I error controlled. We haveobtained the asymptotic results for the proposed instability test and examinedfinite sample behavior of the whole algorithm through simulation studies.Finally, we have applied the LongCART algorithm to study the longitudinalchanges in choline level among HIV patients.
arxiv-6900-270 | Learning Two-input Linear and Nonlinear Analog Functions with a Simple Chemical System | http://arxiv.org/pdf/1404.0427v2.pdf | author:Peter Banda, Christof Teuscher category:q-bio.MN cs.LG published:2014-04-02 summary:The current biochemical information processing systems behave in apredetermined manner because all features are defined during the design phase.To make such unconventional computing systems reusable and programmable forbiomedical applications, adaptation, learning, and self-modification based onexternal stimuli would be highly desirable. However, so far, it has been toochallenging to implement these in wet chemistries. In this paper we extend thechemical perceptron, a model previously proposed by the authors, to function asan analog instead of a binary system. The new analog asymmetric signalperceptron learns through feedback and supports Michaelis-Menten kinetics. Theresults show that our perceptron is able to learn linear and nonlinear(quadratic) functions of two inputs. To the best of our knowledge, it is thefirst simulated chemical system capable of doing so. The small number ofspecies and reactions and their simplicity allows for a mapping to an actualwet implementation using DNA-strand displacement or deoxyribozymes. Our resultsare an important step toward actual biochemical systems that can learn andadapt.
arxiv-6900-271 | Robust Hierarchical Clustering | http://arxiv.org/pdf/1401.0247v2.pdf | author:Maria-Florina Balcan, Yingyu Liang, Pramod Gupta category:cs.LG cs.DS published:2014-01-01 summary:One of the most widely used techniques for data clustering is agglomerativeclustering. Such algorithms have been long used across many different fieldsranging from computational biology to social sciences to computer vision inpart because their output is easy to interpret. Unfortunately, it is wellknown, however, that many of the classic agglomerative clustering algorithmsare not robust to noise. In this paper we propose and analyze a new robustalgorithm for bottom-up agglomerative clustering. We show that our algorithmcan be used to cluster accurately in cases where the data satisfies a number ofnatural properties and where the traditional agglomerative algorithms fail. Wealso show how to adapt our algorithm to the inductive setting where our givendata is only a small random sample of the entire data set. Experimentalevaluations on synthetic and real world data sets show that our algorithmachieves better performance than other hierarchical algorithms in the presenceof noise.
arxiv-6900-272 | Analysis of Amoeba Active Contours | http://arxiv.org/pdf/1310.0097v2.pdf | author:Martin Welk category:cs.CV published:2013-09-30 summary:Subject of this paper is the theoretical analysis of structure-adaptivemedian filter algorithms that approximate curvature-based PDEs for imagefiltering and segmentation. These so-called morphological amoeba filters arebased on a concept introduced by Lerallut et al. They achieve similar resultsas the well-known geodesic active contour and self-snakes PDEs. In the presentwork, the PDE approximated by amoeba active contours is derived for a generalgeometric situation and general amoeba metric. This PDE is structurally similarbut not identical to the geodesic active contour equation. It reproduces theprevious PDE approximation results for amoeba median filters as special cases.Furthermore, modifications of the basic amoeba active contour algorithm areanalysed that are related to the morphological force terms frequently used withgeodesic active contours. Experiments demonstrate the basic behaviour of amoebaactive contours and its similarity to geodesic active contours.
arxiv-6900-273 | Counting Markov Blanket Structures | http://arxiv.org/pdf/1407.2483v2.pdf | author:Shyam Visweswaran, Gregory F. Cooper category:stat.ML cs.AI cs.LG published:2014-07-09 summary:Learning Markov blanket (MB) structures has proven useful in performingfeature selection, learning Bayesian networks (BNs), and discovering causalrelationships. We present a formula for efficiently determining the number ofMB structures given a target variable and a set of other variables. Asexpected, the number of MB structures grows exponentially. However, we showquantitatively that there are many fewer MB structures that contain the targetvariable than there are BN structures that contain it. In particular, the ratioof BN structures to MB structures appears to increase exponentially in thenumber of variables.
arxiv-6900-274 | Extreme State Aggregation Beyond MDPs | http://arxiv.org/pdf/1407.3341v1.pdf | author:Marcus Hutter category:cs.AI cs.LG published:2014-07-12 summary:We consider a Reinforcement Learning setup where an agent interacts with anenvironment in observation-reward-action cycles without any (esp.\ MDP)assumptions on the environment. State aggregation and more generally featurereinforcement learning is concerned with mapping histories/raw-states toreduced/aggregated states. The idea behind both is that the resulting reducedprocess (approximately) forms a small stationary finite-state MDP, which canthen be efficiently solved or learnt. We considerably generalize existingaggregation results by showing that even if the reduced process is not an MDP,the (q-)value functions and (optimal) policies of an associated MDP with samestate-space size solve the original problem, as long as the solution canapproximately be represented as a function of the reduced states. This impliesan upper bound on the required state space size that holds uniformly for all RLproblems. It may also explain why RL algorithms designed for MDPs sometimesperform well beyond MDPs.
arxiv-6900-275 | Offline to Online Conversion | http://arxiv.org/pdf/1407.3334v1.pdf | author:Marcus Hutter category:cs.LG cs.IT math.IT math.ST stat.CO stat.TH published:2014-07-12 summary:We consider the problem of converting offline estimators into an onlinepredictor or estimator with small extra regret. Formally this is the problem ofmerging a collection of probability measures over strings of length 1,2,3,...into a single probability measure over infinite sequences. We describe variousapproaches and their pros and cons on various examples. As a side-result wegive an elementary non-heuristic purely combinatoric derivation of Turing'sfamous estimator. Our main technical contribution is to determine thecomputational complexity of online estimators with good guarantees in general.
arxiv-6900-276 | Beyond Disagreement-based Agnostic Active Learning | http://arxiv.org/pdf/1407.2657v2.pdf | author:Chicheng Zhang, Kamalika Chaudhuri category:cs.LG stat.ML published:2014-07-10 summary:We study agnostic active learning, where the goal is to learn a classifier ina pre-specified hypothesis class interactively with as few label queries aspossible, while making no assumptions on the true function generating thelabels. The main algorithms for this problem are {\em{disagreement-based activelearning}}, which has a high label requirement, and {\em{margin-based activelearning}}, which only applies to fairly restricted settings. A major challengeis to find an algorithm which achieves better label complexity, is consistentin an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on twonovel contributions -- a reduction from consistent active learning toconfidence-rated prediction with guaranteed error, and a novel confidence-ratedpredictor.
arxiv-6900-277 | Density Adaptive Parallel Clustering | http://arxiv.org/pdf/1407.3242v1.pdf | author:Marcello La Rocca category:cs.DS cs.LG stat.ML 91C20 published:2014-07-11 summary:In this paper we are going to introduce a new nearest neighbours basedapproach to clustering, and compare it with previous solutions; the resultingalgorithm, which takes inspiration from both DBscan and minimum spanning treeapproaches, is deterministic but proves simpler, faster and doesnt require toset in advance a value for k, the number of clusters.
arxiv-6900-278 | Image Inpainting Using Directional Tensor Product Complex Tight Framelets | http://arxiv.org/pdf/1407.3234v1.pdf | author:Yi Shen, Bin Han, Elena Braverman category:cs.IT cs.CV math.IT published:2014-07-11 summary:In this paper we are particularly interested in the image inpainting problemusing directional complex tight wavelet frames. Under the assumption that framecoefficients of images are sparse, several iterative thresholding algorithmsfor the image inpainting problem have been proposed in the literature. Theoutputs of such iterative algorithms are closely linked to solutions of severalconvex minimization models using the balanced approach which simultaneouslycombines the $l_1$-regularization for sparsity of frame coefficients and the$l_2$-regularization for smoothness of the solution. Due to the redundancy of atight frame, elements of a tight frame could be highly correlated andtherefore, their corresponding frame coefficients of an image are expected toclose to each other. This is called the grouping effect in statistics. In thispaper, we establish the grouping effect property for frame-based convexminimization models using the balanced approach. This result on grouping effectpartially explains the effectiveness of models using the balanced approach forseveral image restoration problems. Inspired by recent development ondirectional tensor product complex tight framelets (TP-CTFs) and theirimpressive performance for the image denoising problem, in this paper wepropose an iterative thresholding algorithm using a single tight frame derivedfrom TP-CTFs for the image inpainting problem. Experimental results show thatour proposed algorithm can handle well both cartoons and texturessimultaneously and performs comparably and often better than several well-knownframe-based iterative thresholding algorithms for the image inpainting problemwithout noise. For the image inpainting problem with additive zero-mean i.i.d.Gaussian noise, our proposed algorithm using TP-CTFs performs superior thanother known state-of-the-art frame-based image inpainting algorithms.
arxiv-6900-279 | Algorithmic Identification of Probabilities | http://arxiv.org/pdf/1311.7385v3.pdf | author:Paul M. B. Vitanyi, Nick Chater category:cs.LG published:2013-11-28 summary:TThe problem is to identify a probability associated with a set of naturalnumbers, given an infinite data sequence of elements from the set. If the givensequence is drawn i.i.d. and the probability mass function involved (thetarget) belongs to a computably enumerable (c.e.) or co-computably enumerable(co-c.e.) set of computable probability mass functions, then there is analgorithm to almost surely identify the target in the limit. The technical toolis the strong law of large numbers. If the set is finite and the elements ofthe sequence are dependent while the sequence is typical in the sense ofMartin-L\"of for at least one measure belonging to a c.e. or co-c.e. set ofcomputable measures, then there is an algorithm to identify in the limit acomputable measure for which the sequence is typical (there may be more thanone such measure). The technical tool is the theory of Kolmogorov complexity.We give the algorithms and consider the associated predictions.
arxiv-6900-280 | Universal Matrix Completion | http://arxiv.org/pdf/1402.2324v2.pdf | author:Srinadh Bhojanapalli, Prateek Jain category:stat.ML cs.IT cs.LG math.IT published:2014-02-10 summary:The problem of low-rank matrix completion has recently generated a lot ofinterest leading to several results that offer exact solutions to the problem.However, in order to do so, these methods make assumptions that can be quiterestrictive in practice. More specifically, the methods assume that: a) theobserved indices are sampled uniformly at random, and b) for every new matrix,the observed indices are sampled afresh. In this work, we address these issuesby providing a universal recovery guarantee for matrix completion that worksfor a variety of sampling schemes. In particular, we show that if the set ofsampled indices come from the edges of a bipartite graph with large spectralgap (i.e. gap between the first and the second singular value), then thenuclear norm minimization based method exactly recovers all low-rank matricesthat satisfy certain incoherence properties. Moreover, we also show that undercertain stricter incoherence conditions, $O(nr^2)$ uniformly sampled entriesare enough to recover any rank-$r$ $n\times n$ matrix, in contrast to the$O(nr\log n)$ sample complexity required by other matrix completion algorithmsas well as existing analyses of the nuclear norm method.
arxiv-6900-281 | Optimally Stabilized PET Image Denoising Using Trilateral Filtering | http://arxiv.org/pdf/1407.3193v1.pdf | author:Awais Mansoor, Ulas Bagci, Daniel J. Mollura category:cs.CV published:2014-07-11 summary:Low-resolution and signal-dependent noise distribution in positron emissiontomography (PET) images makes denoising process an inevitable step prior toqualitative and quantitative image analysis tasks. Conventional PET denoisingmethods either over-smooth small-sized structures due to resolution limitationor make incorrect assumptions about the noise characteristics. Therefore,clinically important quantitative information may be corrupted. To addressthese challenges, we introduced a novel approach to remove signal-dependentnoise in the PET images where the noise distribution was considered asPoisson-Gaussian mixed. Meanwhile, the generalized Anscombe's transformation(GAT) was used to stabilize varying nature of the PET noise. Other than noisestabilization, it is also desirable for the noise removal filter to preservethe boundaries of the structures while smoothing the noisy regions. Indeed, itis important to avoid significant loss of quantitative information such asstandard uptake value (SUV)-based metrics as well as metabolic lesion volume.To satisfy all these properties, we extended bilateral filtering method intotrilateral filtering through multiscaling and optimal Gaussianization process.The proposed method was tested on more than 50 PET-CT images from variouspatients having different cancers and achieved the superior performancecompared to the widely used denoising techniques in the literature.
arxiv-6900-282 | Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation | http://arxiv.org/pdf/1407.3179v1.pdf | author:Awais Mansoor, Ulas Bagci, Daniel J. Mollura category:cs.CV published:2014-07-11 summary:Accurate delineation of pathological lungs from computed tomography (CT)images remains mostly unsolved because available methods fail to provide areliable generic solution due to high variability of abnormality appearance.Local descriptor-based classification methods have shown to work well inannotating pathologies; however, these methods are usually computationallyintensive which restricts their widespread use in real-time or near-real-timeclinical applications. In this paper, we present a novel approach for fast,accurate, reliable segmentation of pathological lungs from CT scans bycombining region-based segmentation method with local descriptor classificationthat is performed on an optimized sampling grid. Our method works in twostages; during stage one, we adapted the fuzzy connectedness (FC) imagesegmentation algorithm to perform initial lung parenchyma extraction. In thesecond stage, texture-based local descriptors are utilized to segment abnormalimaging patterns using a near optimal keypoint analysis by employing centroidof supervoxel as grid points. The quantitative results show that ourpathological lung segmentation method is fast, robust, and improves on currentstandards and has potential to enhance the performance of routine clinicaltasks.
arxiv-6900-283 | CIDI-Lung-Seg: A Single-Click Annotation Tool for Automatic Delineation of Lungs from CT Scans | http://arxiv.org/pdf/1407.3176v1.pdf | author:Awais Mansoor, Ulas Bagci, Brent Foster, Ziyue Xu, Deborah Douglas, Jeffrey M. Solomon, Jayaram K. Udupa, Daniel J. Mollura category:cs.CV published:2014-07-11 summary:Accurate and fast extraction of lung volumes from computed tomography (CT)scans remains in a great demand in the clinical environment because theavailable methods fail to provide a generic solution due to wide anatomicalvariations of lungs and existence of pathologies. Manual annotation, currentgold standard, is time consuming and often subject to human bias. On the otherhand, current state-of-the-art fully automated lung segmentation methods failto make their way into the clinical practice due to their inability toefficiently incorporate human input for handling misclassifications and praxis.This paper presents a lung annotation tool for CT images that is interactive,efficient, and robust. The proposed annotation tool produces an "as accurate aspossible" initial annotation based on the fuzzy-connectedness imagesegmentation, followed by efficient manual fixation of the initial extractionif deemed necessary by the practitioner. To provide maximum flexibility to theusers, our annotation tool is supported in three major operating systems(Windows, Linux, and the Mac OS X). The quantitative results comparing our freesoftware with commercially available lung segmentation tools show higher degreeof consistency and precision of our software with a considerable potential toenhance the performance of routine clinical tasks.
arxiv-6900-284 | Multiple chaotic central pattern generators with learning for legged locomotion and malfunction compensation | http://arxiv.org/pdf/1407.3269v1.pdf | author:Guanjiao Ren, Weihai Chen, Sakyasingha Dasgupta, Christoph Kolodziejski, Florentin Wörgötter, Poramate Manoonpong category:cs.AI cs.LG cs.NE cs.RO I.2.9; I.2.6 published:2014-07-11 summary:An originally chaotic system can be controlled into various periodicdynamics. When it is implemented into a legged robot's locomotion control as acentral pattern generator (CPG), sophisticated gait patterns arise so that therobot can perform various walking behaviors. However, such a single chaotic CPGcontroller has difficulties dealing with leg malfunction. Specifically, in thescenarios presented here, its movement permanently deviates from the desiredtrajectory. To address this problem, we extend the single chaotic CPG tomultiple CPGs with learning. The learning mechanism is based on a simulatedannealing algorithm. In a normal situation, the CPGs synchronize and theirdynamics are identical. With leg malfunction or disability, the CPGs losesynchronization leading to independent dynamics. In this case, the learningmechanism is applied to automatically adjust the remaining legs' oscillationfrequencies so that the robot adapts its locomotion to deal with themalfunction. As a consequence, the trajectory produced by the multiple chaoticCPGs resembles the original trajectory far better than the one produced by onlya single CPG. The performance of the system is evaluated first in a physicalsimulation of a quadruped as well as a hexapod robot and finally in a realsix-legged walking machine called AMOSII. The experimental results presentedhere reveal that using multiple CPGs with learning is an effective approach foradaptive locomotion generation where, for instance, different body parts haveto perform independent movements for malfunction compensation.
arxiv-6900-285 | Charge Scheduling of an Energy Storage System under Time-of-use Pricing and a Demand Charge | http://arxiv.org/pdf/1407.3077v1.pdf | author:Yourim Yoon, Yong-Hyuk Kim category:cs.NE published:2014-07-11 summary:A real-coded genetic algorithm is used to schedule the charging of an energystorage system (ESS), operated in tandem with renewable power by an electricityconsumer who is subject to time-of-use pricing and a demand charge. Simulationsbased on load and generation profiles of typical residential customers showthat an ESS scheduled by our algorithm can reduce electricity costs byapproximately 17%, compared to a system without an ESS, and by 8% compared to ascheduling algorithm based on net power.
arxiv-6900-286 | Agent Behavior Prediction and Its Generalization Analysis | http://arxiv.org/pdf/1404.4960v2.pdf | author:Fei Tian, Haifang Li, Wei Chen, Tao Qin, Enhong Chen, Tie-Yan Liu category:cs.LG published:2014-04-19 summary:Machine learning algorithms have been applied to predict agent behaviors inreal-world dynamic systems, such as advertiser behaviors in sponsored searchand worker behaviors in crowdsourcing. The behavior data in these systems aregenerated by live agents: once the systems change due to the adoption of theprediction models learnt from the behavior data, agents will observe andrespond to these changes by changing their own behaviors accordingly. As aresult, the behavior data will evolve and will not be identically andindependently distributed, posing great challenges to the theoretical analysison the machine learning algorithms for behavior prediction. To tackle thischallenge, in this paper, we propose to use Markov Chain in Random Environments(MCRE) to describe the behavior data, and perform generalization analysis ofthe machine learning algorithms on its basis. Since the one-step transitionprobability matrix of MCRE depends on both previous states and the randomenvironment, conventional techniques for generalization analysis cannot bedirectly applied. To address this issue, we propose a novel technique thattransforms the original MCRE into a higher-dimensional time-homogeneous Markovchain. The new Markov chain involves more variables but is more regular, andthus easier to deal with. We prove the convergence of the new Markov chain whentime approaches infinity. Then we prove a generalization bound for the machinelearning algorithms on the behavior data generated by the new Markov chain,which depends on both the Markovian parameters and the covering number of thefunction class compounded by the loss function for behavior prediction and thebehavior prediction model. To the best of our knowledge, this is the first workthat performs the generalization analysis on data generated by complexprocesses in real-world dynamic systems.
arxiv-6900-287 | An SVM Based Approach for Cardiac View Planning | http://arxiv.org/pdf/1407.3026v1.pdf | author:Ramasubramanian Sundararajan, Hima Patel, Dattesh Shanbhag, Vivek Vaidya category:cs.LG cs.CV published:2014-07-11 summary:We consider the problem of automatically prescribing oblique planes (shortaxis, 4 chamber and 2 chamber views) in Cardiac Magnetic Resonance Imaging(MRI). A concern with technologist-driven acquisitions of these planes is thequality and time taken for the total examination. We propose an automatedsolution incorporating anatomical features external to the cardiac region. Thesolution uses support vector machine regression models wherein complexity andfeature selection are optimized using multi-objective genetic algorithms.Additionally, we examine the robustness of our approach by training our modelson images with additive Rician-Gaussian mixtures at varying Signal to Noise(SNR) levels. Our approach has shown promising results, with an angulardeviation of less than 15 degrees on 90% cases across oblique planes, measuredin terms of average 6-fold cross validation performance -- this is generallywithin acceptable bounds of variation as specified by clinicians.
arxiv-6900-288 | Biclustering Via Sparse Clustering | http://arxiv.org/pdf/1407.3010v1.pdf | author:Qian Liu, Guanhua Chen, Michael R. Kosorok, Eric Bair category:stat.ME stat.ML published:2014-07-11 summary:In many situations it is desirable to identify clusters that differ withrespect to only a subset of features. Such clusters may represent homogeneoussubgroups of patients with a disease, such as cancer or chronic pain. We definea bicluster to be a submatrix U of a larger data matrix X such that thefeatures and observations in U differ from those not contained in U. Forexample, the observations in U could have different means or variances withrespect to the features in U. We propose a general framework for biclusteringbased on the sparse clustering method of Witten and Tibshirani (2010). Wedevelop a method for identifying features that belong to biclusters. Thisframework can be used to identify biclusters that differ with respect to themeans of the features, the variance of the features, or more generaldifferences. We apply these methods to several simulated and real-world datasets and compare the results of our method with several previously publishedmethods. The results of our method compare favorably with existing methods withrespect to both predictive accuracy and computing time.
arxiv-6900-289 | Evolutionary Robotics on the Web with WebGL and Javascript | http://arxiv.org/pdf/1406.3337v3.pdf | author:Jared Moore, Anthony Clark, Philip McKinley category:cs.NE cs.HC published:2014-06-12 summary:Web-based applications are highly accessible to users, providing rich,interactive content while eliminating the need to install software locally.However, evolutionary robotics (ER) has faced challenges in this domain asweb-based technologies have not been amenable to 3D physics simulations.Traditionally, physics-based simulations require a local installation and ahigh degree of user knowledge to configure an environment, but the emergence ofJavascript-based physics engines enables complex simulations to be executed inweb browsers. These developments create opportunities for ER research to reachnew audiences by increasing accessibility. In this work, we introduce twoweb-based tools we have built to facilitate the exchange of ideas with otherresearchers as well as outreach to K-12 students and the general public. Thefirst tool is intended to distribute and exchange ER research results, whilethe second is a completely browser-based implementation of an ER environment.
arxiv-6900-290 | A Proposed Infrastructure for Adding Online Interaction to Any Evolutionary Domain | http://arxiv.org/pdf/1407.3000v1.pdf | author:Paul Szerlip, Kenneth O. Stanley category:cs.NE published:2014-07-11 summary:To address the difficulty of creating online collaborative evolutionarysystems, this paper presents a new prototype library called WorldwideInfrastructure for Neuroevolution (WIN) and its accompanying site WIN Online(http://winark.org/). The WIN library is a collection of software packagesbuilt on top of Node.js that reduce the complexity of creating fullypersistent, online, and interactive (or automated) evolutionary platformsaround any domain. WIN Online is the public interface for WIN, providing anonline collection of domains built with the WIN library that lets novice andexpert users browse and meaningfully contribute to ongoing experiments. Thelong term goal of WIN is to make it trivial to connect any platform to theworld, providing both a stream of online users, and archives of data anddiscoveries for later extension by humans or computers.
arxiv-6900-291 | Hidden Markov Model Based Part of Speech Tagger for Sinhala Language | http://arxiv.org/pdf/1407.2989v1.pdf | author:A. J. P. M. P. Jayaweera, N. G. J. Dias category:cs.CL I.2.7 published:2014-07-10 summary:In this paper we present a fundamental lexical semantics of Sinhala languageand a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhalalanguage. In any Natural Language processing task, Part of Speech is a veryvital topic, which involves analysing of the construction, behaviour and thedynamics of the language, which the knowledge could utilized in computationallinguistics analysis and automation applications. Though Sinhala is amorphologically rich and agglutinative language, in which words are inflectedwith various grammatical features, tagging is very essential for furtheranalysis of the language. Our research is based on statistical based approach,in which the tagging process is done by computing the tag sequence probabilityand the word-likelihood probability from the given corpus, where the linguisticknowledge is automatically extracted from the annotated corpus. The currenttagger could reach more than 90% of accuracy for known words.
arxiv-6900-292 | FAME: Face Association through Model Evolution | http://arxiv.org/pdf/1407.2987v1.pdf | author:Eren Golge, Pinar Duygulu category:cs.CV cs.AI cs.IR cs.LG published:2014-07-10 summary:We attack the problem of learning face models for public faces fromweakly-labelled images collected from web through querying a name. The data isvery noisy even after face detection, with several irrelevant facescorresponding to other people. We propose a novel method, Face Associationthrough Model Evolution (FAME), that is able to prune the data in an iterativeway, for the face models associated to a name to evolve. The idea is based oncapturing discriminativeness and representativeness of each instance andeliminating the outliers. The final models are used to classify faces on noveldatasets with possibly different characteristics. On benchmark datasets, ourresults are comparable to or better than state-of-the-art studies for the taskof face identification.
arxiv-6900-293 | Efficiently inferring community structure in bipartite networks | http://arxiv.org/pdf/1403.2933v2.pdf | author:Daniel B. Larremore, Aaron Clauset, Abigail Z. Jacobs category:cs.SI physics.soc-ph q-bio.QM stat.ML published:2014-03-12 summary:Bipartite networks are a common type of network data in which there are twotypes of vertices, and only vertices of different types can be connected. Whilebipartite networks exhibit community structure like their unipartitecounterparts, existing approaches to bipartite community detection havedrawbacks, including implicit parameter choices, loss of information throughone-mode projections, and lack of interpretability. Here we solve the communitydetection problem for bipartite networks by formulating a bipartite stochasticblock model, which explicitly includes vertex type information and may betrivially extended to $k$-partite networks. This bipartite stochastic blockmodel yields a projection-free and statistically principled method forcommunity detection that makes clear assumptions and parameter choices andyields interpretable results. We demonstrate this model's ability toefficiently and accurately find community structure in synthetic bipartitenetworks with known structure and in real-world bipartite networks with unknownstructure, and we characterize its performance in practical contexts.
arxiv-6900-294 | Efficient Classification for Metric Data | http://arxiv.org/pdf/1306.2547v3.pdf | author:Lee-Ad Gottlieb, Aryeh Kontorovich, Robert Krauthgamer category:cs.LG cs.DS stat.ML published:2013-06-11 summary:Recent advances in large-margin classification of data residing in generalmetric spaces (rather than Hilbert spaces) enable classification under variousnatural metrics, such as string edit and earthmover distance. A generalframework developed for this purpose by von Luxburg and Bousquet [JMLR, 2004]left open the questions of computational efficiency and of providing directbounds on generalization error. We design a new algorithm for classification in general metric spaces, whoseruntime and accuracy depend on the doubling dimension of the data points, andcan thus achieve superior classification performance in many common scenarios.The algorithmic core of our approach is an approximate (rather than exact)solution to the classical problems of Lipschitz extension and of NearestNeighbor Search. The algorithm's generalization performance is guaranteed viathe fat-shattering dimension of Lipschitz classifiers, and we presentexperimental evidence of its superiority to some common kernel methods. As aby-product, we offer a new perspective on the nearest neighbor classifier,which yields significantly sharper risk asymptotics than the classic analysisof Cover and Hart [IEEE Trans. Info. Theory, 1967].
arxiv-6900-295 | Real-Time Impulse Noise Suppression from Images Using an Efficient Weighted-Average Filtering | http://arxiv.org/pdf/1408.3139v1.pdf | author:Hossein Hosseini, Farzad Hessar, Farokh Marvasti category:cs.CV published:2014-07-10 summary:In this paper, we propose a method for real-time high density impulse noisesuppression from images. In our method, we first apply an impulse detector toidentify the corrupted pixels and then employ an innovative weighted-averagefilter to restore them. The filter takes the nearest neighboring interpolatedimage as the initial image and computes the weights according to the relativepositions of the corrupted and uncorrupted pixels. Experimental results showthat the proposed method outperforms the best existing methods in both PSNRmeasure and visual quality and is quite suitable for real-time applications.
arxiv-6900-296 | On the Convergence of the Mean Shift Algorithm in the One-Dimensional Space | http://arxiv.org/pdf/1407.2961v1.pdf | author:Youness Aliyari Ghassabeh category:cs.CV published:2014-07-10 summary:The mean shift algorithm is a non-parametric and iterative technique that hasbeen used for finding modes of an estimated probability density function. Ithas been successfully employed in many applications in specific areas ofmachine vision, pattern recognition, and image processing. Although the meanshift algorithm has been used in many applications, a rigorous proof of itsconvergence is still missing in the literature. In this paper we address theconvergence of the mean shift algorithm in the one-dimensional space and provethat the sequence generated by the mean shift algorithm is a monotone andconvergent sequence.
arxiv-6900-297 | An eigenanalysis of data centering in machine learning | http://arxiv.org/pdf/1407.2904v1.pdf | author:Paul Honeine category:stat.ML cs.CV cs.LG math.SP math.ST stat.TH published:2014-07-10 summary:Many pattern recognition methods rely on statistical information fromcentered data, with the eigenanalysis of an empirical central moment, such asthe covariance matrix in principal component analysis (PCA), as well as partialleast squares regression, canonical-correlation analysis and Fisherdiscriminant analysis. Recently, many researchers advocate working onnon-centered data. This is the case for instance with the singular valuedecomposition approach, with the (kernel) entropy component analysis, with theinformation-theoretic learning framework, and even with nonnegative matrixfactorization. Moreover, one can also consider a non-centered PCA by using thesecond-order non-central moment. The main purpose of this paper is to bridge the gap between these twoviewpoints in designing machine learning methods. To provide a study at thecornerstone of kernel-based machines, we conduct an eigenanalysis of the innerproduct matrices from centered and non-centered data. We derive several resultsconnecting their eigenvalues and their eigenvectors. Furthermore, we explorethe outer product matrices, by providing several results connecting the largesteigenvectors of the covariance matrix and its non-centered counterpart. Theseresults lay the groundwork to several extensions beyond conventional centering,with the weighted mean shift, the rank-one update, and the multidimensionalscaling. Experiments conducted on simulated and real data illustrate therelevance of this work.
arxiv-6900-298 | A Compilation Target for Probabilistic Programming Languages | http://arxiv.org/pdf/1403.0504v2.pdf | author:Brooks Paige, Frank Wood category:cs.AI cs.PL stat.ML published:2014-03-03 summary:Forward inference techniques such as sequential Monte Carlo and particleMarkov chain Monte Carlo for probabilistic programming can be implemented inany programming language by creative use of standardized operating systemfunctionality including processes, forking, mutexes, and shared memory.Exploiting this we have defined, developed, and tested a probabilisticprogramming language intermediate representation language we call probabilisticC, which itself can be compiled to machine code by standard compilers andlinked to operating system libraries yielding an efficient, scalable, portableprobabilistic programming compilation target. This opens up a new hardware andsystems research path for optimizing probabilistic programming systems.
arxiv-6900-299 | Asynchronous Anytime Sequential Monte Carlo | http://arxiv.org/pdf/1407.2864v1.pdf | author:Brooks Paige, Frank Wood, Arnaud Doucet, Yee Whye Teh category:stat.CO stat.ML published:2014-07-10 summary:We introduce a new sequential Monte Carlo algorithm we call the particlecascade. The particle cascade is an asynchronous, anytime alternative totraditional particle filtering algorithms. It uses no barrier synchronizationswhich leads to improved particle throughput and memory efficiency. It is ananytime algorithm in the sense that it can be run forever to emit an unboundednumber of particles while keeping within a fixed memory budget. We prove thatthe particle cascade is an unbiased marginal likelihood estimator which meansthat it can be straightforwardly plugged into existing pseudomarginal methods.
arxiv-6900-300 | XML Matchers: approaches and challenges | http://arxiv.org/pdf/1407.2845v1.pdf | author:Santa Agreste, Pasquale De Meo, Emilio Ferrara, Domenico Ursino category:cs.DB cs.AI cs.IR cs.LG published:2014-07-10 summary:Schema Matching, i.e. the process of discovering semantic correspondencesbetween concepts adopted in different data source schemas, has been a key topicin Database and Artificial Intelligence research areas for many years. In thepast, it was largely investigated especially for classical database models(e.g., E/R schemas, relational databases, etc.). However, in the latest years,the widespread adoption of XML in the most disparate application fields pusheda growing number of researchers to design XML-specific Schema Matchingapproaches, called XML Matchers, aiming at finding semantic matchings betweenconcepts defined in DTDs and XSDs. XML Matchers do not just take well-knowntechniques originally designed for other data models and apply them onDTDs/XSDs, but they exploit specific XML features (e.g., the hierarchicalstructure of a DTD/XSD) to improve the performance of the Schema Matchingprocess. The design of XML Matchers is currently a well-established researcharea. The main goal of this paper is to provide a detailed description andclassification of XML Matchers. We first describe to what extent thespecificities of DTDs/XSDs impact on the Schema Matching task. Then weintroduce a template, called XML Matcher Template, that describes the maincomponents of an XML Matcher, their role and behavior. We illustrate how eachof these components has been implemented in some popular XML Matchers. Weconsider our XML Matcher Template as the baseline for objectively comparingapproaches that, at first glance, might appear as unrelated. The introductionof this template can be useful in the design of future XML Matchers. Finally,we analyze commercial tools implementing XML Matchers and introduce twochallenging issues strictly related to this topic, namely XML source clusteringand uncertainty management in XML Matchers.
