arxiv-6900-1 | A Topic Model Approach to Multi-Modal Similarity | http://arxiv.org/pdf/1405.6886v1.pdf | author:Rasmus Troelsgård, Bjørn Sand Jensen, Lars Kai Hansen category:cs.IR stat.ML published:2014-05-27 summary:Calculating similarities between objects defined by many heterogeneous datamodalities is an important challenge in many multimedia applications. We use amulti-modal topic model as a basis for defining such a similarity betweenobjects. We propose to compare the resulting similarities from different modelrealizations using the non-parametric Mantel test. The approach is evaluated ona music dataset.
arxiv-6900-2 | Supervised Dictionary Learning by a Variational Bayesian Group Sparse Nonnegative Matrix Factorization | http://arxiv.org/pdf/1405.6914v1.pdf | author:Ivan Ivek category:cs.CV cs.LG stat.ML published:2014-05-27 summary:Nonnegative matrix factorization (NMF) with group sparsity constraints isformulated as a probabilistic graphical model and, assuming some observed datahave been generated by the model, a feasible variational Bayesian algorithm isderived for learning model parameters. When used in a supervised learningscenario, NMF is most often utilized as an unsupervised feature extractorfollowed by classification in the obtained feature subspace. Having mapped theclass labels to a more general concept of groups which underlie sparsity of thecoefficients, what the proposed group sparse NMF model allows is incorporatingclass label information to find low dimensional label-driven dictionaries whichnot only aim to represent the data faithfully, but are also suitable for classdiscrimination. Experiments performed in face recognition and facial expressionrecognition domains point to advantages of classification in such label-drivenfeature subspaces over classification in feature subspaces obtained in anunsupervised manner.
arxiv-6900-3 | Large Scale, Large Margin Classification using Indefinite Similarity Measures | http://arxiv.org/pdf/1405.6922v1.pdf | author:Omid Aghazadeh, Stefan Carlsson category:cs.LG cs.CV stat.ML published:2014-05-27 summary:Despite the success of the popular kernelized support vector machines, theyhave two major limitations: they are restricted to Positive Semi-Definite (PSD)kernels, and their training complexity scales at least quadratically with thesize of the data. Many natural measures of similarity between pairs of samplesare not PSD e.g. invariant kernels, and those that are implicitly or explicitlydefined by latent variable models. In this paper, we investigate scalableapproaches for using indefinite similarity measures in large margin frameworks.In particular we show that a normalization of similarity to a subset of thedata points constitutes a representation suitable for linear classifiers. Theresult is a classifier which is competitive to kernelized SVM in terms ofaccuracy, despite having better training and test time complexities.Experimental results demonstrate that on CIFAR-10 dataset, the model equippedwith similarity measures invariant to rigid and non-rigid deformations, can bemade more than 5 times sparser while being more accurate than kernelized SVMusing RBF kernels.
arxiv-6900-4 | Layered Logic Classifiers: Exploring the `And' and `Or' Relations | http://arxiv.org/pdf/1405.6804v2.pdf | author:Zhuowen Tu, Piotr Dollar, Yingnian Wu category:stat.ML cs.LG published:2014-05-27 summary:Designing effective and efficient classifier for pattern analysis is a keyproblem in machine learning and computer vision. Many the solutions to theproblem require to perform logic operations such as `and', `or', and `not'.Classification and regression tree (CART) include these operations explicitly.Other methods such as neural networks, SVM, and boosting learn/compute aweighted sum on features (weak classifiers), which weakly perform the 'and' and'or' operations. However, it is hard for these classifiers to deal with the'xor' pattern directly. In this paper, we propose layered logic classifiers forpatterns of complicated distributions by combining the `and', `or', and `not'operations. The proposed algorithm is very general and easy to implement. Wetest the classifiers on several typical datasets from the Irvine repository andtwo challenging vision applications, object segmentation and pedestriandetection. We observe significant improvements on all the datasets over thewidely used decision stump based AdaBoost algorithm. The resulting classifiershave much less training complexity than decision tree based AdaBoost, and canbe applied in a wide range of domains.
arxiv-6900-5 | Futility Analysis in the Cross-Validation of Machine Learning Models | http://arxiv.org/pdf/1405.6974v1.pdf | author:Max Kuhn category:stat.ML cs.LG published:2014-05-27 summary:Many machine learning models have important structural tuning parameters thatcannot be directly estimated from the data. The common tactic for setting theseparameters is to use resampling methods, such as cross--validation or thebootstrap, to evaluate a candidate set of values and choose the best based onsome pre--defined criterion. Unfortunately, this process can be time consuming.However, the model tuning process can be streamlined by adaptively resamplingcandidate values so that settings that are clearly sub-optimal can bediscarded. The notion of futility analysis is introduced in this context. Anexample is shown that illustrates how adaptive resampling can be used to reducetraining time. Simulation studies are used to understand how the potentialspeed--up is affected by parallel processing techniques.
arxiv-6900-6 | Differentially Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds | http://arxiv.org/pdf/1405.7085v2.pdf | author:Raef Bassily, Adam Smith, Abhradeep Thakurta category:cs.LG cs.CR stat.ML published:2014-05-27 summary:In this paper, we initiate a systematic investigation of differentiallyprivate algorithms for convex empirical risk minimization. Variousinstantiations of this problem have been studied before. We provide newalgorithms and matching lower bounds for private ERM assuming only that eachdata point's contribution to the loss function is Lipschitz bounded and thatthe domain of optimization is bounded. We provide a separate set of algorithmsand matching lower bounds for the setting in which the loss functions are knownto also be strongly convex. Our algorithms run in polynomial time, and in some cases even match theoptimal non-private running time (as measured by oracle complexity). We giveseparate algorithms (and lower bounds) for $(\epsilon,0)$- and$(\epsilon,\delta)$-differential privacy; perhaps surprisingly, the techniquesused for designing optimal algorithms in the two cases are completelydifferent. Our lower bounds apply even to very simple, smooth function families, such aslinear and quadratic functions. This implies that algorithms from previous workcan be used to obtain optimal error rates, under the additional assumption thatthe contributions of each data point to the loss function is smooth. We showthat simple approaches to smoothing arbitrary loss functions (in order to applyprevious techniques) do not yield optimal error rates. In particular, optimalalgorithms were not previously known for problems such as training supportvector machines and the high-dimensional median.
arxiv-6900-7 | Agnostic Learning of Disjunctions on Symmetric Distributions | http://arxiv.org/pdf/1405.6791v2.pdf | author:Vitaly Feldman, Pravesh Kothari category:cs.LG cs.CC cs.DS published:2014-05-27 summary:We consider the problem of approximating and learning disjunctions (orequivalently, conjunctions) on symmetric distributions over $\{0,1\}^n$.Symmetric distributions are distributions whose PDF is invariant under anypermutation of the variables. We give a simple proof that for every symmetricdistribution $\mathcal{D}$, there exists a set of $n^{O(\log{(1/\epsilon)})}$functions $\mathcal{S}$, such that for every disjunction $c$, there is function$p$, expressible as a linear combination of functions in $\mathcal{S}$, suchthat $p$ $\epsilon$-approximates $c$ in $\ell_1$ distance on $\mathcal{D}$ or$\mathbf{E}_{x \sim \mathcal{D}}[ c(x)-p(x)] \leq \epsilon$. This directlygives an agnostic learning algorithm for disjunctions on symmetricdistributions that runs in time $n^{O( \log{(1/\epsilon)})}$. The best knownprevious bound is $n^{O(1/\epsilon^4)}$ and follows from approximation of themore general class of halfspaces (Wimmer, 2010). We also show that there existsa symmetric distribution $\mathcal{D}$, such that the minimum degree of apolynomial that $1/3$-approximates the disjunction of all $n$ variables is$\ell_1$ distance on $\mathcal{D}$ is $\Omega( \sqrt{n})$. Therefore thelearning result above cannot be achieved via $\ell_1$-regression with apolynomial basis used in most other agnostic learning algorithms. Our technique also gives a simple proof that for any product distribution$\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ ofdegree $O(\log{(1/\epsilon)})$ such that $p$ $\epsilon$-approximates $c$ in$\ell_1$ distance on $\mathcal{D}$. This was first proved by Blais et al.(2008) via a more involved argument.
arxiv-6900-8 | Fast and Robust Archetypal Analysis for Representation Learning | http://arxiv.org/pdf/1405.6472v1.pdf | author:Yuansi Chen, Julien Mairal, Zaid Harchaoui category:cs.CV cs.LG stat.ML published:2014-05-26 summary:We revisit a pioneer unsupervised learning technique called archetypalanalysis, which is related to successful data analysis methods such as sparsecoding and non-negative matrix factorization. Since it was proposed, archetypalanalysis did not gain a lot of popularity even though it produces moreinterpretable models than other alternatives. Because no efficientimplementation has ever been made publicly available, its application toimportant scientific problems may have been severely limited. Our goal is tobring back into favour archetypal analysis. We propose a fast optimizationscheme using an active-set strategy, and provide an efficient open-sourceimplementation interfaced with Matlab, R, and Python. Then, we demonstrate theusefulness of archetypal analysis for computer vision tasks, such as codebooklearning, signal classification, and large image collection visualization.
arxiv-6900-9 | The role of dimensionality reduction in linear classification | http://arxiv.org/pdf/1405.6444v1.pdf | author:Weiran Wang, Miguel Á. Carreira-Perpiñán category:cs.LG math.OC stat.ML published:2014-05-26 summary:Dimensionality reduction (DR) is often used as a preprocessing step inclassification, but usually one first fixes the DR mapping, possibly usinglabel information, and then learns a classifier (a filter approach). Bestperformance would be obtained by optimizing the classification error jointlyover DR mapping and classifier (a wrapper approach), but this is a difficultnonconvex problem, particularly with nonlinear DR. Using the method ofauxiliary coordinates, we give a simple, efficient algorithm to train acombination of nonlinear DR and a classifier, and apply it to a RBF mappingwith a linear SVM. This alternates steps where we train the RBF mapping and alinear SVM as usual regression and classification, respectively, with aclosed-form step that coordinates both. The resulting nonlinear low-dimensionalclassifier achieves classification errors competitive with the state-of-the-artbut is fast at training and testing, and allows the user to trade off runtimefor classification accuracy easily. We then study the role of nonlinear DR inlinear classification, and the interplay between the DR mapping, the number oflatent dimensions and the number of classes. When trained jointly, the DRmapping takes an extreme role in eliminating variation: it tends to collapseclasses in latent space, erasing all manifold structure, and lay out classcentroids so they are linearly separable with maximum margin.
arxiv-6900-10 | Robust Temporally Coherent Laplacian Protrusion Segmentation of 3D Articulated Bodies | http://arxiv.org/pdf/1405.6563v1.pdf | author:Fabio Cuzzolin, Diana Mateus, Radu Horaud category:cs.CV cs.GR cs.LG published:2014-05-26 summary:In motion analysis and understanding it is important to be able to fit asuitable model or structure to the temporal series of observed data, in orderto describe motion patterns in a compact way, and to discriminate between them.In an unsupervised context, i.e., no prior model of the moving object(s) isavailable, such a structure has to be learned from the data in a bottom-upfashion. In recent times, volumetric approaches in which the motion is capturedfrom a number of cameras and a voxel-set representation of the body is builtfrom the camera views, have gained ground due to attractive features such asinherent view-invariance and robustness to occlusions. Automatic, unsupervisedsegmentation of moving bodies along entire sequences, in a temporally-coherentand robust way, has the potential to provide a means of constructing abottom-up model of the moving body, and track motion cues that may be laterexploited for motion classification. Spectral methods such as locally linearembedding (LLE) can be useful in this context, as they preserve "protrusions",i.e., high-curvature regions of the 3D volume, of articulated shapes, whileimproving their separation in a lower dimensional space, making them in thisway easier to cluster. In this paper we therefore propose a spectral approachto unsupervised and temporally-coherent body-protrusion segmentation along timesequences. Volumetric shapes are clustered in an embedding space, clusters arepropagated in time to ensure coherence, and merged or split to accommodatechanges in the body's topology. Experiments on both synthetic and realsequences of dense voxel-set data are shown. This supports the ability of theproposed method to cluster body-parts consistently over time in a totallyunsupervised fashion, its robustness to sampling density and shape quality, andits potential for bottom-up model construction
arxiv-6900-11 | Inferring gender of a Twitter user using celebrities it follows | http://arxiv.org/pdf/1405.6667v1.pdf | author:Puneet Singh Ludu category:cs.IR cs.CL published:2014-05-26 summary:This paper addresses the task of user gender classification in social media,with an application to Twitter. The approach automatically predicts gender byleveraging observable information such as the tweet behavior, linguisticcontent of the user's Twitter feed and the celebrities followed by the user.This paper first evaluates linguistic content based features using LIWCdictionary and popular neighborhood features using Wikipedia and Freebase. Thenaugments both features which yielded a significant increase in the accuracy forgender prediction. Results show that rich linguistic features combined withpopular neighborhood prove valuables and promising for additional userclassification needs.
arxiv-6900-12 | Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning | http://arxiv.org/pdf/1405.6524v1.pdf | author:Dan Stowell, Mark D. Plumbley category:cs.SD cs.LG published:2014-05-26 summary:Automatic species classification of birds from their sound is a computationaltool of increasing importance in ecology, conservation monitoring and vocalcommunication studies. To make classification useful in practice, it is crucialto improve its accuracy while ensuring that it can run at big data scales. Manyapproaches use acoustic measures based on spectrogram-type data, such as theMel-frequency cepstral coefficient (MFCC) features which represent amanually-designed summary of spectral information. However, recent work inmachine learning has demonstrated that features learnt automatically from datacan often outperform manually-designed feature transforms. Feature learning canbe performed at large scale and "unsupervised", meaning it requires no manualdata labelling, yet it can improve performance on "supervised" tasks such asclassification. In this work we introduce a technique for feature learning fromlarge volumes of bird sound recordings, inspired by techniques that have provenuseful in other domains. We experimentally compare twelve different featurerepresentations derived from the Mel spectrum (of which six use thistechnique), using four large and diverse databases of bird vocalisations, witha random forest classifier. We demonstrate that MFCCs are of limited power inthis context, leading to worse performance than the raw Mel spectral data.Conversely, we demonstrate that unsupervised feature learning provides asubstantial boost over MFCCs and Mel spectra without adding computationalcomplexity after the model has been trained. The boost is particularly notablefor single-label classification tasks at large scale. The spectro-temporalactivations learned through our procedure resemble spectro-temporal receptivefields calculated from avian primary auditory forebrain.
arxiv-6900-13 | On the Computational Intractability of Exact and Approximate Dictionary Learning | http://arxiv.org/pdf/1405.6664v2.pdf | author:Andreas M. Tillmann category:cs.IT cs.LG math.IT published:2014-05-26 summary:The efficient sparse coding and reconstruction of signal vectors via linearobservations has received a tremendous amount of attention over the lastdecade. In this context, the automated learning of a suitable basis orovercomplete dictionary from training data sets of certain signal classes foruse in sparse representations has turned out to be of particular importanceregarding practical signal processing applications. Most popular dictionarylearning algorithms involve NP-hard sparse recovery problems in each iteration,which may give some indication about the complexity of dictionary learning butdoes not constitute an actual proof of computational intractability. In thistechnical note, we show that learning a dictionary with which a given set oftraining signals can be represented as sparsely as possible is indeed NP-hard.Moreover, we also establish hardness of approximating the solution to withinlarge factors of the optimal sparsity level. Furthermore, we give NP-hardnessand non-approximability results for a recent dictionary learning variationcalled the sensor permutation problem. Along the way, we also obtain a newnon-approximability result for the classical sparse recovery problem fromcompressed sensing.
arxiv-6900-14 | Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces | http://arxiv.org/pdf/1405.6757v1.pdf | author:Sridhar Mahadevan, Bo Liu, Philip Thomas, Will Dabney, Steve Giguere, Nicholas Jacek, Ian Gemp, Ji Liu category:cs.LG published:2014-05-26 summary:In this paper, we set forth a new vision of reinforcement learning developedby us over the past few years, one that yields mathematically rigoroussolutions to longstanding important questions that have remained unresolved:(i) how to design reliable, convergent, and robust reinforcement learningalgorithms (ii) how to guarantee that reinforcement learning satisfiespre-specified "safety" guarantees, and remains in a stable region of theparameter space (iii) how to design "off-policy" temporal difference learningalgorithms in a reliable and stable manner, and finally (iv) how to integratethe study of reinforcement learning into the rich theory of stochasticoptimization. In this paper, we provide detailed answers to all these questionsusing the powerful framework of proximal operators. The key idea that emerges is the use of primal dual spaces connected throughthe use of a Legendre transform. This allows temporal difference updates tooccur in dual spaces, allowing a variety of important technical advantages. TheLegendre transform elegantly generalizes past algorithms for solvingreinforcement learning problems, such as natural gradient methods, which weshow relate closely to the previously unconnected framework of mirror descentmethods. Equally importantly, proximal operator theory enables the systematicdevelopment of operator splitting methods that show how to safely and reliablydecompose complex products of gradients that occur in recent variants ofgradient-based temporal difference learning. This key technical innovationmakes it possible to finally design "true" stochastic gradient methods forreinforcement learning. Finally, Legendre transforms enable a variety of otherbenefits, including modeling sparsity and domain geometry. Our work buildsextensively on recent work on the convergence of saddle-point algorithms, andon the theory of monotone operators.
arxiv-6900-15 | Visualizing Random Forest with Self-Organising Map | http://arxiv.org/pdf/1405.6684v1.pdf | author:Piotr Płoński, Krzysztof Zaremba category:cs.LG published:2014-05-26 summary:Random Forest (RF) is a powerful ensemble method for classification andregression tasks. It consists of decision trees set. Although, a single tree iswell interpretable for human, the ensemble of trees is a black-box model. Thepopular technique to look inside the RF model is to visualize a RF proximitymatrix obtained on data samples with Multidimensional Scaling (MDS) method.Herein, we present a novel method based on Self-Organising Maps (SOM) forrevealing intrinsic relationships in data that lay inside the RF used forclassification tasks. We propose an algorithm to learn the SOM with theproximity matrix obtained from the RF. The visualization of RF proximity matrixwith MDS and SOM is compared. What is more, the SOM learned with the RFproximity matrix has better classification accuracy in comparison to SOMlearned with Euclidean distance. Presented approach enables betterunderstanding of the RF and additionally improves accuracy of the SOM.
arxiv-6900-16 | An Ordered Lasso and Sparse Time-Lagged Regression | http://arxiv.org/pdf/1405.6447v2.pdf | author:Xiaotong Suo, Robert Tibshirani category:stat.AP stat.ML published:2014-05-26 summary:We consider regression scenarios where it is natural to impose an orderconstraint on the coefficients. We propose an order-constrained version ofL1-regularized regression for this problem, and show how to solve itefficiently using the well-known Pool Adjacent Violators Algorithm as itsproximal operator. The main application of this idea is time-lagged regression,where we predict an outcome at time t from features at the previous K timepoints. In this setting it is natural to assume that the coefficients decay aswe move farther away from t, and hence the order constraint is reasonable.Potential applications include financial time series and prediction of dynamicpatient out- comes based on clinical measurements. We illustrate this idea onreal and simulated data.
arxiv-6900-17 | Statistique et Big Data Analytics; Volumétrie, L'Attaque des Clones | http://arxiv.org/pdf/1405.6676v2.pdf | author:Philippe Besse, Nathalie Villa-Vialaneix category:stat.OT cs.LG math.ST stat.TH published:2014-05-26 summary:This article assumes acquired the skills and expertise of a statistician inunsupervised (NMF, k-means, SVD) and supervised learning (regression, CART,random forest). What skills and knowledge do a statistician must acquire toreach the "Volume" scale of big data? After a quick overview of the differentstrategies available and especially of those imposed by Hadoop, the algorithmsof some available learning methods are outlined in order to understand how theyare adapted to the strong stresses of the Map-Reduce functionalities
arxiv-6900-18 | Hybrid Type-Logical Grammars, First-Order Linear Logic and the Descriptive Inadequacy of Lambda Grammars | http://arxiv.org/pdf/1405.6678v1.pdf | author:Richard Moot category:cs.LO cs.CL published:2014-05-26 summary:In this article we show that hybrid type-logical grammars are a fragment offirst-order linear logic. This embedding result has several importantconsequences: it not only provides a simple new proof theory for the calculus,thereby clarifying the proof-theoretic foundations of hybrid type-logicalgrammars, but, since the translation is simple and direct, it also providesseveral new parsing strategies for hybrid type-logical grammars. Second,NP-completeness of hybrid type-logical grammars follows immediately. The mainembedding result also sheds new light on problems with lambda grammars/abstractcategorial grammars and shows lambda grammars/abstract categorial grammarssuffer from problems of over-generation and from problems at thesyntax-semantics interface unlike any other categorial grammar.
arxiv-6900-19 | Optimality Theory as a Framework for Lexical Acquisition | http://arxiv.org/pdf/1405.6682v1.pdf | author:Thierry Poibeau category:cs.CL published:2014-05-26 summary:This paper re-investigates a lexical acquisition system initially developedfor French.We show that, interestingly, the architecture of the systemreproduces and implements the main components of Optimality Theory. However, weformulate the hypothesis that some of its limitations are mainly due to a poorrepresentation of the constraints used. Finally, we show how a betterrepresentation of the constraints used would yield better results.
arxiv-6900-20 | Stabilized Nearest Neighbor Classifier and Its Statistical Properties | http://arxiv.org/pdf/1405.6642v2.pdf | author:Wei Sun, Xingye Qiao, Guang Cheng category:stat.ML cs.LG published:2014-05-26 summary:The stability of statistical analysis is an important indicator forreproducibility, which is one main principle of scientific method. It entailsthat similar statistical conclusions can be reached based on independentsamples from the same underlying population. In this paper, we introduce ageneral measure of classification instability (CIS) to quantify the samplingvariability of the prediction made by a classification method. Interestingly,the asymptotic CIS of any weighted nearest neighbor classifier turns out to beproportional to the Euclidean norm of its weight vector. Based on this conciseform, we propose a stabilized nearest neighbor (SNN) classifier, whichdistinguishes itself from other nearest neighbor classifiers, by taking thestability into consideration. In theory, we prove that SNN attains the minimaxoptimal convergence rate in risk, and a sharp convergence rate in CIS. Thelatter rate result is established for general plug-in classifiers under alow-noise condition. Extensive simulated and real examples demonstrate that SNNachieves a considerable improvement in CIS over existing nearest neighborclassifiers, with comparable classification accuracy. We implement thealgorithm in a publicly available R package snn.
arxiv-6900-21 | A Novel Stochastic Decoding of LDPC Codes with Quantitative Guarantees | http://arxiv.org/pdf/1405.6353v1.pdf | author:Nima Noorshams, Aravind Iyengar category:cs.IT math.IT stat.ML published:2014-05-25 summary:Low-density parity-check codes, a class of capacity-approaching linear codes,are particularly recognized for their efficient decoding scheme. The decodingscheme, known as the sum-product, is an iterative algorithm consisting ofpassing messages between variable and check nodes of the factor graph. Thesum-product algorithm is fully parallelizable, owing to the fact that allmessages can be update concurrently. However, since it requires extensivenumber of highly interconnected wires, the fully-parallel implementation of thesum-product on chips is exceedingly challenging. Stochastic decodingalgorithms, which exchange binary messages, are of great interest formitigating this challenge and have been the focus of extensive research overthe past decade. They significantly reduce the required wiring andcomputational complexity of the message-passing algorithm. Even thoughstochastic decoders have been shown extremely effective in practice, thetheoretical aspect and understanding of such algorithms remains limited atlarge. Our main objective in this paper is to address this issue. We firstpropose a novel algorithm referred to as the Markov based stochastic decoding.Then, we provide concrete quantitative guarantees on its performance fortree-structured as well as general factor graphs. More specifically, we provideupper-bounds on the first and second moments of the error, illustrating thatthe proposed algorithm is an asymptotically consistent estimate of thesum-product algorithm. We also validate our theoretical predictions withexperimental results, showing we achieve comparable performance to otherpractical stochastic decoders.
arxiv-6900-22 | Multi-view Metric Learning for Multi-view Video Summarization | http://arxiv.org/pdf/1405.6434v2.pdf | author:Yanwei Fu, Lingbo Wang, Yanwen Guo category:cs.CV cs.LG cs.MM published:2014-05-25 summary:Traditional methods on video summarization are designed to generate summariesfor single-view video records; and thus they cannot fully exploit theredundancy in multi-view video records. In this paper, we present a multi-viewmetric learning framework for multi-view video summarization that combines theadvantages of maximum margin clustering with the disagreement minimizationcriterion. The learning framework thus has the ability to find a metric thatbest separates the data, and meanwhile to force the learned metric to maintainoriginal intrinsic information between data points, for example geometricinformation. Facilitated by such a framework, a systematic solution to themulti-view video summarization problem is developed. To the best of ourknowledge, it is the first time to address multi-view video summarization fromthe viewpoint of metric learning. The effectiveness of the proposed method isdemonstrated by experiments.
arxiv-6900-23 | Traversing News with Ant Colony Optimisation and Negative Pheromones | http://arxiv.org/pdf/1405.6285v1.pdf | author:David M. S. Rodrigues, Vitorino Ramos category:cs.IR cs.NE published:2014-05-24 summary:The past decade has seen the rapid development of the online newsroom. Newspublished online are the main outlet of news surpassing traditional printednewspapers. This poses challenges to the production and to the consumption ofthose news. With those many sources of information available it is important tofind ways to cluster and organise the documents if one wants to understand thisnew system. A novel bio inspired approach to the problem of traversing the newsis presented. It finds Hamiltonian cycles over documents published by thenewspaper The Guardian. A Second Order Swarm Intelligence algorithm based onAnt Colony Optimisation was developed that uses a negative pheromone to markunrewarding paths with a "no-entry" signal. This approach follows recentfindings of negative pheromone usage in real ants.
arxiv-6900-24 | Cross-Language Personal Name Mapping | http://arxiv.org/pdf/1405.6293v1.pdf | author:Ahmed H. Yousef category:cs.CL published:2014-05-24 summary:Name matching between multiple natural languages is an important step incross-enterprise integration applications and data mining. It is difficult todecide whether or not two syntactic values (names) from two heterogeneous datasources are alternative designation of the same semantic entity (person), thisprocess becomes more difficult with Arabic language due to several factorsincluding spelling and pronunciation variation, dialects and special vowel andconsonant distinction and other linguistic characteristics. This paper proposesa new framework for name matching between the Arabic language and otherlanguages. The framework uses a dictionary based on a new proposed version ofthe Soundex algorithm to encapsulate the recognition of special features ofArabic names. The framework proposes a new proximity matching algorithm to suitthe high importance of order sensitivity in Arabic name matching. Newperformance evaluation metrics are proposed as well. The framework isimplemented and verified empirically in several case studies demonstratingsubstantial improvements compared to other well-known techniques found inliterature.
arxiv-6900-25 | Four Classes of Morphogenetic Collective Systems | http://arxiv.org/pdf/1405.6296v1.pdf | author:Hiroki Sayama category:nlin.AO cs.NE published:2014-05-24 summary:We studied the roles of morphogenetic principles---heterogeneity ofcomponents, dynamic differentiation/re-differentiation of components, and localinformation sharing among components---in the self-organization ofmorphogenetic collective systems. By incrementally introducing these principlesto collectives, we defined four distinct classes of morphogenetic collectivesystems. Monte Carlo simulations were conducted using an extended version ofthe Swarm Chemistry model that was equipped with dynamicdifferentiation/re-differentiation and local information sharing capabilities.Self-organization of swarms was characterized by several kinetic andtopological measurements, the latter of which were facilitated by a newlydeveloped network-based method. Results of simulations revealed that, whileheterogeneity of components had a strong impact on the structure and behaviorof the swarms, dynamic differentiation/re-differentiation of components andlocal information sharing helped the swarms maintain spatially adjacent,coherent organization.
arxiv-6900-26 | Improvements and Experiments of a Compact Statistical Background Model | http://arxiv.org/pdf/1405.6275v1.pdf | author:Dong Liang, Shun'ichi Kaneko category:cs.CV published:2014-05-24 summary:Change detection plays an important role in most video-based applications.The first stage is to build appropriate background model, which is now becomingincreasingly complex as more sophisticated statistical approaches areintroduced to cover challenging situations and provide reliable detection. Thispaper reports a simple and intuitive statistical model based on deeper learningspatial correlation among pixels: For each observed pixel, we select a group ofsupporting pixels with high correlation, and then use a single Gaussian tomodel the intensity deviations between the observed pixel and the supportingones. In addition, a multi-channel model updating is integrated on-line and atemporal intensity constraint for each pixel is defined. Although this methodis mainly designed for coping with sudden illumination changes, experimentalresults using all the video sequences provided on changedetection.net validateit is comparable with other recent methods under various situations.
arxiv-6900-27 | Geometric Polynomial Constraints in Higher-Order Graph Matching | http://arxiv.org/pdf/1405.6261v1.pdf | author:Mayank Bansal, Kostas Daniilidis category:cs.CV published:2014-05-24 summary:Correspondence is a ubiquitous problem in computer vision and graph matchinghas been a natural way to formalize correspondence as an optimization problem.Recently, graph matching solvers have included higher-order terms representingaffinities beyond the unary and pairwise level. Such higher-order terms have aparticular appeal for geometric constraints that include three or morecorrespondences like the PnP 2D-3D pose problems. In this paper, we address theproblem of finding correspondences in the absence of unary or pairwiseconstraints as it emerges in problems where unary appearance similarity likeSIFT matches is not available. Current higher order matching approaches havetargeted problems where higher order affinity can simply be formulated as adifference of invariances such as lengths, angles, or cross-ratios. In thispaper, we present a method of how to apply geometric constraints modeled aspolynomial equation systems. As opposed to RANSAC where such systems have to besolved and then tested for inlier hypotheses, our constraints are derived as asingle affinity weight based on $n>2$ hypothesized correspondences withoutsolving the polynomial system. Since the result is directly a correspondencewithout a transformation model, our approach supports correspondence matchingin the presence of multiple geometric transforms like articulated motions.
arxiv-6900-28 | Efficient Model Learning for Human-Robot Collaborative Tasks | http://arxiv.org/pdf/1405.6341v1.pdf | author:Stefanos Nikolaidis, Keren Gu, Ramya Ramakrishnan, Julie Shah category:cs.RO cs.AI cs.LG cs.SY published:2014-05-24 summary:We present a framework for learning human user models from joint-actiondemonstrations that enables the robot to compute a robust policy for acollaborative task with a human. The learning takes place completelyautomatically, without any human intervention. First, we describe theclustering of demonstrated action sequences into different human types using anunsupervised learning algorithm. These demonstrated sequences are also used bythe robot to learn a reward function that is representative for each type,through the employment of an inverse reinforcement learning algorithm. Thelearned model is then used as part of a Mixed Observability Markov DecisionProcess formulation, wherein the human type is a partially observable variable.With this framework, we can infer, either offline or online, the human type ofa new user that was not included in the training set, and can compute a policyfor the robot that will be aligned to the preference of this new user and willbe robust to deviations of the human actions from prior demonstrations. Finallywe validate the approach using data collected in human subject experiments, andconduct proof-of-concept demonstrations in which a person performs acollaborative task with a small industrial robot.
arxiv-6900-29 | Connection graph Laplacian methods can be made robust to noise | http://arxiv.org/pdf/1405.6231v1.pdf | author:Noureddine El Karoui, Hau-tieng Wu category:math.ST math.SP stat.ME stat.ML stat.TH 60F99, 53A99 published:2014-05-23 summary:Recently, several data analytic techniques based on connection graphlaplacian (CGL) ideas have appeared in the literature. At this point, theproperties of these methods are starting to be understood in the setting wherethe data is observed without noise. We study the impact of additive noise onthese methods, and show that they are remarkably robust. As a by-product of ouranalysis, we propose modifications of the standard algorithms that increasetheir robustness to noise. We illustrate our results in numerical simulations.
arxiv-6900-30 | Convex Banding of the Covariance Matrix | http://arxiv.org/pdf/1405.6210v1.pdf | author:Jacob Bien, Florentina Bunea, Luo Xiao category:math.ST stat.CO stat.ME stat.ML stat.TH published:2014-05-23 summary:We introduce a new sparse estimator of the covariance matrix forhigh-dimensional models in which the variables have a known ordering. Ourestimator, which is the solution to a convex optimization problem, isequivalently expressed as an estimator which tapers the sample covariancematrix by a Toeplitz, sparsely-banded, data-adaptive matrix. As a result ofthis adaptivity, the convex banding estimator enjoys theoretical optimalityproperties not attained by previous banding or tapered estimators. Inparticular, our convex banding estimator is minimax rate adaptive in Frobeniusand operator norms, up to log factors, over commonly-studied classes ofcovariance matrices, and over more general classes. Furthermore, it correctlyrecovers the bandwidth when the true covariance is exactly banded. Our convexformulation admits a simple and efficient algorithm. Empirical studiesdemonstrate its practical effectiveness and illustrate that our exactly-bandedestimator works well even when the true covariance matrix is only close to abanded matrix, confirming our theoretical results. Our method comparesfavorably with all existing methods, in terms of accuracy and speed. Weillustrate the practical merits of the convex banding estimator by showing thatit can be used to improve the performance of discriminant analysis forclassifying sound recordings.
arxiv-6900-31 | Empirical Bayes Estimation for the Stochastic Blockmodel | http://arxiv.org/pdf/1405.6070v3.pdf | author:Shakira Suwan, Dominic S. Lee, Runze Tang, Daniel L. Sussman, Minh Tang, Carey E. Priebe category:stat.ME stat.ML published:2014-05-23 summary:Inference for the stochastic blockmodel is currently of burgeoning interestin the statistical community, as well as in various application domains asdiverse as social networks, citation networks, brain connectivity networks(connectomics), etc. Recent theoretical developments have shown that spectralembedding of graphs yields tractable distributional results; in particular, arandom dot product latent position graph formulation of the stochasticblockmodel informs a mixture of normal distributions for the adjacency spectralembedding. We employ this new theory to provide an empirical Bayes methodologyfor estimation of block memberships of vertices in a random graph drawn fromthe stochastic blockmodel, and demonstrate its practical utility. The posteriorinference is conducted using a Metropolis-within-Gibbs algorithm. The theoryand methods are illustrated through Monte Carlo simulation studies, both withinthe stochastic blockmodel and beyond, and experimental results on a Wikipediadata set are presented.
arxiv-6900-32 | LASS: a simple assignment model with Laplacian smoothing | http://arxiv.org/pdf/1405.5960v1.pdf | author:Miguel Á. Carreira-Perpiñán, Weiran Wang category:cs.LG math.OC stat.ML published:2014-05-23 summary:We consider the problem of learning soft assignments of $N$ items to $K$categories given two sources of information: an item-category similaritymatrix, which encourages items to be assigned to categories they are similar to(and to not be assigned to categories they are dissimilar to), and an item-itemsimilarity matrix, which encourages similar items to have similar assignments.We propose a simple quadratic programming model that captures this intuition.We give necessary conditions for its solution to be unique, define anout-of-sample mapping, and derive a simple, effective training algorithm basedon the alternating direction method of multipliers. The model predictsreasonable assignments from even a few similarity values, and can be seen as ageneralization of semisupervised learning. It is particularly useful when itemsnaturally belong to multiple categories, as for example when annotatingdocuments with keywords or pictures with tags, with partially tagged items, orwhen the categories have complex interrelations (e.g. hierarchical) that areunknown.
arxiv-6900-33 | On the Optimal Solution of Weighted Nuclear Norm Minimization | http://arxiv.org/pdf/1405.6012v1.pdf | author:Qi Xie, Deyu Meng, Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng, Zongben Xu category:cs.CV cs.LG stat.ML published:2014-05-23 summary:In recent years, the nuclear norm minimization (NNM) problem has beenattracting much attention in computer vision and machine learning. The NNMproblem is capitalized on its convexity and it can be solved efficiently. Thestandard nuclear norm regularizes all singular values equally, which is howevernot flexible enough to fit real scenarios. Weighted nuclear norm minimization(WNNM) is a natural extension and generalization of NNM. By assigning properlydifferent weights to different singular values, WNNM can lead tostate-of-the-art results in applications such as image denoising. Nevertheless,so far the global optimal solution of WNNM problem is not completely solved yetdue to its non-convexity in general cases. In this article, we study thetheoretical properties of WNNM and prove that WNNM can be equivalentlytransformed into a quadratic programming problem with linear constraints. Thisimplies that WNNM is equivalent to a convex problem and its global optimum canbe readily achieved by off-the-shelf convex optimization solvers. We furthershow that when the weights are non-descending, the globally optimal solution ofWNNM can be obtained in closed-form.
arxiv-6900-34 | Building of Networks of Natural Hierarchies of Terms Based on Analysis of Texts Corpora | http://arxiv.org/pdf/1405.6068v1.pdf | author:Dmitry Lande category:cs.CL published:2014-05-23 summary:The technique of building of networks of hierarchies of terms based on theanalysis of chosen text corpora is offered. The technique is based on themethodology of horizontal visibility graphs. Constructed and investigatedlanguage network, formed on the basis of electronic preprints arXiv on topicsof information retrieval.
arxiv-6900-35 | Evaluating the fully automatic multi-language translation of the Swiss avalanche bulletin | http://arxiv.org/pdf/1405.6103v1.pdf | author:Kurt Winkler, Tobias Kuhn, Martin Volk category:cs.CL published:2014-05-23 summary:The Swiss avalanche bulletin is produced twice a day in four languages. Dueto the lack of time available for manual translation, a fully automatedtranslation system is employed, based on a catalogue of predefined phrases andpredetermined rules of how these phrases can be combined to produce sentences.The system is able to automatically translate such sentences from German intothe target languages French, Italian and English without subsequentproofreading or correction. Our catalogue of phrases is limited to a smallsublanguage. The reduction of daily translation costs is expected to offset theinitial development costs within a few years. After being operational for twowinter seasons, we assess here the quality of the produced texts based on anevaluation where participants rate real danger descriptions from both origins,the catalogue of phrases versus the manually written and translated texts. Witha mean recognition rate of 55%, users can hardly distinguish between the twotypes of texts, and give similar ratings with respect to their languagequality. Overall, the output from the catalogue system can be consideredvirtually equivalent to a text written by avalanche forecasters and thenmanually translated by professional translators. Furthermore, forecastersdeclared that all relevant situations were captured by the system withsufficient accuracy and within the limited time available.
arxiv-6900-36 | Online Linear Optimization via Smoothing | http://arxiv.org/pdf/1405.6076v1.pdf | author:Jacob Abernethy, Chansoo Lee, Abhinav Sinha, Ambuj Tewari category:cs.LG published:2014-05-23 summary:We present a new optimization-theoretic approach to analyzingFollow-the-Leader style algorithms, particularly in the setting whereperturbations are used as a tool for regularization. We show that adding astrongly convex penalty function to the decision rule and adding stochasticperturbations to data correspond to deterministic and stochastic smoothingoperations, respectively. We establish an equivalence between "Follow theRegularized Leader" and "Follow the Perturbed Leader" up to the smoothnessproperties. This intuition leads to a new generic analysis framework thatrecovers and improves the previous known regret bounds of the class ofalgorithms commonly known as Follow the Perturbed Leader.
arxiv-6900-37 | Compressive Mining: Fast and Optimal Data Mining in the Compressed Domain | http://arxiv.org/pdf/1405.5873v1.pdf | author:Michail Vlachos, Nikolaos Freris, Anastasios Kyrillidis category:stat.ML cs.DS cs.IT math.IT published:2014-05-22 summary:Real-world data typically contain repeated and periodic patterns. Thissuggests that they can be effectively represented and compressed using only afew coefficients of an appropriate basis (e.g., Fourier, Wavelets, etc.).However, distance estimation when the data are represented using different setsof coefficients is still a largely unexplored area. This work studies theoptimization problems related to obtaining the \emph{tightest} lower/upperbound on Euclidean distances when each data object is potentially compressedusing a different set of orthonormal coefficients. Our technique leads totighter distance estimates, which translates into more accurate search,learning and mining operations \textit{directly} in the compressed domain. We formulate the problem of estimating lower/upper distance bounds as anoptimization problem. We establish the properties of optimal solutions, andleverage the theoretical analysis to develop a fast algorithm to obtain an\emph{exact} solution to the problem. The suggested solution provides thetightest estimation of the $L_2$-norm or the correlation. We show that typicaldata-analysis operations, such as k-NN search or k-Means clustering, canoperate more accurately using the proposed compression and distancereconstruction technique. We compare it with many other prevalent compressionand reconstruction techniques, including random projections and PCA-basedtechniques. We highlight a surprising result, namely that when the data arehighly sparse in some basis, our technique may even outperform PCA-basedcompression. The contributions of this work are generic as our methodology is applicableto any sequential or high-dimensional data as well as to any orthogonal datatransformation used for the underlying data compression scheme.
arxiv-6900-38 | Learning to Generate Networks | http://arxiv.org/pdf/1405.5868v2.pdf | author:James Atwood, Don Towsley, Krista Gile, David Jensen category:cs.LG cs.SI physics.soc-ph published:2014-05-22 summary:We investigate the problem of learning to generate complex networks fromdata. Specifically, we consider whether deep belief networks, dependencynetworks, and members of the exponential random graph family can learn togenerate networks whose complex behavior is consistent with a set of inputexamples. We find that the deep model is able to capture the complex behaviorof small networks, but that no model is able capture this behavior for networkswith more than a handful of nodes.
arxiv-6900-39 | MotàMot project: conversion of a French-Khmer published dictionary for building a multilingual lexical system | http://arxiv.org/pdf/1405.5674v1.pdf | author:Mathieu Mangeot category:cs.CL published:2014-05-22 summary:Economic issues related to the information processing techniques are veryimportant. The development of such technologies is a major asset for developingcountries like Cambodia and Laos, and emerging ones like Vietnam, Malaysia andThailand. The MotAMot project aims to computerize an under-resourced language:Khmer, spoken mainly in Cambodia. The main goal of the project is thedevelopment of a multilingual lexical system targeted for Khmer. Themacrostructure is a pivot one with each word sense of each language linked to apivot axi. The microstructure comes from a simplification of the explanatoryand combinatory dictionary. The lexical system has been initialized with datacoming mainly from the conversion of the French-Khmer bilingual dictionary ofDenis Richer from Word to XML format. The French part was completed withpronunciation and parts-of-speech coming from the FeM French-english-Malaydictionary. The Khmer headwords noted in IPA in the Richer dictionary wereconverted to Khmer writing with OpenFST, a finite state transducer tool. Theresulting resource is available online for lookup, editing, download and remoteprogramming via a REST API on a Jibiki platform.
arxiv-6900-40 | Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT | http://arxiv.org/pdf/1405.5769v2.pdf | author:Philipp Fischer, Alexey Dosovitskiy, Thomas Brox category:cs.CV cs.LG published:2014-05-22 summary:Latest results indicate that features learned via convolutional neuralnetworks outperform previous descriptors on classification tasks by a largemargin. It has been shown that these networks still work well when they areapplied to datasets or recognition tasks different from those they were trainedon. However, descriptors like SIFT are not only used in recognition but alsofor many correspondence problems that rely on descriptor matching. In thispaper we compare features from various layers of convolutional neural nets tostandard SIFT descriptors. We consider a network that was trained on ImageNetand another one that was trained without supervision. Surprisingly,convolutional neural networks clearly outperform SIFT on descriptor matching.This paper has been merged with arXiv:1406.6909
arxiv-6900-41 | Semi-supervised Spectral Clustering for Classification | http://arxiv.org/pdf/1405.5737v2.pdf | author:Arif Mahmood, Ajmal S. Mian category:cs.CV published:2014-05-22 summary:We propose a Classification Via Clustering (CVC) algorithm which enablesexisting clustering methods to be efficiently employed in classificationproblems. In CVC, training and test data are co-clustered and class-clusterdistributions are used to find the label of the test data. To determine anefficient number of clusters, a Semi-supervised Hierarchical Clustering (SHC)algorithm is proposed. Clusters are obtained by hierarchically applying two-wayNCut by using signs of the Fiedler vector of the normalized graph Laplacian. Tothis end, a Direct Fiedler Vector Computation algorithm is proposed. The graphcut is based on the data structure and does not consider labels. Labels areused only to define the stopping criterion for graph cut. We propose clusteringto be performed on the Grassmannian manifolds facilitating the formation ofspectral ensembles. The proposed algorithm outperformed state-of-the-artimage-set classification algorithms on five standard datasets.
arxiv-6900-42 | Node Classification in Uncertain Graphs | http://arxiv.org/pdf/1405.5829v1.pdf | author:Michele Dallachiesa, Charu Aggarwal, Themis Palpanas category:cs.DB cs.LG published:2014-05-22 summary:In many real applications that use and analyze networked data, the links inthe network graph may be erroneous, or derived from probabilistic techniques.In such cases, the node classification problem can be challenging, since theunreliability of the links may affect the final results of the classificationprocess. If the information about link reliability is not used explicitly, theclassification accuracy in the underlying network may be affected adversely. Inthis paper, we focus on situations that require the analysis of the uncertaintythat is present in the graph structure. We study the novel problem of nodeclassification in uncertain graphs, by treating uncertainty as a first-classcitizen. We propose two techniques based on a Bayes model and automaticparameter selection, and show that the incorporation of uncertainty in theclassification process as a first-class citizen is beneficial. Weexperimentally evaluate the proposed approach using different real data sets,and study the behavior of the algorithms under different conditions. Theresults demonstrate the effectiveness and efficiency of our approach.
arxiv-6900-43 | Machine Translation Model based on Non-parallel Corpus and Semi-supervised Transductive Learning | http://arxiv.org/pdf/1405.5654v1.pdf | author:Lijiang Chen category:cs.CL published:2014-05-22 summary:Although the parallel corpus has an irreplaceable role in machinetranslation, its scale and coverage is still beyond the actual needs.Non-parallel corpus resources on the web have an inestimable potential value inmachine translation and other natural language processing tasks. This articleproposes a semi-supervised transductive learning method for expanding thetraining corpus in statistical machine translation system by extractingparallel sentences from the non-parallel corpus. This method only requires asmall amount of labeled corpus and a large unlabeled corpus to build ahigh-performance classifier, especially for when there is short of labeledcorpus. The experimental results show that by combining the non-parallel corpusalignment and the semi-supervised transductive learning method, we can moreeffectively use their respective strengths to improve the performance ofmachine translation system.
arxiv-6900-44 | Self-tuned Visual Subclass Learning with Shared Samples An Incremental Approach | http://arxiv.org/pdf/1405.5732v2.pdf | author:Hossein Azizpour, Stefan Carlsson category:cs.CV published:2014-05-22 summary:Computer vision tasks are traditionally defined and evaluated using semanticcategories. However, it is known to the field that semantic classes do notnecessarily correspond to a unique visual class (e.g. inside and outside of acar). Furthermore, many of the feasible learning techniques at hand cannotmodel a visual class which appears consistent to the human eye. These problemshave motivated the use of 1) Unsupervised or supervised clustering as apreprocessing step to identify the visual subclasses to be used in amixture-of-experts learning regime. 2) Felzenszwalb et al. part model and otherworks model mixture assignment with latent variables which is optimized duringlearning 3) Highly non-linear classifiers which are inherently capable ofmodelling multi-modal input space but are inefficient at the test time. In thiswork, we promote an incremental view over the recognition of semantic classeswith varied appearances. We propose an optimization technique whichincrementally finds maximal visual subclasses in a regularized riskminimization framework. Our proposed approach unifies the clustering andclassification steps in a single algorithm. The importance of this approach isits compliance with the classification via the fact that it does not need toknow about the number of clusters, the representation and similarity measuresused in pre-processing clustering methods a priori. Following this approach weshow both qualitatively and quantitatively significant results. We show thatthe visual subclasses demonstrate a long tail distribution. Finally, we showthat state of the art object detection methods (e.g. DPM) are unable to use thetails of this distribution comprising 50\% of the training samples. In fact weshow that DPM performance slightly increases on average by the removal of thishalf of the data.
arxiv-6900-45 | Computerization of African languages-French dictionaries | http://arxiv.org/pdf/1405.5893v1.pdf | author:Chantal Enguehard, Mathieu Mangeot category:cs.CL published:2014-05-22 summary:This paper relates work done during the DiLAF project. It consists inconverting 5 bilingual African language-French dictionaries originally in Wordformat into XML following the LMF model. The languages processed are Bambara,Hausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourcedlanguages concerning Natural Language Processing tools. Once converted, thedictionaries are available online on the Jibiki platform for lookup andmodification. The DiLAF project is first presented. A description of eachdictionary follows. Then, the conversion methodology from .doc format to XMLfiles is presented. A specific point on the usage of Unicode follows. Then,each step of the conversion into XML and LMF is detailed. The last partpresents the Jibiki lexical resources management platform used for the project.
arxiv-6900-46 | Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS) | http://arxiv.org/pdf/1405.5869v1.pdf | author:Anshumali Shrivastava, Ping Li category:stat.ML cs.DS cs.IR cs.LG published:2014-05-22 summary:We present the first provably sublinear time algorithm for approximate\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the firsthashing algorithm for searching with (un-normalized) inner product as theunderlying similarity measure. Finding hashing schemes for MIPS was consideredhard. We formally show that the existing Locality Sensitive Hashing (LSH)framework is insufficient for solving MIPS, and then we extend the existing LSHframework to allow asymmetric hashing schemes. Our proposal is based on aninteresting mathematical phenomenon in which inner products, after independentasymmetric transformations, can be converted into the problem of approximatenear neighbor search. This key observation makes efficient sublinear hashingscheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, weprovide an explicit construction of provably fast hashing scheme for MIPS. Theproposed construction and the extended LSH framework could be of independenttheoretical interest. Our proposed algorithm is simple and easy to implement.We evaluate the method, for retrieving inner products, in the collaborativefiltering task of item recommendations on Netflix and Movielens datasets.
arxiv-6900-47 | Compressive Sampling Using EM Algorithm | http://arxiv.org/pdf/1405.5311v1.pdf | author:Atanu Kumar Ghosh, Arnab Chakraborty category:stat.ME cs.LG stat.ML published:2014-05-21 summary:Conventional approaches of sampling signals follow the celebrated theorem ofNyquist and Shannon. Compressive sampling, introduced by Donoho, Romberg andTao, is a new paradigm that goes against the conventional methods in dataacquisition and provides a way of recovering signals using fewer samples thanthe traditional methods use. Here we suggest an alternative way ofreconstructing the original signals in compressive sampling using EM algorithm.We first propose a naive approach which has certain computational difficultiesand subsequently modify it to a new approach which performs better than theconventional methods of compressive sampling. The comparison of the differentapproaches and the performance of the new approach has been studied usingsimulated data.
arxiv-6900-48 | Circle detection on images using Learning Automata | http://arxiv.org/pdf/1405.5406v1.pdf | author:Erik Cuevas, Fernando Wario, Daniel Zaldivar, Marco Perez category:cs.CV published:2014-05-21 summary:Circle detection over digital images has received considerable attention fromthe computer vision community over the last few years devoting a tremendousamount of research seeking for an optimal detector. This article presents analgorithm for the automatic detection of circular shapes from complicated andnoisy images with no consideration of conventional Hough transform principles.The proposed algorithm is based on Learning Automata (LA) which is aprobabilistic optimization method that explores an unknown random environmentby progressively improving the performance via a reinforcement signal(objective function). The approach uses the encoding of three non-collinearpoints as a candidate circle over the edge image. A reinforcement signal(matching function) indicates if such candidate circles are actually present inthe edge map. Guided by the values of such reinforcement signal, theprobability set of the encoded candidate circles is modified through the LAalgorithm so that they can fit to the actual circles on the edge map.Experimental results over several complex synthetic and natural images havevalidated the efficiency of the proposed technique regarding accuracy, speedand robustness.
arxiv-6900-49 | Off-Policy Shaping Ensembles in Reinforcement Learning | http://arxiv.org/pdf/1405.5358v1.pdf | author:Anna Harutyunyan, Tim Brys, Peter Vrancx, Ann Nowe category:cs.AI cs.LG published:2014-05-21 summary:Recent advances of gradient temporal-difference methods allow to learnoff-policy multiple value functions in parallel with- out sacrificingconvergence guarantees or computational efficiency. This opens up newpossibilities for sound ensemble techniques in reinforcement learning. In thiswork we propose learning an ensemble of policies related throughpotential-based shaping rewards. The ensemble induces a combination policy byusing a voting mechanism on its components. Learning happens in real time, andwe empirically show the combination policy to outperform the individualpolicies of the ensemble.
arxiv-6900-50 | Approximate resilience, monotonicity, and the complexity of agnostic learning | http://arxiv.org/pdf/1405.5268v2.pdf | author:Dana Dachman-Soled, Vitaly Feldman, Li-Yang Tan, Andrew Wan, Karl Wimmer category:cs.LG cs.CC cs.DM published:2014-05-21 summary:A function $f$ is $d$-resilient if all its Fourier coefficients of degree atmost $d$ are zero, i.e., $f$ is uncorrelated with all low-degree parities. Westudy the notion of $\mathit{approximate}$ $\mathit{resilience}$ of Booleanfunctions, where we say that $f$ is $\alpha$-approximately $d$-resilient if $f$is $\alpha$-close to a $[-1,1]$-valued $d$-resilient function in $\ell_1$distance. We show that approximate resilience essentially characterizes thecomplexity of agnostic learning of a concept class $C$ over the uniformdistribution. Roughly speaking, if all functions in a class $C$ are far frombeing $d$-resilient then $C$ can be learned agnostically in time $n^{O(d)}$ andconversely, if $C$ contains a function close to being $d$-resilient thenagnostic learning of $C$ in the statistical query (SQ) framework of Kearns hascomplexity of at least $n^{\Omega(d)}$. This characterization is based on theduality between $\ell_1$ approximation by degree-$d$ polynomials andapproximate $d$-resilience that we establish. In particular, it implies that$\ell_1$ approximation by low-degree polynomials, known to be sufficient foragnostic learning over product distributions, is in fact necessary. Focusing on monotone Boolean functions, we exhibit the existence ofnear-optimal $\alpha$-approximately$\widetilde{\Omega}(\alpha\sqrt{n})$-resilient monotone functions for all$\alpha>0$. Prior to our work, it was conceivable even that every monotonefunction is $\Omega(1)$-far from any $1$-resilient function. Furthermore, weconstruct simple, explicit monotone functions based on ${\sf Tribes}$ and ${\sfCycleRun}$ that are close to highly resilient functions. Our constructions arebased on a fairly general resilience analysis and amplification. Thesestructural results, together with the characterization, imply nearly optimallower bounds for agnostic learning of monotone juntas.
arxiv-6900-51 | Robust Fuzzy corner detector | http://arxiv.org/pdf/1405.5422v1.pdf | author:Erik Cuevas, Daniel Zaldivar, Marco Perez, Edgar Sanchez, Marte Ramirez category:cs.CV published:2014-05-21 summary:Reliable corner detection is an important task in determining the shape ofdifferent regions within an image. Real-life image data are always imprecisedue to inherent uncertainties that may arise from the imaging process such asdefocusing, illumination changes, noise, etc. Therefore, the localization anddetection of corners has become a difficult task to accomplish under suchimperfect situations. On the other hand, Fuzzy systems are well known for theirefficient handling of impreciseness and incompleteness, which make theminherently suitable for modelling corner properties by means of a rule-basedfuzzy system. The paper presents a corner detection algorithm which employssuch fuzzy reasoning. The robustness of the proposed algorithm is compared towell-known conventional corner detectors and its performance is also testedover a number of benchmark images to illustrate the efficiency of the algorithmunder uncertainty.
arxiv-6900-52 | New Perspectives in Sinographic Language Processing Through the Use of Character Structure | http://arxiv.org/pdf/1405.5474v1.pdf | author:Yannis Haralambous category:cs.CL published:2014-05-21 summary:Chinese characters have a complex and hierarchical graphical structurecarrying both semantic and phonetic information. We use this structure toenhance the text model and obtain better results in standard NLP operations.First of all, to tackle the problem of graphical variation we defineallographic classes of characters. Next, the relation of inclusion of asubcharacter in a characters, provides us with a directed graph of allographicclasses. We provide this graph with two weights: semanticity (semantic relationbetween subcharacter and character) and phoneticity (phonetic relation) andcalculate "most semantic subcharacter paths" for each character. Finally,adding the information contained in these paths to unigrams we claim toincrease the efficiency of text mining methods. We evaluate our method on atext classification task on two corpora (Chinese and Japanese) of a total of 18million characters and get an improvement of 3% on an already high baseline of89.6% precision, obtained by a linear SVM classifier. Other possibleapplications and perspectives of the system are discussed.
arxiv-6900-53 | Fast algorithm for Multiple-Circle detection on images using Learning Automata | http://arxiv.org/pdf/1405.5531v1.pdf | author:Erik Cuevas, Fernando Wario, Valentin Osuna, Daniel Zaldivar, Marco Perez category:cs.CV published:2014-05-21 summary:Hough transform (HT) has been the most common method for circle detectionexhibiting robustness but adversely demanding a considerable computational loadand large storage. Alternative approaches include heuristic methods that employiterative optimization procedures for detecting multiple circles under theinconvenience that only one circle can be marked at each optimization cycledemanding a longer execution time. On the other hand, Learning Automata (LA) isa heuristic method to solve complex multi-modal optimization problems. AlthoughLA converges to just one global minimum, the final probability distributionholds valuable information regarding other local minima which have emergedduring the optimization process. The detection process is considered as amulti-modal optimization problem, allowing the detection of multiple circularshapes through only one optimization procedure. The algorithm uses acombination of three edge points as parameters to determine circles candidates.A reinforcement signal determines if such circle candidates are actuallypresent at the image. Guided by the values of such reinforcement signal, theset of encoded candidate circles are evolved using the LA so that they can fitinto actual circular shapes over the edge-only map of the image. The overallapproach is a fast multiple-circle detector despite facing complicatedconditions.
arxiv-6900-54 | Sparse Precision Matrix Selection for Fitting Gaussian Random Field Models to Large Data Sets | http://arxiv.org/pdf/1405.5576v4.pdf | author:Sam Davanloo Tajbakhsh, Necdet Serhat Aybat, Enrique Del Castillo category:stat.ML stat.CO published:2014-05-21 summary:Iterative methods for fitting a Gaussian Random Field (GRF) model to spatialdata via maximum likelihood (ML) require $\mathcal{O}(n^3)$ floating pointoperations per iteration, where $n$ denotes the number of data locations. Forlarge data sets, the $\mathcal{O}(n^3)$ complexity per iteration together withthe non-convexity of the ML problem render traditional ML methods inefficientfor GRF fitting. The problem is even more aggravated for anisotropic GRFs wherethe number of covariance function parameters increases with the process domaindimension. In this paper, we propose a new two-step GRF estimation procedurewhen the process is second-order stationary. First, a \emph{convex} likelihoodproblem regularized with a weighted $\ell_1$-norm, utilizing the availabledistance information between observation locations, is solved to fit a sparse\emph{{precision} (inverse covariance) matrix to the observed data using theAlternating Direction Method of Multipliers. Second, the parameters of the GRFspatial covariance function are estimated by solving a least squares problem.Theoretical error bounds for the proposed estimator are provided; moreover,convergence of the estimator is shown as the number of samples per locationincreases. The proposed method is numerically compared with state-of-the-artmethods for big $n$. Data segmentation schemes are implemented to handle largedata sets.
arxiv-6900-55 | Kernel Mean Shrinkage Estimators | http://arxiv.org/pdf/1405.5505v3.pdf | author:Krikamol Muandet, Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf category:stat.ML cs.LG published:2014-05-21 summary:A mean function in a reproducing kernel Hilbert space (RKHS), or a kernelmean, is central to kernel methods in that it is used by many classicalalgorithms such as kernel principal component analysis, and it also forms thecore inference step of modern kernel methods that rely on embedding probabilitydistributions in RKHSs. Given a finite sample, an empirical average has beenused commonly as a standard estimator of the true kernel mean. Despite awidespread use of this estimator, we show that it can be improved thanks to thewell-known Stein phenomenon. We propose a new family of estimators calledkernel mean shrinkage estimators (KMSEs), which benefit from both theoreticaljustifications and good empirical performance. The results demonstrate that theproposed estimators outperform the standard one, especially in a "large d,small n" paradigm.
arxiv-6900-56 | Fast Distributed Coordinate Descent for Non-Strongly Convex Losses | http://arxiv.org/pdf/1405.5300v2.pdf | author:Olivier Fercoq, Zheng Qu, Peter Richtárik, Martin Takáč category:math.OC cs.LG published:2014-05-21 summary:We propose an efficient distributed randomized coordinate descent method forminimizing regularized non-strongly convex loss functions. The method attainsthe optimal $O(1/k^2)$ convergence rate, where $k$ is the iteration counter.The core of the work is the theoretical study of stepsize parameters. We haveimplemented the method on Archer - the largest supercomputer in the UK - andshow that the method is capable of solving a (synthetic) LASSO optimizationproblem with 50 billion variables.
arxiv-6900-57 | Learning to Exploit Different Translation Resources for Cross Language Information Retrieval | http://arxiv.org/pdf/1405.5447v1.pdf | author:Hosein Azarbonyad, Azadeh Shakery, Heshaam Faili category:cs.IR cs.CL published:2014-05-20 summary:One of the important factors that affects the performance of Cross LanguageInformation Retrieval(CLIR)is the quality of translations being employed inCLIR. In order to improve the quality of translations, it is important toexploit available resources efficiently. Employing different translationresources with different characteristics has many challenges. In this paper, wepropose a method for exploiting available translation resources simultaneously.This method employs Learning to Rank(LTR) for exploiting different translationresources. To apply LTR methods for query translation, we define differenttranslation relation based features in addition to context based features. Weuse the contextual information contained in translation resources forextracting context based features.The proposed method uses LTR to construct atranslation ranking model based on defined features. The constructed model isused for ranking translation candidates of query words. To evaluate theproposed method we do English-Persian CLIR, in which we employ the translationranking model to find translations of English queries and employ thetranslations to retrieve Persian documents. Experimental results show that ourapproach significantly outperforms single resource based CLIR methods.
arxiv-6900-58 | A Genetic Algorithm for solving Quadratic Assignment Problem(QAP) | http://arxiv.org/pdf/1405.5050v1.pdf | author:Hosein Azarbonyad, Reza Babazadeh category:cs.NE published:2014-05-20 summary:The Quadratic Assignment Problem (QAP) is one of the models used for themulti-row layout problem with facilities of equal area. There are a set of nfacilities and a set of n locations. For each pair of locations, a distance isspecified and for each pair of facilities a weight or flow is specified (e.g.,the amount of supplies transported between the two facilities). The problem isto assign all facilities to different locations with the aim of minimizing thesum of the distances multiplied by the corresponding flows. The QAP is amongthe most difficult NP-hard combinatorial optimization problems. Because ofthis, this paper presents an efficient Genetic algorithm (GA) to solve thisproblem in reasonable time. For validation the proposed GA some examples areselected from QAP library. The obtained results in reasonable time show theefficiency of proposed GA.
arxiv-6900-59 | Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms | http://arxiv.org/pdf/1405.5096v1.pdf | author:Richard Combes, Alexandre Proutiere category:cs.LG stat.ML published:2014-05-20 summary:We consider stochastic multi-armed bandits where the expected reward is aunimodal function over partially ordered arms. This important class of problemshas been recently investigated in (Cope 2009, Yu 2011). The set of arms iseither discrete, in which case arms correspond to the vertices of a finitegraph whose structure represents similarity in rewards, or continuous, in whichcase arms belong to a bounded interval. For discrete unimodal bandits, wederive asymptotic lower bounds for the regret achieved under any algorithm, andpropose OSUB, an algorithm whose regret matches this lower bound. Our algorithmoptimally exploits the unimodal structure of the problem, and surprisingly, itsasymptotic regret does not depend on the number of arms. We also provide aregret upper bound for OSUB in non-stationary environments where the expectedrewards smoothly evolve over time. The analytical results are supported bynumerical experiments showing that OSUB performs significantly better than thestate-of-the-art algorithms. For continuous sets of arms, we provide a briefdiscussion. We show that combining an appropriate discretization of the set ofarms with the UCB algorithm yields an order-optimal regret, and in practice,outperforms recently proposed algorithms designed to exploit the unimodalstructure.
arxiv-6900-60 | Predicting Online Video Engagement Using Clickstreams | http://arxiv.org/pdf/1405.5147v1.pdf | author:Everaldo Aguiar, Saurabh Nagrecha, Nitesh V. Chawla category:cs.LG cs.IR I.5.2 published:2014-05-20 summary:In the nascent days of e-content delivery, having a superior product wasenough to give companies an edge against the competition. With today's fiercelycompetitive market, one needs to be multiple steps ahead, especially when itcomes to understanding consumers. Focusing on a large set of web portals ownedand managed by a private communications company, we propose methods by whichthese sites' clickstream data can be used to provide a deep understanding oftheir visitors, as well as their interests and preferences. We further expandthe use of this data to show that it can be effectively used to predict userengagement to video streams.
arxiv-6900-61 | Gaussian Approximation of Collective Graphical Models | http://arxiv.org/pdf/1405.5156v1.pdf | author:Li-Ping Liu, Daniel Sheldon, Thomas G. Dietterich category:cs.LG cs.AI stat.ML published:2014-05-20 summary:The Collective Graphical Model (CGM) models a population of independent andidentically distributed individuals when only collective statistics (i.e.,counts of individuals) are observed. Exact inference in CGMs is intractable,and previous work has explored Markov Chain Monte Carlo (MCMC) and MAPapproximations for learning and inference. This paper studies Gaussianapproximations to the CGM. As the population grows large, we show that the CGMdistribution converges to a multivariate Gaussian distribution (GCGM) thatmaintains the conditional independence properties of the original CGM. If theobservations are exact marginals of the CGM or marginals that are corrupted byGaussian noise, inference in the GCGM approximation can be computed efficientlyin closed form. If the observations follow a different noise model (e.g.,Poisson), then expectation propagation provides efficient and accurateapproximate inference. The accuracy and speed of GCGM inference is compared tothe MCMC and MAP methods on a simulated bird migration problem. The GCGMmatches or exceeds the accuracy of the MAP method while being significantlyfaster.
arxiv-6900-62 | Multi-ellipses detection on images inspired by collective animal behavior | http://arxiv.org/pdf/1405.5164v1.pdf | author:Erik Cuevas, Maurici Gonzalez, Daniel Zaldivar, Marco Perez category:cs.CV published:2014-05-20 summary:This paper presents a novel and effective technique for extracting multipleellipses from an image. The approach employs an evolutionary algorithm to mimicthe way animals behave collectively assuming the overall detection process as amulti-modal optimization problem. In the algorithm, searcher agents emulate agroup of animals that interact to each other using simple biological ruleswhich are modeled as evolutionary operators. In turn, such operators areapplied to each agent considering that the complete group has a memory to storeoptimal solutions (ellipses) seen so-far by applying a competition principle.The detector uses a combination of five edge points as parameters to determineellipse candidates (possible solutions) while a matching function determines ifsuch ellipse candidates are actually present in the image. Guided by the valuesof such matching functions, the set of encoded candidate ellipses are evolvedthrough the evolutionary algorithm so that the best candidates can be fittedinto the actual ellipses within the image. Just after the optimization processends, an analysis over the embedded memory is executed in order to find thebest obtained solution (the best ellipse) and significant local minima(remaining ellipses). Experimental results over several complex synthetic andnatural images have validated the efficiency of the proposed techniqueregarding accuracy, speed and robustness.
arxiv-6900-63 | Sequential Advantage Selection for Optimal Treatment Regimes | http://arxiv.org/pdf/1405.5239v1.pdf | author:Ailin Fan, Wenbin Lu, Rui Song category:stat.ME stat.ML published:2014-05-20 summary:Variable selection for optimal treatment regime in a clinical trial or anobservational study is getting more attention. Most existing variable selectiontechniques focused on selecting variables that are important for prediction,therefore some variables that are poor in prediction but are critical fordecision-making may be ignored. A qualitative interaction of a variable withtreatment arises when treatment effect changes direction as the value of thisvariable varies. The qualitative interaction indicates the importance of thisvariable for decision-making. Gunter et al. (2011) proposed S-score whichcharacterizes the magnitude of qualitative interaction of each variable withtreatment individually. In this article, we developed a sequential advantageselection method based on the modified S-score. Our method selectsqualitatively interacted variables sequentially, and hence excludes marginallyimportant but jointly unimportant variables {or vice versa}. The optimaltreatment regime based on variables selected via joint model is morecomprehensive and reliable. With the proposed stopping criteria, our method canhandle a large amount of covariates even if sample size is small. Simulationresults show our method performs well in practical settings. We further appliedour method to data from a clinical trial for depression.
arxiv-6900-64 | Dynamic Hierarchical Bayesian Network for Arabic Handwritten Word Recognition | http://arxiv.org/pdf/1405.5248v1.pdf | author:Khaoula jayech, Nesrine Trimech, Mohamed Ali Mahjoub, Najoua Essoukri Ben Amara category:cs.CV published:2014-05-20 summary:This paper presents a new probabilistic graphical model used to model andrecognize words representing the names of Tunisian cities. In fact, this workis based on a dynamic hierarchical Bayesian network. The aim is to find thebest model of Arabic handwriting to reduce the complexity of the recognitionprocess by permitting the partial recognition. Actually, we propose asegmentation of the word based on smoothing the vertical histogram projectionusing different width values to reduce the error of segmentation. Then, weextract the characteristics of each cell using the Zernike and HU moments,which are invariant to rotation, translation and scaling. Our approach istested using the IFN / ENIT database, and the experiment results are verypromising.
arxiv-6900-65 | Secure Friend Discovery via Privacy-Preserving and Decentralized Community Detection | http://arxiv.org/pdf/1405.4951v1.pdf | author:Pili Hu, Sherman S. M. Chow, Wing Cheong Lau category:cs.CR cs.SI stat.ML published:2014-05-20 summary:The problem of secure friend discovery on a social network has long beenproposed and studied. The requirement is that a pair of nodes can makebefriending decisions with minimum information exposed to the other party. Inthis paper, we propose to use community detection to tackle the problem ofsecure friend discovery. We formulate the first privacy-preserving anddecentralized community detection problem as a multi-objective optimization. Wedesign the first protocol to solve this problem, which transforms communitydetection to a series of Private Set Intersection (PSI) instances usingTruncated Random Walk (TRW). Preliminary theoretical results show that ourprotocol can uncover communities with overwhelming probability and preserveprivacy. We also discuss future works, potential extensions and variations.
arxiv-6900-66 | Convex Optimization: Algorithms and Complexity | http://arxiv.org/pdf/1405.4980v2.pdf | author:Sébastien Bubeck category:math.OC cs.CC cs.LG cs.NA stat.ML published:2014-05-20 summary:This monograph presents the main complexity theorems in convex optimizationand their corresponding algorithms. Starting from the fundamental theory ofblack-box optimization, the material progresses towards recent advances instructural optimization and stochastic optimization. Our presentation ofblack-box optimization, strongly influenced by Nesterov's seminal book andNemirovski's lecture notes, includes the analysis of cutting plane methods, aswell as (accelerated) gradient descent schemes. We also pay special attentionto non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirrordescent, and dual averaging) and discuss their relevance in machine learning.We provide a gentle introduction to structural optimization with FISTA (tooptimize a sum of a smooth and a simple non-smooth term), saddle-point mirrorprox (Nemirovski's alternative to Nesterov's smoothing), and a concisedescription of interior point methods. In stochastic optimization we discussstochastic gradient descent, mini-batches, random coordinate descent, andsublinear algorithms. We also briefly touch upon convex relaxation ofcombinatorial problems and the use of randomness to round solutions, as well asrandom walks based methods.
arxiv-6900-67 | Adapted Approach for Fruit Disease Identification using Images | http://arxiv.org/pdf/1405.4930v5.pdf | author:Shiv Ram Dubey, Anand Singh Jalal category:cs.CV published:2014-05-20 summary:Diseases in fruit cause devastating problem in economic losses and productionin agricultural industry worldwide. In this paper, an adaptive approach for theidentification of fruit diseases is proposed and experimentally validated. Theimage processing based proposed approach is composed of the following mainsteps; in the first step K-Means clustering technique is used for the defectsegmentation, in the second step some state of the art features are extractedfrom the segmented image, and finally images are classified into one of theclasses by using a Multi-class Support Vector Machine. We have considereddiseases of apple as a test case and evaluated our approach for three types ofapple diseases namely apple scab, apple blotch and apple rot. Our experimentalresults express that the proposed solution can significantly support accuratedetection and automatic identification of fruit diseases. The classificationaccuracy for the proposed solution is achieved up to 93%.
arxiv-6900-68 | The ROMES method for statistical modeling of reduced-order-model error | http://arxiv.org/pdf/1405.5170v3.pdf | author:Martin Drohmann, Kevin Carlberg category:cs.NA math.NA stat.ML published:2014-05-20 summary:This work presents a technique for statistically modeling errors introducedby reduced-order models. The method employs Gaussian-process regression toconstruct a mapping from a small number of computationally inexpensive `errorindicators' to a distribution over the true error. The variance of thisdistribution can be interpreted as the (epistemic) uncertainty introduced bythe reduced-order model. To model normed errors, the method employs existingrigorous error bounds and residual norms as indicators; numerical experimentsshow that the method leads to a near-optimal expected effectivity in contrastto typical error bounds. To model errors in general outputs, the method usesdual-weighted residuals---which are amenable to uncertainty control---asindicators. Experiments illustrate that correcting the reduced-order-modeloutput with this surrogate can improve prediction accuracy by an order ofmagnitude; this contrasts with existing `multifidelity correction' approaches,which often fail for reduced-order models and suffer from the curse ofdimensionality. The proposed error surrogates also lead to a notion of`probabilistic rigor', i.e., the surrogate bounds the error with specifiedprobability.
arxiv-6900-69 | Sparsity Based Methods for Overparameterized Variational Problems | http://arxiv.org/pdf/1405.4969v5.pdf | author:Raja Giryes, Michael Elad, Alfred M. Bruckstein category:cs.CV stat.ML published:2014-05-20 summary:Two complementary approaches have been extensively used in signal and imageprocessing leading to novel results, the sparse representation methodology andthe variational strategy. Recently, a new sparsity based model has beenproposed, the cosparse analysis framework, which may potentially help inbridging sparse approximation based methods to the traditional total-variationminimization. Based on this, we introduce a sparsity based framework forsolving overparameterized variational problems. The latter has been used toimprove the estimation of optical flow and also for general denoising ofsignals and images. However, the recovery of the space varying parametersinvolved was not adequately addressed by traditional variational methods. Wefirst demonstrate the efficiency of the new framework for one dimensionalsignals in recovering a piecewise linear and polynomial function. Then, weillustrate how the new technique can be used for denoising and segmentation ofimages.
arxiv-6900-70 | Single camera pose estimation using Bayesian filtering and Kinect motion priors | http://arxiv.org/pdf/1405.5047v2.pdf | author:Michael Burke, Joan Lasenby category:cs.CV cs.HC published:2014-05-20 summary:Traditional approaches to upper body pose estimation using monocular visionrely on complex body models and a large variety of geometric constraints. Weargue that this is not ideal and somewhat inelegant as it results in largeprocessing burdens, and instead attempt to incorporate these constraintsthrough priors obtained directly from training data. A prior distributioncovering the probability of a human pose occurring is used to incorporatelikely human poses. This distribution is obtained offline, by fitting aGaussian mixture model to a large dataset of recorded human body poses, trackedusing a Kinect sensor. We combine this prior information with a random walktransition model to obtain an upper body model, suitable for use within arecursive Bayesian filtering framework. Our model can be viewed as a mixture ofdiscrete Ornstein-Uhlenbeck processes, in that states behave as random walks,but drift towards a set of typically observed poses. This model is combinedwith measurements of the human head and hand positions, using recursiveBayesian estimation to incorporate temporal information. Measurements areobtained using face detection and a simple skin colour hand detector, trainedusing the detected face. The suggested model is designed with analyticaltractability in mind and we show that the pose tracking can beRao-Blackwellised using the mixture Kalman filter, allowing for computationalefficiency while still incorporating bio-mechanical properties of the upperbody. In addition, the use of the proposed upper body model allows reliablethree-dimensional pose estimates to be obtained indirectly for a number ofjoints that are often difficult to detect using traditional object recognitionstrategies. Comparisons with Kinect sensor results and the state of the art in2D pose estimation highlight the efficacy of the proposed approach.
arxiv-6900-71 | Screening Tests for Lasso Problems | http://arxiv.org/pdf/1405.4897v1.pdf | author:Zhen James Xiang, Yun Wang, Peter J. Ramadge category:cs.LG stat.ML published:2014-05-19 summary:This paper is a survey of dictionary screening for the lasso problem. Thelasso problem seeks a sparse linear combination of the columns of a dictionaryto best match a given target vector. This sparse representation has provenuseful in a variety of subsequent processing and decision tasks. For a giventarget vector, dictionary screening quickly identifies a subset of dictionarycolumns that will receive zero weight in a solution of the corresponding lassoproblem. These columns can be removed from the dictionary, prior to solving thelasso problem, without impacting the optimality of the solution obtained. Thishas two potential advantages: it reduces the size of the dictionary, allowingthe lasso problem to be solved with less resources, and it may speed upobtaining a solution. Using a geometrically intuitive framework, we providebasic insights for understanding useful lasso screening tests and theirlimitations. We also provide illustrative numerical studies on severaldatasets.
arxiv-6900-72 | Fighting Authorship Linkability with Crowdsourcing | http://arxiv.org/pdf/1405.4918v1.pdf | author:Mishari Almishari, Ekin Oguz, Gene Tsudik category:cs.DL cs.CL published:2014-05-19 summary:Massive amounts of contributed content -- including traditional literature,blogs, music, videos, reviews and tweets -- are available on the Internettoday, with authors numbering in many millions. Textual information, such asproduct or service reviews, is an important and increasingly popular type ofcontent that is being used as a foundation of many trendy community-basedreviewing sites, such as TripAdvisor and Yelp. Some recent results have shownthat, due partly to their specialized/topical nature, sets of reviews authoredby the same person are readily linkable based on simple stylometric features.In practice, this means that individuals who author more than a few reviewsunder different accounts (whether within one site or across multiple sites) canbe linked, which represents a significant loss of privacy. In this paper, we start by showing that the problem is actually worse thanpreviously believed. We then explore ways to mitigate authorship linkability incommunity-based reviewing. We first attempt to harness the global power ofcrowdsourcing by engaging random strangers into the process of re-writingreviews. As our empirical results (obtained from Amazon Mechanical Turk)clearly demonstrate, crowdsourcing yields impressively sensible reviews thatreflect sufficiently different stylometric characteristics such that priorstylometric linkability techniques become largely ineffective. We also considerusing machine translation to automatically re-write reviews. Contrary to whatwas previously believed, our results show that translation decreases authorshiplinkability as the number of intermediate languages grows. Finally, we explorethe combination of crowdsourcing and machine translation and report on theresults.
arxiv-6900-73 | Use of Computer Vision to Detect Tangles in Tangled Objects | http://arxiv.org/pdf/1405.4802v2.pdf | author:Paritosh Parmar category:cs.CV published:2014-05-19 summary:Untangling of structures like ropes and wires by autonomous robots can beuseful in areas such as personal robotics, industries and electrical wiring &repairing by robots. This problem can be tackled by using computer visionsystem in robot. This paper proposes a computer vision based method foranalyzing visual data acquired from camera for perceiving the overlap of wires,ropes, hoses i.e. detecting tangles. Information obtained after processingimage according to the proposed method comprises of position of tangles intangled object and which wire passes over which wire. This information can thenbe used to guide robot to untangle wire/s. Given an image, preprocessing isdone to remove noise. Then edges of wire are detected. After that, the image isdivided into smaller blocks and each block is checked for wire overlap/s andfinding other relevant information. TANGLED-100 dataset was introduced, whichconsists of images of tangled linear deformable objects. Method discussed inhere was tested on the TANGLED-100 dataset. Accuracy achieved duringexperiments was found to be 74.9%. Robotic simulations were carried out todemonstrate the use of the proposed method in applications of robot. Proposedmethod is a general method that can be used by robots working in differentsituations.
arxiv-6900-74 | A Parallel Way to Select the Parameters of SVM Based on the Ant Optimization Algorithm | http://arxiv.org/pdf/1405.4589v2.pdf | author:Chao Zhang, Hong-cen Mei, Hao Yang category:cs.NE cs.LG published:2014-05-19 summary:A large number of experimental data shows that Support Vector Machine (SVM)algorithm has obvious advantages in text classification, handwritingrecognition, image classification, bioinformatics, and some other fields. Tosome degree, the optimization of SVM depends on its kernel function and Slackvariable, the determinant of which is its parameters $\delta$ and c in theclassification function. That is to say,to optimize the SVM algorithm, theoptimization of the two parameters play a huge role. Ant Colony Optimization(ACO) is optimization algorithm which simulate ants to find the optimal path.Inthe available literature, we mix the ACO algorithm and Parallel algorithmtogether to find a well parameters.
arxiv-6900-75 | Scalable Semidefinite Relaxation for Maximum A Posterior Estimation | http://arxiv.org/pdf/1405.4807v1.pdf | author:Qixing Huang, Yuxin Chen, Leonidas Guibas category:cs.LG cs.CV cs.IT math.IT math.OC stat.ML published:2014-05-19 summary:Maximum a posteriori (MAP) inference over discrete Markov random fields is afundamental task spanning a wide spectrum of real-world applications, which isknown to be NP-hard for general graphs. In this paper, we propose a novelsemidefinite relaxation formulation (referred to as SDR) to estimate the MAPassignment. Algorithmically, we develop an accelerated variant of thealternating direction method of multipliers (referred to as SDPAD-LR) that caneffectively exploit the special structure of the new relaxation. Encouragingly,the proposed procedure allows solving SDR for large-scale problems, e.g.,problems on a grid graph comprising hundreds of thousands of variables withmultiple states per node. Compared with prior SDP solvers, SDPAD-LR is capableof attaining comparable accuracy while exhibiting remarkably improvedscalability, in contrast to the commonly held belief that semidefiniterelaxation can only been applied on small-scale MRF problems. We have evaluatedthe performance of SDR on various benchmark datasets including OPENGM2 and PICin terms of both the quality of the solutions and computation time.Experimental results demonstrate that for a broad class of problems, SDPAD-LRoutperforms state-of-the-art algorithms in producing better MAP assignment inan efficient manner.
arxiv-6900-76 | Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms | http://arxiv.org/pdf/1405.4758v1.pdf | author:Stefan Magureanu, Richard Combes, Alexandre Proutiere category:cs.LG published:2014-05-19 summary:We consider stochastic multi-armed bandit problems where the expected rewardis a Lipschitz function of the arm, and where the set of arms is eitherdiscrete or continuous. For discrete Lipschitz bandits, we derive asymptoticproblem specific lower bounds for the regret satisfied by any algorithm, andpropose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitzstructure of the problem. In fact, we prove that OSLB is asymptoticallyoptimal, as its asymptotic regret matches the lower bound. The regret analysisof our algorithms relies on a new concentration inequality for weighted sums ofKL divergences between the empirical distributions of rewards and their truedistributions. For continuous Lipschitz bandits, we propose to first discretizethe action space, and then apply OSLB or CKL-UCB, algorithms that provablyexploit the structure efficiently. This approach is shown, through numericalexperiments, to significantly outperform existing algorithms that directly dealwith the continuous set of arms. Finally the results and algorithms areextended to contextual bandits with similarities.
arxiv-6900-77 | Modelling Data Dispersion Degree in Automatic Robust Estimation for Multivariate Gaussian Mixture Models with an Application to Noisy Speech Processing | http://arxiv.org/pdf/1405.4599v1.pdf | author:Dalei Wu, Haiqing Wu category:cs.CL cs.LG stat.ML published:2014-05-19 summary:The trimming scheme with a prefixed cutoff portion is known as a method ofimproving the robustness of statistical models such as multivariate Gaussianmixture models (MG- MMs) in small scale tests by alleviating the impacts ofoutliers. However, when this method is applied to real- world data, such asnoisy speech processing, it is hard to know the optimal cut-off portion toremove the outliers and sometimes removes useful data samples as well. In thispaper, we propose a new method based on measuring the dispersion degree (DD) ofthe training data to avoid this problem, so as to realise automatic robustestimation for MGMMs. The DD model is studied by using two different measures.For each one, we theoretically prove that the DD of the data samples in acontext of MGMMs approximately obeys a specific (chi or chi-square)distribution. The proposed method is evaluated on a real-world application witha moderately-sized speaker recognition task. Experiments show that the proposedmethod can significantly improve the robustness of the conventional trainingmethod of GMMs for speaker recognition.
arxiv-6900-78 | Kronecker PCA Based Spatio-Temporal Modeling of Video for Dismount Classification | http://arxiv.org/pdf/1405.4574v1.pdf | author:Kristjan H. Greenewald, Alfred O. Hero III category:cs.CV stat.ME published:2014-05-19 summary:We consider the application of KronPCA spatio-temporal modeling techniques[Greenewald et al 2013, Tsiligkaridis et al 2013] to the extraction ofspatiotemporal features for video dismount classification. KronPCA performs alow-rank type of dimensionality reduction that is adapted to spatio-temporaldata and is characterized by the T frame multiframe mean and covariance of pspatial features. For further regularization and improved inverse estimation,we also use the diagonally corrected KronPCA shrinkage methods we presented in[Greenewald et al 2013]. We apply this very general method to the modeling ofthe multivariate temporal behavior of HOG features extracted from pedestrianbounding boxes in video, with gender classification in a challenging datasetchosen as a specific application. The learned covariances for each class areused to extract spatiotemporal features which are then classified, achievingcompetitive classification performance.
arxiv-6900-79 | ESSP: An Efficient Approach to Minimizing Dense and Nonsubmodular Energy Functions | http://arxiv.org/pdf/1405.4583v1.pdf | author:Wei Feng, Jiaya Jia, Zhi-Qiang Liu category:cs.CV cs.LG published:2014-05-19 summary:Many recent advances in computer vision have demonstrated the impressivepower of dense and nonsubmodular energy functions in solving visual labelingproblems. However, minimizing such energies is challenging. None of existingtechniques (such as s-t graph cut, QPBO, BP and TRW-S) can individually do thiswell. In this paper, we present an efficient method, namely ESSP, to optimizebinary MRFs with arbitrary pairwise potentials, which could be nonsubmodularand with dense connectivity. We also provide a comparative study of ourapproach and several recent promising methods. From our study, we make somereasonable recommendations of combining existing methods that perform the bestin different situations for this challenging problem. Experimental resultsvalidate that for dense and nonsubmodular energy functions, the proposedapproach can usually obtain lower energies than the best combination of othertechniques using comparably reasonable time.
arxiv-6900-80 | On the saddle point problem for non-convex optimization | http://arxiv.org/pdf/1405.4604v2.pdf | author:Razvan Pascanu, Yann N. Dauphin, Surya Ganguli, Yoshua Bengio category:cs.LG cs.NE published:2014-05-19 summary:A central challenge to many fields of science and engineering involvesminimizing non-convex error functions over continuous, high dimensional spaces.Gradient descent or quasi-Newton methods are almost ubiquitously used toperform such minimizations, and it is often thought that a main source ofdifficulty for the ability of these local methods to find the global minimum isthe proliferation of local minima with much higher error than the globalminimum. Here we argue, based on results from statistical physics, randommatrix theory, and neural network theory, that a deeper and more profounddifficulty originates from the proliferation of saddle points, not localminima, especially in high dimensional problems of practical interest. Suchsaddle points are surrounded by high error plateaus that can dramatically slowdown learning, and give the illusory impression of the existence of a localminimum. Motivated by these arguments, we propose a new algorithm, thesaddle-free Newton method, that can rapidly escape high dimensional saddlepoints, unlike gradient descent and quasi-Newton methods. We apply thisalgorithm to deep neural network training, and provide preliminary numericalevidence for its superior performance.
arxiv-6900-81 | A distributed block coordinate descent method for training $l_1$ regularized linear classifiers | http://arxiv.org/pdf/1405.4544v2.pdf | author:Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan category:cs.LG published:2014-05-18 summary:Distributed training of $l_1$ regularized classifiers has received greatattention recently. Most existing methods approach this problem by taking stepsobtained from approximating the objective by a quadratic approximation that isdecoupled at the individual variable level. These methods are designed formulticore and MPI platforms where communication costs are low. They areineffi?cient on systems such as Hadoop running on a cluster of commoditymachines where communication costs are substantial. In this paper we design adistributed algorithm for $l_1$ regularization that is much better suited forsuch systems than existing algorithms. A careful cost analysis is used tosupport these points and motivate our method. The main idea of our algorithm isto do block optimization of many variables on the actual objective functionwithin each computing node; this increases the computational cost per step thatis matched with the communication cost, and decreases the number of outeriterations, thus yielding a faster overall method. Distributed Gauss-Seidel andGauss-Southwell greedy schemes are used for choosing variables to update ineach step. We establish global convergence theory for our algorithm, includingQ-linear rate of convergence. Experiments on two benchmark problems show ourmethod to be much faster than existing methods.
arxiv-6900-82 | Machine Learning in Wireless Sensor Networks: Algorithms, Strategies, and Applications | http://arxiv.org/pdf/1405.4463v2.pdf | author:Mohammad Abu Alsheikh, Shaowei Lin, Dusit Niyato, Hwee-Pink Tan category:cs.NI cs.LG published:2014-05-18 summary:Wireless sensor networks monitor dynamic environments that change rapidlyover time. This dynamic behavior is either caused by external factors orinitiated by the system designers themselves. To adapt to such conditions,sensor networks often adopt machine learning techniques to eliminate the needfor unnecessary redesign. Machine learning also inspires many practicalsolutions that maximize resource utilization and prolong the lifespan of thenetwork. In this paper, we present an extensive literature review over theperiod 2002-2013 of machine learning methods that were used to address commonissues in wireless sensor networks (WSNs). The advantages and disadvantages ofeach proposed algorithm are evaluated against the corresponding problem. Wealso provide a comparative guide to aid WSN designers in developing suitablemachine learning solutions for their specific application challenges.
arxiv-6900-83 | A Distributed Algorithm for Training Nonlinear Kernel Machines | http://arxiv.org/pdf/1405.4543v1.pdf | author:Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan category:cs.LG published:2014-05-18 summary:This paper concerns the distributed training of nonlinear kernel machines onMap-Reduce. We show that a re-formulation of Nystr\"om approximation basedsolution which is solved using gradient based techniques is well suited forthis, especially when it is necessary to work with a large number of basispoints. The main advantages of this approach are: avoidance of computing thepseudo-inverse of the kernel sub-matrix corresponding to the basis points;simplicity and efficiency of the distributed part of the computations; and,friendliness to stage-wise addition of basis points. We implement the methodusing an AllReduce tree on Hadoop and demonstrate its value on a few largebenchmark datasets.
arxiv-6900-84 | A Memetic Algorithm for the Linear Ordering Problem with Cumulative Costs | http://arxiv.org/pdf/1405.4510v1.pdf | author:Tao Ye, Kan Zhou, Zhipeng Lu, Jin-Kao Hao category:cs.NE published:2014-05-18 summary:This paper introduces an effective memetic algorithm for the linear orderingproblem with cumulative costs. The proposed algorithm combines an order-basedrecombination operator with an improved forward-backward local search procedureand employs a solution quality based replacement criterion for pool updating.Extensive experiments on 118 well-known benchmark instances show that theproposed algorithm achieves competitive results by identifying 46 new upperbounds. Furthermore, some critical ingredients of our algorithm are analyzed tounderstand the source of its performance.
arxiv-6900-85 | A Multi-parent Memetic Algorithm for the Linear Ordering Problem | http://arxiv.org/pdf/1405.4507v1.pdf | author:Tao Ye, Tao Wang, Zhipeng Lu, Jin-Kao Hao category:cs.NE math.OC published:2014-05-18 summary:In this paper, we present a multi-parent memetic algorithm (denoted by MPM)for solving the classic Linear Ordering Problem (LOP). The MPM algorithmintegrates in particular a multi-parent recombination operator for generatingoffspring solutions and a distance-and-quality based criterion for poolupdating. Our MPM algorithm is assessed on 8 sets of 484 widely used LOPinstances and compared with several state-of-the-art algorithms in theliterature, showing the efficacy of the MPM algorithm. Specifically, for the255 instances whose optimal solutions are unknown, the MPM is able to detectbetter solutions than the previous best-known ones for 66 instances, whilematching the previous best-known results for 163 instances. Furthermore, someadditional experiments are carried out to analyze the key elements andimportant parameters of MPM.
arxiv-6900-86 | Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice | http://arxiv.org/pdf/1405.4506v1.pdf | author:Xiaojiang Peng, Limin Wang, Xingxing Wang, Yu Qiao category:cs.CV published:2014-05-18 summary:Video based action recognition is one of the important and challengingproblems in computer vision research. Bag of Visual Words model (BoVW) withlocal features has become the most popular method and obtained thestate-of-the-art performance on several realistic datasets, such as the HMDB51,UCF50, and UCF101. BoVW is a general pipeline to construct a globalrepresentation from a set of local features, which is mainly composed of fivesteps: (i) feature extraction, (ii) feature pre-processing, (iii) codebookgeneration, (iv) feature encoding, and (v) pooling and normalization. Manyefforts have been made in each step independently in different scenarios andtheir effect on action recognition is still unknown. Meanwhile, video dataexhibits different views of visual pattern, such as static appearance andmotion dynamics. Multiple descriptors are usually extracted to represent thesedifferent views. Many feature fusion methods have been developed in other areasand their influence on action recognition has never been investigated before.This paper aims to provide a comprehensive study of all steps in BoVW anddifferent fusion methods, and uncover some good practice to produce astate-of-the-art action recognition system. Specifically, we explore two kindsof local features, ten kinds of encoding methods, eight kinds of pooling andnormalization strategies, and three kinds of fusion methods. We conclude thatevery step is crucial for contributing to the final recognition rate.Furthermore, based on our comprehensive study, we propose a simple yeteffective representation, called hybrid representation, by exploring thecomplementarity of different BoVW frameworks and local descriptors. Using thisrepresentation, we obtain the state-of-the-art on the three challengingdatasets: HMDB51 (61.1%), UCF50 (92.3%), and UCF101 (87.9%).
arxiv-6900-87 | Online Learning with Composite Loss Functions | http://arxiv.org/pdf/1405.4471v1.pdf | author:Ofer Dekel, Jian Ding, Tomer Koren, Yuval Peres category:cs.LG published:2014-05-18 summary:We study a new class of online learning problems where each of the onlinealgorithm's actions is assigned an adversarial value, and the loss of thealgorithm at each step is a known and deterministic function of the valuesassigned to its recent actions. This class includes problems where thealgorithm's loss is the minimum over the recent adversarial values, the maximumover the recent values, or a linear combination of the recent values. Weanalyze the minimax regret of this class of problems when the algorithmreceives bandit feedback, and prove that when the minimum or maximum functionsare used, the minimax regret is $\tilde \Omega(T^{2/3})$ (so called hard onlinelearning problems), and when a linear function is used, the minimax regret is$\tilde O(\sqrt{T})$ (so called easy learning problems). Previously, the onlyonline learning problem that was known to be provably hard was the multi-armedbandit with switching costs.
arxiv-6900-88 | Efficient Tracking of a Moving Object using Inter-Frame Coding | http://arxiv.org/pdf/1405.4389v1.pdf | author:Shraddha Mehta, Vaishali Kalariya category:cs.CV published:2014-05-17 summary:Video surveillance has long been in use to monitor security sensitive areassuch as banks, department stores, highways, crowded public places andborders.The advance in computing power, availability of large-capacity storagedevices and high speed network infrastructure paved the way for cheaper,multi-sensor video surveillance systems.Traditionally, the video outputs areprocessed online by human operators and are usually saved to tapes for lateruse only after a forensic event.The increase in the number of cameras inordinary surveillance systems overloaded both the human operators and thestorage devices with high volumes of data and made it in-feasible to ensureproper monitoring of sensitive areas for long times.
arxiv-6900-89 | Multi-layered graph-based multi-document summarization model | http://arxiv.org/pdf/1405.7975v1.pdf | author:Ercan Canhasi category:cs.IR cs.CL published:2014-05-17 summary:Multi-document summarization is a process of automatic generation of acompressed version of the given collection of documents. Recently, thegraph-based models and ranking algorithms have been actively investigated bythe extractive document summarization community. While most work to datefocuses on homogeneous connecteness of sentences and heterogeneous connectenessof documents and sentences (e.g. sentence similarity weighted by documentimportance), in this paper we present a novel 3-layered graph model thatemphasizes not only sentence and document level relations but also theinfluence of under sentence level relations (e.g. a part of sentencesimilarity).
arxiv-6900-90 | Real Time Object Tracking Based on Inter-frame Coding: A Review | http://arxiv.org/pdf/1405.4390v1.pdf | author:Shraddha Mehta, Vaishali Kalariya category:cs.CV published:2014-05-17 summary:Inter-frame Coding plays significant role for video Compression and ComputerVision. Computer vision systems have been incorporated in many real lifeapplications (e.g. surveillance systems, medical imaging, robot navigation andidentity verification systems). Object tracking is a key computer vision topic,which aims at detecting the position of a moving object from a video sequence.The application of Inter-frame Coding for low frame rate video, as well as forlow resolution video. Various methods based on Top-down approach just likekernel based or mean shift technique are used to track the object for video,So, Inter-frame Coding algorithms are widely adopted by video coding standards,mainly due to their simplicity and good distortion performance for objecttracking.
arxiv-6900-91 | Preliminary Report on the Structure of Croatian Linguistic Co-occurrence Networks | http://arxiv.org/pdf/1405.4433v1.pdf | author:Domagoj Margan, Sanda Martinčić-Ipšić, Ana Meštrović category:cs.CL cs.SI physics.soc-ph published:2014-05-17 summary:In this article, we investigate the structure of Croatian linguisticco-occurrence networks. We examine the change of network structure propertiesby systematically varying the co-occurrence window sizes, the corpus sizes andremoving stopwords. In a co-occurrence window of size $n$ we establish a linkbetween the current word and $n-1$ subsequent words. The results point out thatthe increase of the co-occurrence window size is followed by a decrease indiameter, average path shortening and expectedly condensing the averageclustering coefficient. The same can be noticed for the removal of thestopwords. Finally, since the size of texts is reflected in the networkproperties, our results suggest that the corpus influence can be reduced byincreasing the co-occurrence window size.
arxiv-6900-92 | Thematically Reinforced Explicit Semantic Analysis | http://arxiv.org/pdf/1405.4364v1.pdf | author:Yannis Haralambous, Vitaly Klyuev category:cs.CL 68T50 published:2014-05-17 summary:We present an extended, thematically reinforced version of Gabrilovich andMarkovitch's Explicit Semantic Analysis (ESA), where we obtain thematicinformation through the category structure of Wikipedia. For this we firstdefine a notion of categorical tfidf which measures the relevance of terms incategories. Using this measure as a weight we calculate a maximal spanning treeof the Wikipedia corpus considered as a directed graph of pages and categories.This tree provides us with a unique path of "most related categories" betweeneach page and the top of the hierarchy. We reinforce tfidf of words in a pageby aggregating it with categorical tfidfs of the nodes of these paths, anddefine a thematically reinforced ESA semantic relatedness measure which is morerobust than standard ESA and less sensitive to noise caused by out-of-contextwords. We apply our method to the French Wikipedia corpus, evaluate it througha text classification on a 37.5 MB corpus of 20 French newsgroups and obtain aprecision increase of 9-10% compared with standard ESA.
arxiv-6900-93 | Identification of functionally related enzymes by learning-to-rank methods | http://arxiv.org/pdf/1405.4394v1.pdf | author:Michiel Stock, Thomas Fober, Eyke Hüllermeier, Serghei Glinca, Gerhard Klebe, Tapio Pahikkala, Antti Airola, Bernard De Baets, Willem Waegeman category:cs.LG cs.CE q-bio.QM stat.ML published:2014-05-17 summary:Enzyme sequences and structures are routinely used in the biological sciencesas queries to search for functionally related enzymes in online databases. Tothis end, one usually departs from some notion of similarity, comparing twoenzymes by looking for correspondences in their sequences, structures orsurfaces. For a given query, the search operation results in a ranking of theenzymes in the database, from very similar to dissimilar enzymes, whileinformation about the biological function of annotated database enzymes isignored. In this work we show that rankings of that kind can be substantially improvedby applying kernel-based learning algorithms. This approach enables thedetection of statistical dependencies between similarities of the active cleftand the biological function of annotated enzymes. This is in contrast tosearch-based approaches, which do not take annotated training data intoaccount. Similarity measures based on the active cleft are known to outperformsequence-based or structure-based measures under certain conditions. Weconsider the Enzyme Commission (EC) classification hierarchy for obtainingannotated enzymes during the training phase. The results of a set of sizeableexperiments indicate a consistent and significant improvement for a set ofsimilarity measures that exploit information about small cavities in thesurface of enzymes.
arxiv-6900-94 | A two-step learning approach for solving full and almost full cold start problems in dyadic prediction | http://arxiv.org/pdf/1405.4423v1.pdf | author:Tapio Pahikkala, Michiel Stock, Antti Airola, Tero Aittokallio, Bernard De Baets, Willem Waegeman category:cs.LG published:2014-05-17 summary:Dyadic prediction methods operate on pairs of objects (dyads), aiming toinfer labels for out-of-sample dyads. We consider the full and almost full coldstart problem in dyadic prediction, a setting that occurs when both objects inan out-of-sample dyad have not been observed during training, or if one of themhas been observed, but very few times. A popular approach for addressing thisproblem is to train a model that makes predictions based on a pairwise featurerepresentation of the dyads, or, in case of kernel methods, based on a tensorproduct pairwise kernel. As an alternative to such a kernel approach, weintroduce a novel two-step learning algorithm that borrows ideas from thefields of pairwise learning and spectral filtering. We show theoretically thatthe two-step method is very closely related to the tensor product kernelapproach, and experimentally that it yields a slightly better predictiveperformance. Moreover, unlike existing tensor product kernel methods, thetwo-step method allows closed-form solutions for training and parameterselection via cross-validation estimates both in the full and almost full coldstart settings, making the approach much more efficient and straightforward toimplement.
arxiv-6900-95 | That's sick dude!: Automatic identification of word sense change across different timescales | http://arxiv.org/pdf/1405.4392v1.pdf | author:Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Biemann, Animesh Mukherjee, Pawan Goyal category:cs.CL cs.AI 68T50 published:2014-05-17 summary:In this paper, we propose an unsupervised method to identify noun sensechanges based on rigorous analysis of time-varying text data available in theform of millions of digitized books. We construct distributional thesauri basednetworks from data at different time points and cluster each of them separatelyto obtain word-centric sense clusters corresponding to the different timepoints. Subsequently, we compare these sense clusters of two different timepoints to find if (i) there is birth of a new sense or (ii) if an older sensehas got split into more than one sense or (iii) if a newer sense has beenformed from the joining of older senses or (iv) if a particular sense has died.We conduct a thorough evaluation of the proposed methodology both manually aswell as through comparison with WordNet. Manual evaluation indicates that thealgorithm could correctly identify 60.4% birth cases from a set of 48 randomlypicked samples and 57% split/join cases from a set of 21 randomly pickedsamples. Remarkably, in 44% cases the birth of a novel sense is attested byWordNet, while in 46% cases and 43% cases split and join are respectivelyconfirmed by WordNet. Our approach can be applied for lexicography, as well asfor applications like word sense disambiguation or semantic search.
arxiv-6900-96 | Leveraging Evolutionary Search to Discover Self-Adaptive and Self-Organizing Cellular Automata | http://arxiv.org/pdf/1405.4322v1.pdf | author:David B. Knoester, Heather J. Goldsby, Christoph Adami category:cs.NE nlin.CG published:2014-05-16 summary:Building self-adaptive and self-organizing (SASO) systems is a challengingproblem, in part because SASO principles are not yet well understood and fewplatforms exist for exploring them. Cellular automata (CA) are a well-studiedapproach to exploring the principles underlying self-organization. A CAcomprises a lattice of cells whose states change over time based on a discreteupdate function. One challenge to developing CA is that the relationship of anupdate function, which describes the local behavior of each cell, to the globalbehavior of the entire CA is often unclear. As a result, many researchers haveused stochastic search techniques, such as evolutionary algorithms, toautomatically discover update functions that produce a desired global behavior.However, these update functions are typically defined in a way that does notprovide for self-adaptation. Here we describe an approach to discovering CAupdate functions that are both self-adaptive and self-organizing. Specifically,we use a novel evolutionary algorithm-based approach to discover finite statemachines (FSMs) that implement update functions for CA. We show how thisapproach is able to evolve FSM-based update functions that perform well on thedensity classification task for 1-, 2-, and 3-dimensional CA. Moreover, we showthat these FSMs are self-adaptive, self-organizing, and highly scalable, oftenperforming well on CA that are orders of magnitude larger than those used toevaluate performance during the evolutionary search. These results demonstratethat CA are a viable platform for studying the integration of self-adaptationand self-organization, and strengthen the case for using evolutionaryalgorithms as a component of SASO systems.
arxiv-6900-97 | Computer Vision Approach for Low Cost, High Precision Measurement of Grapevine Trunk Diameter in Outdoor Conditions | http://arxiv.org/pdf/1406.4845v2.pdf | author:Diego Sebastián Pérez, Facundo Bromberg, Francisco Gonzalez Antivilo category:cs.CV published:2014-05-16 summary:Trunk diameter is a variable of agricultural interest, used mainly in theprediction of fruit trees production. It is correlated with leaf area andbiomass of trees, and consequently gives a good estimate of the potentialproduction of the plants. This work presents a low cost, high precision methodfor the measurement of trunk diameter of grapevines based on Computer Visiontechniques. Several methods based on Computer Vision and other techniques areintroduced in the literature. These methods present different advantages forcrop management: they are amenable to be operated by unknowledgeable personnel,with lower operational costs; they result in lower stress levels toknowledgeable personnel, avoiding the deterioration of the measurement qualityover time; and they make the measurement process amenable to be embedded inlarger autonomous systems, allowing more measurements to be taken withequivalent costs. To date, all existing autonomous methods are either of lowprecision, or have a prohibitive cost for massive agricultural adoption,leaving the manual Vernier caliper or tape measure as the only choice in mostsituations. In this work we present a semi-autonomous measurement method thatis susceptible to be fully automated, cost effective for mass adoption, and itsprecision is competitive (with slight improvements) over the caliper manualmethod.
arxiv-6900-98 | Active Semi-Supervised Learning Using Sampling Theory for Graph Signals | http://arxiv.org/pdf/1405.4324v1.pdf | author:Akshay Gadde, Aamir Anis, Antonio Ortega category:cs.LG stat.ML published:2014-05-16 summary:We consider the problem of offline, pool-based active semi-supervisedlearning on graphs. This problem is important when the labeled data is scarceand expensive whereas unlabeled data is easily available. The data points arerepresented by the vertices of an undirected graph with the similarity betweenthem captured by the edge weights. Given a target number of nodes to label, thegoal is to choose those nodes that are most informative and then predict theunknown labels. We propose a novel framework for this problem based on ourrecent results on sampling theory for graph signals. A graph signal is areal-valued function defined on each node of the graph. A notion of frequencyfor such signals can be defined using the spectrum of the graph Laplacianmatrix. The sampling theory for graph signals aims to extend the traditionalNyquist-Shannon sampling theory by allowing us to identify the class of graphsignals that can be reconstructed from their values on a subset of vertices.This approach allows us to define a criterion for active learning based onsampling set selection which aims at maximizing the frequency of the signalsthat can be reconstructed from their samples on the set. Experiments show theeffectiveness of our method.
arxiv-6900-99 | Classification using log Gaussian Cox processes | http://arxiv.org/pdf/1405.4141v2.pdf | author:Alexander G. de. G Matthews, Zoubin Ghahramani category:stat.ML stat.CO stat.ME published:2014-05-16 summary:McCullagh and Yang (2006) suggest a family of classification algorithms basedon Cox processes. We further investigate the log Gaussian variant which has anumber of appealing properties. Conditioned on the covariates, the distributionover labels is given by a type of conditional Markov random field. In thesupervised case, computation of the predictive probability of a single testpoint scales linearly with the number of training points and the multiclassgeneralization is straightforward. We show new links between the supervisedmethod and classical nonparametric methods. We give a detailed analysis of thepairwise graph representable Markov random field, which we use to extend themodel to semi-supervised learning problems, and propose an inference methodbased on graph min-cuts. We give the first experimental analysis on supervisedand semi-supervised datasets and show good empirical performance.
arxiv-6900-100 | Methods and Models for Interpretable Linear Classification | http://arxiv.org/pdf/1405.4047v2.pdf | author:Berk Ustun, Cynthia Rudin category:stat.ME cs.LG stat.ML published:2014-05-16 summary:We present an integer programming framework to build accurate andinterpretable discrete linear classification models. Unlike existingapproaches, our framework is designed to provide practitioners with the controland flexibility they need to tailor accurate and interpretable models for adomain of choice. To this end, our framework can produce models that are fullyoptimized for accuracy, by minimizing the 0--1 classification loss, and thataddress multiple aspects of interpretability, by incorporating a range ofdiscrete constraints and penalty functions. We use our framework to producemodels that are difficult to create with existing methods, such as scoringsystems and M-of-N rule tables. In addition, we propose specially designedoptimization methods to improve the scalability of our framework throughdecomposition and data reduction. We show that discrete linear classifiers canattain the training accuracy of any other linear classifier, and provide anOccam's Razor type argument as to why the use of small discrete coefficientscan provide better generalization. We demonstrate the performance andflexibility of our framework through numerical experiments and a case study inwhich we construct a highly tailored clinical tool for sleep apnea diagnosis.
arxiv-6900-101 | Distributed Representations of Sentences and Documents | http://arxiv.org/pdf/1405.4053v2.pdf | author:Quoc V. Le, Tomas Mikolov category:cs.CL cs.AI cs.LG published:2014-05-16 summary:Many machine learning algorithms require the input to be represented as afixed-length feature vector. When it comes to texts, one of the most commonfixed-length features is bag-of-words. Despite their popularity, bag-of-wordsfeatures have two major weaknesses: they lose the ordering of the words andthey also ignore semantics of the words. For example, "powerful," "strong" and"Paris" are equally distant. In this paper, we propose Paragraph Vector, anunsupervised algorithm that learns fixed-length feature representations fromvariable-length pieces of texts, such as sentences, paragraphs, and documents.Our algorithm represents each document by a dense vector which is trained topredict words in the document. Its construction gives our algorithm thepotential to overcome the weaknesses of bag-of-words models. Empirical resultsshow that Paragraph Vectors outperform bag-of-words models as well as othertechniques for text representations. Finally, we achieve new state-of-the-artresults on several text classification and sentiment analysis tasks.
arxiv-6900-102 | A preliminary study of Croatian Language Syllable Networks | http://arxiv.org/pdf/1405.4097v2.pdf | author:Kristina Ban, Ivan Ivakić, Ana Meštrović category:cs.CL published:2014-05-16 summary:This paper presents preliminary results of Croatian syllable networksanalysis. Syllable network is a network in which nodes are syllables and linksbetween them are constructed according to their connections within words. Inthis paper we analyze networks of syllables generated from texts collected fromthe Croatian Wikipedia and Blogs. As a main tool we use complex networkanalysis methods which provide mechanisms that can reveal new patterns in alanguage structure. We aim to show that syllable networks have much higherclustering coefficient in comparison to Erd\"os-Renyi random networks. Theresults indicate that Croatian syllable networks exhibit certain properties ofa small world networks. Furthermore, we compared Croatian syllable networkswith Portuguese and Chinese syllable networks and we showed that they havesimilar properties.
arxiv-6900-103 | Coarse-to-Fine Classification via Parametric and Nonparametric Models for Computer-Aided Diagnosis | http://arxiv.org/pdf/1405.4308v1.pdf | author:Meizhu Liu, Le Lu, Xiaojing Ye, Shipeng Yu category:cs.CV published:2014-05-16 summary:Classification is one of the core problems in Computer-Aided Diagnosis (CAD),targeting for early cancer detection using 3D medical imaging interpretation.High detection sensitivity with desirably low false positive (FP) rate iscritical for a CAD system to be accepted as a valuable or even indispensabletool in radiologists' workflow. Given various spurious imagery noises whichcause observation uncertainties, this remains a very challenging task. In thispaper, we propose a novel, two-tiered coarse-to-fine (CTF) classificationcascade framework to tackle this problem. We first obtainclassification-critical data samples (e.g., samples on the decision boundary)extracted from the holistic data distributions using a robust parametric model(e.g., \cite{Raykar08}); then we build a graph-embedding based nonparametricclassifier on sampled data, which can more accurately preserve or formulate thecomplex classification boundary. These two steps can also be considered aseffective "sample pruning" and "feature pursuing + $k$NN/template matching",respectively. Our approach is validated comprehensively in colorectal polypdetection and lung nodule detection CAD systems, as the top two deadly cancers,using hospital scale, multi-site clinical datasets. The results show that ourmethod achieves overall better classification/detection performance thanexisting state-of-the-art algorithms using single-layer classifiers, such asthe support vector machine variants \cite{Wang08}, boosting \cite{Slabaugh10},logistic regression \cite{Ravesteijn10}, relevance vector machine\cite{Raykar08}, $k$-nearest neighbor \cite{Murphy09} or spectral projectionson graph \cite{Cai08}.
arxiv-6900-104 | Compositional Morphology for Word Representations and Language Modelling | http://arxiv.org/pdf/1405.4273v1.pdf | author:Jan A. Botha, Phil Blunsom category:cs.CL 68T50 I.2.7; I.2.6 published:2014-05-16 summary:This paper presents a scalable method for integrating compositionalmorphological representations into a vector-based probabilistic language model.Our approach is evaluated in the context of log-bilinear language models,rendered suitably efficient for implementation inside a machine translationdecoder by factoring the vocabulary. We perform both intrinsic and extrinsicevaluations, presenting results on a range of languages which demonstrate thatour model learns morphological representations that both perform well on wordsimilarity tasks and lead to substantial reductions in perplexity. When usedfor translation into morphologically rich languages with large vocabularies,our models obtain improvements of up to 1.2 BLEU points relative to a baselinesystem using back-off n-gram models.
arxiv-6900-105 | Les mathématiques de la langue : l'approche formelle de Montague | http://arxiv.org/pdf/1405.4248v1.pdf | author:Yannis Haralambous category:cs.CL 68T50 published:2014-05-16 summary:We present a natural language modelization method which is strongely relyingon mathematics. This method, called "Formal Semantics," has been initiated bythe American linguist Richard M. Montague in the 1970's. It uses mathematicaltools such as formal languages and grammars, first-order logic, type theory and$\lambda$-calculus. Our goal is to have the reader discover both Montagovianformal semantics and the mathematical tools that he used in his method. ----- Nous pr\'esentons une m\'ethode de mod\'elisation de la langue naturelle quiest fortement bas\'ee sur les math\'ematiques. Cette m\'ethode, appel\'ee{\guillemotleft}s\'emantique formelle{\guillemotright}, a \'et\'e initi\'ee parle linguiste am\'ericain Richard M. Montague dans les ann\'ees 1970. Elleutilise des outils math\'ematiques tels que les langages et grammaires formels,la logique du 1er ordre, la th\'eorie de types et le $\lambda$-calcul. Nousnous proposons de faire d\'ecouvrir au lecteur tant la s\'emantique formelle deMontague que les outils math\'ematiques dont il s'est servi.
arxiv-6900-106 | Optimized Cartesian $K$-Means | http://arxiv.org/pdf/1405.4054v1.pdf | author:Jianfeng Wang, Jingdong Wang, Jingkuan Song, Xin-Shun Xu, Heng Tao Shen, Shipeng Li category:cs.CV published:2014-05-16 summary:Product quantization-based approaches are effective to encodehigh-dimensional data points for approximate nearest neighbor search. The spaceis decomposed into a Cartesian product of low-dimensional subspaces, each ofwhich generates a sub codebook. Data points are encoded as compact binary codesusing these sub codebooks, and the distance between two data points can beapproximated efficiently from their codes by the precomputed lookup tables.Traditionally, to encode a subvector of a data point in a subspace, only onesub codeword in the corresponding sub codebook is selected, which may imposestrict restrictions on the search accuracy. In this paper, we propose a novelapproach, named Optimized Cartesian $K$-Means (OCKM), to better encode the datapoints for more accurate approximate nearest neighbor search. In OCKM, multiplesub codewords are used to encode the subvector of a data point in a subspace.Each sub codeword stems from different sub codebooks in each subspace, whichare optimally generated with regards to the minimization of the distortionerrors. The high-dimensional data point is then encoded as the concatenation ofthe indices of multiple sub codewords from all the subspaces. This can providemore flexibility and lower distortion errors than traditional methods.Experimental results on the standard real-life datasets demonstrate thesuperiority over state-of-the-art approaches for approximate nearest neighborsearch.
arxiv-6900-107 | Selection Bias Correction and Effect Size Estimation under Dependence | http://arxiv.org/pdf/1405.4251v2.pdf | author:Kean Ming Tan, Noah Simon, Daniela Witten category:stat.ME stat.AP stat.ML published:2014-05-16 summary:We consider large-scale studies in which it is of interest to test a verylarge number of hypotheses, and then to estimate the effect sizes correspondingto the rejected hypotheses. For instance, this setting arises in the analysisof gene expression or DNA sequencing data. However, naive estimates of theeffect sizes suffer from selection bias, i.e., some of the largest naiveestimates are large due to chance alone. Many authors have proposed methods toreduce the effects of selection bias under the assumption that the naiveestimates of the effect sizes are independent. Unfortunately, when the effectsize estimates are dependent, these existing techniques can have very poorperformance, and in practice there will often be dependence. We propose anestimator that adjusts for selection bias under a recently-proposed frequentistframework, without the independence assumption. We study some properties of theproposed estimator, and illustrate that it outperforms past proposals in asimulation study and on two gene expression data sets.
arxiv-6900-108 | INAUT, a Controlled Language for the French Coast Pilot Books Instructions nautiques | http://arxiv.org/pdf/1405.3772v1.pdf | author:Yannis Haralambous, Julie Sauvage-Vincent, John Puentes category:cs.CL I.3.5 published:2014-05-15 summary:We describe INAUT, a controlled natural language dedicated to collaborativeupdate of a knowledge base on maritime navigation and to automatic generationof coast pilot books (Instructions nautiques) of the French NationalHydrographic and Oceanographic Service SHOM. INAUT is based on French languageand abundantly uses georeferenced entities. After describing the structure ofthe overall system, giving details on the language and on its generation, anddiscussing the three major applications of INAUT (document production,interaction with ENCs and collaborative updates of the knowledge base), weconclude with future extensions and open problems.
arxiv-6900-109 | Fast Ridge Regression with Randomized Principal Component Analysis and Gradient Descent | http://arxiv.org/pdf/1405.3952v1.pdf | author:Yichao Lu, Dean P. Foster category:stat.ML published:2014-05-15 summary:We propose a new two stage algorithm LING for large scale regressionproblems. LING has the same risk as the well known Ridge Regression under thefixed design setting and can be computed much faster. Our experiments haveshown that LING performs well in terms of both prediction accuracy andcomputational efficiency compared with other large scale regression algorithmslike Gradient Descent, Stochastic Gradient Descent and Principal ComponentRegression on both simulated and real datasets.
arxiv-6900-110 | Complex Networks Measures for Differentiation between Normal and Shuffled Croatian Texts | http://arxiv.org/pdf/1405.3786v1.pdf | author:Domagoj Margan, Ana Meštrović, Sanda Martinčić-Ipšić category:cs.CL physics.soc-ph published:2014-05-15 summary:This paper studies the properties of the Croatian texts via complex networks.We present network properties of normal and shuffled Croatian texts fordifferent shuffling principles: on the sentence level and on the text level. Inboth experiments we preserved the vocabulary size, word and sentence frequencydistributions. Additionally, in the first shuffling approach we preserved thesentence structure of the text and the number of words per sentence. Obtainedresults showed that degree rank distributions exhibit no substantial deviationin shuffled networks, and strength rank distributions are preserved due to thesame word frequencies. Therefore, standard approach to study the structure oflinguistic co-occurrence networks showed no clear difference among thetopologies of normal and shuffled texts. Finally, we showed that the in- andout- selectivity values from shuffled texts are constantly below selectivityvalues calculated from normal texts. Our results corroborate that the nodeselectivity measure can capture structural differences between original andshuffled Croatian texts.
arxiv-6900-111 | Iterative Non-Local Shrinkage Algorithm for MR Image Reconstruction | http://arxiv.org/pdf/1405.5494v1.pdf | author:Yasir Q. Moshin, Greg Ongie, Mathews Jacob category:cs.CV published:2014-05-15 summary:We introduce a fast iterative non-local shrinkage algorithm to recover MRIdata from undersampled Fourier measurements. This approach is enabled by thereformulation of current non-local schemes as an alternating algorithm tominimize a global criterion. The proposed algorithm alternates between anon-local shrinkage step and a quadratic subproblem. We derive analyticalshrinkage rules for several penalties that are relevant in non-localregularization. The redundancy in the searches used to evaluate the shrinkagesteps are exploited using filtering operations. The resulting algorithm isobserved to be considerably faster than current alternating non-localalgorithms. The comparisons of the proposed scheme with state-of-the-artregularization schemes show a considerable reduction in alias artifacts andpreservation of edges.
arxiv-6900-112 | Effective Bayesian Modeling of Groups of Related Count Time Series | http://arxiv.org/pdf/1405.3738v1.pdf | author:Nicolas Chapados category:stat.ML stat.AP published:2014-05-15 summary:Time series of counts arise in a variety of forecasting applications, forwhich traditional models are generally inappropriate. This paper introduces ahierarchical Bayesian formulation applicable to count time series that caneasily account for explanatory variables and share statistical strength acrossgroups of related time series. We derive an efficient approximate inferencetechnique, and illustrate its performance on a number of datasets from supplychain planning.
arxiv-6900-113 | Topic words analysis based on LDA model | http://arxiv.org/pdf/1405.3726v1.pdf | author:Xi Qiu, Christopher Stewart category:cs.SI cs.DC cs.IR cs.LG stat.ML published:2014-05-15 summary:Social network analysis (SNA), which is a research field describing andmodeling the social connection of a certain group of people, is popular amongnetwork services. Our topic words analysis project is a SNA method to visualizethe topic words among emails from Obama.com to accounts registered in Columbus,Ohio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic modelof SNA, our project characterizes the preference of senders for target group ofreceptors. Gibbs sampling is used to estimate topic and word distribution. Ourtraining and testing data are emails from the carbon-free serverDatagreening.com. We use parallel computing tool BashReduce for word processingand generate related words under each latent topic to discovers typicalinformation of political news sending specially to local Columbus receptors.Running on two instances using paralleling tool BashReduce, our projectcontributes almost 30% speedup processing the raw contents, comparing withprocessing contents on one instance locally. Also, the experimental resultshows that the LDA model applied in our project provides precision rate 53.96%higher than TF-IDF model finding target words, on the condition thatappropriate size of topic words list is selected.
arxiv-6900-114 | Méthodes pour la représentation informatisée de données lexicales / Methoden der Speicherung lexikalischer Daten | http://arxiv.org/pdf/1405.3925v1.pdf | author:Laurent Romary, Andreas Witt category:cs.CL published:2014-05-15 summary:In recent years, new developments in the area of lexicography have alterednot only the management, processing and publishing of lexicographical data, butalso created new types of products such as electronic dictionaries andthesauri. These expand the range of possible uses of lexical data and supportusers with more flexibility, for instance in assisting human translation. Inthis article, we give a short and easy-to-understand introduction to theproblematic nature of the storage, display and interpretation of lexical data.We then describe the main methods and specifications used to build andrepresent lexical data. This paper is targeted for the following groups ofpeople: linguists, lexicographers, IT specialists, computer linguists and allothers who wish to learn more about the modelling, representation andvisualization of lexical knowledge. This paper is written in two languages:French and German.
arxiv-6900-115 | Logistic Regression: Tight Bounds for Stochastic and Online Optimization | http://arxiv.org/pdf/1405.3843v1.pdf | author:Elad Hazan, Tomer Koren, Kfir Y. Levy category:cs.LG published:2014-05-15 summary:The logistic loss function is often advocated in machine learning andstatistics as a smooth and strictly convex surrogate for the 0-1 loss. In thispaper we investigate the question of whether these smoothness and convexityproperties make the logistic loss preferable to other widely considered optionssuch as the hinge loss. We show that in contrast to known asymptotic bounds, aslong as the number of prediction/optimization iterations is sub exponential,the logistic loss provides no improvement over a generic non-smooth lossfunction such as the hinge loss. In particular we show that the convergencerate of stochastic logistic optimization is bounded from below by a polynomialin the diameter of the decision set and the number of prediction iterations,and provide a matching tight upper bound. This resolves the COLT open problemof McMahan and Streeter (2012).
arxiv-6900-116 | Speeding up Convolutional Neural Networks with Low Rank Expansions | http://arxiv.org/pdf/1405.3866v1.pdf | author:Max Jaderberg, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2014-05-15 summary:The focus of this paper is speeding up the evaluation of convolutional neuralnetworks. While delivering impressive results across a range of computer visionand machine learning tasks, these networks are computationally demanding,limiting their deployability. Convolutional layers generally consume the bulkof the processing time, and so in this work we present two simple schemes fordrastically speeding up these layers. This is achieved by exploitingcross-channel or filter redundancy to construct a low rank basis of filtersthat are rank-1 in the spatial domain. Our methods are architecture agnostic,and can be easily applied to existing CPU and GPU convolutional frameworks fortuneable speedup performance. We demonstrate this with a real world networkdesigned for scene text character recognition, showing a possible 2.5x speedupwith no loss in accuracy, and 4.5x speedup with less than 1% drop in accuracy,still achieving state-of-the-art on standard benchmarks.
arxiv-6900-117 | Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme for Sentiment Analysis and Text Classification | http://arxiv.org/pdf/1405.3518v2.pdf | author:Yoon Kim, Owen Zhang category:cs.CL cs.IR published:2014-05-14 summary:We provide a simple but novel supervised weighting scheme for adjusting termfrequency in tf-idf for sentiment analysis and text classification. We compareour method to baseline weighting schemes and find that it outperforms them onmultiple benchmarks. The method is robust and works well on both snippets andlonger documents.
arxiv-6900-118 | Group-based Sparse Representation for Image Restoration | http://arxiv.org/pdf/1405.3351v1.pdf | author:Jian Zhang, Debin Zhao, Wen Gao category:cs.CV published:2014-05-14 summary:Traditional patch-based sparse representation modeling of natural imagesusually suffer from two problems. First, it has to solve a large-scaleoptimization problem with high computational complexity in dictionary learning.Second, each patch is considered independently in dictionary learning andsparse coding, which ignores the relationship among patches, resulting ininaccurate sparse coding coefficients. In this paper, instead of using patch asthe basic unit of sparse representation, we exploit the concept of group as thebasic unit of sparse representation, which is composed of nonlocal patches withsimilar structures, and establish a novel sparse representation modeling ofnatural images, called group-based sparse representation (GSR). The proposedGSR is able to sparsely represent natural images in the domain of group, whichenforces the intrinsic local sparsity and nonlocal self-similarity of imagessimultaneously in a unified framework. Moreover, an effective self-adaptivedictionary learning method for each group with low complexity is designed,rather than dictionary learning from natural images. To make GSR tractable androbust, a split Bregman based technique is developed to solve the proposedGSR-driven minimization problem for image restoration efficiently. Extensiveexperiments on image inpainting, image deblurring and image compressive sensingrecovery manifest that the proposed GSR modeling outperforms many currentstate-of-the-art schemes in both PSNR and visual perception.
arxiv-6900-119 | Learning rates for the risk of kernel based quantile regression estimators in additive models | http://arxiv.org/pdf/1405.3379v1.pdf | author:Andreas Christmann, Ding-Xuan Zhou category:stat.ML published:2014-05-14 summary:Additive models play an important role in semiparametric statistics. Thispaper gives learning rates for regularized kernel based methods for additivemodels. These learning rates compare favourably in particular in highdimensions to recent results on optimal learning rates for purely nonparametricregularized kernel based quantile regression using the Gaussian radial basisfunction kernel, provided the assumption of an additive model is valid.Additionally, a concrete example is presented to show that a Gaussian functiondepending only on one variable lies in a reproducing kernel Hilbert spacegenerated by an additive Gaussian kernel, but does not belong to thereproducing kernel Hilbert space generated by the multivariate Gaussian kernelof the same variance.
arxiv-6900-120 | Active Mining of Parallel Video Streams | http://arxiv.org/pdf/1405.3382v1.pdf | author:Samaneh Khoshrou, Jaime S. Cardoso, Luis F. Teixeira category:cs.CV cs.LG published:2014-05-14 summary:The practicality of a video surveillance system is adversely limited by theamount of queries that can be placed on human resources and their vigilance inresponse. To transcend this limitation, a major effort under way is to includesoftware that (fully or at least semi) automatically mines video footage,reducing the burden imposed to the system. Herein, we propose a semi-supervisedincremental learning framework for evolving visual streams in order to developa robust and flexible track classification system. Our proposed method learnsfrom consecutive batches by updating an ensemble in each time. It tries tostrike a balance between performance of the system and amount of data whichneeds to be labelled. As no restriction is considered, the system can addressmany practical problems in an evolving multi-camera scenario, such as conceptdrift, class evolution and various length of video streams which have not beenaddressed before. Experiments were performed on synthetic as well as real-worldvisual data in non-stationary environments, showing high accuracy with fairlylittle human collaboration.
arxiv-6900-121 | Credal Model Averaging for classification: representing prior ignorance and expert opinions | http://arxiv.org/pdf/1405.3559v1.pdf | author:Giorgio Corani, Andrea Mignatti category:stat.ME cs.AI q-bio.PE stat.ML published:2014-05-14 summary:Bayesian model averaging (BMA) is the state of the art approach forovercoming model uncertainty. Yet, especially on small data sets, the resultsyielded by BMA might be sensitive to the prior over the models. Credal ModelAveraging (CMA) addresses this problem by substituting the single prior overthe models by a set of priors (credal set). Such approach solves the problem ofhow to choose the prior over the models and automates sensitivity analysis. Wediscuss various CMA algorithms for building an ensemble of logistic regressorscharacterized by different sets of covariates. We show how CMA can beappropriately tuned to the case in which one is prior-ignorant and to the casein which instead domain knowledge is available. CMA detects prior-dependentinstances, namely instances in which a different class is more probabledepending on the prior over the models. On such instances CMA suspends thejudgment, returning multiple classes. We thoroughly compare different BMA andCMA variants on a real case study, predicting presence of Alpine marmot burrowsin an Alpine valley. We find that BMA is almost a random guesser on theinstances recognized as prior-dependent by CMA.
arxiv-6900-122 | Reducing Dueling Bandits to Cardinal Bandits | http://arxiv.org/pdf/1405.3396v1.pdf | author:Nir Ailon, Thorsten Joachims, Zohar Karnin category:cs.LG published:2014-05-14 summary:We present algorithms for reducing the Dueling Bandits problem to theconventional (stochastic) Multi-Armed Bandits problem. The Dueling Banditsproblem is an online model of learning with ordinal feedback of the form "A ispreferred to B" (as opposed to cardinal feedback like "A has value 2.5"),giving it wide applicability in learning from implicit user feedback andrevealed and stated preferences. In contrast to existing algorithms for theDueling Bandits problem, our reductions -- named $\Doubler$, $\MultiSbm$ and$\DoubleSbm$ -- provide a generic schema for translating the extensive body ofknown results about conventional Multi-Armed Bandit algorithms to the DuelingBandits setting. For $\Doubler$ and $\MultiSbm$ we prove regret upper bounds inboth finite and infinite settings, and conjecture about the performance of$\DoubleSbm$ which empirically outperforms the other two as well as previousalgorithms in our experiments. In addition, we provide the first almost optimalregret bound in terms of second order terms, such as the differences betweenthe values of the arms.
arxiv-6900-123 | Pattern Recognition in Narrative: Tracking Emotional Expression in Context | http://arxiv.org/pdf/1405.3539v3.pdf | author:Fionn Murtagh, Adam Ganz category:cs.AI cs.CL published:2014-05-14 summary:Using geometric data analysis, our objective is the analysis of narrative,with narrative of emotion being the focus in this work. The following twoprinciples for analysis of emotion inform our work. Firstly, emotion isrevealed not as a quality in its own right but rather through interaction. Westudy the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the3-way relationship of Emma, Charles and Rodolphe in the novel {\em MadameBovary}. Secondly, emotion, that is expression of states of mind of subjects,is formed and evolves within the narrative that expresses external events and(personal, social, physical) context. In addition to the analysis methodologywith key aspects that are innovative, the input data used is crucial. We use,firstly, dialogue, and secondly, broad and general description thatincorporates dialogue. In a follow-on study, we apply our unsupervisednarrative mapping to data streams with very low emotional expression. We mapthe narrative of Twitter streams. Thus we demonstrate map analysis of generalnarratives.
arxiv-6900-124 | Newton-Type Iterative Solver for Multiple View $L2$ Triangulation | http://arxiv.org/pdf/1405.3352v2.pdf | author:F. Lu, Z. Chen category:cs.CV cs.GR published:2014-05-14 summary:In this note, we show that the L2 optimal solutions to most real multipleview L2 triangulation problems can be efficiently obtained by two-stageNewton-like iterative methods, while the difficulty of such problems mainlylies in how to verify the L2 optimality. Such a working two-stage bundleadjustment approach features: first, the algorithm is initialized by symmedianpoint triangulation, a multiple-view generalization of the mid-point method;second, a symbolic-numeric method is employed to compute derivativesaccurately; third, globalizing strategy such as line search or trust region issmoothly applied to the underlying iteration which assures algorithm robustnessin general cases. Numerical comparison with tfml method shows that the local minimizersobtained by the two-stage iterative bundle adjustment approach proposed hereare also the L2 optimal solutions to all the calibrated data sets availableonline by the Oxford visual geometry group. Extensive numerical experimentsindicate the bundle adjustment approach solves more than 99% the realtriangulation problems optimally. An IEEE 754 double precision C++implementation shows that it takes only about 0.205 second tocompute allthe4983 points in the Oxford dinosaur data setvia Gauss-Newton iteration hybridwith a line search strategy on a computer with a 3.4GHz Intel i7 CPU.
arxiv-6900-125 | Global disease monitoring and forecasting with Wikipedia | http://arxiv.org/pdf/1405.3612v2.pdf | author:Nicholas Generous, Geoffrey Fairchild, Alina Deshpande, Sara Y. Del Valle, Reid Priedhorsky category:cs.SI cs.LG physics.soc-ph published:2014-05-14 summary:Infectious disease is a leading threat to public health, economic stability,and other key social structures. Efforts to mitigate these impacts depend onaccurate and timely monitoring to measure the risk and progress of disease.Traditional, biologically-focused monitoring techniques are accurate but costlyand slow; in response, new techniques based on social internet data such associal media and search queries are emerging. These efforts are promising, butimportant challenges in the areas of scientific peer review, breadth ofdiseases and countries, and forecasting hamper their operational usefulness. We examine a freely available, open data source for this use: access logsfrom the online encyclopedia Wikipedia. Using linear models, language as aproxy for location, and a systematic yet simple article selection procedure, wetested 14 location-disease combinations and demonstrate that these datafeasibly support an approach that overcomes these challenges. Specifically, ourproof-of-concept yields models with $r^2$ up to 0.92, forecasting value up tothe 28 days tested, and several pairs of models similar enough to suggest thattransferring models from one location to another without re-training isfeasible. Based on these preliminary results, we close with a research agenda designedto overcome these challenges and produce a disease monitoring and forecastingsystem that is significantly more effective, robust, and globally comprehensivethan the current state of the art.
arxiv-6900-126 | Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques | http://arxiv.org/pdf/1405.3536v1.pdf | author:Olivier Nicol, Jérémie Mary, Philippe Preux category:stat.ML cs.LG published:2014-05-14 summary:In many recommendation applications such as news recommendation, the itemsthat can be rec- ommended come and go at a very fast pace. This is a challengefor recommender systems (RS) to face this setting. Online learning algorithmsseem to be the most straight forward solution. The contextual bandit frameworkwas introduced for that very purpose. In general the evaluation of a RS is acritical issue. Live evaluation is of- ten avoided due to the potential loss ofrevenue, hence the need for offline evaluation methods. Two options areavailable. Model based meth- ods are biased by nature and are thus difficult totrust when used alone. Data driven methods are therefore what we consider here.Evaluat- ing online learning algorithms with past data is not simple but somemethods exist in the litera- ture. Nonetheless their accuracy is not satisfac-tory mainly due to their mechanism of data re- jection that only allow theexploitation of a small fraction of the data. We precisely address this issuein this paper. After highlighting the limita- tions of the previous methods, wepresent a new method, based on bootstrapping techniques. This new method comeswith two important improve- ments: it is much more accurate and it provides ameasure of quality of its estimation. The latter is a highly desirable propertyin order to minimize the risks entailed by putting online a RS for the firsttime. We provide both theoretical and ex- perimental proofs of its superioritycompared to state-of-the-art methods, as well as an analysis of the convergenceof the measure of quality.
arxiv-6900-127 | Return of the Devil in the Details: Delving Deep into Convolutional Nets | http://arxiv.org/pdf/1405.3531v4.pdf | author:Ken Chatfield, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2014-05-14 summary:The latest generation of Convolutional Neural Networks (CNN) have achievedimpressive results in challenging benchmarks on image recognition and objectdetection, significantly raising the interest of the community in thesemethods. Nevertheless, it is still unclear how different CNN methods comparewith each other and with previous state-of-the-art shallow representations suchas the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conductsa rigorous evaluation of these new techniques, exploring different deeparchitectures and comparing them on a common ground, identifying and disclosingimportant implementation details. We identify several useful properties ofCNN-based representations, including the fact that the dimensionality of theCNN output layer can be reduced significantly without having an adverse effecton performance. We also identify aspects of deep and shallow methods that canbe successfully shared. In particular, we show that the data augmentationtechniques commonly applied to CNN-based methods can also be applied to shallowmethods, and result in an analogous performance boost. Source code and modelsto reproduce the experiments in the paper is made publicly available.
arxiv-6900-128 | Temporal Analysis of Language through Neural Language Models | http://arxiv.org/pdf/1405.3515v1.pdf | author:Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, Slav Petrov category:cs.CL published:2014-05-14 summary:We provide a method for automatically detecting change in language acrosstime through a chronologically trained neural language model. We train themodel on the Google Books Ngram corpus to obtain word vector representationsspecific to each year, and identify words that have changed significantly from1900 to 2009. The model identifies words such as "cell" and "gay" as havingchanged during that time period. The model simultaneously identifies thespecific years during which such words underwent change.
arxiv-6900-129 | Efficient classification using parallel and scalable compressed model and Its application on intrusion detection | http://arxiv.org/pdf/1405.3410v1.pdf | author:Tieming Chen, Xu Zhang, Shichao Jin, Okhee Kim category:cs.LG cs.CR published:2014-05-14 summary:In order to achieve high efficiency of classification in intrusion detection,a compressed model is proposed in this paper which combines horizontalcompression with vertical compression. OneR is utilized as horizontalcom-pression for attribute reduction, and affinity propagation is employed asvertical compression to select small representative exemplars from largetraining data. As to be able to computationally compress the larger volume oftraining data with scalability, MapReduce based parallelization approach isthen implemented and evaluated for each step of the model compression processabovementioned, on which common but efficient classification methods can bedirectly used. Experimental application study on two publicly availabledatasets of intrusion detection, KDD99 and CMDC2012, demonstrates that theclassification using the compressed model proposed can effectively speed up thedetection procedure at up to 184 times, most importantly at the cost of aminimal accuracy difference with less than 1% on average.
arxiv-6900-130 | Accelerating Minibatch Stochastic Gradient Descent using Stratified Sampling | http://arxiv.org/pdf/1405.3080v1.pdf | author:Peilin Zhao, Tong Zhang category:stat.ML cs.LG math.OC published:2014-05-13 summary:Stochastic Gradient Descent (SGD) is a popular optimization method which hasbeen applied to many important machine learning tasks such as Support VectorMachines and Deep Neural Networks. In order to parallelize SGD, minibatchtraining is often employed. The standard approach is to uniformly sample aminibatch at each step, which often leads to high variance. In this paper wepropose a stratified sampling strategy, which divides the whole dataset intoclusters with low within-cluster variance; we then take examples from theseclusters using a stratified sampling technique. It is shown that theconvergence rate can be significantly improved by the algorithm. Encouragingexperimental results confirm the effectiveness of the proposed method.
arxiv-6900-131 | Circulant Binary Embedding | http://arxiv.org/pdf/1405.3162v1.pdf | author:Felix X. Yu, Sanjiv Kumar, Yunchao Gong, Shih-Fu Chang category:stat.ML cs.LG published:2014-05-13 summary:Binary embedding of high-dimensional data requires long codes to preserve thediscriminative power of the input space. Traditional binary coding methodsoften suffer from very high computation and storage costs in such a scenario.To address this problem, we propose Circulant Binary Embedding (CBE) whichgenerates binary codes by projecting the data with a circulant matrix. Thecirculant structure enables the use of Fast Fourier Transformation to speed upthe computation. Compared to methods that use unstructured matrices, theproposed method improves the time complexity from $\mathcal{O}(d^2)$ to$\mathcal{O}(d\log{d})$, and the space complexity from $\mathcal{O}(d^2)$ to$\mathcal{O}(d)$ where $d$ is the input dimensionality. We also propose a noveltime-frequency alternating optimization to learn data-dependent circulantprojections, which alternatively minimizes the objective in original andFourier domains. We show by extensive experiments that the proposed approachgives much better performance than the state-of-the-art approaches for fixedtime, and provides much faster computation with no performance degradation forfixed number of bits.
arxiv-6900-132 | Phonetic based SoundEx & ShapeEx algorithm for Sindhi Spell Checker System | http://arxiv.org/pdf/1405.3033v1.pdf | author:Zeeshan Bhatti, Ahmad Waqas, Imdad Ali Ismaili, Dil Nawaz Hakro, Waseem Javaid Soomro category:cs.CL published:2014-05-13 summary:This paper presents a novel combinational phonetic algorithm for SindhiLanguage, to be used in developing Sindhi Spell Checker which has yet not beendeveloped prior to this work. The compound textual forms and glyphs of Sindhilanguage presents a substantial challenge for developing Sindhi spell checkersystem and generating similar suggestion list for misspelled words. In order toimplement such a system, phonetic based Sindhi language rules and patterns mustbe considered into account for increasing the accuracy and efficiency. Theproposed system is developed with a blend between Phonetic based SoundExalgorithm and ShapeEx algorithm for pattern or glyph matching, generatingaccurate and efficient suggestion list for incorrect or misspelled Sindhiwords. A table of phonetically similar sounding Sindhi characters for SoundExalgorithm is also generated along with another table containing similar glyphor shape based character groups for ShapeEx algorithm. Both these are firstever attempt of any such type of categorization and representation for SindhiLanguage.
arxiv-6900-133 | Efficient Implementations of the Generalized Lasso Dual Path Algorithm | http://arxiv.org/pdf/1405.3222v2.pdf | author:Taylor Arnold, Ryan Tibshirani category:stat.CO cs.LG stat.ML published:2014-05-13 summary:We consider efficient implementations of the generalized lasso dual pathalgorithm of Tibshirani and Taylor (2011). We first describe a generic approachthat covers any penalty matrix D and any (full column rank) matrix X ofpredictor variables. We then describe fast implementations for the specialcases of trend filtering problems, fused lasso problems, and sparse fused lassoproblems, both with X=I and a general matrix X. These specializedimplementations offer a considerable improvement over the genericimplementation, both in terms of numerical stability and efficiency of thesolution path computation. These algorithms are all available for use in thegenlasso R package, which can be found in the CRAN repository.
arxiv-6900-134 | On the Complexity of A/B Testing | http://arxiv.org/pdf/1405.3224v2.pdf | author:Emilie Kaufmann, Olivier Cappé, Aurélien Garivier category:math.ST cs.LG stat.ML stat.TH published:2014-05-13 summary:A/B testing refers to the task of determining the best option among twoalternatives that yield random outcomes. We provide distribution-dependentlower bounds for the performance of A/B testing that improve over the resultscurrently available both in the fixed-confidence (or delta-PAC) andfixed-budget settings. When the distribution of the outcomes are Gaussian, weprove that the complexity of the fixed-confidence and fixed-budget settings areequivalent, and that uniform sampling of both alternatives is optimal only inthe case of equal variances. In the common variance case, we also provide astopping rule that terminates faster than existing fixed-confidence algorithms.In the case of Bernoulli distributions, we show that the complexity offixed-budget setting is smaller than that of fixed-confidence setting and thatuniform sampling of both alternatives -though not optimal- is advisable inpractice when combined with an appropriate stopping criterion.
arxiv-6900-135 | Clustering, Hamming Embedding, Generalized LSH and the Max Norm | http://arxiv.org/pdf/1405.3167v1.pdf | author:Behnam Neyshabur, Yury Makarychev, Nathan Srebro category:cs.LG published:2014-05-13 summary:We study the convex relaxation of clustering and hamming embedding, focusingon the asymmetric case (co-clustering and asymmetric hamming embedding),understanding their relationship to LSH as studied by (Charikar 2002) and tothe max-norm ball, and the differences between their symmetric and asymmetricversions.
arxiv-6900-136 | Fast and Fuzzy Private Set Intersection | http://arxiv.org/pdf/1405.3272v2.pdf | author:Nicholas Kersting category:cs.CR cs.CL published:2014-05-13 summary:Private Set Intersection (PSI) is usually implemented as a sequence ofencryption rounds between pairs of users, whereas the present work implementsPSI in a simpler fashion: each set only needs to be encrypted once, after whicheach pair of users need only one ordinary set comparison. This is typicallyorders of magnitude faster than ordinary PSI at the cost of some ``fuzziness"in the matching, which may nonetheless be tolerable or even desirable. This isdemonstrated in the case where the sets consist of English words processed withWordNet.
arxiv-6900-137 | Graph Matching: Relax at Your Own Risk | http://arxiv.org/pdf/1405.3133v3.pdf | author:Vince Lyzinski, Donniell Fishkind, Marcelo Fiori, Joshua T. Vogelstein, Carey E. Priebe, Guillermo Sapiro category:stat.ML math.OC published:2014-05-13 summary:Graph matching---aligning a pair of graphs to minimize their edgedisagreements---has received wide-spread attention from both theoretical andapplied communities over the past several decades, including combinatorics,computer vision, and connectomics. Its attention can be partially attributed toits computational difficulty. Although many heuristics have previously beenproposed in the literature to approximately solve graph matching, very few haveany theoretical support for their performance. A common technique is to relaxthe discrete problem to a continuous problem, therefore enabling practitionersto bring gradient-descent-type algorithms to bear. We prove that an indefiniterelaxation (when solved exactly) almost always discovers the optimalpermutation, while a common convex relaxation almost always fails to discoverthe optimal permutation. These theoretical results suggest that initializingthe indefinite algorithm with the convex optimum might yield improved practicalperformance. Indeed, experimental results illuminate and corroborate thesetheoretical findings, demonstrating that excellent results are achieved in bothbenchmark and real data problems by amalgamating the two approaches.
arxiv-6900-138 | An Intelligent Pixel Replication Technique by Binary Decomposition for Digital Image Zooming | http://arxiv.org/pdf/1405.3195v1.pdf | author:Kaeser M Sabrin, M Haider Ali category:cs.CV published:2014-05-13 summary:Image zooming is the process of enlarging the spatial resolution of a givendigital image. We present a novel technique that intelligently modifies theclassical pixel replication method for zooming. Our method decomposes a givenimage into layer of binary images, interpolates them by magnifying the binarypatterns preserving their geometric shape and finally aggregates them all toobtain the zoomed image. Although the quality of our zoomed images is muchhigher than that of nearest neighbor and bilinear interpolation and comparablewith bicubic interpolation, the running time of our technique is extremely fastlike nearest neighbor interpolation and much faster than bilinear and bicubicinterpolation.
arxiv-6900-139 | Rate of Convergence and Error Bounds for LSTD($λ$) | http://arxiv.org/pdf/1405.3229v1.pdf | author:Manel Tagorti, Bruno Scherrer category:cs.LG cs.AI math.OC math.ST stat.TH published:2014-05-13 summary:We consider LSTD($\lambda$), the least-squares temporal-difference algorithmwith eligibility traces algorithm proposed by Boyan (2002). It computes alinear approximation of the value function of a fixed policy in a large MarkovDecision Process. Under a $\beta$-mixing assumption, we derive, for any valueof $\lambda \in (0,1)$, a high-probability estimate of the rate of convergenceof this algorithm to its limit. We deduce a high-probability bound on the errorof this algorithm, that extends (and slightly improves) that derived by Lazaricet al. (2012) in the specific case where $\lambda=0$. In particular, ouranalysis sheds some light on the choice of $\lambda$ with respect to thequality of the chosen linear space and the number of samples, that complieswith simulations.
arxiv-6900-140 | Scalable sparse covariance estimation via self-concordance | http://arxiv.org/pdf/1405.3263v1.pdf | author:Anastasios Kyrillidis, Rabeeh Karimi Mahabadi, Quoc Tran-Dinh, Volkan Cevher category:stat.ML cs.IT math.IT math.OC published:2014-05-13 summary:We consider the class of convex minimization problems, composed of aself-concordant function, such as the $\log\det$ metric, a convex data fidelityterm $h(\cdot)$ and, a regularizing -- possibly non-smooth -- function$g(\cdot)$. This type of problems have recently attracted a great deal ofinterest, mainly due to their omnipresence in top-notch applications. Underthis \emph{locally} Lipschitz continuous gradient setting, we analyze theconvergence behavior of proximal Newton schemes with the added twist of aprobable presence of inexact evaluations. We prove attractive convergence rateguarantees and enhance state-of-the-art optimization schemes to accommodatesuch developments. Experimental results on sparse covariance estimation showthe merits of our algorithm, both in terms of recovery efficiency andcomplexity.
arxiv-6900-141 | How to Ask for a Favor: A Case Study on the Success of Altruistic Requests | http://arxiv.org/pdf/1405.3282v1.pdf | author:Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky category:cs.CL cs.SI physics.soc-ph I.2.7; J.4 published:2014-05-13 summary:Requests are at the core of many social media systems such as question &answer sites and online philanthropy communities. While the success of suchrequests is critical to the success of the community, the factors that leadcommunity members to satisfy a request are largely unknown. Success of arequest depends on factors like who is asking, how they are asking, when arethey asking, and most critically what is being requested, ranging from smallfavors to substantial monetary donations. We present a case study of altruisticrequests in an online community where all requests ask for the very samecontribution and do not offer anything tangible in return, allowing us todisentangle what is requested from textual and social factors. Drawing fromsocial psychology literature, we extract high-level social features from textthat operationalize social relations between recipient and donor anddemonstrate that these extracted relations are predictive of success. Morespecifically, we find that clearly communicating need through the narrative isessential and that that linguistic indications of gratitude, evidentiality, andgeneralized reciprocity, as well as high status of the asker further increasethe likelihood of success. Building on this understanding, we develop a modelthat can predict the success of unseen requests, significantly improving overseveral baselines. We link these findings to research in psychology on helpingbehavior, providing a basis for further analysis of success in social mediasystems.
arxiv-6900-142 | Learning with many experts: model selection and sparsity | http://arxiv.org/pdf/1405.3292v1.pdf | author:Rafael Izbicki, Rafael Bassi Stern category:stat.ME cs.LG published:2014-05-13 summary:Experts classifying data are often imprecise. Recently, several models havebeen proposed to train classifiers using the noisy labels generated by theseexperts. How to choose between these models? In such situations, the truelabels are unavailable. Thus, one cannot perform model selection using thestandard versions of methods such as empirical risk minimization and crossvalidation. In order to allow model selection, we present a surrogate loss andprovide theoretical guarantees that assure its consistency. Next, we discusshow this loss can be used to tune a penalization which introduces sparsity inthe parameters of a traditional class of models. Sparsity provides moreparsimonious models and can avoid overfitting. Nevertheless, it has seldom beendiscussed in the context of noisy labels due to the difficulty in modelselection and, therefore, in choosing tuning parameters. We apply thesetechniques to several sets of simulated and real data.
arxiv-6900-143 | Effects of Sampling Methods on Prediction Quality. The Case of Classifying Land Cover Using Decision Trees | http://arxiv.org/pdf/1405.3295v1.pdf | author:Ronald Hochreiter, Christoph Waldhauser category:stat.ML cs.LG stat.AP published:2014-05-13 summary:Clever sampling methods can be used to improve the handling of big data andincrease its usefulness. The subject of this study is remote sensing,specifically airborne laser scanning point clouds representing differentclasses of ground cover. The aim is to derive a supervised learning model forthe classification using CARTs. In order to measure the effect of differentsampling methods on the classification accuracy, various experiments withvarying types of sampling methods, sample sizes, and accuracy metrics have beendesigned. Numerical results for a subset of a large surveying project coveringthe lower Rhine area in Germany are shown. General conclusions regardingsampling design are drawn and presented.
arxiv-6900-144 | Optimal Exploration-Exploitation in a Multi-Armed-Bandit Problem with Non-stationary Rewards | http://arxiv.org/pdf/1405.3316v1.pdf | author:Omar Besbes, Yonatan Gur, Assaf Zeevi category:cs.LG math.OC stat.ML published:2014-05-13 summary:In a multi-armed bandit (MAB) problem a gambler needs to choose at each roundof play one of K arms, each characterized by an unknown reward distribution.Reward realizations are only observed when an arm is selected, and thegambler's objective is to maximize his cumulative expected earnings over somegiven horizon of play T. To do this, the gambler needs to acquire informationabout arms (exploration) while simultaneously optimizing immediate rewards(exploitation); the price paid due to this trade off is often referred to asthe regret, and the main question is how small can this price be as a functionof the horizon length T. This problem has been studied extensively when thereward distributions do not change over time; an assumption that supports asharp characterization of the regret, yet is often violated in practicalsettings. In this paper, we focus on a MAB formulation which allows for a broadrange of temporal uncertainties in the rewards, while still maintainingmathematical tractability. We fully characterize the (regret) complexity ofthis class of MAB problems by establishing a direct link between the extent ofallowable reward "variation" and the minimal achievable regret. Our analysisdraws some connections between two rather disparate strands of literature: theadversarial and the stochastic MAB frameworks.
arxiv-6900-145 | Locally Boosted Graph Aggregation for Community Detection | http://arxiv.org/pdf/1405.3210v1.pdf | author:Jeremy Kun, Rajmonda Caceres, Kevin Carter category:cs.LG cs.SI physics.soc-ph published:2014-05-13 summary:Learning the right graph representation from noisy, multi-source data hasgarnered significant interest in recent years. A central tenet of this problemis relational learning. Here the objective is to incorporate the partialinformation each data source gives us in a way that captures the trueunderlying relationships. To address this challenge, we present a general,boosting-inspired framework for combining weak evidence of entity associationsinto a robust similarity metric. Building on previous work, we explore theextent to which different local quality measurements yield graphrepresentations that are suitable for community detection. We present empiricalresults on a variety of datasets demonstrating the utility of this framework,especially with respect to real datasets where noise and scale present seriouschallenges. Finally, we prove a convergence theorem in an ideal setting andoutline future research into other application domains.
arxiv-6900-146 | Adaptive Monte Carlo via Bandit Allocation | http://arxiv.org/pdf/1405.3318v1.pdf | author:James Neufeld, András György, Dale Schuurmans, Csaba Szepesvári category:cs.AI cs.LG published:2014-05-13 summary:We consider the problem of sequentially choosing between a set of unbiasedMonte Carlo estimators to minimize the mean-squared-error (MSE) of a finalcombined estimate. By reducing this task to a stochastic multi-armed banditproblem, we show that well developed allocation strategies can be used toachieve an MSE that approaches that of the best estimator chosen in retrospect.We then extend these developments to a scenario where alternative estimatorshave different, possibly stochastic costs. The outcome is a new set of adaptiveMonte Carlo strategies that provide stronger guarantees than previousapproaches while offering practical advantages.
arxiv-6900-147 | G-AMA: Sparse Gaussian graphical model estimation via alternating minimization | http://arxiv.org/pdf/1405.3034v2.pdf | author:Onkar Dalal, Bala Rajaratnam category:stat.CO stat.ML published:2014-05-13 summary:Several methods have been recently proposed for estimating sparse Gaussiangraphical models using $\ell_{1}$ regularization on the inverse covariancematrix. Despite recent advances, contemporary applications require methods thatare even faster in order to handle ill-conditioned high dimensional modern daydatasets. In this paper, we propose a new method, G-AMA, to solve the sparseinverse covariance estimation problem using Alternating Minimization Algorithm(AMA), that effectively works as a proximal gradient algorithm on the dualproblem. Our approach has several novel advantages over existing methods.First, we demonstrate that G-AMA is faster than the previous best algorithms bymany orders of magnitude and is thus an ideal approach for modern highthroughput applications. Second, global linear convergence of G-AMA isdemonstrated rigorously, underscoring its good theoretical properties. Third,the dual algorithm operates on the covariance matrix, and thus easilyfacilitates incorporating additional constraints on pairwise/marginalrelationships between feature pairs based on domain specific knowledge. Overand above estimating a sparse inverse covariance matrix, we also illustrate howto (1) incorporate constraints on the (bivariate) correlations and, (2)incorporate equality (equisparsity) or linear constraints between individualinverse covariance elements. Fourth, we also show that G-AMA is better adept athandling extremely ill-conditioned problems, as is often the case with realdata. The methodology is demonstrated on both simulated and real datasets toillustrate its superior performance over recently proposed methods.
arxiv-6900-148 | Multi Modal Face Recognition Using Block Based Curvelet Features | http://arxiv.org/pdf/1405.2641v2.pdf | author:Jyothi K, Prabhakar C. J category:cs.CV published:2014-05-12 summary:In this paper, we present multimodal 2D +3D face recognition method usingblock based curvelet features. The 3D surface of face (Depth Map) is computedfrom the stereo face images using stereo vision technique. The statisticalmeasures such as mean, standard deviation, variance and entropy are extractedfrom each block of curvelet subband for both depth and intensity imagesindependently.In order to compute the decision score, the KNN classifier isemployed independently for both intensity and depth map. Further, computeddecision scoresof intensity and depth map are combined at decision level toimprove the face recognition rate. The combination of intensity and depth mapis verified experimentally using benchmark face database. The experimentalresults show that the proposed multimodal method is better than individualmodality.
arxiv-6900-149 | Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems | http://arxiv.org/pdf/1405.2875v2.pdf | author:Chien-Ju Ho, Aleksandrs Slivkins, Jennifer Wortman Vaughan category:cs.DS cs.GT cs.LG published:2014-05-12 summary:Crowdsourcing markets have emerged as a popular platform for matchingavailable workers with tasks to complete. The payment for a particular task istypically set by the task's requester, and may be adjusted based on the qualityof the completed work, for example, through the use of "bonus" payments. Inthis paper, we study the requester's problem of dynamically adjustingquality-contingent payments for tasks. We consider a multi-round version of thewell-known principal-agent model, whereby in each round a worker makes astrategic choice of the effort level which is not directly observable by therequester. In particular, our formulation significantly generalizes thebudget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each "arm"representing a potential contract. To cope with the large (and in fact,infinite) number of arms, we propose a new algorithm, AgnosticZooming, whichdiscretizes the contract space into a finite number of regions, effectivelytreating each region as a single arm. This discretization is adaptivelyrefined, so that more promising regions of the contract space are eventuallydiscretized more finely. We analyze this algorithm, showing that it achievesregret sublinear in the time horizon and substantially improves overnon-adaptive discretization (which is the only competing approach in theliterature). Our results advance the state of art on several different topics: the theoryof crowdsourcing markets, principal-agent problems, multi-armed bandits, anddynamic pricing.
arxiv-6900-150 | Structural Return Maximization for Reinforcement Learning | http://arxiv.org/pdf/1405.2606v1.pdf | author:Joshua Joseph, Javier Velez, Nicholas Roy category:stat.ML cs.LG published:2014-05-12 summary:Batch Reinforcement Learning (RL) algorithms attempt to choose a policy froma designer-provided class of policies given a fixed set of training data.Choosing the policy which maximizes an estimate of return often leads toover-fitting when only limited data is available, due to the size of the policyclass in relation to the amount of data available. In this work, we focus onlearning policy classes that are appropriately sized to the amount of dataavailable. We accomplish this by using the principle of Structural RiskMinimization, from Statistical Learning Theory, which uses Rademachercomplexity to identify a policy class that maximizes a bound on the return ofthe best policy in the chosen policy class, given the available data. Unlikesimilar batch RL approaches, our bound on return requires only extremely weakassumptions on the true system.
arxiv-6900-151 | Sharp Finite-Time Iterated-Logarithm Martingale Concentration | http://arxiv.org/pdf/1405.2639v4.pdf | author:Akshay Balsubramani category:math.PR cs.LG stat.ML published:2014-05-12 summary:We give concentration bounds for martingales that are uniform over finitetimes and extend classical Hoeffding and Bernstein inequalities. We alsodemonstrate our concentration bounds to be optimal with a matchinganti-concentration inequality, proved using the same method. Together theseconstitute a finite-time version of the law of the iterated logarithm, and shedlight on the relationship between it and the central limit theorem.
arxiv-6900-152 | Two-Stage Metric Learning | http://arxiv.org/pdf/1405.2798v1.pdf | author:Jun Wang, Ke Sun, Fei Sha, Stephane Marchand-Maillet, Alexandros Kalousis category:cs.LG cs.AI stat.ML published:2014-05-12 summary:In this paper, we present a novel two-stage metric learning algorithm. Wefirst map each learning instance to a probability distribution by computing itssimilarities to a set of fixed anchor points. Then, we define the distance inthe input data space as the Fisher information distance on the associatedstatistical manifold. This induces in the input data space a new family ofdistance metric with unique properties. Unlike kernelized metric learning, wedo not require the similarity measure to be positive semi-definite. Moreover,it can also be interpreted as a local metric learning algorithm with welldefined distance approximation. We evaluate its performance on a number ofdatasets. It outperforms significantly other metric learning methods and SVM.
arxiv-6900-153 | Selecting Near-Optimal Approximate State Representations in Reinforcement Learning | http://arxiv.org/pdf/1405.2652v6.pdf | author:Ronald Ortner, Odalric-Ambrym Maillard, Daniil Ryabko category:cs.LG published:2014-05-12 summary:We consider a reinforcement learning setting introduced in (Maillard et al.,NIPS 2011) where the learner does not have explicit access to the states of theunderlying Markov decision process (MDP). Instead, she has access to severalmodels that map histories of past interactions to states. Here we improve overknown regret bounds in this setting, and more importantly generalize to thecase where the models given to the learner do not contain a true modelresulting in an MDP representation but only approximations of it. We also giveimproved error bounds for state aggregation.
arxiv-6900-154 | Consistency of random forests | http://arxiv.org/pdf/1405.2881v4.pdf | author:Erwan Scornet, Gérard Biau, Jean-Philippe Vert category:math.ST stat.ML stat.TH published:2014-05-12 summary:Random forests are a learning algorithm proposed by Breiman [Mach. Learn. 45(2001) 5--32] that combines several randomized decision trees and aggregatestheir predictions by averaging. Despite its wide usage and outstandingpractical performance, little is known about the mathematical properties of theprocedure. This disparity between theory and practice originates in thedifficulty to simultaneously analyze both the randomization process and thehighly data-dependent tree structure. In the present paper, we take a stepforward in forest exploration by proving a consistency result for Breiman's[Mach. Learn. 45 (2001) 5--32] original algorithm in the context of additiveregression models. Our analysis also sheds an interesting light on how randomforests can nicely adapt to sparsity. 1. Introduction. Random forests are anensemble learning method for classification and regression that constructs anumber of randomized decision trees during the training phase and predicts byaveraging the results. Since its publication in the seminal paper of Breiman(2001), the procedure has become a major data analysis tool, that performs wellin practice in comparison with many standard methods. What has greatlycontributed to the popularity of forests is the fact that they can be appliedto a wide range of prediction problems and have few parameters to tune. Asidefrom being simple to use, the method is generally recognized for its accuracyand its ability to deal with small sample sizes, high-dimensional featurespaces and complex data structures. The random forest methodology has beensuccessfully involved in many practical problems, including air qualityprediction (winning code of the EMC data science global hackathon in 2012, seehttp://www.kaggle.com/c/dsg-hackathon), chemoinformatics [Svetnik et al.(2003)], ecology [Prasad, Iverson and Liaw (2006), Cutler et al. (2007)], 3D
arxiv-6900-155 | Comparison of the language networks from literature and blogs | http://arxiv.org/pdf/1405.2702v2.pdf | author:Sabina Šišović, Sanda Martinčić-Ipšić, Ana Meštrović category:cs.CL cs.SI physics.soc-ph published:2014-05-12 summary:In this paper we present the comparison of the linguistic networks fromliterature and blog texts. The linguistic networks are constructed from textsas directed and weighted co-occurrence networks of words. Words are nodes andlinks are established between two nodes if they are directly co-occurringwithin the sentence. The comparison of the networks structure is performed atglobal level (network) in terms of: average node degree, average shortest pathlength, diameter, clustering coefficient, density and number of components.Furthermore, we perform analysis on the local level (node) by comparing therank plots of in and out degree, strength and selectivity. Theselectivity-based results point out that there are differences between thestructure of the networks constructed from literature and blogs.
arxiv-6900-156 | Policy Gradients for CVaR-Constrained MDPs | http://arxiv.org/pdf/1405.2690v1.pdf | author:Prashanth L. A category:stat.ML cs.LG math.OC published:2014-05-12 summary:We study a risk-constrained version of the stochastic shortest path (SSP)problem, where the risk measure considered is Conditional Value-at-Risk (CVaR).We propose two algorithms that obtain a locally risk-optimal policy byemploying four tools: stochastic approximation, mini batches, policy gradientsand importance sampling. Both the algorithms incorporate a CVaR estimationprocedure, along the lines of Bardou et al. [2009], which in turn is based onRockafellar-Uryasev's representation for CVaR and utilize the likelihood ratioprinciple for estimating the gradient of the sum of one cost function(objective of the SSP) and the gradient of the CVaR of the sum of another costfunction (in the constraint of SSP). The algorithms differ in the manner inwhich they approximate the CVaR estimates/necessary gradients - the firstalgorithm uses stochastic approximation, while the second employ mini-batchesin the spirit of Monte Carlo methods. We establish asymptotic convergence ofboth the algorithms. Further, since estimating CVaR is related to rare-eventsimulation, we incorporate an importance sampling based variance reductionscheme into our proposed algorithms.
arxiv-6900-157 | Resource-Aware Programming for Robotic Vision | http://arxiv.org/pdf/1405.2908v1.pdf | author:Johny Paul, Walter Stechele, Manfred Kröhnert, Tamim Asfour category:cs.CV cs.DC cs.RO published:2014-05-12 summary:Humanoid robots are designed to operate in human centered environments. Theyface changing, dynamic environments in which they need to fulfill a multitudeof challenging tasks. Such tasks differ in complexity, resource requirements,and execution time. Latest computer architectures of humanoid robots consist ofseveral industrial PCs containing single- or dual-core processors. According tothe SIA roadmap for semiconductors, many-core chips with hundreds to thousandsof cores are expected to be available in the next decade. Utilizing the fullpower of a chip with huge amounts of resources requires new computing paradigmsand methodologies. In this paper, we analyze a resource-aware computing methodology namedInvasive Computing, to address these challenges. The benefits and limitationsof the new programming model is analyzed using two widely used computer visionalgorithms, the Harris Corner detector and SIFT (Scale Invariant FeatureTransform) feature matching. The result indicate that the new programming modeltogether with the extensions within the application layer, makes them highlyadaptable; leading to better quality in the results obtained.
arxiv-6900-158 | FastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample Test | http://arxiv.org/pdf/1405.2664v2.pdf | author:Ji Zhao, Deyu Meng category:cs.AI cs.LG stat.ML published:2014-05-12 summary:The maximum mean discrepancy (MMD) is a recently proposed test statistic fortwo-sample test. Its quadratic time complexity, however, greatly hampers itsavailability to large-scale applications. To accelerate the MMD calculation, inthis study we propose an efficient method called FastMMD. The core idea ofFastMMD is to equivalently transform the MMD with shift-invariant kernels intothe amplitude expectation of a linear combination of sinusoid components basedon Bochner's theorem and Fourier transform (Rahimi & Recht, 2007). Takingadvantage of sampling of Fourier transform, FastMMD decreases the timecomplexity for MMD calculation from $O(N^2 d)$ to $O(L N d)$, where $N$ and $d$are the size and dimension of the sample set, respectively. Here $L$ is thenumber of basis functions for approximating kernels which determines theapproximation accuracy. For kernels that are spherically invariant, thecomputation can be further accelerated to $O(L N \log d)$ by using the Fastfoodtechnique (Le et al., 2013). The uniform convergence of our method has alsobeen theoretically proved in both unbiased and biased estimates. We havefurther provided a geometric explanation for our method, namely ensemble ofcircular discrepancy, which facilitates us to understand the insight of MMD,and is hopeful to help arouse more extensive metrics for assessing two-sampletest. Experimental results substantiate that FastMMD is with similar accuracyas exact MMD, while with faster computation speed and lower variance than theexisting MMD approximation methods.
arxiv-6900-159 | A Study of Entanglement in a Categorical Framework of Natural Language | http://arxiv.org/pdf/1405.2874v2.pdf | author:Dimitri Kartsaklis, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT quant-ph published:2014-05-12 summary:In both quantum mechanics and corpus linguistics based on vector spaces, thenotion of entanglement provides a means for the various subsystems tocommunicate with each other. In this paper we examine a number ofimplementations of the categorical framework of Coecke, Sadrzadeh and Clark(2010) for natural language, from an entanglement perspective. Specifically,our goal is to better understand in what way the level of entanglement of therelational tensors (or the lack of it) affects the compositional structures inpractical situations. Our findings reveal that a number of proposals for verbconstruction lead to almost separable tensors, a fact that considerablysimplifies the interactions between the words. We examine the ramifications ofthis fact, and we show that the use of Frobenius algebras mitigates thepotential problems to a great extent. Finally, we briefly examine a machinelearning method that creates verb tensors exhibiting a sufficient level ofentanglement.
arxiv-6900-160 | Approximate Policy Iteration Schemes: A Comparison | http://arxiv.org/pdf/1405.2878v1.pdf | author:Bruno Scherrer category:cs.AI cs.LG stat.ML published:2014-05-12 summary:We consider the infinite-horizon discounted optimal control problemformalized by Markov Decision Processes. We focus on several approximatevariations of the Policy Iteration algorithm: Approximate Policy Iteration,Conservative Policy Iteration (CPI), a natural adaptation of the Policy Searchby Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\infty$),and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For allalgorithms, we describe performance bounds, and make a comparison by paying aparticular attention to the concentrability constants involved, the number ofiterations and the memory required. Our analysis highlights the followingpoints: 1) The performance guarantee of CPI can be arbitrarily better than thatof API/API($\alpha$), but this comes at the cost of a relative---exponential in$\frac{1}{\epsilon}$---increase of the number of iterations. 2) PSDP$_\infty$enjoys the best of both worlds: its performance guarantee is similar to that ofCPI, but within a number of iterations similar to that of API. 3) Contrary toAPI that requires a constant memory, the memory needed by CPI and PSDP$_\infty$is proportional to their number of iterations, which may be problematic whenthe discount factor $\gamma$ is close to 1 or the approximation error$\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to makean overall trade-off between memory and performance. Simulations with theseschemes confirm our analysis.
arxiv-6900-161 | Cross-view Action Modeling, Learning and Recognition | http://arxiv.org/pdf/1405.2941v1.pdf | author:Jiang wang, Xiaohan Nie, Yin Xia, Ying Wu, Song-Chun Zhu category:cs.CV published:2014-05-12 summary:Existing methods on video-based action recognition are generallyview-dependent, i.e., performing recognition from the same views seen in thetraining data. We present a novel multiview spatio-temporal AND-OR graph(MST-AOG) representation for cross-view action recognition, i.e., therecognition is performed on the video from an unknown and unseen view. As acompositional model, MST-AOG compactly represents the hierarchicalcombinatorial structures of cross-view actions by explicitly modeling thegeometry, appearance and motion variations. This paper proposes effectivemethods to learn the structure and parameters of MST-AOG. The inference basedon MST-AOG enables action recognition from novel views. The training of MST-AOGtakes advantage of the 3D human skeleton data obtained from Kinect cameras toavoid annotating enormous multi-view video frames, which is error-prone andtime-consuming, but the recognition does not need 3D information and is basedon 2D video input. A new Multiview Action3D dataset has been created and willbe released. Extensive experiments have demonstrated that this new actionrepresentation significantly improves the accuracy and robustness forcross-view action recognition on 2D videos.
arxiv-6900-162 | A Neuron as a Signal Processing Device | http://arxiv.org/pdf/1405.2951v1.pdf | author:Tao Hu, Zaid J. Towfic, Cengiz Pehlevan, Alex Genkin, Dmitri B. Chklovskii category:q-bio.NC stat.ML published:2014-05-12 summary:A neuron is a basic physiological and computational unit of the brain. Whilemuch is known about the physiological properties of a neuron, its computationalrole is poorly understood. Here we propose to view a neuron as a signalprocessing device that represents the incoming streaming data matrix as asparse vector of synaptic weights scaled by an outgoing sparse activity vector.Formally, a neuron minimizes a cost function comprising a cumulative squaredrepresentation error and regularization terms. We derive an online algorithmthat minimizes such cost function by alternating between the minimization withrespect to activity and with respect to synaptic weights. The steps of thisalgorithm reproduce well-known physiological properties of a neuron, such asweighted summation and leaky integration of synaptic inputs, as well as anOja-like, but parameter-free, synaptic learning rule. Our theoretical frameworkmakes several predictions, some of which can be verified by the existing data,others require further experiments. Such framework should allow modeling thefunction of neuronal circuits without necessarily measuring all the microscopicbiophysical parameters, as well as facilitate the design of neuromorphicelectronics.
arxiv-6900-163 | Estimating Diffusion Network Structures: Recovery Conditions, Sample Complexity & Soft-thresholding Algorithm | http://arxiv.org/pdf/1405.2936v1.pdf | author:Hadi Daneshmand, Manuel Gomez-Rodriguez, Le Song, Bernhard Schoelkopf category:cs.SI physics.soc-ph stat.ML published:2014-05-12 summary:Information spreads across social and technological networks, but often thenetwork structures are hidden from us and we only observe the traces left bythe diffusion processes, called cascades. Can we recover the hidden networkstructures from these observed cascades? What kind of cascades and how manycascades do we need? Are there some network structures which are more difficultthan others to recover? Can we design efficient inference algorithms withprovable guarantees? Despite the increasing availability of cascade data and methods for inferringnetworks from these data, a thorough theoretical understanding of the abovequestions remains largely unexplored in the literature. In this paper, weinvestigate the network structure inference problem for a general family ofcontinuous-time diffusion models using an $l_1$-regularized likelihoodmaximization framework. We show that, as long as the cascade sampling processsatisfies a natural incoherence condition, our framework can recover thecorrect network structure with high probability if we observe $O(d^3 \log N)$cascades, where $d$ is the maximum number of parents of a node and $N$ is thetotal number of nodes. Moreover, we develop a simple and efficientsoft-thresholding inference algorithm, which we use to illustrate theconsequences of our theoretical results, and show that our frameworkoutperforms other alternatives in practice.
arxiv-6900-164 | A Review of Image Mosaicing Techniques | http://arxiv.org/pdf/1405.2539v1.pdf | author:Dushyant Vaghela, Prof. Kapildev Naina category:cs.CV published:2014-05-11 summary:Image Mosaicing is a method of constructing multiple images of the same sceneinto a larger image. The output of the image mosaic will be the union of twoinput images. Image-mosaicing algorithms are used to get mosaiced image. ImageMosaicing processed is basically divided in to 5 phases. Which includes;Feature point extraction, Image registration, Homography computation, Warpingand Blending if Image. Various corner detection algorithm is being used forFeature extraction. This corner produces an efficient and informative outputmosaiced image. Image mosaicing is widely used in creating 3D images, medicalimaging, computer vision, data from satellites, and military automatic targetrecognition.
arxiv-6900-165 | Sentiment Analysis: A Survey | http://arxiv.org/pdf/1405.2584v1.pdf | author:Rahul Tejwani category:cs.IR cs.CL published:2014-05-11 summary:Sentiment analysis (also known as opinion mining) refers to the use ofnatural language processing, text analysis and computational linguistics toidentify and extract subjective information in source materials. Miningopinions expressed in the user generated content is a challenging yetpractically very useful problem. This survey would cover various approaches andmethodology used in Sentiment Analysis and Opinion Mining in general. The focuswould be on Internet text like, Product review, tweets and other social media.
arxiv-6900-166 | Learning from networked examples | http://arxiv.org/pdf/1405.2600v1.pdf | author:Yuyi Wang, Jan Ramon, Zheng-Chu Guo category:cs.AI cs.LG stat.ML published:2014-05-11 summary:Many machine learning algorithms are based on the assumption that trainingexamples are drawn identically and independently. However, this assumption doesnot hold anymore when learning from a networked sample because two or moretraining examples may share some common objects, and hence share the featuresof these shared objects. We first show that the classic approach of ignoringthis problem potentially can have a disastrous effect on the accuracy ofstatistics, and then consider alternatives. One of these is to only useindependent examples, discarding other information. However, this is clearlysuboptimal. We analyze sample error bounds in a networked setting, providingboth improved and new results. Next, we propose an efficient weighting methodwhich achieves a better sample error bound than those of previous methods. Ourapproach is based on novel concentration inequalities for networked variables.
arxiv-6900-167 | Image Restoration Using Joint Statistical Modeling in Space-Transform Domain | http://arxiv.org/pdf/1405.3173v1.pdf | author:Jian Zhang, Debin Zhao, Ruiqin Xiong, Siwei Ma, Wen Gao category:cs.MM cs.CV published:2014-05-11 summary:This paper presents a novel strategy for high-fidelity image restoration bycharacterizing both local smoothness and nonlocal self-similarity of naturalimages in a unified statistical manner. The main contributions are three-folds.First, from the perspective of image statistics, a joint statistical modeling(JSM) in an adaptive hybrid space-transform domain is established, which offersa powerful mechanism of combining local smoothness and nonlocal self-similaritysimultaneously to ensure a more reliable and robust estimation. Second, a newform of minimization functional for solving image inverse problem is formulatedusing JSM under regularization-based framework. Finally, in order to make JSMtractable and robust, a new Split-Bregman based algorithm is developed toefficiently solve the above severely underdetermined inverse problem associatedwith theoretical proof of convergence. Extensive experiments on imageinpainting, image deblurring and mixed Gaussian plus salt-and-pepper noiseremoval applications verify the effectiveness of the proposed algorithm.
arxiv-6900-168 | Anomaly-Sensitive Dictionary Learning for Unsupervised Diagnostics of Solid Media | http://arxiv.org/pdf/1405.2496v1.pdf | author:Jeffrey M. Druce, Jarvis D. Haupt, Stefano Gonella category:cs.CV published:2014-05-11 summary:This paper proposes a strategy for the detection and triangulation ofstructural anomalies in solid media. The method revolves around theconstruction of sparse representations of the medium's dynamic response,obtained by learning instructive dictionaries which form a suitable basis forthe response data. The resulting sparse coding problem is recast as a modifieddictionary learning task with additional spatial sparsity constraints enforcedon the atoms of the learned dictionaries, which provides them with a prescribedspatial topology that is designed to unveil anomalous regions in the physicaldomain. The proposed methodology is model agnostic, i.e., it forsakes the needfor a physical model and requires virtually no a priori knowledge of thestructure's material properties, as all the inferences are exclusively informedby the data through the layers of information that are available in theintrinsic salient structure of the material's dynamic response. Thischaracteristic makes the approach powerful for anomaly identification insystems with unknown or heterogeneous property distribution, for which a modelis unsuitable or unreliable. The method is validated using both synthetically
arxiv-6900-169 | Learning modular structures from network data and node variables | http://arxiv.org/pdf/1405.2566v1.pdf | author:Elham Azizi, James E. Galagan, Edoardo M. Airoldi category:stat.ML cs.SI physics.soc-ph q-bio.QM stat.AP published:2014-05-11 summary:A standard technique for understanding underlying dependency structures amonga set of variables posits a shared conditional probability distribution for thevariables measured on individuals within a group. This approach is oftenreferred to as module networks, where individuals are represented by nodes in anetwork, groups are termed modules, and the focus is on estimating the networkstructure among modules. However, estimation solely from node-specificvariables can lead to spurious dependencies, and unverifiable structuralassumptions are often used for regularization. Here, we propose an extendedmodel that leverages direct observations about the network in addition tonode-specific variables. By integrating complementary data types, we avoid theneed for structural assumptions. We illustrate theoretical and practicalsignificance of the model and develop a reversible-jump MCMC learning procedurefor learning modules and model parameters. We demonstrate the method accuracyin predicting modular structures from synthetic data and capability to learninfluence structures in twitter data and regulatory modules in theMycobacterium tuberculosis gene regulatory network.
arxiv-6900-170 | Predicting Central Topics in a Blog Corpus from a Networks Perspective | http://arxiv.org/pdf/1405.2386v1.pdf | author:Srayan Datta category:cs.IR cs.CL cs.SI physics.soc-ph published:2014-05-10 summary:In today's content-centric Internet, blogs are becoming increasingly popularand important from a data analysis perspective. According to Wikipedia, therewere over 156 million public blogs on the Internet as of February 2011. Blogsare a reflection of our contemporary society. The contents of different blogposts are important from social, psychological, economical and politicalperspectives. Discovery of important topics in the blogosphere is an area whichstill needs much exploring. We try to come up with a procedure usingprobabilistic topic modeling and network centrality measures which identifiesthe central topics in a blog corpus.
arxiv-6900-171 | Hyperspectral pan-sharpening: a variational convex constrained formulation to impose parallel level lines, solved with ADMM | http://arxiv.org/pdf/1405.2403v1.pdf | author:Alexis Huck, François de Vieilleville, Pierre Weiss, Manuel Grizonnet category:cs.CV published:2014-05-10 summary:In this paper, we address the issue of hyperspectral pan-sharpening, whichconsists in fusing a (low spatial resolution) hyperspectral image HX and a(high spatial resolution) panchromatic image P to obtain a high spatialresolution hyperspectral image. The problem is addressed under a variationalconvex constrained formulation. The objective favors high resolution spectralbands with level lines parallel to those of the panchromatic image. This termis balanced with a total variation term as regularizer. Fit-to-P data andfit-to-HX data constraints are effectively considered as mathematicalconstraints, which depend on the statistics of the data noise measurements. Thedeveloped Alternating Direction Method of Multipliers (ADMM) optimizationscheme enables us to solve this problem efficiently despite the nondifferentiabilities and the huge number of unknowns.
arxiv-6900-172 | A Hybrid Monte Carlo Architecture for Parameter Optimization | http://arxiv.org/pdf/1405.2377v1.pdf | author:James Brofos category:stat.ML cs.LG stat.ME published:2014-05-10 summary:Much recent research has been conducted in the area of Bayesian learning,particularly with regard to the optimization of hyper-parameters via Gaussianprocess regression. The methodologies rely chiefly on the method of maximizingthe expected improvement of a score function with respect to adjustments in thehyper-parameters. In this work, we present a novel algorithm that exploitsnotions of confidence intervals and uncertainties to enable the discovery ofthe best optimal within a targeted region of the parameter space. Wedemonstrate the efficacy of our algorithm with respect to machine learningproblems and show cases where our algorithm is competitive with the method ofmaximizing expected improvement.
arxiv-6900-173 | Coordinate System Selection for Minimum Error Rate Training in Statistical Machine Translation | http://arxiv.org/pdf/1405.2434v1.pdf | author:Chen Lijiang category:cs.CL published:2014-05-10 summary:Minimum error rate training (MERT) is a widely used training procedure forstatistical machine translation. A general problem of this approach is that thesearch space is easy to converge to a local optimum and the acquired weight setis not in accord with the real distribution of feature functions. This paperintroduces coordinate system selection (RSS) into the search algorithm forMERT. Contrary to previous approaches in which every dimension only correspondsto one independent feature function, we create several coordinate systems bymoving one of the dimensions to a new direction. The basic idea is quite simplebut critical that the training procedure of MERT should be based on acoordinate system formed by search directions but not directly on featurefunctions. Experiments show that by selecting coordinate systems with tuningset results, better results can be obtained without any other languageknowledge.
arxiv-6900-174 | Functional Bandits | http://arxiv.org/pdf/1405.2432v1.pdf | author:Long Tran-Thanh, Jia Yuan Yu category:stat.ML cs.LG published:2014-05-10 summary:We introduce the functional bandit problem, where the objective is to find anarm that optimises a known functional of the unknown arm-reward distributions.These problems arise in many settings such as maximum entropy methods innatural language processing, and risk-averse decision-making, but currentbest-arm identification techniques fail in these domains. We propose a newapproach, that combines functional estimation and arm elimination, to tacklethis problem. This method achieves provably efficient performance guarantees.In addition, we illustrate this method on a number of important functionals inrisk management and information theory, and refine our generic theoreticalresults in those cases.
arxiv-6900-175 | Optimal Learners for Multiclass Problems | http://arxiv.org/pdf/1405.2420v1.pdf | author:Amit Daniely, Shai Shalev-Shwartz category:cs.LG published:2014-05-10 summary:The fundamental theorem of statistical learning states that for binaryclassification problems, any Empirical Risk Minimization (ERM) learning rulehas close to optimal sample complexity. In this paper we seek for a genericoptimal learner for multiclass prediction. We start by proving a surprisingresult: a generic optimal multiclass learner must be improper, namely, it musthave the ability to output hypotheses which do not belong to the hypothesisclass, even though it knows that all the labels are generated by somehypothesis from the class. In particular, no ERM learner is optimal. Thisbrings back the fundmamental question of "how to learn"? We give a completeanswer to this question by giving a new analysis of the one-inclusionmulticlass learner of Rubinstein et al (2006) showing that its samplecomplexity is essentially optimal. Then, we turn to study the popularhypothesis class of generalized linear classifiers. We derive optimal learnersthat, unlike the one-inclusion algorithm, are computationally efficient.Furthermore, we show that the sample complexity of these learners is betterthan the sample complexity of the ERM rule, thus settling in negative an openquestion due to Collins (2005).
arxiv-6900-176 | A Canonical Semi-Deterministic Transducer | http://arxiv.org/pdf/1405.2476v3.pdf | author:Achilles Beros, Colin de la Higuera category:cs.LG published:2014-05-10 summary:We prove the existence of a canonical form for semi-deterministic transducerswith incomparable sets of output strings. Based on this, we develop analgorithm which learns semi-deterministic transducers given access totranslation queries. We also prove that there is no learning algorithm forsemi-deterministic transducers that uses only domain knowledge.
arxiv-6900-177 | Gaussian-Chain Filters for Heavy-Tailed Noise with Application to Detecting Big Buyers and Big Sellers in Stock Market | http://arxiv.org/pdf/1405.2220v1.pdf | author:Li-Xin Wang category:q-fin.TR cs.CE cs.CV cs.SY q-fin.ST published:2014-05-09 summary:We propose a new heavy-tailed distribution --- Gaussian-Chain (GC)distribution, which is inspirited by the hierarchical structures prevailing insocial organizations. We determine the mean, variance and kurtosis of theGaussian-Chain distribution to show its heavy-tailed property, and compute thetail distribution table to give specific numbers showing how heavy is theheavy-tails. To filter out the heavy-tailed noise, we construct two filters ---2nd and 3rd-order GC filters --- based on the maximum likelihood principle.Simulation results show that the GC filters perform much better than thebenchmark least-squares algorithm when the noise is heavy-tail distributed.Using the GC filters, we propose a trading strategy, named Ride-the-Mood, tofollow the mood of the market by detecting the actions of the big buyers andthe big sellers in the market based on the noisy, heavy-tailed price data.Application of the Ride-the-Mood strategy to five blue-chip Hong Kong stocksover the recent two-year period from April 2, 2012 to March 31, 2014 shows thattheir returns are higher than the returns of the benchmark Buy-and-Holdstrategy and the Hang Seng Index Fund.
arxiv-6900-178 | Evaluation The Efficiency Of Cuckoo Optimization Algorithm | http://arxiv.org/pdf/1405.2168v1.pdf | author:Elham Shadkam, Mehdi Bijari category:cs.NE cs.NA math.NA published:2014-05-09 summary:In this paper a new evolutionary algorithm, for continuous nonlinearoptimization problems, is surveyed. This method is inspired by the life of abird, called Cuckoo. The Cuckoo Optimization Algorithm (COA) is evaluated byusing the Rastrigin function. The problem is a non-linear continuous functionwhich is used for evaluating optimization algorithms. The efficiency of the COAhas been studied by obtaining optimal solution of various dimensions Rastriginfunction in this paper. The mentioned function also was solved by FA and ABCalgorithms. Comparing the results shows the COA has better performance thanother algorithms. Application of algorithm to test function has proven itscapability to deal with difficult optimization problems.
arxiv-6900-179 | Cognitive-mapping and contextual pyramid based Digital Elevation Model Registration and its effective storage using fractal based compression | http://arxiv.org/pdf/1405.6662v1.pdf | author:Suma Dawn, Vikas Saxena, Bhudev Sharma category:cs.AI cs.CV published:2014-05-09 summary:Digital Elevation models (DEM) are images having terrain information embeddedinto them. Using cognitive mapping concepts for DEM registration, has evolvedfrom this basic idea of using the mapping between the space to objects anddefining their relationships to form the basic landmarks that need to bemarked, stored and manipulated in and about the environment or other candidateenvironments, namely, in our case, the DEMs. The progressive two-levelencapsulation of methods of geo-spatial cognition includes landmark knowledgeand layout knowledge and can be useful for DEM registration. Space-basedapproach, that emphasizes on explicit extent of the environment underconsideration, and object-based approach, that emphasizes on the relationshipsbetween objects in the local environment being the two paradigms of cognitivemapping can be methodically integrated in this three-architecture for DEMregistration. Initially, P-model based segmentation is performed followed bylandmark formation for contextual mapping that uses contextual pyramidformation. Apart from landmarks being used for registration key-point finding,Euclidean distance based deformation calculation has been used fortransformation and change detection. Landmarks have been categorized to belongto either being flat-plain areas without much variation in the land heights;peaks that can be found when there is gradual increase in height as compared tothe flat areas; valleys, marked with gradual decrease in the height seen inDEM; and finally, ripple areas with very shallow crests and nadirs. Fractalbased compression was used for storage of co-registered DEMs. This method mayfurther be extended for DEM-topographic map and DEM-to-remote sensed imageregistration. Experimental results further cement the fact that DEMregistration may be effectively done using the proposed method.
arxiv-6900-180 | Graph Regularized Non-negative Matrix Factorization By Maximizing Correntropy | http://arxiv.org/pdf/1405.2246v1.pdf | author:Le Li, Jianjun Yang, Kaili Zhao, Yang Xu, Honggang Zhang, Zhuoyi Fan category:cs.CV published:2014-05-09 summary:Non-negative matrix factorization (NMF) has proved effective in manyclustering and classification tasks. The classic ways to measure the errorsbetween the original and the reconstructed matrix are $l_2$ distance orKullback-Leibler (KL) divergence. However, nonlinear cases are not properlyhandled when we use these error measures. As a consequence, alternativemeasures based on nonlinear kernels, such as correntropy, are proposed.However, the current correntropy-based NMF only targets on the low-levelfeatures without considering the intrinsic geometrical distribution of data. Inthis paper, we propose a new NMF algorithm that preserves local invariance byadding graph regularization into the process of max-correntropy-based matrixfactorization. Meanwhile, each feature can learn corresponding kernel from thedata. The experiment results of Caltech101 and Caltech256 show the benefits ofsuch combination against other NMF algorithms for the unsupervised imageclustering.
arxiv-6900-181 | Variational Image Segmentation Model Coupled with Image Restoration Achievements | http://arxiv.org/pdf/1405.2128v1.pdf | author:Xiaohao Cai category:cs.CV math.NA 65Kxx, 65Yxx G.1.0; G.1.6 published:2014-05-09 summary:Image segmentation and image restoration are two important topics in imageprocessing with great achievements. In this paper, we propose a new multiphasesegmentation model by combining image restoration and image segmentationmodels. Utilizing image restoration aspects, the proposed segmentation modelcan effectively and robustly tackle high noisy images, blurry images, imageswith missing pixels, and vector-valued images. In particular, one of the mostimportant segmentation models, the piecewise constant Mumford-Shah model, canbe extended easily in this way to segment gray and vector-valued imagescorrupted for example by noise, blur or missing pixels after coupling a newdata fidelity term which comes from image restoration topics. It can be solvedefficiently using the alternating minimization algorithm, and we prove theconvergence of this algorithm with three variables under mild condition.Experiments on many synthetic and real-world images demonstrate that our methodgives better segmentation results in comparison to others state-of-the-artsegmentation models especially for blurry images and images with missing pixelsvalues.
arxiv-6900-182 | An Overview of Face Liveness Detection | http://arxiv.org/pdf/1405.2227v1.pdf | author:Saptarshi Chakraborty, Dhrubajyoti Das category:cs.CV published:2014-05-09 summary:Face recognition is a widely used biometric approach. Face recognitiontechnology has developed rapidly in recent years and it is more direct, userfriendly and convenient compared to other methods. But face recognition systemsare vulnerable to spoof attacks made by non-real faces. It is an easy way tospoof face recognition systems by facial pictures such as portrait photographs.A secure system needs Liveness detection in order to guard against suchspoofing. In this work, face liveness detection approaches are categorizedbased on the various types techniques used for liveness detection. Thiscategorization helps understanding different spoof attacks scenarios and theirrelation to the developed solutions. A review of the latest works regardingface liveness detection works is presented. The main aim is to provide a simplepath for the future development of novel and more secured face livenessdetection approach.
arxiv-6900-183 | Image Segmentation Using Frequency Locking of Coupled Oscillators | http://arxiv.org/pdf/1405.2362v1.pdf | author:Yan Fang, Matthew J. Cotter, Donald M. Chiarulli, Steven P. Levitan category:cs.CV q-bio.NC C.1.3 published:2014-05-09 summary:Synchronization of coupled oscillators is observed at multiple levels ofneural systems, and has been shown to play an important function in visualperception. We propose a computing system based on locally coupled oscillatornetworks for image segmentation. The system can serve as the preprocessingfront-end of an image processing pipeline where the common frequencies ofclusters of oscillators reflect the segmentation results. To demonstrate thefeasibility of our design, the system is simulated and tested on a human faceimage dataset and its performance is compared with traditional intensitythreshold based algorithms. Our system shows both better performance and highernoise tolerance than traditional methods.
arxiv-6900-184 | Training Deep Fourier Neural Networks To Fit Time-Series Data | http://arxiv.org/pdf/1405.2262v1.pdf | author:Michael S. Gashler, Stephen C. Ashmore category:cs.NE cs.LG published:2014-05-09 summary:We present a method for training a deep neural network containing sinusoidalactivation functions to fit to time-series data. Weights are initialized usinga fast Fourier transform, then trained with regularization to improvegeneralization. A simple dynamic parameter tuning method is employed to adjustboth the learning rate and regularization term, such that stability andefficient training are both achieved. We show how deeper layers can be utilizedto model the observed sequence using a sparser set of sinusoid units, and hownon-uniform regularization can improve generalization by promoting the shiftingof weight toward simpler units. The method is demonstrated with time-seriesproblems to show that it leads to effective extrapolation of nonlinear trends.
arxiv-6900-185 | Better Feature Tracking Through Subspace Constraints | http://arxiv.org/pdf/1405.2316v1.pdf | author:Bryan Poling, Gilad Lerman, Arthur Szlam category:cs.CV published:2014-05-09 summary:Feature tracking in video is a crucial task in computer vision. Usually, thetracking problem is handled one feature at a time, using a single-featuretracker like the Kanade-Lucas-Tomasi algorithm, or one of its derivatives.While this approach works quite well when dealing with high-quality video and"strong" features, it often falters when faced with dark and noisy videocontaining low-quality features. We present a framework for jointly tracking aset of features, which enables sharing information between the differentfeatures in the scene. We show that our method can be employed to trackfeatures for both rigid and nonrigid motions (possibly of few moving bodies)even when some features are occluded. Furthermore, it can be used tosignificantly improve tracking results in poorly-lit scenes (where there is amix of good and bad features). Our approach does not require direct modeling ofthe structure or the motion of the scene, and runs in real time on a single CPUcore.
arxiv-6900-186 | Hellinger Distance Trees for Imbalanced Streams | http://arxiv.org/pdf/1405.2278v1.pdf | author:R. J. Lyon, J. M. Brooke, J. D. Knowles, B. W. Stappers category:cs.LG astro-ph.IM stat.ML published:2014-05-09 summary:Classifiers trained on data sets possessing an imbalanced class distributionare known to exhibit poor generalisation performance. This is known as theimbalanced learning problem. The problem becomes particularly acute when weconsider incremental classifiers operating on imbalanced data streams,especially when the learning objective is rare class identification. Asaccuracy may provide a misleading impression of performance on imbalanced data,existing stream classifiers based on accuracy can suffer poor minority classperformance on imbalanced streams, with the result being low minority classrecall rates. In this paper we address this deficiency by proposing the use ofthe Hellinger distance measure, as a very fast decision tree split criterion.We demonstrate that by using Hellinger a statistically significant improvementin recall rates on imbalanced data streams can be achieved, with an acceptableincrease in the false positive rate.
arxiv-6900-187 | Implementation And Performance Evaluation Of Background Subtraction Algorithms | http://arxiv.org/pdf/1405.1815v1.pdf | author:Deepjoy Das, Dr. Sarat Saharia category:cs.CV published:2014-05-08 summary:The study evaluates three background subtraction techniques. The techniquesranges from very basic algorithm to state of the art published techniquescategorized based on speed, memory requirements and accuracy. Such a review caneffectively guide the designer to select the most suitable method for a givenapplication in a principled way. The algorithms used in the study ranges fromvarying levels of accuracy and computational complexity. Few of them can alsodeal with real time challenges like rain, snow, hails, swaying branches,objects overlapping, varying light intensity or slow moving objects.
arxiv-6900-188 | An Expert System for Automatic Reading of A Text Written in Standard Arabic | http://arxiv.org/pdf/1405.1924v1.pdf | author:Tebbi Hanane, Azzoune Hamid category:cs.CL published:2014-05-08 summary:In this work we present our expert system of Automatic reading or speechsynthesis based on a text written in Standard Arabic, our work is carried outin two great stages: the creation of the sound data base, and thetransformation of the written text into speech (Text To Speech TTS). Thistransformation is done firstly by a Phonetic Orthographical Transcription (POT)of any written Standard Arabic text with the aim of transforming it into hiscorresponding phonetics sequence, and secondly by the generation of the voicesignal which corresponds to the chain transcribed. We spread out the differentof conception of the system, as well as the results obtained compared to othersworks studied to realize TTS based on Standard Arabic.
arxiv-6900-189 | Image Resolution and Contrast Enhancement of Satellite Geographical Images with Removal of Noise using Wavelet Transforms | http://arxiv.org/pdf/1405.1967v1.pdf | author:Prajakta P. Khairnar, C. A. Manjare category:cs.CV published:2014-05-08 summary:In this paper the technique for resolution and contrast enhancement ofsatellite geographical images based on discrete wavelet transform (DWT),stationary wavelet transform (SWT) and singular value decomposition (SVD) hasbeen proposed. In this, the noise is added in the input low resolution and lowcontrast image. The median filter is used remove noise from the input image.This low resolution, low contrast image without noise is decomposed into foursub-bands by using DWT and SWT. The resolution enhancement technique is basedon the interpolation of high frequency components obtained by DWT and inputimage. SWT is used to enhance input image. DWT is used to decompose an imageinto four frequency sub bands and these four sub-bands are interpolated usingbicubic interpolation technique. All these sub-bands are reconstructed as highresolution image by using inverse DWT (IDWT). To increase the contrast theproposed technique uses DWT and SVD. GHE is used to equalize an image. Theequalized image is decomposed into four sub-bands using DWT and new LL sub-bandis reconstructed using SVD. All sub-bands are reconstructed using IDWT togenerate high resolution and contrast image over conventional techniques. Theexperimental result shows superiority of the proposed technique overconventional techniques. Key words: Discrete wavelet transform (DWT), General histogram equalization(GHE), Median filter, Singular value decomposition (SVD), Stationary wavelettransform (SWT).
arxiv-6900-190 | Improving Image Clustering using Sparse Text and the Wisdom of the Crowds | http://arxiv.org/pdf/1405.2102v1.pdf | author:Anna Ma, Arjuna Flenner, Deanna Needell, Allon G. Percus category:cs.LG cs.CV published:2014-05-08 summary:We propose a method to improve image clustering using sparse text and thewisdom of the crowds. In particular, we present a method to fuse two differentkinds of document features, image and text features, and use a commondictionary or "wisdom of the crowds" as the connection between the twodifferent kinds of documents. With the proposed fusion matrix, we use topicmodeling via non-negative matrix factorization to cluster documents.
arxiv-6900-191 | Initial Comparison of Linguistic Networks Measures for Parallel Texts | http://arxiv.org/pdf/1405.1893v2.pdf | author:Kristina Ban, Ana Meštrović, Sanda Martinčić-Ipšić category:cs.CL cs.SI physics.soc-ph published:2014-05-08 summary:This paper presents preliminary results of Croatian syllable networksanalysis. Syllable network is a network in which nodes are syllables and linksbetween them are constructed according to their connections within words. Inthis paper we analyze networks of syllables generated from texts collected fromthe Croatian Wikipedia and Blogs. As a main tool we use complex networkanalysis methods which provide mechanisms that can reveal new patterns in alanguage structure. We aim to show that syllable networks have much higherclustering coefficient in comparison to Erd\"os-Renyi random networks. Theresults indicate that Croatian syllable networks exhibit certain properties ofa small world networks. Furthermore, we compared Croatian syllable networkswith Portuguese and Chinese syllable networks and we showed that they havesimilar properties.
arxiv-6900-192 | A Self-Adaptive Network Protection System | http://arxiv.org/pdf/1405.1958v2.pdf | author:Mohamed Hassan category:cs.NE cs.AI cs.CR published:2014-05-08 summary:In this treatise we aim to build a hybrid network automated (self-adaptive)security threats discovery and prevention system; by using unconventionaltechniques and methods, including fuzzy logic and biological inspiredalgorithms under the context of soft computing.
arxiv-6900-193 | On Communication Cost of Distributed Statistical Estimation and Dimensionality | http://arxiv.org/pdf/1405.1665v2.pdf | author:Ankit Garg, Tengyu Ma, Huy L. Nguyen category:cs.LG cs.IT math.IT published:2014-05-07 summary:We explore the connection between dimensionality and communication cost indistributed learning problems. Specifically we study the problem of estimatingthe mean $\vec{\theta}$ of an unknown $d$ dimensional gaussian distribution inthe distributed setting. In this problem, the samples from the unknowndistribution are distributed among $m$ different machines. The goal is toestimate the mean $\vec{\theta}$ at the optimal minimax rate whilecommunicating as few bits as possible. We show that in this setting, thecommunication cost scales linearly in the number of dimensions i.e. one needsto deal with different dimensions individually. Applying this result toprevious lower bounds for one dimension in the interactive setting\cite{ZDJW13} and to our improved bounds for the simultaneous setting, we provenew lower bounds of $\Omega(md/\log(m))$ and $\Omega(md)$ for the bits ofcommunication needed to achieve the minimax squared loss, in the interactiveand simultaneous settings respectively. To complement, we also demonstrate aninteractive protocol achieving the minimax squared loss with $O(md)$ bits ofcommunication, which improves upon the simple simultaneous protocol by alogarithmic factor. Given the strong lower bounds in the general setting, weinitiate the study of the distributed parameter estimation problems withstructured parameters. Specifically, when the parameter is promised to be$s$-sparse, we show a simple thresholding based protocol that achieves the samesquared loss while saving a $d/s$ factor of communication. We conjecture thatthe tradeoff between communication and squared loss demonstrated by thisprotocol is essentially optimal up to logarithmic factor.
arxiv-6900-194 | RPCA-KFE: Key Frame Extraction for Consumer Video based Robust Principal Component Analysis | http://arxiv.org/pdf/1405.1678v3.pdf | author:Chinh Dang, Abdolreza Moghadam, Hayder Radha category:cs.CV published:2014-05-07 summary:Key frame extraction algorithms consider the problem of selecting a subset ofthe most informative frames from a video to summarize its content.
arxiv-6900-195 | Representative Selection for Big Data via Sparse Graph and Geodesic Grassmann Manifold Distance | http://arxiv.org/pdf/1405.1681v2.pdf | author:Chinh Dang, Hayder Radha category:cs.CV published:2014-05-07 summary:This paper addresses the problem of identifying a very small subset of datapoints that belong to a significantly larger massive dataset (i.e., Big Data).The small number of selected data points must adequately represent andfaithfully characterize the massive Big Data. Such identification process isknown as representative selection [19]. We propose a novel representativeselection framework by generating an l1 norm sparse graph for a given Big-Datadataset. The Big Data is partitioned recursively into clusters using a spectralclustering algorithm on the generated sparse graph. We consider each cluster asone point in a Grassmann manifold, and measure the geodesic distance amongthese points. The distances are further analyzed using a min-max algorithm [1]to extract an optimal subset of clusters. Finally, by considering a sparsesubgraph of each selected cluster, we detect a representative using principalcomponent centrality [11]. We refer to the proposed representative selectionframework as a Sparse Graph and Grassmann Manifold (SGGM) based approach. Tovalidate the proposed SGGM framework, we apply it onto the problem of videosummarization where only few video frames, known as key frames, are selectedamong a much longer video sequence. A comparison of the results obtained by theproposed algorithm with the ground truth, which is agreed by multiple humanjudges, and with some state-of-the-art methods clearly indicates the viabilityof the SGGM framework.
arxiv-6900-196 | A consistent deterministic regression tree for non-parametric prediction of time series | http://arxiv.org/pdf/1405.1533v2.pdf | author:Pierre Gaillard, Paul Baudin category:math.ST cs.LG stat.ML stat.TH published:2014-05-07 summary:We study online prediction of bounded stationary ergodic processes. To do so,we consider the setting of prediction of individual sequences and build adeterministic regression tree that performs asymptotically as well as the bestL-Lipschitz constant predictors. Then, we show why the obtained regret boundentails the asymptotical optimality with respect to the class of boundedstationary ergodic processes.
arxiv-6900-197 | A Mathematical Theory of Learning | http://arxiv.org/pdf/1405.1513v1.pdf | author:Ibrahim Alabdulmohsin category:cs.LG cs.AI cs.IT math.IT published:2014-05-07 summary:In this paper, a mathematical theory of learning is proposed that has manyparallels with information theory. We consider Vapnik's General Setting ofLearning in which the learning process is defined to be the act of selecting ahypothesis in response to a given training set. Such hypothesis can, forexample, be a decision boundary in classification, a set of centroids inclustering, or a set of frequent item-sets in association rule mining.Depending on the hypothesis space and how the final hypothesis is selected, weshow that a learning process can be assigned a numeric score, called learningcapacity, which is analogous to Shannon's channel capacity and satisfiessimilar interesting properties as well such as the data-processing inequalityand the information-cannot-hurt inequality. In addition, learning capacityprovides the tightest possible bound on the difference between true risk andempirical risk of the learning process for all loss functions that areparametrized by the chosen hypothesis. It is also shown that the notion oflearning capacity equivalently quantifies how sensitive the choice of the finalhypothesis is to a small perturbation in the training set. Consequently,algorithmic stability is both necessary and sufficient for generalization.While the theory does not rely on concentration inequalities, we finally showthat analogs to classical results in learning theory using the ProbablyApproximately Correct (PAC) model can be immediately deduced using this theory,and conclude with information-theoretic bounds to learning capacity.
arxiv-6900-198 | Learning Boolean Halfspaces with Small Weights from Membership Queries | http://arxiv.org/pdf/1405.1535v1.pdf | author:Hasan Abasi, Ali Z. Abdi, Nader H. Bshouty category:cs.LG published:2014-05-07 summary:We consider the problem of proper learning a Boolean Halfspace with integerweights $\{0,1,\ldots,t\}$ from membership queries only. The best knownalgorithm for this problem is an adaptive algorithm that asks $n^{O(t^5)}$membership queries where the best lower bound for the number of membershipqueries is $n^t$ [Learning Threshold Functions with Small Weights UsingMembership Queries. COLT 1999] In this paper we close this gap and give an adaptive proper learningalgorithm with two rounds that asks $n^{O(t)}$ membership queries. We also givea non-adaptive proper learning algorithm that asks $n^{O(t^3)}$ membershipqueries.
arxiv-6900-199 | Adaptation Algorithm and Theory Based on Generalized Discrepancy | http://arxiv.org/pdf/1405.1503v3.pdf | author:Corinna Cortes, Mehryar Mohri, Andres Muñoz Medina category:cs.LG published:2014-05-07 summary:We present a new algorithm for domain adaptation improving upon a discrepancyminimization algorithm previously shown to outperform a number of algorithmsfor this task. Unlike many previous algorithms for domain adaptation, ouralgorithm does not consist of a fixed reweighting of the losses over thetraining sample. We show that our algorithm benefits from a solid theoreticalfoundation and more favorable learning bounds than discrepancy minimization. Wepresent a detailed description of our algorithm and give several efficientsolutions for solving its optimization problem. We also report the results ofseveral experiments showing that it outperforms discrepancy minimization.
arxiv-6900-200 | DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News | http://arxiv.org/pdf/1405.1605v1.pdf | author:Jacopo Staiano, Marco Guerini category:cs.CL cs.CY published:2014-05-07 summary:While many lexica annotated with words polarity are available for sentimentanalysis, very few tackle the harder task of emotion analysis and are usuallyquite limited in coverage. In this paper, we present a novel approach forextracting - in a totally automated way - a high-coverage and high-precisionlexicon of roughly 37 thousand terms annotated with emotion scores, calledDepecheMood. Our approach exploits in an original way 'crowd-sourced' affectiveannotation implicitly provided by readers of news articles from rappler.com. Byproviding new state-of-the-art performances in unsupervised settings forregression and classification tasks, even using a na\"{\i}ve approach, ourexperiments show the beneficial impact of harvesting social media data foraffective lexicon building.
arxiv-6900-201 | Entropy Based Cartoon Texture Separation | http://arxiv.org/pdf/1405.1717v1.pdf | author:Kutlu Emre Yilmaz category:cs.CV published:2014-05-07 summary:Separating an image into cartoon and texture components comes useful in imageprocessing applications, such as image compression, image segmentation, imageinpainting. Yves Meyer's influential cartoon texture decomposition modelinvolves deriving an energy functional by choosing appropriate spaces andfunctionals. Minimizers of the derived energy functional are cartoon andtexture components of an image. In this study, cartoon part of an image isseparated, by reconstructing it from pixels of multi scale Total-Variationfiltered versions of the original image which is sought to be decomposed intocartoon and texture parts. An information theoretic pixel by pixel selectioncriteria is employed to choose the contributing pixels and their scales.
arxiv-6900-202 | On Tensor Completion via Nuclear Norm Minimization | http://arxiv.org/pdf/1405.1773v1.pdf | author:Ming Yuan, Cun-Hui Zhang category:stat.ML cs.IT math.IT math.NA math.OC math.PR published:2014-05-07 summary:Many problems can be formulated as recovering a low-rank tensor. Although anincreasingly common task, tensor recovery remains a challenging problem becauseof the delicacy associated with the decomposition of higher order tensors. Toovercome these difficulties, existing approaches often proceed by unfoldingtensors into matrices and then apply techniques for matrix completion. We showhere that such matricization fails to exploit the tensor structure and may leadto suboptimal procedure. More specifically, we investigate a convexoptimization approach to tensor completion by directly minimizing a tensornuclear norm and prove that this leads to an improved sample size requirement.To establish our results, we develop a series of algebraic and probabilistictechniques such as characterization of subdifferetial for tensor nuclear normand concentration inequalities for tensor martingales, which may be ofindependent interests and could be useful in other tensor related problems.
arxiv-6900-203 | PAC-Bayes Mini-tutorial: A Continuous Union Bound | http://arxiv.org/pdf/1405.1580v1.pdf | author:Tim van Erven category:stat.ML published:2014-05-07 summary:When I first encountered PAC-Bayesian concentration inequalities they seemedto me to be rather disconnected from good old-fashioned results likeHoeffding's and Bernstein's inequalities. But, at least for one flavour of thePAC-Bayesian bounds, there is actually a very close relation, and the maininnovation is a continuous version of the union bound, along with someingenious applications. Here's the gist of what's going on, presented from amachine learning perspective.
arxiv-6900-204 | Learning Alternative Name Spellings | http://arxiv.org/pdf/1405.2048v1.pdf | author:Jeffrey Sukharev, Leonid Zhukov, Alexandrin Popescul category:cs.IR cs.CL published:2014-05-07 summary:Name matching is a key component of systems for entity resolution or recordlinkage. Alternative spellings of the same names are a com- mon occurrence inmany applications. We use the largest collection of genealogy person records inthe world together with user search query logs to build name matching models.The procedure for building a crowd-sourced training set is outlined togetherwith the presentation of our method. We cast the problem of learningalternative spellings as a machine translation problem at the character level.We use in- formation retrieval evaluation methodology to show that this methodsubstantially outperforms on our data a number of standard well known phoneticand string similarity methods in terms of precision and re- call. Additionally,we rigorously compare the performance of standard methods when compared witheach other. Our result can lead to a significant practical impact in entityresolution applications.
arxiv-6900-205 | Feature selection for classification with class-separability strategy and data envelopment analysis | http://arxiv.org/pdf/1405.1119v2.pdf | author:Yishi Zhang, Chao Yang, Anrong Yang, Chan Xiong, Xingchi Zhou, Zigang Zhang category:cs.LG cs.IT math.IT stat.ML published:2014-05-06 summary:In this paper, a novel feature selection method is presented, which is basedon Class-Separability (CS) strategy and Data Envelopment Analysis (DEA). Tobetter capture the relationship between features and the class, class labelsare separated into individual variables and relevance and redundancy areexplicitly handled on each class label. Super-efficiency DEA is employed toevaluate and rank features via their conditional dependence scores on all classlabels, and the feature with maximum super-efficiency score is then added inthe conditioning set for conditional dependence estimation in the nextiteration, in such a way as to iteratively select features and get the finalselected features. Eventually, experiments are conducted to evaluate theeffectiveness of proposed method comparing with four state-of-the-art methodsfrom the viewpoint of classification accuracy. Empirical results verify thefeasibility and the superiority of proposed feature selection method.
arxiv-6900-206 | Is Joint Training Better for Deep Auto-Encoders? | http://arxiv.org/pdf/1405.1380v4.pdf | author:Yingbo Zhou, Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju category:stat.ML cs.LG cs.NE published:2014-05-06 summary:Traditionally, when generative models of data are developed via deeparchitectures, greedy layer-wise pre-training is employed. In a well-trainedmodel, the lower layer of the architecture models the data distributionconditional upon the hidden variables, while the higher layers model the hiddendistribution prior. But due to the greedy scheme of the layerwise trainingtechnique, the parameters of lower layers are fixed when training higherlayers. This makes it extremely challenging for the model to learn the hiddendistribution prior, which in turn leads to a suboptimal model for the datadistribution. We therefore investigate joint training of deep autoencoders,where the architecture is viewed as one stack of two or more single-layerautoencoders. A single global reconstruction objective is jointly optimized,such that the objective for the single autoencoders at each layer acts as alocal, layer-level regularizer. We empirically evaluate the performance of thisjoint training scheme and observe that it not only learns a better data model,but also learns better higher layer representations, which highlights itspotential for unsupervised feature learning. In addition, we find that theusage of regularizations in the joint training scheme is crucial in achievinggood performance. In the supervised setting, joint training also shows superiorperformance when training deeper models. The joint training framework can thusprovide a platform for investigating more efficient usage of different types ofregularizers, especially in light of the growing volumes of available unlabeleddata.
arxiv-6900-207 | A Corpus of Sentence-level Revisions in Academic Writing: A Step towards Understanding Statement Strength in Communication | http://arxiv.org/pdf/1405.1439v2.pdf | author:Chenhao Tan, Lillian Lee category:cs.CL published:2014-05-06 summary:The strength with which a statement is made can have a significant impact onthe audience. For example, international relations can be strained by how themedia in one country describes an event in another; and papers can be rejectedbecause they overstate or understate their findings. It is thus important tounderstand the effects of statement strength. A first step is to be able todistinguish between strong and weak statements. However, even this problem isunderstudied, partly due to a lack of data. Since strength is inherentlyrelative, revisions of texts that make claims are a natural source of data onstrength differences. In this paper, we introduce a corpus of sentence-levelrevisions from academic writing. We also describe insights gained from ourannotation efforts for this task.
arxiv-6900-208 | How Community Feedback Shapes User Behavior | http://arxiv.org/pdf/1405.1429v1.pdf | author:Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8 published:2014-05-06 summary:Social media systems rely on user feedback and rating mechanisms forpersonalization, ranking, and content filtering. However, when users evaluatecontent contributed by fellow users (e.g., by liking a post or voting on acomment), these evaluations create complex social feedback effects. This paperinvestigates how ratings on a piece of content affect its author's futurebehavior. By studying four large comment-based news communities, we find thatnegative feedback leads to significant behavioral changes that are detrimentalto the community. Not only do authors of negatively-evaluated contentcontribute more, but also their future posts are of lower quality, and areperceived by the community as such. Moreover, these authors are more likely tosubsequently evaluate their fellow users negatively, percolating these effectsthrough the community. In contrast, positive feedback does not carry similareffects, and neither encourages rewarded authors to write more, nor improvesthe quality of their posts. Interestingly, the authors that receive no feedbackare most likely to leave a community. Furthermore, a structural analysis of thevoter network reveals that evaluations polarize the community the most whenpositive and negative votes are equally split.
arxiv-6900-209 | D-Bees: A Novel Method Inspired by Bee Colony Optimization for Solving Word Sense Disambiguation | http://arxiv.org/pdf/1405.1406v1.pdf | author:Sallam Abualhaija, Karl-Heinz Zimmermann category:cs.CL published:2014-05-06 summary:Word sense disambiguation (WSD) is a problem in the field of computationallinguistics given as finding the intended sense of a word (or a set of words)when it is activated within a certain context. WSD was recently addressed as acombinatorial optimization problem in which the goal is to find a sequence ofsenses that maximize the semantic relatedness among the target words. In thisarticle, a novel algorithm for solving the WSD problem called D-Bees isproposed which is inspired by bee colony optimization (BCO)where artificial beeagents collaborate to solve the problem. The D-Bees algorithm is evaluated on astandard dataset (SemEval 2007 coarse-grained English all-words task corpus)andis compared to simulated annealing, genetic algorithms, and two ant colonyoptimization techniques (ACO). It will be observed that the BCO and ACOapproaches are on par.
arxiv-6900-210 | Latent semantics of action verbs reflect phonetic parameters of intensity and emotional content | http://arxiv.org/pdf/1405.1359v3.pdf | author:Michael Kai Petersen category:cs.CL 68T50 I.2.4; I.2.7 published:2014-05-06 summary:Conjuring up our thoughts, language reflects statistical patterns of wordco-occurrences which in turn come to describe how we perceive the world.Whether counting how frequently nouns and verbs combine in Google searchqueries, or extracting eigenvectors from term document matrices made up ofWikipedia lines and Shakespeare plots, the resulting latent semantics capturenot only the associative links which form concepts, but also spatial dimensionsembedded within the surface structure of language. As both the shape andmovements of objects have been found to be associated with phonetic contrastsalready in toddlers, this study explores whether articulatory and acousticparameters may likewise differentiate the latent semantics of action verbs.Selecting 3 x 20 emotion, face, and hand related verbs known to activatepremotor areas in the brain, their mutual cosine similarities were computedusing latent semantic analysis LSA, and the resulting adjacency matrices werecompared based on two different large scale text corpora; HAWIK and TASA.Applying hierarchical clustering to identify common structures across the twotext corpora, the verbs largely divide into combined mouth and hand movementsversus emotional expressions. Transforming the verbs into their constituentphonemes, the clustered small and large size movements appear differentiated byfront versus back vowels corresponding to increasing levels of arousal. Whereasthe clustered emotional verbs seem characterized by sequences of close versusopen jaw produced phonemes, generating up- or downwards shifts in formantfrequencies that may influence their perceived valence. Suggesting, that thelatent semantics of action verbs reflect parameters of intensity and emotionalpolarity that appear correlated with the articulatory contrasts and acousticcharacteristics of phonemes
arxiv-6900-211 | Pulling back error to the hidden-node parameter technology: Single-hidden-layer feedforward network without output weight | http://arxiv.org/pdf/1405.1445v1.pdf | author:Yimin Yang, Q. M. Jonathan Wu, Guangbin Huang, Yaonan Wang category:cs.NE 68Txx F.1.1 published:2014-05-06 summary:According to conventional neural network theories, the feature ofsingle-hidden-layer feedforward neural networks(SLFNs) resorts to parameters ofthe weighted connections and hidden nodes. SLFNs are universal approximatorswhen at least the parameters of the networks including hidden-node parameterand output weight are exist. Unlike above neural network theories, this paperindicates that in order to let SLFNs work as universal approximators, one maysimply calculate the hidden node parameter only and the output weight is notneeded at all. In other words, this proposed neural network architecture can beconsidered as a standard SLFNs with fixing output weight equal to an unitvector. Further more, this paper presents experiments which show that theproposed learning method tends to extremely reduce network output error to avery small number with only 1 hidden node. Simulation results demonstrate thatthe proposed method can provide several to thousands of times faster than otherlearning algorithm including BP, SVM/SVR and other ELM methods.
arxiv-6900-212 | Understanding Protein Dynamics with L1-Regularized Reversible Hidden Markov Models | http://arxiv.org/pdf/1405.1444v1.pdf | author:Robert T. McGibbon, Bharath Ramsundar, Mohammad M. Sultan, Gert Kiss, Vijay S. Pande category:q-bio.BM stat.AP stat.ML published:2014-05-06 summary:We present a machine learning framework for modeling protein dynamics. Ourapproach uses L1-regularized, reversible hidden Markov models to understandlarge protein datasets generated via molecular dynamics simulations. Our modelis motivated by three design principles: (1) the requirement of massivescalability; (2) the need to adhere to relevant physical law; and (3) thenecessity of providing accessible interpretations, critical for both cellularbiology and rational drug design. We present an EM algorithm for learning andintroduce a model selection criteria based on the physical notion ofconvergence in relaxation timescales. We contrast our model with standardmethods in biophysics and demonstrate improved robustness. We implement ouralgorithm on GPUs and apply the method to two large protein simulation datasetsgenerated respectively on the NCSA Bluewaters supercomputer and theFolding@Home distributed computing network. Our analysis identifies theconformational dynamics of the ubiquitin protein critical to cellularsignaling, and elucidates the stepwise activation mechanism of the c-Src kinaseprotein.
arxiv-6900-213 | Automatic Method Of Domain Ontology Construction based on Characteristics of Corpora POS-Analysis | http://arxiv.org/pdf/1405.1346v1.pdf | author:Olena Orobinska category:cs.CL published:2014-05-06 summary:It is now widely recognized that ontologies, are one of the fundamentalcornerstones of knowledge-based systems. What is lacking, however, is acurrently accepted strategy of how to build ontology; what kinds of theresources and techniques are indispensables to optimize the expenses and thetime on the one hand and the amplitude, the completeness, the robustness of enontology on the other hand. The paper offers a semi-automatic ontologyconstruction method from text corpora in the domain of radiological protection.This method is composed from next steps: 1) text annotation with part-of-speechtags; 2) revelation of the significant linguistic structures and forming thetemplates; 3) search of text fragments corresponding to these templates; 4)basic ontology instantiation process
arxiv-6900-214 | The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter | http://arxiv.org/pdf/1405.1438v1.pdf | author:Chenhao Tan, Lillian Lee, Bo Pang category:cs.SI cs.CL physics.soc-ph published:2014-05-06 summary:Consider a person trying to spread an important message on a social network.He/she can spend hours trying to craft the message. Does it actually matter?While there has been extensive prior work looking into predicting popularity ofsocial-media content, the effect of wording per se has rarely been studiedsince it is often confounded with the popularity of the author and the topic.To control for these confounding factors, we take advantage of the surprisingfact that there are many pairs of tweets containing the same url and written bythe same user but employing different wording. Given such pairs, we ask: whichversion attracts more retweets? This turns out to be a more difficult task thanpredicting popular topics. Still, humans can answer this question better thanchance (but far from perfectly), and the computational methods we develop cando better than both an average human and a strong competing method trained onnon-controlled data.
arxiv-6900-215 | Human Pose Estimation from RGB Input Using Synthetic Training Data | http://arxiv.org/pdf/1405.1213v2.pdf | author:Oscar Danielsson, Omid Aghazadeh category:cs.CV published:2014-05-06 summary:We address the problem of estimating the pose of humans using RGB imageinput. More specifically, we are using a random forest classifier to classifypixels into joint-based body part categories, much similar to the famous Kinectpose estimator [11], [12]. However, we are using pure RGB input, i.e. no depth.Since the random forest requires a large number of training examples, we areusing computer graphics generated, synthetic training data. In addition, weassume that we have access to a large number of real images with bounding boxlabels, extracted for example by a pedestrian detector or a tracking system. Wepropose a new objective function for random forest training that uses theweakly labeled data from the target domain to encourage the learner to selectfeatures that generalize from the synthetic source domain to the real targetdomain. We demonstrate on a publicly available dataset [6] that the proposedobjective function yields a classifier that significantly outperforms abaseline classifier trained using the standard entropy objective [10].
arxiv-6900-216 | Combining Multiple Clusterings via Crowd Agreement Estimation and Multi-Granularity Link Analysis | http://arxiv.org/pdf/1405.1297v1.pdf | author:Dong Huang, Jian-Huang Lai, Chang-Dong Wang category:stat.ML cs.LG published:2014-05-06 summary:The clustering ensemble technique aims to combine multiple clusterings into aprobably better and more robust clustering and has been receiving an increasingattention in recent years. There are mainly two aspects of limitations in theexisting clustering ensemble approaches. Firstly, many approaches lack theability to weight the base clusterings without access to the original data andcan be affected significantly by the low-quality, or even ill clusterings.Secondly, they generally focus on the instance level or cluster level in theensemble system and fail to integrate multi-granularity cues into a unifiedmodel. To address these two limitations, this paper proposes to solve theclustering ensemble problem via crowd agreement estimation andmulti-granularity link analysis. We present the normalized crowd agreementindex (NCAI) to evaluate the quality of base clusterings in an unsupervisedmanner and thus weight the base clusterings in accordance with their clusteringvalidity. To explore the relationship between clusters, the source awareconnected triple (SACT) similarity is introduced with regard to their commonneighbors and the source reliability. Based on NCAI and multi-granularityinformation collected among base clusterings, clusters, and data instances, wefurther propose two novel consensus functions, termed weighted evidenceaccumulation clustering (WEAC) and graph partitioning with multi-granularitylink analysis (GP-MGLA) respectively. The experiments are conducted on eightreal-world datasets. The experimental results demonstrate the effectiveness androbustness of the proposed methods.
arxiv-6900-217 | Training Restricted Boltzmann Machine by Perturbation | http://arxiv.org/pdf/1405.1436v1.pdf | author:Siamak Ravanbakhsh, Russell Greiner, Brendan Frey category:cs.NE cs.LG stat.ML published:2014-05-06 summary:A new approach to maximum likelihood learning of discrete graphical modelsand RBM in particular is introduced. Our method, Perturb and Descend (PD) isinspired by two ideas (I) perturb and MAP method for sampling (II) learning byContrastive Divergence minimization. In contrast to perturb and MAP, PDleverages training data to learn the models that do not allow efficient MAPestimation. During the learning, to produce a sample from the current model, westart from a training data and descend in the energy landscape of the"perturbed model", for a fixed number of steps, or until a local optima isreached. For RBM, this involves linear calculations and thresholding which canbe very fast. Furthermore we show that the amount of perturbation is closelyrelated to the temperature parameter and it can regularize the model byproducing robust features resulting in sparse hidden layer activation.
arxiv-6900-218 | Nuclear Norm based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes | http://arxiv.org/pdf/1405.1207v1.pdf | author:Jian Yang, Jianjun Qian, Lei Luo, Fanlong Zhang, Yicheng Gao category:cs.CV published:2014-05-06 summary:Recently regression analysis becomes a popular tool for face recognition. Theexisting regression methods all use the one-dimensional pixel-based errormodel, which characterizes the representation error pixel by pixel individuallyand thus neglects the whole structure of the error image. We observe thatocclusion and illumination changes generally lead to a low-rank error image. Tomake use of this low-rank structural information, this paper presents atwo-dimensional image matrix based error model, i.e. matrix regression, forface representation and classification. Our model uses the minimal nuclear normof representation error image as a criterion, and the alternating directionmethod of multipliers method to calculate the regression coefficients. Comparedwith the current regression methods, the proposed Nuclear Norm based MatrixRegression (NMR) model is more robust for alleviating the effect ofillumination, and more intuitive and powerful for removing the structural noisecaused by occlusion. We experiment using four popular face image databases, theExtended Yale B database, the AR database, the Multi-PIE and the FRGC database.Experimental results demonstrate the performance advantage of NMR over thestate-of-the-art regression based face recognition methods.
arxiv-6900-219 | K-NS: Section-Based Outlier Detection in High Dimensional Space | http://arxiv.org/pdf/1405.1027v1.pdf | author:Zhana Bao category:cs.AI cs.LG stat.ML published:2014-05-05 summary:Finding rare information hidden in a huge amount of data from the Internet isa necessary but complex issue. Many researchers have studied this issue andhave found effective methods to detect anomaly data in low dimensional space.However, as the dimension increases, most of these existing methods performpoorly in detecting outliers because of "high dimensional curse". Even thoughsome approaches aim to solve this problem in high dimensional space, they canonly detect some anomaly data appearing in low dimensional space and cannotdetect all of anomaly data which appear differently in high dimensional space.To cope with this problem, we propose a new k-nearest section-based method(k-NS) in a section-based space. Our proposed approach not only detectsoutliers in low dimensional space with section-density ratio but also detectsoutliers in high dimensional space with the ratio of k-nearest section againstaverage value. After taking a series of experiments with the dimension from 10to 10000, the experiment results show that our proposed method achieves 100%precision and 100% recall result in the case of extremely high dimensionalspace, and better improvement in low dimensional space compared to ourpreviously proposed method.
arxiv-6900-220 | Design and Optimization of a Speech Recognition Front-End for Distant-Talking Control of a Music Playback Device | http://arxiv.org/pdf/1405.1379v1.pdf | author:Ramin Pichevar, Jason Wung, Daniele Giacobello, Joshua Atkins category:cs.SD cs.CL published:2014-05-05 summary:This paper addresses the challenging scenario for the distant-talking controlof a music playback device, a common portable speaker with four smallloudspeakers in close proximity to one microphone. The user controls the devicethrough voice, where the speech-to-music ratio can be as low as -30 dB duringmusic playback. We propose a speech enhancement front-end that relies on knownrobust methods for echo cancellation, double-talk detection, and noisesuppression, as well as a novel adaptive quasi-binary mask that is well suitedfor speech recognition. The optimization of the system is then formulated as alarge scale nonlinear programming problem where the recognition rate ismaximized and the optimal values for the system parameters are found through agenetic algorithm. We validate our methodology by testing over the TIMITdatabase for different music playback levels and noise types. Finally, we showthat the proposed front-end allows a natural interaction with the device forlimited-vocabulary voice commands.
arxiv-6900-221 | Towards a Benchmark of Natural Language Arguments | http://arxiv.org/pdf/1405.0941v1.pdf | author:Elena Cabrio, Serena Villata category:cs.AI cs.CL published:2014-05-05 summary:The connections among natural language processing and argumentation theoryare becoming stronger in the latest years, with a growing amount of works goingin this direction, in different scenarios and applying heterogeneoustechniques. In this paper, we present two datasets we built to cope with thecombination of the Textual Entailment framework and bipolar abstractargumentation. In our approach, such datasets are used to automaticallyidentify through a Textual Entailment system the relations among the arguments(i.e., attack, support), and then the resulting bipolar argumentation graphsare analyzed to compute the accepted arguments.
arxiv-6900-222 | Learning Bilingual Word Representations by Marginalizing Alignments | http://arxiv.org/pdf/1405.0947v1.pdf | author:Tomáš Kočiský, Karl Moritz Hermann, Phil Blunsom category:cs.CL published:2014-05-05 summary:We present a probabilistic model that simultaneously learns alignments anddistributed representations for bilingual data. By marginalizing over wordalignments the model captures a larger semantic context than prior work relyingon hard alignments. The advantage of this approach is demonstrated in across-lingual classification task, where we outperform the prior publishedstate of the art.
arxiv-6900-223 | Robust Subspace Outlier Detection in High Dimensional Space | http://arxiv.org/pdf/1405.0869v1.pdf | author:Zhana Bao category:cs.AI cs.LG stat.ML published:2014-05-05 summary:Rare data in a large-scale database are called outliers that revealsignificant information in the real world. The subspace-based outlier detectionis regarded as a feasible approach in very high dimensional space. However, theoutliers found in subspaces are only part of the true outliers in highdimensional space, indeed. The outliers hidden in normal-clustered points aresometimes neglected in the projected dimensional subspace. In this paper, wepropose a robust subspace method for detecting such inner outliers in a givendataset, which uses two dimensional-projections: detecting outliers insubspaces with local density ratio in the first projected dimensions; findingoutliers by comparing neighbor's positions in the second projected dimensions.Each point's weight is calculated by summing up all related values got in thetwo steps projected dimensions, and then the points scoring the largest weightvalues are taken as outliers. By taking a series of experiments with the numberof dimensions from 10 to 10000, the results show that our proposed methodachieves high precision in the case of extremely high dimensional space, andworks well in low dimensional space.
arxiv-6900-224 | Generalized Risk-Aversion in Stochastic Multi-Armed Bandits | http://arxiv.org/pdf/1405.0833v1.pdf | author:Alexander Zimin, Rasmus Ibsen-Jensen, Krishnendu Chatterjee category:cs.LG stat.ML published:2014-05-05 summary:We consider the problem of minimizing the regret in stochastic multi-armedbandit, when the measure of goodness of an arm is not the mean return, but somegeneral function of the mean and the variance.We characterize the conditionsunder which learning is possible and present examples for which no naturalalgorithm can achieve sublinear regret.
arxiv-6900-225 | On Exact Learning Monotone DNF from Membership Queries | http://arxiv.org/pdf/1405.0792v1.pdf | author:Hasan Abasi, Nader H. Bshouty, Hanna Mazzawi category:cs.LG published:2014-05-05 summary:In this paper, we study the problem of learning a monotone DNF with at most$s$ terms of size (number of variables in each term) at most $r$ ($s$ term$r$-MDNF) from membership queries. This problem is equivalent to the problem oflearning a general hypergraph using hyperedge-detecting queries, a problemmotivated by applications arising in chemical reactions and genome sequencing. We first present new lower bounds for this problem and then presentdeterministic and randomized adaptive algorithms with query complexities thatare almost optimal. All the algorithms we present in this paper run in timelinear in the query complexity and the number of variables $n$. In addition,all of the algorithms we present in this paper are asymptotically tight forfixed $r$ and/or $s$.
arxiv-6900-226 | Model Consistency of Partly Smooth Regularizers | http://arxiv.org/pdf/1405.1004v3.pdf | author:Samuel Vaiter, Gabriel Peyré, Jalal M. Fadili category:math.OC cs.IT math.IT stat.ML published:2014-05-05 summary:This paper studies least-square regression penalized with partly smoothconvex regularizers. This class of functions is very large and versatileallowing to promote solutions conforming to some notion of low-complexity.Indeed, they force solutions of variational problems to belong to alow-dimensional manifold (the so-called model) which is stable under smallperturbations of the function. This property is crucial to make the underlyinglow-complexity model robust to small noise. We show that a generalized"irrepresentable condition" implies stable model selection under small noiseperturbations in the observations and the design matrix, when theregularization parameter is tuned proportionally to the noise level. Thiscondition is shown to be almost a necessary condition. We then show that thiscondition implies model consistency of the regularized estimator. That is, witha probability tending to one as the number of measurements increases, theregularized estimator belongs to the correct low-dimensional model manifold.This work unifies and generalizes several previous ones, where modelconsistency is known to hold for sparse, group sparse, total variation andlow-rank regularizations.
arxiv-6900-227 | Optimality guarantees for distributed statistical estimation | http://arxiv.org/pdf/1405.0782v2.pdf | author:John C. Duchi, Michael I. Jordan, Martin J. Wainwright, Yuchen Zhang category:cs.IT cs.LG math.IT math.ST stat.TH published:2014-05-05 summary:Large data sets often require performing distributed statistical estimation,with a full data set split across multiple machines and limited communicationbetween machines. To study such scenarios, we define and study some refinementsof the classical minimax risk that apply to distributed settings, comparing tothe performance of estimators with access to the entire data. Lower bounds onthese quantities provide a precise characterization of the minimum amount ofcommunication required to achieve the centralized minimax risk. We study twoclasses of distributed protocols: one in which machines send messagesindependently over channels without feedback, and a second allowing forinteractive communication, in which a central server broadcasts the messagesfrom a given machine to all other machines. We establish lower bounds for avariety of problems, including location estimation in several families andparameter estimation in different types of regression models. Our resultsinclude a novel class of quantitative data-processing inequalities used tocharacterize the effects of limited communication.
arxiv-6900-228 | A Continuous Max-Flow Approach to Multi-Labeling Problems under Arbitrary Region Regularization | http://arxiv.org/pdf/1405.0892v2.pdf | author:John S. H. Baxter, Martin Rajchl, Jing Yuan, Terry M. Peters category:cs.CV published:2014-05-05 summary:The incorporation of region regularization into max-flow segmentation hastraditionally focused on ordering and part-whole relationships. A side effectof the development of such models is that it constrained regularization only tothose cases, rather than allowing for arbitrary region regularization. DirectedAcyclic Graphical Max-Flow (DAGMF) segmentation overcomes these limitations byallowing for the algorithm designer to specify an arbitrary directed acyclicgraph to structure a max-flow segmentation. This allows for individual 'parts'to be a member of multiple distinct 'wholes.'
arxiv-6900-229 | Comparing apples to apples in the evaluation of binary coding methods | http://arxiv.org/pdf/1405.1005v2.pdf | author:Mohammad Rastegari, Shobeir Fakhraei, Jonghyun Choi, David Jacobs, Larry S. Davis category:cs.CV cs.LG published:2014-05-05 summary:We discuss methodological issues related to the evaluation of unsupervisedbinary code construction methods for nearest neighbor search. These issues havebeen widely ignored in literature. These coding methods attempt to preserveeither Euclidean distance or angular (cosine) distance in the binary embeddingspace. We explain why when comparing a method whose goal is preserving cosinesimilarity to one designed for preserving Euclidean distance, the originalfeatures should be normalized by mapping them to the unit hypersphere beforelearning the binary mapping functions. To compare a method whose goal is topreserves Euclidean distance to one that preserves cosine similarity, theoriginal feature data must be mapped to a higher dimension by including a biasterm in binary mapping functions. These conditions ensure the fair comparisonbetween different binary code methods for the task of nearest neighbor search.Our experiments show under these conditions the very simple methods (e.g. LSHand ITQ) often outperform recent state-of-the-art methods (e.g. MDSH andOK-means).
arxiv-6900-230 | Universal Memcomputing Machines | http://arxiv.org/pdf/1405.0931v2.pdf | author:Fabio L. Traversa, Massimiliano Di Ventra category:cs.NE cs.ET cs.IT math.IT published:2014-05-05 summary:We introduce the notion of universal memcomputing machines (UMMs): a class ofbrain-inspired general-purpose computing machines based on systems with memory,whereby processing and storing of information occur on the same physicallocation. We analytically prove that the memory properties of UMMs endow themwith universal computing power - they are Turing-complete -, intrinsicparallelism, functional polymorphism, and information overhead, namely theircollective states can support exponential data compression directly in memory.We also demonstrate that a UMM has the same computational power as anon-deterministic Turing machine, namely it can solve NP--complete problems inpolynomial time. However, by virtue of its information overhead, a UMM needsonly an amount of memory cells (memprocessors) that grows polynomially with theproblem size. As an example we provide the polynomial-time solution of thesubset-sum problem and a simple hardware implementation of the same. Eventhough these results do not prove the statement NP=P within the Turingparadigm, the practical realization of these UMMs would represent a paradigmshift from present von Neumann architectures bringing us closer to brain-likeneural computation.
arxiv-6900-231 | "Translation can't change a name": Using Multilingual Data for Named Entity Recognition | http://arxiv.org/pdf/1405.0701v1.pdf | author:Manaal Faruqui category:cs.CL published:2014-05-04 summary:Named Entities (NEs) are often written with no orthographic changes acrossdifferent languages that share a common alphabet. We show that this can beleveraged so as to improve named entity recognition (NER) by using unsupervisedword clusters from secondary languages as features in state-of-the-artdiscriminative NER systems. We observe significant increases in performance,finding that person and location identification is particularly improved, andthat phylogenetically close languages provide more valuable features than moredistant languages.
arxiv-6900-232 | Rule of Three for Superresolution of Still Images with Applications to Compression and Denoising | http://arxiv.org/pdf/1405.0632v1.pdf | author:Mario Mastriani category:cs.CV published:2014-05-04 summary:We describe a new method for superresolution of still images (in the waveletdomain) based on the reconstruction of missing details subbands pixels at agiven ith level via Rule of Three (Ro3) between pixels of approximation subbandof such level, and pixels of approximation and detail subbands of (i+1)thlevel. The histogramic profiles demonstrate that Ro3 is the appropriatemechanism to recover missing detail subband pixels in these cases. Besides,with the elimination of the details subbands pixels (in an eventual compressionscheme), we obtain a bigger compression rate. Experimental results demonstratethat our approach compares favorably to more typical methods of denoising andcompression in wavelet domain. Our method does not compress, but facilitatesthe action of the real compressor, in our case, Joint Photographic ExpertsGroup (JPEG) and JPEg2000, that is, Ro3 acts as a catalyst compression
arxiv-6900-233 | Automated Attribution and Intertextual Analysis | http://arxiv.org/pdf/1405.0616v1.pdf | author:James Brofos, Ajay Kannan, Rui Shu category:cs.CL cs.DL stat.ML published:2014-05-03 summary:In this work, we employ quantitative methods from the realm of statistics andmachine learning to develop novel methodologies for author attribution andtextual analysis. In particular, we develop techniques and software suitablefor applications to Classical study, and we illustrate the efficacy of ourapproach in several interesting open questions in the field. We apply ournumerical analysis techniques to questions of authorship attribution in thecase of the Greek tragedian Euripides, to instances of intertextuality andinfluence in the poetry of the Roman statesman Seneca the Younger, and to casesof "interpolated" text with respect to the histories of Livy.
arxiv-6900-234 | Extracting Family Relationship Networks from Novels | http://arxiv.org/pdf/1405.0603v1.pdf | author:Aibek Makazhanov, Denilson Barbosa, Grzegorz Kondrak category:cs.CL published:2014-05-03 summary:We present an approach to the extraction of family relations from literarynarrative, which incorporates a technique for utterance attribution proposedrecently by Elson and McKeown (2010). In our work this technique is used incombination with the detection of vocatives - the explicit forms of addressused by the characters in a novel. We take advantage of the fact that certainvocatives indicate family relations between speakers. The extracted relationsare then propagated using a set of rules. We report the results of theapplication of our method to Jane Austen's Pride and Prejudice.
arxiv-6900-235 | Why (and When and How) Contrastive Divergence Works | http://arxiv.org/pdf/1405.0602v1.pdf | author:Ian E Fellows category:stat.ML stat.ME published:2014-05-03 summary:Contrastive divergence (CD) is a promising method of inference in highdimensional distributions with intractable normalizing constants, however, thetheoretical foundations justifying its use are somewhat shaky. This documentproposes a framework for understanding CD inference, how/when it works, andprovides multiple justifications for the CD moment conditions, includingframing them as a variational approximation. Algorithms for performinginference are discussed and are applied to social network data using anexponential-family random graph models (ERGM). The framework also providesguidance about how to construct MCMC kernels providing good CD inference, whichturn out to be quite different from those used typically to provide fast globalmixing.
arxiv-6900-236 | Supervised Descent Method for Solving Nonlinear Least Squares Problems in Computer Vision | http://arxiv.org/pdf/1405.0601v1.pdf | author:Xuehan Xiong, Fernando De la Torre category:cs.CV published:2014-05-03 summary:Many computer vision problems (e.g., camera calibration, image alignment,structure from motion) are solved with nonlinear optimization methods. It isgenerally accepted that second order descent methods are the most robust, fast,and reliable approaches for nonlinear optimization of a general smoothfunction. However, in the context of computer vision, second order descentmethods have two main drawbacks: (1) the function might not be analyticallydifferentiable and numerical approximations are impractical, and (2) theHessian may be large and not positive definite. To address these issues, thispaper proposes generic descent maps, which are average "descent directions" andrescaling factors learned in a supervised fashion. Using generic descent maps,we derive a practical algorithm - Supervised Descent Method (SDM) - forminimizing Nonlinear Least Squares (NLS) problems. During training, SDM learnsa sequence of decent maps that minimize the NLS. In testing, SDM minimizes theNLS objective using the learned descent maps without computing the Jacobian orthe Hessian. We prove the conditions under which the SDM is guaranteed toconverge. We illustrate the effectiveness and accuracy of SDM in three computervision problems: rigid image alignment, non-rigid image alignment, and 3D poseestimation. In particular, we show how SDM achieves state-of-the-artperformance in the problem of facial feature detection. The code has been madeavailable at www.humansensing.cs.cmu.edu/intraface.
arxiv-6900-237 | Perceptron-like Algorithms and Generalization Bounds for Learning to Rank | http://arxiv.org/pdf/1405.0591v1.pdf | author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG stat.ML published:2014-05-03 summary:Learning to rank is a supervised learning problem where the output space isthe space of rankings but the supervision space is the space of relevancescores. We make theoretical contributions to the learning to rank problem bothin the online and batch settings. First, we propose a perceptron-like algorithmfor learning a ranking function in an online setting. Our algorithm is anextension of the classic perceptron algorithm for the classification problem.Second, in the setting of batch learning, we introduce a sufficient conditionfor convex ranking surrogates to ensure a generalization bound that isindependent of number of objects per query. Our bound holds when linear rankingfunctions are used: a common practice in many learning to rank algorithms. Enroute to developing the online algorithm and generalization bound, we propose anovel family of listwise large margin ranking surrogates. Our novel surrogatefamily is obtained by modifying a well-known pairwise large margin rankingsurrogate and is distinct from the listwise large margin surrogates developedusing the structured prediction framework. Using the proposed family, weprovide a guaranteed upper bound on the cumulative NDCG (or MAP) induced lossunder the perceptron-like algorithm. We also show that the novel surrogatessatisfy the generalization bound condition.
arxiv-6900-238 | Application of Machine Learning Techniques in Aquaculture | http://arxiv.org/pdf/1405.1304v1.pdf | author:Akhlaqur Rahman, Sumaira Tasnim category:cs.CE cs.LG published:2014-05-03 summary:In this paper we present applications of different machine learningalgorithms in aquaculture. Machine learning algorithms learn models fromhistorical data. In aquaculture historical data are obtained from farmpractices, yields, and environmental data sources. Associations between thesedifferent variables can be obtained by applying machine learning algorithms tohistorical data. In this paper we present applications of different machinelearning algorithms in aquaculture applications.
arxiv-6900-239 | Spatial Neural Networks and their Functional Samples: Similarities and Differences | http://arxiv.org/pdf/1405.0573v1.pdf | author:Lucas Antiqueira, Liang Zhao category:cs.NE q-bio.NC published:2014-05-03 summary:Models of neural networks have proven their utility in the development oflearning algorithms in computer science and in the theoretical study of braindynamics in computational neuroscience. We propose in this paper a spatialneural network model to analyze the important class of functional networks,which are commonly employed in computational studies of clinical brain imagingtime series. We developed a simulation framework inspired by multichannel brainsurface recordings (more specifically, EEG -- electroencephalogram) in order tolink the mesoscopic network dynamics (represented by sampled functionalnetworks) and the microscopic network structure (represented by anintegrate-and-fire neural network located in a 3D space -- hence the termspatial neural network). Functional networks are obtained by computing pairwisecorrelations between time-series of mesoscopic electric potential dynamics,which allows the construction of a graph where each node represents onetime-series. The spatial neural network model is central in this study in thesense that it allowed us to characterize sampled functional networks in termsof what features they are able to reproduce from the underlying spatialnetwork. Our modeling approach shows that, in specific conditions of samplesize and edge density, it is possible to precisely estimate several networkmeasurements of spatial networks by just observing functional samples.
arxiv-6900-240 | Classification of Diabetes Mellitus using Modified Particle Swarm Optimization and Least Squares Support Vector Machine | http://arxiv.org/pdf/1405.0549v1.pdf | author:Omar S. Soliman, Eman AboElhamd category:cs.CE cs.NE published:2014-05-03 summary:Diabetes Mellitus is a major health problem all over the world. Manyclassification algorithms have been applied for its diagnoses and treatment. Inthis paper, a hybrid algorithm of Modified-Particle Swarm Optimization andLeast Squares- Support Vector Machine is proposed for the classification oftype II DM patients. LS-SVM algorithm is used for classification by findingoptimal hyper-plane which separates various classes. Since LS-SVM is sosensitive to the changes of its parameter values, Modified-PSO algorithm isused as an optimization technique for LS-SVM parameters. This will Guaranteethe robustness of the hybrid algorithm by searching for the optimal values forLS-SVM parameters. The pro-posed Algorithm is implemented and evaluated usingPima Indians Diabetes Data set from UCI repository of machine learningdatabases. It is also compared with different classifier algorithms which wereapplied on the same database. The experimental results showed the superiorityof the proposed algorithm which could achieve an average classificationaccuracy of 97.833%.
arxiv-6900-241 | A Network Intrusions Detection System based on a Quantum Bio Inspired Algorithm | http://arxiv.org/pdf/1405.1404v1.pdf | author:Omar S. Soliman, Aliaa Rassem category:cs.NE cs.CR published:2014-05-03 summary:Network intrusion detection systems (NIDSs) have a role of identifyingmalicious activities by monitoring the behavior of networks. Due to thecurrently high volume of networks trafic in addition to the increased number ofattacks and their dynamic properties, NIDSs have the challenge of improvingtheir classification performance. Bio-Inspired Optimization Algorithms (BIOs)are used to automatically extract the the discrimination rules of normal orabnormal behavior to improve the classification accuracy and the detectionability of NIDS. A quantum vaccined immune clonal algorithm with the estimationof distribution algorithm (QVICA-with EDA) is proposed in this paper to build anew NIDS. The proposed algorithm is used as classification algorithm of the newNIDS where it is trained and tested using the KDD data set. Also, the new NIDSis compared with another detection system based on particle swarm optimization(PSO). Results shows the ability of the proposed algorithm of achieving highintrusions classification accuracy where the highest obtained accuracy is 94.8%.
arxiv-6900-242 | Optimal measurement of visual motion across spatial and temporal scales | http://arxiv.org/pdf/1405.0545v1.pdf | author:Sergei Gepshtein, Ivan Tyukin category:cs.CV q-bio.NC published:2014-05-03 summary:Sensory systems use limited resources to mediate the perception of a greatvariety of objects and events. Here a normative framework is presented forexploring how the problem of efficient allocation of resources can be solved invisual perception. Starting with a basic property of every measurement,captured by Gabor's uncertainty relation about the location and frequencycontent of signals, prescriptions are developed for optimal allocation ofsensors for reliable perception of visual motion. This study reveals that alarge-scale characteristic of human vision (the spatiotemporal contrastsensitivity function) is similar to the optimal prescription, and it suggeststhat some previously puzzling phenomena of visual sensitivity, adaptation, andperceptual organization have simple principled explanations.
arxiv-6900-243 | On Lipschitz Continuity and Smoothness of Loss Functions in Learning to Rank | http://arxiv.org/pdf/1405.0586v2.pdf | author:Ambuj Tewari, Sougata Chaudhuri category:cs.LG stat.ML published:2014-05-03 summary:In binary classification and regression problems, it is well understood thatLipschitz continuity and smoothness of the loss function play key roles ingoverning generalization error bounds for empirical risk minimizationalgorithms. In this paper, we show how these two properties affectgeneralization error bounds in the learning to rank problem. The learning torank problem involves vector valued predictions and therefore the choice of thenorm with respect to which Lipschitz continuity and smoothness are definedbecomes crucial. Choosing the $\ell_\infty$ norm in our definition of Lipschitzcontinuity allows us to improve existing bounds. Furthermore, under smoothnessassumptions, our choice enables us to prove rates that interpolate between$1/\sqrt{n}$ and $1/n$ rates. Application of our results to ListNet, a popularlearning to rank method, gives state-of-the-art performance guarantees.
arxiv-6900-244 | The Falling Factorial Basis and Its Statistical Applications | http://arxiv.org/pdf/1405.0558v2.pdf | author:Yu-Xiang Wang, Alex Smola, Ryan J. Tibshirani category:stat.ML published:2014-05-03 summary:We study a novel spline-like basis, which we name the "falling factorialbasis", bearing many similarities to the classic truncated power basis. Theadvantage of the falling factorial basis is that it enables rapid, linear-timecomputations in basis matrix multiplication and basis matrix inversion. Thefalling factorial functions are not actually splines, but are close enough tosplines that they provably retain some of the favorable properties of thelatter functions. We examine their application in two problems: trend filteringover arbitrary input points, and a higher-order variant of the two-sampleKolmogorov-Smirnov test.
arxiv-6900-245 | Kaggle LSHTC4 Winning Solution | http://arxiv.org/pdf/1405.0546v2.pdf | author:Antti Puurula, Jesse Read, Albert Bifet category:cs.AI cs.CL cs.IR published:2014-05-03 summary:Our winning submission to the 2014 Kaggle competition for Large ScaleHierarchical Text Classification (LSHTC) consists mostly of an ensemble ofsparse generative models extending Multinomial Naive Bayes. Thebase-classifiers consist of hierarchically smoothed models combining document,label, and hierarchy level Multinomials, with feature pre-processing usingvariants of TF-IDF and BM25. Additional diversification is introduced bydifferent types of folds and random search optimization for different measures.The ensemble algorithm optimizes macroFscore by predicting the documents foreach label, instead of the usual prediction of labels per document. Scores fordocuments are predicted by weighted voting of base-classifier outputs with avariant of Feature-Weighted Linear Stacking. The number of documents per labelis chosen using label priors and thresholding of vote scores. This documentdescribes the models and software used to build our solution. Reproducing theresults for our solution can be done by running the scripts included in theKaggle package. A package omitting precomputed result files is alsodistributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0for Weka and Meka dependencies.
arxiv-6900-246 | Asymptotic Theory for Random Forests | http://arxiv.org/pdf/1405.0352v2.pdf | author:Stefan Wager category:math.ST stat.ML stat.TH published:2014-05-02 summary:Random forests have proven to be reliable predictive algorithms in manyapplication areas. Not much is known, however, about the statistical propertiesof random forests. Several authors have established conditions under whichtheir predictions are consistent, but these results do not provide practicalestimates of random forest errors. In this paper, we analyze a random forestmodel based on subsampling, and show that random forest predictions areasymptotically normal provided that the subsample size s scales as s(n)/n =o(log(n)^{-d}), where n is the number of training examples and d is the numberof features. Moreover, we show that the asymptotic variance can consistently beestimated using an infinitesimal jackknife for bagged ensembles recentlyproposed by Efron (2014). In other words, our results let us both characterizeand estimate the error-distribution of random forest predictions, thus taking astep towards making random forests tools for statistical inference instead ofjust black-box predictive algorithms.
arxiv-6900-247 | Complexity of Equivalence and Learning for Multiplicity Tree Automata | http://arxiv.org/pdf/1405.0514v2.pdf | author:Ines Marusic, James Worrell category:cs.LG cs.FL published:2014-05-02 summary:We consider the complexity of equivalence and learning for multiplicity treeautomata, i.e., weighted tree automata over a field. We first show that theequivalence problem is logspace equivalent to polynomial identity testing, thecomplexity of which is a longstanding open problem. Secondly, we derive lowerbounds on the number of queries needed to learn multiplicity tree automata inAngluin's exact learning model, over both arbitrary and fixed fields. Habrard and Oncina (2006) give an exact learning algorithm for multiplicitytree automata, in which the number of queries is proportional to the size ofthe target automaton and the size of a largest counterexample, represented as atree, that is returned by the Teacher. However, the smallesttree-counterexample may be exponential in the size of the target automaton.Thus the above algorithm does not run in time polynomial in the size of thetarget automaton, and has query complexity exponential in the lower bound. Assuming a Teacher that returns minimal DAG representations ofcounterexamples, we give a new exact learning algorithm whose query complexityis quadratic in the target automaton size, almost matching the lower bound, andimproving the best previously-known algorithm by an exponential factor.
arxiv-6900-248 | Exchangeable Variable Models | http://arxiv.org/pdf/1405.0501v1.pdf | author:Mathias Niepert, Pedro Domingos category:cs.LG cs.AI published:2014-05-02 summary:A sequence of random variables is exchangeable if its joint distribution isinvariant under variable permutations. We introduce exchangeable variablemodels (EVMs) as a novel class of probabilistic models whose basic buildingblocks are partially exchangeable sequences, a generalization of exchangeablesequences. We prove that a family of tractable EVMs is optimal under zero-oneloss for a large class of functions, including parity and threshold functions,and strictly subsumes existing tractable independence-based model families.Extensive experiments show that EVMs outperform state of the art classifierssuch as SVMs and probabilistic models which are solely based on independenceassumptions.
arxiv-6900-249 | A Rank-SVM Approach to Anomaly Detection | http://arxiv.org/pdf/1405.0530v1.pdf | author:Jing Qian, Jonathan Root, Venkatesh Saligrama, Yuting Chen category:stat.ML published:2014-05-02 summary:We propose a novel non-parametric adaptive anomaly detection algorithm forhigh dimensional data based on rank-SVM. Data points are first ranked based onscores derived from nearest neighbor graphs on n-point nominal data. We thentrain a rank-SVM using this ranked data. A test-point is declared as an anomalyat alpha-false alarm level if the predicted score is in the alpha-percentile.The resulting anomaly detector is shown to be asymptotically optimal andadaptive in that for any false alarm rate alpha, its decision region convergesto the alpha-percentile level set of the unknown underlying density. Inaddition we illustrate through a number of synthetic and real-data experimentsboth the statistical performance and computational efficiency of our anomalydetector.
arxiv-6900-250 | Fast MLE Computation for the Dirichlet Multinomial | http://arxiv.org/pdf/1405.0099v1.pdf | author:Max Sklar category:stat.ML cs.LG published:2014-05-01 summary:Given a collection of categorical data, we want to find the parameters of aDirichlet distribution which maximizes the likelihood of that data. Newton'smethod is typically used for this purpose but current implementations requirereading through the entire dataset on each iteration. In this paper, we proposea modification which requires only a single pass through the dataset andsubstantially decreases running time. Furthermore we analyze both theoreticallyand empirically the performance of the proposed algorithm, and provide an opensource implementation.
arxiv-6900-251 | A Structural Approach to Coordinate-Free Statistics | http://arxiv.org/pdf/1405.0110v2.pdf | author:Tom LaGatta, P. Richard Hahn category:math.PR math.FA math.ST stat.ML stat.TH published:2014-05-01 summary:We consider the question of learning in general topological vector spaces. Byexploiting known (or parametrized) covariance structures, our Main Theoremdemonstrates that any continuous linear map corresponds to a certainisomorphism of embedded Hilbert spaces. By inverting this isomorphism andextending continuously, we construct a version of the Ordinary Least Squaresestimator in absolute generality. Our Gauss-Markov theorem demonstrates thatOLS is a "best linear unbiased estimator", extending the classical result. Weconstruct a stochastic version of the OLS estimator, which is a continuousdisintegration exactly for the class of "uncorrelated implies independent"(UII) measures. As a consequence, Gaussian measures always exhibit continuousdisintegrations through continuous linear maps, extending a theorem of thefirst author. Applying this framework to some problems in machine learning, weprove a useful representation theorem for covariance tensors, and show that OLSdefines a good kriging predictor for vector-valued arrays on general indexspaces. We also construct a support-vector machine classifier in this setting.We hope that our article shines light on some deeper connections betweenprobability theory, statistics and machine learning, and may serve as a pointof intersection for these three communities.
arxiv-6900-252 | Relative Facial Action Unit Detection | http://arxiv.org/pdf/1405.0085v1.pdf | author:Mahmoud Khademi, Louis-Philippe Morency category:cs.CV published:2014-05-01 summary:This paper presents a subject-independent facial action unit (AU) detectionmethod by introducing the concept of relative AU detection, for scenarios wherethe neutral face is not provided. We propose a new classification objectivefunction which analyzes the temporal neighborhood of the current frame todecide if the expression recently increased, decreased or showed no change.This approach is a significant change from the conventional absolute methodwhich decides about AU classification using the current frame, without anexplicit comparison with its neighboring frames. Our proposed method improvesrobustness to individual differences such as face scale and shape, age-relatedwrinkles, and transitions among expressions (e.g., lower intensity ofexpressions). Our experiments on three publicly available datasets (ExtendedCohn-Kanade (CK+), Bosphorus, and DISFA databases) show significant improvementof our approach over conventional absolute techniques. Keywords: facial actioncoding system (FACS); relative facial action unit detection; temporalinformation;
arxiv-6900-253 | Retrieval in Long Surveillance Videos using User Described Motion and Object Attributes | http://arxiv.org/pdf/1405.0234v1.pdf | author:Greg Castanon, Mohamed Elgharib, Venkatesh Saligrama, Pierre-Marc Jodoin category:cs.CV published:2014-05-01 summary:We present a content-based retrieval method for long surveillance videos bothfor wide-area (Airborne) as well as near-field imagery (CCTV). Our goal is toretrieve video segments, with a focus on detecting objects moving on routes,that match user-defined events of interest. The sheer size and remote locationswhere surveillance videos are acquired, necessitates highly compressedrepresentations that are also meaningful for supporting user-defined queries.To address these challenges we archive long-surveillance video throughlightweight processing based on low-level local spatio-temporal extraction ofmotion and object features. These are then hashed into an inverted index usinglocality-sensitive hashing (LSH). This local approach allows for queryflexibility as well as leads to significant gains in compression. Our secondtask is to extract partial matches to the user-created query and assembles theminto full matches using Dynamic Programming (DP). DP exploits causality toassemble the indexed low level features into a video segment which matches thequery route. We examine CCTV and Airborne footage, whose low contrast makesmotion extraction more difficult. We generate robust motion estimates forAirborne data using a tracklets generation algorithm while we use Horn andSchunck approach to generate motion estimates for CCTV. Our approach handleslong routes, low contrasts and occlusion. We derive bounds on the rate of falsepositives and demonstrate the effectiveness of the approach for counting,motion pattern recognition and abandoned object applications.
arxiv-6900-254 | Contextual Semantic Parsing using Crowdsourced Spatial Descriptions | http://arxiv.org/pdf/1405.0145v1.pdf | author:Kais Dukes category:cs.CL published:2014-05-01 summary:We describe a contextual parser for the Robot Commands Treebank, a newcrowdsourced resource. In contrast to previous semantic parsers that select themost-probable parse, we consider the different problem of parsing usingadditional situational context to disambiguate between different readings of asentence. We show that multiple semantic analyses can be searched using dynamicprogramming via interaction with a spatial planner, to guide the parsingprocess. We are able to parse sentences in near linear-time by ruling outanalyses early on that are incompatible with spatial context. We report a 34%upper bound on accuracy, as our planner correctly processes spatial context for3,394 out of 10,000 sentences. However, our parser achieves a 96.53%exact-match score for parsing within the subset of sentences recognized by theplanner, compared to 82.14% for a non-contextual parser.
arxiv-6900-255 | Geodesic Distance Function Learning via Heat Flow on Vector Fields | http://arxiv.org/pdf/1405.0133v2.pdf | author:Binbin Lin, Ji Yang, Xiaofei He, Jieping Ye category:cs.LG math.DG stat.ML published:2014-05-01 summary:Learning a distance function or metric on a given data manifold is of greatimportance in machine learning and pattern recognition. Many of the previousworks first embed the manifold to Euclidean space and then learn the distancefunction. However, such a scheme might not faithfully preserve the distancefunction if the original manifold is not Euclidean. Note that the distancefunction on a manifold can always be well-defined. In this paper, we propose tolearn the distance function directly on the manifold without embedding. Wefirst provide a theoretical characterization of the distance function by itsgradient field. Based on our theoretical analysis, we propose to first learnthe gradient field of the distance function and then learn the distancefunction itself. Specifically, we set the gradient field of a local distancefunction as an initial vector field. Then we transport it to the whole manifoldvia heat flow on vector fields. Finally, the geodesic distance function can beobtained by requiring its gradient field to be close to the normalized vectorfield. Experimental results on both synthetic and real data demonstrate theeffectiveness of our proposed algorithm.
arxiv-6900-256 | VSCAN: An Enhanced Video Summarization using Density-based Spatial Clustering | http://arxiv.org/pdf/1405.0174v1.pdf | author:Karim M. Mohamed, Mohamed A. Ismail, Nagia M. Ghanem category:cs.CV cs.MM published:2014-05-01 summary:In this paper, we present VSCAN, a novel approach for generating static videosummaries. This approach is based on a modified DBSCAN clustering algorithm tosummarize the video content utilizing both color and texture features of thevideo frames. The paper also introduces an enhanced evaluation method thatdepends on color and texture features. Video Summaries generated by VSCAN arecompared with summaries generated by other approaches found in the literatureand those created by users. Experimental results indicate that the videosummaries generated by VSCAN have a higher quality than those generated byother approaches.
arxiv-6900-257 | Microsoft COCO: Common Objects in Context | http://arxiv.org/pdf/1405.0312v3.pdf | author:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Dollár category:cs.CV published:2014-05-01 summary:We present a new dataset with the goal of advancing the state-of-the-art inobject recognition by placing the question of object recognition in the contextof the broader question of scene understanding. This is achieved by gatheringimages of complex everyday scenes containing common objects in their naturalcontext. Objects are labeled using per-instance segmentations to aid in preciseobject localization. Our dataset contains photos of 91 objects types that wouldbe easily recognizable by a 4 year old. With a total of 2.5 million labeledinstances in 328k images, the creation of our dataset drew upon extensive crowdworker involvement via novel user interfaces for category detection, instancespotting and instance segmentation. We present a detailed statistical analysisof the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we providebaseline performance analysis for bounding box and segmentation detectionresults using a Deformable Parts Model.
arxiv-6900-258 | Exemplar Dynamics Models of the Stability of Phonological Categories | http://arxiv.org/pdf/1405.0049v2.pdf | author:P. F. Tupper category:cs.CL cs.SD 91F20 published:2014-04-30 summary:We develop a model for the stability and maintenance of phonologicalcategories. Examples of phonological categories are vowel sounds such as "i"and "e". We model such categories as consisting of collections of labeledexemplars that language users store in their memory. Each exemplar is adetailed memory of an instance of the linguistic entity in question. Startingfrom an exemplar-level model we derive integro-differential equations for thelong-term evolution of the density of exemplars in different portions ofphonetic space. Using these latter equations we investigate under whatconditions two phonological categories merge or not. Our main conclusion isthat for the preservation of distinct phonological categories, it is necessarythat anomalous speech tokens of a given category are discarded, and not merelystored in memory as an exemplar of another category.
arxiv-6900-259 | Deep Learning in Neural Networks: An Overview | http://arxiv.org/pdf/1404.7828v4.pdf | author:Juergen Schmidhuber category:cs.NE cs.LG published:2014-04-30 summary:In recent years, deep artificial neural networks (including recurrent ones)have won numerous contests in pattern recognition and machine learning. Thishistorical survey compactly summarises relevant work, much of it from theprevious millennium. Shallow and deep learners are distinguished by the depthof their credit assignment paths, which are chains of possibly learnable,causal links between actions and effects. I review deep supervised learning(also recapitulating the history of backpropagation), unsupervised learning,reinforcement learning & evolutionary computation, and indirect search forshort programs encoding deep and large networks.
arxiv-6900-260 | Learning with incremental iterative regularization | http://arxiv.org/pdf/1405.0042v2.pdf | author:Lorenzo Rosasco, Silvia Villa category:stat.ML cs.LG math.OC math.PR published:2014-04-30 summary:Within a statistical learning setting, we propose and study an iterativeregularization algorithm for least squares defined by an incremental gradientmethod. In particular, we show that, if all other parameters are fixed apriori, the number of passes over the data (epochs) acts as a regularizationparameter, and prove strong universal consistency, i.e. almost sure convergenceof the risk, as well as sharp finite sample bounds for the iterates. Ourresults are a step towards understanding the effect of multiple epochs instochastic gradient techniques in machine learning and rely on integratingstatistical and optimization results.
arxiv-6900-261 | Phase transitions in semisupervised clustering of sparse networks | http://arxiv.org/pdf/1404.7789v1.pdf | author:Pan Zhang, Cristopher Moore, Lenka Zdeborová category:cs.SI physics.soc-ph stat.ML published:2014-04-30 summary:Predicting labels of nodes in a network, such as community memberships ordemographic variables, is an important problem with applications in social andbiological networks. A recently-discovered phase transition puts fundamentallimits on the accuracy of these predictions if we have access only to thenetwork topology. However, if we know the correct labels of some fraction$\alpha$ of the nodes, we can do better. We study the phase diagram of this"semisupervised" learning problem for networks generated by the stochasticblock model. We use the cavity method and the associated belief propagationalgorithm to study what accuracy can be achieved as a function of $\alpha$. For$k = 2$ groups, we find that the detectability transition disappears for any$\alpha > 0$, in agreement with previous work. For larger $k$ where a hard butdetectable regime exists, we find that the easy/hard transition (the point atwhich efficient algorithms can do better than chance) becomes a line oftransitions where the accuracy jumps discontinuously at a critical value of$\alpha$. This line ends in a critical point with a second-order transition,beyond which the accuracy is a continuous function of $\alpha$. We demonstratequalitatively similar transitions in two real-world networks.
arxiv-6900-262 | A Bi-clustering Framework for Consensus Problems | http://arxiv.org/pdf/1405.6159v3.pdf | author:Mariano Tepper, Guillermo Sapiro category:cs.CV cs.LG stat.ML published:2014-04-30 summary:We consider grouping as a general characterization for problems such asclustering, community detection in networks, and multiple parametric modelestimation. We are interested in merging solutions from different groupingalgorithms, distilling all their good qualities into a consensus solution. Inthis paper, we propose a bi-clustering framework and perspective for reachingconsensus in such grouping problems. In particular, this is the first time thatthe task of finding/fitting multiple parametric models to a dataset is formallyposed as a consensus problem. We highlight the equivalence of these tasks andestablish the connection with the computational Gestalt program, that seeks toprovide a psychologically-inspired detection theory for visual events. We alsopresent a simple but powerful bi-clustering algorithm, specially tuned to thenature of the problem we address, though general enough to handle manydifferent instances inscribed within our characterization. The presentation isaccompanied with diverse and extensive experimental results in clustering,community detection, and multiple parametric model estimation in imageprocessing applications.
arxiv-6900-263 | Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction | http://arxiv.org/pdf/1405.0006v1.pdf | author:Moritz Kassner, William Patera, Andreas Bulling category:cs.CV cs.HC published:2014-04-30 summary:Commercial head-mounted eye trackers provide useful features to customers inindustry and research but are expensive and rely on closed source hardware andsoftware. This limits the application areas and use of mobile eye tracking toexpert users and inhibits user-driven development, customisation, andextension. In this paper we present Pupil -- an accessible, affordable, andextensible open source platform for mobile eye tracking and gaze-basedinteraction. Pupil comprises 1) a light-weight headset with high-resolutioncameras, 2) an open source software framework for mobile eye tracking, as wellas 3) a graphical user interface (GUI) to playback and visualize video and gazedata. Pupil features high-resolution scene and eye cameras for monocular andbinocular gaze estimation. The software and GUI are platform-independent andinclude state-of-the-art algorithms for real-time pupil detection and tracking,calibration, and accurate gaze estimation. Results of a performance evaluationshow that Pupil can provide an average gaze estimation accuracy of 0.6 degreeof visual angle (0.08 degree precision) with a latency of the processingpipeline of only 0.045 seconds.
arxiv-6900-264 | A graph-based mathematical morphology reader | http://arxiv.org/pdf/1404.7748v1.pdf | author:Laurent Najman, Jean Cousty category:cs.CV published:2014-04-30 summary:This survey paper aims at providing a "literary" anthology of mathematicalmorphology on graphs. It describes in the English language many ideas stemmingfrom a large number of different papers, hence providing a unified view of anactive and diverse field of research.
arxiv-6900-265 | A semantic network-based evolutionary algorithm for computational creativity | http://arxiv.org/pdf/1404.7765v2.pdf | author:Atilim Gunes Baydin, Ramon Lopez de Mantaras, Santiago Ontanon category:cs.NE published:2014-04-30 summary:We introduce a novel evolutionary algorithm (EA) with a semanticnetwork-based representation. For enabling this, we establish new formulationsof EA variation operators, crossover and mutation, that we adapt to work onsemantic networks. The algorithm employs commonsense reasoning to ensure alloperations preserve the meaningfulness of the networks, using ConceptNet andWordNet knowledge bases. The algorithm can be interpreted as a novel memeticalgorithm (MA), given that (1) individuals represent pieces of information thatundergo evolution, as in the original sense of memetics as it was introduced byDawkins; and (2) this is different from existing MA, where the word "memetic"has been used as a synonym for local refinement after global optimization. Forevaluating the approach, we introduce an analogical similarity-based fitnessmeasure that is computed through structure mapping. This setup enables theopen-ended generation of networks analogous to a given base network.
arxiv-6900-266 | Gabor Filter and Rough Clustering Based Edge Detection | http://arxiv.org/pdf/1405.0921v1.pdf | author:Chandranath Adak category:cs.CV cs.AI published:2014-04-30 summary:This paper introduces an efficient edge detection method based on Gaborfilter and rough clustering. The input image is smoothed by Gabor function, andthe concept of rough clustering is used to focus on edge detection with softcomputational approach. Hysteresis thresholding is used to get the actualoutput, i.e. edges of the input image. To show the effectiveness, the proposedtechnique is compared with some other edge detection methods.
arxiv-6900-267 | Image Compressive Sensing Recovery Using Adaptively Learned Sparsifying Basis via L0 Minimization | http://arxiv.org/pdf/1404.7566v1.pdf | author:Jian Zhang, Chen Zhao, Debin Zhao, Wen Gao category:cs.CV published:2014-04-30 summary:From many fewer acquired measurements than suggested by the Nyquist samplingtheory, compressive sensing (CS) theory demonstrates that, a signal can bereconstructed with high probability when it exhibits sparsity in some domain.Most of the conventional CS recovery approaches, however, exploited a set offixed bases (e.g. DCT, wavelet and gradient domain) for the entirety of asignal, which are irrespective of the non-stationarity of natural signals andcannot achieve high enough degree of sparsity, thus resulting in poor CSrecovery performance. In this paper, we propose a new framework for imagecompressive sensing recovery using adaptively learned sparsifying basis via L0minimization. The intrinsic sparsity of natural images is enforcedsubstantially by sparsely representing overlapped image patches using theadaptively learned sparsifying basis in the form of L0 norm, greatly reducingblocking artifacts and confining the CS solution space. To make our proposedscheme tractable and robust, a split Bregman iteration based technique isdeveloped to solve the non-convex L0 minimization problem efficiently.Experimental results on a wide range of natural images for CS recovery haveshown that our proposed algorithm achieves significant performance improvementsover many current state-of-the-art schemes and exhibits good convergenceproperty.
arxiv-6900-268 | Selecting a Small Set of Optimal Gestures from an Extensive Lexicon | http://arxiv.org/pdf/1404.7594v1.pdf | author:Jacob Grosek, J. Nathan Kutz category:cs.CV published:2014-04-30 summary:Finding the best set of gestures to use for a given computer recognitionproblem is an essential part of optimizing the recognition performance whilebeing mindful to those who may articulate the gestures. An objective function,called the ellipsoidal distance ratio metric (EDRM), for determining the bestgestures from a larger lexicon library is presented, along with a numericalmethod for incorporating subjective preferences. In particular, we demonstratean efficient algorithm that chooses the best $n$ gestures from a lexicon of $m$gestures where typically $n \ll m$ using a weighting of both subjective andobjective measures.
arxiv-6900-269 | High-Speed Tracking with Kernelized Correlation Filters | http://arxiv.org/pdf/1404.7584v3.pdf | author:João F. Henriques, Rui Caseiro, Pedro Martins, Jorge Batista category:cs.CV published:2014-04-30 summary:The core component of most modern trackers is a discriminative classifier,tasked with distinguishing between the target and the surrounding environment.To cope with natural image changes, this classifier is typically trained withtranslated and scaled sample patches. Such sets of samples are riddled withredundancies -- any overlapping pixels are constrained to be the same. Based onthis simple observation, we propose an analytic model for datasets of thousandsof translated patches. By showing that the resulting data matrix is circulant,we can diagonalize it with the Discrete Fourier Transform, reducing bothstorage and computation by several orders of magnitude. Interestingly, forlinear regression our formulation is equivalent to a correlation filter, usedby some of the fastest competitive trackers. For kernel regression, however, wederive a new Kernelized Correlation Filter (KCF), that unlike other kernelalgorithms has the exact same complexity as its linear counterpart. Building onit, we also propose a fast multi-channel extension of linear correlationfilters, via a linear kernel, which we call Dual Correlation Filter (DCF). BothKCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50videos benchmark, despite running at hundreds of frames-per-second, and beingimplemented in a few lines of code (Algorithm 1). To encourage furtherdevelopments, our tracking framework was made open-source.
arxiv-6900-270 | Majority Vote of Diverse Classifiers for Late Fusion | http://arxiv.org/pdf/1404.7796v2.pdf | author:Emilie Morvant, Amaury Habrard, Stéphane Ayache category:stat.ML cs.LG cs.MM published:2014-04-30 summary:In the past few years, a lot of attention has been devoted to multimediaindexing by fusing multimodal informations. Two kinds of fusion schemes aregenerally considered: The early fusion and the late fusion. We focus on lateclassifier fusion, where one combines the scores of each modality at thedecision level. To tackle this problem, we investigate a recent and elegantwell-founded quadratic program named MinCq coming from the machine learningPAC-Bayesian theory. MinCq looks for the weighted combination, over a set ofreal-valued functions seen as voters, leading to the lowest misclassificationrate, while maximizing the voters' diversity. We propose an extension of MinCqtailored to multimedia indexing. Our method is based on an order-preservingpairwise loss adapted to ranking that allows us to improve Mean AveragedPrecision measure while taking into account the diversity of the voters that wewant to fuse. We provide evidence that this method is naturally adapted to latefusion procedures and confirm the good behavior of our approach on thechallenging PASCAL VOC'07 benchmark.
arxiv-6900-271 | Dynamic Mode Decomposition for Real-Time Background/Foreground Separation in Video | http://arxiv.org/pdf/1404.7592v1.pdf | author:Jacob Grosek, J. Nathan Kutz category:cs.CV published:2014-04-30 summary:This paper introduces the method of dynamic mode decomposition (DMD) forrobustly separating video frames into background (low-rank) and foreground(sparse) components in real-time. The method is a novel application of atechnique used for characterizing nonlinear dynamical systems in anequation-free manner by decomposing the state of the system into low-rank termswhose Fourier components in time are known. DMD terms with Fourier frequenciesnear the origin (zero-modes) are interpreted as background (low-rank) portionsof the given video frames, and the terms with Fourier frequencies bounded awayfrom the origin are their sparse counterparts. An approximate low-rank/sparseseparation is achieved at the computational cost of just one singular valuedecomposition and one linear equation solve, thus producing results orders ofmagnitude faster than a leading separation method, namely robust principalcomponent analysis (RPCA). The DMD method that is developed here isdemonstrated to work robustly in real-time with personal laptop-class computingpower and without any parameter tuning, which is a transformative improvementin performance that is ideal for video surveillance and recognitionapplications.
arxiv-6900-272 | Code Minimization for Fringe Projection Based 3D Stereo Sensors by Calibration Improvement | http://arxiv.org/pdf/1404.7298v1.pdf | author:Christian Bräuer-Burchardt, Peter Kühmstedt, Gunther Notni category:math.MG cs.CV published:2014-04-29 summary:Code minimization provides a speed-up of the processing time of fringeprojection based stereo sensors and possibly makes them real-time applicable.This paper reports a methodology which enables such sensors to completely omitGray code or other additional code. Only a sequence of sinusoidal images isnecessary. The code reduction is achieved by involvement of the projection unitinto the measurement, double triangulation, and a precise projector calibrationor significant projector calibration improvement, respectively.
arxiv-6900-273 | Implementing spectral methods for hidden Markov models with real-valued emissions | http://arxiv.org/pdf/1404.7472v1.pdf | author:Carl Mattfeld category:cs.LG published:2014-04-29 summary:Hidden Markov models (HMMs) are widely used statistical models for modelingsequential data. The parameter estimation for HMMs from time series data is animportant learning problem. The predominant methods for parameter estimationare based on local search heuristics, most notably the expectation-maximization(EM) algorithm. These methods are prone to local optima and oftentimes sufferfrom high computational and sample complexity. Recent years saw the emergenceof spectral methods for the parameter estimation of HMMs, based on a method ofmoments approach. Two spectral learning algorithms as proposed by Hsu, Kakadeand Zhang 2012 (arXiv:0811.4413) and Anandkumar, Hsu and Kakade 2012(arXiv:1203.0683) are assessed in this work. Using experiments with syntheticdata, the algorithms are compared with each other. Furthermore, the spectralmethods are compared to the Baum-Welch algorithm, a well-established methodapplying the EM algorithm to HMMs. The spectral algorithms are found to have amuch more favorable computational and sample complexity. Even though thealgorithms readily handle high dimensional observation spaces, instabilityissues are encountered in this regime. In view of learning from real-worldexperimental data, the representation of real-valued observations for the usein spectral methods is discussed, presenting possible methods to represent datafor the use in the learning algorithms.
arxiv-6900-274 | Concise comparative summaries (CCS) of large text corpora with a human experiment | http://arxiv.org/pdf/1404.7362v1.pdf | author:Jinzhu Jia, Luke Miratrix, Bin Yu, Brian Gawalt, Laurent El Ghaoui, Luke Barnesmoore, Sophie Clavier category:cs.CL stat.AP published:2014-04-29 summary:In this paper we propose a general framework for topic-specific summarizationof large text corpora and illustrate how it can be used for the analysis ofnews databases. Our framework, concise comparative summarization (CCS), isbuilt on sparse classification methods. CCS is a lightweight and flexible toolthat offers a compromise between simple word frequency based methods currentlyin wide use and more heavyweight, model-intensive methods such as latentDirichlet allocation (LDA). We argue that sparse methods have much to offer fortext analysis and hope CCS opens the door for a new branch of research in thisimportant field. For a particular topic of interest (e.g., China or energy),CSS automatically labels documents as being either on- or off-topic (usuallyvia keyword search), and then uses sparse classification methods to predictthese labels with the high-dimensional counts of all the other words andphrases in the documents. The resulting small set of phrases found aspredictive are then harvested as the summary. To validate our tool, we, usingnews articles from the New York Times international section, designed andconducted a human survey to compare the different summarizers with humanunderstanding. We demonstrate our approach with two case studies, a mediaanalysis of the framing of "Egypt" in the New York Times throughout the ArabSpring and an informal comparison of the New York Times' and Wall StreetJournal's coverage of "energy." Overall, we find that the Lasso with $L^2$normalization can be effectively and usefully used to summarize large corpora,regardless of document size.
arxiv-6900-275 | Fast Approximation of Rotations and Hessians matrices | http://arxiv.org/pdf/1404.7195v1.pdf | author:Michael Mathieu, Yann LeCun category:cs.LG published:2014-04-29 summary:A new method to represent and approximate rotation matrices is introduced.The method represents approximations of a rotation matrix $Q$ with linearithmiccomplexity, i.e. with $\frac{1}{2}n\lg(n)$ rotations over pairs of coordinates,arranged in an FFT-like fashion. The approximation is "learned" using gradientdescent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ isa diagonal matrix. It can be used to approximate covariance matrix of Gaussianmodels in order to speed up inference, or to estimate and track the inverseHessian of an objective function by relating changes in parameters to changesin gradient along the trajectory followed by the optimization procedure.Experiments were conducted to approximate synthetic matrices, covariancematrices of real data, and Hessian matrices of objective functions involved inmachine learning problems.
arxiv-6900-276 | Randomized Sketches of Convex Programs with Sharp Guarantees | http://arxiv.org/pdf/1404.7203v1.pdf | author:Mert Pilanci, Martin J. Wainwright category:cs.IT cs.DS math.IT math.OC stat.ML published:2014-04-29 summary:Random projection (RP) is a classical technique for reducing storage andcomputational costs. We analyze RP-based approximations of convex programs, inwhich the original optimization problem is approximated by the solution of alower-dimensional problem. Such dimensionality reduction is essential incomputation-limited settings, since the complexity of general convexprogramming can be quite high (e.g., cubic for quadratic programs, andsubstantially higher for semidefinite programs). In addition to computationalsavings, random projection is also useful for reducing memory usage, and hasuseful properties for privacy-sensitive optimization. We prove that theapproximation ratio of this procedure can be bounded in terms of the geometryof constraint set. For a broad class of random projections, including thosebased on various sub-Gaussian distributions as well as randomized Hadamard andFourier transforms, the data matrix defining the cost function can be projecteddown to the statistical dimension of the tangent cone of the constraints at theoriginal solution, which is often substantially smaller than the originaldimension. We illustrate consequences of our theory for various cases,including unconstrained and $\ell_1$-constrained least squares, support vectormachines, low-rank matrix estimation, and discuss implications onprivacy-sensitive optimization and some connections with de-noising andcompressed sensing.
arxiv-6900-277 | Spatially Directional Predictive Coding for Block-based Compressive Sensing of Natural Images | http://arxiv.org/pdf/1404.7211v1.pdf | author:Jian Zhang, Debin Zhao, Feng Jiang category:cs.CV published:2014-04-29 summary:A novel coding strategy for block-based compressive sens-ing named spatiallydirectional predictive coding (SDPC) is proposed, which efficiently utilizesthe intrinsic spatial cor-relation of natural images. At the encoder, for eachblock of compressive sensing (CS) measurements, the optimal pre-diction isselected from a set of prediction candidates that are generated by fourdesigned directional predictive modes. Then, the resulting residual isprocessed by scalar quantiza-tion (SQ). At the decoder, the same prediction isadded onto the de-quantized residuals to produce the quantized CS measurements,which is exploited for CS reconstruction. Experimental results substantiatesignificant improvements achieved by SDPC-plus-SQ in rate distortionperformance as compared with SQ alone and DPCM-plus-SQ.
arxiv-6900-278 | Characterization and Compensation of Network-Level Anomalies in Mixed-Signal Neuromorphic Modeling Platforms | http://arxiv.org/pdf/1404.7514v2.pdf | author:Mihai A. Petrovici, Bernhard Vogginger, Paul Müller, Oliver Breitwieser, Mikael Lundqvist, Lyle Muller, Matthias Ehrlich, Alain Destexhe, Anders Lansner, René Schüffny, Johannes Schemmel, Karlheinz Meier category:q-bio.NC cs.NE published:2014-04-29 summary:Advancing the size and complexity of neural network models leads to an everincreasing demand for computational resources for their simulation.Neuromorphic devices offer a number of advantages over conventional computingarchitectures, such as high emulation speed or low power consumption, but thisusually comes at the price of reduced configurability and precision. In thisarticle, we investigate the consequences of several such factors that arecommon to neuromorphic devices, more specifically limited hardware resources,limited parameter configurability and parameter variations. Our final aim is toprovide an array of methods for coping with such inevitable distortionmechanisms. As a platform for testing our proposed strategies, we use anexecutable system specification (ESS) of the BrainScaleS neuromorphic system,which has been designed as a universal emulation back-end for neuroscientificmodeling. We address the most essential limitations of this device in detailand study their effects on three prototypical benchmark network models within awell-defined, systematic workflow. For each network model, we start by definingquantifiable functionality measures by which we then assess the effects oftypical hardware-specific distortion mechanisms, both in idealized softwaresimulations and on the ESS. For those effects that cause unacceptabledeviations from the original network dynamics, we suggest generic compensationmechanisms and demonstrate their effectiveness. Both the suggested workflow andthe investigated compensation mechanisms are largely back-end independent anddo not require additional hardware configurability beyond the one required toemulate the benchmark networks in the first place. We hereby provide a genericmethodological environment for configurable neuromorphic devices that aretargeted at emulating large-scale, functional neural networks.
arxiv-6900-279 | Structural Group Sparse Representation for Image Compressive Sensing Recovery | http://arxiv.org/pdf/1404.7212v1.pdf | author:Jian Zhang, Debin Zhao, Feng Jiang, Wen Gao category:cs.CV published:2014-04-29 summary:Compressive Sensing (CS) theory shows that a signal can be decoded from manyfewer measurements than suggested by the Nyquist sampling theory, when thesignal is sparse in some domain. Most of conventional CS recovery approaches,however, exploited a set of fixed bases (e.g. DCT, wavelet, contourlet andgradient domain) for the entirety of a signal, which are irrespective of thenonstationarity of natural signals and cannot achieve high enough degree ofsparsity, thus resulting in poor rate-distortion performance. In this paper, wepropose a new framework for image compressive sensing recovery via structuralgroup sparse representation (SGSR) modeling, which enforces image sparsity andself-similarity simultaneously under a unified framework in an adaptive groupdomain, thus greatly confining the CS solution space. In addition, an efficientiterative shrinkage/thresholding algorithm based technique is developed tosolve the above optimization problem. Experimental results demonstrate that thenovel CS recovery strategy achieves significant performance improvements overthe current state-of-the-art schemes and exhibits nice convergence.
arxiv-6900-280 | High Dimensional Semiparametric Latent Graphical Model for Mixed Data | http://arxiv.org/pdf/1404.7236v1.pdf | author:Jianqing Fan, Han Liu, Yang Ning, Hui Zou category:stat.ML published:2014-04-29 summary:Graphical models are commonly used tools for modeling multivariate randomvariables. While there exist many convenient multivariate distributions such asGaussian distribution for continuous data, mixed data with the presence ofdiscrete variables or a combination of both continuous and discrete variablesposes new challenges in statistical modeling. In this paper, we propose asemiparametric model named latent Gaussian copula model for binary and mixeddata. The observed binary data are assumed to be obtained by dichotomizing alatent variable satisfying the Gaussian copula distribution or thenonparanormal distribution. The latent Gaussian model with the assumption thatthe latent variables are multivariate Gaussian is a special case of theproposed model. A novel rank-based approach is proposed for both latent graphestimation and latent principal component analysis. Theoretically, the proposedmethods achieve the same rates of convergence for both precision matrixestimation and eigenvector estimation, as if the latent variables wereobserved. Under similar conditions, the consistency of graph structure recoveryand feature selection for leading eigenvectors is established. The performanceof the proposed methods is numerically assessed through simulation studies, andthe usage of our methods is illustrated by a genetic dataset.
arxiv-6900-281 | Meteorological time series forecasting based on MLP modelling using heterogeneous transfer functions | http://arxiv.org/pdf/1404.7255v1.pdf | author:Cyril Voyant, Marie Laure Nivet, Christophe Paoli, Marc Muselli, Gilles Notton category:cs.LG published:2014-04-29 summary:In this paper, we propose to study four meteorological and seasonal timeseries coupled with a multi-layer perceptron (MLP) modeling. We chose tocombine two transfer functions for the nodes of the hidden layer, and to use atemporal indicator (time index as input) in order to take into account theseasonal aspect of the studied time series. The results of the predictionconcern two years of measurements and the learning step, eight independentyears. We show that this methodology can improve the accuracy of meteorologicaldata estimation compared to a classical MLP modelling with a homogenoustransfer function.
arxiv-6900-282 | A Deep Architecture for Semantic Parsing | http://arxiv.org/pdf/1404.7296v1.pdf | author:Edward Grefenstette, Phil Blunsom, Nando de Freitas, Karl Moritz Hermann category:cs.CL published:2014-04-29 summary:Many successful approaches to semantic parsing build on top of the syntacticanalysis of text, and make use of distributional representations or statisticalmodels to match parses to ontology-specific queries. This paper presents anovel deep learning architecture which provides a semantic parsing systemthrough the union of two neural models of language semantics. It allows for thegeneration of ontology-specific queries from natural language statements andquestions without the need for parsing, which makes it especially suitable togrammatically malformed or syntactically atypical text, such as tweets, as wellas permitting the development of semantic parsers for resource-poor languages.
arxiv-6900-283 | The geometry of kernelized spectral clustering | http://arxiv.org/pdf/1404.7552v3.pdf | author:Geoffrey Schiebinger, Martin J. Wainwright, Bin Yu category:math.ST stat.ML stat.TH published:2014-04-29 summary:Clustering of data sets is a standard problem in many areas of science andengineering. The method of spectral clustering is based on embedding the dataset using a kernel function, and using the top eigenvectors of the normalizedLaplacian to recover the connected components. We study the performance ofspectral clustering in recovering the latent labels of i.i.d. samples from afinite mixture of nonparametric distributions. The difficulty of this labelrecovery problem depends on the overlap between mixture components and howeasily a mixture component is divided into two nonoverlapping components. Whenthe overlap is small compared to the indivisibility of the mixture components,the principal eigenspace of the population-level normalized Laplacian operatoris approximately spanned by the square-root kernelized component densities. Inthe finite sample setting, and under the same assumption, embedded samples fromdifferent components are approximately orthogonal with high probability whenthe sample size is large. As a corollary we control the fraction of samplesmislabeled by spectral clustering under finite mixtures with nonparametriccomponents.
arxiv-6900-284 | Generalized Nonconvex Nonsmooth Low-Rank Minimization | http://arxiv.org/pdf/1404.7306v1.pdf | author:Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin category:cs.CV cs.LG stat.ML published:2014-04-29 summary:As surrogate functions of $L_0$-norm, many nonconvex penalty functions havebeen proposed to enhance the sparse vector recovery. It is easy to extend thesenonconvex penalty functions on singular values of a matrix to enhance low-rankmatrix recovery. However, different from convex optimization, solving thenonconvex low-rank minimization problem is much more challenging than thenonconvex sparse minimization problem. We observe that all the existingnonconvex penalty functions are concave and monotonically increasing on$[0,\infty)$. Thus their gradients are decreasing functions. Based on thisproperty, we propose an Iteratively Reweighted Nuclear Norm (IRNN) algorithm tosolve the nonconvex nonsmooth low-rank minimization problem. IRNN iterativelysolves a Weighted Singular Value Thresholding (WSVT) problem. By setting theweight vector as the gradient of the concave penalty function, the WSVT problemhas a closed form solution. In theory, we prove that IRNN decreases theobjective function value monotonically, and any limit point is a stationarypoint. Extensive experiments on both synthetic data and real images demonstratethat IRNN enhances the low-rank matrix recovery compared with state-of-the-artconvex algorithms.
arxiv-6900-285 | A Map of Update Constraints in Inductive Inference | http://arxiv.org/pdf/1404.7527v2.pdf | author:Timo Kötzing, Raphaela Palenta category:cs.LG published:2014-04-29 summary:We investigate how different learning restrictions reduce learning power andhow the different restrictions relate to one another. We give a complete mapfor nine different restrictions both for the cases of complete informationlearning and set-driven learning. This completes the picture for thesewell-studied \emph{delayable} learning restrictions. A further insight isgained by different characterizations of \emph{conservative} learning in termsof variants of \emph{cautious} learning. Our analyses greatly benefit from general theorems we give, for exampleshowing that learners with exclusively delayable restrictions can always beassumed total.
arxiv-6900-286 | Probably Approximately Correct MDP Learning and Control With Temporal Logic Constraints | http://arxiv.org/pdf/1404.7073v2.pdf | author:Jie Fu, Ufuk Topcu category:cs.SY cs.LG cs.LO cs.RO 93E35 I.2.8; G.3 published:2014-04-28 summary:We consider synthesis of control policies that maximize the probability ofsatisfying given temporal logic specifications in unknown, stochasticenvironments. We model the interaction between the system and its environmentas a Markov decision process (MDP) with initially unknown transitionprobabilities. The solution we develop builds on the so-called model-basedprobably approximately correct Markov decision process (PAC-MDP) methodology.The algorithm attains an $\varepsilon$-approximately optimal policy withprobability $1-\delta$ using samples (i.e. observations), time and space thatgrow polynomially with the size of the MDP, the size of the automatonexpressing the temporal logic specification, $\frac{1}{\varepsilon}$,$\frac{1}{\delta}$ and a finite time horizon. In this approach, the systemmaintains a model of the initially unknown MDP, and constructs a product MDPbased on its learned model and the specification automaton that expresses thetemporal logic constraints. During execution, the policy is iteratively updatedusing observation of the transitions taken by the system. The iterationterminates in finitely many steps. With high probability, the resulting policyis such that, for any state, the difference between the probability ofsatisfying the specification under this policy and the optimal one is within apredefined bound.
arxiv-6900-287 | Stereo on a budget | http://arxiv.org/pdf/1404.7059v2.pdf | author:Dana Menaker, Shai Avidan category:cs.CV published:2014-04-28 summary:We propose an algorithm for recovering depth using less than two images.Instead of having both cameras send their entire image to the host computer,the left camera sends its image to the host while the right camera sends only afraction $\epsilon$ of its image. The key aspect is that the cameras send theinformation without communicating at all. Hence, the required communicationbandwidth is significantly reduced. While standard image compression techniques can reduce the communicationbandwidth, this requires additional computational resources on the part of theencoder (camera). We aim at designing a light weight encoder that only touchesa fraction of the pixels. The burden of decoding is placed on the decoder(host). We show that it is enough for the encoder to transmit a sparse set of pixels.Using only $1+\epsilon$ images, with $\epsilon$ as little as 2% of the image,the decoder can compute a depth map. The depth map's accuracy is comparable totraditional stereo matching algorithms that require both images as input. Usingthe depth map and the left image, the right image can be synthesized. Nocomputations are required at the encoder, and the decoder's runtime is linearin the images' size.
arxiv-6900-288 | Automatic Differentiation of Algorithms for Machine Learning | http://arxiv.org/pdf/1404.7456v1.pdf | author:Atilim Gunes Baydin, Barak A. Pearlmutter category:cs.LG cs.SC stat.ML G.1.4; I.2.6 published:2014-04-28 summary:Automatic differentiation---the mechanical transformation of numeric computerprograms to calculate derivatives efficiently and accurately---dates to theorigin of the computer age. Reverse mode automatic differentiation bothantedates and generalizes the method of backwards propagation of errors used inmachine learning. Despite this, practitioners in a variety of fields, includingmachine learning, have been little influenced by automatic differentiation, andmake scant use of available tools. Here we review the technique of automaticdifferentiation, describe its two main modes, and explain how it can benefitmachine learning practitioners. To reach the widest possible audience ourtreatment assumes only elementary differential calculus, and does not assumeany knowledge of linear algebra.
arxiv-6900-289 | Conditional Density Estimation with Dimensionality Reduction via Squared-Loss Conditional Entropy Minimization | http://arxiv.org/pdf/1404.6876v1.pdf | author:Voot Tangkaratt, Ning Xie, Masashi Sugiyama category:cs.LG stat.ML published:2014-04-28 summary:Regression aims at estimating the conditional mean of output given input.However, regression is not informative enough if the conditional density ismultimodal, heteroscedastic, and asymmetric. In such a case, estimating theconditional density itself is preferable, but conditional density estimation(CDE) is challenging in high-dimensional space. A naive approach to coping withhigh-dimensionality is to first perform dimensionality reduction (DR) and thenexecute CDE. However, such a two-step process does not perform well in practicebecause the error incurred in the first DR step can be magnified in the secondCDE step. In this paper, we propose a novel single-shot procedure that performsCDE and DR simultaneously in an integrated way. Our key idea is to formulate DRas the problem of minimizing a squared-loss variant of conditional entropy, andthis is solved via CDE. Thus, an additional CDE step is not needed after DR. Wedemonstrate the usefulness of the proposed method through extensive experimentson various datasets including humanoid robot transition and computer art.
arxiv-6900-290 | Proximal Iteratively Reweighted Algorithm with Multiple Splitting for Nonconvex Sparsity Optimization | http://arxiv.org/pdf/1404.6871v1.pdf | author:Canyi Lu, Yunchao Wei, Zhouchen Lin, Shuicheng Yan category:cs.NA cs.CV published:2014-04-28 summary:This paper proposes the Proximal Iteratively REweighted (PIRE) algorithm forsolving a general problem, which involves a large body of nonconvex sparse andstructured sparse related problems. Comparing with previous iterative solversfor nonconvex sparse problem, PIRE is much more general and efficient. Thecomputational cost of PIRE in each iteration is usually as low as thestate-of-the-art convex solvers. We further propose the PIRE algorithm withParallel Splitting (PIRE-PS) and PIRE algorithm with Alternative Updating(PIRE-AU) to handle the multi-variable problems. In theory, we prove that ourproposed methods converge and any limit solution is a stationary point.Extensive experiments on both synthesis and real data sets demonstrate that ourmethods achieve comparative learning performance, but are much more efficient,by comparing with previous nonconvex solvers.
arxiv-6900-291 | Data Requirement for Phylogenetic Inference from Multiple Loci: A New Distance Method | http://arxiv.org/pdf/1404.7055v2.pdf | author:Gautam Dasarathy, Robert Nowak, Sebastien Roch category:q-bio.PE cs.CE cs.DS math.PR math.ST stat.ML stat.TH published:2014-04-28 summary:We consider the problem of estimating the evolutionary history of a set ofspecies (phylogeny or species tree) from several genes. It is known that theevolutionary history of individual genes (gene trees) might be topologicallydistinct from each other and from the underlying species tree, possiblyconfounding phylogenetic analysis. A further complication in practice is thatone has to estimate gene trees from molecular sequences of finite length. Weprovide the first full data-requirement analysis of a species treereconstruction method that takes into account estimation errors at the genelevel. Under that criterion, we also devise a novel reconstruction algorithmthat provably improves over all previous methods in a regime of interest.
arxiv-6900-292 | One-bit compressive sensing with norm estimation | http://arxiv.org/pdf/1404.6853v3.pdf | author:Karin Knudson, Rayan Saab, Rachel Ward category:stat.ML math.NA math.OC math.PR 90C05 published:2014-04-28 summary:Consider the recovery of an unknown signal ${x}$ from quantized linearmeasurements. In the one-bit compressive sensing setting, one typically assumesthat ${x}$ is sparse, and that the measurements are of the form$\operatorname{sign}(\langle {a}_i, {x} \rangle) \in \{\pm1\}$. Since suchmeasurements give no information on the norm of ${x}$, recovery methods fromsuch measurements typically assume that $\ {x} \_2=1$. We show that if oneallows more generally for quantized affine measurements of the form$\operatorname{sign}(\langle {a}_i, {x} \rangle + b_i)$, and if the vectors${a}_i$ are random, an appropriate choice of the affine shifts $b_i$ allowsnorm recovery to be easily incorporated into existing methods for one-bitcompressive sensing. Additionally, we show that for arbitrary fixed ${x}$ inthe annulus $r \leq \ {x} \_2 \leq R$, one may estimate the norm $\ {x}\_2$ up to additive error $\delta$ from $m \gtrsim R^4 r^{-2} \delta^{-2}$such binary measurements through a single evaluation of the inverse Gaussianerror function. Finally, all of our recovery guarantees can be made universalover sparse vectors, in the sense that with high probability, one set ofmeasurements and thresholds can successfully estimate all sparse vectors ${x}$within a Euclidean ball of known radius.
arxiv-6900-293 | Computer vision-based recognition of liquid surfaces and phase boundaries in transparent vessels, with emphasis on chemistry applications | http://arxiv.org/pdf/1404.7174v7.pdf | author:Sagi Eppel, Tal Kachman category:cs.CV published:2014-04-28 summary:The ability to recognize the liquid surface and the liquid level intransparent containers is perhaps the most commonly used evaluation method whendealing with fluids. Such recognition is essential in determining the liquidvolume, fill level, phase boundaries and phase separation in various fluidsystems. The recognition of liquid surfaces is particularly important insolution chemistry, where it is essential to many laboratory techniques (e.g.,extraction, distillation, titration). A general method for the recognition ofinterfaces between liquid and air or between phase-separating liquids couldhave a wide range of applications and contribute to the understanding of thevisual properties of such interfaces. This work examines a computer visionmethod for the recognition of liquid surfaces and liquid levels in varioustransparent containers. The method can be applied to recognition of bothliquid-air and liquid-liquid surfaces. No prior knowledge of the number ofphases is required. The method receives the image of the liquid container andthe boundaries of the container in the image and scans all possible curves thatcould correspond to the outlines of liquid surfaces in the image. The methodthen compares each curve to the image to rate its correspondence with theoutline of the real liquid surface by examining various image properties in thearea surrounding each point of the curve. The image properties that were foundto give the best indication of the liquid surface are the relative intensitychange, the edge density change and the gradient direction relative to thecurve normal.
arxiv-6900-294 | A Constrained Matrix-Variate Gaussian Process for Transposable Data | http://arxiv.org/pdf/1404.6702v1.pdf | author:Oluwasanmi Koyejo, Cheng Lee, Joydeep Ghosh category:stat.ML cs.LG published:2014-04-27 summary:Transposable data represents interactions among two sets of entities, and aretypically represented as a matrix containing the known interaction values.Additional side information may consist of feature vectors specific to entitiescorresponding to the rows and/or columns of such a matrix. Further informationmay also be available in the form of interactions or hierarchies among entitiesalong the same mode (axis). We propose a novel approach for modelingtransposable data with missing interactions given additional side information.The interactions are modeled as noisy observations from a latent noise freematrix generated from a matrix-variate Gaussian process. The construction ofrow and column covariances using side information provides a flexible mechanismfor specifying a-priori knowledge of the row and column correlations in thedata. Further, the use of such a prior combined with the side informationenables predictions for new rows and columns not observed in the training data.In this work, we combine the matrix-variate Gaussian process model with lowrank constraints. The constrained Gaussian process approach is applied to theprediction of hidden associations between genes and diseases using a small setof observed associations as well as prior covariances induced by gene-geneinteraction networks and disease ontologies. The proposed approach is alsoapplied to recommender systems data which involves predicting the item ratingsof users using known associations as well as prior covariances induced bysocial networks. We present experimental results that highlight the performanceof constrained matrix-variate Gaussian process as compared to state of the artapproaches in each domain.
arxiv-6900-295 | Robust and Efficient Subspace Segmentation via Least Squares Regression | http://arxiv.org/pdf/1404.6736v1.pdf | author:Can-Yi Lu, Hai Min, Zhong-Qiu Zhao, Lin Zhu, De-Shuang Huang, Shuicheng Yan category:cs.CV published:2014-04-27 summary:This paper studies the subspace segmentation problem which aims to segmentdata drawn from a union of multiple linear subspaces. Recent works by usingsparse representation, low rank representation and their extensions attractmuch attention. If the subspaces from which the data drawn are independent ororthogonal, they are able to obtain a block diagonal affinity matrix, whichusually leads to a correct segmentation. The main differences among them aretheir objective functions. We theoretically show that if the objective functionsatisfies some conditions, and the data are sufficiently drawn from independentsubspaces, the obtained affinity matrix is always block diagonal. Furthermore,the data sampling can be insufficient if the subspaces are orthogonal. Someexisting methods are all special cases. Then we present the Least SquaresRegression (LSR) method for subspace segmentation. It takes advantage of datacorrelation, which is common in real data. LSR encourages a grouping effectwhich tends to group highly correlated data together. Experimental results onthe Hopkins 155 database and Extended Yale Database B show that our methodsignificantly outperforms state-of-the-art methods. Beyond segmentationaccuracy, all experiments demonstrate that LSR is much more efficient.
arxiv-6900-296 | Subspace clustering of dimensionality-reduced data | http://arxiv.org/pdf/1404.6818v1.pdf | author:Reinhard Heckel, Michael Tschannen, Helmut Bölcskei category:cs.IT math.IT stat.ML published:2014-04-27 summary:Subspace clustering refers to the problem of clustering unlabeledhigh-dimensional data points into a union of low-dimensional linear subspaces,assumed unknown. In practice one may have access to dimensionality-reducedobservations of the data only, resulting, e.g., from "undersampling" due tocomplexity and speed constraints on the acquisition device. More pertinently,even if one has access to the high-dimensional data set it is often desirableto first project the data points into a lower-dimensional space and to performthe clustering task there; this reduces storage requirements and computationalcost. The purpose of this paper is to quantify the impact ofdimensionality-reduction through random projection on the performance of thesparse subspace clustering (SSC) and the thresholding based subspace clustering(TSC) algorithms. We find that for both algorithms dimensionality reductiondown to the order of the subspace dimensions is possible without incurringsignificant performance degradation. The mathematical engine behind ourtheorems is a result quantifying how the affinities between subspaces changeunder random dimensionality reducing projections.
arxiv-6900-297 | Aggregation of predictors for nonstationary sub-linear processes and online adaptive forecasting of time varying autoregressive processes | http://arxiv.org/pdf/1404.6769v5.pdf | author:Christophe Giraud, François Roueff, Andres Sanchez-Perez category:math.ST stat.ML stat.TH published:2014-04-27 summary:In this work, we study the problem of aggregating a finite number ofpredictors for nonstationary sub-linear processes. We provide oracleinequalities relying essentially on three ingredients: (1) a uniform bound ofthe $\ell^1$ norm of the time varying sub-linear coefficients, (2) a Lipschitzassumption on the predictors and (3) moment conditions on the noise appearingin the linear representation. Two kinds of aggregations are considered givingrise to different moment conditions on the noise and more or less sharp oracleinequalities. We apply this approach for deriving an adaptive predictor forlocally stationary time varying autoregressive (TVAR) processes. It is obtainedby aggregating a finite number of well chosen predictors, each of them enjoyingan optimal minimax convergence rate under specific smoothness conditions on theTVAR coefficients. We show that the obtained aggregated predictor achieves aminimax rate while adapting to the unknown smoothness. To prove this result, alower bound is established for the minimax rate of the prediction risk for theTVAR process. Numerical experiments complete this study. An important featureof this approach is that the aggregated predictor can be computed recursivelyand is thus applicable in an online prediction context.
arxiv-6900-298 | Estimation of positive definite M-matrices and structure learning for attractive Gaussian Markov Random fields | http://arxiv.org/pdf/1404.6640v1.pdf | author:Martin Slawski, Matthias Hein category:math.ST stat.ML stat.TH published:2014-04-26 summary:Consider a random vector with finite second moments. If its precision matrixis an M-matrix, then all partial correlations are non-negative. If that randomvector is additionally Gaussian, the corresponding Markov random field (GMRF)is called attractive. We study estimation of M-matrices taking the role ofinverse second moment or precision matrices using sign-constrainedlog-determinant divergence minimization. We also treat the high-dimensionalcase with the number of variables exceeding the sample size. The additionalsign-constraints turn out to greatly simplify the estimation problem: weprovide evidence that explicit regularization is no longer required. To solvethe resulting convex optimization problem, we propose an algorithm based onblock coordinate descent, in which each sub-problem can be recast asnon-negative least squares problem. Illustrations on both simulated and realworld data are provided.
arxiv-6900-299 | A Comparison of First-order Algorithms for Machine Learning | http://arxiv.org/pdf/1404.6674v1.pdf | author:Yu Wei, Pock Thomas category:cs.LG published:2014-04-26 summary:Using an optimization algorithm to solve a machine learning problem is one ofmainstreams in the field of science. In this work, we demonstrate acomprehensive comparison of some state-of-the-art first-order optimizationalgorithms for convex optimization problems in machine learning. We concentrateon several smooth and non-smooth machine learning problems with a loss functionplus a regularizer. The overall experimental results show the superiority ofprimal-dual algorithms in solving a machine learning problem from theperspectives of the ease to construct, running time and accuracy.
arxiv-6900-300 | Sinogram constrained TV-minimization for metal artifact reduction in CT | http://arxiv.org/pdf/1404.6691v1.pdf | author:Clemens Schiffer, Kristian Bredies category:math.NA cs.CV physics.med-ph published:2014-04-26 summary:A new method for reducing metal artifacts in X-ray computed tomography (CT)images is presented. It bases on the solution of a convex optimization problemwith inequality constraints on the sinogram, and total variation regularizationfor the reconstructed image. The Chambolle-Pock algorithm is used tonumerically solve the discretized version of the optimization problem. As proofof concept we present and discuss numerical results for synthetic data.
