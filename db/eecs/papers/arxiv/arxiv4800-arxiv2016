arxiv-1310-7782 | Individual Biases, Cultural Evolution, and the Statistical Nature of Language Universals: The Case of Colour Naming Systems |  http://arxiv.org/abs/1310.7782  | author:Andrea Baronchelli, Vittorio Loreto, Andrea Puglisi category:physics.soc-ph cs.CL cs.MA q-bio.PE published:2013-10-29 summary:Language universals have long been attributed to an innate Universal Grammar.An alternative explanation states that linguistic universals emergedindependently in every language in response to shared cognitive or perceptualbiases. A computational model has recently shown how this could be the case,focusing on the paradigmatic example of the universal properties of colournaming patterns, and producing results in quantitative agreement with theexperimental data. Here we investigate the role of an individual perceptualbias in the framework of the model. We study how, and to what extent, thestructure of the bias influences the corresponding linguistic universalpatterns. We show that the cultural history of a group of speakers introducespopulation-specific constraints that act against the pressure for uniformityarising from the individual bias, and we clarify the interplay between thesetwo forces.
arxiv-1310-7855 | A comparison of bandwidth selectors for mean shift clustering |  http://arxiv.org/abs/1310.7855  | author:José E. Chacón, Pablo Monfort category:stat.ML published:2013-10-29 summary:We explore the performance of several automatic bandwidth selectors,originally designed for density gradient estimation, as data-based proceduresfor nonparametric, modal clustering. The key tool to obtain a clustering fromdensity gradient estimators is the mean shift algorithm, which allows to obtaina partition not only of the data sample, but also of the whole space. Theresults of our simulation study suggest that most of the methods consideredhere, like cross validation and plug in bandwidth selectors, are useful forcluster analysis via the mean shift algorithm.
arxiv-1310-7795 | An Unsupervised Feature Learning Approach to Improve Automatic Incident Detection |  http://arxiv.org/abs/1310.7795  | author:Jimmy SJ. Ren, Wei Wang, Jiawei Wang, Stephen Liao category:cs.LG published:2013-10-29 summary:Sophisticated automatic incident detection (AID) technology plays a key rolein contemporary transportation systems. Though many papers were devoted tostudy incident classification algorithms, few study investigated how to enhancefeature representation of incidents to improve AID performance. In this paper,we propose to use an unsupervised feature learning algorithm to generate higherlevel features to represent incidents. We used real incident data in theexperiments and found that effective feature mapping function can be learntfrom the data crosses the test sites. With the enhanced features, detectionrate (DR), false alarm rate (FAR) and mean time to detect (MTTD) aresignificantly improved in all of the three representative cases. This approachalso provides an alternative way to reduce the amount of labeled data, which isexpensive to obtain, required in training better incident classifiers since thefeature learning is unsupervised.
arxiv-1310-7300 | Relax but stay in control: from value to algorithms for online Markov decision processes |  http://arxiv.org/abs/1310.7300  | author:Peng Guan, Maxim Raginsky, Rebecca Willett category:cs.LG math.OC stat.ML published:2013-10-28 summary:Online learning algorithms are designed to perform in non-stationaryenvironments, but generally there is no notion of a dynamic state to modelconstraints on current and future actions as a function of past actions.State-based models are common in stochastic control settings, but commonly usedframeworks such as Markov Decision Processes (MDPs) assume a known stationaryenvironment. In recent years, there has been a growing interest in combiningthe above two frameworks and considering an MDP setting in which the costfunction is allowed to change arbitrarily after each time step. However, mostof the work in this area has been algorithmic: given a problem, one woulddevelop an algorithm almost from scratch. Moreover, the presence of the stateand the assumption of an arbitrarily varying environment complicate both thetheoretical analysis and the development of computationally efficient methods.This paper describes a broad extension of the ideas proposed by Rakhlin et al.to give a general framework for deriving algorithms in an MDP setting witharbitrarily changing costs. This framework leads to a unifying view of existingmethods and provides a general procedure for constructing new ones. Several newmethods are presented, and one of them is shown to have important advantagesover a similar method developed from scratch via an online version ofapproximate dynamic programming.
arxiv-1310-7529 | Successive Nonnegative Projection Algorithm for Robust Nonnegative Blind Source Separation |  http://arxiv.org/abs/1310.7529  | author:Nicolas Gillis category:stat.ML cs.LG math.NA math.OC published:2013-10-28 summary:In this paper, we propose a new fast and robust recursive algorithm fornear-separable nonnegative matrix factorization, a particular nonnegative blindsource separation problem. This algorithm, which we refer to as the successivenonnegative projection algorithm (SNPA), is closely related to the popularsuccessive projection algorithm (SPA), but takes advantage of the nonnegativityconstraint in the decomposition. We prove that SNPA is more robust than SPA andcan be applied to a broader class of nonnegative matrices. This is illustratedon some synthetic data sets, and on a real-world hyperspectral image.
arxiv-1310-7637 | Regularization of $\ell_1$ minimization for dealing with outliers and noise in Statistics and Signal Recovery |  http://arxiv.org/abs/1310.7637  | author:Salvador Flores, Luis M. Briceno-Arias category:math.OC stat.ML published:2013-10-28 summary:We study the robustness properties of $\ell_1$ norm minimization for theclassical linear regression problem with a given design matrix andcontamination restricted to the dependent variable. We perform a fine erroranalysis of the $\ell_1$ estimator for measurements errors consisting ofoutliers coupled with noise. We introduce a new estimation technique resultingfrom a regularization of $\ell_1$ minimization by inf-convolution with the$\ell_2$ norm. Concerning robustness to large outliers, the proposed estimatorkeeps the breakdown point of the $\ell_1$ estimator, and reduces to leastsquares when there are not outliers. We present a globally convergentforward-backward algorithm for computing our estimator and some numericalexperiments confirming its theoretical properties.
arxiv-1310-7163 | Generalized Thompson Sampling for Contextual Bandits |  http://arxiv.org/abs/1310.7163  | author:Lihong Li category:cs.LG cs.AI stat.ML stat.OT 62L05 I.2.6 published:2013-10-27 summary:Thompson Sampling, one of the oldest heuristics for solving multi-armedbandits, has recently been shown to demonstrate state-of-the-art performance.The empirical success has led to great interests in theoretical understandingof this heuristic. In this paper, we approach this problem in a way verydifferent from existing efforts. In particular, motivated by the connectionbetween Thompson Sampling and exponentiated updates, we propose a new family ofalgorithms called Generalized Thompson Sampling in the expert-learningframework, which includes Thompson Sampling as a special case. Similar to mostexpert-learning algorithms, Generalized Thompson Sampling uses a loss functionto adjust the experts' weights. General regret bounds are derived, which arealso instantiated to two important loss functions: square loss and logarithmicloss. In contrast to existing bounds, our results apply to quite generalcontextual bandits. More importantly, they quantify the effect of the "prior"distribution on the regret bounds.
arxiv-1310-7217 | Compressed Sensing SAR Imaging with Multilook Processing |  http://arxiv.org/abs/1310.7217  | author:Jian Fang, Zongben Xu, Bingchen Zhang, Wen Hong, Yirong Wu category:cs.IT cs.CV math.IT published:2013-10-27 summary:Multilook processing is a widely used speckle reduction approach in syntheticaperture radar (SAR) imaging. Conventionally, it is achieved by incoherentlysumming of some independent low-resolution images formulated from overlappingsubbands of the SAR signal. However, in the context of compressive sensing (CS)SAR imaging, where the samples are collected at sub-Nyquist rate, the dataspectrum is highly aliased that hinders the direct application of the existingmultilook techniques. In this letter, we propose a new CS-SAR imaging methodthat can realize multilook processing simultaneously during imagereconstruction. The main idea is to replace the SAR observation matrix by theinverse of multilook procedures, which is then combined with random samplingmatrix to yield a multilook CS-SAR observation model. Then a joint sparseregularization model, considering pixel dependency of subimages, is derived toform multilook images. The suggested SAR imaging method can not onlyreconstruct sparse scene efficiently below Nyquist rate, but is also able toachieve a comparable reduction of speckles during reconstruction. Simulationresults are finally provided to demonstrate the effectiveness of the proposedmethod.
arxiv-1310-7170 | Object Recognition System Design in Computer Vision: a Universal Approach |  http://arxiv.org/abs/1310.7170  | author:Andrew Gleibman category:cs.CV published:2013-10-27 summary:The first contribution of this paper is architecture of a multipurposesystem, which delegates a range of object detection tasks to a classifier,applied in special grid positions of the tested image. The second contributionis Gray Level-Radius Co-occurrence Matrix, which describes local image textureand topology and, unlike common second order statistics methods, is robust toimage resolution. The third contribution is a parametrically controlledautomatic synthesis of unlimited number of numerical features forclassification. The fourth contribution is a method of optimizing parameters Cand gamma in LibSVM-based classifier, which is 20-100 times faster than thecommonly applied method. The work is essentially experimental, withdemonstration of various methods for definition of objects of interest inimages and video sequences.
arxiv-1310-7114 | Efficient Information Theoretic Clustering on Discrete Lattices |  http://arxiv.org/abs/1310.7114  | author:Christian Bauckhage, Kristian Kersting category:cs.CV published:2013-10-26 summary:We consider the problem of clustering data that reside on discrete, lowdimensional lattices. Canonical examples for this setting are found in imagesegmentation and key point extraction. Our solution is based on a recentapproach to information theoretic clustering where clusters result from aniterative procedure that minimizes a divergence measure. We replace costlyprocessing steps in the original algorithm by means of convolutions. Theseallow for highly efficient implementations and thus significantly reduceruntime. This paper therefore bridges a gap between machine learning and signalprocessing.
arxiv-1310-7115 | Studying a Chaotic Spiking Neural Model |  http://arxiv.org/abs/1310.7115  | author:Mohammad Alhawarat, Waleed Nazih, Mohammad Eldesouki category:cs.AI cs.NE published:2013-10-26 summary:Dynamics of a chaotic spiking neuron model are being studied mathematicallyand experimentally. The Nonlinear Dynamic State neuron (NDS) is analysed tofurther understand the model and improve it. Chaos has many interestingproperties such as sensitivity to initial conditions, space filling, controland synchronization. As suggested by biologists, these properties may beexploited and play vital role in carrying out computational tasks in humanbrain. The NDS model has some limitations; in thus paper the model isinvestigated to overcome some of these limitations in order to enhance themodel. Therefore, the models parameters are tuned and the resulted dynamics arestudied. Also, the discretization method of the model is considered. Moreover,a mathematical analysis is carried out to reveal the underlying dynamics of themodel after tuning of its parameters. The results of the aforementioned methodsrevealed some facts regarding the NDS attractor and suggest the stabilizationof a large number of unstable periodic orbits (UPOs) which might correspond tomemories in phase space.
arxiv-1310-6808 | Gender Classification Using Gradient Direction Pattern |  http://arxiv.org/abs/1310.6808  | author:Mohammad shahidul Islam category:cs.CV I.5.4 published:2013-10-25 summary:A novel methodology for gender classification is presented in this paper. Itextracts feature from local region of a face using gray color intensitydifference. The facial area is divided into sub-regions and GDP histogramextracted from those regions are concatenated into a single vector to representthe face. The classification accuracy obtained by using support vector machinehas outperformed all traditional feature descriptors for gender classification.It is evaluated on the images collected from FERET database and obtained veryhigh accuracy.
arxiv-1310-7048 | Scaling SVM and Least Absolute Deviations via Exact Data Reduction |  http://arxiv.org/abs/1310.7048  | author:Jie Wang, Peter Wonka, Jieping Ye category:cs.LG stat.ML published:2013-10-25 summary:The support vector machine (SVM) is a widely used method for classification.Although many efforts have been devoted to develop efficient solvers, itremains challenging to apply SVM to large-scale problems. A nice property ofSVM is that the non-support vectors have no effect on the resulting classifier.Motivated by this observation, we present fast and efficient screening rules todiscard non-support vectors by analyzing the dual problem of SVM viavariational inequalities (DVI). As a result, the number of data instances to beentered into the optimization can be substantially reduced. Some appealingfeatures of our screening method are: (1) DVI is safe in the sense that thevectors discarded by DVI are guaranteed to be non-support vectors; (2) the dataset needs to be scanned only once to run the screening, whose computationalcost is negligible compared to that of solving the SVM problem; (3) DVI isindependent of the solvers and can be integrated with any existing efficientsolvers. We also show that the DVI technique can be extended to detectnon-support vectors in the least absolute deviations regression (LAD). To thebest of our knowledge, there are currently no screening methods for LAD. Wehave evaluated DVI on both synthetic and real data sets. Experiments indicatethat DVI significantly outperforms the existing state-of-the-art screeningrules for SVM, and is very effective in discarding non-support vectors for LAD.The speedup gained by DVI rules can be up to two orders of magnitude.
arxiv-1310-7033 | A feasible roadmap for unsupervised deconvolution of two-source mixed gene expressions |  http://arxiv.org/abs/1310.7033  | author:Niya Wang, Eric P. Hoffman, Robert Clarke, Zhen Zhang, David M. Herrington, Ie-Ming Shih, Douglas A. Levine, Guoqiang Yu, Jianhua Xuan, Yue Wang category:stat.ML q-bio.GN q-bio.QM stat.AP published:2013-10-25 summary:Tissue heterogeneity is a major confounding factor in studying individualpopulations that cannot be resolved directly by global profiling. Experimentalsolutions to mitigate tissue heterogeneity are expensive, time consuming,inapplicable to existing data, and may alter the original gene expressionpatterns. Here we ask whether it is possible to deconvolute two-source mixedexpressions (estimating both proportions and cell-specific profiles) from twoor more heterogeneous samples without requiring any prior knowledge. Supportedby a well-grounded mathematical framework, we argue that both constituentproportions and cell-specific expressions can be estimated in a completelyunsupervised mode when cell-specific marker genes exist, which do not have tobe known a priori, for each of constituent cell types. We demonstrate theperformance of unsupervised deconvolution on both simulation and real geneexpression data, together with perspective discussions.
arxiv-1310-6998 | Predicting the NFL using Twitter |  http://arxiv.org/abs/1310.6998  | author:Shiladitya Sinha, Chris Dyer, Kevin Gimpel, Noah A. Smith category:cs.SI cs.LG physics.soc-ph stat.ML published:2013-10-25 summary:We study the relationship between social media output and National FootballLeague (NFL) games, using a dataset containing messages from Twitter and NFLgame statistics. Specifically, we consider tweets pertaining to specific teamsand games in the NFL season and use them alongside statistical game data tobuild predictive models for future game outcomes (which team will win?) andsports betting outcomes (which team will win with the point spread? will thetotal points be over/under the line?). We experiment with several feature setsand find that simple features using large volumes of tweets can match or exceedthe performance of more traditional features that use game statistics.
arxiv-1310-6778 | Bayesian estimation of possible causal direction in the presence of latent confounders using a linear non-Gaussian acyclic structural equation model with individual-specific effects |  http://arxiv.org/abs/1310.6778  | author:Shohei Shimizu, Kenneth Bollen category:stat.ML published:2013-10-24 summary:We consider learning the possible causal direction of two observed variablesin the presence of latent confounding variables. Several existing methods havebeen shown to consistently estimate causal direction assuming linear or sometype of nonlinear relationship and no latent confounders. However, theestimation results could be distorted if either assumption is actuallyviolated. In this paper, we first propose a new linear non-Gaussian acyclicstructural equation model with individual-specific effects that allows latentconfounders to be considered. We then propose an empirical Bayesian approachfor estimating possible causal direction using the new model. We demonstratethe effectiveness of our method using artificial and real-world data.
arxiv-1310-6654 | Pseudo vs. True Defect Classification in Printed Circuits Boards using Wavelet Features |  http://arxiv.org/abs/1310.6654  | author:Sahil Sikka, Karan Sikka, M. K. Bhuyan, Yuji Iwahori category:cs.CV published:2013-10-24 summary:In recent years, Printed Circuit Boards (PCB) have become the backbone of alarge number of consumer electronic devices leading to a surge in theirproduction. This has made it imperative to employ automatic inspection systemsto identify manufacturing defects in PCB before they are installed in therespective systems. An important task in this regard is the classification ofdefects as either true or pseudo defects, which decides if the PCB is to bere-manufactured or not. This work proposes a novel approach to detect mostcommon defects in the PCBs. The problem has been approached by employing highlydiscriminative features based on multi-scale wavelet transform, which arefurther boosted by using a kernalized version of the support vector machines(SVM). A real world printed circuit board dataset has been used forquantitative analysis. Experimental results demonstrated the efficacy of theproposed method.
arxiv-1310-6775 | Durkheim Project Data Analysis Report |  http://arxiv.org/abs/1310.6775  | author:Linas Vepstas category:cs.AI cs.CL cs.LG published:2013-10-24 summary:This report describes the suicidality prediction models created under theDARPA DCAPS program in association with the Durkheim Project[http://durkheimproject.org/]. The models were built primarily fromunstructured text (free-format clinician notes) for several hundred patientrecords obtained from the Veterans Health Administration (VHA). The models wereconstructed using a genetic programming algorithm applied to bag-of-words andbag-of-phrases datasets. The influence of additional structured data wasexplored but was found to be minor. Given the small dataset size,classification between cohorts was high fidelity (98%). Cross-validationsuggests these models are reasonably predictive, with an accuracy of 50% to 69%on five rotating folds, with ensemble averages of 58% to 67%. One particularlynoteworthy result is that word-pairs can dramatically improve classificationaccuracy; but this is the case only when one of the words in the pair isalready known to have a high predictive value. By contrast, the set of allpossible word-pairs does not improve on a simple bag-of-words model.
arxiv-1310-6547 | Sparse Predictive Structure of Deconvolved Functional Brain Networks |  http://arxiv.org/abs/1310.6547  | author:Tommaso Furlanello, Marco Cristoforetti, Cesare Furlanello, Giuseppe Jurman category:q-bio.NC q-bio.QM stat.ML published:2013-10-24 summary:The functional and structural representation of the brain as a complexnetwork is marked by the fact that the comparison of noisy and intrinsicallycorrelated high-dimensional structures between experimental conditions orgroups shuns typical mass univariate methods. Furthermore most networkestimation methods cannot distinguish between real and spurious correlationarising from the convolution due to nodes' interaction, which thus introducesadditional noise in the data. We propose a machine learning pipeline aimed atidentifying multivariate differences between brain networks associated todifferent experimental conditions. The pipeline (1) leverages the deconvolvedindividual contribution of each edge and (2) maps the task into a sparseclassification problem in order to construct the associated "sparse deconvolvedpredictive network", i.e., a graph with the same nodes of those compared butwhose edge weights are defined by their relevance for out of sample predictionsin classification. We present an application of the proposed method by decodingthe covert attention direction (left or right) based on the single-trialfunctional connectivity matrix extracted from high-frequencymagnetoencephalography (MEG) data. Our results demonstrate how networkdeconvolution matched with sparse classification methods outperforms typicalapproaches for MEG decoding.
arxiv-1310-6772 | Sockpuppet Detection in Wikipedia: A Corpus of Real-World Deceptive Writing for Linking Identities |  http://arxiv.org/abs/1310.6772  | author:Thamar Solorio, Ragib Hasan, Mainul Mizan category:cs.CL cs.CR cs.CY published:2013-10-24 summary:This paper describes the corpus of sockpuppet cases we gathered fromWikipedia. A sockpuppet is an online user account created with a fake identityfor the purpose of covering abusive behavior and/or subverting the editingregulation process. We used a semi-automated method for crawling and curating adataset of real sockpuppet investigation cases. To the best of our knowledge,this is the first corpus available on real-world deceptive writing. We describethe process for crawling the data and some preliminary results that can be usedas baseline for benchmarking research. The dataset will be released under aCreative Commons license from our project website: http://docsig.cis.uab.edu.
arxiv-1310-6740 | Active Learning of Linear Embeddings for Gaussian Processes |  http://arxiv.org/abs/1310.6740  | author:Roman Garnett, Michael A. Osborne, Philipp Hennig category:stat.ML cs.LG 68T05 published:2013-10-24 summary:We propose an active learning method for discovering low-dimensionalstructure in high-dimensional Gaussian process (GP) tasks. Such problems areincreasingly frequent and important, but have hitherto presented severepractical difficulties. We further introduce a novel technique forapproximately marginalizing GP hyperparameters, yielding marginal predictionsrobust to hyperparameter mis-specification. Our method offers an efficientmeans of performing GP regression, quadrature, or Bayesian optimization inhigh-dimensional spaces.
arxiv-1310-6536 | Randomized co-training: from cortical neurons to machine learning and back again |  http://arxiv.org/abs/1310.6536  | author:David Balduzzi category:cs.LG q-bio.NC stat.ML published:2013-10-24 summary:Despite its size and complexity, the human cortex exhibits strikinganatomical regularities, suggesting there may simple meta-algorithms underlyingcortical learning and computation. We expect such meta-algorithms to be ofinterest since they need to operate quickly, scalably and effectively withlittle-to-no specialized assumptions. This note focuses on a specific question: How can neurons use vast quantitiesof unlabeled data to speed up learning from the comparatively rare labelsprovided by reward systems? As a partial answer, we propose randomizedco-training as a biologically plausible meta-algorithm satisfying the aboverequirements. As evidence, we describe a biologically-inspired algorithm,Correlated Nystrom Views (XNV) that achieves state-of-the-art performance insemi-supervised learning, and sketch work in progress on a neuronalimplementation.
arxiv-1310-6736 | Fast 3D Salient Region Detection in Medical Images using GPUs |  http://arxiv.org/abs/1310.6736  | author:Rahul Thota, Sharan Vaswani, Amit Kale, Nagavijayalakshmi Vydyanathan category:cs.CV published:2013-10-24 summary:Automated detection of visually salient regions is an active area of researchin computer vision. Salient regions can serve as inputs for object detectors aswell as inputs for region based registration algorithms. In this paper weconsider the problem of speeding up computationally intensive bottom-up salientregion detection in 3D medical volumes.The method uses the Kadir Bradyformulation of saliency. We show that in the vicinity of a salient region,entropy is a monotonically increasing function of the degree of overlap of acandidate window with the salient region. This allows us to initialize a sparseseed-point grid as the set of tentative salient region centers and iterativelyconverge to the local entropy maxima, thereby reducing the computationcomplexity compared to the Kadir Brady approach of performing this computationat every point in the image. We propose two different approaches for achievingthis. The first approach involves evaluating entropy in the four quadrantsaround the seed point and iteratively moving in the direction that increasesentropy. The second approach we propose makes use of mean shift trackingframework to affect entropy maximizing moves. Specifically, we propose the useof uniform pmf as the target distribution to seek high entropy regions. Wedemonstrate the use of our algorithm on medical volumes for left ventricledetection in PET images and tumor localization in brain MR sequences.
arxiv-1310-6719 | Two Dimensional Array Imaging with Beam Steered Data |  http://arxiv.org/abs/1310.6719  | author:Sujeet Patole, Murat Torlak category:cs.CV cs.IT math.IT stat.AP published:2013-10-24 summary:This paper discusses different approaches used for millimeter wave imaging oftwo-dimensional objects. Imaging of a two dimensional object requires reflectedwave data to be collected across two distinct dimensions. In this paper, wepropose a reconstruction method that uses narrowband waveforms along with twodimensional beam steering. The beam is steered in azimuthal and elevationdirection, which forms the two distinct dimensions required for thereconstruction. The Reconstruction technique uses inverse Fourier transformalong with amplitude and phase correction factors. In addition, thisreconstruction technique does not require interpolation of the data in eitherwavenumber or spatial domain. Use of the two dimensional beam steering offersbetter performance in the presence of noise compared with the existing methods,such as switched array imaging system. Effects of RF impairments such asquantization of the phase of beam steering weights and timing jitter which addto phase noise, are analyzed.
arxiv-1310-6319 | Efficient State-Space Inference of Periodic Latent Force Models |  http://arxiv.org/abs/1310.6319  | author:Steven Reece, Stephen Roberts, Siddhartha Ghosh, Alex Rogers, Nicholas Jennings category:stat.ML published:2013-10-23 summary:Latent force models (LFM) are principled approaches to incorporatingsolutions to differential equations within non-parametric inference methods.Unfortunately, the development and application of LFMs can be inhibited bytheir computational cost, especially when closed-form solutions for the LFM areunavailable, as is the case in many real world problems where these latentforces exhibit periodic behaviour. Given this, we develop a new sparserepresentation of LFMs which considerably improves their computationalefficiency, as well as broadening their applicability, in a principled way, todomains with periodic or near periodic latent forces. Our approach uses alinear basis model to approximate one generative model for each periodic force.We assume that the latent forces are generated from Gaussian process priors anddevelop a linear basis model which fully expresses these priors. We apply ourapproach to model the thermal dynamics of domestic buildings and show that itis effective at predicting day-ahead temperatures within the homes. We alsoapply our approach within queueing theory in which quasi-periodic arrival ratesare modelled as latent forces. In both cases, we demonstrate that our approachcan be implemented efficiently using state-space methods which encode thelinear dynamic systems via LFMs. Further, we show that state estimates obtainedusing periodic latent force models can reduce the root mean squared error to17% of that from non-periodic models and 27% of the nearest rival approachwhich is the resonator model.
arxiv-1310-6338 | Risk aversion as an evolutionary adaptation |  http://arxiv.org/abs/1310.6338  | author:Arend Hintze, Randal S. Olson, Christoph Adami, Ralph Hertwig category:q-bio.PE cs.GT cs.NE published:2013-10-23 summary:Risk aversion is a common behavior universal to humans and animals alike.Economists have traditionally defined risk preferences by the curvature of theutility function. Psychologists and behavioral economists also make use ofconcepts such as loss aversion and probability weighting to model riskaversion. Neurophysiological evidence suggests that loss aversion has itsorigins in relatively ancient neural circuitries (e.g., ventral striatum).Could there thus be an evolutionary origin to risk avoidance? We study thisquestion by evolving strategies that adapt to play the equivalent mean payoffgamble. We hypothesize that risk aversion in the equivalent mean payoff gambleis beneficial as an adaptation to living in small groups, and find that apreference for risk averse strategies only evolves in small populations of lessthan 1,000 individuals, while agents exhibit no such strategy preference inlarger populations. Further, we discover that risk aversion can also evolve inlarger populations, but only when the population is segmented into small groupsof around 150 individuals. Finally, we observe that risk aversion only evolveswhen the gamble is a rare event that has a large impact on the individual'sfitness. These findings align with earlier reports that humans lived in smallgroups for a large portion of their evolutionary history. As such, we suggestthat rare, high-risk, high-payoff events such as mating and mate competitioncould have driven the evolution of risk averse behavior in humans living insmall groups.
arxiv-1310-6288 | Spatial-Spectral Boosting Analysis for Stroke Patients' Motor Imagery EEG in Rehabilitation Training |  http://arxiv.org/abs/1310.6288  | author:Hao Zhang, Liqing Zhang category:stat.ML cs.AI cs.LG published:2013-10-23 summary:Current studies about motor imagery based rehabilitation training systems forstroke subjects lack an appropriate analytic method, which can achieve aconsiderable classification accuracy, at the same time detects gradual changesof imagery patterns during rehabilitation process and disinters potentialmechanisms about motor function recovery. In this study, we propose an adaptiveboosting algorithm based on the cortex plasticity and spectral band shifts.This approach models the usually predetermined spatial-spectral configurationsin EEG study into variable preconditions, and introduces a new heuristic ofstochastic gradient boost for training base learners under these preconditions.We compare our proposed algorithm with commonly used methods on datasetscollected from 2 months' clinical experiments. The simulation resultsdemonstrate the effectiveness of the method in detecting the variations ofstroke patients' EEG patterns. By chronologically reorganizing the weightparameters of the learned additive model, we verify the spatial compensatorymechanism on impaired cortex and detect the changes of accentuation bands inspectral domain, which may contribute important prior knowledge forrehabilitation practice.
arxiv-1310-6304 | Combining Structured and Unstructured Randomness in Large Scale PCA |  http://arxiv.org/abs/1310.6304  | author:Nikos Karampatziakis, Paul Mineiro category:cs.LG published:2013-10-23 summary:Principal Component Analysis (PCA) is a ubiquitous tool with manyapplications in machine learning including feature construction, subspaceembedding, and outlier detection. In this paper, we present an algorithm forcomputing the top principal components of a dataset with a large number of rows(examples) and columns (features). Our algorithm leverages both structured andunstructured random projections to retain good accuracy while beingcomputationally efficient. We demonstrate the technique on the winningsubmission the KDD 2010 Cup.
arxiv-1310-6376 | Can Facial Uniqueness be Inferred from Impostor Scores? |  http://arxiv.org/abs/1310.6376  | author:Abhishek Dutta, Raymond Veldhuis, Luuk Spreeuwers category:cs.CV published:2013-10-23 summary:In Biometrics, facial uniqueness is commonly inferred from impostorsimilarity scores. In this paper, we show that such uniqueness measures arehighly unstable in the presence of image quality variations like pose, noiseand blur. We also experimentally demonstrate the instability of a recentlyintroduced impostor-based uniqueness measure of [Klare and Jain 2013] whensubject to poor quality facial images.
arxiv-1310-6092 | A Ray-based Approach for Boundary Estimation of Fiber Bundles Derived from Diffusion Tensor Imaging |  http://arxiv.org/abs/1310.6092  | author:Miriam H. A. Bauer, Sebastiano Barbieri, Jan Klein, Jan Egger, Daniela Kuhnt, Bernd Freisleben, Horst K. Hahn, Christopher Nimsky category:cs.CV published:2013-10-23 summary:Diffusion Tensor Imaging (DTI) is a non-invasive imaging technique thatallows estimation of the location of white matter tracts in-vivo, based on themeasurement of water diffusion properties. For each voxel, a second-ordertensor can be calculated by using diffusion-weighted sequences (DWI) that aresensitive to the random motion of water molecules. Given at least 6diffusion-weighted images with different gradients and one unweighted image,the coefficients of the symmetric diffusion tensor matrix can be calculated.Deriving the eigensystem of the tensor, the eigenvectors and eigenvalues can becalculated to describe the three main directions of diffusion and itsmagnitude. Using DTI data, fiber bundles can be determined, to gain informationabout eloquent brain structures. Especially in neurosurgery, information aboutlocation and dimension of eloquent structures like the corticospinal tract orthe visual pathways is of major interest. Therefore, the fiber bundle boundaryhas to be determined. In this paper, a novel ray-based approach for boundaryestimation of tubular structures is presented.
arxiv-1310-6343 | Provable Bounds for Learning Some Deep Representations |  http://arxiv.org/abs/1310.6343  | author:Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma category:cs.LG cs.AI stat.ML published:2013-10-23 summary:We give algorithms with provable guarantees that learn a class of deep netsin the generative model view popularized by Hinton and others. Our generativemodel is an $n$ node multilayer neural net that has degree at most $n^{\gamma}$for some $\gamma <1$ and each edge has a random edge weight in $[-1,1]$. Ouralgorithm learns {\em almost all} networks in this class with polynomialrunning time. The sample complexity is quadratic or cubic depending upon thedetails of the model. The algorithm uses layerwise learning. It is based upon a novel idea ofobserving correlations among features and using these to infer the underlyingedge structure via a global graph recovery procedure. The analysis of thealgorithm reveals interesting structure of neural networks with random edgeweights.
arxiv-1310-5796 | Relative Deviation Learning Bounds and Generalization with Unbounded Loss Functions |  http://arxiv.org/abs/1310.5796  | author:Corinna Cortes, Spencer Greenberg, Mehryar Mohri category:cs.LG published:2013-10-22 summary:We present an extensive analysis of relative deviation bounds, includingdetailed proofs of two-sided inequalities and their implications. We also givedetailed proofs of two-sided generalization bounds that hold in the generalcase of unbounded loss functions, under the assumption that a moment of theloss is bounded. These bounds are useful in the analysis of importanceweighting and other learning tasks such as unbounded regression.
arxiv-1310-5884 | The optimality of attaching unlinked labels to unlinked meanings |  http://arxiv.org/abs/1310.5884  | author:Ramon Ferrer-i-Cancho category:cs.CL physics.soc-ph published:2013-10-22 summary:Vocabulary learning by children can be characterized by many biases. Whenencountering a new word, children as well as adults, are biased towardsassuming that it means something totally different from the words that theyalready know. To the best of our knowledge, the 1st mathematical proof of theoptimality of this bias is presented here. First, it is shown that this bias isa particular case of the maximization of mutual information between words andmeanings. Second, the optimality is proven within a more general informationtheoretic framework where mutual information maximization competes with otherinformation theoretic principles. The bias is a prediction from moderninformation theory. The relationship between information theoretic principlesand the principles of contrast and mutual exclusivity is also shown.
arxiv-1310-6012 | Evolution of swarming behavior is shaped by how predators attack |  http://arxiv.org/abs/1310.6012  | author:Randal S. Olson, David B. Knoester, Christoph Adami category:q-bio.PE cs.NE published:2013-10-22 summary:Animal grouping behaviors have been widely studied due to their implicationsfor understanding social intelligence, collective cognition, and potentialapplications in engineering, artificial intelligence, and robotics. Animportant biological aspect of these studies is discerning which selectionpressures favor the evolution of grouping behavior. In the past decade,researchers have begun using evolutionary computation to study the evolutionaryeffects of these selection pressures in predator-prey models. The selfish herdhypothesis states that concentrated groups arise because prey selfishly attemptto place their conspecifics between themselves and the predator, thus causingan endless cycle of movement toward the center of the group. Using anevolutionary model of a predator-prey system, we show that how predators attackis critical to the evolution of the selfish herd. Following this discovery, weshow that density-dependent predation provides an abstraction of Hamilton'soriginal formulation of ``domains of danger.'' Finally, we verify thatdensity-dependent predation provides a sufficient selective advantage for preyto evolve the selfish herd in response to predation by coevolving predators.Thus, our work corroborates Hamilton's selfish herd hypothesis in a digitalevolutionary model, refines the assumptions of the selfish herd hypothesis, andgeneralizes the domain of danger concept to density-dependent predation.
arxiv-1310-5963 | Improving the methods of email classification based on words ontology |  http://arxiv.org/abs/1310.5963  | author:Foruzan Kiamarzpour, Rouhollah Dianat, Mohammad bahrani, Mehdi Sadeghzadeh category:cs.IR cs.CL published:2013-10-22 summary:The Internet has dramatically changed the relationship among people and theirrelationships with others people and made the valuable information availablefor the users. Email is the service, which the Internet provides today for itsown users; this service has attracted most of the users' attention due to thelow cost. Along with the numerous benefits of Email, one of the weaknesses ofthis service is that the number of received emails is continually beingenhanced, thus the ways are needed to automatically filter these disturbingletters. Most of these filters utilize a combination of several techniques suchas the Black or white List, using the keywords and so on in order to identifythe spam more accurately In this paper, we introduce a new method to classifythe spam. We are seeking to increase the accuracy of Email classification bycombining the output of several decision trees and the concept of ontology.
arxiv-1310-5965 | Fusion of Hyperspectral and Panchromatic Images using Spectral Uumixing Results |  http://arxiv.org/abs/1310.5965  | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2013-10-22 summary:Hyperspectral imaging, due to providing high spectral resolution images, isone of the most important tools in the remote sensing field. Because oftechnological restrictions hyperspectral sensors has a limited spatialresolution. On the other hand panchromatic image has a better spatialresolution. Combining this information together can provide a betterunderstanding of the target scene. Spectral unmixing of mixed pixels inhyperspectral images results in spectral signature and abundance fractions ofendmembers but gives no information about their location in a mixed pixel. Inthis paper we have used spectral unmixing results of hyperspectral images andsegmentation results of panchromatic image for data fusion. The proposed methodhas been applied on simulated data using AVRIS Indian Pines datasets. Resultsshow that this method can effectively combine information in hyperspectral andpanchromatic images.
arxiv-1310-5781 | RANSAC: Identification of Higher-Order Geometric Features and Applications in Humanoid Robot Soccer |  http://arxiv.org/abs/1310.5781  | author:Madison Flannery, Shannon Fenn, David Budden category:cs.RO cs.AI cs.CV published:2013-10-22 summary:The ability for an autonomous agent to self-localise is directly proportionalto the accuracy and precision with which it can perceive salient featureswithin its local environment. The identification of such features byrecognising geometric profile allows robustness against lighting variations,which is necessary in most industrial robotics applications. This paper detailsa framework by which the random sample consensus (RANSAC) algorithm, oftenapplied to parameter fitting in linear models, can be extended to identifyhigher-order geometric features. Goalpost identification within humanoid robotsoccer is investigated as an application, with the developed system yielding anorder-of-magnitude improvement in classification performance relative to atraditional histogramming methodology.
arxiv-1310-5764 | Minimax Optimal Convergence Rates for Estimating Ground Truth from Crowdsourced Labels |  http://arxiv.org/abs/1310.5764  | author:Chao Gao, Dengyong Zhou category:stat.ML math.ST stat.TH published:2013-10-22 summary:Crowdsourcing has become a primary means for label collection in manyreal-world machine learning applications. A classical method for inferring thetrue labels from the noisy labels provided by crowdsourcing workers isDawid-Skene estimator. In this paper, we prove convergence rates of globaloptimizers of Dawid-Skene estimator. The revealed exponent in the rate ofconvergence is shown to be optimal via a lower bound argument. A projected EMalgorithm is analyzed and is shown to achieve nearly the same exponent as thatof the global optimizers. Our work resolves the long standing issue of whetherDawid-Skene estimator has sound theoretical guarantees besides its goodperformance observed in practice. In addition, a comparative study withmajority voting illustrates both advantages and pitfalls of Dawid-Skeneestimator.
arxiv-1310-5767 | Contextual Hypergraph Modelling for Salient Object Detection |  http://arxiv.org/abs/1310.5767  | author:Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton van den Hengel category:cs.CV published:2013-10-22 summary:Salient object detection aims to locate objects that capture human attentionwithin images. Previous approaches often pose this as a problem of imagecontrast analysis. In this work, we model an image as a hypergraph thatutilizes a set of hyperedges to capture the contextual properties of imagepixels or regions. As a result, the problem of salient object detection becomesone of finding salient vertices and hyperedges in the hypergraph. The mainadvantage of hypergraph modeling is that it takes into account each pixel's (orregion's) affinity with its neighborhood as well as its separation from imagebackground. Furthermore, we propose an alternative approach based oncenter-versus-surround contextual contrast analysis, which performs salientobject detection by optimizing a cost-sensitive support vector machine (SVM)objective function. Experimental results on four challenging datasetsdemonstrate the effectiveness of the proposed approaches against thestate-of-the-art approaches to salient object detection.
arxiv-1310-5999 | Improvement of Automatic Hemorrhages Detection Methods Using Shapes Recognition |  http://arxiv.org/abs/1310.5999  | author:Nidhal Khdhair El Abbadi, Enas Hamood Al Saadi category:cs.CV published:2013-10-22 summary:Diabetic Retinopathy is a medical condition where the retina is damagedbecause fluid leaks from blood vessels into the retina. The presence ofhemorrhages in the retina is the earliest symptom of diabetic retinopathy. Thenumber and shape of hemorrhages is used to indicate the severity of thedisease. Early automated hemorrhage detection can help reduce the incidence ofblindness. This paper introduced new method depending on the hemorrhage shapeto detect the dot hemorrhage (DH), its number, and size at early stage, thiscan be achieved by reducing the retinal image details. Detection and recognizethe DH by following three sequential steps, removing the fovea, removing thevasculature and recognize DH by determining the circularity for all the objectsin the image, finally determine the shape factor which is related to DHrecognition, this stage strengthens the recognition process. The proposedmethod recognizes and separates all the DH.
arxiv-1310-6062 | Combined l_1 and greedy l_0 penalized least squares for linear model selection |  http://arxiv.org/abs/1310.6062  | author:Piotr Pokarowski, Jan Mielniczuk category:stat.ML published:2013-10-22 summary:We introduce a computationally effective algorithm for a linear modelselection consisting of three steps: screening--ordering--selection (SOS).Screening of predictors is based on the thresholded Lasso that is l_1 penalizedleast squares. The screened predictors are then fitted using least squares (LS)and ordered with respect to their t statistics. Finally, a model is selectedusing greedy generalized information criterion (GIC) that is l_0 penalized LSin a nested family induced by the ordering. We give non-asymptotic upper boundson error probability of each step of the SOS algorithm in terms of bothpenalties. Then we obtain selection consistency for different (n, p) scenariosunder conditions which are needed for screening consistency of the Lasso. Forthe traditional setting (n >p) we give Sanov-type bounds on the errorprobabilities of the ordering--selection algorithm. Its surprising consequenceis that the selection error of greedy GIC is asymptotically not larger than ofexhaustive GIC. We also obtain new bounds on prediction and estimation errorsfor the Lasso which are proved in parallel for the algorithm used in practiceand its formal version.
arxiv-1310-6063 | Word Spotting in Cursive Handwritten Documents using Modified Character Shape Codes |  http://arxiv.org/abs/1310.6063  | author:Sayantan Sarkar category:cs.CV published:2013-10-22 summary:There is a large collection of Handwritten English paper documents ofHistorical and Scientific importance. But paper documents are not recognizeddirectly by computer. Hence the closest way of indexing these documents is bystoring their document digital image. Hence a large database of document imagescan replace the paper documents. But the document and data corresponding toeach image cannot be directly recognized by the computer. This paper applies the technique of word spotting using Modified CharacterShape Code to Handwritten English document images for quick and efficient querysearch of words on a database of document images. It is different from otherWord Spotting techniques as it implements two level of selection for wordsegments to match search query. First based on word size and then based oncharacter shape code of query. It makes the process faster and more efficientand reduces the need of multiple pre-processing.
arxiv-1310-6007 | Efficient Optimization for Sparse Gaussian Process Regression |  http://arxiv.org/abs/1310.6007  | author:Yanshuai Cao, Marcus A. Brubaker, David J. Fleet, Aaron Hertzmann category:cs.LG published:2013-10-22 summary:We propose an efficient optimization algorithm for selecting a subset oftraining data to induce sparsity for Gaussian process regression. The algorithmestimates an inducing set and the hyperparameters using a single objective,either the marginal likelihood or a variational free energy. The space and timecomplexity are linear in training set size, and the algorithm can be applied tolarge regression problems on discrete or continuous domains. Empiricalevaluation shows state-of-art performance in discrete cases and competitiveresults in the continuous case.
arxiv-1310-5791 | ROP: Matrix recovery via rank-one projections |  http://arxiv.org/abs/1310.5791  | author:T. Tony Cai, Anru Zhang category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH published:2013-10-22 summary:Estimation of low-rank matrices is of significant interest in a range ofcontemporary applications. In this paper, we introduce a rank-one projectionmodel for low-rank matrix recovery and propose a constrained nuclear normminimization method for stable recovery of low-rank matrices in the noisy case.The procedure is adaptive to the rank and robust against small perturbations.Both upper and lower bounds for the estimation accuracy under the Frobeniusnorm loss are obtained. The proposed estimator is shown to be rate-optimalunder certain conditions. The estimator is easy to implement via convexprogramming and performs well numerically. The techniques and main resultsdeveloped in the paper also have implications to other related statisticalproblems. An application to estimation of spiked covariance matrices fromone-dimensional random projections is considered. The results demonstrate thatit is still possible to accurately estimate the covariance matrix of ahigh-dimensional distribution based only on one-dimensional projections.
arxiv-1310-6066 | Skin Segmentation based Elastic Bunch Graph Matching for efficient multiple Face Recognition |  http://arxiv.org/abs/1310.6066  | author:Sayantan Sarkar category:cs.CV published:2013-10-22 summary:This paper is aimed at developing and combining different algorithms for facedetection and face recognition to generate an efficient mechanism that candetect and recognize the facial regions of input image. For the detection offace from complex region, skin segmentation isolates the face-like regions in acomplex image and following operations of morphology and template matchingrejects false matches to extract facial region. For the recognition of theface, the image database is now converted into a database of facial segments.Hence, implementing the technique of Elastic Bunch Graph matching (EBGM) afterskin segmentation generates Face Bunch Graphs that acutely represents thefeatures of an individual face enhances the quality of the training set. Thisincreases the matching probability significantly.
arxiv-1310-6067 | Multiple Kernel Learning for Brain-Computer Interfacing |  http://arxiv.org/abs/1310.6067  | author:Wojciech Samek, Alexander Binder, Klaus-Robert Müller category:stat.ML published:2013-10-22 summary:Combining information from different sources is a common way to improveclassification accuracy in Brain-Computer Interfacing (BCI). For instance, insmall sample settings it is useful to integrate data from other subjects orsessions in order to improve the estimation quality of the spatial filters orthe classifier. Since data from different subjects may show large variability,it is crucial to weight the contributions according to importance. Manymulti-subject learning algorithms determine the optimal weighting in a separatestep by using heuristics, however, without ensuring that the selected weightsare optimal with respect to classification. In this work we apply MultipleKernel Learning (MKL) to this problem. MKL has been widely used for featurefusion in computer vision and allows to simultaneously learn the classifier andthe optimal weighting. We compare the MKL method to two baseline approaches andinvestigate the reasons for performance improvement.
arxiv-1310-5393 | Multi-Task Regularization with Covariance Dictionary for Linear Classifiers |  http://arxiv.org/abs/1310.5393  | author:Fanyi Xiao, Ruikun Luo, Zhiding Yu category:cs.LG published:2013-10-21 summary:In this paper we propose a multi-task linear classifier learning problemcalled D-SVM (Dictionary SVM). D-SVM uses a dictionary of parameter covarianceshared by all tasks to do multi-task knowledge transfer among different tasks.We formally define the learning problem of D-SVM and show two interpretationsof this problem, from both the probabilistic and kernel perspectives. From theprobabilistic perspective, we show that our learning formulation is actually aMAP estimation on all optimization variables. We also show its equivalence to amultiple kernel learning problem in which one is trying to find a re-weightingkernel for features from a dictionary of basis (despite the fact that onlylinear classifiers are learned). Finally, we describe an alternativeoptimization scheme to minimize the objective function and present empiricalstudies to valid our algorithm.
arxiv-1310-5415 | Disease Prediction based on Functional Connectomes using a Scalable and Spatially-Informed Support Vector Machine |  http://arxiv.org/abs/1310.5415  | author:Takanori Watanabe, Daniel Kessler, Clayton Scott, Michael Angstadt, Chandra Sripada category:stat.ML published:2013-10-21 summary:Substantial evidence indicates that major psychiatric disorders areassociated with distributed neural dysconnectivity, leading to strong interestin using neuroimaging methods to accurately predict disorder status. In thiswork, we are specifically interested in a multivariate approach that usesfeatures derived from whole-brain resting state functional connectomes.However, functional connectomes reside in a high dimensional space, whichcomplicates model interpretation and introduces numerous statistical andcomputational challenges. Traditional feature selection techniques are used toreduce data dimensionality, but are blind to the spatial structure of theconnectomes. We propose a regularization framework where the 6-D structure ofthe functional connectome is explicitly taken into account via the fused Lassoor the GraphNet regularizer. Our method only restricts the loss function to beconvex and margin-based, allowing non-differentiable loss functions such as thehinge-loss to be used. Using the fused Lasso or GraphNet regularizer with thehinge-loss leads to a structured sparse support vector machine (SVM) withembedded feature selection. We introduce a novel efficient optimizationalgorithm based on the augmented Lagrangian and the classical alternatingdirection method, which can solve both fused Lasso and GraphNet regularized SVMwith very little modification. We also demonstrate that the inner subproblemsof the algorithm can be solved efficiently in analytic form by coupling thevariable splitting strategy with a data augmentation scheme. Experiments onsimulated data and resting state scans from a large schizophrenia dataset showthat our proposed approach can identify predictive regions that are spatiallycontiguous in the 6-D "connectome space," offering an additional layer ofinterpretability that could provide new insights about various diseaseprocesses.
arxiv-1310-5543 | Universalities of Reproducing Kernels Revisited |  http://arxiv.org/abs/1310.5543  | author:Benxun Wang, Haizhang Zhang category:stat.ML published:2013-10-21 summary:Kernel methods have been widely applied to machine learning and otherquestions of approximating an unknown function from its finite sample data. Toensure arbitrary accuracy of such approximation, various denseness conditionsare imposed on the selected kernel. This note contributes to the study ofuniversal, characteristic, and $C_0$-universal kernels. We first give simpleand direct description of the difference and relation among these three kindsof universalities of kernels. We then focus on translation-invariant andweighted polynomial kernels. A simple and shorter proof of the knowncharacterization of characteristic translation-invariant kernels will bepresented. The main purpose of the note is to give a delicate discussion on theuniversalities of weighted polynomial kernels.
arxiv-1310-5542 | Ship Detection and Segmentation using Image Correlation |  http://arxiv.org/abs/1310.5542  | author:Alexander Kadyrov, Hui Yu, Honghai Liu category:cs.CV published:2013-10-21 summary:There have been intensive research interests in ship detection andsegmentation due to high demands on a wide range of civil applications in thelast two decades. However, existing approaches, which are mainly based onstatistical properties of images, fail to detect smaller ships and boats.Specifically, known techniques are not robust enough in view of inevitablesmall geometric and photometric changes in images consisting of ships. In thispaper a novel approach for ship detection is proposed based on correlation ofmaritime images. The idea comes from the observation that a fine pattern of thesea surface changes considerably from time to time whereas the ship appearancebasically keeps unchanged. We want to examine whether the images have a commonunaltered part, a ship in this case. To this end, we developed a method -Focused Correlation (FC) to achieve robustness to geometric distortions of theimage content. Various experiments have been conducted to evaluate theeffectiveness of the proposed approach.
arxiv-1310-5665 | Learning Theory and Algorithms for Revenue Optimization in Second-Price Auctions with Reserve |  http://arxiv.org/abs/1310.5665  | author:Mehryar Mohri, Andres Muñoz Medina category:cs.LG published:2013-10-21 summary:Second-price auctions with reserve play a critical role for modern searchengine and popular online sites since the revenue of these companies oftendirectly de- pends on the outcome of such auctions. The choice of the reserveprice is the main mechanism through which the auction revenue can be influencedin these electronic markets. We cast the problem of selecting the reserve priceto optimize revenue as a learning problem and present a full theoreticalanalysis dealing with the complex properties of the corresponding lossfunction. We further give novel algorithms for solving this problem and reportthe results of several experiments in both synthetic and real datademonstrating their effectiveness.
arxiv-1310-5715 | Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm |  http://arxiv.org/abs/1310.5715  | author:Deanna Needell, Nathan Srebro, Rachel Ward category:math.NA cs.CV cs.LG math.OC stat.ML published:2013-10-21 summary:We obtain an improved finite-sample guarantee on the linear convergence ofstochastic gradient descent for smooth and strongly convex objectives,improving from a quadratic dependence on the conditioning $(L/\mu)^2$ (where$L$ is a bound on the smoothness and $\mu$ on the strong convexity) to a lineardependence on $L/\mu$. Furthermore, we show how reweighting the samplingdistribution (i.e. importance sampling) is necessary in order to furtherimprove convergence, and obtain a linear dependence in the average smoothness,dominating previous results. We also discuss importance sampling for SGD morebroadly and show how it can improve convergence also in other scenarios. Ourresults are based on a connection we make between SGD and the randomizedKaczmarz algorithm, which allows us to transfer ideas between the separatebodies of literature studying each of the two methods. In particular, we recastthe randomized Kaczmarz algorithm as an instance of SGD, and apply our resultsto prove its exponential convergence, but to the solution of a weighted leastsquares problem rather than the original least squares problem. We then presenta modified Kaczmarz algorithm with partially biased sampling which doesconverge to the original least squares solution with the same exponentialconvergence rate.
arxiv-1310-5568 | Towards Application of the RBNK Model |  http://arxiv.org/abs/1310.5568  | author:Larry Bull category:cs.CE cs.NE published:2013-10-21 summary:The computational modeling of genetic regulatory networks is now commonplace, either by fitting a system to experimental data or by exploring thebehaviour of abstract systems with the aim of identifying underlyingprinciples. This paper presents an approach to the latter, considering theresponse to environmental changes of a well-known model placed upon tunablefitness landscapes. The effects on genome size and gene connectivity areexplored.
arxiv-1310-5426 | MLI: An API for Distributed Machine Learning |  http://arxiv.org/abs/1310.5426  | author:Evan R. Sparks, Ameet Talwalkar, Virginia Smith, Jey Kottalam, Xinghao Pan, Joseph Gonzalez, Michael J. Franklin, Michael I. Jordan, Tim Kraska category:cs.LG cs.DC stat.ML published:2013-10-21 summary:MLI is an Application Programming Interface designed to address thechallenges of building Machine Learn- ing algorithms in a distributed settingbased on data-centric computing. Its primary goal is to simplify thedevelopment of high-performance, scalable, distributed algorithms. Our initialresults show that, relative to existing systems, this interface can be used tobuild distributed implementations of a wide variety of common Machine Learningalgorithms with minimal complexity and highly competitive performance andscalability.
arxiv-1310-5755 | Determination, Calculation and Representation of the Upper and Lower Sealing Zones During Virtual Stenting of Aneurysms |  http://arxiv.org/abs/1310.5755  | author:Jan Egger, Miriam H. A. Bauer, Stefan Großkopf, Christina Biermann, Bernd Freisleben, Christopher Nimsky category:cs.CV physics.med-ph q-bio.TO published:2013-10-21 summary:In this contribution, a novel method for stent simulation in preoperativecomputed tomography angiography (CTA) acquisitions of patients is presentedwhere the sealing zones are automatically calculated and visualized. The methodis eligible for non-bifurcated and bifurcated stents (Y-stents). Results of theproposed stent simulation with an automatic calculation of the sealing zonesfor specific diseases (abdominal aortic aneurysms (AAA), thoracic aorticaneurysms (TAA), iliac aneurysms) are presented. The contribution is organizedas follows. Section 2 presents the proposed approach. In Section 3,experimental results are discussed. Section 4 concludes the contribution andoutlines areas for future work.
arxiv-1310-5738 | A Kernel for Hierarchical Parameter Spaces |  http://arxiv.org/abs/1310.5738  | author:Frank Hutter, Michael A. Osborne category:stat.ML cs.LG published:2013-10-21 summary:We define a family of kernels for mixed continuous/discrete hierarchicalparameter spaces and show that they are positive definite.
arxiv-1310-5438 | Variational Bayesian inference for linear and logistic regression |  http://arxiv.org/abs/1310.5438  | author:Jan Drugowitsch category:stat.ML published:2013-10-21 summary:The article describe the model, derivation, and implementation of variationalBayesian inference for linear and logistic regression, both with and withoutautomatic relevance determination. It has the dual function of acting as atutorial for the derivation of variational Bayesian inference for simplemodels, as well as documenting, and providing brief examples for the MATLABfunctions that implement this inference. These functions are freely availableonline.
arxiv-1310-5666 | Distributed parameter estimation of discrete hierarchical models via marginal likelihoods |  http://arxiv.org/abs/1310.5666  | author:Helene Massam, Nanwei Wang category:stat.ML published:2013-10-21 summary:We consider discrete graphical models Markov with respect to a graph $G$ andpropose two distributed marginal methods to estimate the maximum likelihoodestimate of the canonical parameter of the model. Both methods are based on arelaxation of the marginal likelihood obtained by considering the density ofthe variables represented by a vertex $v$ of $G$ and a neighborhood. The twomethods differ by the size of the neighborhood of $v$. We show that theestimates are consistent and that those obtained with the larger neighborhoodhave smaller asymptotic variance than the ones obtained through the smallerneighborhood.
arxiv-1310-5619 | Devnagari Handwritten Numeral Recognition using Geometric Features and Statistical Combination Classifier |  http://arxiv.org/abs/1310.5619  | author:Vikas J. Dongre, Vijay H. Mankar category:cs.CV published:2013-10-21 summary:This paper presents a Devnagari Numerical recognition method based onstatistical discriminant functions. 17 geometric features based on pixelconnectivity, lines, line directions, holes, image area, perimeter,eccentricity, solidity, orientation etc. are used for representing thenumerals. Five discriminant functions viz. Linear, Quadratic, Diaglinear,Diagquadratic and Mahalanobis distance are used for classification. 1500handwritten numerals are used for training. Another 1500 handwritten numeralsare used for testing. Experimental results show that Linear, Quadratic andMahalanobis discriminant functions provide better results. Results of thesethree Discriminants are fed to a majority voting type Combination classifier.It is found that Combination classifier offers better results over individualclassifiers.
arxiv-1310-5347 | Bayesian Extensions of Kernel Least Mean Squares |  http://arxiv.org/abs/1310.5347  | author:Il Memming Park, Sohan Seth, Steven Van Vaerenbergh category:stat.ML cs.LG published:2013-10-20 summary:The kernel least mean squares (KLMS) algorithm is a computationally efficientnonlinear adaptive filtering method that "kernelizes" the celebrated (linear)least mean squares algorithm. We demonstrate that the least mean squaresalgorithm is closely related to the Kalman filtering, and thus, the KLMS can beinterpreted as an approximate Bayesian filtering method. This allows us tosystematically develop extensions of the KLMS by modifying the underlyingstate-space and observation models. The resulting extensions introduce manydesirable properties such as "forgetting", and the ability to learn fromdiscrete data, while retaining the computational simplicity and time complexityof the original algorithm.
arxiv-1310-5288 | GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian Processes |  http://arxiv.org/abs/1310.5288  | author:Andrew Gordon Wilson, Elad Gilboa, Arye Nehorai, John P. Cunningham category:stat.ML cs.AI cs.LG stat.ME published:2013-10-20 summary:Gaussian processes are typically used for smoothing and interpolation onsmall datasets. We introduce a new Bayesian nonparametric framework -- GPatt --enabling automatic pattern extrapolation with Gaussian processes on largemultidimensional datasets. GPatt unifies and extends highly expressive kernelsand fast exact inference techniques. Without human intervention -- no handcrafting of kernel features, and no sophisticated initialisation procedures --we show that GPatt can solve large scale pattern extrapolation, inpainting, andkernel discovery problems, including a problem with 383400 training points. Wefind that GPatt significantly outperforms popular alternative scalable Gaussianprocess methods in speed and accuracy. Moreover, we discover profounddifferences between each of these methods, suggesting expressive kernels,nonparametric representations, and exact inference are useful for modellinglarge scale multidimensional patterns.
arxiv-1310-5249 | Graph-Based Approaches to Clustering Network-Constrained Trajectory Data |  http://arxiv.org/abs/1310.5249  | author:Mohamed Khalil El Mahrsi, Fabrice Rossi category:cs.LG published:2013-10-19 summary:Clustering trajectory data attracted considerable attention in the last fewyears. Most of prior work assumed that moving objects can move freely in aneuclidean space and did not consider the eventual presence of an underlyingroad network and its influence on evaluating the similarity betweentrajectories. In this paper, we present an approach to clustering suchnetwork-constrained trajectory data. More precisely we aim at discoveringgroups of road segments that are often travelled by the same trajectories. Toachieve this end, we model the interactions between segments w.r.t. theirsimilarity as a weighted graph to which we apply a community detectionalgorithm to discover meaningful clusters. We showcase our proposition throughexperimental results obtained on synthetic datasets.
arxiv-1310-4891 | Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution |  http://arxiv.org/abs/1310.4891  | author:Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, Brian C. Lovell category:cs.CV published:2013-10-18 summary:Recent advances in computer vision and machine learning suggest that a widerange of problems can be addressed more appropriately by consideringnon-Euclidean geometry. In this paper we explore sparse dictionary learningover the space of linear subspaces, which form Riemannian structures known asGrassmann manifolds. To this end, we propose to embed Grassmann manifolds intothe space of symmetric matrices by an isometric mapping, which enables us todevise a closed-form solution for updating a Grassmann dictionary, atom byatom. Furthermore, to handle non-linearity in data, we propose a kernelisedversion of the dictionary learning algorithm. Experiments on severalclassification tasks (face recognition, action recognition, dynamic textureclassification) show that the proposed approach achieves considerableimprovements in discrimination accuracy, in comparison to state-of-the-artmethods such as kernelised Affine Hull Method and graph-embedding Grassmanndiscriminant analysis.
arxiv-1310-4909 | Text Classification For Authorship Attribution Analysis |  http://arxiv.org/abs/1310.4909  | author:M. Sudheep Elayidom, Chinchu Jose, Anitta Puthussery, Neenu K Sasi category:cs.DL cs.CL cs.LG published:2013-10-18 summary:Authorship attribution mainly deals with undecided authorship of literarytexts. Authorship attribution is useful in resolving issues like uncertainauthorship, recognize authorship of unknown texts, spot plagiarism so on.Statistical methods can be used to set apart the approach of an authornumerically. The basic methodologies that are made use in computationalstylometry are word length, sentence length, vocabulary affluence, frequenciesetc. Each author has an inborn style of writing, which is particular tohimself. Statistical quantitative techniques can be used to differentiate theapproach of an author in a numerical way. The problem can be broken down intothree sub problems as author identification, author characterization andsimilarity detection. The steps involved are pre-processing, extractingfeatures, classification and author identification. For this differentclassifiers can be used. Here fuzzy learning classifier and SVM are used. Afterauthor identification the SVM was found to have more accuracy than Fuzzyclassifier. Later combined the classifiers to obtain a better accuracy whencompared to individual SVM and fuzzy classifier.
arxiv-1310-5042 | Distributional semantics beyond words: Supervised learning of analogy and paraphrase |  http://arxiv.org/abs/1310.5042  | author:Peter D. Turney category:cs.LG cs.AI cs.CL cs.IR published:2013-10-18 summary:There have been several efforts to extend distributional semantics beyondindividual words, to measure the similarity of word pairs, phrases, andsentences (briefly, tuples; ordered sets of words, contiguous ornoncontiguous). One way to extend beyond words is to compare two tuples using afunction that combines pairwise similarities between the component words in thetuples. A strength of this approach is that it works with both relationalsimilarity (analogy) and compositional similarity (paraphrase). However, pastwork required hand-coding the combination function for different tasks. Themain contribution of this paper is that combination functions are generated bysupervised learning. We achieve state-of-the-art results in measuringrelational similarity between word pairs (SAT analogies and SemEval~2012 Task2) and measuring compositional similarity between noun-modifier phrases andunigrams (multiple-choice paraphrase questions).
arxiv-1310-5114 | Explore or exploit? A generic model and an exactly solvable case |  http://arxiv.org/abs/1310.5114  | author:Thomas Gueudré, Alexander Dobrinevski, Jean-Philippe Bouchaud category:cs.LG physics.soc-ph q-fin.GN published:2013-10-18 summary:Finding a good compromise between the exploitation of known resources and theexploration of unknown, but potentially more profitable choices, is a generalproblem, which arises in many different scientific disciplines. We propose astylized model for these exploration-exploitation situations, includingpopulation or economic growth, portfolio optimisation, evolutionary dynamics,or the problem of optimal pinning of vortices or dislocations in disorderedmaterials. We find the exact growth rate of this model for tree-like geometriesand prove the existence of an optimal migration rate in this case. Numericalsimulations in the one-dimensional case confirm the generic existence of anoptimum.
arxiv-1310-5035 | Linearized Alternating Direction Method with Parallel Splitting and Adaptive Penalty for Separable Convex Programs in Machine Learning |  http://arxiv.org/abs/1310.5035  | author:Zhouchen Lin, Risheng Liu, Huan Li category:cs.NA cs.LG math.OC stat.ML published:2013-10-18 summary:Many problems in machine learning and other fields can be (re)for-mulated aslinearly constrained separable convex programs. In most of the cases, there aremultiple blocks of variables. However, the traditional alternating directionmethod (ADM) and its linearized version (LADM, obtained by linearizing thequadratic penalty term) are for the two-block case and cannot be naivelygeneralized to solve the multi-block case. So there is great demand onextending the ADM based methods for the multi-block case. In this paper, wepropose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solvemulti-block separable convex programs efficiently. When all the componentobjective functions have bounded subgradients, we obtain convergence resultsthat are stronger than those of ADM and LADM, e.g., allowing the penaltyparameter to be unbounded and proving the sufficient and necessary conditions}for global convergence. We further propose a simple optimality measure andreveal the convergence rate of LADMPSAP in an ergodic sense. For programs withextra convex set constraints, with refined parameter estimation we devise apractical version of LADMPSAP for faster convergence. Finally, we generalizeLADMPSAP to handle programs with more difficult objective functions bylinearizing part of the objective function as well. LADMPSAP is particularlysuitable for sparse representation and low-rank recovery problems because itssubproblems have closed form solutions and the sparsity and low-rankness of theiterates can be preserved during the iteration. It is also highlyparallelizable and hence fits for parallel or distributed computing. Numericalexperiments testify to the advantages of LADMPSAP in speed and numericalaccuracy.
arxiv-1310-5107 | Advances in Hyperspectral Image Classification: Earth monitoring with statistical learning methods |  http://arxiv.org/abs/1310.5107  | author:Gustavo Camps-Valls, Devis Tuia, Lorenzo Bruzzone, Jón Atli Benediktsson category:cs.CV published:2013-10-18 summary:Hyperspectral images show similar statistical properties to natural grayscaleor color photographic images. However, the classification of hyperspectralimages is more challenging because of the very high dimensionality of thepixels and the small number of labeled examples typically available forlearning. These peculiarities lead to particular signal processing problems,mainly characterized by indetermination and complex manifolds. The framework ofstatistical learning has gained popularity in the last decade. New methods havebeen presented to account for the spatial homogeneity of images, to includeuser's interaction via active learning, to take advantage of the manifoldstructure with semisupervised learning, to extract and encode invariances, orto adapt classifiers and image representations to unseen yet similar scenes.This tutuorial reviews the main advances for hyperspectral remote sensing imageclassification through illustrative examples.
arxiv-1310-5034 | A Theoretical and Experimental Comparison of the EM and SEM Algorithm |  http://arxiv.org/abs/1310.5034  | author:Johannes Blömer, Kathrin Bujna, Daniel Kuntze category:cs.LG stat.ML published:2013-10-18 summary:In this paper we provide a new analysis of the SEM algorithm. Unlike previouswork, we focus on the analysis of a single run of the algorithm. First, wediscuss the algorithm for general mixture distributions. Second, we considerGaussian mixture models and show that with high probability the updateequations of the EM algorithm and its stochastic variant are almost the same,given that the input set is sufficiently large. Our experiments confirm thatthis still holds for a large number of successive update steps. In particular,for Gaussian mixture models, we show that the stochastic variant runs nearlytwice as fast.
arxiv-1310-4938 | A Logic-based Approach for Recognizing Textual Entailment Supported by Ontological Background Knowledge |  http://arxiv.org/abs/1310.4938  | author:Andreas Wotzlaw, Ravi Coote category:cs.CL cs.AI cs.LO published:2013-10-18 summary:We present the architecture and the evaluation of a new system forrecognizing textual entailment (RTE). In RTE we want to identify automaticallythe type of a logical relation between two input texts. In particular, we areinterested in proving the existence of an entailment between them. We conceiveour system as a modular environment allowing for a high-coverage syntactic andsemantic text analysis combined with logical inference. For the syntactic andsemantic analysis we combine a deep semantic analysis with a shallow onesupported by statistical models in order to increase the quality and theaccuracy of results. For RTE we use logical inference of first-order employingmodel-theoretic techniques and automated reasoning tools. The inference issupported with problem-relevant background knowledge extracted automaticallyand on demand from external sources like, e.g., WordNet, YAGO, and OpenCyc, orother, more experimental sources with, e.g., manually defined presuppositionresolutions, or with axiomatized general and common sense knowledge. Theresults show that fine-grained and consistent knowledge coming from diversesources is a necessary condition determining the correctness and traceabilityof results.
arxiv-1310-4977 | Learning Tensors in Reproducing Kernel Hilbert Spaces with Multilinear Spectral Penalties |  http://arxiv.org/abs/1310.4977  | author:Marco Signoretto, Lieven De Lathauwer, Johan A. K. Suykens category:cs.LG published:2013-10-18 summary:We present a general framework to learn functions in tensor productreproducing kernel Hilbert spaces (TP-RKHSs). The methodology is based on anovel representer theorem suitable for existing as well as new spectralpenalties for tensors. When the functions in the TP-RKHS are defined on theCartesian product of finite discrete sets, in particular, our main problemformulation admits as a special case existing tensor completion problems. Otherspecial cases include transfer learning with multimodal side information andmultilinear multitask learning. For the latter case, our kernel-based view isinstrumental to derive nonlinear extensions of existing model classes. We givea novel algorithm and show in experiments the usefulness of the proposedextensions.
arxiv-1310-5082 | On the Suitable Domain for SVM Training in Image Coding |  http://arxiv.org/abs/1310.5082  | author:Gustavo Camps-Valls, Juan Gutiérrez, Gabriel Gómez-Pérez, Jesús Malo category:cs.CV cs.LG stat.ML published:2013-10-18 summary:Conventional SVM-based image coding methods are founded on independentlyrestricting the distortion in every image coefficient at some particular imagerepresentation. Geometrically, this implies allowing arbitrary signaldistortions in an $n$-dimensional rectangle defined by the$\varepsilon$-insensitivity zone in each dimension of the selected imagerepresentation domain. Unfortunately, not every image representation domain iswell-suited for such a simple, scalar-wise, approach because statistical and/orperceptual interactions between the coefficients may exist. These interactionsimply that scalar approaches may induce distortions that do not follow theimage statistics and/or are perceptually annoying. Taking into account theserelations would imply using non-rectangular $\varepsilon$-insensitivity regions(allowing coupled distortions in different coefficients), which is beyond theconventional SVM formulation. In this paper, we report a condition on the suitable domain for developingefficient SVM image coding schemes. We analytically demonstrate that no lineardomain fulfills this condition because of the statistical and perceptualinter-coefficient relations that exist in these domains. This theoreticalresult is experimentally confirmed by comparing SVM learning in previouslyreported linear domains and in a recently proposed non-linear perceptual domainthat simultaneously reduces the statistical and perceptual relations (so it iscloser to fulfilling the proposed condition). These results highlight therelevance of an appropriate choice of the image representation before SVMlearning.
arxiv-1310-5095 | Regularization in Relevance Learning Vector Quantization Using l one Norms |  http://arxiv.org/abs/1310.5095  | author:Martin Riedel, Marika Kästner, Fabrice Rossi, Thomas Villmann category:stat.ML cs.LG published:2013-10-18 summary:We propose in this contribution a method for l one regularization inprototype based relevance learning vector quantization (LVQ) for sparserelevance profiles. Sparse relevance profiles in hyperspectral data analysisfade down those spectral bands which are not necessary for classification. Inparticular, we consider the sparsity in the relevance profile enforced by LASSOoptimization. The latter one is obtained by a gradient learning scheme using adifferentiable parametrized approximation of the $l_{1}$-norm, which has anupper error bound. We extend this regularization idea also to the matrixlearning variant of LVQ as the natural generalization of relevance learning.
arxiv-1310-5089 | Kernel Multivariate Analysis Framework for Supervised Subspace Learning: A Tutorial on Linear and Kernel Multivariate Methods |  http://arxiv.org/abs/1310.5089  | author:Jerónimo Arenas-García, Kaare Brandt Petersen, Gustavo Camps-Valls, Lars Kai Hansen category:stat.ML cs.LG published:2013-10-18 summary:Feature extraction and dimensionality reduction are important tasks in manyfields of science dealing with signal processing and analysis. The relevance ofthese techniques is increasing as current sensory devices are developed withever higher resolution, and problems involving multimodal data sources becomemore common. A plethora of feature extraction methods are available in theliterature collectively grouped under the field of Multivariate Analysis (MVA).This paper provides a uniform treatment of several methods: Principal ComponentAnalysis (PCA), Partial Least Squares (PLS), Canonical Correlation Analysis(CCA) and Orthonormalized PLS (OPLS), as well as their non-linear extensionsderived by means of the theory of reproducing kernel Hilbert spaces. We alsoreview their connections to other methods for classification and statisticaldependence estimation, and introduce some recent developments to deal with theextreme cases of large-scale and low-sized problems. To illustrate the wideapplicability of these methods in both classification and regression problems,we analyze their performance in a benchmark of publicly available data sets,and pay special attention to specific real applications involving audioprocessing for music genre prediction and hyperspectral satellite images forEarth and climate monitoring.
arxiv-1310-4945 | A novel sparsity and clustering regularization |  http://arxiv.org/abs/1310.4945  | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.LG cs.CV stat.ML published:2013-10-18 summary:We propose a novel SPARsity and Clustering (SPARC) regularizer, which is amodified version of the previous octagonal shrinkage and clustering algorithmfor regression (OSCAR), where, the proposed regularizer consists of a$K$-sparse constraint and a pair-wise $\ell_{\infty}$ norm restricted on the$K$ largest components in magnitude. The proposed regularizer is able toseparably enforce $K$-sparsity and encourage the non-zeros to be equal inmagnitude. Moreover, it can accurately group the features without shrinkingtheir magnitude. In fact, SPARC is closely related to OSCAR, so that theproximity operator of the former can be efficiently computed based on that ofthe latter, allowing using proximal splitting algorithms to solve problems withSPARC regularization. Experiments on synthetic data and with benchmark breastcancer data show that SPARC is a competitive group-sparsity inducingregularizer for regression and classification.
arxiv-1310-4794 | The Gaussian Radon Transform and Machine Learning |  http://arxiv.org/abs/1310.4794  | author:Irina Holmes, Ambar Sengupta category:stat.ML math.FA published:2013-10-17 summary:There has been growing recent interest in probabilistic interpretations ofkernel-based methods as well as learning in Banach spaces. The absence of auseful Lebesgue measure on an infinite-dimensional reproducing kernel Hilbertspace is a serious obstacle for such stochastic models. We propose anestimation model for the ridge regression problem within the framework ofabstract Wiener spaces and show how the support vector machine solution to suchproblems can be interpreted in terms of the Gaussian Radon transform.
arxiv-1310-4822 | Principal motion components for gesture recognition using a single-example |  http://arxiv.org/abs/1310.4822  | author:Hugo Jair Escalante, Isabelle Guyon, Vassilis Athitsos, Pat Jangyodsuk, Jun Wan category:cs.CV 68T45 published:2013-10-17 summary:This paper introduces principal motion components (PMC), a new method forone-shot gesture recognition. In the considered scenario a singletraining-video is available for each gesture to be recognized, which limits theapplication of traditional techniques (e.g., HMMs). In PMC, a 2D map of motionenergy is obtained per each pair of consecutive frames in a video. Motion mapsassociated to a video are processed to obtain a PCA model, which is used forrecognition under a reconstruction-error approach. The main benefits of theproposed approach are its simplicity, easiness of implementation, competitiveperformance and efficiency. We report experimental results in one-shot gesturerecognition using the ChaLearn Gesture Dataset; a benchmark comprising morethan 50,000 gestures, recorded as both RGB and depth video with a Kinectcamera. Results obtained with PMC are competitive with alternative methodsproposed for the same data set.
arxiv-1310-4713 | Calibration of an Articulated Camera System with Scale Factor Estimation |  http://arxiv.org/abs/1310.4713  | author:Junzhou Chen, Kin Hong Wong category:cs.CV cs.CG published:2013-10-17 summary:Multiple Camera Systems (MCS) have been widely used in many visionapplications and attracted much attention recently. There are two principletypes of MCS, one is the Rigid Multiple Camera System (RMCS); the other is theArticulated Camera System (ACS). In a RMCS, the relative poses (relative 3-Dposition and orientation) between the cameras are invariant. While, in an ACS,the cameras are articulated through movable joints, the relative pose betweenthem may change. Therefore, through calibration of an ACS we want to find notonly the relative poses between the cameras but also the positions of thejoints in the ACS. In this paper, we developed calibration algorithms for the ACS using a simpleconstraint: the joint is fixed relative to the cameras connected with it duringthe transformations of the ACS. When the transformations of the cameras in anACS can be estimated relative to the same coordinate system, the positions ofthe joints in the ACS can be calculated by solving linear equations. However,in a non-overlapping view ACS, only the ego-transformations of the cameras andcan be estimated. We proposed a two-steps method to deal with this problem. Inboth methods, the ACS is assumed to have performed general transformations in astatic environment. The efficiency and robustness of the proposed methods aretested by simulation and real experiments. In the real experiment, theintrinsic and extrinsic parameters of the ACS are obtained simultaneously byour calibration procedure using the same image sequences, no extra datacapturing step is required. The corresponding trajectory is recovered andillustrated using the calibration results of the ACS. Since the estimatedtranslations of different cameras in an ACS may scaled by different scalefactors, a scale factor estimation algorithm is also proposed. To ourknowledge, we are the first to study the calibration of ACS.
arxiv-1310-4849 | On the Bayes-optimality of F-measure maximizers |  http://arxiv.org/abs/1310.4849  | author:Willem Waegeman, Krzysztof Dembczynski, Arkadiusz Jachnik, Weiwei Cheng, Eyke Hullermeier category:stat.ML cs.LG published:2013-10-17 summary:The F-measure, which has originally been introduced in information retrieval,is nowadays routinely used as a performance metric for problems such as binaryclassification, multi-label classification, and structured output prediction.Optimizing this measure is a statistically and computationally challengingproblem, since no closed-form solution exists. Adopting a decision-theoreticperspective, this article provides a formal and experimental analysis ofdifferent approaches for maximizing the F-measure. We start with a Bayes-riskanalysis of related loss functions, such as Hamming loss and subset zero-oneloss, showing that optimizing such losses as a surrogate of the F-measure leadsto a high worst-case regret. Subsequently, we perform a similar type ofanalysis for F-measure maximizing algorithms, showing that such algorithms areapproximate, while relying on additional assumptions regarding the statisticaldistribution of the binary response variables. Furthermore, we present a newalgorithm which is not only computationally efficient but also Bayes-optimal,regardless of the underlying distribution. To this end, the algorithm requiresonly a quadratic (with respect to the number of binary responses) number ofparameters of the joint distribution. We illustrate the practical performanceof all analyzed methods by means of experiments with multi-label classificationproblems.
arxiv-1310-4579 | Discriminative Link Prediction using Local Links, Node Features and Community Structure |  http://arxiv.org/abs/1310.4579  | author:Abir De, Niloy Ganguly, Soumen Chakrabarti category:cs.LG cs.SI physics.soc-ph published:2013-10-17 summary:A link prediction (LP) algorithm is given a graph, and has to rank, for eachnode, other nodes that are candidates for new linkage. LP is strongly motivatedby social search and recommendation applications. LP techniques often focus onglobal properties (graph conductance, hitting or commute times, Katz score) orlocal properties (Adamic-Adar and many variations, or node feature vectors),but rarely combine these signals. Furthermore, neither of these extremesexploit link densities at the intermediate level of communities. In this paperwe describe a discriminative LP algorithm that exploits two new signals. First,a co-clustering algorithm provides community level link density estimates,which are used to qualify observed links with a surprise value. Second, linksin the immediate neighborhood of the link to be predicted are not interpretedat face value, but through a local model of node feature similarities. Thesesignals are combined into a discriminative link predictor. We evaluate the newpredictor using five diverse data sets that are standard in the literature. Wereport on significant accuracy boosts compared to standard LP methods(including Adamic-Adar and random walk). Apart from the new predictor, anothercontribution is a rigorous protocol for benchmarking and reporting LPalgorithms, which reveals the regions of strengths and weaknesses of all thepredictors studied here, and establishes the new proposal as the most robust.
arxiv-1311-0202 | A systematic comparison of supervised classifiers |  http://arxiv.org/abs/1311.0202  | author:D. R. Amancio, C. H. Comin, D. Casanova, G. Travieso, O. M. Bruno, F. A. Rodrigues, L. da F. Costa category:cs.LG published:2013-10-17 summary:Pattern recognition techniques have been employed in a myriad of industrial,medical, commercial and academic applications. To tackle such a diversity ofdata, many techniques have been devised. However, despite the long tradition ofpattern recognition research, there is no technique that yields the bestclassification in all scenarios. Therefore, the consideration of as many aspossible techniques presents itself as an fundamental practice in applicationsaiming at high accuracy. Typical works comparing methods either emphasize theperformance of a given algorithm in validation tests or systematically comparevarious algorithms, assuming that the practical use of these methods is done byexperts. In many occasions, however, researchers have to deal with theirpractical classification tasks without an in-depth knowledge about theunderlying mechanisms behind parameters. Actually, the adequate choice ofclassifiers and parameters alike in such practical circumstances constitutes along-standing problem and is the subject of the current paper. We carried out astudy on the performance of nine well-known classifiers implemented by the Wekaframework and compared the dependence of the accuracy with their configurationparameter configurations. The analysis of performance with default parametersrevealed that the k-nearest neighbors method exceeds by a large margin theother methods when high dimensional datasets are considered. When otherconfiguration of parameters were allowed, we found that it is possible toimprove the quality of SVM in more than 20% even if parameters are setrandomly. Taken together, the investigation conducted in this paper suggeststhat, apart from the SVM implementation, Weka's default configuration ofparameters provides an performance close the one achieved with the optimalconfiguration.
arxiv-1310-5007 | Online Classification Using a Voted RDA Method |  http://arxiv.org/abs/1310.5007  | author:Tianbing Xu, Jianfeng Gao, Lin Xiao, Amelia Regan category:cs.LG stat.ML published:2013-10-17 summary:We propose a voted dual averaging method for online classification problemswith explicit regularization. This method employs the update rule of theregularized dual averaging (RDA) method, but only on the subsequence oftraining examples where a classification error is made. We derive a bound onthe number of mistakes made by this method on the training set, as well as itsgeneralization error rate. We also introduce the concept of relative strengthof regularization, and show how it affects the mistake bound and generalizationperformance. We experimented with the method using $\ell_1$ regularization on alarge-scale natural language processing task, and obtained state-of-the-artclassification performance with fairly sparse models.
arxiv-1310-5008 | Thompson Sampling in Dynamic Systems for Contextual Bandit Problems |  http://arxiv.org/abs/1310.5008  | author:Tianbing Xu, Yaming Yu, John Turner, Amelia Regan category:cs.LG published:2013-10-17 summary:We consider the multiarm bandit problems in the timevarying dynamic systemfor rich structural features. For the nonlinear dynamic model, we propose theapproximate inference for the posterior distributions based on LaplaceApproximation. For the context bandit problems, Thompson Sampling is adoptedbased on the underlying posterior distributions of the parameters. Morespecifically, we introduce the discount decays on the previous samples impactand analyze the different decay rates with the underlying sample dynamics.Consequently, the exploration and exploitation is adaptively tradeoff accordingto the dynamics in the system.
arxiv-1310-4759 | Fine-grained Categorization -- Short Summary of our Entry for the ImageNet Challenge 2012 |  http://arxiv.org/abs/1310.4759  | author:Christoph Göring, Alexander Freytag, Erik Rodner, Joachim Denzler category:cs.CV published:2013-10-17 summary:In this paper, we tackle the problem of visual categorization of dog breeds,which is a surprisingly challenging task due to simultaneously present lowinterclass distances and high intra-class variances. Our approach combinesseveral techniques well known in our community but often not utilized forfine-grained recognition: (1) automatic segmentation, (2) efficient part detection, and (3) combinationof multiple features. In particular, we demonstrate that a simple head detectorembedded in an off-the-shelf recognition pipeline can improve recognitionaccuracy quite significantly, highlighting the importance of part features forfine-grained recognition tasks. Using our approach, we achieved a 24.59% meanaverage precision performance on the Stanford dog dataset.
arxiv-1310-4661 | Minimax rates in permutation estimation for feature matching |  http://arxiv.org/abs/1310.4661  | author:Olivier Collier, Arnak S. Dalalyan category:math.ST cs.LG stat.TH published:2013-10-17 summary:The problem of matching two sets of features appears in various tasks ofcomputer vision and can be often formalized as a problem of permutationestimation. We address this problem from a statistical point of view andprovide a theoretical analysis of the accuracy of several natural estimators.To this end, the minimax rate of separation is investigated and its expressionis obtained as a function of the sample size, noise level and dimension. Weconsider the cases of homoscedastic and heteroscedastic noise and establish, ineach case, tight upper bounds on the separation distance of several estimators.These upper bounds are shown to be unimprovable both in the homoscedastic andheteroscedastic settings. Interestingly, these bounds demonstrate that a phasetransition occurs when the dimension $d$ of the features is of the order of thelogarithm of the number of features $n$. For $d=O(\log n)$, the rate isdimension free and equals $\sigma (\log n)^{1/2}$, where $\sigma$ is the noiselevel. In contrast, when $d$ is larger than $c\log n$ for some constant $c>0$,the minimax rate increases with $d$ and is of the order $\sigma(d\logn)^{1/4}$. We also discuss the computational aspects of the estimators andprovide empirical evidence of their consistency on synthetic data. Finally, weshow that our results extend to more general matching criteria.
arxiv-1310-4375 | Fast Computation of Wasserstein Barycenters |  http://arxiv.org/abs/1310.4375  | author:Marco Cuturi, Arnaud Doucet category:stat.ML published:2013-10-16 summary:We present new algorithms to compute the mean of a set of empiricalprobability measures under the optimal transport metric. This mean, known asthe Wasserstein barycenter, is the measure that minimizes the sum of itsWasserstein distances to each element in that set. We propose two originalalgorithms to compute Wasserstein barycenters that build upon the subgradientmethod. A direct implementation of these algorithms is, however, too costlybecause it would require the repeated resolution of large primal and dualoptimal transport problems to compute subgradients. Extending the work ofCuturi (2013), we propose to smooth the Wasserstein distance used in thedefinition of Wasserstein barycenters with an entropic regularizer and recoverin doing so a strictly convex objective whose gradients can be computed for aconsiderably cheaper computational cost using matrix scaling algorithms. We usethese algorithms to visualize a large family of images and to solve aconstrained clustering problem.
arxiv-1310-4377 | Hierarchical Block Structures and High-resolution Model Selection in Large Networks |  http://arxiv.org/abs/1310.4377  | author:Tiago P. Peixoto category:cs.SI physics.soc-ph stat.ML published:2013-10-16 summary:Discovering and characterizing the large-scale topological features inempirical networks are crucial steps in understanding how complex systemsfunction. However, most existing methods used to obtain the modular structureof networks suffer from serious problems, such as being oblivious to thestatistical evidence supporting the discovered patterns, which results in theinability to separate actual structure from noise. In addition to this, onealso observes a resolution limit on the size of communities, where smaller butwell-defined clusters are not detectable when the network becomes large. Thisphenomenon occurs not only for the very popular approach of modularityoptimization, which lacks built-in statistical validation, but also for moreprincipled methods based on statistical inference and model selection, which doincorporate statistical validation in a formally correct way. Here we constructa nested generative model that, through a complete description of the entirenetwork hierarchy at multiple scales, is capable of avoiding this limitation,and enables the detection of modular structure at levels far beyond thosepossible with current approaches. Even with this increased resolution, themethod is based on the principle of parsimony, and is capable of separatingsignal from noise, and thus will not lead to the identification of spuriousmodules even on sparse networks. Furthermore, it fully generalizes otherapproaches in that it is not restricted to purely assortative mixing patterns,directed or undirected graphs, and ad hoc hierarchical structures such asbinary trees. Despite its general character, the approach is tractable, and canbe combined with advanced techniques of community detection to yield anefficient algorithm that scales well for very large networks.
arxiv-1310-4252 | Multilabel Consensus Classification |  http://arxiv.org/abs/1310.4252  | author:Sihong Xie, Xiangnan Kong, Jing Gao, Wei Fan, Philip S. Yu category:stat.ML cs.LG published:2013-10-16 summary:In the era of big data, a large amount of noisy and incomplete data can becollected from multiple sources for prediction tasks. Combining multiple modelsor data sources helps to counteract the effects of low data quality and thebias of any single model or data source, and thus can improve the robustnessand the performance of predictive models. Out of privacy, storage and bandwidthconsiderations, in certain circumstances one has to combine the predictionsfrom multiple models or data sources to obtain the final predictions withoutaccessing the raw data. Consensus-based prediction combination algorithms areeffective for such situations. However, current research on predictioncombination focuses on the single label setting, where an instance can have oneand only one label. Nonetheless, data nowadays are usually multilabeled, suchthat more than one label have to be predicted at the same time. Directapplications of existing prediction combination methods to multilabel settingscan lead to degenerated performance. In this paper, we address the challengesof combining predictions from multiple multilabel classifiers and propose twonovel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) andMLCM-a (MLCM for microAUC). These algorithms can capture label correlationsthat are common in multilabel classifications, and optimize correspondingperformance metrics. Experimental results on popular multilabel classificationtasks verify the theoretical analysis and effectiveness of the proposedmethods.
arxiv-1310-4362 | Bayesian Information Sharing Between Noise And Regression Models Improves Prediction of Weak Effects |  http://arxiv.org/abs/1310.4362  | author:Jussi Gillberg, Pekka Marttinen, Matti Pirinen, Antti J Kangas, Pasi Soininen, Marjo-Riitta Järvelin, Mika Ala-Korpela, Samuel Kaski category:stat.ML cs.LG published:2013-10-16 summary:We consider the prediction of weak effects in a multiple-output regressionsetup, when covariates are expected to explain a small amount, less than$\approx 1%$, of the variance of the target variables. To facilitate theprediction of the weak effects, we constrain our model structure by introducinga novel Bayesian approach of sharing information between the regression modeland the noise model. Further reduction of the effective number of parameters isachieved by introducing an infinite shrinkage prior and group sparsity in thecontext of the Bayesian reduced rank regression, and using the Bayesianinfinite factor model as a flexible low-rank noise model. In our experimentsthe model incorporating the novelties outperformed alternatives in genomicprediction of rich phenotype data. In particular, the information sharingbetween the noise and regression models led to significant improvement inprediction accuracy.
arxiv-1310-4366 | An FCA-based Boolean Matrix Factorisation for Collaborative Filtering |  http://arxiv.org/abs/1310.4366  | author:Elena Nenova, Dmitry I. Ignatov, Andrey V. Konstantinov category:cs.IR cs.DS stat.ML H.2.8; H.2.3 published:2013-10-16 summary:We propose a new approach for Collaborative Filtering which is based onBoolean Matrix Factorisation (BMF) and Formal Concept Analysis. In a series ofexperiments on real data (Movielens dataset) we compare the approach with theSVD- and NMF-based algorithms in terms of Mean Average Error (MAE). One of theexperimental consequences is that it is enough to have a binary-scaled ratingdata to obtain almost the same quality in terms of MAE by BMF than for theSVD-based algorithm in case of non-scaled data.
arxiv-1310-4495 | Multiple Attractor Cellular Automata (MACA) for Addressing Major Problems in Bioinformatics |  http://arxiv.org/abs/1310.4495  | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi Nedunuri category:cs.CE cs.LG published:2013-10-16 summary:CA has grown as potential classifier for addressing major problems inbioinformatics. Lot of bioinformatics problems like predicting the proteincoding region, finding the promoter region, predicting the structure of proteinand many other problems in bioinformatics can be addressed through CellularAutomata. Even though there are some prediction techniques addressing theseproblems, the approximate accuracy level is very less. An automated procedurewas proposed with MACA (Multiple Attractor Cellular Automata) which can addressall these problems. The genetic algorithm is also used to find rules with goodfitness values. Extensive experiments are conducted for reporting the accuracyof the proposed tool. The average accuracy of MACA when tested with ENCODE,BG570, HMR195, Fickett and Tongue, ASP67 datasets is 78%.
arxiv-1310-4546 | Distributed Representations of Words and Phrases and their Compositionality |  http://arxiv.org/abs/1310.4546  | author:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean category:cs.CL cs.LG stat.ML published:2013-10-16 summary:The recently introduced continuous Skip-gram model is an efficient method forlearning high-quality distributed vector representations that capture a largenumber of precise syntactic and semantic word relationships. In this paper wepresent several extensions that improve both the quality of the vectors and thetraining speed. By subsampling of the frequent words we obtain significantspeedup and also learn more regular word representations. We also describe asimple alternative to the hierarchical softmax called negative sampling. Aninherent limitation of word representations is their indifference to word orderand their inability to represent idiomatic phrases. For example, the meaningsof "Canada" and "Air" cannot be easily combined to obtain "Air Canada".Motivated by this example, we present a simple method for finding phrases intext, and show that learning good vector representations for millions ofphrases is possible.
arxiv-1310-4456 | Inference, Sampling, and Learning in Copula Cumulative Distribution Networks |  http://arxiv.org/abs/1310.4456  | author:Stefan Douglas Webb category:stat.ML cs.LG published:2013-10-16 summary:The cumulative distribution network (CDN) is a recently developed class ofprobabilistic graphical models (PGMs) permitting a copula factorization, inwhich the CDF, rather than the density, is factored. Despite there being muchrecent interest within the machine learning community about copularepresentations, there has been scarce research into the CDN, its amalgamationwith copula theory, and no evaluation of its performance. Algorithms forinference, sampling, and learning in these models are underdeveloped comparedthose of other PGMs, hindering widerspread use. One advantage of the CDN is that it allows the factors to be parameterized ascopulae, combining the benefits of graphical models with those of copulatheory. In brief, the use of a copula parameterization enables greatermodelling flexibility by separating representation of the marginals from thedependence structure, permitting more efficient and robust learning. Anotheradvantage is that the CDN permits the representation of implicit latentvariables, whose parameterization and connectivity are not required to bespecified. Unfortunately, that the model can encode only latent relationshipsbetween variables severely limits its utility. In this thesis, we present inference, learning, and sampling for CDNs, andfurther the state-of-the-art. First, we explain the basics of copula theory andthe representation of copula CDNs. Then, we discuss inference in the models,and develop the first sampling algorithm. We explain standard learning methods,propose an algorithm for learning from data missing completely at random(MCAR), and develop a novel algorithm for learning models of arbitrarytreewidth and size. Properties of the models and algorithms are investigatedthrough Monte Carlo simulations. We conclude with further discussion of theadvantages and limitations of CDNs, and suggest future work.
arxiv-1310-4249 | Mapping the stereotyped behaviour of freely-moving fruit flies |  http://arxiv.org/abs/1310.4249  | author:Gordon J. Berman, Daniel M. Choi, William Bialek, Joshua W. Shaevitz category:q-bio.QM cs.CV physics.bio-ph stat.ML published:2013-10-16 summary:Most animals possess the ability to actuate a vast diversity of movements,ostensibly constrained only by morphology and physics. In practice, however, afrequent assumption in behavioral science is that most of an animal'sactivities can be described in terms of a small set of stereotyped motifs. Herewe introduce a method for mapping the behavioral space of organisms, relyingonly upon the underlying structure of postural movement data to organize andclassify behaviors. We find that six different drosophilid species each performa mix of non-stereotyped actions and over one hundred hierarchically-organized,stereotyped behaviors. Moreover, we use this approach to compare these species'behavioral spaces, systematically identifying subtle behavioral differencesbetween closely-related species.
arxiv-1310-4389 | ImageSpirit: Verbal Guided Image Parsing |  http://arxiv.org/abs/1310.4389  | author:Ming-Ming Cheng, Shuai Zheng, Wen-Yan Lin, Jonathan Warrell, Vibhav Vineet, Paul Sturgess, Nigel Crook, Niloy Mitra, Philip Torr category:cs.GR cs.CV I.3.6; I.4.8 published:2013-10-16 summary:Humans describe images in terms of nouns and adjectives while algorithmsoperate on images represented as sets of pixels. Bridging this gap between howhumans would like to access images versus their typical representation is thegoal of image parsing, which involves assigning object and attribute labels topixel. In this paper we propose treating nouns as object labels and adjectivesas visual attribute labels. This allows us to formulate the image parsingproblem as one of jointly estimating per-pixel object and attribute labels froma set of training images. We propose an efficient (interactive time) solution.Using the extracted labels as handles, our system empowers a user to verballyrefine the results. This enables hands-free parsing of an image into pixel-wiseobject/attribute labels that correspond to human semantics. Verbally selectingobjects of interests enables a novel and natural interaction modality that canpossibly be used to interact with new generation devices (e.g. smart phones,Google Glass, living room devices). We demonstrate our system on a large numberof real-world images with varying complexity. To help understand the tradeoffscompared to traditional mouse based interactions, results are reported for botha large scale quantitative evaluation and a user study.
arxiv-1310-4378 | Efficient Monte Carlo and greedy heuristic for the inference of stochastic block models |  http://arxiv.org/abs/1310.4378  | author:Tiago P. Peixoto category:cs.SI stat.ML published:2013-10-16 summary:We present an efficient algorithm for the inference of stochastic blockmodels in large networks. The algorithm can be used as an optimized Markovchain Monte Carlo (MCMC) method, with a fast mixing time and a much reducedsusceptibility to getting trapped in metastable states, or as a greedyagglomerative heuristic, with an almost linear $O(N\ln^2N)$ complexity, where$N$ is the number of nodes in the network, independent on the number of blocksbeing inferred. We show that the heuristic is capable of delivering resultswhich are indistinguishable from the more exact and numerically expensive MCMCmethod in many artificial and empirical networks, despite being much faster.The method is entirely unbiased towards any specific mixing pattern, and inparticular it does not favor assortative community structures.
arxiv-1310-4227 | On Measure Concentration of Random Maximum A-Posteriori Perturbations |  http://arxiv.org/abs/1310.4227  | author:Francesco Orabona, Tamir Hazan, Anand D. Sarwate, Tommi Jaakkola category:cs.LG math.PR published:2013-10-15 summary:The maximum a-posteriori (MAP) perturbation framework has emerged as a usefulapproach for inference and learning in high dimensional complex models. Bymaximizing a randomly perturbed potential function, MAP perturbations generateunbiased samples from the Gibbs distribution. Unfortunately, the computationalcost of generating so many high-dimensional random variables can beprohibitive. More efficient algorithms use sequential sampling strategies basedon the expected value of low dimensional MAP perturbations. This paper developsnew measure concentration inequalities that bound the number of samples neededto estimate such expected values. Applying the general result to MAPperturbations can yield a more efficient algorithm to approximate sampling fromthe Gibbs distribution. The measure concentration result is of general interestand may be applicable to other areas involving expected estimations.
arxiv-1310-4217 | Optimal Sensor Placement and Enhanced Sparsity for Classification |  http://arxiv.org/abs/1310.4217  | author:B. W. Brunton, S. L. Brunton, J. L. Proctor, J. N. Kutz category:cs.CV published:2013-10-15 summary:The goal of compressive sensing is efficient reconstruction of data from fewmeasurements, sometimes leading to a categorical decision. If onlyclassification is required, reconstruction can be circumvented and themeasurements needed are orders-of-magnitude sparser still. We define enhancedsparsity as the reduction in number of measurements required for classificationover reconstruction. In this work, we exploit enhanced sparsity and learnspatial sensor locations that optimally inform a categorical decision. Thealgorithm solves an l1-minimization to find the fewest entries of the fullmeasurement vector that exactly reconstruct the discriminant vector in featurespace. Once the sensor locations have been identified from the training data,subsequent test samples are classified with remarkable efficiency, achievingperformance comparable to that obtained by discrimination using the full image.Sensor locations may be learned from full images, or from a random subsample ofpixels. For classification between more than two categories, we introduce acoupling parameter whose value tunes the number of sensors selected, tradingaccuracy for economy. We demonstrate the algorithm on example datasets fromimage recognition using PCA for feature extraction and LDA for discrimination;however, the method can be broadly applied to non-image data and adapted towork with other methods for feature extraction and discrimination.
arxiv-1310-4223 | Exact Learning of RNA Energy Parameters From Structure |  http://arxiv.org/abs/1310.4223  | author:Hamidreza Chitsaz, Mohammad Aminisharifabad category:q-bio.BM cs.LG published:2013-10-15 summary:We consider the problem of exact learning of parameters of a linear RNAenergy model from secondary structure data. A necessary and sufficientcondition for learnability of parameters is derived, which is based oncomputing the convex hull of union of translated Newton polytopes of inputsequences. The set of learned energy parameters is characterized as the convexcone generated by the normal vectors to those facets of the resulting polytopethat are incident to the origin. In practice, the sufficient condition may notbe satisfied by the entire training data set; hence, computing a maximal subsetof training data for which the sufficient condition is satisfied is oftendesired. We show that problem is NP-hard in general for an arbitrarydimensional feature space. Using a randomized greedy algorithm, we select asubset of RNA STRAND v2.0 database that satisfies the sufficient condition forseparate A-U, C-G, G-U base pair counting model. The set of learned energyparameters includes experimentally measured energies of A-U, C-G, and G-Upairs; hence, our parameter set is in agreement with the Turner parameters.
arxiv-1310-4485 | The BeiHang Keystroke Dynamics Authentication System |  http://arxiv.org/abs/1310.4485  | author:Juan Liu, Baochang Zhang, Linlin Shen, Jianzhuang Liu, Jason Zhao category:cs.CR cs.LG published:2013-10-15 summary:Keystroke Dynamics is an important biometric solution for personauthentication. Based upon keystroke dynamics, this paper designs an embeddedpassword protection device, develops an online system, collects two publicdatabases for promoting the research on keystroke authentication, exploits theGabor filter bank to characterize keystroke dynamics, and provides benchmarkresults of three popular classification algorithms, one-class support vectormachine, Gaussian classifier, and nearest neighbour classifier.
arxiv-1310-3892 | Ridge Fusion in Statistical Learning |  http://arxiv.org/abs/1310.3892  | author:Bradley S. Price, Charles J. Geyer, Adam J. Rothman category:stat.ML cs.LG stat.CO published:2013-10-15 summary:We propose a penalized likelihood method to jointly estimate multipleprecision matrices for use in quadratic discriminant analysis and model basedclustering. A ridge penalty and a ridge fusion penalty are used to introduceshrinkage and promote similarity between precision matrix estimates. Block-wisecoordinate descent is used for optimization, and validation likelihood is usedfor tuning parameter selection. Our method is applied in quadratic discriminantanalysis and semi-supervised model based clustering.
arxiv-1310-4210 | Demystifying Information-Theoretic Clustering |  http://arxiv.org/abs/1310.4210  | author:Greg Ver Steeg, Aram Galstyan, Fei Sha, Simon DeDeo category:cs.LG cs.IT math.IT stat.ML published:2013-10-15 summary:We propose a novel method for clustering data which is grounded ininformation-theoretic principles and requires no parametric assumptions.Previous attempts to use information theory to define clusters in anassumption-free way are based on maximizing mutual information between data andcluster labels. We demonstrate that this intuition suffers from a fundamentalconceptual flaw that causes clustering performance to deteriorate as the amountof data increases. Instead, we return to the axiomatic foundations ofinformation theory to define a meaningful clustering measure based on thenotion of consistency under coarse-graining for finite data.
arxiv-1310-3745 | Alternating Minimization for Mixed Linear Regression |  http://arxiv.org/abs/1310.3745  | author:Xinyang Yi, Constantine Caramanis, Sujay Sanghavi category:stat.ML published:2013-10-14 summary:Mixed linear regression involves the recovery of two (or more) unknownvectors from unlabeled linear measurements; that is, where each sample comesfrom exactly one of the vectors, but we do not know which one. It is a classicproblem, and the natural and empirically most popular approach to its solutionhas been the EM algorithm. As in other settings, this is prone to bad localminima; however, each iteration is very fast (alternating between guessinglabels, and solving with those labels). In this paper we provide a new initialization procedure for EM, based onfinding the leading two eigenvectors of an appropriate matrix. We then showthat with this, a re-sampled version of the EM algorithm provably converges tothe correct vectors, under natural assumptions on the sampling distribution,and with nearly optimal (unimprovable) sample complexity. This provides notonly the first characterization of EM's performance, but also much lower samplecomplexity as compared to both standard (randomly initialized) EM, and othermethods for this problem.
arxiv-1310-3717 | Misfire Detection in IC Engine using Kstar Algorithm |  http://arxiv.org/abs/1310.3717  | author:Anish Bahri, V Sugumaran, S Babu Devasenapati category:cs.CV published:2013-10-14 summary:Misfire in an IC Engine continues to be a problem leading to reduced fuelefficiency, increased power loss and emissions containing heavy concentrationof hydrocarbons. Misfiring creates a unique vibration pattern attributed to aparticular cylinder. Useful features can be extracted from these patterns andcan be analyzed to detect misfire. Statistical features from these vibrationsignals were extracted. Out of these, useful features were identified using theJ48 decision tree algorithm and selected features were used for classificationusing the Kstar algorithm. In this paper performance analysis of Kstaralgorithm is presented.
arxiv-1310-3609 | Scalable Verification of Markov Decision Processes |  http://arxiv.org/abs/1310.3609  | author:Axel Legay, Sean Sedwards, Louis-Marie Traonouez category:cs.DS cs.DC cs.LG cs.LO published:2013-10-14 summary:Markov decision processes (MDP) are useful to model concurrent processoptimisation problems, but verifying them with numerical methods is oftenintractable. Existing approximative approaches do not scale well and arelimited to memoryless schedulers. Here we present the basis of scalableverification for MDPSs, using an O(1) memory representation ofhistory-dependent schedulers. We thus facilitate scalable learning techniquesand the use of massively parallel verification.
arxiv-1310-3561 | ECA: High Dimensional Elliptical Component Analysis in non-Gaussian Distributions |  http://arxiv.org/abs/1310.3561  | author:Fang Han, Han Liu category:stat.ML published:2013-10-14 summary:We propose a robust alternative to principal component analysis (PCA) --named elliptical component analysis (ECA) -- for analyzing high dimensionalelliptically distributed data. ECA aims at estimating the eigenspace of thecovariance matrix of the elliptical data. To cope with the heavy-tailedelliptical distributions, a multivariate rank statistic is exploited. At themodel-level, we consider two settings that the leading eigenvectors of thecovariance matrix are non-sparse or sparse. Methodologically, we propose ECAprocedures corresponding to both non-sparse and sparse settings. Theoretically,we provide both non-asymptotic and asymptotic analysis in quantifying thetheoretical performances of ECA. Under the non-sparse setting, we show thatECA's performance is highly related to the effective rank of the covariancematrix. Under the sparse setting, the results are in two folds: (i) We showthat the sparse ECA estimator based on a combinatoric program attains theoptimal rate of convergence; (ii) Built upon some recent developments inestimating sparse leading eigenvectors, we show that a computationallyefficient sparse ECA estimator attains the optimal rate of convergence under asuboptimal scaling.
arxiv-1310-3697 | Variance Adjusted Actor Critic Algorithms |  http://arxiv.org/abs/1310.3697  | author:Aviv Tamar, Shie Mannor category:stat.ML cs.LG cs.SY published:2013-10-14 summary:We present an actor-critic framework for MDPs where the objective is thevariance-adjusted expected return. Our critic uses linear functionapproximation, and we extend the concept of compatible features to thevariance-adjusted setting. We present an episodic actor-critic algorithm andshow that it converges almost surely to a locally optimal point of theobjective function.
arxiv-1310-3567 | An Extreme Learning Machine Approach to Predicting Near Chaotic HCCI Combustion Phasing in Real-Time |  http://arxiv.org/abs/1310.3567  | author:Adam Vaughan, Stanislav V. Bohac category:cs.LG cs.CE published:2013-10-14 summary:Fuel efficient Homogeneous Charge Compression Ignition (HCCI) enginecombustion timing predictions must contend with non-linear chemistry,non-linear physics, period doubling bifurcation(s), turbulent mixing, modelparameters that can drift day-to-day, and air-fuel mixture state informationthat cannot typically be resolved on a cycle-to-cycle basis, especially duringtransients. In previous work, an abstract cycle-to-cycle mapping functioncoupled with $\epsilon$-Support Vector Regression was shown to predictexperimentally observed cycle-to-cycle combustion timing over a wide range ofengine conditions, despite some of the aforementioned difficulties. The mainlimitation of the previous approach was that a partially acausual randomlysampled training dataset was used to train proof of concept offlinepredictions. The objective of this paper is to address this limitation byproposing a new online adaptive Extreme Learning Machine (ELM) extension namedWeighted Ring-ELM. This extension enables fully causal combustion timingpredictions at randomly chosen engine set points, and is shown to achieveresults that are as good as or better than the previous offline method. Thebroader objective of this approach is to enable a new class of real-time modelpredictive control strategies for high variability HCCI and, ultimately, tobring HCCI's low engine-out NOx and reduced CO2 emissions to productionengines.
arxiv-1310-3607 | Predicting college basketball match outcomes using machine learning techniques: some results and lessons learned |  http://arxiv.org/abs/1310.3607  | author:Albrecht Zimmermann, Sruthi Moorthy, Zifan Shi category:cs.LG stat.AP published:2013-10-14 summary:Most existing work on predicting NCAAB matches has been developed in astatistical context. Trusting the capabilities of ML techniques, particularlyclassification learners, to uncover the importance of features and learn theirrelationships, we evaluated a number of different paradigms on this task. Inthis paper, we summarize our work, pointing out that attributes seem to be moreimportant than models, and that there seems to be an upper limit to predictivequality.
arxiv-1310-3556 | Identifying Influential Entries in a Matrix |  http://arxiv.org/abs/1310.3556  | author:Abhisek Kundu, Srinivas Nambirajan, Petros Drineas category:cs.NA cs.LG stat.ML published:2013-10-14 summary:For any matrix A in R^(m x n) of rank \rho, we present a probabilitydistribution over the entries of A (the element-wise leverage scores ofequation (2)) that reveals the most influential entries in the matrix. From atheoretical perspective, we prove that sampling at most s = O ((m + n) \rho^2ln (m + n)) entries of the matrix (see eqn. (3) for the precise value of s)with respect to these scores and solving the nuclear norm minimization problemon the sampled entries, reconstructs A exactly. To the best of our knowledge,these are the strongest theoretical guarantees on matrix completion without anyincoherence assumptions on the matrix A. From an experimental perspective, weshow that entries corresponding to high element-wise leverage scores revealstructural properties of the data matrix that are of interest to domainscientists.
arxiv-1310-3863 | Estimating Time-varying Brain Connectivity Networks from Functional MRI Time Series |  http://arxiv.org/abs/1310.3863  | author:Ricardo Pio Monti, Peter Hellyer, David Sharp, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana category:stat.ML stat.AP published:2013-10-14 summary:Understanding the functional architecture of the brain in terms of networksis becoming increasingly common. In most fMRI applications functional networksare assumed to be stationary, resulting in a single network estimated for theentire time course. However recent results suggest that the connectivitybetween brain regions is highly non-stationary even at rest. As a result, thereis a need for new brain imaging methodologies that comprehensively account forthe dynamic (i.e., non-stationary) nature of the fMRI data. In this work wepropose the Smooth Incremental Graphical Lasso Estimation (SINGLE) algorithmwhich estimates dynamic brain networks from fMRI data. We apply the SINGLEalgorithm to functional MRI data from 24 healthy patients performing achoice-response task to demonstrate the dynamic changes in network structurethat accompany a simple but attentionally demanding cognitive task. Using graphtheoretic measures we show that the Right Inferior Frontal Gyrus, frequentlyreported as playing an important role in cognitive control, dynamically changeswith the task. Our results suggest that the Right Inferior Frontal Gyrus playsa fundamental role in the attention and executive function during cognitivelydemanding tasks and may play a key role in regulating the balance between otherbrain regions.
arxiv-1310-3805 | Green Heron Swarm Optimization Algorithm - State-of-the-Art of a New Nature Inspired Discrete Meta-Heuristics |  http://arxiv.org/abs/1310.3805  | author:Chiranjib Sur, Anupam Shukla category:cs.NE published:2013-10-14 summary:Many real world problems are NP-Hard problems are a very large part of themcan be represented as graph based problems. This makes graph theory a veryimportant and prevalent field of study. In this work a new bio-inspiredmeta-heuristics called Green Heron Swarm Optimization (GHOSA) Algorithm isbeing introduced which is inspired by the fishing skills of the bird. Thealgorithm basically suited for graph based problems like combinatorialoptimization etc. However introduction of an adaptive mathematical variationoperator called Location Based Neighbour Influenced Variation (LBNIV) makes itsuitable for high dimensional continuous domain problems. The new algorithm isbeing operated on the traditional benchmark equations and the results arecompared with Genetic Algorithm and Particle Swarm Optimization. The algorithmis also operated on Travelling Salesman Problem, Quadratic Assignment Problem,Knapsack Problem dataset. The procedure to operate the algorithm on theResource Constraint Shortest Path and road network optimization is alsodiscussed. The results clearly demarcates the GHOSA algorithm as an efficientalgorithm specially considering that the number of algorithms for the discreteoptimization is very low and robust and more explorative algorithm is requiredin this age of social networking and mostly graph based problem scenarios.
arxiv-1310-3438 | On Optimal Probabilities in Stochastic Coordinate Descent Methods |  http://arxiv.org/abs/1310.3438  | author:Peter Richtárik, Martin Takáč category:stat.ML cs.DC math.OC published:2013-10-13 summary:We propose and analyze a new parallel coordinate descent method---`NSync---inwhich at each iteration a random subset of coordinates is updated, in parallel,allowing for the subsets to be chosen non-uniformly. We derive convergencerates under a strong convexity assumption, and comment on how to assignprobabilities to the sets to optimize the bound. The complexity and practicalperformance of the method can outperform its uniform variant by an order ofmagnitude. Surprisingly, the strategy of updating a single randomly selectedcoordinate per iteration---with optimal probabilities---may require lessiterations, both in theory and practice, than the strategy of updating allcoordinates at every iteration.
arxiv-1310-3447 | Image Restoration using Total Variation with Overlapping Group Sparsity |  http://arxiv.org/abs/1310.3447  | author:Jun Liu, Ting-Zhu Huang, Ivan W. Selesnick, Xiao-Guang Lv, Po-Yu Chen category:cs.CV math.NA published:2013-10-13 summary:Image restoration is one of the most fundamental issues in imaging science.Total variation (TV) regularization is widely used in image restorationproblems for its capability to preserve edges. In the literature, however, itis also well known for producing staircase-like artifacts. Usually, thehigh-order total variation (HTV) regularizer is an good option except itsover-smoothing property. In this work, we study a minimization problem wherethe objective includes an usual $l_2$ data-fidelity term and an overlappinggroup sparsity total variation regularizer which can avoid staircase effect andallow edges preserving in the restored image. We also proposed a fast algorithmfor solving the corresponding minimization problem and compare our method withthe state-of-the-art TV based methods and HTV based method. The numericalexperiments illustrate the efficiency and effectiveness of the proposed methodin terms of PSNR, relative error and computing time.
arxiv-1310-3452 | Dense Scattering Layer Removal |  http://arxiv.org/abs/1310.3452  | author:Qiong Yan, Li Xu, Jiaya Jia category:cs.CV I.4.1 published:2013-10-13 summary:We propose a new model, together with advanced optimization, to separate athick scattering media layer from a single natural image. It is able to handlechallenging underwater scenes and images taken in fog and sandstorm, both ofwhich are with significantly reduced visibility. Our method addresses thecritical issue -- this is, originally unnoticeable impurities will be greatlymagnified after removing the scattering media layer -- with transmission-awareoptimization. We introduce non-local structure-aware regularization to properlyconstrain transmission estimation without introducing the halo artifacts. Aselective-neighbor criterion is presented to convert the unconventionalconstrained optimization problem to an unconstrained one where the latter canbe efficiently solved.
arxiv-1310-3499 | Forecasting of Events by Tweet Data Mining |  http://arxiv.org/abs/1310.3499  | author:Bohdan Pavlyshenko category:cs.SI cs.CL cs.CY published:2013-10-13 summary:This paper describes the analysis of quantitative characteristics of frequentsets and association rules in the posts of Twitter microblogs related todifferent event discussions. For the analysis, we used a theory of frequentsets, association rules and a theory of formal concept analysis. We revealedthe frequent sets and association rules which characterize the semanticrelations between the concepts of analyzed subjects. The support of somefrequent sets reaches its global maximum before the expected event but withsome time delay. Such frequent sets may be considered as predictive markersthat characterize the significance of expected events for blogosphere users. Weshowed that the time dynamics of confidence in some revealed association rulescan also have predictive characteristics. Exceeding a certain threshold may bea signal for corresponding reaction in the society within the time intervalbetween the maximum and the probable coming of an event. In this paper, weconsidered two types of events: the Olympic tennis tournament final in London,2012 and the prediction of Eurovision 2013 winner.
arxiv-1310-3492 | Predicting Social Links for New Users across Aligned Heterogeneous Social Networks |  http://arxiv.org/abs/1310.3492  | author:Jiawei Zhang, Xiangnan Kong, Philip S. Yu category:cs.SI cs.LG physics.soc-ph published:2013-10-13 summary:Online social networks have gained great success in recent years and many ofthem involve multiple kinds of nodes and complex relationships. Among theserelationships, social links among users are of great importance. Many existinglink prediction methods focus on predicting social links that will appear inthe future among all users based upon a snapshot of the social network. Inreal-world social networks, many new users are joining in the service everyday. Predicting links for new users are more important. Different fromconventional link prediction problems, link prediction for new users are morechallenging due to the following reasons: (1) differences in informationdistributions between new users and the existing active users (i.e., oldusers); (2) lack of information from the new users in the network. We propose alink prediction method called SCAN-PS (Supervised Cross Aligned Networks linkprediction with Personalized Sampling), to solve the link prediction problemfor new users with information transferred from both the existing active usersin the target network and other source networks through aligned accounts. Weproposed a within-target-network personalized sampling method to process theexisting active users' information in order to accommodate the differences ininformation distributions before the intra-network knowledge transfer. SCAN-PScan also exploit information in other source networks, where the user accountsare aligned with the target network. In this way, SCAN-PS could solve the coldstart problem when information of these new users is total absent in the targetnetwork.
arxiv-1310-3500 | Can Twitter Predict Royal Baby's Name ? |  http://arxiv.org/abs/1310.3500  | author:Bohdan Pavlyshenko category:cs.SI cs.CL cs.CY published:2013-10-13 summary:In this paper, we analyze the existence of possible correlation betweenpublic opinion of twitter users and the decision-making of persons who areinfluential in the society. We carry out this analysis on the example of thediscussion of probable name of the British crown baby, born in July, 2013. Inour study, we use the methods of quantitative processing of natural language,the theory of frequent sets, the algorithms of visual displaying of users'communities. We also analyzed the time dynamics of keyword frequencies. Theanalysis showed that the main predictable name was dominating in the spectrumof names before the official announcement. Using the theories of frequent sets,we showed that the full name consisting of three component names was the partof top 5 by the value of support. It was revealed that the structure ofdynamically formed users' communities participating in the discussion isdetermined by only a few leaders who influence significantly the viewpoints ofother users.
arxiv-1310-3407 | Joint Indoor Localization and Radio Map Construction with Limited Deployment Load |  http://arxiv.org/abs/1310.3407  | author:Sameh Sorour, Yves Lostanlen, Shahrokh Valaee category:cs.NI cs.LG published:2013-10-12 summary:One major bottleneck in the practical implementation of received signalstrength (RSS) based indoor localization systems is the extensive deploymentefforts required to construct the radio maps through fingerprinting. In thispaper, we aim to design an indoor localization scheme that can be directlyemployed without building a full fingerprinted radio map of the indoorenvironment. By accumulating the information of localized RSSs, this scheme canalso simultaneously construct the radio map with limited calibration. To designthis scheme, we employ a source data set that possesses the same spatialcorrelation of the RSSs in the indoor environment under study. The knowledge ofthis data set is then transferred to a limited number of calibrationfingerprints and one or several RSS observations with unknown locations, inorder to perform direct localization of these observations using manifoldalignment. We test two different source data sets, namely a simulated radiopropagation map and the environments plan coordinates. For moving users, weexploit the correlation of their observations to improve the localizationaccuracy. The online testing in two indoor environments shows that the plancoordinates achieve better results than the simulated radio maps, and anegligible degradation with 70-85% reduction in calibration load.
arxiv-1310-3399 | An Improved K-means Clustering Based Approach to Detect a DNA Structure in H&E Image of Mouse Tissue Reacted with CD4-Green Antigen |  http://arxiv.org/abs/1310.3399  | author:B U V Prashanth, P Narahari Sastry, V Rajesh category:cs.CV published:2013-10-12 summary:In this manuscript we present the technique to detect and analyze the DNArich structure in Haemotoxylin & Eosin (H&E) image of a tissue treated withanti CD4 green antigen. The detection of DNA rich structure can be consideredas a detection of blue nuclei present through the biomedical signal/imageprocessing technique performed on the image of the tissue obtained by theScanning Electron Microscope(SEM). Earlier the tissue treated with the anti CD4green antigen, is stained with the H&E staining solution.
arxiv-1310-3366 | PCG-Cut: Graph Driven Segmentation of the Prostate Central Gland |  http://arxiv.org/abs/1310.3366  | author:Jan Egger category:cs.CV published:2013-10-12 summary:Prostate cancer is the most abundant cancer in men, with over 200,000expected new cases and around 28,000 deaths in 2012 in the US alone. In thisstudy, the segmentation results for the prostate central gland (PCG) in MRscans are presented. The aim of this research study is to apply a graph-basedalgorithm to automated segmentation (i.e. delineation) of organ limits for theprostate central gland. The ultimate goal is to apply automated segmentationapproach to facilitate efficient MR-guided biopsy and radiation treatmentplanning. The automated segmentation algorithm used is graph-driven based on aspherical template. Therefore, rays are sent through the surface points of apolyhedron to sample the graph's nodes. After graph construction - which onlyrequires the center of the polyhedron defined by the user and located insidethe prostate center gland - the minimal cost closed set on the graph iscomputed via a polynomial time s-t-cut, which results in the segmentation ofthe prostate center gland's boundaries and volume. The algorithm has beenrealized as a C++ modul within the medical research platform MeVisLab and theground truth of the central gland boundaries were manually extracted byclinical experts (interventional radiologists) with several years of experiencein prostate treatment. For evaluation the automated segmentations of theproposed scheme have been compared with the manual segmentations, yielding anaverage Dice Similarity Coefficient (DSC) of 78.94 +/- 10.85%.
arxiv-1310-3333 | Visualizing Bags of Vectors |  http://arxiv.org/abs/1310.3333  | author:Sriramkumar Balasubramanian, Raghuram Reddy Nagireddy category:cs.IR cs.CL cs.LG published:2013-10-12 summary:The motivation of this work is two-fold - a) to compare between two differentmodes of visualizing data that exists in a bag of vectors format b) to proposea theoretical model that supports a new mode of visualizing data. Visualizinghigh dimensional data can be achieved using Minimum Volume Embedding, but thedata has to exist in a format suitable for computing similarities whilepreserving local distances. This paper compares the visualization between twomethods of representing data and also proposes a new method providing samplevisualizations for that method.
arxiv-1310-3004 | Flexible High-dimensional Classification Machines and Their Asymptotic Properties |  http://arxiv.org/abs/1310.3004  | author:Xingye Qiao, Lingsong Zhang category:stat.ML published:2013-10-11 summary:Classification is an important topic in statistics and machine learning withgreat potential in many real applications. In this paper, we investigate twopopular large margin classification methods, Support Vector Machine (SVM) andDistance Weighted Discrimination (DWD), under two contexts: thehigh-dimensional, low-sample size data and the imbalanced data. A unifiedfamily of classification machines, the FLexible Assortment MachinE (FLAME) isproposed, within which DWD and SVM are special cases. The FLAME family helps toidentify the similarities and differences between SVM and DWD. It is well knownthat many classifiers overfit the data in the high-dimensional setting; andothers are sensitive to the imbalanced data, that is, the class with a largersample size overly influences the classifier and pushes the decision boundarytowards the minority class. SVM is resistant to the imbalanced data issue, butit overfits high-dimensional data sets by showing the undesired data-pilingphenomena. The DWD method was proposed to improve SVM in the high-dimensionalsetting, but its decision boundary is sensitive to the imbalanced ratio ofsample sizes. Our FLAME family helps to understand an intrinsic connectionbetween SVM and DWD, and improves both methods by providing a better trade-offbetween sensitivity to the imbalanced data and overfitting the high-dimensionaldata. Several asymptotic properties of the FLAME classifiers are studied.Simulations and real data applications are investigated to illustrate theusefulness of the FLAME classifiers.
arxiv-1310-2997 | Bandits with Switching Costs: T^{2/3} Regret |  http://arxiv.org/abs/1310.2997  | author:Ofer Dekel, Jian Ding, Tomer Koren, Yuval Peres category:cs.LG math.PR published:2013-10-11 summary:We study the adversarial multi-armed bandit problem in a setting where theplayer incurs a unit cost each time he switches actions. We prove that theplayer's $T$-round minimax regret in this setting is$\widetilde{\Theta}(T^{2/3})$, thereby closing a fundamental gap in ourunderstanding of learning with bandit feedback. In the correspondingfull-information version of the problem, the minimax regret is known to grow ata much slower rate of $\Theta(\sqrt{T})$. The difference between these tworates provides the \emph{first} indication that learning with bandit feedbackcan be significantly harder than learning with full-information feedback(previous results only showed a different dependence on the number of actions,but not on $T$.) In addition to characterizing the inherent difficulty of the multi-armedbandit problem with switching costs, our results also resolve several otheropen problems in online learning. One direct implication is that learning withbandit feedback against bounded-memory adaptive adversaries has a minimaxregret of $\widetilde{\Theta}(T^{2/3})$. Another implication is that theminimax regret of online learning in adversarial Markov decision processes(MDPs) is $\widetilde{\Theta}(T^{2/3})$. The key to all of our results is a newrandomized construction of a multi-scale random walk, which is of independentinterest and likely to prove useful in additional settings.
arxiv-1310-3099 | A Bayesian Network View on Acoustic Model-Based Techniques for Robust Speech Recognition |  http://arxiv.org/abs/1310.3099  | author:Roland Maas, Christian Huemmer, Armin Sehr, Walter Kellermann category:cs.LG cs.CL stat.ML published:2013-10-11 summary:This article provides a unifying Bayesian network view on various approachesfor acoustic model adaptation, missing feature, and uncertainty decoding thatare well-known in the literature of robust automatic speech recognition. Therepresentatives of these classes can often be deduced from a Bayesian networkthat extends the conventional hidden Markov models used in speech recognition.These extensions, in turn, can in many cases be motivated from an underlyingobservation model that relates clean and distorted feature vectors. Byconverting the observation models into a Bayesian network representation, weformulate the corresponding compensation rules leading to a unified view onknown derivations as well as to new formulations for certain approaches. Thegeneric Bayesian perspective provided in this contribution thus highlightsstructural differences and similarities between the analyzed approaches.
arxiv-1310-3003 | Distance-weighted Support Vector Machine |  http://arxiv.org/abs/1310.3003  | author:Xingye Qiao, Lingsong Zhang category:stat.ML published:2013-10-11 summary:A novel linear classification method that possesses the merits of both theSupport Vector Machine (SVM) and the Distance-weighted Discrimination (DWD) isproposed in this article. The proposed Distance-weighted Support Vector Machinemethod can be viewed as a hybrid of SVM and DWD that finds the classificationdirection by minimizing mainly the DWD loss, and determines the intercept termin the SVM manner. We show that our method inheres the merit of DWD, and hence,overcomes the data-piling and overfitting issue of SVM. On the other hand, thenew method is not subject to imbalanced data issue which was a main advantageof SVM over DWD. It uses an unusual loss which combines the Hinge loss (of SVM)and the DWD loss through a trick of axillary hyperplane. Several theoreticalproperties, including Fisher consistency and asymptotic normality of the DWSVMsolution are developed. We use some simulated examples to show that the newmethod can compete DWD and SVM on both classification performance andinterpretability. A real data application further establishes the usefulness ofour approach.
arxiv-1310-3101 | Deep Multiple Kernel Learning |  http://arxiv.org/abs/1310.3101  | author:Eric Strobl, Shyam Visweswaran category:stat.ML cs.LG published:2013-10-11 summary:Deep learning methods have predominantly been applied to large artificialneural networks. Despite their state-of-the-art performance, these largenetworks typically do not generalize well to datasets with limited samplesizes. In this paper, we take a different approach by learning multiple layersof kernels. We combine kernels at each layer and then optimize over an estimateof the support vector machine leave-one-out error rather than the dualobjective function. Our experiments on a variety of datasets show that eachlayer successively increases performance with only a few base kernels.
arxiv-1310-2700 | Analyzing Big Data with Dynamic Quantum Clustering |  http://arxiv.org/abs/1310.2700  | author:M. Weinstein, F. Meirer, A. Hume, Ph. Sciau, G. Shaked, R. Hofstetter, E. Persi, A. Mehta, D. Horn category:cs.LG published:2013-10-10 summary:How does one search for a needle in a multi-dimensional haystack withoutknowing what a needle is and without knowing if there is one in the haystack?This kind of problem requires a paradigm shift - away from hypothesis drivensearches of the data - towards a methodology that lets the data speak foritself. Dynamic Quantum Clustering (DQC) is such a methodology. DQC is apowerful visual method that works with big, high-dimensional data. It exploitsvariations of the density of the data (in feature space) and unearths subsetsof the data that exhibit correlations among all the measured variables. Theoutcome of a DQC analysis is a movie that shows how and why sets of data-pointsare eventually classified as members of simple clusters or as members of - whatwe call - extended structures. This allows DQC to be successfully used in anon-conventional exploratory mode where one searches data for unexpectedinformation without the need to model the data. We show how this works for big,complex, real-world datasets that come from five distinct fields: i.e., x-raynano-chemistry, condensed matter, biology, seismology and finance. Thesestudies show how DQC excels at uncovering unexpected, small - but meaningful -subsets of the data that contain important information. We also establish animportant new result: namely, that big, complex datasets often containinteresting structures that will be missed by many conventional clusteringtechniques. Experience shows that these structures appear frequently enoughthat it is crucial to know they can exist, and that when they do, they encodeimportant hidden information. In short, we not only demonstrate that DQC can beflexibly applied to datasets that present significantly different challenges,we also show how a simple analysis can be used to look for the needle in thehaystack, determine what it is, and find what this means.
arxiv-1310-2905 | Two discussions of the paper "Bayesian measures of model complexity and fit" by D. Spiegelhalter et al., Read before The Royal Statistical Society at a meeting organized by the Research Section on Wednesday, March 13th, 2002 |  http://arxiv.org/abs/1310.2905  | author:E. Moreno, F. -J. Vazquez-Polo, C. P. Robert category:stat.ME stat.ML published:2013-10-10 summary:These are the written discussions of the paper "Bayesian measures of modelcomplexity and fit" by D. Spiegelhalter et al. (2002), following thediscussions given at the Annual Meeting of the Royal Statistical Society inNewcastle-upon-Tyne on September 3rd, 2013.
arxiv-1310-2816 | Gibbs Max-margin Topic Models with Data Augmentation |  http://arxiv.org/abs/1310.2816  | author:Jun Zhu, Ning Chen, Hugh Perkins, Bo Zhang category:stat.ML cs.LG stat.CO stat.ME published:2013-10-10 summary:Max-margin learning is a powerful approach to building classifiers andstructured output predictors. Recent work on max-margin supervised topic modelshas successfully integrated it with Bayesian topic models to discoverdiscriminative latent semantic structures and make accurate predictions forunseen testing data. However, the resulting learning problems are usually hardto solve because of the non-smoothness of the margin loss. Existing approachesto building max-margin supervised topic models rely on an iterative procedureto solve multiple latent SVM subproblems with additional mean-field assumptionson the desired posterior distributions. This paper presents an alternativeapproach by defining a new max-margin loss. Namely, we present Gibbs max-marginsupervised topic models, a latent variable Gibbs classifier to discover hiddentopic representations for various tasks, including classification, regressionand multi-task learning. Gibbs max-margin supervised topic models minimize anexpected margin loss, which is an upper bound of the existing margin lossderived from an expected prediction rule. By introducing augmented variablesand integrating out the Dirichlet variables analytically by conjugacy, wedevelop simple Gibbs sampling algorithms with no restricting assumptions and noneed to solve SVM subproblems. Furthermore, each step of the"augment-and-collapse" Gibbs sampling algorithms has an analytical conditionaldistribution, from which samples can be easily drawn. Experimental resultsdemonstrate significant improvements on time efficiency. The classificationperformance is also significantly improved over competitors on binary,multi-class and multi-label classification tasks.
arxiv-1310-2880 | Feature Selection with Annealing for Computer Vision and Big Data Learning |  http://arxiv.org/abs/1310.2880  | author:Adrian Barbu, Yiyuan She, Liangjing Ding, Gary Gramajo category:stat.ML cs.CV cs.LG math.ST stat.TH published:2013-10-10 summary:Many computer vision and medical imaging problems are faced with learningfrom large-scale datasets, with millions of observations and features. In thispaper we propose a novel efficient learning scheme that tightens a sparsityconstraint by gradually removing variables based on a criterion and a schedule.The attractive fact that the problem size keeps dropping throughout theiterations makes it particularly suitable for big data learning. Our approachapplies generically to the optimization of any differentiable loss function,and finds applications in regression, classification and ranking. The resultantalgorithms build variable screening into estimation and are extremely simple toimplement. We provide theoretical guarantees of convergence and selectionconsistency. In addition, one dimensional piecewise linear response functionsare used to account for nonlinearity and a second order prior is imposed onthese functions to avoid overfitting. Experiments on real and synthetic datashow that the proposed method compares very well with other state of the artmethods in regression, classification and ranking while being computationallyvery efficient and scalable.
arxiv-1310-2959 | Scaling Graph-based Semi Supervised Learning to Large Number of Labels Using Count-Min Sketch |  http://arxiv.org/abs/1310.2959  | author:Partha Pratim Talukdar, William Cohen category:cs.LG published:2013-10-10 summary:Graph-based Semi-supervised learning (SSL) algorithms have been successfullyused in a large number of applications. These methods classify initiallyunlabeled nodes by propagating label information over the structure of graphstarting from seed nodes. Graph-based SSL algorithms usually scale linearlywith the number of distinct labels (m), and require O(m) space on each node.Unfortunately, there exist many applications of practical significance withvery large m over large graphs, demanding better space and time complexity. Inthis paper, we propose MAD-SKETCH, a novel graph-based SSL algorithm whichcompactly stores label distribution on each node using Count-min Sketch, arandomized data structure. We present theoretical analysis showing that undermild conditions, MAD-SKETCH can reduce space complexity at each node from O(m)to O(log m), and achieve similar savings in time complexity as well. We supportour analysis through experiments on multiple real world datasets. We observethat MAD-SKETCH achieves similar performance as existing state-of-the-artgraph- based SSL algorithms, while requiring smaller memory footprint and atthe same time achieving up to 10x speedup. We find that MAD-SKETCH is able toscale to datasets with one million labels, which is beyond the scope ofexisting graph- based SSL algorithms.
arxiv-1310-2916 | From Shading to Local Shape |  http://arxiv.org/abs/1310.2916  | author:Ying Xiong, Ayan Chakrabarti, Ronen Basri, Steven J. Gortler, David W. Jacobs, Todd Zickler category:cs.CV published:2013-10-10 summary:We develop a framework for extracting a concise representation of the shapeinformation available from diffuse shading in a small image patch. Thisproduces a mid-level scene descriptor, comprised of local shape distributionsthat are inferred separately at every image patch across multiple scales. Theframework is based on a quadratic representation of local shape that, in theabsence of noise, has guarantees on recovering accurate local shape andlighting. And when noise is present, the inferred local shape distributionsprovide useful shape information without over-committing to any particularimage explanation. These local shape distributions naturally encode the factthat some smooth diffuse regions are more informative than others, and theyenable efficient and robust reconstruction of object-scale shape. Experimentalresults show that this approach to surface reconstruction compares well againstthe state-of-art on both synthetic images and captured photographs.
arxiv-1310-2805 | MizAR 40 for Mizar 40 |  http://arxiv.org/abs/1310.2805  | author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO cs.MS published:2013-10-10 summary:As a present to Mizar on its 40th anniversary, we develop an AI/ATP systemthat in 30 seconds of real time on a 14-CPU machine automatically proves 40% ofthe theorems in the latest official version of the Mizar Mathematical Library(MML). This is a considerable improvement over previous performance of large-theory AI/ATP methods measured on the whole MML. To achieve that, a large suiteof AI/ATP methods is employed and further developed. We implement the mostuseful methods efficiently, to scale them to the 150000 formulas in MML. Thisreduces the training times over the corpus to 1-3 seconds, allowing a simplepractical deployment of the methods in the online automated reasoning servicefor the Mizar users (MizAR).
arxiv-1310-2797 | Lemma Mining over HOL Light |  http://arxiv.org/abs/1310.2797  | author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO published:2013-10-10 summary:Large formal mathematical libraries consist of millions of atomic inferencesteps that give rise to a corresponding number of proved statements (lemmas).Analogously to the informal mathematical practice, only a tiny fraction of suchstatements is named and re-used in later proofs by formal mathematicians. Inthis work, we suggest and implement criteria defining the estimated usefulnessof the HOL Light lemmas for proving further theorems. We use these criteria tomine the large inference graph of all lemmas in the core HOL Light library,adding thousands of the best lemmas to the pool of named statements that can bere-used in later proofs. The usefulness of the new lemmas is then evaluated bycomparing the performance of automated proving of the core HOL Light theoremswith and without such added lemmas.
arxiv-1310-2955 | Spontaneous Analogy by Piggybacking on a Perceptual System |  http://arxiv.org/abs/1310.2955  | author:Marc Pickett, David W. Aha category:cs.AI cs.LG published:2013-10-10 summary:Most computational models of analogy assume they are given a delineatedsource domain and often a specified target domain. These systems do not addresshow analogs can be isolated from large domains and spontaneously retrieved fromlong-term memory, a process we call spontaneous analogy. We present a systemthat represents relational structures as feature bags. Using thisrepresentation, our system leverages perceptual algorithms to automaticallycreate an ontology of relational structures and to efficiently retrieve analogsfor new relational structures from long-term memory. We provide a demonstrationof our approach that takes a set of unsegmented stories, constructs an ontologyof analogical schemas (corresponding to plot devices), and uses this ontologyto efficiently find analogs within new stories, yielding significanttime-savings over linear analog retrieval at a small accuracy cost.
arxiv-1310-2931 | Feedback Detection for Live Predictors |  http://arxiv.org/abs/1310.2931  | author:Stefan Wager, Nick Chamandy, Omkar Muralidharan, Amir Najmi category:stat.ME cs.LG stat.ML published:2013-10-10 summary:A predictor that is deployed in a live production system may perturb thefeatures it uses to make predictions. Such a feedback loop can occur, forexample, when a model that predicts a certain type of behavior ends up causingthe behavior it predicts, thus creating a self-fulfilling prophecy. In thispaper we analyze predictor feedback detection as a causal inference problem,and introduce a local randomization scheme that can be used to detectnon-linear feedback in real-world problems. We conduct a pilot study for ourproposed methodology using a predictive system currently deployed as a part ofa search engine.
arxiv-1310-2842 | Wavelet methods for shape perception in electro-sensing |  http://arxiv.org/abs/1310.2842  | author:Habib Ammari, Stéphane Mallat, Irène Waldspurger, Han Wang category:math.NA cs.CV published:2013-10-10 summary:This paper aims at presenting a new approach to the electro-sensing problemusing wavelets. It provides an efficient algorithm for recognizing the shape ofa target from micro-electrical impedance measurements. Stability and resolutioncapabilities of the proposed algorithm are quantified in numerical simulations.
arxiv-1310-3233 | Bayesian Estimation of White Matter Atlas from High Angular Resolution Diffusion Imaging |  http://arxiv.org/abs/1310.3233  | author:Jia Du, Alvina Goh, Anqi Qiu category:cs.CV published:2013-10-10 summary:We present a Bayesian probabilistic model to estimate the brain white matteratlas from high angular resolution diffusion imaging (HARDI) data. This modelincorporates a shape prior of the white matter anatomy and the likelihood ofindividual observed HARDI datasets. We first assume that the atlas is generatedfrom a known hyperatlas through a flow of diffeomorphisms and its shape priorcan be constructed based on the framework of large deformation diffeomorphicmetric mapping (LDDMM). LDDMM characterizes a nonlinear diffeomorphic shapespace in a linear space of initial momentum uniquely determining diffeomorphicgeodesic flows from the hyperatlas. Therefore, the shape prior of the HARDIatlas can be modeled using a centered Gaussian random field (GRF) model of theinitial momentum. In order to construct the likelihood of observed HARDIdatasets, it is necessary to study the diffeomorphic transformation ofindividual observations relative to the atlas and the probabilisticdistribution of orientation distribution functions (ODFs). To this end, weconstruct the likelihood related to the transformation using the sameconstruction as discussed for the shape prior of the atlas. The probabilisticdistribution of ODFs is then constructed based on the ODF Riemannian manifold.We assume that the observed ODFs are generated by an exponential map of randomtangent vectors at the deformed atlas ODF. Hence, the likelihood of the ODFscan be modeled using a GRF of their tangent vectors in the ODF Riemannianmanifold. We solve for the maximum a posteriori using theExpectation-Maximization algorithm and derive the corresponding updateequations. Finally, we illustrate the HARDI atlas constructed based on aChinese aging cohort of 94 adults and compare it with that generated byaveraging the coefficients of spherical harmonics of the ODF across subjects.
arxiv-1310-2409 | Discriminative Relational Topic Models |  http://arxiv.org/abs/1310.2409  | author:Ning Chen, Jun Zhu, Fei Xia, Bo Zhang category:cs.LG cs.IR stat.ML published:2013-10-09 summary:Many scientific and engineering fields involve analyzing network data. Fordocument networks, relational topic models (RTMs) provide a probabilisticgenerative process to describe both the link structure and document contents,and they have shown promise on predicting network structures and discoveringlatent topic representations. However, existing RTMs have limitations in boththe restricted model expressiveness and incapability of dealing with imbalancednetwork data. To expand the scope and improve the inference accuracy of RTMs,this paper presents three extensions: 1) unlike the common link likelihood witha diagonal weight matrix that allows the-same-topic interactions only, wegeneralize it to use a full weight matrix that captures all pairwise topicinteractions and is applicable to asymmetric networks; 2) instead of doingstandard Bayesian inference, we perform regularized Bayesian inference(RegBayes) with a regularization parameter to deal with the imbalanced linkstructure issue in common real networks and improve the discriminative abilityof learned latent representations; and 3) instead of doing variationalapproximation with strict mean-field assumptions, we present collapsed Gibbssampling algorithms for the generalized relational topic models by exploringdata augmentation without making restricting assumptions. Under the genericRegBayes framework, we carefully investigate two popular discriminative lossfunctions, namely, the logistic log-loss and the max-margin hinge loss.Experimental results on several real network datasets demonstrate thesignificance of these extensions on improving the prediction performance, andthe time efficiency can be dramatically improved with a simple fastapproximation method.
arxiv-1310-2479 | Spatio-temporal variation of conversational utterances on Twitter |  http://arxiv.org/abs/1310.2479  | author:Christian M. Alis, May T. Lim category:physics.soc-ph cs.CL cs.SI published:2013-10-09 summary:Conversations reflect the existing norms of a language. Previously, we foundthat utterance lengths in English fictional conversations in books and movieshave shortened over a period of 200 years. In this work, we show that thisshortening occurs even for a brief period of 3 years (September 2009-December2012) using 229 million utterances from Twitter. Furthermore, the subset ofgeographically-tagged tweets from the United States show an inverse proportionbetween utterance lengths and the state-level percentage of the Blackpopulation. We argue that shortening of utterances can be explained by theincreasing usage of jargon including coined words.
arxiv-1310-2646 | Localized Iterative Methods for Interpolation in Graph Structured Data |  http://arxiv.org/abs/1310.2646  | author:Sunil K. Narang, Akshay Gadde, Eduard Sanou, Antonio Ortega category:cs.LG published:2013-10-09 summary:In this paper, we present two localized graph filtering based methods forinterpolating graph signals defined on the vertices of arbitrary graphs fromonly a partial set of samples. The first method is an extension of previouswork on reconstructing bandlimited graph signals from partially observedsamples. The iterative graph filtering approach very closely approximates thesolution proposed in the that work, while being computationally more efficient.As an alternative, we propose a regularization based framework in which wedefine the cost of reconstruction to be a combination of smoothness of thegraph signal and the reconstruction error with respect to the known samples,and find solutions that minimize this cost. We provide both a closed formsolution and a computationally efficient iterative solution of the optimizationproblem. The experimental results on the recommendation system datasetsdemonstrate effectiveness of the proposed methods.
arxiv-1310-2627 | A Sparse and Adaptive Prior for Time-Dependent Model Parameters |  http://arxiv.org/abs/1310.2627  | author:Dani Yogatama, Bryan R. Routledge, Noah A. Smith category:stat.ML cs.AI cs.LG published:2013-10-09 summary:We consider the scenario where the parameters of a probabilistic model areexpected to vary over time. We construct a novel prior distribution thatpromotes sparsity and adapts the strength of correlation between parameters atsuccessive timesteps, based on the data. We derive approximate variationalinference procedures for learning and prediction with this prior. We test theapproach on two tasks: forecasting financial quantities from relevant text, andmodeling language contingent on time-varying financial measurements.
arxiv-1310-2418 | Linear Algorithm for Digital Euclidean Connected Skeleton |  http://arxiv.org/abs/1310.2418  | author:Aurélie Leborgne, Julien Mille, Laure Tougne category:cs.CV published:2013-10-09 summary:The skeleton is an essential shape characteristic providing a compactrepresentation of the studied shape. Its computation on the image grid raisesmany issues. Due to the effects of discretization, the required properties ofthe skeleton - thinness, homotopy to the shape, reversibility, connectivity -may become incompatible. However, as regards practical use, the choice of aspecific skeletonization algorithm depends on the application. This allows toclassify the desired properties by order of importance, and tend towards themost critical ones. Our goal is to make a skeleton dedicated to shape matchingfor recognition. So, the discrete skeleton has to be thin - so that it can berepresented by a graph -, robust to noise, reversible - so that the initialshape can be fully reconstructed - and homotopic to the shape. We propose alinear-time skeletonization algorithm based on the squared Euclidean distancemap from which we extract the maximal balls and ridges. After a thinning andpruning process, we obtain the skeleton. The proposed method is finallycompared to fairly recent methods.
arxiv-1310-2641 | Duality in Graphical Models |  http://arxiv.org/abs/1310.2641  | author:Dhafer Malouche, Bala Rajaratnam, Benjamin T. Rolfs category:math.PR stat.ML published:2013-10-09 summary:Graphical models have proven to be powerful tools for representinghigh-dimensional systems of random variables. One example of such a model isthe undirected graph, in which lack of an edge represents conditionalindependence between two random variables given the rest. Another example isthe bidirected graph, in which absence of edges encodes pairwise marginalindependence. Both of these classes of graphical models have been extensivelystudied, and while they are considered to be dual to one another, except in afew instances this duality has not been thoroughly investigated. In this paper,we demonstrate how duality between undirected and bidirected models can be usedto transport results for one class of graphical models to the dual model in atransparent manner. We proceed to apply this technique to extend previouslyexisting results as well as to prove new ones, in three important domains.First, we discuss the pairwise and global Markov properties for undirected andbidirected models, using the pseudographoid and reverse-pseudographoid ruleswhich are weaker conditions than the typically used intersection andcomposition rules. Second, we investigate these pseudographoid and reversepseudographoid rules in the context of probability distributions, using theconcept of duality in the process. Duality allows us to quickly relate them tothe more familiar intersection and composition properties. Third and finally,we apply the dualization method to understand the implications of faithfulness,which in turn leads to a more general form of an existing result.
arxiv-1310-2451 | M-Power Regularized Least Squares Regression |  http://arxiv.org/abs/1310.2451  | author:Julien Audiffren, Hachem Kadri category:stat.ML cs.LG published:2013-10-09 summary:Regularization is used to find a solution that both fits the data and issufficiently smooth, and thereby is very effective for designing and refininglearning algorithms. But the influence of its exponent remains poorlyunderstood. In particular, it is unclear how the exponent of the reproducingkernel Hilbert space (RKHS) regularization term affects the accuracy and theefficiency of kernel-based learning algorithms. Here we consider regularizedleast squares regression (RLSR) with an RKHS regularization raised to the powerof m, where m is a variable real exponent. We design an efficient algorithm forsolving the associated minimization problem, we provide a theoretical analysisof its stability, and we {compare it %/ demonstrate its advantage with respectto computational complexity, speed of convergence and prediction accuracy to%/over} the classical kernel ridge regression algorithm where theregularization exponent m is fixed at 2. Our results show that the m-power RLSRproblem can be solved efficiently, and support the suggestion that one can usea regularization term that grows significantly slower than the standardquadratic growth in the RKHS norm.}
arxiv-1310-2408 | Improved Bayesian Logistic Supervised Topic Models with Data Augmentation |  http://arxiv.org/abs/1310.2408  | author:Jun Zhu, Xun Zheng, Bo Zhang category:cs.LG cs.CL stat.AP stat.ML published:2013-10-09 summary:Supervised topic models with a logistic likelihood have two issues thatpotentially limit their practical use: 1) response variables are usuallyover-weighted by document word counts; and 2) existing variational inferencemethods make strict mean-field assumptions. We address these issues by: 1)introducing a regularization constant to better balance the two parts based onan optimization formulation of Bayesian inference; and 2) developing a simpleGibbs sampling algorithm by introducing auxiliary Polya-Gamma variables andcollapsing out Dirichlet variables. Our augment-and-collapse sampling algorithmhas analytical forms of each conditional distribution without making anyrestricting assumptions and can be easily parallelized. Empirical resultsdemonstrate significant improvements on prediction performance and timeefficiency.
arxiv-1310-2350 | The Generalized Traveling Salesman Problem solved with Ant Algorithms |  http://arxiv.org/abs/1310.2350  | author:Camelia-M. Pintea, Petrica C. Pop, Camelia Chira category:cs.AI cs.NE published:2013-10-09 summary:A well known N P-hard problem called the Generalized Traveling SalesmanProblem (GTSP) is considered. In GTSP the nodes of a complete undirected graphare partitioned into clusters. The objective is to find a minimum cost tourpassing through exactly one node from each cluster. An exact exponential timealgorithm and an effective meta-heuristic algorithm for the problem arepresented. The meta-heuristic proposed is a modified Ant Colony System (ACS)algorithm called Reinforcing Ant Colony System (RACS) which introduces newcorrection rules in the ACS algorithm. Computational results are reported formany standard test problems. The proposed algorithm is competitive with theother already proposed heuristics for the GTSP in both solution quality andcomputational time.
arxiv-1310-7440 | Neural perceptual model to global-local vision for recognition of the logical structure of administrative documents |  http://arxiv.org/abs/1310.7440  | author:Boulbaba Ben Ammar category:cs.CV published:2013-10-09 summary:This paper gives the definition of Transparent Neural Network "TNN" for thesimulation of the globallocal vision and its application to the segmentation ofadministrative document image. We have developed and have adapted a recognitionmethod which models the contextual effects reported from studies inexperimental psychology. Then, we evaluated and tested the TNN and themulti-layer perceptron "MLP", which showed its effectiveness in the field ofthe recognition, in order to show that the TNN is clearer for the user and morepowerful on the level of the recognition. Indeed, the TNN is the only systemwhich makes it possible to recognize the document and its structure.
arxiv-1310-2273 | Semidefinite Programming Based Preconditioning for More Robust Near-Separable Nonnegative Matrix Factorization |  http://arxiv.org/abs/1310.2273  | author:Nicolas Gillis, Stephen A. Vavasis category:stat.ML cs.LG math.OC published:2013-10-08 summary:Nonnegative matrix factorization (NMF) under the separability assumption canprovably be solved efficiently, even in the presence of noise, and has beenshown to be a powerful technique in document classification and hyperspectralunmixing. This problem is referred to as near-separable NMF and requires thatthere exists a cone spanned by a small subset of the columns of the inputnonnegative matrix approximately containing all columns. In this paper, wepropose a preconditioning based on semidefinite programming making the inputmatrix well-conditioned. This in turn can improve significantly the performanceof near-separable NMF algorithms which is illustrated on the popular successiveprojection algorithm (SPA). The new preconditioned SPA is provably more robustto noise, and outperforms SPA on several synthetic data sets. We also show howan active-set method allow us to apply the preconditioning on large-scalereal-world hyperspectral images.
arxiv-1310-2053 | The role of RGB-D benchmark datasets: an overview |  http://arxiv.org/abs/1310.2053  | author:Kai Berger category:cs.CV published:2013-10-08 summary:The advent of the Microsoft Kinect three years ago stimulated not only thecomputer vision community for new algorithms and setups to tackle well-knownproblems in the community but also sparked the launch of several new benchmarkdatasets to which future algorithms can be compared 019 to. This review of theliterature and industry developments concludes that the current RGB-D benchmarkdatasets can be useful to determine the accuracy of a variety of applicationsof a single or multiple RGB-D sensors.
arxiv-1310-2125 | Retrieval of Experiments with Sequential Dirichlet Process Mixtures in Model Space |  http://arxiv.org/abs/1310.2125  | author:Ritabrata Dutta, Sohan Seth, Samuel Kaski category:stat.ML cs.IR stat.AP published:2013-10-08 summary:We address the problem of retrieving relevant experiments given a queryexperiment, motivated by the public databases of datasets in molecular biologyand other experimental sciences, and the need of scientists to relate toearlier work on the level of actual measurement data. Since experiments areinherently noisy and databases ever accumulating, we argue that a retrievalengine should possess two particular characteristics. First, it should comparemodels learnt from the experiments rather than the raw measurements themselves:this allows incorporating experiment-specific prior knowledge to suppress noiseeffects and focus on what is important. Second, it should be updatedsequentially from newly published experiments, without explicitly storingeither the measurements or the models, which is critical for saving storagespace and protecting data privacy: this promotes life long learning. Weformulate the retrieval as a ``supermodelling'' problem, of sequentiallylearning a model of the set of posterior distributions, represented as sets ofMCMC samples, and suggest the use of Particle-Learning-based sequentialDirichlet process mixture (DPM) for this purpose. The relevance measure forretrieval is derived from the supermodel through the mixture representation. Wedemonstrate the performance of the proposed retrieval method on simulated dataand molecular biological experiments.
arxiv-1310-2085 | A Robust Variational Model for Positive Image Deconvolution |  http://arxiv.org/abs/1310.2085  | author:Martin Welk category:cs.CV published:2013-10-08 summary:In this paper, an iterative method for robust deconvolution with positivityconstraints is discussed. It is based on the known variational interpretationof the Richardson-Lucy iterative deconvolution as fixed-point iteration for theminimisation of an information divergence functional under a multiplicativeperturbation model. The asymmetric penaliser function involved in thisfunctional is then modified into a robust penaliser, and complemented with aregulariser. The resulting functional gives rise to a fixed point iterationthat we call robust and regularised Richardson-Lucy deconvolution. It achievesan image restoration quality comparable to state-of-the-art robust variationaldeconvolution with a computational efficiency similar to that of the originalRichardson-Lucy method. Experiments on synthetic and real-world image datademonstrate the performance of the proposed method.
arxiv-1310-7813 | Smoothness-Constrained Image Recovery from Block-Based Random Projections |  http://arxiv.org/abs/1310.7813  | author:Giulio Coluccia, Diego Valsesia, Enrico Magli category:cs.CV cs.IT math.IT published:2013-10-08 summary:In this paper we address the problem of visual quality of imagesreconstructed from block-wise random projections. Independent reconstruction ofthe blocks can severely affect visual quality, by displaying artifacts alongblock borders. We propose a method to enforce smoothness across block bordersby modifying the sensing and reconstruction process so as to employ partiallyoverlapping blocks. The proposed algorithm accomplishes this by computing afast preview from the blocks, whose purpose is twofold. On one hand, it allowsto enforce a set of constraints to drive the reconstruction algorithm towards asmooth solution, imposing the similarity of block borders. On the other hand,the preview is used as a predictor of the entire block, allowing to recover theprediction error, only. The quality improvement over the result of independentreconstruction can be easily assessed both visually and in terms of PSNR andSSIM index.
arxiv-1310-1976 | Feature Selection Strategies for Classifying High Dimensional Astronomical Data Sets |  http://arxiv.org/abs/1310.1976  | author:Ciro Donalek, Arun Kumar A., S. G. Djorgovski, Ashish A. Mahabal, Matthew J. Graham, Thomas J. Fuchs, Michael J. Turmon, N. Sajeeth Philip, Michael Ting-Chang Yang, Giuseppe Longo category:astro-ph.IM cs.CV published:2013-10-08 summary:The amount of collected data in many scientific fields is increasing, all ofthem requiring a common task: extract knowledge from massive, multi parametricdata sets, as rapidly and efficiently possible. This is especially true inastronomy where synoptic sky surveys are enabling new research frontiers in thetime domain astronomy and posing several new object classification challengesin multi dimensional spaces; given the high number of parameters available foreach object, feature selection is quickly becoming a crucial task in analyzingastronomical data sets. Using data sets extracted from the ongoing CatalinaReal-Time Transient Surveys (CRTS) and the Kepler Mission we illustrate avariety of feature selection strategies used to identify the subsets that givethe most information and the results achieved applying these techniques tothree major astronomical problems.
arxiv-1310-2049 | Fast Multi-Instance Multi-Label Learning |  http://arxiv.org/abs/1310.2049  | author:Sheng-Jun Huang, Zhi-Hua Zhou category:cs.LG published:2013-10-08 summary:In many real-world tasks, particularly those involving data objects withcomplicated semantics such as images and texts, one object can be representedby multiple instances and simultaneously be associated with multiple labels.Such tasks can be formulated as multi-instance multi-label learning (MIML)problems, and have been extensively studied during the past few years. ExistingMIML approaches have been found useful in many applications; however, most ofthem can only handle moderate-sized data. To efficiently handle large datasets, in this paper we propose the MIMLfast approach, which first constructs alow-dimensional subspace shared by all labels, and then trains label specificlinear models to optimize approximated ranking loss via stochastic gradientdescent. Although the MIML problem is complicated, MIMLfast is able to achieveexcellent performance by exploiting label relations with shared space anddiscovering sub-concepts for complicated labels. Experiments show that theperformance of MIMLfast is highly competitive to state-of-the-art techniques,whereas its time cost is much less; particularly, on a data set with 20K bagsand 180K instances, MIMLfast is more than 100 times faster than existing MIMLapproaches. On a larger data set where none of existing approaches can returnresults in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach isable to identify the most representative instance for each label, and thusproviding a chance to understand the relation between input patterns and outputlabel semantics.
arxiv-1310-2071 | Predicting Students' Performance Using ID3 And C4.5 Classification Algorithms |  http://arxiv.org/abs/1310.2071  | author:Kalpesh Adhatrao, Aditya Gaykar, Amiraj Dhawan, Rohit Jha, Vipul Honrao category:cs.CY cs.LG published:2013-10-08 summary:An educational institution needs to have an approximate prior knowledge ofenrolled students to predict their performance in future academics. This helpsthem to identify promising students and also provides them an opportunity topay attention to and improve those who would probably get lower grades. As asolution, we have developed a system which can predict the performance ofstudents from their previous performances using concepts of data miningtechniques under Classification. We have analyzed the data set containinginformation about students, such as gender, marks scored in the boardexaminations of classes X and XII, marks and rank in entrance examinations andresults in first year of the previous batch of students. By applying the ID3(Iterative Dichotomiser 3) and C4.5 classification algorithms on this data, wehave predicted the general and individual performance of freshly admittedstudents in future examinations.
arxiv-1310-1975 | ARKref: a rule-based coreference resolution system |  http://arxiv.org/abs/1310.1975  | author:Brendan O'Connor, Michael Heilman category:cs.CL published:2013-10-08 summary:ARKref is a tool for noun phrase coreference. It is a deterministic,rule-based system that uses syntactic information from a constituent parser,and semantic information from an entity recognition component. Its architectureis based on the work of Haghighi and Klein (2009). ARKref was originallywritten in 2009. At the time of writing, the last released version was in March2011. This document describes that version, which is open-source and publiclyavailable at: http://www.ark.cs.cmu.edu/ARKref
arxiv-1401-3230 | Optimization Of Cross Domain Sentiment Analysis Using Sentiwordnet |  http://arxiv.org/abs/1401.3230  | author:K Paramesha, K C Ravishankar category:cs.CL cs.IR published:2013-10-08 summary:The task of sentiment analysis of reviews is carried out using manually built/ automatically generated lexicon resources of their own with which terms arematched with lexicon to compute the term count for positive and negativepolarity. On the other hand the Sentiwordnet, which is quite different fromother lexicon resources that gives scores (weights) of the positive andnegative polarity for each word. The polarity of a word namely positive,negative and neutral have the score ranging between 0 to 1 indicates thestrength/weight of the word with that sentiment orientation. In this paper, weshow that using the Sentiwordnet, how we could enhance the performance of theclassification at both sentence and document level.
arxiv-1310-2527 | Treating clitics with minimalist grammars |  http://arxiv.org/abs/1310.2527  | author:Maxime Amblard category:cs.CL cs.LO published:2013-10-08 summary:We propose an extension of Stabler's version of clitics treatment for a widercoverage of the French language. For this, we present the lexical entriesneeded in the lexicon. Then, we show the recognition of complex syntacticphenomena as (left and right) dislo- cation, clitic climbing over modal andextraction from determiner phrase. The aim of this presentation is thesyntax-semantic interface for clitics analyses in which we will stress onclitic climbing over verb and raising verb.
arxiv-1310-2059 | Distributed Coordinate Descent Method for Learning with Big Data |  http://arxiv.org/abs/1310.2059  | author:Peter Richtárik, Martin Takáč category:stat.ML cs.DC cs.LG math.OC published:2013-10-08 summary:In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent methodfor solving loss minimization problems with big data. We initially partitionthe coordinates (features) and assign each partition to a different node of acluster. At every iteration, each node picks a random subset of the coordinatesfrom those it owns, independently from the other computers, and in parallelcomputes and applies updates to the selected coordinates based on a simpleclosed-form formula. We give bounds on the number of iterations sufficient toapproximately solve the problem with high probability, and show how it dependson the data and on the partitioning. We perform numerical experiments with aLASSO instance described by a 3TB matrix.
arxiv-1310-2063 | Active causation and the origin of meaning |  http://arxiv.org/abs/1310.2063  | author:J. H. van Hateren category:q-bio.PE cs.NE nlin.AO q-bio.NC published:2013-10-08 summary:Purpose and meaning are necessary concepts for understanding mind andculture, but appear to be absent from the physical world and are not part ofthe explanatory framework of the natural sciences. Understanding how meaning(in the broad sense of the term) could arise from a physical world has provento be a tough problem. The basic scheme of Darwinian evolution producesadaptations that only represent apparent ("as if") goals and meaning. Here Iuse evolutionary models to show that a slight, evolvable extension of the basicscheme is sufficient to produce genuine goals. The extension, targetedmodulation of mutation rate, is known to be generally present in biologicalcells, and gives rise to two phenomena that are absent from the non-livingworld: intrinsic meaning and the ability to initiate goal-directed chains ofcausation (active causation). The extended scheme accomplishes this byutilizing randomness modulated by a feedback loop that is itself regulated byevolutionary pressure. The mechanism can be extended to behavioural variabilityas well, and thus shows how freedom of behaviour is possible. A furtherextension to communication suggests that the active exchange of intrinsicmeaning between organisms may be the origin of consciousness, which incombination with active causation can provide a physical basis for thephenomenon of free will.
arxiv-1310-2050 | A State Of the Art Report on Research in Multiple RGB-D sensor Setups |  http://arxiv.org/abs/1310.2050  | author:Kai Berger category:cs.CV published:2013-10-08 summary:That the Microsoft Kinect, an RGB-D sensor, transformed the gaming and endconsumer sector has been anticipated by the developers. That it also impactedin rigorous computer vision research has probably been a surprise to the wholecommunity. Shortly before the commercial deployment of its successor, KinectOne, the research literature fills with resumees and state-of-the art papers tosummarize the development over the past 3 years. This particular reportdescribes significant research projects which have built on sensoring setupsthat include two or more RGB-D sensors in one scene.
arxiv-1310-1811 | End-to-End Text Recognition with Hybrid HMM Maxout Models |  http://arxiv.org/abs/1310.1811  | author:Ouais Alsharif, Joelle Pineau category:cs.CV published:2013-10-07 summary:The problem of detecting and recognizing text in natural scenes has proved tobe more challenging than its counterpart in documents, with most of theprevious work focusing on a single part of the problem. In this work, wepropose new solutions to the character and word recognition problems and thenshow how to combine these solutions in an end-to-end text-recognition system.We do so by leveraging the recently introduced Maxout networks along withhybrid HMM models that have proven useful for voice recognition. Using theseelements, we build a tunable and highly accurate recognition system that beatsstate-of-the-art results on all the sub-problems for both the ICDAR 2003 andSVT benchmark datasets.
arxiv-1310-1803 | A Fast Hadamard Transform for Signals with Sub-linear Sparsity in the Transform Domain |  http://arxiv.org/abs/1310.1803  | author:Robin Scheibler, Saeid Haghighatshoar, Martin Vetterli category:cs.IT math.IT stat.ML published:2013-10-07 summary:A new iterative low complexity algorithm has been presented for computing theWalsh-Hadamard transform (WHT) of an $N$ dimensional signal with a $K$-sparseWHT, where $N$ is a power of two and $K = O(N^\alpha)$, scales sub-linearly in$N$ for some $0 < \alpha < 1$. Assuming a random support model for the non-zerotransform domain components, the algorithm reconstructs the WHT of the signalwith a sample complexity $O(K \log_2(\frac{N}{K}))$, a computational complexity$O(K\log_2(K)\log_2(\frac{N}{K}))$ and with a very high probabilityasymptotically tending to 1. The approach is based on the subsampling (aliasing) property of the WHT,where by a carefully designed subsampling of the time domain signal, one caninduce a suitable aliasing pattern in the transform domain. By treating thealiasing patterns as parity-check constraints and borrowing ideas from erasurecorrecting sparse-graph codes, the recovery of the non-zero spectral values hasbeen formulated as a belief propagation (BP) algorithm (peeling decoding) overa sparse-graph code for the binary erasure channel (BEC). Tools from codingtheory are used to analyze the asymptotic performance of the algorithm in thevery sparse ($\alpha\in(0,\frac{1}{3}]$) and the less sparse($\alpha\in(\frac{1}{3},1)$) regime.
arxiv-1310-1869 | Singular Value Decomposition of Images from Scanned Photographic Plates |  http://arxiv.org/abs/1310.1869  | author:Vasil Kolev, Katya Tsvetkova, Milcho Tsvetkov category:cs.CV astro-ph.IM cs.CE published:2013-10-07 summary:We want to approximate the mxn image A from scanned astronomical photographicplates (from the Sofia Sky Archive Data Center) by using far fewer entries thanin the original matrix. By using rank of a matrix, k we remove the redundantinformation or noise and use as Wiener filter, when rank k<m or k<n. With thisapproximation more than 98% compression ration of image of astronomical platewithout that image details, is obtained. The SVD of images from scannedphotographic plates (SPP) is considered and its possible image compression.
arxiv-1310-1800 | Generalized Negative Binomial Processes and the Representation of Cluster Structures |  http://arxiv.org/abs/1310.1800  | author:Mingyuan Zhou category:stat.ME math.ST stat.ML stat.TH published:2013-10-07 summary:The paper introduces the concept of a cluster structure to define a jointdistribution of the sample size and its exchangeable random partitions. Thecluster structure allows the probability distribution of the random partitionsof a subset of the sample to be dependent on the sample size, a feature notpresented in a partition structure. A generalized negative binomial processcount-mixture model is proposed to generate a cluster structure, where in theprior the number of clusters is finite and Poisson distributed and the clustersizes follow a truncated negative binomial distribution. The number and sizesof clusters can be controlled to exhibit distinct asymptotic behaviors. Uniquemodel properties are illustrated with example clustering results using ageneralized Polya urn sampling scheme. The paper provides new methods togenerate exchangeable random partitions and to control both the cluster-numberand cluster-size distributions.
arxiv-1310-1964 | Named entity recognition using conditional random fields with non-local relational constraints |  http://arxiv.org/abs/1310.1964  | author:Flavio Massimiliano Cecchini, Elisabetta Fersini category:cs.CL published:2013-10-07 summary:We begin by introducing the Computer Science branch of Natural LanguageProcessing, then narrowing the attention on its subbranch of InformationExtraction and particularly on Named Entity Recognition, discussing briefly itsmain methodological approaches. It follows an introduction to state-of-the-artConditional Random Fields under the form of linear chains. Subsequently, theidea of constrained inference as a way to model long-distance relationships ina text is presented, based on an Integer Linear Programming representation ofthe problem. Adding such relationships to the problem as automatically inferredlogical formulas, translatable into linear conditions, we propose to solve theresulting more complex problem with the aid of Lagrangian relaxation, of whichsome technical details are explained. Lastly, we give some experimentalresults.
arxiv-1310-1949 | Least Squares Revisited: Scalable Approaches for Multi-class Prediction |  http://arxiv.org/abs/1310.1949  | author:Alekh Agarwal, Sham M. Kakade, Nikos Karampatziakis, Le Song, Gregory Valiant category:cs.LG stat.ML published:2013-10-07 summary:This work provides simple algorithms for multi-class (and multi-label)prediction in settings where both the number of examples n and the datadimension d are relatively large. These robust and parameter free algorithmsare essentially iterative least-squares updates and very versatile both intheory and in practice. On the theoretical front, we present several variantswith convergence guarantees. Owing to their effective use of second-orderstructure, these algorithms are substantially better than first-order methodsin many practical scenarios. On the empirical side, we present a scalablestagewise variant of our approach, which achieves dramatic computationalspeedups over popular optimization packages such as Liblinear and Vowpal Wabbiton standard datasets (MNIST and CIFAR-10), while attaining state-of-the-artaccuracies.
arxiv-1310-1826 | Learning Non-Parametric Basis Independent Models from Point Queries via Low-Rank Methods |  http://arxiv.org/abs/1310.1826  | author:Hemant Tyagi, Volkan Cevher category:stat.ML math.NA published:2013-10-07 summary:We consider the problem of learning multi-ridge functions of the form f(x) =g(Ax) from point evaluations of f. We assume that the function f is defined onan l_2-ball in R^d, g is twice continuously differentiable almost everywhere,and A \in R^{k \times d} is a rank k matrix, where k << d. We propose arandomized, polynomial-complexity sampling scheme for estimating suchfunctions. Our theoretical developments leverage recent techniques from lowrank matrix recovery, which enables us to derive a polynomial time estimator ofthe function f along with uniform approximation guarantees. We prove that ourscheme can also be applied for learning functions of the form: f(x) =\sum_{i=1}^{k} g_i(a_i^T x), provided f satisfies certain smoothness conditionsin a neighborhood around the origin. We also characterize the noise robustnessof the scheme. Finally, we present numerical examples to illustrate thetheoretical bounds in action.
arxiv-1310-1867 | Mean Field Bayes Backpropagation: scalable training of multilayer neural networks with binary weights |  http://arxiv.org/abs/1310.1867  | author:Daniel Soudry, Ron Meir category:stat.ML published:2013-10-07 summary:Significant success has been reported recently using deep neural networks forclassification. Such large networks can be computationally intensive, evenafter training is over. Implementing these trained networks in hardware chipswith a limited precision of synaptic weights may improve their speed and energyefficiency by several orders of magnitude, thus enabling their integration intosmall and low-power electronic devices. With this motivation, we develop acomputationally efficient learning algorithm for multilayer neural networkswith binary weights, assuming all the hidden neurons have a fan-out of one.This algorithm, derived within a Bayesian probabilistic online setting, isshown to work well for both synthetic and real-world problems, performingcomparably to algorithms with real-valued weights, while retainingcomputational tractability.
arxiv-1310-1757 | A Deep and Tractable Density Estimator |  http://arxiv.org/abs/1310.1757  | author:Benigno Uria, Iain Murray, Hugo Larochelle category:stat.ML cs.LG published:2013-10-07 summary:The Neural Autoregressive Distribution Estimator (NADE) and its real-valuedversion RNADE are competitive density models of multidimensional data across avariety of domains. These models use a fixed, arbitrary ordering of the datadimensions. One can easily condition on variables at the beginning of theordering, and marginalize out variables at the end of the ordering, howeverother inference tasks require approximate inference. In this work we introducean efficient procedure to simultaneously train a NADE model for each possibleordering of the variables, by sharing parameters across all these models. Wecan thus use the most convenient model for each inference task at hand, andensembles of such models with different orderings are immediately available.Moreover, unlike the original NADE, our training procedure scales to deepmodels. Empirically, ensembles of Deep NADE models obtain state of the artdensity estimation performance.
arxiv-1310-1947 | Bayesian Optimization With Censored Response Data |  http://arxiv.org/abs/1310.1947  | author:Frank Hutter, Holger Hoos, Kevin Leyton-Brown category:cs.AI cs.LG stat.ML G.3; G.1.6 published:2013-10-07 summary:Bayesian optimization (BO) aims to minimize a given blackbox function using amodel that is updated whenever new evidence about the function becomesavailable. Here, we address the problem of BO under partially right-censoredresponse data, where in some evaluations we only obtain a lower bound on thefunction value. The ability to handle such response data allows us toadaptively censor costly function evaluations in minimization problems wherethe cost of a function evaluation corresponds to the function value. Oneimportant application giving rise to such censored data is theruntime-minimizing variant of the algorithm configuration problem: findingsettings of a given parametric algorithm that minimize the runtime required forsolving problem instances from a given distribution. We demonstrate thatterminating slow algorithm runs prematurely and handling the resultingright-censored observations can substantially improve the state of the art inmodel-based algorithm configuration.
arxiv-1310-1771 | Potts model, parametric maxflow and k-submodular functions |  http://arxiv.org/abs/1310.1771  | author:Igor Gridchyn, Vladimir Kolmogorov category:cs.CV published:2013-10-07 summary:The problem of minimizing the Potts energy function frequently occurs incomputer vision applications. One way to tackle this NP-hard problem wasproposed by Kovtun [19,20]. It identifies a part of an optimal solution byrunning $k$ maxflow computations, where $k$ is the number of labels. The numberof "labeled" pixels can be significant in some applications, e.g. 50-93% in ourtests for stereo. We show how to reduce the runtime to $O(\log k)$ maxflowcomputations (or one {\em parametric maxflow} computation). Furthermore, theoutput of our algorithm allows to speed-up the subsequent alpha expansion forthe unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al.[7] for {\em Tree Metrics}. We also show a connection to {\em $k$-submodularfunctions} from combinatorial optimization, and discuss {\em $k$-submodularrelaxations} for general energy functions.
arxiv-1310-1855 | Early Fire Detection Using HEP and Space-time Analysis |  http://arxiv.org/abs/1310.1855  | author:Junzhou Chen, Yong You category:cs.CV cs.MM published:2013-10-07 summary:In this article, a video base early fire alarm system is developed bymonitoring the smoke in the scene. There are two major contributions in thiswork. First, to find the best texture feature for smoke detection, a generalframework, named Histograms of Equivalent Patterns (HEP), is adopted to achievean extensive evaluation of various kinds of texture features. Second, the\emph{Block based Inter-Frame Difference} (BIFD) and a improved version ofLBP-TOP are proposed and ensembled to describe the space-time characteristicsof the smoke. In order to reduce the false alarms, the Smoke History Image(SHI) is utilized to register the recent classification results of candidatesmoke blocks. Experimental results using SVM show that the proposed method canachieve better accuracy and less false alarm compared with the state-of-the-arttechnologies.
arxiv-1310-1690 | Online Unsupervised Feature Learning for Visual Tracking |  http://arxiv.org/abs/1310.1690  | author:Fayao Liu, Chunhua Shen, Ian Reid, Anton van den Hengel category:cs.CV published:2013-10-07 summary:Feature encoding with respect to an over-complete dictionary learned byunsupervised methods, followed by spatial pyramid pooling, and linearclassification, has exhibited powerful strength in various vision applications.Here we propose to use the feature learning pipeline for visual tracking.Tracking is implemented using tracking-by-detection and the resulted frameworkis very simple yet effective. First, online dictionary learning is used tobuild a dictionary, which captures the appearance changes of the trackingtarget as well as the background changes. Given a test image window, we extractlocal image patches from it and each local patch is encoded with respect to thedictionary. The encoded features are then pooled over a spatial pyramid to forman aggregated feature vector. Finally, a simple linear classifier is trained onthese features. Our experiments show that the proposed powerful---albeit simple---tracker,outperforms all the state-of-the-art tracking methods that we have tested.Moreover, we evaluate the performance of different dictionary learning andfeature encoding methods in the proposed tracking framework, and analyse theimpact of each component in the tracking scenario. We also demonstrate theflexibility of feature learning by plugging it into Hare et al.'s trackingmethod. The outcome is, to our knowledge, the best tracker ever reported, whichfacilitates the advantages of both feature learning and structured outputprediction.
arxiv-1310-1934 | Discriminative Features via Generalized Eigenvectors |  http://arxiv.org/abs/1310.1934  | author:Nikos Karampatziakis, Paul Mineiro category:cs.LG stat.ML published:2013-10-07 summary:Representing examples in a way that is compatible with the underlyingclassifier can greatly enhance the performance of a learning system. In thispaper we investigate scalable techniques for inducing discriminative featuresby taking advantage of simple second order structure in the data. We focus onmulticlass classification and show that features extracted from the generalizedeigenvectors of the class conditional second moments lead to classifiers withexcellent empirical performance. Moreover, these features have attractivetheoretical properties, such as inducing representations that are invariant tolinear transformations of the input. We evaluate classifiers built from thesefeatures on three different tasks, obtaining state of the art results.
arxiv-1310-1659 | MINT: Mutual Information based Transductive Feature Selection for Genetic Trait Prediction |  http://arxiv.org/abs/1310.1659  | author:Dan He, Irina Rish, David Haws, Simon Teyssedre, Zivan Karaman, Laxmi Parida category:cs.LG cs.CE published:2013-10-07 summary:Whole genome prediction of complex phenotypic traits using high-densitygenotyping arrays has attracted a great deal of attention, as it is relevant tothe fields of plant and animal breeding and genetic epidemiology. As the numberof genotypes is generally much bigger than the number of samples, predictivemodels suffer from the curse-of-dimensionality. The curse-of-dimensionalityproblem not only affects the computational efficiency of a particular genomicselection method, but can also lead to poor performance, mainly due tocorrelation among markers. In this work we proposed the first transductivefeature selection method based on the MRMR (Max-Relevance and Min-Redundancy)criterion which we call MINT. We applied MINT on genetic trait predictionproblems and showed that in general MINT is a better feature selection methodthan the state-of-the-art inductive method mRMR.
arxiv-1310-1840 | Parallel coordinate descent for the Adaboost problem |  http://arxiv.org/abs/1310.1840  | author:Olivier Fercoq category:cs.LG math.OC stat.ML published:2013-10-07 summary:We design a randomised parallel version of Adaboost based on previous studieson parallel coordinate descent. The algorithm uses the fact that the logarithmof the exponential loss is a function with coordinate-wise Lipschitz continuousgradient, in order to define the step lengths. We provide the proof ofconvergence for this randomised Adaboost algorithm and a theoreticalparallelisation speedup factor. We finally provide numerical examples onlearning problems of various sizes that show that the algorithm is competitivewith concurrent approaches, especially for large scale problems.
arxiv-1310-1531 | DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition |  http://arxiv.org/abs/1310.1531  | author:Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell category:cs.CV published:2013-10-06 summary:We evaluate whether features extracted from the activation of a deepconvolutional network trained in a fully supervised fashion on a large, fixedset of object recognition tasks can be re-purposed to novel generic tasks. Ourgeneric tasks may differ significantly from the originally trained tasks andthere may be insufficient labeled or unlabeled data to conventionally train oradapt a deep architecture to the new tasks. We investigate and visualize thesemantic clustering of deep convolutional features with respect to a variety ofsuch tasks, including scene recognition, domain adaptation, and fine-grainedrecognition challenges. We compare the efficacy of relying on various networklevels to define a fixed feature, and report novel results that significantlyoutperform the state-of-the-art on several important vision challenges. We arereleasing DeCAF, an open-source implementation of these deep convolutionalactivation features, along with all associated network parameters to enablevision researchers to be able to conduct experimentation with deeprepresentations across a range of visual concept learning paradigms.
arxiv-1310-1562 | Dependence Measure for non-additive model |  http://arxiv.org/abs/1310.1562  | author:Hangjin Jiang, Yiming Ding category:stat.ML published:2013-10-06 summary:We proposed a new statistical dependency measure called Copula DependencyCoefficient(CDC) for two sets of variables based on copula. It is robust tooutliers, easy to implement, powerful and appropriate to high-dimensionalvariables. These properties are important in many applications. Experimentalresults show that CDC can detect the dependence between variables in bothadditive and non-additive models.
arxiv-1310-1545 | Learning Hidden Structures with Relational Models by Adequately Involving Rich Information in A Network |  http://arxiv.org/abs/1310.1545  | author:Xuhui Fan, Richard Yi Da Xu, Longbing Cao, Yin Song category:cs.LG cs.SI stat.ML published:2013-10-06 summary:Effectively modelling hidden structures in a network is very practical buttheoretically challenging. Existing relational models only involve very limitedinformation, namely the binary directional link data, embedded in a network tolearn hidden networking structures. There is other rich and meaningfulinformation (e.g., various attributes of entities and more granular informationthan binary elements such as "like" or "dislike") missed, which play a criticalrole in forming and understanding relations in a network. In this work, wepropose an informative relational model (InfRM) framework to adequately involverich information and its granularity in a network, including metadatainformation about each entity and various forms of link data. Firstly, aneffective metadata information incorporation method is employed on the priorinformation from relational models MMSB and LFRM. This is to encourage theentities with similar metadata information to have similar hidden structures.Secondly, we propose various solutions to cater for alternative forms of linkdata. Substantial efforts have been made towards modelling appropriateness andefficiency, for example, using conjugate priors. We evaluate our framework andits inference algorithms in different datasets, which shows the generality andeffectiveness of our models in capturing implicit structures in networks.
arxiv-1310-1597 | Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning |  http://arxiv.org/abs/1310.1597  | author:Mengqiu Wang, Christopher D. Manning category:cs.CL cs.AI published:2013-10-06 summary:We consider a multilingual weakly supervised learning scenario whereknowledge from annotated corpora in a resource-rich language is transferred viabitext to guide the learning in other languages. Past approaches project labelsacross bitext and use them as features or gold labels for training. We proposea new method that projects model expectations rather than labels, whichfacilities transfer of model uncertainty across language boundaries. We encodeexpectations as constraints and train a discriminative CRF model usingGeneralized Expectation Criteria (Mann and McCallum, 2010). Evaluated onstandard Chinese-English and German-English NER datasets, our methoddemonstrates F1 scores of 64% and 60% when no labeled data is used. Attainingthe same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences.Furthermore, when combined with labeled examples, our method yields significantimprovements over state-of-the-art supervised methods, achieving best reportednumbers to date on Chinese OntoNotes and German CoNLL-03 datasets.
arxiv-1310-1590 | Evolution of the Modern Phase of Written Bangla: A Statistical Study |  http://arxiv.org/abs/1310.1590  | author:Paheli Bhattacharya, Arnab Bhattacharya category:cs.CL I.2.7 published:2013-10-06 summary:Active languages such as Bangla (or Bengali) evolve over time due to avariety of social, cultural, economic, and political issues. In this paper, weanalyze the change in the written form of the modern phase of Banglaquantitatively in terms of character-level, syllable-level, morpheme-level andword-level features. We collect three different types of corpora---classical,newspapers and blogs---and test whether the differences in their features arestatistically significant. Results suggest that there are significant changesin the length of a word when measured in terms of characters, but there is notmuch difference in usage of different characters, syllables and morphemes in aword or of different words in a sentence. To the best of our knowledge, this isthe first work on Bangla of this kind.
arxiv-1310-1533 | CAM: Causal additive models, high-dimensional order search and penalized regression |  http://arxiv.org/abs/1310.1533  | author:Peter Bühlmann, Jonas Peters, Jan Ernest category:stat.ME cs.LG stat.ML published:2013-10-06 summary:We develop estimation for potentially high-dimensional additive structuralequation models. A key component of our approach is to decouple order searchamong the variables from feature or edge selection in a directed acyclic graphencoding the causal structure. We show that the former can be done withnonregularized (restricted) maximum likelihood estimation while the latter canbe efficiently addressed using sparse regression techniques. Thus, wesubstantially simplify the problem of structure search and estimation for animportant class of causal models. We establish consistency of the (restricted)maximum likelihood estimator for low- and high-dimensional scenarios, and wealso allow for misspecification of the error distribution. Furthermore, wedevelop an efficient computational algorithm which can deal with manyvariables, and the new method's accuracy and performance is illustrated onsimulated and real data.
arxiv-1310-1519 | Moments and Root-Mean-Square Error of the Bayesian MMSE Estimator of Classification Error in the Gaussian Model |  http://arxiv.org/abs/1310.1519  | author:Amin Zollanvari, Edward R. Dougherty category:stat.ML published:2013-10-05 summary:The most important aspect of any classifier is its error rate, because thisquantifies its predictive capacity. Thus, the accuracy of error estimation iscritical. Error estimation is problematic in small-sample classifier designbecause the error must be estimated using the same data from which theclassifier has been designed. Use of prior knowledge, in the form of a priordistribution on an uncertainty class of feature-label distributions to whichthe true, but unknown, feature-distribution belongs, can facilitate accurateerror estimation (in the mean-square sense) in circumstances where accuratecompletely model-free error estimation is impossible. This paper providesanalytic asymptotically exact finite-sample approximations for variousperformance metrics of the resulting Bayesian Minimum Mean-Square-Error (MMSE)error estimator in the case of linear discriminant analysis (LDA) in themultivariate Gaussian model. These performance metrics include the first,second, and cross moments of the Bayesian MMSE error estimator with the trueerror of LDA, and therefore, the Root-Mean-Square (RMS) error of the estimator.We lay down the theoretical groundwork for Kolmogorov double-asymptotics in aBayesian setting, which enables us to derive asymptotic expressions of thedesired performance metrics. From these we produce analytic finite-sampleapproximations and demonstrate their accuracy via numerical examples. Variousexamples illustrate the behavior of these approximations and their use indetermining the necessary sample size to achieve a desired RMS. TheSupplementary Material contains derivations for some equations and addedfigures.
arxiv-1310-1502 | Randomized Approximation of the Gram Matrix: Exact Computation and Probabilistic Bounds |  http://arxiv.org/abs/1310.1502  | author:John T. Holodnak, Ilse C. F. Ipsen category:math.NA cs.LG stat.ML published:2013-10-05 summary:Given a real matrix A with n columns, the problem is to approximate the Gramproduct AA^T by c << n weighted outer products of columns of A. Necessary andsufficient conditions for the exact computation of AA^T (in exact arithmetic)from c >= rank(A) columns depend on the right singular vector matrix of A. Fora Monte-Carlo matrix multiplication algorithm by Drineas et al. that samplesouter products, we present probabilistic bounds for the 2-norm relative errordue to randomization. The bounds depend on the stable rank or the rank of A,but not on the matrix dimensions. Numerical experiments illustrate that thebounds are informative, even for stringent success probabilities and matricesof small dimension. We also derive bounds for the smallest singular value andthe condition number of matrices obtained by sampling rows from orthonormalmatrices.
arxiv-1310-1425 | A State of the Art of Word Sense Induction: A Way Towards Word Sense Disambiguation for Under-Resourced Languages |  http://arxiv.org/abs/1310.1425  | author:Mohammad Nasiruddin category:cs.CL 68T50 I.2.7 published:2013-10-05 summary:Word Sense Disambiguation (WSD), the process of automatically identifying themeaning of a polysemous word in a sentence, is a fundamental task in NaturalLanguage Processing (NLP). Progress in this approach to WSD opens up manypromising developments in the field of NLP and its applications. Indeed,improvement over current performance levels could allow us to take a first steptowards natural language understanding. Due to the lack of lexical resources itis sometimes difficult to perform WSD for under-resourced languages. This paperis an investigation on how to initiate research in WSD for under-resourcedlanguages by applying Word Sense Induction (WSI) and suggests some interestingtopics to focus on.
arxiv-1310-1518 | Contraction Principle based Robust Iterative Algorithms for Machine Learning |  http://arxiv.org/abs/1310.1518  | author:Rangeet Mitra, Amit Kumar Mishra category:cs.LG stat.ML published:2013-10-05 summary:Iterative algorithms are ubiquitous in the field of data mining. Widely knownexamples of such algorithms are the least mean square algorithm,backpropagation algorithm of neural networks. Our contribution in this paper isan improvement upon this iterative algorithms in terms of their respectiveperformance metrics and robustness. This improvement is achieved by a newscaling factor which is multiplied to the error term. Our analysis shows thatin essence, we are minimizing the corresponding LASSO cost function, which isthe reason of its increased robustness. We also give closed form expressionsfor the number of iterations for convergence and the MSE floor of the originalcost function for a minimum targeted value of the L1 norm. As a concludingtheme based on the stochastic subgradient algorithm, we give a comparisonbetween the well known Dantzig selector and our algorithm based on contractionprinciple. By these simulations we attempt to show the optimality of ourapproach for any widely used parent iterative optimization problem.
arxiv-1310-1495 | Role of normalization in spectral clustering for stochastic blockmodels |  http://arxiv.org/abs/1310.1495  | author:Purnamrita Sarkar, Peter J. Bickel category:stat.ML published:2013-10-05 summary:Spectral clustering is a technique that clusters elements using the top feweigenvectors of their (possibly normalized) similarity matrix. The quality ofspectral clustering is closely tied to the convergence properties of theseprincipal eigenvectors. This rate of convergence has been shown to be identicalfor both the normalized and unnormalized variants in recent random matrixtheory literature. However, normalization for spectral clustering is commonlybelieved to be beneficial [Stat. Comput. 17 (2007) 395-416]. Indeed, ourexperiments show that normalization improves prediction accuracy. In thispaper, for the popular stochastic blockmodel, we theoretically show thatnormalization shrinks the spread of points in a class by a constant fractionunder a broad parameter regime. As a byproduct of our work, we also obtainsharp deviation bounds of empirical principal eigenvalues of graphs generatedfrom a stochastic blockmodel.
arxiv-1310-1426 | Local Feature or Mel Frequency Cepstral Coefficients - Which One is Better for MLN-Based Bangla Speech Recognition? |  http://arxiv.org/abs/1310.1426  | author:Foyzul Hassan, Mohammed Rokibul Alam Kotwal, Md. Mostafizur Rahman, Mohammad Nasiruddin, Md. Abdul Latif, Mohammad Nurul Huda category:cs.CL 68T50 I.2.7 published:2013-10-05 summary:This paper discusses the dominancy of local features (LFs), as input to themultilayer neural network (MLN), extracted from a Bangla input speech over melfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises threestages: (i) LF extraction from input speech, (ii) phoneme probabilitiesextraction using MLN from LF and (iii) the hidden Markov model (HMM) basedclassifier to obtain more accurate phoneme strings. In the experiments onBangla speech corpus prepared by us, it is observed that the LFbased automaticspeech recognition (ASR) system provides higher phoneme correct rate than theMFCC-based system. Moreover, the proposed system requires fewer mixturecomponents in the HMMs.
arxiv-1310-1363 | Weakly supervised clustering: Learning fine-grained signals from coarse labels |  http://arxiv.org/abs/1310.1363  | author:Stefan Wager, Alexander Blocker, Niall Cardin category:stat.ML cs.LG published:2013-10-04 summary:Consider a classification problem where we do not have access to labels forindividual training examples, but only have average labels over subpopulations.We give practical examples of this setup and show how such a classificationtask can usefully be analyzed as a weakly supervised clustering problem. Wepropose three approaches to solving the weakly supervised clustering problem,including a latent variables model that performs well in our experiments. Weillustrate our methods on an analysis of aggregated elections data and anindustry data set that was the original motivation for this research.
arxiv-1310-1404 | Sequential Monte Carlo Bandits |  http://arxiv.org/abs/1310.1404  | author:Michael Cherkassky, Luke Bornn category:stat.ML cs.LG stat.ME published:2013-10-04 summary:In this paper we propose a flexible and efficient framework for handlingmulti-armed bandits, combining sequential Monte Carlo algorithms withhierarchical Bayesian modeling techniques. The framework naturally encompassesrestless bandits, contextual bandits, and other bandit variants under a singleinferential model. Despite the model's generality, we propose efficient MonteCarlo algorithms to make inference scalable, based on recent developments insequential Monte Carlo methods. Through two simulation studies, the frameworkis shown to outperform other empirical methods, while also naturally scaling tomore complex problems for which existing approaches can not cope. Additionally,we successfully apply our framework to online video-based advertisingrecommendation, and show its increased efficacy as compared to current state ofthe art bandit algorithms.
arxiv-1310-1187 | Labeled Directed Acyclic Graphs: a generalization of context-specific independence in directed graphical models |  http://arxiv.org/abs/1310.1187  | author:Johan Pensar, Henrik Nyman, Timo Koski, Jukka Corander category:stat.ML cs.AI cs.LG published:2013-10-04 summary:We introduce a novel class of labeled directed acyclic graph (LDAG) modelsfor finite sets of discrete variables. LDAGs generalize earlier proposals forallowing local structures in the conditional probability distribution of anode, such that unrestricted label sets determine which edges can be deletedfrom the underlying directed acyclic graph (DAG) for a given context. Severalproperties of these models are derived, including a generalization of theconcept of Markov equivalence classes. Efficient Bayesian learning of LDAGs isenabled by introducing an LDAG-based factorization of the Dirichlet prior forthe model parameters, such that the marginal likelihood can be calculatedanalytically. In addition, we develop a novel prior distribution for the modelstructures that can appropriately penalize a model for its labeling complexity.A non-reversible Markov chain Monte Carlo algorithm combined with a greedy hillclimbing approach is used for illustrating the useful properties of LDAG modelsfor both real and synthetic data sets.
arxiv-1310-1297 | Spectral Clustering for Divide-and-Conquer Graph Matching |  http://arxiv.org/abs/1310.1297  | author:Vince Lyzinski, Daniel L. Sussman, Donniell E. Fishkind, Henry Pao, Li Chen, Joshua T. Vogelstein, Youngser Park, Carey E. Priebe category:stat.ML math.OC stat.CO published:2013-10-04 summary:We present a parallelized bijective graph matching algorithm that leveragesseeds and is designed to match very large graphs. Our algorithm combinesspectral graph embedding with existing state-of-the-art seeded graph matchingprocedures. We justify our approach by proving that modestly correlated, largestochastic block model random graphs are correctly matched utilizing very fewseeds through our divide-and-conquer procedure. We also demonstrate theeffectiveness of our approach in matching very large graphs in simulated andreal data examples, showing up to a factor of 8 improvement in runtime withminimal sacrifice in accuracy.
arxiv-1310-1259 | A Novel Progressive Image Scanning and Reconstruction Scheme based on Compressed Sensing and Linear Prediction |  http://arxiv.org/abs/1310.1259  | author:Giulio Coluccia, Enrico Magli category:cs.IT cs.CV math.IT published:2013-10-04 summary:Compressed sensing (CS) is an innovative technique allowing to representsignals through a small number of their linear projections. In this paper weaddress the application of CS to the scenario of progressive acquisition of 2Dvisual signals in a line-by-line fashion. This is an important setting whichencompasses diverse systems such as flatbed scanners and remote sensingimagers. The use of CS in such setting raises the problem of reconstructing avery high number of samples, as are contained in an image, from their linearprojections. Conventional reconstruction algorithms, whose complexity is cubicin the number of samples, are computationally intractable. In this paper wedevelop an iterative reconstruction algorithm that reconstructs an image byiteratively estimating a row, and correlating adjacent rows by means of linearprediction. We develop suitable predictors and test the proposed algorithm inthe context of flatbed scanners and remote sensing imaging systems. We showthat this approach can significantly improve the results of separatereconstruction of each row, providing very good reconstruction quality withreasonable complexity.
arxiv-1310-1177 | Clustering on Multiple Incomplete Datasets via Collective Kernel Learning |  http://arxiv.org/abs/1310.1177  | author:Weixiang Shao, Xiaoxiao Shi, Philip S. Yu category:cs.LG H.2.8; I.5.3 published:2013-10-04 summary:Multiple datasets containing different types of features may be available fora given task. For instance, users' profiles can be used to group users forrecommendation systems. In addition, a model can also use users' historicalbehaviors and credit history to group users. Each dataset contains differentinformation and suffices for learning. A number of clustering algorithms onmultiple datasets were proposed during the past few years. These algorithmsassume that at least one dataset is complete. So far as we know, all theprevious methods will not be applicable if there is no complete datasetavailable. However, in reality, there are many situations where no dataset iscomplete. As in building a recommendation system, some new users may not have aprofile or historical behaviors, while some may not have a credit history.Hence, no available dataset is complete. In order to solve this problem, wepropose an approach called Collective Kernel Learning to infer hidden samplesimilarity from multiple incomplete datasets. The idea is to collectivelycompletes the kernel matrices of incomplete datasets by optimizing thealignment of the shared instances of the datasets. Furthermore, a clusteringalgorithm is proposed based on the kernel matrix. The experiments on bothsynthetic and real datasets demonstrate the effectiveness of the proposedapproach. The proposed clustering algorithm outperforms the comparisonalgorithms by as much as two times in normalized mutual information.
arxiv-1310-1415 | Narrowing the Gap: Random Forests In Theory and In Practice |  http://arxiv.org/abs/1310.1415  | author:Misha Denil, David Matheson, Nando de Freitas category:stat.ML cs.LG published:2013-10-04 summary:Despite widespread interest and practical use, the theoretical properties ofrandom forests are still not well understood. In this paper we contribute tothis understanding in two ways. We present a new theoretically tractablevariant of random regression forests and prove that our algorithm isconsistent. We also provide an empirical evaluation, comparing our algorithmand other theoretically tractable random forest models to the random forestalgorithm used in practice. Our experiments provide insight into the relativeimportance of different simplifications that theoreticians have made to obtaintractable models for analysis.
arxiv-1310-1249 | Reading Stockholm Riots 2013 in social media by text-mining |  http://arxiv.org/abs/1310.1249  | author:Andrzej Jarynowski, Amir Rostami category:cs.SI cs.CL physics.soc-ph stat.AP published:2013-10-04 summary:The riots in Stockholm in May 2013 were an event that reverberated in theworld media for its dimension of violence that had spread through the Swedishcapital. In this study we have investigated the role of social media increating media phenomena via text mining and natural language processing. Wehave focused on two channels of communication for our analysis: Twitter andPoloniainfo.se (Forum of Polish community in Sweden). Our preliminary resultsshow some hot topics driving discussion related mostly to Swedish Police andSwedish Politics by counting word usage. Typical features for mediaintervention are presented. We have built networks of most popular phrases,clustered by categories (geography, media institution, etc.). Sentimentanalysis shows negative connotation with Police. The aim of this preliminaryexploratory quantitative study was to generate questions and hypotheses, whichwe could carefully follow by deeper more qualitative methods.
arxiv-1310-1221 | Spatially Scalable Compressed Image Sensing with Hybrid Transform and Inter-layer Prediction Model |  http://arxiv.org/abs/1310.1221  | author:Diego Valsesia, Enrico Magli category:cs.IT cs.CV cs.MM math.IT published:2013-10-04 summary:Compressive imaging is an emerging application of compressed sensing, devotedto acquisition, encoding and reconstruction of images using random projectionsas measurements. In this paper we propose a novel method to provide a scalableencoding of an image acquired by means of compressed sensing techniques. Twobit-streams are generated to provide two distinct quality levels: alow-resolution base layer and full-resolution enhancement layer. In theproposed method we exploit a fast preview of the image at the encoder in orderto perform inter-layer prediction and encode the prediction residuals only. Theproposed method successfully provides resolution and quality scalability withmodest complexity and it provides gains in the quality of the reconstructedimages with respect to separate encoding of the quality layers. Remarkably, wealso show that the scheme can also provide significant gains with respect to adirect, non-scalable system, thus accomplishing two features at once:scalability and improved reconstruction performance.
arxiv-1310-1285 | Semantic Measures for the Comparison of Units of Language, Concepts or Instances from Text and Knowledge Base Analysis |  http://arxiv.org/abs/1310.1285  | author:Sébastien Harispe, Sylvie Ranwez, Stefan Janaqi, Jacky Montmain category:cs.CL published:2013-10-04 summary:Semantic measures are widely used today to estimate the strength of thesemantic relationship between elements of various types: units of language(e.g., words, sentences, documents), concepts or even instances semanticallycharacterized (e.g., diseases, genes, geographical locations). Semanticmeasures play an important role to compare such elements according to semanticproxies: texts and knowledge representations, which support their meaning ordescribe their nature. Semantic measures are therefore essential for designingintelligent agents which will for example take advantage of semantic analysisto mimic human ability to compare abstract or concrete objects. This paperproposes a comprehensive survey of the broad notion of semantic measure for thecomparison of units of language, concepts or instances based on semantic proxyanalyses. Semantic measures generalize the well-known notions of semanticsimilarity, semantic relatedness and semantic distance, which have beenextensively studied by various communities over the last decades (e.g.,Cognitive Sciences, Linguistics, and Artificial Intelligence to mention a few).
arxiv-1310-1227 | The Novel Approach of Adaptive Twin Probability for Genetic Algorithm |  http://arxiv.org/abs/1310.1227  | author:Anagha P. Khedkar, Shaila Subbaraman category:cs.NE published:2013-10-04 summary:The performance of GA is measured and analyzed in terms of its performanceparameters against variations in its genetic operators and associatedparameters. Since last four decades huge numbers of researchers have beenworking on the performance of GA and its enhancement. This earlier researchwork on analyzing the performance of GA enforces the need to furtherinvestigate the exploration and exploitation characteristics and observe itsimpact on the behavior and overall performance of GA. This paper introduces thenovel approach of adaptive twin probability associated with the advanced twinoperator that enhances the performance of GA. The design of the advanced twinoperator is extrapolated from the twin offspring birth due to single ovulationin natural genetic systems as mentioned in the earlier works. The twinprobability of this operator is adaptively varied based on the fitness of bestindividual thereby relieving the GA user from statically defining its value.This novel approach of adaptive twin probability is experimented and tested onthe standard benchmark optimization test functions. The experimental resultsshow the increased accuracy in terms of the best individual and reducedconvergence time.
arxiv-1310-1341 | Director Field Model of the Primary Visual Cortex for Contour Detection |  http://arxiv.org/abs/1310.1341  | author:Vijay Singh, Martin Tchernookov, Rebecca Butterfield, Ilya Nemenman category:q-bio.NC cs.CV published:2013-10-04 summary:We aim to build the simplest possible model capable of detecting long, noisycontours in a cluttered visual scene. For this, we model the neural dynamics inthe primate primary visual cortex in terms of a continuous director field thatdescribes the average rate and the average orientational preference of activeneurons at a particular point in the cortex. We then use a linear-nonlineardynamical model with long range connectivity patterns to enforce long-rangestatistical context present in the analyzed images. The resulting model hassubstantially fewer degrees of freedom than traditional models, and yet it candistinguish large contiguous objects from the background clutter by suppressingthe clutter and by filling-in occluded elements of object contours. Thisresults in high-precision, high-recall detection of large objects in clutteredscenes. Parenthetically, our model has a direct correspondence with the Landau- de Gennes theory of nematic liquid crystal in two dimensions.
arxiv-1310-1147 | A Primal Dual Active Set Algorithm for a Class of Nonconvex Sparsity Optimization |  http://arxiv.org/abs/1310.1147  | author:Yuling Jiao, Bangti Jin, Xiliang Lu, Weina Ren category:math.OC stat.ML published:2013-10-04 summary:In this paper, we consider the problem of recovering a sparse vector fromnoisy measurement data. Traditionally, it is formulated as a penalizedleast-squares problem with an $\ell^1$ penalty. Recent studies show thatnonconvex penalties, e.g., $\ell^0$ and bridge, allow more effective sparserecovery. We develop an algorithm of primal dual active set type for a class ofnonconvex sparsity-promoting penalties, which cover $\ell^0$, bridge, smoothlyclipped absolute deviation, capped $\ell^1$ and minimax concavity penalty.First we establish the existence of a global minimizer for the class ofoptimization problems. Then we derive a novel necessary optimality conditionfor the global minimizer using the associated thresholding operator. Thesolutions to the optimality system are coordinate-wise minimizers, and underminor conditions, they are also local minimizers. Upon introducing the dualvariable, the active set can be determined from the primal and dual variables.This relation lends itself to an iterative algorithm of active set type whichat each step involves updating the primal variable only on the active set andthen updating the dual variable explicitly. When combined with a continuationstrategy on the regularization parameter, it has a global convergence propertyunder the restricted isometry property. Extensive numerical experimentsdemonstrate its efficiency and accuracy.
arxiv-1310-1022 | Multivariate regression and fit function uncertainty |  http://arxiv.org/abs/1310.1022  | author:Peter Kovesarki, Ian C. Brock category:stat.ML stat.CO 62J02 published:2013-10-03 summary:This article describes a multivariate polynomial regression method where theuncertainty of the input parameters are approximated with Gaussiandistributions, derived from the central limit theorem for large weighted sums,directly from the training sample. The estimated uncertainties can bepropagated into the optimal fit function, as an alternative to the statisticalbootstrap method. This uncertainty can be propagated further into a lossfunction like quantity, with which it is possible to calculate the expectedloss function, and allows to select the optimal polynomial degree withstatistical significance. Combined with simple phase space splitting methods,it is possible to model most features of the training data even with low degreepolynomials or constants.
arxiv-1310-1076 | Compressed Counting Meets Compressed Sensing |  http://arxiv.org/abs/1310.1076  | author:Ping Li, Cun-Hui Zhang, Tong Zhang category:stat.ME cs.DS cs.IT cs.LG math.IT published:2013-10-03 summary:Compressed sensing (sparse signal recovery) has been a popular and importantresearch topic in recent years. By observing that natural signals are oftennonnegative, we propose a new framework for nonnegative signal recovery usingCompressed Counting (CC). CC is a technique built on maximally-skewed p-stablerandom projections originally developed for data stream computations. Ourrecovery procedure is computationally very efficient in that it requires onlyone linear scan of the coordinates. Our analysis demonstrates that, when0<p<=0.5, it suffices to use M= O(C/eps^p log N) measurements so that allcoordinates will be recovered within eps additive precision, in one scan of thecoordinates. The constant C=1 when p->0 and C=pi/2 when p=0.5. In particular,when p->0 the required number of measurements is essentially M=K\log N, where Kis the number of nonzero coordinates of the signal.
arxiv-1310-0890 | Multiple Kernel Learning in the Primal for Multi-modal Alzheimer's Disease Classification |  http://arxiv.org/abs/1310.0890  | author:Fayao Liu, Luping Zhou, Chunhua Shen, Jianping Yin category:cs.LG cs.CE published:2013-10-03 summary:To achieve effective and efficient detection of Alzheimer's disease (AD),many machine learning methods have been introduced into this realm. However,the general case of limited training samples, as well as different featurerepresentations typically makes this problem challenging. In this work, wepropose a novel multiple kernel learning framework to combine multi-modalfeatures for AD classification, which is scalable and easy to implement.Contrary to the usual way of solving the problem in the dual space, we look atthe optimization from a new perspective. By conducting Fourier transform on theGaussian kernel, we explicitly compute the mapping function, which leads to amore straightforward solution of the problem in the primal space. Furthermore,we impose the mixed $L_{21}$ norm constraint on the kernel weights, known asthe group lasso regularization, to enforce group sparsity among differentfeature modalities. This actually acts as a role of feature modality selection,while at the same time exploiting complementary information among differentkernels. Therefore it is able to extract the most discriminative features forclassification. Experiments on the ADNI data set demonstrate the effectivenessof the proposed method.
arxiv-1310-0900 | Efficient pedestrian detection by directly optimize the partial area under the ROC curve |  http://arxiv.org/abs/1310.0900  | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG published:2013-10-03 summary:Many typical applications of object detection operate within a prescribedfalse-positive range. In this situation the performance of a detector should beassessed on the basis of the area under the ROC curve over that range, ratherthan over the full curve, as the performance outside the range is irrelevant. This measure is labelled as the partial area under the ROC curve (pAUC).Effective cascade-based classification, for example, depends on training nodeclassifiers that achieve the maximal detection rate at a moderate falsepositive rate, e.g., around 40% to 50%. We propose a novel ensemble learningmethod which achieves a maximal detection rate at a user-defined range of falsepositive rates by directly optimizing the partial AUC using structuredlearning. By optimizing for different ranges of false positive rates, theproposed method can be used to train either a single strong classifier or anode classifier forming part of a cascade classifier. Experimental results onboth synthetic and real-world data sets demonstrate the effectiveness of ourapproach, and we show that it is possible to train state-of-the-art pedestriandetectors using the proposed structured ensemble learning method.
arxiv-1310-0575 | Development of Marathi Part of Speech Tagger Using Statistical Approach |  http://arxiv.org/abs/1310.0575  | author:Jyoti Singh, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-10-02 summary:Part-of-speech (POS) tagging is a process of assigning the words in a textcorresponding to a particular part of speech. A fundamental version of POStagging is the identification of words as nouns, verbs, adjectives etc. Forprocessing natural languages, Part of Speech tagging is a prominent tool. It isone of the simplest as well as most constant and statistical model for many NLPapplications. POS Tagging is an initial stage of linguistics, text analysislike information retrieval, machine translator, text to speech synthesis,information extraction etc. In POS Tagging we assign a Part of Speech tag toeach word in a sentence and literature. Various approaches have been proposedto implement POS taggers. In this paper we present a Marathi part of speechtagger. It is morphologically rich language. Marathi is spoken by the nativepeople of Maharashtra. The general approach used for development of tagger isstatistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clearidea about all the algorithms with suitable examples. It also introduces a tagset for Marathi which can be used for tagging Marathi text. In this paper wehave shown the development of the tagger as well as compared to check theaccuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram,Trigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82%respectively.
arxiv-1310-1371 | Robust and highly performant ring detection algorithm for 3d particle tracking using 2d microscope imaging |  http://arxiv.org/abs/1310.1371  | author:Eldad Afik category:cs.CV cond-mat.soft published:2013-10-02 summary:Three-dimensional particle tracking is an essential tool in studying dynamicsunder the microscope, namely, fluid dynamics in microfluidic devices, bacteriataxis, cellular trafficking. The 3d position can be determined using 2d imagingalone by measuring the diffraction rings generated by an out-of-focusfluorescent particle, imaged on a single camera. Here I present a ringdetection algorithm exhibiting a high detection rate, which is robust to thechallenges arising from ring occlusion, inclusions and overlaps, and allowsresolving particles even when near to each other. It is capable of real timeanalysis thanks to its high performance and low memory footprint. The proposedalgorithm, an offspring of the circle Hough transform, addresses the need toefficiently trace the trajectories of many particles concurrently, when theirnumber in not necessarily fixed, by solving a classification problem, andovercomes the challenges of finding local maxima in the complex parameter spacewhich results from ring clusters and noise. Several algorithmic conceptsintroduced here can be advantageous in other cases, particularly when dealingwith noisy and sparse data. The implementation is based on open-source andcross-platform software packages only, making it easy to distribute and modify.It is implemented in a microfluidic experiment allowing real-timemulti-particle tracking at 70 Hz, achieving a detection rate which exceeds 94%and only 1% false-detection.
arxiv-1310-0740 | Pseudo-Marginal Bayesian Inference for Gaussian Processes |  http://arxiv.org/abs/1310.0740  | author:Maurizio Filippone, Mark Girolami category:stat.ML cs.LG stat.ME published:2013-10-02 summary:The main challenges that arise when adopting Gaussian Process priors inprobabilistic modeling are how to carry out exact Bayesian inference and how toaccount for uncertainty on model parameters when making model-based predictionson out-of-sample data. Using probit regression as an illustrative workingexample, this paper presents a general and effective methodology based on thepseudo-marginal approach to Markov chain Monte Carlo that efficiently addressesboth of these issues. The results presented in this paper show improvementsover existing sampling methods to simulate from the posterior distribution overthe parameters defining the covariance function of the Gaussian Process prior.This is particularly important as it offers a powerful tool to carry out fullBayesian inference of Gaussian Process based hierarchic statistical models ingeneral. The results also demonstrate that Monte Carlo based integration of allmodel parameters is actually feasible in this class of models providing asuperior quantification of uncertainty in predictions. Extensive comparisonswith respect to state-of-the-art probabilistic classifiers confirm thisassertion.
arxiv-1310-0754 | Stemmers for Tamil Language: Performance Analysis |  http://arxiv.org/abs/1310.0754  | author:M. Thangarasu, R. Manavalan category:cs.CL published:2013-10-02 summary:Stemming is the process of extracting root word from the given inflectionword and also plays significant role in numerous application of NaturalLanguage Processing (NLP). Tamil Language raises several challenges to NLP,since it has rich morphological patterns than other languages. The rule basedapproach light-stemmer is proposed in this paper, to find stem word for giveninflection Tamil word. The performance of proposed approach is compared to arule based suffix removal stemmer based on correctly and incorrectly predicted.The experimental result clearly show that the proposed approach light stemmerfor Tamil language perform better than suffix removal stemmer and also moreeffective in Information Retrieval System (IRS).
arxiv-1310-0581 | Rule Based Stemmer in Urdu |  http://arxiv.org/abs/1310.0581  | author:Vaishali Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-10-02 summary:Urdu is a combination of several languages like Arabic, Hindi, English,Turkish, Sanskrit etc. It has a complex and rich morphology. This is the reasonwhy not much work has been done in Urdu language processing. Stemming is usedto convert a word into its respective root form. In stemming, we separate thesuffix and prefix from the word. It is useful in search engines, naturallanguage processing and word processing, spell checkers, word parsing, wordfrequency and count studies. This paper presents a rule based stemmer for Urdu.The stemmer that we have discussed here is used in information retrieval. Wehave also evaluated our results by verifying it with a human expert.
arxiv-1310-0578 | Subjective and Objective Evaluation of English to Urdu Machine Translation |  http://arxiv.org/abs/1310.0578  | author:Vaishali Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-10-02 summary:Machine translation is research based area where evaluation is very importantphenomenon for checking the quality of MT output. The work is based on theevaluation of English to Urdu Machine translation. In this research work wehave evaluated the translation quality of Urdu language which has beentranslated by using different Machine Translation systems like Google, Babylonand Ijunoon. The evaluation process is done by using two approaches - Humanevaluation and Automatic evaluation. We have worked for both the approacheswhere in human evaluation emphasis is given to scales and parameters while inautomatic evaluation emphasis is given to some automatic metric such as BLEU,GTM, METEOR and ATEC.
arxiv-1310-0576 | Learning Lambek grammars from proof frames |  http://arxiv.org/abs/1310.0576  | author:Roberto Bonato, Christian Retoré category:cs.LG cs.AI cs.LO math.LO published:2013-10-02 summary:In addition to their limpid interface with semantics, categorial grammarsenjoy another important property: learnability. This was first noticed byBuskowsky and Penn and further studied by Kanazawa, for Bar-Hillel categorialgrammars. What about Lambek categorial grammars? In a previous paper we showed thatproduct free Lambek grammars where learnable from structured sentences, thestructures being incomplete natural deductions. These grammars were shown to beunlearnable from strings by Foret and Le Nir. In the present paper we show thatLambek grammars, possibly with product, are learnable from proof frames thatare incomplete proof nets. After a short reminder on grammatical inference \`a la Gold, we provide analgorithm that learns Lambek grammars with product from proof frames and weprove its convergence. We do so for 1-valued also known as rigid Lambekgrammars with product, since standard techniques can extend our result to$k$-valued grammars. Because of the correspondence between cut-free proof netsand normal natural deductions, our initial result on product free Lambekgrammars can be recovered. We are sad to dedicate the present paper to Philippe Darondeau, with whom westarted to study such questions in Rennes at the beginning of the millennium,and who passed away prematurely. We are glad to dedicate the present paper to Jim Lambek for his 90 birthday:he is the living proof that research is an eternal learning process.
arxiv-1310-0573 | Improving the Quality of MT Output using Novel Name Entity Translation Scheme |  http://arxiv.org/abs/1310.0573  | author:Deepti Bhalla, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-10-02 summary:This paper presents a novel approach to machine translation by combining thestate of art name entity translation scheme. Improper translation of nameentities lapse the quality of machine translated output. In this work, nameentities are transliterated by using statistical rule based approach. Thispaper describes the translation and transliteration of name entities fromEnglish to Punjabi. We have experimented on four types of name entities whichare: Proper names, Location names, Organization names and miscellaneous.Various rules for the purpose of syllabification have been constructed.Transliteration of name entities is accomplished with the help of Probabilitycalculation. N-Gram probabilities for the extracted syllables have beencalculated using statistical machine translation toolkit MOSES.
arxiv-1310-0532 | Perfect Clustering for Stochastic Blockmodel Graphs via Adjacency Spectral Embedding |  http://arxiv.org/abs/1310.0532  | author:Vince Lyzinski, Daniel Sussman, Minh Tang, Avanti Athreya, Carey Priebe category:stat.ML published:2013-10-02 summary:Vertex clustering in a stochastic blockmodel graph has wide applicability andhas been the subject of extensive research. In thispaper, we provide a shortproof that the adjacency spectral embedding can be used to obtain perfectclustering for the stochastic blockmodel and the degree-corrected stochasticblockmodel. We also show an analogous result for the more general random dotproduct graph model.
arxiv-1310-0807 | Exact and Stable Covariance Estimation from Quadratic Sampling via Convex Programming |  http://arxiv.org/abs/1310.0807  | author:Yuxin Chen, Yuejie Chi, Andrea Goldsmith category:cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH published:2013-10-02 summary:Statistical inference and information processing of high-dimensional dataoften require efficient and accurate estimation of their second-orderstatistics. With rapidly changing data, limited processing power and storage atthe acquisition devices, it is desirable to extract the covariance structurefrom a single pass over the data and a small number of stored measurements. Inthis paper, we explore a quadratic (or rank-one) measurement model whichimposes minimal memory requirements and low computational complexity during thesampling process, and is shown to be optimal in preserving variouslow-dimensional covariance structures. Specifically, four popular structuralassumptions of covariance matrices, namely low rank, Toeplitz low rank,sparsity, jointly rank-one and sparse structure, are investigated, whilerecovery is achieved via convex relaxation paradigms for the respectivestructure. The proposed quadratic sampling framework has a variety of potentialapplications including streaming data processing, high-frequency wirelesscommunication, phase space tomography and phase retrieval in optics, andnon-coherent subspace detection. Our method admits universally accuratecovariance estimation in the absence of noise, as soon as the number ofmeasurements exceeds the information theoretic limits. We also demonstrate therobustness of this approach against noise and imperfect structural assumptions.Our analysis is established upon a novel notion called the mixed-normrestricted isometry property (RIP-$\ell_{2}/\ell_{1}$), as well as theconventional RIP-$\ell_{2}/\ell_{2}$ for near-isotropic and boundedmeasurements. In addition, our results improve upon the best-known phaseretrieval (including both dense and sparse signals) guarantees using PhaseLiftwith a significantly simpler approach.
arxiv-1310-0865 | Electricity Market Forecasting via Low-Rank Multi-Kernel Learning |  http://arxiv.org/abs/1310.0865  | author:Vassilis Kekatos, Yu Zhang, Georgios B. Giannakis category:stat.ML cs.LG cs.SY published:2013-10-02 summary:The smart grid vision entails advanced information technology and dataanalytics to enhance the efficiency, sustainability, and economics of the powergrid infrastructure. Aligned to this end, modern statistical learning tools areleveraged here for electricity market inference. Day-ahead price forecasting iscast as a low-rank kernel learning problem. Uniquely exploiting the marketclearing process, congestion patterns are modeled as rank-one components in thematrix of spatio-temporally varying prices. Through a novel nuclear norm-basedregularization, kernels across pricing nodes and hours can be systematicallyselected. Even though market-wide forecasting is beneficial from a learningperspective, it involves processing high-dimensional market data. The latterbecomes possible after devising a block-coordinate descent algorithm forsolving the non-convex optimization problem involved. The algorithm utilizesresults from block-sparse vector recovery and is guaranteed to converge to astationary point. Numerical tests on real data from the Midwest ISO (MISO)market corroborate the prediction accuracy, computational efficiency, and theinterpretative merits of the developed approach over existing alternatives.
arxiv-1310-0306 | Flexible Visual Quality Inspection in Discrete Manufacturing |  http://arxiv.org/abs/1310.0306  | author:Tomislav Petković, Darko Jurić, Sven Lončarić category:cs.CV published:2013-10-01 summary:Most visual quality inspections in discrete manufacturing are composed oflength, surface, angle or intensity measurements. Those are implemented asend-user configurable inspection tools that should not require an imageprocessing expert to set up. Currently available software solutions providingsuch capability use a flowchart based programming environment, but do not fullyaddress an inspection flowchart robustness and can require a redefinition ofthe flowchart if a small variation is introduced. In this paper we propose anacquire-register-analyze image processing pattern designed for discretemanufacturing that aims to increase the robustness of the inspection flowchartby consistently addressing variations in product position, orientation andsize. A proposed pattern is transparent to the end-user and simplifies theflowchart. We describe a developed software solution that is a practicalimplementation of the proposed pattern. We give an example of its real-life usein industrial production of electric components.
arxiv-1310-0305 | Filtering for More Accurate Dense Tissue Segmentation in Digitized Mammograms |  http://arxiv.org/abs/1310.0305  | author:Mario Muštra, Mislav Grgić category:cs.CV published:2013-10-01 summary:Breast tissue segmentation into dense and fat tissue is important fordetermining the breast density in mammograms. Knowing the breast density isimportant both in diagnostic and computer-aided detection applications. Thereare many different ways to express the density of a breast and good qualitysegmentation should provide the possibility to perform accurate classificationno matter which classification rule is being used. Knowing the right breastdensity and having the knowledge of changes in the breast density could give ahint of a process which started to happen within a patient. Mammogramsgenerally suffer from a problem of different tissue overlapping which resultsin the possibility of inaccurate detection of tissue types. Fibroglandulartissue presents rather high attenuation of X-rays and is visible as brighter inthe resulting image but overlapping fibrous tissue and blood vessels couldeasily be replaced with fibroglandular tissue in automatic segmentationalgorithms. Small blood vessels and microcalcifications are also shown asbright objects with similar intensities as dense tissue but do have someproperties which makes possible to suppress them from the final results. Inthis paper we try to divide dense and fat tissue by suppressing the scatteredstructures which do not represent glandular or dense tissue in order to dividemammograms more accurately in the two major tissue types. For suppressing bloodvessels and microcalcifications we have used Gabor filters of different sizeand orientation and a combination of morphological operations on filtered imagewith enhanced contrast.
arxiv-1310-0302 | Surface Registration Using Genetic Algorithm in Reduced Search Space |  http://arxiv.org/abs/1310.0302  | author:Vedran Hrgetić, Tomislav Pribanić category:cs.CV published:2013-10-01 summary:Surface registration is a technique that is used in various areas such asobject recognition and 3D model reconstruction. Problem of surface registrationcan be analyzed as an optimization problem of seeking a rigid motion betweentwo different views. Genetic algorithms can be used for solving thisoptimization problem, both for obtaining the robust parameter estimation andfor its fine-tuning. The main drawback of genetic algorithms is that they aretime consuming which makes them unsuitable for online applications. Modernacquisition systems enable the implementation of the solutions that wouldimmediately give the information on the rotational angles between the differentviews, thus reducing the dimension of the optimization problem. The paper givesan analysis of the genetic algorithm implemented in the conditions when therotation matrix is known and a comparison of these results with results whenthis information is not available.
arxiv-1310-0316 | Classifying Traffic Scenes Using The GIST Image Descriptor |  http://arxiv.org/abs/1310.0316  | author:Ivan Sikirić, Karla Brkić, Siniša Šegvić category:cs.CV published:2013-10-01 summary:This paper investigates classification of traffic scenes in a very lowbandwidth scenario, where an image should be coded by a small number offeatures. We introduce a novel dataset, called the FM1 dataset, consisting of5615 images of eight different traffic scenes: open highway, open road,settlement, tunnel, tunnel exit, toll booth, heavy traffic and the overpass. Weevaluate the suitability of the GIST descriptor as a representation of theseimages, first by exploring the descriptor space using PCA and k-meansclustering, and then by using an SVM classifier and recording its 10-foldcross-validation performance on the introduced FM1 dataset. The obtainedrecognition rates are very encouraging, indicating that the use of the GISTdescriptor alone could be sufficiently descriptive even when very highperformance is required.
arxiv-1310-0310 | A Novel Georeferenced Dataset for Stereo Visual Odometry |  http://arxiv.org/abs/1310.0310  | author:Ivan Krešo, Marko Ševrović, Siniša Šegvić category:cs.CV published:2013-10-01 summary:In this work, we present a novel dataset for assessing the accuracy of stereovisual odometry. The dataset has been acquired by a small-baseline stereo rigmounted on the top of a moving car. The groundtruth is supplied by a consumergrade GPS device without IMU. Synchronization and alignment between GPSreadings and stereo frames are recovered after the acquisition. We show thatthe attained groundtruth accuracy allows to draw useful conclusions inpractice. The presented experiments address influence of camera calibration,baseline distance and zero-disparity features to the achieved reconstructionperformance.
arxiv-1310-0317 | An Overview and Evaluation of Various Face and Eyes Detection Algorithms for Driver Fatigue Monitoring Systems |  http://arxiv.org/abs/1310.0317  | author:Markan Lopar, Slobodan Ribarić category:cs.CV published:2013-10-01 summary:In this work various methods and algorithms for face and eyes detection areexamined in order to decide which of them are applicable for use in a driverfatigue monitoring system. In the case of face detection the standardViola-Jones face detector has shown best results, while the method of findingthe eye centers by means of gradients has proven to be most appropriate in thecase of eyes detection. The later method has also a potential for retrievingbehavioral parameters needed for estimation of the level of driver fatigue.This possibility will be examined in future work.
arxiv-1310-0314 | Global Localization Based on 3D Planar Surface Segments |  http://arxiv.org/abs/1310.0314  | author:Robert Cupec, Emmanuel Karlo Nyarko, Damir Filko, Andrej Kitanov, Ivan Petrović category:cs.CV published:2013-10-01 summary:Global localization of a mobile robot using planar surface segments extractedfrom depth images is considered. The robot's environment is represented by atopological map consisting of local models, each representing a particularlocation modeled by a set of planar surface segments. The discussedlocalization approach segments a depth image acquired by a 3D camera intoplanar surface segments which are then matched to model surface segments. Therobot pose is estimated by the Extended Kalman Filter using surface segmentpairs as measurements. The reliability and accuracy of the considered approachare experimentally evaluated using a mobile robot equipped by a MicrosoftKinect sensor.
arxiv-1310-0322 | Optical Flow on Evolving Surfaces with Space and Time Regularisation |  http://arxiv.org/abs/1310.0322  | author:Clemens Kirisits, Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV published:2013-10-01 summary:We extend the concept of optical flow with spatiotemporal regularisation to adynamic non-Euclidean setting. Optical flow is traditionally computed from asequence of flat images. The purpose of this paper is to introduce variationalmotion estimation for images that are defined on an evolving surface.Volumetric microscopy images depicting a live zebrafish embryo serve as bothbiological motivation and test data.
arxiv-1310-0308 | Combining Spatio-Temporal Appearance Descriptors and Optical Flow for Human Action Recognition in Video Data |  http://arxiv.org/abs/1310.0308  | author:Karla Brkić, Srđan Rašić, Axel Pinz, Siniša Šegvić, Zoran Kalafatić category:cs.CV published:2013-10-01 summary:This paper proposes combining spatio-temporal appearance (STA) descriptorswith optical flow for human action recognition. The STA descriptors are localhistogram-based descriptors of space-time, suitable for building a partialrepresentation of arbitrary spatio-temporal phenomena. Because of thepossibility of iterative refinement, they are interesting in the context ofonline human action recognition. We investigate the use of dense optical flowas the image function of the STA descriptor for human action recognition, usingtwo different algorithms for computing the flow: the Farneb\"ack algorithm andthe TVL1 algorithm. We provide a detailed analysis of the influencing opticalflow algorithm parameters on the produced optical flow fields. An extensiveexperimental validation of optical flow-based STA descriptors in human actionrecognition is performed on the KTH human action dataset. The encouragingexperimental results suggest the potential of our approach in online humanaction recognition.
arxiv-1310-0315 | Computer Vision Systems in Road Vehicles: A Review |  http://arxiv.org/abs/1310.0315  | author:Kristian Kovačić, Edouard Ivanjko, Hrvoje Gold category:cs.CV published:2013-10-01 summary:The number of road vehicles significantly increased in recent decades. Thistrend accompanied a build-up of road infrastructure and development of variouscontrol systems to increase road traffic safety, road capacity and travelcomfort. In traffic safety significant development has been made and today'ssystems more and more include cameras and computer vision methods. Cameras areused as part of the road infrastructure or in vehicles. In this paper a reviewon computer vision systems in vehicles from the stand point of trafficengineering is given. Safety problems of road vehicles are presented, currentstate of the art in-vehicle vision systems is described and open problems withfuture research directions are discussed.
arxiv-1310-0311 | Multiclass Road Sign Detection using Multiplicative Kernel |  http://arxiv.org/abs/1310.0311  | author:Valentina Zadrija, Siniša Šegvić category:cs.CV published:2013-10-01 summary:We consider the problem of multiclass road sign detection using aclassification function with multiplicative kernel comprised from two kernels.We show that problems of detection and within-foreground classification can bejointly solved by using one kernel to measure object-background differences andanother one to account for within-class variations. The main idea behind thisapproach is that road signs from different foreground variations can sharefeatures that discriminate them from backgrounds. The classification functiontraining is accomplished using SVM, thus feature sharing is obtained throughsupport vector sharing. Training yields a family of linear detectors, whereeach detector corresponds to a specific foreground training sample. Theredundancy among detectors is alleviated using k-medoids clustering. Finally,we report detection and classification results on a set of road sign imagesobtained from a camera on a moving vehicle.
arxiv-1310-0171 | Object Detection Using Keygraphs |  http://arxiv.org/abs/1310.0171  | author:Marcelo Hashimoto, Roberto Marcondes Cesar Junior category:cs.CV published:2013-10-01 summary:We propose a new framework for object detection based on a generalization ofthe keypoint correspondence framework. This framework is based on replacingkeypoints by keygraphs, i.e. isomorph directed graphs whose vertices arekeypoints, in order to explore relative and structural information. Unlikesimilar works in the literature, we deal directly with graphs in the entirepipeline: we search for graph correspondences instead of searching forindividual point correspondences and then building graph correspondences fromthem afterwards. We also estimate the pose from graph correspondences insteadof falling back to point correspondences through a voting table. Thecontributions of this paper are the proposed framework and an implementationthat properly handles its inherent issues of loss of locality and combinatorialexplosion, showing its viability for real-time applications. In particular, weintroduce the novel concept of keytuples to solve a running time issue. Theaccuracy of the implementation is shown by results of over 800 experiments witha well-known database of images. The speed is illustrated by real-time trackingwith two different cameras in ordinary hardware.
arxiv-1310-0509 | Summary Statistics for Partitionings and Feature Allocations |  http://arxiv.org/abs/1310.0509  | author:Işık Barış Fidaner, Ali Taylan Cemgil category:cs.LG stat.ML published:2013-10-01 summary:Infinite mixture models are commonly used for clustering. One can sample fromthe posterior of mixture assignments by Monte Carlo methods or find its maximuma posteriori solution by optimization. However, in some problems the posterioris diffuse and it is hard to interpret the sampled partitionings. In thispaper, we introduce novel statistics based on block sizes for representingsample sets of partitionings and feature allocations. We develop anelement-based definition of entropy to quantify segmentation among theirelements. Then we propose a simple algorithm called entropy agglomeration (EA)to summarize and visualize this information. Experiments on various infinitemixture posteriors as well as a feature allocation dataset demonstrate that theproposed statistics are useful in practice.
arxiv-1310-0319 | Second Croatian Computer Vision Workshop (CCVW 2013) |  http://arxiv.org/abs/1310.0319  | author:Sven Lončarić, Siniša Šegvić category:cs.CV published:2013-10-01 summary:Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013,http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb,Croatia. Workshop was organized by the Center of Excellence for Computer Visionof the University of Zagreb.
arxiv-1310-0365 | The complex-valued encoding for dicision-making based on aliasing data |  http://arxiv.org/abs/1310.0365  | author:P. A. Golovinski, V. A. Astapenko category:cs.CV published:2013-10-01 summary:It is proposed a complex valued channel encoding for multidimensional data.The basic approach contains overlapping of complex nonlinear mappings. Itsdevelopment leads to sparse representation of multi-channel data, increasingtheir dimensions and the distance between the images.
arxiv-1310-0201 | Cross-Recurrence Quantification Analysis of Categorical and Continuous Time Series: an R package |  http://arxiv.org/abs/1310.0201  | author:Moreno I. Coco, Rick Dale category:cs.CL stat.AP published:2013-10-01 summary:This paper describes the R package crqa to perform cross-recurrencequantification analysis of two time series of either a categorical orcontinuous nature. Streams of behavioral information, from eye movements tolinguistic elements, unfold over time. When two people interact, such as inconversation, they often adapt to each other, leading these behavioral levelsto exhibit recurrent states. In dialogue, for example, interlocutors adapt toeach other by exchanging interactive cues: smiles, nods, gestures, choice ofwords, and so on. In order for us to capture closely the goings-on of dynamicinteraction, and uncover the extent of coupling between two individuals, weneed to quantify how much recurrence is taking place at these levels. Methodsavailable in crqa would allow researchers in cognitive science to pose suchquestions as how much are two people recurrent at some level of analysis, whatis the characteristic lag time for one person to maximally match another, orwhether one person is leading another. First, we set the theoretical ground tounderstand the difference between 'correlation' and 'co-visitation' whencomparing two time series, using an aggregative or cross-recurrence approach.Then, we describe more formally the principles of cross-recurrence, and showwith the current package how to carry out analyses applying them. We end thepaper by comparing computational efficiency, and results' consistency, of crqaR package, with the benchmark MATLAB toolbox crptoolbox. We show perfectcomparability between the two libraries on both levels.
arxiv-1310-0522 | EVOC: A Computer Model of the Evolution of Culture |  http://arxiv.org/abs/1310.0522  | author:Liane Gabora category:cs.MA cs.NE published:2013-10-01 summary:EVOC is a computer model of the EVOlution of Culture. It consists of neuralnetwork based agents that invent ideas for actions, and imitate neighbors'actions. EVOC replicates using a different fitness function the resultsobtained with an earlier model (MAV), including (1) an increase in mean fitnessof actions, and (2) an increase and then decrease in the diversity of actions.Diversity of actions is positively correlated with number of needs, populationsize and density, and with the erosion of borders between populations. Slowlyeroding borders maximize diversity, fostering specialization followed bysharing of fit actions. Square (as opposed to toroidal) worlds also exhibithigher diversity. Introducing a leader that broadcasts its actions throughoutthe population increases the fitness of actions but reduces diversity; theseeffects diminish the more leaders there are. Low density populations have lessfit ideas but broadcasting diminishes this effect.
arxiv-1310-0188 | Graph connection Laplacian and random matrices with random blocks |  http://arxiv.org/abs/1310.0188  | author:Noureddine El Karoui, Hau-tieng Wu category:math.PR math.SP stat.ME stat.ML published:2013-10-01 summary:Graph connection Laplacian (GCL) is a modern data analysis technique that isstarting to be applied for the analysis of high dimensional and massivedatasets. Motivated by this technique, we study matrices that are akin to theones appearing in the null case of GCL, i.e the case where there is nostructure in the dataset under investigation. Developing this understanding isimportant in making sense of the output of the algorithms based on GCL. Wehence develop a theory explaining the behavior of the spectral distribution ofa large class of random matrices, in particular random matrices with randomblock entries of fixed size. Part of the theory covers the case where there issignificant dependence between the blocks. Numerical work shows that theagreement between our theoretical predictions and numerical simulations isgenerally very good.
arxiv-1310-0307 | Using the Random Sprays Retinex Algorithm for Global Illumination Estimation |  http://arxiv.org/abs/1310.0307  | author:Nikola Banić, Sven Lončarić category:cs.CV published:2013-10-01 summary:In this paper the use of Random Sprays Retinex (RSR) algorithm for globalillumination estimation is proposed and its feasibility tested. Like otheralgorithms based on the Retinex model, RSR also provides local illuminationestimation and brightness adjustment for each pixel and it is faster than otherpath-wise Retinex algorithms. As the assumption of the uniform illuminationholds in many cases, it should be possible to use the mean of localillumination estimations of RSR as a global illumination estimation for imageswith (assumed) uniform illumination allowing also the accuracy to be easilymeasured. Therefore we propose a method for estimating global illuminationestimation based on local RSR results. To our best knowledge this is the firsttime that RSR algorithm is used to obtain global illumination estimation. Forour tests we use a publicly available color constancy image database fortesting. The results are presented and discussed and it turns out that theproposed method outperforms many existing unsupervised color constancyalgorithms. The source code is available athttp://www.fer.unizg.hr/ipg/resources/color_constancy/.
arxiv-1310-0154 | Incoherence-Optimal Matrix Completion |  http://arxiv.org/abs/1310.0154  | author:Yudong Chen category:cs.IT cs.LG math.IT stat.ML published:2013-10-01 summary:This paper considers the matrix completion problem. We show that it is notnecessary to assume joint incoherence, which is a standard but unintuitive andrestrictive condition that is imposed by previous studies. This leads to asample complexity bound that is order-wise optimal with respect to theincoherence parameter (as well as to the rank $r$ and the matrix dimension $n$up to a log factor). As a consequence, we improve the sample complexity ofrecovering a semidefinite matrix from $O(nr^{2}\log^{2}n)$ to $O(nr\log^{2}n)$,and the highest allowable rank from $\Theta(\sqrt{n}/\log n)$ to$\Theta(n/\log^{2}n)$. The key step in proof is to obtain new bounds on the$\ell_{\infty,2}$-norm, defined as the maximum of the row and column norms of amatrix. To illustrate the applicability of our techniques, we discussextensions to SVD projection, structured matrix completion and semi-supervisedclustering, for which we provide order-wise improvements over existing results.Finally, we turn to the closely-related problem of low-rank-plus-sparse matrixdecomposition. We show that the joint incoherence condition is unavoidable herefor polynomial-time algorithms conditioned on the Planted Clique conjecture.This means it is intractable in general to separate a rank-$\omega(\sqrt{n})$positive semidefinite matrix and a sparse matrix. Interestingly, our resultsshow that the standard and joint incoherence conditions are associatedrespectively with the information (statistical) and computational aspects ofthe matrix decomposition problem.
arxiv-1310-0432 | Online Learning of Dynamic Parameters in Social Networks |  http://arxiv.org/abs/1310.0432  | author:Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie category:math.OC cs.LG cs.SI stat.ML published:2013-10-01 summary:This paper addresses the problem of online learning in a dynamic setting. Weconsider a social network in which each individual observes a private signalabout the underlying state of the world and communicates with her neighbors ateach time period. Unlike many existing approaches, the underlying state isdynamic, and evolves according to a geometric random walk. We view the scenarioas an optimization problem where agents aim to learn the true state whilesuffering the smallest possible loss. Based on the decomposition of the globalloss function, we introduce two update mechanisms, each of which generates anestimate of the true state. We establish a tight bound on the rate of change ofthe underlying state, under which individuals can track the parameter with abounded variance. Then, we characterize explicit expressions for the steadystate mean-square deviation(MSD) of the estimates from the truth, perindividual. We observe that only one of the estimators recovers the optimalMSD, which underscores the impact of the objective function decomposition onthe learning quality. Finally, we provide an upper bound on the regret of theproposed methods, measured as an average of errors in estimating the parameterin a finite time.
arxiv-1310-0376 | Joint Bayesian estimation of close subspaces from noisy measurements |  http://arxiv.org/abs/1310.0376  | author:Olivier Besson, Nicolas Dobigeon, Jean-Yves Tourneret category:stat.ME stat.ML published:2013-10-01 summary:In this letter, we consider two sets of observations defined as subspacesignals embedded in noise and we wish to analyze the distance between these twosubspaces. The latter entails evaluating the angles between the subspaces, anissue reminiscent of the well-known Procrustes problem. A Bayesian approach isinvestigated where the subspaces of interest are considered as random with ajoint prior distribution (namely a Bingham distribution), which allows thecloseness of the two subspaces to be adjusted. Within this framework, theminimum mean-square distance estimator of both subspaces is formulated andimplemented via a Gibbs sampler. A simpler scheme based on alternative maximuma posteriori estimation is also presented. The new schemes are shown to providemore accurate estimates of the angles between the subspaces, compared tosingular value decomposition based independent estimation of the two subspaces.
arxiv-1310-0512 | Jointly Clustering Rows and Columns of Binary Matrices: Algorithms and Trade-offs |  http://arxiv.org/abs/1310.0512  | author:Jiaming Xu, Rui Wu, Kai Zhu, Bruce Hajek, R. Srikant, Lei Ying category:stat.ML published:2013-10-01 summary:In standard clustering problems, data points are represented by vectors, andby stacking them together, one forms a data matrix with row or column clusterstructure. In this paper, we consider a class of binary matrices, arising inmany applications, which exhibit both row and column cluster structure, and ourgoal is to exactly recover the underlying row and column clusters by observingonly a small fraction of noisy entries. We first derive a lower bound on theminimum number of observations needed for exact cluster recovery. Then, wepropose three algorithms with different running time and compare the number ofobservations needed by them for successful cluster recovery. Our analyticalresults show smooth time-data trade-offs: one can gradually reduce thecomputational complexity when increasingly more observations are available.
arxiv-1310-0354 | Deep and Wide Multiscale Recursive Networks for Robust Image Labeling |  http://arxiv.org/abs/1310.0354  | author:Gary B. Huang, Viren Jain category:cs.CV cs.LG published:2013-10-01 summary:Feedforward multilayer networks trained by supervised learning have recentlydemonstrated state of the art performance on image labeling problems such asboundary prediction and scene parsing. As even very low error rates can limitpractical usage of such systems, methods that perform closer to human accuracyremain desirable. In this work, we propose a new type of network with thefollowing properties that address what we hypothesize to be limiting aspects ofexisting methods: (1) a `wide' structure with thousands of features, (2) alarge field of view, (3) recursive iterations that exploit statisticaldependencies in label space, and (4) a parallelizable architecture that can betrained in a fraction of the time compared to benchmark multilayerconvolutional networks. For the specific image labeling problem of boundaryprediction, we also introduce a novel example weighting algorithm that improvessegmentation accuracy. Experiments in the challenging domain of connectomicreconstruction of neural circuity from 3d electron microscopy data show thatthese "Deep And Wide Multiscale Recursive" (DAWMR) networks lead to new levelsof image labeling performance. The highest performing architecture has twelvelayers, interwoven supervised and unsupervised stages, and uses an input fieldof view of 157,464 voxels ($54^3$) to make a prediction at each image location.We present an associated open source software package that enables the simpleand flexible creation of DAWMR networks.
arxiv-1310-0423 | Inference of Network Summary Statistics Through Network Denoising |  http://arxiv.org/abs/1310.0423  | author:Prakash Balachandran, Edoardo Airoldi, Eric Kolaczyk category:stat.ML math.SP published:2013-10-01 summary:Consider observing an undirected network that is `noisy' in the sense thatthere are Type I and Type II errors in the observation of edges. Such errorscan arise, for example, in the context of inferring gene regulatory networks ingenomics or functional connectivity networks in neuroscience. Given a singleobserved network then, to what extent are summary statistics for that networkrepresentative of their analogues for the true underlying network? Can we infersuch statistics more accurately by taking into account the noise in theobserved network edges? In this paper, we answer both of these questions. In particular, we develop aspectral-based methodology using the adjacency matrix to `denoise' the observednetwork data and produce more accurate inference of the summary statistics ofthe true network. We characterize performance of our methodology through boundson appropriate notions of risk in the $L^2$ sense, and conclude by illustratingthe practical impact of this work on synthetic and real-world data.
arxiv-1310-0110 | An information measure for comparing top $k$ lists |  http://arxiv.org/abs/1310.0110  | author:Arun Konagurthu, James Collier category:cs.IT cs.LG math.IT published:2013-10-01 summary:Comparing the top $k$ elements between two or more ranked results is a commontask in many contexts and settings. A few measures have been proposed tocompare top $k$ lists with attractive mathematical properties, but they face anumber of pitfalls and shortcomings in practice. This work introduces a newmeasure to compare any two top k lists based on measuring the information theselists convey. Our method investigates the compressibility of the lists, and thelength of the message to losslessly encode them gives a natural and robustmeasure of their variability. This information-theoretic measure objectivelyreconciles all the main considerations that arise when measuring(dis-)similarity between lists: the extent of their non-overlapping elements ineach of the lists; the amount of disarray among overlapping elements betweenthe lists; the measurement of displacement of actual ranks of their overlappingelements.
arxiv-1310-0097 | Analysis of Amoeba Active Contours |  http://arxiv.org/abs/1310.0097  | author:Martin Welk category:cs.CV published:2013-09-30 summary:Subject of this paper is the theoretical analysis of structure-adaptivemedian filter algorithms that approximate curvature-based PDEs for imagefiltering and segmentation. These so-called morphological amoeba filters arebased on a concept introduced by Lerallut et al. They achieve similar resultsas the well-known geodesic active contour and self-snakes PDEs. In the presentwork, the PDE approximated by amoeba active contours is derived for a generalgeometric situation and general amoeba metric. This PDE is structurally similarbut not identical to the geodesic active contour equation. It reproduces theprevious PDE approximation results for amoeba median filters as special cases.Furthermore, modifications of the basic amoeba active contour algorithm areanalysed that are related to the morphological force terms frequently used withgeodesic active contours. Experiments demonstrate the basic behaviour of amoebaactive contours and its similarity to geodesic active contours.
arxiv-1309-7821 | MPBART - Multinomial Probit Bayesian Additive Regression Trees |  http://arxiv.org/abs/1309.7821  | author:Bereket P. Kindo, Hao Wang, Edsel A. Peña category:stat.ML published:2013-09-30 summary:This article proposes Multinomial Probit Bayesian Additive Regression Trees(MPBART) as a multinomial probit extension of BART - Bayesian AdditiveRegression Trees (Chipman et al (2010)). MPBART is flexible to allow inclusionof predictors that describe the observed units as well as the available choicealternatives. Through two simulation studies and four real data examples, weshow that MPBART exhibits very good predictive performance in comparison toother discrete choice and multiclass classification methods. To implementMPBART, we have developed an R package mpbart available freely from CRANrepositories.
arxiv-1309-7733 | Regression Trees for Longitudinal Data |  http://arxiv.org/abs/1309.7733  | author:Madan Gopal Kundu, Jaroslaw Harezlak category:stat.ME stat.ML published:2013-09-30 summary:While studying response trajectory, often the population of interest may bediverse enough to exist distinct subgroups within it and the longitudinalchange in response may not be uniform in these subgroups. That is, thetimeslope and/or influence of covariates in longitudinal profile may vary amongthese different subgroups. For example, Raudenbush (2001) used depression as anexample to argue that it is incorrect to assume that all the people in a givenpopulation would be experiencing either increasing or decreasing levels ofdepression. In such cases, traditional linear mixed effects model (assumingcommon parametric form for covariates and time) is not directly applicable forthe entire population as a group-averaged trajectory can mask importantsubgroup differences. Our aim is to identify and characterize longitudinallyhomogeneous subgroups based on the combination of baseline covariates in themost parsimonious way. This goal can be achieved via constructing regressiontree for longitudinal data using baseline covariates as partitioning variables.We have proposed LongCART algorithm to construct regression tree for thelongitudinal data. In each node, the proposed LongCART algorithm determines theneed for further splitting (i.e. whether parameter(s) of longitudinal profileis influenced by any baseline attributes) via parameter instability tests andthus the decision of further splitting is type-I error controlled. We haveobtained the asymptotic results for the proposed instability test and examinedfinite sample behavior of the whole algorithm through simulation studies.Finally, we have applied the LongCART algorithm to study the longitudinalchanges in choline level among HIV patients.
arxiv-1309-7824 | Linear Regression as a Non-Cooperative Game |  http://arxiv.org/abs/1309.7824  | author:Stratis Ioannidis, Patrick Loiseau category:cs.GT cs.LG math.ST stat.TH published:2013-09-30 summary:Linear regression amounts to estimating a linear model that maps features(e.g., age or gender) to corresponding data (e.g., the answer to a survey orthe outcome of a medical exam). It is a ubiquitous tool in experimentalsciences. We study a setting in which features are public but the data isprivate information. While the estimation of the linear model may be useful toparticipating individuals, (if, e.g., it leads to the discovery of a treatmentto a disease), individuals may be reluctant to disclose their data due toprivacy concerns. In this paper, we propose a generic game-theoretic model toexpress this trade-off. Users add noise to their data before releasing it. Inparticular, they choose the variance of this noise to minimize a costcomprising two components: (a) a privacy cost, representing the loss of privacyincurred by the release; and (b) an estimation cost, representing theinaccuracy in the linear model estimate. We study the Nash equilibria of thisgame, establishing the existence of a unique non-trivial equilibrium. Wedetermine its efficiency for several classes of privacy and estimation costs,using the concept of the price of stability. Finally, we prove that, for aspecific estimation cost, the generalized least-square estimator is optimalamong all linear unbiased estimators in our non-cooperative setting: thisresult extends the famous Aitken/Gauss-Markov theorem in statistics,establishing that its conclusion persists even in the presence of strategicindividuals.
arxiv-1309-7912 | An Image-Based Fluid Surface Pattern Model |  http://arxiv.org/abs/1309.7912  | author:Mauro de Amorim, Ricardo Fabbri, Lucia Maria dos Santos Pinto, Francisco Duarte Moura Neto category:cs.CV published:2013-09-30 summary:This work aims at generating a model of the ocean surface and its dynamicsfrom one or more video cameras. The idea is to model wave patterns from videoas a first step towards a larger system of photogrammetric monitoring of marineconditions for use in offshore oil drilling platforms. The first part of theproposed approach consists in reducing the dimensionality of sensor data madeup of the many pixels of each frame of the input video streams. This enablesfinding a concise number of most relevant parameters to model the temporaldataset, yielding an efficient data-driven model of the evolution of theobserved surface. The second part proposes stochastic modeling to bettercapture the patterns embedded in the data. One can then draw samples from thefinal model, which are expected to simulate the behavior of previously observedflow, in order to determine conditions that match new observations. In thispaper we focus on proposing and discussing the overall approach and oncomparing two different techniques for dimensionality reduction in the firststage: principal component analysis and diffusion maps. Work is underway on thesecond stage of constructing better stochastic models of fluid surface dynamicsas proposed here.
arxiv-1310-0036 | Personal Identification from Lip-Print Features using a Statistical Model |  http://arxiv.org/abs/1310.0036  | author:Saptarshi Bhattacharjee, S Arunkumar, Samir Kumar Bandyopadhyay category:cs.CV published:2013-09-30 summary:This paper presents a novel approach towards identification of human beingsfrom the statistical analysis of their lip prints. Lip features are extractedby studying the spatial orientations of the grooves present in lip prints ofindividuals using standard edge detection techniques. Horizontal, vertical anddiagonal groove features are analysed using connected-component analysis togenerate the region-specific edge datasets. Comparison between test andreference sample datasets against a threshold value to define a match yieldsatisfactory results. FAR, FRR and ROC metrics have been used to gauge theperformance of the algorithm for real-world deployment in unimodal andmultimodal biometric verification systems.
arxiv-1309-7690 | A Hybrid Monte Carlo Ant Colony Optimization Approach for Protein Structure Prediction in the HP Model |  http://arxiv.org/abs/1309.7690  | author:Andrea G. Citrolo, Giancarlo Mauri category:cs.NE cs.CE published:2013-09-30 summary:The hydrophobic-polar (HP) model has been widely studied in the field ofprotein structure prediction (PSP) both for theoretical purposes and as abenchmark for new optimization strategies. In this work we introduce a newheuristics based on Ant Colony Optimization (ACO) and Markov Chain Monte Carlo(MCMC) that we called Hybrid Monte Carlo Ant Colony Optimization (HMCACO). Wedescribe this method and compare results obtained on well known HP instances inthe 3 dimensional cubic lattice to those obtained with standard ACO andSimulated Annealing (SA). All methods were implemented using an unconstrainedneighborhood and a modified objective function to prevent the creation ofoverlapping walks. Results show that our methods perform better than the otherheuristics in all benchmark instances.
arxiv-1309-7697 | Semi-structured data extraction and modelling: the WIA Project |  http://arxiv.org/abs/1309.7697  | author:Gianluca Colombo, Ettore Colombo, Andrea Bonomi, Alessandro Mosca, Simone Bassis category:cs.SE cs.CY cs.NE H3; I.2; H.1.2 published:2013-09-30 summary:Over the last decades, the amount of data of all kinds availableelectronically has increased dramatically. Data are accessible through a rangeof interfaces including Web browsers, database query languages,application-specific interfaces, built on top of a number of different dataexchange formats. All these data span from un-structured to highly structureddata. Very often, some of them have structure even if the structure isimplicit, and not as rigid or regular as that found in standard databasesystems. Spreadsheet documents are prototypical in this respect. Spreadsheetsare the lightweight technology able to supply companies with easy to buildbusiness management and business intelligence applications, and business peoplelargely adopt spreadsheets as smart vehicles for data files generation andsharing. Actually, the more spreadsheets grow in complexity (e.g., their use inproduct development plans and quoting), the more their arrangement,maintenance, and analysis appear as a knowledge-driven activity. Thealgorithmic approach to the problem of automatic data structure extraction fromspreadsheet documents (i.e., grid-structured and free topological-related data)emerges from the WIA project: Worksheets Intelligent Analyser. TheWIA-algorithm shows how to provide a description of spreadsheet contents interms of higher level of abstractions or conceptualisations. In particular, theWIA-algorithm target is about the extraction of i) the calculus work-flowimplemented in the spreadsheets formulas and ii) the logical role played by thedata which take part into the calculus. The aim of the resultingconceptualisations is to provide spreadsheets with abstract representationsuseful for further model refinements and optimizations through evolutionaryalgorithms computations.
arxiv-1309-7750 | An Extensive Experimental Study on the Cluster-based Reference Set Reduction for speeding-up the k-NN Classifier |  http://arxiv.org/abs/1309.7750  | author:Stefanos Ougiaroglou, Georgios Evangelidis, Dimitris A. Dervos category:cs.LG published:2013-09-30 summary:The k-Nearest Neighbor (k-NN) classification algorithm is one of the mostwidely-used lazy classifiers because of its simplicity and ease ofimplementation. It is considered to be an effective classifier and has manyapplications. However, its major drawback is that when sequential search isused to find the neighbors, it involves high computational cost. Speeding-upk-NN search is still an active research field. Hwang and Cho have recentlyproposed an adaptive cluster-based method for fast Nearest Neighbor searching.The effectiveness of this method is based on the adjustment of threeparameters. However, the authors evaluated their method by setting specificparameter values and using only one dataset. In this paper, an extensiveexperimental study of this method is presented. The results, which are based onfive real life datasets, illustrate that if the parameters of the method arecarefully defined, one can achieve even better classification performance.
arxiv-1309-7698 | Signed Networks, Triadic Interactions and the Evolution of Cooperation |  http://arxiv.org/abs/1309.7698  | author:Simone Righi, Károly Takács category:cs.SI cs.GT cs.NE physics.soc-ph published:2013-09-30 summary:We outline a model to study the evolution of cooperation in a population ofagents playing the prisoner's dilemma in signed networks. We highlight that ifonly dyadic interactions are taken into account, cooperation never evolves.However, when triadic considerations are introduced, a window of opportunityfor emergence of cooperation as a stable behaviour emerges.
arxiv-1309-7857 | Generalized system identification with stable spline kernels |  http://arxiv.org/abs/1309.7857  | author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:stat.ML math.OC 62F35, 65K10 published:2013-09-30 summary:Regularized least-squares approaches have been successfully applied to linearsystem identification. Recent approaches use quadratic penalty terms on theunknown impulse response defined by stable spline kernels, which control modelspace complexity by leveraging regularity and bounded-input bounded-outputstability. This paper extends linear system identification to a wide class ofnonsmooth stable spline estimators, where regularization functionals and datamisfits can be selected from a rich set of piecewise linear quadraticpenalties. This class encompasses the 1-norm, huber, and vapnik, in addition tothe least-squares penalty, and the approach allows linear inequalityconstraints on the unknown impulse response. We develop a customized interior point solver for the entire class ofproposed formulations. By representing penalties through their conjugates, weallow a simple interface that enables the user to specify any piecewise linearquadratic penalty for misfit and regularizer, together with inequalityconstraints on the response. The solver is locally quadratically convergent,with O(n2(m+n)) arithmetic operations per iteration, for n impulse responsecoefficients and m output measurements. In the system identification context,where n << m, IPsolve is competitive with available alternatives, illustratedby a comparison with TFOCS and libSVM. The modeling framework is illustrated with a range of numerical experiments,featuring robust formulations for contaminated data, relaxation systems, andnonnegativity and unimodality constraints on the impulse response.Incorporating constraints yields significant improvements in systemidentification. The solver used to obtain the results is distributed via anopen source code repository.
arxiv-1309-7804 | On statistics, computation and scalability |  http://arxiv.org/abs/1309.7804  | author:Michael I. Jordan category:stat.ML cs.LG math.ST stat.TH published:2013-09-30 summary:How should statistical procedures be designed so as to be scalablecomputationally to the massive datasets that are increasingly the norm? Whencoupled with the requirement that an answer to an inferential question bedelivered within a certain time budget, this question has significantrepercussions for the field of statistics. With the goal of identifying"time-data tradeoffs," we investigate some of the statistical consequences ofcomputational perspectives on scability, in particular divide-and-conquermethodology and hierarchies of convex relaxations.
arxiv-1309-7676 | An upper bound on prototype set size for condensed nearest neighbor |  http://arxiv.org/abs/1309.7676  | author:Eric Christiansen category:cs.LG stat.ML published:2013-09-29 summary:The condensed nearest neighbor (CNN) algorithm is a heuristic for reducingthe number of prototypical points stored by a nearest neighbor classifier,while keeping the classification rule given by the reduced prototypical setconsistent with the full set. I present an upper bound on the number ofprototypical points accumulated by CNN. The bound originates in a bound on thenumber of times the decision rule is updated during training in the multiclassperceptron algorithm, and thus is independent of training set size.
arxiv-1309-7615 | Correcting Multi-focus Images via Simple Standard Deviation for Image Fusion |  http://arxiv.org/abs/1309.7615  | author:Firas A. Jassim category:cs.CV published:2013-09-29 summary:Image fusion is one of the recent trends in image registration which is anessential field of image processing. The basic principle of this paper is tofuse multi-focus images using simple statistical standard deviation. Firstly,the simple standard deviation for the k-by-k window inside each of themulti-focus images was computed. The contribution in this paper came from theidea that the focused part inside an image had high details rather than theunfocused part. Hence, the dispersion between pixels inside the focused part ishigher than the dispersion inside the unfocused part. Secondly, a simplecomparison between the standard deviation for each k-by-k window in themulti-focus images could be computed. The highest standard deviation betweenall the computed standard deviations for the multi-focus images could betreated as the optimal that is to be placed in the fused image. Theexperimental visual results show that the proposed method produces verysatisfactory results in spite of its simplicity.
arxiv-1309-7611 | Context-aware recommendations from implicit data via scalable tensor factorization |  http://arxiv.org/abs/1309.7611  | author:Balázs Hidasi, Domonkos Tikk category:cs.LG cs.IR published:2013-09-29 summary:Albeit the implicit feedback based recommendation problem - when only theuser history is available but there are no ratings - is the most typicalsetting in real-world applications, it is much less researched than theexplicit feedback case. State-of-the-art algorithms that are efficient on theexplicit case cannot be automatically transformed to the implicit case ifscalability should be maintained. There are few implicit feedback benchmarkdata sets, therefore new ideas are usually experimented on explicit benchmarks.In this paper, we propose a generic context-aware implicit feedback recommenderalgorithm, coined iTALS. iTALS applies a fast, ALS-based tensor factorizationlearning method that scales linearly with the number of non-zero elements inthe tensor. We also present two approximate and faster variants of iTALS usingcoordinate descent and conjugate gradient methods at learning. The method alsoallows us to incorporate various contextual information into the model whilemaintaining its computational efficiency. We present two context-aware variantsof iTALS incorporating seasonality and item purchase sequentiality into themodel to distinguish user behavior at different time intervals, and producttypes with different repetitiveness. Experiments run on six data sets showsthat iTALS clearly outperforms context-unaware models and context awarebaselines, while it is on par with factorization machines (beats 7 times out of12 cases) both in terms of recall and MAP.
arxiv-1309-7643 | Rotationally Invariant Image Representation for Viewing Direction Classification in Cryo-EM |  http://arxiv.org/abs/1309.7643  | author:Zhizhen Zhao, Amit Singer category:q-bio.BM cs.CV published:2013-09-29 summary:We introduce a new rotationally invariant viewing angle classification methodfor identifying, among a large number of Cryo-EM projection images, similarviews without prior knowledge of the molecule. Our rotationally invariantfeatures are based on the bispectrum. Each image is denoised and compressedusing steerable principal component analysis (PCA) such that rotating an imageis equivalent to phase shifting the expansion coefficients. Thus we are able toextend the theory of bispectrum of 1D periodic signals to 2D images. Therandomized PCA algorithm is then used to efficiently reduce the dimensionalityof the bispectrum coefficients, enabling fast computation of the similaritybetween any pair of images. The nearest neighbors provide an initialclassification of similar viewing angles. In this way, rotational alignment isonly performed for images with their nearest neighbors. The initial nearestneighbor classification and alignment are further improved by a newclassification method called vector diffusion maps. Our pipeline for viewingangle classification and alignment is experimentally shown to be faster andmore accurate than reference-free alignment with rotationally invariant K-meansclustering, MSA/MRA 2D classification, and their modern approximations.
arxiv-1309-7609 | Identificación y Registro Catastral de Cuerpos de Agua mediante Técnicas de Procesamiento Digital de Imagenes |  http://arxiv.org/abs/1309.7609  | author:Kevin Rojas Laura, Christhian Cardenas Alvarez category:cs.CV published:2013-09-29 summary:The effects of global climate change on Peruvian glaciers have brought aboutseveral processes of deglaciation during the last few years. The immediateeffect is the change of size of lakes and rivers. Public institutions thatmonitor water resources currently have only recent studies which make up lessthan 10% of the total. The effects of climate change and the lack of updatedinformation intensify social-economic problems related to water resources inPeru. The objective of this research is to develop a software application toautomate the Cadastral Registry of Water Bodies in Peru, using techniques ofdigital image processing, which would provide tools for detection, record,temporal analysis and visualization of water bodies. The images used are fromthe satellite Landsat5, which undergo a pre-processing of calibration andcorrection of the satellite. Detection results are archived into a file thatcontains location vectors and images of the segmentated bodies of water.
arxiv-1309-7598 | On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations |  http://arxiv.org/abs/1309.7598  | author:Tamir Hazan, Subhransu Maji, Tommi Jaakkola category:cs.LG published:2013-09-29 summary:In this paper we describe how MAP inference can be used to sample efficientlyfrom Gibbs distributions. Specifically, we provide means for drawing eitherapproximate or unbiased samples from Gibbs' distributions by introducing lowdimensional perturbations and solving the corresponding MAP assignments. Ourapproach also leads to new ways to derive lower bounds on partition functions.We demonstrate empirically that our method excels in the typical "high signal -high coupling" regime. The setting results in ragged energy landscapes that arechallenging for alternative approaches to sampling and/or lower bounds.
arxiv-1309-7522 | An Application of Backpropagation Artificial Neural Network Method for Measuring The Severity of Osteoarthritis |  http://arxiv.org/abs/1309.7522  | author:Dian Pratiwi, Diaz D. Santika, Bens Pardamean category:cs.NE cs.CE cs.CV published:2013-09-29 summary:The examination of Osteoarthritis disease through X-ray by rheumatology canbe classified into four grade of severity. This paper discusses about theapplication of artificial neural network backpropagation method for measuringthe severity of the disease, where the observed X-ray range from wrist tofingers. The main procedures of system in this paper is divided into three,which are image processing, feature extraction, and artificial neural networkprocess. First, an X-ray image digital (200x150 pixels and greyscale) will bethresholded, then extracted features based on probabilistic values of the colorintensity of seven bit quantization result, and statistical textures. Thatfeature values then will be normalizing to interval [0.1, 0.9], and then theresult would be processing on backpropagation artificial neural network systemas input to determine the severity of disease from an X-ray had input beforeit. From testing with learning rate 0.3, momentum 0.4, hidden units five piecesand about 132 feature vectors, this system had had a level of accuracy of 100%for learning data, 80% for learning and non-learning data, and 66.6% fornon-learning data
arxiv-1309-7524 | Meme and Variations: A Computer Model of Cultural Evolution |  http://arxiv.org/abs/1309.7524  | author:Liane Gabora category:cs.MA cs.NE published:2013-09-29 summary:Holland's (1975) genetic algorithm is a minimal computer model of naturalselection that made it possible to investigate the effect of manipulatingspecific parameters on the evolutionary process. If culture is, like biology, aform of evolution, it should be possible to similarly abstract the underlyingskeleton of the process and develop a minimal model of it. Meme and Variations,or MAV, is a computational model, inspired by the genetic algorithm, of howideas evolve in a society of interacting individuals (Gabora 1995). The name isa pun on the classical music form 'theme and variations', because it is basedon the premise that novel ideas are variations of old ones; they result fromtweaking or combining existing ideas in new ways (Holland et al. 1981). MAVexplores the impact of biological phenomena such as over-dominance andepistasis as well as cognitive and social phenomena such as the ability tolearn generalizations or imitate others on the fitness and diversity ofcultural transmissible actions.
arxiv-1309-7484 | CSIFT Based Locality-constrained Linear Coding for Image Classification |  http://arxiv.org/abs/1309.7484  | author:Chen Junzhou, Li Qing, Peng Qiang, Kin Hong Wong category:cs.CV published:2013-09-28 summary:In the past decade, SIFT descriptor has been witnessed as one of the mostrobust local invariant feature descriptors and widely used in various visiontasks. Most traditional image classification systems depend on theluminance-based SIFT descriptors, which only analyze the gray level variationsof the images. Misclassification may happen since their color contents areignored. In this article, we concentrate on improving the performance ofexisting image classification algorithms by adding color information. Toachieve this purpose, different kinds of colored SIFT descriptors areintroduced and implemented. Locality-constrained Linear Coding (LLC), astate-of-the-art sparse coding technology, is employed to construct the imageclassification system for the evaluation. The real experiments are carried outon several benchmarks. With the enhancements of color SIFT, the proposed imageclassification system obtains approximate 3% improvement of classificationaccuracy on the Caltech-101 dataset and approximate 4% improvement ofclassification accuracy on the Caltech-256 dataset.
arxiv-1309-7439 | Optimal Hybrid Channel Allocation:Based On Machine Learning Algorithms |  http://arxiv.org/abs/1309.7439  | author:K Viswanadh, Dr. G Rama Murthy category:cs.NI cs.LG published:2013-09-28 summary:Recent advances in cellular communication systems resulted in a huge increasein spectrum demand. To meet the requirements of the ever-growing need forspectrum, efficient utilization of the existing resources is of utmostimportance. Channel Allocation, has thus become an inevitable research topic inwireless communications. In this paper, we propose an optimal channelallocation scheme, Optimal Hybrid Channel Allocation (OHCA) for an effectiveallocation of channels. We improvise upon the existing Fixed Channel Allocation(FCA) technique by imparting intelligence to the existing system by employingthe multilayer perceptron technique.
arxiv-1309-7512 | Structured learning of sum-of-submodular higher order energy functions |  http://arxiv.org/abs/1309.7512  | author:Alexander Fix, Thorsten Joachims, Sam Park, Ramin Zabih category:cs.CV cs.LG stat.ML published:2013-09-28 summary:Submodular functions can be exactly minimized in polynomial time, and thespecial case that graph cuts solve with max flow \cite{KZ:PAMI04} has hadsignificant impact in computer vision\cite{BVZ:PAMI01,Kwatra:SIGGRAPH03,Rother:GrabCut04}. In this paper we addressthe important class of sum-of-submodular (SoS) functions\cite{Arora:ECCV12,Kolmogorov:DAM12}, which can be efficiently minimized via avariant of max flow called submodular flow \cite{Edmonds:ADM77}. SoS functionscan naturally express higher order priors involving, e.g., local image patches;however, it is difficult to fully exploit their expressive power because theyhave so many parameters. Rather than trying to formulate existing higher orderpriors as an SoS function, we take a discriminative learning approach,effectively searching the space of SoS functions for a higher order prior thatperforms well on our training set. We adopt a structural SVM approach\cite{Joachims/etal/09a,Tsochantaridis/etal/04} and formulate the trainingproblem in terms of quadratic programming; as a result we can efficientlysearch the space of SoS priors via an extended cutting-plane algorithm. We alsoshow how the state-of-the-art max flow method for vision problems\cite{Goldberg:ESA11} can be modified to efficiently solve the submodular flowproblem. Experimental comparisons are made against the OpenCV implementation ofthe GrabCut interactive segmentation technique \cite{Rother:GrabCut04}, whichuses hand-tuned parameters instead of machine learning. On a standard dataset\cite{Gulshan:CVPR10} our method learns higher order priors with hundreds ofparameter values, and produces significantly better segmentations. While ourfocus is on binary labeling problems, we show that our techniques can benaturally generalized to handle more than two labels.
arxiv-1309-7434 | Face Verification Using Boosted Cross-Image Features |  http://arxiv.org/abs/1309.7434  | author:Dong Zhang, Omar Oreifej, Mubarak Shah category:cs.CV published:2013-09-28 summary:This paper proposes a new approach for face verification, where a pair ofimages needs to be classified as belonging to the same person or not. Thisproblem is relatively new and not well-explored in the literature. Currentmethods mostly adopt techniques borrowed from face recognition, and processeach of the images in the pair independently, which is counter intuitive. Incontrast, we propose to extract cross-image features, i.e. features across thepair of images, which, as we demonstrate, is more discriminative to thesimilarity and the dissimilarity of faces. Our features are derived from thepopular Haar-like features, however, extended to handle the face verificationproblem instead of face detection. We collect a large bank of cross-imagefeatures using filters of different sizes, locations, and orientations.Consequently, we use AdaBoost to select and weight the most discriminativefeatures. We carried out extensive experiments on the proposed ideas usingthree standard face verification datasets, and obtained promising resultsoutperforming state-of-the-art.
arxiv-1309-7958 | A Statistical Learning Based System for Fake Website Detection |  http://arxiv.org/abs/1309.7958  | author:Ahmed Abbasi, Zhu Zhang, Hsinchun Chen category:cs.CY cs.LG published:2013-09-27 summary:Existing fake website detection systems are unable to effectively detect fakewebsites. In this study, we advocate the development of fake website detectionsystems that employ classification methods grounded in statistical learningtheory (SLT). Experimental results reveal that a prototype system developedusing SLT-based methods outperforms seven existing fake website detectionsystems on a test bed encompassing 900 real and fake websites.
arxiv-1309-7367 | Stochastic Online Shortest Path Routing: The Value of Feedback |  http://arxiv.org/abs/1309.7367  | author:M. Sadegh Talebi, Zhenhua Zou, Richard Combes, Alexandre Proutiere, Mikael Johansson category:cs.NI cs.LG math.OC published:2013-09-27 summary:This paper studies online shortest path routing over multi-hop networks. Linkcosts or delays are time-varying and modeled by independent and identicallydistributed random processes, whose parameters are initially unknown. Theparameters, and hence the optimal path, can only be estimated by routingpackets through the network and observing the realized delays. Our aim is tofind a routing policy that minimizes the regret (the cumulative difference ofexpected delay) between the path chosen by the policy and the unknown optimalpath. We formulate the problem as a combinatorial bandit optimization problemand consider several scenarios that differ in where routing decisions are madeand in the information available when making the decisions. For each scenario,we derive a tight asymptotic lower bound on the regret that has to be satisfiedby any online routing policy. These bounds help us to understand theperformance improvements we can expect when (i) taking routing decisions ateach hop rather than at the source only, and (ii) observing per-link delaysrather than end-to-end path delays. In particular, we show that (i) is of nouse while (ii) can have a spectacular impact. Three algorithms, with atrade-off between computational complexity and performance, are proposed. Theregret upper bounds of these algorithms improve over those of the existingalgorithms, and they significantly outperform state-of-the-art algorithms innumerical experiments.
arxiv-1309-7122 | Proceedings Wivace 2013 - Italian Workshop on Artificial Life and Evolutionary Computation |  http://arxiv.org/abs/1309.7122  | author:Alex Graudenzi, Giulio Caravagna, Giancarlo Mauri, Marco Antoniotti category:cs.CE cs.NE published:2013-09-27 summary:The Wivace 2013 Electronic Proceedings in Theoretical Computer Science(EPTCS) contain some selected long and short articles accepted for thepresentation at Wivace 2013 - Italian Workshop on Artificial Life andEvolutionary Computation, which was held at the University of Milan-Bicocca,Milan, on the 1st and 2nd of July, 2013.
arxiv-1309-7119 | Market Index and Stock Price Direction Prediction using Machine Learning Techniques: An empirical study on the KOSPI and HSI |  http://arxiv.org/abs/1309.7119  | author:Yanshan Wang, In-Chan Choi category:cs.CE cs.LG q-fin.ST published:2013-09-27 summary:The prediction of a stock market direction may serve as an earlyrecommendation system for short-term investors and as an early financialdistress warning system for long-term shareholders. In this paper, we proposean empirical study on the Korean and Hong Kong stock market with an integratedmachine learning framework that employs Principal Component Analysis (PCA) andSupport Vector Machine (SVM). We try to predict the upward or downwarddirection of stock market index and stock price. In the proposed framework,PCA, as a feature selection method, identifies principal components in thestock market movement and SVM, as a classifier for future stock marketmovement, processes them along with other economic factors in training andforecasting. We present the results of an extensive empirical study of theproposed method on the Korean composite stock price index (KOSPI) and Hangsengindex (HSI), as well as the individual constituents included in the indices. Inour experiment, ten years data (from January 1st, 2002 to January 1st, 2012)are collected and schemed by rolling windows to predict one-day-aheaddirections. The experimental results show notably high hit ratios in predictingthe movements of the individual constituents in the KOSPI and HSI. The resultsalso varify the \textit{co-movement} effect between the Korean (Hong Kong)stock market and the American stock market.
arxiv-1309-7261 | Detecting Fake Escrow Websites using Rich Fraud Cues and Kernel Based Methods |  http://arxiv.org/abs/1309.7261  | author:Ahmed Abbasi, Hsinchun Chen category:cs.CY cs.LG published:2013-09-27 summary:The ability to automatically detect fraudulent escrow websites is importantin order to alleviate online auction fraud. Despite research on related topics,fake escrow website categorization has received little attention. In this studywe evaluated the effectiveness of various features and techniques for detectingfake escrow websites. Our analysis included a rich set of features extractedfrom web page text, image, and link information. We also proposed a compositekernel tailored to represent the properties of fake websites, including contentduplication and structural attributes. Experiments were conducted to assess theproposed features, techniques, and kernels on a test bed encompassing nearly90,000 web pages derived from 410 legitimate and fake escrow sites. Thecombination of an extended feature set and the composite kernel attained over98% accuracy when differentiating fake sites from real ones, using the supportvector machines algorithm. The results suggest that automated web-basedinformation systems for detecting fake escrow sites could be feasible and maybe utilized as authentication mechanisms.
arxiv-1309-7340 | Early Stage Influenza Detection from Twitter |  http://arxiv.org/abs/1309.7340  | author:Jiwei Li, Claire Cardie category:cs.SI cs.CL published:2013-09-27 summary:Influenza is an acute respiratory illness that occurs virtually every yearand results in substantial disease, death and expense. Detection of Influenzain its earliest stage would facilitate timely action that could reduce thespread of the illness. Existing systems such as CDC and EISS which try tocollect diagnosis data, are almost entirely manual, resulting in about two-weekdelays for clinical data acquisition. Twitter, a popular microblogging service,provides us with a perfect source for early-stage flu detection due to itsreal- time nature. For example, when a flu breaks out, people that get the flumay post related tweets which enables the detection of the flu breakoutpromptly. In this paper, we investigate the real-time flu detection problem onTwitter data by proposing Flu Markov Network (Flu-MN): a spatio-temporalunsupervised Bayesian algorithm based on a 4 phase Markov Network, trying toidentify the flu breakout at the earliest stage. We test our model on realTwitter datasets from the United States along with baselines in multipleapplications, such as real-time flu breakout detection, future epidemic phaseprediction, or Influenza-like illness (ILI) physician visits. Experimentalresults show the robustness and effectiveness of our approach. We build up areal time flu reporting system based on the proposed approach, and we arehopeful that it would help government or health organizations in identifyingflu outbreaks and facilitating timely actions to decrease unnecessarymortality.
arxiv-1309-7170 | An Efficient Index for Visual Search in Appearance-based SLAM |  http://arxiv.org/abs/1309.7170  | author:Kiana Hajebi, Hong Zhang category:cs.CV cs.RO published:2013-09-27 summary:Vector-quantization can be a computationally expensive step in visualbag-of-words (BoW) search when the vocabulary is large. A BoW-based appearanceSLAM needs to tackle this problem for an efficient real-time operation. Wepropose an effective method to speed up the vector-quantization process inBoW-based visual SLAM. We employ a graph-based nearest neighbor search (GNNS)algorithm to this aim, and experimentally show that it can outperform thestate-of-the-art. The graph-based search structure used in GNNS can efficientlybe integrated into the BoW model and the SLAM framework. The graph-based index,which is a k-NN graph, is built over the vocabulary words and can be extractedfrom the BoW's vocabulary construction procedure, by adding one iteration tothe k-means clustering, which adds small extra cost. Moreover, exploiting thefact that images acquired for appearance-based SLAM are sequential, GNNS searchcan be initiated judiciously which helps increase the speedup of thequantization process considerably.
arxiv-1309-7270 | Evaluating the Usefulness of Sentiment Information for Focused Crawlers |  http://arxiv.org/abs/1309.7270  | author:Tianjun Fu, Ahmed Abbasi, Daniel Zeng, Hsinchun Chen category:cs.IR cs.CL published:2013-09-27 summary:Despite the prevalence of sentiment-related content on the Web, there hasbeen limited work on focused crawlers capable of effectively collecting suchcontent. In this study, we evaluated the efficacy of using sentiment-relatedinformation for enhanced focused crawling of opinion-rich web content regardinga particular topic. We also assessed the impact of using sentiment-labeled webgraphs to further improve collection accuracy. Experimental results on a largetest bed encompassing over half a million web pages revealed that focusedcrawlers utilizing sentiment information as well as sentiment-labeled webgraphs are capable of gathering more holistic collections of opinion-relatedcontent regarding a particular topic. The results have important implicationsfor business and marketing intelligence gathering efforts in the Web 2.0 era.
arxiv-1309-7311 | Bayesian Inference in Sparse Gaussian Graphical Models |  http://arxiv.org/abs/1309.7311  | author:Peter Orchard, Felix Agakov, Amos Storkey category:stat.ML cs.LG published:2013-09-27 summary:One of the fundamental tasks of science is to find explainable relationshipsbetween observed phenomena. One approach to this task that has receivedattention in recent years is based on probabilistic graphical modelling withsparsity constraints on model structures. In this paper, we describe two newapproaches to Bayesian inference of sparse structures of Gaussian graphicalmodels (GGMs). One is based on a simple modification of the cutting-edge blockGibbs sampler for sparse GGMs, which results in significant computational gainsin high dimensions. The other method is based on a specific construction of theHamiltonian Monte Carlo sampler, which results in further significantimprovements. We compare our fully Bayesian approaches with the popularregularisation-based graphical LASSO, and demonstrate significant advantages ofthe Bayesian treatment under the same computing costs. We apply the methods toa broad range of simulated data sets, and a real-life financial data set.
arxiv-1309-7312 | Development and Transcription of Assamese Speech Corpus |  http://arxiv.org/abs/1309.7312  | author:Himangshu Sarma, Navanath Saharia, Utpal Sharma, Smriti Kumar Sinha, Mancha Jyoti Malakar category:cs.CL published:2013-09-27 summary:A balanced speech corpus is the basic need for any speech processing task. Inthis report we describe our effort on development of Assamese speech corpus. Wemainly focused on some issues and challenges faced during development of thecorpus. Being a less computationally aware language, this is the first effortto develop speech corpus for Assamese. As corpus development is an ongoingprocess, in this paper we report only the initial task.
arxiv-1309-7266 | Evaluating Link-Based Techniques for Detecting Fake Pharmacy Websites |  http://arxiv.org/abs/1309.7266  | author:Ahmed Abbasi, Siddharth Kaza, F. Mariam Zahedi category:cs.CY cs.LG published:2013-09-27 summary:Fake online pharmacies have become increasingly pervasive, constituting over90% of online pharmacy websites. There is a need for fake website detectiontechniques capable of identifying fake online pharmacy websites with a highdegree of accuracy. In this study, we compared several well-known link-baseddetection techniques on a large-scale test bed with the hyperlink graphencompassing over 80 million links between 15.5 million web pages, including1.2 million known legitimate and fake pharmacy pages. We found that the QoC andQoL class propagation algorithms achieved an accuracy of over 90% on ourdataset. The results revealed that algorithms that incorporate dual classpropagation as well as inlink and outlink information, on page-level orsite-level graphs, are better suited for detecting fake pharmacy websites. Inaddition, site-level analysis yielded significantly better results thanpage-level analysis for most algorithms evaluated.
arxiv-1309-6831 | Batch-iFDD for Representation Expansion in Large MDPs |  http://arxiv.org/abs/1309.6831  | author:Alborz Geramifard, Thomas J. Walsh, Nicholas Roy, Jonathan How category:cs.LG stat.ML published:2013-09-26 summary:Matching pursuit (MP) methods are a promising class of feature constructionalgorithms for value function approximation. Yet existing MP methods requirecreating a pool of potential features, mandating expert knowledge orenumeration of a large feature pool, both of which hinder scalability. Thispaper introduces batch incremental feature dependency discovery (Batch-iFDD) asan MP method that inherits a provable convergence property. Additionally,Batch-iFDD does not require a large pool of features, leading to lowercomputational complexity. Empirical policy evaluation results across threedomains with up to one million states highlight the scalability of Batch-iFDDover the previous state of the art MP algorithm.
arxiv-1309-6833 | Multiple Instance Learning by Discriminative Training of Markov Networks |  http://arxiv.org/abs/1309.6833  | author:Hossein Hajimirsadeghi, Jinling Li, Greg Mori, Mohammad Zaki, Tarek Sayed category:cs.LG stat.ML published:2013-09-26 summary:We introduce a graphical framework for multiple instance learning (MIL) basedon Markov networks. This framework can be used to model the traditional MILdefinition as well as more general MIL definitions. Different levels ofambiguity -- the portion of positive instances in a bag -- can be explored inweakly supervised data. To train these models, we propose a discriminativemax-margin learning algorithm leveraging efficient inference forcardinality-based cliques. The efficacy of the proposed framework is evaluatedon a variety of data sets. Experimental results verify that encoding orlearning the degree of ambiguity can improve classification performance.
arxiv-1309-6834 | Unsupervised Learning of Noisy-Or Bayesian Networks |  http://arxiv.org/abs/1309.6834  | author:Yonatan Halpern, David Sontag category:cs.LG stat.ML published:2013-09-26 summary:This paper considers the problem of learning the parameters in Bayesiannetworks of discrete variables with known structure and hidden variables.Previous approaches in these settings typically use expectation maximization;when the network has high treewidth, the required expectations might beapproximated using Monte Carlo or variational methods. We show how to avoidinference altogether during learning by giving a polynomial-time algorithmbased on the method-of-moments, building upon recent work on learningdiscrete-valued mixture models. In particular, we show how to learn theparameters for a family of bipartite noisy-or Bayesian networks. In ourexperimental results, we demonstrate an application of our algorithm tolearning QMR-DT, a large Bayesian network used for medical diagnosis. We showthat it is possible to fully learn the parameters of QMR-DT even when only thefindings are observed in the training data (ground truth diseases unknown).
arxiv-1309-6835 | Gaussian Processes for Big Data |  http://arxiv.org/abs/1309.6835  | author:James Hensman, Nicolo Fusi, Neil D. Lawrence category:cs.LG stat.ML published:2013-09-26 summary:We introduce stochastic variational inference for Gaussian process models.This enables the application of Gaussian process (GP) models to data setscontaining millions of data points. We show how GPs can be vari- ationallydecomposed to depend on a set of globally relevant inducing variables whichfactorize the model in the necessary manner to perform variational inference.Our ap- proach is readily extended to models with non-Gaussian likelihoods andlatent variable models based around Gaussian processes. We demonstrate theapproach on a simple toy problem and two real world data sets.
arxiv-1309-6821 | Sample Complexity of Multi-task Reinforcement Learning |  http://arxiv.org/abs/1309.6821  | author:Emma Brunskill, Lihong Li category:cs.LG stat.ML published:2013-09-26 summary:Transferring knowledge across a sequence of reinforcement-learning tasks ischallenging, and has a number of important applications. Though there isencouraging empirical evidence that transfer can improve performance insubsequent reinforcement-learning tasks, there has been very little theoreticalanalysis. In this paper, we introduce a new multi-task algorithm for a sequenceof reinforcement-learning tasks when each task is sampled independently from(an unknown) distribution over a finite set of Markov decision processes whoseparameters are initially unknown. For this setting, we prove under certainassumptions that the per-task sample complexity of exploration is reducedsignificantly due to transfer compared to standard single-task algorithms. Ourmulti-task algorithm also has the desired characteristic that it is guaranteednot to exhibit negative transfer: in the worst case its per-task samplecomplexity is comparable to the corresponding single-task algorithm.
arxiv-1309-6820 | SparsityBoost: A New Scoring Function for Learning Bayesian Network Structure |  http://arxiv.org/abs/1309.6820  | author:Eliot Brenner, David Sontag category:cs.LG cs.AI stat.ML published:2013-09-26 summary:We give a new consistent scoring function for structure learning of Bayesiannetworks. In contrast to traditional approaches to scorebased structurelearning, such as BDeu or MDL, the complexity penalty that we propose isdata-dependent and is given by the probability that a conditional independencetest correctly shows that an edge cannot exist. What really distinguishes thisnew scoring function from earlier work is that it has the property of becomingcomputationally easier to maximize as the amount of data increases. We prove apolynomial sample complexity result, showing that maximizing this score isguaranteed to correctly learn a structure with no false edges and adistribution close to the generating distribution, whenever there exists aBayesian network which is a perfect map for the data generating distribution.Although the new score can be used with any search algorithm, we give empiricalresults showing that it is particularly effective when used together with alinear programming relaxation approach to Bayesian network structure learning.
arxiv-1309-6819 | Hilbert Space Embeddings of Predictive State Representations |  http://arxiv.org/abs/1309.6819  | author:Byron Boots, Geoffrey Gordon, Arthur Gretton category:cs.LG stat.ML published:2013-09-26 summary:Predictive State Representations (PSRs) are an expressive class of models forcontrolled stochastic processes. PSRs represent state as a set of predictionsof future observable events. Because PSRs are defined entirely in terms ofobservable data, statistically consistent estimates of PSR parameters can belearned efficiently by manipulating moments of observed training data. Mostlearning algorithms for PSRs have assumed that actions and observations arefinite with low cardinality. In this paper, we generalize PSRs to infinite setsof observations and actions, using the recent concept of Hilbert spaceembeddings of distributions. The essence is to represent the state as anonparametric conditional embedding operator in a Reproducing Kernel HilbertSpace (RKHS) and leverage recent work in kernel methods to estimate, predict,and update the representation. We show that these Hilbert space embeddings ofPSRs are able to gracefully handle continuous actions and observations, andthat our learned models outperform competing system identification algorithmson several prediction benchmarks.
arxiv-1309-6838 | Inverse Covariance Estimation for High-Dimensional Data in Linear Time and Space: Spectral Methods for Riccati and Sparse Models |  http://arxiv.org/abs/1309.6838  | author:Jean Honorio, Tommi S. Jaakkola category:cs.LG stat.ML published:2013-09-26 summary:We propose maximum likelihood estimation for learning Gaussian graphicalmodels with a Gaussian (ell_2^2) prior on the parameters. This is in contrastto the commonly used Laplace (ell_1) prior for encouraging sparseness. We showthat our optimization problem leads to a Riccati matrix equation, which has aclosed form solution. We propose an efficient algorithm that performs asingular value decomposition of the training data. Our algorithm isO(NT^2)-time and O(NT)-space for N variables and T samples. Our method istailored to high-dimensional problems (N gg T), in which sparseness promotingmethods become intractable. Furthermore, instead of obtaining a single solutionfor a specific regularization parameter, our algorithm finds the whole solutionpath. We show that the method has logarithmic sample complexity under thespiked covariance model. We also propose sparsification of the dense solutionwith provable performance guarantees. We provide techniques for using ourlearnt models, such as removing unimportant variables, computing likelihoodsand conditional distributions. Finally, we show promising results in severalgene expressions datasets.
arxiv-1309-6840 | Constrained Bayesian Inference for Low Rank Multitask Learning |  http://arxiv.org/abs/1309.6840  | author:Oluwasanmi Koyejo, Joydeep Ghosh category:cs.LG stat.ML published:2013-09-26 summary:We present a novel approach for constrained Bayesian inference. Unlikecurrent methods, our approach does not require convexity of the constraint set.We reduce the constrained variational inference to a parametric optimizationover the feasible set of densities and propose a general recipe for suchproblems. We apply the proposed constrained Bayesian inference approach tomultitask learning subject to rank constraints on the weight matrix. Further,constrained parameter estimation is applied to recover the sparse conditionalindependence structure encoded by prior precision matrices. Our approach ismotivated by reverse inference for high dimensional functional neuroimaging, adomain where the high dimensionality and small number of examples requires theuse of constraints to ensure meaningful and effective models. For thisapplication, we propose a model that jointly learns a weight matrix and theprior inverse covariance structure between different tasks. We presentexperimental validation showing that the proposed approach outperforms strongbaseline models in terms of predictive performance and structure recovery.
arxiv-1309-6818 | Boosting in the presence of label noise |  http://arxiv.org/abs/1309.6818  | author:Jakramate Bootkrajang, Ata Kaban category:cs.LG stat.ML published:2013-09-26 summary:Boosting is known to be sensitive to label noise. We studied two approachesto improve AdaBoost's robustness against labelling errors. One is to employ alabel-noise robust classifier as a base learner, while the other is to modifythe AdaBoost algorithm to be more robust. Empirical evaluation shows that acommittee of robust classifiers, although converges faster than non label-noiseaware AdaBoost, is still susceptible to label noise. However, pairing it withthe new robust Boosting algorithm we propose here results in a more resilientalgorithm under mislabelling.
arxiv-1309-6814 | High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning |  http://arxiv.org/abs/1309.6814  | author:Krishnakumar Balasubramanian, Kai Yu, Tong Zhang category:cs.LG stat.ML published:2013-09-26 summary:Joint sparsity regularization in multi-task learning has attracted muchattention in recent years. The traditional convex formulation employs the groupLasso relaxation to achieve joint sparsity across tasks. Although this approachleads to a simple convex formulation, it suffers from several issues due to thelooseness of the relaxation. To remedy this problem, we view jointly sparsemulti-task learning as a specialized random effects model, and derive a convexrelaxation approach that involves two steps. The first step learns thecovariance matrix of the coefficients using a convex formulation which we referto as sparse covariance coding; the second step solves a ridge regressionproblem with a sparse quadratic regularizer based on the covariance matrixobtained in the first step. It is shown that this approach produces anasymptotically optimal quadratic regularizer in the multitask learning settingwhen the number of tasks approaches infinity. Experimental results demonstratethat the convex formulation obtained via the proposed model significantlyoutperforms group Lasso (and related multi-stage formulations
arxiv-1309-6813 | Hinge-loss Markov Random Fields: Convex Inference for Structured Prediction |  http://arxiv.org/abs/1309.6813  | author:Stephen Bach, Bert Huang, Ben London, Lise Getoor category:cs.LG stat.ML published:2013-09-26 summary:Graphical models for structured domains are powerful tools, but thecomputational complexities of combinatorial prediction spaces can forcerestrictions on models, or require approximate inference in order to betractable. Instead of working in a combinatorial space, we use hinge-lossMarkov random fields (HL-MRFs), an expressive class of graphical models withlog-concave density functions over continuous variables, which can representconfidences in discrete predictions. This paper demonstrates that HL-MRFs aregeneral tools for fast and accurate structured prediction. We introduce thefirst inference algorithm that is both scalable and applicable to the fullclass of HL-MRFs, and show how to train HL-MRFs with several learningalgorithms. Our experiments show that HL-MRFs match or surpass the predictiveperformance of state-of-the-art methods, including discrete models, in fourapplication domains.
arxiv-1309-6867 | Speedy Model Selection (SMS) for Copula Models |  http://arxiv.org/abs/1309.6867  | author:Yaniv Tenzer, Gal Elidan category:cs.LG stat.ME published:2013-09-26 summary:We tackle the challenge of efficiently learning the structure of expressivemultivariate real-valued densities of copula graphical models. We start bytheoretically substantiating the conjecture that for many copula families themagnitude of Spearman's rank correlation coefficient is monotone in theexpected contribution of an edge in network, namely the negative copulaentropy. We then build on this theory and suggest a novel Bayesian approachthat makes use of a prior over values of Spearman's rho for learningcopula-based models that involve a mix of copula families. We demonstrate thegeneralization effectiveness of our highly efficient approach on sizable andvaried real-life datasets.
arxiv-1309-6868 | Approximate Kalman Filter Q-Learning for Continuous State-Space MDPs |  http://arxiv.org/abs/1309.6868  | author:Charles Tripp, Ross D. Shachter category:cs.LG stat.ML published:2013-09-26 summary:We seek to learn an effective policy for a Markov Decision Process (MDP) withcontinuous states via Q-Learning. Given a set of basis functions over stateaction pairs we search for a corresponding set of linear weights that minimizesthe mean Bellman residual. Our algorithm uses a Kalman filter model to estimatethose weights and we have developed a simpler approximate Kalman filter modelthat outperforms the current state of the art projected TD-Learning methods onseveral standard benchmark problems.
arxiv-1309-6865 | Modeling Documents with Deep Boltzmann Machines |  http://arxiv.org/abs/1309.6865  | author:Nitish Srivastava, Ruslan R Salakhutdinov, Geoffrey E. Hinton category:cs.LG cs.IR stat.ML published:2013-09-26 summary:We introduce a Deep Boltzmann Machine model suitable for modeling andextracting latent semantic representations from a large unstructured collectionof documents. We overcome the apparent difficulty of training a DBM withjudicious parameter tying. This parameter tying enables an efficientpretraining algorithm and a state initialization scheme that aids inference.The model can be trained just as efficiently as a standard Restricted BoltzmannMachine. Our experiments show that the model assigns better log probability tounseen data than the Replicated Softmax model. Features extracted from ourmodel outperform LDA, Replicated Softmax, and DocNADE models on documentretrieval and document classification tasks.
arxiv-1309-6869 | Finite-Time Analysis of Kernelised Contextual Bandits |  http://arxiv.org/abs/1309.6869  | author:Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, Nelo Cristianini category:cs.LG stat.ML published:2013-09-26 summary:We tackle the problem of online reward maximisation over a large finite setof actions described by their contexts. We focus on the case when the number ofactions is too big to sample all of them even once. However we assume that wehave access to the similarities between actions' contexts and that the expectedreward is an arbitrary linear function of the contexts' images in the relatedreproducing kernel Hilbert space (RKHS). We propose KernelUCB, a kernelised UCBalgorithm, and give a cumulative regret bound through a frequentist analysis.For contextual bandits, the related algorithm GP-UCB turns out to be a specialcase of our algorithm, and our finite-time analysis improves the regret boundof GP-UCB for the agnostic case, both in the terms of the kernel-dependentquantity and the RKHS norm of the reward function. Moreover, for the linearkernel, our regret bound matches the lower bound for contextual linear bandits.
arxiv-1309-6933 | Estimating Undirected Graphs Under Weak Assumptions |  http://arxiv.org/abs/1309.6933  | author:Larry Wasserman, Mladen Kolar, Alessandro Rinaldo category:math.ST cs.LG stat.ML stat.TH 62H12 published:2013-09-26 summary:We consider the problem of providing nonparametric confidence guarantees forundirected graphs under weak assumptions. In particular, we do not assumesparsity, incoherence or Normality. We allow the dimension $D$ to increase withthe sample size $n$. First, we prove lower bounds that show that if we wantaccurate inferences with low assumptions then there are limitations on thedimension as a function of sample size. When the dimension increases slowlywith sample size, we show that methods based on Normal approximations and onthe bootstrap lead to valid inferences and we provide Berry-Esseen bounds onthe accuracy of the Normal approximation. When the dimension is large relativeto sample size, accurate inferences for graphs under low assumptions are notpossible. Instead we propose to estimate something less demanding than theentire partial correlation graph. In particular, we consider: cluster graphs,restricted partial correlation graphs and correlation graphs.
arxiv-1309-6863 | Sparse Nested Markov models with Log-linear Parameters |  http://arxiv.org/abs/1309.6863  | author:Ilya Shpitser, Robin J. Evans, Thomas S. Richardson, James M. Robins category:cs.LG cs.AI stat.ML published:2013-09-26 summary:Hidden variables are ubiquitous in practical data analysis, and thereforemodeling marginal densities and doing inference with the resulting models is animportant problem in statistics, machine learning, and causal inference.Recently, a new type of graphical model, called the nested Markov model, wasdeveloped which captures equality constraints found in marginals of directedacyclic graph (DAG) models. Some of these constraints, such as the so called`Verma constraint', strictly generalize conditional independence. To makemodeling and inference with nested Markov models practical, it is necessary tolimit the number of parameters in the model, while still correctly capturingthe constraints in the marginal of a DAG model. Placing such limits is similarin spirit to sparsity methods for undirected graphical models, and regressionmodels. In this paper, we give a log-linear parameterization which allowssparse modeling with nested Markov models. We illustrate the advantages of thisparameterization with a simulation study.
arxiv-1309-7982 | On the Feature Discovery for App Usage Prediction in Smartphones |  http://arxiv.org/abs/1309.7982  | author:Zhung-Xun Liao, Shou-Chung Li, Wen-Chih Peng, Philip S Yu category:cs.LG published:2013-09-26 summary:With the increasing number of mobile Apps developed, they are now closelyintegrated into daily life. In this paper, we develop a framework to predictmobile Apps that are most likely to be used regarding the current device statusof a smartphone. Such an Apps usage prediction framework is a crucialprerequisite for fast App launching, intelligent user experience, and powermanagement of smartphones. By analyzing real App usage log data, we discovertwo kinds of features: The Explicit Feature (EF) from sensing readings ofbuilt-in sensors, and the Implicit Feature (IF) from App usage relations. TheIF feature is derived by constructing the proposed App Usage Graph (abbreviatedas AUG) that models App usage transitions. In light of AUG, we are able todiscover usage relations among Apps. Since users may have different usagebehaviors on their smartphones, we further propose one personalized featureselection algorithm. We explore minimum description length (MDL) from thetraining data and select those features which need less length to describe thetraining data. The personalized feature selection can successfully reduce thelog size and the prediction time. Finally, we adopt the kNN classificationmodel to predict Apps usage. Note that through the features selected by theproposed personalized feature selection algorithm, we only need to keep thesefeatures, which in turn reduces the prediction time and avoids the curse ofdimensionality when using the kNN classifier. We conduct a comprehensiveexperimental study based on a real mobile App usage dataset. The resultsdemonstrate the effectiveness of the proposed framework and show the predictivecapability for App usage prediction.
arxiv-1309-6876 | Bennett-type Generalization Bounds: Large-deviation Case and Faster Rate of Convergence |  http://arxiv.org/abs/1309.6876  | author:Chao Zhang category:stat.ML cs.LG published:2013-09-26 summary:In this paper, we present the Bennett-type generalization bounds of thelearning process for i.i.d. samples, and then show that the generalizationbounds have a faster rate of convergence than the traditional results. Inparticular, we first develop two types of Bennett-type deviation inequality forthe i.i.d. learning process: one provides the generalization bounds based onthe uniform entropy number; the other leads to the bounds based on theRademacher complexity. We then adopt a new method to obtain the alternativeexpressions of the Bennett-type generalization bounds, which imply that thebounds have a faster rate o(N^{-1/2}) of convergence than the traditionalresults O(N^{-1/2}). Additionally, we find that the rate of the bounds willbecome faster in the large-deviation case, which refers to a situation wherethe empirical risk is far away from (at least not close to) the expected risk.Finally, we analyze the asymptotical convergence of the learning process andcompare our analysis with the existing results.
