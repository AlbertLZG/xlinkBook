arxiv-4800-1 | Use Case Point Approach Based Software Effort Estimation using Various Support Vector Regression Kernel Methods | http://arxiv.org/pdf/1401.3069v2.pdf | author:Shashank Mouli Satapathy, Santanu Kumar Rath category:cs.SE cs.LG published:2014-01-14 summary:The job of software effort estimation is a critical one in the early stagesof the software development life cycle when the details of requirements areusually not clearly identified. Various optimization techniques help inimproving the accuracy of effort estimation. The Support Vector Regression(SVR) is one of several different soft-computing techniques that help ingetting optimal estimated values. The idea of SVR is based upon the computationof a linear regression function in a high dimensional feature space where theinput data are mapped via a nonlinear function. Further, the SVR kernel methodscan be applied in transforming the input data and then based on thesetransformations, an optimal boundary between the possible outputs can beobtained. The main objective of the research work carried out in this paper isto estimate the software effort using use case point approach. The use casepoint approach relies on the use case diagram to estimate the size and effortof software projects. Then, an attempt has been made to optimize the resultsobtained from use case point analysis using various SVR kernel methods toachieve better prediction accuracy.
arxiv-4800-2 | Hrebs and Cohesion Chains as similar tools for semantic text properties research | http://arxiv.org/pdf/1401.3669v1.pdf | author:D. Tatar, M. Lupea, E. Kapetanios category:cs.CL published:2014-01-15 summary:In this study it is proven that the Hrebs used in Denotation analysis oftexts and Cohesion Chains (de?ned as a fusion between Lexical Chains andCoreference Chains) represent similar linguistic tools. This result gives usthe possibility to extend to Cohesion Chains (CCs) some important indicatorsas, for example the Kernel of CCs, the topicality of a CC, text concentration,CC-di?useness and mean di?useness of the text. Let us mention that nowhere inthe Lexical Chains or Coreference Chains literature these kinds of indicatorsare introduced and used since now. Similarly, some applications of CCs in thestudy of a text (as for example segmentation or summarization of a text) couldbe realized starting from hrebs. As an illustration of the similarity betweenHrebs and CCs a detailed analyze of the poem "Lacul" by Mihai Eminescu isgiven.
arxiv-4800-3 | Intelligent Systems for Information Security | http://arxiv.org/pdf/1401.3592v1.pdf | author:Ayman M. Bahaa-Eldin category:cs.NE cs.CR published:2014-01-15 summary:This thesis aims to use intelligent systems to extend and improve performanceand security of cryptographic techniques. Genetic algorithms framework forcryptanalysis problem is addressed. A novel extension to the differentialcryptanalysis using genetic algorithm is proposed and a fitness measure basedon the differential characteristics of the cipher being attacked is alsoproposed. The complexity of the proposed attack is shown to be less thanquarter of normal differential cryptanalysis of the same cipher by applying theproposed attack to both the basic Substitution Permutation Network and theFeistel Network. The basic models of modern block ciphers are attacked insteadof actual cipher to prove that the attack is applicable to other ciphersvulnerable to differential cryptanalysis. A new attack for block cipher basedon the ability of neural networks to perform an approximation of mapping isproposed. A complete problem formulation is explained and implementation of theattack on some hypothetical Feistel cipher not vulnerable to differential orlinear attacks is presented. A new block cipher based on the neural networks isproposed. A complete cipher structure is given and a key scheduling is alsoshown. The main properties of neural network being able to perform mappingbetween large dimension domains in a very fast and a very small memory comparedto S-Boxes is used as a base for the cipher.
arxiv-4800-4 | Improving Performance Of English-Hindi Cross Language Information Retrieval Using Transliteration Of Query Terms | http://arxiv.org/pdf/1401.3510v1.pdf | author:Saurabh Varshney, Jyoti Bajpai category:cs.IR cs.CL published:2014-01-15 summary:The main issue in Cross Language Information Retrieval (CLIR) is the poorperformance of retrieval in terms of average precision when compared tomonolingual retrieval performance. The main reasons behind poor performance ofCLIR are mismatching of query terms, lexical ambiguity and un-translated queryterms. The existing problems of CLIR are needed to be addressed in order toincrease the performance of the CLIR system. In this paper, we are putting oureffort to solve the given problem by proposed an algorithm for improving theperformance of English-Hindi CLIR system. We used all possible combination ofHindi translated query using transliteration of English query terms andchoosing the best query among them for retrieval of documents. The experimentis performed on FIRE 2010 (Forum of Information Retrieval Evaluation) datasets.The experimental result show that the proposed approach gives betterperformance of English-Hindi CLIR system and also helps in overcoming existingproblems and outperforms the existing English-Hindi CLIR system in terms ofaverage precision.
arxiv-4800-5 | Text Relatedness Based on a Word Thesaurus | http://arxiv.org/pdf/1401.5699v1.pdf | author:George Tsatsaronis, Iraklis Varlamis, Michalis Vazirgiannis category:cs.CL published:2014-01-15 summary:The computation of relatedness between two fragments of text in an automatedmanner requires taking into account a wide range of factors pertaining to themeaning the two fragments convey, and the pairwise relations between theirwords. Without doubt, a measure of relatedness between text segments must takeinto account both the lexical and the semantic relatedness between words. Sucha measure that captures well both aspects of text relatedness may help in manytasks, such as text retrieval, classification and clustering. In this paper wepresent a new approach for measuring the semantic relatedness between wordsbased on their implicit semantic links. The approach exploits only a wordthesaurus in order to devise implicit semantic links between words. Based onthis approach, we introduce Omiotis, a new measure of semantic relatednessbetween texts which capitalizes on the word-to-word semantic relatednessmeasure (SR) and extends it to measure the relatedness between texts. Wegradually validate our method: we first evaluate the performance of thesemantic relatedness measure between individual words, covering word-to-wordsimilarity and relatedness, synonym identification and word analogy; then, weproceed with evaluating the performance of our method in measuring text-to-textsemantic relatedness in two tasks, namely sentence-to-sentence similarity andparaphrase recognition. Experimental evaluation shows that the proposed methodoutperforms every lexicon-based method of semantic relatedness in the selectedtasks and the used data sets, and competes well against corpus-based and hybridapproaches.
arxiv-4800-6 | Cross-lingual Annotation Projection for Semantic Roles | http://arxiv.org/pdf/1401.5694v1.pdf | author:Sebastian Pado, Mirella Lapata category:cs.CL published:2014-01-15 summary:This article considers the task of automatically inducing role-semanticannotations in the FrameNet paradigm for new languages. We propose a generalframework that is based on annotation projection, phrased as a graphoptimization problem. It is relatively inexpensive and has the potential toreduce the human effort involved in creating role-semantic resources. Withinthis framework, we present projection models that exploit lexical and syntacticinformation. We provide an experimental evaluation on an English-Germanparallel corpus which demonstrates the feasibility of inducing high-precisionGerman semantic role annotation both for manually and automatically annotatedEnglish data.
arxiv-4800-7 | Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches | http://arxiv.org/pdf/1401.5695v1.pdf | author:Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, Regina Barzilay category:cs.CL published:2014-01-15 summary:We demonstrate the effectiveness of multilingual learning for unsupervisedpart-of-speech tagging. The central assumption of our work is that by combiningcues from multiple languages, the structure of each becomes more apparent. Weconsider two ways of applying this intuition to the problem of unsupervisedpart-of-speech tagging: a model that directly merges tag structures for a pairof languages into a single sequence and a second model which insteadincorporates multilingual context using latent variables. Both approaches areformulated as hierarchical Bayesian models, using Markov Chain Monte Carlosampling techniques for inference. Our results demonstrate that byincorporating multilingual evidence we can achieve impressive performance gainsacross a range of scenarios. We also found that performance improves steadilyas the number of available languages increases.
arxiv-4800-8 | Content Modeling Using Latent Permutations | http://arxiv.org/pdf/1401.3488v1.pdf | author:Harr Chen, S. R. K. Branavan, Regina Barzilay, David R. Karger category:cs.IR cs.CL cs.LG published:2014-01-15 summary:We present a novel Bayesian topic model for learning discourse-level documentstructure. Our model leverages insights from discourse theory to constrainlatent topic assignments in a way that reflects the underlying organization ofdocument topics. We propose a global model in which both topic selection andordering are biased to be similar across a collection of related documents. Weshow that this space of orderings can be effectively represented using adistribution over permutations called the Generalized Mallows Model. We applyour method to three complementary discourse-level tasks: cross-documentalignment, document segmentation, and information ordering. Our experimentsshow that incorporating our permutation-based model in these applicationsyields substantial improvements in performance over previously proposedmethods.
arxiv-4800-9 | Enhancing QA Systems with Complex Temporal Question Processing Capabilities | http://arxiv.org/pdf/1401.3482v1.pdf | author:Estela Saquete, Jose Luis Vicedo, Patricio Martínez-Barco, Rafael Muñoz, Hector Llorens category:cs.CL cs.AI cs.IR published:2014-01-15 summary:This paper presents a multilayered architecture that enhances thecapabilities of current QA systems and allows different types of complexquestions or queries to be processed. The answers to these questions need to begathered from factual information scattered throughout different documents.Specifically, we designed a specialized layer to process the different types oftemporal questions. Complex temporal questions are first decomposed into simplequestions, according to the temporal relations expressed in the originalquestion. In the same way, the answers to the resulting simple questions arerecomposed, fulfilling the temporal restrictions of the original complexquestion. A novel aspect of this approach resides in the decomposition whichuses a minimal quantity of resources, with the final aim of obtaining aportable platform that is easily extensible to other languages. In this paperwe also present a methodology for evaluation of the decomposition of thequestions as well as the ability of the implemented temporal layer to performat a multilingual level. The temporal layer was first performed for English,then evaluated and compared with: a) a general purpose QA system (F-measure65.47% for QA plus English temporal layer vs. 38.01% for the general QAsystem), and b) a well-known QA system. Much better results were obtained fortemporal questions with the multilayered system. This system was thereforeextended to Spanish and very good results were again obtained in the evaluation(F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the generalQA system).
arxiv-4800-10 | Complex Question Answering: Unsupervised Learning Approaches and Experiments | http://arxiv.org/pdf/1401.3479v1.pdf | author:Yllias Chali, Shafiq Rayhan Joty, Sadid A. Hasan category:cs.CL cs.IR cs.LG published:2014-01-15 summary:Complex questions that require inferencing and synthesizing information frommultiple documents can be seen as a kind of topic-oriented, informativemulti-document summarization where the goal is to produce a single text as acompressed version of a set of documents with a minimum loss of relevantinformation. In this paper, we experiment with one empirical method and twounsupervised statistical machine learning techniques: K-means and ExpectationMaximization (EM), for computing relative importance of the sentences. Wecompare the results of these approaches. Our experiments show that theempirical approach outperforms the other two techniques and EM performs betterthan K-means. However, the performance of these approaches depends entirely onthe feature set used and the weighting of these features. In order to measurethe importance and relevance to the user query we extract different kinds offeatures (i.e. lexical, lexical semantic, cosine similarity, basic element,tree kernel based syntactic and shallow-semantic) for each of the documentsentences. We use a local search technique to learn the weights of thefeatures. To the best of our knowledge, no study has used tree kernel functionsto encode syntactic/semantic information for more complex tasks such ascomputing the relatedness between the query sentences and the documentsentences in order to generate query-focused summaries (or answers to complexquestions). For each of our methods of generating summaries (i.e. empirical,K-means and EM) we show the effects of syntactic and shallow-semantic featuresover the bag-of-words (BOW) features.
arxiv-4800-11 | Efficient Markov Network Structure Discovery Using Independence Tests | http://arxiv.org/pdf/1401.3478v1.pdf | author:Facundo Bromberg, Dimitris Margaritis, Vasant Honavar category:cs.LG cs.AI stat.ML published:2014-01-15 summary:We present two algorithms for learning the structure of a Markov network fromdata: GSMN* and GSIMN. Both algorithms use statistical independence tests toinfer the structure by successively constraining the set of structuresconsistent with the results of these tests. Until very recently, algorithms forstructure learning were based on maximum likelihood estimation, which has beenproved to be NP-hard for Markov networks due to the difficulty of estimatingthe parameters of the network, needed for the computation of the datalikelihood. The independence-based approach does not require the computation ofthe likelihood, and thus both GSMN* and GSIMN can compute the structureefficiently (as shown in our experiments). GSMN* is an adaptation of theGrow-Shrink algorithm of Margaritis and Thrun for learning the structure ofBayesian networks. GSIMN extends GSMN* by additionally exploiting Pearlswell-known properties of the conditional independence relation to infer novelindependences from known ones, thus avoiding the performance of statisticaltests to estimate them. To accomplish this efficiently GSIMN uses the Triangletheorem, also introduced in this work, which is a simplified version of the setof Markov axioms. Experimental comparisons on artificial and real-world datasets show GSIMN can yield significant savings with respect to GSMN*, whilegenerating a Markov network with comparable or in some cases improved quality.We also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH,that produces all possible conditional independences resulting from repeatedlyapplying Pearls theorems on the known conditional independence tests. Theresults of this comparison show that GSIMN, by the sole use of the Triangletheorem, is nearly optimal in terms of the set of independences tests that itinfers.
arxiv-4800-12 | Unsupervised Methods for Determining Object and Relation Synonyms on the Web | http://arxiv.org/pdf/1401.5696v1.pdf | author:Alexander Pieter Yates, Oren Etzioni category:cs.CL published:2014-01-15 summary:The task of identifying synonymous relations and objects, or synonymresolution, is critical for high-quality information extraction. This paperinvestigates synonym resolution in the context of unsupervised informationextraction, where neither hand-tagged training examples nor domain knowledge isavailable. The paper presents a scalable, fully-implemented system that runs inO(KN log N) time in the number of extractions, N, and the maximum number ofsynonyms per word, K. The system, called Resolver, introduces a probabilisticrelational model for predicting whether two strings are co-referential based onthe similarity of the assertions containing them. On a set of two millionassertions extracted from the Web, Resolver resolves objects with 78% precisionand 68% recall, and resolves relations with 90% precision and 35% recall.Several variations of resolvers probabilistic model are explored, andexperiments demonstrate that under appropriate conditions these variations canimprove F1 by 5%. An extension to the basic Resolver system allows it to handlepolysemous names with 97% precision and 95% recall on a data set from the TRECcorpus.
arxiv-4800-13 | Inferring Shallow-Transfer Machine Translation Rules from Small Parallel Corpora | http://arxiv.org/pdf/1401.5700v1.pdf | author:Felipe Sánchez-Martínez, Mikel L. Forcada category:cs.CL published:2014-01-15 summary:This paper describes a method for the automatic inference of structuraltransfer rules to be used in a shallow-transfer machine translation (MT) systemfrom small parallel corpora. The structural transfer rules are based onalignment templates, like those used in statistical MT. Alignment templates areextracted from sentence-aligned parallel corpora and extended with a set ofrestrictions which are derived from the bilingual dictionary of the MT systemand control their application as transfer rules. The experiments conductedusing three different language pairs in the free/open-source MT platformApertium show that translation quality is improved as compared to word-for-wordtranslation (when no transfer rules are used), and that the resultingtranslation quality is close to that obtained using hand-coded transfer rules.The method we present is entirely unsupervised and benefits from information inthe rest of modules of the MT system in which the inferred rules are applied.
arxiv-4800-14 | Learning Bayesian Network Equivalence Classes with Ant Colony Optimization | http://arxiv.org/pdf/1401.3464v1.pdf | author:Rónán Daly, Qiang Shen category:cs.NE cs.AI cs.LG published:2014-01-15 summary:Bayesian networks are a useful tool in the representation of uncertainknowledge. This paper proposes a new algorithm called ACO-E, to learn thestructure of a Bayesian network. It does this by conducting a search throughthe space of equivalence classes of Bayesian networks using Ant ColonyOptimization (ACO). To this end, two novel extensions of traditional ACOtechniques are proposed and implemented. Firstly, multiple types of moves areallowed. Secondly, moves can be given in terms of indices that are not based onconstruction graph nodes. The results of testing show that ACO-E performsbetter than a greedy search and other state-of-the-art and metaheuristicalgorithms whilst searching in the space of equivalence classes.
arxiv-4800-15 | Wikipedia-based Semantic Interpretation for Natural Language Processing | http://arxiv.org/pdf/1401.5697v1.pdf | author:Evgeniy Gabrilovich, Shaul Markovitch category:cs.CL published:2014-01-15 summary:Adequate representation of natural language semantics requires access to vastamounts of common sense and domain-specific world knowledge. Prior work in thefield was based on purely statistical techniques that did not make use ofbackground knowledge, on limited lexicographic knowledge bases such as WordNet,or on huge manual efforts such as the CYC project. Here we propose a novelmethod, called Explicit Semantic Analysis (ESA), for fine-grained semanticinterpretation of unrestricted natural language texts. Our method representsmeaning in a high-dimensional space of concepts derived from Wikipedia, thelargest encyclopedia in existence. We explicitly represent the meaning of anytext in terms of Wikipedia-based concepts. We evaluate the effectiveness of ourmethod on text categorization and on computing the degree of semanticrelatedness between fragments of natural language text. Using ESA results insignificant improvements over the previous state of the art in both tasks.Importantly, due to the use of natural concepts, the ESA model is easy toexplain to human users.
arxiv-4800-16 | Sentence Compression as Tree Transduction | http://arxiv.org/pdf/1401.5693v1.pdf | author:Trevor Anthony Cohn, Mirella Lapata category:cs.CL published:2014-01-15 summary:This paper presents a tree-to-tree transduction method for sentencecompression. Our model is based on synchronous tree substitution grammar, aformalism that allows local distortion of the tree topology and can thusnaturally capture structural mismatches. We describe an algorithm for decodingin this framework and show how the model can be trained discriminatively withina large margin framework. Experimental results on sentence compression bringsignificant improvements over a state-of-the-art model.
arxiv-4800-17 | Learning Document-Level Semantic Properties from Free-Text Annotations | http://arxiv.org/pdf/1401.3457v1.pdf | author:S. R. K. Branavan, Harr Chen, Jacob Eisenstein, Regina Barzilay category:cs.CL cs.IR published:2014-01-15 summary:This paper presents a new method for inferring the semantic properties ofdocuments by leveraging free-text keyphrase annotations. Such annotations arebecoming increasingly abundant due to the recent dramatic growth insemi-structured, user-generated online content. One especially relevant domainis product reviews, which are often annotated by their authors with pros/conskeyphrases such as a real bargain or good value. These annotations arerepresentative of the underlying semantic properties; however, unlike expertannotations, they are noisy: lay authors may use different labels to denote thesame property, and some labels may be missing. To learn using such noisyannotations, we find a hidden paraphrase structure which clusters thekeyphrases. The paraphrase structure is linked with a latent topic model of thereview texts, enabling the system to predict the properties of unannotateddocuments and to effectively aggregate the semantic properties of multiplereviews. Our approach is implemented as a hierarchical Bayesian model withjoint inference. We find that joint inference increases the robustness of thekeyphrase clustering and encourages the latent topics to correlate withsemantically meaningful properties. Multiple evaluations demonstrate that ourmodel substantially outperforms alternative approaches for summarizing singleand multiple documents into a set of semantically salient keyphrases.
arxiv-4800-18 | A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics | http://arxiv.org/pdf/1401.3454v1.pdf | author:Sherief Abdallah, Victor Lesser category:cs.LG cs.MA published:2014-01-15 summary:Several multiagent reinforcement learning (MARL) algorithms have beenproposed to optimize agents decisions. Due to the complexity of the problem,the majority of the previously developed MARL algorithms assumed agents eitherhad some knowledge of the underlying game (such as Nash equilibria) and/orobserved other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL),which allows agents to reach a Nash Equilibrium (NE) in benchmark2-player-2-action games with minimum knowledge. Using WPL, the only feedback anagent needs is its own local reward (the agent does not observe other agentsactions or rewards). Furthermore, WPL does not assume that agents know theunderlying game or the corresponding Nash Equilibrium a priori. Weexperimentally show that our algorithm converges in benchmarktwo-player-two-action games. We also show that our algorithm converges in thechallenging Shapleys game where previous MARL algorithms failed to convergewithout knowing the underlying game or the NE. Furthermore, we show that WPLoutperforms the state-of-the-art algorithms in a more realistic setting of 100agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm isanalyzing the dynamics of the algorithm: how the policies of multiple learningagents evolve over time as agents interact with one another. Such an analysisnot only verifies whether agents using a given MARL algorithm will eventuallyconverge, but also reveals the behavior of the MARL algorithm prior toconvergence. We analyze our algorithm in two-player-two-action games and showthat symbolically proving WPLs convergence is difficult, because of thenon-linear nature of WPLs dynamics, unlike previous MARL algorithms that hadeither linear or piece-wise-linear dynamics. Instead, we numerically solve WPLsdynamics differential equations and compare the solution to the dynamics ofprevious MARL algorithms.
arxiv-4800-19 | Identification of Pleonastic It Using the Web | http://arxiv.org/pdf/1401.5698v1.pdf | author:Yifan Li, Petr Musilek, Marek Reformat, Loren Wyard-Scott category:cs.CL published:2014-01-15 summary:In a significant minority of cases, certain pronouns, especially the pronounit, can be used without referring to any specific entity. This phenomenon ofpleonastic pronoun usage poses serious problems for systems aiming at even ashallow understanding of natural language texts. In this paper, a novelapproach is proposed to identify such uses of it: the extrapositional cases areidentified using a series of queries against the web, and the cleft cases areidentified using a simple set of syntactic rules. The system is evaluated withfour sets of news articles containing 679 extrapositional cases as well as 78cleft constructs. The identification results are comparable to those obtainedby human efforts.
arxiv-4800-20 | Anytime Induction of Low-cost, Low-error Classifiers: a Sampling-based Approach | http://arxiv.org/pdf/1401.3447v1.pdf | author:Saher Esmeir, Shaul Markovitch category:cs.LG published:2014-01-15 summary:Machine learning techniques are gaining prevalence in the production of awide range of classifiers for complex real-world applications with nonuniformtesting and misclassification costs. The increasing complexity of theseapplications poses a real challenge to resource management during learning andclassification. In this work we introduce ACT (anytime cost-sensitive treelearner), a novel framework for operating in such complex environments. ACT isan anytime algorithm that allows learning time to be increased in return forlower classification costs. It builds a tree top-down and exploits additionaltime resources to obtain better estimations for the utility of the differentcandidate splits. Using sampling techniques, ACT approximates the cost of thesubtree under each candidate split and favors the one with a minimal cost. As astochastic algorithm, ACT is expected to be able to escape local minima, intowhich greedy methods may be trapped. Experiments with a variety of datasetswere conducted to compare ACT to the state-of-the-art cost-sensitive treelearners. The results show that for the majority of domains ACT producessignificantly less costly trees. ACT also exhibits good anytime behavior withdiminishing returns.
arxiv-4800-21 | Amino Acid Interaction Network Prediction using Multi-objective Optimization | http://arxiv.org/pdf/1401.3446v1.pdf | author:Md. Shiplu Hawlader, Saifuddin Md. Tareeq category:cs.CE cs.NE published:2014-01-15 summary:Protein can be represented by amino acid interaction network. This network isa graph whose vertices are the proteins amino acids and whose edges are theinteractions between them. This interaction network is the first step ofproteins three-dimensional structure prediction. In this paper we present amulti-objective evolutionary algorithm for interaction prediction and antcolony probabilistic optimization algorithm is used to confirm the interaction.
arxiv-4800-22 | Transductive Rademacher Complexity and its Applications | http://arxiv.org/pdf/1401.3441v1.pdf | author:Ran El-Yaniv, Dmitry Pechyony category:cs.LG cs.AI stat.ML published:2014-01-15 summary:We develop a technique for deriving data-dependent error bounds fortransductive learning algorithms based on transductive Rademacher complexity.Our technique is based on a novel general error bound for transduction in termsof transductive Rademacher complexity, together with a novel bounding techniquefor Rademacher averages for particular algorithms, in terms of their"unlabeled-labeled" representation. This technique is relevant to many advancedgraph-based transductive algorithms and we demonstrate its effectiveness byderiving error bounds to three well known algorithms. Finally, we present a newPAC-Bayesian bound for mixtures of transductive algorithms based on ourRademacher bounds.
arxiv-4800-23 | Adaptive Stochastic Resource Control: A Machine Learning Approach | http://arxiv.org/pdf/1401.3434v1.pdf | author:Balázs Csanád Csáji, László Monostori category:cs.LG published:2014-01-15 summary:The paper investigates stochastic resource allocation problems with scarce,reusable resources and non-preemtive, time-dependent, interconnected tasks.This approach is a natural generalization of several standard resourcemanagement problems, such as scheduling and transportation problems. First,reactive solutions are considered and defined as control policies of suitablyreformulated Markov decision processes (MDPs). We argue that this reformulationhas several favorable properties, such as it has finite state and actionspaces, it is aperiodic, hence all policies are proper and the space of controlpolicies can be safely restricted. Next, approximate dynamic programming (ADP)methods, such as fitted Q-learning, are suggested for computing an efficientcontrol policy. In order to compactly maintain the cost-to-go function, tworepresentations are studied: hash tables and support vector regression (SVR),particularly, nu-SVRs. Several additional improvements, such as the applicationof limited-lookahead rollout algorithms in the initial phases, action spacedecomposition, task clustering and distributed sampling are investigated, too.Finally, experimental results on both benchmark and industry-related data arepresented.
arxiv-4800-24 | A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for Range Finders in Dynamic Environments | http://arxiv.org/pdf/1401.3432v1.pdf | author:Tinne De Laet, Joris De Schutter, Herman Bruyninckx category:cs.AI cs.LG published:2014-01-15 summary:This paper proposes and experimentally validates a Bayesian network model ofa range finder adapted to dynamic environments. All modeling assumptions arerigorously explained, and all model parameters have a physical interpretation.This approach results in a transparent and intuitive model. With respect to thestate of the art beam model this paper: (i) proposes a different functionalform for the probability of range measurements caused by unmodeled objects,(ii) intuitively explains the discontinuity encountered in te state of the artbeam model, and (iii) reduces the number of model parameters, while maintainingthe same representational power for experimental data. The proposed beam modelis called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihoodand a variational Bayesian estimator (both based on expectation-maximization)are proposed to learn the model parameters. Furthermore, the RBBM is extended to a full scan model in two steps: first,to a full scan model for static environments and next, to a full scan model forgeneral, dynamic environments. The full scan model accounts for the dependencybetween beams and adapts to the local sample density when using a particlefilter. In contrast to Gaussian-based state of the art models, the proposedfull scan model uses a sample-based approximation. This sample-basedapproximation enables handling dynamic environments and capturingmulti-modality, which occurs even in simple static environments.
arxiv-4800-25 | Latent Tree Models and Approximate Inference in Bayesian Networks | http://arxiv.org/pdf/1401.3429v1.pdf | author:Yi Wang, Nevin L. Zhang, Tao Chen category:cs.LG published:2014-01-15 summary:We propose a novel method for approximate inference in Bayesian networks(BNs). The idea is to sample data from a BN, learn a latent tree model (LTM)from the data offline, and when online, make inference with the LTM instead ofthe original BN. Because LTMs are tree-structured, inference takes linear time.In the meantime, they can represent complex relationship among leaf nodes andhence the approximation accuracy is often good. Empirical evidence shows thatour method can achieve good approximation accuracy at low online computationalcost.
arxiv-4800-26 | Analogical Dissimilarity: Definition, Algorithms and Two Experiments in Machine Learning | http://arxiv.org/pdf/1401.3427v1.pdf | author:Laurent Miclet, Sabri Bayoudh, Arnaud Delhay category:cs.LG cs.AI published:2014-01-15 summary:This paper defines the notion of analogical dissimilarity between fourobjects, with a special focus on objects structured as sequences. Firstly, itstudies the case where the four objects have a null analogical dissimilarity,i.e. are in analogical proportion. Secondly, when one of these objects isunknown, it gives algorithms to compute it. Thirdly, it tackles the problem ofdefining analogical dissimilarity, which is a measure of how far four objectsare from being in analogical proportion. In particular, when objects aresequences, it gives a definition and an algorithm based on an optimal alignmentof the four sequences. It gives also learning algorithms, i.e. methods to findthe triple of objects in a learning sample which has the least analogicaldissimilarity with a given object. Two practical experiments are described: thefirst is a classification problem on benchmarks of binary and nominal data, thesecond shows how the generation of sequences by solving analogical equationsenables a handwritten character recognition system to rapidly be adapted to anew writer.
arxiv-4800-27 | Infinite Mixed Membership Matrix Factorization | http://arxiv.org/pdf/1401.3413v1.pdf | author:Avneesh Saluja, Mahdi Pakdaman, Dongzhen Piao, Ankur P. Parikh category:cs.LG cs.IR published:2014-01-15 summary:Rating and recommendation systems have become a popular application area forapplying a suite of machine learning techniques. Current approaches relyprimarily on probabilistic interpretations and extensions of matrixfactorization, which factorizes a user-item ratings matrix into latent user anditem vectors. Most of these methods fail to model significant variations initem ratings from otherwise similar users, a phenomenon known as the "NapoleonDynamite" effect. Recent efforts have addressed this problem by adding acontextual bias term to the rating, which captures the mood under which a userrates an item or the context in which an item is rated by a user. In this work,we extend this model in a nonparametric sense by learning the optimal number ofmoods or contexts from the data, and derive Gibbs sampling inference proceduresfor our model. We evaluate our approach on the MovieLens 1M dataset, and showsignificant improvements over the optimal parametric baseline, more than twicethe improvements previously encountered for this task. We also extract andevaluate a DBLP dataset, wherein we predict the number of papers co-authored bytwo authors, and present improvements over the parametric baseline on thisalternative domain as well.
arxiv-4800-28 | MRFalign: Protein Homology Detection through Alignment of Markov Random Fields | http://arxiv.org/pdf/1401.2668v2.pdf | author:Jianzhu Ma, Sheng Wang, Zhiyong Wang, Jinbo Xu category:q-bio.QM cs.CE cs.LG published:2014-01-12 summary:Sequence-based protein homology detection has been extensively studied and sofar the most sensitive method is based upon comparison of protein sequenceprofiles, which are derived from multiple sequence alignment (MSA) of sequencehomologs in a protein family. A sequence profile is usually represented as aposition-specific scoring matrix (PSSM) or an HMM (Hidden Markov Model) andaccordingly PSSM-PSSM or HMM-HMM comparison is used for homolog detection. Thispaper presents a new homology detection method MRFalign, consisting of threekey components: 1) a Markov Random Fields (MRF) representation of a proteinfamily; 2) a scoring function measuring similarity of two MRFs; and 3) anefficient ADMM (Alternating Direction Method of Multipliers) algorithm aligningtwo MRFs. Compared to HMM that can only model very short-range residuecorrelation, MRFs can model long-range residue interaction pattern and thus,encode information for the global 3D structure of a protein family.Consequently, MRF-MRF comparison for remote homology detection shall be muchmore sensitive than HMM-HMM or PSSM-PSSM comparison. Experiments confirm thatMRFalign outperforms several popular HMM or PSSM-based methods in terms of bothalignment accuracy and remote homology detection and that MRFalign worksparticularly well for mainly beta proteins. For example, tested on thebenchmark SCOP40 (8353 proteins) for homology detection, PSSM-PSSM and HMM-HMMsucceed on 48% and 52% of proteins, respectively, at superfamily level, and on15% and 27% of proteins, respectively, at fold level. In contrast, MRFalignsucceeds on 57.3% and 42.5% of proteins at superfamily and fold level,respectively. This study implies that long-range residue interaction patternsare very helpful for sequence-based homology detection. The software isavailable for download at http://raptorx.uchicago.edu/download/.
arxiv-4800-29 | Binary Classifier Calibration: Non-parametric approach | http://arxiv.org/pdf/1401.3390v1.pdf | author:Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht category:stat.ML cs.LG published:2014-01-14 summary:Accurate calibration of probabilistic predictive models learned is criticalfor many practical prediction and decision-making tasks. There are two maincategories of methods for building calibrated classifiers. One approach is todevelop methods for learning probabilistic models that are well-calibrated, abinitio. The other approach is to use some post-processing methods fortransforming the output of a classifier to be well calibrated, as for examplehistogram binning, Platt scaling, and isotonic regression. One advantage of thepost-processing approach is that it can be applied to any existingprobabilistic classification model that was constructed using anymachine-learning method. In this paper, we first introduce two measures for evaluating how well aclassifier is calibrated. We prove three theorems showing that using a simplehistogram binning post-processing method, it is possible to make a classifierbe well calibrated while retaining its discrimination capability. Also, bycasting the histogram binning method as a density-based non-parametric binaryclassifier, we can extend it using two simple non-parametric density estimationmethods. We demonstrate the performance of the proposed calibration methods onsynthetic and real datasets. Experimental results show that the proposedmethods either outperform or are comparable to existing calibration methods.
arxiv-4800-30 | A programme to determine the exact interior of any connected digital picture | http://arxiv.org/pdf/1401.3385v1.pdf | author:Antonio Elias Fabris, Valério Ramos Batista category:cs.CG cs.CV cs.GR published:2014-01-14 summary:Region filling is one of the most important and fundamental operations incomputer graphics and image processing. Many filling algorithms and theirimplementations are based on the Euclidean geometry, which are then translatedinto computational models moving carelessly from the continuous to the finitediscrete space of the computer. The consequences of this approach is that mostimplementations fail when tested for challenging degenerate and nearlydegenerate regions. We present a correct integer-only procedure that works forall connected digital pictures. It finds all possible interior points, whichare then displayed and stored in a locating matrix. Namely, we present afilling and locating procedure that can be used in computer graphics and imageprocessing applications.
arxiv-4800-31 | Learning Language from a Large (Unannotated) Corpus | http://arxiv.org/pdf/1401.3372v1.pdf | author:Linas Vepstas, Ben Goertzel category:cs.CL cs.LG published:2014-01-14 summary:A novel approach to the fully automated, unsupervised extraction ofdependency grammars and associated syntax-to-semantic-relationship mappingsfrom large text corpora is described. The suggested approach builds on theauthors' prior work with the Link Grammar, RelEx and OpenCog systems, as wellas on a number of prior papers and approaches from the statistical languagelearning literature. If successful, this approach would enable the mining ofall the information needed to power a natural language comprehension andgeneration system, directly from a large, unannotated corpus.
arxiv-4800-32 | Survey On The Estimation Of Mutual Information Methods as a Measure of Dependency Versus Correlation Analysis | http://arxiv.org/pdf/1401.3358v1.pdf | author:D. Gencaga, N. K. Malakar, D. J. Lary category:stat.ML 94A17 published:2014-01-14 summary:In this survey, we present and compare different approaches to estimateMutual Information (MI) from data to analyse general dependencies betweenvariables of interest in a system. We demonstrate the performance difference ofMI versus correlation analysis, which is only optimal in case of lineardependencies. First, we use a piece-wise constant Bayesian methodology using ageneral Dirichlet prior. In this estimation method, we use a two-stage approachwhere we approximate the probability distribution first and then calculate themarginal and joint entropies. Here, we demonstrate the performance of thisBayesian approach versus the others for computing the dependency betweendifferent variables. We also compare these with linear correlation analysis.Finally, we apply MI and correlation analysis to the identification of the biasin the determination of the aerosol optical depth (AOD) by the satellite basedModerate Resolution Imaging Spectroradiometer (MODIS) and the ground basedAErosol RObotic NETwork (AERONET). Here, we observe that the AOD measurementsby these two instruments might be different for the same location. The reasonof this bias is explored by quantifying the dependencies between the bias and15 other variables including cloud cover, surface reflectivity and others.
arxiv-4800-33 | A Boosting Approach to Learning Graph Representations | http://arxiv.org/pdf/1401.3258v1.pdf | author:Rajmonda Caceres, Kevin Carter, Jeremy Kun category:cs.LG cs.SI stat.ML published:2014-01-14 summary:Learning the right graph representation from noisy, multisource data hasgarnered significant interest in recent years. A central tenet of this problemis relational learning. Here the objective is to incorporate the partialinformation each data source gives us in a way that captures the trueunderlying relationships. To address this challenge, we present a general,boosting-inspired framework for combining weak evidence of entity associationsinto a robust similarity metric. We explore the extent to which differentquality measurements yield graph representations that are suitable forcommunity detection. We then present empirical results on both synthetic andreal datasets demonstrating the utility of this framework. Our framework leadsto suitable global graph representations from quality measurements local toeach edge. Finally, we discuss future extensions and theoretical considerationsof learning useful graph representations from weak feedback in generalapplication settings.
arxiv-4800-34 | Online Markov decision processes with Kullback-Leibler control cost | http://arxiv.org/pdf/1401.3198v1.pdf | author:Peng Guan, Maxim Raginsky, Rebecca Willett category:math.OC cs.LG cs.SY published:2014-01-14 summary:This paper considers an online (real-time) control problem that involves anagent performing a discrete-time random walk over a finite state space. Theagent's action at each time step is to specify the probability distribution forthe next state given the current state. Following the set-up of Todorov, thestate-action cost at each time step is a sum of a state cost and a control costgiven by the Kullback-Leibler (KL) divergence between the agent's next-statedistribution and that determined by some fixed passive dynamics. The onlineaspect of the problem is due to the fact that the state cost functions aregenerated by a dynamic environment, and the agent learns the current state costonly after selecting an action. An explicit construction of a computationallyefficient strategy with small regret (i.e., expected difference between itsactual total cost and the smallest cost attainable using noncausal knowledge ofthe state costs) under mild regularity conditions is presented, along with ademonstration of the performance of the proposed strategy on a simulated targettracking problem. A number of new results on Markov decision processes with KLcontrol cost are also obtained.
arxiv-4800-35 | Dynamic Topology Adaptation and Distributed Estimation for Smart Grids | http://arxiv.org/pdf/1401.3148v1.pdf | author:S. Xu, R. C. de Lamare, H. V. Poor category:cs.IT cs.LG math.IT published:2014-01-14 summary:This paper presents new dynamic topology adaptation strategies fordistributed estimation in smart grids systems. We propose a dynamic exhaustivesearch--based topology adaptation algorithm and a dynamic sparsity--inspiredtopology adaptation algorithm, which can exploit the topology of smart gridswith poor--quality links and obtain performance gains. We incorporate anoptimized combining rule, named Hastings rule into our proposed dynamictopology adaptation algorithms. Compared with the existing works in theliterature on distributed estimation, the proposed algorithms have a betterconvergence rate and significantly improve the system performance. Theperformance of the proposed algorithms is compared with that of existingalgorithms in the IEEE 14--bus system.
arxiv-4800-36 | Dependence Measure for non-additive model | http://arxiv.org/pdf/1310.1562v3.pdf | author:Hangjin Jiang, Yiming Ding category:stat.ML published:2013-10-06 summary:We proposed a new statistical dependency measure called Copula DependencyCoefficient(CDC) for two sets of variables based on copula. It is robust tooutliers, easy to implement, powerful and appropriate to high-dimensionalvariables. These properties are important in many applications. Experimentalresults show that CDC can detect the dependence between variables in bothadditive and non-additive models.
arxiv-4800-37 | Binary Classifier Calibration: Bayesian Non-Parametric Approach | http://arxiv.org/pdf/1401.2955v1.pdf | author:Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht category:stat.ML cs.LG published:2014-01-13 summary:A set of probabilistic predictions is well calibrated if the events that arepredicted to occur with probability p do in fact occur about p fraction of thetime. Well calibrated predictions are particularly important when machinelearning models are used in decision analysis. This paper presents two newnon-parametric methods for calibrating outputs of binary classification models:a method based on the Bayes optimal selection and a method based on theBayesian model averaging. The advantage of these methods is that they areindependent of the algorithm used to learn a predictive model, and they can beapplied in a post-processing step, after the model is learned. This makes themapplicable to a wide variety of machine learning models and methods. Thesecalibration methods, as well as other methods, are tested on a variety ofdatasets in terms of both discrimination and calibration performance. Theresults show the methods either outperform or are comparable in performance tothe state-of-the-art calibration methods.
arxiv-4800-38 | ONTS: "Optima" News Translation System | http://arxiv.org/pdf/1401.2943v1.pdf | author:Marco Turchi, Martin Atkinson, Alastair Wilcox, Brett Crawley, Stefano Bucci, Ralf Steinberger, Erik Van der Goot category:cs.CL published:2014-01-13 summary:We propose a real-time machine translation system that allows users to selecta news category and to translate the related live news articles from Arabic,Czech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish andTurkish into English. The Moses-based system was optimised for the news domainand differs from other available systems in four ways: (1) News items areautomatically categorised on the source side, before translation; (2) Namedentity translation is optimised by recognising and extracting them on thesource side and by re-inserting their translation in the target language,making use of a separate entity repository; (3) News titles are translated witha separate translation system which is optimised for the specific style of newstitles; (4) The system was optimised for speed in order to cope with the largevolume of daily news articles.
arxiv-4800-39 | A survey of methods to ease the development of highly multilingual text mining applications | http://arxiv.org/pdf/1401.2937v1.pdf | author:Ralf Steinberger category:cs.CL published:2014-01-13 summary:Multilingual text processing is useful because the information content foundin different languages is complementary, both regarding facts and opinions.While Information Extraction and other text mining software can, in principle,be developed for many languages, most text analysis tools have only beenapplied to small sets of languages because the development effort per languageis large. Self-training tools obviously alleviate the problem, but even theeffort of providing training data and of manually tuning the results is usuallyconsiderable. In this paper, we gather insights by various multilingual systemdevelopers on how to minimise the effort of developing natural languageprocessing applications for many languages. We also explain the main guidelinesunderlying our own effort to develop complex text mining software for tens oflanguages. While these guidelines - most of all: extreme simplicity - can bevery restrictive and limiting, we believe to have shown the feasibility of theapproach through the development of the Europe Media Monitor (EMM) family ofapplications (http://emm.newsbrief.eu/overview.html). EMM is a set of complexmedia monitoring tools that process and analyse up to 100,000 online newsarticles per day in between twenty and fifty languages. We will also touch uponthe kind of language resources that would make it easier for all to develophighly multilingual text mining applications. We will argue that - to achievethis - the most needed resources would be freely available, simple, paralleland uniform multilingual dictionaries, corpora and software tools.
arxiv-4800-40 | A Solution of Degree Constrained Spanning Tree Using Hybrid GA | http://arxiv.org/pdf/1401.1753v2.pdf | author:Sounak Sadhukhan, Samar Sen Sarma category:cs.NE cs.DS published:2014-01-08 summary:In real life, it is always an urge to reach our goal in minimum effort i.e.,it should have a minimum constrained path. The path may be shortest route inpractical life, either physical or electronic medium. The scenario is torepresents the ambiance as a graph and to find a spanning tree with customdesign criteria. Here, we have chosen a minimum degree spanning tree, which canbe generated in real time with minimum turnaround time. The problem isNP-complete in nature [1, 2]. The solution approach, in general, isapproximate. We have used a heuristic approach, namely hybrid genetic algorithm(GA), with motivated criteria of encoded data structures of graph. We comparethe experimental result with the existing approximate algorithm and the resultis so encouraging that we are interested to use it in our future applications.
arxiv-4800-41 | Efficient Monte Carlo and greedy heuristic for the inference of stochastic block models | http://arxiv.org/pdf/1310.4378v3.pdf | author:Tiago P. Peixoto category:cs.SI stat.ML published:2013-10-16 summary:We present an efficient algorithm for the inference of stochastic blockmodels in large networks. The algorithm can be used as an optimized Markovchain Monte Carlo (MCMC) method, with a fast mixing time and a much reducedsusceptibility to getting trapped in metastable states, or as a greedyagglomerative heuristic, with an almost linear $O(N\ln^2N)$ complexity, where$N$ is the number of nodes in the network, independent on the number of blocksbeing inferred. We show that the heuristic is capable of delivering resultswhich are indistinguishable from the more exact and numerically expensive MCMCmethod in many artificial and empirical networks, despite being much faster.The method is entirely unbiased towards any specific mixing pattern, and inparticular it does not favor assortative community structures.
arxiv-4800-42 | Tensor Representation and Manifold Learning Methods for Remote Sensing Images | http://arxiv.org/pdf/1401.2871v1.pdf | author:Lefei Zhang category:cs.CV 68 published:2014-01-13 summary:One of the main purposes of earth observation is to extract interestedinformation and knowledge from remote sensing (RS) images with high efficiencyand accuracy. However, with the development of RS technologies, RS systemprovide images with higher spatial and temporal resolution and more spectralchannels than before, and it is inefficient and almost impossible to manuallyinterpret these images. Thus, it is of great interests to explore automatic andintelligent algorithms to quickly process such massive RS data with highaccuracy. This thesis targets to develop some efficient information extractionalgorithms for RS images, by relying on the advanced technologies in machinelearning. More precisely, we adopt the manifold learning algorithms as themainline and unify the regularization theory, tensor-based method, sparselearning and transfer learning into the same framework. The main contributionsof this thesis are as follows.
arxiv-4800-43 | lp-Recovery of the Most Significant Subspace among Multiple Subspaces with Outliers | http://arxiv.org/pdf/1012.4116v4.pdf | author:Gilad Lerman, Teng Zhang category:stat.ML cs.CV math.FA published:2010-12-18 summary:We assume data sampled from a mixture of d-dimensional linear subspaces withspherically symmetric distributions within each subspace and an additionaloutlier component with spherically symmetric distribution within the ambientspace (for simplicity we may assume that all distributions are uniform on theircorresponding unit spheres). We also assume mixture weights for the differentcomponents. We say that one of the underlying subspaces of the model is mostsignificant if its mixture weight is higher than the sum of the mixture weightsof all other subspaces. We study the recovery of the most significant subspaceby minimizing the lp-averaged distances of data points from d-dimensionalsubspaces, where p>0. Unlike other lp minimization problems, this minimizationis non-convex for all p>0 and thus requires different methods for its analysis.We show that if 0<p<=1, then for any fraction of outliers the most significantsubspace can be recovered by lp minimization with overwhelming probability(which depends on the generating distribution and its parameters). We show thatwhen adding small noise around the underlying subspaces the most significantsubspace can be nearly recovered by lp minimization for any 0<p<=1 with anerror proportional to the noise level. On the other hand, if p>1 and there ismore than one underlying subspace, then with overwhelming probability the mostsignificant subspace cannot be recovered or nearly recovered. This last resultdoes not require spherically symmetric outliers.
arxiv-4800-44 | GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation | http://arxiv.org/pdf/1401.2838v1.pdf | author:Edward Meeds, Max Welling category:cs.LG q-bio.QM stat.ML published:2014-01-13 summary:Scientists often express their understanding of the world through acomputationally demanding simulation program. Analyzing the posteriordistribution of the parameters given observations (the inverse problem) can beextremely challenging. The Approximate Bayesian Computation (ABC) framework isthe standard statistical tool to handle these likelihood free problems, butthey require a very large number of simulations. In this work we develop twonew ABC sampling algorithms that significantly reduce the number of simulationsnecessary for posterior inference. Both algorithms use confidence estimates forthe accept probability in the Metropolis Hastings step to adaptively choose thenumber of necessary simulations. Our GPS-ABC algorithm stores the informationobtained from every simulation in a Gaussian process which acts as a surrogatefunction for the simulated statistics. Experiments on a challenging realisticbiological problem illustrate the potential of these algorithms.
arxiv-4800-45 | Insights into analysis operator learning: From patch-based sparse models to higher-order MRFs | http://arxiv.org/pdf/1401.2804v1.pdf | author:Yunjin Chen, René Ranftl, Thomas Pock category:cs.CV published:2014-01-13 summary:This paper addresses a new learning algorithm for the recently introducedco-sparse analysis model. First, we give new insights into the co-sparseanalysis model by establishing connections to filter-based MRF models, such asthe Field of Experts (FoE) model of Roth and Black. For training, we introducea technique called bi-level optimization to learn the analysis operators.Compared to existing analysis operator learning approaches, our trainingprocedure has the advantage that it is unconstrained with respect to theanalysis operator. We investigate the effect of different aspects of theco-sparse analysis model and show that the sparsity promoting function (alsocalled penalty function) is the most important factor in the model. In order todemonstrate the effectiveness of our training approach, we apply our trainedmodels to various classical image restoration problems. Numerical experimentsshow that our trained models clearly outperform existing analysis operatorlearning approaches and are on par with state-of-the-art image denoisingalgorithms. Our approach develops a framework that is intuitive to understandand easy to implement.
arxiv-4800-46 | A Low-Dimensional Representation for Robust Partial Isometric Correspondences Computation | http://arxiv.org/pdf/1308.6804v2.pdf | author:Alan Brunton, Michael Wand, Stefanie Wuhrer, Hans-Peter Seidel, Tino Weinkauf category:cs.CV cs.GR published:2013-08-30 summary:Intrinsic isometric shape matching has become the standard approach for poseinvariant correspondence estimation among deformable shapes. Most existingapproaches assume global consistency, i.e., the metric structure of the wholemanifold must not change significantly. While global isometric matching is wellunderstood, only a few heuristic solutions are known for partial matching.Partial matching is particularly important for robustness to topological noise(incomplete data and contacts), which is a common problem in real-world 3Dscanner data. In this paper, we introduce a new approach to partial, intrinsicisometric matching. Our method is based on the observation that isometries arefully determined by purely local information: a map of a single point and itstangent space fixes an isometry for both global and the partial maps. From thisidea, we develop a new representation for partial isometric maps based onequivalence classes of correspondences between pairs of points and theirtangent spaces. From this, we derive a local propagation algorithm that findsuch mappings efficiently. In contrast to previous heuristics based on RANSACor expectation maximization, our method is based on a simple and soundtheoretical model and fully deterministic. We apply our approach to registerpartial point clouds and compare it to the state-of-the-art methods, where weobtain significant improvements over global methods for real-world data andstronger guarantees than previous heuristic partial matching algorithms.
arxiv-4800-47 | A variational Bayes framework for sparse adaptive estimation | http://arxiv.org/pdf/1401.2771v1.pdf | author:Konstantinos E. Themelis, Athanasios A. Rontogiannis, Konstantinos D. Koutroumbas category:stat.ML published:2014-01-13 summary:Recently, a number of mostly $\ell_1$-norm regularized least squares typedeterministic algorithms have been proposed to address the problem of\emph{sparse} adaptive signal estimation and system identification. From aBayesian perspective, this task is equivalent to maximum a posterioriprobability estimation under a sparsity promoting heavy-tailed prior for theparameters of interest. Following a different approach, this paper develops aunifying framework of sparse \emph{variational Bayes} algorithms that employheavy-tailed priors in conjugate hierarchical form to facilitate posteriorinference. The resulting fully automated variational schemes are firstpresented in a batch iterative form. Then it is shown that by properlyexploiting the structure of the batch estimation task, new sparse adaptivevariational Bayes algorithms can be derived, which have the ability to imposeand track sparsity during real-time processing in a time-varying environment.The most important feature of the proposed algorithms is that they completelyeliminate the need for computationally costly parameter fine-tuning, anecessary ingredient of sparse adaptive deterministic algorithms. Extensivesimulation results are provided to demonstrate the effectiveness of the newsparse variational Bayes algorithms against state-of-the-art deterministictechniques for adaptive channel estimation. The results show that the proposedalgorithms are numerically robust and exhibit in general superior estimationperformance compared to their deterministic counterparts.
arxiv-4800-48 | PSMACA: An Automated Protein Structure Prediction Using MACA (Multiple Attractor Cellular Automata) | http://arxiv.org/pdf/1401.2688v1.pdf | author:Pokkuluri Kiran Sree, Inamupudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG published:2014-01-13 summary:Protein Structure Predication from sequences of amino acid has gained aremarkable attention in recent years. Even though there are some predictiontechniques addressing this problem, the approximate accuracy in predicting theprotein structure is closely 75%. An automated procedure was evolved with MACA(Multiple Attractor Cellular Automata) for predicting the structure of theprotein. Most of the existing approaches are sequential which will classify theinput into four major classes and these are designed for similar sequences.PSMACA is designed to identify ten classes from the sequences that sharetwilight zone similarity and identity with the training sequences. This methodalso predicts three states (helix, strand, and coil) for the structure. Ourcomprehensive design considers 10 feature selection methods and 4 classifiersto develop MACA (Multiple Attractor Cellular Automata) based classifiers thatare build for each of the ten classes. We have tested the proposed classifierwith twilight-zone and 1-high-similarity benchmark datasets with over threedozens of modern competing predictors shows that PSMACA provides the bestoverall accuracy that ranges between 77% and 88.7% depending on the dataset.
arxiv-4800-49 | A parameterless scale-space approach to find meaningful modes in histograms - Application to image and spectrum segmentation | http://arxiv.org/pdf/1401.2686v1.pdf | author:Jérôme Gilles, Kathryn Heal category:cs.CV published:2014-01-13 summary:In this paper, we present an algorithm to automatically detect meaningfulmodes in a histogram. The proposed method is based on the behavior of localminima in a scale-space representation. We show that the detection of suchmeaningful modes is equivalent in a two classes clustering problem on thelength of minima scale-space curves. The algorithm is easy to implement, fast,and does not require any parameters. We present several results on histogramand spectrum segmentation, grayscale image segmentation and color imagereduction.
arxiv-4800-50 | Dictionary-Based Concept Mining: An Application for Turkish | http://arxiv.org/pdf/1401.2663v1.pdf | author:Cem Rıfkı Aydın, Ali Erkan, Tunga Güngör, Hidayet Takçı category:cs.CL I.2.7 published:2014-01-12 summary:In this study, a dictionary-based method is used to extract expressiveconcepts from documents. So far, there have been many studies concerningconcept mining in English, but this area of study for Turkish, an agglutinativelanguage, is still immature. We used dictionary instead of WordNet, a lexicaldatabase grouping words into synsets that is widely used for conceptextraction. The dictionaries are rarely used in the domain of concept mining,but taking into account that dictionary entries have synonyms, hypernyms,hyponyms and other relationships in their meaning texts, the success rate hasbeen high for determining concepts. This concept extraction method isimplemented on documents, that are collected from different corpora.
arxiv-4800-51 | An Overview of Schema Theory | http://arxiv.org/pdf/1401.2651v1.pdf | author:David White category:cs.NE published:2014-01-12 summary:The purpose of this paper is to give an introduction to the field of SchemaTheory written by a mathematician and for mathematicians. In particular, weendeavor to to highlight areas of the field which might be of interest to amathematician, to point out some related open problems, and to suggest somelarge-scale projects. Schema theory seeks to give a theoretical justificationfor the efficacy of the field of genetic algorithms, so readers who havestudied genetic algorithms stand to gain the most from this paper. However,nothing beyond basic probability theory is assumed of the reader, and for thisreason we write in a fairly informal style. Because the mathematics behind the theorems in schema theory is relativelyelementary, we focus more on the motivation and philosophy. Many of theseresults have been proven elsewhere, so this paper is designed to serve aprimarily expository role. We attempt to cast known results in a new light,which makes the suggested future directions natural. This involves devoting asubstantial amount of time to the history of the field. We hope that this exposition will entice some mathematicians to do researchin this area, that it will serve as a road map for researchers new to thefield, and that it will help explain how schema theory developed. Furthermore,we hope that the results collected in this document will serve as a usefulreference. Finally, as far as the author knows, the questions raised in thefinal section are new.
arxiv-4800-52 | Graphical Modelling in Genetics and Systems Biology | http://arxiv.org/pdf/1210.3831v2.pdf | author:Marco Scutari category:stat.ME q-bio.MN stat.ML published:2012-10-14 summary:Graphical modelling has a long history in statistics as a tool for theanalysis of multivariate data, starting from Wright's path analysis and Gibbs'applications to statistical physics at the beginning of the last century. Inits modern form, it was pioneered by Lauritzen and Wermuth and Pearl in the1980s, and has since found applications in fields as diverse as bioinformatics,customer satisfaction surveys and weather forecasts. Genetics and systems biology are unique among these fields in the dimensionof the data sets they study, which often contain several hundreds of variablesand only a few tens or hundreds of observations. This raises problems in bothcomputational complexity and the statistical significance of the resultingnetworks, collectively known as the "curse of dimensionality". Furthermore, thedata themselves are difficult to model correctly due to the limitedunderstanding of the underlying mechanisms. In the following, we willillustrate how such challenges affect practical graphical modelling and somepossible solutions.
arxiv-4800-53 | Towards a Generic Framework for the Development of Unicode Based Digital Sindhi Dictionaries | http://arxiv.org/pdf/1401.2641v1.pdf | author:Imdad Ali Ismaili, Zeeshan Bhatti, Azhar Ali Shah category:cs.CL published:2014-01-12 summary:Dictionaries are essence of any language providing vital linguistic recoursefor the language learners, researchers and scholars. This paper focuses on themethodology and techniques used in developing software architecture for aUBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). Theproposed system provides an accurate solution for construction andrepresentation of Unicode based Sindhi characters in a dictionary implementingHash Structure algorithm and a custom java Object as its internal datastructure saved in a file. The System provides facilities for Insertion,Deletion and Editing of new records of Sindhi. Through this framework any typeof Sindhi to English and English to Sindhi Dictionary (belonging to differentdomains of knowledge, e.g. engineering, medicine, computer, biology etc.) couldbe developed easily with accurate representation of Unicode Characters in fontindependent manner.
arxiv-4800-54 | Sentiment Analysis Using Collaborated Opinion Mining | http://arxiv.org/pdf/1401.2618v1.pdf | author:Deepali Virmani, Vikrant Malhotra, Ridhi Tyagi category:cs.IR cs.CL published:2014-01-12 summary:Opinion mining and Sentiment analysis have emerged as a field of study sincethe widespread of World Wide Web and internet. Opinion refers to extraction ofthose lines or phrase in the raw and huge data which express an opinion.Sentiment analysis on the other hand identifies the polarity of the opinionbeing extracted. In this paper we propose the sentiment analysis incollaboration with opinion extraction, summarization, and tracking the recordsof the students. The paper modifies the existing algorithm in order to obtainthe collaborated opinion about the students. The resultant opinion isrepresented as very high, high, moderate, low and very low. The paper is basedon a case study where teachers give their remarks about the students and byapplying the proposed sentiment analysis algorithm the opinion is extracted andrepresented.
arxiv-4800-55 | Multi Terminal Probabilistic Compressed Sensing | http://arxiv.org/pdf/1401.2569v1.pdf | author:Saeid Haghighatshoar category:cs.IT math.IT stat.ML published:2014-01-11 summary:In this paper, the `Approximate Message Passing' (AMP) algorithm, initiallydeveloped for compressed sensing of signals under i.i.d. Gaussian measurementmatrices, has been extended to a multi-terminal setting (MAMP algorithm). Ithas been shown that similar to its single terminal counterpart, the behavior ofMAMP algorithm is fully characterized by a `State Evolution' (SE) equation forlarge block-lengths. This equation has been used to obtain the rate-distortioncurve of a multi-terminal memoryless source. It is observed that by spatiallycoupling the measurement matrices, the rate-distortion curve of MAMP algorithmundergoes a phase transition, where the measurement rate region correspondingto a low distortion (approximately zero distortion) regime is fullycharacterized by the joint and conditional Renyi information dimension (RID) ofthe multi-terminal source. This measurement rate region is very similar to therate region of the Slepian-Wolf distributed source coding problem where the RIDplays a role similar to the discrete entropy. Simulations have been done to investigate the empirical behavior of MAMPalgorithm. It is observed that simulation results match very well withpredictions of SE equation for reasonably large block-lengths.
arxiv-4800-56 | A Deep and Tractable Density Estimator | http://arxiv.org/pdf/1310.1757v2.pdf | author:Benigno Uria, Iain Murray, Hugo Larochelle category:stat.ML cs.LG published:2013-10-07 summary:The Neural Autoregressive Distribution Estimator (NADE) and its real-valuedversion RNADE are competitive density models of multidimensional data across avariety of domains. These models use a fixed, arbitrary ordering of the datadimensions. One can easily condition on variables at the beginning of theordering, and marginalize out variables at the end of the ordering, howeverother inference tasks require approximate inference. In this work we introducean efficient procedure to simultaneously train a NADE model for each possibleordering of the variables, by sharing parameters across all these models. Wecan thus use the most convenient model for each inference task at hand, andensembles of such models with different orderings are immediately available.Moreover, unlike the original NADE, our training procedure scales to deepmodels. Empirically, ensembles of Deep NADE models obtain state of the artdensity estimation performance.
arxiv-4800-57 | The semantic similarity ensemble | http://arxiv.org/pdf/1401.2517v1.pdf | author:Andrea Ballatore, Michela Bertolotto, David C. Wilson category:cs.CL published:2014-01-11 summary:Computational measures of semantic similarity between geographic termsprovide valuable support across geographic information retrieval, data mining,and information integration. To date, a wide variety of approaches togeo-semantic similarity have been devised. A judgment of similarity is notintrinsically right or wrong, but obtains a certain degree of cognitiveplausibility, depending on how closely it mimics human behavior. Thus selectingthe most appropriate measure for a specific task is a significant challenge. Toaddress this issue, we make an analogy between computational similaritymeasures and soliciting domain expert opinions, which incorporate a subjectiveset of beliefs, perceptions, hypotheses, and epistemic biases. Following thisanalogy, we define the semantic similarity ensemble (SSE) as a composition ofdifferent similarity measures, acting as a panel of experts having to reach adecision on the semantic similarity of a set of geographic terms. The approachis evaluated in comparison to human judgments, and results indicate that an SSEperforms better than the average of its parts. Although the best member tendsto outperform the ensemble, all ensembles outperform the average performance ofeach ensemble's member. Hence, in contexts where the best measure is unknown,the ensemble provides a more cognitively plausible approach.
arxiv-4800-58 | Multi-Step-Ahead Time Series Prediction using Multiple-Output Support Vector Regression | http://arxiv.org/pdf/1401.2504v1.pdf | author:Yukun Bao, Tao Xiong, Zhongyi Hu category:cs.LG stat.ML published:2014-01-11 summary:Accurate time series prediction over long future horizons is challenging andof great interest to both practitioners and academics. As a well-knownintelligent algorithm, the standard formulation of Support Vector Regression(SVR) could be taken for multi-step-ahead time series prediction, only relyingeither on iterated strategy or direct strategy. This study proposes a novelmultiple-step-ahead time series prediction approach which employsmultiple-output support vector regression (M-SVR) with multiple-inputmultiple-output (MIMO) prediction strategy. In addition, the rank of threeleading prediction strategies with SVR is comparatively examined, providingpractical implications on the selection of the prediction strategy formulti-step-ahead forecasting while taking SVR as modeling technique. Theproposed approach is validated with the simulated and real datasets. Thequantitative and comprehensive assessments are performed on the basis of theprediction accuracy and computational cost. The results indicate that: 1) theM-SVR using MIMO strategy achieves the best accurate forecasts with accreditedcomputational load, 2) the standard SVR using direct strategy achieves thesecond best accurate forecasts, but with the most expensive computational cost,and 3) the standard SVR using iterated strategy is the worst in terms ofprediction accuracy, but with the least computational cost.
arxiv-4800-59 | Multiscale Shrinkage and Lévy Processes | http://arxiv.org/pdf/1401.2497v1.pdf | author:Xin Yuan, Vinayak Rao, Shaobo Han, Lawrence Carin category:stat.ML published:2014-01-11 summary:A new shrinkage-based construction is developed for a compressible vector$\boldsymbol{x}\in\mathbb{R}^n$, for cases in which the components of $\xv$ arenaturally associated with a tree structure. Important examples are when $\xv$corresponds to the coefficients of a wavelet or block-DCT representation ofdata. The method we consider in detail, and for which numerical results arepresented, is based on increments of a gamma process. However, we demonstratethat the general framework is appropriate for many other types of shrinkagepriors, all within the L\'{e}vy process family, with the gamma process aspecial case. Bayesian inference is carried out by approximating the posteriorwith samples from an MCMC algorithm, as well as by constructing a heuristicvariational approximation to the posterior. We also considerexpectation-maximization (EM) for a MAP (point) solution. State-of-the-artresults are manifested for compressive sensing and denoising applications, thelatter with spiky (non-Gaussian) noise.
arxiv-4800-60 | An Online Expectation-Maximisation Algorithm for Nonnegative Matrix Factorisation Models | http://arxiv.org/pdf/1401.2490v1.pdf | author:Sinan Yildirim, A. Taylan Cemgil, Sumeetpal S. Singh category:cs.LG stat.CO stat.ML published:2014-01-11 summary:In this paper we formulate the nonnegative matrix factorisation (NMF) problemas a maximum likelihood estimation problem for hidden Markov models and proposeonline expectation-maximisation (EM) algorithms to estimate the NMF and theother unknown static parameters. We also propose a sequential Monte Carloapproximation of our online EM algorithm. We show the performance of theproposed method with two numerical examples.
arxiv-4800-61 | Autocalibration with the Minimum Number of Cameras with Known Pixel Shape | http://arxiv.org/pdf/1203.0905v2.pdf | author:José I. Ronda, Antonio Valdés, Guillermo Gallego category:cs.CV published:2012-03-05 summary:In 3D reconstruction, the recovery of the calibration parameters of thecameras is paramount since it provides metric information about the observedscene, e.g., measures of angles and ratios of distances. Autocalibrationenables the estimation of the camera parameters without using a calibrationdevice, but by enforcing simple constraints on the camera parameters. In theabsence of information about the internal camera parameters such as the focallength and the principal point, the knowledge of the camera pixel shape isusually the only available constraint. Given a projective reconstruction of arigid scene, we address the problem of the autocalibration of a minimal set ofcameras with known pixel shape and otherwise arbitrarily varying intrinsic andextrinsic parameters. We propose an algorithm that only requires 5 cameras (thetheoretical minimum), thus halving the number of cameras required by previousalgorithms based on the same constraint. To this purpose, we introduce as ourbasic geometric tool the six-line conic variety (SLCV), consisting in the setof planes intersecting six given lines of 3D space in points of a conic. Weshow that the set of solutions of the Euclidean upgrading problem for threecameras with known pixel shape can be parameterized in a computationallyefficient way. This parameterization is then used to solve autocalibration fromfive or more cameras, reducing the three-dimensional search space to atwo-dimensional one. We provide experiments with real images showing the goodperformance of the technique.
arxiv-4800-62 | Learning Paired-associate Images with An Unsupervised Deep Learning Architecture | http://arxiv.org/pdf/1312.6171v2.pdf | author:Ti Wang, Daniel L. Silver category:cs.NE cs.CV cs.LG published:2013-12-20 summary:This paper presents an unsupervised multi-modal learning system that learnsassociative representation from two input modalities, or channels, such thatinput on one channel will correctly generate the associated response at theother and vice versa. In this way, the system develops a kind of supervisedclassification model meant to simulate aspects of human associative memory. Thesystem uses a deep learning architecture (DLA) composed of two input/outputchannels formed from stacked Restricted Boltzmann Machines (RBM) and anassociative memory network that combines the two channels. The DLA is trainedon pairs of MNIST handwritten digit images to develop hierarchical features andassociative representations that are able to reconstruct one image given itspaired-associate. Experiments show that the multi-modal learning systemgenerates models that are as accurate as back-propagation networks but with theadvantage of a bi-directional network and unsupervised learning from eitherpaired or non-paired training examples.
arxiv-4800-63 | N2Sky - Neural Networks as Services in the Clouds | http://arxiv.org/pdf/1401.2468v1.pdf | author:Erich Schikuta, Erwin Mann category:cs.NE H.3.5; I.2 published:2014-01-10 summary:We present the N2Sky system, which provides a framework for the exchange ofneural network specific knowledge, as neural network paradigms and objects, bya virtual organization environment. It follows the sky computing paradigmdelivering ample resources by the usage of federated Clouds. N2Sky is a novelCloud-based neural network simulation environment, which follows a pure serviceoriented approach. The system implements a transparent environment aiming toenable both novice and experienced users to do neural network research easilyand comfortably. N2Sky is built using the RAVO reference architecture ofvirtual organizations which allows itself naturally integrating into the Cloudservice stack (SaaS, PaaS, and IaaS) of service oriented architectures.
arxiv-4800-64 | Online Matrix Completion Through Nuclear Norm Regularisation | http://arxiv.org/pdf/1401.2451v1.pdf | author:Charanpal Dhanjal, Romaric Gaudel, Stéphan Clémençon category:stat.ML published:2014-01-10 summary:It is the main goal of this paper to propose a novel method to perform matrixcompletion on-line. Motivated by a wide variety of applications, ranging fromthe design of recommender systems to sensor network localization throughseismic data reconstruction, we consider the matrix completion problem whenentries of the matrix of interest are observed gradually. Precisely, we placeourselves in the situation where the predictive rule should be refinedincrementally, rather than recomputed from scratch each time the sample ofobserved entries increases. The extension of existing matrix completion methodsto the sequential prediction context is indeed a major issue in the Big Dataera, and yet little addressed in the literature. The algorithm promoted in thisarticle builds upon the Soft Impute approach introduced in Mazumder et al.(2010). The major novelty essentially arises from the use of a randomisedtechnique for both computing and updating the Singular Value Decomposition(SVD) involved in the algorithm. Though of disarming simplicity, the methodproposed turns out to be very efficient, while requiring reduced computations.Several numerical experiments based on real datasets illustrating itsperformance are displayed, together with preliminary results giving it atheoretical basis.
arxiv-4800-65 | Satellite image classification and segmentation using non-additive entropy | http://arxiv.org/pdf/1401.2416v1.pdf | author:Lucas Assirati, Alexandre Souto Martinez, Odemir Martinez Bruno category:cs.CV published:2014-01-10 summary:Here we compare the Boltzmann-Gibbs-Shannon (standard) with the Tsallisentropy on the pattern recognition and segmentation of coloured images obtainedby satellites, via "Google Earth". By segmentation we mean split an image tolocate regions of interest. Here, we discriminate and define an image partitionclasses according to a training basis. This training basis consists of threepattern classes: aquatic, urban and vegetation regions. Our numericalexperiments demonstrate that the Tsallis entropy, used as a feature vectorcomposed of distinct entropic indexes $q$ outperforms the standard entropy.There are several applications of our proposed methodology, once satelliteimages can be used to monitor migration form rural to urban regions,agricultural activities, oil spreading on the ocean etc.
arxiv-4800-66 | Clustering, Coding, and the Concept of Similarity | http://arxiv.org/pdf/1401.2411v1.pdf | author:L. Thorne McCarty category:cs.LG published:2014-01-10 summary:This paper develops a theory of clustering and coding which combines ageometric model with a probabilistic model in a principled way. The geometricmodel is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\bf x})$,which we interpret as a measure of dissimilarity. The probabilistic modelconsists of a stochastic process with an invariant probability measure whichmatches the density of the sample input data. The link between the two modelsis a potential function, $U({\bf x})$, and its gradient, $\nabla U({\bf x})$.We use the gradient to define the dissimilarity metric, which guarantees thatour measure of dissimilarity will depend on the probability measure. Finally,we use the dissimilarity metric to define a coordinate system on the embeddedRiemannian manifold, which gives us a low-dimensional encoding of our originaldata.
arxiv-4800-67 | Exploiting generalisation symmetries in accuracy-based learning classifier systems: An initial study | http://arxiv.org/pdf/1401.2949v1.pdf | author:Larry Bull category:cs.NE cs.LG published:2014-01-10 summary:Modern learning classifier systems typically exploit a niched geneticalgorithm to facilitate rule discovery. When used for reinforcement learning,such rules represent generalisations over the state-action-reward space. Whilstencouraging maximal generality, the niching can potentially hinder theformation of generalisations in the state space which are symmetrical, or verysimilar, over different actions. This paper introduces the use of rules whichcontain multiple actions, maintaining accuracy and reward metrics for eachaction. It is shown that problem symmetries can be exploited, improvingperformance, whilst not degrading performance when symmetries are reduced.
arxiv-4800-68 | Lasso and equivalent quadratic penalized models | http://arxiv.org/pdf/1401.2304v1.pdf | author:Stefan Hummelsheim category:stat.ML cs.LG published:2014-01-10 summary:The least absolute shrinkage and selection operator (lasso) and ridgeregression produce usually different estimates although input, loss functionand parameterization of the penalty are identical. In this paper we look forridge and lasso models with identical solution set. It turns out, that the lasso model with shrink vector $\lambda$ and aquadratic penalized model with shrink matrix as outer product of $\lambda$ withitself are equivalent, in the sense that they have equal solutions. To achievethis, we have to restrict the estimates to be positive. This doesn't limit thearea of application since we can easily decompose every estimate in a positiveand negative part. The resulting problem can be solved with a non negativeleast square algorithm. Beside this quadratic penalized model, an augmented regression model withpositive bounded estimates is developed which is also equivalent to the lassomodel, but is probably faster to solve.
arxiv-4800-69 | Assessing Wikipedia-Based Cross-Language Retrieval Models | http://arxiv.org/pdf/1401.2258v1.pdf | author:Benjamin Roth category:cs.IR cs.CL published:2014-01-10 summary:This work compares concept models for cross-language retrieval: First, weadapt probabilistic Latent Semantic Analysis (pLSA) for multilingual documents.Experiments with different weighting schemes show that a weighting methodfavoring documents of similar length in both language sides gives best results.Considering that both monolingual and multilingual Latent Dirichlet Allocation(LDA) behave alike when applied for such documents, we use a training corpusbuilt on Wikipedia where all documents are length-normalized and obtainimprovements over previously reported scores for LDA. Another focus of our workis on model combination. For this end we include Explicit Semantic Analysis(ESA) in the experiments. We observe that ESA is not competitive with LDA in aquery based retrieval task on CLEF 2000 data. The combination of machinetranslation with concept models increased performance by 21.1% map incomparison to machine translation alone. Machine translation relies on parallelcorpora, which may not be available for many language pairs. We further explorehow much cross-lingual information can be carried over by a specificinformation source in Wikipedia, namely linked text. The best results areobtained using a language modeling approach, entirely without information fromparallel corpora. The need for smoothing raises interesting questions onsoundness and efficiency. Link models capture only a certain kind ofinformation and suggest weighting schemes to emphasize particular words. For acombined model, another interesting question is therefore how to integratedifferent weighting schemes. Using a very simple combination scheme, we obtainresults that compare favorably to previously reported results on the CLEF 2000dataset.
arxiv-4800-70 | A Comparative Study of Reservoir Computing for Temporal Signal Processing | http://arxiv.org/pdf/1401.2224v1.pdf | author:Alireza Goudarzi, Peter Banda, Matthew R. Lakin, Christof Teuscher, Darko Stefanovic category:cs.NE cs.LG published:2014-01-10 summary:Reservoir computing (RC) is a novel approach to time series prediction usingrecurrent neural networks. In RC, an input signal perturbs the intrinsicdynamics of a medium called a reservoir. A readout layer is then trained toreconstruct a target output from the reservoir's state. The multitude of RCarchitectures and evaluation metrics poses a challenge to both practitionersand theorists who study the task-solving performance and computational power ofRC. In addition, in contrast to traditional computation models, the reservoiris a dynamical system in which computation and memory are inseparable, andtherefore hard to analyze. Here, we compare echo state networks (ESN), apopular RC architecture, with tapped-delay lines (DL) and nonlinearautoregressive exogenous (NARX) networks, which we use to model systems withlimited computation and limited memory respectively. We compare the performanceof the three systems while computing three common benchmark time series:H{\'e}non Map, NARMA10, and NARMA20. We find that the role of the reservoir inthe reservoir computing paradigm goes beyond providing a memory of the pastinputs. The DL and the NARX network have higher memorization capability, butfall short of the generalization power of the ESN.
arxiv-4800-71 | Distinguishing noise from chaos: objective versus subjective criteria using Horizontal Visibility Graph | http://arxiv.org/pdf/1401.2139v1.pdf | author:Martín Gómez Ravetti, Laura C. Carpi, Bruna Amin Gonçalves, Alejandro C. Frery, Osvaldo A. Rosso category:stat.ML cs.IT math.IT nlin.CD published:2014-01-09 summary:A recently proposed methodology called the Horizontal Visibility Graph (HVG)[Luque {\it et al.}, Phys. Rev. E., 80, 046103 (2009)] that constitutes ageometrical simplification of the well known Visibility Graph algorithm [Lacasa{\it et al.\/}, Proc. Natl. Sci. U.S.A. 105, 4972 (2008)], has been used tostudy the distinction between deterministic and stochastic components in timeseries [L. Lacasa and R. Toral, Phys. Rev. E., 82, 036120 (2010)].Specifically, the authors propose that the node degree distribution of theseprocesses follows an exponential functional of the form $P(\kappa)\sim\exp(-\lambda~\kappa)$, in which $\kappa$ is the node degree and $\lambda$ is apositive parameter able to distinguish between deterministic (chaotic) andstochastic (uncorrelated and correlated) dynamics. In this work, we investigatethe characteristics of the node degree distributions constructed by using HVG,for time series corresponding to $28$ chaotic maps and $3$ different stochasticprocesses. We thoroughly study the methodology proposed by Lacasa and Toralfinding several cases for which their hypothesis is not valid. We propose amethodology that uses the HVG together with Information Theory quantifiers. Anextensive and careful analysis of the node degree distributions obtained byapplying HVG allow us to conclude that the Fisher-Shannon information plane isa remarkable tool able to graphically represent the different nature,deterministic or stochastic, of the systems under study.
arxiv-4800-72 | Statistical Analysis based Hypothesis Testing Method in Biological Knowledge Discovery | http://arxiv.org/pdf/1401.2851v1.pdf | author:Md. Naseef-Ur-Rahman Chowdhury, Suvankar Paul, Kazi Zakia Sultana category:cs.IR cs.CL published:2014-01-09 summary:The correlation and interactions among different biological entities comprisethe biological system. Although already revealed interactions contribute to theunderstanding of different existing systems, researchers face many questionseveryday regarding inter-relationships among entities. Their queries havepotential role in exploring new relations which may open up a new area ofinvestigation. In this paper, we introduce a text mining based method foranswering the biological queries in terms of statistical computation such thatresearchers can come up with new knowledge discovery. It facilitates user tosubmit their query in natural linguistic form which can be treated ashypothesis. Our proposed approach analyzes the hypothesis and measures thep-value of the hypothesis with respect to the existing literature. Based on themeasured value, the system either accepts or rejects the hypothesis fromstatistical point of view. Moreover, even it does not find any directrelationship among the entities of the hypothesis, it presents a network togive an integral overview of all the entities through which the entities mightbe related. This is also congenial for the researchers to widen their view andthus think of new hypothesis for further investigation. It assists researcherto get a quantitative evaluation of their assumptions such that they can reacha logical conclusion and thus aids in relevant re-searches of biologicalknowledge discovery. The system also provides the researchers a graphicalinteractive interface to submit their hypothesis for assessment in a moreconvenient way.
arxiv-4800-73 | Gesture recognition based mouse events | http://arxiv.org/pdf/1401.2058v1.pdf | author:Rachit Puri category:cs.CV published:2014-01-09 summary:This paper presents the maneuver of mouse pointer and performs various mouseoperations such as left click, right click, double click, drag etc usinggestures recognition technique. Recognizing gestures is a complex task whichinvolves many aspects such as motion modeling, motion analysis, patternrecognition and machine learning. Keeping all the essential factors in mind asystem has been created which recognizes the movement of fingers and variouspatterns formed by them. Color caps have been used for fingers to distinguishit from the background color such as skin color. Thus recognizing the gesturesvarious mouse events have been performed. The application has been created onMATLAB environment with operating system as windows 7.
arxiv-4800-74 | Enhancement performance of road recognition system of autonomous robots in shadow scenario | http://arxiv.org/pdf/1401.2051v1.pdf | author:Olusanya Y. Agunbiade, Tranos Zuva, Awosejo O. Johnson, Keneilwe Zuva category:cs.RO cs.CV published:2014-01-09 summary:Road region recognition is a main feature that is gaining increasingattention from intellectuals because it helps autonomous vehicle to achieve asuccessful navigation without accident. However, different techniques based oncamera sensor have been used by various researchers and outstanding resultshave been achieved. Despite their success, environmental noise like shadowleads to inaccurate recognition of road region which eventually leads toaccident for autonomous vehicle. In this research, we conducted aninvestigation on shadow and its effects, optimized the road region recognitionsystem of autonomous vehicle by introducing an algorithm capable of detectingand eliminating the effects of shadow. The experimental performance of oursystem was tested and compared using the following schemes: Total Positive Rate(TPR), False Negative Rate (FNR), Total Negative Rate (TNR), Error Rate (ERR)and False Positive Rate (FPR). The performance result of the system improved onroad recognition in shadow scenario and this advancement has added tremendouslyto successful navigation approaches for autonomous vehicle.
arxiv-4800-75 | Brazilian License Plate Detection Using Histogram of Oriented Gradients and Sliding Windows | http://arxiv.org/pdf/1401.1990v1.pdf | author:R. F. Prates, G. Cámara-Chávez, William R. Schwartz, D. Menotti category:cs.CV published:2014-01-09 summary:Due to the increasingly need for automatic traffic monitoring, vehiclelicense plate detection is of high interest to perform automatic tollcollection, traffic law enforcement, parking lot access control, among others.In this paper, a sliding window approach based on Histogram of OrientedGradients (HOG) features is used for Brazilian license plate detection. Thisapproach consists in scanning the whole image in a multiscale fashion such thatthe license plate is located precisely. The main contribution of this workconsists in a deep study of the best setup for HOG descriptors on the detectionof Brazilian license plates, in which HOG have never been applied before. Wealso demonstrate the reliability of this method ensured by a recall higher than98% (with a precision higher than 78%) in a publicly available data set.
arxiv-4800-76 | RNADE: The real-valued neural autoregressive density-estimator | http://arxiv.org/pdf/1306.0186v2.pdf | author:Benigno Uria, Iain Murray, Hugo Larochelle category:stat.ML cs.LG published:2013-06-02 summary:We introduce RNADE, a new model for joint density estimation of real-valuedvectors. Our model calculates the density of a datapoint as the product ofone-dimensional conditionals modeled using mixture density networks with sharedparameters. RNADE learns a distributed representation of the data, while havinga tractable expression for the calculation of densities. A tractable likelihoodallows direct comparison with other methods and training by standardgradient-based optimizers. We compare the performance of RNADE on severaldatasets of heterogeneous and perceptual data, finding it outperforms mixturemodels in all but one case.
arxiv-4800-77 | Hand-guided 3D surface acquisition by combining simple light sectioning with real-time algorithms | http://arxiv.org/pdf/1401.1946v1.pdf | author:Oliver Arold, Svenja Ettl, Florian Willomitzer, Gerd Häusler category:physics.optics cs.CV published:2014-01-09 summary:Precise 3D measurements of rigid surfaces are desired in many fields ofapplication like quality control or surgery. Often, views from all around theobject have to be acquired for a full 3D description of the object surface. Wepresent a sensor principle called "Flying Triangulation" which avoids anelaborate "stop-and-go" procedure. It combines a low-cost classicallight-section sensor with an algorithmic pipeline. A hand-guided sensorcaptures a continuous movie of 3D views while being moved around the object.The views are automatically aligned and the acquired 3D model is displayed inreal time. In contrast to most existing sensors no bandwidth is wasted forspatial or temporal encoding of the projected lines. Nor is an expensive colorcamera necessary for 3D acquisition. The achievable measurement uncertainty andlateral resolution of the generated 3D data is merely limited by physics. Analternating projection of vertical and horizontal lines guarantees theexistence of corresponding points in successive 3D views. This enables aprecise registration without surface interpolation. For registration, a variantof the iterative closest point algorithm - adapted to the specific nature ofour 3D views - is introduced. Furthermore, data reduction and smoothing withoutlosing lateral resolution as well as the acquisition and mapping of a colortexture is presented. The precision and applicability of the sensor isdemonstrated by simulation and measurement results.
arxiv-4800-78 | Radial basis function process neural network training based on generalized frechet distance and GA-SA hybrid strategy | http://arxiv.org/pdf/1405.7349v1.pdf | author:Bing Wang, Yao-hua Meng, Xiao-hong Yu category:cs.NE published:2014-01-09 summary:For learning problem of Radial Basis Function Process Neural Network(RBF-PNN), an optimization training method based on GA combined with SA isproposed in this paper. Through building generalized Fr\'echet distance tomeasure similarity between time-varying function samples, the learning problemof radial basis centre functions and connection weights is converted into thetraining on corresponding discrete sequence coefficients. Network trainingobjective function is constructed according to the least square errorcriterion, and global optimization solving of network parameters is implementedin feasible solution space by use of global optimization feature of GA andprobabilistic jumping property of SA . The experiment results illustrate thatthe training algorithm improves the network training efficiency and stability.
arxiv-4800-79 | A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters Optimization | http://arxiv.org/pdf/1401.1926v1.pdf | author:Yukun Bao, Zhongyi Hu, Tao Xiong category:cs.LG cs.AI cs.NE stat.ML published:2014-01-09 summary:Addressing the issue of SVMs parameters optimization, this study proposes anefficient memetic algorithm based on Particle Swarm Optimization algorithm(PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO isresponsible for exploration of the search space and the detection of thepotential regions with optimum solutions, while pattern search (PS) is used toproduce an effective exploitation on the potential regions obtained by PSO.Moreover, a novel probabilistic selection strategy is proposed to select theappropriate individuals among the current population to undergo localrefinement, keeping a well balance between exploration and exploitation.Experimental results confirm that the local refinement with PS and our proposedselection strategy are effective, and finally demonstrate effectiveness androbustness of the proposed PSO-PS based MA for SVMs parameters optimization.
arxiv-4800-80 | Multiple-output support vector regression with a firefly algorithm for interval-valued stock price index forecasting | http://arxiv.org/pdf/1401.1916v1.pdf | author:Tao Xiong, Yukun Bao, Zhongyi Hu category:cs.CE cs.LG q-fin.ST published:2014-01-09 summary:Highly accurate interval forecasting of a stock price index is fundamental tosuccessfully making a profit when making investment decisions, by providing arange of values rather than a point estimate. In this study, we investigate thepossibility of forecasting an interval-valued stock price index series overshort and long horizons using multi-output support vector regression (MSVR).Furthermore, this study proposes a firefly algorithm (FA)-based approach, builton the established MSVR, for determining the parameters of MSVR (abbreviated asFA-MSVR). Three globally traded broad market indices are used to compare theperformance of the proposed FA-MSVR method with selected counterparts. Thequantitative and comprehensive assessments are performed on the basis ofstatistical criteria, economic criteria, and computational cost. In terms ofstatistical criteria, we compare the out-of-sample forecasting usinggoodness-of-forecast measures and testing approaches. In terms of economiccriteria, we assess the relative forecast performance with a simple tradingstrategy. The results obtained in this study indicate that the proposed FA-MSVRmethod is a promising alternative for forecasting interval-valued financialtime series.
arxiv-4800-81 | A Parameterized Complexity Analysis of Bi-level Optimisation with Evolutionary Algorithms | http://arxiv.org/pdf/1401.1905v1.pdf | author:Dogan Corus, Per Kristian Lehre, Frank Neumann, Mojgan Pourhassan category:cs.NE published:2014-01-09 summary:Bi-level optimisation problems have gained increasing interest in the fieldof combinatorial optimisation in recent years. With this paper, we start theruntime analysis of evolutionary algorithms for bi-level optimisation problems.We examine two NP-hard problems, the generalised minimum spanning tree problem(GMST), and the generalised travelling salesman problem (GTSP) in the contextof parameterised complexity. For the generalised minimum spanning tree problem, we analyse the twoapproaches presented by Hu and Raidl (2012) with respect to the number ofclusters that distinguish each other by the chosen representation of possiblesolutions. Our results show that a (1+1) EA working with the spanning nodesrepresentation is not a fixed-parameter evolutionary algorithm for the problem,whereas the global structure representation enables to solve the problem infixed-parameter time. We present hard instances for each approach and show thatthe two approaches are highly complementary by proving that they solve eachother's hard instances very efficiently. For the generalised travelling salesman problem, we analyse the problem withrespect to the number of clusters in the problem instance. Our results showthat a (1+1) EA working with the global structure representation is afixed-parameter evolutionary algorithm for the problem.
arxiv-4800-82 | Efficient unimodality test in clustering by signature testing | http://arxiv.org/pdf/1401.1895v1.pdf | author:Mahdi Shahbaba, Soosan Beheshti category:cs.LG stat.ML published:2014-01-09 summary:This paper provides a new unimodality test with application in hierarchicalclustering methods. The proposed method denoted by signature test (Sigtest),transforms the data based on its statistics. The transformed data has muchsmaller variation compared to the original data and can be evaluated in asimple proposed unimodality test. Compared with the existing unimodality tests,Sigtest is more accurate in detecting the overlapped clusters and has a muchless computational complexity. Simulation results demonstrate the efficiency ofthis statistic test for both real and synthetic data sets.
arxiv-4800-83 | Image reconstruction from few views by L0-norm optimization | http://arxiv.org/pdf/1401.1882v1.pdf | author:Yuli Sun, Jinxu Tao category:cs.IT cs.CV math.IT published:2014-01-09 summary:The L1-norm of the gradient-magnitude images (GMI), which is the well-knowntotal variation (TV) model, is widely used as regularization in the few viewsCT reconstruction. As the L1-norm TV regularization is tending to uniformlypenalize the image gradient and the low-contrast structures are sometimes oversmoothed, we proposed a new algorithm based on the L0-norm of the GMI to dealwith the few views problem. To rise to the challenges introduced by the L0-normDGT, the algorithm uses a pseudo-inverse transform of DGT and adapts aniterative hard thresholding (IHT) algorithm, whose convergence and effectiveefficiency have been theoretically proven. The simulation indicates that thealgorithm proposed in this paper can obviously improve the reconstructionquality.
arxiv-4800-84 | Robust Large Scale Non-negative Matrix Factorization using Proximal Point Algorithm | http://arxiv.org/pdf/1401.1842v1.pdf | author:Jason Gejie Liu, Shuchin Aeron category:stat.ML cs.IT cs.LG cs.NA math.IT published:2014-01-08 summary:A robust algorithm for non-negative matrix factorization (NMF) is presentedin this paper with the purpose of dealing with large-scale data, where theseparability assumption is satisfied. In particular, we modify the LinearProgramming (LP) algorithm of [9] by introducing a reduced set of constraintsfor exact NMF. In contrast to the previous approaches, the proposed algorithmdoes not require the knowledge of factorization rank (extreme rays [3] ortopics [7]). Furthermore, motivated by a similar problem arising in the contextof metabolic network analysis [13], we consider an entirely different regimewhere the number of extreme rays or topics can be much larger than thedimension of the data vectors. The performance of the algorithm for differentsynthetic data sets are provided.
arxiv-4800-85 | Learning Multilingual Word Representations using a Bag-of-Words Autoencoder | http://arxiv.org/pdf/1401.1803v1.pdf | author:Stanislas Lauly, Alex Boulanger, Hugo Larochelle category:cs.CL cs.LG stat.ML published:2014-01-08 summary:Recent work on learning multilingual word representations usually relies onthe use of word-level alignements (e.g. infered with the help of GIZA++)between translated sentences, in order to align the word embeddings indifferent languages. In this workshop paper, we investigate an autoencodermodel for learning multilingual word representations that does without suchword-level alignements. The autoencoder is trained to reconstruct thebag-of-word representation of given sentence from an encoded representationextracted from its translation. We evaluate our approach on a multilingualdocument classification task, where labeled data is available only for onelanguage (e.g. English) while classification must be performed in a differentlanguage (e.g. French). In our experiments, we observe that our method comparesfavorably with a previously proposed method that exploits word-level alignmentsto learn word representations.
arxiv-4800-86 | Variations on Memetic Algorithms for Graph Coloring Problems | http://arxiv.org/pdf/1401.2184v1.pdf | author:Laurent Moalic, Alexandre Gondran category:cs.AI cs.NE math.OC published:2014-01-08 summary:Graph vertices coloring with a given number of colors is a famous andmuch-studied NP-complete problem. The best methods to solve this problem arehybrid algorithms such as memetic algorithms [Galinier99, Lu10, Wu12] orquantum annealing [Titiloye11a, Titiloye11b, Titiloye12]. Those hybridalgorithms use a powerful local search inside a population-based algorithm. Thebalance between intensification and diversification is essential for thosemetaheuristics but difficult to archieve. Customizing metaheuristics takes longtime and is one of the main weak points of these approaches. This paper studiesthe impact of the increase and the decrease of diversification in one of themost effective algorithms known: the Hybrid Evolutionary Algorithm (HEA) fromGalinier and Hao [Galinier99]. We then propose a modification of this memeticalgorithm in order to work with a population of only two individuals. This newalgorithm more effectively manages the correct 'dose' of diversification to addinto the included local search - TabuCol [Hertz87] in the case of the HEA. Ithas produced several good results for the well-known DIMACS benchmark graphs,such as 47-colorings for DSJC500.5, 82-colorings for DSJC1000.5, 222-coloringsfor DSJC1000.9 and 81-colorings for flat1000\_76\_0, which have so far onlybeen produced by quantum annealing [Titiloye12] in 2012 with massivemulti-CPUs.
arxiv-4800-87 | Large Scale Visual Recommendations From Street Fashion Images | http://arxiv.org/pdf/1401.1778v1.pdf | author:Vignesh Jagadeesh, Robinson Piramuthu, Anurag Bhardwaj, Wei Di, Neel Sundaresan category:cs.CV published:2014-01-08 summary:We describe a completely automated large scale visual recommendation systemfor fashion. Our focus is to efficiently harness the availability of largequantities of online fashion images and their rich meta-data. Specifically, wepropose four data driven models in the form of Complementary Nearest NeighborConsensus, Gaussian Mixture Models, Texture Agnostic Retrieval and Markov ChainLDA for solving this problem. We analyze relative merits and pitfalls of thesealgorithms through extensive experimentation on a large-scale data set andbaseline them against existing ideas from color science. We also illustrate keyfashion insights learned through these experiments and show how they can beemployed to design better recommendation systems. Finally, we also outline alarge-scale annotated data set of fashion images (Fashion-136K) that can beexploited for future vision research.
arxiv-4800-88 | Natural Language Processing in Biomedicine: A Unified System Architecture Overview | http://arxiv.org/pdf/1401.0569v2.pdf | author:Son Doan, Mike Conway, Tu Minh Phuong, Lucila Ohno-Machado category:cs.CL published:2014-01-03 summary:In modern electronic medical records (EMR) much of the clinically importantdata - signs and symptoms, symptom severity, disease status, etc. - are notprovided in structured data fields, but rather are encoded in cliniciangenerated narrative text. Natural language processing (NLP) provides a means of"unlocking" this important data source for applications in clinical decisionsupport, quality assurance, and public health. This chapter provides anoverview of representative NLP systems in biomedicine based on a unifiedarchitectural view. A general architecture in an NLP system consists of twomain components: background knowledge that includes biomedical knowledgeresources and a framework that integrates NLP tools to process text. Systemsdiffer in both components, which we will review briefly. Additionally,challenges facing current research efforts in biomedical NLP include thepaucity of large, publicly available annotated corpora, although initiativesthat facilitate data sharing, system evaluation, and collaborative work betweenresearchers in clinical NLP are starting to emerge.
arxiv-4800-89 | Content Based Image Indexing and Retrieval | http://arxiv.org/pdf/1401.1742v1.pdf | author:Avinash N Bhute, B. B. Meshram category:cs.CV cs.GR cs.IR cs.MM published:2014-01-08 summary:In this paper, we present the efficient content based image retrieval systemswhich employ the color, texture and shape information of images to facilitatethe retrieval process. For efficient feature extraction, we extract the color,texture and shape feature of images automatically using edge detection which iswidely used in signal processing and image compression. For facilitated thespeedy retrieval we are implements the antipole-tree algorithm for indexing theimages.
arxiv-4800-90 | Application of the Modified Fractal Signature Method for Terrain Classification from Synthetic Aperture Radar Images | http://arxiv.org/pdf/1401.2899v1.pdf | author:A. Malamou, C. Pandis, P. Frangos, P. Stefaneas, A. Karakasiliotis, D. Kodokostas category:cs.CV published:2014-01-08 summary:In this paper the Modified Fractal Signature method is applied to realSynthetic Aperture Radar images provided to our research group by SET 163Working Group on SAR radar techniques. This method uses the blanket techniqueto provide useful information for SAR image classification. It is based on thecalculation of the volume of a blanket, corresponding to the image to beclassified, and then on the calculation of the corresponding Fractal Area curveand Fractal Dimension curve of the image. The main idea concerning thisproposed technique is the fact that different terrain types encountered in SARimages yield different values of Fractal Area curves and Fractal Dimensioncurves, upon which classification of different types of terrain is possible. Asa result, a classification technique for five different terrain types, i.e.urban, suburban, rural, mountain and sea, is presented in this paper.
arxiv-4800-91 | Estimating the historical and future probabilities of large terrorist events | http://arxiv.org/pdf/1209.0089v3.pdf | author:Aaron Clauset, Ryan Woodard category:cs.LG physics.soc-ph stat.AP stat.ME published:2012-09-01 summary:Quantities with right-skewed distributions are ubiquitous in complex socialsystems, including political conflict, economics and social networks, and thesesystems sometimes produce extremely large events. For instance, the 9/11terrorist events produced nearly 3000 fatalities, nearly six times more thanthe next largest event. But, was this enormous loss of life statisticallyunlikely given modern terrorism's historical record? Accurately estimating theprobability of such an event is complicated by the large fluctuations in theempirical distribution's upper tail. We present a generic statistical algorithmfor making such estimates, which combines semi-parametric models of tailbehavior and a nonparametric bootstrap. Applied to a global database ofterrorist events, we estimate the worldwide historical probability of observingat least one 9/11-sized or larger event since 1968 to be 11-35%. These resultsare robust to conditioning on global variations in economic development,domestic versus international events, the type of weapon used and a truncatedhistory that stops at 1998. We then use this procedure to make a data-drivenstatistical forecast of at least one similar event over the next decade.
arxiv-4800-92 | A New Continuous-Time Equality-Constrained Optimization Method to Avoid Singularity | http://arxiv.org/pdf/1209.5218v2.pdf | author:Quan Quan, Kai-Yuan Cai category:cs.NE published:2012-09-24 summary:In equality-constrained optimization, a standard regularity assumption isoften associated with feasible point methods, namely the gradients ofconstraints are linearly independent. In practice, the regularity assumptionmay be violated. To avoid such a singularity, we propose a new projectionmatrix, based on which a feasible point method for the continuous-time,equality-constrained optimization problem is developed. First, the equalityconstraint is transformed into a continuous-time dynamical system withsolutions that always satisfy the equality constraint. Then, the singularity isexplained in detail and a new projection matrix is proposed to avoidsingularity. An update (or say a controller) is subsequently designed todecrease the objective function along the solutions of the transformed system.The invariance principle is applied to analyze the behavior of the solution. Wealso propose a modified approach for addressing cases in which solutions do notsatisfy the equality constraint. Finally, the proposed optimization approachesare applied to two examples to demonstrate its effectiveness.
arxiv-4800-93 | Beyond One-Step-Ahead Forecasting: Evaluation of Alternative Multi-Step-Ahead Forecasting Models for Crude Oil Prices | http://arxiv.org/pdf/1401.1560v1.pdf | author:Tao Xiong, Yukun Bao, Zhongyi Hu category:cs.LG cs.AI published:2014-01-08 summary:An accurate prediction of crude oil prices over long future horizons ischallenging and of great interest to governments, enterprises, and investors.This paper proposes a revised hybrid model built upon empirical modedecomposition (EMD) based on the feed-forward neural network (FNN) modelingframework incorporating the slope-based method (SBM), which is capable ofcapturing the complex dynamic of crude oil prices. Three commonly usedmulti-step-ahead prediction strategies proposed in the literature, includingiterated strategy, direct strategy, and MIMO (multiple-input multiple-output)strategy, are examined and compared, and practical considerations for theselection of a prediction strategy for multi-step-ahead forecasting relating tocrude oil prices are identified. The weekly data from the WTI (West TexasIntermediate) crude oil spot price are used to compare the performance of thealternative models under the EMD-SBM-FNN modeling framework with selectedcounterparts. The quantitative and comprehensive assessments are performed onthe basis of prediction accuracy and computational cost. The results obtainedin this study indicate that the proposed EMD-SBM-FNN model using the MIMOstrategy is the best in terms of prediction accuracy with accreditedcomputational load.
arxiv-4800-94 | The Continuity of Images by Transmission Imaging Revisited | http://arxiv.org/pdf/1401.1558v1.pdf | author:Zhitao Fan, Feng Guan, Chunlin Wu, Ming Yan category:math.DG cs.CV math.NA published:2014-01-08 summary:Transmission imaging, as an important imaging technique widely used inastronomy, medical diagnosis, and biology science, has been shown in [49] quitedifferent from reflection imaging used in our everyday life. Understanding thestructures of images (the prior information) is important for designing,testing, and choosing image processing methods, and good image processingmethods are helpful for further uses of the image data, e.g., increasing theaccuracy of the object reconstruction methods in transmission imagingapplications. In reflection imaging, the images are usually modeled asdiscontinuous functions and even piecewise constant functions. In transmissionimaging, it was shown very recently in [49] that almost all images arecontinuous functions. However, the author in [49] considered only the case ofparallel beam geometry and used some too strong assumptions in the proof, whichexclude some common cases such as cylindrical objects. In this paper, weconsider more general beam geometries and simplify the assumptions by usingtotally different techniques. In particular, we will prove that almost allimages in transmission imaging with both parallel and divergent beam geometries(two most typical beam geometries) are continuous functions, under much weakerassumptions than those in [49], which admit almost all practical cases.Besides, taking into accounts our analysis, we compare two image processingmethods for Poisson noise (which is the most significant noise in transmissionimaging) removal. Numerical experiments will be provided to demonstrate ouranalysis.
arxiv-4800-95 | Factorized Point Process Intensities: A Spatial Analysis of Professional Basketball | http://arxiv.org/pdf/1401.0942v2.pdf | author:Andrew Miller, Luke Bornn, Ryan Adams, Kirk Goldsberry category:stat.ML stat.AP published:2014-01-05 summary:We develop a machine learning approach to represent and analyze theunderlying spatial structure that governs shot selection among professionalbasketball players in the NBA. Typically, NBA players are discussed andcompared in an heuristic, imprecise manner that relies on unmeasured intuitionsabout player behavior. This makes it difficult to draw comparisons betweenplayers and make accurate player specific predictions. Modeling shot attemptdata as a point process, we create a low dimensional representation ofoffensive player types in the NBA. Using non-negative matrix factorization(NMF), an unsupervised dimensionality reduction technique, we show that alow-rank spatial decomposition summarizes the shooting habits of NBA players.The spatial representations discovered by the algorithm correspond to intuitivedescriptions of NBA player types, and can be used to model other spatialeffects, such as shooting accuracy.
arxiv-4800-96 | Key point selection and clustering of swimmer coordination through Sparse Fisher-EM | http://arxiv.org/pdf/1401.1489v1.pdf | author:John Komar, Romain Hérault, Ludovic Seifert category:stat.ML cs.CV cs.LG stat.AP published:2014-01-07 summary:To answer the existence of optimal swimmer learning/teaching strategies, thiswork introduces a two-level clustering in order to analyze temporal dynamics ofmotor learning in breaststroke swimming. Each level have been performed throughSparse Fisher-EM, a unsupervised framework which can be applied efficiently onlarge and correlated datasets. The induced sparsity selects key points of thecoordination phase without any prior knowledge.
arxiv-4800-97 | Design & Development of the Graphical User Interface for Sindhi Language | http://arxiv.org/pdf/1401.1486v1.pdf | author:Imdad Ali Ismaili, Zeeshan Bhatti, Azhar Ali Shah category:cs.HC cs.CL published:2014-01-07 summary:This paper describes the design and implementation of a Unicode-based GUISL(Graphical User Interface for Sindhi Language). The idea is to provide asoftware platform to the people of Sindh as well as Sindhi diasporas livingacross the globe to make use of computing for basic tasks such as editing,composition, formatting, and printing of documents in Sindhi by using GUISL.The implementation of the GUISL has been done in the Java technology to makethe system platform independent. The paper describes several design issues ofSindhi GUI in the context of existing software tools and technologies andexplains how mapping and concatenation techniques have been employed to achievethe cursive shape of Sindhi script.
arxiv-4800-98 | Cortical prediction markets | http://arxiv.org/pdf/1401.1465v1.pdf | author:David Balduzzi category:cs.AI cs.GT cs.LG cs.MA q-bio.NC published:2014-01-07 summary:We investigate cortical learning from the perspective of mechanism design.First, we show that discretizing standard models of neurons and synapticplasticity leads to rational agents maximizing simple scoring rules. Second,our main result is that the scoring rules are proper, implying that neuronsfaithfully encode expected utilities in their synaptic weights and encodehigh-scoring outcomes in their spikes. Third, with this foundation in hand, wepropose a biologically plausible mechanism whereby neurons backpropagateincentives which allows them to optimize their usefulness to the rest ofcortex. Finally, experiments show that networks that backpropagate incentivescan learn simple tasks.
arxiv-4800-99 | Forward-Backward Greedy Algorithms for General Convex Smooth Functions over A Cardinality Constraint | http://arxiv.org/pdf/1401.0086v2.pdf | author:Ji Liu, Ryohei Fujimaki, Jieping Ye category:stat.ML published:2013-12-31 summary:We consider forward-backward greedy algorithms for solving sparse featureselection problems with general convex smooth functions. A state-of-the-artgreedy method, the Forward-Backward greedy algorithm (FoBa-obj) requires tosolve a large number of optimization problems, thus it is not scalable forlarge-size problems. The FoBa-gdt algorithm, which uses the gradientinformation for feature selection at each forward iteration, significantlyimproves the efficiency of FoBa-obj. In this paper, we systematically analyzethe theoretical properties of both forward-backward greedy algorithms. Our maincontributions are: 1) We derive better theoretical bounds than existinganalyses regarding FoBa-obj for general smooth convex functions; 2) We showthat FoBa-gdt achieves the same theoretical performance as FoBa-obj under thesame condition: restricted strong convexity condition. Our new bounds areconsistent with the bounds of a special case (least squares) and fills apreviously existing theoretical gap for general convex smooth functions; 3) Weshow that the restricted strong convexity condition is satisfied if the numberof independent samples is more than $\bar{k}\log d$ where $\bar{k}$ is thesparsity number and $d$ is the dimension of the variable; 4) We apply FoBa-gdt(with the conditional random field objective) to the sensor selection problemfor human indoor activity recognition and our results show that FoBa-gdtoutperforms other methods (including the ones based on forward greedy selectionand L1-regularization).
arxiv-4800-100 | Insights from the Wikipedia Contest (IEEE Contest for Data Mining 2011) | http://arxiv.org/pdf/1405.7393v1.pdf | author:Kalpit V Desai, Roopesh Ranjan category:cs.CY physics.soc-ph stat.ML published:2014-01-07 summary:The Wikimedia Foundation has recently observed that newly joining editors onWikipedia are increasingly failing to integrate into the Wikipedia editors'community, i.e. the community is becoming increasingly harder to penetrate. Tosustain healthy growth of the community, the Wikimedia Foundation aims toquantitatively understand the factors that determine the editing behavior, andexplain why most new editors become inactive soon after joining. As a steptowards this broader goal, the Wikimedia foundation sponsored the ICDM (IEEEInternational Conference for Data Mining) contest for the year 2011. The objective for the participants was to develop models to predict thenumber of edits that an editor will make in future five months based on theediting history of the editor. Here we describe the approach we followed fordeveloping predictive models towards this goal, the results that we obtainedand the modeling insights that we gained from this exercise. In addition,towards the broader goal of Wikimedia Foundation, we also summarize the factorsthat emerged during our model building exercise as powerful predictors offuture editing activity.
arxiv-4800-101 | Time series forecasting using neural networks | http://arxiv.org/pdf/1401.1333v1.pdf | author:Bogdan Oancea, ŞTefan Cristian Ciucu category:cs.NE published:2014-01-07 summary:Recent studies have shown the classification and prediction power of theNeural Networks. It has been demonstrated that a NN can approximate anycontinuous function. Neural networks have been successfully used forforecasting of financial data series. The classical methods used for timeseries prediction like Box-Jenkins or ARIMA assumes that there is a linearrelationship between inputs and outputs. Neural Networks have the advantagethat can approximate nonlinear functions. In this paper we compared theperformances of different feed forward and recurrent neural networks andtraining algorithms for predicting the exchange rate EUR/RON and USD/RON. Weused data series with daily exchange rates starting from 2005 until 2013.
arxiv-4800-102 | A New Approach To Two-View Motion Segmentation Using Global Dimension Minimization | http://arxiv.org/pdf/1304.2999v2.pdf | author:Bryan Poling, Gilad Lerman category:cs.CV published:2013-04-10 summary:We present a new approach to rigid-body motion segmentation from two views.We use a previously developed nonlinear embedding of two-view pointcorrespondences into a 9-dimensional space and identify the different motionsby segmenting lower-dimensional subspaces. In order to overcome nonuniformdistributions along the subspaces, whose dimensions are unknown, we suggest thenovel concept of global dimension and its minimization for clustering subspaceswith some theoretical motivation. We propose a fast projected gradientalgorithm for minimizing global dimension and thus segmenting motions from2-views. We develop an outlier detection framework around the proposed method,and we present state-of-the-art results on outlier-free and outlier-corruptedtwo-view data for segmenting motion.
arxiv-4800-103 | Quantile Regression for Large-scale Applications | http://arxiv.org/pdf/1305.0087v3.pdf | author:Jiyan Yang, Xiangrui Meng, Michael W. Mahoney category:cs.DS cs.DC cs.NA stat.ML published:2013-05-01 summary:Quantile regression is a method to estimate the quantiles of the conditionaldistribution of a response variable, and as such it permits a much moreaccurate portrayal of the relationship between the response variable andobserved covariates than methods such as Least-squares or Least AbsoluteDeviations regression. It can be expressed as a linear program, and, withappropriate preprocessing, interior-point methods can be used to find asolution for moderately large problems. Dealing with very large problems,\emph(e.g.), involving data up to and beyond the terabyte regime, remains achallenge. Here, we present a randomized algorithm that runs in nearly lineartime in the size of the input and that, with constant probability, computes a$(1+\epsilon)$ approximate solution to an arbitrary quantile regressionproblem. As a key step, our algorithm computes a low-distortionsubspace-preserving embedding with respect to the loss function of quantileregression. Our empirical evaluation illustrates that our algorithm iscompetitive with the best previous work on small to medium-sized problems, andthat in addition it can be implemented in MapReduce-like environments andapplied to terabyte-sized problems.
arxiv-4800-104 | Bangla Text Recognition from Video Sequence: A New Focus | http://arxiv.org/pdf/1401.1190v1.pdf | author:Souvik Bhowmick, Purnendu Banerjee category:cs.CV published:2014-01-06 summary:Extraction and recognition of Bangla text from video frame images ischallenging due to complex color background, low-resolution etc. In this paper,we propose an algorithm for extraction and recognition of Bangla text form suchvideo frames with complex background. Here, a two-step approach has beenproposed. First, the text line is segmented into words using information basedon line contours. First order gradient value of the text blocks are used tofind the word gap. Next, a local binarization technique is applied on each wordand text line is reconstructed using those words. Secondly, this binarized textblock is sent to OCR for recognition purpose.
arxiv-4800-105 | Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images | http://arxiv.org/pdf/1312.5412v3.pdf | author:Taichi Kiwaki, Takaki Makino, Kazuyuki Aihara category:stat.ML cs.LG published:2013-12-19 summary:We pursue an early stopping technique that helps Gaussian RestrictedBoltzmann Machines (GRBMs) to gain good natural image representations in termsof overcompleteness and data fitting. GRBMs are widely considered as anunsuitable model for natural images because they gain non-overcompleterepresentations which include uniform filters that do not represent usefulimage features. We have recently found that GRBMs once gain and subsequentlylose useful filters during their training, contrary to this common perspective.We attribute this phenomenon to a tradeoff between overcompleteness of GRBMrepresentations and data fitting. To gain GRBM representations that areovercomplete and fit data well, we propose a measure for GRBM representationquality, approximated mutual information, and an early stopping technique basedon this measure. The proposed method boosts performance of classifiers trainedon GRBM representations.
arxiv-4800-106 | Effective Slot Filling Based on Shallow Distant Supervision Methods | http://arxiv.org/pdf/1401.1158v1.pdf | author:Benjamin Roth, Tassilo Barth, Michael Wiegand, Mittul Singh, Dietrich Klakow category:cs.CL published:2014-01-06 summary:Spoken Language Systems at Saarland University (LSV) participated this yearwith 5 runs at the TAC KBP English slot filling track. Effective algorithms forall parts of the pipeline, from document retrieval to relation prediction andresponse post-processing, are bundled in a modular end-to-end relationextraction system called RelationFactory. The main run solely focuses onshallow techniques and achieved significant improvements over LSV's last year'ssystem, while using the same training data and patterns. Improvements mainlyhave been obtained by a feature representation focusing on surface skip n-gramsand improved scoring for extracted distant supervision patterns. Importantfactors for effective extraction are the training and tuning scheme for distantsupervision classifiers, and the query expansion by a translation model basedon Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, thesubmitted main run of the LSV RelationFactory system achieved the top-rankedF1-score of 37.3%.
arxiv-4800-107 | Exploration vs Exploitation vs Safety: Risk-averse Multi-Armed Bandits | http://arxiv.org/pdf/1401.1123v1.pdf | author:Nicolas Galichet, Michèle Sebag, Olivier Teytaud category:cs.LG published:2014-01-06 summary:Motivated by applications in energy management, this paper presents theMulti-Armed Risk-Aware Bandit (MARAB) algorithm. With the goal of limiting theexploration of risky arms, MARAB takes as arm quality its conditional value atrisk. When the user-supplied risk level goes to 0, the arm quality tends towardthe essential infimum of the arm distribution density, and MARAB tends towardthe MIN multi-armed bandit algorithm, aimed at the arm with maximal minimalvalue. As a first contribution, this paper presents a theoretical analysis ofthe MIN algorithm under mild assumptions, establishing its robustnesscomparatively to UCB. The analysis is supported by extensive experimentalvalidation of MIN and MARAB compared to UCB and state-of-art risk-aware MABalgorithms on artificial and real-world problems.
arxiv-4800-108 | Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses | http://arxiv.org/pdf/1212.0478v2.pdf | author:Po-Ling Loh, Martin J. Wainwright category:stat.ML math.ST stat.TH published:2012-12-03 summary:We investigate the relationship between the structure of a discrete graphicalmodel and the support of the inverse of a generalized covariance matrix. Weshow that for certain graph structures, the support of the inverse covariancematrix of indicator variables on the vertices of a graph reflects theconditional independence structure of the graph. Our work extends results thathave previously been established only in the context of multivariate Gaussiangraphical models, thereby addressing an open question about the significance ofthe inverse covariance matrix of a non-Gaussian distribution. The proofexploits a combination of ideas from the geometry of exponential families,junction tree theory and convex analysis. These population-level results havevarious consequences for graph selection methods, both known and novel,including a novel method for structure estimation for missing or corruptedobservations. We provide nonasymptotic guarantees for such methods andillustrate the sharpness of these predictions via simulations.
arxiv-4800-109 | Differentially Private Data Releasing for Smooth Queries with Synthetic Database Output | http://arxiv.org/pdf/1401.0987v1.pdf | author:Chi Jin, Ziteng Wang, Junliang Huang, Yiqiao Zhong, Liwei Wang category:cs.DB stat.ML published:2014-01-06 summary:We consider accurately answering smooth queries while preserving differentialprivacy. A query is said to be $K$-smooth if it is specified by a functiondefined on $[-1,1]^d$ whose partial derivatives up to order $K$ are allbounded. We develop an $\epsilon$-differentially private mechanism for theclass of $K$-smooth queries. The major advantage of the algorithm is that itoutputs a synthetic database. In real applications, a synthetic database outputis appealing. Our mechanism achieves an accuracy of $O(n^{-\frac{K}{2d+K}}/\epsilon )$, and runs in polynomial time. We alsogeneralize the mechanism to preserve $(\epsilon, \delta)$-differential privacywith slightly improved accuracy. Extensive experiments on benchmark datasetsdemonstrate that the mechanisms have good accuracy and are efficient.
arxiv-4800-110 | Feature Selection Using Classifier in High Dimensional Data | http://arxiv.org/pdf/1401.0898v1.pdf | author:Vijendra Singh, Shivani Pathak category:cs.CV cs.LG stat.ML published:2014-01-05 summary:Feature selection is frequently used as a pre-processing step to machinelearning. It is a process of choosing a subset of original features so that thefeature space is optimally reduced according to a certain evaluation criterion.The central objective of this paper is to reduce the dimension of the data byfinding a small set of important features which can give good classificationperformance. We have applied filter and wrapper approach with differentclassifiers QDA and LDA respectively. A widely-used filter method is used forbioinformatics data i.e. a univariate criterion separately on each feature,assuming that there is no interaction between features and then appliedSequential Feature Selection method. Experimental results show that filterapproach gives better performance in respect of Misclassification Error Rate.
arxiv-4800-111 | Learning parametric dictionaries for graph signals | http://arxiv.org/pdf/1401.0887v1.pdf | author:Dorina Thanou, David I Shuman, Pascal Frossard category:cs.LG cs.SI stat.ML published:2014-01-05 summary:In sparse signal representation, the choice of a dictionary often involves atradeoff between two desirable properties -- the ability to adapt to specificsignal data and a fast implementation of the dictionary. To sparsely representsignals residing on weighted graphs, an additional design challenge is toincorporate the intrinsic geometric structure of the irregular data domain intothe atoms of the dictionary. In this work, we propose a parametric dictionarylearning algorithm to design data-adapted, structured dictionaries thatsparsely represent graph signals. In particular, we model graph signals ascombinations of overlapping local patterns. We impose the constraint that eachdictionary is a concatenation of subdictionaries, with each subdictionary beinga polynomial of the graph Laplacian matrix, representing a single patterntranslated to different areas of the graph. The learning algorithm adapts thepatterns to a training set of graph signals. Experimental results on bothsynthetic and real datasets demonstrate that the dictionaries learned by theproposed algorithm are competitive with and often better than unstructureddictionaries learned by state-of-the-art numerical learning algorithms in termsof sparse approximation of graph signals. In contrast to the unstructureddictionaries, however, the dictionaries learned by the proposed algorithmfeature localized atoms and can be implemented in a computationally efficientmanner in signal processing tasks such as compression, denoising, andclassification.
arxiv-4800-112 | Spectrum Hole Prediction Based On Historical Data: A Neural Network Approach | http://arxiv.org/pdf/1401.0886v1.pdf | author:Barau Gafai Najashi, Feng Wenjiang, Mohammed Dikko Almustapha category:cs.NE published:2014-01-05 summary:The concept of cognitive radio pioneered by Mitola promises to change thefuture of wireless communication especially in the area of spectrum management.Currently, the command and control strategy employed in spectrum assignment istoo rigid and needs to be reviewed. Recent studies have shown that assignedspectrum is underutilized spectrally and temporally. Cognitive radio provides aviable solution whereby licensed users can share the spectrum with unlicensedusers opportunistically without causing interference. Unlicensed users must beable to sense weather the channel is busy or idle, failure to do so will leadto interference to the licensed user. In this paper, a neural network basedprediction model for predicting the channel status using historical dataobtained during a spectrum occupancy measurement is presented. Geneticalgorithm is combined with LM BP for increasing the probability of obtainingthe best weights thus optimizing the network. The results obtained indicatehigh prediction accuracy over all bands considered
arxiv-4800-113 | Stylistic Clusters and the Syrian/South Syrian Tradition of First-Millennium BCE Levantine Ivory Carving: A Machine Learning Approach | http://arxiv.org/pdf/1401.0871v1.pdf | author:Amy Rebecca Gansell, Jan-Willem van de Meent, Sakellarios Zairis, Chris H. Wiggins category:stat.ML stat.AP published:2014-01-05 summary:Thousands of first-millennium BCE ivory carvings have been excavated fromNeo-Assyrian sites in Mesopotamia (primarily Nimrud, Khorsabad, and ArslanTash) hundreds of miles from their Levantine production contexts. At present,their specific manufacture dates and workshop localities are unknown. Relyingon subjective, visual methods, scholars have grappled with their classificationand regional attribution for over a century. This study combines visualapproaches with machine-learning techniques to offer data-driven perspectiveson the classification and attribution of this early Iron Age corpus. The studysample consisted of 162 sculptures of female figures. We have developed analgorithm that clusters the ivories based on a combination of descriptive andanthropometric data. The resulting categories, which are based on purelystatistical criteria, show good agreement with conventional art historicalclassifications, while revealing new perspectives, especially with regard tothe contested Syrian/South Syrian/Intermediate tradition. Specifically, we haveidentified that objects of the Syrian/South Syrian/Intermediate tradition maybe more closely related to Phoenician objects than to North Syrian objects; weoffer a reconsideration of a subset of Phoenician objects, and we confirmSyrian/South Syrian/Intermediate stylistic subgroups that might distinguishnetworks of acquisition among the sites of Nimrud, Khorsabad, Arslan Tash andthe Levant. We have also identified which features are most significant in ourcluster assignments and might thereby be most diagnostic of regional carvingtraditions. In short, our study both corroborates traditional visualclassification methods and demonstrates how machine-learning techniques may beemployed to reveal complementary information not accessible through theexclusively visual analysis of an archaeological corpus.
arxiv-4800-114 | Pectoral Muscles Suppression in Digital Mammograms using Hybridization of Soft Computing Methods | http://arxiv.org/pdf/1401.0870v1.pdf | author:I. Laurence Aroquiaraj, K. Thangavel category:cs.CV cs.CE published:2014-01-05 summary:Breast region segmentation is an essential prerequisite in computerizedanalysis of mammograms. It aims at separating the breast tissue from thebackground of the mammogram and it includes two independent segmentations. Thefirst segments the background region which usually contains annotations, labelsand frames from the whole breast region, while the second removes the pectoralmuscle portion (present in Medio Lateral Oblique (MLO) views) from the rest ofthe breast tissue. In this paper we propose hybridization of ConnectedComponent Labeling (CCL), Fuzzy, and Straight line methods. Our proposedmethods worked good for separating pectoral region. After removal pectoralmuscle from the mammogram, further processing is confined to the breast regionalone. To demonstrate the validity of our segmentation algorithm, it isextensively tested using over 322 mammographic images from the MammographicImage Analysis Society (MIAS) database. The segmentation results were evaluatedusing a Mean Absolute Error (MAE), Hausdroff Distance (HD), Probabilistic RandIndex (PRI), Local Consistency Error (LCE) and Tanimoto Coefficient (TC). Thehybridization of fuzzy with straight line method is given more than 96% of thecurve segmentations to be adequate or better. In addition a comparison withsimilar approaches from the state of the art has been given, obtaining slightlyimproved results. Experimental results demonstrate the effectiveness of theproposed approach.
arxiv-4800-115 | An Empirical Evaluation of Portfolios Approaches for solving CSPs | http://arxiv.org/pdf/1212.0692v2.pdf | author:Roberto Amadini, Maurizio Gabbrielli, Jacopo Mauro category:cs.AI cs.LG published:2012-12-04 summary:Recent research in areas such as SAT solving and Integer Linear Programminghas shown that the performances of a single arbitrarily efficient solver can besignificantly outperformed by a portfolio of possibly slower on-averagesolvers. We report an empirical evaluation and comparison of portfolioapproaches applied to Constraint Satisfaction Problems (CSPs). We comparedmodels developed on top of off-the-shelf machine learning algorithms withrespect to approaches used in the SAT field and adapted for CSPs, consideringdifferent portfolio sizes and using as evaluation metrics the number of solvedproblems and the time taken to solve them. Results indicate that the best SATapproaches have top performances also in the CSP field and are slightly morecompetitive than simple models built on top of classification algorithms.
arxiv-4800-116 | Multimodal Optimization by Sparkling Squid Populations | http://arxiv.org/pdf/1401.0858v1.pdf | author:Videh Seksaria category:cs.NE published:2014-01-05 summary:The swarm intelligence of animals is a natural paradigm to apply tooptimization problems. Ant colony, bee colony, firefly and bat algorithms areamongst those that have been demonstrated to efficiently to optimize complexconstraints. This paper proposes the new Sparkling Squid Algorithm (SSA) formultimodal optimization, inspired by the intelligent swarm behavior of itsnamesake. After an introduction, formulation and discussion of itsimplementation, it will be compared to other popular metaheuristics. Finally,applications to well - known problems such as image registration and thetraveling salesperson problem will be discussed.
arxiv-4800-117 | Least Squares Policy Iteration with Instrumental Variables vs. Direct Policy Search: Comparison Against Optimal Benchmarks Using Energy Storage | http://arxiv.org/pdf/1401.0843v1.pdf | author:Warren R. Scott, Warren B. Powell, Somayeh Moazehi category:math.OC cs.LG published:2014-01-04 summary:This paper studies approximate policy iteration (API) methods which useleast-squares Bellman error minimization for policy evaluation. We addressseveral of its enhancements, namely, Bellman error minimization usinginstrumental variables, least-squares projected Bellman error minimization, andprojected Bellman error minimization using instrumental variables. We provethat for a general discrete-time stochastic control problem, Bellman errorminimization using instrumental variables is equivalent to both variants ofprojected Bellman error minimization. An alternative to these API methods isdirect policy search based on knowledge gradient. The practical performance ofthese three approximate dynamic programming methods are then investigated inthe context of an application in energy storage, integrated with anintermittent wind energy supply to fully serve a stochastic time-varyingelectricity demand. We create a library of test problems using real-world dataand apply value iteration to find their optimal policies. These benchmarks arethen used to compare the developed policies. Our analysis indicates that APIwith instrumental variables Bellman error minimization prominently outperformsAPI with least-squares Bellman error minimization. However, these approachesunderperform our direct policy search implementation.
arxiv-4800-118 | Properties of phoneme N -grams across the world's language families | http://arxiv.org/pdf/1401.0794v1.pdf | author:Taraka Rama, Lars Borin category:cs.CL stat.CO published:2014-01-04 summary:In this article, we investigate the properties of phoneme N-grams across halfof the world's languages. We investigate if the sizes of three different N-gramdistributions of the world's language families obey a power law. Further, theN-gram distributions of language families parallel the sizes of the families,which seem to obey a power law distribution. The correlation between N-gramdistributions and language family sizes improves with increasing values of N.We applied statistical tests, originally given by physicists, to test thehypothesis of power law fit to twelve different datasets. The study also raisessome new questions about the use of N-gram distributions in linguisticresearch, which we answer by running a statistical test.
arxiv-4800-119 | From Kernel Machines to Ensemble Learning | http://arxiv.org/pdf/1401.0767v1.pdf | author:Chunhua Shen, Fayao Liu category:cs.LG cs.CV published:2014-01-04 summary:Ensemble methods such as boosting combine multiple learners to obtain betterprediction than could be obtained from any individual learner. Here we proposea principled framework for directly constructing ensemble learning methods fromkernel methods. Unlike previous studies showing the equivalence betweenboosting and support vector machines (SVMs), which needs a translationprocedure, we show that it is possible to design boosting-like procedure tosolve the SVM optimization problems. In other words, it is possible to design ensemble methods directly from SVMwithout any middle procedure. This finding not only enables us to design new ensemble learning methodsdirectly from kernel methods, but also makes it possible to take advantage ofthose highly-optimized fast linear SVM solvers for ensemble learning. We exemplify this framework for designing binary ensemble learning as well asa new multi-class ensemble learning methods. Experimental results demonstrate the flexibility and usefulness of theproposed framework.
arxiv-4800-120 | Context-Aware Hypergraph Construction for Robust Spectral Clustering | http://arxiv.org/pdf/1401.0764v1.pdf | author:Xi Li, Weiming Hu, Chunhua Shen, Anthony Dick, Zhongfei Zhang category:cs.CV cs.LG published:2014-01-04 summary:Spectral clustering is a powerful tool for unsupervised data analysis. Inthis paper, we propose a context-aware hypergraph similarity measure (CAHSM),which leads to robust spectral clustering in the case of noisy data. Weconstruct three types of hypergraph---the pairwise hypergraph, thek-nearest-neighbor (kNN) hypergraph, and the high-order over-clusteringhypergraph. The pairwise hypergraph captures the pairwise similarity of datapoints; the kNN hypergraph captures the neighborhood of each point; and theclustering hypergraph encodes high-order contexts within the dataset. Bycombining the affinity information from these three hypergraphs, the CAHSMalgorithm is able to explore the intrinsic topological information of thedataset. Therefore, data clustering using CAHSM tends to be more robust.Considering the intra-cluster compactness and the inter-cluster separability ofvertices, we further design a discriminative hypergraph partitioning criterion(DHPC). Using both CAHSM and DHPC, a robust spectral clustering algorithm isdeveloped. Theoretical analysis and experimental evaluation demonstrate theeffectiveness and robustness of the proposed algorithm.
arxiv-4800-121 | Data Smashing | http://arxiv.org/pdf/1401.0742v1.pdf | author:Ishanu Chattopadhyay, Hod Lipson category:cs.LG cs.AI cs.CE cs.IT math.IT stat.ML published:2014-01-03 summary:Investigation of the underlying physics or biology from empirical datarequires a quantifiable notion of similarity - when do two observed data setsindicate nearly identical generating processes, and when they do not. Thediscriminating characteristics to look for in data is often determined byheuristics designed by experts, $e.g.$, distinct shapes of "folded" lightcurvesmay be used as "features" to classify variable stars, while determination ofpathological brain states might require a Fourier analysis of brainwaveactivity. Finding good features is non-trivial. Here, we propose a universalsolution to this problem: we delineate a principle for quantifying similaritybetween sources of arbitrary data streams, without a priori knowledge, featuresor training. We uncover an algebraic structure on a space of symbolic modelsfor quantized data, and show that such stochastic generators may be added anduniquely inverted; and that a model and its inverse always sum to the generatorof flat white noise. Therefore, every data stream has an anti-stream: datagenerated by the inverse model. Similarity between two streams, then, is thedegree to which one, when summed to the other's anti-stream, mutuallyannihilates all statistical structure to noise. We call this data smashing. Wepresent diverse applications, including disambiguation of brainwaves pertainingto epileptic seizures, detection of anomalous cardiac rhythms, andclassification of astronomical objects from raw photometry. In our examples,the data smashing principle, without access to any domain knowledge, meets orexceeds the performance of specialized algorithms tuned by domain experts.
arxiv-4800-122 | Quantitative methods for Phylogenetic Inference in Historical Linguistics: An experimental case study of South Central Dravidian | http://arxiv.org/pdf/1401.0708v1.pdf | author:Taraka Rama, Sudheer Kolachina, Lakshmi Bai B category:cs.CL cs.AI published:2014-01-03 summary:In this paper we examine the usefulness of two classes of algorithms DistanceMethods, Discrete Character Methods (Felsenstein and Felsenstein 2003) widelyused in genetics, for predicting the family relationships among a set ofrelated languages and therefore, diachronic language change. Applying thesealgorithms to the data on the numbers of shared cognates- with-change andchanged as well as unchanged cognates for a group of six languages belonging toa Dravidian language sub-family given in Krishnamurti et al. (1983), weobserved that the resultant phylogenetic trees are largely in agreement withthe linguistic family tree constructed using the comparative method ofreconstruction with only a few minor differences. Furthermore, we studied theseminor differences and found that they were cases of genuine ambiguity even fora well-trained historical linguist. We evaluated the trees obtained through ourexperiments using a well-defined criterion and report the results here. Wefinally conclude that quantitative methods like the ones we examined are quiteuseful in predicting family relationships among languages. In addition, weconclude that a modest degree of confidence attached to the intuition thatthere could indeed exist a parallelism between the processes of linguistic andgenetic change is not totally misplaced.
arxiv-4800-123 | Sparse Signal Estimation by Maximally Sparse Convex Optimization | http://arxiv.org/pdf/1302.5729v3.pdf | author:Ivan W. Selesnick, Ilker Bayram category:cs.LG stat.ML published:2013-02-22 summary:This paper addresses the problem of sparsity penalized least squares forapplications in sparse signal processing, e.g. sparse deconvolution. This paperaims to induce sparsity more strongly than L1 norm regularization, whileavoiding non-convex optimization. For this purpose, this paper describes thedesign and use of non-convex penalty functions (regularizers) constrained so asto ensure the convexity of the total cost function, F, to be minimized. Themethod is based on parametric penalty functions, the parameters of which areconstrained to ensure convexity of F. It is shown that optimal parameters canbe obtained by semidefinite programming (SDP). This maximally sparse convex(MSC) approach yields maximally non-convex sparsity-inducing penalty functionsconstrained such that the total cost function, F, is convex. It is demonstratedthat iterative MSC (IMSC) can yield solutions substantially more sparse thanthe standard convex sparsity-inducing approach, i.e., L1 norm minimization.
arxiv-4800-124 | A type theoretical framework for natural language semantics: the Montagovian generative lexicon | http://arxiv.org/pdf/1301.4938v3.pdf | author:Christian Retoré category:cs.LO cs.CL published:2013-01-21 summary:We present a framework, named the Montagovian generative lexicon, forcomputing the semantics of natural language sentences, expressed in many sortedhigher order logic. Word meaning is depicted by lambda terms of second orderlambda calculus (Girard's system F) with base types including a type forpropositions and many types for sorts of a many sorted logic. This framework isable to integrate a proper treatment of lexical phenomena into a Montagoviancompositional semantics, including the restriction of selection which imposesthe nature of the arguments of a predicate, and the possible adaptation of aword meaning to some contexts. Among these adaptations of a word's sense to thecontext, ontological inclusions are handled by an extension of system F withcoercive subtyping that is introduced in the present paper. The benefits ofthis framework for lexical pragmatics are illustrated on meaning transfers andcoercions, on possible and impossible copredication over different senses, ondeverbal ambiguities, and on "fictive motion". Next we show that thecompositional treatment of determiners, quantifiers, plurals,... are finergrained in our framework. We then conclude with the linguistic, logical andcomputational perspectives opened by the Montagovian generative lexicon.
arxiv-4800-125 | Plurals: individuals and sets in a richly typed semantics | http://arxiv.org/pdf/1401.0660v1.pdf | author:Bruno Mery, Richard Moot, Christian Retoré category:cs.CL published:2014-01-03 summary:We developed a type-theoretical framework for natural lan- guage semanticsthat, in addition to the usual Montagovian treatment of compositionalsemantics, includes a treatment of some phenomena of lex- ical semantic:coercions, meaning, transfers, (in)felicitous co-predication. In this settingwe see how the various readings of plurals (collective, dis- tributive,coverings,...) can be modelled.
arxiv-4800-126 | Multi-Topic Multi-Document Summarizer | http://arxiv.org/pdf/1401.0640v1.pdf | author:Fatma El-Ghannam, Tarek El-Shishtawy category:cs.CL published:2014-01-03 summary:Current multi-document summarization systems can successfully extract summarysentences, however with many limitations including: low coverage, inaccurateextraction to important sentences, redundancy and poor coherence among theselected sentences. The present study introduces a new concept of centroidapproach and reports new techniques for extracting summary sentences formulti-document. In both techniques keyphrases are used to weigh sentences anddocuments. The first summarization technique (Sen-Rich) prefers maximumrichness sentences. While the second (Doc-Rich), prefers sentences fromcentroid document. To demonstrate the new summarization system application toextract summaries of Arabic documents we performed two experiments. First, weapplied Rouge measure to compare the new techniques among systems presented atTAC2011. The results show that Sen-Rich outperformed all systems in ROUGE-S.Second, the system was applied to summarize multi-topic documents. Using humanevaluators, the results show that Doc-Rich is the superior, where summarysentences characterized by extra coverage and more cohesion.
arxiv-4800-127 | Particle Gibbs with Ancestor Sampling | http://arxiv.org/pdf/1401.0604v1.pdf | author:Fredrik Lindsten, Michael I. Jordan, Thomas B. Schön category:stat.CO stat.ML published:2014-01-03 summary:Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combiningthe two main tools used for Monte Carlo statistical inference: sequential MonteCarlo (SMC) and Markov chain Monte Carlo (MCMC). We present a novel PMCMCalgorithm that we refer to as particle Gibbs with ancestor sampling (PGAS).PGAS provides the data analyst with an off-the-shelf class of Markov kernelsthat can be used to simulate the typically high-dimensional and highlyautocorrelated state trajectory in a state-space model. The ancestor samplingprocedure enables fast mixing of the PGAS kernel even when using seemingly fewparticles in the underlying SMC sampler. This is important as it cansignificantly reduce the computational burden that is typically associated withusing SMC. PGAS is conceptually similar to the existing PG with backwardsimulation (PGBS) procedure. Instead of using separate forward and backwardsweeps as in PGBS, however, we achieve the same effect in a single forwardsweep. This makes PGAS well suited for addressing inference problems not onlyin state-space models, but also in models with more complex dependencies, suchas non-Markovian, Bayesian nonparametric, and general probabilistic graphicalmodels.
arxiv-4800-128 | Minimax sparse principal subspace estimation in high dimensions | http://arxiv.org/pdf/1211.0373v4.pdf | author:Vincent Q. Vu, Jing Lei category:math.ST stat.ML stat.TH published:2012-11-02 summary:We study sparse principal components analysis in high dimensions, where $p$(the number of variables) can be much larger than $n$ (the number ofobservations), and analyze the problem of estimating the subspace spanned bythe principal eigenvectors of the population covariance matrix. We introducetwo complementary notions of $\ell_q$ subspace sparsity: row sparsity andcolumn sparsity. We prove nonasymptotic lower and upper bounds on the minimaxsubspace estimation error for $0\leq q\leq1$. The bounds are optimal for rowsparse subspaces and nearly optimal for column sparse subspaces, they apply togeneral classes of covariance matrices, and they show that $\ell_q$ constrainedestimates can achieve optimal minimax rates without restrictive spikedcovariance conditions. Interestingly, the form of the rates matches knownresults for sparse regression when the effective noise variance is definedappropriately. Our proof employs a novel variational $\sin\Theta$ theorem thatmay be useful in other regularized spectral estimation problems.
arxiv-4800-129 | Adaptive-Rate Compressive Sensing Using Side Information | http://arxiv.org/pdf/1401.0583v1.pdf | author:Garrett Warnell, Sourabh Bhattacharya, Rama Chellappa, Tamer Basar category:cs.CV published:2014-01-03 summary:We provide two novel adaptive-rate compressive sensing (CS) strategies forsparse, time-varying signals using side information. Our first method utilizesextra cross-validation measurements, and the second one exploits extralow-resolution measurements. Unlike the majority of current CS techniques, wedo not assume that we know an upper bound on the number of significantcoefficients that comprise the images in the video sequence. Instead, we usethe side information to predict the number of significant coefficients in thesignal at the next time instant. For each image in the video sequence, ourtechniques specify a fixed number of spatially-multiplexed CS measurements toacquire, and adjust this quantity from image to image. Our strategies aredeveloped in the specific context of background subtraction for surveillancevideo, and we experimentally validate the proposed methods on real videosequences.
arxiv-4800-130 | More Algorithms for Provable Dictionary Learning | http://arxiv.org/pdf/1401.0579v1.pdf | author:Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma category:cs.DS cs.LG stat.ML published:2014-01-03 summary:In dictionary learning, also known as sparse coding, the algorithm is givensamples of the form $y = Ax$ where $x\in \mathbb{R}^m$ is an unknown randomsparse vector and $A$ is an unknown dictionary matrix in $\mathbb{R}^{n\timesm}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$and $x$. This problem has been studied in neuroscience, machine learning,visions, and image processing. In practice it is solved by heuristic algorithmsand provable algorithms seemed hard to find. Recently, provable algorithms werefound that work if the unknown feature vector $x$ is $\sqrt{n}$-sparse or evensparser. Spielman et al. \cite{DBLP:journals/jmlr/SpielmanWW12} did this fordictionaries where $m=n$; Arora et al. \cite{AGM} gave an algorithm forovercomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al.\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weakerguarantees. This raised the problem of designing provable algorithms that allow sparsity$\gg \sqrt{n}$ in the hidden vector $x$. The current paper designs algorithmsthat allow sparsity up to $n/poly(\log n)$. It works for a class of matriceswhere features are individually recoverable, a new notion identified in thispaper that may motivate further work. The algorithm runs in quasipolynomial time because they use limitedenumeration.
arxiv-4800-131 | Simple Deep Random Model Ensemble | http://arxiv.org/pdf/1305.1019v2.pdf | author:Xiao-Lei Zhang, Ji Wu category:cs.LG published:2013-05-05 summary:Representation learning and unsupervised learning are two central topics ofmachine learning and signal processing. Deep learning is one of the mosteffective unsupervised representation learning approach. The main contributionsof this paper to the topics are as follows. (i) We propose to view therepresentative deep learning approaches as special cases of the knowledge reuseframework of clustering ensemble. (ii) We propose to view sparse coding whenused as a feature encoder as the consensus function of clustering ensemble, andview dictionary learning as the training process of the base clusterings ofclustering ensemble. (ii) Based on the above two views, we propose a verysimple deep learning algorithm, named deep random model ensemble (DRME). It isa stack of random model ensembles. Each random model ensemble is a specialk-means ensemble that discards the expectation-maximization optimization ofeach base k-means but only preserves the default initialization method of thebase k-means. (iv) We propose to select the most powerful representation amongthe layers by applying DRME to clustering where the single-linkage is used asthe clustering algorithm. Moreover, the DRME based clustering can also detectthe number of the natural clusters accurately. Extensive experimentalcomparisons with 5 representation learning methods on 19 benchmark data setsdemonstrate the effectiveness of DRME.
arxiv-4800-132 | Learning Deep Representation Without Parameter Inference for Nonlinear Dimensionality Reduction | http://arxiv.org/pdf/1308.4922v2.pdf | author:Xiao-Lei Zhang category:cs.LG stat.ML published:2013-08-22 summary:Unsupervised deep learning is one of the most powerful representationlearning techniques. Restricted Boltzman machine, sparse coding, regularizedauto-encoders, and convolutional neural networks are pioneering building blocksof deep learning. In this paper, we propose a new building block -- distributedrandom models. The proposed method is a special full implementation of theproduct of experts: (i) each expert owns multiple hidden units and differentexperts have different numbers of hidden units; (ii) the model of each expertis a k-center clustering, whose k-centers are only uniformly sampled examples,and whose output (i.e. the hidden units) is a sparse code that only thesimilarity values from a few nearest neighbors are reserved. The relationshipbetween the pioneering building blocks, several notable research branches andthe proposed method is analyzed. Experimental results show that the proposeddeep model can learn better representations than deep belief networks andmeanwhile can train a much larger network with much less time than deep beliefnetworks.
arxiv-4800-133 | Low-Complexity Particle Swarm Optimization for Time-Critical Applications | http://arxiv.org/pdf/1401.0546v1.pdf | author:Muhammad Saqib Sohail, Muhammad Omer Bin Saeed, Syed Zeeshan Rizvi, Mobien Shoaib, Asrar Ul Haq Sheikh category:cs.NE published:2014-01-02 summary:Particle swam optimization (PSO) is a popular stochastic optimization methodthat has found wide applications in diverse fields. However, PSO suffers fromhigh computational complexity and slow convergence speed. High computationalcomplexity hinders its use in applications that have limited power resourceswhile slow convergence speed makes it unsuitable for time criticalapplications. In this paper, we propose two techniques to overcome theselimitations. The first technique reduces the computational complexity of PSOwhile the second technique speeds up its convergence. These techniques can beapplied, either separately or in conjunction, to any existing PSO variant. Theproposed techniques are robust to the number of dimensions of the optimizationproblem. Simulation results are presented for the proposed techniques appliedto the standard PSO as well as to several PSO variants. The results show thatthe use of both these techniques in conjunction results in a reduction in thenumber of computations required as well as faster convergence speed whilemaintaining an acceptable error performance for time-critical applications.
arxiv-4800-134 | Solving Poisson Equation by Genetic Algorithms | http://arxiv.org/pdf/1401.0523v1.pdf | author:Khalid Jebari, Mohammed Madiafi, Abdelaziz El Moujahid category:cs.NE published:2014-01-02 summary:This paper deals with a method for solving Poisson Equation (PE) based ongenetic algorithms and grammatical evolution. The method forms generations ofsolutions expressed in an analytical form. Several examples of PE are testedand in most cases the exact solution is recovered. But, when the solutioncannot be expressed in an analytical form, our method produces a satisfactorysolution with a good level of accuracy
arxiv-4800-135 | A Hybrid NN/HMM Modeling Technique for Online Arabic Handwriting Recognition | http://arxiv.org/pdf/1401.0486v1.pdf | author:Najiba Tagougui, Houcine Boubaker, Monji Kherallah, Adel M. ALIMI category:cs.CV published:2014-01-02 summary:In this work we propose a hybrid NN/HMM model for online Arabic handwritingrecognition. The proposed system is based on Hidden Markov Models (HMMs) andMulti Layer Perceptron Neural Networks (MLPNNs). The input signal is segmentedto continuous strokes called segments based on the Beta-Elliptical strategy byinspecting the extremum points of the curvilinear velocity profile. A neuralnetwork trained with segment level contextual information is used to extractclass character probabilities. The output of this network is decoded by HMMs toprovide character level recognition. In evaluations on the ADAB database, weachieved 96.4% character recognition accuracy that is statisticallysignificantly important in comparison with character recognition accuraciesobtained from state-of-the-art online Arabic systems.8
arxiv-4800-136 | Distinction between features extracted using deep belief networks | http://arxiv.org/pdf/1312.6157v2.pdf | author:Mohammad Pezeshki, Sajjad Gholami, Ahmad Nickabadi category:cs.LG cs.NE published:2013-12-20 summary:Data representation is an important pre-processing step in many machinelearning algorithms. There are a number of methods used for this task such asDeep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since someof the features extracted using automated feature extraction methods may notalways be related to a specific machine learning task, in this paper we proposetwo methods in order to make a distinction between extracted features based ontheir relevancy to the task. We applied these two methods to a Deep BeliefNetwork trained for a face recognition task.
arxiv-4800-137 | Deep Belief Networks for Image Denoising | http://arxiv.org/pdf/1312.6158v2.pdf | author:Mohammad Ali Keyvanrad, Mohammad Pezeshki, Mohammad Ali Homayounpour category:cs.LG cs.CV cs.NE published:2013-12-20 summary:Deep Belief Networks which are hierarchical generative models are effectivetools for feature representation and extraction. Furthermore, DBNs can be usedin numerous aspects of Machine Learning such as image denoising. In this paper,we propose a novel method for image denoising which relies on the DBNs' abilityin feature representation. This work is based upon learning of the noisebehavior. Generally, features which are extracted using DBNs are presented asthe values of the last layer nodes. We train a DBN a way that the networktotally distinguishes between nodes presenting noise and nodes presenting imagecontent in the last later of DBN, i.e. the nodes in the last layer of trainedDBN are divided into two distinct groups of nodes. After detecting the nodeswhich are presenting the noise, we are able to make the noise nodes inactiveand reconstruct a noiseless image. In section 4 we explore the results ofapplying this method on the MNIST dataset of handwritten digits which iscorrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% inaverage mean square error (MSE) was achieved when the proposed method was usedfor the reconstruction of the noisy images.
arxiv-4800-138 | An empirical analysis of dropout in piecewise linear networks | http://arxiv.org/pdf/1312.6197v2.pdf | author:David Warde-Farley, Ian J. Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2013-12-21 summary:The recently introduced dropout training criterion for neural networks hasbeen the subject of much attention due to its simplicity and remarkableeffectiveness as a regularizer, as well as its interpretation as a trainingprocedure for an exponentially large ensemble of networks that shareparameters. In this work we empirically investigate several questions relatedto the efficacy of dropout, specifically as it concerns networks employing thepopular rectified linear activation function. We investigate the quality of thetest time weight-scaling inference procedure by evaluating the geometricaverage exactly in small models, as well as compare the performance of thegeometric mean to the arithmetic mean more commonly employed by ensembletechniques. We explore the effect of tied weights on the ensembleinterpretation by training ensembles of masked networks without tied weights.Finally, we investigate an alternative criterion based on a biased estimator ofthe maximum likelihood ensemble gradient.
arxiv-4800-139 | Reducing the Computational Cost in Multi-objective Evolutionary Algorithms by Filtering Worthless Individuals | http://arxiv.org/pdf/1401.5808v1.pdf | author:Zahra Pourbahman, Ali Hamzeh category:cs.NE published:2014-01-02 summary:The large number of exact fitness function evaluations makes evolutionaryalgorithms to have computational cost. In some real-world problems, reducingnumber of these evaluations is much more valuable even by increasingcomputational complexity and spending more time. To fulfill this target, weintroduce an effective factor, in spite of applied factor in Adaptive FuzzyFitness Granulation with Non-dominated Sorting Genetic Algorithm-II, to filterout worthless individuals more precisely. Our proposed approach is comparedwith respect to Adaptive Fuzzy Fitness Granulation with Non-dominated SortingGenetic Algorithm-II, using the Hyper volume and the Inverted GenerationalDistance performance measures. The proposed method is applied to 1 traditionaland 1 state-of-the-art benchmarks with considering 3 different dimensions. Froman average performance view, the results indicate that although decreasing thenumber of fitness evaluations leads to have performance reduction but it is nottangible compared to what we gain.
arxiv-4800-140 | Hybrid Approach to Face Recognition System using Principle component and Independent component with score based fusion process | http://arxiv.org/pdf/1401.0395v1.pdf | author:Trupti M. Kodinariya category:cs.CV published:2014-01-02 summary:Hybrid approach has a special status among Face Recognition Systems as theycombine different recognition approaches in an either serial or parallel toovercome the shortcomings of individual methods. This paper explores the areaof Hybrid Face Recognition using score based strategy as a combiner/fusionprocess. In proposed approach, the recognition system operates in two modes:training and classification. Training mode involves normalization of the faceimages (training set), extracting appropriate features using PrincipleComponent Analysis (PCA) and Independent Component Analysis (ICA). Theextracted features are then trained in parallel using Back-propagation neuralnetworks (BPNNs) to partition the feature space in to different face classes.In classification mode, the trained PCA BPNN and ICA BPNN are fed with new faceimage(s). The score based strategy which works as a combiner is applied to theresults of both PCA BPNN and ICA BPNN to classify given new face image(s)according to face classes obtained during the training mode. The proposedapproach has been tested on ORL and other face databases; the experimentedresults show that the proposed system has higher accuracy than face recognitionsystems using single feature extractor.
arxiv-4800-141 | Relaxations for inference in restricted Boltzmann machines | http://arxiv.org/pdf/1312.6205v2.pdf | author:Sida I. Wang, Roy Frostig, Percy Liang, Christopher D. Manning category:stat.ML cs.LG published:2013-12-21 summary:We propose a relaxation-based approximate inference algorithm that samplesnear-MAP configurations of a binary pairwise Markov random field. We experimenton MAP inference tasks in several restricted Boltzmann machines. We also useour underlying sampler to estimate the log-partition function of restrictedBoltzmann machines and compare against other sampling-based methods.
arxiv-4800-142 | Generalization Bounds for Representative Domain Adaptation | http://arxiv.org/pdf/1401.0376v1.pdf | author:Chao Zhang, Lei Zhang, Wei Fan, Jieping Ye category:cs.LG stat.ML published:2014-01-02 summary:In this paper, we propose a novel framework to analyze the theoreticalproperties of the learning process for a representative type of domainadaptation, which combines data from multiple sources and one target (orbriefly called representative domain adaptation). In particular, we use theintegral probability metric to measure the difference between the distributionsof two domains and meanwhile compare it with the H-divergence and thediscrepancy distance. We develop the Hoeffding-type, the Bennett-type and theMcDiarmid-type deviation inequalities for multiple domains respectively, andthen present the symmetrization inequality for representative domainadaptation. Next, we use the derived inequalities to obtain the Hoeffding-typeand the Bennett-type generalization bounds respectively, both of which arebased on the uniform entropy number. Moreover, we present the generalizationbounds based on the Rademacher complexity. Finally, we analyze the asymptoticconvergence and the rate of convergence of the learning process forrepresentative domain adaptation. We discuss the factors that affect theasymptotic behavior of the learning process and the numerical experimentssupport our theoretical findings as well. Meanwhile, we give a comparison withthe existing results of domain adaptation and the classical results under thesame-distribution assumption.
arxiv-4800-143 | Convex optimization on Banach Spaces | http://arxiv.org/pdf/1401.0334v1.pdf | author:R. A. DeVore, V. N. Temlyakov category:stat.ML math.OC published:2014-01-01 summary:Greedy algorithms which use only function evaluations are applied to convexoptimization in a general Banach space $X$. Along with algorithms that useexact evaluations, algorithms with approximate evaluations are treated. Apriori upper bounds for the convergence rate of the proposed algorithms aregiven. These bounds depend on the smoothness of the objective function and thesparsity or compressibility (with respect to a given dictionary) of a point in$X$ where the minimum is attained.
arxiv-4800-144 | Design of an Agent for Answering Back in Smart Phones | http://arxiv.org/pdf/1306.5884v2.pdf | author:Sandeep Venkatesh, Meera V Patil, Nanditha Swamy category:cs.AI cs.HC cs.LG published:2013-06-25 summary:The objective of the paper is to design an agent which provides efficientresponse to the caller when a call goes unanswered in smartphones. The agentprovides responses through text messages, email etc stating the most likelyreason as to why the callee is unable to answer a call. Responses are composedtaking into consideration the importance of the present call and the situationthe callee is in at the moment like driving, sleeping, at work etc. The agentmakes decisons in the compostion of response messages based on the patterns ithas come across in the learning environment. Initially the user helps the agentto compose response messages. The agent associates this message to the perceptit recieves with respect to the environment the callee is in. The user maythereafter either choose to make to response system automatic or choose torecieve suggestions from the agent for responses messages and confirm what isto be sent to the caller.
arxiv-4800-145 | Direct Learning of Sparse Changes in Markov Networks by Density Ratio Estimation | http://arxiv.org/pdf/1304.6803v5.pdf | author:Song Liu, John A. Quinn, Michael U. Gutmann, Taiji Suzuki, Masashi Sugiyama category:stat.ML published:2013-04-25 summary:We propose a new method for detecting changes in Markov network structurebetween two sets of samples. Instead of naively fitting two Markov networkmodels separately to the two data sets and figuring out their difference, we\emph{directly} learn the network structure change by estimating the ratio ofMarkov network models. This density-ratio formulation naturally allows us tointroduce sparsity in the network structure change, which highly contributes toenhancing interpretability. Furthermore, computation of the normalization term,which is a critical bottleneck of the naive approach, can be remarkablymitigated. We also give the dual formulation of the optimization problem, whichfurther reduces the computation cost for large-scale Markov networks. Throughexperiments, we demonstrate the usefulness of our method.
arxiv-4800-146 | Modeling Attractiveness and Multiple Clicks in Sponsored Search Results | http://arxiv.org/pdf/1401.0255v1.pdf | author:Dinesh Govindaraj, Tao Wang, S. V. N. Vishwanathan category:cs.IR cs.LG published:2014-01-01 summary:Click models are an important tool for leveraging user feedback, and are usedby commercial search engines for surfacing relevant search results. However,existing click models are lacking in two aspects. First, they do not shareinformation across search results when computing attractiveness. Second, theyassume that users interact with the search results sequentially. Based on ouranalysis of the click logs of a commercial search engine, we observe that thesequential scan assumption does not always hold, especially for sponsoredsearch results. To overcome the above two limitations, we propose a new clickmodel. Our key insight is that sharing information across search results helpsin identifying important words or key-phrases which can then be used toaccurately compute attractiveness of a search result. Furthermore, we arguethat the click probability of a position as well as its attractiveness changesduring a user session and depends on the user's past click experience. Ourmodel seamlessly incorporates the effect of externalities (quality of othersearch results displayed in response to a user query), user fatigue, as well aspre and post-click relevance of a sponsored search result. We propose anefficient one-pass inference scheme and empirically evaluate the performance ofour model via extensive experiments using the click logs of a large commercialsearch engine.
arxiv-4800-147 | Consistent Bounded-Asynchronous Parameter Servers for Distributed ML | http://arxiv.org/pdf/1312.7869v2.pdf | author:Jinliang Wei, Wei Dai, Abhimanu Kumar, Xun Zheng, Qirong Ho, Eric P. Xing category:stat.ML cs.DC cs.LG published:2013-12-30 summary:In distributed ML applications, shared parameters are usually replicatedamong computing nodes to minimize network overhead. Therefore, properconsistency model must be carefully chosen to ensure algorithm's correctnessand provide high throughput. Existing consistency models used ingeneral-purpose databases and modern distributed ML systems are either tooloose to guarantee correctness of the ML algorithms or too strict and thus failto fully exploit the computing power of the underlying distributed system. Many ML algorithms fall into the category of \emph{iterative convergentalgorithms} which start from a randomly chosen initial point and converge tooptima by repeating iteratively a set of procedures. We've found that many suchalgorithms are to a bounded amount of inconsistency and still convergecorrectly. This property allows distributed ML to relax strict consistencymodels to improve system performance while theoretically guarantees algorithmiccorrectness. In this paper, we present several relaxed consistency models forasynchronous parallel computation and theoretically prove their algorithmiccorrectness. The proposed consistency models are implemented in a distributedparameter server and evaluated in the context of a popular ML application:topic modeling.
arxiv-4800-148 | Sparse Recovery with Very Sparse Compressed Counting | http://arxiv.org/pdf/1401.0201v1.pdf | author:Ping Li, Cun-Hui Zhang, Tong Zhang category:stat.ME cs.DS cs.IT cs.LG math.IT published:2013-12-31 summary:Compressed sensing (sparse signal recovery) often encounters nonnegative data(e.g., images). Recently we developed the methodology of using (dense)Compressed Counting for recovering nonnegative K-sparse signals. In this paper,we adopt very sparse Compressed Counting for nonnegative signal recovery. Ourdesign matrix is sampled from a maximally-skewed p-stable distribution (0<p<1),and we sparsify the design matrix so that on average (1-g)-fraction of theentries become zero. The idea is related to very sparse stable randomprojections (Li et al 2006 and Li 2007), the prior work for estimating summarystatistics of the data. In our theoretical analysis, we show that, when p->0, it suffices to use M=K/(1-exp(-gK) log N measurements, so that all coordinates can be recovered inone scan of the coordinates. If g = 1 (i.e., dense design), then M = K log N.If g= 1/K or 2/K (i.e., very sparse design), then M = 1.58K log N or M = 1.16Klog N. This means the design matrix can be indeed very sparse at only a minorinflation of the sample complexity. Interestingly, as p->1, the required number of measurements is essentially M= 2.7K log N, provided g= 1/K. It turns out that this result is a generalworst-case bound.
arxiv-4800-149 | Query-focused Multi-document Summarization: Combining a Novel Topic Model with Graph-based Semi-supervised Learning | http://arxiv.org/pdf/1212.2036v3.pdf | author:Jiwei Li, Sujian Li category:cs.CL cs.IR published:2012-12-10 summary:Graph-based semi-supervised learning has proven to be an effective approachfor query-focused multi-document summarization. The problem of previoussemi-supervised learning is that sentences are ranked without considering thehigher level information beyond sentence level. Researches on generalsummarization illustrated that the addition of topic level can effectivelyimprove the summary quality. Inspired by previous researches, we propose atwo-layer (i.e. sentence layer and topic layer) graph-based semi-supervisedlearning approach. At the same time, we propose a novel topic model which makesfull use of the dependence between sentences and words. Experimental results onDUC and TAC data sets demonstrate the effectiveness of our proposed approach.
arxiv-4800-150 | Gaussian Process Kernels for Pattern Discovery and Extrapolation | http://arxiv.org/pdf/1302.4245v3.pdf | author:Andrew Gordon Wilson, Ryan Prescott Adams category:stat.ML cs.AI stat.ME published:2013-02-18 summary:Gaussian processes are rich distributions over functions, which provide aBayesian nonparametric approach to smoothing and interpolation. We introducesimple closed form kernels that can be used with Gaussian processes to discoverpatterns and enable extrapolation. These kernels are derived by modelling aspectral density -- the Fourier transform of a kernel -- with a Gaussianmixture. The proposed kernels support a broad class of stationary covariances,but Gaussian process inference remains simple and analytic. We demonstrate theproposed kernels by discovering patterns and performing long rangeextrapolation on synthetic examples, as well as atmospheric CO2 trends andairline passenger data. We also show that we can reconstruct standardcovariances within our framework.
arxiv-4800-151 | Medical Image Fusion: A survey of the state of the art | http://arxiv.org/pdf/1401.0166v1.pdf | author:A. P. James, B. V. Dasarathy category:cs.CV cs.AI physics.med-ph published:2013-12-31 summary:Medical image fusion is the process of registering and combining multipleimages from single or multiple imaging modalities to improve the imagingquality and reduce randomness and redundancy in order to increase the clinicalapplicability of medical images for diagnosis and assessment of medicalproblems. Multi-modal medical image fusion algorithms and devices have shownnotable achievements in improving clinical accuracy of decisions based onmedical images. This review article provides a factual listing of methods andsummarizes the broad scientific challenges faced in the field of medical imagefusion. We characterize the medical image fusion research based on (1) thewidely used image fusion methods, (2) imaging modalities, and (3) imaging oforgans that are under study. This review concludes that even though thereexists several open ended technological and scientific challenges, the fusionof medical images has proved to be useful for advancing the clinicalreliability of using medical imaging for medical diagnostics and analysis, andis a scientific discipline that has the potential to significantly grow in thecoming years.
arxiv-4800-152 | Speeding-Up Convergence via Sequential Subspace Optimization: Current State and Future Directions | http://arxiv.org/pdf/1401.0159v1.pdf | author:Michael Zibulevsky category:cs.NA cs.LG published:2013-12-31 summary:This is an overview paper written in style of research proposal. In recentyears we introduced a general framework for large-scale unconstrainedoptimization -- Sequential Subspace Optimization (SESOP) and demonstrated itsusefulness for sparsity-based signal/image denoising, deconvolution,compressive sensing, computed tomography, diffraction imaging, support vectormachines. We explored its combination with Parallel Coordinate Descent andSeparable Surrogate Function methods, obtaining state of the art results inabove-mentioned areas. There are several methods, that are faster than plainSESOP under specific conditions: Trust region Newton method - for problems witheasily invertible Hessian matrix; Truncated Newton method - when fastmultiplication by Hessian is available; Stochastic optimization methods - forproblems with large stochastic-type data; Multigrid methods - for problems withnested multilevel structure. Each of these methods can be further improved bymerge with SESOP. One can also accelerate Augmented Lagrangian method forconstrained optimization problems and Alternating Direction Method ofMultipliers for problems with separable objective function and non-separableconstraints.
arxiv-4800-153 | GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian Processes | http://arxiv.org/pdf/1310.5288v3.pdf | author:Andrew Gordon Wilson, Elad Gilboa, Arye Nehorai, John P. Cunningham category:stat.ML cs.AI cs.LG stat.ME published:2013-10-20 summary:Gaussian processes are typically used for smoothing and interpolation onsmall datasets. We introduce a new Bayesian nonparametric framework -- GPatt --enabling automatic pattern extrapolation with Gaussian processes on largemultidimensional datasets. GPatt unifies and extends highly expressive kernelsand fast exact inference techniques. Without human intervention -- no handcrafting of kernel features, and no sophisticated initialisation procedures --we show that GPatt can solve large scale pattern extrapolation, inpainting, andkernel discovery problems, including a problem with 383400 training points. Wefind that GPatt significantly outperforms popular alternative scalable Gaussianprocess methods in speed and accuracy. Moreover, we discover profounddifferences between each of these methods, suggesting expressive kernels,nonparametric representations, and exact inference are useful for modellinglarge scale multidimensional patterns.
arxiv-4800-154 | Inference of Network Summary Statistics Through Network Denoising | http://arxiv.org/pdf/1310.0423v3.pdf | author:Prakash Balachandran, Edoardo Airoldi, Eric Kolaczyk category:stat.ML math.SP published:2013-10-01 summary:Consider observing an undirected network that is `noisy' in the sense thatthere are Type I and Type II errors in the observation of edges. Such errorscan arise, for example, in the context of inferring gene regulatory networks ingenomics or functional connectivity networks in neuroscience. Given a singleobserved network then, to what extent are summary statistics for that networkrepresentative of their analogues for the true underlying network? Can we infersuch statistics more accurately by taking into account the noise in theobserved network edges? In this paper, we answer both of these questions. In particular, we develop aspectral-based methodology using the adjacency matrix to `denoise' the observednetwork data and produce more accurate inference of the summary statistics ofthe true network. We characterize performance of our methodology through boundson appropriate notions of risk in the $L^2$ sense, and conclude by illustratingthe practical impact of this work on synthetic and real-world data.
arxiv-4800-155 | System Analysis And Design For Multimedia Retrieval Systems | http://arxiv.org/pdf/1401.0131v1.pdf | author:Avinash N Bhute, B B Meshram category:cs.IR cs.CV cs.MM published:2013-12-31 summary:Due to the extensive use of information technology and the recentdevelopments in multimedia systems, the amount of multimedia data available tousers has increased exponentially. Video is an example of multimedia data as itcontains several kinds of data such as text, image, meta-data, visual andaudio. Content based video retrieval is an approach for facilitating thesearching and browsing of large multimedia collections over WWW. In order tocreate an effective video retrieval system, visual perception must be takeninto account. We conjectured that a technique which employs multiple featuresfor indexing and retrieval would be more effective in the discrimination andsearch tasks of videos. In order to validate this, content based indexing andretrieval systems were implemented using color histogram, Texture feature(GLCM), edge density and motion..
arxiv-4800-156 | Black Box Variational Inference | http://arxiv.org/pdf/1401.0118v1.pdf | author:Rajesh Ranganath, Sean Gerrish, David M. Blei category:stat.ML cs.LG stat.CO stat.ME published:2013-12-31 summary:Variational inference has become a widely used method to approximateposteriors in complex latent variables models. However, deriving a variationalinference algorithm generally requires significant model-specific analysis, andthese efforts can hinder and deter us from quickly developing and exploring avariety of models for a problem at hand. In this paper, we present a "blackbox" variational inference algorithm, one that can be quickly applied to manymodels with little additional derivation. Our method is based on a stochasticoptimization of the variational objective where the noisy gradient is computedfrom Monte Carlo samples from the variational distribution. We develop a numberof methods to reduce the variance of the gradient, always maintaining thecriterion that we want to avoid difficult model-based derivations. We evaluateour method against the corresponding black box sampling based methods. We findthat our method reaches better predictive likelihoods much faster than samplingmethods. Finally, we demonstrate that Black Box Variational Inference lets useasily explore a wide space of models by quickly constructing and evaluatingseveral models of longitudinal healthcare data.
arxiv-4800-157 | Controlled Sparsity Kernel Learning | http://arxiv.org/pdf/1401.0116v1.pdf | author:Dinesh Govindaraj, Raman Sankaran, Sreedal Menon, Chiranjib Bhattacharyya category:cs.LG published:2013-12-31 summary:Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been apopular front of research in recent times due to its success in applicationproblems like Object Categorization. This success is due to the fact that MKLhas the ability to choose from a variety of feature kernels to identify theoptimal kernel combination. But the initial formulation of MKL was only able toselect the best of the features and misses out many other informative kernelspresented. To overcome this, the Lp norm based formulation was proposed byKloft et. al. This formulation is capable of choosing a non-sparse set ofkernels through a control parameter p. Unfortunately, the parameter p does nothave a direct meaning to the number of kernels selected. We have observed thatstricter control over the number of kernels selected gives us an edge overthese techniques in terms of accuracy of classification and also helps us tofine tune the algorithms to the time requirements at hand. In this work, wepropose a Controlled Sparsity Kernel Learning (CSKL) formulation that canstrictly control the number of kernels which we wish to select. The CSKLformulation introduces a parameter t which directly corresponds to the numberof kernels selected. It is important to note that a search in t space is finiteand fast as compared to p. We have also provided an efficient Reduced GradientDescent based algorithm to solve the CSKL formulation, which is proven toconverge. Through our experiments on the Caltech101 Object Categorizationdataset, we have also shown that one can achieve better accuracies than theprevious formulations through the right choice of t.
arxiv-4800-158 | PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction | http://arxiv.org/pdf/1401.0104v1.pdf | author:Yukun Bao, Tao Xiong, Zhongyi Hu category:cs.AI cs.LG cs.NE stat.ML published:2013-12-31 summary:Multi-step-ahead time series prediction is one of the most challengingresearch topics in the field of time series modeling and prediction, and iscontinually under research. Recently, the multiple-input severalmultiple-outputs (MISMO) modeling strategy has been proposed as a promisingalternative for multi-step-ahead time series prediction, exhibiting advantagescompared with the two currently dominating strategies, the iterated and thedirect strategies. Built on the established MISMO strategy, this study proposesa particle swarm optimization (PSO)-based MISMO modeling strategy, which iscapable of determining the number of sub-models in a self-adaptive mode, withvarying prediction horizons. Rather than deriving crisp divides with equal-sizes prediction horizons from the established MISMO, the proposed PSO-MISMOstrategy, implemented with neural networks, employs a heuristic to createflexible divides with varying sizes of prediction horizons and to generatecorresponding sub-models, providing considerable flexibility in modelconstruction, which has been validated with simulated and real datasets.
arxiv-4800-159 | A Novel Approach For Generating Face Template Using Bda | http://arxiv.org/pdf/1401.0092v1.pdf | author:Shraddha S. Shinde, Prof. Anagha P. Khedkar category:cs.CV published:2013-12-31 summary:In identity management system, commonly used biometric recognition systemneeds attention towards issue of biometric template protection as far as morereliable solution is concerned. In view of this biometric template protectionalgorithm should satisfy security, discriminability and cancelability. As nosingle template protection method is capable of satisfying the basicrequirements, a novel technique for face template generation and protection isproposed. The novel approach is proposed to provide security and accuracy innew user enrollment as well as authentication process. This novel techniquetakes advantage of both the hybrid approach and the binary discriminantanalysis algorithm. This algorithm is designed on the basis of randomprojection, binary discriminant analysis and fuzzy commitment scheme. Threepublicly available benchmark face databases are used for evaluation. Theproposed novel technique enhances the discriminability and recognition accuracyby 80% in terms of matching score of the face images and provides highsecurity.
arxiv-4800-160 | Fused Multiple Graphical Lasso | http://arxiv.org/pdf/1209.2139v2.pdf | author:Sen Yang, Zhaosong Lu, Xiaotong Shen, Peter Wonka, Jieping Ye category:cs.LG stat.ML published:2012-09-10 summary:In this paper, we consider the problem of estimating multiple graphicalmodels simultaneously using the fused lasso penalty, which encourages adjacentgraphs to share similar structures. A motivating example is the analysis ofbrain networks of Alzheimer's disease using neuroimaging data. Specifically, wemay wish to estimate a brain network for the normal controls (NC), a brainnetwork for the patients with mild cognitive impairment (MCI), and a brainnetwork for Alzheimer's patients (AD). We expect the two brain networks for NCand MCI to share common structures but not to be identical to each other;similarly for the two brain networks for MCI and AD. The proposed formulationcan be solved using a second-order method. Our key technical contribution is toestablish the necessary and sufficient condition for the graphs to bedecomposable. Based on this key property, a simple screening rule is presented,which decomposes the large graphs into small subgraphs and allows an efficientestimation of multiple independent (small) subgraphs, dramatically reducing thecomputational cost. We perform experiments on both synthetic and real data; ourresults demonstrate the effectiveness and efficiency of the proposed approach.
arxiv-4800-161 | Approximating the Bethe partition function | http://arxiv.org/pdf/1401.0044v1.pdf | author:Adrian Weller, Tony Jebara category:cs.LG published:2013-12-30 summary:When belief propagation (BP) converges, it does so to a stationary point ofthe Bethe free energy $F$, and is often strikingly accurate. However, it mayconverge only to a local optimum or may not converge at all. An algorithm wasrecently introduced for attractive binary pairwise MRFs which is guaranteed toreturn an $\epsilon$-approximation to the global minimum of $F$ in polynomialtime provided the maximum degree $\Delta=O(\log n)$, where $n$ is the number ofvariables. Here we significantly improve this algorithm and derive severalresults including a new approach based on analyzing first derivatives of $F$,which leads to performance that is typically far superior and yields a fullypolynomial-time approximation scheme (FPTAS) for attractive models without anydegree restriction. Further, the method applies to general (non-attractive)models, though with no polynomial time guarantee in this case, leading to theimportant result that approximating $\log$ of the Bethe partition function,$\log Z_B=-\min F$, for a general model to additive $\epsilon$-accuracy may bereduced to a discrete MAP inference problem. We explore an application topredicting equipment failure on an urban power network and demonstrate that theBethe approximation can perform well even when BP fails to converge.
arxiv-4800-162 | Evolutionary Design of Numerical Methods: Generating Finite Difference and Integration Schemes by Differential Evolution | http://arxiv.org/pdf/1312.7852v1.pdf | author:C. D. Erdbrink, V. V. Krzhizhanovskaya, P. M. A. Sloot category:cs.NE cs.NA published:2013-12-30 summary:Classical and new numerical schemes are generated using evolutionarycomputing. Differential Evolution is used to find the coefficients of finitedifference approximations of function derivatives, and of single and multi-stepintegration methods. The coefficients are reverse engineered based on samplesfrom a target function and its derivative used for training. The Runge-Kuttaschemes are trained using the order condition equations. An appealing featureof the evolutionary method is the low number of model parameters. Thepopulation size, termination criterion and number of training points aredetermined in a sensitivity analysis. Computational results show good agreementbetween evolved and analytical coefficients. In particular, a new fifth-orderRunge-Kutta scheme is computed which adheres to the order conditions with a sumof absolute errors of order 10^-14. Execution of the evolved schemes proved theintended orders of accuracy. The outcome of this study is valuable for futuredevelopments in the design of complex numerical methods that are out of reachby conventional means.
arxiv-4800-163 | A Fused Elastic Net Logistic Regression Model for Multi-Task Binary Classification | http://arxiv.org/pdf/1312.7750v1.pdf | author:Venelin Mitov, Manfred Claassen category:stat.ML published:2013-12-30 summary:Multi-task learning has shown to significantly enhance the performance ofmultiple related learning tasks in a variety of situations. We present thefused logistic regression, a sparse multi-task learning approach for binaryclassification. Specifically, we introduce sparsity inducing penalties overparameter differences of related logistic regression models to encodesimilarity across related tasks. The resulting joint learning task is cast intoa form that lends itself to be efficiently optimized with a recursive variantof the alternating direction method of multipliers. We show results onsynthetic data and describe the regime of settings where our multi-taskapproach achieves significant improvements over the single task learningapproach and discuss the implications on applying the fused logistic regressionin different real world settings.
arxiv-4800-164 | Total variation regularization for manifold-valued data | http://arxiv.org/pdf/1312.7710v1.pdf | author:Andreas Weinmann, Laurent Demaret, Martin Storath category:math.OC cs.CV physics.med-ph published:2013-12-30 summary:We consider total variation minimization for manifold valued data. We proposea cyclic proximal point algorithm and a parallel proximal point algorithm tominimize TV functionals with $\ell^p$-type data terms in the manifold case.These algorithms are based on iterative geodesic averaging which makes themeasily applicable to a large class of data manifolds. As an application, weconsider denoising images which take their values in a manifold. We apply ouralgorithms to diffusion tensor images, interferometric SAR images as well assphere and cylinder valued images. For the class of Cartan-Hadamard manifolds(which includes the data space in diffusion tensor imaging) we show theconvergence of the proposed TV minimizing algorithms to a global minimizer.
arxiv-4800-165 | A Multi-Orientation Analysis Approach to Retinal Vessel Tracking | http://arxiv.org/pdf/1212.3530v5.pdf | author:Erik Bekkers, Remco Duits, Tos Berendschot, Bart ter Haar Romeny category:cs.CV published:2012-12-14 summary:This paper presents a method for retinal vasculature extraction based onbiologically inspired multi-orientation analysis. We apply multi-orientationanalysis via so-called invertible orientation scores, modeling the corticalcolumns in the visual system of higher mammals. This allows us to genericallydeal with many hitherto complex problems inherent to vessel tracking, such ascrossings, bifurcations, parallel vessels, vessels of varying widths andvessels with high curvature. Our approach applies tracking in invertibleorientation scores via a novel geometrical principle for curve optimization inthe Euclidean motion group SE(2). The method runs fully automatically andprovides a detailed model of the retinal vasculature, which is crucial as asound basis for further quantitative analysis of the retina, especially inscreening applications.
arxiv-4800-166 | Feature vector regularization in machine learning | http://arxiv.org/pdf/1212.4569v2.pdf | author:Yue Fan, Louise Raphael, Mark Kon category:stat.ML published:2012-12-19 summary:Problems in machine learning (ML) can involve noisy input data, and MLclassification methods have reached limiting accuracies when based on standardML data sets consisting of feature vectors and their classes. Greater accuracywill require incorporation of prior structural information on data intolearning. We study methods to regularize feature vectors (unsupervisedregularization methods), analogous to supervised regularization for estimatingfunctions in ML. We study regularization (denoising) of ML feature vectorsusing Tikhonov and other regularization methods for functions on ${\bf R}^n$. Afeature vector ${\bf x}=(x_1,\ldots,x_n)=\{x_q\}_{q=1}^n$ is viewed as afunction of its index $q$, and smoothed using prior information on itsstructure. This can involve a penalty functional on feature vectors analogousto those in statistical learning, or use of proximity (e.g. graph) structure onthe set of indices. Such feature vector regularization inherits a property fromfunction denoising on ${\bf R}^n$, in that accuracy is non-monotonic in thedenoising (regularization) parameter $\alpha$. Under some assumptions about thenoise level and the data structure, we show that the best reconstructionaccuracy also occurs at a finite positive $\alpha$ in index spaces with graphstructures. We adapt two standard function denoising methods used on ${\bfR}^n$, local averaging and kernel regression. In general the index space can beany discrete set with a notion of proximity, e.g. a metric space, a subset of${\bf R}^n$, or a graph/network, with feature vectors as functions with somenotion of continuity. We show this improves feature vector recovery, and thusthe subsequent classification or regression done on them. We give an example ingene expression analysis for cancer classification with the genome as an indexspace and network structure based protein-protein interactions.
arxiv-4800-167 | Response-Based Approachability and its Application to Generalized No-Regret Algorithms | http://arxiv.org/pdf/1312.7658v1.pdf | author:Andrey Bernstein, Nahum Shimkin category:cs.LG cs.GT published:2013-12-30 summary:Approachability theory, introduced by Blackwell (1956), provides fundamentalresults on repeated games with vector-valued payoffs, and has been usefullyapplied since in the theory of learning in games and to learning algorithms inthe online adversarial setup. Given a repeated game with vector payoffs, atarget set $S$ is approachable by a certain player (the agent) if he can ensurethat the average payoff vector converges to that set no matter what hisadversary opponent does. Blackwell provided two equivalent sets of conditionsfor a convex set to be approachable. The first (primary) condition is ageometric separation condition, while the second (dual) condition requires thatthe set be {\em non-excludable}, namely that for every mixed action of theopponent there exists a mixed action of the agent (a {\em response}) such thatthe resulting payoff vector belongs to $S$. Existing approachability algorithmsrely on the primal condition and essentially require to compute at each stage aprojection direction from a given point to $S$. In this paper, we introduce anapproachability algorithm that relies on Blackwell's {\em dual} condition.Thus, rather than projection, the algorithm relies on computation of theresponse to a certain action of the opponent at each stage. The utility of theproposed algorithm is demonstrated by applying it to certain generalizations ofthe classical regret minimization problem, which include regret minimizationwith side constraints and regret minimization for global cost functions. Inthese problems, computation of the required projections is generally complexbut a response is readily obtainable.
arxiv-4800-168 | Generative Maximum Entropy Learning for Multiclass Classification | http://arxiv.org/pdf/1205.0651v3.pdf | author:Ambedkar Dukkipati, Gaurav Pandey, Debarghya Ghoshdastidar, Paramita Koley, D. M. V. Satya Sriram category:cs.IT cs.LG math.IT published:2012-05-03 summary:Maximum entropy approach to classification is very well studied in appliedstatistics and machine learning and almost all the methods that exists inliterature are discriminative in nature. In this paper, we introduce a maximumentropy classification method with feature selection for large dimensional datasuch as text datasets that is generative in nature. To tackle the curse ofdimensionality of large data sets, we employ conditional independenceassumption (Naive Bayes) and we perform feature selection simultaneously, byenforcing a `maximum discrimination' between estimated class conditionaldensities. For two class problems, in the proposed method, we use Jeffreys($J$) divergence to discriminate the class conditional densities. To extend ourmethod to the multi-class case, we propose a completely new approach byconsidering a multi-distribution divergence: we replace Jeffreys divergence byJensen-Shannon ($JS$) divergence to discriminate conditional densities ofmultiple classes. In order to reduce computational complexity, we employ amodified Jensen-Shannon divergence ($JS_{GM}$), based on AM-GM inequality. Weshow that the resulting divergence is a natural generalization of Jeffreysdivergence to a multiple distributions case. As far as the theoreticaljustifications are concerned we show that when one intends to select the bestfeatures in a generative maximum entropy approach, maximum discrimination using$J-$divergence emerges naturally in binary classification. Performance andcomparative study of the proposed algorithms have been demonstrated on largedimensional text and gene expression datasets that show our methods scale upvery well with large dimensional datasets.
arxiv-4800-169 | Structure-Aware Dynamic Scheduler for Parallel Machine Learning | http://arxiv.org/pdf/1312.5766v2.pdf | author:Seunghak Lee, Jin Kyu Kim, Qirong Ho, Garth A. Gibson, Eric P. Xing category:stat.ML cs.LG published:2013-12-19 summary:Training large machine learning (ML) models with many variables or parameterscan take a long time if one employs sequential procedures even with stochasticupdates. A natural solution is to turn to distributed computing on a cluster;however, naive, unstructured parallelization of ML algorithms does not usuallylead to a proportional speedup and can even result in divergence, becausedependencies between model elements can attenuate the computational gains fromparallelization and compromise correctness of inference. Recent efforts towardthis issue have benefited from exploiting the static, a priori block structuresresiding in ML algorithms. In this paper, we take this path further byexploring the dynamic block structures and workloads therein present during MLprogram execution, which offers new opportunities for improving convergence,correctness, and load balancing in distributed ML. We propose and showcase ageneral-purpose scheduler, STRADS, for coordinating distributed updates in MLalgorithms, which harnesses the aforementioned opportunities in a systematicway. We provide theoretical guarantees for our scheduler, and demonstrate itsefficacy versus static block structures on Lasso and Matrix Factorization.
arxiv-4800-170 | Consensus Sequence Segmentation | http://arxiv.org/pdf/1308.3839v2.pdf | author:Tamal Chowdhury, Rabindra Rakshit, Arko Banerjee category:cs.CL 68T10 published:2013-08-18 summary:In this paper we introduce a method to detect words or phrases in a givensequence of alphabets without knowing the lexicon. Our linear time unsupervisedalgorithm relies entirely on statistical relationships among alphabets in theinput sequence to detect location of word boundaries. We compare our algorithmto previous approaches from unsupervised sequence segmentation literature andprovide superior segmentation over number of benchmarks.
arxiv-4800-171 | Ant Colony Optimization and Hypergraph Covering Problems | http://arxiv.org/pdf/1105.2894v2.pdf | author:Ankit Pat, Ashish Ranjan Hota category:cs.NE published:2011-05-14 summary:Ant Colony Optimization (ACO) is a very popular metaheuristic for solvingcomputationally hard combinatorial optimization problems. Runtime analysis ofACO with respect to various pseudo-boolean functions and different graph basedcombinatorial optimization problems has been taken up in recent years. In thispaper, we investigate the runtime behavior of an MMAS*(Max-Min Ant System) ACOalgorithm on some well known hypergraph covering problems that are NP-Hard. Inparticular, we have addressed the Minimum Edge Cover problem, the MinimumVertex Cover problem and the Maximum Weak- Independent Set problem. Theinfluence of pheromone values and heuristic information on the running time isanalysed. The results indicate that the heuristic information has greaterimpact towards improving the expected optimization time as compared topheromone values. For certain instances of hypergraphs, we show that the MMAS*algorithm gives a constant order expected optimization time when the dominanceof heuristic information is suitably increased.
arxiv-4800-172 | A Novel Method for Automatic Segmentation of Brain Tumors in MRI Images | http://arxiv.org/pdf/1312.7573v1.pdf | author:Saeid Fazli, Parisa Nadirkhanlou category:cs.CV published:2013-12-29 summary:The brain tumor segmentation on MRI images is a very difficult and importanttask which is used in surgical and medical planning and assessments. If expertsdo the segmentation manually with their own medical knowledge, it will betime-consuming. Therefore, researchers propose methods and systems which can dothe segmentation automatically and without any interference. In this article,an unsupervised automatic method for brain tumor segmentation on MRI images ispresented. In this method, at first in the pre-processing level, the extraparts which are outside the skull and don't have any helpful information areremoved and then anisotropic diffusion filter with 8-connected neighborhood isapplied to the MRI images to remove noise. By applying the fast boundingbox(FBB) algorithm, the tumor area is displayed on the MRI image with abounding box and the central part is selected as sample points for training ofa One Class SVM classifier. A database is also provided by the Zanjan MRICenter. The MRI images are related to 10 patients who have brain tumor. 100T2-weighted MRI images are used in this study. Experimental results show thehigh precision and dependability of the proposed algorithm. The results arealso highly helpful for specialists and radiologists to easily estimate thesize and position of a tumor.
arxiv-4800-173 | Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition | http://arxiv.org/pdf/1312.7570v1.pdf | author:Stefan Mathe, Cristian Sminchisescu category:cs.CV published:2013-12-29 summary:Systems based on bag-of-words models from image features collected at maximaof sparse interest point operators have been used successfully for bothcomputer visual object and action recognition tasks. While the sparse,interest-point based approach to recognition is not inconsistent with visualprocessing in biological systems that operate in `saccade and fixate' regimes,the methodology and emphasis in the human and the computer vision communitiesremains sharply distinct. Here, we make three contributions aiming to bridgethis gap. First, we complement existing state-of-the art large scale dynamiccomputer vision annotated datasets like Hollywood-2 and UCF Sports with humaneye movements collected under the ecological constraints of the visual actionrecognition task. To our knowledge these are the first large human eye trackingdatasets to be collected and made publicly available for video,vision.imar.ro/eyetracking (497,107 frames, each viewed by 16 subjects), uniquein terms of their (a) large scale and computer vision relevance, (b) dynamic,video stimuli, (c) task control, as opposed to free-viewing. Second, weintroduce novel sequential consistency and alignment measures, which underlinethe remarkable stability of patterns of visual search among subjects. Third, weleverage the significant amount of collected data in order to pursue studiesand build automatic, end-to-end trainable computer vision systems based onhuman eye movements. Our studies not only shed light on the differences betweencomputer vision spatio-temporal interest point image sampling strategies andthe human fixations, as well as their impact for visual recognitionperformance, but also demonstrate that human fixations can be accuratelypredicted, and when used in an end-to-end automatic system, leveraging some ofthe advanced computer vision practice, can lead to state of the art results.
arxiv-4800-174 | Nonparametric Inference For Density Modes | http://arxiv.org/pdf/1312.7567v1.pdf | author:Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman category:stat.ME cs.LG 62G07 published:2013-12-29 summary:We derive nonparametric confidence intervals for the eigenvalues of theHessian at modes of a density estimate. This provides information about thestrength and shape of modes and can also be used as a significance test. We usea data-splitting approach in which potential modes are identified using thefirst half of the data and inference is done with the second half of the data.To get valid confidence sets for the eigenvalues, we use a bootstrap based onan elementary-symmetric-polynomial (ESP) transformation. This leads to validbootstrap confidence sets regardless of any multiplicities in the eigenvalues.We also suggest a new method for bandwidth selection, namely, choosing thebandwidth to maximize the number of significant modes. We show by example thatthis method works well. Even when the true distribution is singular, and hencedoes not have a density, (in which case cross validation chooses a zerobandwidth), our method chooses a reasonable bandwidth.
arxiv-4800-175 | Implementation of Hand Detection based Techniques for Human Computer Interaction | http://arxiv.org/pdf/1312.7560v1.pdf | author:Amiraj Dhawan, Vipul Honrao category:cs.CV cs.HC published:2013-12-29 summary:The computer industry is developing at a fast pace. With this developmentalmost all of the fields under computers have advanced in the past couple ofdecades. But the same technology is being used for human computer interactionthat was used in 1970s. Even today the same type of keyboard and mouse is usedfor interacting with computer systems. With the recent boom in the mobilesegment touchscreens have become popular for interaction with cell phones. Butthese touchscreens are rarely used on traditional systems. This paper tries tointroduce methods for human computer interaction using the users hand which canbe used both on traditional computer platforms as well as cell phones. Themethods explain how the users detected hand can be used as input forapplications and also explain applications that can take advantage of this typeof interaction mechanism.
arxiv-4800-176 | A Novel Retinal Vessel Segmentation Based On Histogram Transformation Using 2-D Morlet Wavelet and Supervised Classification | http://arxiv.org/pdf/1312.7557v1.pdf | author:Saeid Fazli, Sevin Samadi category:cs.CV published:2013-12-29 summary:The appearance and structure of blood vessels in retinal images have animportant role in diagnosis of diseases. This paper proposes a method forautomatic retinal vessel segmentation. In this work, a novel preprocessingbased on local histogram equalization is used to enhance the original imagethen pixels are classified as vessel and non-vessel using a classifier. Forthis classification, special feature vectors are organized based on responsesto Morlet wavelet. Morlet wavelet is a continues transform which has theability to filter existing noises after preprocessing. Bayesian classifier isused and Gaussian mixture model (GMM) is its likelihood function. Theprobability distributions are approximated according to training set of manualthat has been segmented by a specialist. After this, morphological transformsare used in different directions to make the existing discontinuities uniformon the DRIVE database, it achieves the accuracy about 0.9571 which shows thatit is an accurate method among the available ones for retinal vesselsegmentation.
arxiv-4800-177 | A Fast Hadamard Transform for Signals with Sub-linear Sparsity in the Transform Domain | http://arxiv.org/pdf/1310.1803v2.pdf | author:Robin Scheibler, Saeid Haghighatshoar, Martin Vetterli category:cs.IT math.IT stat.ML published:2013-10-07 summary:A new iterative low complexity algorithm has been presented for computing theWalsh-Hadamard transform (WHT) of an $N$ dimensional signal with a $K$-sparseWHT, where $N$ is a power of two and $K = O(N^\alpha)$, scales sub-linearly in$N$ for some $0 < \alpha < 1$. Assuming a random support model for the non-zerotransform domain components, the algorithm reconstructs the WHT of the signalwith a sample complexity $O(K \log_2(\frac{N}{K}))$, a computational complexity$O(K\log_2(K)\log_2(\frac{N}{K}))$ and with a very high probabilityasymptotically tending to 1. The approach is based on the subsampling (aliasing) property of the WHT,where by a carefully designed subsampling of the time domain signal, one caninduce a suitable aliasing pattern in the transform domain. By treating thealiasing patterns as parity-check constraints and borrowing ideas from erasurecorrecting sparse-graph codes, the recovery of the non-zero spectral values hasbeen formulated as a belief propagation (BP) algorithm (peeling decoding) overa sparse-graph code for the binary erasure channel (BEC). Tools from codingtheory are used to analyze the asymptotic performance of the algorithm in thevery sparse ($\alpha\in(0,\frac{1}{3}]$) and the less sparse($\alpha\in(\frac{1}{3},1)$) regime.
arxiv-4800-178 | Learning Temporal Logical Properties Discriminating ECG models of Cardiac Arrhytmias | http://arxiv.org/pdf/1312.7523v1.pdf | author:Ezio Bartocci, Luca Bortolussi, Guido Sanguinetti category:cs.LO cs.CV q-bio.QM published:2013-12-29 summary:We present a novel approach to learn the formulae characterising the emergentbehaviour of a dynamical system from system observations. At a high level, theapproach starts by devising a statistical dynamical model of the system whichoptimally fits the observations. We then propose general optimisationstrategies for selecting high support formulae (under the learnt model of thesystem) either within a discrete set of formulae of bounded complexity, or aparametric family of formulae. We illustrate and apply the methodology on anin-depth case study of characterising cardiac malfunction fromelectro-cardiogram data, where our approach enables us to quantitativelydetermine the diagnostic power of a formula in discriminating between differentcardiac conditions.
arxiv-4800-179 | A Novel Scheme for Generating Secure Face Templates Using BDA | http://arxiv.org/pdf/1312.7511v1.pdf | author:Shraddha S. Shinde, Prof. Anagha P. Khedkar category:cs.CV cs.CR published:2013-12-29 summary:In identity management system, frequently used biometric recognition systemneeds awareness towards issue of protecting biometric template as far as morereliable solution is apprehensive. In sight of this biometric templateprotection algorithm should gratify the basic requirements viz. security,discriminability and cancelability. As no single template protection method iscapable of satisfying these requirements, a novel scheme for face templategeneration and protection is proposed. The novel scheme is proposed to providesecurity and accuracy in new user enrolment and authentication process. Thisnovel scheme takes advantage of both the hybrid approach and the binarydiscriminant analysis algorithm. This algorithm is designed on the basis ofrandom projection, binary discriminant analysis and fuzzy commitment scheme.Publicly available benchmark face databases (FERET, FRGC, CMU-PIE) and otherdatasets are used for evaluation. The proposed novel scheme enhances thediscriminability and recognition accuracy in terms of matching score of theface images for each stage and provides high security against potential attacksnamely brute force and smart attacks. In this paper, we discuss results viz.averages matching score, computation time and security for hybrid approach andnovel approach.
arxiv-4800-180 | A General Algorithm for Deciding Transportability of Experimental Results | http://arxiv.org/pdf/1312.7485v1.pdf | author:Elias Bareinboim, Judea Pearl category:cs.AI stat.ME stat.ML published:2013-12-29 summary:Generalizing empirical findings to new environments, settings, or populationsis essential in most scientific explorations. This article treats a particularproblem of generalizability, called "transportability", defined as a license totransfer information learned in experimental studies to a different population,on which only observational studies can be conducted. Given a set ofassumptions concerning commonalities and differences between the twopopulations, Pearl and Bareinboim (2011) derived sufficient conditions thatpermit such transfer to take place. This article summarizes their findings andsupplements them with an effective procedure for deciding when and howtransportability is feasible. It establishes a necessary and sufficientcondition for deciding when causal effects in the target population areestimable from both the statistical information available and the causalinformation transferred from the experiments. The article further provides acomplete algorithm for computing the transport formula, that is, a way ofcombining observational and experimental information to synthesize bias-freeestimate of the desired causal relation. Finally, the article examines thedifferences between transportability and other variants of generalizability.
arxiv-4800-181 | Generalized Ambiguity Decomposition for Understanding Ensemble Diversity | http://arxiv.org/pdf/1312.7463v1.pdf | author:Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran, Shrikanth S. Narayanan category:stat.ML cs.CV cs.LG I.5 published:2013-12-28 summary:Diversity or complementarity of experts in ensemble pattern recognition andinformation processing systems is widely-observed by researchers to be crucialfor achieving performance improvement upon fusion. Understanding this linkbetween ensemble diversity and fusion performance is thus an important researchquestion. However, prior works have theoretically characterized ensemblediversity and have linked it with ensemble performance in very restrictedsettings. We present a generalized ambiguity decomposition (GAD) theorem as abroad framework for answering these questions. The GAD theorem applies to ageneric convex ensemble of experts for any arbitrary twice-differentiable lossfunction. It shows that the ensemble performance approximately decomposes intoa difference of the average expert performance and the diversity of theensemble. It thus provides a theoretical explanation for theempirically-observed benefit of fusing outputs from diverse classifiers andregressors. It also provides a loss function-dependent, ensemble-dependent, anddata-dependent definition of diversity. We present extensions of thisdecomposition to common regression and classification loss functions, andreport a simulation-based analysis of the diversity term and the accuracy ofthe decomposition. We finally present experiments on standard patternrecognition data sets which indicate the accuracy of the decomposition forreal-world classification and regression problems.
arxiv-4800-182 | Stopping Rules for Bag-of-Words Image Search and Its Application in Appearance-Based Localization | http://arxiv.org/pdf/1312.7414v1.pdf | author:Kiana Hajebi, Hong Zhang category:cs.CV cs.RO published:2013-12-28 summary:We propose a technique to improve the search efficiency of the bag-of-words(BoW) method for image retrieval. We introduce a notion of difficulty for theimage matching problems and propose methods that reduce the amount ofcomputations required for the feature vector-quantization task in BoW byexploiting the fact that easier queries need less computational resources.Measuring the difficulty of a query and stopping the search accordingly isformulated as a stopping problem. We introduce stopping rules that terminatethe image search depending on the difficulty of each query, therebysignificantly reducing the computational cost. Our experimental results showthe effectiveness of our approach when it is applied to appearance-basedlocalization problem.
arxiv-4800-183 | Positive definite matrices and the S-divergence | http://arxiv.org/pdf/1110.1773v4.pdf | author:Suvrit Sra category:math.FA stat.ML published:2011-10-08 summary:Positive definite matrices abound in a dazzling variety of applications. Thisubiquity can be in part attributed to their rich geometric structure: positivedefinite matrices form a self-dual convex cone whose strict interior is aRiemannian manifold. The manifold view is endowed with a "natural" distancefunction while the conic view is not. Nevertheless, drawing motivation from theconic view, we introduce the S-Divergence as a "natural" distance-like functionon the open cone of positive definite matrices. We motivate the S-divergencevia a sequence of results that connect it to the Riemannian distance. Inparticular, we show that (a) this divergence is the square of a distance; and(b) that it has several geometric properties similar to those of the Riemanniandistance, though without being computationally as demanding. The S-divergenceis even more intriguing: although nonconvex, we can still compute matrix meansand medians using it to global optimality. We complement our results with somenumerical experiments illustrating our theorems and our optimization algorithmfor computing matrix medians.
arxiv-4800-184 | lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits | http://arxiv.org/pdf/1312.7308v1.pdf | author:Kevin Jamieson, Matthew Malloy, Robert Nowak, Sébastien Bubeck category:stat.ML cs.LG published:2013-12-27 summary:The paper proposes a novel upper confidence bound (UCB) procedure foridentifying the arm with the largest mean in a multi-armed bandit game in thefixed confidence setting using a small number of total samples. The procedurecannot be improved in the sense that the number of samples required to identifythe best arm is within a constant factor of a lower bound based on the law ofthe iterated logarithm (LIL). Inspired by the LIL, we construct our confidencebounds to explicitly account for the infinite time horizon of the algorithm. Inaddition, by using a novel stopping time for the algorithm we avoid a unionbound over the arms that has been observed in other UCB-type algorithms. Weprove that the algorithm is optimal up to constants and also show throughsimulations that it provides superior performance with respect to thestate-of-the-art.
arxiv-4800-185 | A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization | http://arxiv.org/pdf/1212.2006v2.pdf | author:Jiwei Li, Sujian Li category:cs.CL cs.IR published:2012-12-10 summary:Both supervised learning methods and LDA based topic model have beensuccessfully applied in the field of query focused multi-documentsummarization. In this paper, we propose a novel supervised approach that canincorporate rich sentence features into Bayesian topic models in a principledway, thus taking advantages of both topic model and feature based supervisedlearning methods. Experiments on TAC2008 and TAC2009 demonstrate theeffectiveness of our approach.
arxiv-4800-186 | A Comprehensive Approach to Universal Piecewise Nonlinear Regression Based on Trees | http://arxiv.org/pdf/1311.6392v2.pdf | author:N. Denizcan Vanli, Suleyman S. Kozat category:cs.LG stat.ML published:2013-11-25 summary:In this paper, we investigate adaptive nonlinear regression and introducetree based piecewise linear regression algorithms that are highly efficient andprovide significantly improved performance with guaranteed upper bounds in anindividual sequence manner. We use a tree notion in order to partition thespace of regressors in a nested structure. The introduced algorithms adapt notonly their regression functions but also the complete tree structure whileachieving the performance of the "best" linear mixture of a doubly exponentialnumber of partitions, with a computational complexity only polynomial in thenumber of nodes of the tree. While constructing these algorithms, we also avoidusing any artificial "weighting" of models (with highly data dependentparameters) and, instead, directly minimize the final regression error, whichis the ultimate performance goal. The introduced methods are generic such thatthey can readily incorporate different tree construction methods such as randomtrees in their framework and can use different regressor or partitioningfunctions as demonstrated in the paper.
arxiv-4800-187 | Quality Estimation of English-Hindi Outputs using Naive Bayes Classifier | http://arxiv.org/pdf/1312.7223v1.pdf | author:Rashmi Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL published:2013-12-27 summary:In this paper we present an approach for estimating the quality of machinetranslation system. There are various methods for estimating the quality ofoutput sentences, but in this paper we focus on Na\"ive Bayes classifier tobuild model using features which are extracted from the input sentences. Thesefeatures are used for finding the likelihood of each of the sentences of thetraining data which are then further used for determining the scores of thetest data. On the basis of these scores we determine the class labels of thetest data.
arxiv-4800-188 | Sub-Classifier Construction for Error Correcting Output Code Using Minimum Weight Perfect Matching | http://arxiv.org/pdf/1312.7179v1.pdf | author:Patoomsiri Songsiri, Thimaporn Phetkaew, Ryutaro Ichise, Boonserm Kijsirikul category:cs.LG cs.IT math.IT published:2013-12-27 summary:Multi-class classification is mandatory for real world problems and one ofpromising techniques for multi-class classification is Error Correcting OutputCode. We propose a method for constructing the Error Correcting Output Code toobtain the suitable combination of positive and negative classes encoded torepresent binary classifiers. The minimum weight perfect matching algorithm isapplied to find the optimal pairs of subset of classes by using thegeneralization performance as a weighting criterion. Based on our method, eachsubset of classes with positive and negative labels is appropriately combinedfor learning the binary classifiers. Experimental results show that ourtechnique gives significantly higher performance compared to traditionalmethods including the dense random code and the sparse random code both interms of accuracy and classification times. Moreover, our method requiressignificantly smaller number of binary classifiers while maintaining accuracycompared to the One-Versus-One.
arxiv-4800-189 | Near-separable Non-negative Matrix Factorization with $\ell_1$- and Bregman Loss Functions | http://arxiv.org/pdf/1312.7167v1.pdf | author:Abhishek Kumar, Vikas Sindhwani category:stat.ML cs.CV cs.LG published:2013-12-27 summary:Recently, a family of tractable NMF algorithms have been proposed under theassumption that the data matrix satisfies a separability condition Donoho &Stodden (2003); Arora et al. (2012). Geometrically, this condition reformulatesthe NMF problem as that of finding the extreme rays of the conical hull of afinite set of vectors. In this paper, we develop several extensions of theconical hull procedures of Kumar et al. (2013) for robust ($\ell_1$)approximations and Bregman divergences. Our methods inherit all the advantagesof Kumar et al. (2013) including scalability and noise-tolerance. We show thaton foreground-background separation problems in computer vision, robustnear-separable NMFs match the performance of Robust PCA, considered state ofthe art on these problems, with an order of magnitude faster training time. Wealso demonstrate applications in exemplar selection settings.
arxiv-4800-190 | Lesion Border Detection in Dermoscopy Images Using Ensembles of Thresholding Methods | http://arxiv.org/pdf/1312.7345v1.pdf | author:M. Emre Celebi, Quan Wen, Sae Hwang, Hitoshi Iyatomi, Gerald Schaefer category:cs.CV I.4.6 published:2013-12-26 summary:Dermoscopy is one of the major imaging modalities used in the diagnosis ofmelanoma and other pigmented skin lesions. Due to the difficulty andsubjectivity of human interpretation, automated analysis of dermoscopy imageshas become an important research area. Border detection is often the first stepin this analysis. In many cases, the lesion can be roughly separated from thebackground skin using a thresholding method applied to the blue channel.However, no single thresholding method appears to be robust enough tosuccessfully handle the wide variety of dermoscopy images encountered inclinical practice. In this paper, we present an automated method for detectinglesion borders in dermoscopy images using ensembles of thresholding methods.Experiments on a difficult set of 90 images demonstrate that the proposedmethod is robust, fast, and accurate when compared to nine state-of-the-artmethods.
arxiv-4800-191 | Finding More Relevance: Propagating Similarity on Markov Random Field for Image Retrieval | http://arxiv.org/pdf/1312.7085v1.pdf | author:Peng Lu, Xujun Peng, Xinshan Zhu, Xiaojie Wang category:cs.CV published:2013-12-26 summary:To effectively retrieve objects from large corpus with high accuracy is achallenge task. In this paper, we propose a method that propagates visualfeature level similarities on a Markov random field (MRF) to obtain a highlevel correspondence in image space for image pairs. The proposedcorrespondence between image pair reflects not only the similarity of low-levelvisual features but also the relations built through other images in thedatabase and it can be easily integrated into the existingbag-of-visual-words(BoW) based systems to reduce the missing rate. We evaluateour method on the standard Oxford-5K, Oxford-105K and Paris-6K dataset. Theexperiment results show that the proposed method significantly improves theretrieval accuracy on three datasets and exceeds the current state-of-the-artretrieval performance.
arxiv-4800-192 | (More) Efficient Reinforcement Learning via Posterior Sampling | http://arxiv.org/pdf/1306.0940v5.pdf | author:Ian Osband, Daniel Russo, Benjamin Van Roy category:stat.ML cs.LG published:2013-06-04 summary:Most provably-efficient learning algorithms introduce optimism aboutpoorly-understood states and actions to encourage exploration. We study analternative approach for efficient exploration, posterior sampling forreinforcement learning (PSRL). This algorithm proceeds in repeated episodes ofknown duration. At the start of each episode, PSRL updates a prior distributionover Markov decision processes and takes one sample from this posterior. PSRLthen follows the policy that is optimal for this sample during the episode. Thealgorithm is conceptually simple, computationally efficient and allows an agentto encode prior knowledge in a natural way. We establish an $\tilde{O}(\tau S\sqrt{AT})$ bound on the expected regret, where $T$ is time, $\tau$ is theepisode length and $S$ and $A$ are the cardinalities of the state and actionspaces. This bound is one of the first for an algorithm not based on optimism,and close to the state of the art for any reinforcement learning algorithm. Weshow through simulation that PSRL significantly outperforms existing algorithmswith similar regret bounds.
arxiv-4800-193 | Top Down Approach to Multiple Plane Detection | http://arxiv.org/pdf/1312.6506v2.pdf | author:Prateek Singhal, Aditya Deshpande, N Dinesh Reddy, K Madhava Krishna category:cs.CV published:2013-12-23 summary:Detecting multiple planes in images is a challenging problem, but one withmany applications. Recent work such as J-Linkage and Ordered Residual Kernelshave focussed on developing a domain independent approach to detect multiplestructures. These multiple structure detection methods are then used forestimating multiple homographies given feature matches between two images.Features participating in the multiple homographies detected, provide us themultiple scene planes. We show that these methods provide locally optimalresults and fail to merge detected planar patches to the true scene planes.These methods use only residues obtained on applying homography of one plane toanother as cue for merging. In this paper, we develop additional cues such aslocal consistency of planes, local normals, texture etc. to perform betterclassification and merging . We formulate the classification as an MRF problemand use TRWS message passing algorithm to solve non metric energy terms andcomplex sparse graph structure. We show results on challenging dataset commonin robotics navigation scenarios where our method shows accuracy of more than85 percent on average while being close or same as the actual number of sceneplanes.
arxiv-4800-194 | Shape-constrained Estimation of Value Functions | http://arxiv.org/pdf/1312.7035v1.pdf | author:Mohammad Mousavi, Peter W. Glynn category:math.PR cs.CE math.OC stat.ML published:2013-12-26 summary:We present a fully nonparametric method to estimate the value function, viasimulation, in the context of expected infinite-horizon discounted rewards forMarkov chains. Estimating such value functions plays an important role inapproximate dynamic programming and applied probability in general. Weincorporate "soft information" into the estimation algorithm, such as knowledgeof convexity, monotonicity, or Lipchitz constants. In the presence of suchinformation, a nonparametric estimator for the value function can be computedthat is provably consistent as the simulated time horizon tends to infinity. Asan application, we implement our method on price tolling agreement contracts inenergy markets.
arxiv-4800-195 | Model-based clustering with Hidden Markov Model regression for time series with regime changes | http://arxiv.org/pdf/1312.7024v1.pdf | author:Faicel Chamroukhi, Allou Samé, Patrice Aknin, Gérard Govaert category:stat.ML cs.LG stat.ME published:2013-12-25 summary:This paper introduces a novel model-based clustering approach for clusteringtime series which present changes in regime. It consists of a mixture ofpolynomial regressions governed by hidden Markov chains. The underlying hiddenprocess for each cluster activates successively several polynomial regimesduring time. The parameter estimation is performed by the maximum likelihoodmethod through a dedicated Expectation-Maximization (EM) algorithm. Theproposed approach is evaluated using simulated time series and real-world timeseries issued from a railway diagnosis application. Comparisons with existingapproaches for time series clustering, including the stand EM for Gaussianmixtures, $K$-means clustering, the standard mixture of regression models andmixture of Hidden Markov Models, demonstrate the effectiveness of the proposedapproach.
arxiv-4800-196 | Robust EM algorithm for model-based curve clustering | http://arxiv.org/pdf/1312.7022v1.pdf | author:Faicel Chamroukhi category:stat.ME cs.LG stat.ML published:2013-12-25 summary:Model-based clustering approaches concern the paradigm of exploratory dataanalysis relying on the finite mixture model to automatically find a latentstructure governing observed data. They are one of the most popular andsuccessful approaches in cluster analysis. The mixture density estimation isgenerally performed by maximizing the observed-data log-likelihood by using theexpectation-maximization (EM) algorithm. However, it is well-known that the EMalgorithm initialization is crucial. In addition, the standard EM algorithmrequires the number of clusters to be known a priori. Some solutions have beenprovided in [31, 12] for model-based clustering with Gaussian mixture modelsfor multivariate data. In this paper we focus on model-based curve clusteringapproaches, when the data are curves rather than vectorial data, based onregression mixtures. We propose a new robust EM algorithm for clusteringcurves. We extend the model-based clustering approach presented in [31] forGaussian mixture models, to the case of curve clustering by regressionmixtures, including polynomial regression mixtures as well as spline orB-spline regressions mixtures. Our approach both handles the problem ofinitialization and the one of choosing the optimal number of clusters as the EMlearning proceeds, rather than in a two-fold scheme. This is achieved byoptimizing a penalized log-likelihood criterion. A simulation study confirmsthe potential benefit of the proposed algorithm in terms of robustnessregarding initialization and funding the actual number of clusters.
arxiv-4800-197 | Mixture model-based functional discriminant analysis for curve classification | http://arxiv.org/pdf/1312.7018v1.pdf | author:Faicel Chamroukhi, Hervé Glotin category:stat.ME cs.LG stat.ML published:2013-12-25 summary:Statistical approaches for Functional Data Analysis concern the paradigm forwhich the individuals are functions or curves rather than finite dimensionalvectors. In this paper, we particularly focus on the modeling and theclassification of functional data which are temporal curves presenting regimechanges over time. More specifically, we propose a new mixture model-baseddiscriminant analysis approach for functional data using a specific hiddenprocess regression model. Our approach is particularly adapted to both handlethe problem of complex-shaped classes of curves, where each class is composedof several sub-classes, and to deal with the regime changes within eachhomogeneous sub-class. The model explicitly integrates the heterogeneity ofeach class of curves via a mixture model formulation, and the regime changeswithin each sub-class through a hidden logistic process. The approach allowstherefore for fitting flexible curve-models to each class of complex-shapedcurves presenting regime changes through an unsupervised learning scheme, toautomatically summarize it into a finite number of homogeneous clusters, eachof them is decomposed into several regimes. The model parameters are learned bymaximizing the observed-data log-likelihood for each class by using a dedicatedexpectation-maximization (EM) algorithm. Comparisons on simulated data and realdata with alternative approaches, including functional linear discriminantanalysis and functional mixture discriminant analysis with polynomialregression mixtures and spline regression mixtures, show that the proposedapproach provides better results regarding the discrimination results andsignificantly improves the curves approximation.
arxiv-4800-198 | Classification automatique de données temporelles en classes ordonnées | http://arxiv.org/pdf/1312.7011v1.pdf | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ML stat.ME published:2013-12-25 summary:This paper proposes a method of segmenting temporal data into orderedclasses. It is based on mixture models and a discrete latent process, whichenables to successively activates the classes. The classification can beperformed by maximizing the likelihood via the EM algorithm or bysimultaneously optimizing the model parameters and the partition by the CEMalgorithm. These two algorithms can be seen as alternatives to Fisher'salgorithm, which improve its computing time.
arxiv-4800-199 | Functional Mixture Discriminant Analysis with hidden process regression for curve classification | http://arxiv.org/pdf/1312.7007v1.pdf | author:Faicel Chamroukhi, Heré Glotin, Céline Rabouy category:stat.ME cs.LG stat.ML published:2013-12-25 summary:We present a new mixture model-based discriminant analysis approach forfunctional data using a specific hidden process regression model. The approachallows for fitting flexible curve-models to each class of complex-shaped curvespresenting regime changes. The model parameters are learned by maximizing theobserved-data log-likelihood for each class by using a dedicatedexpectation-maximization (EM) algorithm. Comparisons on simulated data withalternative approaches show that the proposed approach provides better results.
arxiv-4800-200 | Supervised learning of a regression model based on latent process. Application to the estimation of fuel cell life time | http://arxiv.org/pdf/1312.7003v1.pdf | author:Raïssa Onanena, Faicel Chamroukhi, Latifa Oukhellou, Denis Candusso, Patrice Aknin, Daniel Hissel category:stat.ML cs.LG stat.AP published:2013-12-25 summary:This paper describes a pattern recognition approach aiming to estimate fuelcell duration time from electrochemical impedance spectroscopy measurements. Itconsists in first extracting features from both real and imaginary parts of theimpedance spectrum. A parametric model is considered in the case of the realpart, whereas regression model with latent variables is used in the lattercase. Then, a linear regression model using different subsets of extractedfeatures is used fo r the estimation of fuel cell time duration. Theperformances of the proposed approach are evaluated on experimental data set toshow its feasibility. This could lead to interesting perspectives forpredictive maintenance policy of fuel cell.
arxiv-4800-201 | A regression model with a hidden logistic process for feature extraction from time series | http://arxiv.org/pdf/1312.7001v1.pdf | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:A new approach for feature extraction from time series is proposed in thispaper. This approach consists of a specific regression model incorporating adiscrete hidden logistic process. The model parameters are estimated by themaximum likelihood method performed by a dedicated Expectation Maximization(EM) algorithm. The parameters of the hidden logistic process, in the innerloop of the EM algorithm, are estimated using a multi-class IterativeReweighted Least-Squares (IRLS) algorithm. A piecewise regression algorithm andits iterative variant have also been considered for comparisons. Anexperimental study using simulated and real data reveals good performances ofthe proposed approach.
arxiv-4800-202 | A regression model with a hidden logistic process for signal parametrization | http://arxiv.org/pdf/1312.6994v1.pdf | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG stat.ML published:2013-12-25 summary:A new approach for signal parametrization, which consists of a specificregression model incorporating a discrete hidden logistic process, is proposed.The model parameters are estimated by the maximum likelihood method performedby a dedicated Expectation Maximization (EM) algorithm. The parameters of thehidden logistic process, in the inner loop of the EM algorithm, are estimatedusing a multi-class Iterative Reweighted Least-Squares (IRLS) algorithm. Anexperimental study using simulated and real data reveals good performances ofthe proposed approach.
arxiv-4800-203 | Parallel Computation Is ESS | http://arxiv.org/pdf/1304.0160v8.pdf | author:Nabarun Mondal, Partha P. Ghosh category:cs.LG cs.AI cs.GT published:2013-03-31 summary:There are enormous amount of examples of Computation in nature, exemplifiedacross multiple species in biology. One crucial aim for these computationsacross all life forms their ability to learn and thereby increase the chance oftheir survival. In the current paper a formal definition of autonomous learningis proposed. From that definition we establish a Turing Machine model forlearning, where rule tables can be added or deleted, but can not be modified.Sequential and parallel implementations of this model are discussed. It isfound that for general purpose learning based on this model, theimplementations capable of parallel execution would be evolutionarily stable.This is proposed to be of the reasons why in Nature parallelism in computationis found in abundance.
arxiv-4800-204 | Modèle à processus latent et algorithme EM pour la régression non linéaire | http://arxiv.org/pdf/1312.6978v1.pdf | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:math.ST cs.LG stat.ME stat.ML stat.TH published:2013-12-25 summary:A non linear regression approach which consists of a specific regressionmodel incorporating a latent process, allowing various polynomial regressionmodels to be activated preferentially and smoothly, is introduced in thispaper. The model parameters are estimated by maximum likelihood performed via adedicated expecation-maximization (EM) algorithm. An experimental study usingsimulated and real data sets reveals good performances of the proposedapproach.
arxiv-4800-205 | Time series modeling by a regression approach based on a latent process | http://arxiv.org/pdf/1312.6969v1.pdf | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:Time series are used in many domains including finance, engineering,economics and bioinformatics generally to represent the change of a measurementover time. Modeling techniques may then be used to give a syntheticrepresentation of such data. A new approach for time series modeling isproposed in this paper. It consists of a regression model incorporating adiscrete hidden logistic process allowing for activating smoothly or abruptlydifferent polynomial regression models. The model parameters are estimated bythe maximum likelihood method performed by a dedicated Expectation Maximization(EM) algorithm. The M step of the EM algorithm uses a multi-class IterativeReweighted Least-Squares (IRLS) algorithm to estimate the hidden processparameters. To evaluate the proposed approach, an experimental study onsimulated data and real world data was performed using two alternativeapproaches: a heteroskedastic piecewise regression model using a globaloptimization algorithm based on dynamic programming, and a Hidden MarkovRegression Model whose parameters are estimated by the Baum-Welch algorithm.Finally, in the context of the remote monitoring of components of the Frenchrailway infrastructure, and more particularly the switch mechanism, theproposed approach has been applied to modeling and classifying time seriesrepresenting the condition measurements acquired during switch operations.
arxiv-4800-206 | A hidden process regression model for functional data description. Application to curve discrimination | http://arxiv.org/pdf/1312.6968v1.pdf | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG stat.ML published:2013-12-25 summary:A new approach for functional data description is proposed in this paper. Itconsists of a regression model with a discrete hidden logistic process which isadapted for modeling curves with abrupt or smooth regime changes. The modelparameters are estimated in a maximum likelihood framework through a dedicatedExpectation Maximization (EM) algorithm. From the proposed generative model, acurve discrimination rule is derived using the Maximum A Posteriori rule. Theproposed model is evaluated using simulated curves and real world curvesacquired during railway switch operations, by performing comparisons with thepiecewise regression approach in terms of curve modeling and classification.
arxiv-4800-207 | Model-based clustering and segmentation of time series with changes in regime | http://arxiv.org/pdf/1312.6967v1.pdf | author:Allou Samé, Faicel Chamroukhi, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:Mixture model-based clustering, usually applied to multidimensional data, hasbecome a popular approach in many data analysis problems, both for its goodstatistical properties and for the simplicity of implementation of theExpectation-Maximization (EM) algorithm. Within the context of a railwayapplication, this paper introduces a novel mixture model for dealing with timeseries that are subject to changes in regime. The proposed approach consists inmodeling each cluster by a regression model in which the polynomialcoefficients vary according to a discrete hidden process. In particular, thisapproach makes use of logistic functions to model the (smooth or abrupt)transitions between regimes. The model parameters are estimated by the maximumlikelihood method solved by an Expectation-Maximization algorithm. The proposedapproach can also be regarded as a clustering approach which operates byfinding groups of time series having common changes in regime. In addition toproviding a time series partition, it therefore provides a time seriessegmentation. The problem of selecting the optimal numbers of clusters andsegments is solved by means of the Bayesian Information Criterion (BIC). Theproposed approach is shown to be efficient using a variety of simulated timeseries and real-world time series of electrical power consumption from railswitching operations.
arxiv-4800-208 | Clustering for high-dimension, low-sample size data using distance vectors | http://arxiv.org/pdf/1312.3386v2.pdf | author:Yoshikazu Terada category:stat.ML cs.LG published:2013-12-12 summary:In high-dimension, low-sample size (HDLSS) data, it is not always true thatcloseness of two objects reflects a hidden cluster structure. We point out theimportant fact that it is not the closeness, but the "values" of distance thatcontain information of the cluster structure in high-dimensional space. Basedon this fact, we propose an efficient and simple clustering approach, calleddistance vector clustering, for HDLSS data. Under the assumptions given in thework of Hall et al. (2005), we show the proposed approach provides a truecluster label under milder conditions when the dimension tends to infinity withthe sample size fixed. The effectiveness of the distance vector clusteringapproach is illustrated through a numerical experiment and real data analysis.
arxiv-4800-209 | Model-based functional mixture discriminant analysis with hidden process regression for curve classification | http://arxiv.org/pdf/1312.6966v1.pdf | author:Faicel Chamroukhi, Hervé Glotin, Allou Samé category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:In this paper, we study the modeling and the classification of functionaldata presenting regime changes over time. We propose a new model-basedfunctional mixture discriminant analysis approach based on a specific hiddenprocess regression model that governs the regime changes over time. Ourapproach is particularly adapted to handle the problem of complex-shapedclasses of curves, where each class is potentially composed of severalsub-classes, and to deal with the regime changes within each homogeneoussub-class. The proposed model explicitly integrates the heterogeneity of eachclass of curves via a mixture model formulation, and the regime changes withineach sub-class through a hidden logistic process. Each class of complex-shapedcurves is modeled by a finite number of homogeneous clusters, each of thembeing decomposed into several regimes. The model parameters of each class arelearned by maximizing the observed-data log-likelihood by using a dedicatedexpectation-maximization (EM) algorithm. Comparisons are performed withalternative curve classification approaches, including functional lineardiscriminant analysis and functional mixture discriminant analysis withpolynomial regression mixtures and spline regression mixtures. Results obtainedon simulated data and real data show that the proposed approach outperforms thealternative approaches in terms of discrimination, and significantly improvesthe curves approximation.
arxiv-4800-210 | An Unsupervised Approach for Automatic Activity Recognition based on Hidden Markov Model Regression | http://arxiv.org/pdf/1312.6965v1.pdf | author:Dorra Trabelsi, Samer Mohammed, Faicel Chamroukhi, Latifa Oukhellou, Yacine Amirat category:stat.ML cs.CV cs.LG published:2013-12-25 summary:Using supervised machine learning approaches to recognize human activitiesfrom on-body wearable accelerometers generally requires a large amount oflabelled data. When ground truth information is not available, too expensive,time consuming or difficult to collect, one has to rely on unsupervisedapproaches. This paper presents a new unsupervised approach for human activityrecognition from raw acceleration data measured using inertial wearablesensors. The proposed method is based upon joint segmentation ofmultidimensional time series using a Hidden Markov Model (HMM) in a multipleregression context. The model is learned in an unsupervised framework using theExpectation-Maximization (EM) algorithm where no activity labels are needed.The proposed method takes into account the sequential appearance of the data.It is therefore adapted for the temporal acceleration data to accurately detectthe activities. It allows both segmentation and classification of the humanactivities. Experimental results are provided to demonstrate the efficiency ofthe proposed approach with respect to standard supervised and unsupervisedclassification approaches
arxiv-4800-211 | Subjectivity Classification using Machine Learning Techniques for Mining Feature-Opinion Pairs from Web Opinion Sources | http://arxiv.org/pdf/1312.6962v1.pdf | author:Ahmad Kamal category:cs.IR cs.CL cs.LG published:2013-12-25 summary:Due to flourish of the Web 2.0, web opinion sources are rapidly emergingcontaining precious information useful for both customers and manufactures.Recently, feature based opinion mining techniques are gaining momentum in whichcustomer reviews are processed automatically for mining product features anduser opinions expressed over them. However, customer reviews may contain bothopinionated and factual sentences. Distillations of factual contents improvemining performance by preventing noisy and irrelevant extraction. In thispaper, combination of both supervised machine learning and rule-basedapproaches are proposed for mining feasible feature-opinion pairs fromsubjective review sentences. In the first phase of the proposed approach, asupervised machine learning technique is applied for classifying subjective andobjective sentences from customer reviews. In the next phase, a rule basedmethod is implemented which applies linguistic and semantic analysis of textsto mine feasible feature-opinion pairs from subjective sentences retained afterthe first phase. The effectiveness of the proposed methods is establishedthrough experimentation over customer reviews on different electronic products.
arxiv-4800-212 | Joint segmentation of multivariate time series with hidden process regression for human activity recognition | http://arxiv.org/pdf/1312.6956v1.pdf | author:Faicel Chamroukhi, Samer Mohammed, Dorra Trabelsi, Latifa Oukhellou, Yacine Amirat category:stat.ML cs.LG published:2013-12-25 summary:The problem of human activity recognition is central for understanding andpredicting the human behavior, in particular in a prospective of assistiveservices to humans, such as health monitoring, well being, security, etc. Thereis therefore a growing need to build accurate models which can take intoaccount the variability of the human activities over time (dynamic models)rather than static ones which can have some limitations in such a dynamiccontext. In this paper, the problem of activity recognition is analyzed throughthe segmentation of the multidimensional time series of the acceleration datameasured in the 3-d space using body-worn accelerometers. The proposed modelfor automatic temporal segmentation is a specific statistical latent processmodel which assumes that the observed acceleration sequence is governed bysequence of hidden (unobserved) activities. More specifically, the proposedapproach is based on a specific multiple regression model incorporating ahidden discrete logistic process which governs the switching from one activityto another over time. The model is learned in an unsupervised context bymaximizing the observed-data log-likelihood via a dedicatedexpectation-maximization (EM) algorithm. We applied it on a real-worldautomatic human activity recognition problem and its performance was assessedby performing comparisons with alternative approaches, including well-knownsupervised static classifiers and the standard hidden Markov model (HMM). Theobtained results are very encouraging and show that the proposed approach isquite competitive even it works in an entirely unsupervised way and does notrequires a feature extraction preprocessing step.
arxiv-4800-213 | Description Logics based Formalization of Wh-Queries | http://arxiv.org/pdf/1312.6948v1.pdf | author:Sourish Dasgupta, Rupali KaPatel, Ankur Padia, Kushal Shah category:cs.CL cs.AI published:2013-12-25 summary:The problem of Natural Language Query Formalization (NLQF) is to translate agiven user query in natural language (NL) into a formal language so that thesemantic interpretation has equivalence with the NL interpretation.Formalization of NL queries enables logic based reasoning during informationretrieval, database query, question-answering, etc. Formalization also helps inWeb query normalization and indexing, query intent analysis, etc. In this paperwe are proposing a Description Logics based formal methodology for wh-queryintent (also called desire) identification and corresponding formaltranslation. We evaluated the scalability of our proposed formalism usingMicrosoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.
arxiv-4800-214 | Outlier robust system identification: a Bayesian kernel-based approach | http://arxiv.org/pdf/1312.6317v2.pdf | author:Giulio Bottegal, Aleksandr Y. Aravkin, Hakan Hjalmarsson, Gianluigi Pillonetto category:stat.ML 47N30, 65K10 published:2013-12-21 summary:In this paper, we propose an outlier-robust regularized kernel-based methodfor linear system identification. The unknown impulse response is modeled as azero-mean Gaussian process whose covariance (kernel) is given by the recentlyproposed stable spline kernel, which encodes information on regularity andexponential stability. To build robustness to outliers, we model themeasurement noise as realizations of independent Laplacian random variables.The identification problem is cast in a Bayesian framework, and solved by a newMarkov Chain Monte Carlo (MCMC) scheme. In particular, exploiting therepresentation of the Laplacian random variables as scale mixtures ofGaussians, we design a Gibbs sampler which quickly converges to the targetdistribution. Numerical simulations show a substantial improvement in theaccuracy of the estimates over state-of-the-art kernel-based methods.
arxiv-4800-215 | Deep learning for class-generic object detection | http://arxiv.org/pdf/1312.6885v1.pdf | author:Brody Huval, Adam Coates, Andrew Ng category:cs.CV cs.LG cs.NE published:2013-12-24 summary:We investigate the use of deep neural networks for the novel task of classgeneric object detection. We show that neural networks originally designed forimage recognition can be trained to detect objects within images, regardless oftheir class, including objects for which no bounding box labels have beenprovided. In addition, we show that bounding box labels yield a 1% performanceincrease on the ImageNet recognition challenge.
arxiv-4800-216 | Greedy Column Subset Selection for Large-scale Data Sets | http://arxiv.org/pdf/1312.6838v1.pdf | author:Ahmed K. Farahat, Ahmed Elgohary, Ali Ghodsi, Mohamed S. Kamel category:cs.DS cs.LG published:2013-12-24 summary:In today's information systems, the availability of massive amounts of datanecessitates the development of fast and accurate algorithms to summarize thesedata and represent them in a succinct format. One crucial problem in big dataanalytics is the selection of representative instances from large andmassively-distributed data, which is formally known as the Column SubsetSelection (CSS) problem. The solution to this problem enables data analysts tounderstand the insights of the data and explore its hidden structure. Theselected instances can also be used for data preprocessing tasks such aslearning a low-dimensional embedding of the data points or computing a low-rankapproximation of the corresponding matrix. This paper presents a fast andaccurate greedy algorithm for large-scale column subset selection. Thealgorithm minimizes an objective function which measures the reconstructionerror of the data matrix based on the subset of selected columns. The paperfirst presents a centralized greedy algorithm for column subset selection whichdepends on a novel recursive formula for calculating the reconstruction errorof the data matrix. The paper then presents a MapReduce algorithm which selectsa few representative columns from a matrix whose columns are massivelydistributed across several commodity machines. The algorithm first learns aconcise representation of all columns using random projection, and it thensolves a generalized column subset selection problem at each machine in which asubset of columns are selected from the sub-matrix on that machine such thatthe reconstruction error of the concise representation is minimized. The paperdemonstrates the effectiveness and efficiency of the proposed algorithm throughan empirical evaluation on benchmark data sets.
arxiv-4800-217 | 3D Interest Point Detection via Discriminative Learning | http://arxiv.org/pdf/1312.6826v1.pdf | author:Leizer Teran, Philippos Mordohai category:cs.CV published:2013-12-24 summary:The task of detecting the interest points in 3D meshes has typically beenhandled by geometric methods. These methods, while greatly describing humanpreference, can be ill-equipped for handling the variety and subjectivity inhuman responses. Different tasks have different requirements for interest pointdetection; some tasks may necessitate high precision while other tasks mayrequire high recall. Sometimes points with high curvature may be desirable,while in other cases high curvature may be an indication of noise. Geometricmethods lack the required flexibility to adapt to such changes. As aconsequence, interest point detection seems to be well suited for machinelearning methods that can be trained to match the criteria applied on theannotated training data. In this paper, we formulate interest point detectionas a supervised binary classification problem using a random forest as ourclassifier. Among other challenges, we are faced with an imbalanced learningproblem due to the substantial difference in the priors between interest andnon-interest points. We address this by re-sampling the training set. Wevalidate the accuracy of our method and compare our results to those of fivestate of the art methods on a new, standard benchmark.
arxiv-4800-218 | A Fast Greedy Algorithm for Generalized Column Subset Selection | http://arxiv.org/pdf/1312.6820v1.pdf | author:Ahmed K. Farahat, Ali Ghodsi, Mohamed S. Kamel category:cs.DS cs.LG stat.ML published:2013-12-24 summary:This paper defines a generalized column subset selection problem which isconcerned with the selection of a few columns from a source matrix A that bestapproximate the span of a target matrix B. The paper then proposes a fastgreedy algorithm for solving this problem and draws connections to differentproblems that can be efficiently solved using the proposed algorithm.
arxiv-4800-219 | Iterative Nearest Neighborhood Oversampling in Semisupervised Learning from Imbalanced Data | http://arxiv.org/pdf/1312.6807v1.pdf | author:Fengqi Li, Chuang Yu, Nanhai Yang, Feng Xia, Guangming Li, Fatemeh Kaveh-Yazdy category:cs.LG 68P20 H.3.3 published:2013-12-24 summary:Transductive graph-based semi-supervised learning methods usually build anundirected graph utilizing both labeled and unlabeled samples as vertices.Those methods propagate label information of labeled samples to neighborsthrough their edges in order to get the predicted labels of unlabeled samples.Most popular semi-supervised learning approaches are sensitive to initial labeldistribution happened in imbalanced labeled datasets. The class boundary willbe severely skewed by the majority classes in an imbalanced classification. Inthis paper, we proposed a simple and effective approach to alleviate theunfavorable influence of imbalance problem by iteratively selecting a fewunlabeled samples and adding them into the minority classes to form a balancedlabeled dataset for the learning methods afterwards. The experiments on UCIdatasets and MNIST handwritten digits dataset showed that the proposed approachoutperforms other existing state-of-art methods.
arxiv-4800-220 | Suffix Stripping Problem as an Optimization Problem | http://arxiv.org/pdf/1312.6802v1.pdf | author:B. P. Pande, Pawan Tamta, H. S. Dhami category:cs.IR cs.CL published:2013-12-24 summary:Stemming or suffix stripping, an important part of the modern InformationRetrieval systems, is to find the root word (stem) out of a given cluster ofwords. Existing algorithms targeting this problem have been developed in ahaphazard manner. In this work, we model this problem as an optimizationproblem. An Integer Program is being developed to overcome the shortcomings ofthe existing approaches. The sample results of the proposed method are alsobeing compared with an established technique in the field for English language.An AMPL code for the same IP has also been given.
arxiv-4800-221 | Comparative study of Authorship Identification Techniques for Cyber Forensics Analysis | http://arxiv.org/pdf/1401.6118v1.pdf | author:Smita Nirkhi, R. V. Dharaskar category:cs.CY cs.CR cs.IR cs.LG published:2013-12-24 summary:Authorship Identification techniques are used to identify the mostappropriate author from group of potential suspects of online messages and findevidences to support the conclusion. Cybercriminals make misuse of onlinecommunication for sending blackmail or a spam email and then attempt to hidetheir true identities to void detection.Authorship Identification of onlinemessages is the contemporary research issue for identity tracing in cyberforensics. This is highly interdisciplinary area as it takes advantage ofmachine learning, information retrieval, and natural language processing. Inthis paper, a study of recent techniques and automated approaches toattributing authorship of online messages is presented. The focus of thisreview study is to summarize all existing authorship identification techniquesused in literature to identify authors of online messages. Also it discussesevaluation criteria and parameters for authorship attribution studies and listopen questions that will attract future work in this area.
arxiv-4800-222 | IVSS Integration of Color Feature Extraction Techniques for Intelligent Video Search Systems | http://arxiv.org/pdf/1312.6782v1.pdf | author:Avinash N Bhute, B. B. Meshram category:cs.CV cs.IR cs.MM published:2013-12-24 summary:As large amount of visual Information is available on web in form of images,graphics, animations and videos, so it is important in internet era to have aneffective video search system. As there are number of video search engine(blinkx, Videosurf, Google, YouTube, etc.) which search for relevant videosbased on user keyword or term, But very less commercial video search engine areavailable which search videos based on visual image/clip/video. In this paperwe are recommending a system that will search for relevant video using colorfeature of video in response of user Query.
arxiv-4800-223 | A Subband-Based SVM Front-End for Robust ASR | http://arxiv.org/pdf/1401.3322v1.pdf | author:Jibran Yousafzai, Zoran Cvetkovic, Peter Sollich, Matthew Ager category:cs.CL cs.LG cs.SD published:2013-12-24 summary:This work proposes a novel support vector machine (SVM) based robustautomatic speech recognition (ASR) front-end that operates on an ensemble ofthe subband components of high-dimensional acoustic waveforms. The key issuesof selecting the appropriate SVM kernels for classification in frequencysubbands and the combination of individual subband classifiers using ensemblemethods are addressed. The proposed front-end is compared with state-of-the-artASR front-ends in terms of robustness to additive noise and linear filtering.Experiments performed on the TIMIT phoneme classification task demonstrate thebenefits of the proposed subband based SVM front-end: it outperforms thestandard cepstral front-end in the presence of noise and linear filtering forsignal-to-noise ratio (SNR) below 12-dB. A combination of the proposedfront-end with a conventional front-end such as MFCC yields furtherimprovements over the individual front ends across the full range of noiselevels.
arxiv-4800-224 | Time-Series Classification Through Histograms of Symbolic Polynomials | http://arxiv.org/pdf/1307.6365v4.pdf | author:Josif Grabocka, Martin Wistuba, Lars Schmidt-Thieme category:cs.AI cs.DB cs.LG published:2013-07-24 summary:Time-series classification has attracted considerable research attention dueto the various domains where time-series data are observed, ranging frommedicine to econometrics. Traditionally, the focus of time-seriesclassification has been on short time-series data composed of a unique patternwith intraclass pattern distortions and variations, while recently there havebeen attempts to focus on longer series composed of various local patterns.This study presents a novel method which can detect local patterns in longtime-series via fitting local polynomial functions of arbitrary degrees. Thecoefficients of the polynomial functions are converted to symbolic words viaequivolume discretizations of the coefficients' distributions. The symbolicpolynomial words enable the detection of similar local patterns by assigningthe same words to similar polynomials. Moreover, a histogram of the frequenciesof the words is constructed from each time-series' bag of words. Each row ofthe histogram enables a new representation for the series and symbolize theexistence of local patterns and their frequencies. Experimental evidencedemonstrates outstanding results of our method compared to the state-of-artbaselines, by exhibiting the best classification accuracies in all the datasetsand having statistically significant improvements in the absolute majority ofexperiments.
arxiv-4800-225 | Invariant Factorization Of Time-Series | http://arxiv.org/pdf/1312.6712v1.pdf | author:Josif Grabocka, Lars Schmidt-Thieme category:cs.LG published:2013-12-23 summary:Time-series classification is an important domain of machine learning and aplethora of methods have been developed for the task. In comparison to existingapproaches, this study presents a novel method which decomposes a time-seriesdataset into latent patterns and membership weights of local segments to thosepatterns. The process is formalized as a constrained objective function and atailored stochastic coordinate descent optimization is applied. The time-seriesare projected to a new feature representation consisting of the sums of themembership weights, which captures frequencies of local patterns. Features fromvarious sliding window sizes are concatenated in order to encapsulate theinteraction of patterns from different sizes. Finally, a large-scaleexperimental comparison against 6 state of the art baselines and 43 real lifedatasets is conducted. The proposed method outperforms all the baselines withstatistically significant margins in terms of prediction accuracy.
arxiv-4800-226 | A central limit theorem for scaled eigenvectors of random dot product graphs | http://arxiv.org/pdf/1305.7388v2.pdf | author:Avanti Athreya, Vince Lyzinski, David J. Marchette, Carey E. Priebe, Daniel L. Sussman, Minh Tang category:math.ST stat.ML stat.TH published:2013-05-31 summary:We prove a central limit theorem for the components of the largesteigenvectors of the adjacency matrix of a finite-dimensional random dot productgraph whose true latent positions are unknown. In particular, we follow themethodology outlined in \citet{sussman2012universally} to construct consistentestimates for the latent positions, and we show that the appropriately scaleddifferences between the estimated and true latent positions converge to amixture of Gaussian random variables. As a corollary, we obtain a central limittheorem for the first eigenvector of the adjacency matrix of an Erd\"os-Renyirandom graph.
arxiv-4800-227 | Rounding Sum-of-Squares Relaxations | http://arxiv.org/pdf/1312.6652v1.pdf | author:Boaz Barak, Jonathan Kelner, David Steurer category:cs.DS cs.LG quant-ph published:2013-12-23 summary:We present a general approach to rounding semidefinite programmingrelaxations obtained by the Sum-of-Squares method (Lasserre hierarchy). Ourapproach is based on using the connection between these relaxations and theSum-of-Squares proof system to transform a *combining algorithm* -- analgorithm that maps a distribution over solutions into a (possibly weaker)solution -- into a *rounding algorithm* that maps a solution of the relaxationto a solution of the original problem. Using this approach, we obtain algorithms that yield improved results fornatural variants of three well-known problems: 1) We give a quasipolynomial-time algorithm that approximates the maximum ofa low degree multivariate polynomial with non-negative coefficients over theEuclidean unit sphere. Beyond being of interest in its own right, this isrelated to an open question in quantum information theory, and our techniqueshave already led to improved results in this area (Brand\~{a}o and Harrow, STOC'13). 2) We give a polynomial-time algorithm that, given a d dimensional subspaceof R^n that (almost) contains the characteristic function of a set of size n/k,finds a vector $v$ in the subspace satisfying $v_4^4 > c(k/d^{1/3}) v_2^2$,where $v_p = (E_i v_i^p)^{1/p}$. Aside from being a natural relaxation, thisis also motivated by a connection to the Small Set Expansion problem shown byBarak et al. (STOC 2012) and our results yield a certain improvement for thatproblem. 3) We use this notion of L_4 vs. L_2 sparsity to obtain a polynomial-timealgorithm with substantially improved guarantees for recovering a planted$\mu$-sparse vector v in a random d-dimensional subspace of R^n. If v has mu nnonzero coordinates, we can recover it with high probability whenever $\mu <O(\min(1,n/d^2))$, improving for $d < n^{2/3}$ prior methods whichintrinsically required $\mu < O(1/\sqrt(d))$.
arxiv-4800-228 | Automated Coin Recognition System using ANN | http://arxiv.org/pdf/1312.6615v1.pdf | author:Shatrughan Modi, Dr. Seema Bawa category:cs.CV cs.AI published:2013-12-23 summary:Coins are integral part of our day to day life. We use coins everywhere likegrocery store, banks, buses, trains etc. So it becomes a basic need that coinscan be sorted and counted automatically. For this it is necessary that coinscan be recognized automatically. In this paper we have developed an ANN(Artificial Neural Network) based Automated Coin Recognition System for therecognition of Indian Coins of denomination Rs. 1, 2, 5 and 10 with rotationinvariance. We have taken images from both sides of coin. So this system iscapable of recognizing coins from both sides. Features are extracted fromimages using techniques of Hough Transformation, Pattern Averaging etc. Then,the extracted features are passed as input to a trained Neural Network. 97.74%recognition rate has been achieved during the experiments i.e. only 2.26% missrecognition, which is quite encouraging.
arxiv-4800-229 | A comprehensive review of firefly algorithms | http://arxiv.org/pdf/1312.6609v1.pdf | author:Iztok Fister, Iztok Fister Jr., Xin-She Yang, Janez Brest category:cs.NE published:2013-12-23 summary:The firefly algorithm has become an increasingly important tool of SwarmIntelligence that has been applied in almost all areas of optimization, as wellas engineering practice. Many problems from various areas have beensuccessfully solved using the firefly algorithm and its variants. In order touse the algorithm to solve diverse problems, the original firefly algorithmneeds to be modified or hybridized. This paper carries out a comprehensivereview of this living and evolving discipline of Swarm Intelligence, in orderto show that the firefly algorithm could be applied to every problem arising inpractice. On the other hand, it encourages new researchers and algorithmdevelopers to use this simple and yet very efficient algorithm for problemsolving. It often guarantees that the obtained results will meet theexpectations.
arxiv-4800-230 | Using Latent Binary Variables for Online Reconstruction of Large Scale Systems | http://arxiv.org/pdf/1312.6607v1.pdf | author:Victorin Martin, Jean-Marc Lasgouttes, Cyril Furtlehner category:math.PR cs.LG stat.ML published:2013-12-23 summary:We propose a probabilistic graphical model realizing a minimal encoding ofreal variables dependencies based on possibly incomplete observation and anempirical cumulative distribution function per variable. The target applicationis a large scale partially observed system, like e.g. a traffic network, wherea small proportion of real valued variables are observed, and the othervariables have to be predicted. Our design objective is therefore to have goodscalability in a real-time setting. Instead of attempting to encode thedependencies of the system directly in the description space, we propose a wayto encode them in a latent space of binary variables, reflecting a roughperception of the observable (congested/non-congested for a traffic road). Themethod relies in part on message passing algorithms, i.e. belief propagation,but the core of the work concerns the definition of meaningful latent variablesassociated to the variables of interest and their pairwise dependencies.Numerical experiments demonstrate the applicability of the method in practice.
arxiv-4800-231 | Image Processing based Systems and Techniques for the Recognition of Ancient and Modern Coins | http://arxiv.org/pdf/1312.6599v1.pdf | author:Shatrughan Modi, Dr. Seema Bawa category:cs.CV cs.AI published:2013-12-23 summary:Coins are frequently used in everyday life at various places like in banks,grocery stores, supermarkets, automated weighing machines, vending machinesetc. So, there is a basic need to automate the counting and sorting of coins.For this machines need to recognize the coins very fast and accurately, asfurther transaction processing depends on this recognition. Three types ofsystems are available in the market: Mechanical method based systems,Electromagnetic method based systems and Image processing based systems. Thispaper presents an overview of available systems and techniques based on imageprocessing to recognize ancient and modern coins.
arxiv-4800-232 | A Survey on Eye-Gaze Tracking Techniques | http://arxiv.org/pdf/1312.6410v1.pdf | author:H. R. Chennamma, Xiaohui Yuan category:cs.CV published:2013-12-22 summary:Study of eye-movement is being employed in Human Computer Interaction (HCI)research. Eye - gaze tracking is one of the most challenging problems in thearea of computer vision. The goal of this paper is to present a review oflatest research in this continued growth of remote eye-gaze tracking. Thisoverview includes the basic definitions and terminologies, recent advances inthe field and finally the need of future development in the field.
arxiv-4800-233 | An Efficient Edge Detection Technique by Two Dimensional Rectangular Cellular Automata | http://arxiv.org/pdf/1312.6370v1.pdf | author:Jahangir Mohammed, Deepak Ranjan Nayak category:cs.CV published:2013-12-22 summary:This paper proposes a new pattern of two dimensional cellular automata linearrules that are used for efficient edge detection of an image. Since cellularautomata is inherently parallel in nature, it has produced desired outputwithin a unit time interval. We have observed four linear rules among 512 totallinear rules of a rectangular cellular automata in adiabatic or reflexiveboundary condition that produces an optimal result. These four rules aredirectly applied once to the images and produced edge detected output. Wecompare our results with the existing edge detection algorithms and found thatour results shows better edge detection with an enhancement of edges.
arxiv-4800-234 | Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning | http://arxiv.org/pdf/1312.6282v1.pdf | author:François Denis, Mattias Gybels, Amaury Habrard category:cs.LG published:2013-12-21 summary:Learning probabilistic models over strings is an important issue for manyapplications. Spectral methods propose elegant solutions to the problem ofinferring weighted automata from finite samples of variable-length stringsdrawn from an unknown target distribution. These methods rely on a singularvalue decomposition of a matrix $H_S$, called the Hankel matrix, that recordsthe frequencies of (some of) the observed strings. The accuracy of the learneddistribution depends both on the quantity of information embedded in $H_S$ andon the distance between $H_S$ and its mean $H_r$. Existing concentration boundsseem to indicate that the concentration over $H_r$ gets looser with the size of$H_r$, suggesting to make a trade-off between the quantity of used informationand the size of $H_r$. We propose new dimension-free concentration bounds forseveral variants of Hankel matrices. Experiments demonstrate that these boundsare tight and that they significantly improve existing bounds. These resultssuggest that the concentration rate of the Hankel matrix around its mean doesnot constitute an argument for limiting its size.
arxiv-4800-235 | Parallel architectures for fuzzy triadic similarity learning | http://arxiv.org/pdf/1312.6273v1.pdf | author:Sonia Alouane-Ksouri, Minyar Sassi-Hidri, Kamel Barkaoui category:cs.DC cs.LG stat.ML published:2013-12-21 summary:In a context of document co-clustering, we define a new similarity measurewhich iteratively computes similarity while combining fuzzy sets in athree-partite graph. The fuzzy triadic similarity (FT-Sim) model can deal withuncertainty offers by the fuzzy sets. Moreover, with the development of the Weband the high availability of storage spaces, more and more documents becomeaccessible. Documents can be provided from multiple sites and make similaritycomputation an expensive processing. This problem motivated us to use parallelcomputing. In this paper, we introduce parallel architectures which are able totreat large and multi-source data sets by a sequential, a merging or asplitting-based process. Then, we proceed to a local and a central (or global)computing using the basic FT-Sim measure. The idea behind these architecturesis to reduce both time and space complexities thanks to parallel computation.
arxiv-4800-236 | G-invariant Persistent Homology | http://arxiv.org/pdf/1212.0655v5.pdf | author:Patrizio Frosini category:math.AT cs.CG cs.CV I.4.7; I.5.1 published:2012-12-04 summary:Classical persistent homology is a powerful mathematical tool for shapecomparison. Unfortunately, it is not tailored to study the action oftransformation groups that are different from the group Homeo(X) of allself-homeomorphisms of a topological space X. This fact restricts its use inapplications. In order to obtain better lower bounds for the naturalpseudo-distance d_G associated with a subgroup G of Homeo(X), we need to adaptpersistent homology and consider G-invariant persistent homology. Roughlyspeaking, the main idea consists in defining persistent homology by means of aset of chains that is invariant under the action of G. In this paper weformalize this idea, and prove the stability of the persistent Betti numberfunctions in G-invariant persistent homology with respect to the naturalpseudo-distance d_G. We also show how G-invariant persistent homology could beused in applications concerning shape comparison, when the invariance group isa proper subgroup of the group of all self-homeomorphisms of a topologicalspace. In this paper we will assume that the space X is triangulable, in orderto guarantee that the persistent Betti number functions are finite withoutusing any tameness assumption.
arxiv-4800-237 | Extracting Region of Interest for Palm Print Authentication | http://arxiv.org/pdf/1312.6219v1.pdf | author:Kasturika B. Ray category:cs.CV published:2013-12-21 summary:Biometrics authentication is an effective method for automaticallyrecognizing individuals. The authentication consists of an enrollment phase andan identification or verification phase. In the stages of enrollment known(training) samples after the pre-processing stage are used for suitable featureextraction to generate the template database. In the verification stage, thetest sample is similarly pre processed and subjected to feature extractionmodules, and then it is matched with the training feature templates to decidewhether it is a genuine or not. This paper presents use of a region of interest(ROI) for palm print technology. First some of the existing methods for palmprint identification have been introduced. Then focus has been given onextraction of a suitable smaller region from the acquired palm print to improvethe identification method accuracy. Several existing work in the topic ofregion extraction have been examined. Subsequently, a simple and originalmethod has then proposed for locating the ROI that can be effectively used forpalm print analysis. The ROI extracted using this new technique is suitable fordifferent types of processing as it creates a rectangular or square area aroundthe center of activity represented by the lines, wrinkles and ridges of thepalm print. The effectiveness of the ROI approach has been tested byintegrating it with a texture based identification / authentication systemproposed earlier. The improvement has been shown by comparing theidentification accuracy rate before and after the ROI pre-processing.
arxiv-4800-238 | Total variation with overlapping group sparsity for image deblurring under impulse noise | http://arxiv.org/pdf/1312.6208v1.pdf | author:Gang Liu, Ting-Zhu Huang, Jun Liu, Xiao-Guang Lv category:math.NA cs.CV published:2013-12-21 summary:The total variation (TV) regularization method is an effective method forimage deblurring in preserving edges. However, the TV based solutions usuallyhave some staircase effects. In this paper, in order to alleviate the staircaseeffect, we propose a new model for restoring blurred images with impulse noise.The model consists of an $\ell_1$-fidelity term and a TV with overlapping groupsparsity (OGS) regularization term. Moreover, we impose a box constraint to theproposed model for getting more accurate solutions. An efficient and effectivealgorithm is proposed to solve the model under the framework of the alternatingdirection method of multipliers (ADMM). We use an inner loop which is nestedinside the majorization minimization (MM) iteration for the subproblem of theproposed method. Compared with other methods, numerical results illustrate thatthe proposed method, can significantly improve the restoration quality, both inavoiding staircase effects and in terms of peak signal-to-noise ratio (PSNR)and relative error (ReE).
arxiv-4800-239 | GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training | http://arxiv.org/pdf/1312.6186v1.pdf | author:Thomas Paine, Hailin Jin, Jianchao Yang, Zhe Lin, Thomas Huang category:cs.CV cs.DC cs.LG cs.NE published:2013-12-21 summary:The ability to train large-scale neural networks has resulted instate-of-the-art performance in many areas of computer vision. These resultshave largely come from computational break throughs of two forms: modelparallelism, e.g. GPU accelerated training, which has seen quick adoption incomputer vision circles, and data parallelism, e.g. A-SGD, whose large scalehas been used mostly in industry. We report early experiments with a systemthat makes use of both model parallelism and data parallelism, we call GPUA-SGD. We show using GPU A-SGD it is possible to speed up training of largeconvolutional neural networks useful for computer vision. We believe GPU A-SGDwill make it possible to train larger networks on larger training sets in areasonable amount of time.
arxiv-4800-240 | Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data | http://arxiv.org/pdf/1312.4740v2.pdf | author:Yalong Bai, Kuiyuan Yang, Wei Yu, Wei-Ying Ma, Tiejun Zhao category:cs.CV published:2013-12-17 summary:Image retrieval refers to finding relevant images from an image database fora query, which is considered difficult for the gap between low-levelrepresentation of images and high-level representation of queries. Recentlyfurther developed Deep Neural Network sheds light on automatically learninghigh-level image representation from raw pixels. In this paper, we proposed amulti-task DNN learned for image retrieval, which contains two parts, i.e.,query-sharing layers for image representation computation and query-specificlayers for relevance estimation. The weights of multi-task DNN are learned onclickthrough data by Ring Training. Experimental results on both simulated andreal dataset show the effectiveness of the proposed method.
arxiv-4800-241 | Large-Scale Paralleled Sparse Principal Component Analysis | http://arxiv.org/pdf/1312.6182v1.pdf | author:W. Liu, H. Zhang, D. Tao, Y. Wang, K. Lu category:cs.MS cs.LG cs.NA stat.ML published:2013-12-21 summary:Principal component analysis (PCA) is a statistical technique commonly usedin multivariate data analysis. However, PCA can be difficult to interpret andexplain since the principal components (PCs) are linear combinations of theoriginal variables. Sparse PCA (SPCA) aims to balance statistical fidelity andinterpretability by approximating sparse PCs whose projections capture themaximal variance of original data. In this paper we present an efficient andparalleled method of SPCA using graphics processing units (GPUs), which canprocess large blocks of data in parallel. Specifically, we construct parallelimplementations of the four optimization formulations of the generalized powermethod of SPCA (GP-SPCA), one of the most efficient and effective SPCAapproaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS)is up to eleven times faster than the corresponding CPU implementation (usingCBLAS), and up to 107 times faster than a MatLab implementation. Extensivecomparative experiments in several real-world datasets confirm that SPCA offersa practical advantage.
arxiv-4800-242 | Manifold regularized kernel logistic regression for web image annotation | http://arxiv.org/pdf/1312.6180v1.pdf | author:W. Liu, H. Liu, D. Tao, Y. Wang, K. Lu category:cs.LG cs.MM published:2013-12-21 summary:With the rapid advance of Internet technology and smart devices, users oftenneed to manage large amounts of multimedia information using smart devices,such as personal image and video accessing and browsing. These requirementsheavily rely on the success of image (video) annotation, and thus large scaleimage annotation through innovative machine learning methods has attractedintensive attention in recent years. One representative work is support vectormachine (SVM). Although it works well in binary classification, SVM has anon-smooth loss function and can not naturally cover multi-class case. In thispaper, we propose manifold regularized kernel logistic regression (KLR) for webimage annotation. Compared to SVM, KLR has the following advantages: (1) theKLR has a smooth loss function; (2) the KLR produces an explicit estimate ofthe probability instead of class label; and (3) the KLR can naturally begeneralized to the multi-class case. We carefully conduct experiments on MIRFLICKR dataset and demonstrate the effectiveness of manifold regularized kernellogistic regression for image annotation.
arxiv-4800-243 | Learned versus Hand-Designed Feature Representations for 3d Agglomeration | http://arxiv.org/pdf/1312.6159v1.pdf | author:John A. Bogovic, Gary B. Huang, Viren Jain category:cs.CV published:2013-12-20 summary:For image recognition and labeling tasks, recent results suggest that machinelearning methods that rely on manually specified feature representations may beoutperformed by methods that automatically derive feature representations basedon the data. Yet for problems that involve analysis of 3d objects, such as meshsegmentation, shape retrieval, or neuron fragment agglomeration, there remainsa strong reliance on hand-designed feature descriptors. In this paper, weevaluate a large set of hand-designed 3d feature descriptors alongside featureslearned from the raw data using both end-to-end and unsupervised learningtechniques, in the context of agglomeration of 3d neuron fragments. Bycombining unsupervised learning techniques with a novel dynamic pooling scheme,we show how pure learning-based methods are for the first time competitive withhand-designed 3d shape descriptors. We investigate data augmentation strategiesfor dramatically increasing the size of the training set, and show howcombining both learned and hand-designed features leads to the highestaccuracy.
arxiv-4800-244 | The return of AdaBoost.MH: multi-class Hamming trees | http://arxiv.org/pdf/1312.6086v1.pdf | author:Balázs Kégl category:cs.LG published:2013-12-20 summary:Within the framework of AdaBoost.MH, we propose to train vector-valueddecision trees to optimize the multi-class edge without reducing themulti-class problem to $K$ binary one-against-all classifications. The keyelement of the method is a vector-valued decision stump, factorized into aninput-independent vector of length $K$ and label-independent scalar classifier.At inner tree nodes, the label-dependent vector is discarded and the binaryclassifier can be used for partitioning the input space into two regions. Thealgorithm retains the conceptual elegance, power, and computational efficiencyof binary AdaBoost. In experiments it is on par with support vector machinesand with the best existing multi-class boosting algorithm AOSOLogitBoost, andit is significantly better than other known implementations of AdaBoost.MH.
arxiv-4800-245 | Using Web Co-occurrence Statistics for Improving Image Categorization | http://arxiv.org/pdf/1312.5697v2.pdf | author:Samy Bengio, Jeff Dean, Dumitru Erhan, Eugene Ie, Quoc Le, Andrew Rabinovich, Jonathon Shlens, Yoram Singer category:cs.CV cs.LG published:2013-12-19 summary:Object recognition and localization are important tasks in computer vision.The focus of this work is the incorporation of contextual information in orderto improve object recognition and localization. For instance, it is natural toexpect not to see an elephant to appear in the middle of an ocean. We considera simple approach to encapsulate such common sense knowledge usingco-occurrence statistics from web documents. By merely counting the number oftimes nouns (such as elephants, sharks, oceans, etc.) co-occur in webdocuments, we obtain a good estimate of expected co-occurrences in visual data.We then cast the problem of combining textual co-occurrence statistics with thepredictions of image-based classifiers as an optimization problem. Theresulting optimization problem serves as a surrogate for our inferenceprocedure. Albeit the simplicity of the resulting optimization problem, it iseffective in improving both recognition and localization accuracy. Concretely,we observe significant improvements in recognition and localization rates forboth ImageNet Detection 2012 and Sun 2012 datasets.
arxiv-4800-246 | Occupancy Detection in Vehicles Using Fisher Vector Image Representation | http://arxiv.org/pdf/1312.6024v1.pdf | author:Yusuf Artan, Peter Paul category:cs.CV published:2013-12-20 summary:Due to the high volume of traffic on modern roadways, transportation agencieshave proposed High Occupancy Vehicle (HOV) lanes and High Occupancy Tolling(HOT) lanes to promote car pooling. However, enforcement of the rules of theselanes is currently performed by roadside enforcement officers using visualobservation. Manual roadside enforcement is known to be inefficient, costly,potentially dangerous, and ultimately ineffective. Violation rates up to50%-80% have been reported, while manual enforcement rates of less than 10% aretypical. Therefore, there is a need for automated vehicle occupancy detectionto support HOV/HOT lane enforcement. A key component of determining vehicleoccupancy is to determine whether or not the vehicle's front passenger seat isoccupied. In this paper, we examine two methods of determining vehicle frontseat occupancy using a near infrared (NIR) camera system pointed at thevehicle's front windshield. The first method examines a state-of-the-artdeformable part model (DPM) based face detection system that is robust tofacial pose. The second method examines state-of- the-art local aggregationbased image classification using bag-of-visual-words (BOW) and Fisher vectors(FV). A dataset of 3000 images was collected on a public roadway and is used toperform the comparison. From these experiments it is clear that the imageclassification approach is superior for this problem.
arxiv-4800-247 | High-Dimensional Regression with Gaussian Mixtures and Partially-Latent Response Variables | http://arxiv.org/pdf/1308.2302v3.pdf | author:Antoine Deleforge, Florence Forbes, Radu Horaud category:cs.LG stat.ML published:2013-08-10 summary:In this work we address the problem of approximating high-dimensional datawith a low-dimensional representation. We make the following contributions. Wepropose an inverse regression method which exchanges the roles of input andresponse, such that the low-dimensional variable becomes the regressor, andwhich is tractable. We introduce a mixture of locally-linear probabilisticmapping model that starts with estimating the parameters of inverse regression,and follows with inferring closed-form solutions for the forward parameters ofthe high-dimensional regression problem of interest. Moreover, we introduce apartially-latent paradigm, such that the vector-valued response variable iscomposed of both observed and latent entries, thus being able to deal with datacontaminated by experimental artifacts that cannot be explained with noisemodels. The proposed probabilistic formulation could be viewed as alatent-variable augmentation of regression. We devise expectation-maximization(EM) procedures based on a data augmentation strategy which facilitates themaximum-likelihood search over the model parameters. We propose twoaugmentation schemes and we describe in detail the associated EM inferenceprocedures that may well be viewed as generalizations of a number of EMregression, dimension reduction, and factor analysis algorithms. The proposedframework is validated with both synthetic and real data. We provideexperimental evidence that our method outperforms several existing regressiontechniques.
arxiv-4800-248 | Simple Methods for Initializing the EM Algorithm for Gaussian Mixture Models | http://arxiv.org/pdf/1312.5946v1.pdf | author:Johannes Blömer, Kathrin Bujna category:cs.LG published:2013-12-20 summary:In this paper, we consider simple and fast approaches to initialize theExpectation-Maximization algorithm (EM) for multivariate Gaussian mixturemodels. We present new initialization methods based on the well-known$K$-means++ algorithm and the Gonzalez algorithm. These methods close the gapbetween simple uniform initialization techniques and complex methods, that havebeen specifically designed for Gaussian mixture models and depend on the rightchoice of hyperparameters. In our evaluation we compare our methods with acommonly used random initialization method, an approach based on agglomerativehierarchical clustering, and a known, plain adaption of the Gonzalez algorithm.Our results indicate that algorithms based on $K$-means++ outperform the othermethods.
arxiv-4800-249 | The Sparse Principal Component of a Constant-rank Matrix | http://arxiv.org/pdf/1312.5891v1.pdf | author:Megasthenis Asteris, Dimitris S. Papailiopoulos, George N. Karystinos category:cs.IT math.IT stat.ML published:2013-12-20 summary:The computation of the sparse principal component of a matrix is equivalentto the identification of its principal submatrix with the largest maximumeigenvalue. Finding this optimal submatrix is what renders the problem${\mathcal{NP}}$-hard. In this work, we prove that, if the matrix is positivesemidefinite and its rank is constant, then its sparse principal component ispolynomially computable. Our proof utilizes the auxiliary unit vector techniquethat has been recently developed to identify problems that are polynomiallysolvable. Moreover, we use this technique to design an algorithm which, for anysparsity value, computes the sparse principal component with complexity${\mathcal O}\left(N^{D+1}\right)$, where $N$ and $D$ are the matrix size andrank, respectively. Our algorithm is fully parallelizable and memory efficient.
arxiv-4800-250 | Non-parametric Bayesian modeling of complex networks | http://arxiv.org/pdf/1312.5889v1.pdf | author:Mikkel N. Schmidt, Morten Mørup category:stat.ML published:2013-12-20 summary:Modeling structure in complex networks using Bayesian non-parametrics makesit possible to specify flexible model structures and infer the adequate modelcomplexity from the observed data. This paper provides a gentle introduction tonon-parametric Bayesian modeling of complex networks: Using an infinite mixturemodel as running example we go through the steps of deriving the model as aninfinite limit of a finite parametric model, inferring the model parameters byMarkov chain Monte Carlo, and checking the model's fit and predictiveperformance. We explain how advanced non-parametric models for complex networkscan be derived and point out relevant literature.
arxiv-4800-251 | Optimal parameter selection for unsupervised neural network using genetic algorithm | http://arxiv.org/pdf/1312.5814v1.pdf | author:suneetha chittineni, Raveendra Babu Bhogapathi category:cs.NE published:2013-12-20 summary:K-means Fast Learning Artificial Neural Network (K-FLANN) is an unsupervisedneural network requires two parameters: tolerance and vigilance. BestClustering results are feasible only by finest parameters specified to theneural network. Selecting optimal values for these parameters is a majorproblem. To solve this issue, Genetic Algorithm (GA) is used to determineoptimal parameters of K-FLANN for finding groups in multidimensional data.K-FLANN is a simple topological network, in which output nodes growsdynamically during the clustering process on receiving input patterns. OriginalK-FLANN is enhanced to select winner unit out of the matched nodes so thatstable clusters are formed with in a less number of epochs. The experimentalresults show that the GA is efficient in finding optimal values of parametersfrom the large search space and is tested using artificial and synthetic datasets.
arxiv-4800-252 | Unsupervised Feature Learning by Deep Sparse Coding | http://arxiv.org/pdf/1312.5783v1.pdf | author:Yunlong He, Koray Kavukcuoglu, Yun Wang, Arthur Szlam, Yanjun Qi category:cs.LG cs.CV cs.NE published:2013-12-20 summary:In this paper, we propose a new unsupervised feature learning framework,namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layerarchitecture for visual object recognition tasks. The main innovation of theframework is that it connects the sparse-encoders from different layers by asparse-to-dense module. The sparse-to-dense module is a composition of a localspatial pooling step and a low-dimensional embedding process, which takesadvantage of the spatial smoothness information in the image. As a result, thenew method is able to learn several levels of sparse representation of theimage which capture features at a variety of abstraction levels andsimultaneously preserve the spatial smoothness between the neighboring imagepatches. Combining the feature representations from multiple layers, DeepSCachieves the state-of-the-art performance on multiple object recognition tasks.
arxiv-4800-253 | Time-varying Learning and Content Analytics via Sparse Factor Analysis | http://arxiv.org/pdf/1312.5734v1.pdf | author:Andrew S. Lan, Christoph Studer, Richard G. Baraniuk category:stat.ML cs.LG math.OC stat.AP published:2013-12-19 summary:We propose SPARFA-Trace, a new machine learning-based framework fortime-varying learning and content analytics for education applications. Wedevelop a novel message passing-based, blind, approximate Kalman filter forsparse factor analysis (SPARFA), that jointly (i) traces learner conceptknowledge over time, (ii) analyzes learner concept knowledge state transitions(induced by interacting with learning resources, such as textbook sections,lecture videos, etc, or the forgetting effect), and (iii) estimates the contentorganization and intrinsic difficulty of the assessment questions. Thesequantities are estimated solely from binary-valued (correct/incorrect) gradedlearner response data and a summary of the specific actions each learnerperforms (e.g., answering a question or studying a learning resource) at eachtime instance. Experimental results on two online course datasets demonstratethat SPARFA-Trace is capable of tracing each learner's concept knowledgeevolution over time, as well as analyzing the quality and content organizationof learning resources, the question-concept associations, and the questionintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparableor better performance in predicting unobserved learner responses than existingcollaborative filtering and knowledge tracing approaches for personalizededucation.
arxiv-4800-254 | Flower Pollination Algorithm for Global Optimization | http://arxiv.org/pdf/1312.5673v1.pdf | author:Xin-She Yang category:math.OC cs.NE nlin.AO published:2013-12-19 summary:Flower pollination is an intriguing process in the natural world. Itsevolutionary characteristics can be used to design new optimization algorithms.In this paper, we propose a new algorithm, namely, flower pollinationalgorithm, inspired by the pollination process of flowers. We first use tentest functions to validate the new algorithm, and compare its performance withgenetic algorithms and particle swarm optimization. Our simulation results showthe flower algorithm is more efficient than both GA and PSO. We also use theflower algorithm to solve a nonlinear design benchmark, which shows theconvergence rate is almost exponential.
arxiv-4800-255 | Abstraction in decision-makers with limited information processing capabilities | http://arxiv.org/pdf/1312.4353v2.pdf | author:Tim Genewein, Daniel A. Braun category:cs.AI cs.IT math.IT stat.ML published:2013-12-16 summary:A distinctive property of human and animal intelligence is the ability toform abstractions by neglecting irrelevant information which allows to separatestructure from noise. From an information theoretic point of view abstractionsare desirable because they allow for very efficient information processing. Inartificial systems abstractions are often implemented through computationallycostly formations of groups or clusters. In this work we establish the relationbetween the free-energy framework for decision making and rate-distortiontheory and demonstrate how the application of rate-distortion fordecision-making leads to the emergence of abstractions. We argue thatabstractions are induced due to a limit in information processing capacity.
arxiv-4800-256 | Playing Atari with Deep Reinforcement Learning | http://arxiv.org/pdf/1312.5602v1.pdf | author:Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller category:cs.LG published:2013-12-19 summary:We present the first deep learning model to successfully learn controlpolicies directly from high-dimensional sensory input using reinforcementlearning. The model is a convolutional neural network, trained with a variantof Q-learning, whose input is raw pixels and whose output is a value functionestimating future rewards. We apply our method to seven Atari 2600 games fromthe Arcade Learning Environment, with no adjustment of the architecture orlearning algorithm. We find that it outperforms all previous approaches on sixof the games and surpasses a human expert on three of them.
arxiv-4800-257 | An Adaptive Dictionary Learning Approach for Modeling Dynamical Textures | http://arxiv.org/pdf/1312.5568v1.pdf | author:Xian Wei, Hao Shen, Martin Kleinsteuber category:cs.CV published:2013-12-19 summary:Video representation is an important and challenging task in the computervision community. In this paper, we assume that image frames of a moving scenecan be modeled as a Linear Dynamical System. We propose a sparse codingframework, named adaptive video dictionary learning (AVDL), to model a videoadaptively. The developed framework is able to capture the dynamics of a movingscene by exploring both sparse properties and the temporal correlations ofconsecutive video frames. The proposed method is compared with state of the artvideo processing methods on several benchmark data sequences, which exhibitappearance changes and heavy occlusions.
arxiv-4800-258 | My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013 | http://arxiv.org/pdf/1312.5548v1.pdf | author:Jürgen Schmidhuber category:cs.NE published:2013-12-19 summary:Deep Learning has attracted significant attention in recent years. Here Ipresent a brief overview of my first Deep Learner of 1991, and its historiccontext, with a timeline of Deep Learning highlights.
arxiv-4800-259 | Deep Learning Embeddings for Discontinuous Linguistic Units | http://arxiv.org/pdf/1312.5129v2.pdf | author:Wenpeng Yin, Hinrich Schütze category:cs.CL published:2013-12-18 summary:Deep learning embeddings have been successfully used for many naturallanguage processing problems. Embeddings are mostly computed for word formsalthough a number of recent papers have extended this to other linguistic unitslike morphemes and phrases. In this paper, we argue that learning embeddingsfor discontinuous linguistic units should also be considered. In anexperimental evaluation on coreference resolution, we show that such embeddingsperform better than word form embeddings.
arxiv-4800-260 | Codebook based Audio Feature Representation for Music Information Retrieval | http://arxiv.org/pdf/1312.5457v1.pdf | author:Yonatan Vaizman, Brian McFee, Gert Lanckriet category:cs.IR cs.LG cs.MM published:2013-12-19 summary:Digital music has become prolific in the web in recent decades. Automatedrecommendation systems are essential for users to discover music they love andfor artists to reach appropriate audience. When manual annotations and userpreference data is lacking (e.g. for new artists) these systems must rely on\emph{content based} methods. Besides powerful machine learning tools forclassification and retrieval, a key component for successful recommendation isthe \emph{audio content representation}. Good representations should capture informative musical patterns in the audiosignal of songs. These representations should be concise, to enable efficient(low storage, easy indexing, fast search) management of huge musicrepositories, and should also be easy and fast to compute, to enable real-timeinteraction with a user supplying new songs to the system. Before designing new audio features, we explore the usage of traditionallocal features, while adding a stage of encoding with a pre-computed\emph{codebook} and a stage of pooling to get compact vectorialrepresentations. We experiment with different encoding methods, namely\emph{the LASSO}, \emph{vector quantization (VQ)} and \emph{cosine similarity(CS)}. We evaluate the representations' quality in two music informationretrieval applications: query-by-tag and query-by-example. Our results showthat concise representations can be used for successful performance in bothapplications. We recommend using top-$\tau$ VQ encoding, which consistentlyperforms well in both applications, and requires much less computation timethan the LASSO.
arxiv-4800-261 | Delegating Custom Object Detection Tasks to a Universal Classification System | http://arxiv.org/pdf/1401.6126v1.pdf | author:Andrew Gleibman category:cs.CV 68T10 published:2013-12-19 summary:In this paper, a concept of multipurpose object detection system, recentlyintroduced in our previous work, is clarified. The business aspect of thismethod is transformation of a classifier into an object detector/locator via animage grid. This is a universal framework for locating objects of interestthrough classification. The framework standardizes and simplifiesimplementation of custom systems by doing only a custom analysis of theclassification results on the image grid.
arxiv-4800-262 | Some Improvements on Deep Convolutional Neural Network Based Image Classification | http://arxiv.org/pdf/1312.5402v1.pdf | author:Andrew G. Howard category:cs.CV published:2013-12-19 summary:We investigate multiple techniques to improve upon the current state of theart deep convolutional neural network based image classification pipeline. Thetechiques include adding more image transformations to training data, addingmore transformations to generate additional predictions at test time and usingcomplementary models applied to higher resolution images. This paper summarizesour entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Oursystem achieved a top 5 classification error rate of 13.55% using no externaldata which is over a 20% relative improvement on the previous year's winner.
arxiv-4800-263 | Missing Value Imputation With Unsupervised Backpropagation | http://arxiv.org/pdf/1312.5394v1.pdf | author:Michael S. Gashler, Michael R. Smith, Richard Morris, Tony Martinez category:cs.NE cs.LG stat.ML published:2013-12-19 summary:Many data mining and data analysis techniques operate on dense matrices orcomplete tables of data. Real-world data sets, however, often contain unknownvalues. Even many classification algorithms that are designed to operate withmissing values still exhibit deteriorated accuracy. One approach to handlingmissing values is to fill in (impute) the missing values. In this paper, wepresent a technique for unsupervised learning called UnsupervisedBackpropagation (UBP), which trains a multi-layer perceptron to fit to themanifold sampled by a set of observed point-vectors. We evaluate UBP with thetask of imputing missing values in datasets, and show that UBP is able topredict missing values with significantly lower sum-squared error than othercollaborative filtering and imputation techniques. We also demonstrate with 24datasets and 9 supervised learning algorithms that classification accuracy isusually higher when randomly-withheld values are imputed using UBP, rather thanwith other methods.
arxiv-4800-264 | Detecting Parameter Symmetries in Probabilistic Models | http://arxiv.org/pdf/1312.5386v1.pdf | author:Robert Nishihara, Thomas Minka, Daniel Tarlow category:stat.ML published:2013-12-19 summary:Probabilistic models often have parameters that can be translated, scaled,permuted, or otherwise transformed without changing the model. These symmetriescan lead to strong correlation and multimodality in the posterior distributionover the model's parameters, which can pose challenges both for performinginference and interpreting the results. In this work, we address the automaticdetection of common problematic model symmetries. To do so, we introduce localsymmetries, which cover many common cases and are amenable to automaticdetection. We show how to derive algorithms to detect several broad classes oflocal symmetries. Our algorithms are compatible with probabilistic programmingconstructs such as arrays, for loops, and if statements, and they scale tomodels with many variables.
arxiv-4800-265 | Perturbed Gibbs Samplers for Synthetic Data Release | http://arxiv.org/pdf/1312.5370v1.pdf | author:Yubin Park, Joydeep Ghosh category:stat.ML stat.AP published:2013-12-18 summary:We propose a categorical data synthesizer with a quantifiable disclosurerisk. Our algorithm, named Perturbed Gibbs Sampler, can handle high-dimensionalcategorical data that are often intractable to represent as contingency tables.The algorithm extends a multiple imputation strategy for fully synthetic databy utilizing feature hashing and non-parametric distribution approximations.California Patient Discharge data are used to demonstrate statisticalproperties of the proposed synthesizing methodology. Marginal and conditionaldistributions, as well as the coefficients of regression models built on thesynthesized data are compared to those obtained from the original data.Intruder scenarios are simulated to evaluate disclosure risks of thesynthesized data from multiple angles. Limitations and extensions of theproposed algorithm are also discussed.
arxiv-4800-266 | Generative NeuroEvolution for Deep Learning | http://arxiv.org/pdf/1312.5355v1.pdf | author:Phillip Verbancsics, Josh Harguess category:cs.NE cs.CV published:2013-12-18 summary:An important goal for the machine learning (ML) community is to createapproaches that can learn solutions with human-level capability. One domainwhere humans have held a significant advantage is visual processing. Asignificant approach to addressing this gap has been machine learningapproaches that are inspired from the natural systems, such as artificialneural networks (ANNs), evolutionary computation (EC), and generative anddevelopmental systems (GDS). Research into deep learning has demonstrated thatsuch architectures can achieve performance competitive with humans on somevisual tasks; however, these systems have been primarily trained throughsupervised and unsupervised learning algorithms. Alternatively, research isshowing that evolution may have a significant role in the development of visualsystems. Thus this paper investigates the role neuro-evolution (NE) can take indeep learning. In particular, the Hypercube-based NeuroEvolution of AugmentingTopologies is a NE approach that can effectively learn large neural structuresby training an indirect encoding that compresses the ANN weight pattern as afunction of geometry. The results show that HyperNEAT struggles with performingimage classification by itself, but can be effective in training a featureextractor that other ML approaches can learn from. Thus NeuroEvolution combinedwith other ML methods provides an intriguing area of research that canreplicate the processes in nature.
arxiv-4800-267 | SOMz: photometric redshift PDFs with self organizing maps and random atlas | http://arxiv.org/pdf/1312.5753v1.pdf | author:M. Carrasco Kind, R. J. Brunner category:astro-ph.IM astro-ph.CO cs.LG stat.ML published:2013-12-18 summary:In this paper we explore the applicability of the unsupervised machinelearning technique of Self Organizing Maps (SOM) to estimate galaxy photometricredshift probability density functions (PDFs). This technique takes aspectroscopic training set, and maps the photometric attributes, but not theredshifts, to a two dimensional surface by using a process of competitivelearning where neurons compete to more closely resemble the training datamultidimensional space. The key feature of a SOM is that it retains thetopology of the input set, revealing correlations between the attributes thatare not easily identified. We test three different 2D topological mapping:rectangular, hexagonal, and spherical, by using data from the DEEP2 survey. Wealso explore different implementations and boundary conditions on the map andalso introduce the idea of a random atlas where a large number of differentmaps are created and their individual predictions are aggregated to produce amore robust photometric redshift PDF. We also introduced a new metric, the$I$-score, which efficiently incorporates different metrics, making it easierto compare different results (from different parameters or differentphotometric redshift codes). We find that by using a spherical topology mappingwe obtain a better representation of the underlying multidimensional topology,which provides more accurate results that are comparable to other,state-of-the-art machine learning algorithms. Our results illustrate thatunsupervised approaches have great potential for many astronomical problems,and in particular for the computation of photometric redshifts.
arxiv-4800-268 | Systematic and multifactor risk models revisited | http://arxiv.org/pdf/1312.5271v1.pdf | author:Michel Fliess, Cédric Join category:q-fin.RM cs.CE math.LO q-fin.CP stat.ML published:2013-12-18 summary:Systematic and multifactor risk models are revisited via methods which werealready successfully developed in signal processing and in automatic control.The results, which bypass the usual criticisms on those risk modeling, areillustrated by several successful computer experiments.
arxiv-4800-269 | Shape Tracking With Occlusions via Coarse-To-Fine Region-Based Sobolev Descent | http://arxiv.org/pdf/1208.4391v2.pdf | author:Yanchao Yang, Ganesh Sundaramoorthi category:cs.CV cs.SY published:2012-08-21 summary:We present a method to track the precise shape of an object in video based onnew modeling and optimization on a new Riemannian manifold of parameterizedregions. Joint dynamic shape and appearance models, in which a template of the objectis propagated to match the object shape and radiance in the next frame, areadvantageous over methods employing global image statistics in cases of complexobject radiance and cluttered background. In cases of 3D object motion andviewpoint change, self-occlusions and dis-occlusions of the object areprominent, and current methods employing joint shape and appearance models areunable to adapt to new shape and appearance information, leading to inaccurateshape detection. In this work, we model self-occlusions and dis-occlusions in ajoint shape and appearance tracking framework. Self-occlusions and the warp to propagate the template are coupled, thus ajoint problem is formulated. We derive a coarse-to-fine optimization scheme,advantageous in object tracking, that initially perturbs the template by coarseperturbations before transitioning to finer-scale perturbations, traversing allscales, seamlessly and automatically. The scheme is a gradient descent on anovel infinite-dimensional Riemannian manifold that we introduce. The manifoldconsists of planar parameterized regions, and the metric that we introduce is anovel Sobolev-type metric defined on infinitesimal vector fields on regions.The metric has the property of resulting in a gradient descent thatautomatically favors coarse-scale deformations (when they reduce the energy)before moving to finer-scale deformations. Experiments on video exhibiting occlusion/dis-occlusion, complex radiance andbackground show that occlusion/dis-occlusion modeling leads to superior shapeaccuracy compared to recent methods employing joint shape/appearance models oremploying global statistics.
arxiv-4800-270 | A U-statistic estimator for the variance of resampling-based error estimators | http://arxiv.org/pdf/1310.8203v2.pdf | author:Mathias Fuchs, Roman Hornung, Riccardo De Bin, Anne-Laure Boulesteix category:math.ST stat.ML stat.TH published:2013-10-30 summary:We revisit resampling procedures for error estimation in binaryclassification in terms of U-statistics. In particular, we exploit the factthat the error rate estimator involving all learning-testing splits is aU-statistic. Thus, it has minimal variance among all unbiased estimators and isasymptotically normally distributed. Moreover, there is an unbiased estimatorfor this minimal variance if the total sample size is at least the doublelearning set size plus two. In this case, we exhibit such an estimator which isanother U-statistic. It enjoys, again, various optimality properties and yieldsan asymptotically exact hypothesis test of the equality of error rates when twolearning algorithms are compared. Our statements apply to any deterministiclearning algorithms under weak non-degeneracy assumptions.
arxiv-4800-271 | The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited | http://arxiv.org/pdf/1312.5179v1.pdf | author:Matthias Hein, Simon Setzer, Leonardo Jost, Syama Sundar Rangapuram category:stat.ML cs.LG math.OC published:2013-12-18 summary:Hypergraphs allow one to encode higher-order relationships in data and arethus a very flexible modeling tool. Current learning methods are either basedon approximations of the hypergraphs via graphs or on tensor methods which areonly applicable under special conditions. In this paper, we present a newlearning framework on hypergraphs which fully uses the hypergraph structure.The key element is a family of regularization functionals based on the totalvariation on hypergraphs.
arxiv-4800-272 | Permuted NMF: A Simple Algorithm Intended to Minimize the Volume of the Score Matrix | http://arxiv.org/pdf/1312.5124v1.pdf | author:Paul Fogel category:stat.AP cs.LG stat.ML published:2013-12-18 summary:Non-Negative Matrix Factorization, NMF, attempts to find a number ofarchetypal response profiles, or parts, such that any sample profile in thedataset can be approximated by a close profile among these archetypes or alinear combination of these profiles. The non-negativity constraint is imposedwhile estimating archetypal profiles, due to the non-negative nature of theobserved signal. Apart from non negativity, a volume constraint can be appliedon the Score matrix W to enhance the ability of learning parts of NMF. In thisreport, we describe a very simple algorithm, which in effect achieves volumeminimization, although indirectly.
arxiv-4800-273 | Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search | http://arxiv.org/pdf/1205.3109v4.pdf | author:Arthur Guez, David Silver, Peter Dayan category:cs.LG cs.AI stat.ML published:2012-05-14 summary:Bayesian model-based reinforcement learning is a formally elegant approach tolearning optimal behaviour under model uncertainty, trading off exploration andexploitation in an ideal way. Unfortunately, finding the resultingBayes-optimal policies is notoriously taxing, since the search space becomesenormous. In this paper we introduce a tractable, sample-based method forapproximate Bayes-optimal planning which exploits Monte-Carlo tree search. Ourapproach outperformed prior Bayesian model-based RL algorithms by a significantmargin on several well-known benchmark problems -- because it avoids expensiveapplications of Bayes rule within the search tree by lazily sampling modelsfrom the current beliefs. We illustrate the advantages of our approach byshowing it working in an infinite state space domain which is qualitatively outof reach of almost all previous work in Bayesian exploration.
arxiv-4800-274 | Functional Bipartite Ranking: a Wavelet-Based Filtering Approach | http://arxiv.org/pdf/1312.5066v1.pdf | author:Stéphan Clémençon, Marine Depecker category:stat.ML published:2013-12-18 summary:It is the main goal of this article to address the bipartite ranking issuefrom the perspective of functional data analysis (FDA). Given a training set ofindependent realizations of a (possibly sampled) second-order random functionwith a (locally) smooth autocorrelation structure and to which a binary labelis randomly assigned, the objective is to learn a scoring function s withoptimal ROC curve. Based on linear/nonlinear wavelet-based approximations, itis shown how to select compact finite dimensional representations of the inputcurves adaptively, in order to build accurate ranking rules, using recentadvances in the ranking problem for multivariate data with binary feedback.Beyond theoretical considerations, the performance of the learning methods forfunctional bipartite ranking proposed in this paper are illustrated bynumerical experiments.
arxiv-4800-275 | Comparative analysis of evolutionary algorithms for image enhancement | http://arxiv.org/pdf/1312.5045v1.pdf | author:Anupriya Gogna, Akash Tayal category:cs.CV cs.NE published:2013-12-18 summary:Evolutionary algorithms are metaheuristic techniques that derive inspirationfrom the natural process of evolution. They can efficiently solve (generateacceptable quality of solution in reasonable time) complex optimization(NP-Hard) problems. In this paper, automatic image enhancement is considered asan optimization problem and three evolutionary algorithms (Genetic Algorithm,Differential Evolution and Self Organizing Migration Algorithm) are employed tosearch for an optimum solution. They are used to find an optimum parameter setfor an image enhancement transfer function. The aim is to maximize a fitnesscriterion which is a measure of image contrast and the visibility of details inthe enhanced image. The enhancement results obtained using all threeevolutionary algorithms are compared amongst themselves and also with theoutput of histogram equalization method.
arxiv-4800-276 | Evaluation of Plane Detection with RANSAC According to Density of 3D Point Clouds | http://arxiv.org/pdf/1312.5033v1.pdf | author:Tomofumi Fujiwara, Tetsushi Kamegawa, Akio Gofuku category:cs.RO cs.CV published:2013-12-18 summary:We have implemented a method that detects planar regions from 3D scan datausing Random Sample Consensus (RANSAC) algorithm to address the issue of atrade-off between the scanning speed and the point density of 3D scanning.However, the limitation of the implemented method has not been clear yet. Inthis paper, we conducted an additional experiment to evaluate the implementedmethod by changing its parameter and environments in both high and low pointdensity data. As a result, the number of detected planes in high point densitydata was different from that in low point density data with the same parametervalue.
arxiv-4800-277 | Contextually Supervised Source Separation with Application to Energy Disaggregation | http://arxiv.org/pdf/1312.5023v1.pdf | author:Matt Wytock, J. Zico Kolter category:stat.ML cs.LG math.OC published:2013-12-18 summary:We propose a new framework for single-channel source separation that liesbetween the fully supervised and unsupervised setting. Instead of supervision,we provide input features for each source signal and use convex methods toestimate the correlations between these features and the unobserved signaldecomposition. We analyze the case of $\ell_2$ loss theoretically and show thatrecovery of the signal components depends only on cross-correlation betweenfeatures for different signals, not on correlations between features for thesame signal. Contextually supervised source separation is a natural fit fordomains with large amounts of data but no explicit supervision; our motivatingapplication is energy disaggregation of hourly smart meter data (the separationof whole-home power signals into different energy uses). Here we applycontextual supervision to disaggregate the energy usage of thousands homes overfour years, a significantly larger scale than previously published efforts, anddemonstrate on synthetic data that our method outperforms the unsupervisedapproach.
arxiv-4800-278 | Efficient Online Bootstrapping for Large Scale Learning | http://arxiv.org/pdf/1312.5021v1.pdf | author:Zhen Qin, Vaclav Petricek, Nikos Karampatziakis, Lihong Li, John Langford category:cs.LG published:2013-12-18 summary:Bootstrapping is a useful technique for estimating the uncertainty of apredictor, for example, confidence intervals for prediction. It is typicallyused on small to moderate sized datasets, due to its high computation cost.This work describes a highly scalable online bootstrapping strategy,implemented inside Vowpal Wabbit, that is several times faster than traditionalstrategies. Our experiments indicate that, in addition to providing a blackbox-like method for estimating uncertainty, our implementation of onlinebootstrapping may also help to train models with better prediction performancedue to model averaging.
arxiv-4800-279 | A Comparative Evaluation of Curriculum Learning with Filtering and Boosting | http://arxiv.org/pdf/1312.4986v1.pdf | author:Michael R. Smith, Tony Martinez category:cs.LG published:2013-12-17 summary:Not all instances in a data set are equally beneficial for inferring a modelof the data. Some instances (such as outliers) are detrimental to inferring amodel of the data. Several machine learning techniques treat instances in adata set differently during training such as curriculum learning, filtering,and boosting. However, an automated method for determining how beneficial aninstance is for inferring a model of the data does not exist. In this paper, wepresent an automated method that orders the instances in a data set bycomplexity based on the their likelihood of being misclassified (instancehardness). The underlying assumption of this method is that instances with ahigh likelihood of being misclassified represent more complex concepts in adata set. Ordering the instances in a data set allows a learning algorithm tofocus on the most beneficial instances and ignore the detrimental ones. Wecompare ordering the instances in a data set in curriculum learning, filteringand boosting. We find that ordering the instances significantly increasesclassification accuracy and that filtering has the largest impact onclassification accuracy. On a set of 52 data sets, ordering the instancesincreases the average accuracy from 81% to 84%.
arxiv-4800-280 | Recursive Compressed Sensing | http://arxiv.org/pdf/1312.4895v1.pdf | author:Nikolaos M. Freris, Orhan Öçal, Martin Vetterli category:stat.ML cs.IT math.IT 94 published:2013-12-17 summary:We introduce a recursive algorithm for performing compressed sensing onstreaming data. The approach consists of a) recursive encoding, where we samplethe input stream via overlapping windowing and make use of the previousmeasurement in obtaining the next one, and b) recursive decoding, where thesignal estimate from the previous window is utilized in order to achieve fasterconvergence in an iterative optimization scheme applied to decode the new one.To remove estimation bias, a two-step estimation procedure is proposedcomprising support set detection and signal amplitude estimation. Estimationaccuracy is enhanced by a non-linear voting method and averaging estimates overmultiple windows. We analyze the computational complexity and estimation error,and show that the normalized error variance asymptotically goes to zero forsublinear sparsity. Our simulation results show speed up of an order ofmagnitude over traditional CS, while obtaining significantly lowerreconstruction error under mild conditions on the signal magnitudes and thenoise level.
arxiv-4800-281 | Identification of Gaussian Process State-Space Models with Particle Stochastic Approximation EM | http://arxiv.org/pdf/1312.4852v1.pdf | author:Roger Frigola, Fredrik Lindsten, Thomas B. Schön, Carl E. Rasmussen category:stat.ML cs.SY published:2013-12-17 summary:Gaussian process state-space models (GP-SSMs) are a very flexible family ofmodels of nonlinear dynamical systems. They comprise a Bayesian nonparametricrepresentation of the dynamics of the system and additional (hyper-)parametersgoverning the properties of this nonparametric representation. The Bayesianformalism enables systematic reasoning about the uncertainty in the systemdynamics. We present an approach to maximum likelihood identification of theparameters in GP-SSMs, while retaining the full nonparametric description ofthe dynamics. The method is based on a stochastic approximation version of theEM algorithm that employs recent developments in particle Markov chain MonteCarlo for efficient identification.
arxiv-4800-282 | Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC | http://arxiv.org/pdf/1306.2861v2.pdf | author:Roger Frigola, Fredrik Lindsten, Thomas B. Schön, Carl E. Rasmussen category:stat.ML cs.LG cs.SY published:2013-06-12 summary:State-space models are successfully used in many areas of science,engineering and economics to model time series and dynamical systems. Wepresent a fully Bayesian approach to inference \emph{and learning} (i.e. stateestimation and system identification) in nonlinear nonparametric state-spacemodels. We place a Gaussian process prior over the state transition dynamics,resulting in a flexible model able to capture complex dynamical phenomena. Toenable efficient inference, we marginalize over the transition dynamicsfunction and infer directly the joint smoothing distribution using speciallytailored Particle Markov Chain Monte Carlo samplers. Once a sample from thesmoothing distribution is computed, the state transition predictivedistribution can be formulated analytically. Our approach preserves the fullnonparametric expressivity of the model and can make use of sparse Gaussianprocesses to greatly reduce computational complexity.
arxiv-4800-283 | Matrix recovery using Split Bregman | http://arxiv.org/pdf/1312.6872v1.pdf | author:Anupriya Gogna, Ankita Shukla, Angshul Majumdar category:cs.NA cs.LG published:2013-12-17 summary:In this paper we address the problem of recovering a matrix, with inherentlow rank structure, from its lower dimensional projections. This problem isfrequently encountered in wide range of areas including pattern recognition,wireless sensor networks, control systems, recommender systems, image/videoreconstruction etc. Both in theory and practice, the most optimal way to solvethe low rank matrix recovery problem is via nuclear norm minimization. In thispaper, we propose a Split Bregman algorithm for nuclear norm minimization. Theuse of Bregman technique improves the convergence speed of our algorithm andgives a higher success rate. Also, the accuracy of reconstruction is muchbetter even for cases where small number of linear measurements are available.Our claim is supported by empirical results obtained using our algorithm andits comparison to other existing methods for matrix recovery. The algorithmsare compared on the basis of NMSE, execution time and success rate for varyingranks and sampling ratios.
arxiv-4800-284 | Performance Engineering for a Medical Imaging Application on the Intel Xeon Phi Accelerator | http://arxiv.org/pdf/1401.3615v1.pdf | author:Johannes Hofmann, Jan Treibig, Georg Hager, Gerhard Wellein category:cs.DC cs.CV cs.PF published:2013-12-17 summary:We examine the Xeon Phi, which is based on Intel's Many Integrated Coresarchitecture, for its suitability to run the FDK algorithm--the most commonlyused algorithm to perform the 3D image reconstruction in cone-beam computedtomography. We study the challenges of efficiently parallelizing theapplication and means to enable sensible data sharing between threads despitethe lack of a shared last level cache. Apart from parallelization, SIMDvectorization is critical for good performance on the Xeon Phi; we performvarious micro-benchmarks to investigate the platform's new set of vectorinstructions and put a special emphasis on the newly introduced vector gathercapability. We refine a previous performance model for the application andadapt it for the Xeon Phi to validate the performance of our optimizedhand-written assembly implementation, as well as the performance of severaldifferent auto-vectorization approaches.
arxiv-4800-285 | BW - Eye Ophthalmologic decision support system based on clinical workflow and data mining techniques-image registration algorithm | http://arxiv.org/pdf/1312.4752v1.pdf | author:Ricardo Martins category:cs.CV published:2013-12-17 summary:Blueworks - Medical Expert Diagnosis is developing an application, BWEye, tobe used as an ophthalmology consultation decision support system. Theimplementation of this application involves several different tasks and one ofthem is the implementation of an ophthalmology images registration algorithm.The work reported in this document is related with the implementation of analgorithm to register images of angiography, colour retinography and redfreeretinography. The implementations described were developed in the softwareMATLAB. The implemented algorithm is based in the detection of the bifurcation points(y-features) of the vascular structures of the retina that usually are visiblein the referred type of images. There are proposed two approaches to establishan initial set of features correspondences. The first approach is based in themaximization of the mutual information of the bifurcation regions of thefeatures of images. The second approach is based in the characterization ofeach bifurcation point and in the minimization of the Euclidean distancebetween the descriptions of the features of the images in the descriptorsspace. The final set of the matching features for a pair of images is definedthrough the application of the RANSAC algorithm. Although, it was not achieved the implementation of a full functionalalgorithm, there were made several analysis that can be important to futureimprovement of the current implementation.
arxiv-4800-286 | Co-Sparse Textural Similarity for Image Segmentation | http://arxiv.org/pdf/1312.4746v1.pdf | author:Claudia Nieuwenhuis, Daniel Cremers, Simon Hawe, Martin Kleinsteuber category:cs.CV published:2013-12-17 summary:We propose an algorithm for segmenting natural images based on texture andcolor information, which leverages the co-sparse analysis model for imagesegmentation within a convex multilabel optimization framework. As a keyingredient of this method, we introduce a novel textural similarity measure,which builds upon the co-sparse representation of image patches. We propose aBayesian approach to merge textural similarity with information about color andlocation. Combined with recently developed convex multilabel optimizationmethods this leads to an efficient algorithm for both supervised andunsupervised segmentation, which is easily parallelized on graphics hardware.The approach provides competitive results in unsupervised segmentation andoutperforms state-of-the-art interactive segmentation methods on the GrazBenchmark.
arxiv-4800-287 | The Bernstein Function: A Unifying Framework of Nonconvex Penalization in Sparse Estimation | http://arxiv.org/pdf/1312.4719v1.pdf | author:Zhihua Zhang category:stat.ML published:2013-12-17 summary:In this paper we study nonconvex penalization using Bernstein functions.Since the Bernstein function is concave and nonsmooth at the origin, it caninduce a class of nonconvex functions for high-dimensional sparse estimationproblems. We derive a threshold function based on the Bernstein penalty andgive its mathematical properties in sparsity modeling. We show that acoordinate descent algorithm is especially appropriate for penalized regressionproblems with the Bernstein penalty. Additionally, we prove that the Bernsteinfunction can be defined as the concave conjugate of a $\varphi$-divergence anddevelop a conjugate maximization algorithm for finding the sparse solution.Finally, we particularly exemplify a family of Bernstein nonconvex penaltiesbased on a generalized Gamma measure and conduct empirical analysis for thisfamily.
arxiv-4800-288 | The Matrix Ridge Approximation: Algorithms and Applications | http://arxiv.org/pdf/1312.4717v1.pdf | author:Zhihua Zhang category:stat.ML published:2013-12-17 summary:We are concerned with an approximation problem for a symmetric positivesemidefinite matrix due to motivation from a class of nonlinear machinelearning methods. We discuss an approximation approach that we call {matrixridge approximation}. In particular, we define the matrix ridge approximationas an incomplete matrix factorization plus a ridge term. Moreover, we presentprobabilistic interpretations using a normal latent variable model and aWishart model for this approximation approach. The idea behind the latentvariable model in turn leads us to an efficient EM iterative method forhandling the matrix ridge approximation problem. Finally, we illustrate theapplications of the approximation approach in multivariate data analysis.Empirical studies in spectral clustering and Gaussian process regression showthat the matrix ridge approximation with the EM iteration is potentiallyuseful.
arxiv-4800-289 | Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem | http://arxiv.org/pdf/1312.3393v2.pdf | author:Masrour Zoghi, Shimon Whiteson, Remi Munos, Maarten de Rijke category:cs.LG published:2013-12-12 summary:This paper proposes a new method for the K-armed dueling bandit problem, avariation on the regular K-armed bandit problem that offers only relativefeedback about pairs of arms. Our approach extends the Upper Confidence Boundalgorithm to the relative setting by using estimates of the pairwiseprobabilities to select a promising arm and applying Upper Confidence Boundwith the winner as a benchmark. We prove a finite-time regret bound of orderO(log t). In addition, our empirical results using real data from aninformation retrieval application show that it greatly outperforms the state ofthe art.
arxiv-4800-290 | Markov Network Structure Learning via Ensemble-of-Forests Models | http://arxiv.org/pdf/1312.4710v1.pdf | author:Eirini Arvaniti, Manfred Claassen category:stat.ML published:2013-12-17 summary:Real world systems typically feature a variety of different dependency typesand topologies that complicate model selection for probabilistic graphicalmodels. We introduce the ensemble-of-forests model, a generalization of theensemble-of-trees model. Our model enables structure learning of Markov randomfields (MRF) with multiple connected components and arbitrary potentials. Wepresent two approximate inference techniques for this model and demonstratetheir performance on synthetic data. Our results suggest that theensemble-of-forests approach can accurately recover sparse, possiblydisconnected MRF topologies, even in presence of non-Gaussian dependenciesand/or low sample size. We applied the ensemble-of-forests model to learn thestructure of perturbed signaling networks of immune cells and found that thesefrequently exhibit non-Gaussian dependencies with disconnected MRF topologies.In summary, we expect that the ensemble-of-forests model will enable MRFstructure learning in other high dimensional real world settings that aregoverned by non-trivial dependencies.
arxiv-4800-291 | Designing Spontaneous Speech Search Interface for Historical Archives | http://arxiv.org/pdf/1312.4706v1.pdf | author:Donna Vakharia, Rachel Gibbs category:cs.HC cs.CL published:2013-12-17 summary:Spontaneous speech in the form of conversations, meetings, voice-mail,interviews, oral history, etc. is one of the most ubiquitous forms of humancommunication. Search engines providing access to such speech collections havethe potential to better inform intelligence and make relevant data over vastaudio/video archives available to users. This project presents a search userinterface design supporting search tasks over a speech collection consisting ofan historical archive with nearly 52,000 audiovisual testimonies of survivorsand witnesses of the Holocaust and other genocides. The design incorporatesfaceted search, along with other UI elements like highlighted search items,tags, snippets, etc., to promote discovery and exploratory search. Twodifferent designs have been created to support both manual and automatedtranscripts. Evaluation was performed using human subjects to measure accuracyin retrieving results, understanding user-perspective on the design elements,and ease of parsing information.
arxiv-4800-292 | Stable mixed graphs | http://arxiv.org/pdf/1110.4168v3.pdf | author:Kayvan Sadeghi category:stat.OT math.ST stat.ML stat.TH published:2011-10-19 summary:In this paper, we study classes of graphs with three types of edges thatcapture the modified independence structure of a directed acyclic graph (DAG)after marginalisation over unobserved variables and conditioning on selectionvariables using the $m$-separation criterion. These include MC, summary, andancestral graphs. As a modification of MC graphs, we define the class ofribbonless graphs (RGs) that permits the use of the $m$-separation criterion.RGs contain summary and ancestral graphs as subclasses, and each RG can begenerated by a DAG after marginalisation and conditioning. We derive simplealgorithms to generate RGs, from given DAGs or RGs, and also to generatesummary and ancestral graphs in a simple way by further extension of theRG-generating algorithm. This enables us to develop a parallel theory on thesethree classes and to study the relationships between them as well as the use ofeach class.
arxiv-4800-293 | Compact Random Feature Maps | http://arxiv.org/pdf/1312.4626v1.pdf | author:Raffay Hamid, Ying Xiao, Alex Gittens, Dennis DeCoste category:stat.ML cs.LG published:2013-12-17 summary:Kernel approximation using randomized feature maps has recently gained a lotof interest. In this work, we identify that previous approaches for polynomialkernel approximation create maps that are rank deficient, and therefore do notutilize the capacity of the projected feature space effectively. To addressthis challenge, we propose compact random feature maps (CRAFTMaps) toapproximate polynomial kernels more concisely and accurately. We prove theerror bounds of CRAFTMaps demonstrating their superior kernel reconstructionperformance compared to the previous approximation schemes. We show howstructured random matrices can be used to efficiently generate CRAFTMaps, andpresent a single-pass algorithm using CRAFTMaps to learn non-linear multi-classclassifiers. We present experiments on multiple standard data-sets withperformance competitive with state-of-the-art results.
arxiv-4800-294 | Evolution and Computational Learning Theory: A survey on Valiant's paper | http://arxiv.org/pdf/1312.4599v1.pdf | author:Arka Bhattacharya category:cs.LG 68Q32 published:2013-12-17 summary:Darwin's theory of evolution is considered to be one of the greatestscientific gems in modern science. It not only gives us a description of howliving things evolve, but also shows how a population evolves through time andalso, why only the fittest individuals continue the generation forward. Thepaper basically gives a high level analysis of the works of Valiant[1]. Though,we know the mechanisms of evolution, but it seems that there does not exist anystrong quantitative and mathematical theory of the evolution of certainmechanisms. What is defined exactly as the fitness of an individual, why isthat only certain individuals in a population tend to mutate, how computationis done in finite time when we have exponentially many examples: there seems tobe a lot of questions which need to be answered. [1] basically treats Darwiniantheory as a form of computational learning theory, which calculates the netfitness of the hypotheses and thus distinguishes functions and their classeswhich could be evolvable using polynomial amount of resources. Evolution isconsidered as a function of the environment and the previous evolutionarystages that chooses the best hypothesis using learning techniques that makesmutation possible and hence, gives a quantitative idea that why only thefittest individuals tend to survive and have the power to mutate.
arxiv-4800-295 | Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs | http://arxiv.org/pdf/1312.4551v1.pdf | author:Armen E. Allahverdyan, Aram Galstyan category:stat.ML cs.LG published:2013-12-16 summary:We present an asymptotic analysis of Viterbi Training (VT) and contrast itwith a more conventional Maximum Likelihood (ML) approach to parameterestimation in Hidden Markov Models. While ML estimator works by (locally)maximizing the likelihood of the observed data, VT seeks to maximize theprobability of the most likely hidden state sequence. We develop an analyticalframework based on a generating function formalism and illustrate it on anexactly solvable model of HMM with one unambiguous symbol. For this particularmodel the ML objective function is continuously degenerate. VT objective, incontrast, is shown to have only finite degeneracy. Furthermore, VT convergesfaster and results in sparser (simpler) models, thus realizing an automaticOccam's razor for HMM learning. For more general scenario VT can be worsecompared to ML but still capable of correctly recovering most of theparameters.
arxiv-4800-296 | Parametric Modelling of Multivariate Count Data Using Probabilistic Graphical Models | http://arxiv.org/pdf/1312.4479v1.pdf | author:Pierre Fernique, Jean-Baptiste Durand, Yann Guédon category:stat.ML cs.LG stat.ME published:2013-12-16 summary:Multivariate count data are defined as the number of items of differentcategories issued from sampling within a population, which individuals aregrouped into categories. The analysis of multivariate count data is a recurrentand crucial issue in numerous modelling problems, particularly in the fields ofbiology and ecology (where the data can represent, for example, children countsassociated with multitype branching processes), sociology and econometrics. Wefocus on I) Identifying categories that appear simultaneously, or on thecontrary that are mutually exclusive. This is achieved by identifyingconditional independence relationships between the variables; II)Buildingparsimonious parametric models consistent with these relationships; III)Characterising and testing the effects of covariates on the joint distributionof the counts. To achieve these goals, we propose an approach based ongraphical probabilistic models, and more specifically partially directedacyclic graphs.
arxiv-4800-297 | A Review on Automated Brain Tumor Detection and Segmentation from MRI of Brain | http://arxiv.org/pdf/1312.6150v1.pdf | author:Sudipta Roy, Sanjay Nag, Indra Kanta Maitra, Samir Kumar Bandyopadhyay category:cs.CV published:2013-12-16 summary:Tumor segmentation from magnetic resonance imaging (MRI) data is an importantbut time consuming manual task performed by medical experts. Automating thisprocess is a challenging task because of the high diversity in the appearanceof tumor tissues among different patients and in many cases similarity with thenormal tissues. MRI is an advanced medical imaging technique providing richinformation about the human soft-tissue anatomy. There are different braintumor detection and segmentation methods to detect and segment a brain tumorfrom MRI images. These detection and segmentation approaches are reviewed withan importance placed on enlightening the advantages and drawbacks of thesemethods for brain tumor detection and segmentation. The use of MRI imagedetection and segmentation in different procedures are also described. Here abrief review of different segmentation for detection of brain tumor from MRI ofbrain has been discussed.
arxiv-4800-298 | Optimization for Compressed Sensing: the Simplex Method and Kronecker Sparsification | http://arxiv.org/pdf/1312.4426v1.pdf | author:Robert Vanderbei, Han Liu, Lie Wang, Kevin Lin category:stat.ML cs.LG published:2013-12-16 summary:In this paper we present two new approaches to efficiently solve large-scalecompressed sensing problems. These two ideas are independent of each other andcan therefore be used either separately or together. We consider allpossibilities. For the first approach, we note that the zero vector can be taken as theinitial basic (infeasible) solution for the linear programming problem andtherefore, if the true signal is very sparse, some variants of the simplexmethod can be expected to take only a small number of pivots to arrive at asolution. We implemented one such variant and demonstrate a dramaticimprovement in computation time on very sparse signals. The second approach requires a redesigned sensing mechanism in which thevector signal is stacked into a matrix. This allows us to exploit the Kroneckercompressed sensing (KCS) mechanism. We show that the Kronecker sensing requiresstronger conditions for perfect recovery compared to the original vectorproblem. However, the Kronecker sensing, modeled correctly, is a much sparserlinear optimization problem. Hence, algorithms that benefit from sparse problemrepresentation, such as interior-point methods, can solve the Kronecker sensingproblems much faster than the corresponding vector problem. In our numericalstudies, we demonstrate a ten-fold improvement in the computation time.
arxiv-4800-299 | Unsupervised learning of depth and motion | http://arxiv.org/pdf/1312.3429v2.pdf | author:Kishore Konda, Roland Memisevic category:cs.CV cs.LG stat.ML published:2013-12-12 summary:We present a model for the joint estimation of disparity and motion. Themodel is based on learning about the interrelations between images frommultiple cameras, multiple frames in a video, or the combination of both. Weshow that learning depth and motion cues, as well as their combinations, fromdata is possible within a single type of architecture and a single type oflearning algorithm, by using biologically inspired "complex cell" like units,which encode correlations between the pixels across image pairs. Ourexperimental results show that the learning of depth and motion makes itpossible to achieve state-of-the-art performance in 3-D activity analysis, andto outperform existing hand-engineered 3-D motion features by a very largemargin.
arxiv-4800-300 | Learning Deep Representations By Distributed Random Samplings | http://arxiv.org/pdf/1312.4405v1.pdf | author:Xiao-Lei Zhang category:cs.LG published:2013-12-16 summary:In this paper, we propose an extremely simple deep model for the unsupervisednonlinear dimensionality reduction -- deep distributed random samplings, whichperforms like a stack of unsupervised bootstrap aggregating. First, its networkstructure is novel: each layer of the network is a group of mutuallyindependent $k$-centers clusterings. Second, its learning method is extremelysimple: the $k$ centers of each clustering are only $k$ randomly selectedexamples from the training data; for small-scale data sets, the $k$ centers arefurther randomly reconstructed by a simple cyclic-shift operation. Experimentalresults on nonlinear dimensionality reduction show that the proposed method canlearn abstract representations on both large-scale and small-scale problems,and meanwhile is much faster than deep neural networks on large-scale problems.
