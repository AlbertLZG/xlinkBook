arxiv-14700-1 | Geometric-Algebra LMS Adaptive Filter and its Application to Rotation Estimation | http://arxiv.org/pdf/1601.06044v1.pdf | author:Wilder B. Lopes, Anas Al-Nuaimi, Cassio G. Lopes category:cs.CV cs.CG published:2016-01-22 summary:This paper exploits Geometric (Clifford) Algebra (GA) theory in order todevise and introduce a new adaptive filtering strategy. From a least-squarescost function, the gradient is calculated following results from GeometricCalculus (GC), the extension of GA to handle differential and integralcalculus. The novel GA least-mean-squares (GA-LMS) adaptive filter, whichinherits properties from standard adaptive filters and from GA, is developed torecursively estimate a rotor (multivector), a hypercomplex quantity able todescribe rotations in any dimension. The adaptive filter (AF) performance isassessed via a 3D point-clouds registration problem, which contains a rotationestimation step. Calculating the AF computational complexity suggests that itcan contribute to reduce the cost of a full-blown 3D registration algorithm,especially when the number of points to be processed grows. Moreover, theemployed GA/GC framework allows for easily applying the resulting filter toestimating rotors in higher dimensions.
arxiv-14700-2 | Pushing the Boundaries of Boundary Detection using Deep Learning | http://arxiv.org/pdf/1511.07386v2.pdf | author:Iasonas Kokkinos category:cs.CV cs.LG published:2015-11-23 summary:In this work we show that adapting Deep Convolutional Neural Network trainingto the task of boundary detection can result in substantial improvements overthe current state-of-the-art in boundary detection. Our contributions consist firstly in combining a careful design of the lossfor boundary detection training, a multi-resolution architecture and trainingwith external data to improve the detection accuracy of the current state ofthe art. When measured on the standard Berkeley Segmentation Dataset, weimprove theoptimal dataset scale F-measure from 0.780 to 0.808 - while humanperformance is at 0.803. We further improve performance to 0.813 by combiningdeep learning with grouping, integrating the Normalized Cuts technique within adeep network. We also examine the potential of our boundary detector in conjunction withthe task of semantic segmentation and demonstrate clear improvements overstate-of-the-art systems. Our detector is fully integrated in the popular Caffeframework and processes a 320x420 image in less than a second.
arxiv-14700-3 | Online Event Recognition from Moving Vessel Trajectories | http://arxiv.org/pdf/1601.06041v1.pdf | author:Kostas Patroumpas, Elias Alevizos, Alexander Artikis, Marios Vodas, Nikos Pelekis, Yannis Theodoridis category:cs.CV cs.AI published:2016-01-22 summary:We present a system for online monitoring of maritime activity over streamingpositions from numerous vessels sailing at sea. It employs an online trackingmodule for detecting important changes in the evolving trajectory of eachvessel across time, and thus can incrementally retain concise, yet reliablesummaries of its recent movement. In addition, thanks to its complex eventrecognition module, this system can also offer instant notification to marineauthorities regarding emergency situations, such as risk of collisions,suspicious moves in protected zones, or package picking at open sea. Not onlydid our extensive tests validate the performance, efficiency, and robustness ofthe system against scalable volumes of real-world and synthetically enlargeddatasets, but its deployment against online feeds from vessels has alsoconfirmed its capabilities for effective, real-time maritime surveillance.
arxiv-14700-4 | Recommender systems inspired by the structure of quantum theory | http://arxiv.org/pdf/1601.06035v1.pdf | author:Cyril Stark category:cs.LG cs.IT math.IT math.OC quant-ph stat.ML published:2016-01-22 summary:Physicists use quantum models to describe the behavior of physical systems.Quantum models owe their success to their interpretability, to their relationto probabilistic models (quantization of classical models) and to their highpredictive power. Beyond physics, these properties are valuable in general datascience. This motivates the use of quantum models to analyze generalnonphysical datasets. Here we provide both empirical and theoretical insightsinto the application of quantum models in data science. In the theoretical partof this paper, we firstly show that quantum models can be exponentially moreefficient than probabilistic models because there exist datasets that admitlow-dimensional quantum models and only exponentially high-dimensionalprobabilistic models. Secondly, we explain in what sense quantum models realizea useful relaxation of compressed probabilistic models. Thirdly, we show thatsparse datasets admit low-dimensional quantum models and finally, we introducea method to compute hierarchical orderings of properties of users (e.g.,personality traits) and items (e.g., genres of movies). In the empirical partof the paper, we evaluate quantum models in item recommendation and observethat the predictive power of quantum-inspired recommender systems can competewith state-of-the-art recommender systems like SVD++ and PureSVD. Furthermore,we make use of the interpretability of quantum models by computing hierarchicalorderings of properties of users and items. This work establishes a connectionbetween data science (item recommendation), information theory (communicationcomplexity), mathematical programming (positive semidefinite factorizations)and physics (quantum models).
arxiv-14700-5 | Learning Support Correlation Filters for Visual Tracking | http://arxiv.org/pdf/1601.06032v1.pdf | author:Wangmeng Zuo, Xiaohe Wu, Liang Lin, Lei Zhang, Ming-Hsuan Yang category:cs.CV published:2016-01-22 summary:Sampling and budgeting training examples are two essential factors intracking algorithms based on support vector machines (SVMs) as a trade-offbetween accuracy and efficiency. Recently, the circulant matrix formed by densesampling of translated image patches has been utilized in correlation filtersfor fast tracking. In this paper, we derive an equivalent formulation of a SVMmodel with circulant matrix expression and present an efficient alternatingoptimization method for visual tracking. We incorporate the discrete Fouriertransform with the proposed alternating optimization process, and pose thetracking problem as an iterative learning of support correlation filters (SCFs)which find the global optimal solution with real-time performance. For a givencirculant data matrix with n^2 samples of size n*n, the computationalcomplexity of the proposed algorithm is O(n^2*logn) whereas that of thestandard SVM-based approaches is at least O(n^4). In addition, we extend theSCF-based tracking algorithm with multi-channel features, kernel functions, andscale-adaptive approaches to further improve the tracking performance.Experimental results on a large benchmark dataset show that the proposedSCF-based algorithms perform favorably against the state-of-the-art trackingmethods in terms of accuracy and speed.
arxiv-14700-6 | A Robust Frame-based Nonlinear Prediction System for Automatic Speech Coding | http://arxiv.org/pdf/1601.06008v1.pdf | author:Mahmood Yousefi-Azar, Farbod Razzazi category:cs.SD cs.NE published:2016-01-22 summary:In this paper, we propose a neural-based coding scheme in which an artificialneural network is exploited to automatically compress and decompress speechsignals by a trainable approach. Having a two-stage training phase, the systemcan be fully specified to each speech frame and have robust performance acrossdifferent speakers and wide range of spoken utterances. Indeed, Frame-basednonlinear predictive coding (FNPC) would code a frame in the procedure oftraining to predict the frame samples. The motivating objective is to analyzethe system behavior in regenerating not only the envelope of spectra, but alsothe spectra phase. This scheme has been evaluated in time and discrete cosinetransform (DCT) domains and the output of predicted phonemes show thepotentiality of the FNPC to reconstruct complicated signals. The experimentswere conducted on three voiced plosive phonemes, b/d/g/ in time and DCT domainsversus the number of neurons in the hidden layer. Experiments approve the FNPCcapability as an automatic coding system by which /b/d/g/ phonemes have beenreproduced with a good accuracy. Evaluations revealed that the performance ofFNPC system, trained to predict DCT coefficients is more desirable,particularly for frames with the wider distribution of energy, compared to timesamples.
arxiv-14700-7 | The Latent Structure of Dictionaries | http://arxiv.org/pdf/1411.0129v2.pdf | author:Philippe Vincent-Lamarre, Alexandre Blondin Massé, Marcos Lopes, Mélanie Lord, Odile Marcotte, Stevan Harnad category:cs.CL cs.IR published:2014-11-01 summary:How many words (and which ones) are sufficient to define all other words?When dictionaries are analyzed as directed graphs with links from definingwords to defined words, they reveal a latent structure. Recursively removingall words that are reachable by definition but that do not define any furtherwords reduces the dictionary to a Kernel of about 10%. This is still not thesmallest number of words that can define all the rest. About 75% of the Kernelturns out to be its Core, a Strongly Connected Subset of words with adefinitional path to and from any pair of its words and no word's definitiondepending on a word outside the set. But the Core cannot define all the rest ofthe dictionary. The 25% of the Kernel surrounding the Core consists of smallstrongly connected subsets of words: the Satellites. The size of the smallestset of words that can define all the rest (the graph's Minimum Feedback VertexSet or MinSet) is about 1% of the dictionary, 15% of the Kernel, and half-Core,half-Satellite. But every dictionary has a huge number of MinSets. The Corewords are learned earlier, more frequent, and less concrete than theSatellites, which in turn are learned earlier and more frequent but moreconcrete than the rest of the Dictionary. In principle, only one MinSet's wordswould need to be grounded through the sensorimotor capacity to recognize andcategorize their referents. In a dual-code sensorimotor-symbolic model of themental lexicon, the symbolic code could do all the rest via re-combinatorydefinition.
arxiv-14700-8 | Depth and Reflection Total Variation for Single Image Dehazing | http://arxiv.org/pdf/1601.05994v1.pdf | author:Wei Wang, Chuanjiang He category:cs.CV published:2016-01-22 summary:Haze removal has been a very challenging problem due to its ill-posedness,which is more ill-posed if the input data is only a single hazy image. In thispaper, we present a new approach for removing haze from a single input image.The proposed method combines the model widely used to describe the formation ofa haze image with the assumption in Retinex that an image is the product of theillumination and the reflection. We assume that the depth and reflectionfunctions are spatially piecewise smooth in the model, where the totalvariation is used for the regularization. The proposed model is defined as aconstrained optimization problem, which is solved by an alternatingminimization scheme and the fast gradient projection algorithm. Some theoreticanalyses are given for the proposed model and algorithm. Finally, numericalexamples are presented to demonstrate that our method can restore vivid andcontrastive hazy images effectively.
arxiv-14700-9 | Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic Modeling in Speech Recognition | http://arxiv.org/pdf/1601.05936v1.pdf | author:Pranay Dighe, Gil Luyet, Afsaneh Asaei, Herve Bourlard category:cs.CL cs.LG stat.ML published:2016-01-22 summary:We propose to model the acoustic space of deep neural network (DNN)class-conditional posterior probabilities as a union of low-dimensionalsubspaces. To that end, the training posteriors are used for dictionarylearning and sparse coding. Sparse representation of the test posteriors usingthis dictionary enables projection to the space of training data. Relying onthe fact that the intrinsic dimensions of the posterior subspaces are indeedvery small and the matrix of all posteriors belonging to a class has a very lowrank, we demonstrate how low-dimensional structures enable further enhancementof the posteriors and rectify the spurious errors due to mismatch conditions.The enhanced acoustic modeling method leads to improvements in continuousspeech recognition task using hybrid DNN-HMM (hidden Markov model) framework inboth clean and noisy conditions, where upto 15.4% relative reduction in worderror rate (WER) is achieved.
arxiv-14700-10 | When is Clustering Perturbation Robust? | http://arxiv.org/pdf/1601.05900v1.pdf | author:Margareta Ackerman, Jarrod Moore category:cs.LG cs.CV published:2016-01-22 summary:Clustering is a fundamental data mining tool that aims to divide data intogroups of similar items. Generally, intuition about clustering reflects theideal case -- exact data sets endowed with flawless dissimilarity betweenindividual instances. In practice however, these cases are in the minority, and clusteringapplications are typically characterized by noisy data sets with approximatepairwise dissimilarities. As such, the efficacy of clustering methods inpractical applications necessitates robustness to perturbations. In this paper, we perform a formal analysis of perturbation robustness,revealing that the extent to which algorithms can exhibit this desirablecharacteristic is inherently limited, and identifying the types of structuresthat allow popular clustering paradigms to discover meaningful clusters inspite of faulty data.
arxiv-14700-11 | GeoTextTagger: High-Precision Location Tagging of Textual Documents using a Natural Language Processing Approach | http://arxiv.org/pdf/1601.05893v1.pdf | author:Shawn Brunsting, Hans De Sterck, Remco Dolman, Teun van Sprundel category:cs.AI cs.CL cs.DB cs.IR published:2016-01-22 summary:Location tagging, also known as geotagging or geolocation, is the process ofassigning geographical coordinates to input data. In this paper we present analgorithm for location tagging of textual documents. Our approach makes use ofprevious work in natural language processing by using a state-of-the-artpart-of-speech tagger and named entity recognizer to find blocks of text whichmay refer to locations. A knowledge base (OpenStreatMap) is then used to find alist of possible locations for each block. Finally, one location is chosen foreach block by assigning distance-based scores to each location and repeatedlyselecting the location and block with the best score. We tested our geolocationalgorithm with Wikipedia articles about topics with a well-defined geographicallocation that are geotagged by the articles' authors, where classificationapproaches have achieved median errors as low as 11 km, with attainableaccuracy limited by the class size. Our approach achieved a 10th percentileerror of 490 metres and median error of 54 kilometres on the Wikipedia datasetwe used. When considering the five location tags with the greatest scores, 50%of articles were assigned at least one tag within 8.5 kilometres of thearticle's author-assigned true location. We also tested our approach on Twittermessages that are tagged with the location from which the message was sent.Twitter texts are challenging because they are short and unstructured and oftendo not contain words referring to the location they were sent from, but weobtain potentially useful results. We explain how we use the Spark frameworkfor data analytics to collect and process our test data. In general,classification-based approaches for location tagging may be reaching theirupper accuracy limit, but our precision-focused approach has high accuracy forsome texts and shows significant potential for improvement overall.
arxiv-14700-12 | Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model | http://arxiv.org/pdf/1601.03317v3.pdf | author:Shi Feng, Shujie Liu, Mu Li, Ming Zhou category:cs.CL published:2016-01-13 summary:Neural machine translation has shown very promising results lately. Most NMTmodels follow the encoder-decoder framework. To make encoder-decoder modelsmore flexible, attention mechanism was introduced to machine translation andalso other tasks like speech recognition and image captioning. We observe thatthe quality of translation by attention-based encoder-decoder can besignificantly damaged when the alignment is incorrect. We attribute theseproblems to the lack of distortion and fertility models. Aiming to resolvethese problems, we propose new variations of attention-based encoder-decoderand compare them with other models on machine translation. Our proposed methodachieved an improvement of 2 BLEU points over the original attention-basedencoder-decoder.
arxiv-14700-13 | Manifold-Kernels Comparison in MKPLS for Visual Speech Recognition | http://arxiv.org/pdf/1601.05861v1.pdf | author:Amr Bakry, Ahmed Elgammal category:cs.CV published:2016-01-22 summary:Speech recognition is a challenging problem. Due to the acoustic limitations,using visual information is essential for improving the recognition accuracy inreal-life unconstraint situations. One common approach is to model the visualrecognition as nonlinear optimization problem. Measuring the distances betweenvisual units is essential for solving this problem. Embedding the visual unitson a manifold and using manifold kernels is one way to measure these distances.This work is intended to evaluate the performance of several manifold kernelsfor solving the problem of visual speech recognition. We show the theory behindeach kernel. We apply manifold kernel partial least squares framework to OuluVsand AvLetters databases, and show empirical comparison between all kernels.This framework provides convenient way to explore different kernels.
arxiv-14700-14 | Community detection in multi-relational data with restricted multi-layer stochastic blockmodel | http://arxiv.org/pdf/1506.02699v2.pdf | author:Subhadeep Paul, Yuguo Chen category:stat.ML published:2015-06-08 summary:In recent years there has been an increased interest in statistical analysisof data with multiple types of relations among a set of entities. Suchmulti-relational data can be represented as multi-layer graphs where the set ofvertices represents the entities and multiple types of edges represent thedifferent relations among them. For community detection in multi-layer graphs,we consider two random graph models, the multi-layer stochastic blockmodel(MLSBM) and a model with a restricted parameter space, the restrictedmulti-layer stochastic blockmodel (RMLSBM). We derive consistency results forcommunity assignments of the maximum likelihood estimators (MLEs) in bothmodels where MLSBM is assumed to be the true model, and either the number ofnodes or the number of types of edges or both grow. We compare MLEs in the twomodels with other baseline approaches, such as separate modeling of layers,aggregating the layers and majority voting. RMLSBM is shown to have advantageover MLSBM when either the growth rate of the number of communities is high orthe growth rate of the average degree of the component graphs in themulti-graph is low. We also derive minimax rates of error and sharp thresholdsfor achieving consistency of community detection in both models, which are thenused to compare the multi-layer models with a baseline model, the aggregatestochastic block model. The simulation studies and real data applicationsconfirm the superior performance of the multi-layer approaches in comparison tothe baseline procedures.
arxiv-14700-15 | Cluster-Aided Mobility Predictions | http://arxiv.org/pdf/1507.03292v4.pdf | author:Jaeseong Jeong, Mathieu Leconte, Alexandre Proutiere category:cs.LG published:2015-07-12 summary:Predicting the future location of users in wireless net- works has numerousapplications, and can help service providers to improve the quality of serviceperceived by their clients. The location predictors proposed so far estimatethe next location of a specific user by inspecting the past individualtrajectories of this user. As a consequence, when the training data collectedfor a given user is limited, the resulting prediction is inaccurate. In thispaper, we develop cluster-aided predictors that exploit past trajectoriescollected from all users to predict the next location of a given user. Thesepredictors rely on clustering techniques and extract from the training datasimilarities among the mobility patterns of the various users to improve theprediction accuracy. Specifically, we present CAMP (Cluster-Aided MobilityPredictor), a cluster-aided predictor whose design is based on recentnon-parametric bayesian statistical tools. CAMP is robust and adaptive in thesense that it exploits similarities in users' mobility only if suchsimilarities are really present in the training data. We analytically prove theconsistency of the predictions provided by CAMP, and investigate itsperformance using two large-scale datasets. CAMP significantly outperformsexisting predictors, and in particular those that only exploit individual pasttrajectories.
arxiv-14700-16 | Local community detection by seed expansion: from conductance to weighted kernel 1-mean optimization | http://arxiv.org/pdf/1601.05775v1.pdf | author:Twan van Laarhoven, Elena Marchiori category:cs.SI cs.LG stat.ML published:2016-01-21 summary:In local community detection by seed expansion a single cluster concentratedaround few given query nodes in a graph is discovered in a localized way.Conductance is a popular objective function used in many algorithms for localcommunity detection. Algorithms that directly optimize conductance usually addor remove one node at a time to find a local optimum. This amounts to fix aspecific neighborhood structure over clusters. A natural way to avoid theproblem of choosing a specific neighborhood structure is to use a continuousrelaxation of conductance. This paper studies such a continuous relaxation ofconductance. We show that in this setting continuous optimization leads to hardclusters. We investigate the relation of conductance with weighted kernelk-means for a single cluster, which leads to the introduction of a weightedkernel 1-mean objective function, called \sigma-conductance, where {\sigma} isa parameter which influences the size of the community. Conductance is obtainedby setting {\sigma} to 0. Two algorithms for local optimization of\sigma-conductance based on the expectation maximization and the projectedgradient descend method are developed, called EMc and PGDc, respectively. Weshow that for \sigma=0 EMc corresponds to gradient descend with an infinitestep size at each iteration. We design a procedure to automatically select avalue for {\sigma}. Performance guarantee for these algorithms is proven for aclass of dense communities centered around the seeds and well separated fromthe rest of the network. On this class we also prove that our algorithms staylocalized. A comparative experimental analysis on networks with ground-truthcommunities is performed using state-of-the-art algorithms based on the graphdiffusion method. Our experiments indicate that EMc and PGDc stay localized andproduce communities most similar to the ground.
arxiv-14700-17 | SYNTAGMA. A Linguistic Approach to Parsing | http://arxiv.org/pdf/1303.5960v3.pdf | author:Daniel Christen category:cs.CL published:2013-03-24 summary:SYNTAGMA is a rule-based parsing system, structured on two levels: a generalparsing engine and a language specific grammar. The parsing engine is alanguage independent program, while grammar and language specific rules andresources are given as text files, consisting in a list of constituentstructuresand a lexical database with word sense related features andconstraints. Since its theoretical background is principally Tesniere'sElements de syntaxe, SYNTAGMA's grammar emphasizes the role of argumentstructure (valency) in constraint satisfaction, and allows also horizontalbounds, for instance treating coordination. Notions such as Pro, traces, emptycategories are derived from Generative Grammar and some solutions are close toGovernment&Binding Theory, although they are the result of an autonomousresearch. These properties allow SYNTAGMA to manage complex syntacticconfigurations and well known weak points in parsing engineering. An importantresource is the semantic network, which is used in disambiguation tasks.Parsing process follows a bottom-up, rule driven strategy. Its behavior can becontrolled and fine-tuned.
arxiv-14700-18 | Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA | http://arxiv.org/pdf/1601.05768v1.pdf | author:Daniel Christen category:cs.CL published:2016-01-21 summary:This paper discusses SYNTAGMA, a rule based NLP system addressing the trickyissues of syntactic ambiguity reduction and word sense disambiguation as wellas providing innovative and original solutions for constituent generation andconstraints management. To provide an insight into how it operates, thesystem's general architecture and components, as well as its lexical, syntacticand semantic resources are described. After that, the paper addresses themechanism that performs selective parsing through an interaction betweensyntactic and semantic information, leading the parser to a coherent andaccurate interpretation of the input text.
arxiv-14700-19 | Spatial Scaling of Satellite Soil Moisture using Temporal Correlations and Ensemble Learning | http://arxiv.org/pdf/1601.05767v1.pdf | author:Subit Chakrabarti, Jasmeet Judge, Tara Bongiovanni, Anand Rangarajan, Sanjay Ranka category:cs.CV published:2016-01-21 summary:A novel algorithm is developed to downscale soil moisture (SM), obtained atsatellite scales of 10-40 km by utilizing its temporal correlations tohistorical auxiliary data at finer scales. Including such correlationsdrastically reduces the size of the training set needed, accounts fortime-lagged relationships, and enables downscaling even in the presence ofshort gaps in the auxiliary data. The algorithm is based upon bagged regressiontrees (BRT) and uses correlations between high-resolution remote sensingproducts and SM observations. The algorithm trains multiple regression treesand automatically chooses the trees that generate the best downscaledestimates. The algorithm was evaluated using a multi-scale synthetic dataset innorth central Florida for two years, including two growing seasons of corn andone growing season of cotton per year. The time-averaged error across theregion was found to be 0.01 $\mathrm{m}^3/\mathrm{m}^3$, with a standarddeviation of 0.012 $\mathrm{m}^3/\mathrm{m}^3$ when 0.02% of the data were usedfor training in addition to temporal correlations from the past seven days, andall available data from the past year. The maximum spatially averaged errorsobtained using this algorithm in downscaled SM were 0.005$\mathrm{m}^3/\mathrm{m}^3$, for pixels with cotton land-cover. When landsurface temperature~(LST) on the day of downscaling was not included in thealgorithm to simulate "data gaps", the spatially averaged error increasedminimally by 0.015 $\mathrm{m}^3/\mathrm{m}^3$ when LST is unavailable on theday of downscaling. The results indicate that the BRT-based algorithm provideshigh accuracy for downscaling SM using complex non-linear spatio-temporalcorrelations, under heterogeneous micro meteorological conditions.
arxiv-14700-20 | A Confidence-Based Approach for Balancing Fairness and Accuracy | http://arxiv.org/pdf/1601.05764v1.pdf | author:Benjamin Fish, Jeremy Kun, Ádám D. Lelkes category:cs.LG cs.CY published:2016-01-21 summary:We study three classical machine learning algorithms in the context ofalgorithmic fairness: adaptive boosting, support vector machines, and logisticregression. Our goal is to maintain the high accuracy of these learningalgorithms while reducing the degree to which they discriminate againstindividuals because of their membership in a protected group. Our first contribution is a method for achieving fairness by shifting thedecision boundary for the protected group. The method is based on the theory ofmargins for boosting. Our method performs comparably to or outperforms previousalgorithms in the fairness literature in terms of accuracy and lowdiscrimination, while simultaneously allowing for a fast and transparentquantification of the trade-off between bias and error. Our second contribution addresses the shortcomings of the bias-errortrade-off studied in most of the algorithmic fairness literature. Wedemonstrate that even hopelessly naive modifications of a biased algorithm,which cannot be reasonably said to be fair, can still achieve low bias and highaccuracy. To help to distinguish between these naive algorithms and moresensible algorithms we propose a new measure of fairness, called resilience torandom bias (RRB). We demonstrate that RRB distinguishes well between our naiveand sensible fairness algorithms. RRB together with bias and accuracy providesa more complete picture of the fairness of an algorithm.
arxiv-14700-21 | High-Dimensional Continuous Control Using Generalized Advantage Estimation | http://arxiv.org/pdf/1506.02438v4.pdf | author:John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel category:cs.LG cs.RO cs.SY published:2015-06-08 summary:Policy gradient methods are an appealing approach in reinforcement learningbecause they directly optimize the cumulative reward and can straightforwardlybe used with nonlinear function approximators such as neural networks. The twomain challenges are the large number of samples typically required, and thedifficulty of obtaining stable and steady improvement despite thenonstationarity of the incoming data. We address the first challenge by usingvalue functions to substantially reduce the variance of policy gradientestimates at the cost of some bias, with an exponentially-weighted estimator ofthe advantage function that is analogous to TD(lambda). We address the secondchallenge by using trust region optimization procedure for both the policy andthe value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3Dlocomotion tasks, learning running gaits for bipedal and quadrupedal simulatedrobots, and learning a policy for getting the biped to stand up from startingout lying on the ground. In contrast to a body of prior work that useshand-crafted policy representations, our neural network policies map directlyfrom raw kinematics to joint torques. Our algorithm is fully model-free, andthe amount of simulated experience required for the learning tasks on 3D bipedscorresponds to 1-2 weeks of real time.
arxiv-14700-22 | A Framework for Individualizing Predictions of Disease Trajectories by Exploiting Multi-Resolution Structure | http://arxiv.org/pdf/1601.04674v2.pdf | author:Peter Schulam, Suchi Saria category:stat.ML published:2016-01-18 summary:For many complex diseases, there is a wide variety of ways in which anindividual can manifest the disease. The challenge of personalized medicine isto develop tools that can accurately predict the trajectory of an individual'sdisease, which can in turn enable clinicians to optimize treatments. Werepresent an individual's disease trajectory as a continuous-valuedcontinuous-time function describing the severity of the disease over time. Wepropose a hierarchical latent variable model that individualizes predictions ofdisease trajectories. This model shares statistical strength acrossobservations at different resolutions--the population, subpopulation and theindividual level. We describe an algorithm for learning population andsubpopulation parameters offline, and an online procedure for dynamicallylearning individual-specific parameters. Finally, we validate our model on thetask of predicting the course of interstitial lung disease, a leading cause ofdeath among patients with the autoimmune disease scleroderma. We compare ourapproach against state-of-the-art and demonstrate significant improvements inpredictive accuracy.
arxiv-14700-23 | Network Topology Identification using PCA and its Graph Theoretic Interpretations | http://arxiv.org/pdf/1506.00438v2.pdf | author:Aravind Rajeswaran, Shankar Narasimhan category:cs.LG cs.DM cs.SY stat.ME published:2015-06-01 summary:We solve the problem of identifying (reconstructing) network topology fromsteady state network measurements. Concretely, given only a data matrix$\mathbf{X}$ where the $X_{ij}$ entry corresponds to flow in edge $i$ inconfiguration (steady-state) $j$, we wish to find a network structure for whichflow conservation is obeyed at all the nodes. This models many network problemsinvolving conserved quantities like water, power, and metabolic networks. Weshow that identification is equivalent to learning a model $\mathbf{A_n}$ whichcaptures the approximate linear relationships between the different variablescomprising $\mathbf{X}$ (i.e. of the form $\mathbf{A_n X \approx 0}$) such that$\mathbf{A_n}$ is full rank (highest possible) and consistent with a networknode-edge incidence structure. The problem is solved through a sequence ofsteps like estimating approximate linear relationships using PrincipalComponent Analysis, obtaining f-cut-sets from these approximate relationships,and graph realization from f-cut-sets (or equivalently f-circuits). Each stepand the overall process is polynomial time. The method is illustrated byidentifying topology of a water distribution network. We also study the extentof identifiability from steady-state data.
arxiv-14700-24 | Sparse Recovery via Differential Inclusions | http://arxiv.org/pdf/1406.7728v5.pdf | author:Stanley Osher, Feng Ruan, Jiechao Xiong, Yuan Yao, Wotao Yin category:math.ST stat.ML stat.TH published:2014-06-30 summary:In this paper, we recover sparse signals from their noisy linear measurementsby solving nonlinear differential inclusions, which is based on the notion ofinverse scale space (ISS) developed in applied mathematics. Our goal here is tobring this idea to address a challenging problem in statistics, \emph{i.e.}finding the oracle estimator which is unbiased and sign-consistent usingdynamics. We call our dynamics \emph{Bregman ISS} and \emph{Linearized BregmanISS}. A well-known shortcoming of LASSO and any convex regularizationapproaches lies in the bias of estimators. However, we show that under properconditions, there exists a bias-free and sign-consistent point on the solutionpaths of such dynamics, which corresponds to a signal that is the unbiasedestimate of the true signal and whose entries have the same signs as those ofthe true signs, \emph{i.e.} the oracle estimator. Therefore, their solutionpaths are regularization paths better than the LASSO regularization path, sincethe points on the latter path are biased when sign-consistency is reached. Wealso show how to efficiently compute their solution paths in both continuousand discretized settings: the full solution paths can be exactly computed pieceby piece, and a discretization leads to \emph{Linearized Bregman iteration},which is a simple iterative thresholding rule and easy to parallelize.Theoretical guarantees such as sign-consistency and minimax optimal $l_2$-errorbounds are established in both continuous and discrete settings for specificpoints on the paths. Early-stopping rules for identifying these points aregiven. The key treatment relies on the development of differential inequalitiesfor differential inclusions and their discretizations, which extends theprevious results and leads to exponentially fast recovering of sparse signalsbefore selecting wrong ones.
arxiv-14700-25 | Incremental Spectral Sparsification for Large-Scale Graph-Based Semi-Supervised Learning | http://arxiv.org/pdf/1601.05675v1.pdf | author:Daniele Calandriello, Alessandro Lazaric, Michal Valko, Ioannis Koutis category:stat.ML cs.LG published:2016-01-21 summary:While the harmonic function solution performs well in many semi-supervisedlearning (SSL) tasks, it is known to scale poorly with the number of samples.Recent successful and scalable methods, such as the eigenfunction method focuson efficiently approximating the whole spectrum of the graph Laplacianconstructed from the data. This is in contrast to various subsampling andquantization methods proposed in the past, which may fail in preserving thegraph spectra. However, the impact of the approximation of the spectrum on thefinal generalization error is either unknown, or requires strong assumptions onthe data. In this paper, we introduce Sparse-HFS, an efficientedge-sparsification algorithm for SSL. By constructing an edge-sparse andspectrally similar graph, we are able to leverage the approximation guaranteesof spectral sparsification methods to bound the generalization error ofSparse-HFS. As a result, we obtain a theoretically-grounded approximationscheme for graph-based SSL that also empirically matches the performance ofknown large-scale methods.
arxiv-14700-26 | Finding structure in data using multivariate tree boosting | http://arxiv.org/pdf/1511.02025v2.pdf | author:Patrick J. Miller, Gitta H. Lubke, Daniel B. McArtor, C. S. Bergeman category:stat.ML cs.LG published:2015-11-06 summary:Technology and collaboration enable dramatic increases in the size ofpsychological and psychiatric data collections, but finding structure in theselarge data sets with many collected variables is challenging. Decision treeensembles like random forests (Strobl, Malley, and Tutz, 2009) are a usefultool for finding structure, but are difficult to interpret with multipleoutcome variables which are often of interest in psychology. To find andinterpret structure in data sets with multiple outcomes and many predictors(possibly exceeding the sample size), we introduce a multivariate extension toa decision tree ensemble method called Gradient Boosted Regression Trees(Friedman, 2001). Our method, multivariate tree boosting, can be used foridentifying important predictors, detecting predictors with non-linear effectsand interactions without specification of such effects, and for identifyingpredictors that cause two or more outcome variables to covary withoutparametric assumptions. We provide the R package 'mvtboost' to estimate, tune,and interpret the resulting model, which extends the implementation ofunivariate boosting in the R package 'gbm' (Ridgeway, 2013) to continuous,multivariate outcomes. To illustrate the approach, we analyze predictors ofpsychological well-being (Ryff and Keyes, 1995). Simulations verify that ourapproach identifies predictors with non-linear effects and achieves highprediction accuracy, exceeding or matching the performance of (penalized)multivariate multiple regression and multivariate decision trees over a widerange of conditions.
arxiv-14700-27 | Model-Coupled Autoencoder for Time Series Visualisation | http://arxiv.org/pdf/1601.05654v1.pdf | author:Nikolaos Gianniotis, Sven D. Kügler, Peter Tiňo, Kai L. Polsterer category:astro-ph.IM cs.NE published:2016-01-21 summary:We present an approach for the visualisation of a set of time series thatcombines an echo state network with an autoencoder. For each time series in thedataset we train an echo state network, using a common and fixed reservoir ofhidden neurons, and use the optimised readout weights as the newrepresentation. Dimensionality reduction is then performed via an autoencoderon the readout weight representations. The crux of the work is to equip theautoencoder with a loss function that correctly interprets the reconstructedreadout weights by associating them with a reconstruction error measured in thedata space of sequences. This essentially amounts to measuring the predictiveperformance that the reconstructed readout weights exhibit on theircorresponding sequences when plugged back into the echo state network with thesame fixed reservoir. We demonstrate that the proposed visualisation frameworkcan deal both with real valued sequences as well as binary sequences. We derivemagnification factors in order to analyse distance preservations anddistortions in the visualisation space. The versatility and advantages of theproposed method are demonstrated on datasets of time series that originate fromdiverse domains.
arxiv-14700-28 | B-spline Shape from Motion & Shading: An Automatic Free-form Surface Modeling for Face Reconstruction | http://arxiv.org/pdf/1601.05644v1.pdf | author:Weilong Peng, Zhiyong Feng, Chao Xu category:cs.CV cs.GR published:2016-01-21 summary:Recently, many methods have been proposed for face reconstruction frommultiple images, most of which involve fundamental principles of Shape fromShading and Structure from motion. However, a majority of the methods justgenerate discrete surface model of face. In this paper, B-spline Shape fromMotion and Shading (BsSfMS) is proposed to reconstruct continuous B-splinesurface for multi-view face images, according to an assumption that shading andmotion information in the images contain 1st- and 0th-order derivative ofB-spline face respectively. Face surface is expressed as a B-spline surfacethat can be reconstructed by optimizing B-spline control points. Therefore,normals and 3D feature points computed from shading and motion of imagesrespectively are used as the 1st- and 0th- order derivative information, to bejointly applied in optimizing the B-spline face. Additionally, an IMLS(iterative multi-least-square) algorithm is proposed to handle the difficultcontrol point optimization. Furthermore, synthetic samples and LFW dataset areintroduced and conducted to verify the proposed approach, and the experimentalresults demonstrate the effectiveness with different poses, illuminations,expressions etc., even with wild images.
arxiv-14700-29 | Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs | http://arxiv.org/pdf/1601.05610v1.pdf | author:Hui Li, Chunhua Shen category:cs.CV published:2016-01-21 summary:In this work, we tackle the problem of car license plate detection andrecognition in natural scene images. Inspired by the success of deep neuralnetworks (DNNs) in various vision applications, here we leverage DNNs to learnhigh-level features in a cascade framework, which lead to improved performanceon both detection and recognition. Firstly, we train a $37$-class convolutional neural network (CNN) to detectall characters in an image, which results in a high recall, compared withconventional approaches such as training a binary text/non-text classifier.False positives are then eliminated by the second plate/non-plate CNNclassifier. Bounding box refinement is then carried out based on the edgeinformation of the license plates, in order to improve theintersection-over-union (IoU) ratio. The proposed cascade framework extractslicense plates effectively with both high recall and precision. Last, wepropose to recognize the license characters as a {sequence labelling} problem.A recurrent neural network (RNN) with long short-term memory (LSTM) is trainedto recognize the sequential features extracted from the whole license plate viaCNNs. The main advantage of this approach is that it is segmentation free. Byexploring context information and avoiding errors caused by segmentation, theRNN method performs better than a baseline method of combining segmentation anddeep CNN classification; and achieves state-of-the-art recognition accuracy.
arxiv-14700-30 | Automatic 3D modelling of craniofacial form | http://arxiv.org/pdf/1601.05593v1.pdf | author:Nick Pears, Christian Duncan category:cs.CV I.4.8 published:2016-01-21 summary:Three-dimensional models of craniofacial variation over the generalpopulation are useful for assessing pre- and post-operative head shape whentreating various craniofacial conditions, such as craniosynostosis. We presenta new method of automatically building both sagittal profile models and full 3Dsurface models of the human head using a range of techniques in 3D surfaceimage analysis; in particular, automatic facial landmarking using supervisedmachine learning, global and local symmetry plane detection using a variant oftrimmed iterative closest points, locally-affine template warping (for full 3Dmodels) and a novel pose normalisation using robust iterative ellipse fitting.The PCA-based models built using the new pose normalisation are more compactthan those using Generalised Procrustes Analysis and we demonstrate theirutility in a clinical case study.
arxiv-14700-31 | MOCICE-BCubed F$_1$: A New Evaluation Measure for Biclustering Algorithms | http://arxiv.org/pdf/1512.00228v2.pdf | author:Henry Rosales-Méndez, Yunior Ramírez-Cruz category:cs.LG cs.IR I.5.3; H.3.3 published:2015-12-01 summary:The validation of biclustering algorithms remains a challenging task, eventhough a number of measures have been proposed for evaluating the quality ofthese algorithms. Although no criterion is universally accepted as the overallbest, a number of meta-evaluation conditions to be satisfied by biclusteringalgorithms have been enunciated. In this work, we present MOCICE-BCubed F$_1$,a new external measure for evaluating biclusterings, in the scenario where goldstandard annotations are available for both the object clusters and theassociated feature subspaces. Our proposal relies on the so-calledmicro-objects transformation and satisfies the most comprehensive set ofmeta-evaluation conditions so far enunciated for biclusterings. Additionally,the proposed measure adequately handles the occurrence of overlapping in boththe object and feature spaces. Moreover, when used for evaluating traditionalclusterings, which are viewed as a particular case of biclustering, theproposed measure also satisfies the most comprehensive set of meta-evaluationconditions so far enunciated for this task.
arxiv-14700-32 | On the Diagnostic of Road Pathway Visibility | http://arxiv.org/pdf/1601.05535v1.pdf | author:Pierre Charbonnier, Jean-Philippe Tarel, Francois Goulette category:cs.CV published:2016-01-21 summary:Visibility distance on the road pathway plays a significant role in roadsafety and in particular, has a clear impact on the choice of speed limits.Visibility distance is thus of importance for road engineers and authorities.While visibility distance criteria are routinely taken into account in roaddesign, only a few systems exist for estimating it on existing road networks.Most existing systems comprise a target vehicle followed at a constant distanceby an observer vehicle, which only allows to check if a given, fixed visibilitydistance is available. We propose two new approaches that allow estimating themaximum available visibility distance, involving only one vehicle and based ondifferent sensor technologies, namely binocular stereovision and 3D rangesensing (LIDAR). The first approach is based on the processing of two viewstaken by digital cameras onboard the diagnostic vehicle. The main stages of theprocess are: road segmentation, edge registration between the two views, roadprofile 3D reconstruction and finally, maximal road visibility distanceestimation. The second approach involves the use of a Terrestrial LIDAR MobileMapping System. The triangulated 3D model of the road and its surroundingsprovided by the system is used to simulate targets at different distances,which allows estimating the maximum geometric visibility distance along thepathway. These approaches were developed in the context of the SARI-VIZIRPREDIT project. Both approaches are described, evaluated and compared. Theirpros and cons with respect to vehicle following systems are also discussed.
arxiv-14700-33 | Space-Time Representation of People Based on 3D Skeletal Data: A Review | http://arxiv.org/pdf/1601.01006v2.pdf | author:Fei Han, Brian Reily, William Hoff, Hao Zhang category:cs.CV published:2016-01-05 summary:Spatiotemporal human representation based on 3D visual perception data is arapidly growing research area. Based on the information sources, theserepresentations can be broadly categorized into two groups based on RGB-Dinformation or 3D skeleton data. Recently, skeleton-based human representationshave been intensively studied and kept attracting an increasing attention, dueto their robustness to variations of viewpoint, human body scale and motionspeed as well as the realtime, online performance. This paper presents acomprehensive survey of existing space-time representations of people based on3D skeletal data, and provides an informative categorization and analysis ofthese methods from the perspectives, including information modality,representation encoding, structure and transition, and feature engineering. Wealso provide a brief overview of skeleton acquisition devices and constructionmethods, enlist a number of public benchmark datasets with skeleton data, anddiscuss potential future research directions.
arxiv-14700-34 | RGB-D-based Action Recognition Datasets: A Survey | http://arxiv.org/pdf/1601.05511v1.pdf | author:Jing Zhang, Wanqing Li, Philip O. Ogunbona, Pichao Wang, Chang Tang category:cs.CV published:2016-01-21 summary:Human action recognition from RGB-D (Red, Green, Blue and Depth) data hasattracted increasing attention since the first work reported in 2010. Over thisperiod, many benchmark datasets have been created to facilitate the developmentand evaluation of new algorithms. This raises the question of which dataset toselect and how to use it in providing a fair and objective comparativeevaluation against state-of-the-art methods. To address this issue, this paperprovides a comprehensive review of the most commonly used action recognitionrelated RGB-D video datasets, including 27 single-view datasets, 10 multi-viewdatasets, and 7 multi-person datasets. The detailed information and analysis ofthese datasets is a useful resource in guiding insightful selection of datasetsfor future research. In addition, the issues with current algorithm evaluationvis-\'{a}-vis limitations of the available datasets and evaluation protocolsare also highlighted; resulting in a number of recommendations for collectionof new datasets and use of evaluation protocols.
arxiv-14700-35 | Sparse Convex Clustering | http://arxiv.org/pdf/1601.04586v2.pdf | author:Binhuan Wang, Yilong Zhang, Wei Sun, Yixin Fang category:stat.ME cs.LG stat.ML published:2016-01-18 summary:Convex clustering, a convex relaxation of k-means clustering and hierarchicalclustering, has drawn recent attentions since it nicely addresses theinstability issue of traditional nonconvex clustering methods. Although itscomputational and statistical properties have been recently studied, theperformance of convex clustering has not yet been investigated in thehigh-dimensional clustering scenario, where the data contains a large number offeatures and many of them carry no information about the clustering structure.In this paper, we demonstrate that the performance of convex clustering couldbe distorted when the uninformative features are included in the clustering. Toovercome it, we introduce a new clustering method, referred to as Sparse ConvexClustering, to simultaneously cluster observations and conduct featureselection. The key idea is to formulate convex clustering in a form ofregularization, with an adaptive group-lasso penalty term on cluster centers.In order to optimally balance the tradeoff between the cluster fitting andsparsity, a tuning criterion based on clustering stability is developed. Intheory, we provide an unbiased estimator for the degrees of freedom of theproposed sparse convex clustering method. Finally, the effectiveness of thesparse convex clustering is examined through a variety of numerical experimentsand a real data application.
arxiv-14700-36 | Neural Enquirer: Learning to Query Tables with Natural Language | http://arxiv.org/pdf/1512.00965v2.pdf | author:Pengcheng Yin, Zhengdong Lu, Hang Li, Ben Kao category:cs.AI cs.CL cs.LG cs.NE published:2015-12-03 summary:We proposed Neural Enquirer as a neural network architecture to execute anatural language (NL) query on a knowledge-base (KB) for answers. Basically,Neural Enquirer finds the distributed representation of a query and thenexecutes it on knowledge-base tables to obtain the answer as one of the valuesin the tables. Unlike similar efforts in end-to-end training of semanticparsers, Neural Enquirer is fully "neuralized": it not only givesdistributional representation of the query and the knowledge-base, but alsorealizes the execution of compositional queries as a series of differentiableoperations, with intermediate results (consisting of annotations of the tablesat different levels) saved on multiple layers of memory. Neural Enquirer can betrained with gradient descent, with which not only the parameters of thecontrolling components and semantic parsing component, but also the embeddingsof the tables and query words can be learned from scratch. The training can bedone in an end-to-end fashion, but it can take stronger guidance, e.g., thestep-by-step supervision for complicated queries, and benefit from it. NeuralEnquirer is one step towards building neural network systems which seek tounderstand language by executing it on real-world. Our experiments show thatNeural Enquirer can learn to execute fairly complicated NL queries on tableswith rich structures.
arxiv-14700-37 | Data-driven Rank Breaking for Efficient Rank Aggregation | http://arxiv.org/pdf/1601.05495v1.pdf | author:Ashish Khetan, Sewoong Oh category:cs.LG stat.ML published:2016-01-21 summary:Rank aggregation systems collect ordinal preferences from individuals toproduce a global ranking that represents the social preference. Rank-breakingis a common practice to reduce the computational complexity of learning theglobal ranking. The individual preferences are broken into pairwise comparisonsand applied to efficient algorithms tailored for independent pairedcomparisons. However, due to the ignored dependencies in the data, naiverank-breaking approaches can result in inconsistent estimates. The key idea toproduce accurate and unbiased estimates is to treat the pairwise comparisonsunequally, depending on the topology of the collected data. In this paper, weprovide the optimal rank-breaking estimator, which not only achievesconsistency but also achieves the best error bound. This allows us tocharacterize the fundamental tradeoff between accuracy and complexity. Further,the analysis identifies how the accuracy depends on the spectral gap of acorresponding comparison graph.
arxiv-14700-38 | Bayesian model comparison with un-normalised likelihoods | http://arxiv.org/pdf/1504.00298v3.pdf | author:Richard G. Everitt, Adam M. Johansen, Ellen Rowing, Melina Evdemon-Hogan category:stat.CO stat.ME stat.ML published:2015-04-01 summary:Models for which the likelihood function can be evaluated only up to aparameter-dependent unknown normalising constant, such as Markov random fieldmodels, are used widely in computer science, statistical physics, spatialstatistics, and network analysis. However, Bayesian analysis of these modelsusing standard Monte Carlo methods is not possible due to the intractability oftheir likelihood functions. Several methods that permit exact, or close toexact, simulation from the posterior distribution have recently been developed.However, estimating the evidence and Bayes' factors (BFs) for these modelsremains challenging in general. This paper describes new random weightimportance sampling and sequential Monte Carlo methods for estimating BFs thatuse simulation to circumvent the evaluation of the intractable likelihood, andcompares them to existing methods. In some cases we observe an advantage in theuse of biased weight estimates. An initial investigation into the theoreticaland empirical properties of this class of methods is presented. Some supportfor the use of biased estimates is presented, but we advocate caution in theuse of such estimates.
arxiv-14700-39 | Hierarchical Latent Word Clustering | http://arxiv.org/pdf/1601.05472v1.pdf | author:Halid Ziya Yerebakan, Fitsum Reda, Yiqiang Zhan, Yoshihisa Shinagawa category:cs.CL published:2016-01-20 summary:This paper presents a new Bayesian non-parametric model by extending theusage of Hierarchical Dirichlet Allocation to extract tree structured wordclusters from text data. The inference algorithm of the model collects words ina cluster if they share similar distribution over documents. In ourexperiments, we observed meaningful hierarchical structures on NIPS corpus andradiology reports collected from public repositories.
arxiv-14700-40 | Monotonic Calibrated Interpolated Look-Up Tables | http://arxiv.org/pdf/1505.06378v3.pdf | author:Maya Gupta, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojtek Moczydlowski, Alex van Esbroeck category:cs.LG published:2015-05-23 summary:Real-world machine learning applications may require functions that arefast-to-evaluate and interpretable. In particular, guaranteed monotonicity ofthe learned function can be critical to user trust. We propose meeting thesegoals for low-dimensional machine learning problems by learning flexible,monotonic functions using calibrated interpolated look-up tables. We extend thestructural risk minimization framework of lattice regression to train monotoniclook-up tables by solving a convex problem with appropriate linear inequalityconstraints. In addition, we propose jointly learning interpretablecalibrations of each feature to normalize continuous features and handlecategorical or missing data, at the cost of making the objective non-convex. Weaddress large-scale learning through parallelization, mini-batching, andpropose random sampling of additive regularizer terms. Case studies withreal-world problems with five to sixteen features and thousands to millions oftraining samples demonstrate the proposed monotonic functions can achievestate-of-the-art accuracy on practical problems while providing greatertransparency to users.
arxiv-14700-41 | Matrix Product State for Feature Extraction of Higher-Order Tensors | http://arxiv.org/pdf/1503.00516v4.pdf | author:Johann A. Bengua, Ho N. Phien, Hoang D. Tuan, Minh N. Do category:cs.CV cs.DS cs.LG published:2015-03-02 summary:This paper introduces matrix product state (MPS) decomposition as acomputational tool for extracting features of multidimensional data representedby higher-order tensors. Regardless of tensor order, MPS extracts its relevantfeatures to the so-called core tensor of maximum order three which can be usedfor classification. Mainly based on a successive sequence of singular valuedecompositions (SVD), MPS is quite simple to implement without any recursiveprocedure needed for optimizing local tensors. Thus, it leads to substantialcomputational savings compared to other tensor feature extraction methods suchas higher-order orthogonal iteration (HOOI) underlying the Tucker decomposition(TD). Benchmark results show that MPS can reduce significantly the featurespace of data while achieving better classification performance compared toHOOI.
arxiv-14700-42 | Detecting Temporally Consistent Objects in Videos through Object Class Label Propagation | http://arxiv.org/pdf/1601.05447v1.pdf | author:Subarna Tripathi, Serge Belongie, Youngbae Hwang, Truong Nguyen category:cs.CV published:2016-01-20 summary:Object proposals for detecting moving or static video objects need to addressissues such as speed, memory complexity and temporal consistency. We propose anefficient Video Object Proposal (VOP) generation method and show its efficacyin learning a better video object detector. A deep-learning based video objectdetector learned using the proposed VOP achieves state-of-the-art detectionperformance on the Youtube-Objects dataset. We further propose a clustering ofVOPs which can efficiently be used for detecting objects in video in astreaming fashion. As opposed to applying per-frame convolutional neuralnetwork (CNN) based object detection, our proposed method called Objects inVideo Enabler thRough LAbel Propagation (OVERLAP) needs to classify only asmall fraction of all candidate proposals in every video frame throughstreaming clustering of object proposals and class-label propagation. Sourcecode will be made available soon.
arxiv-14700-43 | QUOTE: "Querying" Users as Oracles in Tag Engines - A Semi-Supervised Learning Approach to Personalized Image Tagging | http://arxiv.org/pdf/1601.06440v1.pdf | author:Amandianeze O. Nwana, Tsuhan Chen category:cs.IR cs.LG cs.MM cs.SI published:2016-01-20 summary:One common trend in image tagging research is to focus on visually relevanttags, and this tends to ignore the personal and social aspect of tags,especially on photoblogging websites such as Flickr. Previous work hascorrectly identified that many of the tags that users provide on images are notvisually relevant (i.e. representative of the salient content in the image) andthey go on to treat such tags as noise, ignoring that the users chose toprovide those tags over others that could have been more visually relevant.Another common assumption about user generated tags for images is that theorder of these tags provides no useful information for the prediction of tagson future images. This assumption also tends to define usefulness in terms ofwhat is visually relevant to the image. For general tagging or labelingapplications that focus on providing visual information about image content,these assumptions are reasonable, but when considering personalized imagetagging applications, these assumptions are at best too rigid, ignoring userchoice and preferences. We challenge the aforementioned assumptions, and provide a machine learningapproach to the problem of personalized image tagging with the followingcontributions: 1.) We reformulate the personalized image tagging problem as asearch/retrieval ranking problem, 2.) We leverage the order of tags, which doesnot always reflect visual relevance, provided by the user in the past as a cueto their tag preferences, similar to click data, 3.) We propose a technique toaugment sparse user tag data (semi-supervision), and 4.) We demonstrate theefficacy of our method on a subset of Flickr images, showing improvement overprevious state-of-art methods.
arxiv-14700-44 | Selecting Efficient Features via a Hyper-Heuristic Approach | http://arxiv.org/pdf/1601.05409v1.pdf | author:Mitra Montazeri, Mahdieh Soleymani Baghshah, Aliakbar Niknafs category:cs.CV cs.NE published:2016-01-20 summary:By Emerging huge databases and the need to efficient learning algorithms onthese datasets, new problems have appeared and some methods have been proposedto solve these problems by selecting efficient features. Feature selection is aproblem of finding efficient features among all features in which the finalfeature set can improve accuracy and reduce complexity. One way to solve thisproblem is to evaluate all possible feature subsets. However, evaluating allpossible feature subsets is an exhaustive search and thus it has highcomputational complexity. Until now many heuristic algorithms have been studiedfor solving this problem. Hyper-heuristic is a new heuristic approach which cansearch the solution space effectively by applying local searches appropriately.Each local search is a neighborhood searching algorithm. Since each region ofthe solution space can have its own characteristics, it should be chosen anappropriate local search and apply it to current solution. This task is tackledto a supervisor. The supervisor chooses a local search based on the functionalhistory of local searches. By doing this task, it can trade of betweenexploitation and exploration. Since the existing heuristic cannot trade ofbetween exploration and exploitation appropriately, the solution space has notbeen searched appropriately in these methods and thus they have low convergencerate. For the first time, in this paper use a hyper-heuristic approach to findan efficient feature subset. In the proposed method, genetic algorithm is usedas a supervisor and 16 heuristic algorithms are used as local searches.Empirical study of the proposed method on several commonly used data sets fromUCI data sets indicates that it outperforms recent existing methods in theliterature for feature selection.
arxiv-14700-45 | Semantic Word Clusters Using Signed Normalized Graph Cuts | http://arxiv.org/pdf/1601.05403v1.pdf | author:João Sedoc, Jean Gallier, Lyle Ungar, Dean Foster category:cs.CL cs.AI published:2016-01-20 summary:Vector space representations of words capture many aspects of wordsimilarity, but such methods tend to make vector spaces in which antonyms (aswell as synonyms) are close to each other. We present a new signed spectralnormalized graph cut algorithm, signed clustering, that overlays existingthesauri upon distributionally derived vector representations of words, so thatantonym relationships between word pairs are represented by negative weights.Our signed clustering algorithm produces clusters of words which simultaneouslycapture distributional and synonym relations. We evaluate these clustersagainst the SimLex-999 dataset (Hill et al.,2014) of human judgments of wordpair similarities, and also show the benefit of using our clusters to predictthe sentiment of a given text.
arxiv-14700-46 | Disaggregation of Remotely Sensed Soil Moisture in Heterogeneous Landscapes using Holistic Structure based Models | http://arxiv.org/pdf/1501.07680v2.pdf | author:Subit Chakrabarti, Jasmeet Judge, Anand Rangarajan, Sanjay Ranka category:cs.CV 68 published:2015-01-30 summary:In this study, a novel machine learning algorithm is presented fordisaggregation of satellite soil moisture (SM) based on self-regularizedregressive models (SRRM) using high-resolution correlated information fromauxiliary sources. It includes regularized clustering that assigns softmemberships to each pixel at fine-scale followed by a kernel regression thatcomputes the value of the desired variable at all pixels. Coarse-scale remotelysensed SM were disaggregated from 10km to 1km using land cover, precipitation,land surface temperature, leaf area index, and in-situ observations of SM. Thisalgorithm was evaluated using multi-scale synthetic observations in NC Floridafor heterogeneous agricultural land covers. It was found that the root meansquare error (RMSE) for 96% of the pixels was less than 0.02 $m^3/m^3$. Theclusters generated represented the data well and reduced the RMSE by upto 40%during periods of high heterogeneity in land-cover and meteorologicalconditions. The Kullback Leibler divergence (KLD) between the true SM and thedisaggregated estimates is close to 0, for both vegetated and baresoillandcovers. The disaggregated estimates were compared to those generated by thePrinciple of Relevant Information (PRI) method. The RMSE for the PRIdisaggregated estimates is higher than the RMSE for the SRRM on each day of theseason. The KLD of the disaggregated estimates generated by the SRRM is atleast four orders of magnitude lower than those for the PRI disaggregatedestimates, while the computational time needed was reduced by three times. Theresults indicate that the SRRM can be used for disaggregating SM with complexnon-linear correlations on a grid with high accuracy.
arxiv-14700-47 | Binding via Reconstruction Clustering | http://arxiv.org/pdf/1511.06418v4.pdf | author:Klaus Greff, Rupesh Kumar Srivastava, Jürgen Schmidhuber category:cs.LG cs.NE published:2015-11-19 summary:Disentangled distributed representations of data are desirable for machinelearning, since they are more expressive and can generalize from fewerexamples. However, for complex data, the distributed representations ofmultiple objects present in the same input can interfere and lead toambiguities, which is commonly referred to as the binding problem. We argue forthe importance of the binding problem to the field of representation learning,and develop a probabilistic framework that explicitly models inputs as acomposition of multiple objects. We propose an unsupervised algorithm that usesdenoising autoencoders to dynamically bind features together in multi-objectinputs through an Expectation-Maximization-like clustering process. Theeffectiveness of this method is demonstrated on artificially generated datasetsof binary images, showing that it can even generalize to bind together newobjects never seen by the autoencoder during training.
arxiv-14700-48 | Deep Perceptual Mapping for Cross-Modal Face Recognition | http://arxiv.org/pdf/1601.05347v1.pdf | author:M. Saquib Sarfraz, Rainer Stiefelhagen category:cs.CV published:2016-01-20 summary:Cross modal face matching between the thermal and visible spectrum is a muchdesired capability for night-time surveillance and security applications. Dueto a very large modality gap, thermal-to-visible face recognition is one of themost challenging face matching problem. In this paper, we present an approachto bridge this modality gap by a significant margin. Our approach captures thehighly non-linear relationship between the two modalities by using a deepneural network. Our model attempts to learn a non-linear mapping from visibleto thermal spectrum while preserving the identity information. We showsubstantive performance improvement on three difficult thermal-visible facedatasets. The presented approach improves the state-of-the-art by more than10\% on UND-X1 dataset and by more than 15-30\% on NVESD dataset in terms ofRank-1 identification. Our method bridges the drop in performance due to themodality gap by more than 40\%.
arxiv-14700-49 | Political Speech Generation | http://arxiv.org/pdf/1601.03313v2.pdf | author:Valentin Kassarnig category:cs.CL published:2016-01-13 summary:In this report we present a system that can generate political speeches for adesired political party. Furthermore, the system allows to specify whether aspeech should hold a supportive or opposing opinion. The system relies on acombination of several state-of-the-art NLP methods which are discussed in thisreport. These include n-grams, Justeson & Katz POS tag filter, recurrent neuralnetworks, and latent Dirichlet allocation. Sequences of words are generatedbased on probabilities obtained from two underlying models: A language modeltakes care of the grammatical correctness while a topic model aims for textualconsistency. Both models were trained on the Convote dataset which containstranscripts from US congressional floor debates. Furthermore, we present amanual and an automated approach to evaluate the quality of generated speeches.In an experimental evaluation generated speeches have shown very high qualityin terms of grammatical correctness and sentence transitions.
arxiv-14700-50 | How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies | http://arxiv.org/pdf/1512.02011v2.pdf | author:Vincent François-Lavet, Raphael Fonteneau, Damien Ernst category:cs.LG cs.AI published:2015-12-07 summary:Using deep neural nets as function approximator for reinforcement learningtasks have recently been shown to be very powerful for solving problemsapproaching real-world complexity. Using these results as a benchmark, wediscuss the role that the discount factor may play in the quality of thelearning process of a deep Q-network (DQN). When the discount factorprogressively increases up to its final value, we empirically show that it ispossible to significantly reduce the number of learning steps. When used inconjunction with a varying learning rate, we empirically show that itoutperforms original DQN on several experiments. We relate this phenomenon withthe instabilities of neural networks when they are used in an approximateDynamic Programming setting. We also describe the possibility to fall within alocal optimum during the learning process, thus connecting our discussion withthe exploration/exploitation dilemma.
arxiv-14700-51 | Improved Spoken Document Summarization with Coverage Modeling Techniques | http://arxiv.org/pdf/1601.05194v1.pdf | author:Kuan-Yu Chen, Shih-Hung Liu, Berlin Chen, Hsin-Min Wang category:cs.CL cs.IR published:2016-01-20 summary:Extractive summarization aims at selecting a set of indicative sentences froma source document as a summary that can express the major theme of thedocument. A general consensus on extractive summarization is that bothrelevance and coverage are critical issues to address. The existing methodsdesigned to model coverage can be characterized by either reducing redundancyor increasing diversity in the summary. Maximal margin relevance (MMR) is awidely-cited method since it takes both relevance and redundancy into accountwhen generating a summary for a given document. In addition to MMR, there isonly a dearth of research concentrating on reducing redundancy or increasingdiversity for the spoken document summarization task, as far as we are aware.Motivated by these observations, two major contributions are presented in thispaper. First, in contrast to MMR, which considers coverage by reducingredundancy, we propose two novel coverage-based methods, which directlyincrease diversity. With the proposed methods, a set of representativesentences, which not only are relevant to the given document but also covermost of the important sub-themes of the document, can be selectedautomatically. Second, we make a step forward to plug in severaldocument/sentence representation methods into the proposed framework to furtherenhance the summarization performance. A series of empirical evaluationsdemonstrate the effectiveness of our proposed methods.
arxiv-14700-52 | A Framework to Adjust Dependency Measure Estimates for Chance | http://arxiv.org/pdf/1510.07786v2.pdf | author:Simone Romano, Nguyen Xuan Vinh, James Bailey, Karin Verspoor category:stat.ML published:2015-10-27 summary:Estimating the strength of dependency between two variables is fundamentalfor exploratory analysis and many other applications in data mining. Forexample: non-linear dependencies between two continuous variables can beexplored with the Maximal Information Coefficient (MIC); and categoricalvariables that are dependent to the target class are selected using Gini gainin random forests. Nonetheless, because dependency measures are estimated onfinite samples, the interpretability of their quantification and the accuracywhen ranking dependencies become challenging. Dependency estimates are notequal to 0 when variables are independent, cannot be compared if computed ondifferent sample size, and they are inflated by chance on variables with morecategories. In this paper, we propose a framework to adjust dependency measureestimates on finite samples. Our adjustments, which are simple and applicableto any dependency measure, are helpful in improving interpretability whenquantifying dependency and in improving accuracy on the task of rankingdependencies. In particular, we demonstrate that our approach enhances theinterpretability of MIC when used as a proxy for the amount of noise betweenvariables, and to gain accuracy when ranking variables during the splittingprocedure in random forests.
arxiv-14700-53 | Unsupervised Learning of Visual Structure using Predictive Generative Networks | http://arxiv.org/pdf/1511.06380v2.pdf | author:William Lotter, Gabriel Kreiman, David Cox category:cs.LG cs.AI cs.CV q-bio.NC published:2015-11-19 summary:The ability to predict future states of the environment is a central pillarof intelligence. At its core, effective prediction requires an internal modelof the world and an understanding of the rules by which the world changes.Here, we explore the internal models developed by deep neural networks trainedusing a loss based on predicting future frames in synthetic video sequences,using a CNN-LSTM-deCNN framework. We first show that this architecture canachieve excellent performance in visual sequence prediction tasks, includingstate-of-the-art performance in a standard 'bouncing balls' dataset (Sutskeveret al., 2009). Using a weighted mean-squared error and adversarial loss(Goodfellow et al., 2014), the same architecture successfully extrapolatesout-of-the-plane rotations of computer-generated faces. Furthermore, despitebeing trained end-to-end to predict only pixel-level information, ourPredictive Generative Networks learn a representation of the latent structureof the underlying three-dimensional objects themselves. Importantly, we findthat this representation is naturally tolerant to object transformations, andgeneralizes well to new tasks, such as classification of static images. Similarmodels trained solely with a reconstruction loss fail to generalize aseffectively. We argue that prediction can serve as a powerful unsupervised lossfor learning rich internal representations of high-level object features.
arxiv-14700-54 | A Closed-Form Solution to Tensor Voting: Theory and Applications | http://arxiv.org/pdf/1601.04888v2.pdf | author:Tai-Pang Wu, Sai-Kit Yeung, Jiaya Jia, Chi-Keung Tang, Gerard Medioni category:cs.CV published:2016-01-19 summary:We prove a closed-form solution to tensor voting (CFTV): given a point set inany dimensions, our closed-form solution provides an exact, continuous andefficient algorithm for computing a structure-aware tensor that simultaneouslyachieves salient structure detection and outlier attenuation. Using CFTV, weprove the convergence of tensor voting on a Markov random field (MRF), thustermed as MRFTV, where the structure-aware tensor at each input site reaches astationary state upon convergence in structure propagation. We then embedstructure-aware tensor into expectation maximization (EM) for optimizing asingle linear structure to achieve efficient and robust parameter estimation.Specifically, our EMTV algorithm optimizes both the tensor and fittingparameters and does not require random sampling consensus typically used inexisting robust statistical techniques. We performed quantitative evaluation onits accuracy and robustness, showing that EMTV performs better than theoriginal TV and other state-of-the-art techniques in fundamental matrixestimation for multiview stereo matching. The extensions of CFTV and EMTV forextracting multiple and nonlinear structures are underway. An addendum isincluded in this arXiv version.
arxiv-14700-55 | Causality, Information and Biological Computation: An algorithmic software approach to life, disease and the immune system | http://arxiv.org/pdf/1508.06538v5.pdf | author:Hector Zenil, Angelika Schmidt, Jesper Tegnér category:cs.NE cs.AI published:2015-08-24 summary:Biology has taken strong steps towards becoming a computer science aiming atreprogramming nature after the realisation that nature herself has reprogrammedorganisms by harnessing the power of natural selection and the digitalprescriptive nature of replicating DNA. Here we further unpack ideas related tocomputability, algorithmic information theory and software engineering, in thecontext of the extent to which biology can be (re)programmed, and with how wemay go about doing so in a more systematic way with all the tools and conceptsoffered by theoretical computer science in a translation exercise fromcomputing to molecular biology and back. These concepts provide a means to ahierarchical organization thereby blurring previously clear-cut lines betweenconcepts like matter and life, or between tumour types that are otherwise takenas different and may not have however a different cause. This does not diminishthe properties of life or make its components and functions less interesting.On the contrary, this approach makes for a more encompassing and integratedview of nature, one that subsumes observer and observed within the same system,and can generate new perspectives and tools with which to view complex diseaseslike cancer, approaching them afresh from a software-engineering viewpoint thatcasts evolution in the role of programmer, cells as computing machines, DNA andgenes as instructions and computer programs, viruses as hacking devices, theimmune system as a software debugging tool, and diseases as aninformation-theoretic battlefield where all these forces deploy. We show howinformation theory and algorithmic programming may explain fundamentalmechanisms of life and death.
arxiv-14700-56 | Habits vs Environment: What really causes asthma? | http://arxiv.org/pdf/1601.05141v1.pdf | author:Mengfan Tang, Pranav Agrawal, Ramesh Jain category:cs.CY cs.LG H.4; D.2.8 published:2016-01-20 summary:Despite considerable number of studies on risk factors for asthma onset, verylittle is known about their relative importance. To have a full picture ofthese factors, both categories, personal and environmental data, have to betaken into account simultaneously, which is missing in previous studies. Wepropose a framework to rank the risk factors from heterogeneous data sources ofthe two categories. Established on top of EventShop and Personal EventShop,this framework extracts about 400 features, and analyzes them by employing agradient boosting tree. The features come from sources including personalprofile and life-event data, and environmental data on air pollution, weatherand PM2.5 emission sources. The top ranked risk factors derived from ourframework agree well with the general medical consensus. Thus, our framework isa reliable approach, and the discovered rankings of relative importance of riskfactors can provide insights for the prevention of asthma.
arxiv-14700-57 | Origami: A 803 GOp/s/W Convolutional Network Accelerator | http://arxiv.org/pdf/1512.04295v2.pdf | author:Lukas Cavigelli, Luca Benini category:cs.CV cs.AI cs.LG cs.NE B.7.1; I.2.6 published:2015-12-14 summary:An ever increasing number of computer vision and image/video processingchallenges are being approached using deep convolutional neural networks,obtaining state-of-the-art results in object recognition and detection,semantic segmentation, action recognition, optical flow and superresolution.Hardware acceleration of these algorithms is essential to adopt theseimprovements in embedded and mobile computer vision systems. We present a newarchitecture, design and implementation as well as the first reported siliconmeasurements of such an accelerator, outperforming previous work in terms ofpower-, area- and I/O-efficiency. The manufactured device provides up to 196GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a powerefficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make itthe first architecture scalable to TOp/s performance.
arxiv-14700-58 | A Theory of Local Matching: SIFT and Beyond | http://arxiv.org/pdf/1601.05116v1.pdf | author:Hossein Mobahi, Stefano Soatto category:cs.CV cs.LG published:2016-01-19 summary:Why has SIFT been so successful? Why its extension, DSP-SIFT, can furtherimprove SIFT? Is there a theory that can explain both? How can such theorybenefit real applications? Can it suggest new algorithms with reducedcomputational complexity or new descriptors with better accuracy for matching?We construct a general theory of local descriptors for visual matching. Ourtheory relies on concepts in energy minimization and heat diffusion. We showthat SIFT and DSP-SIFT approximate the solution the theory suggests. Inparticular, DSP-SIFT gives a better approximation to the theoretical solution;justifying why DSP-SIFT outperforms SIFT. Using the developed theory, we derivenew descriptors that have fewer parameters and are potentially better inhandling affine deformations.
arxiv-14700-59 | Learning Theory for Distribution Regression | http://arxiv.org/pdf/1411.2066v3.pdf | author:Zoltan Szabo, Bharath Sriperumbudur, Barnabas Poczos, Arthur Gretton category:math.ST cs.LG math.FA stat.ML stat.TH G.3; I.2.6 published:2014-11-08 summary:We focus on the distribution regression problem: regressing to vector-valuedoutputs from probability measures. Many important machine learning andstatistical tasks fit into this framework, including multi-instance learning,and point estimation problems without analytical solution (such ashyperparameter or entropy estimation). Despite the large number of availableheuristics in the literature, the inherent two-stage sampled nature of theproblem makes the theoretical analysis quite challenging, since in practiceonly samples from sampled distributions are observable, and the estimates haveto rely on similarities computed between sets of points. To the best of ourknowledge, the only existing technique with consistency guarantees fordistribution regression requires kernel density estimation as an intermediatestep (which often performs poorly in practice), and the domain of thedistributions to be compact Euclidean. In this paper, we study a simple,analytically computable, ridge regression-based alternative to distributionregression, where we embed the distributions to a reproducing kernel Hilbertspace, and learn the regressor from the embeddings to the outputs. Our maincontribution is to prove that this scheme is consistent in the two-stagesampled setup under mild conditions (on separable topological domains enrichedwith kernels): we present an exact computational-statistical efficiencytradeoff analysis showing that the studied estimator is able to match theone-stage sampled minimax optimal rate. This result answers a 16-year-old openquestion, establishing the consistency of the classical set kernel [Haussler,1999; Gaertner et. al, 2002] in regression. We also cover consistency for morerecent kernels on distributions, including those due to [Christmann andSteinwart, 2010].
arxiv-14700-60 | Learning Visual Predictive Models of Physics for Playing Billiards | http://arxiv.org/pdf/1511.07404v3.pdf | author:Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, Jitendra Malik category:cs.CV published:2015-11-23 summary:The ability to plan and execute goal specific actions in varied, unexpectedsettings is a central requirement of intelligent agents. In this paper, weexplore how an agent can be equipped with an internal model of the dynamics ofthe external world, and how it can use this model to plan novel actions byrunning multiple internal simulations ("visual imagination"). Our modelsdirectly process raw visual input, and use a novel object-centric predictionformulation based on visual glimpses centered on objects (fixations) to enforcetranslational invariance of the learned physical laws. The agent gatherstraining data through random interaction with a collection of differentenvironments, and the resulting model can then be used to plan goal-directedactions in novel environments that the agent has not seen before. Wedemonstrate that our agent can accurately plan actions for playing a simulatedbilliards game, which requires pushing a ball into a target position or intocollision with another ball.
arxiv-14700-61 | Data-Dependent Path Normalization in Neural Networks | http://arxiv.org/pdf/1511.06747v4.pdf | author:Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, Nathan Srebro category:cs.LG published:2015-11-20 summary:We propose a unified framework for neural net normalization, regularizationand optimization, which includes Path-SGD and Batch-Normalization andinterpolates between them across two different dimensions. Through thisframework we investigate issue of invariance of the optimization, datadependence and the connection with natural gradients.
arxiv-14700-62 | Task Loss Estimation for Sequence Prediction | http://arxiv.org/pdf/1511.06456v4.pdf | author:Dzmitry Bahdanau, Dmitriy Serdyuk, Philémon Brakel, Nan Rosemary Ke, Jan Chorowski, Aaron Courville, Yoshua Bengio category:cs.LG published:2015-11-19 summary:Often, the performance on a supervised machine learning task is evaluatedwith a emph{task loss} function that cannot be optimized directly. Examples ofsuch loss functions include the classification error, the edit distance and theBLEU score. A common workaround for this problem is to instead optimize aemph{surrogate loss} function, such as for instance cross-entropy or hingeloss. In order for this remedy to be effective, it is important to ensure thatminimization of the surrogate loss results in minimization of the task loss, acondition that we call emph{consistency with the task loss}. In this work, wepropose another method for deriving differentiable surrogate losses thatprovably meet this requirement. We focus on the broad class of models thatdefine a score for every input-output pair. Our idea is that this score can beinterpreted as an estimate of the task loss, and that the estimation error maybe used as a consistent surrogate loss. A distinct feature of such an approachis that it defines the desirable value of the score for every input-outputpair. We use this property to design specialized surrogate losses forEncoder-Decoder models often used for sequence prediction tasks. In ourexperiment, we benchmark on the task of speech recognition. Using a newsurrogate loss instead of cross-entropy to train an Encoder-Decoder speechrecognizer brings a significant ~13% relative improvement in terms of CharacterError Rate (CER) in the case when no extra corpora are used for languagemodeling.
arxiv-14700-63 | Search-Convolutional Neural Networks | http://arxiv.org/pdf/1511.02136v5.pdf | author:James Atwood, Don Towsley category:cs.LG published:2015-11-06 summary:We present a new deterministic relational model derived from convolutionalneural networks. Search-Convolutional Neural Networks (SCNNs) extend the notionof convolution to graph search to construct a rich latent representation thatextracts local behavior from general graph-structured data. Unlike other neuralnetwork models that take graph-structured data as input, SCNNs have aparameterization that is independent of input size, a property that enablestransfer learning between datasets. SCNNs can be applied to a wide variety ofprediction tasks, including node classification, community detection, and linkprediction. Our results indicate that SCNNs can offer considerable lift overoff-the-shelf classifiers and simple multilayer perceptrons, and comparableperformance to state-of-the-art probabilistic graphical models at considerablylower computational cost.
arxiv-14700-64 | PN-Net: Conjoined Triple Deep Network for Learning Local Image Descriptors | http://arxiv.org/pdf/1601.05030v1.pdf | author:Vassileios Balntas, Edward Johns, Lilian Tang, Krystian Mikolajczyk category:cs.CV published:2016-01-19 summary:In this paper we propose a new approach for learning local descriptors formatching image patches. It has recently been demonstrated that descriptorsbased on convolutional neural networks (CNN) can significantly improve thematching performance. Unfortunately their computational complexity isprohibitive for any practical application. We address this problem and proposea CNN based descriptor with improved matching performance, significantlyreduced training and execution time, as well as low dimensionality. We propose to train the network with triplets of patches that include apositive and negative pairs. To that end we introduce a new loss function thatexploits the relations within the triplets. We compare our approach to recentlyintroduced MatchNet and DeepCompare and demonstrate the advantages of ourdescriptor in terms of performance, memory footprint and speed i.e. when run inGPU, the extraction time of our 128 dimensional feature is comparable to thefastest available binary descriptors such as BRIEF and ORB.
arxiv-14700-65 | Foveation-based Mechanisms Alleviate Adversarial Examples | http://arxiv.org/pdf/1511.06292v3.pdf | author:Yan Luo, Xavier Boix, Gemma Roig, Tomaso Poggio, Qi Zhao category:cs.LG cs.CV published:2015-11-19 summary:We show that adversarial examples, i.e., the visually imperceptibleperturbations that result in Convolutional Neural Networks (CNNs) fail, can bealleviated with a mechanism based on foveations---applying the CNN in differentimage regions. To see this, first, we report results in ImageNet that lead to arevision of the hypothesis that adversarial perturbations are a consequence ofCNNs acting as a linear classifier: CNNs act locally linearly to changes in theimage regions with objects recognized by the CNN, and in other regions the CNNmay act non-linearly. Then, we corroborate that when the neural responses arelinear, applying the foveation mechanism to the adversarial example tends tosignificantly reduce the effect of the perturbation. This is because,hypothetically, the CNNs for ImageNet are robust to changes of scale andtranslation of the object produced by the foveation, but this property does notgeneralize to transformations of the perturbation. As a result, the accuracyafter a foveation is almost the same as the accuracy of the CNN without theadversarial perturbation, even if the adversarial perturbation is calculatedtaking into account a foveation.
arxiv-14700-66 | Variable projection without smoothness | http://arxiv.org/pdf/1601.05011v1.pdf | author:Aleksandr Aravkin, Dmitriy Drusvyatskiy, Tristan van Leeuwen category:math.OC stat.CO stat.ML published:2016-01-19 summary:Variable projection is a powerful technique in optimization. Over the last 30years, it has been applied broadly, with empirical and theoretical resultsdemonstrating both greater efficacy and greater stability than competingapproaches. In this paper, we illustrate the technique on a large class ofstructured nonsmooth optimization problems, with numerical examples in sparsedeconvolution and machine learning applications.
arxiv-14700-67 | Convexified Modularity Maximization for Degree-corrected Stochastic Block Models | http://arxiv.org/pdf/1512.08425v2.pdf | author:Yudong Chen, Xiaodong Li, Jiaming Xu category:math.ST cs.LG cs.SI stat.ML stat.TH published:2015-12-28 summary:The stochastic block model (SBM) is a popular framework for studyingcommunity detection in networks. This model is limited by the assumption thatall nodes in the same community are statistically equivalent and have equalexpected degrees. The degree-corrected stochastic block model (DCSBM) is anatural extension of SBM that allows for degree heterogeneity withincommunities. This paper proposes a convexified modularity maximization approachfor estimating the hidden communities under DCSBM. Our approach is based on aconvex programming relaxation of the classical (generalized) modularitymaximization formulation, followed by a novel doubly-weighted $ \ell_1 $-norm $k $-median procedure. We establish non-asymptotic theoretical guarantees forboth approximate clustering and perfect clustering. Our approximate clusteringresults are insensitive to the minimum degree, and hold even in sparse regimewith bounded average degrees. In the special case of SBM, these theoreticalresults match the best-known performance guarantees of computationally feasiblealgorithms. Numerically, we provide an efficient implementation of ouralgorithm, which is applied to both synthetic and real-world networks.Experiment results show that our method enjoys competitive performance comparedto the state of the art in the literature.
arxiv-14700-68 | Online Optimization in Dynamic Environments | http://arxiv.org/pdf/1307.5944v3.pdf | author:Eric C. Hall, Rebecca M. Willett category:stat.ML cs.LG math.OC published:2013-07-23 summary:High-velocity streams of high-dimensional data pose significant "big data"analysis challenges across a range of applications and settings. Onlinelearning and online convex programming play a significant role in the rapidrecovery of important or anomalous information from these large datastreams.While recent advances in online learning have led to novel and rapidlyconverging algorithms, these methods are unable to adapt to nonstationaryenvironments arising in real-world problems. This paper describes a dynamicmirror descent framework which addresses this challenge, yielding lowtheoretical regret bounds and accurate, adaptive, and computationally efficientalgorithms which are applicable to broad classes of problems. The methods arecapable of learning and adapting to an underlying and possibly time-varyingdynamical model. Empirical results in the context of dynamic texture analysis,solar flare detection, sequential compressed sensing of a dynamic scene,traffic surveillance,tracking self-exciting point processes and networkbehavior in the Enron email corpus support the core theoretical findings.
arxiv-14700-69 | Using Deep Learning for Detecting Spoofing Attacks on Speech Signals | http://arxiv.org/pdf/1508.01746v2.pdf | author:Alan Godoy, Flávio Simões, José Augusto Stuchi, Marcus de Assis Angeloni, Mário Uliani, Ricardo Violato category:cs.SD cs.CL cs.CR cs.LG stat.ML published:2015-08-07 summary:It is well known that speaker verification systems are subject to spoofingattacks. The Automatic Speaker Verification Spoofing and CountermeasuresChallenge -- ASVSpoof2015 -- provides a standard spoofing database, containingattacks based on synthetic speech, along with a protocol for experiments. Thispaper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, basedon deep neural networks, working both as a classifier and as a featureextraction module for a GMM and a SVM classifier. Results show the validity ofthis approach, achieving less than 0.5\% EER for known attacks.
arxiv-14700-70 | Predicting popularity of online videos using Support Vector Regression | http://arxiv.org/pdf/1510.06223v3.pdf | author:Tomasz Trzcinski, Przemyslaw Rokita category:cs.SI cs.CV published:2015-10-21 summary:In this work, we propose a regression method to predict the popularity of anonline video based on temporal and visual cues. Our method uses Support VectorRegression with Gaussian Radial Basis Functions. We show that modellingpopularity patterns with this approach provides higher and more stableprediction results, mainly thanks to the non-linearity character of theproposed method as well as its resistance against overfitting. We compare ourmethod with the state of the art on datasets containing over 14,000 videos fromYouTube and Facebook. Furthermore, we show that results obtained relying onlyon the early distribution patterns, can be improved by adding social and visualmetadata.
arxiv-14700-71 | 3D Gaze Estimation from 2D Pupil Positions on Monocular Head-Mounted Eye Trackers | http://arxiv.org/pdf/1601.02644v2.pdf | author:Mohsen Mansouryar, Julian Steil, Yusuke Sugano, Andreas Bulling category:cs.HC cs.CV published:2016-01-11 summary:3D gaze information is important for scene-centric attention analysis butaccurate estimation and analysis of 3D gaze in real-world environments remainschallenging. We present a novel 3D gaze estimation method for monocularhead-mounted eye trackers. In contrast to previous work, our method does notaim to infer 3D eyeball poses but directly maps 2D pupil positions to 3D gazedirections in scene camera coordinate space. We first provide a detaileddiscussion of the 3D gaze estimation task and summarize different methods,including our own. We then evaluate the performance of different 3D gazeestimation approaches using both simulated and real data. Through experimentalvalidation, we demonstrate the effectiveness of our method in reducing parallaxerror, and we identify research challenges for the design of 3D calibrationprocedures.
arxiv-14700-72 | Understanding Deep Convolutional Networks | http://arxiv.org/pdf/1601.04920v1.pdf | author:Stéphane Mallat category:stat.ML cs.CV cs.LG published:2016-01-19 summary:Deep convolutional networks provide state of the art classifications andregressions results over many high-dimensional problems. We review theirarchitecture, which scatters data with a cascade of linear filter weights andnon-linearities. A mathematical framework is introduced to analyze theirproperties. Computations of invariants involve multiscale contractions, thelinearization of hierarchical symmetries, and sparse separations. Applicationsare discussed.
arxiv-14700-73 | Time Series Classification using the Hidden-Unit Logistic Model | http://arxiv.org/pdf/1506.05085v2.pdf | author:Wenjie Pei, Hamdi Dibeklioğlu, David M. J. Tax, Laurens van der Maaten category:cs.LG cs.CV published:2015-06-16 summary:We present a new model for time series classification, called the hidden-unitlogistic model, that uses binary stochastic hidden units to model latentstructure in the data. The hidden units are connected in a chain structure thatmodels temporal dependencies in the data. Compared to the prior models for timeseries classification such as the hidden conditional random field, our modelcan model very complex decision boundaries because the number of latent statesgrows exponentially with the number of hidden units. We demonstrate the strongperformance of our model in experiments on a variety of (computer vision)tasks, including handwritten character recognition, speech recognition, facialexpression, and action recognition. We also present a state-of-the-art systemfor facial action unit detection based on the hidden-unit logistic model.
arxiv-14700-74 | PupilNet: Convolutional Neural Networks for Robust Pupil Detection | http://arxiv.org/pdf/1601.04902v1.pdf | author:Wolfgang Fuhl, Thiago Santini, Gjergji Kasneci, Enkelejda Kasneci category:cs.CV published:2016-01-19 summary:Real-time, accurate, and robust pupil detection is an essential prerequisitefor pervasive video-based eye-tracking. However, automated pupil detection inreal-world scenarios has proven to be an intricate challenge due to fastillumination changes, pupil occlusion, non centered and off-axis eye recording,and physiological eye characteristics. In this paper, we propose and evaluate amethod based on a novel dual convolutional neural network pipeline. In itsfirst stage the pipeline performs coarse pupil position identification using aconvolutional neural network and subregions from a downscaled input image todecrease computational costs. Using subregions derived from a small windowaround the initial pupil position estimate, the second pipeline stage employsanother convolutional neural network to refine this position, resulting in anincreased pupil detection rate up to 25% in comparison with the best performingstate-of-the-art algorithm. Annotated data sets can be made available uponrequest.
arxiv-14700-75 | Scalability in Neural Control of Musculoskeletal Robots | http://arxiv.org/pdf/1601.04862v1.pdf | author:Christoph Richter, Sören Jentzsch, Rafael Hostettler, Jesús A. Garrido, Eduardo Ros, Alois C. Knoll, Florian Röhrbein, Patrick van der Smagt, Jörg Conradt category:cs.RO cs.DC cs.NE cs.SY published:2016-01-19 summary:Anthropomimetic robots are robots that sense, behave, interact and feel likehumans. By this definition, anthropomimetic robots require human-like physicalhardware and actuation, but also brain-like control and sensing. The mostself-evident realization to meet those requirements would be a human-likemusculoskeletal robot with a brain-like neural controller. While bothmusculoskeletal robotic hardware and neural control software have existed fordecades, a scalable approach that could be used to build and control ananthropomimetic human-scale robot has not been demonstrated yet. CombiningMyorobotics, a framework for musculoskeletal robot development, with SpiNNaker,a neuromorphic computing platform, we present the proof-of-principle of asystem that can scale to dozens of neurally-controlled, physically compliantjoints. At its core, it implements a closed-loop cerebellar model whichprovides real-time low-level neural control at minimal power consumption andmaximal extensibility: higher-order (e.g., cortical) neural networks andneuromorphic sensors like silicon-retinae or -cochleae can naturally beincorporated.
arxiv-14700-76 | Top-N Recommender System via Matrix Completion | http://arxiv.org/pdf/1601.04800v1.pdf | author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.IR cs.AI cs.LG stat.ML published:2016-01-19 summary:Top-N recommender systems have been investigated widely both in industry andacademia. However, the recommendation quality is far from satisfactory. In thispaper, we propose a simple yet promising algorithm. We fill the user-itemmatrix based on a low-rank assumption and simultaneously keep the originalinformation. To do that, a nonconvex rank relaxation rather than the nuclearnorm is adopted to provide a better rank approximation and an efficientoptimization strategy is designed. A comprehensive set of experiments on realdatasets demonstrates that our method pushes the accuracy of Top-Nrecommendation to a new level.
arxiv-14700-77 | One-bit compressive sensing with norm estimation | http://arxiv.org/pdf/1404.6853v3.pdf | author:Karin Knudson, Rayan Saab, Rachel Ward category:stat.ML math.NA math.OC math.PR 90C05 published:2014-04-28 summary:Consider the recovery of an unknown signal ${x}$ from quantized linearmeasurements. In the one-bit compressive sensing setting, one typically assumesthat ${x}$ is sparse, and that the measurements are of the form$\operatorname{sign}(\langle {a}_i, {x} \rangle) \in \{\pm1\}$. Since suchmeasurements give no information on the norm of ${x}$, recovery methods fromsuch measurements typically assume that $\ {x} \_2=1$. We show that if oneallows more generally for quantized affine measurements of the form$\operatorname{sign}(\langle {a}_i, {x} \rangle + b_i)$, and if the vectors${a}_i$ are random, an appropriate choice of the affine shifts $b_i$ allowsnorm recovery to be easily incorporated into existing methods for one-bitcompressive sensing. Additionally, we show that for arbitrary fixed ${x}$ inthe annulus $r \leq \ {x} \_2 \leq R$, one may estimate the norm $\ {x}\_2$ up to additive error $\delta$ from $m \gtrsim R^4 r^{-2} \delta^{-2}$such binary measurements through a single evaluation of the inverse Gaussianerror function. Finally, all of our recovery guarantees can be made universalover sparse vectors, in the sense that with high probability, one set ofmeasurements and thresholds can successfully estimate all sparse vectors ${x}$within a Euclidean ball of known radius.
arxiv-14700-78 | Feature and Region Selection for Visual Learning | http://arxiv.org/pdf/1407.5245v2.pdf | author:Ji Zhao, Liantao Wang, Ricardo Cabral, Fernando De la Torre category:cs.CV cs.LG published:2014-07-20 summary:Visual learning problems such as object classification and action recognitionare typically approached using extensions of the popular bag-of-words (BoW)model. Despite its great success, it is unclear what visual features the BoWmodel is learning: Which regions in the image or video are used to discriminateamong classes? Which are the most discriminative visual words? Answering thesequestions is fundamental for understanding existing BoW models and inspiringbetter models for visual recognition. To answer these questions, this paper presents a method for feature selectionand region selection in the visual BoW model. This allows for an intermediatevisualization of the features and regions that are important for visuallearning. The main idea is to assign latent weights to the features or regions,and jointly optimize these latent variables with the parameters of a classifier(e.g., support vector machine). There are four main benefits of our approach:(1) Our approach accommodates non-linear additive kernels such as the popular$\chi^2$ and intersection kernel; (2) our approach is able to handle bothregions in images and spatio-temporal regions in videos in a unified way; (3)the feature selection problem is convex, and both problems can be solved usinga scalable reduced gradient method; (4) we point out strong connections withmultiple kernel learning and multiple instance learning approaches.Experimental results in the PASCAL VOC 2007, MSR Action Dataset II and YouTubeillustrate the benefits of our approach.
arxiv-14700-79 | Semi-supervised Learning for Convolutional Neural Networks via Online Graph Construction | http://arxiv.org/pdf/1511.06104v2.pdf | author:Sheng-Yi Bai, Sebastian Agethen, Ting-Hsuan Chao, Winston Hsu category:cs.NE cs.CV cs.LG published:2015-11-19 summary:The recent promising achievements of deep learning rely on the large amountof labeled data. Considering the abundance of data on the web, most of them donot have labels at all. Therefore, it is important to improve generalizationperformance using unlabeled data on supervised tasks with few labeledinstances. In this work, we revisit graph-based semi-supervised learningalgorithms and propose an online graph construction technique which suits deepconvolutional neural network better. We consider an EM-like algorithm forsemi-supervised learning on deep neural networks: In forward pass, the graph isconstructed based on the network output, and the graph is then used for losscalculation to help update the network by back propagation in the backwardpass. We demonstrate the strength of our online approach compared to theconventional ones whose graph is constructed on static but not robust enoughfeature representations beforehand.
arxiv-14700-80 | Improved Sampling Techniques for Learning an Imbalanced Data Set | http://arxiv.org/pdf/1601.04756v1.pdf | author:Maureen Lyndel C. Lauron, Jaderick P. Pabico category:cs.LG published:2016-01-18 summary:This paper presents the performance of a classifier built using the stackingCalgorithm in nine different data sets. Each data set is generated using asampling technique applied on the original imbalanced data set. Five newsampling techniques are proposed in this paper (i.e., SMOTERandRep, Lax RandomOversampling, Lax Random Undersampling, Combined-Lax Random OversamplingUndersampling, and Combined-Lax Random Undersampling Oversampling) that werebased on the three sampling techniques (i.e., Random Undersampling, RandomOversampling, and Synthetic Minority Oversampling Technique) usually used assolutions in imbalance learning. The metrics used to evaluate the classifier'sperformance were F-measure and G-mean. F-measure determines the performance ofthe classifier for every class, while G-mean measures the overall performanceof the classifier. The results using F-measure showed that for the data withouta sampling technique, the classifier's performance is good only for themajority class. It also showed that among the eight sampling techniques, RU andLRU have the worst performance while other techniques (i.e., RO, C-LRUO andC-LROU) performed well only on some classes. The best performing techniques inall data sets were SMOTE, SMOTERandRep, and LRO having the lowest F-measurevalues between 0.5 and 0.65. The results using G-mean showed that theoversampling technique that attained the highest G-mean value is LRO (0.86),next is C-LROU (0.85), then SMOTE (0.84) and finally is SMOTERandRep (0.83).Combining the result of the two metrics (F-measure and G-mean), only the threesampling techniques are considered as good performing (i.e., LRO, SMOTE, andSMOTERandRep).
arxiv-14700-81 | Sparse Signal Processing with Linear and Non-Linear Observations: A Unified Shannon Theoretic Approach | http://arxiv.org/pdf/1304.0682v7.pdf | author:Cem Aksoylar, George Atia, Venkatesh Saligrama category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2013-04-02 summary:We derive fundamental sample complexity bounds for recovering sparse andstructured signals for linear and nonlinear observation models including sparseregression, group testing, multivariate regression and problems with missingfeatures. In general, sparse signal processing problems can be characterized interms of the following Markovian property. We are given a set of $N$ variables$X_1,X_2,\ldots,X_N$, and there is an unknown subset of variables $S \subset[N]$ that are \emph{relevant} for predicting outcomes $Y$. More specifically,when $Y$ is conditioned on $\{X_n\}_{n\in S}$ it is conditionally independentof the other variables, $\{X_n\}_{n \not \in S}$. Our goal is to identify theset $S$ from samples of the variables $X$ and the associated outcomes $Y$. Wecharacterize this problem as a version of the noisy channel coding problem.Using asymptotic information theoretic analyses, we establish mutualinformation formulas that provide sufficient and necessary conditions on thenumber of samples required to successfully recover the salient variables. Thesemutual information expressions unify conditions for both linear and nonlinearobservations. We then compute sample complexity bounds for the aforementionedmodels, based on the mutual information expressions in order to demonstrate theapplicability and flexibility of our results in general sparse signalprocessing models.
arxiv-14700-82 | Spectral Theory of Unsigned and Signed Graphs. Applications to Graph Clustering: a Survey | http://arxiv.org/pdf/1601.04692v1.pdf | author:Jean Gallier category:cs.LG cs.DS 68 published:2016-01-18 summary:This is a survey of the method of graph cuts and its applications to graphclustering of weighted unsigned and signed graphs. I provide a fairly thoroughtreatment of the method of normalized graph cuts, a deeply original method dueto Shi and Malik, including complete proofs. The main thrust of this paper isthe method of normalized cuts. I give a detailed account for K = 2 clusters,and also for K > 2 clusters, based on the work of Yu and Shi. I also show howboth graph drawing and normalized cut K-clustering can be easily generalized tohandle signed graphs, which are weighted graphs in which the weight matrix Wmay have negative coefficients. Intuitively, negative coefficients indicatedistance or dissimilarity. The solution is to replace the degree matrix by thematrix in which absolute values of the weights are used, and to replace theLaplacian by the Laplacian with the new degree matrix of absolute values. Asfar as I know, the generalization of K-way normalized clustering to signedgraphs is new. Finally, I show how the method of ratio cuts, in which a cut isnormalized by the size of the cluster rather than its volume, is just a specialcase of normalized cuts.
arxiv-14700-83 | Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference | http://arxiv.org/pdf/1506.02158v6.pdf | author:Yarin Gal, Zoubin Ghahramani category:stat.ML cs.LG published:2015-06-06 summary:Convolutional neural networks (CNNs) work well on large datasets. Butlabelled data is hard to collect, and in some applications larger amounts ofdata are not available. The problem then is how to use CNNs with small data --as CNNs overfit quickly. We present an efficient Bayesian CNN, offering betterrobustness to over-fitting on small data than traditional approaches. This isby placing a probability distribution over the CNN's kernels. We approximateour model's intractable posterior with Bernoulli variational distributions,requiring no additional model parameters. On the theoretical side, we cast dropout network training as approximateinference in Bayesian neural networks. This allows us to implement our modelusing existing tools in deep learning with no increase in time complexity,while highlighting a negative result in the field. We show a considerableimprovement in classification accuracy compared to standard techniques andimprove on published state-of-the-art results for CIFAR-10.
arxiv-14700-84 | The Image Torque Operator for Contour Processing | http://arxiv.org/pdf/1601.04669v1.pdf | author:Morimichi Nishigaki, Cornelia Fermüller category:cs.CV published:2016-01-18 summary:Contours are salient features for image description, but the detection andlocalization of boundary contours is still considered a challenging problem.This paper introduces a new tool for edge processing implementing theGestaltism idea of edge grouping. This tool is a mid-level image operator,called the Torque operator, that is designed to help detect closed contours inimages. The torque operator takes as input the raw image and creates an imagemap by computing from the image gradients within regions of multiple sizes ameasure of how well the edges are aligned to form closed convex contours.Fundamental properties of the torque are explored and illustrated throughexamples. Then it is applied in pure bottom-up processing in a variety ofapplications, including edge detection, visual attention and segmentation andexperimentally demonstrated a useful tool that can improve existing techniques.Finally, its extension as a more general grouping mechanism and application inobject recognition is discussed.
arxiv-14700-85 | Proactive Message Passing on Memory Factor Networks | http://arxiv.org/pdf/1601.04667v1.pdf | author:Patrick Eschenfeldt, Dan Schmidt, Stark Draper, Jonathan Yedidia category:cs.AI cs.CV published:2016-01-18 summary:We introduce a new type of graphical model that we call a "memory factornetwork" (MFN). We show how to use MFNs to model the structure inherent in manytypes of data sets. We also introduce an associated message-passing stylealgorithm called "proactive message passing"' (PMP) that performs inference onMFNs. PMP comes with convergence guarantees and is efficient in comparison tocompeting algorithms such as variants of belief propagation. We specialize MFNsand PMP to a number of distinct types of data (discrete, continuous, labelled)and inference problems (interpolation, hypothesis testing), provide examples,and discuss approaches for efficient implementation.
arxiv-14700-86 | Online Semi-Supervised Learning with Deep Hybrid Boltzmann Machines and Denoising Autoencoders | http://arxiv.org/pdf/1511.06964v7.pdf | author:Alexander G. Ororbia II, C. Lee Giles, David Reitter category:cs.LG published:2015-11-22 summary:Two novel deep hybrid architectures, the Deep Hybrid Boltzmann Machine andthe Deep Hybrid Denoising Auto-encoder, are proposed for handlingsemi-supervised learning problems. The models combine experts that modelrelevant distributions at different levels of abstraction to improve overallpredictive performance on discriminative tasks. Theoretical motivations andalgorithms for joint learning for each are presented. We apply the new modelsto the domain of data-streams in work towards life-long learning. The proposedarchitectures show improved performance compared to a pseudo-labeled, drop-outrectifier network.
arxiv-14700-87 | Detecting the Age of Twitter Users | http://arxiv.org/pdf/1601.04621v1.pdf | author:Benjamin Paul Chamberlain, Clive Humby, Marc Peter Deisenroth category:cs.SI stat.ML published:2016-01-18 summary:Twitter provides an extremely rich and open source of data for studying humanbehaviour at scale. It has been used to advance our understanding of socialnetwork structure, the viral flow of information and how new ideas develop.Enriching Twitter with demographic information would permit more precisescience and better generalisation to the real world. The only demographicindicators associated with a Twitter account are the free text name, locationand description fields. We show how the age of most Twitter accounts can beinferred with high accuracy using the structure of the social graph. Besidesclassical social science applications, there are obvious privacy and childprotection implications to this discovery. Previous work on Twitter agedetection has focussed on either user-name or linguistic features of tweets. Ashortcoming of the user-name approach is that it requires real names (Twitternames are often false) and census data from each user's (unknown) birthcountry. Problems with linguistic approaches are that most Twitter users do nottweet (the median number of Tweets is 4) and a different model must be learntfor each language. To address these issues, we devise a language-independentmethodology for determining the age of Twitter users from data that is nativeto the Twitter ecosystem. Roughly 150,000 Twitter users specify an age in theirfree text description field. We generalize this to the entire Twitter networkby showing that age can be predicted based on what or whom they follow. Weadopt a Bayesian classification paradigm, which offers a consistent frameworkfor handling uncertainty in our data, e.g., inaccurate age descriptions orspurious edges in the graph. Working within this paradigm we have successfullyapplied age detection to 700 million Twitter accounts with an F1 Score of 0.86.
arxiv-14700-88 | Comparison-based Image Quality Assessment for Parameter Selection | http://arxiv.org/pdf/1601.04619v1.pdf | author:Haoyi Liang, Daniel S. Weller category:cs.CV published:2016-01-18 summary:Image quality assessment (IQA) is traditionally classified intofull-reference (FR) IQA and no-reference (NR) IQA according to whether theoriginal image is required. Although NR-IQA is widely used in practicalapplications, room for improvement still remains because of the lack of thereference image. Inspired by the fact that in many applications, such asparameter selection, a series of distorted images are available, the authorspropose a novel comparison-based image quality assessment (C-IQA) method. Thenew comparison-based framework parallels FR-IQA by requiring two input images,and resembles NR-IQA by not using the original image. As a result, the newcomparison-based approach has more application scenarios than FR-IQA does, andtakes greater advantage of the accessible information than the traditionalsingle-input NR-IQA does. Further, C-IQA is compared with otherstate-of-the-art NR-IQA methods on two widely used IQA databases. Experimentalresults show that C-IQA outperforms the other NR-IQA methods for parameterselection, and the parameter trimming framework combined with C-IQA saves thecomputation of iterative image reconstruction up to 80%.
arxiv-14700-89 | Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis | http://arxiv.org/pdf/1601.04589v1.pdf | author:Chuan Li, Michael Wand category:cs.CV published:2016-01-18 summary:This paper studies a combination of generative Markov random field (MRF)models and discriminatively trained deep convolutional neural networks (dCNNs)for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNNfeature pyramid, controling the image layout at an abstract level. We apply themethod to both photographic and non-photo-realistic (artwork) synthesis tasks.The MRF regularizer prevents over-excitation artifacts and reduces implausiblefeature mixtures common to previous dCNN inversion approaches, permittingsynthezing photographic content with increased visual plausibility. Unlikestandard MRF-based texture synthesis, the combined system can both match andadapt local features with considerable variability, yielding results far out ofreach of classic generative MRF methods.
arxiv-14700-90 | Nonparametric Bayesian Storyline Detection from Microtexts | http://arxiv.org/pdf/1601.04580v1.pdf | author:Vinodh Krishnan, Jacob Eisenstein category:cs.CL cs.LG published:2016-01-18 summary:News events and social media are composed of evolving storylines, whichcapture public attention for a limited period of time. Identifying thesestorylines would enable many high-impact applications, such as tracking publicinterest and opinion in ongoing crisis events. However, this requiresintegrating temporal and linguistic information, and prior work takes a largelyheuristic approach. We present a novel online non-parametric Bayesian frameworkfor storyline detection, using the distance-dependent Chinese RestaurantProcess (dd-CRP). To ensure efficient linear-time inference, we employ afixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We evaluateour baseline and proposed models on the TREC Twitter Timeline Generation taskand show strong results.
arxiv-14700-91 | Content Aware Neural Style Transfer | http://arxiv.org/pdf/1601.04568v1.pdf | author:Rujie Yin category:cs.CV 68T10 published:2016-01-18 summary:This paper presents a content-aware style transfer algorithm for paintingsand photos of similar content using pre-trained neural network, obtainingbetter results than the previous work. In addition, the numerical experimentsshow that the style pattern and the content information is not completelyseparated by neural network.
arxiv-14700-92 | Incremental Semiparametric Inverse Dynamics Learning | http://arxiv.org/pdf/1601.04549v1.pdf | author:Raffaello Camoriano, Silvio Traversaro, Lorenzo Rosasco, Giorgio Metta, Francesco Nori category:stat.ML cs.LG cs.RO published:2016-01-18 summary:This paper presents a novel approach for incremental semiparametric inversedynamics learning. In particular, we consider the mixture of two approaches:Parametric modeling based on rigid body dynamics equations and nonparametricmodeling based on incremental kernel methods, with no prior information on themechanical properties of the system. This yields to an incrementalsemiparametric approach, leveraging the advantages of both the parametric andnonparametric models. We validate the proposed technique learning the dynamicsof one arm of the iCub humanoid robot.
arxiv-14700-93 | Domain based classification | http://arxiv.org/pdf/1601.04530v1.pdf | author:Robert P. W. Duin, Elzbieta Pekalska category:stat.ML cs.LG published:2016-01-18 summary:The majority of traditional classification ru les minimizing the expectedprobability of error (0-1 loss) are inappropriate if the class probabilitydistributions are ill-defined or impossible to estimate. We argue that in suchcases class domains should be used instead of class distributions or densitiesto construct a reliable decision function. Proposals are presented for someevaluation criteria and classifier learning schemes, illustrated by an example.
arxiv-14700-94 | Probabilistic Line Searches for Stochastic Optimization | http://arxiv.org/pdf/1502.02846v4.pdf | author:Maren Mahsereci, Philipp Hennig category:cs.LG math.OC stat.ML published:2015-02-10 summary:In deterministic optimization, line searches are a standard tool ensuringstability and efficiency. Where only stochastic gradients are available, nodirect equivalent has so far been formulated, because uncertain gradients donot allow for a strict sequence of decisions collapsing the search space. Weconstruct a probabilistic line search by combining the structure of existingdeterministic methods with notions from Bayesian optimization. Our methodretains a Gaussian process surrogate of the univariate optimization objective,and uses a probabilistic belief over the Wolfe conditions to monitor thedescent. The algorithm has very low computational cost, and no user-controlledparameters. Experiments show that it effectively removes the need to define alearning rate for stochastic gradient descent.
arxiv-14700-95 | Bandit Structured Prediction for Learning from Partial Feedback in Statistical Machine Translation | http://arxiv.org/pdf/1601.04468v1.pdf | author:Artem Sokolov, Stefan Riezler, Tanguy Urvoy category:cs.CL cs.LG published:2016-01-18 summary:We present an approach to structured prediction from bandit feedback, calledBandit Structured Prediction, where only the value of a task loss function at asingle predicted point, instead of a correct structure, is observed inlearning. We present an application to discriminative reranking in StatisticalMachine Translation (SMT) where the learning algorithm only has access to a1-BLEU loss evaluation of a predicted translation instead of obtaining a goldstandard reference translation. In our experiment bandit feedback is obtainedby evaluating BLEU on reference translations without revealing them to thealgorithm. This can be thought of as a simulation of interactive machinetranslation where an SMT system is personalized by a user who provides singlepoint feedback to predicted translations. Our experiments show that ourapproach improves translation quality and is comparable to approaches thatemploy more informative feedback in learning.
arxiv-14700-96 | Zero-error dissimilarity based classifiers | http://arxiv.org/pdf/1601.04451v1.pdf | author:Robert P. W. Duin, Elzbieta Pekalska category:stat.ML cs.LG published:2016-01-18 summary:We consider general non-Euclidean distance measures between real worldobjects that need to be classified. It is assumed that objects are representedby distances to other objects only. Conditions for zero-error dissimilaritybased classifiers are derived. Additional conditions are given under which thezero-error decision boundary is a continues function of the distances to afinite set of training samples. These conditions affect the objects as well asthe distance measure used. It is argued that they can be met in practice.
arxiv-14700-97 | Kauffman's adjacent possible in word order evolution | http://arxiv.org/pdf/1512.05582v2.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL cs.IT math.IT physics.soc-ph published:2015-12-17 summary:Word order evolution has been hypothesized to be constrained by a word orderpermutation ring: transitions involving orders that are closer in thepermutation ring are more likely. The hypothesis can be seen as a particularcase of Kauffman's adjacent possible in word order evolution. Here we considerthe problem of the association of the six possible orders of S, V and O toyield a couple of primary alternating orders as a window to word orderevolution. We evaluate the suitability of various competing hypotheses topredict one member of the couple from the other with the help of informationtheoretic model selection. Our ensemble of models includes a six-way model thatis based on the word order permutation ring (Kauffman's adjacent possible) andanother model based on the dual two-way of standard typology, that reduces wordorder to basic orders preferences (e.g., a preference for SV over VS andanother for SO over OS). Our analysis indicates that the permutation ringyields the best model when favoring parsimony strongly, providing support forKauffman's general view and a six-way typology.
arxiv-14700-98 | Unifying Decision Trees Split Criteria Using Tsallis Entropy | http://arxiv.org/pdf/1511.08136v4.pdf | author:Yisen Wang, Chaobing Song, Shu-Tao Xia category:stat.ML cs.AI cs.LG published:2015-11-25 summary:The construction of efficient and effective decision trees remains a keytopic in machine learning because of their simplicity and flexibility. A lot ofheuristic algorithms have been proposed to construct near-optimal decisiontrees. ID3, C4.5 and CART are classical decision tree algorithms and the splitcriteria they used are Shannon entropy, Gain Ratio and Gini index respectively.All the split criteria seem to be independent, actually, they can be unified ina Tsallis entropy framework. Tsallis entropy is a generalization of Shannonentropy and provides a new approach to enhance decision trees' performance withan adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC)algorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index,which generalizes the split criteria of decision trees. More importantly, wereveal the relations between Tsallis entropy with different $q$ and other splitcriteria. Experimental results on UCI data sets indicate that the TEC algorithmachieves statistically significant improvement over the classical algorithms.
arxiv-14700-99 | Discovering Picturesque Highlights from Egocentric Vacation Videos | http://arxiv.org/pdf/1601.04406v1.pdf | author:Vinay Bettadapura, Daniel Castro, Irfan Essa category:cs.CV published:2016-01-18 summary:We present an approach for identifying picturesque highlights from largeamounts of egocentric video data. Given a set of egocentric videos capturedover the course of a vacation, our method analyzes the videos and looks forimages that have good picturesque and artistic properties. We introduce noveltechniques to automatically determine aesthetic features such as composition,symmetry and color vibrancy in egocentric videos and rank the video framesbased on their photographic qualities to generate highlights. Our approach alsouses contextual information such as GPS, when available, to assess the relativeimportance of each geographic location where the vacation videos were shot.Furthermore, we specifically leverage the properties of egocentric videos toimprove our highlight detection. We demonstrate results on a new egocentricvacation dataset which includes 26.5 hours of videos taken over a 14 dayvacation that spans many famous tourist destinations and also provide resultsfrom a user-study to access our results.
arxiv-14700-100 | A Comparative Study of Object Trackers for Infrared Flying Bird Tracking | http://arxiv.org/pdf/1601.04386v1.pdf | author:Ying Huang, Hong Zheng, Haibin Ling, Erik Blasch, Hao Yang category:cs.CV published:2016-01-18 summary:Bird strikes present a huge risk for aircraft, especially since traditionalairport bird surveillance is mainly dependent on inefficient human observation.Computer vision based technology has been proposed to automatically detectbirds, determine bird flying trajectories, and predict aircraft takeoff delays.However, the characteristics of bird flight using imagery and the performanceof existing methods applied to flying bird task are not well known. Therefore,we perform infrared flying bird tracking experiments using 12 state-of-the-artalgorithms on a real BIRDSITE-IR dataset to obtain useful clues and recommendfeature analysis. We also develop a Struck-scale method to demonstrate theeffectiveness of multiple scale sampling adaption in handling the object offlying bird with varying shape and scale. The general analysis can be used todevelop specialized bird tracking methods for airport safety, wildness andurban bird population studies.
arxiv-14700-101 | Information Extraction Under Privacy Constraints | http://arxiv.org/pdf/1511.02381v3.pdf | author:Shahab Asoodeh, Mario Diaz, Fady Alajaji, Tamás Linder category:cs.IT math.IT math.ST stat.ML stat.TH published:2015-11-07 summary:A privacy-constrained information extraction problem is considered where fora pair of correlated discrete random variables $(X,Y)$ governed by a givenjoint distribution, an agent observes $Y$ and wants to convey to a potentiallypublic user as much information about $Y$ as possible without compromising theamount of information revealed about $X$. To this end, the so-called {\emrate-privacy function} is introduced to quantify the maximal amount ofinformation (measured in terms of mutual information) that can be extractedfrom $Y$ under a privacy constraint between $X$ and the extracted information,where privacy is measured using either mutual information or maximalcorrelation. Properties of the rate-privacy function are analyzed andinformation-theoretic and estimation-theoretic interpretations of it arepresented for both the mutual information and maximal correlation privacymeasures. It is also shown that the rate-privacy function admits a closed-formexpression for a large family of joint distributions of $(X,Y)$. Finally, therate-privacy function under the mutual information privacy measure isconsidered for the case where $(X,Y)$ has a joint probability density functionby studying the problem where the extracted information is a uniformquantization of $Y$ corrupted by additive Gaussian noise. The asymptoticbehavior of the rate-privacy function is studied as the quantization resolutiongrows without bound and it is observed that not all of the properties of therate-privacy function carry over from the discrete to the continuous case.
arxiv-14700-102 | A Novel Regularized Principal Graph Learning Framework on Explicit Graph Representation | http://arxiv.org/pdf/1512.02752v2.pdf | author:Qi Mao, Li Wang, Ivor W. Tsang, Yijun Sun category:cs.AI cs.LG stat.ML published:2015-12-09 summary:Many scientific datasets are of high dimension, and the analysis usuallyrequires visual manipulation by retaining the most important structures ofdata. Principal curve is a widely used approach for this purpose. However, manyexisting methods work only for data with structures that are notself-intersected, which is quite restrictive for real applications. A fewmethods can overcome the above problem, but they either require complicatedhuman-made rules for a specific task with lack of convergence guarantee andadaption flexibility to different tasks, or cannot obtain explicit structuresof data. To address these issues, we develop a new regularized principal graphlearning framework that captures the local information of the underlying graphstructure based on reversed graph embedding. As showcases, models that canlearn a spanning tree or a weighted undirected $\ell_1$ graph are proposed, anda new learning algorithm is developed that learns a set of principal points anda graph structure from data, simultaneously. The new algorithm is simple withguaranteed convergence. We then extend the proposed framework to deal withlarge-scale data. Experimental results on various synthetic and six real worlddatasets show that the proposed method compares favorably with baselines andcan uncover the underlying structure correctly.
arxiv-14700-103 | Building a Learning Database for the Neural Network Retrieval of Sea Surface Salinity from SMOS Brightness Temperatures | http://arxiv.org/pdf/1601.04296v1.pdf | author:Adel Ammar, Sylvie Labroue, Estelle Obligis, Michel Crépon, Sylvie Thiria category:cs.NE physics.ao-ph published:2016-01-17 summary:This article deals with an important aspect of the neural network retrievalof sea surface salinity (SSS) from SMOS brightness temperatures (TBs). Theneural network retrieval method is an empirical approach that offers thepossibility of being independent from any theoretical emissivity model, duringthe in-flight phase. A Previous study [1] has proven that this approach isapplicable to all pixels on ocean, by designing a set of neural networks withdifferent inputs. The present study focuses on the choice of the learningdatabase and demonstrates that a judicious distribution of the geophysicalparameters allows to markedly reduce the systematic regional biases of theretrieved SSS, which are due to the high noise on the TBs. An equalization ofthe distribution of the geophysical parameters, followed by a new technique forboosting the learning process, makes the regional biases almost disappear forlatitudes between 40{\deg}S and 40{\deg}N, while the global standard deviationremains between 0.6 psu (at the center of the of the swath) and 1 psu (at theedges).
arxiv-14700-104 | Face-space Action Recognition by Face-Object Interactions | http://arxiv.org/pdf/1601.04293v1.pdf | author:Amir Rosenfeld, Shimon Ullman category:cs.CV published:2016-01-17 summary:Action recognition in still images has seen major improvement in recent yearsdue to advances in human pose estimation, object recognition and strongerfeature representations. However, there are still many cases in whichperformance remains far from that of humans. In this paper, we approach theproblem by learning explicitly, and then integrating three components oftransitive actions: (1) the human body part relevant to the action (2) theobject being acted upon and (3) the specific form of interaction between theperson and the object. The process uses class-specific features and relationsnot used in the past for action recognition and which use inherently two cyclesin the process unlike most standard approaches. We focus on face-relatedactions (FRA), a subset of actions that includes several currently challengingcategories. We present an average relative improvement of 52% over state-of-theart. We also make a new benchmark publicly available.
arxiv-14700-105 | Discriminative Learning of the Prototype Set for Nearest Neighbor Classification | http://arxiv.org/pdf/1509.08102v4.pdf | author:Shin Ando category:cs.LG published:2015-09-27 summary:The nearest neighbor rule is one of the most widely used models forclassification and selecting a compact set of prototype instances is animportant problem for its applications. Many existing approaches on theprototype selection problem have relied on instance-based analyses of the classdistribution, which can be computationally expensive for large datasets. Inthis paper, we revisit this problem to explore a parametric approach, whichapproximates the violation of the nearest neighbor rule over the training setand learns the prioritization of prototypes that minimizes the violation. Weshow that our approach reduces the problem to large-margin learning anddemonstrate its advantage by empirical comparisons using public benchmark data.
arxiv-14700-106 | On-line Bayesian System Identification | http://arxiv.org/pdf/1601.04251v1.pdf | author:Diego Romeres, Giulia Prando, Gianluigi Pillonetto, Alessandro Chiuso category:cs.SY cs.LG stat.AP stat.ML published:2016-01-17 summary:We consider an on-line system identification setting, in which new databecome available at given time steps. In order to meet real-time estimationrequirements, we propose a tailored Bayesian system identification procedure,in which the hyper-parameters are still updated through Marginal Likelihoodmaximization, but after only one iteration of a suitable iterative optimizationalgorithm. Both gradient methods and the EM algorithm are considered for theMarginal Likelihood optimization. We compare this "1-step" procedure with thestandard one, in which the optimization method is run until convergence to alocal minimum. The experiments we perform confirm the effectiveness of theapproach we propose.
arxiv-14700-107 | Deep Recurrent Neural Networks for Sequential Phenotype Prediction in Genomics | http://arxiv.org/pdf/1511.02554v3.pdf | author:Farhad Pouladi, Hojjat Salehinejad, Amir Mohammad Gilani category:cs.NE cs.CE cs.LG published:2015-11-09 summary:In analyzing of modern biological data, we are often dealing with ill-posedproblems and missing data, mostly due to high dimensionality andmulticollinearity of the dataset. In this paper, we have proposed a systembased on matrix factorization (MF) and deep recurrent neural networks (DRNNs)for genotype imputation and phenotype sequences prediction. In order to modelthe long-term dependencies of phenotype data, the new Recurrent Linear Units(ReLU) learning strategy is utilized for the first time. The proposed model isimplemented for parallel processing on central processing units (CPUs) andgraphic processing units (GPUs). Performance of the proposed model is comparedwith other training algorithms for learning long-term dependencies as well asthe sparse partial least square (SPLS) method on a set of genotype andphenotype data with 604 samples, 1980 single-nucleotide polymorphisms (SNPs),and two traits. The results demonstrate performance of the ReLU trainingalgorithm in learning long-term dependencies in RNNs.
arxiv-14700-108 | Deep Reinforcement Learning with an Action Space Defined by Natural Language | http://arxiv.org/pdf/1511.04636v4.pdf | author:Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf category:cs.AI cs.CL cs.LG published:2015-11-14 summary:In this paper, we propose the deep reinforcement relevance network (DRRN), anovel deep architecture, to design a model for handling an action spacecharacterized using natural language with applications to text-based games. Fora particular class of games, a user must choose among a number of actionsdescribed by text, with the goal of maximizing long-term reward. In thesegames, the best action is typically what fits the current situation best(modeled as a state in the DRRN), also described by text. Because of theexponential complexity of natural language with respect to sentence length,there is typically an unbounded set of unique actions. Even with a constrainedvocabulary, the action space is very large and sparse, posing challenges forlearning. To address this challenge, the DRRN extracts separate high-levelembedding vectors from the texts that describe states and actions,respectively, using a general interaction function, such as inner product,bilinear, and DNN interaction, between these embedding vectors to approximatethe Q-function. We evaluate the DRRN on two popular text games, showingsuperior performance over other deep Q-learning architectures.
arxiv-14700-109 | Unsupervised Visual Representation Learning by Context Prediction | http://arxiv.org/pdf/1505.05192v3.pdf | author:Carl Doersch, Abhinav Gupta, Alexei A. Efros category:cs.CV published:2015-05-19 summary:This work explores the use of spatial context as a source of free andplentiful supervisory signal for training a rich visual representation. Givenonly a large, unlabeled image collection, we extract random pairs of patchesfrom each image and train a convolutional neural net to predict the position ofthe second patch relative to the first. We argue that doing well on this taskrequires the model to learn to recognize objects and their parts. Wedemonstrate that the feature representation learned using this within-imagecontext indeed captures visual similarity across images. For example, thisrepresentation allows us to perform unsupervised visual discovery of objectslike cats, people, and even birds from the Pascal VOC 2011 detection dataset.Furthermore, we show that the learned ConvNet can be used in the R-CNNframework and provides a significant boost over a randomly-initialized ConvNet,resulting in state-of-the-art performance among algorithms which use onlyPascal-provided training set annotations.
arxiv-14700-110 | Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization | http://arxiv.org/pdf/1511.05432v3.pdf | author:Uri Shaham, Yutaro Yamada, Sahand Negahban category:stat.ML cs.LG cs.NE published:2015-11-17 summary:We propose a general framework for increasing local stability of ArtificialNeural Nets (ANNs) using Robust Optimization (RO). We achieve this through analternating minimization-maximization procedure, in which the loss of thenetwork is minimized over perturbed examples that are generated at eachparameter update. We show that adversarial training of ANNs is in factrobustification of the network optimization, and that our proposed frameworkgeneralizes previous approaches for increasing local stability of ANNs.Experimental results reveal that our approach increases the robustness of thenetwork to existing adversarial examples, while making it harder to generatenew ones. Furthermore, our algorithm improves the accuracy of the network alsoon the original test data.
arxiv-14700-111 | Conversion of Artificial Recurrent Neural Networks to Spiking Neural Networks for Low-power Neuromorphic Hardware | http://arxiv.org/pdf/1601.04187v1.pdf | author:Peter U. Diehl, Guido Zarrella, Andrew Cassidy, Bruno U. Pedroni, Emre Neftci category:cs.NE published:2016-01-16 summary:In recent years the field of neuromorphic low-power systems that consumeorders of magnitude less power gained significant momentum. However, theirwider use is still hindered by the lack of algorithms that can harness thestrengths of such architectures. While neuromorphic adaptations ofrepresentation learning algorithms are now emerging, efficient processing oftemporal sequences or variable length-inputs remain difficult. Recurrent neuralnetworks (RNN) are widely used in machine learning to solve a variety ofsequence learning tasks. In this work we present a train-and-constrainmethodology that enables the mapping of machine learned (Elman) RNNs on asubstrate of spiking neurons, while being compatible with the capabilities ofcurrent and near-future neuromorphic systems. This "train-and-constrain" methodconsists of first training RNNs using backpropagation through time, thendiscretizing the weights and finally converting them to spiking RNNs bymatching the responses of artificial neurons with those of the spiking neurons.We demonstrate our approach by mapping a natural language processing task(question classification), where we demonstrate the entire mapping process ofthe recurrent layer of the network on IBM's Neurosynaptic System "TrueNorth", aspike-based digital neuromorphic hardware architecture. TrueNorth imposesspecific constraints on connectivity, neural and synaptic parameters. Tosatisfy these constraints, it was necessary to discretize the synaptic weightsand neural activities to 16 levels, and to limit fan-in to 64 inputs. We findthat short synaptic delays are sufficient to implement the dynamical (temporal)aspect of the RNN in the question classification task. The hardware-constrainedmodel achieved 74% accuracy in question classification while using less than0.025% of the cores on one TrueNorth chip, resulting in an estimated powerconsumption of ~17 uW.
arxiv-14700-112 | TrueHappiness: Neuromorphic Emotion Recognition on TrueNorth | http://arxiv.org/pdf/1601.04183v1.pdf | author:Peter U. Diehl, Bruno U. Pedroni, Andrew Cassidy, Paul Merolla, Emre Neftci, Guido Zarrella category:q-bio.NC cs.NE published:2016-01-16 summary:We present an approach to constructing a neuromorphic device that responds tolanguage input by producing neuron spikes in proportion to the strength of theappropriate positive or negative emotional response. Specifically, we perform afine-grained sentiment analysis task with implementations on two differentsystems: one using conventional spiking neural network (SNN) simulators and theother one using IBM's Neurosynaptic System TrueNorth. Input words are projectedinto a high-dimensional semantic space and processed through a fully-connectedneural network (FCNN) containing rectified linear units trained viabackpropagation. After training, this FCNN is converted to a SNN bysubstituting the ReLUs with integrate-and-fire neurons. We show that there ispractically no performance loss due to conversion to a spiking network on asentiment analysis test set, i.e. correlations between predictions and humanannotations differ by less than 0.02 comparing the original DNN and its spikingequivalent. Additionally, we show that the SNN generated with this techniquecan be mapped to existing neuromorphic hardware -- in our case, the TrueNorthchip. Mapping to the chip involves 4-bit synaptic weight discretization andadjustment of the neuron thresholds. The resulting end-to-end system can take auser input, i.e. a word in a vocabulary of over 300,000 words, and estimate itssentiment on TrueNorth with a power consumption of approximately 50 uW.
arxiv-14700-113 | Image Segmentation Using Hierarchical Merge Tree | http://arxiv.org/pdf/1505.06389v2.pdf | author:Ting Liu, Mojtaba Seyedhosseini, Tolga Tasdizen category:cs.CV published:2015-05-24 summary:This paper investigates one of the most fundamental computer vision problems:image segmentation. We propose a supervised hierarchical approach toobject-independent image segmentation. Starting with over-segmentingsuperpixels, we use a tree structure to represent the hierarchy of regionmerging, by which we reduce the problem of segmenting image regions to findinga set of label assignment to tree nodes. We formulate the tree structure as aconstrained conditional model to associate region merging with likelihoodspredicted using an ensemble boundary classifier. Final segmentations can thenbe inferred by finding globally optimal solutions to the model efficiently. Wealso present an iterative training and testing algorithm that generates varioustree structures and combines them to emphasize accurate boundaries bysegmentation accumulation. Experiment results and comparisons with other veryrecent methods on six public data sets demonstrate that our approach achievesthe state-of-the-art region accuracy and is very competitive in imagesegmentation without semantic priors.
arxiv-14700-114 | Regularized Orthogonal Tensor Decompositions for Multi-Relational Learning | http://arxiv.org/pdf/1512.08120v2.pdf | author:Fanhua Shang, James Cheng, Hong Cheng category:cs.LG cs.AI published:2015-12-26 summary:Multi-relational learning has received lots of attention from researchers invarious research communities. Most existing methods either suffer fromsuperlinear per-iteration cost, or are sensitive to the given ranks. To addressboth issues, we propose a scalable core tensor trace norm RegularizedOrthogonal Iteration Decomposition (ROID) method for full or incomplete tensoranalytics, which can be generalized as a graph Laplacian regularized version byusing auxiliary information or a sparse higher-order orthogonal iteration(SHOOI) version. We first induce the equivalence relation of the Schattenp-norm (0<p<\infty) of a low multi-linear rank tensor and its core tensor. Thenwe achieve a much smaller matrix trace norm minimization problem. Finally, wedevelop two efficient augmented Lagrange multiplier algorithms to solve ourproblems with convergence guarantees. Extensive experiments using both real andsynthetic datasets, even though with only a few observations, verified both theefficiency and effectiveness of our methods.
arxiv-14700-115 | Regret bounds for Narendra-Shapiro bandit algorithms | http://arxiv.org/pdf/1502.04874v2.pdf | author:Sébastien Gadat, Fabien Panloup, Sofiane Saadane category:math.PR math.ST stat.ML stat.TH published:2015-02-17 summary:Narendra-Shapiro (NS) algorithms are bandit-type algorithms that have beenintroduced in the sixties (with a view to applications in Psychology orlearning automata), whose convergence has been intensively studied in thestochastic algorithm literature. In this paper, we adress the followingquestion: are the Narendra-Shapiro (NS) bandit algorithms competitive from a\textit{regret} point of view? In our main result, we show that somecompetitive bounds can be obtained for such algorithms in their penalizedversion (introduced in \cite{Lamberton_Pages}). More precisely, up to anover-penalization modification, the pseudo-regret $\bar{R}_n$ related to thepenalized two-armed bandit algorithm is uniformly bounded by $C \sqrt{n}$(where $C$ is made explicit in the paper). \noindent We also generalizeexisting convergence and rates of convergence results to the multi-armed caseof the over-penalized bandit algorithm, including the convergence toward theinvariant measure of a Piecewise Deterministic Markov Process (PDMP) after asuitable renormalization. Finally, ergodic properties of this PDMP are given inthe multi-armed case.
arxiv-14700-116 | Compositional Model based Fisher Vector Coding for Image Classification | http://arxiv.org/pdf/1601.04143v1.pdf | author:Lingqiao Liu, Peng Wang, Chunhua Shen, Lei Wang, Anton van den Hengel, Chao Wang, Heng Tao Shen category:cs.CV published:2016-01-16 summary:Deriving from the gradient vector of a generative model of local features,Fisher vector coding (FVC) has been identified as an effective coding methodfor image classification. Most, if not all, FVC implementations employ theGaussian mixture model (GMM) to depict the generation process of localfeatures. However, the representative power of the GMM could be limited becauseit essentially assumes that local features can be characterized by a fixednumber of feature prototypes and the number of prototypes is usually small inFVC. To handle this limitation, in this paper we break the convention whichassumes that a local feature is drawn from one of few Gaussian distributions.Instead, we adopt a compositional mechanism which assumes that a local featureis drawn from a Gaussian distribution whose mean vector is composed as thelinear combination of multiple key components and the combination weight is alatent random variable. In this way, we can greatly enhance the representativepower of the generative model of FVC. To implement our idea, we designed twoparticular generative models with such a compositional mechanism.
arxiv-14700-117 | Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval | http://arxiv.org/pdf/1502.06922v3.pdf | author:Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, Rabab Ward category:cs.CL cs.IR cs.LG cs.NE published:2015-02-24 summary:This paper develops a model that addresses sentence embedding, a hot topic incurrent natural language processing research, using recurrent neural networkswith Long Short-Term Memory (LSTM) cells. Due to its ability to capture longterm memory, the LSTM-RNN accumulates increasingly richer information as itgoes through the sentence, and when it reaches the last word, the hidden layerof the network provides a semantic representation of the whole sentence. Inthis paper, the LSTM-RNN is trained in a weakly supervised manner on userclick-through data logged by a commercial web search engine. Visualization andanalysis are performed to understand how the embedding process works. The modelis found to automatically attenuate the unimportant words and detects thesalient keywords in the sentence. Furthermore, these detected keywords arefound to automatically activate different cells of the LSTM-RNN, where wordsbelonging to a similar topic activate the same cell. As a semanticrepresentation of the sentence, the embedding vector can be used in manydifferent applications. These automatic keyword detection and topic allocationabilities enabled by the LSTM-RNN allow the network to perform documentretrieval, a difficult language processing task, where the similarity betweenthe query and documents can be measured by the distance between theircorresponding sentence embedding vectors computed by the LSTM-RNN. On a websearch task, the LSTM-RNN embedding is shown to significantly outperformseveral existing state of the art methods. We emphasize that the proposed modelgenerates sentence embedding vectors that are specially useful for web documentretrieval tasks. A comparison with a well known general sentence embeddingmethod, the Paragraph Vector, is performed. The results show that the proposedmethod in this paper significantly outperforms it for web document retrievaltask.
arxiv-14700-118 | Engineering Safety in Machine Learning | http://arxiv.org/pdf/1601.04126v1.pdf | author:Kush R. Varshney category:stat.ML cs.AI cs.CY cs.LG published:2016-01-16 summary:Machine learning algorithms are increasingly influencing our decisions andinteracting with us in all parts of our daily lives. Therefore, just like forpower plants, highways, and myriad other engineered sociotechnical systems, wemust consider the safety of systems involving machine learning. In this paper,we first discuss the definition of safety in terms of risk, epistemicuncertainty, and the harm incurred by unwanted outcomes. Then we examinedimensions, such as the choice of cost function and the appropriateness ofminimizing the empirical average training cost, along which certain real-worldapplications may not be completely amenable to the foundational principle ofmodern statistical machine learning: empirical risk minimization. Inparticular, we note an emerging dichotomy of applications: ones in which safetyis important and risk minimization is not the complete story (we name theseType A applications), and ones in which safety is not so critical and riskminimization is sufficient (we name these Type B applications). Finally, wediscuss how four different strategies for achieving safety in engineering(inherently safe design, safety reserves, safe fail, and procedural safeguards)can be mapped to the machine learning context through interpretability andcausality of predictive models, objectives beyond expected prediction accuracy,human involvement for labeling difficult or rare examples, and user experiencedesign of software.
arxiv-14700-119 | Constrained Convex Neyman-Pearson Classification Using an Outer Approximation Splitting Method | http://arxiv.org/pdf/1506.02196v2.pdf | author:Michel Barlaud, Wafa Belhajali, Patrick L. Combettes, Lionel Fillatre category:math.ST stat.ML stat.TH published:2015-06-06 summary:We propose an efficient splitting algorithm for solving Neyman-Pearsonclassification problems, which consist in minimizing the type II risk subjectto an upper bound constraint on the type I risk. Since the 1/0 loss function isnot convex, it is customary to replace it by convex surrogates that lead tomanageable optimization problems. While statistical bounds have been be derivedto quantify the cost of using such surrogates, no specific algorithm has yetbeen proposed to solve exactly the resulting constrained minimization problemand existing work has addressed only Langragian approximations. Thecontribution of this paper is to propose an efficient splitting algorithm toaddress this issue. Our method alternates a gradient step on the objective anda projection step onto the lower level set modeling the constraint. Theprojection step is implemented via an outer approximation scheme in which theconstraint set is approximated by a sequence of simple convex sets consistingof the intersection of two half-spaces. Convergence of the iterates generatedby the algorithm is established. Experiments on both synthetic and biologicaldata show that our algorithm outperforms state of the art Lagrangian methodssuch as $\nu$-SVM.
arxiv-14700-120 | Learning with a Strong Adversary | http://arxiv.org/pdf/1511.03034v6.pdf | author:Ruitong Huang, Bing Xu, Dale Schuurmans, Csaba Szepesvari category:cs.LG published:2015-11-10 summary:The robustness of neural networks to intended perturbations has recentlyattracted significant attention. In this paper, we propose a new method,\emph{learning with a strong adversary}, that learns robust classifiers fromsupervised data. The proposed method takes finding adversarial examples as anintermediate step. A new and simple way of finding adversarial examples ispresented and experimentally shown to be efficient. Experimental resultsdemonstrate that resulting learning method greatly improves the robustness ofthe classification models produced.
arxiv-14700-121 | Modification of Question Writing Style Influences Content Popularity in a Social Q&A System | http://arxiv.org/pdf/1601.04075v1.pdf | author:Igor A. Podgorny category:cs.IR cs.CL cs.SI published:2016-01-15 summary:TurboTax AnswerXchange is a social Q&A system supporting users working onfederal and state tax returns. Using 2015 data, we demonstrate that contentpopularity (or number of views per AnswerXchange question) can be predictedwith reasonable accuracy based on attributes of the question alone. We alsoemploy probabilistic topic analysis and uplift modeling to identify questionfeatures with the highest impact on popularity. We demonstrate that contentpopularity is driven by behavioral attributes of AnswerXchange users anddepends on complex interactions between search ranking algorithms,psycholinguistic factors and question writing style. Our findings provide arationale for employing popularity predictions to guide the users intoformulating better questions and editing the existing ones. For example,starting question title with a question word or adding details to the questionincrease number of views per question. Similar approach can be applied topromoting AnswerXchange content indexed by Google to drive organic traffic toTurboTax.
arxiv-14700-122 | Maximum Entropy Kernels for System Identification | http://arxiv.org/pdf/1411.5620v2.pdf | author:Francesca Paola Carli, Tianshi Chen, Lennart Ljung category:math.OC cs.IT math.IT stat.ML published:2014-11-20 summary:A new nonparametric approach for system identification has been recentlyproposed where the impulse response is modeled as the realization of azero-mean Gaussian process whose covariance (kernel) has to be estimated fromdata. In this scheme, quality of the estimates crucially depends on theparametrization of the covariance of the Gaussian process. A family of kernelsthat have been shown to be particularly effective in the system identificationframework is the family of Diagonal/Correlated (DC) kernels. Maximum entropyproperties of a related family of kernels, the Tuned/Correlated (TC) kernels,have been recently pointed out in the literature. In this paper we show thatmaximum entropy properties indeed extend to the whole family of DC kernels. Themaximum entropy interpretation can be exploited in conjunction with results onmatrix completion problems in the graphical models literature to shed light onthe structure of the DC kernel. In particular, we prove that the DC kerneladmits a closed-form factorization, inverse and determinant. These results canbe exploited both to improve the numerical stability and to reduce thecomputational complexity associated with the computation of the DC estimator.
arxiv-14700-123 | Faster Asynchronous SGD | http://arxiv.org/pdf/1601.04033v1.pdf | author:Augustus Odena category:stat.ML cs.LG published:2016-01-15 summary:Asynchronous distributed stochastic gradient descent methods have troubleconverging because of stale gradients. A gradient update sent to a parameterserver by a client is stale if the parameters used to calculate that gradienthave since been updated on the server. Approaches have been proposed tocircumvent this problem that quantify staleness in terms of the number ofelapsed updates. In this work, we propose a novel method that quantifiesstaleness in terms of moving averages of gradient statistics. We show that thismethod outperforms previous methods with respect to convergence speed andscalability to many clients. We also discuss how an extension to this methodcan be used to dramatically reduce bandwidth costs in a distributed trainingcontext. In particular, our method allows reduction of total bandwidth usage bya factor of 5 with little impact on cost convergence. We also describe (andlink to) a software library that we have used to simulate these algorithmsdeterministically on a single machine.
arxiv-14700-124 | Detecting and Extracting Events from Text Documents | http://arxiv.org/pdf/1601.04012v1.pdf | author:Jugal Kalita category:cs.CL published:2016-01-15 summary:Events of various kinds are mentioned and discussed in text documents,whether they are books, news articles, blogs or microblog feeds. The paperstarts by giving an overview of how events are treated in linguistics andphilosophy. We follow this discussion by surveying how events and associatedinformation are handled in computationally. In particular, we look at howtextual documents can be mined to extract events and ancillary information.These days, it is mostly through the application of various machine learningtechniques. We also discuss applications of event detection and extractionsystems, particularly in summarization, in the medical domain and in thecontext of Twitter posts. We end the paper with a discussion of challenges andfuture directions.
arxiv-14700-125 | Real-Time Association Mining in Large Social Networks | http://arxiv.org/pdf/1601.03958v1.pdf | author:Benjamin Paul Chamberlain, Josh Levy-Kramer, Clive Humby, Marc Peter Deisenroth category:cs.SI stat.ML published:2016-01-15 summary:There is a growing realisation that to combat the waning effectiveness oftraditional marketing, social media platform owners need to find new ways tomonetise their data. Social media data contains rich information describing howreal world entities relate to each other. Understanding the allegiances,communities and structure of key entities is of vital importance for decisionsupport in a swathe of industries that have hitherto relied on expensive, smallscale survey data. In this paper, we present a real-time method to query andvisualise regions of networks that are closely related to a set of inputvertices. The input vertices can define an industry, political party, sportetc. The key idea is that in large digital social networks measuring similarityvia direct connections between nodes is not robust, but that robustsimilarities between nodes can be attained through the similarity of theirneighbourhood graphs. We are able to achieve real-time performance bycompressing the neighbourhood graphs using minhash signatures and facilitaterapid queries through Locality Sensitive Hashing. These techniques reduce querytimes from hours using industrial desktop machines to milliseconds on standardlaptops. Our method allows analysts to interactively explore stronglyassociated regions of large networks in real time. Our work has been deployedin software that is actively used by analysts to understand social networkstructure.
arxiv-14700-126 | Improved graph-based SFA: Information preservation complements the slowness principle | http://arxiv.org/pdf/1601.03945v1.pdf | author:Alberto N. Escalante-B., Laurenz Wiskott category:cs.CV cs.LG stat.ML published:2016-01-15 summary:Slow feature analysis (SFA) is an unsupervised-learning algorithm thatextracts slowly varying features from a multi-dimensional time series. Asupervised extension to SFA for classification and regression is graph-basedSFA (GSFA). GSFA is based on the preservation of similarities, which arespecified by a graph structure derived from the labels. It has been shown thathierarchical GSFA (HGSFA) allows learning from images and otherhigh-dimensional data. The feature space spanned by HGSFA is complex due to thecomposition of the nonlinearities of the nodes in the network. However, we showthat the network discards useful information prematurely before it reacheshigher nodes, resulting in suboptimal global slowness and an under-exploitedfeature space. To counteract these problems, we propose an extension called hierarchicalinformation-preserving GSFA (HiGSFA), where information preservationcomplements the slowness-maximization goal. We build a 10-layer HiGSFA networkto estimate human age from facial photographs of the MORPH-II database,achieving a mean absolute error of 3.50 years, improving the state-of-the-artperformance. HiGSFA and HGSFA support multiple-labels and offer a rich featurespace, feed-forward training, and linear complexity in the number of samplesand dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of featureslowness, estimation accuracy and input reconstruction, giving rise to apromising hierarchical supervised-learning approach.
arxiv-14700-127 | Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures | http://arxiv.org/pdf/1601.03896v1.pdf | author:Raffaella Bernardi, Ruket Cakici, Desmond Elliott, Aykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis, Frank Keller, Adrian Muscat, Barbara Plank category:cs.CL cs.CV published:2016-01-15 summary:Automatic description generation from natural images is a challenging problemthat has recently received a large amount of interest from the computer visionand natural language processing communities. In this survey, we classify theexisting approaches based on how they conceptualize this problem, viz., modelsthat cast description as either generation problem or as a retrieval problemover a visual or multimodal representational space. We provide a detailedreview of existing models, highlighting their advantages and disadvantages.Moreover, we give an overview of the benchmark image datasets and theevaluation measures that have been developed to assess the quality ofmachine-generated image descriptions. Finally we extrapolate future directionsin the area of automatic image description generation.
arxiv-14700-128 | A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits | http://arxiv.org/pdf/1601.03855v1.pdf | author:Pratik Gajane, Tanguy Urvoy, Fabrice Clérot category:cs.LG published:2016-01-15 summary:We study the K-armed dueling bandit problem which is a variation of theclassical Multi-Armed Bandit (MAB) problem in which the learner receives onlyrelative feedback about the selected pairs of arms. We propose a new algorithmcalled Relative Exponential-weight algorithm for Exploration and Exploitation(REX3) to handle the adversarial utility-based formulation of this problem.This algorithm is a non-trivial extension of the Exponential-weight algorithmfor Exploration and Exploitation (EXP3) algorithm. We prove a finite timeexpected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and ageneral lower bound of order omega(sqrt(KT)). At the end, we provideexperimental results using real data from information retrieval applications.
arxiv-14700-129 | On the consistency of inversion-free parameter estimation for Gaussian random fields | http://arxiv.org/pdf/1601.03822v1.pdf | author:Hossein Keshavarz, Clayton Scott, XuanLong Nguyen category:math.ST cs.LG stat.ML stat.TH published:2016-01-15 summary:Gaussian random fields are a powerful tool for modeling environmentalprocesses. For high dimensional samples, classical approaches for estimatingthe covariance parameters require highly challenging and massive computations,such as the evaluation of the Cholesky factorization or solving linear systems.Recently, Anitescu, Chen and Stein \cite{M.Anitescu} proposed a fast andscalable algorithm which does not need such burdensome computations. The mainfocus of this article is to study the asymptotic behavior of the algorithm ofAnitescu et al. (ACS) for regular and irregular grids in the increasing domainsetting. Consistency, minimax optimality and asymptotic normality of thisalgorithm are proved under mild differentiability conditions on the covariancefunction. Despite the fact that ACS's method entails a non-concavemaximization, our results hold for any stationary point of the objectivefunction. A numerical study is presented to evaluate the efficiency of thisalgorithm for large data sets.
arxiv-14700-130 | Invariant backpropagation: how to train a transformation-invariant neural network | http://arxiv.org/pdf/1502.04434v3.pdf | author:Sergey Demyanov, James Bailey, Ramamohanarao Kotagiri, Christopher Leckie category:stat.ML cs.LG cs.NE published:2015-02-16 summary:In many classification problems a classifier should be robust to smallvariations in the input vector. This is a desired property not only forparticular transformations, such as translation and rotation in imageclassification problems, but also for all others for which the change is smallenough to retain the object perceptually indistinguishable. We propose twoextensions of the backpropagation algorithm that train a neural network to berobust to variations in the feature vector. While the first of them enforcesrobustness of the loss function to all variations, the second method trains thepredictions to be robust to a particular variation which changes the lossfunction the most. The second methods demonstrates better results, but isslightly slower. We analytically compare the proposed algorithm with two themost similar approaches (Tangent BP and Adversarial Training), and proposetheir fast versions. In the experimental part we perform comparison of allalgorithms in terms of classification accuracy and robustness to noise on MNISTand CIFAR-10 datasets. Additionally we analyze how the performance of theproposed algorithm depends on the dataset size and data augmentation.
arxiv-14700-131 | Matrix Neural Networks | http://arxiv.org/pdf/1601.03805v1.pdf | author:Junbin Gao, Yi Guo, Zhiyong Wang category:cs.LG published:2016-01-15 summary:Traditional neural networks assume vectorial inputs as the network isarranged as layers of single line of computing units called neurons. Thisspecial structure requires the non-vectorial inputs such as matrices to beconverted into vectors. This process can be problematic. Firstly, the spatialinformation among elements of the data may be lost during vectorisation.Secondly, the solution space becomes very large which demands very specialtreatments to the network parameters and high computational cost. To addressthese issues, we propose matrix neural networks (MatNet), which takes matricesdirectly as inputs. Each neuron senses summarised information through bilinearmapping from lower layer units in exactly the same way as the classic feedforward neural networks. Under this structure, back prorogation and gradientdescent combination can be utilised to obtain network parameters efficiently.Furthermore, it can be conveniently extended for multimodal inputs. We applyMatNet to MNIST handwritten digits classification and image super resolutiontasks to show its effectiveness. Without too much tweaking MatNet achievescomparable performance as the state-of-the-art methods in both tasks withconsiderably reduced complexity.
arxiv-14700-132 | ActiveClean: Interactive Data Cleaning While Learning Convex Loss Models | http://arxiv.org/pdf/1601.03797v1.pdf | author:Sanjay Krishnan, Jiannan Wang, Eugene Wu, Michael J. Franklin, Ken Goldberg category:cs.DB cs.LG published:2016-01-15 summary:Data cleaning is often an important step to ensure that predictive models,such as regression and classification, are not affected by systematic errorssuch as inconsistent, out-of-date, or outlier data. Identifying dirty data isoften a manual and iterative process, and can be challenging on large datasets.However, many data cleaning workflows can introduce subtle biases into thetraining processes due to violation of independence assumptions. We proposeActiveClean, a progressive cleaning approach where the model is updatedincrementally instead of re-training and can guarantee accuracy on partiallycleaned data. ActiveClean supports a popular class of models called convex lossmodels (e.g., linear regression and SVMs). ActiveClean also leverages thestructure of a user's model to prioritize cleaning those records likely toaffect the results. We evaluate ActiveClean on five real-world datasets UCIAdult, UCI EEG, MNIST, Dollars For Docs, and WorldBank with both real andsynthetic errors. Our results suggest that our proposed optimizations canimprove model accuracy by up-to 2.5x for the same amount of data cleaned.Furthermore for a fixed cleaning budget and on all real dirty datasets,ActiveClean returns more accurate models than uniform sampling and ActiveLearning.
arxiv-14700-133 | Towards Turkish ASR: Anatomy of a rule-based Turkish g2p | http://arxiv.org/pdf/1601.03783v1.pdf | author:Duygu Altinok category:cs.CL published:2016-01-15 summary:This paper describes the architecture and implementation of a rule-basedgrapheme to phoneme converter for Turkish. The system accepts surface form asinput, outputs SAMPA mapping of the all parallel pronounciations according tothe morphological analysis together with stress positions. The system has beenimplemented in Python
arxiv-14700-134 | Categorization Axioms for Clustering Results | http://arxiv.org/pdf/1403.2065v8.pdf | author:Jian Yu, Zongben Xu category:cs.LG published:2014-03-09 summary:Cluster analysis has attracted more and more attention in the field ofmachine learning and data mining. Numerous clustering algorithms have beenproposed and are being developed due to diverse theories and variousrequirements of emerging applications. Therefore, it is very worth establishingan unified axiomatic framework for data clustering. In the literature, it is anopen problem and has been proved very challenging. In this paper, clusteringresults are axiomatized by assuming that an proper clustering result shouldsatisfy categorization axioms. The proposed axioms not only introduceclassification of clustering results and inequalities of clustering results,but also are consistent with prototype theory and exemplar theory ofcategorization models in cognitive science. Moreover, the proposed axioms leadto three principles of designing clustering algorithm and cluster validityindex, which follow many popular clustering algorithms and cluster validityindices.
arxiv-14700-135 | A Blind Adaptive CDMA Receiver Based on State Space Structures | http://arxiv.org/pdf/1408.0196v2.pdf | author:Zaid Albataineh, Fathi M. Salem category:cs.IT cs.LG math.IT published:2014-08-01 summary:Code Division Multiple Access (CDMA) is a channel access method, based onspread-spectrum technology, used by various radio technologies world-wide. Ingeneral, CDMA is used as an access method in many mobile standards such asCDMA2000 and WCDMA. We address the problem of blind multiuser equalization inthe wideband CDMA system, in the noisy multipath propagation environment.Herein, we propose three new blind receiver schemes, which are based on statespace structures and Independent Component Analysis (ICA). These blindstate-space receivers (BSSR) do not require knowledge of the propagationparameters or spreading code sequences of the users they primarily exploit thenatural assumption of statistical independence among the source signals. Wealso develop three semi blind adaptive detectors by incorporating the newadaptive methods into the standard RAKE receiver structure. Extensivecomparative case study, based on Bit error rate (BER) performance of thesemethods, is carried out for different number of users, symbols per user, andsignal to noise ratio (SNR) in comparison with conventional detectors,including the Blind Multiuser Detectors (BMUD) and Linear Minimum mean squarederror (LMMSE). The results show that the proposed methods outperform the otherdetectors in estimating the symbol signals from the received mixed CDMAsignals. Moreover, the new blind detectors mitigate the multi accessinterference (MAI) in CDMA.
arxiv-14700-136 | Generation of a Supervised Classification Algorithm for Time-Series Variable Stars with an Application to the LINEAR Dataset | http://arxiv.org/pdf/1601.03769v1.pdf | author:Kyle B Johnston, Hakeem M Oluseyi category:astro-ph.IM cs.LG published:2016-01-14 summary:With the advent of digital astronomy, new benefits and new problems have beenpresented to the modern day astronomer. While data can be captured in a moreefficient and accurate manor using digital means, the efficiency of dataretrieval has led to an overload of scientific data for processing and storage.This paper will focus on the construction and application of a supervisedpattern classification algorithm for the identification of variable stars.Given the reduction of a survey of stars into a standard feature space, theproblem of using prior patterns to identify new observed patterns can bereduced to time tested classification methodologies and algorithms. Suchsupervised methods, so called because the user trains the algorithms prior toapplication using patterns with known classes or labels, provide a means toprobabilistically determine the estimated class type of new observations. Thispaper will demonstrate the construction and application of a supervisedclassification algorithm on variable star data. The classifier is applied to aset of 192,744 LINEAR data points. Of the original samples, 34,451 unique starswere classified with high confidence (high level of probability of being thetrue class).
arxiv-14700-137 | Linear Algebraic Structure of Word Senses, with Applications to Polysemy | http://arxiv.org/pdf/1601.03764v1.pdf | author:Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski category:cs.CL cs.LG stat.ML published:2016-01-14 summary:Word embeddings are ubiquitous in NLP and information retrieval, but it'sunclear what they represent when the word is polysemous, i.e., has multiplesenses. Here it is shown that multiple word senses reside in linearsuperposition within the word embedding and can be recovered by simple sparsecoding. The success of the method ---which applies to several embedding methodsincluding word2vec--- is mathematically explained using the random walk ondiscourses model (Arora et al., 2015). A novel aspect of our technique is thateach word sense is also accompanied by one of about 2000 "discourse atoms" thatgive a succinct description of which other words co-occur with that word sense.Discourse atoms seem of independent interest, and make the method potentiallymore useful than the traditional clustering-based approaches to polysemy.
arxiv-14700-138 | Dual-tree $k$-means with bounded iteration runtime | http://arxiv.org/pdf/1601.03754v1.pdf | author:Ryan R. Curtin category:cs.DS cs.LG published:2016-01-14 summary:k-means is a widely used clustering algorithm, but for $k$ clusters and adataset size of $N$, each iteration of Lloyd's algorithm costs $O(kN)$ time.Although there are existing techniques to accelerate single Lloyd iterations,none of these are tailored to the case of large $k$, which is increasinglycommon as dataset sizes grow. We propose a dual-tree algorithm that gives theexact same results as standard $k$-means; when using cover trees, we useadaptive analysis techniques to, under some assumptions, bound thesingle-iteration runtime of the algorithm as $O(N + k log k)$. To our knowledgethese are the first sub-$O(kN)$ bounds for exact Lloyd iterations. We then showthat this theoretically favorable algorithm performs competitively in practice,especially for large $N$ and $k$ in low dimensions. Further, the algorithm istree-independent, so any type of tree may be used.
arxiv-14700-139 | Monitoring Potential Drug Interactions and Reactions via Network Analysis of Instagram User Timelines | http://arxiv.org/pdf/1510.01006v2.pdf | author:Rion Brattig Correia, Lang Li, Luis M. Rocha category:cs.SI cs.CY cs.IR q-bio.QM stat.ML published:2015-10-05 summary:Much recent research aims to identify evidence for Drug-Drug Interactions(DDI) and Adverse Drug reactions (ADR) from the biomedical scientificliterature. In addition to this "Bibliome", the universe of social mediaprovides a very promising source of large-scale data that can help identify DDIand ADR in ways that have not been hitherto possible. Given the large number ofusers, analysis of social media data may be useful to identify under-reported,population-level pathology associated with DDI, thus further contributing toimprovements in population health. Moreover, tapping into this data allows usto infer drug interactions with natural products--including cannabis--whichconstitute an array of DDI very poorly explored by biomedical research thusfar. Our goal is to determine the potential of Instagram for public healthmonitoring and surveillance for DDI, ADR, and behavioral pathology at large.Using drug, symptom, and natural product dictionaries for identification of thevarious types of DDI and ADR evidence, we have collected ~7000 timelines. Wereport on 1) the development of a monitoring tool to easily observe user-leveltimelines associated with drug and symptom terms of interest, and 2)population-level behavior via the analysis of co-occurrence networks computedfrom user timelines at three different scales: monthly, weekly, and dailyoccurrences. Analysis of these networks further reveals 3) drug and symptomdirect and indirect associations with greater support in user timelines, aswell as 4) clusters of symptoms and drugs revealed by the collective behaviorof the observed population. This demonstrates that Instagram contains muchdrug- and pathology specific data for public health monitoring of DDI and ADR,and that complex network analysis provides an important toolbox to extracthealth-related associations and their support from large-scale social mediadata.
arxiv-14700-140 | Inference of hidden structures in complex physical systems by multi-scale clustering | http://arxiv.org/pdf/1503.01626v2.pdf | author:Z. Nussinov, P. Ronhovde, Dandan Hu, S. Chakrabarty, M. Sahu, Bo Sun, N. A. Mauro, K. K. Sahu category:cs.CV published:2015-03-05 summary:We survey the application of a relatively new branch of statisticalphysics--"community detection"-- to data mining. In particular, we focus on thediagnosis of materials and automated image segmentation. Community detectiondescribes the quest of partitioning a complex system involving many elementsinto optimally decoupled subsets or communities of such elements. We review amultiresolution variant which is used to ascertain structures at differentspatial and temporal scales. Significant patterns are obtained by examining thecorrelations between different independent solvers. Similar to othercombinatorial optimization problems in the NP complexity class, communitydetection exhibits several phases. Typically, illuminating orders are revealedby choosing parameters that lead to extremal information theory correlations.
arxiv-14700-141 | Dynamic Concept Composition for Zero-Example Event Detection | http://arxiv.org/pdf/1601.03679v1.pdf | author:Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang, Alexander G. Hauptmann category:cs.CV published:2016-01-14 summary:In this paper, we focus on automatically detecting events in unconstrainedvideos without the use of any visual training exemplars. In principle,zero-shot learning makes it possible to train an event detection model based onthe assumption that events (e.g. \emph{birthday party}) can be described bymultiple mid-level semantic concepts (e.g. "blowing candle", "birthday cake").Towards this goal, we first pre-train a bundle of concept classifiers usingdata from other sources. Then we evaluate the semantic correlation of eachconcept \wrt the event of interest and pick up the relevant conceptclassifiers, which are applied on all test videos to get multiple predictionscore vectors. While most existing systems combine the predictions of theconcept classifiers with fixed weights, we propose to learn the optimal weightsof the concept classifiers for each testing video by exploring a set of onlineavailable videos with free-form text descriptions of their content. To validatethe effectiveness of the proposed approach, we have conducted extensiveexperiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset.The experimental results confirm the superiority of the proposed approach.
arxiv-14700-142 | Multimodal Hierarchical Dirichlet Process-based Active Perception | http://arxiv.org/pdf/1510.00331v3.pdf | author:Tadahiro Taniguchi, Toshiaki Takano, Ryo Yoshino category:cs.RO cs.AI stat.ML published:2015-10-01 summary:In this paper, we propose an active perception method for recognizing objectcategories based on the multimodal hierarchical Dirichlet process (MHDP). TheMHDP enables a robot to form object categories using multimodal information,e.g., visual, auditory, and haptic information, which can be observed byperforming actions on an object. However, performing many actions on a targetobject requires a long time. In a real-time scenario, i.e., when the time islimited, the robot has to determine the set of actions that is most effectivefor recognizing a target object. We propose an MHDP-based active perceptionmethod that uses the information gain (IG) maximization criterion and lazygreedy algorithm. We show that the IG maximization criterion is optimal in thesense that the criterion is equivalent to a minimization of the expectedKullback--Leibler divergence between a final recognition state and therecognition state after the next set of actions. However, a straightforwardcalculation of IG is practically impossible. Therefore, we derive an efficientMonte Carlo approximation method for IG by making use of a property of theMHDP. We also show that the IG has submodular and non-decreasing properties asa set function because of the structure of the graphical model of the MHDP.Therefore, the IG maximization problem is reduced to a submodular maximizationproblem. This means that greedy and lazy greedy algorithms are effective andhave a theoretical justification for their performance. We conducted anexperiment using an upper-torso humanoid robot and a second one using syntheticdata. The experimental results show that the method enables the robot to selecta set of actions that allow it to recognize target objects quickly andaccurately. The results support our theoretical outcomes.
arxiv-14700-143 | Manifold Regularized Deep Neural Networks using Adversarial Examples | http://arxiv.org/pdf/1511.06381v2.pdf | author:Taehoon Lee, Minsuk Choi, Sungroh Yoon category:cs.LG cs.CV published:2015-11-19 summary:Learning meaningful representations using deep neural networks involvesdesigning efficient training schemes and well-structured networks. Currently,the method of stochastic gradient descent that has a momentum with dropout isone of the most popular training protocols. Based on that, more advancedmethods (i.e., Maxout and Batch Normalization) have been proposed in recentyears, but most still suffer from performance degradation caused by smallperturbations, also known as adversarial examples. To address this issue, wepropose manifold regularized networks (MRnet) that utilize a novel trainingobjective function that minimizes the difference between multi-layer embeddingresults of samples and those adversarial. Our experimental results demonstratedthat MRnet is more resilient to adversarial examples and helps us to generalizerepresentations on manifolds. Furthermore, combining MRnet and dropout allowedus to achieve competitive classification performances for three well-knownbenchmarks: MNIST, CIFAR-10, and SVHN.
arxiv-14700-144 | Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation | http://arxiv.org/pdf/1601.03651v1.pdf | author:Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin category:cs.CL cs.LG published:2016-01-14 summary:Nowadays, neural networks play an important role in the task of relationclassification. By designing different neural architectures, researchers haveimproved the performance to a large extent, compared with traditional methods.However, existing neural networks for relation classification are usually ofshallow architectures (e.g., one-layer convolution neural networks or recurrentnetworks). They may fail to explore the potential representation space indifferent abstraction levels. In this paper, we propose deep recurrent neuralnetworks (DRNNs) to tackle this challenge. Further, we propose a dataaugmentation method by leveraging the directionality of relations. We evaluateour DRNNs on the SemEval-2010 Task 8, and achieve an $F_1$-score of 85.81%,outperforming state-of-the-art recorded results.
arxiv-14700-145 | Optimal Supervised Learning in Spiking Neural Networks for Precise Temporal Encoding | http://arxiv.org/pdf/1601.03649v1.pdf | author:Brian Gardner, André Grüning category:cs.NE q-bio.NC published:2016-01-14 summary:Precise spike timing as a means to encode information in neural networks isbiologically supported, and is advantageous over frequency-based codes byprocessing input features on a much shorter time-scale. For these reasons, muchrecent attention has been focused on the development of supervised learningrules for spiking neural networks that utilise a temporal coding scheme.However, despite significant progress in this area, there still lack rules thatare theoretically justified, and yet can be considered biologically relevant.Here we examine the general conditions under which optimal synaptic plasticitytakes place to support the supervised learning of a precise temporal code. Aspart of our analysis we introduce two analytically derived learning rules, oneof which relies on an instantaneous error signal to optimise synaptic weightsin a network (INST rule), and the other one relying on a filtered error signalto minimise the variance of synaptic weight modifications (FILT rule). We testthe optimality of the solutions provided by each rule with respect to theirtemporal encoding precision, and then measure the maximum number of inputpatterns they can learn to memorise using the precise timings of individualspikes. Our results demonstrate the optimality of the FILT rule in most cases,underpinned by the rule's error-filtering mechanism which provides smoothconvergence during learning. We also find the FILT rule to be most efficient atperforming input pattern memorisations, and most noticeably when patterns areidentified using spikes with sub-millisecond temporal precision. In comparisonwith existing work, we determine the performance of the FILT rule to beconsistent with that of the highly efficient E-learning Chronotron rule, butwith the distinct advantage that our FILT rule is also implementable as anonline method for increased biological realism.
arxiv-14700-146 | Jointly Learning Non-negative Projection and Dictionary with Discriminative Graph Constraints for Classification | http://arxiv.org/pdf/1511.04601v3.pdf | author:Weiyang Liu, Zhiding Yu, Yingzhen Yang, Meng Yang, Thomas S. Huang category:cs.CV published:2015-11-14 summary:Dictionary learning (DL) for sparse coding has shown impressive performancein classification tasks. But how to select a feature that can best work withthe learned dictionary remains an open question. Current prevailing DL methodsusually adopt existing well-performing features, ignoring the innerrelationship between dictionaries and features. To address the problem, wepropose a joint non-negative projection and dictionary learning (JNPDL) method.Non-negative projection learning and dictionary learning are complementary toeach other, since the former leads to the intrinsic discriminative parts-basedfeatures for objects while the latter searches a suitable representation in theprojected feature space. Specifically, discrimination of projection anddictionary is achieved by imposing to both projection and coding coefficients agraph constraint that maximizes the intra-class compactness and inter-classseparability. Experimental results on both image classification and image setclassification show the excellent performance of JNPDL by being comparable oroutperforming many state-of-the-art approaches.
arxiv-14700-147 | Hybrid Dialog State Tracker | http://arxiv.org/pdf/1510.03710v3.pdf | author:Miroslav Vodolán, Rudolf Kadlec, Jan Kleindienst category:cs.CL published:2015-10-13 summary:This paper presents a hybrid dialog state tracker that combines a rule basedand a machine learning based approach to belief state tracking. Therefore, wecall it a hybrid tracker. The machine learning in our tracker is realized by aLong Short Term Memory (LSTM) network. To our knowledge, our hybrid trackersets a new state-of-the-art result for the Dialog State Tracking Challenge(DSTC) 2 dataset when the system uses only live SLU as its input.
arxiv-14700-148 | Quantification of Ultrasonic Texture heterogeneity via Volumetric Stochastic Modeling for Tissue Characterization | http://arxiv.org/pdf/1601.03531v1.pdf | author:O. S. Al-Kadi, Daniel Y. F. Chung, Robert C. Carlisle, Constantin C. Coussios, J. Alison Noble category:cs.CV published:2016-01-14 summary:Intensity variations in image texture can provide powerful quantitativeinformation about physical properties of biological tissue. However, tissuepatterns can vary according to the utilized imaging system and areintrinsically correlated to the scale of analysis. In the case of ultrasound,the Nakagami distribution is a general model of the ultrasonic backscatteringenvelope under various scattering conditions and densities where it can beemployed for characterizing image texture, but the subtle intra-heterogeneitieswithin a given mass are difficult to capture via this model as it works at asingle spatial scale. This paper proposes a locally adaptive 3Dmulti-resolution Nakagami-based fractal feature descriptor that extendsNakagami-based texture analysis to accommodate subtle speckle spatial frequencytissue intensity variability in volumetric scans. Local textural fractaldescriptors - which are invariant to affine intensity changes - are extractedfrom volumetric patches at different spatial resolutions from voxellattice-based generated shape and scale Nakagami parameters. Using ultrasoundradio-frequency datasets we found that after applying an adaptive fractaldecomposition label transfer approach on top of the generated Nakagami voxels,tissue characterization results were superior to the state of art. Experimentalresults on real 3D ultrasonic pre-clinical and clinical datasets suggest thatdescribing tumor intra-heterogeneity via this descriptor may facilitateimproved prediction of therapy response and disease characterization.
arxiv-14700-149 | Quantum Privacy-Preserving Data Mining | http://arxiv.org/pdf/1512.04009v2.pdf | author:Shenggang Ying, Mingsheng Ying, Yuan Feng category:quant-ph cs.CR cs.DB cs.LG published:2015-12-13 summary:Data mining is a key technology in big data analytics and it can discoverunderstandable knowledge (patterns) hidden in large data sets. Association ruleis one of the most useful knowledge patterns, and a large number of algorithmshave been developed in the data mining literature to generate association rulescorresponding to different problems and situations. Privacy becomes a vitalissue when data mining is used to sensitive data sets like medical records,commercial data sets and national security. In this Letter, we present aquantum protocol for mining association rules on vertically partitioneddatabases. The quantum protocol can improve the privacy level preserved byknown classical protocols and at the same time it can exponentially reduce thecomputational complexity and communication cost.
arxiv-14700-150 | Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction | http://arxiv.org/pdf/1504.03293v3.pdf | author:Yuting Zhang, Kihyuk Sohn, Ruben Villegas, Gang Pan, Honglak Lee category:cs.CV published:2015-04-13 summary:Object detection systems based on the deep convolutional neural network (CNN)have recently made ground- breaking advances on several object detectionbenchmarks. While the features learned by these high-capacity neural networksare discriminative for categorization, inaccurate localization is still a majorsource of error for detection. Building upon high-capacity CNN architectures,we address the localization problem by 1) using a search algorithm based onBayesian optimization that sequentially proposes candidate regions for anobject bounding box, and 2) training the CNN with a structured loss thatexplicitly penalizes the localization inaccuracy. In experiments, wedemonstrated that each of the proposed methods improves the detectionperformance over the baseline method on PASCAL VOC 2007 and 2012 datasets.Furthermore, two methods are complementary and significantly outperform theprevious state-of-the-art when combined.
arxiv-14700-151 | Tracking Objects with Higher Order Interactions using Delayed Column Generation | http://arxiv.org/pdf/1512.02413v2.pdf | author:Steffen Wolf, Fred A. Hamprecht, Julian Yarkony category:cs.CV published:2015-12-08 summary:We introduce a new approach to tracking a large number of objects.Specifically we consider tracking in the context of higher order Markovinteractions which can not be modeled using network flow techniques ascurrently developed. Our approach relies on delayed column generation and isinspired by the corresponding approach to the cutting stock problem. Columnscan be generated exactly using dynamic programming.
arxiv-14700-152 | A simple yet efficient algorithm for multiple kernel learning under elastic-net constraints | http://arxiv.org/pdf/1506.08536v2.pdf | author:Luca Citi category:stat.ML cs.LG published:2015-06-29 summary:This report presents an algorithm for the solution of multiple kernellearning (MKL) problems with elastic-net constraints on the kernel weights.
arxiv-14700-153 | Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling | http://arxiv.org/pdf/1512.09103v2.pdf | author:Zeyuan Allen-Zhu, Zheng Qu, Peter Richtárik, Yang Yuan category:math.OC cs.DS math.NA stat.ML published:2015-12-30 summary:Accelerated coordinate descent methods are widely used in optimization andmachine learning. By taking cheap-to-compute coordinate gradients in eachiteration, they are usually faster than accelerated full gradient descent, andthus suitable for large-scale optimization problems. In this paper, we improve the running time of accelerated coordinate descent,using a clean and novel non-uniform sampling method. In particular, if afunction $f$ is coordinate-wise smooth with parameters $L_1,L_2,\dots,L_n$, weobtain an algorithm that converges in $O(\sum_i \sqrt{L_i} /\sqrt{\varepsilon})$ iterations for convex $f$ and in $\tilde{O}(\sum_i\sqrt{L_i} / \sqrt{\sigma})$ iterations for $\sigma$-strongly convex $f$. Thebest known result was $\tilde{O}(\sqrt{n \sum_i L_i} / \sqrt{\varepsilon})$ forconvex $f$ and $\tilde{O}(\sqrt{n \sum_i L_i} / \sqrt{\sigma})$ for$\sigma$-strongly convex $f$, due to Lee and Sidford. Thus, our algorithmimproves the running time by a factor up to $\sqrt{n}$. Our speed-up naturally applies to important problems such as empirical riskminimizations and solving linear systems.
arxiv-14700-154 | Multi-Atlas Segmentation with Joint Label Fusion of Osteoporotic Vertebral Compression Fractures on CT | http://arxiv.org/pdf/1601.03375v1.pdf | author:Yinong Wang, Jianhua Yao, Holger R. Roth, Joseph E. Burns, Ronald M. Summers category:cs.CV published:2016-01-13 summary:The precise and accurate segmentation of the vertebral column is essential inthe diagnosis and treatment of various orthopedic, neurological, andoncological traumas and pathologies. Segmentation is especially challenging inthe presence of pathology such as vertebral compression fractures. In thispaper, we propose a method to produce segmentations for osteoporoticcompression fractured vertebrae by applying a multi-atlas joint label fusiontechnique for clinical CT images. A total of 170 thoracic and lumbar vertebraewere evaluated using atlases from five patients with varying degrees of spinaldegeneration. In an osteoporotic cohort of bundled atlases, registrationprovided an average Dice coefficient and mean absolute surface distance of2.7$\pm$4.5% and 0.32$\pm$0.13mm for osteoporotic vertebrae, respectively, and90.9$\pm$3.0% and 0.36$\pm$0.11mm for compression fractured vertebrae.
arxiv-14700-155 | EvoGrader: an online formative assessment tool for automatically evaluating written evolutionary explanations | http://arxiv.org/pdf/1601.03348v1.pdf | author:Kayhan Moharreri, Minsu Ha, Ross H Nehm category:cs.CL published:2016-01-13 summary:EvoGrader is a free, online, on-demand formative assessment service designedfor use in undergraduate biology classrooms. EvoGrader's web portal is poweredby Amazon's Elastic Cloud and run with LightSIDE Lab's open-sourcemachine-learning tools. The EvoGrader web portal allows biology instructors toupload a response file (.csv) containing unlimited numbers of evolutionaryexplanations written in response to 86 different ACORNS (Assessing COntextualReasoning about Natural Selection) instrument items. The system automaticallyanalyzes the responses and provides detailed information about the scientificand naive concepts contained within each student's response, as well as overallstudent (and sample) reasoning model types. Graphs and visual models providedby EvoGrader summarize class-level responses; downloadable files of raw scores(in .csv format) are also provided for more detailed analyses. Although thecomputational machinery that EvoGrader employs is complex, using the system iseasy. Users only need to know how to use spreadsheets to organize studentresponses, upload files to the web, and use a web browser. A series ofexperiments using new samples of 2,200 written evolutionary explanationsdemonstrate that EvoGrader scores are comparable to those of trained humanraters, although EvoGrader scoring takes 99% less time and is free. EvoGraderwill be of interest to biology instructors teaching large classes who seek toemphasize scientific practices such as generating scientific explanations, andto teach crosscutting ideas such as evolution and natural selection. Thesoftware architecture of EvoGrader is described as it may serve as a templatefor developing machine-learning portals for other core concepts within biologyand across other disciplines.
arxiv-14700-156 | A Score-level Fusion Method for Eye Movement Biometrics | http://arxiv.org/pdf/1601.03333v1.pdf | author:Anjith George, Aurobinda Routray category:cs.CV published:2016-01-13 summary:This paper proposes a novel framework for the use of eye movement patternsfor biometric applications. Eye movements contain abundant information aboutcognitive brain functions, neural pathways, etc. In the proposed method, eyemovement data is classified into fixations and saccades. Features extractedfrom fixations and saccades are used by a Gaussian Radial Basis FunctionNetwork (GRBFN) based method for biometric authentication. A score fusionapproach is adopted to classify the data in the output layer. In the evaluationstage, the algorithm has been tested using two types of stimuli: random dotfollowing on a screen and text reading. The results indicate the strength ofeye movement pattern as a biometric modality. The algorithm has been evaluatedon BioEye 2015 database and found to outperform all the other methods. Eyemovements are generated by a complex oculomotor plant which is very hard tospoof by mechanical replicas. Use of eye movement dynamics along with irisrecognition technology may lead to a robust counterfeit-resistant personidentification system.
arxiv-14700-157 | Localized Dictionary design for Geometrically Robust Sonar ATR | http://arxiv.org/pdf/1601.03323v1.pdf | author:John McKay, Vishal Monga, Raghu Raj category:cs.CV published:2016-01-13 summary:Advancements in Sonar image capture have opened the door to powerfulclassification schemes for automatic target recognition (ATR. Recent work hasparticularly seen the application of sparse reconstruction-based classification(SRC) to sonar ATR, which provides compelling accuracy rates even in thepresence of noise and blur. Existing sparsity based sonar ATR techniqueshowever assume that the test images exhibit geometric pose that is consistentwith respect to the training set. This work addresses the outstanding openchallenge of handling inconsistently posed test sonar images relative totraining. We develop a new localized block-based dictionary design that canenable geometric, i.e. pose robustness. Further, a dictionary learning methodis incorporated to increase performance and efficiency. The proposed SRC withLocalized Pose Management (LPM), is shown to outperform the state of the artSIFT feature and SVM approach, due to its power to discern background clutterin Sonar images.
arxiv-14700-158 | Song Recommendation with Non-Negative Matrix Factorization and Graph Total Variation | http://arxiv.org/pdf/1601.01892v2.pdf | author:Kirell Benzi, Vassilis Kalofolias, Xavier Bresson, Pierre Vandergheynst category:stat.ML cs.IR cs.LG published:2016-01-08 summary:This work formulates a novel song recommender system as a matrix completionproblem that benefits from collaborative filtering through Non-negative MatrixFactorization (NMF) and content-based filtering via total variation (TV) ongraphs. The graphs encode both playlist proximity information and songsimilarity, using a rich combination of audio, meta-data and social features.As we demonstrate, our hybrid recommendation system is very versatile andincorporates several well-known methods while outperforming them. Particularly,we show on real-world data that our model overcomes w.r.t. two evaluationmetrics the recommendation of models solely based on low-rank information,graph-based information or a combination of both.
arxiv-14700-159 | Document image classification, with a specific view on applications of patent images | http://arxiv.org/pdf/1601.03295v1.pdf | author:Gabriela Csurka category:cs.CV published:2016-01-13 summary:The main focus of this paper is document image classification and retrieval,where we analyze and compare different parameters for the RunLeght Histogram(RL) and Fisher Vector (FV) based image representations. We do an exhaustiveexperimental study using different document image datasets, including the MARGbenchmarks, two datasets built on customer data and the images from the PatentImage Classification task of the Clef-IP 2011. The aim of the study is to giveguidelines on how to best choose the parameters such that the same featuresperform well on different tasks. As an example of such need, we describe theImage-based Patent Retrieval task's of Clef-IP 2011, where we used the sameimage representation to predict the image type and retrieve relevant patents.
arxiv-14700-160 | Predicting the Effectiveness of Self-Training: Application to Sentiment Classification | http://arxiv.org/pdf/1601.03288v1.pdf | author:Vincent Van Asch, Walter Daelemans category:cs.CL published:2016-01-13 summary:The goal of this paper is to investigate the connection between theperformance gain that can be obtained by selftraining and the similaritybetween the corpora used in this approach. Self-training is a semi-supervisedtechnique designed to increase the performance of machine learning algorithmsby automatically classifying instances of a task and adding these as additionaltraining material to the same classifier. In the context of language processingtasks, this training material is mostly an (annotated) corpus. Unfortunatelyself-training does not always lead to a performance increase and whether itwill is largely unpredictable. We show that the similarity between corpora canbe used to identify those setups for which self-training can be beneficial. Weconsider this research as a step in the process of developing a classifier thatis able to adapt itself to each new test corpus that it is presented with.
arxiv-14700-161 | Digital Image Forensics vs. Image Composition: An Indirect Arms Race | http://arxiv.org/pdf/1601.03239v1.pdf | author:Victor Schetinger, Massimo Iuliani, Alessandro Piva, Manuel M. Oliveira category:cs.CV cs.MM published:2016-01-13 summary:The field of image composition is constantly trying to improve the ways inwhich an image can be altered and enhanced. While this is usually done in thename of aesthetics and practicality, it also provides tools that can be used tomaliciously alter images. In this sense, the field of digital image forensicshas to be prepared to deal with the influx of new technology, in a constantarms-race. In this paper, the current state of this arms-race is analyzed,surveying the state-of-the-art and providing means to compare both sides. Anovel scale to classify image forensics assessments is proposed, andexperiments are performed to test composition techniques in regards todifferent forensics traces. We show that even though research in forensicsseems unaware of the advanced forms of image composition, it possesses thebasic tools to detect it.
arxiv-14700-162 | Multi-Face Tracking by Extended Bag-of-Tracklets in Egocentric Videos | http://arxiv.org/pdf/1507.04576v2.pdf | author:Maedeh Aghaei, Mariella Dimiccoli, Petia Radeva category:cs.CV published:2015-07-16 summary:Wearable cameras offer a hands-free way to record egocentric images of dailyexperiences, where social events are of special interest. The first steptowards detection of social events is to track the appearance of multiplepersons involved in it. In this paper, we propose a novel method to findcorrespondences of multiple faces in low temporal resolution egocentric videosacquired through a wearable camera. This kind of photo-stream imposesadditional challenges to the multi-tracking problem with respect toconventional videos. Due to the free motion of the camera and to its lowtemporal resolution, abrupt changes in the field of view, in illuminationcondition and in the target location are highly frequent. To overcome suchdifficulties, we propose a multi-face tracking method that generates a set oftracklets through finding correspondences along the whole sequence for eachdetected face and takes advantage of the tracklets redundancy to deal withunreliable ones. Similar tracklets are grouped into the so called extendedbag-of-tracklets (eBoT), which is aimed to correspond to a specific person.Finally, a prototype tracklet is extracted for each eBoT, where the occurredocclusions are estimated by relying on a new measure of confidence. Wevalidated our approach over an extensive dataset of egocentric photo-streamsand compared it to state of the art methods, demonstrating its effectivenessand robustness.
arxiv-14700-163 | The scarcity of crossing dependencies: a direct outcome of a specific constraint? | http://arxiv.org/pdf/1601.03210v1.pdf | author:Carlos Gómez-Rodríguez, Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph published:2016-01-13 summary:Crossing syntactic dependencies have been observed to be infrequent innatural language, to the point that some syntactic theories and formalismsdisregard them entirely. This leads to the question of whether the scarcity ofcrossings in languages arises from an independent and specific constraint oncrossings. We provide statistical evidence suggesting that this is not thecase, as the proportion of dependency crossings in a wide range of naturallanguage treebanks can be accurately estimated by a simple predictor based onthe local probability that two dependencies cross given their lengths. Therelative error of this predictor never exceeds 5% on average, whereas abaseline predictor assuming a random ordering of the words of a sentence incursa relative error that is at least 6 times greater. Our results suggest that thelow frequency of crossings in natural languages is neither originated by hiddenknowledge of language nor by the undesirability of crossings per se, but as amere side effect of the principle of dependency length minimization.
arxiv-14700-164 | Dense Bag-of-Temporal-SIFT-Words for Time Series Classification | http://arxiv.org/pdf/1601.01799v2.pdf | author:Adeline Bailly, Simon Malinowski, Romain Tavenard, Thomas Guyet, Laetitia Chapel category:cs.LG published:2016-01-08 summary:Time series classification is an application of particular interest with theincrease of data to monitor. Classical techniques for time seriesclassification rely on point-to-point distances. Recently, Bag-of-Wordsapproaches have been used in this context. Words are quantized versions ofsimple features extracted from sliding windows. The SIFT framework has provedefficient for image classification. In this paper, we design a time seriesclassification scheme that builds on the SIFT framework adapted to time seriesto feed a Bag-of-Words. We then refine our method by studying the impact ofnormalized Bag-of-Words, as well as densely extract point descriptors. Proposedadjustements achieve better performance. The evaluation shows that our methodoutperforms classical techniques in terms of classification.
arxiv-14700-165 | Group Invariant Deep Representations for Image Instance Retrieval | http://arxiv.org/pdf/1601.02093v2.pdf | author:Olivier Morère, Antoine Veillard, Jie Lin, Julie Petta, Vijay Chandrasekhar, Tomaso Poggio category:cs.CV cs.IR published:2016-01-09 summary:Most image instance retrieval pipelines are based on comparison of vectorsknown as global image descriptors between a query image and the databaseimages. Due to their success in large scale image classification,representations extracted from Convolutional Neural Networks (CNN) are quicklygaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptorsfor image instance retrieval. While CNN-based descriptors are generallyremarked for good retrieval performance at lower bitrates, they neverthelesspresent a number of drawbacks including the lack of robustness to common objecttransformations such as rotations compared with their interest point based FVcounterparts. In this paper, we propose a method for computing invariant global descriptorsfrom CNNs. Our method implements a recently proposed mathematical theory forinvariance in a sensory cortex modeled as a feedforward neural network. Theresulting global descriptors can be made invariant to multiple arbitrarytransformation groups while retaining good discriminativeness. Based on a thorough empirical evaluation using several publicly availabledatasets, we show that our method is able to significantly and consistentlyimprove retrieval results every time a new type of invariance is incorporated.We also show that our method which has few parameters is not prone tooverfitting: improvements generalize well across datasets with differentproperties with regard to invariances. Finally, we show that our descriptorsare able to compare favourably to other state-of-the-art compact descriptors insimilar bitranges, exceeding the highest retrieval results reported in theliterature on some datasets. A dedicated dimensionality reduction step--quantization or hashing-- may be able to further improve the competitivenessof the descriptors.
arxiv-14700-166 | Enhancing Energy Minimization Framework for Scene Text Recognition with Top-Down Cues | http://arxiv.org/pdf/1601.03128v1.pdf | author:Anand Mishra, Karteek Alahari, C. V. Jawahar category:cs.CV published:2016-01-13 summary:Recognizing scene text is a challenging problem, even more so than therecognition of scanned documents. This problem has gained significant attentionfrom the computer vision community in recent years, and several methods basedon energy minimization frameworks and deep learning approaches have beenproposed. In this work, we focus on the energy minimization framework andpropose a model that exploits both bottom-up and top-down cues for recognizingcropped words extracted from street images. The bottom-up cues are derived fromindividual character detections from an image. We build a conditional randomfield model on these detections to jointly model the strength of the detectionsand the interactions between them. These interactions are top-down cuesobtained from a lexicon-based prior, i.e., language statistics. The optimalword represented by the text image is obtained by minimizing the energyfunction corresponding to the random field model. We evaluate our proposedalgorithm extensively on a number of cropped scene text benchmark datasets,namely Street View Text, ICDAR 2003, 2011 and 2013 datasets, and IIIT 5K-word,and show better performance than comparable methods. We perform a rigorousanalysis of all the steps in our approach and analyze the results. We also showthat state-of-the-art convolutional neural network features can be integratedin our framework to further improve the recognition performance.
arxiv-14700-167 | Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization | http://arxiv.org/pdf/1601.03124v1.pdf | author:Guangyong Chen, Fengyuan Zhu, Pheng Ann Heng category:cs.LG stat.ML published:2016-01-13 summary:Dyadic Data Prediction (DDP) is an important problem in many research areas.This paper develops a novel fully Bayesian nonparametric framework whichintegrates two popular and complementary approaches, discrete mixed membershipmodeling and continuous latent factor modeling into a unified HeterogeneousMatrix Factorization~(HeMF) model, which can predict the unobserved dyadicsaccurately. The HeMF can determine the number of communities automatically andexploit the latent linear structure for each bicluster efficiently. We proposea Variational Bayesian method to estimate the parameters and missing data. Wefurther develop a novel online learning approach for Variational inference anduse it for the online learning of HeMF, which can efficiently cope with theimportant large-scale DDP problem. We evaluate the performance of our method onthe EachMoive, MovieLens and Netflix Prize collaborative filtering datasets.The experiment shows that, our model outperforms state-of-the-art methods onall benchmarks. Compared with Stochastic Gradient Method (SGD), our onlinelearning approach achieves significant improvement on the estimation accuracyand robustness.
arxiv-14700-168 | Blind Image Denoising via Dependent Dirichlet Process Tree | http://arxiv.org/pdf/1601.03117v1.pdf | author:Fengyuan Zhu, Guangyong Chen, Jianye Hao, Pheng-Ann Heng category:cs.CV stat.ML published:2016-01-13 summary:Most existing image denoising approaches assumed the noise to be homogeneouswhite Gaussian distributed with known intensity. However, in real noisy images,the noise models are usually unknown beforehand and can be much more complex.This paper addresses this problem and proposes a novel blind image denoisingalgorithm to recover the clean image from noisy one with the unknown noisemodel. To model the empirical noise of an image, our method introduces themixture of Gaussian distribution, which is flexible enough to approximatedifferent continuous distributions. The problem of blind image denoising isreformulated as a learning problem. The procedure is to first build a two-layerstructural model for noisy patches and consider the clean ones as latentvariable. To control the complexity of the noisy patch model, this workproposes a novel Bayesian nonparametric prior called "Dependent DirichletProcess Tree" to build the model. Then, this study derives a variationalinference algorithm to estimate model parameters and recover clean patches. Weapply our method on synthesis and real noisy images with different noisemodels. Comparing with previous approaches, ours achieves better performance.The experimental results indicate the efficiency of the proposed algorithm tocope with practical image denoising tasks.
arxiv-14700-169 | Provable Tensor Methods for Learning Mixtures of Generalized Linear Models | http://arxiv.org/pdf/1412.3046v4.pdf | author:Hanie Sedghi, Majid Janzamin, Anima Anandkumar category:cs.LG stat.ML published:2014-12-09 summary:We consider the problem of learning mixtures of generalized linear models(GLM) which arise in classification and regression problems. Typical learningapproaches such as expectation maximization (EM) or variational Bayes can getstuck in spurious local optima. In contrast, we present a tensor decompositionmethod which is guaranteed to correctly recover the parameters. The key insightis to employ certain feature transformations of the input, which depend on theinput generative model. Specifically, we employ score function tensors of theinput and compute their cross-correlation with the response variable. Weestablish that the decomposition of this tensor consistently recovers theparameters, under mild non-degeneracy conditions. We demonstrate that thecomputational and sample complexity of our method is a low order polynomial ofthe input and the latent dimensions.
arxiv-14700-170 | A note on the sample complexity of the Er-SpUD algorithm by Spielman, Wang and Wright for exact recovery of sparsely used dictionaries | http://arxiv.org/pdf/1601.02049v2.pdf | author:Radosław Adamczak category:math.PR cs.LG math.ST stat.TH published:2016-01-08 summary:We consider the problem of recovering an invertible $n \times n$ matrix $A$and a sparse $n \times p$ random matrix $X$ based on the observation of $Y =AX$ (up to a scaling and permutation of columns of $A$ and rows of $X$). Usingonly elementary tools from the theory of empirical processes we show that aversion of the Er-SpUD algorithm by Spielman, Wang and Wright with highprobability recovers $A$ and $X$ exactly, provided that $p \ge Cn\log n$, whichis optimal up to the constant $C$.
arxiv-14700-171 | Creativity in Machine Learning | http://arxiv.org/pdf/1601.03642v1.pdf | author:Martin Thoma category:cs.CV cs.LG published:2016-01-12 summary:Recent machine learning techniques can be modified to produce creativeresults. Those results did not exist before; it is not a trivial combination ofthe data which was fed into the machine learning system. The obtained resultscome in multiple forms: As images, as text and as audio. This paper gives a high level overview of how they are created and gives someexamples. It is meant to be a summary of the current work and give people whoare new to machine learning some starting points.
arxiv-14700-172 | A metric for sets of trajectories that is practical and mathematically consistent | http://arxiv.org/pdf/1601.03094v1.pdf | author:José Bento category:cs.CV cs.SY math.OC published:2016-01-12 summary:Metrics on the space of sets of trajectories are important for scientists inthe field of computer vision, machine learning, robotics and general artificialintelligence. Yet existing notions of closeness are either mathematicallyinconsistent or of limited practical use. In this paper we outline thelimitations in the existing mathematically-consistent metrics, which are basedon Schuhmacher et al. 2008, and the inconsistencies in the heuristic notions ofcloseness used in practice, whose main ideas are common to the CLEAR MOTmeasures widely used in computer vision. In two steps we then propose a newintuitive metric between sets of trajectories and address these problems. Firstwe explain a natural solution that leads to a metric that is hard to compute.Then we modify this formulation to obtain a metric that is easy to compute andkeeps all the good properties of the previous metric. In particular, our notionof closeness is the first that has the following three properties: it can bequickly computed, it incorporates confusion of trajectories' identity in anoptimal way and it is a metric in the mathematical sense.
arxiv-14700-173 | Infomax strategies for an optimal balance between exploration and exploitation | http://arxiv.org/pdf/1601.03073v1.pdf | author:Gautam Reddy, Antonio Celani, Massimo Vergassola category:cs.LG cs.IT math.IT q-bio.PE stat.ML published:2016-01-12 summary:Proper balance between exploitation and exploration is what makes gooddecisions, which achieve high rewards like payoff or evolutionary fitness. TheInfomax principle postulates that maximization of information directs thefunction of diverse systems, from living systems to artificial neural networks.While specific applications are successful, the validity of information as aproxy for reward remains unclear. Here, we consider the multi-armed banditdecision problem, which features arms (slot-machines) of unknown probabilitiesof success and a player trying to maximize cumulative payoff by choosing thesequence of arms to play. We show that an Infomax strategy (Info-p) whichoptimally gathers information on the highest mean reward among the armssaturates known optimal bounds and compares favorably to existing policies. Thehighest mean reward considered by Info-p is not the quantity actually neededfor the choice of the arm to play, yet it allows for optimal tradeoffs betweenexploration and exploitation.
arxiv-14700-174 | Stochastic Gradient Made Stable: A Manifold Propagation Approach for Large-Scale Optimization | http://arxiv.org/pdf/1506.08350v2.pdf | author:Yadong Mu, Wei Liu, Wei Fan category:cs.LG cs.NA 90C06 published:2015-06-28 summary:Stochastic gradient descent (SGD) holds as a classical method to build largescale machine learning models over big data. A stochastic gradient is typicallycalculated from a limited number of samples (known as mini-batch), so itpotentially incurs a high variance and causes the estimated parameters bouncearound the optimal solution. To improve the stability of stochastic gradient,recent years have witnessed the proposal of several semi-stochastic gradientdescent algorithms, which distinguish themselves from standard SGD byincorporating global information into gradient computation. In this paper wecontribute a novel stratified semi-stochastic gradient descent (S3GD) algorithmto this nascent research area, accelerating the optimization of a large familyof composite convex functions. Though theoretically converging faster, priorsemi-stochastic algorithms are found to suffer from high iteration complexity,which makes them even slower than SGD in practice on many datasets. In ourproposed S3GD, the semi-stochastic gradient is calculated based on efficientmanifold propagation, which can be numerically accomplished by sparse matrixmultiplications. This way S3GD is able to generate a highly-accurate estimateof the exact gradient from each mini-batch with largely-reduced computationalcomplexity. Theoretic analysis reveals that the proposed S3GD elegantlybalances the geometric algorithmic convergence rate against the space and timecomplexities during the optimization. The efficacy of S3GD is alsoexperimentally corroborated on several large-scale benchmark datasets.
arxiv-14700-175 | Universality in halting time and its applications in optimization | http://arxiv.org/pdf/1511.06444v2.pdf | author:Levent Sagun, Thomas Trogdon, Yann LeCun category:cs.LG math.NA math.PR published:2015-11-19 summary:The authors present empirical universal distributions for the halting time(measured by the number of iterations to reach a given accuracy) ofoptimization algorithms applied to two random systems: spin glasses and deeplearning. Given an algorithm, which we take to be both the optimization routineand the form of the random landscape, the fluctuations of the halting timefollow a distribution that remains unchanged even when the input is changeddrastically. We observe two main universality classes, a Gumbel-likedistribution that appears in Google searches, human decision times, QRfactorization and spin glasses, and a Gaussian-like distribution that appearsin conjugate gradient method, deep network with MNIST input data and deepnetwork with random input data.
arxiv-14700-176 | Deep Neural Networks predict Hierarchical Spatio-temporal Cortical Dynamics of Human Visual Object Recognition | http://arxiv.org/pdf/1601.02970v1.pdf | author:Radoslaw M. Cichy, Aditya Khosla, Dimitrios Pantazis, Antonio Torralba, Aude Oliva category:cs.CV q-bio.NC published:2016-01-12 summary:The complex multi-stage architecture of cortical visual pathways provides theneural basis for efficient visual object recognition in humans. However, thestage-wise computations therein remain poorly understood. Here, we comparedtemporal (magnetoencephalography) and spatial (functional MRI) visual brainrepresentations with representations in an artificial deep neural network (DNN)tuned to the statistics of real-world visual recognition. We showed that theDNN captured the stages of human visual processing in both time and space fromearly visual areas towards the dorsal and ventral streams. Furtherinvestigation of crucial DNN parameters revealed that while model architecturewas important, training on real-world categorization was necessary to enforcespatio-temporal hierarchical relationships with the brain. Together our resultsprovide an algorithmically informed view on the spatio-temporal dynamics ofvisual object recognition in the human visual brain.
arxiv-14700-177 | Learning Subclass Representations for Visually-varied Image Classification | http://arxiv.org/pdf/1601.02913v1.pdf | author:Xinchao Li, Peng Xu, Yue Shi, Martha Larson, Alan Hanjalic category:cs.MM cs.CV published:2016-01-12 summary:In this paper, we present a subclass-representation approach that predictsthe probability of a social image belonging to one particular class. We explorethe co-occurrence of user-contributed tags to find subclasses with a strongconnection to the top level class. We then project each image on to theresulting subclass space to generate a subclass representation for the image.The novelty of the approach is that subclass representations make use of notonly the content of the photos themselves, but also information on theco-occurrence of their tags, which determines membership in both subclasses andtop-level classes. The novelty is also that the images are classified intosmaller classes, which have a chance of being more visually stable and easierto model. These subclasses are used as a latent space and images arerepresented in this space by their probability of relatedness to all of thesubclasses. In contrast to approaches directly modeling each top-level classbased on the image content, the proposed method can exploit more informationfor visually diverse classes. The approach is evaluated on a set of $2$ millionphotos with 10 classes, released by the Multimedia 2013 Yahoo! Large-scaleFlickr-tag Image Classification Grand Challenge. Experiments show that theproposed system delivers sound performance for visually diverse classescompared with methods that directly model top classes.
arxiv-14700-178 | Human Attention Estimation for Natural Images: An Automatic Gaze Refinement Approach | http://arxiv.org/pdf/1601.02852v1.pdf | author:Jinsoo Choi, Tae-Hyun Oh, In So Kweon category:cs.CV cs.HC cs.MM published:2016-01-12 summary:Photo collections and its applications today attempt to reflect userinteractions in various forms. Moreover, photo collections aim to capture theusers' intention with minimum effort through applications capturing userintentions. Human interest regions in an image carry powerful information aboutthe user's behavior and can be used in many photo applications. Research onhuman visual attention has been conducted in the form of gaze tracking andcomputational saliency models in the computer vision community, and has shownconsiderable progress. This paper presents an integration between implicit gazeestimation and computational saliency model to effectively estimate humanattention regions in images on the fly. Furthermore, our method estimates humanattention via implicit calibration and incremental model updating without anyactive participation from the user. We also present extensive analysis andpossible applications for personal photo collections.
arxiv-14700-179 | Weightless neural network parameters and architecture selection in a quantum computer | http://arxiv.org/pdf/1601.03277v1.pdf | author:Adenilton J. da Silva, Wilson R. de Oliveira, Teresa B. Ludermir category:quant-ph cs.NE published:2016-01-12 summary:Training artificial neural networks requires a tedious empirical evaluationto determine a suitable neural network architecture. To avoid this empiricalprocess several techniques have been proposed to automatise the architectureselection process. In this paper, we propose a method to perform parameter andarchitecture selection for a quantum weightless neural network (qWNN). Thearchitecture selection is performed through the learning procedure of a qWNNwith a learning algorithm that uses the principle of quantum superposition anda non-linear quantum operator. The main advantage of the proposed method isthat it performs a global search in the space of qWNN architecture andparameters rather than a local search.
arxiv-14700-180 | Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation | http://arxiv.org/pdf/1601.02828v1.pdf | author:Pawel Swietojanski, Jinyu Li, Steve Renals category:cs.CL cs.LG cs.SD published:2016-01-12 summary:This work presents a broad study on the adaptation of neural network acousticmodels by means of learning hidden unit contributions (LHUC) -- a method thatlinearly re-combines hidden units in a speaker- or environment-dependent mannerusing small amounts of unsupervised adaptation data. We also extend LHUC to aspeaker adaptive training (SAT) framework that leads to more adaptable DNNacoustic model, which can work in both a speaker-dependent and aspeaker-independent manner, without the requirement to maintain auxiliaryspeaker-dependent feature extractors or to introduce significantspeaker-dependent changes to the DNN structure. Through a series of experimentson four different speech recognition benchmarks (TED talks, Switchboard, AMImeetings and Aurora4) and over 270 test speakers we show that LHUC in both itstest-only and SAT variants results in consistent word error rate reductionsranging from 5% to 23% relative depending on the task and the degree ofmismatch between training and test data. In addition we have investigated theeffect of the amount of adaptation data per speaker, the quality of adaptationtargets when estimating transforms in an unsupervised manner, thecomplementarity to other adaptation techniques, one-shot adaptation, and anextension to adapting DNNs trained in a sequence discriminative manner.
arxiv-14700-181 | Symbol Grounding Association in Multimodal Sequences with Missing Elements | http://arxiv.org/pdf/1511.04401v3.pdf | author:Federico Raue, Thomas M. Breuel, Andreas Dengel, Marcus Liwicki category:cs.CV cs.CL cs.LG cs.NE published:2015-11-13 summary:In this paper, we extend a symbolic association framework to being able tohandle missing elements in multimodal sequences. The general scope of the workis the symbolic associations of object-word mappings as it happens in languagedevelopment on infants. This scenario has been long interested by ArtificialIntelligence, Psychology and Neuroscience. In this work, we extend a recentapproach for multimodal sequences (visual and audio) to also cope with missingelements in one or both modalities. Our approach uses two parallel LongShort-Term Memory (LSTM) networks with a learning rule based on EM-algorithm.It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose toinclude an extra step for the combination with max and mean operations forhandling missing elements in the sequences. The intuition behind is that thecombination acts as a condition selector for choosing the best representationfrom both LSTMs. We evaluated the proposed extension in three differentscenarios: audio sequences with missing elements, visual sequences with missingelements, and sequences with missing elements in both modalities. Theperformance of our extension reaches better results than the original model andsimilar results to a unique LSTM trained in one modality, i.e., where thelearning problem is less difficult.
arxiv-14700-182 | Learning the Correction for Multi-Path Deviations in Time-of-Flight Cameras | http://arxiv.org/pdf/1512.04077v2.pdf | author:Mojmir Mutny, Rahul Nair, Jens-Malte Gottfried category:cs.CV published:2015-12-13 summary:The Multipath effect in Time-of-Flight(ToF) cameras still remains to be achallenging problem that hinders further processing of 3D data information.Based on the evidence from previous literature, we explored the possibility ofusing machine learning techniques to correct this effect. Firstly, we createdtwo new datasets of of ToF images rendered via ToF simulator of LuxRender.These two datasets contain corners in multiple orientations and with differentmaterial properties. We chose scenes with corners as multipath effects are mostpronounced in corners. Secondly, we used this dataset to construct a learningmodel to predict real valued corrections to the ToF data using Random Forests.We found out that in our smaller dataset we were able to predict real valuedcorrection and improve the quality of depth images significantly by removingmultipath bias. With our algorithm, we improved relative per-pixel error fromaverage value of 19% to 3%. Additionally, variance of the error was lowered byan order of magnitude.
arxiv-14700-183 | Comparison and Adaptation of Automatic Evaluation Metrics for Quality Assessment of Re-Speaking | http://arxiv.org/pdf/1601.02789v1.pdf | author:Krzysztof Wołk, Danijel Koržinek category:cs.CL stat.AP stat.ML published:2016-01-12 summary:Re-speaking is a mechanism for obtaining high quality subtitles for use inlive broadcast and other public events. Because it relies on humans performingthe actual re-speaking, the task of estimating the quality of the results isnon-trivial. Most organisations rely on humans to perform the actual qualityassessment, but purely automatic methods have been developed for other similarproblems, like Machine Translation. This paper will try to compare several ofthese methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These willthen be matched to the human-derived NER metric, commonly used in re-speaking.
arxiv-14700-184 | Lock in Feedback in Sequential Experiments | http://arxiv.org/pdf/1502.00598v3.pdf | author:Maurits Kaptein, Davide Iannuzzi category:cs.LG published:2015-02-02 summary:We often encounter situations in which an experimenter wants to find, bysequential experimentation, $x_{max} = \arg\max_{x} f(x)$, where $f(x)$ is a(possibly unknown) function of a well controllable variable $x$. Takinginspiration from physics and engineering, we have designed a new method toaddress this problem. In this paper, we first introduce the method incontinuous time, and then present two algorithms for use in sequentialexperiments. Through a series of simulation studies, we show that the method iseffective for finding maxima of unknown functions by experimentation, even whenthe maximum of the functions drifts or when the signal to noise ratio is low.
arxiv-14700-185 | Robust Visual Tracking via Inverse Nonnegative Matrix Factorization | http://arxiv.org/pdf/1509.06003v3.pdf | author:Fanghui Liu, Tao Zhou, Keren Fu, Irene Y. H. Gu, Jie Yang category:cs.CV published:2015-09-20 summary:The establishment of robust target appearance model over time is anoverriding concern in visual tracking. In this paper, we propose an inversenonnegative matrix factorization (NMF) method for robust appearance modeling.Rather than using a linear combination of nonnegative basis matrices for eachtarget image patch in the conventional NMF, the proposed method is a reversethought to conventional NMF tracker. It utilizes both the foreground andbackground information, and imposes a local coordinate constraint, where thebasis matrix is sparse matrix from the linear combination of candidates withcorresponding nonnegative coefficient vectors. Inverse NMF is used as a featureencoder, where the resulting coefficient vectors are fed into a SVM classifierfor separating the target from the background. The proposed method is tested onseveral videos and compared with seven state-of-the-art methods. Our resultshave provided further support to the effectiveness and robustness of theproposed method.
arxiv-14700-186 | Robust Lineage Reconstruction from High-Dimensional Single-Cell Data | http://arxiv.org/pdf/1601.02748v1.pdf | author:Gregory Giecold, Eugenio Marco, Lorenzo Trippa, Guo-Cheng Yuan category:q-bio.QM stat.AP stat.CO stat.ML published:2016-01-12 summary:Single-cell gene expression data provide invaluable resources for systematiccharacterization of cellular hierarchy in multi-cellular organisms. However,cell lineage reconstruction is still often associated with significantuncertainty due to technological constraints. Such uncertainties have not beentaken into account in current methods. We present ECLAIR, a novel computationalmethod for the statistical inference of cell lineage relationships fromsingle-cell gene expression data. ECLAIR uses an ensemble approach to improvethe robustness of lineage predictions, and provides a quantitative estimate ofthe uncertainty of lineage branchings. We show that the application of ECLAIRto published datasets successfully reconstructs known lineage relationships andsignificantly improves the robustness of predictions. In conclusion, ECLAIR isa powerful bioinformatics tool for single-cell data analysis. It can be usedfor robust lineage reconstruction with quantitative estimate of predictionaccuracy.
arxiv-14700-187 | Reinforcement Learning Neural Turing Machines - Revised | http://arxiv.org/pdf/1505.00521v3.pdf | author:Wojciech Zaremba, Ilya Sutskever category:cs.LG published:2015-05-04 summary:The Neural Turing Machine (NTM) is more expressive than all previouslyconsidered models because of its external memory. It can be viewed as a broadereffort to use abstract external Interfaces and to learn a parametric model thatinteracts with them. The capabilities of a model can be extended by providing it with properInterfaces that interact with the world. These external Interfaces includememory, a database, a search engine, or a piece of software such as a theoremverifier. Some of these Interfaces are provided by the developers of the model.However, many important existing Interfaces, such as databases and searchengines, are discrete. We examine feasibility of learning models to interact with discreteInterfaces. We investigate the following discrete Interfaces: a memory Tape, aninput Tape, and an output Tape. We use a Reinforcement Learning algorithm totrain a neural network that interacts with such Interfaces to solve simplealgorithmic tasks. Our Interfaces are expressive enough to make our modelTuring complete.
arxiv-14700-188 | Deep Learning of Part-based Representation of Data Using Sparse Autoencoders with Nonnegativity Constraints | http://arxiv.org/pdf/1601.02733v1.pdf | author:Ehsan Hosseini-Asl, Jacek M. Zurada, Olfa Nasraoui category:cs.LG stat.ML published:2016-01-12 summary:We demonstrate a new deep learning autoencoder network, trained by anonnegativity constraint algorithm (NCAE), that learns features which showpart-based representation of data. The learning algorithm is based onconstraining negative weights. The performance of the algorithm is assessedbased on decomposing data into parts and its prediction performance is testedon three standard image data sets and one text dataset. The results indicatethat the nonnegativity constraint forces the autoencoder to learn features thatamount to a part-based representation of data, while improving sparsity andreconstruction quality in comparison with the traditional sparse autoencoderand Nonnegative Matrix Factorization. It is also shown that this newly acquiredrepresentation improves the prediction performance of a deep neural network.
arxiv-14700-189 | Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods | http://arxiv.org/pdf/1506.08473v3.pdf | author:Majid Janzamin, Hanie Sedghi, Anima Anandkumar category:cs.LG cs.NE stat.ML published:2015-06-28 summary:Training neural networks is a challenging non-convex optimization problem,and backpropagation or gradient descent can get stuck in spurious local optima.We propose a novel algorithm based on tensor decomposition for guaranteedtraining of two-layer neural networks. We provide risk bounds for our proposedmethod, with a polynomial sample complexity in the relevant parameters, such asinput dimension and number of neurons. While learning arbitrary targetfunctions is NP-hard, we provide transparent conditions on the function and theinput for learnability. Our training method is based on tensor decomposition,which provably converges to the global optimum, under a set of mildnon-degeneracy conditions. It consists of simple embarrassingly parallel linearand multi-linear operations, and is competitive with standard stochasticgradient descent (SGD), in terms of computational complexity. Thus, we proposea computationally efficient method with guaranteed risk bounds for trainingneural networks with one hidden layer.
arxiv-14700-190 | IRLS and Slime Mold: Equivalence and Convergence | http://arxiv.org/pdf/1601.02712v1.pdf | author:Damian Straszak, Nisheeth K. Vishnoi category:cs.DS cs.ET cs.IT math.IT math.NA math.OC stat.ML published:2016-01-12 summary:In this paper we present a connection between two dynamical systems arisingin entirely different contexts: one in signal processing and the other inbiology. The first is the famous Iteratively Reweighted Least Squares (IRLS)algorithm used in compressed sensing and sparse recovery while the second isthe dynamics of a slime mold (Physarum polycephalum). Both of these dynamicsare geared towards finding a minimum l1-norm solution in an affine subspace.Despite its simplicity the convergence of the IRLS method has been shown onlyfor a certain regularization of it and remains an important open problem. Ourfirst result shows that the two dynamics are projections of the same dynamicalsystem in higher dimensions. As a consequence, and building on the recent workon Physarum dynamics, we are able to prove convergence and obtain complexitybounds for a damped version of the IRLS algorithm.
arxiv-14700-191 | Personalized Course Sequence Recommendations | http://arxiv.org/pdf/1512.09176v2.pdf | author:Jie Xu, Tianwei Xing, Mihaela van der Schaar category:cs.CY cs.LG published:2015-12-30 summary:Given the variability in student learning it is becoming increasinglyimportant to tailor courses as well as course sequences to student needs. Thispaper presents a systematic methodology for offering personalized coursesequence recommendations to students. First, a forward-searchbackward-induction algorithm is developed that can optimally select coursesequences to decrease the time required for a student to graduate. Thealgorithm accounts for prerequisite requirements (typically present in higherlevel education) and course availability. Second, using the tools ofmulti-armed bandits, an algorithm is developed that can optimally recommend acourse sequence that both reduces the time to graduate while also increasingthe overall GPA of the student. The algorithm dynamically learns how studentswith different contextual backgrounds perform for given course sequences andthen recommends an optimal course sequence for new students. Using real-worldstudent data from the UCLA Mechanical and Aerospace Engineering department, weillustrate how the proposed algorithms outperform other methods that do notinclude student contextual information when making course sequencerecommendations.
arxiv-14700-192 | Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal Embedding | http://arxiv.org/pdf/1601.02705v1.pdf | author:Jaeyong Sung, Seok Hyun Jin, Ian Lenz, Ashutosh Saxena category:cs.RO cs.AI cs.LG published:2016-01-12 summary:There is a large variety of objects and appliances in human environments,such as stoves, coffee dispensers, juice extractors, and so on. It ischallenging for a roboticist to program a robot for each of these object typesand for each of their instantiations. In this work, we present a novel approachto manipulation planning based on the idea that many household objects sharesimilarly-operated object parts. We formulate the manipulation planning as astructured prediction problem and learn to transfer manipulation strategyacross different objects by embedding point-cloud, natural language, andmanipulation trajectory data into a shared embedding space using a deep neuralnetwork. In order to learn semantically meaningful spaces throughout ournetwork, we introduce a method for pre-training its lower layers for multimodalfeature embedding and a method for fine-tuning this embedding space using aloss-based margin. In order to collect a large number of manipulationdemonstrations for different objects, we develop a new crowd-sourcing platformcalled Robobarista. We test our model on our dataset consisting of 116 objectsand appliances with 249 parts along with 250 language instructions, for whichthere are 1225 crowd-sourced manipulation demonstrations. We further show thatour robot with our model can even prepare a cup of a latte with appliances ithas never seen before.
arxiv-14700-193 | Environmental Noise Embeddings for Robust Speech Recognition | http://arxiv.org/pdf/1601.02553v1.pdf | author:Suyoun Kim, Bhiksha Raj, Ian Lane category:cs.CL published:2016-01-11 summary:We propose a novel deep neural network architecture for speech recognitionthat explicitly employs knowledge of the background environmental noise withina deep neural network acoustic model. A deep neural network is used to predictthe acoustic environment in which the system in being used. The discriminativeembedding generated at the bottleneck layer of this network is thenconcatenated with traditional acoustic features as input to a deep neuralnetwork acoustic model. Using simulated acoustic environments we show that theproposed approach significantly improves speech recognition accuracy in noisyand highly reverberant environments, outperforming multi-condition training andmulti-task learning for this task.
arxiv-14700-194 | Evaluating the Performance of a Speech Recognition based System | http://arxiv.org/pdf/1601.02543v1.pdf | author:Vinod Kumar Pandey, Sunil Kumar Kopparapu category:cs.CL cs.AI cs.HC published:2016-01-11 summary:Speech based solutions have taken center stage with growth in the servicesindustry where there is a need to cater to a very large number of people fromall strata of the society. While natural language speech interfaces are thetalk in the research community, yet in practice, menu based speech solutionsthrive. Typically in a menu based speech solution the user is required torespond by speaking from a closed set of words when prompted by the system. Asequence of human speech response to the IVR prompts results in the completionof a transaction. A transaction is deemed successful if the speech solution cancorrectly recognize all the spoken utterances of the user whenever prompted bythe system. The usual mechanism to evaluate the performance of a speechsolution is to do an extensive test of the system by putting it to actualpeople use and then evaluating the performance by analyzing the logs forsuccessful transactions. This kind of evaluation could lead to dissatisfiedtest users especially if the performance of the system were to result in a poortransaction completion rate. To negate this the Wizard of Oz approach isadopted during evaluation of a speech system. Overall this kind of evaluationsis an expensive proposition both in terms of time and cost. In this paper, wepropose a method to evaluate the performance of a speech solution withoutactually putting it to people use. We first describe the methodology and thenshow experimentally that this can be used to identify the performancebottlenecks of the speech solution even before the system is actually used thussaving evaluation time and expenses.
arxiv-14700-195 | Investigating gated recurrent neural networks for speech synthesis | http://arxiv.org/pdf/1601.02539v1.pdf | author:Zhizheng Wu, Simon King category:cs.CL cs.NE published:2016-01-11 summary:Recently, recurrent neural networks (RNNs) as powerful sequence models havere-emerged as a potential acoustic model for statistical parametric speechsynthesis (SPSS). The long short-term memory (LSTM) architecture isparticularly attractive because it addresses the vanishing gradient problem instandard RNNs, making them easier to train. Although recent studies havedemonstrated that LSTMs can achieve significantly better performance on SPSSthan deep feed-forward neural networks, little is known about why. Here weattempt to answer two questions: a) why do LSTMs work well as a sequence modelfor SPSS; b) which component (e.g., input gate, output gate, forget gate) ismost important. We present a visual analysis alongside a series of experiments,resulting in a proposal for a simplified architecture. The simplifiedarchitecture has significantly fewer parameters than an LSTM, thus reducinggeneration complexity considerably without degrading quality.
arxiv-14700-196 | Optimal Copula Transport for Clustering Multivariate Time Series | http://arxiv.org/pdf/1509.08144v2.pdf | author:Gautier Marti, Frank Nielsen, Philippe Donnat category:cs.LG stat.ML published:2015-09-27 summary:This paper presents a new methodology for clustering multivariate time seriesleveraging optimal transport between copulas. Copulas are used to encode both(i) intra-dependence of a multivariate time series, and (ii) inter-dependencebetween two time series. Then, optimal copula transport allows us to define twodistances between multivariate time series: (i) one for measuringintra-dependence dissimilarity, (ii) another one for measuring inter-dependencedissimilarity based on a new multivariate dependence coefficient which isrobust to noise, deterministic, and which can target specified dependencies.
arxiv-14700-197 | Client Profiling for an Anti-Money Laundering System | http://arxiv.org/pdf/1510.00878v2.pdf | author:Claudio Alexandre, João Balsa category:cs.LG cs.AI stat.ML published:2015-10-03 summary:We present a data mining approach for profiling bank clients in order tosupport the process of detection of anti-money laundering operations. We firstpresent the overall system architecture, and then focus on the relevantcomponent for this paper. We detail the experiments performed on real worlddata from a financial institution, which allowed us to group clients inclusters and then generate a set of classification rules. We discuss therelevance of the founded client profiles and of the generated classificationrules. According to the defined overall agent-based architecture, these ruleswill be incorporated in the knowledge base of the intelligent agentsresponsible for the signaling of suspicious transactions.
arxiv-14700-198 | How to learn a graph from smooth signals | http://arxiv.org/pdf/1601.02513v1.pdf | author:Vassilis Kalofolias category:stat.ML cs.LG published:2016-01-11 summary:We propose a framework that learns the graph structure underlying a set ofsmooth signals. Given $X\in\mathbb{R}^{m\times n}$ whose rows reside on thevertices of an unknown graph, we learn the edge weights$w\in\mathbb{R}_+^{m(m-1)/2}$ under the smoothness assumption that$\text{tr}{X^\top LX}$ is small. We show that the problem is a weighted$\ell$-1 minimization that leads to naturally sparse solutions. We point outhow known graph learning or construction techniques fall within our frameworkand propose a new model that performs better than the state of the art in manysettings. We present efficient, scalable primal-dual based algorithms for bothour model and the previous state of the art, and evaluate their performance onartificial and real data.
arxiv-14700-199 | Trans-gram, Fast Cross-lingual Word-embeddings | http://arxiv.org/pdf/1601.02502v1.pdf | author:Jocelyn Coulmance, Jean-Marc Marty, Guillaume Wenzek, Amine Benhalloum category:cs.CL published:2016-01-11 summary:We introduce Trans-gram, a simple and computationally-efficient method tosimultaneously learn and align wordembeddings for a variety of languages, usingonly monolingual data and a smaller set of sentence-aligned data. We use ournew method to compute aligned wordembeddings for twenty-one languages usingEnglish as a pivot language. We show that some linguistic features are alignedacross languages for which we do not have aligned data, even though thoseproperties do not exist in the pivot language. We also achieve state of the artresults on standard cross-lingual text classification and word translationtasks.
arxiv-14700-200 | Facial Expression Recognition in the Wild using Rich Deep Features | http://arxiv.org/pdf/1601.02487v1.pdf | author:Abubakrelsedik Karali, Ahmad Bassiouny, Motaz El-Saban category:cs.CV published:2016-01-11 summary:Facial Expression Recognition is an active area of research in computervision with a wide range of applications. Several approaches have beendeveloped to solve this problem for different benchmark datasets. However,Facial Expression Recognition in the wild remains an area where much work isstill needed to serve real-world applications. To this end, in this paper wepresent a novel approach towards facial expression recognition. We fuse richdeep features with domain knowledge through encoding discriminant facialpatches. We conduct experiments on two of the most popular benchmark datasets;CK and TFE. Moreover, we present a novel dataset that, unlike its precedents,consists of natural - not acted - expression images. Experimental results showthat our approach achieves state-of-the-art results over standard benchmarksand our own dataset
arxiv-14700-201 | ReSeg: A Recurrent Neural Network for Object Segmentation | http://arxiv.org/pdf/1511.07053v2.pdf | author:Francesco Visin, Kyle Kastner, Aaron Courville, Yoshua Bengio, Matteo Matteucci, Kyunghyun Cho category:cs.CV cs.LG published:2015-11-22 summary:We propose a structured prediction architecture for images centered arounddeep recurrent neural networks. The proposed network, called ReSeg, is based onthe recently introduced ReNet model for object classification. We modify andextend it to perform object segmentation, noting that the avoidance of poolingcan greatly simplify pixel-wise tasks for images. The ReSeg layer is composedof four recurrent neural networks that sweep the image horizontally andvertically in both directions, along with a final layer that expands theprediction back to the original image size. ReSeg combines multiple ReSeglayers with several possible input layers as well as a final layer whichexpands the prediction back to the original image size, making it suitable fora variety of structured prediction tasks. We evaluate ReSeg on the specifictask of object segmentation with three widely-used image segmentation datasets,namely Weizmann Horse, Fashionista and Oxford Flower. The results suggest thatReSeg can challenge the state of the art in object segmentation, and may havefurther applications in structured prediction at large.
arxiv-14700-202 | The Effects of Age, Gender and Region on Non-standard Linguistic Variation in Online Social Networks | http://arxiv.org/pdf/1601.02431v1.pdf | author:Claudia Peersman, Walter Daelemans, Reinhild Vandekerckhove, Bram Vandekerckhove, Leona Van Vaerenbergh category:cs.CL published:2016-01-11 summary:We present a corpus-based analysis of the effects of age, gender and regionof origin on the production of both "netspeak" or "chatspeak" features andregional speech features in Flemish Dutch posts that were collected from aBelgian online social network platform. The present study shows that combiningquantitative and qualitative approaches is essential for understandingnon-standard linguistic variation in a CMC corpus. It also presents amethodology that enables the systematic study of this variation by includingall non-standard words in the corpus. The analyses resulted in a convincingillustration of the Adolescent Peak Principle. In addition, our approachrevealed an intriguing correlation between the use of regional speech featuresand chatspeak features.
arxiv-14700-203 | Somoclu: An Efficient Parallel Library for Self-Organizing Maps | http://arxiv.org/pdf/1305.1422v3.pdf | author:Peter Wittek, Shi Chao Gao, Ik Soo Lim, Li Zhao category:cs.DC cs.MS cs.NE published:2013-05-07 summary:Somoclu is a massively parallel tool for training self-organizing maps onlarge data sets written in C++. It builds on OpenMP for multicore execution,and on MPI for distributing the workload across the nodes in a cluster. It isalso able to boost training by using CUDA if graphics processing units areavailable. A sparse kernel is included, which is useful for high-dimensionalbut sparse data, such as the vector spaces common in text mining workflows.Python, R and MATLAB interfaces facilitate interactive use. Apart from fastexecution, memory use is highly optimized, enabling training large emergentmaps even on a single computer.
arxiv-14700-204 | A Parameter-free Affinity Based Clustering | http://arxiv.org/pdf/1507.05409v2.pdf | author:Bhaskar Mukhoty, Ruchir Gupta, Y. N. Singh category:cs.CV published:2015-07-20 summary:Several methods have been proposed to estimate the number of clusters in adataset; the basic ideal behind all of them has been to study an index thatmeasures inter-cluster separation and intra-cluster cohesion over a range ofcluster numbers and report the number which gives an optimum value of theindex. In this paper we propose a simple, parameter free approach that is likehuman cognition to form clusters, where closely lying points are easilyidentified to form a cluster and total number of clusters are revealed. Toidentify closely lying points, affinity of two points is defined as a functionof distance and a threshold affinity is identified, above which two points in adataset are likely to be in the same cluster. Well separated clusters areidentified even in the presence of outliers, whereas for not so well separateddataset, final number of clusters are estimated and the detected clusters aremerged to produce the final clusters. Experiments performed with several largedimensional synthetic and real datasets show good results with robustness tonoise and density variation within dataset.
arxiv-14700-205 | Implicit Look-alike Modelling in Display Ads: Transfer Collaborative Filtering to CTR Estimation | http://arxiv.org/pdf/1601.02377v1.pdf | author:Weinan Zhang, Lingxi Chen, Jun Wang category:cs.LG cs.IR published:2016-01-11 summary:User behaviour targeting is essential in online advertising. Compared withsponsored search keyword targeting and contextual advertising page contenttargeting, user behaviour targeting builds users' interest profiles viatracking their online behaviour and then delivers the relevant ads according toeach user's interest, which leads to higher targeting accuracy and thus moreimproved advertising performance. The current user profiling methods includebuilding keywords and topic tags or mapping users onto a hierarchical taxonomy.However, to our knowledge, there is no previous work that explicitlyinvestigates the user online visits similarity and incorporates such similarityinto their ad response prediction. In this work, we propose a general frameworkwhich learns the user profiles based on their online browsing behaviour, andtransfers the learned knowledge onto prediction of their ad response.Technically, we propose a transfer learning model based on the probabilisticlatent factor graphic models, where the users' ad response profiles aregenerated from their online browsing profiles. The large-scale experimentsbased on real-world data demonstrate significant improvement of our solutionover some strong baselines.
arxiv-14700-206 | Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction | http://arxiv.org/pdf/1601.02376v1.pdf | author:Weinan Zhang, Tianming Du, Jun Wang category:cs.LG cs.IR published:2016-01-11 summary:Predicting user responses, such as click-through rate and conversion rate,are critical in many web applications including web search, personalisedrecommendation, and online advertising. Different from continuous raw featuresthat we usually found in the image and audio domains, the input features in webspace are always of multi-field and are mostly discrete and categorical whiletheir dependencies are little known. Major user response prediction models haveto either limit themselves to linear models or require manually building uphigh-order combination features. The former loses the ability of exploringfeature interactions, while the latter results in a heavy computation in thelarge feature space. To tackle the issue, we propose two novel models usingdeep neural networks (DNNs) to automatically learn effective patterns fromcategorical feature interactions and make predictions of users' ad clicks. Toget our DNNs efficiently work, we propose to leverage three featuretransformation methods, i.e., factorisation machines (FMs), restrictedBoltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paperpresents the structure of our models and their efficient training algorithms.The large-scale experiments with real-world data demonstrate that our methodswork better than major state-of-the-art models.
arxiv-14700-207 | Highway Long Short-Term Memory RNNs for Distant Speech Recognition | http://arxiv.org/pdf/1510.08983v2.pdf | author:Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yao, Sanjeev Khudanpur, James Glass category:cs.NE cs.AI cs.CL cs.LG published:2015-10-30 summary:In this paper, we extend the deep long short-term memory (DLSTM) recurrentneural networks by introducing gated direct connections between memory cells inadjacent layers. These direct links, called highway connections, enableunimpeded information flow across different layers and thus alleviate thegradient vanishing problem when building deeper LSTMs. We further introduce thelatency-controlled bidirectional LSTMs (BLSTMs) which can exploit the wholehistory while keeping the latency under control. Efficient algorithms areproposed to train these novel networks using both frame and sequencediscriminative criteria. Experiments on the AMI distant speech recognition(DSR) task indicate that we can train deeper LSTMs and achieve betterimprovement from sequence training with highway LSTMs (HLSTMs). Our novel modelobtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming allprevious works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and$5.3\%$ relative improvement respectively.
arxiv-14700-208 | Importance Weighted Autoencoders | http://arxiv.org/pdf/1509.00519v3.pdf | author:Yuri Burda, Roger Grosse, Ruslan Salakhutdinov category:cs.LG stat.ML published:2015-09-01 summary:The variational autoencoder (VAE; Kingma, Welling (2014)) is a recentlyproposed generative model pairing a top-down generative network with abottom-up recognition network which approximates posterior inference. Ittypically makes strong assumptions about posterior inference, for instance thatthe posterior distribution is approximately factorial, and that its parameterscan be approximated with nonlinear regression from the observations. As we showempirically, the VAE objective can lead to overly simplified representationswhich fail to use the network's entire modeling capacity. We present theimportance weighted autoencoder (IWAE), a generative model with the samearchitecture as the VAE, but which uses a strictly tighter log-likelihood lowerbound derived from importance weighting. In the IWAE, the recognition networkuses multiple samples to approximate the posterior, giving it increasedflexibility to model complex posteriors which do not fit the VAE modelingassumptions. We show empirically that IWAEs learn richer latent spacerepresentations than VAEs, leading to improved test log-likelihood on densityestimation benchmarks.
arxiv-14700-209 | Measuring Sample Quality with Stein's Method | http://arxiv.org/pdf/1506.03039v4.pdf | author:Jackson Gorham, Lester Mackey category:stat.ML cs.LG math.PR stat.ME published:2015-06-09 summary:To improve the efficiency of Monte Carlo estimation, practitioners areturning to biased Markov chain Monte Carlo procedures that trade off asymptoticexactness for computational speed. The reasoning is sound: a reduction invariance due to more rapid sampling can outweigh the bias introduced. However,the inexactness creates new challenges for sampler and parameter selection,since standard measures of sample quality like effective sample size do notaccount for asymptotic bias. To address these challenges, we introduce a newcomputable quality measure based on Stein's method that quantifies the maximumdiscrepancy between sample and target expectations over a large class of testfunctions. We use our tool to compare exact, biased, and deterministic samplesequences and illustrate applications to hyperparameter selection, convergencerate assessment, and quantifying bias-variance tradeoffs in posteriorinference.
arxiv-14700-210 | Temporal Multinomial Mixture for Instance-Oriented Evolutionary Clustering | http://arxiv.org/pdf/1601.02300v1.pdf | author:Young-Min Kim, Julien Velcin, Stéphane Bonnevay, Marian-Andrei Rizoiu category:cs.IR cs.LG stat.ML published:2016-01-11 summary:Evolutionary clustering aims at capturing the temporal evolution of clusters.This issue is particularly important in the context of social media data thatare naturally temporally driven. In this paper, we propose a new probabilisticmodel-based evolutionary clustering technique. The Temporal Multinomial Mixture(TMM) is an extension of classical mixture model that optimizes featureco-occurrences in the trade-off with temporal smoothness. Our model isevaluated for two recent case studies on opinion aggregation over time. Wecompare four different probabilistic clustering models and we show thesuperiority of our proposal in the task of instance-oriented clustering.
arxiv-14700-211 | How to Use Temporal-Driven Constrained Clustering to Detect Typical Evolutions | http://arxiv.org/pdf/1601.02603v1.pdf | author:Marian-Andrei Rizoiu, Julien Velcin, Stéphane Lallich category:cs.LG cs.DS published:2016-01-11 summary:In this paper, we propose a new time-aware dissimilarity measure that takesinto account the temporal dimension. Observations that are close in thedescription space, but distant in time are considered as dissimilar. We alsopropose a method to enforce the segmentation contiguity, by introducing, in theobjective function, a penalty term inspired from the Normal DistributionFunction. We combine the two propositions into a novel time-driven constrainedclustering algorithm, called TDCK-Means, which creates a partition of coherentclusters, both in the multidimensional space and in the temporal space. Thisalgorithm uses soft semi-supervised constraints, to encourage adjacentobservations belonging to the same entity to be assigned to the same cluster.We apply our algorithm to a Political Studies dataset in order to detecttypical evolution phases. We adapt the Shannon entropy in order to measure theentity contiguity, and we show that our proposition consistently improvestemporal cohesion of clusters, without any significant loss in themultidimensional variance.
arxiv-14700-212 | Improving performance of recurrent neural network with relu nonlinearity | http://arxiv.org/pdf/1511.03771v2.pdf | author:Sachin S. Talathi, Aniket Vartak category:cs.NE cs.LG published:2015-11-12 summary:In recent years significant progress has been made in successfully trainingrecurrent neural networks (RNNs) on sequence learning problems involving longrange temporal dependencies. The progress has been made on three fronts: (a)Algorithmic improvements involving sophisticated optimization techniques, (b)network design involving complex hidden layer nodes and specialized recurrentlayer connections and (c) weight initialization methods. In this paper, wefocus on recently proposed weight initialization with identity matrix for therecurrent weights in a RNN. This initialization is specifically proposed forhidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simpledynamical systems perspective on weight initialization process, which allows usto propose a modified weight initialization strategy. We show that thisinitialization technique leads to successfully training RNNs composed of ReLUs.We demonstrate that our proposal produces comparable or better solution forthree toy problems involving long range temporal structure: the additionproblem, the multiplication problem and the MNIST classification problem usingsequence of pixels. In addition, we present results for a benchmark actionrecognition problem.
arxiv-14700-213 | Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients | http://arxiv.org/pdf/1504.03655v4.pdf | author:Bo Xie, Yingyu Liang, Le Song category:cs.LG published:2015-04-14 summary:Nonlinear component analysis such as kernel Principle Component Analysis(KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used inmachine learning, statistics and data analysis, but they can not scale up tobig datasets. Recent attempts have employed random feature approximations toconvert the problem to the primal form for linear computational complexity.However, to obtain high quality solutions, the number of random features shouldbe the same order of magnitude as the number of data points, making suchapproach not directly applicable to the regime with millions of data points. We propose a simple, computationally efficient, and memory friendly algorithmbased on the "doubly stochastic gradients" to scale up a range of kernelnonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the\emph{non-convex} nature of these problems, our method enjoys theoreticalguarantees that it converges at the rate $\tilde{O}(1/t)$ to the globaloptimum, even for the top $k$ eigen subspace. Unlike many alternatives, ouralgorithm does not require explicit orthogonalization, which is infeasible onbig datasets. We demonstrate the effectiveness and scalability of our algorithmon large scale synthetic and real world datasets.
arxiv-14700-214 | Directional Decision Lists | http://arxiv.org/pdf/1508.07643v3.pdf | author:Marc Goessling, Shan Kang category:stat.ML cs.LG stat.CO published:2015-08-30 summary:In this paper we introduce a novel family of decision lists consisting ofhighly interpretable models which can be learned efficiently in a greedymanner. The defining property is that all rules are oriented in the samedirection. Particular examples of this family are decision lists withmonotonically decreasing (or increasing) probabilities. On simulated data weempirically confirm that the proposed model family is easier to train thangeneral decision lists. We exemplify the practical usability of our approach byidentifying problem symptoms in a manufacturing process.
arxiv-14700-215 | A Sufficient Statistics Construction of Bayesian Nonparametric Exponential Family Conjugate Models | http://arxiv.org/pdf/1601.02257v1.pdf | author:Robert Finn, Brian Kulis category:cs.LG stat.ML published:2016-01-10 summary:Conjugate pairs of distributions over infinite dimensional spaces areprominent in statistical learning theory, particularly due to the widespreadadoption of Bayesian nonparametric methodologies for a host of models andapplications. Much of the existing literature in the learning community focuseson processes possessing some form of computationally tractable conjugacy as isthe case for the beta and gamma processes (and, via normalization, theDirichlet process). For these processes, proofs of conjugacy and requisitederivation of explicit computational formulae for posterior density parametersare idiosyncratic to the stochastic process in question. As such, BayesianNonparametric models are currently available for a limited number of conjugatepairs, e.g. the Dirichlet-multinomial and beta-Bernoulli process pairs. In eachof these above cases the likelihood process belongs to the class of discreteexponential family distributions. The exclusion of continuous likelihooddistributions from the known cases of Bayesian Nonparametric Conjugate modelsstands as a disparity in the researcher's toolbox. In this paper we first address the problem of obtaining a generalconstruction of prior distributions over infinite dimensional spaces possessingdistributional properties amenable to conjugacy. Second, we bridge the dividebetween the discrete and continuous likelihoods by illustrating a canonicalconstruction for stochastic processes whose Levy measure densities are frompositive exponential families, and then demonstrate that these processes infact form the prior, likelihood, and posterior in a conjugate family. Ourcanonical construction subsumes known computational formulae for posteriordensity parameters in the cases where the likelihood is from a discretedistribution belonging to an exponential family.
arxiv-14700-216 | Bayesian Optimization in a Billion Dimensions via Random Embeddings | http://arxiv.org/pdf/1301.1942v2.pdf | author:Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, Nando de Freitas category:stat.ML cs.LG published:2013-01-09 summary:Bayesian optimization techniques have been successfully applied to robotics,planning, sensor placement, recommendation, advertising, intelligent userinterfaces and automatic algorithm configuration. Despite these successes, theapproach is restricted to problems of moderate dimension, and several workshopson Bayesian optimization have identified its scaling to high-dimensions as oneof the holy grails of the field. In this paper, we introduce a novel randomembedding idea to attack this problem. The resulting Random EMbedding BayesianOptimization (REMBO) algorithm is very simple, has important invarianceproperties, and applies to domains with both categorical and continuousvariables. We present a thorough theoretical analysis of REMBO. Empiricalresults confirm that REMBO can effectively solve problems with billions ofdimensions, provided the intrinsic dimensionality is low. They also show thatREMBO achieves state-of-the-art performance in optimizing the 47 discreteparameters of a popular mixed integer linear programming solver.
arxiv-14700-217 | Parallel Stroked Multi Line: a model-based method for compressing large fingerprint databases | http://arxiv.org/pdf/1601.02225v1.pdf | author:Hamid Mansouri, Hamid-Reza Pourreza category:cs.CV cs.DS published:2016-01-10 summary:With increasing usage of fingerprints as an important biometric data, theneed to compress the large fingerprint databases has become essential. The mostrecommended compression algorithm, even by standards, is JPEG2K. But at highcompression rates, this algorithm is ineffective. In this paper, a model isproposed which is based on parallel lines with same orientations, arbitrarywidths and same gray level values located on rectangle with constant gray levelvalue as background. We refer to this algorithm as Parallel Stroked Multi Line(PSML). By using Adaptive Geometrical Wavelet and employing PSML, a compressionalgorithm is developed. This compression algorithm can preserve fingerprintstructure and minutiae. The exact algorithm of computing the PSML model takeexponential time. However, we have proposed an alternative approximationalgorithm, which reduces the time complexity to $O(n^3)$. The proposed PSMLalg. has significant advantage over Wedgelets Transform in PSNR value andvisual quality in compressed images. The proposed method, despite the lowerPSNR values than JPEG2K algorithm in common range of compression rates, in allcompression rates have nearly equal or greater advantage over JPEG2K when usedby Automatic Fingerprint Identification Systems (AFIS). At high compressionrates, according to PSNR values, mean EER rate and visual quality, the encodedimages with JPEG2K can not be identified from each other after compression.But, images encoded by the PSML alg. retained the sufficient information tomaintain fingerprint identification performances similar to the ones obtainedby raw images without compression. One the U.are.U 400 database, the mean EERrate for uncompressed images is 4.54%, while at 267:1 compression ratio, thisvalue becomes 49.41% and 6.22% for JPEG2K and PSML, respectively. This resultshows a significant improvement over the standard JPEG2K algorithm.
arxiv-14700-218 | Joint Object-Material Category Segmentation from Audio-Visual Cues | http://arxiv.org/pdf/1601.02220v1.pdf | author:Anurag Arnab, Michael Sapienza, Stuart Golodetz, Julien Valentin, Ondrej Miksik, Shahram Izadi, Philip Torr category:cs.CV cs.SD published:2016-01-10 summary:It is not always possible to recognise objects and infer material propertiesfor a scene from visual cues alone, since objects can look visually similarwhilst being made of very different materials. In this paper, we thereforepresent an approach that augments the available dense visual cues with sparseauditory cues in order to estimate dense object and material labels. Sinceestimates of object class and material properties are mutually informative, weoptimise our multi-output labelling jointly using a random-field framework. Weevaluate our system on a new dataset with paired visual and auditory data thatwe make publicly available. We demonstrate that this joint estimation of objectand material labels significantly outperforms the estimation of either categoryin isolation.
arxiv-14700-219 | Where To Look: Focus Regions for Visual Question Answering | http://arxiv.org/pdf/1511.07394v2.pdf | author:Kevin J. Shih, Saurabh Singh, Derek Hoiem category:cs.CV published:2015-11-23 summary:We present a method that learns to answer visual questions by selecting imageregions relevant to the text-based query. Our method exhibits significantimprovements in answering questions such as "what color," where it is necessaryto evaluate a specific location, and "what room," where it selectivelyidentifies informative image regions. Our model is tested on the VQA datasetwhich is the largest human-annotated visual question answering dataset to ourknowledge.
arxiv-14700-220 | On Clustering Time Series Using Euclidean Distance and Pearson Correlation | http://arxiv.org/pdf/1601.02213v1.pdf | author:Michael R. Berthold, Frank Höppner category:cs.LG cs.AI stat.ML published:2016-01-10 summary:For time series comparisons, it has often been observed that z-scorenormalized Euclidean distances far outperform the unnormalized variant. In thispaper we show that a z-score normalized, squared Euclidean Distance is, infact, equal to a distance based on Pearson Correlation. This has profoundimpact on many distance-based classification or clustering methods. In additionto this theoretically sound result we also show that the often used k-Meansalgorithm formally needs a mod ification to keep the interpretation as Pearsoncorrelation strictly valid. Experimental results demonstrate that in many casesthe standard k-Means algorithm generally produces the same results.
arxiv-14700-221 | Empirical Gaussian priors for cross-lingual transfer learning | http://arxiv.org/pdf/1601.02166v1.pdf | author:Anders Søgaard category:cs.CL published:2016-01-09 summary:Sequence model learning algorithms typically maximize log-likelihood minusthe norm of the model (or minimize Hamming loss + norm). In cross-lingualpart-of-speech (POS) tagging, our target language training data consists ofsequences of sentences with word-by-word labels projected from translations in$k$ languages for which we have labeled data, via word alignments. Our trainingdata is therefore very noisy, and if Rademacher complexity is high, learningalgorithms are prone to overfit. Norm-based regularization assumes a constantwidth and zero mean prior. We instead propose to use the $k$ source languagemodels to estimate the parameters of a Gaussian prior for learning new POStaggers. This leads to significantly better performance in multi-sourcetransfer set-ups. We also present a drop-out version that injects (empirical)Gaussian noise during online learning. Finally, we note that using empiricalGaussian priors leads to much lower Rademacher complexity, and is superior tooptimally weighted model interpolation.
arxiv-14700-222 | Kernelized LRR on Grassmann Manifolds for Subspace Clustering | http://arxiv.org/pdf/1601.02124v1.pdf | author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV published:2016-01-09 summary:Low rank representation (LRR) has recently attracted great interest due toits pleasing efficacy in exploring low-dimensional sub- space structuresembedded in data. One of its successful applications is subspace clustering, bywhich data are clustered according to the subspaces they belong to. In thispaper, at a higher level, we intend to cluster subspaces into classes ofsubspaces. This is naturally described as a clustering problem on Grassmannmanifold. The novelty of this paper is to generalize LRR on Euclidean spaceonto an LRR model on Grassmann manifold in a uniform kernelized LRR framework.The new method has many applications in data analysis in computer vision tasks.The proposed models have been evaluated on a number of practical data analysisapplications. The experimental results show that the proposed models outperforma number of state-of-the-art subspace clustering methods.
arxiv-14700-223 | Supervised multiview learning based on simultaneous learning of multiview intact and single view classifier | http://arxiv.org/pdf/1601.02098v1.pdf | author:Qingjun Wang, Haiyan Lv, Jun Yue, Eugene Mitchell category:cs.CV published:2016-01-09 summary:Multiview learning problem refers to the problem of learning a classifierfrom multiple view data. In this data set, each data points is presented bymultiple different views. In this paper, we propose a novel method for thisproblem. This method is based on two assumptions. The first assumption is thateach data point has an intact feature vector, and each view is obtained by alinear transformation from the intact vector. The second assumption is that theintact vectors are discriminative, and in the intact space, we have a linearclassifier to separate the positive class from the negative class. We define anintact vector for each data point, and a view-conditional transformation matrixfor each view, and propose to reconstruct the multiple view feature vectors bythe product of the corresponding intact vectors and transformation matrices.Moreover, we also propose a linear classifier in the intact space, and learn itjointly with the intact vectors. The learning problem is modeled by aminimization problem, and the objective function is composed of a Cauchy errorestimator-based view-conditional reconstruction term over all data points andviews, and a classification error term measured by hinge loss over all theintact vectors of all the data points. Some regularization terms are alsoimposed to different variables in the objective function. The minimizationproblem is solve by an iterative algorithm using alternate optimizationstrategy and gradient descent algorithm. The proposed algorithm shows itadvantage in the compression to other multiview learning algorithms onbenchmark data sets.
arxiv-14700-224 | Multicuts and Perturb & MAP for Probabilistic Graph Clustering | http://arxiv.org/pdf/1601.02088v1.pdf | author:Jörg Hendrik Kappes, Paul Swoboda, Bogdan Savchynskyy, Tamir Hazan, Christoph Schnörr category:cs.CV published:2016-01-09 summary:We present a probabilistic graphical model formulation for the graphclustering problem. This enables to locally represent uncertainty of imagepartitions by approximate marginal distributions in a mathematicallysubstantiated way, and to rectify local data term cues so as to close contoursand to obtain valid partitions. We exploit recent progress on globally optimal MAP inference by integerprogramming and on perturbation-based approximations of the log-partitionfunction, in order to sample clusterings and to estimate marginal distributionsof node-pairs both more accurately and more efficiently than state-of-the-artmethods. Our approach works for any graphically represented problem instance.This is demonstrated for image segmentation and social network clusteranalysis. Our mathematical ansatz should be relevant also for othercombinatorial problems.
arxiv-14700-225 | What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment | http://arxiv.org/pdf/1509.00838v2.pdf | author:Hongyuan Mei, Mohit Bansal, Matthew R. Walter category:cs.CL cs.AI cs.LG cs.NE published:2015-09-02 summary:We propose an end-to-end, domain-independent neural encoder-aligner-decodermodel for selective generation, i.e., the joint task of content selection andsurface realization. Our model first encodes a full set of over-determineddatabase event records via an LSTM-based recurrent neural network, thenutilizes a novel coarse-to-fine aligner to identify the small subset of salientrecords to talk about, and finally employs a decoder to generate free-formdescriptions of the aligned, selected records. Our model achieves the bestselection and generation results reported to-date (with 59% relativeimprovement in generation) on the benchmark WeatherGov dataset, despite usingno specialized features or linguistic resources. Using an improved k-nearestneighbor beam filter helps further. We also perform a series of ablations andvisualizations to elucidate the contributions of our key model components.Lastly, we evaluate the generalizability of our model on the RoboCup dataset,and get results that are competitive with or better than the state-of-the-art,despite being severely data-starved.
arxiv-14700-226 | Convolutional Neural Network for Stereotypical Motor Movement Detection in Autism | http://arxiv.org/pdf/1511.01865v2.pdf | author:Nastaran Mohammadian Rad, Andrea Bizzego, Seyed Mostafa Kia, Giuseppe Jurman, Paola Venuti, Cesare Furlanello category:cs.NE cs.CV cs.LG published:2015-11-05 summary:Autism Spectrum Disorders (ASDs) are often associated with specific atypicalpostural or motor behaviors, of which Stereotypical Motor Movements (SMMs) havea specific visibility. While the identification and the quantification of SMMpatterns remain complex, its automation would provide support to accuratetuning of the intervention in the therapy of autism. Therefore, it is essentialto develop automatic SMM detection systems in a real world setting, taking careof strong inter-subject and intra-subject variability. Wireless accelerometersensing technology can provide a valid infrastructure for real-time SMMdetection, however such variability remains a problem also for machine learningmethods, in particular whenever handcrafted features extracted fromaccelerometer signal are considered. Here, we propose to employ the deeplearning paradigm in order to learn discriminating features from multi-sensoraccelerometer signals. Our results provide preliminary evidence that featurelearning and transfer learning embedded in the deep architecture achieve higheraccurate SMM detectors in longitudinal scenarios.
arxiv-14700-227 | Lifting GIS Maps into Strong Geometric Context for Scene Understanding | http://arxiv.org/pdf/1507.03698v4.pdf | author:Raúl Díaz, Minhaeng Lee, Jochen Schubert, Charless C. Fowlkes category:cs.CV published:2015-07-14 summary:Contextual information can have a substantial impact on the performance ofvisual tasks such as semantic segmentation, object detection, and geometricestimation. Data stored in Geographic Information Systems (GIS) offers a richsource of contextual information that has been largely untapped by computervision. We propose to leverage such information for scene understanding bycombining GIS resources with large sets of unorganized photographs usingStructure from Motion (SfM) techniques. We present a pipeline to quicklygenerate strong 3D geometric priors from 2D GIS data using SfM models alignedwith minimal user input. Given an image resectioned against this model, wegenerate robust predictions of depth, surface normals, and semantic labels. Weshow that the precision of the predicted geometry is substantially moreaccurate other single-image depth estimation methods. We then demonstrate theutility of these contextual constraints for re-scoring pedestrian detections,and use these GIS contextual features alongside object detection score maps toimprove a CRF-based semantic segmentation framework, boosting accuracy overbaseline models.
arxiv-14700-228 | Scale-Free Online Learning | http://arxiv.org/pdf/1601.01974v1.pdf | author:Francesco Orabona, Dávid Pál category:cs.LG published:2016-01-08 summary:We design algorithms for online linear optimization that have optimal regretand at the same time do not need to know any upper or lower bounds on the normof the loss vectors. We achieve adaptiveness to the norms of the loss vectorsby scale invariance, i.e., our algorithms make exactly the same decisions ifthe sequence of loss vectors is multiplied by any positive constant. One of ouralgorithms works for any decision set, bounded or unbounded. For unboundeddecisions sets, this is the first adaptive algorithm for online linearoptimization with a non-vacuous regret bound. We also study a popular scale-free variant of online mirror descentalgorithm, and we show that in two natural settings it has linear or worseregret.
arxiv-14700-229 | Cox process representation and inference for stochastic reaction-diffusion processes | http://arxiv.org/pdf/1601.01972v1.pdf | author:David Schnoerr, Ramon Grima, Guido Sanguinetti category:math.ST q-bio.QM stat.ML stat.TH published:2016-01-08 summary:Complex behaviour in many systems arises from the stochastic interactions ofspatially distributed particles or agents. Stochastic reaction-diffusionprocesses are widely used to model such behaviour in disciplines ranging frombiology to the social sciences, yet they are notoriously difficult to simulateand calibrate to observational data. Here we use ideas from statistical physicsand machine learning to provide a solution to the inverse problem of learning astochastic reaction diffusion process to data. Our solution relies on a novel,non-trivial connection between stochastic reaction-diffusion processes andspatio-temporal Cox processes, a well-studied class of models fromcomputational statistics. We develop an efficient and flexible algorithm whichshows excellent accuracy on numeric and real data examples from systems biologyand epidemiology. By using ideas from multiple disciplines, our approachprovides both new and fundamental insights into spatio-temporal stochasticsystems, and a practical solution to a long-standing problem in computationalmodelling.
arxiv-14700-230 | Numerical Coding of Nominal Data | http://arxiv.org/pdf/1601.01966v1.pdf | author:Zenon Gniazdowski, Michal Grabowski category:stat.ML published:2016-01-08 summary:In this paper, a novel approach for coding nominal data is proposed. For thegiven nominal data, a rank in a form of complex number is assigned. Theproposed method does not lose any information about the attribute and bringsother properties previously unknown. The approach based on these knewproperties can been used for classification. The analyzed example shows thatclassification with the use of coded nominal data or both numerical as well ascoded nominal data is more effective than the classification, which uses onlynumerical data.
arxiv-14700-231 | Visualizing and Understanding Neural Models in NLP | http://arxiv.org/pdf/1506.01066v2.pdf | author:Jiwei Li, Xinlei Chen, Eduard Hovy, Dan Jurafsky category:cs.CL published:2015-06-02 summary:While neural networks have been successfully applied to many NLP tasks theresulting vector-based models are very difficult to interpret. For example it'snot clear how they achieve {\em compositionality}, building sentence meaningfrom the meanings of words and phrases. In this paper we describe fourstrategies for visualizing compositionality in neural models for NLP, inspiredby similar work in computer vision. We first plot unit values to visualizecompositionality of negation, intensification, and concessive clauses, allow usto see well-known markedness asymmetries in negation. We then introduce threesimple and straightforward methods for visualizing a unit's {\em salience}, theamount it contributes to the final composed meaning: (1) gradientback-propagation, (2) the variance of a token from the average word node, (3)LSTM-style gates that measure information flow. We test our methods onsentiment using simple recurrent nets and LSTMs. Our general-purpose methodsmay have wide applications for understanding compositionality and othersemantic properties of deep networks , and also shed light on why LSTMsoutperform simple recurrent nets,
arxiv-14700-232 | Consistent Biclustering | http://arxiv.org/pdf/1206.6927v3.pdf | author:Cheryl J. Flynn, Patrick O. Perry category:stat.ME math.ST stat.ML stat.TH published:2012-06-29 summary:Biclustering, the process of simultaneously clustering the rows and columnsof a data matrix, is a popular and effective tool for finding structure in ahigh-dimensional dataset. Many biclustering procedures appear to work well inpractice, but most do not have associated consistency guarantees. To addressthis shortcoming, we propose a new biclustering procedure based on profilelikelihood. The procedure applies to a broad range of data modalities,including binary, count, and continuous observations. We prove that theprocedure recovers the true row and column classes when the dimensions of thedata matrix tend to infinity, even if the functional form of the datadistribution is misspecified. The procedure requires computing a combinatorialsearch, which can be expensive in practice. Rather than performing this searchdirectly, we propose a new heuristic optimization procedure based on theKernighan-Lin heuristic, which has nice computational properties and performswell in simulations. We demonstrate our procedure with applications tocongressional voting records, and microarray analysis.
arxiv-14700-233 | Nonparametric semi-supervised learning of class proportions | http://arxiv.org/pdf/1601.01944v1.pdf | author:Shantanu Jain, Martha White, Michael W. Trosset, Predrag Radivojac category:stat.ML cs.LG published:2016-01-08 summary:The problem of developing binary classifiers from positive and unlabeled datais often encountered in machine learning. A common requirement in this settingis to approximate posterior probabilities of positive and negative classes fora previously unseen data point. This problem can be decomposed into two steps:(i) the development of accurate predictors that discriminate between positiveand unlabeled data, and (ii) the accurate estimation of the prior probabilitiesof positive and negative examples. In this work we primarily focus on thelatter subproblem. We study nonparametric class prior estimation and formulatethis problem as an estimation of mixing proportions in two-component mixturemodels, given a sample from one of the components and another sample from themixture itself. We show that estimation of mixing proportions is generallyill-defined and propose a canonical form to obtain identifiability whilemaintaining the flexibility to model any distribution. We use insights fromthis theory to elucidate the optimization surface of the class priors andpropose an algorithm for estimating them. To address the problems ofhigh-dimensional density estimation, we provide practical transformations tolow-dimensional spaces that preserve class priors. Finally, we demonstrate theefficacy of our method on univariate and multivariate data.
arxiv-14700-234 | DUAL-LOCO: Distributing Statistical Estimation Using Random Projections | http://arxiv.org/pdf/1506.02554v2.pdf | author:Christina Heinze, Brian McWilliams, Nicolai Meinshausen category:stat.ML cs.DC cs.LG published:2015-06-08 summary:We present DUAL-LOCO, a communication-efficient algorithm for distributedstatistical estimation. DUAL-LOCO assumes that the data is distributedaccording to the features rather than the samples. It requires only a singleround of communication where low-dimensional random projections are used toapproximate the dependences between features available to different workers. Weshow that DUAL-LOCO has bounded approximation error which only depends weaklyon the number of workers. We compare DUAL-LOCO against a state-of-the-artdistributed optimization method on a variety of real world datasets and showthat it obtains better speedups while retaining good accuracy.
arxiv-14700-235 | Regret Guarantees for Item-Item Collaborative Filtering | http://arxiv.org/pdf/1507.05371v2.pdf | author:Guy Bresler, Devavrat Shah, Luis F. Voloch category:cs.LG cs.IR cs.IT math.IT stat.ML published:2015-07-20 summary:There is much empirical evidence that item-item collaborative filtering workswell in practice. Motivated to understand this, we provide a framework todesign and analyze various recommendation algorithms. The setup amounts toonline binary matrix completion, where at each time a random user requests arecommendation and the algorithm chooses an entry to reveal in the user's row.The goal is to minimize regret, or equivalently to maximize the number of +1entries revealed at any time. We analyze an item-item collaborative filteringalgorithm that can achieve fundamentally better performance compared touser-user collaborative filtering. The algorithm achieves good "cold-start"performance (appropriately defined) by quickly making good recommendations tonew users about whom there is little information.
arxiv-14700-236 | A Novel Performance Evaluation Methodology for Single-Target Trackers | http://arxiv.org/pdf/1503.01313v3.pdf | author:Matej Kristan, Jiri Matas, Ales Leonardis, Tomas Vojir, Roman Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih Porikli, Luka Cehovin category:cs.CV published:2015-03-04 summary:This paper addresses the problem of single-target tracker performanceevaluation. We consider the performance measures, the dataset and theevaluation system to be the most important components of tracker evaluation andpropose requirements for each of them. The requirements are the basis of a newevaluation methodology that aims at a simple and easily interpretable trackercomparison. The ranking-based methodology addresses tracker equivalence interms of statistical significance and practical differences. A fully-annotateddataset with per-frame annotations with several visual attributes isintroduced. The diversity of its visual properties is maximized in a novel wayby clustering a large number of videos according to their visual attributes.This makes it the most sophistically constructed and annotated dataset to date.A multi-platform evaluation system allowing easy integration of third-partytrackers is presented as well. The proposed evaluation methodology was testedon the VOT2014 challenge on the new dataset and 38 trackers, making it thelargest benchmark to date. Most of the tested trackers are indeedstate-of-the-art since they outperform the standard baselines, resulting in ahighly-challenging benchmark. An exhaustive analysis of the dataset from theperspective of tracking difficulty is carried out. To facilitate trackercomparison a new performance visualization technique is proposed.
arxiv-14700-237 | Learning Precise Spike Train to Spike Train Transformations in Multilayer Feedforward Neuronal Networks | http://arxiv.org/pdf/1412.4210v2.pdf | author:Arunava Banerjee category:cs.NE published:2014-12-13 summary:We derive a synaptic weight update rule for learning temporally precise spiketrain to spike train transformations in multilayer feedforward networks ofspiking neurons. The framework, aimed at seamlessly generalizing errorbackpropagation to the deterministic spiking neuron setting, is based strictlyon spike timing and avoids invoking concepts pertaining to spike rates orprobabilistic models of spiking. The derivation is founded on two innovations.First, an error functional is proposed that compares the spike train emitted bythe output neuron of the network to the desired spike train by way of theirputative impact on a virtual postsynaptic neuron. This formulation sidestepsthe need for spike alignment and leads to closed form solutions for allquantities of interest. Second, virtual assignment of weights to spikes ratherthan synapses enables a perturbation analysis of individual spike times andsynaptic weights of the output as well as all intermediate neurons in thenetwork, which yields the gradients of the error functional with respect to thesaid entities. Learning proceeds via a gradient descent mechanism thatleverages these quantities. Simulation experiments demonstrate the efficacy ofthe proposed learning framework. The experiments also highlight asymmetriesbetween synapses on excitatory and inhibitory neurons.
arxiv-14700-238 | Deep convolutional acoustic word embeddings using word-pair side information | http://arxiv.org/pdf/1510.01032v2.pdf | author:Herman Kamper, Weiran Wang, Karen Livescu category:cs.CL published:2015-10-05 summary:Recent studies have been revisiting whole words as the basic modelling unitin speech recognition and query applications, instead of phonetic units. Suchwhole-word segmental systems rely on a function that maps a variable-lengthspeech segment to a vector in a fixed-dimensional space; the resulting acousticword embeddings need to allow for accurate discrimination between differentword types, directly in the embedding space. We compare several old and newapproaches in a word discrimination task. Our best approach uses sideinformation in the form of known word pairs to train a Siamese convolutionalneural network (CNN): a pair of tied networks that take two speech segments asinput and produce their embeddings, trained with a hinge loss that separatessame-word pairs and different-word pairs by some margin. A word classifier CNNperforms similarly, but requires much stronger supervision. Both types of CNNsyield large improvements over the best previously published results on the worddiscrimination task.
arxiv-14700-239 | Research Project: Text Engineering Tool for Ontological Scientometry | http://arxiv.org/pdf/1601.01887v1.pdf | author:Rustam Tagiew category:cs.CL cs.DL published:2016-01-08 summary:The number of scientific papers grows exponentially in many disciplines. Theshare of online available papers grows as well. At the same time, the period oftime for a paper to loose at chance to be cited anymore shortens. The decay ofthe citing rate shows similarity to ultradiffusional processes as for otheronline contents in social networks. The distribution of papers per author showssimilarity to the distribution of posts per user in social networks. The rateof uncited papers for online available papers grows while some papers 'goviral' in terms of being cited. Summarized, the practice of scientificpublishing moves towards the domain of social networks. The goal of thisproject is to create a text engineering tool, which can semi-automaticallycategorize a paper according to its type of contribution and extractrelationships between them into an ontological database. Semi-automaticcategorization means that the mistakes made by automatic pre-categorization andrelationship-extraction will be corrected through a wikipedia-like front-end byvolunteers from general public. This tool should not only help researchers andthe general public to find relevant supplementary material and peers faster,but also provide more information for research funding agencies.
arxiv-14700-240 | Visual Script and Language Identification | http://arxiv.org/pdf/1601.01885v1.pdf | author:Anguelos Nicolaou, Andrew Bagdanov, Lluis Gomez-Bigorda, Dimosthenis Karatzas category:cs.CV published:2016-01-08 summary:In this paper we introduce a script identification method based onhand-crafted texture features and an artificial neural network. The proposedpipeline achieves near state-of-the-art performance for script identificationof video-text and state-of-the-art performance on visual languageidentification of handwritten text. More than using the deep network as aclassifier, the use of its intermediary activations as a learned metricdemonstrates remarkable results and allows the use of discriminative models onunknown classes. Comparative experiments in video-text and text in the wilddatasets provide insights on the internals of the proposed deep network.
arxiv-14700-241 | Facial age estimation using BSIF and LBP | http://arxiv.org/pdf/1601.01876v1.pdf | author:Salah Eddine Bekhouche, Abdelkrim Ouafi, Abdelmalik Taleb-Ahmed, Abdenour Hadid, Azeddine Benlamoudi category:cs.CV published:2016-01-08 summary:Human face aging is irreversible process causing changes in human facecharacteristics such us hair whitening, muscles drop and wrinkles. Due to theimportance of human face aging in biometrics systems, age estimation became anattractive area for researchers. This paper presents a novel method to estimatethe age from face images, using binarized statistical image features (BSIF) andlocal binary patterns (LBP)histograms as features performed by support vectorregression (SVR) and kernel ridge regression (KRR). We applied our method onFG-NET and PAL datasets. Our proposed method has shown superiority to that ofthe state-of-the-art methods when using the whole PAL database.
arxiv-14700-242 | Superpixel Convolutional Networks using Bilateral Inceptions | http://arxiv.org/pdf/1511.06739v3.pdf | author:Raghudeep Gadde, Varun Jampani, Martin Kiefel, Peter V. Gehler category:cs.CV I.2.10; I.2.6 published:2015-11-20 summary:In this paper we propose a CNN architecture for image segmentation. Weintroduce a new "bilateral inception" layer that is used on top of aconvolutional architecture. The bilateral inception performs a filteringbetween superpixels in an image. This addresses two problems that arise withCNN segmentation architectures. First, this layer propagates informationbetween (super) pixels while respecting image edges, thus using the structuredinformation of the problem for improved results. Second, the layer recovers afull resolution segmentation result from the lower resolution solution of aCNN. In the experiments we replace the deconvolution networks and Dense-CRFthat have previously been proposed to address these problems with bilateralinception layers. The reduction to superpixels reduces the amount ofcomputations and simplifies the network design. Further, we report betterempirical results by replacing De-convolutional and CNN+Dense-CRF steps in fourdifferent semantic segmentation CNN architecutres, even with-out re-trainingtheir filter weights.
arxiv-14700-243 | Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets | http://arxiv.org/pdf/1511.05236v4.pdf | author:Patrick Judd, Jorge Albericio, Tayler Hetherington, Tor Aamodt, Natalie Enright Jerger, Raquel Urtasun, Andreas Moshovos category:cs.LG cs.NE published:2015-11-17 summary:This work investigates how using reduced precision data in ConvolutionalNeural Networks (CNNs) affects network accuracy during classification. Morespecifically, this study considers networks where each layer may use differentprecision data. Our key result is the observation that the tolerance of CNNs toreduced precision data not only varies across networks, a well establishedobservation, but also within networks. Tuning precision per layer is appealingas it could enable energy and performance improvements. In this paper we studyhow error tolerance across layers varies and propose a method for finding a lowprecision configuration for a network while maintaining high accuracy. Adiverse set of CNNs is analyzed showing that compared to a conventionalimplementation using a 32-bit floating-point representation for all layers, andwith less than 1% loss in relative accuracy, the data footprint required bythese networks can be reduced by an average of 74% and up to 92%.
arxiv-14700-244 | Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance | http://arxiv.org/pdf/1508.01983v3.pdf | author:Amr Bakry, Mohamed Elhoseiny, Tarek El-Gaaly, Ahmed Elgammal category:cs.CV published:2015-08-09 summary:This paper is focused on studying the view-manifold structure in the featurespaces implied by the different layers of Convolutional Neural Networks (CNN).There are several questions that this paper aims to answer: Does the learnedCNN representation achieve viewpoint invariance? How does it achieve viewpointinvariance? Is it achieved by collapsing the view manifolds, or separating themwhile preserving them? At which layer is view invariance achieved? How can thestructure of the view manifold at each layer of a deep convolutional neuralnetwork be quantified experimentally? How does fine-tuning of a pre-trained CNNon a multi-view dataset affect the representation at each layer of the network?In order to answer these questions we propose a methodology to quantify thedeformation and degeneracy of view manifolds in CNN layers. We apply thismethodology and report interesting results in this paper that answer theaforementioned questions.
arxiv-14700-245 | FireCaffe: near-linear acceleration of deep neural network training on compute clusters | http://arxiv.org/pdf/1511.00175v2.pdf | author:Forrest N. Iandola, Khalid Ashraf, Matthew W. Moskewicz, Kurt Keutzer category:cs.CV published:2015-10-31 summary:Long training times for high-accuracy deep neural networks (DNNs) impederesearch into new DNN architectures and slow the development of high-accuracyDNNs. In this paper we present FireCaffe, which successfully scales deep neuralnetwork training across a cluster of GPUs. We also present a number of bestpractices to aid in comparing advancements in methods for scaling andaccelerating the training of deep neural networks. The speed and scalability ofdistributed algorithms is almost always limited by the overhead ofcommunicating between servers; DNN training is not an exception to this rule.Therefore, the key consideration here is to reduce communication overheadwherever possible, while not degrading the accuracy of the DNN models that wetrain. Our approach has three key pillars. First, we select network hardwarethat achieves high bandwidth between GPU servers -- Infiniband or Crayinterconnects are ideal for this. Second, we consider a number of communicationalgorithms, and we find that reduction trees are more efficient and scalablethan the traditional parameter server approach. Third, we optionally increasethe batch size to reduce the total quantity of communication during DNNtraining, and we identify hyperparameters that allow us to reproduce thesmall-batch accuracy while training with large batch sizes. When trainingGoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup,respectively, when training on a cluster of 128 GPUs.
arxiv-14700-246 | Dropout as data augmentation | http://arxiv.org/pdf/1506.08700v4.pdf | author:Xavier Bouthillier, Kishore Konda, Pascal Vincent, Roland Memisevic category:stat.ML cs.LG published:2015-06-29 summary:Dropout is typically interpreted as bagging a large number of models sharingparameters. We show that using dropout in a network can also be interpreted asa kind of data augmentation in the input space without domain knowledge. Wepresent an approach to projecting the dropout noise within a network back intothe input space, thereby generating augmented versions of the training data,and we show that training a deterministic network on the augmented samplesyields similar results. Finally, we propose a new dropout noise scheme based onour observations and show that it improves dropout results without addingsignificant computational cost.
arxiv-14700-247 | Learning Dense Convolutional Embeddings for Semantic Segmentation | http://arxiv.org/pdf/1511.04377v3.pdf | author:Adam W. Harley, Konstantinos G. Derpanis, Iasonas Kokkinos category:cs.CV published:2015-11-13 summary:This paper proposes a new deep convolutional neural network (DCNN)architecture that learns pixel embeddings, such that pairwise distances betweenthe embeddings can be used to infer whether or not the pixels lie on the sameregion. That is, for any two pixels on the same object, the embeddings aretrained to be similar; for any pair that straddles an object boundary, theembeddings are trained to be dissimilar. Experimental results show that whenthis embedding network is used in conjunction with a DCNN trained on semanticsegmentation, there is a systematic improvement in per-pixel classificationaccuracy. Our contributions are integrated in the popular Caffe deep learningframework, and consist in straightforward modifications to convolutionroutines. As such, they can be exploited for any task involving convolutionlayers.
arxiv-14700-248 | Similarity-based Text Recognition by Deeply Supervised Siamese Network | http://arxiv.org/pdf/1511.04397v3.pdf | author:Ehsan Hosseini-Asl, Angshuman Guha category:cs.CV cs.LG published:2015-11-13 summary:In this paper, we propose a new text recognition model based on measuring thevisual similarity of text and predicting the content of unlabeled texts. Firsta Siamese convolutional network is trained with deep supervision on a labeledtraining dataset. This network projects texts into a similarity manifold. TheDeeply Supervised Siamese network learns visual similarity of texts. Then aK-nearest neighbor classifier is used to predict unlabeled text based onsimilarity distance to labeled texts. The performance of the model is evaluatedon three datasets of machine-print and hand-written text combined. Wedemonstrate that the model reduces the cost of human estimation by $50\%-85\%$.The error of the system is less than $0.5\%$. The proposed model outperformconventional Siamese network by finding visually-similar barely-readable andreadable text, e.g. machine-printed, handwritten, due to deep supervision. Theresults also demonstrate that the predicted labels are sometimes better thanhuman labels e.g. spelling correction.
arxiv-14700-249 | A convnet for non-maximum suppression | http://arxiv.org/pdf/1511.06437v3.pdf | author:Jan Hosang, Rodrigo Benenson, Bernt Schiele category:cs.CV cs.LG published:2015-11-19 summary:Non-maximum suppression (NMS) is used in virtually all state-of-the-artobject detection pipelines. While essential object detection ingredients suchas features, classifiers, and proposal methods have been extensively researchedsurprisingly little work has aimed to systematically address NMS. The de-factostandard for NMS is based on greedy clustering with a fixed distance threshold,which forces to trade-off recall versus precision. We propose a convnetdesigned to perform NMS of a given set of detections. We report experiments ona synthetic setup, and results on crowded pedestrian detection scenes. Ourapproach overcomes the intrinsic limitations of greedy NMS, obtaining betterrecall and precision.
arxiv-14700-250 | Efficient Sum of Sparse Outer Products Dictionary Learning (SOUP-DIL) | http://arxiv.org/pdf/1511.06333v3.pdf | author:Saiprasad Ravishankar, Raj Rao Nadakuditi, Jeffrey A. Fessler category:cs.LG published:2015-11-19 summary:The sparsity of natural signals in a transform domain or dictionary has beenextensively exploited in several applications. More recently, the data-drivenadaptation of synthesis dictionaries has shown promise in many applicationscompared to fixed or analytical dictionaries. However, dictionary learningproblems are typically non-convex and NP-hard, and the usual alternatingminimization approaches for these problems are often computationally expensive,with the computations dominated by the NP-hard synthesis sparse coding step. Inthis work, we investigate an efficient method for $\ell_{0}$ "norm"-baseddictionary learning by first approximating the training data set with a sum ofsparse rank-one matrices and then using a block coordinate descent approach toestimate the rank-one terms. The proposed algorithm involves efficientclosed-form solutions. In particular, the sparse coding step involves a simpleform of thresholding. We provide a convergence analysis for the proposed blockcoordinate descent method. Our experiments show the promising performance andsignificant speed-ups provided by our method over the classical K-SVD scheme insparse signal representation and image denoising.
arxiv-14700-251 | Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks | http://arxiv.org/pdf/1511.06434v2.pdf | author:Alec Radford, Luke Metz, Soumith Chintala category:cs.LG cs.CV published:2015-11-19 summary:In recent years, supervised learning with convolutional networks (CNNs) hasseen huge adoption in computer vision applications. Comparatively, unsupervisedlearning with CNNs has received less attention. In this work we hope to helpbridge the gap between the success of CNNs for supervised learning andunsupervised learning. We introduce a class of CNNs called deep convolutionalgenerative adversarial networks (DCGANs), that have certain architecturalconstraints, and demonstrate that they are a strong candidate for unsupervisedlearning. Training on various image datasets, we show convincing evidence thatour deep convolutional adversarial pair learns a hierarchy of representationsfrom object parts to scenes in both the generator and discriminator.Additionally, we use the learned features for novel tasks - demonstrating theirapplicability as general image representations.
arxiv-14700-252 | An Information Retrieval Approach to Finding Dependent Subspaces of Multiple Views | http://arxiv.org/pdf/1511.06423v2.pdf | author:Ziyuan Lin, Jaakko Peltonen category:stat.ML cs.LG published:2015-11-19 summary:Finding relationships between multiple views of data is essential both forexploratory analysis and as pre-processing for predictive tasks. A prominentapproach is to apply variants of Canonical Correlation Analysis (CCA), aclassical method seeking correlated components between views. The basic CCA isrestricted to maximizing a simple dependency criterion, correlation, measureddirectly between data coordinates. We introduce a new method that findsdependent subspaces of views directly optimized for the data analysis task of\textit{neighbor retrieval between multiple views}. We optimize mappings foreach view such as linear transformations to maximize cross-view similaritybetween neighborhoods of data samples. The criterion arises directly from thewell-defined retrieval task, detects nonlinear and local similarities, is ableto measure dependency of data relationships rather than only individual datacoordinates, and is related to well understood measures of informationretrieval quality. In experiments we show the proposed method outperformsalternatives in preserving cross-view neighborhood similarities, and yieldsinsights into local dependencies between multiple views.
arxiv-14700-253 | Conditional Computation in Neural Networks for faster models | http://arxiv.org/pdf/1511.06297v2.pdf | author:Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, Doina Precup category:cs.LG published:2015-11-19 summary:Deep learning has become the state-of-art tool in many applications, but theevaluation and training of deep models can be time-consuming andcomputationally expensive. The conditional computation approach has beenproposed to tackle this problem (Bengio et al., 2013; Davis & Arel, 2013). Itoperates by selectively activating only parts of the network at a time. In thispaper, we use reinforcement learning as a tool to optimize conditionalcomputation policies. More specifically, we cast the problem of learningactivation-dependent policies for dropping out blocks of units as areinforcement learning problem. We propose a learning scheme motivated bycomputation speed, capturing the idea of wanting to have parsimoniousactivations while maintaining prediction accuracy. We apply a policy gradientalgorithm for learning policies that optimize this loss function and propose aregularization mechanism that encourages diversification of the dropout policy.We present encouraging empirical results showing that this approach improvesthe speed of computation without impacting the quality of the approximation.
arxiv-14700-254 | Fixed Point Quantization of Deep Convolutional Networks | http://arxiv.org/pdf/1511.06393v2.pdf | author:Darryl D. Lin, Sachin S. Talathi, V. Sreekanth Annapureddy category:cs.LG published:2015-11-19 summary:In recent years increasingly complex architectures for deep convolutionnetworks (DCNs) have been proposed to boost the performance on imagerecognition tasks. However, the gains in performance have come at a cost ofsubstantial increase in compute resources, the model size and processing speedof the network for training and evaluation. Fixed point implementation of thesenetworks has the potential to alleviate some of the burden of these additionalcomplexities. In this paper, we propose a quantizer design for fixed pointimplementation for DCNs. We then formulate an optimization problem to identifyoptimal fixed point bit-width allocation across DCN layers. We performexperiments on a recently proposed DCN architecture for CIFAR-10 benchmark thatgenerates test error of less than 7%. We evaluate the effectiveness of ourproposed fixed point bit-width allocation for this DCN. Our experiments showthat in comparison to equal bit-width settings, the fixed point DCNs withoptimized bit width allocation offer >20% reduction in the model size withoutany loss in performance. We also demonstrate that fine tuning can furtherenhance the accuracy of fixed point DCNs beyond that of the original floatingpoint model. In doing so, we report a new state-of-the-art fixed pointperformance of 6.78% error-rate on CIFAR-10 benchmark.
arxiv-14700-255 | Recurrent Models for Auditory Attention in Multi-Microphone Distance Speech Recognition | http://arxiv.org/pdf/1511.06407v2.pdf | author:Suyoun Kim, Ian Lane category:cs.LG cs.CL published:2015-11-19 summary:Integration of multiple microphone data is one of the key ways to achieverobust speech recognition in noisy environments or when the speaker is locatedat some distance from the input device. Signal processing techniques such asbeamforming are widely used to extract a speech signal of interest frombackground noise. These techniques, however, are highly dependent on priorspatial information about the microphones and the environment in which thesystem is being used. In this work, we present a neural attention network thatdirectly combines multi-channel audio to generate phonetic states withoutrequiring any prior knowledge of the microphone layout or any explicit signalpreprocessing for speech enhancement. We embed an attention mechanism within aRecurrent Neural Network (RNN) based acoustic model to automatically tune itsattention to a more reliable input source. Unlike traditional multi-channelpreprocessing, our system can be optimized towards the desired output in onestep. Although attention-based models have recently achieved impressive resultson sequence-to-sequence learning, no attention mechanisms have previously beenapplied to learn potentially asynchronous and non-stationary multiple inputs.We evaluate our neural attention model on the CHiME-3 challenge task, and showthat the model achieves comparable performance to beamforming using a purelydata-driven method.
arxiv-14700-256 | Discovering Internal Representations from Object-CNNs Using Population Encoding | http://arxiv.org/pdf/1511.06855v2.pdf | author:Jianyu Wang, Zhishuai Zhang, Vittal Premachandran, Alan Yuille category:cs.LG cs.CV published:2015-11-21 summary:In this paper, we provide a method for understanding the internalrepresentations of Convolutional Neural Networks (CNNs) trained on objects. Wehypothesize that the information is distributed across multiple neuronalresponses and propose a simple clustering technique to extract thisinformation, which we call \emph{population encoding}. The population encodingtechnique looks into the entrails of an object-CNN at multiple layers of thenetwork and shows the implicit presence of mid-level object part semanticsdistributed in the neuronal responses. Our qualitative visualizations show thatpopulation encoding can extract mid-level image patches that are visuallytighter than the patches that produce high single-filter activations. Moreover,our comprehensive quantitative experiments using the object key pointannotations from the PASCAL3D+ dataset corroborate the visualizations bydemonstrating the superiority of population encoding over single-filterdetectors, in the task of object-part detection. We also perform somepreliminary experiments where we uncover the compositional relations betweenthe adjacent layers using the parts detected by population encoding clusters.Finally, based on the insights gained from this work, we point to various newdirections which will enable us to have a better understanding of the CNN'sinternal representations.
arxiv-14700-257 | Manifold Regularized Discriminative Neural Networks | http://arxiv.org/pdf/1511.06328v3.pdf | author:Shuangfei Zhai, Zhongfei Zhang category:cs.LG published:2015-11-19 summary:Unregularized deep neural networks (DNNs) can be easily overfit with alimited sample size. We argue that this is mostly due to the disriminativenature of DNNs which directly model the conditional probability (or score) oflabels given the input. The ignorance of input distribution makes DNNsdifficult to generalize to unseen data. Recent advances in regularizationtechniques, such as pretraining and dropout, indicate that modeling input datadistribution (either explicitly or implicitly) greatly improves thegeneralization ability of a DNN. In this work, we explore the manifoldhypothesis which assumes that instances within the same class lie in a smoothmanifold. We accordingly propose two simple regularizers to a standarddiscriminative DNN. The first one, named Label-Aware Manifold Regularization,assumes the availability of labels and penalizes large norms of the lossfunction w.r.t. data points. The second one, named Label-Independent ManifoldRegularization, does not use label information and instead penalizes theFrobenius norm of the Jacobian matrix of prediction scores w.r.t. data points,which makes semi-supervised learning possible. We perform extensive controlexperiments on fully supervised and semi-supervised tasks using the MNIST,CIFAR10 and SVHN datasets and achieve excellent results.
arxiv-14700-258 | Learning to Compose Neural Networks for Question Answering | http://arxiv.org/pdf/1601.01705v1.pdf | author:Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein category:cs.CL cs.CV cs.NE published:2016-01-07 summary:We describe a question answering model that applies to both images andstructured knowledge bases. The model uses natural language strings toautomatically assemble neural networks from a collection of composable modules.Parameters for these modules are learned jointly with network-assemblyparameters via reinforcement learning, with only (world, question, answer)triples as supervision. Our approach, which we term a dynamic neural modelnetwork, achieves state-of-the-art results on benchmark datasets in both visualand structured domains.
arxiv-14700-259 | How much data is needed to train a medical image deep learning system to achieve necessary high accuracy? | http://arxiv.org/pdf/1511.06348v2.pdf | author:Junghwan Cho, Kyewook Lee, Ellie Shin, Garry Choy, Synho Do category:cs.LG cs.CV cs.NE published:2015-11-19 summary:The use of Convolutional Neural Networks (CNN) in natural imageclassification systems has produced very impressive results. Combined with theinherent nature of medical images that make them ideal for deep-learning,further application of such systems to medical image classification holds muchpromise. However, the usefulness and potential impact of such a system can becompletely negated if it does not reach a target accuracy. In this paper, wepresent a study on determining the optimum size of the training data setnecessary to achieve high classification accuracy with low variance in medicalimage classification systems. The CNN was applied to classify axial ComputedTomography (CT) images into six anatomical classes. We trained the CNN usingsix different sizes of training data set (5, 10, 20, 50, 100, and 200) and thentested the resulting system with a total of 6000 CT images. All images wereacquired from the Massachusetts General Hospital (MGH) Picture Archiving andCommunication System (PACS). Using this data, we employ the learning curveapproach to predict classification accuracy at a given training sample size.Our research will present a general methodology for determining the trainingdata set size necessary to achieve a certain target classification accuracythat can be easily applied to other problems within such systems.
arxiv-14700-260 | Online Sequence Training of Recurrent Neural Networks with Connectionist Temporal Classification | http://arxiv.org/pdf/1511.06841v4.pdf | author:Kyuyeon Hwang, Wonyong Sung category:cs.LG cs.NE published:2015-11-21 summary:Connectionist temporal classification (CTC) based supervised sequencetraining of recurrent neural networks (RNNs) has shown great success in manymachine learning areas including end-to-end speech and handwritten characterrecognition. For the CTC training, however, it is required to unroll (orunfold) the RNN by the length of an input sequence. This unrolling requires alot of memory and hinders a small footprint implementation of online learningor adaptation. Furthermore, the length of training sequences is usually notuniform, which makes parallel training with multiple sequences inefficient onshared memory models such as graphics processing units (GPUs). In this work, weintroduce an expectation-maximization (EM) based online CTC algorithm thatenables unidirectional RNNs to learn sequences that are longer than the amountof unrolling. The RNNs can also be trained to process an infinitely long inputsequence without pre-segmentation or external reset. Moreover, the proposedapproach allows efficient parallel training on GPUs. For evaluation, phonemerecognition and end-to-end speech recognition examples are presented on theTIMIT and Wall Street Journal (WSJ) corpora, respectively. Our online modelachieves 20.7% phoneme error rate (PER) on the very long input sequence that isgenerated by concatenating all 192 utterances in the TIMIT core test set. OnWSJ, a network can be trained with only 64 times of unrolling while sacrificing4.5% relative word error rate (WER).
arxiv-14700-261 | An Automaton Learning Approach to Solving Safety Games over Infinite Graphs | http://arxiv.org/pdf/1601.01660v1.pdf | author:Daniel Neider, Ufuk Topcu category:cs.FL cs.LG cs.LO 05C57 F.1.1; I.2.6 published:2016-01-07 summary:We propose a method to construct finite-state reactive controllers forsystems whose interactions with their adversarial environment are modeled byinfinite-duration two-player games over (possibly) infinite graphs. Theproposed method targets safety games with infinitely many states or with such alarge number of states that it would be impractical---if not impossible---forconventional synthesis techniques that work on the entire state space. Weresort to constructing finite-state controllers for such systems through anautomata learning approach, utilizing a symbolic representation of theunderlying game that is based on finite automata. Throughout the learningprocess, the learner maintains an approximation of the winning region(represented as a finite automaton) and refines it using different types ofcounterexamples provided by the teacher until a satisfactory controller can bederived (if one exists). We present a symbolic representation of safety games(inspired by regular model checking), propose implementations of the learnerand teacher, and evaluate their performance on examples motivated by roboticmotion planning in dynamic environments.
arxiv-14700-262 | PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions | http://arxiv.org/pdf/1504.08362v3.pdf | author:Michael Figurnov, Dmitry Vetrov, Pushmeet Kohli category:cs.CV published:2015-04-30 summary:We propose a novel approach to reduce the computational cost of evaluation ofconvolutional neural networks, a factor that has hindered their deployment inlow-power devices such as mobile phones. Inspired by the loop perforationtechnique from source code optimization, we speed up the bottleneckconvolutional layers by skipping their evaluation in some of the spatialpositions. We propose and analyze several strategies of choosing thesepositions. Our method allows to reduce the evaluation time of modernconvolutional neural networks by 50% with a small decrease in accuracy.
arxiv-14700-263 | Large Collection of Diverse Gene Set Search Queries Recapitulate Known Protein-Protein Interactions and Gene-Gene Functional Associations | http://arxiv.org/pdf/1601.01653v1.pdf | author:Avi Ma'ayan, Neil R. Clark category:q-bio.MN cs.AI cs.SI q-bio.GN stat.ML published:2016-01-07 summary:Popular online enrichment analysis tools from the field of molecular systemsbiology provide users with the ability to submit their experimental results asgene sets for individual analysis. Such queries are kept private, and havenever before been considered as a resource for integrative analysis. Byharnessing gene set query submissions from thousands of users, we aim todiscover biological knowledge beyond the scope of an individual study. In thiswork, we investigated a large collection of gene sets submitted to the toolEnrichr by thousands of users. Based on co-occurrence, we constructed a globalgene-gene association network. We interpret this inferred network as providinga summary of the structure present in this crowdsourced gene set library, andshow that this network recapitulates known protein-protein interactions andfunctional associations between genes. This finding implies that this networkalso offers predictive value. Furthermore, we visualize this gene-geneassociation network using a new edge-pruning algorithm that retains both thelocal and global structures of large-scale networks. Our ability to makepredictions for currently unknown gene associations, that may not be capturedby individual researchers and data sources, is a demonstration of the potentialof harnessing collective knowledge from users of popular tools in the field ofmolecular systems biology.
arxiv-14700-264 | Neural Variational Inference for Text Processing | http://arxiv.org/pdf/1511.06038v3.pdf | author:Yishu Miao, Lei Yu, Phil Blunsom category:cs.CL cs.LG stat.ML published:2015-11-19 summary:Recent advances in neural variational inference have spawned a renaissance indeep latent variable models. In this paper we introduce a generic variationalinference framework for generative and conditional models of text. Whiletraditional variational methods derive an analytic approximation for theintractable distributions over latent variables, here we construct an inferencenetwork conditioned on the discrete text input to provide the variationaldistribution. We validate this framework on two very different text modellingapplications, generative document modelling and supervised question answering.Our neural variational document model combines a continuous stochastic documentrepresentation with a bag-of-words generative model and achieves the lowestreported perplexities on two standard test corpora. The neural answer selectionmodel employs a stochastic representation layer within an attention mechanismto extract the semantics between a question and answer pair. On two questionanswering benchmarks this model exceeds all previous published benchmarks.
arxiv-14700-265 | Expressiveness of Rectifier Networks | http://arxiv.org/pdf/1511.05678v2.pdf | author:Xingyuan Pan, Vivek Srikumar category:cs.LG published:2015-11-18 summary:Rectified Linear Units (ReLUs) have been shown to ameliorate the vanishinggradient problem, allow for efficient backpropagation, and empirically promotesparsity in the learned parameters. Their use has led to state-of-the-artresults in a variety of applications. In this paper, we characterize theexpressiveness of ReLU networks. From this perspective, unlike the sign(threshold) and sigmoid activations, ReLU networks are less explored. We showthat, while the decision boundary of a two-layer ReLU network can be capturedby a sign network, the sign network can require an exponentially larger numberof hidden units. Furthermore, we formulate sufficient conditions for acorresponding logarithmic reduction in the number of hidden units to representa sign network as a ReLU network. Finally, using synthetic data, weexperimentally demonstrate that back propagation can recover the much smallerReLU networks as predicted by the theory.
arxiv-14700-266 | Policy Distillation | http://arxiv.org/pdf/1511.06295v2.pdf | author:Andrei A. Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell category:cs.LG published:2015-11-19 summary:Policies for complex visual tasks have been successfully learned with deepreinforcement learning, using an approach called deep Q-networks (DQN), butrelatively large (task-specific) networks and extensive training are needed toachieve good performance. In this work, we present a novel method called policydistillation that can be used to extract the policy of a reinforcement learningagent and train a new network that performs at the expert level while beingdramatically smaller and more efficient. Furthermore, the same method can beused to consolidate multiple task-specific policies into a single policy. Wedemonstrate these claims using the Atari domain and show that the multi-taskdistilled agent outperforms the single-task teachers as well as ajointly-trained DQN agent.
arxiv-14700-267 | MazeBase: A Sandbox for Learning from Games | http://arxiv.org/pdf/1511.07401v2.pdf | author:Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve, Soumith Chintala, Rob Fergus category:cs.LG cs.AI cs.NE published:2015-11-23 summary:This paper introduces MazeBase: an environment for simple 2D games, designedas a sandbox for machine learning approaches to reasoning and planning. Withinit, we create 10 simple games embodying a range of algorithmic tasks (e.g.if-then statements or set negation). A variety of neural models (fullyconnected, convolutional network, memory network) are deployed viareinforcement learning on these games, with and without a procedurallygenerated curriculum. Despite the tasks' simplicity, the performance of themodels is far from optimal, suggesting directions for future development. Wealso demonstrate the versatility of MazeBase by using it to emulate smallcombat scenarios from StarCraft. Models trained on the MazeBase version can bedirectly applied to StarCraft, where they consistently beat the in-game AI.
arxiv-14700-268 | Grid Long Short-Term Memory | http://arxiv.org/pdf/1507.01526v3.pdf | author:Nal Kalchbrenner, Ivo Danihelka, Alex Graves category:cs.NE cs.CL cs.LG published:2015-07-06 summary:This paper introduces Grid Long Short-Term Memory, a network of LSTM cellsarranged in a multidimensional grid that can be applied to vectors, sequencesor higher dimensional data such as images. The network differs from existingdeep LSTM architectures in that the cells are connected between network layersas well as along the spatiotemporal dimensions of the data. The networkprovides a unified way of using LSTM for both deep and sequential computation.We apply the model to algorithmic tasks such as 15-digit integer addition andsequence memorization, where it is able to significantly outperform thestandard LSTM. We then give results for two empirical tasks. We find that 2DGrid LSTM achieves 1.47 bits per character on the Wikipedia characterprediction benchmark, which is state-of-the-art among neural approaches. Inaddition, we use the Grid LSTM to define a novel two-dimensional translationmodel, the Reencoder, and show that it outperforms a phrase-based referencesystem on a Chinese-to-English translation task.
arxiv-14700-269 | Efficient and Parsimonious Agnostic Active Learning | http://arxiv.org/pdf/1506.08669v3.pdf | author:Tzu-Kuo Huang, Alekh Agarwal, Daniel J. Hsu, John Langford, Robert E. Schapire category:cs.LG stat.ML published:2015-06-29 summary:We develop a new active learning algorithm for the streaming settingsatisfying three important properties: 1) It provably works for any classifierrepresentation and classification problem including those with severe noise. 2)It is efficiently implementable with an ERM oracle. 3) It is more aggressivethan all previous approaches satisfying 1 and 2. To do this we create analgorithm based on a newly defined optimization problem and analyze it. We alsoconduct the first experimental analysis of all efficient agnostic activelearning algorithms, evaluating their strengths and weaknesses in differentsettings.
arxiv-14700-270 | Population-Contrastive-Divergence: Does Consistency help with RBM training? | http://arxiv.org/pdf/1510.01624v3.pdf | author:Oswin Krause, Asja Fischer, Christian Igel category:cs.LG cs.NE stat.ML published:2015-10-06 summary:Estimating the log-likelihood gradient with respect to the parameters of aRestricted Boltzmann Machine (RBM) typically requires sampling using MarkovChain Monte Carlo (MCMC) techniques. To save computation time, the Markovchains are only run for a small number of steps, which leads to a biasedestimate. This bias can cause RBM training algorithms such as ContrastiveDivergence (CD) learning to deteriorate. We adopt the idea behind PopulationMonte Carlo (PMC) methods to devise a new RBM training algorithm termedPopulation-Contrastive-Divergence (pop-CD). Compared to CD, it leads to aconsistent estimate and may have a significantly lower bias. Its computationaloverhead is negligible compared to CD. However, the variance of the gradientestimate increases. We experimentally show that pop-CD can significantlyoutperform CD. In many cases, we observed a smaller bias and achieved higherlog-likelihood values. However, when the RBM distribution has many hiddenneurons, the consistent estimate of pop-CD may still have a considerable biasand the variance of the gradient estimate requires a smaller learning rate.Thus, despite its superior theoretical properties, it is not advisable to usepop-CD in its current form on large problems.
arxiv-14700-271 | NodIO, a JavaScript framework for volunteer-based evolutionary algorithms : first results | http://arxiv.org/pdf/1601.01607v1.pdf | author:Juan-J. Merelo, Mario García-Valdez, Pedro A. Castillo, Pablo García-Sánchez, P. de las Cuevas, Nuria Rico category:cs.DC cs.NE published:2016-01-07 summary:JavaScript is an interpreted language mainly known for its inclusion in webbrowsers, making them a container for rich Internet based applications. Thishas inspired its use, for a long time, as a tool for evolutionary algorithms,mainly so in browser-based volunteer computing environments. Several librarieshave also been published so far and are in use. However, the last years haveseen a resurgence of interest in the language, becoming one of the most popularand thus spawning the improvement of its implementations, which are now thefoundation of many new client-server applications. We present such anapplication for running distributed volunteer-based evolutionary algorithmexperiments, and we make a series of measurements to establish the speed ofJavaScript in evolutionary algorithms that can serve as a baseline forcomparison with other distributed computing experiments. These experiments usedifferent integer and floating point problems, and prove that the speed ofJavaScript is actually competitive with other languages commonly used by theevolutionary algorithm practitioner.
arxiv-14700-272 | Structured Prediction Energy Networks | http://arxiv.org/pdf/1511.06350v2.pdf | author:David Belanger, Andrew McCallum category:cs.LG published:2015-11-19 summary:We introduce structured prediction energy networks (SPENs), a flexibleframework for structured prediction. A deep architecture is used to define anenergy function of candidate labels, and then predictions are produced by usingback-propagation to iteratively optimize the energy with respect to the labels.This deep architecture captures dependencies between labels that would lead tointractable graphical models, and performs structure learning by automaticallylearning discriminative features of the structured output. One naturalapplication of our technique is multi-label classification, which traditionallyhas required strict prior assumptions about the interactions between labels toensure tractable learning and prediction problems. We are able to apply SPENsto multi-label problems with substantially larger label sets than previousapplications of structured prediction, while modeling high-order interactionsusing minimal structural assumptions. Overall, deep learning providesremarkable tools for learning features of the inputs to a prediction problem,and this work extends these techniques to learning features of structuredoutputs. Our experiments provide impressive performance on a variety ofbenchmark multi-label classification tasks, demonstrate that our technique canbe used to provide interpretable structure learning, and illuminate fundamentaltrade-offs between feed-forward and iterative structured prediction techniques.
arxiv-14700-273 | Deep Feature Learning for EEG Recordings | http://arxiv.org/pdf/1511.04306v4.pdf | author:Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn category:cs.NE cs.LG published:2015-11-13 summary:We introduce and compare several strategies for learning discriminativefeatures from electroencephalography (EEG) recordings using deep learningtechniques. EEG data are generally only available in small quantities, they arehigh-dimensional with a poor signal-to-noise ratio, and there is considerablevariability between individual subjects and recording sessions. Our proposedtechniques specifically address these challenges for feature learning.Cross-trial encoding forces auto-encoders to focus on features that are stableacross trials. Similarity-constraint encoders learn features that allow todistinguish between classes by demanding that two trials from the same classare more similar to each other than to trials from other classes. Thistuple-based training approach is especially suitable for small datasets.Hydra-nets allow for separate processing pathways adapting to subsets of adataset and thus combine the advantages of individual feature learning (betteradaptation of early, low-level processing) with group model training (bettergeneralization of higher-level processing in deeper layers). This way, modelscan, for instance, adapt to each subject individually to compensate fordifferences in spatial patterns due to anatomical differences or variance inelectrode positions. The different techniques are evaluated using the publiclyavailable OpenMIIR dataset of EEG recordings taken while participants listenedto and imagined music.
arxiv-14700-274 | Robust EM kernel-based methods for linear system identification | http://arxiv.org/pdf/1411.5915v3.pdf | author:Giulio Bottegal, Aleksandr Y. Aravkin, Håkan Hjalmarsson, Gianluigi Pillonetto category:cs.SY stat.ML published:2014-11-21 summary:Recent developments in system identifi?cation have brought attention toregularized kernel-based methods. This type of approach has been proven tocompare favorably with classic parametric methods. However, currentformulations are not robust with respect to outliers. In this paper, weintroduce a novel method to robustify kernel-based system identi?cationmethods. To this end, we model the output measurement noise using randomvariables with heavy-tailed probability density functions (pdfs), focusing onthe Laplacian and the Student's t distributions. Exploiting the representationof these pdfs as scale mixtures of Gaussians, we cast our systemidentifi?cation problem into a Gaussian process regression framework, whichrequires estimating a number of hyperparameters of the data size order. Toovercome this di?culty, we design a new maximum a posteriori (MAP) estimator ofthe hyperparameters, and solve the related optimization problem with a noveliterative scheme based on the Expectation-Maximization (EM) method. In presenceof outliers, tests on simulated data and on a real system show a substantialperformance improvement compared to currently used kernel-based methods forlinear system identifi?cation.
arxiv-14700-275 | State Space representation of non-stationary Gaussian Processes | http://arxiv.org/pdf/1601.01544v1.pdf | author:Alessio Benavoli, Marco Zaffalon category:cs.LG stat.ML published:2016-01-07 summary:The state space (SS) representation of Gaussian processes (GP) has recentlygained a lot of interest. The main reason is that it allows to compute GPsbased inferences in O(n), where $n$ is the number of observations. Thisimplementation makes GPs suitable for Big Data. For this reason, it isimportant to provide a SS representation of the most important kernels used inmachine learning. The aim of this paper is to show how to exploit the transientbehaviour of SS models to map non-stationary kernels to SS models.
arxiv-14700-276 | Resiliency of Deep Neural Networks under Quantization | http://arxiv.org/pdf/1511.06488v3.pdf | author:Wonyong Sung, Sungho Shin, Kyuyeon Hwang category:cs.LG cs.NE published:2015-11-20 summary:The complexity of deep neural network algorithms for hardware implementationcan be much lowered by optimizing the word-length of weights and signals.Direct quantization of floating-point weights, however, does not show goodperformance when the number of bits assigned is small. Retraining of quantizednetworks has been developed to relieve this problem. In this work, the effectsof retraining are analyzed for a feedforward deep neural network (FFDNN) and aconvolutional neural network (CNN). The network complexity is controlled toknow their effects on the resiliency of quantized networks by retraining. Thecomplexity of the FFDNN is controlled by varying the unit size in each hiddenlayer and the number of layers, while that of the CNN is done by modifying thefeature map configuration. We find that the performance gap between thefloating-point and the retrain-based ternary (+1, 0, -1) weight neural networksexists with a fair amount in 'complexity limited' networks, but the discrepancyalmost vanishes in fully complex networks whose capability is limited by thetraining data, rather than by the number of connections. This research showsthat highly complex DNNs have the capability of absorbing the effects of severeweight quantization through retraining, but connection limited networks areless resilient. This paper also presents the effective compression ratio toguide the trade-off between the network size and the precision when thehardware resource is limited.
arxiv-14700-277 | Leveraging Sentence-level Information with Encoder LSTM for Natural Language Understanding | http://arxiv.org/pdf/1601.01530v1.pdf | author:Gakuto Kurata, Bing Xiang, Bowen Zhou, Mo Yu category:cs.CL published:2016-01-07 summary:Recurrent Neural Network (RNN) and one of its specific architectures, LongShort-Term Memory (LSTM), have been widely used for sequence labeling. In thispaper, we first enhance LSTM-based sequence labeling to explicitly model labeldependencies. Then we propose another enhancement to incorporate the globalinformation spanning over the whole input sequence. The latter proposed method,encoder-labeler LSTM, first encodes the whole input sequence into a fixedlength vector with the encoder LSTM, and then uses this encoded vector as theinitial state of another LSTM for sequence labeling. Combining these methods,we can predict the label sequence with considering label dependencies andinformation of whole input sequence. In the experiments of a slot filling task,which is an essential component of natural language understanding, with usingthe standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%.
arxiv-14700-278 | Ensemble Methods of Classification for Power Systems Security Assessment | http://arxiv.org/pdf/1601.01675v1.pdf | author:Alexei Zhukov, Victor Kurbatsky, Nikita Tomin, Denis Sidorov, Daniil Panasetsky, Aoife Foley category:cs.AI cs.LG 68T05 published:2016-01-07 summary:One of the most promising approaches for complex technical systems analysisemploys ensemble methods of classification. Ensemble methods enable to build areliable decision rules for feature space classification in the presence ofmany possible states of the system. In this paper, novel techniques based ondecision trees are used for evaluation of the reliability of the regime ofelectric power systems. We proposed hybrid approach based on random forestsmodels and boosting models. Such techniques can be applied to predict theinteraction of increasing renewable power, storage devices and swiching ofsmart loads from intelligent domestic appliances, heaters and air-conditioningunits and electric vehicles with grid for enhanced decision making. Theensemble classification methods were tested on the modified 118-bus IEEE powersystem showing that proposed technique can be employed to examine whether thepower system is secured under steady-state operating conditions.
arxiv-14700-279 | Fast Kronecker product kernel methods via sampled vec trick | http://arxiv.org/pdf/1601.01507v1.pdf | author:Antti Airola, Tapio Pahikkala category:stat.ML cs.LG published:2016-01-07 summary:Kronecker product kernel provides the standard approach in the kernel methodsliterature for learning from pair-input data, where both data points andprediction tasks have their own feature representations. The methods allowsimultaneous generalization to both new tasks and data unobserved in thetraining set, a setting known as zero-shot or zero-data learning. Such asetting occurs in numerous applications, including drug-target interactionprediction, collaborative filtering and information retrieval. Efficienttraining algorithms based on the so-called vec trick, that makes use of thespecial structure of the Kronecker product, are known for the case where theoutput matrix for the training set is fully observed, i.e. the correct outputfor each data point-task combination is available. In this work we generalizethese results, proposing an efficient algorithm for sampled Kronecker productmultiplication, where only a subset of the full Kronecker product is computed.This allows us to derive a general framework for training Kronecker kernelmethods, as specific examples we implement Kronecker ridge regression andsupport vector machine algorithms. Experimental results demonstrate that theproposed approach leads to accurate models, while allowing order of magnitudeimprovements in training and prediction time.
arxiv-14700-280 | Measuring and Discovering Correlations in Large Data Sets | http://arxiv.org/pdf/1602.07960v1.pdf | author:Lijue Liu, Ming Li, Sha Wen category:stat.ME stat.ML published:2016-01-07 summary:In this paper, a class of statistics named ART (the alternant recursivetopology statistics) is proposed to measure the properties of correlationbetween two variables. A wide range of bi-variable correlations both linear andnonlinear can be evaluated by ART efficiently and equitably even if nothing isknown about the specific types of those relationships. ART compensates thedisadvantages of Reshef's model in which no polynomial time precise algorithmexists and the "local random" phenomenon can not be identified. As a class ofnonparametric exploration statistics, ART is applied for analyzing a dataset of10 American classical indexes, as a result, lots of bi-variable correlationsare discovered.
arxiv-14700-281 | Continuous parameter working memory in a balanced chaotic neural network | http://arxiv.org/pdf/1508.06944v3.pdf | author:Nimrod Shaham, Yoram Burak category:cs.NE q-bio.NC published:2015-08-27 summary:Working memory, the ability to maintain information over time scales greaterthan those characterizing single neurons, is essential to many brain functions.It remains unclear whether neural networks in the balanced state, an importantmodel for activity in the cortex, can support a continuum of stable states thatwould make it possible to store a continuous variable in working memory whilealso accounting for the stochastic behavior of single neurons. Here we proposea simple neural architecture that achieves this goal. We show analytically thatin the limit of an infinite network a continuous parameter can be storedindefinitely on a continuum of balanced states. For finite networks wecalculate the diffusivity along the attractor driven by the chaotic noise inthe network, and show that it is inversely proportional to the system size.Thus, for large enough (but realistic) neural population sizes, and withsuitable tuning of the network connections, it is possible to maintaincontinuous parameter values over time scales larger by several orders ofmagnitude than the single neuron time scale.
arxiv-14700-282 | A Submodule Clustering Method for Multi-way Data by Sparse and Low-Rank Representation | http://arxiv.org/pdf/1601.00149v2.pdf | author:Xinglin Piao, Yongli Hu, Junbin Gao, Yanfeng Sun, Zhouchen Lin category:cs.CV published:2016-01-02 summary:A new submodule clustering method via sparse and low-rank representation formulti-way data is proposed in this paper. Instead of reshaping multi-way datainto vectors, this method maintains their natural orders to preserve dataintrinsic structures, e.g., image data kept as matrices. To implementclustering, the multi-way data, viewed as tensors, are represented by theproposed tensor sparse and low-rank model to obtain its submodulerepresentation, called a free module, which is finally used for spectralclustering. The proposed method extends the conventional subspace clusteringmethod based on sparse and low-rank representation to multi-way data submoduleclustering by combining t-product operator. The new method is tested on severalpublic datasets, including synthetical data, video sequences and toy images.The experiments show that the new method outperforms the state-of-the-artmethods, such as Sparse Subspace Clustering (SSC), Low-Rank Representation(LRR), Ordered Subspace Clustering (OSC), Robust Latent Low Rank Representation(RobustLatLRR) and Sparse Submodule Clustering method (SSmC).
arxiv-14700-283 | Basic Level Categorization Facilitates Visual Object Recognition | http://arxiv.org/pdf/1511.04103v3.pdf | author:Panqu Wang, Garrison W. Cottrell category:cs.CV published:2015-11-12 summary:Recent advances in deep learning have led to significant progress in thecomputer vision field, especially for visual object recognition tasks. Thefeatures useful for object classification are learned by feed-forward deepconvolutional neural networks (CNNs) automatically, and they are shown to beable to predict and decode neural representations in the ventral visual pathwayof humans and monkeys. However, despite the huge amount of work on optimizingCNNs, there has not been much research focused on linking CNNs with guidingprinciples from the human visual cortex. In this work, we propose a networkoptimization strategy inspired by both of the developmental trajectory ofchildren's visual object recognition capabilities, and Bar (2003), whohypothesized that basic level information is carried in the fast magnocellularpathway through the prefrontal cortex (PFC) and then projected back to inferiortemporal cortex (IT), where subordinate level categorization is achieved. Weinstantiate this idea by training a deep CNN to perform basic level objectcategorization first, and then train it on subordinate level categorization. Weapply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%,demonstrating the effectiveness of the method. We also show that subsequenttransfer learning on smaller datasets gives superior results.
arxiv-14700-284 | A Deep Memory-based Architecture for Sequence-to-Sequence Learning | http://arxiv.org/pdf/1506.06442v4.pdf | author:Fandong Meng, Zhengdong Lu, Zhaopeng Tu, Hang Li, Qun Liu category:cs.CL cs.LG cs.NE published:2015-06-22 summary:We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequencelearning, which performs the task through a series of nonlinear transformationsfrom the representation of the input sequence (e.g., a Chinese sentence) to thefinal output sequence (e.g., translation to English). Inspired by the recentlyproposed Neural Turing Machine (Graves et al., 2014), we store the intermediaterepresentations in stacked layers of memories, and use read-write operations onthe memories to realize the nonlinear transformations between therepresentations. The types of transformations are designed in advance but theparameters are learned from data. Through layer-by-layer transformations,DEEPMEMORY can model complicated relations between sequences necessary forapplications such as machine translation between distant languages. Thearchitecture can be trained with normal back-propagation on sequenceto-sequencedata, and the learning can be easily scaled up to a large corpus. DEEPMEMORY isbroad enough to subsume the state-of-the-art neural translation model in(Bahdanau et al., 2015) as its special case, while significantly improving uponthe model with its deeper architecture. Remarkably, DEEPMEMORY, being purelyneural network-based, can achieve performance comparable to the traditionalphrase-based machine translation system Moses with a small vocabulary and amodest parameter size.
arxiv-14700-285 | Multilayer bootstrap networks | http://arxiv.org/pdf/1408.0848v6.pdf | author:Xiao-Lei Zhang category:cs.LG cs.NE stat.ML published:2014-08-05 summary:We describe a simple multilayer bootstrap network for unsuperviseddimensionality reduction that each layer of the network is a group of mutuallyindependent k-centers clusterings, and the centers of a clustering are randomlysampled data points. We further compress the network size of multilayerbootstrap network by a neural network in a pseudo supervised way forprediction. We report comparison results in data visualization, clustering, anddocument retrieval.
arxiv-14700-286 | Block-Diagonal Sparse Representation by Learning a Linear Combination Dictionary for Recognition | http://arxiv.org/pdf/1601.01432v1.pdf | author:Xinglin Piao, Yongli Hu, Yanfeng Sun, Junbin Gao, Baocai Yin category:cs.CV published:2016-01-07 summary:In a sparse representation based recognition scheme, it is critical to learna desired dictionary, aiming both good representational power anddiscriminative performance. In this paper, we propose a new dictionary learningmodel for recognition applications, in which three strategies are adopted toachieve these two objectives simultaneously. First, a block-diagonal constraintis introduced into the model to eliminate the correlation between classes andenhance the discriminative performance. Second, a low-rank term is adopted tomodel the coherence within classes for refining the sparse representation ofeach class. Finally, instead of using the conventional over-completedictionary, a specific dictionary constructed from the linear combination ofthe training samples is proposed to enhance the representational power of thedictionary and to improve the robustness of the sparse representation model.The proposed method is tested on several public datasets. The experimentalresults show the method outperforms most state-of-the-art methods.
arxiv-14700-287 | Mixture of Bilateral-Projection Two-dimensional Probabilistic Principal Component Analysis | http://arxiv.org/pdf/1601.01431v1.pdf | author:Fujiao Ju, Yanfeng Sun, Junbin Gao, Simeng Liu, Yongli Hu category:cs.CV published:2016-01-07 summary:The probabilistic principal component analysis (PPCA) is built upon a globallinear mapping, with which it is insufficient to model complex data variation.This paper proposes a mixture of bilateral-projection probabilistic principalcomponent analysis model (mixB2DPPCA) on 2D data. With multi-components in themixture, this model can be seen as a soft cluster algorithm and has capabilityof modeling data with complex structures. A Bayesian inference scheme has beenproposed based on the variational EM (Expectation-Maximization) approach forlearning model parameters. Experiments on some publicly available databasesshow that the performance of mixB2DPPCA has been largely improved, resulting inmore accurate reconstruction errors and recognition rates than the existingPCA-based algorithms.
arxiv-14700-288 | Stochastic Dykstra Algorithms for Metric Learning on Positive Semi-Definite Cone | http://arxiv.org/pdf/1601.01422v1.pdf | author:Tomoki Matsuzawa, Raissa Relator, Jun Sese, Tsuyoshi Kato category:cs.CV published:2016-01-07 summary:Recently, covariance descriptors have received much attention as powerfulrepresentations of set of points. In this research, we present a new metriclearning algorithm for covariance descriptors based on the Dykstra algorithm,in which the current solution is projected onto a half-space at each iteration,and runs at O(n^3) time. We empirically demonstrate that randomizing the orderof half-spaces in our Dykstra-based algorithm significantly accelerates theconvergence to the optimal solution. Furthermore, we show that our approachyields promising experimental results on pattern recognition tasks.
arxiv-14700-289 | A Diversity-Promoting Objective Function for Neural Conversation Models | http://arxiv.org/pdf/1510.03055v2.pdf | author:Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan category:cs.CL published:2015-10-11 summary:Sequence-to-sequence neural network models for generation of conversationalresponses tend to generate safe, commonplace responses (e.g., \textit{I don'tknow}) regardless of the input. We suggest that the traditional objectivefunction, i.e., the likelihood of output (responses) given input (messages) isunsuited to response generation tasks. Instead we propose using Maximum MutualInformation (MMI) as objective function in neural models. Experimental resultsdemonstrate that the proposed objective function produces more diverse,interesting, and appropriate responses, yielding substantive gains in \bleuscores on two conversational datasets.
arxiv-14700-290 | DOC: Deep OCclusion Estimation From A Single Image | http://arxiv.org/pdf/1511.06457v3.pdf | author:Peng Wang, Alan Yuille category:cs.CV cs.LG published:2015-11-20 summary:Recovering the occlusion relationships between objects is a fundamental humanvisual ability which yields important information about the 3D world. In thispaper we propose a deep network architecture, called DOC, which acts on asingle image, detects object boundaries and estimates the border ownership(i.e. which side of the boundary is foreground and which is background). Werepresent occlusion relations by a binary edge map, to indicate the objectboundary, and an occlusion orientation variable which is tangential to theboundary and whose direction specifies border ownership by a left-hand rule,see Fig.1. We train two related deep convolutional neural networks, called DOC,which exploit local and non-local image cues to estimate this representationand hence recover occlusion relations. In order to train and test DOC weconstruct a large-scale instance occlusion boundary dataset using PASCAL VOCimages, which we call the PASCAL instance occlusion dataset (PIOD). Thiscontains 10,000 images and hence is two orders of magnitude larger thanexisting occlusion datasets for outdoor images. We test two variants of DOC onPIOD and on the BSDS occlusion dataset and show they outperformstate-of-the-art methods typically by more than 5AP. Finally, we performnumerous experiments investigating multiple settings of DOC and transferbetween BSDS and PIOD, which provides more insights for further study ofocclusion estimation.
arxiv-14700-291 | Learning Kernels for Structured Prediction using Polynomial Kernel Transformations | http://arxiv.org/pdf/1601.01411v1.pdf | author:Chetan Tonde, Ahmed Elgammal category:cs.LG stat.ML published:2016-01-07 summary:Learning the kernel functions used in kernel methods has been a vastlyexplored area in machine learning. It is now widely accepted that to obtain'good' performance, learning a kernel function is the key challenge. In thiswork we focus on learning kernel representations for structured regression. Wepropose use of polynomials expansion of kernels, referred to as Schoenbergtransforms and Gegenbaur transforms, which arise from the seminal result ofSchoenberg (1938). These kernels can be thought of as polynomial combination ofinput features in a high dimensional reproducing kernel Hilbert space (RKHS).We learn kernels over input and output for structured data, such that,dependency between kernel features is maximized. We use Hilbert-SchmidtIndependence Criterion (HSIC) to measure this. We also give an efficient,matrix decomposition-based algorithm to learn these kernel transformations, anddemonstrate state-of-the-art results on several real-world datasets.
arxiv-14700-292 | Angrier Birds: Bayesian reinforcement learning | http://arxiv.org/pdf/1601.01297v2.pdf | author:Imanol Arrieta Ibarra, Bernardo Ramos, Lars Roemheld category:cs.AI cs.LG published:2016-01-06 summary:We train a reinforcement learner to play a simplified version of the gameAngry Birds. The learner is provided with a game state in a manner similar tothe output that could be produced by computer vision algorithms. We improve onthe efficiency of regular {\epsilon}-greedy Q-Learning with linear functionapproximation through more systematic exploration in Randomized Least SquaresValue Iteration (RLSVI), an algorithm that samples its policy from a posteriordistribution on optimal policies. With larger state-action spaces, efficientexploration becomes increasingly important, as evidenced by the faster learningin RLSVI.
arxiv-14700-293 | Scalable and Accurate Online Feature Selection for Big Data | http://arxiv.org/pdf/1511.09263v2.pdf | author:Kui Yu, Xindong Wu, Wei Ding, Jian Pei category:cs.LG published:2015-11-30 summary:Feature selection is important in many big data applications. There are atleast two critical challenges. Firstly, in many applications, thedimensionality is extremely high, in millions, and keeps growing. Secondly,feature selection has to be highly scalable, preferably in an online mannersuch that each feature can be processed in a sequential scan. In this paper, wedevelop SAOLA, a Scalable and Accurate OnLine Approach for feature selection.With a theoretical analysis on bounds of the pairwise correlations betweenfeatures, SAOLA employs novel online pairwise comparison techniques to addressthe two challenges and maintain a parsimonious model over time in an onlinemanner. Furthermore, to tackle the dimensionality that arrives by groups, weextend our SAOLA algorithm, and then propose a novel group-SAOLA algorithm foronline group feature selection. The group-SAOLA algorithm can online maintain aset of feature groups that is sparse at the level of both groups and individualfeatures simultaneously. An empirical study using a series of benchmark realdata sets shows that our two algorithms, SAOLA and group-SAOLA, are scalable ondata sets of extremely high dimensionality, and have superior performance overthe state-of-the-art feature selection methods.
arxiv-14700-294 | Empirical performance upper bounds for image and video captioning | http://arxiv.org/pdf/1511.04590v4.pdf | author:Li Yao, Nicolas Ballas, Kyunghyun Cho, John R. Smith, Yoshua Bengio category:cs.CV cs.CL stat.ML published:2015-11-14 summary:The task of associating images and videos with a natural language descriptionhas attracted a great amount of attention recently. Rapid progress has beenmade in terms of both developing novel algorithms and releasing new datasets.Indeed, the state-of-the-art results on some of the standard datasets have beenpushed into the regime where it has become more and more difficult to makesignificant improvements. Instead of proposing new models, this workinvestigates the possibility of empirically establishing performance upperbounds on various visual captioning datasets without extra data labellingeffort or human evaluation. In particular, it is assumed that visual captioningis decomposed into two steps: from visual inputs to visual concepts, and fromvisual concepts to natural language descriptions. One would be able to obtainan upper bound when assuming the first step is perfect and only requiringtraining a conditional language model for the second step. We demonstrate theconstruction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combinationof M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we usedfor visual concept extraction in the first step and the simplicity of thelanguage model for the second step, we show that current state-of-the-artmodels fall short when being compared with the learned upper bounds.Furthermore, with such a bound, we quantify several important factorsconcerning image and video captioning: the number of visual concepts capturedby different models, the trade-off between the amount of visual elementscaptured and their accuracy, and the intrinsic difficulty and blessing ofdifferent datasets.
arxiv-14700-295 | Bayesian Non-Negative Matrix Factorization | http://arxiv.org/pdf/1601.01345v1.pdf | author:Pierre Alquier, Benjamin Guedj category:stat.ML math.ST stat.TH published:2016-01-06 summary:The aim of this paper is to provide some theoretical understanding ofBayesian non-negative matrix factorization methods, along with practicalimplementations. We provide a sharp oracle inequality for a quasi-Bayesianestimator, also known as the exponentially weighted aggregate (Dalalyan andTsybakov, 2008). This result holds for a very general class of priordistributions and shows how the prior affects the rate of convergence. We thendiscuss possible algorithms. A natural choice in Bayesian statistics is theGibbs sampler, used for example in Salakhutdinov and Mnih (2008). Thisalgorithm is asymptotically exact, yet it suffers from the fact that theconvergence might be very slow on large datasets. When faced with massivedatasets, a more efficient path is to use approximate methods based onoptimisation algorithms: we here describe a blockwise gradient descent which isa Bayesian version of the algorithm in Xu et al. (2012). Here again, thegeneral form of the algorithm helps to understand the role of the prior, andsome priors will clearly lead to more efficient (i.e., faster) implementations.We end the paper with a short simulation study and an application to finance.These numerical studies support our claim that the reconstruction of the matrixis usually not very sensitive to the choice of the hyperparameters whereas rankidentification is.
arxiv-14700-296 | Quality Adaptive Low-Rank Based JPEG Decoding with Applications | http://arxiv.org/pdf/1601.01339v1.pdf | author:Xiao Shu, Xiaolin Wu category:cs.CV published:2016-01-06 summary:Small compression noises, despite being transparent to human eyes, canadversely affect the results of many image restoration processes, if leftunaccounted for. Especially, compression noises are highly detrimental toinverse operators of high-boosting (sharpening) nature, such as deblurring andsuperresolution against a convolution kernel. By incorporating the non-linearDCT quantization mechanism into the formulation for image restoration, wepropose a new sparsity-based convex programming approach for joint compressionnoise removal and image restoration. Experimental results demonstratesignificant performance gains of the new approach over existing imagerestoration methods.
arxiv-14700-297 | Distributed Stochastic Variance Reduced Gradient Methods and A Lower Bound for Communication Complexity | http://arxiv.org/pdf/1507.07595v2.pdf | author:Jason D. Lee, Qihang Lin, Tengyu Ma, Tianbao Yang category:math.OC cs.LG stat.ML published:2015-07-27 summary:We study distributed optimization algorithms for minimizing the average ofconvex functions. The applications include empirical risk minimization problemsin statistical machine learning where the datasets are large and have to bestored on different machines. We design a distributed stochastic variancereduced gradient algorithm that, under certain conditions on the conditionnumber, simultaneously achieves the optimal parallel runtime, amount ofcommunication and rounds of communication among all distributed first-ordermethods up to constant factors. Our method and its accelerated extension alsooutperform existing distributed algorithms in terms of the rounds ofcommunication as long as the condition number is not too large compared to thesize of data in each machine. We also prove a lower bound for the number ofrounds of communication for a broad class of distributed first-order methodsincluding the proposed algorithms in this paper. We show that our accelerateddistributed stochastic variance reduced gradient algorithm achieves this lowerbound so that it uses the fewest rounds of communication among all distributedfirst-order algorithms.
arxiv-14700-298 | Language to Logical Form with Neural Attention | http://arxiv.org/pdf/1601.01280v1.pdf | author:Li Dong, Mirella Lapata category:cs.CL published:2016-01-06 summary:Semantic parsing aims at mapping natural language to machine interpretablemeaning representations. Traditional approaches rely on high-quality lexicons,manually-built templates, and linguistic features which are either domain- orrepresentation-specific. In this paper, we present a general method based on anattention-enhanced sequence-to-sequence model. We encode input sentences intovector representations using recurrent neural networks, and generate theirlogical forms by conditioning the output on the encoding vectors. The model istrained in an end-to-end fashion to maximize the likelihood of target logicalforms given the natural language inputs. Experimental results on four datasetsshow that our approach performs competitively without using hand-engineeredfeatures and is easy to adapt across domains and meaning representations.
arxiv-14700-299 | Shape Animation with Combined Captured and Simulated Dynamics | http://arxiv.org/pdf/1601.01232v1.pdf | author:Benjamin Allain, Li Wang, Jean-Sebastien Franco, Franck Hetroy, Edmond Boyer category:cs.GR cs.CV published:2016-01-06 summary:We present a novel volumetric animation generation framework to create newtypes of animations from raw 3D surface or point cloud sequence of capturedreal performances. The framework considers as input time incoherent 3Dobservations of a moving shape, and is thus particularly suitable for theoutput of performance capture platforms. In our system, a suitable virtualrepresentation of the actor is built from real captures that allows seamlesscombination and simulation with virtual external forces and objects, in whichthe original captured actor can be reshaped, disassembled or reassembled fromuser-specified virtual physics. Instead of using the dominant surface-basedgeometric representation of the capture, which is less suitable for volumetriceffects, our pipeline exploits Centroidal Voronoi tessellation decompositionsas unified volumetric representation of the real captured actor, which we showcan be used seamlessly as a building block for all processing stages, fromcapture and tracking to virtual physic simulation. The representation makes nohuman specific assumption and can be used to capture and re-simulate the actorwith props or other moving scenery elements. We demonstrate the potential ofthis pipeline for virtual reanimation of a real captured event with variousunprecedented volumetric visual effects, such as volumetric distortion,erosion, morphing, gravity pull, or collisions.
arxiv-14700-300 | Adaptive and Efficient Nonlinear Channel Equalization for Underwater Acoustic Communication | http://arxiv.org/pdf/1601.01218v1.pdf | author:Dariush Kari, Nuri Denizcan Vanli, Suleyman Serdar Kozat category:cs.LG cs.IT cs.SD math.IT published:2016-01-06 summary:We investigate underwater acoustic (UWA) channel equalization and introducehierarchical and adaptive nonlinear channel equalization algorithms that arehighly efficient and provide significantly improved bit error rate (BER)performance. Due to the high complexity of nonlinear equalizers and poorperformance of linear ones, to equalize highly difficult underwater acousticchannels, we employ piecewise linear equalizers. However, in order to achievethe performance of the best piecewise linear model, we use a tree structure tohierarchically partition the space of the received signal. Furthermore, theequalization algorithm should be completely adaptive, since due to the highlynon-stationary nature of the underwater medium, the optimal MSE equalizer aswell as the best piecewise linear equalizer changes in time. To this end, weintroduce an adaptive piecewise linear equalization algorithm that not onlyadapts the linear equalizer at each region but also learns the completehierarchical structure with a computational complexity only polynomial in thenumber of nodes of the tree. Furthermore, our algorithm is constructed todirectly minimize the final squared error without introducing any ad-hocparameters. We demonstrate the performance of our algorithms through highlyrealistic experiments performed on accurately simulated underwater acousticchannels.
