arxiv-2400-1 | Feature grouping from spatially constrained multiplicative interaction | http://arxiv.org/pdf/1301.3391v3.pdf | author:Felix Bauer, Roland Memisevic category:cs.LG I.2.6 published:2013-01-15 summary:We present a feature learning model that learns to encode relationshipsbetween images. The model is defined as a Gated Boltzmann Machine, which isconstrained such that hidden units that are nearby in space can gate eachother's connections. We show how frequency/orientation "columns" as well astopographic filter maps follow naturally from training the model on imagepairs. The model also helps explain why square-pooling models yield featuregroups with similar grouping properties. Experimental results on syntheticimage transformations show that spatially constrained gating is an effectiveway to reduce the number of parameters and thereby to regularize atransformation-learning model.
arxiv-2400-2 | Refinement revisited with connections to Bayes error, conditional entropy and calibrated classifiers | http://arxiv.org/pdf/1303.2517v1.pdf | author:Hamed Masnadi-Shirazi category:stat.ML published:2013-03-11 summary:The concept of refinement from probability elicitation is considered forproper scoring rules. Taking directions from the axioms of probability,refinement is further clarified using a Hilbert space interpretation andreformulated into the underlying data distribution setting where connections tomaximal marginal diversity and conditional entropy are considered and used toderive measures that provide arbitrarily tight bounds on the Bayes error.Refinement is also reformulated into the classifier output setting and itsconnections to calibrated classifiers and proper margin losses are established.
arxiv-2400-3 | Monte-Carlo utility estimates for Bayesian reinforcement learning | http://arxiv.org/pdf/1303.2506v1.pdf | author:Christos Dimitrakakis category:cs.LG stat.ML published:2013-03-11 summary:This paper introduces a set of algorithms for Monte-Carlo Bayesianreinforcement learning. Firstly, Monte-Carlo estimation of upper bounds on theBayes-optimal value function is employed to construct an optimistic policy.Secondly, gradient-based algorithms for approximate upper and lower bounds areintroduced. Finally, we introduce a new class of gradient algorithms forBayesian Bellman error minimisation. We theoretically show that the gradientmethods are sound. Experimentally, we demonstrate the superiority of the upperbound method in terms of reward obtained. However, we also show that theBayesian Bellman error method is a close second, despite its significantcomputational simplicity.
arxiv-2400-4 | Visualizing and Interacting with Concept Hierarchies | http://arxiv.org/pdf/1303.2488v1.pdf | author:Michel Crampes, Michel Plantié category:stat.ML published:2013-03-11 summary:Concept Hierarchies and Formal Concept Analysis are theoretically wellgrounded and largely experimented methods. They rely on line diagrams calledGalois lattices for visualizing and analysing object-attribute sets. Galoislattices are visually seducing and conceptually rich for experts. However theypresent important drawbacks due to their concept oriented overall structure:analysing what they show is difficult for non experts, navigation iscumbersome, interaction is poor, and scalability is a deep bottleneck forvisual interpretation even for experts. In this paper we introduce semanticprobes as a means to overcome many of these problems and extend usability andapplication possibilities of traditional FCA visualization methods. Semanticprobes are visual user centred objects which extract and organize reducedGalois sub-hierarchies. They are simpler, clearer, and they provide a betternavigation support through a rich set of interaction possibilities. Since probedriven sub-hierarchies are limited to users focus, scalability is under controland interpretation is facilitated. After some successful experiments, severalapplications are being developed with the remaining problem of finding acompromise between simplicity and conceptual expressivity.
arxiv-2400-5 | Optimization viewpoint on Kalman smoothing, with applications to robust and sparse estimation | http://arxiv.org/pdf/1303.1993v2.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:math.OC stat.CO stat.ML 62F35, 65K10 published:2013-03-08 summary:In this paper, we present the optimization formulation of the Kalmanfiltering and smoothing problems, and use this perspective to develop a varietyof extensions and applications. We first formulate classic Kalman smoothing asa least squares problem, highlight special structure, and show that the classicfiltering and smoothing algorithms are equivalent to a particular algorithm forsolving this problem. Once this equivalence is established, we presentextensions of Kalman smoothing to systems with nonlinear process andmeasurement models, systems with linear and nonlinear inequality constraints,systems with outliers in the measurements or sudden changes in the state, andsystems where the sparsity of the state sequence must be accounted for. Allextensions preserve the computational efficiency of the classic algorithms, andmost of the extensions are illustrated with numerical examples, which are partof an open source Kalman smoothing Matlab/Octave package.
arxiv-2400-6 | A Low-Complexity Algorithm for Static Background Estimation from Cluttered Image Sequences in Surveillance Contexts | http://arxiv.org/pdf/1303.2465v1.pdf | author:Vikas Reddy, Conrad Sanderson, Brian C. Lovell category:cs.CV published:2013-03-11 summary:For the purposes of foreground estimation, the true background model isunavailable in many practical circumstances and needs to be estimated fromcluttered image sequences. We propose a sequential technique for staticbackground estimation in such conditions, with low computational and memoryrequirements. Image sequences are analysed on a block-by-block basis. For eachblock location a representative set is maintained which contains distinctblocks obtained along its temporal line. The background estimation is carriedout in a Markov Random Field framework, where the optimal labelling solution iscomputed using iterated conditional modes. The clique potentials are computedbased on the combined frequency response of the candidate block and itsneighbourhood. It is assumed that the most appropriate block results in thesmoothest response, indirectly enforcing the spatial continuity of structureswithin a scene. Experiments on real-life surveillance videos demonstrate thatthe proposed method obtains considerably better background estimates (bothqualitatively and quantitatively) than median filtering and the recentlyproposed "intervals of stable intensity" method. Further experiments on theWallflower dataset suggest that the combination of the proposed method with aforeground segmentation algorithm results in improved foreground segmentation.
arxiv-2400-7 | Using qualia information to identify lexical semantic classes in an unsupervised clustering task | http://arxiv.org/pdf/1303.2449v1.pdf | author:Lauren Romeo, Sara Mendes, Núria Bel category:cs.CL published:2013-03-11 summary:Acquiring lexical information is a complex problem, typically approached byrelying on a number of contexts to contribute information for classification.One of the first issues to address in this domain is the determination of suchcontexts. The work presented here proposes the use of automatically obtainedFORMAL role descriptors as features used to draw nouns from the same lexicalsemantic class together in an unsupervised clustering task. We have dealt withthree lexical semantic classes (HUMAN, LOCATION and EVENT) in English. Theresults obtained show that it is possible to discriminate between elements fromdifferent lexical semantic classes using only FORMAL role information, hencevalidating our initial hypothesis. Also, iterating our method accuratelyaccounts for fine-grained distinctions within lexical classes, namelydistinctions involving ambiguous expressions. Moreover, a filtering andbootstrapping strategy employed in extracting FORMAL role descriptors proved tominimize effects of sparse data and noise in our task.
arxiv-2400-8 | Automatic Detection of Non-deverbal Event Nouns for Quick Lexicon Production | http://arxiv.org/pdf/1303.2448v1.pdf | author:Núria Bel, Maria Coll, Gabriela Resnik category:cs.CL published:2013-03-11 summary:In this work we present the results of our experimental work on thedevelop-ment of lexical class-based lexica by automatic means. The objective isto as-sess the use of linguistic lexical-class based information as a featureselection methodology for the use of classifiers in quick lexical development.The results show that the approach can help in re-ducing the human effortrequired in the development of language resources sig-nificantly.
arxiv-2400-9 | A Multilingual Semantic Wiki Based on Attempto Controlled English and Grammatical Framework | http://arxiv.org/pdf/1303.4293v1.pdf | author:Kaarel Kaljurand, Tobias Kuhn category:cs.CL cs.HC published:2013-03-11 summary:We describe a semantic wiki system with an underlying controlled naturallanguage grammar implemented in Grammatical Framework (GF). The grammarrestricts the wiki content to a well-defined subset of Attempto ControlledEnglish (ACE), and facilitates a precise bidirectional automatic translationbetween ACE and language fragments of a number of other natural languages,making the wiki content accessible multilingually. Additionally, our approachallows for automatic translation into the Web Ontology Language (OWL), whichenables automatic reasoning over the wiki content. The developed wikienvironment thus allows users to build, query and view OWL knowledge bases viaa user-friendly multilingual natural language interface. As a further feature,the underlying multilingual grammar is integrated into the wiki and can becollaboratively edited to extend the vocabulary of the wiki or even customizeits sentence structures. This work demonstrates the combination of the existingtechnologies of Attempto Controlled English and Grammatical Framework, and isimplemented as an extension of the existing semantic wiki engine AceWiki.
arxiv-2400-10 | Voxel-wise Weighted MR Image Enhancement using an Extended Neighborhood Filter | http://arxiv.org/pdf/1303.2439v1.pdf | author:Joseph Suresh Paul, Joshin John Mathew, Souparnika Kandoth Naroth, Chandrasekar Kesavadas category:cs.CV published:2013-03-11 summary:We present an edge preserving and denoising filter for enhancing the featuresin images, which contain an ROI having a narrow spatial extent. Typicalexamples include angiograms, or ROI spatially distributed in multiple locationsand contained within an outlying region, such as in multiple-sclerosis. Thefiltering involves determination of multiplicative weights in the spatialdomain using an extended set of neighborhood directions. Equivalently, thefiltering operation may be interpreted as a combination of directional filtersin the frequency domain, with selective weighting for spatial frequenciescontained within each direction. The advantages of the proposed filter incomparison to specialized non-linear filters, which operate on diffusionprinciple, are illustrated using numerical phantom data. The performanceevaluation is carried out on simulated images from BrainWeb database formultiple-sclerosis, acute ischemic stroke using clinically acquired FLAIRimages and MR angiograms.
arxiv-2400-11 | Least-Squares FIR Models of Low-Resolution MR data for Efficient Phase-Error Compensation with Simultaneous Artefact Removal | http://arxiv.org/pdf/1303.2437v1.pdf | author:Joseph Suresh Paul, Uma Krishna Swamy Pillai, Nyjin Thomas category:cs.CV published:2013-03-11 summary:Signal space models in both phase-encode, and frequency-encode directions arepresented for extrapolation of 2D partial kspace. Using the boxcarrepresentation of low-resolution spatial data, and a geometrical representationof signal space vectors in both positive and negative phase-encode directions,a robust predictor is constructed using a series of signal space projections.Compared to some of the existing phase-correction methods that requireacquisition of a pre-determined set of fractional kspace lines, the proposedpredictor is found to be more efficient, due to its capability of exhibiting anequivalent degree of performance using only half the number of fractionallines. Robust filtering of noisy data is achieved using a second signal spacemodel in the frequency-encode direction, bypassing the requirement of a priorhighpass filtering operation. The signal space is constructed from FourierTransformed samples of each row in the low-resolution image. A set of FIRfilters are estimated by fitting a least squares model to this signal space.Partial kspace extrapolation using the FIR filters is shown to result inartifact-free reconstruction, particularly in respect of Gibbs ringing andstreaking type artifacts.
arxiv-2400-12 | Improved Performance of Unsupervised Method by Renovated K-Means | http://arxiv.org/pdf/1304.0725v1.pdf | author:P. Ashok, G. M Kadhar Nawaz, E. Elayaraja, V. Vadivel category:cs.LG cs.CV stat.ML published:2013-03-11 summary:Clustering is a separation of data into groups of similar objects. Everygroup called cluster consists of objects that are similar to one another anddissimilar to objects of other groups. In this paper, the K-Means algorithm isimplemented by three distance functions and to identify the optimal distancefunction for clustering methods. The proposed K-Means algorithm is comparedwith K-Means, Static Weighted K-Means (SWK-Means) and Dynamic Weighted K-Means(DWK-Means) algorithm by using Davis Bouldin index, Execution Time andIteration count methods. Experimental results show that the proposed K-Meansalgorithm performed better on Iris and Wine dataset when compared with otherthree clustering methods.
arxiv-2400-13 | Linear NDCG and Pair-wise Loss | http://arxiv.org/pdf/1303.2417v1.pdf | author:Xiao-Bo Jin, Guang-Gang Geng category:cs.LG stat.ML published:2013-03-11 summary:Linear NDCG is used for measuring the performance of the Web content qualityassessment in ECML/PKDD Discovery Challenge 2010. In this paper, we will provethat the DCG error equals a new pair-wise loss.
arxiv-2400-14 | State estimation under non-Gaussian Levy noise: A modified Kalman filtering method | http://arxiv.org/pdf/1303.2395v1.pdf | author:Xu Sun, Jinqiao Duan, Xiaofan Li, Xiangjun Wang category:math.DS cs.IT cs.LG math.IT math.PR stat.ML published:2013-03-10 summary:The Kalman filter is extensively used for state estimation for linear systemsunder Gaussian noise. When non-Gaussian L\'evy noise is present, theconventional Kalman filter may fail to be effective due to the fact that thenon-Gaussian L\'evy noise may have infinite variance. A modified Kalman filterfor linear systems with non-Gaussian L\'evy noise is devised. It workseffectively with reasonable computational cost. Simulation results arepresented to illustrate this non-Gaussian filtering method.
arxiv-2400-15 | Image compression using anti-forensics method | http://arxiv.org/pdf/1303.2330v1.pdf | author:M. S. Sreelakshmi, D. Venkataraman category:cs.MM cs.CV published:2013-03-10 summary:A large number of image forensics methods are available which are capable ofidentifying image tampering. But these techniques are not capable of addressingthe anti-forensics method which is able to hide the trace of image tampering.In this paper anti-forensics method for digital image compression has beenproposed. This anti-forensics method is capable of removing the traces of imagecompression. Additionally, technique is also able to remove the traces ofblocking artifact that are left by image compression algorithms that divide animage into segments during compression process. This method is targeted toremove the compression fingerprints of JPEG compression.
arxiv-2400-16 | The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains | http://arxiv.org/pdf/1211.0053v2.pdf | author:David I Shuman, Sunil K. Narang, Pascal Frossard, Antonio Ortega, Pierre Vandergheynst category:cs.DM cs.LG cs.SI published:2012-10-31 summary:In applications such as social, energy, transportation, sensor, and neuronalnetworks, high-dimensional data naturally reside on the vertices of weightedgraphs. The emerging field of signal processing on graphs merges algebraic andspectral graph theoretic concepts with computational harmonic analysis toprocess such signals on graphs. In this tutorial overview, we outline the mainchallenges of the area, discuss different ways to define graph spectraldomains, which are the analogues to the classical frequency domain, andhighlight the importance of incorporating the irregular structures of graphdata domains when processing signals on graphs. We then review methods togeneralize fundamental operations such as filtering, translation, modulation,dilation, and downsampling to the graph setting, and survey the localized,multiscale transforms that have been proposed to efficiently extractinformation from high-dimensional data on graphs. We conclude with a briefdiscussion of open issues and possible extensions.
arxiv-2400-17 | Mini-Batch Primal and Dual Methods for SVMs | http://arxiv.org/pdf/1303.2314v1.pdf | author:Martin Takáč, Avleen Bijral, Peter Richtárik, Nathan Srebro category:cs.LG math.OC published:2013-03-10 summary:We address the issue of using mini-batches in stochastic optimization ofSVMs. We show that the same quantity, the spectral norm of the data, controlsthe parallelization speedup obtained for both primal stochastic subgradientdescent (SGD) and stochastic dual coordinate ascent (SCDA) methods and use itto derive novel variants of mini-batched SDCA. Our guarantees for both methodsare expressed in terms of the original nonsmooth primal problem based on thehinge-loss.
arxiv-2400-18 | Intelligent Approaches to interact with Machines using Hand Gesture Recognition in Natural way: A Survey | http://arxiv.org/pdf/1303.2292v1.pdf | author:Ankit Chaudhary, J. L. Raheja, Karen Das, Sonia Raheja category:cs.HC cs.CV published:2013-03-10 summary:Hand gestures recognition (HGR) is one of the main areas of research for theengineers, scientists and bioinformatics. HGR is the natural way of HumanMachine interaction and today many researchers in the academia and industry areworking on different application to make interactions more easy, natural andconvenient without wearing any extra device. HGR can be applied from gamescontrol to vision enabled robot control, from virtual reality to smart homesystems. In this paper we are discussing work done in the area of hand gesturerecognition where focus is on the intelligent approaches including softcomputing based methods like artificial neural network, fuzzy logic, geneticalgorithms etc. The methods in the preprocessing of image for segmentation andhand image construction also taken into study. Most researchers used fingertipsfor hand detection in appearance based modeling. Finally the comparison ofresults given by different researchers is also presented.
arxiv-2400-19 | Deterministic Sequencing of Exploration and Exploitation for Multi-Armed Bandit Problems | http://arxiv.org/pdf/1106.6104v3.pdf | author:Sattar Vakili, Keqin Liu, Qing Zhao category:math.OC cs.LG cs.SY math.PR math.ST stat.TH published:2011-06-30 summary:In the Multi-Armed Bandit (MAB) problem, there is a given set of arms withunknown reward models. At each time, a player selects one arm to play, aimingto maximize the total expected reward over a horizon of length T. An approachbased on a Deterministic Sequencing of Exploration and Exploitation (DSEE) isdeveloped for constructing sequential arm selection policies. It is shown thatfor all light-tailed reward distributions, DSEE achieves the optimallogarithmic order of the regret, where regret is defined as the total expectedreward loss against the ideal case with known reward models. For heavy-tailedreward distributions, DSEE achieves O(T^1/p) regret when the moments of thereward distributions exist up to the pth order for 1<p<=2 and O(T^1/(1+p/2))for p>2. With the knowledge of an upperbound on a finite moment of theheavy-tailed reward distributions, DSEE offers the optimal logarithmic regretorder. The proposed DSEE approach complements existing work on MAB by providingcorresponding results for general reward distributions. Furthermore, with aclearly defined tunable parameter-the cardinality of the exploration sequence,the DSEE approach is easily extendable to variations of MAB, including MAB withvarious objectives, decentralized MAB with multiple players and incompletereward observations under collisions, MAB with unknown Markov dynamics, andcombinatorial MAB with dependent arms that often arise in network optimizationproblems such as the shortest path, the minimum spanning, and the dominatingset problems under unknown random weights.
arxiv-2400-20 | Clustering on Multi-Layer Graphs via Subspace Analysis on Grassmann Manifolds | http://arxiv.org/pdf/1303.2221v1.pdf | author:Xiaowen Dong, Pascal Frossard, Pierre Vandergheynst, Nikolai Nefedov category:cs.LG cs.CV cs.SI stat.ML published:2013-03-09 summary:Relationships between entities in datasets are often of multiple nature, likegeographical distance, social relationships, or common interests among peoplein a social network, for example. This information can naturally be modeled bya set of weighted and undirected graphs that form a global multilayer graph,where the common vertex set represents the entities and the edges on differentlayers capture the similarities of the entities in term of the differentmodalities. In this paper, we address the problem of analyzing multi-layergraphs and propose methods for clustering the vertices by efficiently mergingthe information provided by the multiple modalities. To this end, we propose tocombine the characteristics of individual graph layers using tools fromsubspace analysis on a Grassmann manifold. The resulting combination can thenbe viewed as a low dimensional representation of the original data whichpreserves the most important information from diverse relationships betweenentities. We use this information in new clustering methods and test ouralgorithm on several synthetic and real world datasets where we demonstratesuperior or competitive performances compared to baseline and state-of-the-arttechniques. Our generic framework further extends to numerous analysis andlearning problems that involve different types of information on graphs.
arxiv-2400-21 | Expensive Optimisation: A Metaheuristics Perspective | http://arxiv.org/pdf/1303.2215v1.pdf | author:Maumita Bhattacharya category:cs.NE 97R40 published:2013-03-09 summary:Stochastic, iterative search methods such as Evolutionary Algorithms (EAs)are proven to be efficient optimizers. However, they require evaluation of thecandidate solutions which may be prohibitively expensive in many real worldoptimization problems. Use of approximate models or surrogates is beingexplored as a way to reduce the number of such evaluations. In this paper weinvestigated three such methods. The first method (DAFHEA) partially replacesan expensive function evaluation by its approximate model. The approximation isrealized with support vector machine (SVM) regression models. The second method(DAFHEA II) is an enhancement on DAFHEA to accommodate for uncertainenvironments. The third one uses surrogate ranking with preference learning orordinal regression. The fitness of the candidates is estimated by modelingtheir rank. The techniques' performances on some of the benchmark numericaloptimization problems have been reported. The comparative benefits andshortcomings of both techniques have been identified.
arxiv-2400-22 | Medical Information Embedding in Compressed Watermarked Intravascular Ultrasound Video | http://arxiv.org/pdf/1303.2211v1.pdf | author:Nilanjan Dey, Suvojit Acharjee, Debalina Biswas, Achintya Das, Sheli Sinha Chaudhuri category:cs.MM cs.CV published:2013-03-09 summary:In medical field, intravascular ultrasound (IVUS) is a tomographic imagingmodality, which can identify the boundaries of different layers of bloodvessels. IVUS can detect myocardial infarction (heart attack) that remainsignored and unattended when only angioplasty is done. During the past decade,it became easier for some individuals or groups to copy and transmits digitalinformation without the permission of the owner. For increasing authenticationand security of copyrights, digital watermarking, an information hidingtechnique, was introduced. Achieving watermarking technique with lesser amountof distortion in biomedical data is a challenging task. Watermark can beembedded into an image or in a video. As video data is a huge amount ofinformation, therefore a large storage area is needed which is not feasible. Inthis case motion vector based video compression is done to reduce size. In thispresent paper, an Electronic Patient Record (EPR) is embedded as watermarkwithin an IVUS video and then motion vector is calculated. This proposed methodproves robustness as the extracted watermark has good PSNR value and less MSE.
arxiv-2400-23 | Embedding of Blink Frequency in Electrooculography Signal using Difference Expansion based Reversible Watermarking Technique | http://arxiv.org/pdf/1304.2310v1.pdf | author:Nilanjan Dey, Prasenjit Maji, Poulami Das, Shouvik Biswas, Achintya Das, Sheli Sinha Chaudhuri category:cs.CV cs.IR published:2013-03-09 summary:In the past few years, like other fields, rapid expansion of digitization andglobalization has influenced the medical field as well. For progress ofdiagnostic results most of the reputed hospitals and diagnostic centres allover the world have started exchanging medical information. In this proposedmethod, the calculated diagnostic parametric values of the originalElectrooculography (EOG) signal are embedded as a watermark by using DifferenceExpansion (DE) algorithm based reversible watermarking technique. The extractedwatermark provides the required parametric values at the recipient end withoutany post computation of the recovered EOG signal. By computing the parametricvalues from the recovered signal, the integrity of the extracted watermark canbe validated. The time domain features of EOG signal are calculated for thegeneration of watermark. In the current work, various features are studied andtwo major features related to blink frequency are used to generate thewatermark. The high Signal to Noise Ratio (SNR) and the Bit Error Rate (BER)claim the robustness of the proposed method.
arxiv-2400-24 | Transfer Learning for Voice Activity Detection: A Denoising Deep Neural Network Perspective | http://arxiv.org/pdf/1303.2104v1.pdf | author:Xiao-Lei Zhang, Ji Wu category:cs.LG published:2013-03-08 summary:Mismatching problem between the source and target noisy corpora severelyhinder the practical use of the machine-learning-based voice activity detection(VAD). In this paper, we try to address this problem in the transfer learningprospective. Transfer learning tries to find a common learning machine or acommon feature subspace that is shared by both the source corpus and the targetcorpus. The denoising deep neural network is used as the learning machine.Three transfer techniques, which aim to learn common feature representations,are used for analysis. Experimental results demonstrate the effectiveness ofthe transfer learning schemes on the mismatch problem.
arxiv-2400-25 | Gene-Machine, a new search heuristic algorithm | http://arxiv.org/pdf/1303.2096v1.pdf | author:Alfredo Garcia Woods category:cs.NE cs.AI published:2013-03-08 summary:This paper introduces Gene-Machine, an efficient and new search heuristicalgorithm, based in the building-block hypothesis. It is inspired by naturalevolution, but does not use some of the concepts present in genetic algorithmslike population, mutation and generation. This heuristic exhibits goodperformance in comparison with genetic algorithms, and can be used to generateuseful solutions to optimization and search problems.
arxiv-2400-26 | Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks | http://arxiv.org/pdf/1301.3605v3.pdf | author:Dong Yu, Michael L. Seltzer, Jinyu Li, Jui-Ting Huang, Frank Seide category:cs.LG cs.CL cs.NE published:2013-01-16 summary:Recent studies have shown that deep neural networks (DNNs) performsignificantly better than shallow networks and Gaussian mixture models (GMMs)on large vocabulary speech recognition tasks. In this paper, we argue that theimproved accuracy achieved by the DNNs is the result of their ability toextract discriminative internal representations that are robust to the manysources of variability in speech signals. We show that these representationsbecome increasingly insensitive to small perturbations in the input withincreasing network depth, which leads to better speech recognition performancewith deeper networks. We also show that DNNs cannot extrapolate to test samplesthat are substantially different from the training examples. If the trainingdata are sufficiently representative, however, internal features learned by theDNN are relatively stable with respect to speaker differences, bandwidthdifferences, and environment distortion. This enables DNN-based recognizers toperform as well or better than state-of-the-art systems based on GMMs orshallow networks without the need for explicit model adaptation or featurenormalization.
arxiv-2400-27 | Mining Representative Unsubstituted Graph Patterns Using Prior Similarity Matrix | http://arxiv.org/pdf/1303.2054v1.pdf | author:Wajdi Dhifli, Rabie Saidi, Engelbert Mephu Nguifo category:cs.CE cs.LG published:2013-03-08 summary:One of the most powerful techniques to study protein structures is to lookfor recurrent fragments (also called substructures or spatial motifs), then usethem as patterns to characterize the proteins under study. An emergent trendconsists in parsing proteins three-dimensional (3D) structures into graphs ofamino acids. Hence, the search of recurrent spatial motifs is formulated as aprocess of frequent subgraph discovery where each subgraph represents a spatialmotif. In this scope, several efficient approaches for frequent subgraphdiscovery have been proposed in the literature. However, the set of discoveredfrequent subgraphs is too large to be efficiently analyzed and explored in anyfurther process. In this paper, we propose a novel pattern selection approachthat shrinks the large number of discovered frequent subgraphs by selecting therepresentative ones. Existing pattern selection approaches do not exploit thedomain knowledge. Yet, in our approach we incorporate the evolutionaryinformation of amino acids defined in the substitution matrices in order toselect the representative subgraphs. We show the effectiveness of our approachon a number of real datasets. The results issued from our experiments show thatour approach is able to considerably decrease the number of motifs whileenhancing their interestingness.
arxiv-2400-28 | Security Assessment of Software Design using Neural Network | http://arxiv.org/pdf/1303.2017v1.pdf | author:A. Adebiyi, Johnnes Arreymbi, Chris Imafidon category:cs.CR cs.NE published:2013-03-08 summary:Security flaws in software applications today has been attributed mostly todesign flaws. With limited budget and time to release software into the market,many developers often consider security as an afterthought. Previous researchshows that integrating security into software applications at a later stage ofsoftware development lifecycle (SDLC) has been found to be more costly thanwhen it is integrated during the early stages. To assist in the integration ofsecurity early in the SDLC stages, a new approach for assessing security duringthe design phase by neural network is investigated in this paper. Our findingsshow that by training a back propagation neural network to identify attackpatterns, possible attacks can be identified from design scenarios presented toit. The result of performance of the neural network is presented in this paper.
arxiv-2400-29 | Barnes-Hut-SNE | http://arxiv.org/pdf/1301.3342v2.pdf | author:Laurens van der Maaten category:cs.LG cs.CV stat.ML published:2013-01-15 summary:The paper presents an O(N log N)-implementation of t-SNE -- an embeddingtechnique that is commonly used for the visualization of high-dimensional datain scatter plots and that normally runs in O(N^2). The new implementation usesvantage-point trees to compute sparse pairwise similarities between the inputdata objects, and it uses a variant of the Barnes-Hut algorithm - an algorithmused by astronomers to perform N-body simulations - to approximate the forcesbetween the corresponding points in the embedding. Our experiments show thatthe new algorithm, called Barnes-Hut-SNE, leads to substantial computationaladvantages over standard t-SNE, and that it makes it possible to learnembeddings of data sets with millions of objects.
arxiv-2400-30 | Mining and Exploiting Domain-Specific Corpora in the PANACEA Platform | http://arxiv.org/pdf/1303.1932v1.pdf | author:Núria Bel, Vassilis Papavasiliou, Prokopis Prokopidis, Antonio Toral, Victoria Arranz category:cs.CL published:2013-03-08 summary:The objective of the PANACEA ICT-2007.2.2 EU project is to build a platformthat automates the stages involved in the acquisition, production, updating andmaintenance of the large language resources required by, among others, MTsystems. The development of a Corpus Acquisition Component (CAC) for extractingmonolingual and bilingual data from the web is one of the most innovativebuilding blocks of PANACEA. The CAC, which is the first stage in the PANACEApipeline for building Language Resources, adopts an efficient and distributedmethodology to crawl for web documents with rich textual content in specificlanguages and predefined domains. The CAC includes modules that can acquireparallel data from sites with in-domain content available in more than onelanguage. In order to extrinsically evaluate the CAC methodology, we haveconducted several experiments that used crawled parallel corpora for theidentification and extraction of parallel sentences using sentence alignment.The corpora were then successfully used for domain adaptation of MachineTranslation Systems.
arxiv-2400-31 | A Classification of Adjectives for Polarity Lexicons Enhancement | http://arxiv.org/pdf/1303.1931v1.pdf | author:Silvia Vázquez, Núria Bel category:cs.CL published:2013-03-08 summary:Subjective language detection is one of the most important challenges inSentiment Analysis. Because of the weight and frequency in opinionated texts,adjectives are considered a key piece in the opinion extraction process. Thesesubjective units are more and more frequently collected in polarity lexicons inwhich they appear annotated with their prior polarity. However, at the moment,any polarity lexicon takes into account prior polarity variations acrossdomains. This paper proves that a majority of adjectives change their priorpolarity value depending on the domain. We propose a distinction between domaindependent and domain independent adjectives. Moreover, our analysis led us topropose a further classification related to subjectivity degree: constant,mixed and highly subjective adjectives. Following this classification, polarityvalues will be a better support for Sentiment Analysis.
arxiv-2400-32 | Automatic lexical semantic classification of nouns | http://arxiv.org/pdf/1303.1930v1.pdf | author:Núria Bel, Lauren Romeo, Muntsa Padró category:cs.CL published:2013-03-08 summary:The work we present here addresses cue-based noun classification in Englishand Spanish. Its main objective is to automatically acquire lexical semanticinformation by classifying nouns into previously known noun lexical classes.This is achieved by using particular aspects of linguistic contexts as cuesthat identify a specific lexical class. Here we concentrate on the task ofidentifying such cues and the theoretical background that allows for anassessment of the complexity of the task. The results show that, despite of thea-priori complexity of the task, cue-based classification is a useful tool inthe automatic acquisition of lexical semantic classes.
arxiv-2400-33 | Towards the Fully Automatic Merging of Lexical Resources: A Step Forward | http://arxiv.org/pdf/1303.1929v1.pdf | author:Muntsa Padró, Núria Bel, Silvia Necsulescu category:cs.CL published:2013-03-08 summary:This article reports on the results of the research done towards the fullyautomatically merging of lexical resources. Our main goal is to show thegenerality of the proposed approach, which have been previously applied tomerge Spanish Subcategorization Frames lexica. In this work we extend and applythe same technique to perform the merging of morphosyntactic lexica encoded inLMF. The experiments showed that the technique is general enough to obtain goodresults in these two different tasks which is an important step towardsperforming the merging of lexical resources fully automatically.
arxiv-2400-34 | Design and Development of Artificial Neural Networking (ANN) system using sigmoid activation function to predict annual rice production in Tamilnadu | http://arxiv.org/pdf/1303.1913v1.pdf | author:S. Arun Balaji, K. Baskaran category:cs.NE F.2.2; I.2.7 published:2013-03-08 summary:Prediction of annual rice production in all the 31 districts of Tamilnadu isan important decision for the Government of Tamilnadu. Rice production is acomplex process and non linear problem involving soil, crop, weather, pest,disease, capital, labour and management parameters. ANN software was designedand developed with Feed Forward Back Propagation (FFBP) network to predict riceproduction. The input layer has six independent variables like area ofcultivation and rice production in three seasons like Kuruvai, Samba and Kodai.The popular sigmoid activation function was adopted to convert input data intosigmoid values. The hidden layer computes the summation of six sigmoid valueswith six sets of weightages. The final output was converted into sigmoid valuesusing a sigmoid transfer function. ANN outputs are the predicted results. Theerror between original data and ANN output values were computed. A thresholdvalue of 10-9 was used to test whether the error is greater than the thresholdlevel. If the error is greater than threshold then updating of weights was doneall summations were done by back propagation. This process was repeated untilerror equal to zero. The predicted results were printed and it was found to beexactly matching with the expected values. It shows that the ANN prediction was100% accurate.
arxiv-2400-35 | On the performance of a hybrid genetic algorithm in dynamic environments | http://arxiv.org/pdf/1302.5474v2.pdf | author:Quan Yuan, Zhixin Yang category:cs.NE math.OC 68T20 published:2013-02-22 summary:The ability to track the optimum of dynamic environments is important in manypractical applications. In this paper, the capability of a hybrid geneticalgorithm (HGA) to track the optimum in some dynamic environments isinvestigated for different functional dimensions, update frequencies, anddisplacement strengths in different types of dynamic environments. Experimentalresults are reported by using the HGA and some other existing evolutionaryalgorithms in the literature. The results show that the HGA has bettercapability to track the dynamic optimum than some other existing algorithms.
arxiv-2400-36 | Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear Multivariate Regression and Granger Causality | http://arxiv.org/pdf/1210.4792v2.pdf | author:Vikas Sindhwani, Minh Ha Quang, Aurelie C. Lozano category:stat.ML cs.LG published:2012-10-17 summary:We propose a general matrix-valued multiple kernel learning framework forhigh-dimensional nonlinear multivariate regression problems. This frameworkallows a broad class of mixed norm regularizers, including those that inducesparsity, to be imposed on a dictionary of vector-valued Reproducing KernelHilbert Spaces. We develop a highly scalable and eigendecomposition-freealgorithm that orchestrates two inexact solvers for simultaneously learningboth the input and output components of separable matrix-valued kernels. As akey application enabled by our framework, we show how high-dimensional causalinference tasks can be naturally cast as sparse function estimation problems,leading to novel nonlinear extensions of a class of Graphical Granger Causalitytechniques. Our algorithmic developments and extensive empirical studies arecomplemented by theoretical analyses in terms of Rademacher generalizationbounds.
arxiv-2400-37 | Estimation of soil moisture in paddy field using Artificial Neural Networks | http://arxiv.org/pdf/1303.1868v1.pdf | author:Chusnul Arif, Masaru Mizoguchi, Budi Indra Setiawan, Ryoichi Doi category:cs.NE physics.ao-ph published:2013-03-08 summary:In paddy field, monitoring soil moisture is required for irrigationscheduling and water resource allocation, management and planning. The currentstudy proposes an Artificial Neural Networks (ANN) model to estimate soilmoisture in paddy field with limited meteorological data. Dynamic of ANN modelwas adopted to estimate soil moisture with the inputs of referenceevapotranspiration (ETo) and precipitation. ETo was firstly estimated using themaximum, average and minimum values of air temperature as the inputs of model.The models were performed under different weather conditions between the twopaddy cultivation periods. Training process of model was carried out using theobservation data in the first period, while validation process was conductedbased on the observation data in the second period. Dynamic of ANN modelestimated soil moisture with R2 values of 0.80 and 0.73 for training andvalidation processes, respectively, indicated that tight linear correlationsbetween observed and estimated values of soil moisture were observed. Thus, theANN model reliably estimates soil moisture with limited meteorological data.
arxiv-2400-38 | Watersheds on edge or node weighted graphs "par l'exemple" | http://arxiv.org/pdf/1303.1829v1.pdf | author:Fernand Meyer category:cs.CV published:2013-03-07 summary:Watersheds have been defined both for node and edge weighted graphs. We showthat they are identical: for each edge (resp.\ node) weighted graph exists anode (resp. edge) weighted graph with the same minima and catchment basin.
arxiv-2400-39 | Deconvolving Images with Unknown Boundaries Using the Alternating Direction Method of Multipliers | http://arxiv.org/pdf/1210.2687v2.pdf | author:Mariana S. C. Almeida, Mário A. T. Figueiredo category:math.OC cs.CV 68U10 I.4.4 published:2012-10-09 summary:The alternating direction method of multipliers (ADMM) has recently sparkedinterest as a flexible and efficient optimization tool for imaging inverseproblems, namely deconvolution and reconstruction under non-smooth convexregularization. ADMM achieves state-of-the-art speed by adopting a divide andconquer strategy, wherein a hard problem is split into simpler, efficientlysolvable sub-problems (e.g., using fast Fourier or wavelet transforms, orsimple proximity operators). In deconvolution, one of these sub-problemsinvolves a matrix inversion (i.e., solving a linear system), which can be doneefficiently (in the discrete Fourier domain) if the observation operator iscirculant, i.e., under periodic boundary conditions. This paper extendsADMM-based image deconvolution to the more realistic scenario of unknownboundary, where the observation operator is modeled as the composition of aconvolution (with arbitrary boundary conditions) with a spatial mask that keepsonly pixels that do not depend on the unknown boundary. The proposed approachalso handles, at no extra cost, problems that combine the recovery of missingpixels (i.e., inpainting) with deconvolution. We show that the resultingalgorithms inherit the convergence guarantees of ADMM and illustrate itsperformance on non-periodic deblurring (with and without inpainting of interiorpixels) under total-variation and frame-based regularization.
arxiv-2400-40 | Improving Automatic Emotion Recognition from speech using Rhythm and Temporal feature | http://arxiv.org/pdf/1303.1761v1.pdf | author:Mayank Bhargava, Tim Polzehl category:cs.CV published:2013-03-07 summary:This paper is devoted to improve automatic emotion recognition from speech byincorporating rhythm and temporal features. Research on automatic emotionrecognition so far has mostly been based on applying features like MFCCs, pitchand energy or intensity. The idea focuses on borrowing rhythm features fromlinguistic and phonetic analysis and applying them to the speech signal on thebasis of acoustic knowledge only. In addition to this we exploit a set oftemporal and loudness features. A segmentation unit is employed in starting toseparate the voiced/unvoiced and silence parts and features are explored ondifferent segments. Thereafter different classifiers are used forclassification. After selecting the top features using an IGR filter we areable to achieve a recognition rate of 80.60 % on the Berlin Emotion Databasefor the speaker dependent framework.
arxiv-2400-41 | Visualization of Manifold-Valued Elements by Multidimensional Scaling | http://arxiv.org/pdf/1004.0314v2.pdf | author:Simone Fiori category:stat.ML published:2010-04-02 summary:The present contribution suggests the use of a multidimensional scaling (MDS)algorithm as a visualization tool for manifold-valued elements. A visualizationtool of this kind is useful in signal processing and machine learning wheneverlearning/adaptation algorithms insist on high-dimensional parameter manifolds.
arxiv-2400-42 | Concept-based indexing in text information retrieval | http://arxiv.org/pdf/1303.1703v1.pdf | author:Fatiha Boubekeur, Wassila Azzoug category:cs.IR cs.CL published:2013-03-07 summary:Traditional information retrieval systems rely on keywords to index documentsand queries. In such systems, documents are retrieved based on the number ofshared keywords with the query. This lexical-focused retrieval leads toinaccurate and incomplete results when different keywords are used to describethe documents and queries. Semantic-focused retrieval approaches attempt toovercome this problem by relying on concepts rather than on keywords toindexing and retrieval. The goal is to retrieve documents that are semanticallyrelevant to a given user query. This paper addresses this issue by proposing asolution at the indexing level. More precisely, we propose a novel approach forsemantic indexing based on concepts identified from a linguistic resource. Inparticular, our approach relies on the joint use of WordNet and WordNetDomainslexical databases for concept identification. Furthermore, we propose asemantic-based concept weighting scheme that relies on a novel definition ofconcept centrality. The resulting system is evaluated on the TIME testcollection. Experimental results show the effectiveness of our proposition overtraditional IR approaches.
arxiv-2400-43 | K-Nearest Neighbour algorithm coupled with logistic regression in medical case-based reasoning systems. Application to prediction of access to the renal transplant waiting list in Brittany | http://arxiv.org/pdf/1303.1700v1.pdf | author:Boris Campillo-Gimenez, Wassim Jouini, Sahar Bayat, Marc Cuggia category:cs.AI stat.ML published:2013-03-07 summary:Introduction. Case Based Reasoning (CBR) is an emerg- ing decision makingparadigm in medical research where new cases are solved relying on previouslysolved similar cases. Usually, a database of solved cases is provided, andevery case is described through a set of attributes (inputs) and a label(output). Extracting useful information from this database can help the CBRsystem providing more reliable results on the yet to be solved cases.Objective. For that purpose we suggest a general frame- work where a CBRsystem, viz. K-Nearest Neighbor (K-NN) algorithm, is combined with variousinformation obtained from a Logistic Regression (LR) model. Methods. LR isapplied, on the case database, to assign weights to the attributes as well asthe solved cases. Thus, five possible decision making systems based on K-NNand/or LR were identified: a standalone K-NN, a standalone LR and three softK-NN algorithms that rely on the weights based on the results of the LR. Theevaluation of the described approaches is performed in the field of renaltransplant access waiting list. Results and conclusion. The results show thatour suggested approach, where the K-NN algorithm relies on both weightedattributes and cases, can efficiently deal with non relevant attributes,whereas the four other approaches suffer from this kind of noisy setups. Therobustness of this approach suggests interesting perspectives for medicalproblem solving tools using CBR methodology.
arxiv-2400-44 | Objective Improvement in Information-Geometric Optimization | http://arxiv.org/pdf/1211.3831v3.pdf | author:Youhei Akimoto, Yann Ollivier category:cs.LG cs.AI math.OC stat.ML published:2012-11-16 summary:Information-Geometric Optimization (IGO) is a unified framework of stochasticalgorithms for optimization problems. Given a family of probabilitydistributions, IGO turns the original optimization problem into a newmaximization problem on the parameter space of the probability distributions.IGO updates the parameter of the probability distribution along the naturalgradient, taken with respect to the Fisher metric on the parameter manifold,aiming at maximizing an adaptive transform of the objective function. IGOrecovers several known algorithms as particular instances: for the family ofBernoulli distributions IGO recovers PBIL, for the family of Gaussiandistributions the pure rank-mu CMA-ES update is recovered, and for exponentialfamilies in expectation parametrization the cross-entropy/ML method isrecovered. This article provides a theoretical justification for the IGOframework, by proving that any step size not greater than 1 guarantees monotoneimprovement over the course of optimization, in terms of q-quantile values ofthe objective function f. The range of admissible step sizes is independent off and its domain. We extend the result to cover the case of different stepsizes for blocks of the parameters in the IGO algorithm. Moreover, we provethat expected fitness improves over time when fitness-proportional selection isapplied, in which case the RPP algorithm is recovered.
arxiv-2400-45 | ALPRS - A New Approach for License Plate Recognition using the Sift Algorithm | http://arxiv.org/pdf/1303.1667v1.pdf | author:Francisco Assis da Silva, Almir Olivette Artero, Maria Stela Veludo de Paiva, Ricardo Luis Barbosa category:cs.CV published:2013-03-07 summary:This paper presents a new approach for the automatic license platerecognition, which includes the SIFT algorithm in step to locate the plate inthe input image. In this new approach, besides the comparison of the featuresobtained with the SIFT algorithm, the correspondence between the spatialorientations and the positioning associated with the keypoints is alsoobserved. Afterwards, an algorithm is used for the character recognition of theplates, very fast, which makes it possible its application in real time. Theresults obtained with the proposed approach presented very good success rates,so much for locating the characters in the input image, as for theirrecognition.
arxiv-2400-46 | On Robust Face Recognition via Sparse Encoding: the Good, the Bad, and the Ugly | http://arxiv.org/pdf/1303.1624v1.pdf | author:Yongkang Wong, Mehrtash T. Harandi, Conrad Sanderson category:cs.CV published:2013-03-07 summary:In the field of face recognition, Sparse Representation (SR) has receivedconsiderable attention during the past few years. Most of the relevantliterature focuses on holistic descriptors in closed-set identificationapplications. The underlying assumption in SR-based methods is that each classin the gallery has sufficient samples and the query lies on the subspacespanned by the gallery of the same class. Unfortunately, such assumption iseasily violated in the more challenging face verification scenario, where analgorithm is required to determine if two faces (where one or both have notbeen seen before) belong to the same person. In this paper, we first discusswhy previous attempts with SR might not be applicable to verification problems.We then propose an alternative approach to face verification via SR.Specifically, we propose to use explicit SR encoding on local image patchesrather than the entire face. The obtained sparse signals are pooled viaaveraging to form multiple region descriptors, which are then concatenated toform an overall face descriptor. Due to the deliberate loss spatial relationswithin each region (caused by averaging), the resulting descriptor is robust tomisalignment & various image deformations. Within the proposed framework, weevaluate several SR encoding techniques: l1-minimisation, Sparse AutoencoderNeural Network (SANN), and an implicit probabilistic technique based onGaussian Mixture Models. Thorough experiments on AR, FERET, exYaleB, BANCA andChokePoint datasets show that the proposed local SR approach obtainsconsiderably better and more robust performance than several previousstate-of-the-art holistic SR methods, in both verification and closed-setidentification problems. The experiments also show that l1-minimisation basedencoding has a considerably higher computational than the other techniques, butleads to higher recognition rates.
arxiv-2400-47 | Efficient learning strategy of Chinese characters based on network approach | http://arxiv.org/pdf/1303.1599v1.pdf | author:Xiao-Yong Yan, Ying Fan, Zengru Di, Shlomo Havlin, Jinshan Wu category:physics.soc-ph cs.CL cs.SI published:2013-03-07 summary:Based on network analysis of hierarchical structural relations among Chinesecharacters, we develop an efficient learning strategy of Chinese characters. Weregard a more efficient learning method if one learns the same number of usefulChinese characters in less effort or time. We construct a node-weighted networkof Chinese characters, where character usage frequencies are used as nodeweights. Using this hierarchical node-weighted network, we propose a newlearning method, the distributed node weight (DNW) strategy, which is based ona new measure of nodes' importance that takes into account both the weight ofthe nodes and the hierarchical structure of the network. Chinese characterlearning strategies, particularly their learning order, are analyzed asdynamical processes over the network. We compare the efficiency of threetheoretical learning methods and two commonly used methods from mainstreamChinese textbooks, one for Chinese elementary school students and the other forstudents learning Chinese as a second language. We find that the DNW methodsignificantly outperforms the others, implying that the efficiency of currentlearning methods of major textbooks can be greatly improved.
arxiv-2400-48 | Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms | http://arxiv.org/pdf/1208.3719v2.pdf | author:Chris Thornton, Frank Hutter, Holger H. Hoos, Kevin Leyton-Brown category:cs.LG published:2012-08-18 summary:Many different machine learning algorithms exist; taking into account eachalgorithm's hyperparameters, there is a staggeringly large number of possiblealternatives overall. We consider the problem of simultaneously selecting alearning algorithm and setting its hyperparameters, going beyond previous workthat addresses these issues in isolation. We show that this problem can beaddressed by a fully automated approach, leveraging recent innovations inBayesian optimization. Specifically, we consider a wide range of featureselection techniques (combining 3 search and 8 evaluator methods) and allclassification approaches implemented in WEKA, spanning 2 ensemble methods, 10meta-methods, 27 base classifiers, and hyperparameter settings for eachclassifier. On each of 21 popular datasets from the UCI repository, the KDD Cup09, variants of the MNIST dataset and CIFAR-10, we show classificationperformance often much better than using standard selection/hyperparameteroptimization methods. We hope that our approach will help non-expert users tomore effectively identify machine learning algorithms and hyperparametersettings appropriate to their applications, and hence to achieve improvedperformance.
arxiv-2400-49 | On Considering Uncertainty and Alternatives in Low-Level Vision | http://arxiv.org/pdf/1303.1460v1.pdf | author:Steven M. LaValle, Seth A. Hutchinson category:cs.AI cs.CV published:2013-03-06 summary:In this paper we address the uncertainty issues involved in the low-levelvision task of image segmentation. Researchers in computer vision have workedextensively on this problem, in which the goal is to partition (or segment) animage into regions that are homogeneous or uniform in some sense. Thissegmentation is often utilized by some higher level process, such as an objectrecognition system. We show that by considering uncertainty in a Bayesianformalism, we can use statistical image models to build an approximaterepresentation of a probability distribution over a space of alternativesegmentations. We give detailed descriptions of the various levels ofuncertainty associated with this problem, discuss the interaction of prior andposterior distributions, and provide the operations for constructing thisrepresentation.
arxiv-2400-50 | A Fast Iterative Bayesian Inference Algorithm for Sparse Channel Estimation | http://arxiv.org/pdf/1303.1312v1.pdf | author:Niels Lovmand Pedersen, Carles Navarro Manch category:stat.ML cs.IT math.IT published:2013-03-06 summary:In this paper, we present a Bayesian channel estimation algorithm formulticarrier receivers based on pilot symbol observations. The inherent sparsenature of wireless multipath channels is exploited by modeling the priordistribution of multipath components' gains with a hierarchical representationof the Bessel K probability density function; a highly efficient, fastiterative Bayesian inference method is then applied to the proposed model. Theresulting estimator outperforms other state-of-the-art Bayesian andnon-Bayesian estimators, either by yielding lower mean squared estimation erroror by attaining the same accuracy with improved convergence rate, as shown inour numerical evaluation.
arxiv-2400-51 | Large-Margin Metric Learning for Partitioning Problems | http://arxiv.org/pdf/1303.1280v1.pdf | author:Rémi Lajugie, Sylvain Arlot, Francis Bach category:cs.LG stat.ML published:2013-03-06 summary:In this paper, we consider unsupervised partitioning problems, such asclustering, image segmentation, video segmentation and other change-pointdetection problems. We focus on partitioning problems based explicitly orimplicitly on the minimization of Euclidean distortions, which includemean-based change-point detection, K-means, spectral clustering and normalizedcuts. Our main goal is to learn a Mahalanobis metric for these unsupervisedproblems, leading to feature weighting and/or selection. This is done in asupervised way by assuming the availability of several potentially partiallylabelled datasets that share the same metric. We cast the metric learningproblem as a large-margin structured prediction problem, with proper definitionof regularizers and losses, leading to a convex optimization problem which canbe solved efficiently with iterative techniques. We provide experiments wherewe show how learning the metric may significantly improve the partitioningperformance in synthetic examples, bioinformatics, video segmentation and imagesegmentation problems.
arxiv-2400-52 | Discovery of factors in matrices with grades | http://arxiv.org/pdf/1303.1264v1.pdf | author:Radim Belohlavek, Vilem Vychodil category:cs.LG cs.NA published:2013-03-06 summary:We present an approach to decomposition and factor analysis of matrices withordinal data. The matrix entries are grades to which objects represented byrows satisfy attributes represented by columns, e.g. grades to which an imageis red, a product has a given feature, or a person performs well in a test. Weassume that the grades form a bounded scale equipped with certain aggregationoperators and conforms to the structure of a complete residuated lattice. Wepresent a greedy approximation algorithm for the problem of decomposition ofsuch matrix in a product of two matrices with grades under the restriction thatthe number of factors be small. Our algorithm is based on a geometric insightprovided by a theorem identifying particular rectangular-shaped submatrices asoptimal factors for the decompositions. These factors correspond to formalconcepts of the input data and allow an easy interpretation of thedecomposition. We present illustrative examples and experimental evaluation.
arxiv-2400-53 | A Generalized Hybrid Real-Coded Quantum Evolutionary Algorithm Based on Particle Swarm Theory with Arithmetic Crossover | http://arxiv.org/pdf/1303.1243v1.pdf | author:Md. Amjad Hossain, Md. Kawser Hossain, M. M. A. Hashem category:cs.NE published:2013-03-06 summary:This paper proposes a generalized Hybrid Real-coded Quantum EvolutionaryAlgorithm (HRCQEA) for optimizing complex functions as well as combinatorialoptimization. The main idea of HRCQEA is to devise a new technique for mutationand crossover operators. Using the evolutionary equation of PSO aSingle-Multiple gene Mutation (SMM) is designed and the concept of ArithmeticCrossover (AC) is used in the new Crossover operator. In HRCQEA, each triploidchromosome represents a particle and the position of the particle is updatedusing SMM and Quantum Rotation Gate (QRG), which can make the balance betweenexploration and exploitation. Crossover is employed to expand the search space,Hill Climbing Selection (HCS) and elitism help to accelerate the convergencespeed. Simulation results on Knapsack Problem and five benchmark complexfunctions with high dimension show that HRCQEA performs better in terms ofability to discover the global optimum and convergence speed.
arxiv-2400-54 | Japanese-Spanish Thesaurus Construction Using English as a Pivot | http://arxiv.org/pdf/1303.1232v1.pdf | author:Jessica Ramírez, Masayuki Asahara, Yuji Matsumoto category:cs.CL cs.AI published:2013-03-06 summary:We present the results of research with the goal of automatically creating amultilingual thesaurus based on the freely available resources of Wikipedia andWordNet. Our goal is to increase resources for natural language processingtasks such as machine translation targeting the Japanese-Spanish language pair.Given the scarcity of resources, we use existing English resources as a pivotfor creating a trilingual Japanese-Spanish-English thesaurus. Our approachconsists of extracting the translation tuples from Wikipedia, disambiguatingthem by mapping them to WordNet word senses. We present results comparing twomethods of disambiguation, the first using VSM on Wikipedia article texts andWordNet definitions, and the second using categorical information extractedfrom Wikipedia, We find that mixing the two methods produces favorable results.Using the proposed method, we have constructed a multilingualSpanish-Japanese-English thesaurus consisting of 25,375 entries. The samemethod can be applied to any pair of languages that are linked to English inWikipedia.
arxiv-2400-55 | Impulsive Noise Mitigation in Powerline Communications Using Sparse Bayesian Learning | http://arxiv.org/pdf/1303.1217v1.pdf | author:Jing Lin, Marcel Nassar, Brian L. Evans category:stat.ML cs.IT math.IT published:2013-03-05 summary:Additive asynchronous and cyclostationary impulsive noise limitscommunication performance in OFDM powerline communication (PLC) systems.Conventional OFDM receivers assume additive white Gaussian noise and henceexperience degradation in communication performance in impulsive noise.Alternate designs assume a parametric statistical model of impulsive noise anduse the model parameters in mitigating impulsive noise. These receivers requireoverhead in training and parameter estimation, and degrade due to model andparameter mismatch, especially in highly dynamic environments. In this paper,we model impulsive noise as a sparse vector in the time domain without anyother assumptions, and apply sparse Bayesian learning methods for estimationand mitigation without training. We propose three iterative algorithms withdifferent complexity vs. performance trade-offs: (1) we utilize the noiseprojection onto null and pilot tones to estimate and subtract the noiseimpulses; (2) we add the information in the data tones to perform joint noiseestimation and OFDM detection; (3) we embed our algorithm into a decisionfeedback structure to further enhance the performance of coded systems. Whencompared to conventional OFDM PLC receivers, the proposed receivers achieve SNRgains of up to 9 dB in coded and 10 dB in uncoded systems in the presence ofimpulsive noise.
arxiv-2400-56 | A Genetic algorithm to solve the container storage space allocation problem | http://arxiv.org/pdf/1303.1051v1.pdf | author:I. Ayachi, R. Kammarti, M. Ksouri, P. Borne, :, LAGIS, Ecole Centrale de Lille, :, LACS, Ecole Nationale des category:cs.NE published:2013-03-05 summary:This paper presented a genetic algorithm (GA) to solve the container storageproblem in the port. This problem is studied with different container typessuch as regular, open side, open top, tank, empty and refrigerated containers.The objective of this problem is to determine an optimal containersarrangement, which respects customers delivery deadlines, reduces the rehandleoperations of containers and minimizes the stop time of the container ship. Inthis paper, an adaptation of the genetic algorithm to the container storageproblem is detailed and some experimental results are presented and discussed.The proposed approach was compared to a Last In First Out (LIFO) algorithmapplied to the same problem and has recorded good results
arxiv-2400-57 | GBM Volumetry using the 3D Slicer Medical Image Computing Platform | http://arxiv.org/pdf/1303.0964v1.pdf | author:Jan Egger, Tina Kapur, Andriy Fedorov, Steve Pieper, James V. Miller, Harini Veeraraghavan, Bernd Freisleben, Alexandra Golby, Christopher Nimsky, Ron Kikinis category:cs.CV published:2013-03-05 summary:Volumetric change in glioblastoma multiforme (GBM) over time is a criticalfactor in treatment decisions. Typically, the tumor volume is computed on aslice-by-slice basis using MRI scans obtained at regular intervals. (3D)Slicer- a free platform for biomedical research - provides an alternative to thismanual slice-by-slice segmentation process, which is significantly faster andrequires less user interaction. In this study, 4 physicians segmented GBMs in10 patients, once using the competitive region-growing based GrowCutsegmentation module of Slicer, and once purely by drawing boundaries completelymanually on a slice-by-slice basis. Furthermore, we provide a variabilityanalysis for three physicians for 12 GBMs. The time required for GrowCutsegmentation was on an average 61% of the time required for a pure manualsegmentation. A comparison of Slicer-based segmentation with manualslice-by-slice segmentation resulted in a Dice Similarity Coefficient of 88.43+/- 5.23% and a Hausdorff Distance of 2.32 +/- 5.23 mm.
arxiv-2400-58 | GURLS: a Least Squares Library for Supervised Learning | http://arxiv.org/pdf/1303.0934v1.pdf | author:Andrea Tacchetti, Pavan K Mallapragada, Matteo Santoro, Lorenzo Rosasco category:cs.LG cs.AI cs.MS published:2013-03-05 summary:We present GURLS, a least squares, modular, easy-to-extend software libraryfor efficient supervised learning. GURLS is targeted to machine learningpractitioners, as well as non-specialists. It offers a number state-of-the-arttraining strategies for medium and large-scale learning, and routines forefficient model selection. The library is particularly well suited formulti-output problems (multi-category/multi-label). GURLS is currentlyavailable in two independent implementations: Matlab and C++. It takesadvantage of the favorable properties of regularized least squares algorithm toexploit advanced tools in linear algebra. Routines to handle computations withvery large matrices by means of memory-mapped storage and distributed taskexecution are available. The package is distributed under the BSD licence andis available for download at https://github.com/CBCL/GURLS.
arxiv-2400-59 | Prediction and Clustering in Signed Networks: A Local to Global Perspective | http://arxiv.org/pdf/1302.5145v2.pdf | author:Kai-Yang Chiang, Cho-Jui Hsieh, Nagarajan Natarajan, Ambuj Tewari, Inderjit S. Dhillon category:cs.SI cs.LG published:2013-02-20 summary:The study of social networks is a burgeoning research area. However, mostexisting work deals with networks that simply encode whether relationshipsexist or not. In contrast, relationships in signed networks can be positive("like", "trust") or negative ("dislike", "distrust"). The theory of socialbalance shows that signed networks tend to conform to some local patterns that,in turn, induce certain global characteristics. In this paper, we exploit bothlocal as well as global aspects of social balance theory for two fundamentalproblems in the analysis of signed networks: sign prediction and clustering.Motivated by local patterns of social balance, we first propose two families ofsign prediction methods: measures of social imbalance (MOIs), and supervisedlearning using high order cycles (HOCs). These methods predict signs of edgesbased on triangles and \ell-cycles for relatively small values of \ell.Interestingly, by examining measures of social imbalance, we show that theclassic Katz measure, which is used widely in unsigned link prediction,actually has a balance theoretic interpretation when applied to signednetworks. Furthermore, motivated by the global structure of balanced networks,we propose an effective low rank modeling approach for both sign prediction andclustering. For the low rank modeling approach, we provide theoreticalperformance guarantees via convex relaxations, scale it up to large problemsizes using a matrix factorization based algorithm, and provide extensiveexperimental validation including comparisons with local approaches. Ourexperimental results indicate that, by adopting a more global viewpoint ofbalance structure, we get significant performance and computational gains inprediction and clustering tasks on signed networks. Our work thereforehighlights the usefulness of the global aspect of balance theory for theanalysis of signed networks.
arxiv-2400-60 | Spike and Tyke, the Quantized Neuron Model | http://arxiv.org/pdf/1212.2958v2.pdf | author:M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny category:cs.NE cs.AI published:2012-12-07 summary:Modeling spike firing assumes that spiking statistics are Poisson, but realdata violates this assumption. To capture non-Poissonian features, in order tofix the inevitable inherent irregularity, researchers rescale the time axiswith tedious computational overhead instead of searching for anotherdistribution. Spikes or action potentials are precisely-timed changes in theionic transport through synapses adjusting the synaptic weight, successfullymodeled and developed as a memristor. Memristance value is multiples of initialresistance. This reminds us with the foundations of quantum mechanics. We tryto quantize potential and resistance, as done with energy. After reviewingPlanck curve for blackbody radiation, we propose the quantization equations. Weintroduce and prove a theorem that quantizes the resistance. Then we define thetyke showing its basic characteristics. Finally we give the basictransformations to model spiking and link an energy quantum to a tyke.Investigation shows how this perfectly models the neuron spiking, with over 97%match.
arxiv-2400-61 | Multivariate Temporal Dictionary Learning for EEG | http://arxiv.org/pdf/1303.0742v1.pdf | author:Quentin Barthélemy, Cédric Gouy-Pailler, Yoann Isaac, Antoine Souloumiac, Anthony Larue, Jérôme I. Mars category:cs.LG q-bio.NC stat.ML published:2013-03-04 summary:This article addresses the issue of representing electroencephalographic(EEG) signals in an efficient way. While classical approaches use a fixed Gabordictionary to analyze EEG signals, this article proposes a data-driven methodto obtain an adapted dictionary. To reach an efficient dictionary learning,appropriate spatial and temporal modeling is required. Inter-channels links aretaken into account in the spatial multivariate model, and shift-invariance isused for the temporal model. Multivariate learned kernels are informative (afew atoms code plentiful energy) and interpretable (the atoms can have aphysiological meaning). Using real EEG data, the proposed method is shown tooutperform the classical multichannel matching pursuit used with a Gabordictionary, as measured by the representative power of the learned dictionaryand its spatial flexibility. Moreover, dictionary learning can captureinterpretable patterns: this ability is illustrated on real data, learning aP300 evoked potential.
arxiv-2400-62 | The Convergence Rate of Majority Vote under Exchangeability | http://arxiv.org/pdf/1303.0727v1.pdf | author:Miles E. Lopes category:math.PR cs.SI math.ST stat.ML stat.TH published:2013-03-04 summary:Majority vote plays a fundamental role in many applications of statistics,such as ensemble classifiers, crowdsourcing, and elections. When using majorityvote as a prediction rule, it is of basic interest to ask "How many votes areneeded to obtain a reliable prediction?" In the context of binaryclassification with Random Forests or Bagging, we give a precise answer: Iferr_t denotes the test error achieved by the majority vote of t \geq 1classifiers, and err* denotes its nominal limiting value, then under basicregularity conditions, err_t = err* + c/t + o(1/t), where c is a constant givenby a simple formula. More generally, we show that if V_1,V_2,... is anexchangeable Bernoulli sequence with mixture distribution F, and the majorityvote is written as M_t=median(V_1,...,V_t), then 1-\E[M_t] = F(1/2)+(F"(1/2)/8)(1/t)+o(1/t) when F is sufficiently smooth.
arxiv-2400-63 | Boltzmann Machines and Denoising Autoencoders for Image Denoising | http://arxiv.org/pdf/1301.3468v6.pdf | author:Kyunghyun Cho category:stat.ML cs.CV cs.LG published:2013-01-15 summary:Image denoising based on a probabilistic model of local image patches hasbeen employed by various researchers, and recently a deep (denoising)autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] asa good model for this. In this paper, we propose that another popular family ofmodels in the field of deep learning, called Boltzmann machines, can performimage denoising as well as, or in certain cases of high level of noise, betterthan denoising autoencoders. We empirically evaluate the two models on threedifferent sets of images with different types and levels of noise. Throughoutthe experiments we also examine the effect of the depth of the models. Theexperiments confirmed our claim and revealed that the performance can beimproved by adding more hidden layers, especially when the level of noise ishigh.
arxiv-2400-64 | Denoising Deep Neural Networks Based Voice Activity Detection | http://arxiv.org/pdf/1303.0663v1.pdf | author:Xiao-Lei Zhang, Ji Wu category:cs.LG cs.SD stat.ML published:2013-03-04 summary:Recently, the deep-belief-networks (DBN) based voice activity detection (VAD)has been proposed. It is powerful in fusing the advantages of multiplefeatures, and achieves the state-of-the-art performance. However, the deeplayers of the DBN-based VAD do not show an apparent superiority to theshallower layers. In this paper, we propose a denoising-deep-neural-network(DDNN) based VAD to address the aforementioned problem. Specifically, wepre-train a deep neural network in a special unsupervised denoising greedylayer-wise mode, and then fine-tune the whole network in a supervised way bythe common back-propagation algorithm. In the pre-training phase, we take thenoisy speech signals as the visible layer and try to extract a new feature thatminimizes the reconstruction cross-entropy loss between the noisy speechsignals and its corresponding clean speech signals. Experimental results showthat the proposed DDNN-based VAD not only outperforms the DBN-based VAD butalso shows an apparent performance improvement of the deep layers overshallower layers.
arxiv-2400-65 | Spatial Fuzzy C Means PET Image Segmentation of Neurodegenerative Disorder | http://arxiv.org/pdf/1303.0647v1.pdf | author:A. Meena, R. Raja category:cs.CV published:2013-03-04 summary:Nuclear image has emerged as a promising research work in medical field.Images from different modality meet its own challenge. Positron EmissionTomography (PET) image may help to precisely localize disease to assist inplanning the right treatment for each case and saving valuable time. In thispaper, a novel approach of Spatial Fuzzy C Means (PET SFCM) clusteringalgorithm is introduced on PET scan image datasets. The proposed algorithm isincorporated the spatial neighborhood information with traditional FCM andupdating the objective function of each cluster. This algorithm is implementedand tested on huge data collection of patients with brain neuro degenerativedisorder such as Alzheimers disease. It has demonstrated its effectiveness bytesting it for real world patient data sets. Experimental results are comparedwith conventional FCM and K Means clustering algorithm. The performance of thePET SFCM provides satisfactory results compared with other two algorithms
arxiv-2400-66 | Symmetry Based Cluster Approach for Automatic Recognition of the Epileptic Focus in Brain Using PET Scan Image : An Analysis | http://arxiv.org/pdf/1303.0645v1.pdf | author:A. Meena, R. Raja category:cs.CV published:2013-03-04 summary:Recognition of epileptic focal point is the important diagnosis whenscreening the epilepsy patients for latent surgical cures. The accuratelocalization is challenging one because of the low spatial resolution imageswith more noisy data. Positron Emission Tomography (PET) has now replaced theissues and caring a high resolution. This paper focuses the research ofautomated localization of epileptic seizures in brain functional images usingsymmetry based cluster approach. This approach presents a fully automatedsymmetry based brain abnormality detection method for PET sequences. PET imagesare spatially normalized to Digital Imaging and Communications in Medicine(DICOM) standard and then it has been trained using symmetry based clusterapproach using Medical Image Processing, Analysis & Visualization (MIPAV) tool.The performance evolution is considered by the metric like accuracy ofdiagnosis. The obtained result is surely assists the surgeon for the automatedidentification of seizures focus.
arxiv-2400-67 | Automatic symmetry based cluster approach for anomalous brain identification in PET scan image : An Analysis | http://arxiv.org/pdf/1303.0644v1.pdf | author:A. Meena, K. Raja category:cs.CV published:2013-03-04 summary:Medical image segmentation is referred to the segmentation of known anatomicstructures from different medical images. Normally, the medical data researchesare more complicated and an exclusive structures. This computer aided diagnosisis used for assisting doctors in evaluating medical imagery or in recognizingabnormal findings in a medical image. To integrate the specialized knowledgefor medical data processing is helpful to form a real useful healthcaredecision making system. This paper studies the different symmetry baseddistances applied in clustering algorithms and analyzes symmetry approach forPositron Emission Tomography (PET) scan image segmentation. Unlike CT and MRI,the PET scan identifies the structure of blood flow to and from organs. PETscan also helps in early diagnosis of cancer and heart, brain and gastrointestinal ailments and to detect the progress of treatment. In this paper, thescope diagnostic task expands for PET image in various brain functions.
arxiv-2400-68 | Recognition of Facial Expression Using Eigenvector Based Distributed Features and Euclidean Distance Based Decision Making Technique | http://arxiv.org/pdf/1303.0635v1.pdf | author:Jeemoni Kalita, Karen Das category:cs.CV published:2013-03-04 summary:In this paper, an Eigenvector based system has been presented to recognizefacial expressions from digital facial images. In the approach, firstly theimages were acquired and cropping of five significant portions from the imagewas performed to extract and store the Eigenvectors specific to theexpressions. The Eigenvectors for the test images were also computed, andfinally the input facial image was recognized when similarity was obtained bycalculating the minimum Euclidean distance between the test image and thedifferent expressions.
arxiv-2400-69 | Indian Sign Language Recognition Using Eigen Value Weighted Euclidean Distance Based Classification Technique | http://arxiv.org/pdf/1303.0634v1.pdf | author:Joyeeta Singha, Karen Das category:cs.CV published:2013-03-04 summary:Sign Language Recognition is one of the most growing fields of researchtoday. Many new techniques have been developed recently in these fields. Herein this paper, we have proposed a system using Eigen value weighted Euclideandistance as a classification technique for recognition of various SignLanguages of India. The system comprises of four parts: Skin Filtering, HandCropping, Feature Extraction and Classification. Twenty four signs wereconsidered in this paper, each having ten samples, thus a total of two hundredforty images was considered for which recognition rate obtained was 97 percent.
arxiv-2400-70 | Omega Model for Human Detection and Counting for application in Smart Surveillance System | http://arxiv.org/pdf/1303.0633v1.pdf | author:Subra Mukherjee, Karen Das category:cs.CV published:2013-03-04 summary:Driven by the significant advancements in technology and social issues suchas security management, there is a strong need for Smart Surveillance System inour society today. One of the key features of a Smart Surveillance System isefficient human detection and counting such that the system can decide andlabel events on its own. In this paper we propose a new, novel and robustmodel, The Omega Model, for detecting and counting human beings present in thescene. The proposed model employs a set of four distinct descriptors foridentifying the unique features of the head, neck and shoulder regions of aperson. This unique head neck shoulder signature given by the Omega Modelexploits the challenges such as inter person variations in size and shape ofpeoples head, neck and shoulder regions to achieve robust detection of humanbeings even under partial occlusion, dynamically changing background andvarying illumination conditions. After experimentation we observe and analyzethe influences of each of the four descriptors on the system performance andcomputation speed and conclude that a weight based decision making systemproduces the best results. Evaluation results on a number of images indicatethe validation of our method in actual situation.
arxiv-2400-71 | A Semantic approach for effective document clustering using WordNet | http://arxiv.org/pdf/1303.0489v1.pdf | author:Leena H. Patil, Mohammed Atique category:cs.CL cs.IR published:2013-03-03 summary:Now a days, the text document is spontaneously increasing over the internet,e-mail and web pages and they are stored in the electronic database format. Toarrange and browse the document it becomes difficult. To overcome such problemthe document preprocessing, term selection, attribute reduction and maintainingthe relationship between the important terms using background knowledge,WordNet, becomes an important parameters in data mining. In these paper thedifferent stages are formed, firstly the document preprocessing is done byremoving stop words, stemming is performed using porter stemmer algorithm, wordnet thesaurus is applied for maintaining relationship between the importantterms, global unique words, and frequent word sets get generated, Secondly,data matrix is formed, and thirdly terms are extracted from the documents byusing term selection approaches tf-idf, tf-df, and tf2 based on their minimumthreshold value. Further each and every document terms gets preprocessed, wherethe frequency of each term within the document is counted for representation.The purpose of this approach is to reduce the attributes and find the effectiveterm selection method using WordNet for better clustering accuracy. Experimentsare evaluated on Reuters Transcription Subsets, wheat, trade, money grain, andship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group(Hardware), 20 News group (Computer Graphics) etc.
arxiv-2400-72 | Scikit-learn: Machine Learning in Python | http://arxiv.org/pdf/1201.0490v2.pdf | author:Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay category:cs.LG cs.MS published:2012-01-02 summary:Scikit-learn is a Python module integrating a wide range of state-of-the-artmachine learning algorithms for medium-scale supervised and unsupervisedproblems. This package focuses on bringing machine learning to non-specialistsusing a general-purpose high-level language. Emphasis is put on ease of use,performance, documentation, and API consistency. It has minimal dependenciesand is distributed under the simplified BSD license, encouraging its use inboth academic and commercial settings. Source code, binaries, and documentationcan be downloaded from http://scikit-learn.sourceforge.net.
arxiv-2400-73 | Distributed Evolutionary Computation: A New Technique for Solving Large Number of Equations | http://arxiv.org/pdf/1303.0462v1.pdf | author:Moslema Jahan, M. M. A. Hashem, Gazi Abdullah Shahriar category:cs.NE published:2013-03-03 summary:Evolutionary computation techniques have mostly been used to solve variousoptimization and learning problems successfully. Evolutionary algorithm is moreeffective to gain optimal solution(s) to solve complex problems thantraditional methods. In case of problems with large set of parameters,evolutionary computation technique incurs a huge computational burden for asingle processing unit. Taking this limitation into account, this paperpresents a new distributed evolutionary computation technique, which decomposesdecision vectors into smaller components and achieves optimal solution in ashort time. In this technique, a Jacobi-based Time Variant Adaptive (JBTVA)Hybrid Evolutionary Algorithm is distributed incorporating cluster computation.Moreover, two new selection methods named Best All Selection (BAS) and TwinSelection (TS) are introduced for selecting best fit solution vector.Experimental results show that optimal solution is achieved for different kindsof problems having huge parameters and a considerable speedup is obtained inproposed distributed system.
arxiv-2400-74 | Genetic Programming for Document Segmentation and Region Classification Using Discipulus | http://arxiv.org/pdf/1303.0460v1.pdf | author:N. Priyadharshini, M. S. Vijaya category:cs.CV cs.NE published:2013-03-03 summary:Document segmentation is a method of rending the document into distinctregions. A document is an assortment of information and a standard mode ofconveying information to others. Pursuance of data from documents involves tonof human effort, time intense and might severely prohibit the usage of datasystems. So, automatic information pursuance from the document has become a bigissue. It is been shown that document segmentation will facilitate to beat suchproblems. This paper proposes a new approach to segment and classify thedocument regions as text, image, drawings and table. Document image is dividedinto blocks using Run length smearing rule and features are extracted fromevery blocks. Discipulus tool has been used to construct the Geneticprogramming based classifier model and located 97.5% classification accuracy.
arxiv-2400-75 | Statistical sentiment analysis performance in Opinum | http://arxiv.org/pdf/1303.0446v1.pdf | author:Boyan Bonev, Gema Ramírez-Sánchez, Sergio Ortiz Rojas category:cs.CL published:2013-03-03 summary:The classification of opinion texts in positive and negative is becoming asubject of great interest in sentiment analysis. The existence of many labeledopinions motivates the use of statistical and machine-learning methods.First-order statistics have proven to be very limited in this field. The Opinumapproach is based on the order of the words without using any syntactic andsemantic information. It consists of building one probabilistic model for thepositive and another one for the negative opinions. Then the test opinions arecompared to both models and a decision and confidence measure are calculated.In order to reduce the complexity of the training corpus we first lemmatize thetexts and we replace most named-entities with wildcards. Opinum presents anaccuracy above 81% for Spanish opinions in the financial products domain. Inthis work we discuss which are the most important factors that have impact onthe classification performance.
arxiv-2400-76 | Detecting and resolving spatial ambiguity in text using named entity extraction and self learning fuzzy logic techniques | http://arxiv.org/pdf/1303.0445v1.pdf | author:Kanagavalli V R, Raja. K category:cs.IR cs.CL published:2013-03-03 summary:Information extraction identifies useful and relevant text in a document andconverts unstructured text into a form that can be loaded into a databasetable. Named entity extraction is a main task in the process of informationextraction and is a classification problem in which words are assigned to oneor more semantic classes or to a default non-entity class. A word which canbelong to one or more classes and which has a level of uncertainty in it can bebest handled by a self learning Fuzzy Logic Technique. This paper proposes amethod for detecting the presence of spatial uncertainty in the text anddealing with spatial ambiguity using named entity extraction techniques coupledwith self learning fuzzy logic techniques
arxiv-2400-77 | A Cumulative Multi-Niching Genetic Algorithm for Multimodal Function Optimization | http://arxiv.org/pdf/1304.0751v1.pdf | author:Matthew Hall category:cs.NE published:2013-03-03 summary:This paper presents a cumulative multi-niching genetic algorithm (CMN GA),designed to expedite optimization problems that have computationally-expensivemultimodal objective functions. By never discarding individuals from thepopulation, the CMN GA makes use of the information from every objectivefunction evaluation as it explores the design space. A fitness-relatedpopulation density control over the design space reduces unnecessary objectivefunction evaluations. The algorithm's novel arrangement of genetic operationsprovides fast and robust convergence to multiple local optima. Benchmark testsalongside three other multi-niching algorithms show that the CMN GA has agreater convergence ability and provides an order-of-magnitude reduction in thenumber of objective function evaluations required to achieve a given level ofconvergence.
arxiv-2400-78 | Inductive Sparse Subspace Clustering | http://arxiv.org/pdf/1303.0362v1.pdf | author:Xi Peng, Lei Zhang, Zhang Yi category:cs.LG published:2013-03-02 summary:Sparse Subspace Clustering (SSC) has achieved state-of-the-art clusteringquality by performing spectral clustering over a $\ell^{1}$-norm basedsimilarity graph. However, SSC is a transductive method which does not handlewith the data not used to construct the graph (out-of-sample data). For eachnew datum, SSC requires solving $n$ optimization problems in O(n) variables forperforming the algorithm over the whole data set, where $n$ is the number ofdata points. Therefore, it is inefficient to apply SSC in fast onlineclustering and scalable graphing. In this letter, we propose an inductivespectral clustering algorithm, called inductive Sparse Subspace Clustering(iSSC), which makes SSC feasible to cluster out-of-sample data. iSSC adopts theassumption that high-dimensional data actually lie on the low-dimensionalmanifold such that out-of-sample data could be grouped in the embedding spacelearned from in-sample data. Experimental results show that iSSC is promisingin clustering out-of-sample data.
arxiv-2400-79 | Structure-semantics interplay in complex networks and its effects on the predictability of similarity in texts | http://arxiv.org/pdf/1303.0350v1.pdf | author:Diego R. Amancio, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:cs.CL physics.soc-ph published:2013-03-02 summary:There are different ways to define similarity for grouping similar texts intoclusters, as the concept of similarity may depend on the purpose of the task.For instance, in topic extraction similar texts mean those within the samesemantic field, whereas in author recognition stylistic features should beconsidered. In this study, we introduce ways to classify texts employingconcepts of complex networks, which may be able to capture syntactic, semanticand even pragmatic features. The interplay between the various metrics of thecomplex networks is analyzed with three applications, namely identification ofmachine translation (MT) systems, evaluation of quality of machine translatedtexts and authorship recognition. We shall show that topological features ofthe networks representing texts can enhance the ability to identify MT systemsin particular cases. For evaluating the quality of MT texts, on the other hand,high correlation was obtained with methods capable of capturing the semantics.This was expected because the golden standards used are themselves based onword co-occurrence. Notwithstanding, the Katz similarity, which involvessemantic and structure in the comparison of texts, achieved the highestcorrelation with the NIST measurement, indicating that in some cases thecombination of both approaches can improve the ability to quantify quality inMT. In authorship recognition, again the topological features were relevant insome contexts, though for the books and authors analyzed good results wereobtained with semantic features as well. Because hybrid approaches encompassingsemantic and topological features have not been extensively used, we believethat the methodology proposed here may be useful to enhance text classificationconsiderably, as it combines well-established strategies.
arxiv-2400-80 | Probing the statistical properties of unknown texts: application to the Voynich Manuscript | http://arxiv.org/pdf/1303.0347v1.pdf | author:Diego R. Amancio, Eduardo G. Altmann, Diego Rybski, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:physics.soc-ph cs.CL published:2013-03-02 summary:While the use of statistical physics methods to analyze large corpora hasbeen useful to unveil many patterns in texts, no comprehensive investigationhas been performed investigating the properties of statistical measurementsacross different languages and texts. In this study we propose a framework thataims at determining if a text is compatible with a natural language and whichlanguages are closest to it, without any knowledge of the meaning of the words.The approach is based on three types of statistical measurements, i.e. obtainedfrom first-order statistics of word properties in a text, from the topology ofcomplex networks representing text, and from intermittency concepts where textis treated as a time series. Comparative experiments were performed with theNew Testament in 15 different languages and with distinct books in English andPortuguese in order to quantify the dependency of the different measurements onthe language and on the story being told in the book. The metrics found to beinformative in distinguishing real texts from their shuffled versions includeassortativity, degree and selectivity of words. As an illustration, we analyzean undeciphered medieval manuscript known as the Voynich Manuscript. We showthat it is mostly compatible with natural languages and incompatible withrandom texts. We also obtain candidates for key-words of the Voynich Manuscriptwhich could be helpful in the effort of deciphering it. Because we were able toidentify statistical measurements that are more dependent on the syntax than onthe semantics, the framework may also serve for text analysis inlanguage-dependent applications.
arxiv-2400-81 | Learning Hash Functions Using Column Generation | http://arxiv.org/pdf/1303.0339v1.pdf | author:Xi Li, Guosheng Lin, Chunhua Shen, Anton van den Hengel, Anthony Dick category:cs.LG published:2013-03-02 summary:Fast nearest neighbor searching is becoming an increasingly important tool insolving many large-scale problems. Recently a number of approaches to learningdata-dependent hash functions have been developed. In this work, we propose acolumn generation based method for learning data-dependent hash functions onthe basis of proximity comparison information. Given a set of triplets thatencode the pairwise proximity comparison information, our method learns hashfunctions that preserve the relative comparison relationships in the data aswell as possible within the large-margin learning framework. The learningprocedure is implemented using column generation and hence is named CGHash. Ateach iteration of the column generation procedure, the best hash function isselected. Unlike most other hashing methods, our method generalizes to new datapoints naturally; and has a training objective which is convex, thus ensuringthat the global optimum can be identified. Experiments demonstrate that theproposed method learns compact binary codes and that its retrieval performancecompares favorably with state-of-the-art methods when tested on a few benchmarkdatasets.
arxiv-2400-82 | Polyploidy and Discontinuous Heredity Effect on Evolutionary Multi-Objective Optimization | http://arxiv.org/pdf/1302.7051v2.pdf | author:Wesam Elshamy, Hassan M Emara, Ahmed Bahgat category:cs.NE 60G15 published:2013-02-28 summary:This paper examines the effect of mimicking discontinuous heredity caused bycarrying more than one chromosome in some living organisms cells inEvolutionary Multi-Objective Optimization algorithms. In this representation,the phenotype may not fully reflect the genotype. By doing so we are mimickingliving organisms inheritance mechanism, where traits may be silently carriedfor many generations to reappear later. Representations with different numberof chromosomes in each solution vector are tested on different benchmarkproblems with high number of decision variables and objectives. A comparisonwith Non-Dominated Sorting Genetic Algorithm-II is done on all problems.
arxiv-2400-83 | Clubs-based Particle Swarm Optimization | http://arxiv.org/pdf/1303.0323v1.pdf | author:Wesam Elshamy, Hassan M Emara, Ahmed Bahgat category:cs.NE 68T20 published:2013-03-02 summary:This paper introduces a new dynamic neighborhood network for particle swarmoptimization. In the proposed Clubs-based Particle Swarm Optimization (C-PSO)algorithm, each particle initially joins a default number of what we call'clubs'. Each particle is affected by its own experience and the experience ofthe best performing member of the clubs it is a member of. Clubs membership isdynamic, where the worst performing particles socialize more by joining moreclubs to learn from other particles and the best performing particles are madeto socialize less by leaving clubs to reduce their strong influence on othermembers. Particles return gradually to default membership level when they stopshowing extreme performance. Inertia weights of swarm members are made randomwithin a predefined range. This proposed dynamic neighborhood algorithm iscompared with other two algorithms having static neighborhood topologies on aset of classic benchmark problems. The results showed superior performance forC-PSO regarding escaping local optima and convergence speed.
arxiv-2400-84 | Maximal Information Divergence from Statistical Models defined by Neural Networks | http://arxiv.org/pdf/1303.0268v1.pdf | author:Guido Montufar, Johannes Rauh, Nihat Ay category:math.ST stat.ML stat.TH published:2013-03-01 summary:We review recent results about the maximal values of the Kullback-Leiblerinformation divergence from statistical models defined by neural networks,including naive Bayes models, restricted Boltzmann machines, deep beliefnetworks, and various classes of exponential families. We illustrate approachesto compute the maximal divergence from a given model starting from simple sub-or super-models. We give a new result for deep and narrow belief networks withfinite-valued units.
arxiv-2400-85 | Estimating the Maximum Expected Value: An Analysis of (Nested) Cross Validation and the Maximum Sample Average | http://arxiv.org/pdf/1302.7175v2.pdf | author:Hado van Hasselt category:stat.ML cs.AI cs.LG stat.ME published:2013-02-28 summary:We investigate the accuracy of the two most common estimators for the maximumexpected value of a general set of random variables: a generalization of themaximum sample average, and cross validation. No unbiased estimator exists andwe show that it is non-trivial to select a good estimator without knowledgeabout the distributions of the random variables. We investigate and bound thebias and variance of the aforementioned estimators and prove consistency. Thevariance of cross validation can be significantly reduced, but not withoutrisking a large bias. The bias and variance of different variants of crossvalidation are shown to be very problem-dependent, and a wrong choice can leadto very inaccurate estimates.
arxiv-2400-86 | On a link between kernel mean maps and Fraunhofer diffraction, with an application to super-resolution beyond the diffraction limit | http://arxiv.org/pdf/1303.0166v1.pdf | author:Stefan Harmeling, Michael Hirsch, Bernhard Schölkopf category:physics.optics stat.ML published:2013-03-01 summary:We establish a link between Fourier optics and a recent construction from themachine learning community termed the kernel mean map. Using the Fraunhoferapproximation, it identifies the kernel with the squared Fourier transform ofthe aperture. This allows us to use results about the invertibility of thekernel mean map to provide a statement about the invertibility of Fraunhoferdiffraction, showing that imaging processes with arbitrarily small aperturescan in principle be invertible, i.e., do not lose information, provided theobjects to be imaged satisfy a generic condition. A real world experiment showsthat we can super-resolve beyond the Rayleigh limit.
arxiv-2400-87 | Exploiting the Accumulated Evidence for Gene Selection in Microarray Gene Expression Data | http://arxiv.org/pdf/1303.0156v1.pdf | author:G. Prat, Ll. Belanche category:cs.CE cs.LG q-bio.QM I.5.2 published:2013-03-01 summary:Machine Learning methods have of late made significant efforts to solvingmultidisciplinary problems in the field of cancer classification usingmicroarray gene expression data. Feature subset selection methods can play animportant role in the modeling process, since these tasks are characterized bya large number of features and a few observations, making the modeling anon-trivial undertaking. In this particular scenario, it is extremely importantto select genes by taking into account the possible interactions with othergene subsets. This paper shows that, by accumulating the evidence in favour (oragainst) each gene along the search process, the obtained gene subsets mayconstitute better solutions, either in terms of predictive accuracy or genesize, or in both. The proposed technique is extremely simple and applicable ata negligible overhead in cost.
arxiv-2400-88 | Second-Order Non-Stationary Online Learning for Regression | http://arxiv.org/pdf/1303.0140v1.pdf | author:Nina Vaits, Edward Moroshko, Koby Crammer category:cs.LG stat.ML published:2013-03-01 summary:The goal of a learner, in standard online learning, is to have the cumulativeloss not much larger compared with the best-performing function from some fixedclass. Numerous algorithms were shown to have this gap arbitrarily close tozero, compared with the best function that is chosen off-line. Nevertheless,many real-world applications, such as adaptive filtering, are non-stationary innature, and the best prediction function may drift over time. We introduce twonovel algorithms for online regression, designed to work well in non-stationaryenvironment. Our first algorithm performs adaptive resets to forget thehistory, while the second is last-step min-max optimal in context of a drift.We analyze both algorithms in the worst-case regret framework and show thatthey maintain an average loss close to that of the best slowly changingsequence of linear functions, as long as the cumulative drift is sublinear. Inaddition, in the stationary case, when no drift occurs, our algorithms sufferlogarithmic regret, as for previous algorithms. Our bounds improve over theexisting ones, and simulations demonstrate the usefulness of these algorithmscompared with other state-of-the-art approaches.
arxiv-2400-89 | Label-dependent Feature Extraction in Social Networks for Node Classification | http://arxiv.org/pdf/1303.0095v1.pdf | author:Tomasz Kajdanowicz, Przemyslaw Kazienko, Piotr Doskocz category:cs.SI cs.LG I.2.8; I.2.11 published:2013-03-01 summary:A new method of feature extraction in the social network for within-networkclassification is proposed in the paper. The method provides new featurescalculated by combination of both: network structure information and classlabels assigned to nodes. The influence of various features on classificationperformance has also been studied. The experiments on real-world data haveshown that features created owing to the proposed method can lead tosignificant improvement of classification accuracy.
arxiv-2400-90 | Categorizing Bugs with Social Networks: A Case Study on Four Open Source Software Communities | http://arxiv.org/pdf/1302.6764v2.pdf | author:Marcelo Serrano Zanetti, Ingo Scholtes, Claudio Juan Tessone, Frank Schweitzer category:cs.SE cs.LG cs.SI nlin.AO physics.soc-ph published:2013-02-27 summary:Efficient bug triaging procedures are an important precondition forsuccessful collaborative software engineering projects. Triaging bugs canbecome a laborious task particularly in open source software (OSS) projectswith a large base of comparably inexperienced part-time contributors. In thispaper, we propose an efficient and practical method to identify valid bugreports which a) refer to an actual software bug, b) are not duplicates and c)contain enough information to be processed right away. Our classification isbased on nine measures to quantify the social embeddedness of bug reporters inthe collaboration network. We demonstrate its applicability in a case study,using a comprehensive data set of more than 700,000 bug reports obtained fromthe Bugzilla installation of four major OSS communities, for a period of morethan ten years. For those projects that exhibit the lowest fraction of validbug reports, we find that the bug reporters' position in the collaborationnetwork is a strong indicator for the quality of bug reports. Based on thisfinding, we develop an automated classification scheme that can easily beintegrated into bug tracking platforms and analyze its performance in theconsidered OSS communities. A support vector machine (SVM) to identify validbug reports based on the nine measures yields a precision of up to 90.3% withan associated recall of 38.9%. With this, we significantly improve the resultsobtained in previous case studies for an automated early identification of bugsthat are eventually fixed. Furthermore, our study highlights the potential ofusing quantitative measures of social organization in collaborative softwareengineering. It also opens a broad perspective for the integration of socialawareness in the design of support infrastructures.
arxiv-2400-91 | Sparse Shape Reconstruction | http://arxiv.org/pdf/1303.0018v1.pdf | author:Alireza Aghasi, Justin Romberg category:math.FA cs.CV math-ph math.DG math.MP published:2013-02-28 summary:This paper introduces a new shape-based image reconstruction techniqueapplicable to a large class of imaging problems formulated in a variationalsense. Given a collection of shape priors (a shape dictionary), we define ourproblem as choosing the right elements and geometrically composing them throughbasic set operations to characterize desired regions in the image. Thiscombinatorial problem can be relaxed and then solved using classical descentmethods. The main component of this relaxation is forming certain compactlysupported functions which we call "knolls", and reformulating the shaperepresentation as a basis expansion in terms of such functions. To selectsuitable elements of the dictionary, our problem ultimately reduces to solvinga nonlinear program with sparsity constraints. We provide a new sparsenonlinear reconstruction technique to approach this problem. The performance ofproposed technique is demonstrated with some standard imaging problemsincluding image segmentation, X-ray tomography and diffusive tomography.
arxiv-2400-92 | A probabilistic methodology for multilabel classification | http://arxiv.org/pdf/1201.4777v2.pdf | author:Alfonso E. Romero, Luis M. de Campos category:cs.AI cs.LG 68T37, 68T10 published:2012-01-23 summary:Multilabel classification is a relatively recent subfield of machinelearning. Unlike to the classical approach, where instances are labeled withonly one category, in multilabel classification, an arbitrary number ofcategories is chosen to label an instance. Due to the problem complexity (thesolution is one among an exponential number of alternatives), a very commonsolution (the binary method) is frequently used, learning a binary classifierfor every category, and combining them all afterwards. The assumption taken inthis solution is not realistic, and in this work we give examples where thedecisions for all the labels are not taken independently, and thus, asupervised approach should learn those existing relationships among categoriesto make a better classification. Therefore, we show here a generic methodologythat can improve the results obtained by a set of independent probabilisticbinary classifiers, by using a combination procedure with a classifier trainedon the co-occurrences of the labels. We show an exhaustive experimentation inthree different standard corpora of labeled documents (Reuters-21578,Ohsumed-23 and RCV1), which present noticeable improvements in all of them,when using our methodology, in three probabilistic base classifiers.
arxiv-2400-93 | Source Separation using Regularized NMF with MMSE Estimates under GMM Priors with Online Learning for The Uncertainties | http://arxiv.org/pdf/1302.7283v1.pdf | author:Emad M. Grais, Hakan Erdogan category:cs.LG cs.NA published:2013-02-28 summary:We propose a new method to enforce priors on the solution of the nonnegativematrix factorization (NMF). The proposed algorithm can be used for denoising orsingle-channel source separation (SCSS) applications. The NMF solution isguided to follow the Minimum Mean Square Error (MMSE) estimates under Gaussianmixture prior models (GMM) for the source signal. In SCSS applications, thespectra of the observed mixed signal are decomposed as a weighted linearcombination of trained basis vectors for each source using NMF. In this work,the NMF decomposition weight matrices are treated as a distorted image by adistortion operator, which is learned directly from the observed signals. TheMMSE estimate of the weights matrix under GMM prior and log-normal distributionfor the distortion is then found to improve the NMF decomposition results. TheMMSE estimate is embedded within the optimization objective to form a novelregularized NMF cost function. The corresponding update rules for the newobjectives are derived in this paper. Experimental results show that, theproposed regularized NMF algorithm improves the source separation performancecompared with using NMF without prior or with other prior models.
arxiv-2400-94 | Bayesian Consensus Clustering | http://arxiv.org/pdf/1302.7280v1.pdf | author:Eric F. Lock, David B. Dunson category:stat.ML cs.LG published:2013-02-28 summary:The task of clustering a set of objects based on multiple sources of dataarises in several modern applications. We propose an integrative statisticalmodel that permits a separate clustering of the objects for each data source.These separate clusterings adhere loosely to an overall consensus clustering,and hence they are not independent. We describe a computationally scalableBayesian framework for simultaneous estimation of both the consensus clusteringand the source-specific clusterings. We demonstrate that this flexible approachis more robust than joint clustering of all data sources, and is more powerfulthan clustering each data source separately. This work is motivated by theintegrated analysis of heterogeneous biomedical data, and we present anapplication to subtype identification of breast cancer tumor samples usingpublicly available data from The Cancer Genome Atlas. Software is available athttp://people.duke.edu/~el113/software.html.
arxiv-2400-95 | A Linear Time Active Learning Algorithm for Link Classification -- Full Version -- | http://arxiv.org/pdf/1301.4767v2.pdf | author:Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG cs.SI stat.ML published:2013-01-21 summary:We present very efficient active learning algorithms for link classificationin signed networks. Our algorithms are motivated by a stochastic model in whichedge labels are obtained through perturbations of a initial sign assignmentconsistent with a two-clustering of the nodes. We provide a theoreticalanalysis within this model, showing that we can achieve an optimal (to whithina constant factor) number of mistakes on any graph G = (V,E) such that E =\Omega(V^{3/2}) by querying O(V^{3/2}) edge labels. More generally, we showan algorithm that achieves optimality to within a factor of O(k) by querying atmost order of V + (V/k)^{3/2} edge labels. The running time of thisalgorithm is at most of order E + V\logV.
arxiv-2400-96 | A Correlation Clustering Approach to Link Classification in Signed Networks -- Full Version -- | http://arxiv.org/pdf/1301.4769v2.pdf | author:Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG cs.DS stat.ML published:2013-01-21 summary:Motivated by social balance theory, we develop a theory of linkclassification in signed networks using the correlation clustering index asmeasure of label regularity. We derive learning bounds in terms of correlationclustering within three fundamental transductive learning settings: online,batch and active. Our main algorithmic contribution is in the active setting,where we introduce a new family of efficient link classifiers based on coveringthe input graph with small circuits. These are the first active algorithms forlink classification with mistake bounds that hold for arbitrary signednetworks.
arxiv-2400-97 | See the Tree Through the Lines: The Shazoo Algorithm -- Full Version -- | http://arxiv.org/pdf/1301.5160v2.pdf | author:Fabio Vitale, Nicolo Cesa-Bianchi, Claudio Gentile, Giovanni Zappella category:cs.LG published:2013-01-22 summary:Predicting the nodes of a given graph is a fascinating theoretical problemwith applications in several domains. Since graph sparsification via spanningtrees retains enough information while making the task much easier, trees arean important special case of this problem. Although it is known how to predictthe nodes of an unweighted tree in a nearly optimal way, in the weighted case afully satisfactory algorithm is not available yet. We fill this hole andintroduce an efficient node predictor, Shazoo, which is nearly optimal on anyweighted tree. Moreover, we show that Shazoo can be viewed as a commonnontrivial generalization of both previous approaches for unweighted trees andweighted lines. Experiments on real-world datasets confirm that Shazoo performswell in that it fully exploits the structure of the input tree, and gets veryclose to (and sometimes better than) less scalable energy minimization methods.
arxiv-2400-98 | Fast Matching by 2 Lines of Code for Large Scale Face Recognition Systems | http://arxiv.org/pdf/1302.7180v1.pdf | author:Dong Yi, Zhen Lei, Yang Hu, Stan Z. Li category:cs.CV published:2013-02-28 summary:In this paper, we propose a method to apply the popular cascade classifierinto face recognition to improve the computational efficiency while keepinghigh recognition rate. In large scale face recognition systems, because theprobability of feature templates coming from different subjects is very high,most of the matching pairs will be rejected by the early stages of the cascade.Therefore, the cascade can improve the matching speed significantly. On theother hand, using the nested structure of the cascade, we could drop somestages at the end of feature to reduce the memory and bandwidth usage in someresources intensive system while not sacrificing the performance too much. Thecascade is learned by two steps. Firstly, some kind of prepared features aregrouped into several nested stages. And then, the threshold of each stage islearned to achieve user defined verification rate (VR). In the paper, we take alandmark based Gabor+LDA face recognition system as baseline to illustrate theprocess and advantages of the proposed method. However, the use of this methodis very generic and not limited in face recognition, which can be easilygeneralized to other biometrics as a post-processing module. Experiments on theFERET database show the good performance of our baseline and an experiment on aself-collected large scale database illustrates that the cascade can improvethe matching speed significantly.
arxiv-2400-99 | Community Detection in Random Networks | http://arxiv.org/pdf/1302.7099v1.pdf | author:Ery Arias-Castro, Nicolas Verzelen category:math.ST stat.ML stat.TH published:2013-02-28 summary:We formalize the problem of detecting a community in a network into testingwhether in a given (random) graph there is a subgraph that is unusually dense.We observe an undirected and unweighted graph on N nodes. Under the nullhypothesis, the graph is a realization of an Erd\"os-R\'enyi graph withprobability p0. Under the (composite) alternative, there is a subgraph of nnodes where the probability of connection is p1 > p0. We derive a detectionlower bound for detecting such a subgraph in terms of N, n, p0, p1 and exhibita test that achieves that lower bound. We do this both when p0 is known andunknown. We also consider the problem of testing in polynomial-time. As anaside, we consider the problem of detecting a clique, which is intimatelyrelated to the planted clique problem. Our focus in this paper is in thequasi-normal regime where n p0 is either bounded away from zero, or tends tozero slowly.
arxiv-2400-100 | Using Artificial Intelligence Models in System Identification | http://arxiv.org/pdf/1302.7096v1.pdf | author:Wesam Elshamy category:cs.NE cs.SY 68T05 published:2013-02-28 summary:Artificial Intelligence (AI) techniques are known for its ability in tacklingproblems found to be unyielding to traditional mathematical methods. A recentaddition to these techniques are the Computational Intelligence (CI) techniqueswhich, in most cases, are nature or biologically inspired techniques. DifferentCI techniques found their way to many control engineering applications,including system identification, and the results obtained by many researcherswere encouraging. However, most control engineers and researchers used thebasic CI models as is or slightly modified them to match their needs.Henceforth, the merits of one model over the other was not clear, and fullpotential of these models was not exploited. In this research, Genetic Algorithm (GA) and Particle Swarm Optimization(PSO) methods, which are different CI techniques, are modified to best suit themultimodal problem of system identification. In the first case of GA, anextension to the basic algorithm, which is inspired from nature as well, wasdeployed by introducing redundant genetic material. This extension, which comein handy in living organisms, did not result in significant performanceimprovement to the basic algorithm. In the second case, the Clubs-based PSO(C-PSO) dynamic neighborhood structure was introduced to replace the basicstatic structure used in canonical PSO algorithms. This modification of theneighborhood structure resulted in significant performance of the algorithmregarding convergence speed, and equipped it with a tool to handle multimodalproblems. To understand the suitability of different GA and PSO techniques in theproblem of system identification, they were used in an induction motor'sparameter identification problem. The results enforced previous conclusions andshowed the superiority of PSO in general over the GA in such a multimodalproblem.
arxiv-2400-101 | Continuous-time Infinite Dynamic Topic Models | http://arxiv.org/pdf/1302.7088v1.pdf | author:Wesam Elshamy category:cs.IR stat.AP stat.ML 68T10 published:2013-02-28 summary:Topic models are probabilistic models for discovering topical themes incollections of documents. In real world applications, these models provide uswith the means of organizing what would otherwise be unstructured collections.They can help us cluster a huge collection into different topics or find asubset of the collection that resembles the topical theme found in an articleat hand. The first wave of topic models developed were able to discover the prevailingtopics in a big collection of documents spanning a period of time. It was laterrealized that these time-invariant models were not capable of modeling 1) thetime varying number of topics they discover and 2) the time changing structureof these topics. Few models were developed to address this two deficiencies.The online-hierarchical Dirichlet process models the documents with a timevarying number of topics. It varies the structure of the topics over time aswell. However, it relies on document order, not timestamps to evolve the modelover time. The continuous-time dynamic topic model evolves topic structure incontinuous-time. However, it uses a fixed number of topics over time. In this dissertation, I present a model, the continuous-time infinite dynamictopic model, that combines the advantages of these two models 1) theonline-hierarchical Dirichlet process, and 2) the continuous-time dynamic topicmodel. More specifically, the model I present is a probabilistic topic modelthat does the following: 1) it changes the number of topics over continuoustime, and 2) it changes the topic structure over continuous-time. I compared the model I developed with the two other models with differentsetting values. The results obtained were favorable to my model and showed theneed for having a model that has a continuous-time varying number of topics andtopic structure.
arxiv-2400-102 | K Means Segmentation of Alzheimers Disease in PET scan datasets: An implementation | http://arxiv.org/pdf/1302.7082v1.pdf | author:A. Meena, K. Raja category:cs.CV cs.NE published:2013-02-28 summary:The Positron Emission Tomography (PET) scan image requires expertise in thesegmentation where clustering algorithm plays an important role in theautomation process. The algorithm optimization is concluded based on theperformance, quality and number of clusters extracted. This paper is proposedto study the commonly used K Means clustering algorithm and to discuss a brieflist of toolboxes for reproducing and extending works presented in medicalimage analysis. This work is compiled using AForge .NET framework in windowsenvironment and MATrix LABoratory (MATLAB 7.0.1)
arxiv-2400-103 | Parameter Identification of Induction Motor Using Modified Particle Swarm Optimization Algorithm | http://arxiv.org/pdf/1302.7080v1.pdf | author:Hassan M Emara, Wesam Elshamy, Ahmed Bahgat category:cs.NE 68T05 published:2013-02-28 summary:This paper presents a new technique for induction motor parameteridentification. The proposed technique is based on a simple startup test usinga standard V/F inverter. The recorded startup currents are compared to thatobtained by simulation of an induction motor model. A Modified PSO optimizationis used to find out the best model parameter that minimizes the sum squareerror between the measured and the simulated currents. The performance of themodified PSO is compared with other optimization methods including line search,conventional PSO and Genetic Algorithms. Simulation results demonstrate theability of the proposed technique to capture the true values of the machineparameters and the superiority of the results obtained using the modified PSOover other optimization techniques.
arxiv-2400-104 | Learning Theory in the Arithmetic Hierarchy | http://arxiv.org/pdf/1302.7069v1.pdf | author:Achilles Beros category:math.LO cs.LG cs.LO 03D80, 68Q32 published:2013-02-28 summary:We consider the arithmetic complexity of index sets of uniformly computablyenumerable families learnable under different learning criteria. We determinethe exact complexity of these sets for the standard notions of finite learning,learning in the limit, behaviorally correct learning and anomalous learning inthe limit. In proving the $\Sigma_5^0$-completeness result for behaviorallycorrect learning we prove a result of independent interest; if a uniformlycomputably enumerable family is not learnable, then for any computable learnerthere is a $\Delta_2^0$ enumeration witnessing failure.
arxiv-2400-105 | KSU KDD: Word Sense Induction by Clustering in Topic Space | http://arxiv.org/pdf/1302.7056v1.pdf | author:Wesam Elshamy, Doina Caragea, William Hsu category:cs.CL cs.AI stat.AP stat.ML 68T05 published:2013-02-28 summary:We describe our language-independent unsupervised word sense inductionsystem. This system only uses topic features to cluster different word sensesin their global context topic space. Using unlabeled data, this system trains alatent Dirichlet allocation (LDA) topic model then uses it to infer the topicsdistribution of the test instances. By clustering these topics distributions intheir topic space we cluster them into different senses. Our hypothesis is thatcloseness in topic space reflects similarity between different word senses.This system participated in SemEval-2 word sense induction and disambiguationtask and achieved the second highest V-measure score among all other systems.
arxiv-2400-106 | Scoup-SMT: Scalable Coupled Sparse Matrix-Tensor Factorization | http://arxiv.org/pdf/1302.7043v1.pdf | author:Evangelos E. Papalexakis, Tom M. Mitchell, Nicholas D. Sidiropoulos, Christos Faloutsos, Partha Pratim Talukdar, Brian Murphy category:stat.ML cs.LG published:2013-02-28 summary:How can we correlate neural activity in the human brain as it responds towords, with behavioral data expressed as answers to questions about these samewords? In short, we want to find latent variables, that explain both the brainactivity, as well as the behavioral responses. We show that this is an instanceof the Coupled Matrix-Tensor Factorization (CMTF) problem. We proposeScoup-SMT, a novel, fast, and parallel algorithm that solves the CMTF problemand produces a sparse latent low-rank subspace of the data. In our experiments,we find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithmfor CMTF, along with a 5 fold increase in sparsity. Moreover, we extendScoup-SMT to handle missing data without degradation of performance. We applyScoup-SMT to BrainQ, a dataset consisting of a (nouns, brain voxels, humansubjects) tensor and a (nouns, properties) matrix, with coupling along thenouns dimension. Scoup-SMT is able to find meaningful latent variables, as wellas to predict brain activity with competitive accuracy. Finally, we demonstratethe generality of Scoup-SMT, by applying it on a Facebook dataset (users,friends, wall-postings); there, Scoup-SMT spots spammer-like anomalies.
arxiv-2400-107 | Content Based Image Retrieval System Using NOHIS-tree | http://arxiv.org/pdf/1302.7039v1.pdf | author:Mounira Taileb category:cs.IR cs.CV cs.DB H.3.3 published:2013-02-28 summary:Content-based image retrieval (CBIR) has been one of the most importantresearch areas in computer vision. It is a widely used method for searchingimages in huge databases. In this paper we present a CBIR system calledNOHIS-Search. The system is based on the indexing technique NOHIS-tree. The twophases of the system are described and the performance of the system isillustrated with the image database ImagEval. NOHIS-Search system was comparedto other two CBIR systems; the first that using PDDP indexing algorithm and thesecond system is that using the sequential search. Results show thatNOHIS-Search system outperforms the two other systems.
arxiv-2400-108 | Ensemble Sparse Models for Image Analysis | http://arxiv.org/pdf/1302.6957v1.pdf | author:Karthikeyan Natesan Ramamurthy, Jayaraman J. Thiagarajan, Prasanna Sattigeri, Andreas Spanias category:cs.CV published:2013-02-27 summary:Sparse representations with learned dictionaries have been successful inseveral image analysis applications. In this paper, we propose and analyze theframework of ensemble sparse models, and demonstrate their utility in imagerestoration and unsupervised clustering. The proposed ensemble modelapproximates the data as a linear combination of approximations from multiple\textit{weak} sparse models. Theoretical analysis of the ensemble model revealsthat even in the worst-case, the ensemble can perform better than any of itsconstituent individual models. The dictionaries corresponding to the individualsparse models are obtained using either random example selection or boostedapproaches. Boosted approaches learn one dictionary per round such that thedictionary learned in a particular round is optimized for the training exampleshaving high reconstruction error in the previous round. Results with compressedrecovery show that the ensemble representations lead to a better performancecompared to using a single dictionary obtained with the conventionalalternating minimization approach. The proposed ensemble models are also usedfor single image superresolution, and we show that they perform comparably tothe recent approaches. In unsupervised clustering, experiments show that theproposed model performs better than baseline approaches in several standarddatasets.
arxiv-2400-109 | Online Learning for Time Series Prediction | http://arxiv.org/pdf/1302.6927v1.pdf | author:Oren Anava, Elad Hazan, Shie Mannor, Ohad Shamir category:cs.LG published:2013-02-27 summary:In this paper we address the problem of predicting a time series using theARMA (autoregressive moving average) model, under minimal assumptions on thenoise terms. Using regret minimization techniques, we develop effective onlinelearning algorithms for the prediction problem, without assuming that the noiseterms are Gaussian, identically distributed or even independent. Furthermore,we show that our algorithm's performances asymptotically approaches theperformance of the best ARMA model in hindsight.
arxiv-2400-110 | Induction of Selective Bayesian Classifiers | http://arxiv.org/pdf/1302.6828v1.pdf | author:Pat Langley, Stephanie Sage category:cs.LG stat.ML published:2013-02-27 summary:In this paper, we examine previous work on the naive Bayesian classifier andreview its limitations, which include a sensitivity to correlated features. Werespond to this problem by embedding the naive Bayesian induction scheme withinan algorithm that c arries out a greedy search through the space of features.We hypothesize that this approach will improve asymptotic accuracy in domainsthat involve correlated features without reducing the rate of learning in onesthat do not. We report experimental results on six natural domains, includingcomparisons with decision-tree induction, that support these hypotheses. Inclosing, we discuss other approaches to extending naive Bayesian classifiersand outline some directions for future research.
arxiv-2400-111 | Learning Gaussian Networks | http://arxiv.org/pdf/1302.6808v1.pdf | author:Dan Geiger, David Heckerman category:cs.AI cs.LG stat.ML published:2013-02-27 summary:We describe algorithms for learning Bayesian networks from a combination ofuser knowledge and statistical data. The algorithms have two components: ascoring metric and a search procedure. The scoring metric takes a networkstructure, statistical data, and a user's prior knowledge, and returns a scoreproportional to the posterior probability of the network structure given thedata. The search procedure generates networks for evaluation by the scoringmetric. Previous work has concentrated on metrics for domains containing onlydiscrete variables, under the assumption that data represents a multinomialsample. In this paper, we extend this work, developing scoring metrics fordomains containing all continuous variables or a mixture of discrete andcontinuous variables, under the assumption that continuous data is sampled froma multivariate normal distribution. Our work extends traditional statisticalapproaches for identifying vanishing regression coefficients in that weidentify two important assumptions, called event equivalence and parametermodularity, that when combined allow the construction of prior distributionsfor multivariate normal parameters from a single prior Bayesian networkspecified by a user.
arxiv-2400-112 | Ending-based Strategies for Part-of-speech Tagging | http://arxiv.org/pdf/1302.6777v1.pdf | author:Greg Adams, Beth Millar, Eric Neufeld, Tim Philip category:cs.CL published:2013-02-27 summary:Probabilistic approaches to part-of-speech tagging rely primarily onwhole-word statistics about word/tag combinations as well as contextualinformation. But experience shows about 4 per cent of tokens encountered intest sets are unknown even when the training set is as large as a millionwords. Unseen words are tagged using secondary strategies that exploit wordfeatures such as endings, capitalizations and punctuation marks. In this work,word-ending statistics are primary and whole-word statistics are secondary.First, a tagger was trained and tested on word endings only. Subsequentexperiments added back whole-word statistics for the words occurring mostfrequently in the training set. As grew larger, performance was expected toimprove, in the limit performing the same as word-based taggers. Surprisingly,the ending-based tagger initially performed nearly as well as the word-basedtagger; in the best case, its performance significantly exceeded that of theword-based tagger. Lastly, and unexpectedly, an effect of negative returns wasobserved - as grew larger, performance generally improved and then declined. Byvarying factors such as ending length and tag-list strategy, we achieved asuccess rate of 97.5 percent.
arxiv-2400-113 | A bag-of-paths framework for network data analysis | http://arxiv.org/pdf/1302.6766v1.pdf | author:Kevin Françoisse, Ilkka Kivimäki, Amin Mantrach, Fabrice Rossi, Marco Saerens category:stat.ML published:2013-02-27 summary:This work introduces a generic framework, called the bag-of-paths (BoP), thatcan be used for link and network data analysis. The primary application of thisframework, investigated in this paper, is the definition of distance measuresbetween nodes enjoying some nice properties. More precisely, let us assume aweighted directed graph G where a cost is associated to each arc. Within thiscontext, consider a bag containing all the possible paths between pairs ofnodes in G. Then, following, a probability distribution on this countable setof paths through the graph is defined by minimizing the total expected costbetween all pairs of nodes while fixing the total relative entropy spread inthe graph. This results in a Boltzmann distribution on the set of paths suchthat long (high-cost) paths have a low probability of being sampled from thebag, while short (low-cost) paths have a high probability of being sampled.Within this probabilistic framework, the BoP probabilities, P(s=i,e=j), ofdrawing a path starting from node i (s=i) and ending in node j (e=j) can easilybe computed in closed form by a simple matrix inversion. Various applicationsof this framework are currently investigated, e.g., the definition of distancemeasures between the nodes of G, betweenness indexes, network criticalitymeasures, edit distances, etc. As a first step, this paper describes thegeneral BoP framework and introduces two families of distance measures betweennodes. In addition to being a distance measure, one of these two quantities hasthe interesting property of interpolating between the shortest path and thecommute cost distances. Experimental results on semi-supervised tasks show thatthese distance families are competitive with other state-of-the-art approaches.
arxiv-2400-114 | Accuracy guaranties for $\ell_1$ recovery of block-sparse signals | http://arxiv.org/pdf/1111.2546v2.pdf | author:Anatoli Juditsky, Fatma Kılınç Karzan, Arkadi Nemirovski, Boris Polyak category:math.ST math.OC stat.ML stat.TH published:2011-11-10 summary:We introduce a general framework to handle structured models (sparse andblock-sparse with possibly overlapping blocks). We discuss new methods fortheir recovery from incomplete observation, corrupted with deterministic andstochastic noise, using block-$\ell_1$ regularization. While the current theoryprovides promising bounds for the recovery errors under a number of different,yet mostly hard to verify conditions, our emphasis is on verifiable conditionson the problem parameters (sensing matrix and the block structure) whichguarantee accurate recovery. Verifiability of our conditions not only leads toefficiently computable bounds for the recovery error but also allows us tooptimize these error bounds with respect to the method parameters, andtherefore construct estimators with improved statistical properties. To justifyour approach, we also provide an oracle inequality, which links the propertiesof the proposed recovery algorithms and the best estimation performance.Furthermore, utilizing these verifiable conditions, we develop acomputationally cheap alternative to block-$\ell_1$ minimization, thenon-Euclidean Block Matching Pursuit algorithm. We close by presenting anumerical study to investigate the effect of different block regularizationsand demonstrate the performance of the proposed recoveries.
arxiv-2400-115 | Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization | http://arxiv.org/pdf/1302.6677v1.pdf | author:Stefano Ermon, Carla P. Gomes, Ashish Sabharwal, Bart Selman category:cs.LG cs.AI stat.ML published:2013-02-27 summary:Integration is affected by the curse of dimensionality and quickly becomesintractable as the dimensionality of the problem grows. We propose a randomizedalgorithm that, with high probability, gives a constant-factor approximation ofa general discrete integral defined over an exponentially large set. Thisalgorithm relies on solving only a small number of instances of a discretecombinatorial optimization problem subject to randomly generated parityconstraints used as a hash function. As an application, we demonstrate thatwith a small number of MAP queries we can efficiently approximate the partitionfunction of discrete graphical models, which can in turn be used, for instance,for marginal computation or model selection.
arxiv-2400-116 | Convex vs nonconvex approaches for sparse estimation: GLasso, Multiple Kernel Learning and Hyperparameter GLasso | http://arxiv.org/pdf/1302.6434v2.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Alessandro Chiuso, Gianluigi Pillonetto category:stat.ML math.OC published:2013-02-26 summary:The popular Lasso approach for sparse estimation can be derived viamarginalization of a joint density associated with a particular stochasticmodel. A different marginalization of the same probabilistic model leads to adifferent non-convex estimator where hyperparameters are optimized. Extendingthese arguments to problems where groups of variables have to be estimated, westudy a computational scheme for sparse estimation that differs from the GroupLasso. Although the underlying optimization problem defining this estimator isnon-convex, an initialization strategy based on a univariate Bayesian forwardselection scheme is presented. This also allows us to define an effectivenon-convex estimator where only one scalar variable is involved in theoptimization process. Theoretical arguments, independent of the correctness ofthe priors entering the sparse model, are included to clarify the advantages ofthis non-convex technique in comparison with other convex estimators. Numericalexperiments are also used to compare the performance of these approaches.
arxiv-2400-117 | Arriving on time: estimating travel time distributions on large-scale road networks | http://arxiv.org/pdf/1302.6617v1.pdf | author:Timothy Hunter, Aude Hofleitner, Jack Reilly, Walid Krichene, Jerome Thai, Anastasios Kouvelas, Pieter Abbeel, Alexandre Bayen category:cs.LG cs.AI published:2013-02-26 summary:Most optimal routing problems focus on minimizing travel time or distancetraveled. Oftentimes, a more useful objective is to maximize the probability ofon-time arrival, which requires statistical distributions of travel times,rather than just mean values. We propose a method to estimate travel timedistributions on large-scale road networks, using probe vehicle data collectedfrom GPS. We present a framework that works with large input of data, andscales linearly with the size of the network. Leveraging the planar topology ofthe graph, the method computes efficiently the time correlations betweenneighboring streets. First, raw probe vehicle traces are compressed into pairsof travel times and number of stops for each traversed road segment using a`stop-and-go' algorithm developed for this work. The compressed data is thenused as input for training a path travel time model, which couples a Markovmodel along with a Gaussian Markov random field. Finally, scalable inferencealgorithms are developed for obtaining path travel time distributions from thecomposite MM-GMRF model. We illustrate the accuracy and scalability of ourmodel on a 505,000 road link network spanning the San Francisco Bay Area.
arxiv-2400-118 | PSO based Neural Networks vs. Traditional Statistical Models for Seasonal Time Series Forecasting | http://arxiv.org/pdf/1302.6615v1.pdf | author:Ratnadip Adhikari, R. K. Agrawal, Laxmi Kant category:cs.NE 68T05 published:2013-02-26 summary:Seasonality is a distinctive characteristic which is often observed in manypractical time series. Artificial Neural Networks (ANNs) are a class ofpromising models for efficiently recognizing and forecasting seasonal patterns.In this paper, the Particle Swarm Optimization (PSO) approach is used toenhance the forecasting strengths of feedforward ANN (FANN) as well as ElmanANN (EANN) models for seasonal data. Three widely popular versions of the basicPSO algorithm, viz. Trelea-I, Trelea-II and Clerc-Type1 are considered here.The empirical analysis is conducted on three real-world seasonal time series.Results clearly show that each version of the PSO algorithm achieves notablybetter forecasting accuracies than the standard Backpropagation (BP) trainingmethod for both FANN and EANN models. The neural network forecasting resultsare also compared with those from the three traditional statistical models,viz. Seasonal Autoregressive Integrated Moving Average (SARIMA), Holt-Winters(HW) and Support Vector Machine (SVM). The comparison demonstrates that bothPSO and BP based neural networks outperform SARIMA, HW and SVM models for allthree time series datasets. The forecasting performances of ANNs are furtherimproved through combining the outputs from the three PSO based models.
arxiv-2400-119 | An Introductory Study on Time Series Modeling and Forecasting | http://arxiv.org/pdf/1302.6613v1.pdf | author:Ratnadip Adhikari, R. K. Agrawal category:cs.LG stat.ML 68T01 published:2013-02-26 summary:Time series modeling and forecasting has fundamental importance to variouspractical domains. Thus a lot of active research works is going on in thissubject during several years. Many important models have been proposed inliterature for improving the accuracy and effectiveness of time seriesforecasting. The aim of this dissertation work is to present a concisedescription of some popular time series forecasting models used in practice,with their salient features. In this thesis, we have described three importantclasses of time series models, viz. the stochastic, neural networks and SVMbased models, together with their inherent forecasting strengths andweaknesses. We have also discussed about the basic issues related to timeseries modeling, such as stationarity, parsimony, overfitting, etc. Ourdiscussion about different time series models is supported by giving theexperimental forecast results, performed on six real time series datasets.While fitting a model to a dataset, special care is taken to select the mostparsimonious one. To evaluate forecast accuracy as well as to compare amongdifferent models fitted to a time series, we have used the five performancemeasures, viz. MSE, MAD, RMSE, MAPE and Theil's U-statistics. For each of thesix datasets, we have shown the obtained forecast diagram which graphicallydepicts the closeness between the original and forecasted observations. To haveauthenticity as well as clarity in our discussion about time series modelingand forecasting, we have taken the help of various published research worksfrom reputed journals and some standard books.
arxiv-2400-120 | Sparse Frequency Analysis with Sparse-Derivative Instantaneous Amplitude and Phase Functions | http://arxiv.org/pdf/1302.6523v1.pdf | author:Yin Ding, Ivan W. Selesnick category:cs.LG published:2013-02-26 summary:This paper addresses the problem of expressing a signal as a sum of frequencycomponents (sinusoids) wherein each sinusoid may exhibit abrupt changes in itsamplitude and/or phase. The Fourier transform of a narrow-band signal, with adiscontinuous amplitude and/or phase function, exhibits spectral and temporalspreading. The proposed method aims to avoid such spreading by explicitlymodeling the signal of interest as a sum of sinusoids with time-varyingamplitudes. So as to accommodate abrupt changes, it is further assumed that theamplitude/phase functions are approximately piecewise constant (i.e., theirtime-derivatives are sparse). The proposed method is based on a convexvariational (optimization) approach wherein the total variation (TV) of theamplitude functions are regularized subject to a perfect (or approximate)reconstruction constraint. A computationally efficient algorithm is derivedbased on convex optimization techniques. The proposed technique can be used toperform band-pass filtering that is relatively insensitive to narrow-bandamplitude/phase jumps present in data, which normally pose a challenge (due totransients, leakage, etc.). The method is illustrated using both syntheticsignals and human EEG data for the purpose of band-pass filtering and theestimation of phase synchrony indexes.
arxiv-2400-121 | A Conformal Prediction Approach to Explore Functional Data | http://arxiv.org/pdf/1302.6452v1.pdf | author:Jing Lei, Alessandro Rinaldo, Larry Wasserman category:stat.ML cs.LG published:2013-02-26 summary:This paper applies conformal prediction techniques to compute simultaneousprediction bands and clustering trees for functional data. These tools can beused to detect outliers and clusters. Both our prediction bands and clusteringtrees provide prediction sets for the underlying stochastic process with aguaranteed finite sample behavior, under no distributional assumptions. Theprediction sets are also informative in that they correspond to the highdensity region of the underlying process. While ordinary conformal predictionhas high computational cost for functional data, we use the inductive conformalpredictor, together with several novel choices of conformity scores, tosimplify the computation. Our methods are illustrated on some real dataexamples.
arxiv-2400-122 | Segmentation of Alzheimers Disease in PET scan datasets using MATLAB | http://arxiv.org/pdf/1302.6426v1.pdf | author:A. Meena, K. Raja category:cs.NE published:2013-02-26 summary:Positron Emission Tomography (PET) scan images are one of the bio medicalimaging techniques similar to that of MRI scan images but PET scan images arehelpful in finding the development of tumors.The PET scan images requiresexpertise in the segmentation where clustering plays an important role in theautomation process.The segmentation of such images is manual to automate theprocess clustering is used.Clustering is commonly known as unsupervisedlearning process of n dimensional data sets are clustered into k groups so asto maximize the inter cluster similarity and to minimize the intra clustersimilarity.This paper is proposed to implement the commonly used K Means andFuzzy CMeans (FCM) clustering algorithm.This work is implemented using MATrixLABoratory (MATLAB) and tested with sample PET scan image. The sample data iscollected from Alzheimers Disease Neuro imaging Initiative ADNI. Medical ImageProcessing and Visualization Tool (MIPAV) are used to compare the resultantimages.
arxiv-2400-123 | The adaptive Gril estimator with a diverging number of parameters | http://arxiv.org/pdf/1302.6390v1.pdf | author:Mohammed El Anbari, Abdallah Mkhadri category:stat.ME cs.LG published:2013-02-26 summary:We consider the problem of variables selection and estimation in linearregression model in situations where the number of parameters diverges with thesample size. We propose the adaptive Generalized Ridge-Lasso (\mbox{AdaGril})which is an extension of the the adaptive Elastic Net. AdaGril incorporatesinformation redundancy among correlated variables for model selection andestimation. It combines the strengths of the quadratic regularization and theadaptively weighted Lasso shrinkage. In this paper, we highlight the groupedselection property for AdaCnet method (one type of AdaGril) in the equalcorrelation case. Under weak conditions, we establish the oracle property ofAdaGril which ensures the optimal large performance when the dimension is high.Consequently, it achieves both goals of handling the problem of collinearity inhigh dimension and enjoys the oracle property. Moreover, we show that AdaGrilestimator achieves a Sparsity Inequality, i. e., a bound in terms of the numberof non-zero components of the 'true' regression coefficient. This bound isobtained under a similar weak Restricted Eigenvalue (RE) condition used forLasso. Simulations studies show that some particular cases of AdaGriloutperform its competitors.
arxiv-2400-124 | Image-based Face Detection and Recognition: "State of the Art" | http://arxiv.org/pdf/1302.6379v1.pdf | author:Faizan Ahmad, Aaima Najam, Zeeshan Ahmed category:cs.CV published:2013-02-26 summary:Face recognition from image or video is a popular topic in biometricsresearch. Many public places usually have surveillance cameras for videocapture and these cameras have their significant value for security purpose. Itis widely acknowledged that the face recognition have played an important rolein surveillance system as it doesn't need the object's cooperation. The actualadvantages of face based identification over other biometrics are uniquenessand acceptance. As human face is a dynamic object having high degree ofvariability in its appearance, that makes face detection a difficult problem incomputer vision. In this field, accuracy and speed of identification is a mainissue. The goal of this paper is to evaluate various face detection and recognitionmethods, provide complete solution for image based face detection andrecognition with higher accuracy, better response rate as an initial step forvideo surveillance. Solution is proposed based on performed tests on variousface rich databases in terms of subjects, pose, emotions, race and light.
arxiv-2400-125 | Metrics for Multivariate Dictionaries | http://arxiv.org/pdf/1302.4242v2.pdf | author:Sylvain Chevallier, Quentin Barthélemy, Jamal Atif category:cs.LG stat.ML K.3.2 published:2013-02-18 summary:Overcomplete representations and dictionary learning algorithms keptattracting a growing interest in the machine learning community. This paperaddresses the emerging problem of comparing multivariate overcompleterepresentations. Despite a recurrent need to rely on a distance for learning orassessing multivariate overcomplete representations, no metrics in theirunderlying spaces have yet been proposed. Henceforth we propose to studyovercomplete representations from the perspective of frame theory and matrixmanifolds. We consider distances between multivariate dictionaries as distancesbetween their spans which reveal to be elements of a Grassmannian manifold. Weintroduce Wasserstein-like set-metrics defined on Grassmannian spaces and studytheir properties both theoretically and numerically. Indeed a deep experimentalstudy based on tailored synthetic datasetsand real EEG signals forBrain-Computer Interfaces (BCI) have been conducted. In particular, theintroduced metrics have been embedded in clustering algorithm and applied toBCI Competition IV-2a for dataset quality assessment. Besides, a principledconnection is made between three close but still disjoint research fields,namely, Grassmannian packing, dictionary learning and compressed sensing.
arxiv-2400-126 | Non-simplifying Graph Rewriting Termination | http://arxiv.org/pdf/1302.6334v1.pdf | author:Guillaume Bonfante, Bruno Guillaume category:cs.CL cs.CC cs.LO published:2013-02-26 summary:So far, a very large amount of work in Natural Language Processing (NLP) relyon trees as the core mathematical structure to represent linguisticinformations (e.g. in Chomsky's work). However, some linguistic phenomena donot cope properly with trees. In a former paper, we showed the benefit ofencoding linguistic structures by graphs and of using graph rewriting rules tocompute on those structures. Justified by some linguistic considerations, graphrewriting is characterized by two features: first, there is no node creationalong computations and second, there are non-local edge modifications. Underthese hypotheses, we show that uniform termination is undecidable and thatnon-uniform termination is decidable. We describe two termination techniquesbased on weights and we give complexity bound on the derivation length forthese rewriting system.
arxiv-2400-127 | Rate-Distortion Bounds for an Epsilon-Insensitive Distortion Measure | http://arxiv.org/pdf/1302.6315v1.pdf | author:Kazuho Watanabe category:cs.IT cs.LG math.IT published:2013-02-26 summary:Direct evaluation of the rate-distortion function has rarely been achievedwhen it is strictly greater than its Shannon lower bound. In this paper, weconsider the rate-distortion function for the distortion measure defined by anepsilon-insensitive loss function. We first present the Shannon lower boundapplicable to any source distribution with finite differential entropy. Then,focusing on the Laplacian and Gaussian sources, we prove that therate-distortion functions of these sources are strictly greater than theirShannon lower bounds and obtain analytically evaluable upper bounds for therate-distortion functions. Small distortion limit and numerical evaluation ofthe bounds suggest that the Shannon lower bound provides a good approximationto the rate-distortion function for the epsilon-insensitive distortion measure.
arxiv-2400-128 | Estimating Sectoral Pollution Load in Lagos, Nigeria Using Data Mining Techniques | http://arxiv.org/pdf/1302.6310v1.pdf | author:Adesesan . B Adeyemo, Adebola A. Oketola, Emmanuel O. Adetula, O. Osibanjo category:cs.NE published:2013-02-26 summary:Industrial pollution is often considered to be one of the prime factorscontributing to air, water and soil pollution. Sectoral pollution loads(ton/yr) into different media (i.e. air, water and land) in Lagos wereestimated using Industrial Pollution Projected System (IPPS). These werefurther studied using Artificial neural Networks (ANNs), a data miningtechnique that has the ability of detecting and describing patterns in largedata sets with variables that are non- linearly related. Time Lagged RecurrentNetwork (TLRN) appeared as the best Neural Network model among all the neuralnetworks considered which includes Multilayer Perceptron (MLP) Network,Generalized Feed Forward Neural Network (GFNN), Radial Basis Function (RBF)Network and Recurrent Network (RN). TLRN modelled the data-sets better than theothers in terms of the mean average error (MAE) (0.14), time (39 s) and linearcorrelation coefficient (0.84). The results showed that Artificial NeuralNetworks (ANNs) technique (i.e., Time Lagged Recurrent Network) is alsoapplicable and effective in environmental assessment study. Keywords:Artificial Neural Networks (ANNs), Data Mining Techniques, Industrial PollutionProjection System (IPPS), Pollution load, Pollution Intensity.
arxiv-2400-129 | Mixture decompositions of exponential families using a decomposition of their sample spaces | http://arxiv.org/pdf/1008.0204v4.pdf | author:Guido Montufar category:math.ST stat.ML stat.TH published:2010-08-01 summary:We study the problem of finding the smallest $m$ such that every element ofan exponential family can be written as a mixture of $m$ elements of anotherexponential family. We propose an approach based on coverings and packings ofthe face lattice of the corresponding convex support polytopes and results fromcoding theory. We show that $m=q^{N-1}$ is the smallest number for which anydistribution of $N$ $q$-ary variables can be written as mixture of $m$independent $q$-ary variables. Furthermore, we show that any distribution of$N$ binary variables is a mixture of $m = 2^{N-(k+1)}(1+ 1/(2^k-1))$ elementsof the $k$-interaction exponential family.
arxiv-2400-130 | A Homogeneous Ensemble of Artificial Neural Networks for Time Series Forecasting | http://arxiv.org/pdf/1302.6210v1.pdf | author:Ratnadip Adhikari, R. K. Agrawal category:cs.NE cs.LG 68T05 published:2013-02-25 summary:Enhancing the robustness and accuracy of time series forecasting models is anactive area of research. Recently, Artificial Neural Networks (ANNs) have foundextensive applications in many practical forecasting problems. However, thestandard backpropagation ANN training algorithm has some critical issues, e.g.it has a slow convergence rate and often converges to a local minimum, thecomplex pattern of error surfaces, lack of proper training parameters selectionmethods, etc. To overcome these drawbacks, various improved training methodshave been developed in literature; but, still none of them can be guaranteed asthe best for all problems. In this paper, we propose a novel weighted ensemblescheme which intelligently combines multiple training algorithms to increasethe ANN forecast accuracies. The weight for each training algorithm isdetermined from the performance of the corresponding ANN model on thevalidation dataset. Experimental results on four important time series depictsthat our proposed technique reduces the mentioned shortcomings of individualANN training algorithms to a great extent. Also it achieves significantlybetter forecast accuracies than two other popular statistical models.
arxiv-2400-131 | Scaling of Model Approximation Errors and Expected Entropy Distances | http://arxiv.org/pdf/1207.3399v2.pdf | author:Guido F. Montufar, Johannes Rauh category:stat.ML 94A17, 62B15 published:2012-07-14 summary:We compute the expected value of the Kullback-Leibler divergence to variousfundamental statistical models with respect to canonical priors on theprobability simplex. We obtain closed formulas for the expected modelapproximation errors, depending on the dimension of the models and thecardinalities of their sample spaces. For the uniform prior, the expecteddivergence from any model containing the uniform distribution is bounded by aconstant $1-\gamma$, and for the models that we consider, this bound isapproached if the state space is very large and the models' dimension does notgrow too fast. For Dirichlet priors the expected divergence is bounded in asimilar way, if the concentration parameters take reasonable values. Theseresults serve as reference values for more complicated statistical models.
arxiv-2400-132 | Phoneme discrimination using $KS$-algebra II | http://arxiv.org/pdf/1302.6194v1.pdf | author:Ondrej Such, Lenka Mackovicova category:cs.SD cs.LG stat.ML I.2.7; I.5.4 published:2013-02-25 summary:$KS$-algebra consists of expressions constructed with four kinds operations,the minimum, maximum, difference and additively homogeneous generalized means.Five families of $Z$-classifiers are investigated on binary classificationtasks between English phonemes. It is shown that the classifiers are able toreflect well known formant characteristics of vowels, while having very smallKolmogoroff's complexity.
arxiv-2400-133 | Estimating Unknown Sparsity in Compressed Sensing | http://arxiv.org/pdf/1204.4227v2.pdf | author:Miles E. Lopes category:cs.IT math.IT math.ST stat.ME stat.ML stat.TH published:2012-04-19 summary:In the theory of compressed sensing (CS), the sparsity x_0 of the unknownsignal x\in\R^p is commonly assumed to be a known parameter. However, it istypically unknown in practice. Due to the fact that many aspects of CS dependon knowing x_0, it is important to estimate this parameter in a data-drivenway. A second practical concern is that x_0 is a highly unstable functionof x. In particular, for real signals with entries not exactly equal to 0, thevalue x_0=p is not a useful description of the effective number ofcoordinates. In this paper, we propose to estimate a stable measure of sparsitys(x):=x_1^2/x_2^2, which is a sharp lower bound on x_0. Ourestimation procedure uses only a small number of linear measurements, does notrely on any sparsity assumptions, and requires very little computation. Aconfidence interval for s(x) is provided, and its width is shown to have nodependence on the signal dimension p. Moreover, this result extends naturallyto the matrix recovery setting, where a soft version of matrix rank can beestimated with analogous guarantees. Finally, we show that the use ofrandomized measurements is essential to estimating s(x). This is accomplishedby proving that the minimax risk for estimating s(x) with deterministicmeasurements is large when n<<p.
arxiv-2400-134 | Phoneme discrimination using KS algebra I | http://arxiv.org/pdf/1302.6031v1.pdf | author:Ondrej Such category:cs.SD cs.AI cs.NE published:2013-02-25 summary:In our work we define a new algebra of operators as a substitute for fuzzylogic. Its primary purpose is for construction of binary discriminators forphonemes based on spectral content. It is optimized for design ofnon-parametric computational circuits, and makes uses of 4 operations: $\min$,$\max$, the difference and generalized additively homogenuous means.
arxiv-2400-135 | On learning parametric-output HMMs | http://arxiv.org/pdf/1302.6009v1.pdf | author:Aryeh Kontorovich, Boaz Nadler, Roi Weiss category:cs.LG math.ST stat.ML stat.TH published:2013-02-25 summary:We present a novel approach for learning an HMM whose outputs are distributedaccording to a parametric family. This is done by {\em decoupling} the learningtask into two steps: first estimating the output parameters, and thenestimating the hidden states transition probabilities. The first step isaccomplished by fitting a mixture model to the output stationary distribution.Given the parameters of this mixture model, the second step is formulated asthe solution of an easily solvable convex quadratic program. We provide anerror analysis for the estimated transition probabilities and show they arerobust to small perturbations in the estimates of the mixture parameters.Finally, we support our analysis with some encouraging empirical results.
arxiv-2400-136 | A Meta-Theory of Boundary Detection Benchmarks | http://arxiv.org/pdf/1302.5985v1.pdf | author:Xiaodi Hou, Alan Yuille, Christof Koch category:cs.CV published:2013-02-25 summary:Human labeled datasets, along with their corresponding evaluation algorithms,play an important role in boundary detection. We here present a psychophysicalexperiment that addresses the reliability of such benchmarks. To find betterremedies to evaluate the performance of any boundary detection algorithm, wepropose a computational framework to remove inappropriate human labels andestimate the intrinsic properties of boundaries.
arxiv-2400-137 | Shape Characterization via Boundary Distortion | http://arxiv.org/pdf/1302.5957v1.pdf | author:Xavier Descombes, Serguei Komech category:cs.CV published:2013-02-24 summary:In this paper, we derive new shape descriptors based on a directionalcharacterization. The main idea is to study the behavior of the shapeneighborhood under family of transformations. We obtain a description invariantwith respect to rotation, reflection, translation and scaling. A well-definedmetric is then proposed on the associated feature space. We show the continuityof this metric. Some results on shape retrieval are provided on two databasesto show the accuracy of the proposed shape metric.
arxiv-2400-138 | Four Side Distance: A New Fourier Shape Signature | http://arxiv.org/pdf/1302.5894v1.pdf | author:Sonya Eini, Abdolah Chalechale category:cs.CV published:2013-02-24 summary:Shape is one of the main features in content based image retrieval (CBIR).This paper proposes a new shape signature. In this technique, features of eachshape are extracted based on four sides of the rectangle that covers the shape.The proposed technique is Fourier based and it is invariant to translation,scaling and rotation. The retrieval performance between some commonly usedFourier based signatures and the proposed four sides distance (FSD) signaturehas been tested using MPEG-7 database. Experimental results are shown that theFSD signature has better performance compared with those signatures.
arxiv-2400-139 | Authorship Identification in Bengali Literature: a Comparative Analysis | http://arxiv.org/pdf/1208.6268v4.pdf | author:Tanmoy Chakraborty category:cs.CL cs.IR published:2012-08-30 summary:Stylometry is the study of the unique linguistic styles and writing behaviorsof individuals. It belongs to the core task of text categorization likeauthorship identification, plagiarism detection etc. Though reasonable numberof studies have been conducted in English language, no major work has been doneso far in Bengali. In this work, We will present a demonstration of authorshipidentification of the documents written in Bengali. We adopt a set offine-grained stylistic features for the analysis of the text and use them todevelop two different models: statistical similarity model consisting of threemeasures and their combination, and machine learning model with Decision Tree,Neural Network and SVM. Experimental results show that SVM outperforms otherstate-of-the-art methods after 10-fold cross validations. We also validate therelative importance of each stylistic feature to show that some of them remainconsistently significant in every model used in this experiment.
arxiv-2400-140 | Learning a Factor Model via Regularized PCA | http://arxiv.org/pdf/1111.6201v4.pdf | author:Yi-Hao Kao, Benjamin Van Roy category:cs.LG stat.ML published:2011-11-26 summary:We consider the problem of learning a linear factor model. We propose aregularized form of principal component analysis (PCA) and demonstrate throughexperiments with synthetic and real data the superiority of resulting estimatesto those produced by pre-existing factor analysis approaches. We also establishtheoretical results that explain how our algorithm corrects the biases inducedby conventional approaches. An important feature of our algorithm is that itscomputational requirements are similar to those of PCA, which enjoys wide usein large part due to its efficiency.
arxiv-2400-141 | Prediction by Random-Walk Perturbation | http://arxiv.org/pdf/1302.5797v1.pdf | author:Luc Devroye, Gábor Lugosi, Gergely Neu category:cs.LG published:2013-02-23 summary:We propose a version of the follow-the-perturbed-leader online predictionalgorithm in which the cumulative losses are perturbed by independent symmetricrandom walks. The forecaster is shown to achieve an expected regret of theoptimal order O(sqrt(n log N)) where n is the time horizon and N is the numberof experts. More importantly, it is shown that the forecaster changes itsprediction at most O(sqrt(n log N)) times, in expectation. We also extend theanalysis to online combinatorial optimization and show that even in this moregeneral setting, the forecaster rarely switches between experts while having aregret of near-optimal order.
arxiv-2400-142 | Probabilistic Non-Local Means | http://arxiv.org/pdf/1302.5762v1.pdf | author:Yue Wu, Brian Tracey, Premkumar Natarajan, Joseph P. Noonan category:cs.CV stat.AP stat.CO published:2013-02-23 summary:In this paper, we propose a so-called probabilistic non-local means (PNLM)method for image denoising. Our main contributions are: 1) we point out defectsof the weight function used in the classic NLM; 2) we successfully derive alltheoretical statistics of patch-wise differences for Gaussian noise; and 3) weemploy this prior information and formulate the probabilistic weights trulyreflecting the similarity between two noisy patches. The probabilistic natureof the new weight function also provides a theoretical basis to choosethresholds rejecting dissimilar patches for fast computations. Our simulationresults indicate the PNLM outperforms the classic NLM and many NLM recentvariants in terms of peak signal noise ratio (PSNR) and structural similarity(SSIM) index. Encouraging improvements are also found when we replace the NLMweights with the probabilistic weights in tested NLM variants.
arxiv-2400-143 | Learning Theory Approach to Minimum Error Entropy Criterion | http://arxiv.org/pdf/1208.0848v2.pdf | author:Ting Hu, Jun Fan, Qiang Wu, Ding-Xuan Zhou category:cs.LG stat.ML published:2012-08-03 summary:We consider the minimum error entropy (MEE) criterion and an empirical riskminimization learning algorithm in a regression setting. A learning theoryapproach is presented for this MEE algorithm and explicit error bounds areprovided in terms of the approximation ability and capacity of the involvedhypothesis space when the MEE scaling parameter is large. Novel asymptoticanalysis is conducted for the generalization error associated with Renyi'sentropy and a Parzen window function, to overcome technical difficulties arisenfrom the essential differences between the classical least squares problems andthe MEE setting. A semi-norm and the involved symmetrized least squares errorare introduced, which is related to some ranking algorithms.
arxiv-2400-144 | Development of Yes/No Arabic Question Answering System | http://arxiv.org/pdf/1302.5675v1.pdf | author:Wafa N. Bdour, Natheer K. Gharaibeh category:cs.CL cs.IR 14J26 published:2013-02-22 summary:Developing Question Answering systems has been one of the important researchissues because it requires insights from a variety ofdisciplines,including,Artificial Intelligence,Information Retrieval,Information Extraction,Natural Language Processing, and Psychology.In thispaper we realize a formal model for a lightweight semantic based open domainyes/no Arabic question answering system based on paragraph retrieval withvariable length. We propose a constrained semantic representation. Using anexplicit unification framework based on semantic similarities and queryexpansion synonyms and antonyms.This frequently improves the precision of thesystem. Employing the passage retrieval system achieves a better precision byretrieving more paragraphs that contain relevant answers to the question; Itsignificantly reduces the amount of text to be processed by the system.
arxiv-2400-145 | Soft Rule Ensembles for Statistical Learning | http://arxiv.org/pdf/1205.4476v3.pdf | author:Deniz Akdemir, Nicolas Heslot category:stat.ML cs.LG stat.AP published:2012-05-21 summary:In this article supervised learning problems are solved using soft ruleensembles. We first review the importance sampling learning ensembles (ISLE)approach that is useful for generating hard rules. The soft rules are thenobtained with logistic regression from the corresponding hard rules. In orderto deal with the perfect separation problem related to the logistic regression,Firth's bias corrected likelihood is used. Various examples and simulationresults show that soft rule ensembles can improve predictive performance overhard rule ensembles.
arxiv-2400-146 | Accelerated Linear SVM Training with Adaptive Variable Selection Frequencies | http://arxiv.org/pdf/1302.5608v1.pdf | author:Tobias Glasmachers, Ürün Dogan category:stat.ML cs.LG published:2013-02-22 summary:Support vector machine (SVM) training is an active research area since thedawn of the method. In recent years there has been increasing interest inspecialized solvers for the important case of linear models. The algorithmpresented by Hsieh et al., probably best known under the name of the"liblinear" implementation, marks a major breakthrough. The method is analog toestablished dual decomposition algorithms for training of non-linear SVMs, butwith greatly reduced computational complexity per update step. This comes atthe cost of not keeping track of the gradient of the objective any more, whichexcludes the application of highly developed working set selection algorithms.We present an algorithmic improvement to this method. We replace uniformworking set selection with an online adaptation of selection frequencies. Theadaptation criterion is inspired by modern second order working set selectionmethods. The same mechanism replaces the shrinking heuristic. This noveltechnique speeds up training in some cases by more than an order of magnitude.
arxiv-2400-147 | Efficient Discovery of Association Rules and Frequent Itemsets through Sampling with Tight Performance Guarantees | http://arxiv.org/pdf/1111.6937v6.pdf | author:Matteo Riondato, Eli Upfal category:cs.DS cs.DB cs.LG H.2.8 published:2011-11-29 summary:The tasks of extracting (top-$K$) Frequent Itemsets (FI's) and AssociationRules (AR's) are fundamental primitives in data mining and databaseapplications. Exact algorithms for these problems exist and are widely used,but their running time is hindered by the need of scanning the entire dataset,possibly multiple times. High quality approximations of FI's and AR's aresufficient for most practical uses, and a number of recent works explored theapplication of sampling for fast discovery of approximate solutions to theproblems. However, these works do not provide satisfactory performanceguarantees on the quality of the approximation, due to the difficulty ofbounding the probability of under- or over-sampling any one of an unknownnumber of frequent itemsets. In this work we circumvent this issue by applyingthe statistical concept of \emph{Vapnik-Chervonenkis (VC) dimension} to developa novel technique for providing tight bounds on the sample size that guaranteesapproximation within user-specified parameters. Our technique applies both toabsolute and to relative approximations of (top-$K$) FI's and AR's. Theresulting sample size is linearly dependent on the VC-dimension of a rangespace associated with the dataset to be mined. The main theoreticalcontribution of this work is a proof that the VC-dimension of this range spaceis upper bounded by an easy-to-compute characteristic quantity of the datasetwhich we call \emph{d-index}, and is the maximum integer $d$ such that thedataset contains at least $d$ transactions of length at least $d$ such that noone of them is a superset of or equal to another. We show that this bound isstrict for a large class of datasets.
arxiv-2400-148 | Iterative graph cuts for image segmentation with a nonlinear statistical shape prior | http://arxiv.org/pdf/1208.4384v2.pdf | author:Joshua C. Chang, Tom Chou category:cs.CV math.OC q-bio.QM stat.AP published:2012-08-21 summary:Shape-based regularization has proven to be a useful method for delineatingobjects within noisy images where one has prior knowledge of the shape of thetargeted object. When a collection of possible shapes is available, thespecification of a shape prior using kernel density estimation is a naturaltechnique. Unfortunately, energy functionals arising from kernel densityestimation are of a form that makes them impossible to directly minimize usingefficient optimization algorithms such as graph cuts. Our main contribution isto show how one may recast the energy functional into a form that isminimizable iteratively and efficiently using graph cuts.
arxiv-2400-149 | The Importance of Clipping in Neurocontrol by Direct Gradient Descent on the Cost-to-Go Function and in Adaptive Dynamic Programming | http://arxiv.org/pdf/1302.5565v1.pdf | author:Michael Fairbank category:cs.LG published:2013-02-22 summary:In adaptive dynamic programming, neurocontrol and reinforcement learning, theobjective is for an agent to learn to choose actions so as to minimise a totalcost function. In this paper we show that when discretized time is used tomodel the motion of the agent, it can be very important to do "clipping" on themotion of the agent in the final time step of the trajectory. By clipping wemean that the final time step of the trajectory is to be truncated such thatthe agent stops exactly at the first terminal state reached, and no distancefurther. We demonstrate that when clipping is omitted, learning performance canfail to reach the optimum; and when clipping is done properly, learningperformance can improve significantly. The clipping problem we describe affects algorithms which use explicitderivatives of the model functions of the environment to calculate a learninggradient. These include Backpropagation Through Time for Control, and methodsbased on Dual Heuristic Dynamic Programming. However the clipping problem doesnot significantly affect methods based on Heuristic Dynamic Programming,Temporal Differences or Policy Gradient Learning algorithms. Similarly, theclipping problem does not affect fixed-length finite-horizon problems.
arxiv-2400-150 | Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint | http://arxiv.org/pdf/1301.3533v2.pdf | author:Xanadu Halkias, Sebastien Paris, Herve Glotin category:cs.NE cs.LG stat.ML published:2013-01-16 summary:Deep Belief Networks (DBN) have been successfully applied on popular machinelearning tasks. Specifically, when applied on hand-written digit recognition,DBNs have achieved approximate accuracy rates of 98.8%. In an effort tooptimize the data representation achieved by the DBN and maximize theirdescriptive power, recent advances have focused on inducing sparse constraintsat each layer of the DBN. In this paper we present a theoretical approach forsparse constraints in the DBN using the mixed norm for both non-overlapping andoverlapping groups. We explore how these constraints affect the classificationaccuracy for digit recognition in three different datasets (MNIST, USPS, RIMES)and provide initial estimations of their usefulness by altering differentparameters such as the group size and overlap percentage.
arxiv-2400-151 | Nonparametric Basis Pursuit via Sparse Kernel-based Learning | http://arxiv.org/pdf/1302.5449v1.pdf | author:Juan Andres Bazerque, Georgios B. Giannakis category:cs.LG cs.CV cs.IT math.IT stat.ML published:2013-02-21 summary:Signal processing tasks as fundamental as sampling, reconstruction, minimummean-square error interpolation and prediction can be viewed under the prism ofreproducing kernel Hilbert spaces. Endowing this vantage point withcontemporary advances in sparsity-aware modeling and processing, promotes thenonparametric basis pursuit advocated in this paper as the overarchingframework for the confluence of kernel-based learning (KBL) approachesleveraging sparse linear regression, nuclear-norm regularization, anddictionary learning. The novel sparse KBL toolbox goes beyond translatingsparse parametric approaches to their nonparametric counterparts, toincorporate new possibilities such as multi-kernel selection and matrixsmoothing. The impact of sparse KBL to signal processing applications isillustrated through test cases from cognitive radio sensing, microarray dataimputation, and network traffic prediction.
arxiv-2400-152 | Object Detection in Real Images | http://arxiv.org/pdf/1302.5189v1.pdf | author:Dilip K. Prasad category:cs.CV published:2013-02-21 summary:Object detection and recognition are important problems in computer vision.Since these problems are meta-heuristic, despite a lot of research, practicallyusable, intelligent, real-time, and dynamic object detection/recognitionmethods are still unavailable. We propose a new object detection/recognitionmethod, which improves over the existing methods in every stage of the objectdetection/recognition process. In addition to the usual features, we propose touse geometric shapes, like linear cues, ellipses and quadrangles, as additionalfeatures. The full potential of geometric cues is exploited by using them toextract other features in a robust, computationally efficient, and lessmeta-heuristic manner. We also propose a new hierarchical codebook, whichprovides good generalization and discriminative properties. The codebookenables fast multi-path inference mechanisms based on propagation ofconditional likelihoods, that make it robust to occlusion and noise. It has thecapability of dynamic learning. We also propose a new learning method that hasgenerative and discriminative learning capabilities, does not need large andfully supervised training dataset, and is capable of online learning. Thepreliminary work of detecting geometric shapes in real images has beencompleted. This preliminary work is the focus of this report. Future path forrealizing the proposed object detection/recognition method is also discussed inbrief.
arxiv-2400-153 | Basic Classes of Grammars with Prohibition | http://arxiv.org/pdf/1302.5181v1.pdf | author:Mark Burgin category:cs.FL cs.CL published:2013-02-21 summary:A practical tool for natural language modeling and development ofhuman-machine interaction is developed in the context of formal grammars andlanguages. A new type of formal grammars, called grammars with prohibition, isintroduced. Grammars with prohibition provide more powerful tools for naturallanguage generation and better describe processes of language learning than theconventional formal grammars. Here we study relations between languagesgenerated by different grammars with prohibition based on conventional types offormal grammars such as context-free or context sensitive grammars. Besides, wecompare languages generated by different grammars with prohibition andlanguages generated by conventional formal grammars. In particular, it isdemonstrated that they have essentially higher computational power andexpressive possibilities in comparison with the conventional formal grammars.Thus, while conventional formal grammars are recursive and subrecursivealgorithms, many classes of grammars with prohibition are superrecursivealgorithms. Results presented in this work are aimed at the development ofhuman-machine interaction, modeling natural languages, empowerment ofprogramming languages, computer simulation, better software systems, and theoryof recursion.
arxiv-2400-154 | Spectral Clustering with Unbalanced Data | http://arxiv.org/pdf/1302.5134v1.pdf | author:Jing Qian, Venkatesh Saligrama category:stat.ML published:2013-02-20 summary:Spectral clustering (SC) and graph-based semi-supervised learning (SSL)algorithms are sensitive to how graphs are constructed from data. In particularif the data has proximal and unbalanced clusters these algorithms can lead topoor performance on well-known graphs such as $k$-NN, full-RBF,$\epsilon$-graphs. This is because the objectives such as Ratio-Cut (RCut) ornormalized cut (NCut) attempt to tradeoff cut values with cluster sizes, whichare not tailored to unbalanced data. We propose a novel graph partitioningframework, which parameterizes a family of graphs by adaptively modulating nodedegrees in a $k$-NN graph. We then propose a model selection scheme to choosesizable clusters which are separated by smallest cut values. Our framework isable to adapt to varying levels of unbalancedness of data and can be naturallyused for small cluster detection. We theoretically justify our ideas throughlimit cut analysis. Unsupervised and semi-supervised experiments on syntheticand real data sets demonstrate the superiority of our method.
arxiv-2400-155 | High-Dimensional Probability Estimation with Deep Density Models | http://arxiv.org/pdf/1302.5125v1.pdf | author:Oren Rippel, Ryan Prescott Adams category:stat.ML cs.LG published:2013-02-20 summary:One of the fundamental problems in machine learning is the estimation of aprobability distribution from data. Many techniques have been proposed to studythe structure of data, most often building around the assumption thatobservations lie on a lower-dimensional manifold of high probability. It hasbeen more difficult, however, to exploit this insight to build explicit,tractable density models for high-dimensional data. In this paper, we introducethe deep density model (DDM), a new approach to density estimation. We exploitinsights from deep learning to construct a bijective map to a representationspace, under which the transformation of the distribution of the data isapproximately factorized and has identical and known marginal densities. Thesimplicity of the latent distribution under the model allows us to feasiblyexplore it, and the invertibility of the map to characterize contraction ofmeasure across it. This enables us to compute normalized densities forout-of-sample data. This combination of tractability and flexibility allows usto tackle a variety of probabilistic tasks on high-dimensional datasets,including: rapid computation of normalized densities at test-time withoutevaluating a partition function; generation of samples without MCMC; andcharacterization of the joint entropy of the data.
arxiv-2400-156 | Estimating Continuous Distributions in Bayesian Classifiers | http://arxiv.org/pdf/1302.4964v1.pdf | author:George H. John, Pat Langley category:cs.LG cs.AI stat.ML published:2013-02-20 summary:When modeling a probability distribution with a Bayesian network, we arefaced with the problem of how to handle continuous variables. Most previouswork has either solved the problem by discretizing, or assumed that the dataare generated by a single Gaussian. In this paper we abandon the normalityassumption and instead use statistical methods for nonparametric densityestimation. For a naive Bayesian classifier, we present experimental results ona variety of natural and artificial domains, comparing two methods of densityestimation: assuming normality and modeling each conditional distribution witha single Gaussian; and using nonparametric kernel density estimation. Weobserve large reductions in error on several natural and artificial data sets,which suggests that kernel estimation is a useful tool for learning Bayesianmodels.
arxiv-2400-157 | A Characterization of the Dirichlet Distribution with Application to Learning Bayesian Networks | http://arxiv.org/pdf/1302.4949v1.pdf | author:Dan Geiger, David Heckerman category:cs.AI cs.LG published:2013-02-20 summary:We provide a new characterization of the Dirichlet distribution. Thischaracterization implies that under assumptions made by several previousauthors for learning belief networks, a Dirichlet prior on the parameters isinevitable.
arxiv-2400-158 | Constrained Overcomplete Analysis Operator Learning for Cosparse Signal Modelling | http://arxiv.org/pdf/1205.4133v2.pdf | author:Mehrdad Yaghoobi, Sangnam Nam, Remi Gribonval, Mike E. Davies category:math.NA cs.LG published:2012-05-18 summary:We consider the problem of learning a low-dimensional signal model from acollection of training samples. The mainstream approach would be to learn anovercomplete dictionary to provide good approximations of the training samplesusing sparse synthesis coefficients. This famous sparse model has a less wellknown counterpart, in analysis form, called the cosparse analysis model. Inthis new model, signals are characterised by their parsimony in a transformeddomain using an overcomplete (linear) analysis operator. We propose to learn ananalysis operator from a training corpus using a constrained optimisationframework based on L1 optimisation. The reason for introducing a constraint inthe optimisation framework is to exclude trivial solutions. Although there isno final answer here for which constraint is the most relevant constraint, weinvestigate some conventional constraints in the model adaptation field and usethe uniformly normalised tight frame (UNTF) for this purpose. We then derive apractical learning algorithm, based on projected subgradients andDouglas-Rachford splitting technique, and demonstrate its ability to robustlyrecover a ground truth analysis operator, when provided with a clean trainingset, of sufficient size. We also find an analysis operator for images, usingsome noisy cosparse signals, which is indeed a more realistic experiment. Asthe derived optimisation problem is not a convex program, we often find a localminimum using such variational methods. Some local optimality conditions arederived for two different settings, providing preliminary theoretical supportfor the well-posedness of the learning problem under appropriate conditions.
arxiv-2400-159 | A Labeled Graph Kernel for Relationship Extraction | http://arxiv.org/pdf/1302.4874v1.pdf | author:Gonçalo Simões, Helena Galhardas, David Matos category:cs.CL cs.LG published:2013-02-20 summary:In this paper, we propose an approach for Relationship Extraction (RE) basedon labeled graph kernels. The kernel we propose is a particularization of arandom walk kernel that exploits two properties previously studied in the REliterature: (i) the words between the candidate entities or connecting them ina syntactic representation are particularly likely to carry informationregarding the relationship; and (ii) combining information from distinctsources in a kernel may help the RE system make better decisions. We performedexperiments on a dataset of protein-protein interactions and the results showthat our approach obtains effectiveness values that are comparable with thestate-of-the art kernel methods. Moreover, our approach is able to outperformthe state-of-the-art kernels when combined with other kernel methods.
arxiv-2400-160 | Breaking the Small Cluster Barrier of Graph Clustering | http://arxiv.org/pdf/1302.4549v2.pdf | author:Nir Ailon, Yudong Chen, Xu Huan category:cs.LG stat.ML published:2013-02-19 summary:This paper investigates graph clustering in the planted cluster model in thepresence of {\em small clusters}. Traditional results dictate that for analgorithm to provably correctly recover the clusters, {\em all} clusters mustbe sufficiently large (in particular, $\tilde{\Omega}(\sqrt{n})$ where $n$ isthe number of nodes of the graph). We show that this is not really arestriction: by a more refined analysis of the trace-norm based recoveryapproach proposed in Jalali et al. (2011) and Chen et al. (2012), we prove thatsmall clusters, under certain mild assumptions, do not hinder recovery of largeones. Based on this result, we further devise an iterative algorithm to recover{\em almost all clusters} via a "peeling strategy", i.e., recover largeclusters first, leading to a reduced problem, and repeat this procedure. Theseresults are extended to the {\em partial observation} setting, in which only a(chosen) part of the graph is observed.The peeling strategy gives rise to anactive learning algorithm, in which edges adjacent to smaller clusters arequeried more often as large clusters are learned (and removed). From a high level, this paper sheds novel insights on high-dimensionalstatistics and learning structured data, by presenting a structured matrixlearning problem for which a one shot convex relaxation approach necessarilyfails, but a carefully constructed sequence of convex relaxationsdoes the job.
arxiv-2400-161 | NLP and CALL: integration is working | http://arxiv.org/pdf/1302.4814v1.pdf | author:Georges Antoniadis, Sylviane Granger, Olivier Kraif, Claude Ponton, Virginie Zampa category:cs.CL published:2013-02-20 summary:In the first part of this article, we explore the background ofcomputer-assisted learning from its beginnings in the early XIXth century andthe first teaching machines, founded on theories of learning, at the start ofthe XXth century. With the arrival of the computer, it became possible to offerlanguage learners different types of language activities such as comprehensiontasks, simulations, etc. However, these have limits that cannot be overcomewithout some contribution from the field of natural language processing (NLP).In what follows, we examine the challenges faced and the issues raised byintegrating NLP into CALL. We hope to demonstrate that the key to success inintegrating NLP into CALL is to be found in multidisciplinary work betweencomputer experts, linguists, language teachers, didacticians and NLPspecialists.
arxiv-2400-162 | Probabilistic Frame Induction | http://arxiv.org/pdf/1302.4813v1.pdf | author:Jackie Chi Kit Cheung, Hoifung Poon, Lucy Vanderwende category:cs.CL published:2013-02-20 summary:In natural-language discourse, related events tend to appear near each otherto describe a larger scenario. Such structures can be formalized by the notionof a frame (a.k.a. template), which comprises a set of related events andprototypical participants and event transitions. Identifying frames is aprerequisite for information extraction and natural language generation, and isusually done manually. Methods for inducing frames have been proposed recently,but they typically use ad hoc procedures and are difficult to diagnose orextend. In this paper, we propose the first probabilistic approach to frameinduction, which incorporates frames, events, participants as latent topics andlearns those frame and event transitions that best explain the text. The numberof frames is inferred by a novel application of a split-merge method fromsyntactic parsing. In end-to-end evaluations from text to induced frames andextracted facts, our method produced state-of-the-art results whilesubstantially reducing engineering effort.
arxiv-2400-163 | Towards a Semantic-based Approach for Modeling Regulatory Documents in Building Industry | http://arxiv.org/pdf/1302.4811v1.pdf | author:Khalil Riad Bouzidi, Catherine Faron-Zucker, Bruno Fies, Olivier Corby, Le-Thanh Nhan category:cs.CL published:2013-02-20 summary:Regulations in the Building Industry are becoming increasingly complex andinvolve more than one technical area. They cover products, components andproject implementation. They also play an important role to ensure the qualityof a building, and to minimize its environmental impact. In this paper, we areparticularly interested in the modeling of the regulatory constraints derivedfrom the Technical Guides issued by CSTB and used to validate TechnicalAssessments. We first describe our approach for modeling regulatory constraintsin the SBVR language, and formalizing them in the SPARQL language. Second, wedescribe how we model the processes of compliance checking described in theCSTB Technical Guides. Third, we show how we implement these processes toassist industrials in drafting Technical Documents in order to acquire aTechnical Assessment; a compliance report is automatically generated to explainthe compliance or noncompliance of this Technical Documents.
arxiv-2400-164 | An Optical Watermarking Solution for Color Personal Identification Pictures | http://arxiv.org/pdf/1302.4784v1.pdf | author:Tan Yi-zhou, Liu Hai-bo, Huang Shui-hua, Sheng Ben-jian, Pan Zhong-ming category:cs.MM cs.CV physics.optics published:2013-02-20 summary:This paper presents a new approach for embedding authentication informationinto image on printed materials based on optical projection technique. Ourexperimental setup consists of two parts, one is a common camera, and the otheris a LCD projector, which project a pattern on personnel's body (especially onthe face). The pattern, generated by a computer, act as the illumination lightsource with sinusoidal distribution and it is also the watermark signal. For acolor image, the watermark is embedded into the blue channel. While we takepictures (256 *256 and 512*512, 567*390 pixels, respectively), an invisiblemark is embedded directly into magnitude oefficients of Discrete Fouriertransform (DFT) at exposure moment. Both optical an d digital correlation issuitable for detection of this type of watermark. The decoded watermark is aset of concentric circles or sectors in the DFT domain (middle frequenciesregion) which is robust to photographing, printing and scanning. The unlawfulpeople modify or replace the original photograph, and make fake passport(drivers' license and so on). Experiments show, it is difficult to forgecertificates in which a watermark was embedded by our projector-cameracombination based on analogue watermark method rather than classical digitalmethod.
arxiv-2400-165 | Optimal Discriminant Functions Based On Sampled Distribution Distance for Modulation Classification | http://arxiv.org/pdf/1302.4773v1.pdf | author:Paulo Urriza, Eric Rebeiz, Danijela Cabric category:stat.ML cs.LG cs.PF published:2013-02-19 summary:In this letter, we derive the optimal discriminant functions for modulationclassification based on the sampled distribution distance. The proposed methodclassifies various candidate constellations using a low complexity approachbased on the distribution distance at specific testpoints along the cumulativedistribution function. This method, based on the Bayesian decision criteria,asymptotically provides the minimum classification error possible given a setof testpoints. Testpoint locations are also optimized to improve classificationperformance. The method provides significant gains over existing approachesthat also use the distribution of the signal features.
arxiv-2400-166 | An Ontology for Modelling and Supporting the Process of Authoring Technical Assessments | http://arxiv.org/pdf/1302.4726v1.pdf | author:Khalil Riad Bouzidi, Bruno Fies, Marc Bourdeau, Catherine Faron-Zucker, Nhan Le-Thanh category:cs.IR cs.CL cs.DL published:2013-02-19 summary:In this paper, we present a semantic web approach for modelling the processof creating new technical and regulatory documents related to the Buildingsector. This industry, among other industries, is currently experiencing aphenomenal growth in its technical and regulatory texts. Therefore, it isurgent and crucial to improve the process of creating regulations by automatingit as much as possible. We focus on the creation of particular technicaldocuments issued by the French Scientific and Technical Centre for Building(CSTB), called Technical Assessments, and we propose services based on SemanticWeb models and techniques for modelling the process of their creation.
arxiv-2400-167 | Data-driven density derivative estimation, with applications to nonparametric clustering and bump hunting | http://arxiv.org/pdf/1204.6160v3.pdf | author:José E. Chacón, Tarn Duong category:math.ST stat.ME stat.ML stat.TH 62G05, 62H30 published:2012-04-27 summary:Important information concerning a multivariate data set, such as clustersand modal regions, is contained in the derivatives of the probability densityfunction. Despite this importance, nonparametric estimation of higher orderderivatives of the density functions have received only relatively scantattention. Kernel estimators of density functions are widely used as theyexhibit excellent theoretical and practical properties, though theirgeneralization to density derivatives has progressed more slowly due to themathematical intractabilities encountered in the crucial problem of bandwidth(or smoothing parameter) selection. This paper presents the first fullyautomatic, data-based bandwidth selectors for multivariate kernel densityderivative estimators. This is achieved by synthesizing recent advances inmatrix analytic theory which allow mathematically and computationally tractablerepresentations of higher order derivatives of multivariate vector valuedfunctions. The theoretical asymptotic properties as well as the finite samplebehaviour of the proposed selectors are studied. {In addition, we explore indetail the applications of the new data-driven methods for two otherstatistical problems: clustering and bump hunting. The introduced techniquesare combined with the mean shift algorithm to develop novel automatic,nonparametric clustering procedures which are shown to outperform mixture-modelcluster analysis and other recent nonparametric approaches in practice.Furthermore, the advantage of the use of smoothing parameters designed fordensity derivative estimation for feature significance analysis for bumphunting is illustrated with a real data example.
arxiv-2400-168 | Learning Manifolds with K-Means and K-Flats | http://arxiv.org/pdf/1209.1121v4.pdf | author:Guillermo D. Canas, Tomaso Poggio, Lorenzo Rosasco category:cs.LG stat.ML K.3.2 published:2012-09-05 summary:We study the problem of estimating a manifold from random samples. Inparticular, we consider piecewise constant and piecewise linear estimatorsinduced by k-means and k-flats, and analyze their performance. We extendprevious results for k-means in two separate directions. First, we provide newresults for k-means reconstruction on manifolds and, secondly, we provereconstruction bounds for higher-order approximation (k-flats), for which noknown results were previously available. While the results for k-means arenovel, some of the technical tools are well-established in the literature. Inthe case of k-flats, both the results and the mathematical tools are new.
arxiv-2400-169 | Good Recognition is Non-Metric | http://arxiv.org/pdf/1302.4673v1.pdf | author:Walter J. Scheirer, Michael J. Wilber, Michael Eckmann, Terrance E. Boult category:cs.CV published:2013-02-19 summary:Recognition is the fundamental task of visual cognition, yet how to formalizethe general recognition problem for computer vision remains an open issue. Theproblem is sometimes reduced to the simplest case of recognizing matchingpairs, often structured to allow for metric constraints. However, visualrecognition is broader than just pair matching -- especially when we considermulti-class training data and large sets of features in a learning context.What we learn and how we learn it has important implications for effectivealgorithms. In this paper, we reconsider the assumption of recognition as apair matching test, and introduce a new formal definition that captures thebroader context of the problem. Through a meta-analysis and an experimentalassessment of the top algorithms on popular data sets, we gain a sense of howoften metric properties are violated by good recognition algorithms. Bystudying these violations, useful insights come to light: we make the case thatlocally metric algorithms should leverage outside information to solve thegeneral recognition problem.
arxiv-2400-170 | Adaptive Evolutionary Clustering | http://arxiv.org/pdf/1104.1990v3.pdf | author:Kevin S. Xu, Mark Kliger, Alfred O. Hero III category:cs.LG stat.ML published:2011-04-11 summary:In many practical applications of clustering, the objects to be clusteredevolve over time, and a clustering result is desired at each time step. In suchapplications, evolutionary clustering typically outperforms traditional staticclustering by producing clustering results that reflect long-term trends whilebeing robust to short-term variations. Several evolutionary clusteringalgorithms have recently been proposed, often by adding a temporal smoothnesspenalty to the cost function of a static clustering method. In this paper, weintroduce a different approach to evolutionary clustering by accuratelytracking the time-varying proximities between objects followed by staticclustering. We present an evolutionary clustering framework that adaptivelyestimates the optimal smoothing parameter using shrinkage estimation, astatistical approach that improves a naive estimate using additionalinformation. The proposed framework can be used to extend a variety of staticclustering algorithms, including hierarchical, k-means, and spectralclustering, into evolutionary clustering algorithms. Experiments on syntheticand real data sets indicate that the proposed framework outperforms staticclustering and existing evolutionary clustering algorithms in many scenarios.
arxiv-2400-171 | Compactified Horizontal Visibility Graph for the Language Network | http://arxiv.org/pdf/1302.4619v1.pdf | author:D. V. Lande, A. A. Snarskii category:cs.CL cs.DS published:2013-02-19 summary:A compactified horizontal visibility graph for the language network isproposed. It was found that the networks constructed in such way are scalefree, and have a property that among the nodes with largest degrees there arewords that determine not only a text structure communication, but also itsinformational structure.
arxiv-2400-172 | PyPLN: a Distributed Platform for Natural Language Processing | http://arxiv.org/pdf/1301.7738v2.pdf | author:Flávio Codeço Coelho, Renato Rocha Souza, Álvaro Justen, Flávio Amieiro, Heliana Mello category:cs.CL cs.IR published:2013-01-31 summary:This paper presents a distributed platform for Natural Language Processingcalled PyPLN. PyPLN leverages a vast array of NLP and text processing opensource tools, managing the distribution of the workload on a variety ofconfigurations: from a single server to a cluster of linux servers. PyPLN isdeveloped using Python 2.7.3 but makes it very easy to incorporate othersoftwares for specific tasks as long as a linux version is available. PyPLNfacilitates analyses both at document and corpus level, simplifying managementand publication of corpora and analytical results through an easy to use webinterface. In the current (beta) release, it supports English and Portugueselanguages with support to other languages planned for future releases. Tosupport the Portuguese language PyPLN uses the PALAVRAS parser\citep{Bick2000}.Currently PyPLN offers the following features: Text extraction with encodingnormalization (to UTF-8), part-of-speech tagging, token frequency, semanticannotation, n-gram extraction, word and sentence repertoire, and full-textsearch across corpora. The platform is licensed as GPL-v3.
arxiv-2400-173 | A Genetic Algorithm for Power-Aware Virtual Machine Allocation in Private Cloud | http://arxiv.org/pdf/1302.4519v1.pdf | author:Nguyen Quang-Hung, Pham Dac Nien, Nguyen Hoai Nam, Nguyen Huynh Tuong, Nam Thoai category:cs.NE cs.DC published:2013-02-19 summary:Energy efficiency has become an important measurement of scheduling algorithmfor private cloud. The challenge is trade-off between minimizing of energyconsumption and satisfying Quality of Service (QoS) (e.g. performance orresource availability on time for reservation request). We consider resourceneeds in context of a private cloud system to provide resources forapplications in teaching and researching. In which users request computingresources for laboratory classes at start times and non-interrupted duration insome hours in prior. Many previous works are based on migrating techniques tomove online virtual machines (VMs) from low utilization hosts and turn thesehosts off to reduce energy consumption. However, the techniques for migrationof VMs could not use in our case. In this paper, a genetic algorithm forpower-aware in scheduling of resource allocation (GAPA) has been proposed tosolve the static virtual machine allocation problem (SVMAP). Due to limitedresources (i.e. memory) for executing simulation, we created a workload thatcontains a sample of one-day timetable of lab hours in our university. Weevaluate the GAPA and a baseline scheduling algorithm (BFD), which sorts listof virtual machines in start time (i.e. earliest start time first) and usingbest-fit decreasing (i.e. least increased power consumption) algorithm, forsolving the same SVMAP. As a result, the GAPA algorithm obtains total energyconsumption is lower than the baseline algorithm on simulated experimentation.
arxiv-2400-174 | Bilingual Terminology Extraction Using Multi-level Termhood | http://arxiv.org/pdf/1302.4492v1.pdf | author:Chengzhi Zhang, Dan Wu category:cs.CL published:2013-02-19 summary:Purpose: Terminology is the set of technical words or expressions used inspecific contexts, which denotes the core concept in a formal discipline and isusually applied in the fields of machine translation, information retrieval,information extraction and text categorization, etc. Bilingual terminologyextraction plays an important role in the application of bilingual dictionarycompilation, bilingual Ontology construction, machine translation andcross-language information retrieval etc. This paper addresses the issues ofmonolingual terminology extraction and bilingual term alignment based onmulti-level termhood. Design/methodology/approach: A method based on multi-level termhood isproposed. The new method computes the termhood of the terminology candidate aswell as the sentence that includes the terminology by the comparison of thecorpus. Since terminologies and general words usually have differentlydistribution in the corpus, termhood can also be used to constrain and enhancethe performance of term alignment when aligning bilingual terms on the parallelcorpus. In this paper, bilingual term alignment based on termhood constraintsis presented. Findings: Experiment results show multi-level termhood can get betterperformance than existing method for terminology extraction. If termhood isused as constrain factor, the performance of bilingual term alignment can beimproved.
arxiv-2400-175 | Complex networks analysis of language complexity | http://arxiv.org/pdf/1302.4490v1.pdf | author:Diego R. Amancio, Sandra M. Aluisio, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:physics.soc-ph cs.CL cs.SI published:2013-02-19 summary:Methods from statistical physics, such as those involving complex networks,have been increasingly used in quantitative analysis of linguistic phenomena.In this paper, we represented pieces of text with different levels ofsimplification in co-occurrence networks and found that topological regularitycorrelated negatively with textual complexity. Furthermore, in less complextexts the distance between concepts, represented as nodes, tended to decrease.The complex networks metrics were treated with multivariate pattern recognitiontechniques, which allowed us to distinguish between original texts and theirsimplified versions. For each original text, two simplified versions weregenerated manually with increasing number of simplification operations. Asexpected, distinction was easier for the strongly simplified versions, wherethe most relevant metrics were node strength, shortest paths and diversity.Also, the discrimination of complex texts was improved with higher hierarchicalnetwork metrics, thus pointing to the usefulness of considering wider contextsaround the concepts. Though the accuracy rate in the distinction was not ashigh as in methods using deep linguistic knowledge, the complex networkapproach is still useful for a rapid screening of texts whenever assessingcomplexity is essential to guarantee accessibility to readers with limitedreading ability
arxiv-2400-176 | Termhood-based Comparability Metrics of Comparable Corpus in Special Domain | http://arxiv.org/pdf/1302.4489v1.pdf | author:Sa Liu, Chengzhi Zhang category:cs.CL published:2013-02-19 summary:Cross-Language Information Retrieval (CLIR) and machine translation (MT)resources, such as dictionaries and parallel corpora, are scarce and hard tocome by for special domains. Besides, these resources are just limited to a fewlanguages, such as English, French, and Spanish and so on. So, obtainingcomparable corpora automatically for such domains could be an answer to thisproblem effectively. Comparable corpora, that the subcorpora are nottranslations of each other, can be easily obtained from web. Therefore,building and using comparable corpora is often a more feasible option inmultilingual information processing. Comparability metrics is one of key issuesin the field of building and using comparable corpus. Currently, there is nowidely accepted definition or metrics method of corpus comparability. In fact,Different definitions or metrics methods of comparability might be given tosuit various tasks about natural language processing. A new comparability,namely, termhood-based metrics, oriented to the task of bilingual terminologyextraction, is proposed in this paper. In this method, words are ranked bytermhood not frequency, and then the cosine similarities, calculated based onthe ranking lists of word termhood, is used as comparability. Experimentsresults show that termhood-based metrics performs better than traditionalfrequency-based metrics.
arxiv-2400-177 | Word sense disambiguation via high order of learning in complex networks | http://arxiv.org/pdf/1302.4471v1.pdf | author:Thiago C. Silva, Diego R. Amancio category:physics.soc-ph cs.CL cs.SI published:2013-02-18 summary:Complex networks have been employed to model many real systems and as amodeling tool in a myriad of applications. In this paper, we use the frameworkof complex networks to the problem of supervised classification in the worddisambiguation task, which consists in deriving a function from the supervised(or labeled) training data of ambiguous words. Traditional supervised dataclassification takes into account only topological or physical features of theinput data. On the other hand, the human (animal) brain performs both low- andhigh-level orders of learning and it has facility to identify patternsaccording to the semantic meaning of the input data. In this paper, we apply ahybrid technique which encompasses both types of learning in the field of wordsense disambiguation and show that the high-level order of learning can reallyimprove the accuracy rate of the model. This evidence serves to demonstratethat the internal structures formed by the words do present patterns that,generally, cannot be correctly unveiled by only traditional techniques.Finally, we exhibit the behavior of the model for different weights of the low-and high-level classifiers by plotting decision boundaries. This study helpsone to better understand the effectiveness of the model.
arxiv-2400-178 | Unveiling the relationship between complex networks metrics and word senses | http://arxiv.org/pdf/1302.4465v1.pdf | author:Diego R. Amancio, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:physics.soc-ph cs.CL cs.SI published:2013-02-18 summary:The automatic disambiguation of word senses (i.e., the identification ofwhich of the meanings is used in a given context for a word that has multiplemeanings) is essential for such applications as machine translation andinformation retrieval, and represents a key step for developing the so-calledSemantic Web. Humans disambiguate words in a straightforward fashion, but thisdoes not apply to computers. In this paper we address the problem of Word SenseDisambiguation (WSD) by treating texts as complex networks, and show that wordsenses can be distinguished upon characterizing the local structure aroundambiguous words. Our goal was not to obtain the best possible disambiguationsystem, but we nevertheless found that in half of the cases our approachoutperforms traditional shallow methods. We show that the hierarchicalconnectivity and clustering of words are usually the most relevant features forWSD. The results reported here shine light on the relationship between semanticand structural parameters of complex networks. They also indicate that whencombined with traditional techniques the complex network approach may be usefulto enhance the discrimination of senses in large texts
arxiv-2400-179 | Explaining Zipf's Law via Mental Lexicon | http://arxiv.org/pdf/1302.4383v1.pdf | author:Armen E. Allahverdyan, Weibing Deng, Q. A. Wang category:cs.CL published:2013-02-18 summary:The Zipf's law is the major regularity of statistical linguistics that servedas a prototype for rank-frequency relations and scaling laws in naturalsciences. Here we show that the Zipf's law -- together with its applicabilityfor a single text and its generalizations to high and low frequencies includinghapax legomena -- can be derived from assuming that the words are drawn intothe text with random probabilities. Their apriori density relates, via theBayesian statistics, to general features of the mental lexicon of the authorwho produced the text.
arxiv-2400-180 | On Translation Invariant Kernels and Screw Functions | http://arxiv.org/pdf/1302.4343v1.pdf | author:Purushottam Kar, Harish Karnick category:math.FA cs.LG stat.ML published:2013-02-18 summary:We explore the connection between Hilbertian metrics and positive definitekernels on the real line. In particular, we look at a well-knowncharacterization of translation invariant Hilbertian metrics on the real lineby von Neumann and Schoenberg (1941). Using this result we are able to give analternate proof of Bochner's theorem for translation invariant positivedefinite kernels on the real line (Rudin, 1962).
arxiv-2400-181 | No More Pesky Learning Rates | http://arxiv.org/pdf/1206.1106v2.pdf | author:Tom Schaul, Sixin Zhang, Yann LeCun category:stat.ML cs.LG published:2012-06-06 summary:The performance of stochastic gradient descent (SGD) depends critically onhow learning rates are tuned and decreased over time. We propose a method toautomatically adjust multiple learning rates so as to minimize the expectederror at any one time. The method relies on local gradient variations acrosssamples. In our approach, learning rates can increase as well as decrease,making it suitable for non-stationary problems. Using a number of convex andnon-convex learning tasks, we show that the resulting algorithm matches theperformance of SGD or other adaptive approaches with their best settingsobtained through systematic search, and effectively removes the need forlearning rate tuning.
arxiv-2400-182 | Role of temporal inference in the recognition of textual inference | http://arxiv.org/pdf/1302.5645v1.pdf | author:Djallel Bouneffouf category:cs.CL I.2.7 published:2013-02-18 summary:This project is a part of nature language processing and its aims to developa system of recognition inference text-appointed TIMINF. This type of systemcan detect, given two portions of text, if a text is semantically deducted fromthe other. We focused on making the inference time in this type of system. Forthat we have built and analyzed a body built from questions collected throughthe web. This study has enabled us to classify different types of timesinferences and for designing the architecture of TIMINF which seeks tointegrate a module inference time in a detection system inference text. We alsoassess the performance of sorties TIMINF system on a test corpus with the samestrategy adopted in the challenge RTE.
arxiv-2400-183 | Canonical dual solutions to nonconvex radial basis neural network optimization problem | http://arxiv.org/pdf/1302.4141v1.pdf | author:Vittorio Latorre, David Yang Gao category:cs.NE cs.LG stat.ML published:2013-02-18 summary:Radial Basis Functions Neural Networks (RBFNNs) are tools widely used inregression problems. One of their principal drawbacks is that the formulationcorresponding to the training with the supervision of both the centers and theweights is a highly non-convex optimization problem, which leads to somefundamentally difficulties for traditional optimization theory and methods. This paper presents a generalized canonical duality theory for solving thischallenging problem. We demonstrate that by sequential canonical dualtransformations, the nonconvex optimization problem of the RBFNN can bereformulated as a canonical dual problem (without duality gap). Both globaloptimal solution and local extrema can be classified. Several applications toone of the most used Radial Basis Functions, the Gaussian function, areillustrated. Our results show that even for one-dimensional case, the globalminimizer of the nonconvex problem may not be the best solution to the RBFNNs,and the canonical dual theory is a promising tool for solving general neuralnetworks training problems.
arxiv-2400-184 | A Time Series Forest for Classification and Feature Extraction | http://arxiv.org/pdf/1302.2277v2.pdf | author:Houtao Deng, George Runger, Eugene Tuv, Martyanov Vladimir category:cs.LG published:2013-02-09 summary:We propose a tree ensemble method, referred to as time series forest (TSF),for time series classification. TSF employs a combination of the entropy gainand a distance measure, referred to as the Entrance (entropy and distance)gain, for evaluating the splits. Experimental studies show that the Entrancegain criterion improves the accuracy of TSF. TSF randomly samples features ateach tree node and has a computational complexity linear in the length of atime series and can be built using parallel computing techniques such asmulti-core computing used here. The temporal importance curve is also proposedto capture the important temporal characteristics useful for classification.Experimental studies show that TSF using simple features such as mean,deviation and slope outperforms strong competitors such as one-nearest-neighborclassifiers with dynamic time warping, is computationally efficient, and canprovide insights into the temporal characteristics.
arxiv-2400-185 | A new scheme of signature extraction for iris authentication | http://arxiv.org/pdf/1302.4043v1.pdf | author:Belhassen Akrout, Imen Khanfir Kallel, Chokri Ben Amar category:cs.CV published:2013-02-17 summary:Iris recognition, a relatively new biometric technology, has greatadvantages, such as variability, stability and security, thus is the mostpromising for high security environment. Iris recognition is proposed in thisreport. We describe some methods, the first one is based on grey levelhistogram to extract the pupil, the second is based on elliptic and parabolicHOUGH transformation to determinate the edge of iris, upper and lower eyelids,the third we used 2D Gabor Wavelets to encode the iris and finally we used theHamming distance for authentication.
arxiv-2400-186 | Fourier-Bessel rotational invariant eigenimages | http://arxiv.org/pdf/1211.1968v2.pdf | author:Zhizhen Zhao, Amit Singer category:cs.CV published:2012-11-08 summary:We present an efficient and accurate algorithm for principal componentanalysis (PCA) of a large set of two dimensional images, and, for each image,the set of its uniform rotations in the plane and its reflection. The algorithmstarts by expanding each image, originally given on a Cartesian grid, in theFourier-Bessel basis for the disk. Because the images are bandlimited in theFourier domain, we use a sampling criterion to truncate the Fourier-Besselexpansion such that the maximum amount of information is preserved without theeffect of aliasing. The constructed covariance matrix is invariant to rotationand reflection and has a special block diagonal structure. PCA is efficientlydone for each block separately. This Fourier-Bessel based PCA detects moremeaningful eigenimages and has improved denoising capability compared totraditional PCA for a finite number of noisy images.
arxiv-2400-187 | Gaussian Process Vine Copulas for Multivariate Dependence | http://arxiv.org/pdf/1302.3979v1.pdf | author:David Lopez-Paz, José Miguel Hernández-Lobato, Zoubin Ghahramani category:stat.ME stat.ML published:2013-02-16 summary:Copulas allow to learn marginal distributions separately from themultivariate dependence structure (copula) that links them together into adensity function. Vine factorizations ease the learning of high-dimensionalcopulas by constructing a hierarchy of conditional bivariate copulas. However,to simplify inference, it is common to assume that each of these conditionalbivariate copulas is independent from its conditioning variables. In thispaper, we relax this assumption by discovering the latent functions thatspecify the shape of a conditional copula given its conditioning variables Welearn these functions by following a Bayesian approach based on sparse Gaussianprocesses with expectation propagation for scalable, approximate inference.Experiments on real-world datasets show that, when modeling all conditionaldependencies, we obtain better estimates of the underlying copula of the data.
arxiv-2400-188 | Layer-wise learning of deep generative models | http://arxiv.org/pdf/1212.1524v2.pdf | author:Ludovic Arnold, Yann Ollivier category:cs.NE cs.LG stat.ML published:2012-12-07 summary:When using deep, multi-layered architectures to build generative models ofdata, it is difficult to train all layers at once. We propose a layer-wisetraining procedure admitting a performance guarantee compared to the globaloptimum. It is based on an optimistic proxy of future performance, the bestlatent marginal. We interpret auto-encoders in this setting as generativemodels, by showing that they train a lower bound of this criterion. We test thenew learning procedure against a state of the art method (stacked RBMs), andfind it to improve performance. Both theory and experiments highlight theimportance, when training deep architectures, of using an inference model (fromdata to hidden variables) richer than the generative model (from hiddenvariables to data).
arxiv-2400-189 | Clustering validity based on the most similarity | http://arxiv.org/pdf/1302.3956v1.pdf | author:Raheleh Namayandeh, Farzad Didehvar, Zahra Shojaei category:cs.LG stat.ML 68T05 published:2013-02-16 summary:One basic requirement of many studies is the necessity of classifying data.Clustering is a proposed method for summarizing networks. Clustering methodscan be divided into two categories named model-based approaches and algorithmicapproaches. Since the most of clustering methods depend on their inputparameters, it is important to evaluate the result of a clustering algorithmwith its different input parameters, to choose the most appropriate one. Thereare several clustering validity techniques based on inner density and outerdensity of clusters that represent different metrics to choose the mostappropriate clustering independent of the input parameters. According todependency of previous methods on the input parameters, one challenge in facingwith large systems, is to complete data incrementally that effects on the finalchoice of the most appropriate clustering. Those methods define the existenceof high intensity in a cluster, and low intensity among different clusters asthe measure of choosing the optimal clustering. This measure has a tremendousproblem, not availing all data at the first stage. In this paper, we introducean efficient measure in which maximum number of repetitions for various initialvalues occurs.
arxiv-2400-190 | On the difficulty of training Recurrent Neural Networks | http://arxiv.org/pdf/1211.5063v2.pdf | author:Razvan Pascanu, Tomas Mikolov, Yoshua Bengio category:cs.LG published:2012-11-21 summary:There are two widely known issues with properly training Recurrent NeuralNetworks, the vanishing and the exploding gradient problems detailed in Bengioet al. (1994). In this paper we attempt to improve the understanding of theunderlying issues by exploring these problems from an analytical, a geometricand a dynamical systems perspective. Our analysis is used to justify a simpleyet effective solution. We propose a gradient norm clipping strategy to dealwith exploding gradients and a soft constraint for the vanishing gradientsproblem. We validate empirically our hypothesis and proposed solutions in theexperimental section.
arxiv-2400-191 | MAD-Bayes: MAP-based Asymptotic Derivations from Bayes | http://arxiv.org/pdf/1212.2126v2.pdf | author:Tamara Broderick, Brian Kulis, Michael I. Jordan category:stat.ML published:2012-12-10 summary:The classical mixture of Gaussians model is related to K-means viasmall-variance asymptotics: as the covariances of the Gaussians tend to zero,the negative log-likelihood of the mixture of Gaussians model approaches theK-means objective, and the EM algorithm approaches the K-means algorithm. Kulis& Jordan (2012) used this observation to obtain a novel K-means-like algorithmfrom a Gibbs sampler for the Dirichlet process (DP) mixture. We insteadconsider applying small-variance asymptotics directly to the posterior inBayesian nonparametric models. This framework is independent of any specificBayesian inference algorithm, and it has the major advantage that itgeneralizes immediately to a range of models beyond the DP mixture. Toillustrate, we apply our framework to the feature learning setting, where thebeta process and Indian buffet process provide an appropriate Bayesiannonparametric prior. We obtain a novel objective function that goes beyondclustering to learn (and penalize new) groupings for which we relax the mutualexclusivity and exhaustivity assumptions of clustering. We demonstrate severalother algorithms, all of which are scalable and simple to implement. Empiricalresults demonstrate the benefits of the new framework.
arxiv-2400-192 | Robust Image Segmentation in Low Depth Of Field Images | http://arxiv.org/pdf/1302.3900v1.pdf | author:Franz Graf, Hans-Peter Kriegel, Michael Weiler category:cs.CV published:2013-02-15 summary:In photography, low depth of field (DOF) is an important technique toemphasize the object of interest (OOI) within an image. Thus, low DOF imagesare widely used in the application area of macro, portrait or sportsphotography. When viewing a low DOF image, the viewer implicitly concentrateson the regions that are sharper regions of the image and thus segments theimage into regions of interest and non regions of interest which has a majorimpact on the perception of the image. Thus, a robust algorithm for the fullyautomatic detection of the OOI in low DOF images provides valuable informationfor subsequent image processing and image retrieval. In this paper we propose arobust and parameterless algorithm for the fully automatic segmentation of lowDOF images. We compare our method with three similar methods and show thesuperior robustness even though our algorithm does not require any parametersto be set by hand. The experiments are conducted on a real world data set withhigh and low DOF images.
arxiv-2400-193 | Identifying trends in word frequency dynamics | http://arxiv.org/pdf/1302.3892v1.pdf | author:Eduardo G. Altmann, Zakary L. Whichard, Adilson E. Motter category:physics.soc-ph cs.CL q-bio.PE published:2013-02-15 summary:The word-stock of a language is a complex dynamical system in which words canbe created, evolve, and become extinct. Even more dynamic are the short-termfluctuations in word usage by individuals in a population. Building on therecent demonstration that word niche is a strong determinant of future rise orfall in word frequency, here we introduce a model that allows us to distinguishpersistent from temporary increases in frequency. Our model is illustratedusing a 10^8-word database from an online discussion group and a 10^11-wordcollection of digitized books. The model reveals a strong relation betweenchanges in word dissemination and changes in frequency. Aside from theirimplications for short-term word frequency dynamics, these observations arepotentially important for language evolution as new words must survive in theshort term in order to survive in the long term.
arxiv-2400-194 | A Non-Binary Associative Memory with Exponential Pattern Retrieval Capacity and Iterative Learning: Extended Results | http://arxiv.org/pdf/1302.1156v2.pdf | author:Amir Hesam Salavati, K. Raj Kumar, Amin Shokrollahi category:cs.NE published:2013-02-05 summary:We consider the problem of neural association for a network of non-binaryneurons. Here, the task is to first memorize a set of patterns using a networkof neurons whose states assume values from a finite number of integer levels.Later, the same network should be able to recall previously memorized patternsfrom their noisy versions. Prior work in this area consider storing a finitenumber of purely random patterns, and have shown that the pattern retrievalcapacities (maximum number of patterns that can be memorized) scale onlylinearly with the number of neurons in the network. In our formulation of the problem, we concentrate on exploiting redundancyand internal structure of the patterns in order to improve the patternretrieval capacity. Our first result shows that if the given patterns have asuitable linear-algebraic structure, i.e. comprise a sub-space of the set ofall possible patterns, then the pattern retrieval capacity is in factexponential in terms of the number of neurons. The second result extends theprevious finding to cases where the patterns have weak minor components, i.e.the smallest eigenvalues of the correlation matrix tend toward zero. We willuse these minor components (or the basis vectors of the pattern null space) toboth increase the pattern retrieval capacity and error correction capabilities. An iterative algorithm is proposed for the learning phase, and two simpleneural update algorithms are presented for the recall phase. Using analyticalresults and simulations, we show that the proposed methods can tolerate a fairamount of errors in the input while being able to memorize an exponentiallylarge number of patterns.
arxiv-2400-195 | Augment-and-Conquer Negative Binomial Processes | http://arxiv.org/pdf/1209.1119v2.pdf | author:Mingyuan Zhou, Lawrence Carin category:stat.ML stat.ME published:2012-09-05 summary:By developing data augmentation methods unique to the negative binomial (NB)distribution, we unite seemingly disjoint count and mixture models under the NBprocess framework. We develop fundamental properties of the models and deriveefficient Gibbs sampling inference. We show that the gamma-NB process can bereduced to the hierarchical Dirichlet process with normalization, highlightingits unique theoretical, structural and computational advantages. A variety ofNB processes with distinct sharing mechanisms are constructed and applied totopic modeling, with connections to existing algorithms, showing the importanceof inferring both the NB dispersion and probability parameters.
arxiv-2400-196 | Thompson Sampling in Switching Environments with Bayesian Online Change Point Detection | http://arxiv.org/pdf/1302.3721v1.pdf | author:Joseph Mellor, Jonathan Shapiro category:cs.LG published:2013-02-15 summary:Thompson Sampling has recently been shown to be optimal in the BernoulliMulti-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumesstationary distributions for the rewards. It is often unrealistic to model thereal world as a stationary distribution. In this paper we derive and evaluatealgorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem.We propose a Thompson Sampling strategy equipped with a Bayesian change pointmechanism to tackle this problem. We develop algorithms for a variety of caseswith constant switching rate: when switching occurs all arms change (GlobalSwitching), switching occurs independently for each arm (Per-Arm Switching),when the switching rate is known and when it must be inferred from data. Thisleads to a family of algorithms we collectively term Change-Point ThompsonSampling (CTS). We show empirical results of the algorithm in 4 artificialenvironments, and 2 derived from real world data; news click-through[Yahoo!,2011] and foreign exchange data[Dukascopy, 2012], comparing them to some otherbandit algorithms. In real world data CTS is the most effective.
arxiv-2400-197 | A Fresnelet-Based Encryption of Medical Images using Arnold Transform | http://arxiv.org/pdf/1302.3702v1.pdf | author:Muhammad Nazeer, Bibi Nargis, Yasir Mehmood Malik, Dai-Gyoung Kim category:cs.CR cs.CV 68U10, 68M11 published:2013-02-15 summary:Medical images are commonly stored in digital media and transmitted viaInternet for certain uses. If a medical information image alters, this can leadto a wrong diagnosis which may create a serious health problem. Moreover,medical images in digital form can easily be modified by wiping off or addingsmall pieces of information intentionally for certain illegal purposes. Hence,the reliability of medical images is an important criterion in a hospitalinformation system. In this paper, Fresnelet transform is employed along withappropriate handling of the Arnold transform and the discrete cosine transformto provide secure distribution of medical images. This method presents a newdata hiding system in which steganography and cryptography are used to preventunauthorized data access. The experimental results exhibit highimperceptibility for embedded images and significant encryption of informationimages.
arxiv-2400-198 | Density Ratio Hidden Markov Models | http://arxiv.org/pdf/1302.3700v1.pdf | author:John A. Quinn, Masashi Sugiyama category:stat.ML cs.LG published:2013-02-15 summary:Hidden Markov models and their variants are the predominant sequentialclassification method in such domains as speech recognition, bioinformatics andnatural language processing. Being generative rather than discriminativemodels, however, their classification performance is a drawback. In this paperwe apply ideas from the field of density ratio estimation to bypass thedifficult step of learning likelihood functions in HMMs. By reformulatinginference and model fitting in terms of density ratios and applying a fastkernel-based estimation method, we show that it is possible to obtain astriking increase in discriminative performance while retaining theprobabilistic qualities of the HMM. We demonstrate experimentally that thisformulation makes more efficient use of training data than alternativeapproaches.
arxiv-2400-199 | Bio-inspired data mining: Treating malware signatures as biosequences | http://arxiv.org/pdf/1302.3668v1.pdf | author:Ajit Narayanan, Yi Chen category:cs.LG q-bio.QM stat.ML I.2.6 published:2013-02-15 summary:The application of machine learning to bioinformatics problems is wellestablished. Less well understood is the application of bioinformaticstechniques to machine learning and, in particular, the representation ofnon-biological data as biosequences. The aim of this paper is to explore theeffects of giving amino acid representation to problematic machine learningdata and to evaluate the benefits of supplementing traditional machine learningwith bioinformatics tools and techniques. The signatures of 60 computer virusesand 60 computer worms were converted into amino acid representations and firstmultiply aligned separately to identify conserved regions across differentfamilies within each class (virus and worm). This was followed by a secondalignment of all 120 aligned signatures together so that non-conserved regionswere identified prior to input to a number of machine learning techniques.Differences in length between virus and worm signatures after the firstalignment were resolved by the second alignment. Our first set of experimentsindicates that representing computer malware signatures as amino acid sequencesfollowed by alignment leads to greater classification and prediction accuracy.Our second set of experiments indicates that checking the results of datamining from artificial virus and worm data against known proteins can lead togeneralizations being made from the domain of naturally occurring proteins tomalware signatures. However, further work is needed to determine the advantagesand disadvantages of different representations and sequence alignment methodsfor handling problematic machine learning data.
arxiv-2400-200 | An analysis of NK and generalized NK landscapes | http://arxiv.org/pdf/1302.3541v1.pdf | author:Jeffrey S. Buzas, Jeffrey Dinitz category:cs.NE published:2013-02-14 summary:Simulated landscapes have been used for decades to evaluate search strategieswhose goal is to find the landscape location with maximum fitness. Applicationsinclude modeling the capacity of enzymes to catalyze reactions and the clinicaleffectiveness of medical treatments. Understanding properties of landscapes isimportant for understanding search difficulty. This paper presents a novel andtransparent characterization of NK landscapes. We prove that NK landscapes can be represented by parametric linearinteraction models where model coefficients have meaningful interpretations. Wederive the statistical properties of the model coefficients, providing insightinto how the NK algorithm parses importance to main effects and interactions.An important insight derived from the linear model representation is that therank of the linear model defined by the NK algorithm is correlated with thenumber of local optima, a strong determinant of landscape complexity and searchdifficulty. We show that the maximal rank for an NK landscape is achievedthrough epistatic interactions that form partially balanced incomplete blockdesigns. Finally, an analytic expression representing the expected number oflocal optima on the landscape is derived, providing a way to quickly computethe expected number of local optima for very large landscapes.
arxiv-2400-201 | A consistent clustering-based approach to estimating the number of change-points in highly dependent time-series | http://arxiv.org/pdf/1302.3407v1.pdf | author:Azaden Khaleghi, Daniil Ryabko category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2013-02-14 summary:The problem of change-point estimation is considered under a generalframework where the data are generated by unknown stationary ergodic processdistributions. In this context, the consistent estimation of the number ofchange-points is provably impossible. However, it is shown that a consistentclustering method may be used to estimate the number of change points, underthe additional constraint that the correct number of process distributions thatgenerate the data is provided. This additional parameter has a naturalinterpretation in many real-world applications. An algorithm is proposed thatestimates the number of change-points and locates the changes. The proposedalgorithm is shown to be asymptotically consistent; its empirical evaluationsare provided.
arxiv-2400-202 | Pavlov's dog associative learning demonstrated on synaptic-like organic transistors | http://arxiv.org/pdf/1302.3261v1.pdf | author:O. Bichler, W. Zhao, F. Alibart, S. Pleutin, S. Lenfant, D. Vuillaume, C. Gamrat category:q-bio.NC cs.ET cs.NE published:2013-02-13 summary:In this letter, we present an original demonstration of an associativelearning neural network inspired by the famous Pavlov's dogs experiment. Asingle nanoparticle organic memory field effect transistor (NOMFET) is used toimplement each synapse. We show how the physical properties of this dynamicmemristive device can be used to perform low power write operations for thelearning and implement short-term association using temporal coding and spiketiming dependent plasticity based learning. An electronic circuit was built tovalidate the proposed learning scheme with packaged devices, with goodreproducibility despite the complex synaptic-like dynamic of the NOMFET inpulse regime.
arxiv-2400-203 | Exact Methods for Multistage Estimation of a Binomial Proportion | http://arxiv.org/pdf/1302.3447v1.pdf | author:Zhengjia Chen, Xinjia Chen category:math.ST cs.LG cs.NA math.PR stat.TH published:2013-02-13 summary:We first review existing sequential methods for estimating a binomialproportion. Afterward, we propose a new family of group sequential samplingschemes for estimating a binomial proportion with prescribed margin of errorand confidence level. In particular, we establish the uniform controllabilityof coverage probability and the asymptotic optimality for such a family ofsampling schemes. Our theoretical results establish the possibility that theparameters of this family of sampling schemes can be determined so that theprescribed level of confidence is guaranteed with little waste of samples.Analytic bounds for the cumulative distribution functions and expectations ofsample numbers are derived. Moreover, we discuss the inherent connection ofvarious sampling schemes. Numerical issues are addressed for improving theaccuracy and efficiency of computation. Computational experiments are conductedfor comparing sampling schemes. Illustrative examples are given forapplications in clinical trials.
arxiv-2400-204 | Genetic braid optimization: A heuristic approach to compute quasiparticle braids | http://arxiv.org/pdf/1211.7359v2.pdf | author:Ross B. McDonald, Helmut G. Katzgraber category:quant-ph cs.NE published:2012-11-30 summary:In topologically-protected quantum computation, quantum gates can be carriedout by adiabatically braiding two-dimensional quasiparticles, reminiscent ofentangled world lines. Bonesteel et al. [Phys. Rev. Lett. 95, 140503 (2005)],as well as Leijnse and Flensberg [Phys. Rev. B 86, 104511 (2012)] recentlyprovided schemes for computing quantum gates from quasiparticle braids.Mathematically, the problem of executing a gate becomes that of finding aproduct of the generators (matrices) in that set that approximates the gatebest, up to an error. To date, efficient methods to compute these gates onlystrive to optimize for accuracy. We explore the possibility of using a genericapproach applicable to a variety of braiding problems based on evolutionary(genetic) algorithms. The method efficiently finds optimal braids whileallowing the user to optimize for the relative utilities of accuracy and/orlength. Furthermore, when optimizing for error only, the method can quicklyproduce efficient braids.
arxiv-2400-205 | Morphological Analusis Of The Left Ventricular Eendocardial Surface Using A Bag-Of-Features Descriptor | http://arxiv.org/pdf/1302.3155v1.pdf | author:Anirban Mukhopadhyay, Zhen Qian, Suchendra M. Bhandarkar, Tianming Liu, Sarah Rinehart, Szilard Voros category:cs.CV published:2013-02-13 summary:The limitations of conventional imaging techniques have hitherto precluded athorough and formal investigation of the complex morphology of the leftventricular (LV) endocardial surface and its relation to the severity ofCoronary Artery Disease (CAD). Recent developments in high-resolutionMultirow-Detector Computed Tomography (MDCT) scanner technology have enabledthe imaging of LV endocardial surface morphology in a single heart beat.Analysis of high-resolution Computed Tomography (CT) images from a 320-MDCTscanner allows the study of the relationship between percent Diameter Stenosis(DS) of the major coronary arteries and localization of the cardiac segmentsaffected by coronary arterial stenosis. In this paper a novel approach for theanalysis using a combination of rigid transformation-invariant shapedescriptors and a more generalized isometry-invariant Bag-of-Features (BoF)descriptor, is proposed and implemented. The proposed approach is shown to besuccessful in identifying, localizing and quantifying the incidence and extentof CAD and thus, is seen to have a potentially significant clinical impact.Specifically, the association between the incidence and extent of CAD,determined via the percent DS measurements of the major coronary arteries, andthe alterations in the endocardial surface morphology is formally quantified. Amultivariate regression test performed on a strict leave-one-out basis areshown to exhibit a distinct pattern in terms of the correlation coefficientwithin the cardiac segments where the incidence of coronary arterial stenosisis localized.
arxiv-2400-206 | Bayesian Learning of Loglinear Models for Neural Connectivity | http://arxiv.org/pdf/1302.3590v1.pdf | author:Kathryn Blackmond Laskey, Laura Martignon category:cs.LG q-bio.NC stat.AP stat.ML published:2013-02-13 summary:This paper presents a Bayesian approach to learning the connectivitystructure of a group of neurons from data on configuration frequencies. A majorobjective of the research is to provide statistical tools for detecting changesin firing patterns with changing stimuli. Our framework is not restricted tothe well-understood case of pair interactions, but generalizes the Boltzmannmachine model to allow for higher order interactions. The paper applies aMarkov Chain Monte Carlo Model Composition (MC3) algorithm to search overconnectivity structures and uses Laplace's method to approximate posteriorprobabilities of structures. Performance of the methods was tested on syntheticdata. The models were also applied to data obtained by Vaadia on multi-unitrecordings of several neurons in the visual cortex of a rhesus monkey in twodifferent attentional states. Results confirmed the experimenters' conjecturethat different attentional states were associated with different interactionstructures.
arxiv-2400-207 | On the Sample Complexity of Learning Bayesian Networks | http://arxiv.org/pdf/1302.3579v1.pdf | author:Nir Friedman, Zohar Yakhini category:cs.LG stat.ML published:2013-02-13 summary:In recent years there has been an increasing interest in learning Bayesiannetworks from data. One of the most effective methods for learning suchnetworks is based on the minimum description length (MDL) principle. Previouswork has shown that this learning procedure is asymptotically successful: withprobability one, it will converge to the target distribution, given asufficient number of samples. However, the rate of this convergence has beenhitherto unknown. In this work we examine the sample complexity of MDL basedlearning procedures for Bayesian networks. We show that the number of samplesneeded to learn an epsilon-close approximation (in terms of entropy distance)with confidence delta is O((1/epsilon)^(4/3)log(1/epsilon)log(1/delta)loglog(1/delta)). This means that the sample complexity is a low-order polynomial inthe error threshold and sub-linear in the confidence bound. We also discuss howthe constants in this term depend on the complexity of the target distribution.Finally, we address questions of asymptotic minimality and propose a method forusing the sample complexity results to speed up the learning process.
arxiv-2400-208 | Learning Bayesian Networks with Local Structure | http://arxiv.org/pdf/1302.3577v1.pdf | author:Nir Friedman, Moises Goldszmidt category:cs.AI cs.LG stat.ML published:2013-02-13 summary:In this paper we examine a novel addition to the known methods for learningBayesian networks from data that improves the quality of the learned networks.Our approach explicitly represents and learns the local structure in theconditional probability tables (CPTs), that quantify these networks. Thisincreases the space of possible models, enabling the representation of CPTswith a variable number of parameters that depends on the learned localstructures. The resulting learning procedure is capable of inducing models thatbetter emulate the real complexity of the interactions present in the data. Wedescribe the theoretical foundations and practical aspects of learning localstructures, as well as an empirical evaluation of the proposed method. Thisevaluation indicates that learning curves characterizing the procedure thatexploits the local structure converge faster than these of the standardprocedure. Our results also show that networks learned with local structuretend to be more complex (in terms of arcs), yet require less parameters.
arxiv-2400-209 | Learning Equivalence Classes of Bayesian Networks Structures | http://arxiv.org/pdf/1302.3566v1.pdf | author:David Maxwell Chickering category:cs.AI cs.LG stat.ML published:2013-02-13 summary:Approaches to learning Bayesian networks from data typically combine ascoring function with a heuristic search procedure. Given a Bayesian networkstructure, many of the scoring functions derived in the literature return ascore for the entire equivalence class to which the structure belongs. Whenusing such a scoring function, it is appropriate for the heuristic searchalgorithm to search over equivalence classes of Bayesian networks as opposed toindividual structures. We present the general formulation of a search space forwhich the states of the search correspond to equivalence classes of structures.Using this space, any one of a number of heuristic search algorithms can easilybe applied. We compare greedy search performance in the proposed search spaceto greedy search performance in a search space for which the states correspondto individual Bayesian network structures.
arxiv-2400-210 | Object Recognition with Imperfect Perception and Redundant Description | http://arxiv.org/pdf/1302.3556v1.pdf | author:Claude Barrouil, Jerome Lemaire category:cs.CV cs.AI published:2013-02-13 summary:This paper deals with a scene recognition system in a robotics contex. Thegeneral problem is to match images with <I>a priori</I> descriptions. A typicalmission would consist in identifying an object in an installation with a visionsystem situated at the end of a manipulator and with a human operator provideddescription, formulated in a pseudo-natural language, and possibly redundant.The originality of this work comes from the nature of the description, from thespecial attention given to the management of imprecision and uncertainty in theinterpretation process and from the way to assess the description redundancy soas to reinforce the overall matching likelihood.
arxiv-2400-211 | Sampled forms of functional PCA in reproducing kernel Hilbert spaces | http://arxiv.org/pdf/1109.3336v2.pdf | author:Arash A. Amini, Martin J. Wainwright category:math.ST stat.ML stat.TH published:2011-09-15 summary:We consider the sampling problem for functional PCA (fPCA), where thesimplest example is the case of taking time samples of the underlyingfunctional components. More generally, we model the sampling operation as acontinuous linear map from $\mathcal{H}$ to $\mathbb{R}^m$, where thefunctional components to lie in some Hilbert subspace $\mathcal{H}$ of $L^2$,such as a reproducing kernel Hilbert space of smooth functions. This modelincludes time and frequency sampling as special cases. In contrast to classicalapproach in fPCA in which access to entire functions is assumed, having alimited number m of functional samples places limitations on the performance ofstatistical procedures. We study these effects by analyzing the rate ofconvergence of an M-estimator for the subspace spanned by the leadingcomponents in a multi-spiked covariance model. The estimator takes the form ofregularized PCA, and hence is computationally attractive. We analyze thebehavior of this estimator within a nonasymptotic framework, and provide boundsthat hold with high probability as a function of the number of statisticalsamples n and the number of functional samples m. We also derive lower boundsshowing that the rates obtained are minimax optimal.
arxiv-2400-212 | Building a reordering system using tree-to-string hierarchical model | http://arxiv.org/pdf/1302.3057v1.pdf | author:Jacob Dlougach, Irina Galinskaya category:cs.CL published:2013-02-13 summary:This paper describes our submission to the First Workshop on Reordering forStatistical Machine Translation. We have decided to build a reordering systembased on tree-to-string model, using only publicly available tools toaccomplish this task. With the provided training data we have built atranslation model using Moses toolkit, and then we applied a chart decoder,implemented in Moses, to reorder the sentences. Even though our submission onlycovered English-Farsi language pair, we believe that the approach itself shouldwork regardless of the choice of the languages, so we have also carried out theexperiments for English-Italian and English-Urdu. For these language pairs wehave noticed a significant improvement over the baseline in BLEU, Kendall-Tauand Hamming metrics. A detailed description is given, so that everyone canreproduce our results. Also, some possible directions for further improvementsare discussed.
arxiv-2400-213 | An Efficient Dual Approach to Distance Metric Learning | http://arxiv.org/pdf/1302.3219v1.pdf | author:Chunhua Shen, Junae Kim, Fayao Liu, Lei Wang, Anton van den Hengel category:cs.LG published:2013-02-13 summary:Distance metric learning is of fundamental interest in machine learningbecause the distance metric employed can significantly affect the performanceof many learning methods. Quadratic Mahalanobis metric learning is a popularapproach to the problem, but typically requires solving a semidefiniteprogramming (SDP) problem, which is computationally expensive. Standardinterior-point SDP solvers typically have a complexity of $O(D^{6.5})$ (with$D$ the dimension of input data), and can thus only practically solve problemsexhibiting less than a few thousand variables. Since the number of variables is$D (D+1) / 2 $, this implies a limit upon the size of problem that canpractically be solved of around a few hundred dimensions. The complexity of thepopular quadratic Mahalanobis metric learning approach thus limits the size ofproblem to which metric learning can be applied. Here we propose asignificantly more efficient approach to the metric learning problem based onthe Lagrange dual formulation of the problem. The proposed formulation is muchsimpler to implement, and therefore allows much larger Mahalanobis metriclearning problems to be solved. The time complexity of the proposed method is$O (D ^ 3) $, which is significantly lower than that of the SDP approach.Experiments on a variety of datasets demonstrate that the proposed methodachieves an accuracy comparable to the state-of-the-art, but is applicable tosignificantly larger problems. We also show that the proposed method can beapplied to solve more general Frobenius-norm regularized SDP problemsapproximately.
arxiv-2400-214 | Joint variable and rank selection for parsimonious estimation of high-dimensional matrices | http://arxiv.org/pdf/1110.3556v4.pdf | author:Florentina Bunea, Yiyuan She, Marten H. Wegkamp category:math.ST stat.ME stat.ML stat.TH published:2011-10-17 summary:We propose dimension reduction methods for sparse, high-dimensionalmultivariate response regression models. Both the number of responses and thatof the predictors may exceed the sample size. Sometimes viewed ascomplementary, predictor selection and rank reduction are the most popularstrategies for obtaining lower-dimensional approximations of the parametermatrix in such models. We show in this article that important gains inprediction accuracy can be obtained by considering them jointly. We motivate anew class of sparse multivariate regression models, in which the coefficientmatrix has low rank and zero rows or can be well approximated by such a matrix.Next, we introduce estimators that are based on penalized least squares, withnovel penalties that impose simultaneous row and rank restrictions on thecoefficient matrix. We prove that these estimators indeed adapt to the unknownmatrix sparsity and have fast rates of convergence. We support our theoreticalresults with an extensive simulation study and two data analyses.
arxiv-2400-215 | Towards Identification of Relevant Variables in the observed Aerosol Optical Depth Bias between MODIS and AERONET observations | http://arxiv.org/pdf/1302.2969v1.pdf | author:N. K. Malakar, D. J. Lary, D. Gencaga, A. Albayrak, J. Wei category:stat.ML published:2013-02-13 summary:Measurements made by satellite remote sensing, Moderate Resolution ImagingSpectroradiometer (MODIS), and globally distributed Aerosol Robotic Network(AERONET) are compared. Comparison of the two datasets measurements for aerosoloptical depth values show that there are biases between the two data products.In this paper, we present a general framework towards identifying relevant setof variables responsible for the observed bias. We present a general frameworkto identify the possible factors influencing the bias, which might beassociated with the measurement conditions such as the solar and sensor zenithangles, the solar and sensor azimuth, scattering angles, and surfacereflectivity at the various measured wavelengths, etc. Specifically, weperformed analysis for remote sensing Aqua-Land data set, and used machinelearning technique, neural network in this case, to perform multivariateregression between the ground-truth and the training data sets. Finally, weused mutual information between the observed and the predicted values as themeasure of similarity to identify the most relevant set of variables. Thesearch is brute force method as we have to consider all possible combinations.The computations involves a huge number crunching exercise, and we implementedit by writing a job-parallel program.
arxiv-2400-216 | Bounded regret in stochastic multi-armed bandits | http://arxiv.org/pdf/1302.1611v2.pdf | author:Sébastien Bubeck, Vianney Perchet, Philippe Rigollet category:math.ST cs.LG stat.ML stat.TH 62L05 published:2013-02-06 summary:We study the stochastic multi-armed bandit problem when one knows the value$\mu^{(\star)}$ of an optimal arm, as a well as a positive lower bound on thesmallest positive gap $\Delta$. We propose a new randomized policy that attainsa regret {\em uniformly bounded over time} in this setting. We also proveseveral lower bounds, which show in particular that bounded regret is notpossible if one only knows $\Delta$, and bounded regret of order $1/\Delta$ isnot possible if one only knows $\mu^{(\star)}$
arxiv-2400-217 | Competing With Strategies | http://arxiv.org/pdf/1302.2672v1.pdf | author:Wei Han, Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.GT cs.LG published:2013-02-12 summary:We study the problem of online learning with a notion of regret defined withrespect to a set of strategies. We develop tools for analyzing the minimaxrates and for deriving regret-minimization algorithms in this scenario. Whilethe standard methods for minimizing the usual notion of regret fail, throughour analysis we demonstrate existence of regret-minimization methods thatcompete with such sets of strategies as: autoregressive algorithms, strategiesbased on statistical models, regularized least squares, and follow theregularized leader strategies. In several cases we also derive efficientlearning algorithms.
arxiv-2400-218 | The trace norm constrained matrix-variate Gaussian process for multitask bipartite ranking | http://arxiv.org/pdf/1302.2576v1.pdf | author:Oluwasanmi Koyejo, Cheng Lee, Joydeep Ghosh category:cs.LG stat.ML published:2013-02-11 summary:We propose a novel hierarchical model for multitask bipartite ranking. Theproposed approach combines a matrix-variate Gaussian process with a generativemodel for task-wise bipartite ranking. In addition, we employ a novel traceconstrained variational inference approach to impose low rank structure on theposterior matrix-variate Gaussian process. The resulting posterior covariancefunction is derived in closed form, and the posterior mean function is thesolution to a matrix-variate regression with a novel spectral elastic netregularizer. Further, we show that variational inference for the traceconstrained matrix-variate Gaussian process combined with maximum likelihoodparameter estimation for the bipartite ranking model is jointly convex. Ourmotivating application is the prioritization of candidate disease genes. Thegoal of this task is to aid the identification of unobserved associationsbetween human genes and diseases using a small set of observed associations aswell as kernels induced by gene-gene interaction networks and diseaseontologies. Our experimental results illustrate the performance of the proposedmodel on real world datasets. Moreover, we find that the resulting low ranksolution improves the computational scalability of training and testing ascompared to baseline models.
arxiv-2400-219 | Toric grammars: a new statistical approach to natural language modeling | http://arxiv.org/pdf/1302.2569v1.pdf | author:Olivier Catoni, Thomas Mainguy category:stat.ML cs.CL math.PR published:2013-02-11 summary:We propose a new statistical model for computational linguistics. Rather thantrying to estimate directly the probability distribution of a random sentenceof the language, we define a Markov chain on finite sets of sentences with manyfinite recurrent communicating classes and define our language model as theinvariant probability measures of the chain on each recurrent communicatingclass. This Markov chain, that we call a communication model, recombines ateach step randomly the set of sentences forming its current state, using somegrammar rules. When the grammar rules are fixed and known in advance instead ofbeing estimated on the fly, we can prove supplementary mathematical properties.In particular, we can prove in this case that all states are recurrent states,so that the chain defines a partition of its state space into finite recurrentcommunicating classes. We show that our approach is a decisive departure fromMarkov models at the sentence level and discuss its relationships with ContextFree Grammars. Although the toric grammars we use are closely related toContext Free Grammars, the way we generate the language from the grammar isqualitatively different. Our communication model has two purposes. On the onehand, it is used to define indirectly the probability distribution of a randomsentence of the language. On the other hand it can serve as a (crude) model oflanguage transmission from one speaker to another speaker through thecommunication of a (large) set of sentences.
arxiv-2400-220 | Selecting the State-Representation in Reinforcement Learning | http://arxiv.org/pdf/1302.2552v1.pdf | author:Odalric-Ambrym Maillard, Rémi Munos, Daniil Ryabko category:cs.LG published:2013-02-11 summary:The problem of selecting the right state-representation in a reinforcementlearning problem is considered. Several models (functions mapping pastobservations to a finite set) of the observations are given, and it is knownthat for at least one of these models the resulting state dynamics are indeedMarkovian. Without knowing neither which of the models is the correct one, norwhat are the probabilistic characteristics of the resulting MDP, it is requiredto obtain as much reward as the optimal policy for the correct model (or forthe best of the correct models, if there are several). We propose an algorithmthat achieves that, with a regret of order T^{2/3} where T is the horizon time.
arxiv-2400-221 | Online Regret Bounds for Undiscounted Continuous Reinforcement Learning | http://arxiv.org/pdf/1302.2550v1.pdf | author:Ronald Ortner, Daniil Ryabko category:cs.LG published:2013-02-11 summary:We derive sublinear regret bounds for undiscounted reinforcement learning incontinuous state space. The proposed algorithm combines state aggregation withthe use of upper confidence bounds for implementing optimism in the face ofuncertainty. Beside the existence of an optimal policy which satisfies thePoisson equation, the only assumptions made are Holder continuity of rewardsand transition probabilities.
arxiv-2400-222 | Extracting useful rules through improved decision tree induction using information entropy | http://arxiv.org/pdf/1302.2436v1.pdf | author:Mohd Mahmood Ali, Mohd S Qaseem, Lakshmi Rajamani, A Govardhan category:cs.LG published:2013-02-11 summary:Classification is widely used technique in the data mining domain, wherescalability and efficiency are the immediate problems in classificationalgorithms for large databases. We suggest improvements to the existing C4.5decision tree algorithm. In this paper attribute oriented induction (AOI) andrelevance analysis are incorporated with concept hierarchys knowledge andHeightBalancePriority algorithm for construction of decision tree along withMulti level mining. The assignment of priorities to attributes is done byevaluating information entropy, at different levels of abstraction for buildingdecision tree using HeightBalancePriority algorithm. Modified DMQL queries areused to understand and explore the shortcomings of the decision trees generatedby C4.5 classifier for education dataset and the results are compared with theproposed approach.
arxiv-2400-223 | Anomalous Vacillatory Learning | http://arxiv.org/pdf/1210.2051v2.pdf | author:Achilles Beros category:math.LO cs.LG cs.LO 03D80, 68Q32 published:2012-10-07 summary:In 1986, Osherson, Stob and Weinstein asked whether two variants of anomalousvacillatory learning, TxtFex^*_* and TxtFext^*_*, could be distinguished. Inboth, a machine is permitted to vacillate between a finite number of hypothesesand to make a finite number of errors. TxtFext^*_*-learning requires thathypotheses output infinitely often must describe the same finite variant of thecorrect set, while TxtFex^*_*-learning permits the learner to vacillate betweenfinitely many different finite variants of the correct set. In this paper weshow that TxtFex^*_* \neq TxtFext^*_*, thereby answering the question posed byOsherson, \textit{et al}. We prove this in a strong way by exhibiting a familyin TxtFex^*_2 \setminus {TxtFext}^*_*.
arxiv-2400-224 | Learning Universally Quantified Invariants of Linear Data Structures | http://arxiv.org/pdf/1302.2273v1.pdf | author:Pranav Garg, Christof Loding, P. Madhusudan, Daniel Neider category:cs.PL cs.FL cs.LG published:2013-02-09 summary:We propose a new automaton model, called quantified data automata over words,that can model quantified invariants over linear data structures, and buildpoly-time active learning algorithms for them, where the learner is allowed toquery the teacher with membership and equivalence queries. In order to expressinvariants in decidable logics, we invent a decidable subclass of QDAs, calledelastic QDAs, and prove that every QDA has a uniqueminimally-over-approximating elastic QDA. We then give an application of thesetheoretically sound and efficient active learning algorithms in a passivelearning framework and show that we can efficiently learn quantified lineardata structure invariants from samples obtained from dynamic runs for a largeclass of programs.
arxiv-2400-225 | Metabolic cost as an organizing principle for cooperative learning | http://arxiv.org/pdf/1202.4482v2.pdf | author:David Balduzzi, Pedro A Ortega, Michel Besserve category:q-bio.NC cs.LG nlin.AO published:2012-02-20 summary:This paper investigates how neurons can use metabolic cost to facilitatelearning at a population level. Although decision-making by individual neuronshas been extensively studied, questions regarding how neurons should behave tocooperate effectively remain largely unaddressed. Under assumptions thatcapture a few basic features of cortical neurons, we show that constrainingreward maximization by metabolic cost aligns the information content of actionswith their expected reward. Thus, metabolic cost provides a mechanism wherebyneurons encode expected reward into their outputs. Further, aside from reducingenergy expenditures, imposing a tight metabolic constraint also increases theaccuracy of empirical estimates of rewards, increasing the robustness ofdistributed learning. Finally, we present two implementations of metabolicallyconstrained learning that confirm our theoretical finding. These resultssuggest that metabolic cost may be an organizing principle underlying theneural code, and may also provide a useful guide to the design and analysis ofother cooperating populations.
arxiv-2400-226 | Hybrid Deterministic-Stochastic Methods for Data Fitting | http://arxiv.org/pdf/1104.2373v4.pdf | author:Michael P. Friedlander, Mark Schmidt category:cs.NA cs.SY math.OC stat.ML published:2011-04-13 summary:Many structured data-fitting applications require the solution of anoptimization problem involving a sum over a potentially large number ofmeasurements. Incremental gradient algorithms offer inexpensive iterations bysampling a subset of the terms in the sum. These methods can make greatprogress initially, but often slow as they approach a solution. In contrast,full-gradient methods achieve steady convergence at the expense of evaluatingthe full objective and gradient on each iteration. We explore hybrid methodsthat exhibit the benefits of both approaches. Rate-of-convergence analysisshows that by controlling the sample size in an incremental gradient algorithm,it is possible to maintain the steady convergence rates of full-gradientmethods. We detail a practical quasi-Newton implementation based on thisapproach. Numerical experiments illustrate its potential benefits.
arxiv-2400-227 | Quadratic Basis Pursuit | http://arxiv.org/pdf/1301.7002v2.pdf | author:Henrik Ohlsson, Allen Y. Yang, Roy Dong, Michel Verhaegen, S. Shankar Sastry category:cs.IT math.IT stat.ML published:2013-01-29 summary:In many compressive sensing problems today, the relationship between themeasurements and the unknowns could be nonlinear. Traditional treatment of suchnonlinear relationships have been to approximate the nonlinearity via a linearmodel and the subsequent un-modeled dynamics as noise. The ability to moreaccurately characterize nonlinear models has the potential to improve theresults in both existing compressive sensing applications and those where alinear approximation does not suffice, e.g., phase retrieval. In this paper, weextend the classical compressive sensing framework to a second-order Taylorexpansion of the nonlinearity. Using a lifting technique and a method we callquadratic basis pursuit, we show that the sparse signal can be recoveredexactly when the sampling rate is sufficiently high. We further presentefficient numerical algorithms to recover sparse signals in second-ordernonlinear systems, which are considerably more difficult to solve than theirlinear counterparts in sparse optimization.
arxiv-2400-228 | Minimax Optimal Algorithms for Unconstrained Linear Optimization | http://arxiv.org/pdf/1302.2176v1.pdf | author:H. Brendan McMahan category:cs.LG published:2013-02-08 summary:We design and analyze minimax-optimal algorithms for online linearoptimization games where the player's choice is unconstrained. The playerstrives to minimize regret, the difference between his loss and the loss of apost-hoc benchmark strategy. The standard benchmark is the loss of the beststrategy chosen from a bounded comparator set. When the the comparison set andthe adversary's gradients satisfy L_infinity bounds, we give the value of thegame in closed form and prove it approaches sqrt(2T/pi) as T -> infinity. Interesting algorithms result when we consider soft constraints on thecomparator, rather than restricting it to a bounded set. As a warmup, weanalyze the game with a quadratic penalty. The value of this game is exactlyT/2, and this value is achieved by perhaps the simplest online algorithm ofall: unprojected gradient descent with a constant learning rate. We then derive a minimax-optimal algorithm for a much softer penaltyfunction. This algorithm achieves good bounds under the standard notion ofregret for any comparator point, without needing to specify the comparator setin advance. The value of this game converges to sqrt{e} as T ->infinity; wegive a closed-form for the exact value as a function of T. The resultingalgorithm is natural in unconstrained investment or betting scenarios, since itguarantees at worst constant loss, while allowing for exponential rewardagainst an "easy" adversary.
arxiv-2400-229 | Data Mining of the Concept "End of the World" in Twitter Microblogs | http://arxiv.org/pdf/1302.2131v1.pdf | author:Bohdan Pavlyshenko category:cs.SI cs.CL cs.IR physics.soc-ph published:2013-02-08 summary:This paper describes the analysis of quantitative characteristics of frequentsets and association rules in the posts of Twitter microblogs, related to thediscussion of "end of the world", which was allegedly predicted on December 21,2012 due to the Mayan calendar. Discovered frequent sets and association rulescharacterize semantic relations between the concepts of analyzed subjects.Thesupport for some fequent sets reaches the global maximum before the expectedevent with some time delay. Such frequent sets may be considered as predictivemarkers that characterize the significance of expected events for blogosphereusers. It was shown that time dynamics of confidence of some revealedassociation rules can also have predictive characteristics. Exceeding a certainthreshold, it may be a signal for the corresponding reaction in the societyduring the time interval between the maximum and probable coming of an event.
arxiv-2400-230 | Efficiency for Regularization Parameter Selection in Penalized Likelihood Estimation of Misspecified Models | http://arxiv.org/pdf/1302.2068v1.pdf | author:Cheryl J. Flynn, Clifford M. Hurvich, Jeffrey S. Simonoff category:stat.ML published:2013-02-08 summary:It has been shown that AIC-type criteria are asymptotically efficientselectors of the tuning parameter in non-concave penalized regression methodsunder the assumption that the population variance is known or that a consistentestimator is available. We relax this assumption to prove that AIC itself isasymptotically efficient and we study its performance in finite samples. Inclassical regression, it is known that AIC tends to select overly complexmodels when the dimension of the maximum candidate model is large relative tothe sample size. Simulation studies suggest that AIC suffers from the sameshortcomings when used in penalized regression. We therefore propose the use ofthe classical corrected AIC (AICc) as an alternative and prove that itmaintains the desired asymptotic properties. To broaden our results, we furtherprove the efficiency of AIC for penalized likelihood methods in the context ofgeneralized linear models with no dispersion parameter. Similar results existin the literature but only for a restricted set of candidate models. Byemploying results from the classical literature on maximum-likelihoodestimation in misspecified models, we are able to establish this result for ageneral set of candidate models. We use simulations to assess the performanceof AIC and AICc, as well as that of other selectors, in finite samples for bothSCAD-penalized and Lasso regressions and a real data example is considered.
arxiv-2400-231 | Political Disaffection: a case study on the Italian Twitter community | http://arxiv.org/pdf/1301.6630v2.pdf | author:Corrado Monti, Alessandro Rozza, Giovanni Zappella, Matteo Zignani, Adam Arvidsson, Monica Poletti category:cs.SI cs.LG physics.soc-ph published:2013-01-28 summary:In our work we analyse the political disaffection or "the subjective feelingof powerlessness, cynicism, and lack of confidence in the political process,politicians, and democratic institutions, but with no questioning of thepolitical regime" by exploiting Twitter data through machine learningtechniques. In order to validate the quality of the time-series generated bythe Twitter data, we highlight the relations of these data with politicaldisaffection as measured by means of public opinion surveys. Moreover, we showthat important political news of Italian newspapers are often correlated withthe highest peaks of the produced time-series.
arxiv-2400-232 | A new compressive video sensing framework for mobile broadcast | http://arxiv.org/pdf/1302.1947v1.pdf | author:Chengbo Li, Hong Jiang, Paul Wilford, Yin Zhang, Mike Scheutzow category:cs.MM cs.CV cs.IT math.IT published:2013-02-08 summary:A new video coding method based on compressive sampling is proposed. In thismethod, a video is coded using compressive measurements on video cubes. Videoreconstruction is performed by minimization of total variation (TV) of thepixelwise DCT coefficients along the temporal direction. A new reconstructionalgorithm is developed from TVAL3, an efficient TV minimization algorithm basedon the alternating minimization and augmented Lagrangian methods. Video codingwith this method is inherently scalable, and has applications in mobilebroadcast.
arxiv-2400-233 | Surveillance Video Processing Using Compressive Sensing | http://arxiv.org/pdf/1302.1942v1.pdf | author:Hong Jiang, Wei Deng, Zuowei Shen category:cs.CV cs.IT math.IT published:2013-02-08 summary:A compressive sensing method combined with decomposition of a matrix formedwith image frames of a surveillance video into low rank and sparse matrices isproposed to segment the background and extract moving objects in a surveillancevideo. The video is acquired by compressive measurements, and the measurementsare used to reconstruct the video by a low rank and sparse decomposition ofmatrix. The low rank component represents the background, and the sparsecomponent is used to identify moving objects in the surveillance video. Thedecomposition is performed by an augmented Lagrangian alternating directionmethod. Experiments are carried out to demonstrate that moving objects can bereliably extracted with a small amount of measurements.
arxiv-2400-234 | Optimal rates for first-order stochastic convex optimization under Tsybakov noise condition | http://arxiv.org/pdf/1207.3012v2.pdf | author:Aaditya Ramdas, Aarti Singh category:cs.LG stat.ML published:2012-07-12 summary:We focus on the problem of minimizing a convex function $f$ over a convex set$S$ given $T$ queries to a stochastic first order oracle. We argue that thecomplexity of convex minimization is only determined by the rate of growth ofthe function around its minimizer $x^*_{f,S}$, as quantified by a Tsybakov-likenoise condition. Specifically, we prove that if $f$ grows at least as fast as$\x-x^*_{f,S}\^\kappa$ around its minimum, for some $\kappa > 1$, then theoptimal rate of learning $f(x^*_{f,S})$ is$\Theta(T^{-\frac{\kappa}{2\kappa-2}})$. The classic rate $\Theta(1/\sqrt T)$for convex functions and $\Theta(1/T)$ for strongly convex functions arespecial cases of our result for $\kappa \rightarrow \infty$ and $\kappa=2$, andeven faster rates are attained for $\kappa <2$. We also derive tight bounds forthe complexity of learning $x_{f,S}^*$, where the optimal rate is$\Theta(T^{-\frac{1}{2\kappa-2}})$. Interestingly, these precise rates forconvex optimization also characterize the complexity of active learning and ourresults further strengthen the connections between the two fields, both ofwhich rely on feedback-driven queries.
arxiv-2400-235 | The Role of Weight Shrinking in Large Margin Perceptron Learning | http://arxiv.org/pdf/1205.4698v2.pdf | author:Constantinos Panagiotakopoulos, Petroula Tsampouka category:cs.LG published:2012-05-21 summary:We introduce into the classical perceptron algorithm with margin a mechanismthat shrinks the current weight vector as a first step of the update. If theshrinking factor is constant the resulting algorithm may be regarded as amargin-error-driven version of NORMA with constant learning rate. In this casewe show that the allowed strength of shrinking depends on the value of themaximum margin. We also consider variable shrinking factors for which there isno such dependence. In both cases we obtain new generalizations of theperceptron with margin able to provably attain in a finite number of steps anydesirable approximation of the maximal margin hyperplane. The new approximatemaximum margin classifiers appear experimentally to be very competitive in2-norm soft margin tasks involving linear kernels.
arxiv-2400-236 | Lensless Compressive Sensing Imaging | http://arxiv.org/pdf/1302.1789v1.pdf | author:Gang Huang, Hong Jiang, Kim Matthews, Paul Wilford category:cs.CV cs.IT math.IT published:2013-02-07 summary:In this paper, we propose a lensless compressive sensing imagingarchitecture. The architecture consists of two components, an aperture assemblyand a sensor. No lens is used. The aperture assembly consists of a twodimensional array of aperture elements. The transmittance of each apertureelement is independently controllable. The sensor is a single detectionelement, such as a single photo-conductive cell. Each aperture element togetherwith the sensor defines a cone of a bundle of rays, and the cones of theaperture assembly define the pixels of an image. Each pixel value of an imageis the integration of the bundle of rays in a cone. The sensor is used fortaking compressive measurements. Each measurement is the integration of rays inthe cones modulated by the transmittance of the aperture elements. Acompressive sensing matrix is implemented by adjusting the transmittance of theindividual aperture elements according to the values of the sensing matrix. Theproposed architecture is simple and reliable because no lens is used.Furthermore, the sharpness of an image from our device is only limited by theresolution of the aperture assembly, but not affected by blurring due todefocus. The architecture can be used for capturing images of visible lights,and other spectra such as infrared, or millimeter waves. Such devices may beused in surveillance applications for detecting anomalies or extractingfeatures such as speed of moving objects. Multiple sensors may be used with asingle aperture assembly to capture multi-view images simultaneously. Aprototype was built by using a LCD panel and a photoelectric sensor forcapturing images of visible spectrum.
arxiv-2400-237 | An ANN-based Method for Detecting Vocal Fold Pathology | http://arxiv.org/pdf/1302.1772v1.pdf | author:Vahid Majidnezhad, Igor Kheidorov category:cs.LG cs.CV cs.SD published:2013-02-07 summary:There are different algorithms for vocal fold pathology diagnosis. Thesealgorithms usually have three stages which are Feature Extraction, FeatureReduction and Classification. While the third stage implies a choice of avariety of machine learning methods, the first and second stages play acritical role in performance and accuracy of the classification system. In thispaper we present initial study of feature extraction and feature reduction inthe task of vocal fold pathology diagnosis. A new type of feature vector, basedon wavelet packet decomposition and Mel-Frequency-Cepstral-Coefficients(MFCCs), is proposed. Also Principal Component Analysis (PCA) is used forfeature reduction. An Artificial Neural Network is used as a classifier forevaluating the performance of our proposed method.
arxiv-2400-238 | Correcting Camera Shake by Incremental Sparse Approximation | http://arxiv.org/pdf/1302.0439v2.pdf | author:Paul Shearer, Anna C. Gilbert, Alfred O. Hero III category:cs.CV cs.GR published:2013-02-03 summary:The problem of deblurring an image when the blur kernel is unknown remainschallenging after decades of work. Recently there has been rapid progress oncorrecting irregular blur patterns caused by camera shake, but there is stillmuch room for improvement. We propose a new blind deconvolution method usingincremental sparse edge approximation to recover images blurred by camerashake. We estimate the blur kernel first from only the strongest edges in theimage, then gradually refine this estimate by allowing for weaker and weakeredges. Our method competes with the benchmark deblurring performance of thestate-of-the-art while being significantly faster and easier to generalize.
arxiv-2400-239 | Feature Selection for Microarray Gene Expression Data using Simulated Annealing guided by the Multivariate Joint Entropy | http://arxiv.org/pdf/1302.1733v1.pdf | author:Fernando González, Lluís A. Belanche category:q-bio.QM cs.CE cs.LG stat.ML published:2013-02-07 summary:In this work a new way to calculate the multivariate joint entropy ispresented. This measure is the basis for a fast information-theoretic basedevaluation of gene relevance in a Microarray Gene Expression data context. Itslow complexity is based on the reuse of previous computations to calculatecurrent feature relevance. The mu-TAFS algorithm --named as such todifferentiate it from previous TAFS algorithms-- implements a simulatedannealing technique specially designed for feature subset selection. Thealgorithm is applied to the maximization of gene subset relevance in severalpublic-domain microarray data sets. The experimental results show a notoriouslyhigh classification performance and low size subsets formed by biologicallymeaningful genes.
arxiv-2400-240 | Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks | http://arxiv.org/pdf/1302.1700v1.pdf | author:Alessandro Giusti, Dan C. Cireşan, Jonathan Masci, Luca M. Gambardella, Jürgen Schmidhuber category:cs.CV cs.AI published:2013-02-07 summary:Deep Neural Networks now excel at image classification, detection andsegmentation. When used to scan images by means of a sliding window, however,their high computational complexity can bring even the most powerful hardwareto its knees. We show how dynamic programming can speedup the process by ordersof magnitude, even when max-pooling layers are present.
arxiv-2400-241 | A Fast Learning Algorithm for Image Segmentation with Max-Pooling Convolutional Networks | http://arxiv.org/pdf/1302.1690v1.pdf | author:Jonathan Masci, Alessandro Giusti, Dan Cireşan, Gabriel Fricout, Jürgen Schmidhuber category:cs.CV published:2013-02-07 summary:We present a fast algorithm for training MaxPooling Convolutional Networks tosegment images. This type of network yields record-breaking performance in avariety of tasks, but is normally trained on a computationally expensivepatch-by-patch basis. Our new method processes each training image in a singlepass, which is vastly more efficient. We validate the approach in different scenarios and report a 1500-foldspeed-up. In an application to automated steel defect detection andsegmentation, we obtain excellent performance with short training times.
arxiv-2400-242 | Eye-GUIDE (Eye-Gaze User Interface Design) Messaging for Physically-Impaired People | http://arxiv.org/pdf/1302.1649v1.pdf | author:Rommel Anacan, James Greggory Alcayde, Retchel Antegra, Leah Luna category:cs.HC cs.CV published:2013-02-07 summary:Eye-GUIDE is an assistive communication tool designed for the paralyzed orphysically impaired people who were unable to move parts of their bodiesespecially people whose communications are limited only to eye movements. Theprototype consists of a camera and a computer. Camera captures images then itwill be send to the computer, where the computer will be the one to interpretthe data. Thus, Eye-GUIDE focuses on camera-based gaze tracking. The proponentdesigned the prototype to perform simple tasks and provides graphical userinterface in order the paralyzed or physically impaired person can easily useit.
arxiv-2400-243 | Arabic text summarization based on latent semantic analysis to enhance arabic documents clustering | http://arxiv.org/pdf/1302.1612v1.pdf | author:Hanane Froud, Abdelmonaime Lachkar, Said Alaoui Ouatik category:cs.IR cs.CL published:2013-02-06 summary:Arabic Documents Clustering is an important task for obtaining good resultswith the traditional Information Retrieval (IR) systems especially with therapid growth of the number of online documents present in Arabic language.Documents clustering aim to automatically group similar documents in onecluster using different similarity/distance measures. This task is oftenaffected by the documents length, useful information on the documents is oftenaccompanied by a large amount of noise, and therefore it is necessary toeliminate this noise while keeping useful information to boost the performanceof Documents clustering. In this paper, we propose to evaluate the impact oftext summarization using the Latent Semantic Analysis Model on Arabic DocumentsClustering in order to solve problems cited above, using fivesimilarity/distance measures: Euclidean Distance, Cosine Similarity, JaccardCoefficient, Pearson Correlation Coefficient and Averaged Kullback-LeiblerDivergence, for two times: without and with stemming. Our experimental resultsindicate that our proposed approach effectively solves the problems of noisyinformation and documents length, and thus significantly improve the clusteringperformance.
arxiv-2400-244 | A Generalized Fellegi-Sunter Framework for Multiple Record Linkage With Application to Homicide Record Systems | http://arxiv.org/pdf/1205.3217v2.pdf | author:Mauricio Sadinle, Stephen E. Fienberg category:stat.AP stat.ME stat.ML stat.OT published:2012-05-14 summary:We present a probabilistic method for linking multiple datafiles. This taskis not trivial in the absence of unique identifiers for the individualsrecorded. This is a common scenario when linking census data to coveragemeasurement surveys for census coverage evaluation, and in general whenmultiple record-systems need to be integrated for posterior analysis. Ourmethod generalizes the Fellegi-Sunter theory for linking records from twodatafiles and its modern implementations. The multiple record linkage goal isto classify the record K-tuples coming from K datafiles according to thedifferent matching patterns. Our method incorporates the transitivity ofagreement in the computation of the data used to model matching probabilities.We use a mixture model to fit matching probabilities via maximum likelihoodusing the EM algorithm. We present a method to decide the record K-tuplesmembership to the subsets of matching patterns and we prove its optimality. Weapply our method to the integration of three Colombian homicide record systemsand we perform a simulation study in order to explore the performance of themethod under measurement error and different scenarios. The proposed methodworks well and opens some directions for future research.
arxiv-2400-245 | Lexical Access for Speech Understanding using Minimum Message Length Encoding | http://arxiv.org/pdf/1302.1572v1.pdf | author:Ian Thomas, Ingrid Zukerman, Jonathan Oliver, David Albrecht, Bhavani Raskutti category:cs.CL published:2013-02-06 summary:The Lexical Access Problem consists of determining the intended sequence ofwords corresponding to an input sequence of phonemes (basic speech sounds) thatcome from a low-level phoneme recognizer. In this paper we present aninformation-theoretic approach based on the Minimum Message Length Criterionfor solving the Lexical Access Problem. We model sentences using phonemerealizations seen in training, and word and part-of-speech information obtainedfrom text corpora. We show results on multiple-speaker, continuous, read speechand discuss a heuristic using equivalence classes of similar sounding wordswhich speeds up the recognition process without significant deterioration inrecognition accuracy.
arxiv-2400-246 | Learning Bayesian Networks from Incomplete Databases | http://arxiv.org/pdf/1302.1565v1.pdf | author:Marco Ramoni, Paola Sebastiani category:cs.AI cs.LG published:2013-02-06 summary:Bayesian approaches to learn the graphical structure of Bayesian BeliefNetworks (BBNs) from databases share the assumption that the database iscomplete, that is, no entry is reported as unknown. Attempts to relax thisassumption involve the use of expensive iterative methods to discriminate amongdifferent structures. This paper introduces a deterministic method to learn thegraphical structure of a BBN from a possibly incomplete database. Experimentalevaluations show a significant robustness of this method and a remarkableindependence of its execution time from the number of missing data.
arxiv-2400-247 | An Information-Theoretic Analysis of Hard and Soft Assignment Methods for Clustering | http://arxiv.org/pdf/1302.1552v1.pdf | author:Michael Kearns, Yishay Mansour, Andrew Y. Ng category:cs.LG stat.ML published:2013-02-06 summary:Assignment methods are at the heart of many algorithms for unsupervisedlearning and clustering - in particular, the well-known K-means andExpectation-Maximization (EM) algorithms. In this work, we study severaldifferent methods of assignment, including the "hard" assignments used byK-means and the ?soft' assignments used by EM. While it is known that K-meansminimizes the distortion on the data and EM maximizes the likelihood, little isknown about the systematic differences of behavior between the two algorithms.Here we shed light on these differences via an information-theoretic analysis.The cornerstone of our results is a simple decomposition of the expecteddistortion, showing that K-means (and its extension for inferring generalparametric densities from unlabeled sample data) must implicitly manage atrade-off between how similar the data assigned to each cluster are, and howthe data are balanced among the clusters. How well the data are balanced ismeasured by the entropy of the partition defined by the hard assignments. Inaddition to letting us predict and verify systematic differences betweenK-means and EM on specific examples, the decomposition allows us to give arather general argument showing that K ?means will consistently find densitieswith less "overlap" than EM. We also study a third natural assignment methodthat we call posterior assignment, that is close in spirit to the softassignments of EM, but leads to a surprisingly different algorithm.
arxiv-2400-248 | Learning Belief Networks in Domains with Recursively Embedded Pseudo Independent Submodels | http://arxiv.org/pdf/1302.1549v1.pdf | author:Jun Hu, Yang Xiang category:cs.AI cs.LG published:2013-02-06 summary:A pseudo independent (PI) model is a probabilistic domain model (PDM) whereproper subsets of a set of collectively dependent variables display marginalindependence. PI models cannot be learned correctly by many algorithms thatrely on a single link search. Earlier work on learning PI models has suggesteda straightforward multi-link search algorithm. However, when a domain containsrecursively embedded PI submodels, it may escape the detection of such analgorithm. In this paper, we propose an improved algorithm that ensures thelearning of all embedded PI submodels whose sizes are upper bounded by apredetermined parameter. We show that this improved learning capability onlyincreases the complexity slightly beyond that of the previous algorithm. Theperformance of the new algorithm is demonstrated through experiment.
arxiv-2400-249 | Models and Selection Criteria for Regression and Classification | http://arxiv.org/pdf/1302.1545v1.pdf | author:David Heckerman, Christopher Meek category:cs.LG stat.ML published:2013-02-06 summary:When performing regression or classification, we are interested in theconditional probability distribution for an outcome or class variable Y given aset of explanatoryor input variables X. We consider Bayesian models for thistask. In particular, we examine a special class of models, which we callBayesian regression/classification (BRC) models, that can be factored intoindependent conditional (yx) and input (x) models. These models areconvenient, because the conditional model (the portion of the full model thatwe care about) can be analyzed by itself. We examine the practice oftransforming arbitrary Bayesian models to BRC models, and argue that thispractice is often inappropriate because it ignores prior knowledge that may beimportant for learning. In addition, we examine Bayesian methods for learningmodels from data. We discuss two criteria for Bayesian model selection that areappropriate for repression/classification: one described by Spiegelhalter etal. (1993), and another by Buntine (1993). We contrast these two criteria usingthe prequential framework of Dawid (1984), and give sufficient conditions underwhich the criteria agree.
arxiv-2400-250 | Learning Bayesian Nets that Perform Well | http://arxiv.org/pdf/1302.1542v1.pdf | author:Russell Greiner, Adam J. Grove, Dale Schuurmans category:cs.AI cs.LG published:2013-02-06 summary:A Bayesian net (BN) is more than a succinct way to encode a probabilisticdistribution; it also corresponds to a function used to answer queries. A BNcan therefore be evaluated by the accuracy of the answers it returns. Manyalgorithms for learning BNs, however, attempt to optimize another criterion(usually likelihood, possibly augmented with a regularizing term), which isindependent of the distribution of queries that are posed. This paper takes the"performance criteria" seriously, and considers the challenge of computing theBN whose performance - read "accuracy over the distribution of queries" - isoptimal. We show that many aspects of this learning task are more difficultthan the corresponding subtasks in the standard model.
arxiv-2400-251 | Parallel D2-Clustering: Large-Scale Clustering of Discrete Distributions | http://arxiv.org/pdf/1302.0435v2.pdf | author:Yu Zhang, James Z. Wang, Jia Li category:cs.LG cs.CV I.5.3; D.1.3 published:2013-02-02 summary:The discrete distribution clustering algorithm, namely D2-clustering, hasdemonstrated its usefulness in image classification and annotation where eachobject is represented by a bag of weighed vectors. The high computationalcomplexity of the algorithm, however, limits its applications to large-scaleproblems. We present a parallel D2-clustering algorithm with substantiallyimproved scalability. A hierarchical structure for parallel computing isdevised to achieve a balance between the individual-node computation and theintegration process of the algorithm. Additionally, it is shown that even witha single CPU, the hierarchical structure results in significant speed-up.Experiments on real-world large-scale image data, Youtube video data, andprotein sequence data demonstrate the efficiency and wide applicability of theparallel D2-clustering algorithm. The loss in clustering accuracy is minor incomparison with the original sequential algorithm.
arxiv-2400-252 | Image Segmentation in Video Sequences: A Probabilistic Approach | http://arxiv.org/pdf/1302.1539v1.pdf | author:Nir Friedman, Stuart Russell category:cs.CV cs.AI published:2013-02-06 summary:"Background subtraction" is an old technique for finding moving objects in avideo sequence for example, cars driving on a freeway. The idea is thatsubtracting the current image from a timeaveraged background image will leaveonly nonstationary objects. It is, however, a crude approximation to the taskof classifying each pixel of the current image; it fails with slow-movingobjects and does not distinguish shadows from moving objects. The basic idea ofthis paper is that we can classify each pixel using a model of how that pixellooks when it is part of different classes. We learn a mixture-of-Gaussiansclassification model for each pixel using an unsupervised technique- anefficient, incremental version of EM. Unlike the standard image-averagingapproach, this automatically updates the mixture component for each classaccording to likelihood of membership; hence slow-moving objects are handledperfectly. Our approach also identifies and eliminates shadows much moreeffectively than other techniques such as thresholding. Application of thismethod as part of the Roadwatch traffic surveillance project is expected toresult in significant improvements in vehicle identification and tracking.
arxiv-2400-253 | Sequential Update of Bayesian Network Structure | http://arxiv.org/pdf/1302.1538v1.pdf | author:Nir Friedman, Moises Goldszmidt category:cs.AI cs.LG published:2013-02-06 summary:There is an obvious need for improving the performance and accuracy of aBayesian network as new data is observed. Because of errors in modelconstruction and changes in the dynamics of the domains, we cannot afford toignore the information in new data. While sequential update of parameters for afixed structure can be accomplished using standard techniques, sequentialupdate of network structure is still an open problem. In this paper, weinvestigate sequential update of Bayesian networks were both parameters andstructure are expected to change. We introduce a new approach that allows forthe flexible manipulation of the tradeoff between the quality of the learnednetworks and the amount of information that is maintained about pastobservations. We formally describe our approach including the necessarymodifications to the scoring functions for learning Bayesian networks, evaluateits effectiveness through an empirical study, and extend it to the case ofmissing data.
arxiv-2400-254 | Exploring Parallelism in Learning Belief Networks | http://arxiv.org/pdf/1302.1529v1.pdf | author:TongSheng Chu, Yang Xiang category:cs.AI cs.LG published:2013-02-06 summary:It has been shown that a class of probabilistic domain models cannot belearned correctly by several existing algorithms which employ a single-linklook ahead search. When a multi-link look ahead search is used, thecomputational complexity of the learning algorithm increases. We study how touse parallelism to tackle the increased complexity in learning such models andto speed up learning in large domains. An algorithm is proposed to decomposethe learning task for parallel processing. A further task decomposition is usedto balance load among processors and to increase the speed-up and efficiency.For learning from very large datasets, we present a regrouping of the availableprocessors such that slow data access through file can be replaced by fastmemory access. Our implementation in a parallel computer demonstrates theeffectiveness of the algorithm.
arxiv-2400-255 | Update Rules for Parameter Estimation in Bayesian Networks | http://arxiv.org/pdf/1302.1519v1.pdf | author:Eric Bauer, Daphne Koller, Yoram Singer category:cs.LG stat.ML published:2013-02-06 summary:This paper re-examines the problem of parameter estimation in Bayesiannetworks with missing values and hidden variables from the perspective ofrecent work in on-line learning [Kivinen & Warmuth, 1994]. We provide a unifiedframework for parameter estimation that encompasses both on-line learning,where the model is continuously adapted to new data cases as they arrive, andthe more traditional batch learning, where a pre-accumulated set of samples isused in a one-time model selection process. In the batch case, our frameworkencompasses both the gradient projection algorithm and the EM algorithm forBayesian networks. The framework also leads to new on-line and batch parameterupdate schemes, including a parameterized version of EM. We provide bothempirical and theoretical results indicating that parameterized EM allowsfaster convergence to the maximum likelihood parameters than does standard EM.
arxiv-2400-256 | Towards the Rapid Development of a Natural Language Understanding Module | http://arxiv.org/pdf/1302.1380v1.pdf | author:Catarina Moreira, Ana Cristina Mendes, Luísa Coheur, Bruno Martins category:cs.CL published:2013-02-06 summary:When developing a conversational agent, there is often an urgent need to havea prototype available in order to test the application with real users. AWizard of Oz is a possibility, but sometimes the agent should be simplydeployed in the environment where it will be used. Here, the agent should beable to capture as many interactions as possible and to understand how peoplereact to failure. In this paper, we focus on the rapid development of a naturallanguage understanding module by non experts. Our approach follows the learningparadigm and sees the process of understanding natural language as aclassification problem. We test our module with a conversational agent thatanswers questions in the art domain. Moreover, we show how our approach can beused by a natural language interface to a cinema database.
arxiv-2400-257 | Cloud Computing framework for Computer Vision Research:An Introduction | http://arxiv.org/pdf/1302.1326v1.pdf | author:Yu Zhou category:cs.CV cs.DC published:2013-02-06 summary:Cloud computing offers the potential to help scientists to process massivenumber of computing resources often required in machine learning applicationsuch as computer vision problems. This proposal would like to show that whichbenefits can be obtained from cloud in order to help medical image analysisusers (including scientists, clinicians, and research institutes). As securityand privacy of algorithms are important for most of algorithms inventors, thesealgorithms can be hidden in a cloud to allow the users to use the algorithms asa package without any access to see/change their inside. In another word, inthe user part, users send their images to the cloud and configure the algorithmvia an interface. In the cloud part, the algorithms are applied to this imageand the results are returned back to the user. My proposal has two parts: (1)investigate the potential of cloud computing for computer vision problems and(2) study the components of a proposed cloud-based framework for medical imageanalysis application and develop them (depending on the length of theinternship). The investigation part will involve a study on several aspects ofthe problem including security, usability (for medical end users of theservice), appropriate programming abstractions for vision problems, scalabilityand resource requirements. In the second part of this proposal I am going tothoroughly study of the proposed framework components and their relations anddevelop them. The proposed cloud-based framework includes an integratedenvironment to enable scientists and clinicians to access to the previous andcurrent medical image analysis algorithms using a handful user interfacewithout any access to the algorithm codes and procedures.
arxiv-2400-258 | Kriging Interpolation Filter to Reduce High Density Salt and Pepper Noise | http://arxiv.org/pdf/1302.1300v1.pdf | author:Firas Ajil Jassim category:cs.CV published:2013-02-06 summary:Image denoising is a critical issue in the field of digital image processing.This paper proposes a novel Salt & Pepper noise suppression by developing aKriging Interpolation Filter (KIF) for image denoising. Gray-level imagesdegraded with Salt & Pepper noise have been considered. A sequential search fornoise detection was made using kXk window size to determine non-noisy pixelsonly. The non-noisy pixels are passed into Kriging interpolation method topredict their absent neighbor pixels that were noisy pixels at the first phase.The utilization of Kriging interpolation filter proves that it is veryimpressive to suppress high noise density. It has been found that KrigingInterpolation filter achieves noise reduction without loss of edges anddetailed information. Comparisons with existing algorithms are done usingquality metrics like PSNR and MSE to assess the proposed filter.
arxiv-2400-259 | Sparse group lasso and high dimensional multinomial classification | http://arxiv.org/pdf/1205.1245v2.pdf | author:Martin Vincent, Niels Richard Hansen category:stat.ML cs.LG stat.CO published:2012-05-06 summary:The sparse group lasso optimization problem is solved using a coordinategradient descent algorithm. The algorithm is applicable to a broad class ofconvex loss functions. Convergence of the algorithm is established, and thealgorithm is used to investigate the performance of the multinomial sparsegroup lasso classifier. On three different real data examples the multinomialgroup lasso clearly outperforms multinomial lasso in terms of achievedclassification error rate and in terms of including fewer features for theclassification. The run-time of our sparse group lasso implementation is of thesame order of magnitude as the multinomial lasso algorithm implemented in the Rpackage glmnet. Our implementation scales well with the problem size. One ofthe high dimensional examples considered is a 50 class classification problemwith 10k features, which amounts to estimating 500k parameters. Theimplementation is available as the R package msgl.
arxiv-2400-260 | Hybrid Image Segmentation using Discerner Cluster in FCM and Histogram Thresholding | http://arxiv.org/pdf/1302.1296v1.pdf | author:Firas Ajil Jassim category:cs.CV published:2013-02-06 summary:Image thresholding has played an important role in image segmentation. Thispaper presents a hybrid approach for image segmentation based on thethresholding by fuzzy c-means (THFCM) algorithm for image segmentation. Thegoal of the proposed approach is to find a discerner cluster able to find anautomatic threshold. The algorithm is formulated by applying the standard FCMclustering algorithm to the frequencies (y-values) on the smoothed histogram.Hence, the frequencies of an image can be used instead of the conventionalwhole data of image. The cluster that has the highest peak which represents themaximum frequency in the image histogram will play as an excellent role indetermining a discerner cluster to the grey level image. Then, the pixelsbelong to the discerner cluster represent an object in the gray level histogramwhile the other clusters represent a background. Experimental results withstandard test images have been obtained through the proposed approach (THFCM).
arxiv-2400-261 | Image Interpolation Using Kriging Technique for Spatial Data | http://arxiv.org/pdf/1302.1294v1.pdf | author:Firas Ajil Jassim, Fawzi Hasan Altaany category:cs.CV published:2013-02-06 summary:Image interpolation has been used spaciously by customary interpolationtechniques. Recently, Kriging technique has been widely implemented insimulation area and geostatistics for prediction. In this article, Krigingtechnique was used instead of the classical interpolation methods to predictthe unknown points in the digital image array. The efficiency of the proposedtechnique was proven using the PSNR and compared with the traditionalinterpolation techniques. The results showed that Kriging technique is almostaccurate as cubic interpolation and in some images Kriging has higher accuracy.A miscellaneous test images have been used to consolidate the proposedtechnique.
arxiv-2400-262 | When are the most informative components for inference also the principal components? | http://arxiv.org/pdf/1302.1232v1.pdf | author:Raj Rao Nadakuditi category:math.ST cs.DS cs.IT cs.LG math.IT math.PR stat.TH published:2013-02-05 summary:Which components of the singular value decomposition of a signal-plus-noisedata matrix are most informative for the inferential task of detecting orestimating an embedded low-rank signal matrix? Principal component analysisascribes greater importance to the components that capture the greatestvariation, i.e., the singular vectors associated with the largest singularvalues. This choice is often justified by invoking the Eckart-Young theoremeven though that work addresses the problem of how to best represent asignal-plus-noise matrix using a low-rank approximation and not how tobest_infer_ the underlying low-rank signal component. Here we take a first-principles approach in which we start with asignal-plus-noise data matrix and show how the spectrum of the noise-onlycomponent governs whether the principal or the middle components of thesingular value decomposition of the data matrix will be the informativecomponents for inference. Simply put, if the noise spectrum is supported on aconnected interval, in a sense we make precise, then the use of the principalcomponents is justified. When the noise spectrum is supported on multipleintervals, then the middle components might be more informative than theprincipal components. The end result is a proper justification of the use of principal componentsin the setting where the noise matrix is i.i.d. Gaussian and the identificationof scenarios, generically involving heterogeneous noise models such as mixturesof Gaussians, where the middle components might be more informative than theprincipal components so that they may be exploited to extract additionalprocessing gain. Our results show how the blind use of principal components canlead to suboptimal or even faulty inference because of phase transitions thatseparate a regime where the principal components are informative from a regimewhere they are uninformative.
arxiv-2400-263 | Evolvability Is Inevitable: Increasing Evolvability Without the Pressure to Adapt | http://arxiv.org/pdf/1302.1143v1.pdf | author:Joel Lehman, Kenneth O. Stanley category:cs.NE q-bio.PE published:2013-02-05 summary:Why evolvability appears to have increased over evolutionary time is animportant unresolved biological question. Unlike most candidate explanations,this paper proposes that increasing evolvability can result without anypressure to adapt. The insight is that if evolvability is heritable, then anunbiased drifting process across genotypes can still create a distribution ofphenotypes biased towards evolvability, because evolvable organisms diffusemore quickly through the space of possible phenotypes. Furthermore, becausephenotypic divergence often correlates with founding niches, niche founders mayon average be more evolvable, which through population growth provides agenotypic bias towards evolvability. Interestingly, the combination of thesetwo mechanisms can lead to increasing evolvability without any pressure toout-compete other organisms, as demonstrated through experiments with a seriesof simulated models. Thus rather than from pressure to adapt, evolvability mayinevitably result from any drift through genotypic space combined withevolution's passive tendency to accumulate niches.
arxiv-2400-264 | Large Scale Distributed Acoustic Modeling With Back-off N-grams | http://arxiv.org/pdf/1302.1123v1.pdf | author:Ciprian Chelba, Peng Xu, Fernando Pereira, Thomas Richardson category:cs.CL 68T10 I.2.7 published:2013-02-05 summary:The paper revives an older approach to acoustic modeling that borrows fromn-gram language modeling in an attempt to scale up both the amount of trainingdata and model size (as measured by the number of parameters in the model), toapproximately 100 times larger than current sizes used in automatic speechrecognition. In such a data-rich setting, we can expand the phonetic contextsignificantly beyond triphones, as well as increase the number of Gaussianmixture components for the context-dependent states that allow it. We haveexperimented with contexts that span seven or more context-independent phones,and up to 620 mixture components per state. Dealing with unseen phoneticcontexts is accomplished using the familiar back-off technique used in languagemodeling due to implementation simplicity. The back-off acoustic model isestimated, stored and served using MapReduce distributed computinginfrastructure. Speech recognition experiments are carried out in an N-best list rescoringframework for Google Voice Search. Training big models on large amounts of dataproves to be an effective way to increase the accuracy of a state-of-the-artautomatic speech recognition system. We use 87,000 hours of training data(speech along with transcription) obtained by filtering utterances in VoiceSearch logs on automatic speech recognition confidence. Models ranging in sizebetween 20--40 million Gaussians are estimated using maximum likelihoodtraining. They achieve relative reductions in word-error-rate of 11% and 6%when combined with first-pass models trained using maximum likelihood, andboosted maximum mutual information, respectively. Increasing the context sizebeyond five phones (quinphones) does not help.
arxiv-2400-265 | Two Algorithms for Finding $k$ Shortest Paths of a Weighted Pushdown Automaton | http://arxiv.org/pdf/1212.0927v3.pdf | author:Ke Wu, Philip Resnik category:cs.CL cs.DS cs.FL published:2012-12-05 summary:We introduce efficient algorithms for finding the $k$ shortest paths of aweighted pushdown automaton (WPDA), a compact representation of a weighted setof strings with potential applications in parsing and machine translation. Bothof our algorithms are derived from the same weighted deductive logicdescription of the execution of a WPDA using different search strategies.Experimental results show our Algorithm 2 adds very little overhead vs. thesingle shortest path algorithm, even with a large $k$.
arxiv-2400-266 | Distance Transform Gradient Density Estimation using the Stationary Phase Approximation | http://arxiv.org/pdf/1104.3621v4.pdf | author:Karthik S. Gurumoorthy, Anand Rangarajan category:stat.ML math.PR 42B10, 41A60 published:2011-04-19 summary:The complex wave representation (CWR) converts unsigned 2D distancetransforms into their corresponding wave functions. Here, the distancetransform S(X) appears as the phase of the wave function\phi(X)---specifically, \phi(X)=exp(iS(X)/\tau where \tau is a free parameter.In this work, we prove a novel result using the higher-order stationary phaseapproximation: we show convergence of the normalized power spectrum (squaredmagnitude of the Fourier transform) of the wave function to the densityfunction of the distance transform gradients as the free parameter \tau-->0. Incolloquial terms, spatial frequencies are gradient histogram bins. Since thedistance transform gradients have only orientation information (as theirmagnitudes are identically equal to one almost everywhere), as \tau-->0, the 2DFourier transform values mainly lie on the unit circle in the spatial frequencydomain. The proof of the result involves standard integration techniques andrequires proper ordering of limits. Our mathematical relation indicates thatthe CWR of distance transforms is an intriguing, new representation.
arxiv-2400-267 | Image Denoising Using Interquartile Range Filter with Local Averaging | http://arxiv.org/pdf/1302.1007v1.pdf | author:Firas Ajil Jassim category:cs.CV published:2013-02-05 summary:Image denoising is one of the fundamental problems in image processing. Inthis paper, a novel approach to suppress noise from the image is conducted byapplying the interquartile range (IQR) which is one of the statistical methodsused to detect outlier effect from a dataset. A window of size kXk wasimplemented to support IQR filter. Each pixel outside the IQR range of the kXkwindow is treated as noisy pixel. The estimation of the noisy pixels wasobtained by local averaging. The essential advantage of applying IQR filter isto preserve edge sharpness better of the original image. A variety of testimages have been used to support the proposed filter and PSNR was calculatedand compared with median filter. The experimental results on standard testimages demonstrate this filter is simpler and better performing than medianfilter.
arxiv-2400-268 | A Comparison of Relaxations of Multiset Cannonical Correlation Analysis and Applications | http://arxiv.org/pdf/1302.0974v1.pdf | author:Jan Rupnik, Primoz Skraba, John Shawe-Taylor, Sabrina Guettes category:cs.LG published:2013-02-05 summary:Canonical correlation analysis is a statistical technique that is used tofind relations between two sets of variables. An important extension in patternanalysis is to consider more than two sets of variables. This problem can beexpressed as a quadratically constrained quadratic program (QCQP), commonlyreferred to Multi-set Canonical Correlation Analysis (MCCA). This is anon-convex problem and so greedy algorithms converge to local optima withoutany guarantees on global optimality. In this paper, we show that despite beinghighly structured, finding the optimal solution is NP-Hard. This motivates ourrelaxation of the QCQP to a semidefinite program (SDP). The SDP is convex, canbe solved reasonably efficiently and comes with both absolute andoutput-sensitive approximation quality. In addition to theoretical guarantees,we do an extensive comparison of the QCQP method and the SDP relaxation on avariety of synthetic and real world data. Finally, we present two usefulextensions: we incorporate kernel methods and computing multiple sets ofcanonical vectors.
arxiv-2400-269 | RandomBoost: Simplified Multi-class Boosting through Randomization | http://arxiv.org/pdf/1302.0963v1.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Qinfeng Shi, Anton van den Hengel category:cs.LG published:2013-02-05 summary:We propose a novel boosting approach to multi-class classification problems,in which multiple classes are distinguished by a set of random projectionmatrices in essence. The approach uses random projections to alleviate theproliferation of binary classifiers typically required to perform multi-classclassification. The result is a multi-class classifier with a singlevector-valued parameter, irrespective of the number of classes involved. Twovariants of this approach are proposed. The first method randomly projects theoriginal data into new spaces, while the second method randomly projects theoutputs of learned weak classifiers. These methods are not only conceptuallysimple but also effective and easy to implement. A series of experiments onsynthetic, machine learning and visual recognition data sets demonstrate thatour proposed methods compare favorably to existing multi-class boostingalgorithms in terms of both the convergence rate and classification accuracy.
arxiv-2400-270 | Improved Accuracy of PSO and DE using Normalization: an Application to Stock Price Prediction | http://arxiv.org/pdf/1302.0962v1.pdf | author:Savinderjit Kaur, Veenu Mangat category:cs.NE cs.LG published:2013-02-05 summary:Data Mining is being actively applied to stock market since 1980s. It hasbeen used to predict stock prices, stock indexes, for portfolio management,trend detection and for developing recommender systems. The various algorithmswhich have been used for the same include ANN, SVM, ARIMA, GARCH etc. Differenthybrid models have been developed by combining these algorithms with otheralgorithms like roughest, fuzzy logic, GA, PSO, DE, ACO etc. to improve theefficiency. This paper proposes DE-SVM model (Differential EvolutionSupportvector Machine) for stock price prediction. DE has been used to select bestfree parameters combination for SVM to improve results. The paper also comparesthe results of prediction with the outputs of SVM alone and PSO-SVM model(Particle Swarm Optimization). The effect of normalization of data on theaccuracy of prediction has also been studied.
arxiv-2400-271 | Multi-Robot Informative Path Planning for Active Sensing of Environmental Phenomena: A Tale of Two Algorithms | http://arxiv.org/pdf/1302.0723v2.pdf | author:Nannan Cao, Kian Hsiang Low, John M. Dolan category:cs.LG cs.AI cs.MA cs.RO published:2013-02-04 summary:A key problem of robotic environmental sensing and monitoring is that ofactive sensing: How can a team of robots plan the most informative observationpaths to minimize the uncertainty in modeling and predicting an environmentalphenomenon? This paper presents two principled approaches to efficientinformation-theoretic path planning based on entropy and mutual informationcriteria for in situ active sensing of an important broad class ofwidely-occurring environmental phenomena called anisotropic fields. Ourproposed algorithms are novel in addressing a trade-off between active sensingperformance and time efficiency. An important practical consequence is that ouralgorithms can exploit the spatial correlation structure of Gaussianprocess-based anisotropic fields to improve time efficiency while preservingnear-optimal active sensing performance. We analyze the time complexity of ouralgorithms and prove analytically that they scale better than state-of-the-artalgorithms with increasing planning horizon length. We provide theoreticalguarantees on the active sensing performance of our algorithms for a class ofexploration tasks called transect sampling, which, in particular, can beimproved with longer planning time and/or lower spatial correlation along thetransect. Empirical evaluation on real-world anisotropic field data shows thatour algorithms can perform better or at least as well as the state-of-the-artalgorithms while often incurring a few orders of magnitude less computationaltime, even when the field conditions are less favorable.
arxiv-2400-272 | Sparse Subspace Clustering: Algorithm, Theory, and Applications | http://arxiv.org/pdf/1203.1005v3.pdf | author:Ehsan Elhamifar, Rene Vidal category:cs.CV cs.IR cs.IT cs.LG math.IT math.OC stat.ML published:2012-03-05 summary:In many real-world problems, we are dealing with collections ofhigh-dimensional data, such as images, videos, text and web documents, DNAmicroarray data, and more. Often, high-dimensional data lie close tolow-dimensional structures corresponding to several classes or categories thedata belongs to. In this paper, we propose and study an algorithm, calledSparse Subspace Clustering (SSC), to cluster data points that lie in a union oflow-dimensional subspaces. The key idea is that, among infinitely many possiblerepresentations of a data point in terms of other points, a sparserepresentation corresponds to selecting a few points from the same subspace.This motivates solving a sparse optimization program whose solution is used ina spectral clustering framework to infer the clustering of data into subspaces.Since solving the sparse optimization program is in general NP-hard, weconsider a convex relaxation and show that, under appropriate conditions on thearrangement of subspaces and the distribution of data, the proposedminimization program succeeds in recovering the desired sparse representations.The proposed algorithm can be solved efficiently and can handle data pointsnear the intersections of subspaces. Another key advantage of the proposedalgorithm with respect to the state of the art is that it can deal with datanuisances, such as noise, sparse outlying entries, and missing entries,directly by incorporating the model of the data into the sparse optimizationprogram. We demonstrate the effectiveness of the proposed algorithm throughexperiments on synthetic data as well as the two real-world problems of motionsegmentation and face clustering.
arxiv-2400-273 | Coded aperture compressive temporal imaging | http://arxiv.org/pdf/1302.2575v1.pdf | author:Patrick Llull, Xuejun Liao, Xin Yuan, Jianbo Yang, David Kittle, Lawrence Carin, Guillermo Sapiro, David J. Brady category:cs.CV cs.IT math.IT published:2013-02-04 summary:We use mechanical translation of a coded aperture for code division multipleaccess compression of video. We present experimental results for reconstructionat 148 frames per coded snapshot.
arxiv-2400-274 | Exact Sparse Recovery with L0 Projections | http://arxiv.org/pdf/1302.0895v1.pdf | author:Ping Li, Cun-Hui Zhang category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2013-02-04 summary:Many applications concern sparse signals, for example, detecting anomaliesfrom the differences between consecutive images taken by surveillance cameras.This paper focuses on the problem of recovering a K-sparse signal x in Ndimensions. In the mainstream framework of compressed sensing (CS), the vectorx is recovered from M non-adaptive linear measurements y = xS, where S (of sizeN x M) is typically a Gaussian (or Gaussian-like) design matrix, through someoptimization procedure such as linear programming (LP). In our proposed method, the design matrix S is generated from an$\alpha$-stable distribution with $\alpha\approx 0$. Our decoding algorithmmainly requires one linear scan of the coordinates, followed by a fewiterations on a small number of coordinates which are "undetermined" in theprevious iteration. Comparisons with two strong baselines, linear programming(LP) and orthogonal matching pursuit (OMP), demonstrate that our algorithm canbe significantly faster in decoding speed and more accurate in recoveryquality, for the task of exact spare recovery. Our procedure is robust againstmeasurement noise. Even when there are no sufficient measurements, ouralgorithm can still reliably recover a significant portion of the nonzerocoordinates. To provide the intuition for understanding our method, we also analyze theprocedure by assuming an idealistic setting. Interestingly, when K=2, the"idealized" algorithm achieves exact recovery with merely 3 measurements,regardless of N. For general K, the required sample size of the "idealized"algorithm is about 5K.
arxiv-2400-275 | Centrality-constrained graph embedding | http://arxiv.org/pdf/1302.0870v1.pdf | author:Brian Baingana, Georgios B. Giannakis category:stat.ML cs.CV math.OC published:2013-02-04 summary:Visual rendering of graphs is a key task in the mapping of complex networkdata. Although most graph drawing algorithms emphasize aesthetic appeal,certain applications such as travel-time maps place more importance onvisualization of structural network properties. The present paper advocates agraph embedding approach with centrality considerations to comply with nodehierarchy. The problem is formulated as one of constrained multi-dimensionalscaling (MDS), and it is solved via block coordinate descent iterations withsuccessive approximations and guaranteed convergence to a KKT point. Inaddition, a regularization term enforcing graph smoothness is incorporated withthe goal of reducing edge crossings. Experimental results demonstrate that thealgorithm converges, and can be used to efficiently embed large graphs on theorder of thousands of nodes.
arxiv-2400-276 | Comparison of Ant-Inspired Gatherer Allocation Approaches using Memristor-Based Environmental Models | http://arxiv.org/pdf/1302.0797v1.pdf | author:Ella Gale, Ben de Lacy Costello, Andrew Adamatzky category:cs.NE published:2013-02-04 summary:Memristors are used to compare three gathering techniques in analready-mapped environment where resource locations are known. The All Sitemodel, which apportions gatherers based on the modeled memristance of thatpath, proves to be good at increasing overall efficiency and decreasing time tofully deplete an environment, however it only works well when the resources areof similar quality. The Leaf Cutter method, based on Leaf Cutter Ant behaviour,assigns all gatherers first to the best resource, and once depleted, uses theAll Site model to spread them out amongst the rest. The Leaf Cutter model isbetter at increasing resource influx in the short-term and vastly out-performsthe All Site model in a more varied environments. It is demonstrated thatmemristor based abstractions of gatherer models provide potential methods forboth the comparison and implementation of agent controls.
arxiv-2400-277 | Beyond Markov Chains, Towards Adaptive Memristor Network-based Music Generation | http://arxiv.org/pdf/1302.0785v1.pdf | author:Ella Gale, Oliver Matthews, Ben de Lacy Costello, Andrew Adamatzky category:cs.ET cs.AI cs.NE cs.SD 68Txx published:2013-02-04 summary:We undertook a study of the use of a memristor network for music generation,making use of the memristor's memory to go beyond the Markov hypothesis. Seedtransition matrices are created and populated using memristor equations, andwhich are shown to generate musical melodies and change in style over time as aresult of feedback into the transition matrix. The spiking properties of simplememristor networks are demonstrated and discussed with reference toapplications of music making. The limitations of simulating composing memristornetworks in von Neumann hardware is discussed and a hardware solution based onphysical memristor properties is presented.
arxiv-2400-278 | Identifying Metaphoric Antonyms in a Corpus Analysis of Finance Articles | http://arxiv.org/pdf/1212.3139v2.pdf | author:Aaron Gerow, Mark Keane category:cs.CL published:2012-12-13 summary:Using a corpus of 17,000+ financial news reports (involving over 10M words),we perform an analysis of the argument-distributions of the UP and DOWN verbsused to describe movements of indices, stocks and shares. In Study 1participants identified antonyms of these verbs in a free-response task and amatching task from which the most commonly identified antonyms were compiled.In Study 2, we determined whether the argument-distributions for the verbs inthese antonym-pairs were sufficiently similar to predict the mostfrequently-identified antonym. Cosine similarity correlates moderately with theproportions of antonym-pairs identified by people (r = 0.31). Moreimpressively, 87% of the time the most frequently-identified antonym is eitherthe first- or second-most similar pair in the set of alternatives. Theimplications of these results for distributional approaches to determiningmetaphoric knowledge are discussed.
arxiv-2400-279 | Multi-scale Visual Attention & Saliency Modelling with Decision Theory | http://arxiv.org/pdf/1302.0689v1.pdf | author:Anh Cat Le Ngo, Li-Minn Ang, Guoping Qiu, Kah-Phooi Seng category:cs.CV published:2013-02-04 summary:Bottom-up saliency, an early human visual processing, behaves like binaryclassification of interest and null hypothesis. Its discriminant power, mutualinformation of image features and class distribution, is closely related tosaliency value by the well-known centre-surround theory. As classificationaccuracy very much depends on window sizes, the discriminant saliency (power)varies according to sampling scales. Discriminating power estimation inmulti-scales framework needs integrating with wavelet transformation and thenestimating statistical discrepancy of two consecutive scales (centre-surroundwindows) by Hidden Markov Tree (HMT) model. Finally, multi-scale discriminantsaliency (MDIS) maps are combined by the maximum information rule to synthesizea final saliency map. All MDIS maps are evaluated with standard quantitativetools (NSS,LCC,AUC) on N.Bruce's database with ground truth data aseye-tracking locations ; as well assessed qualitatively by visual examinationof individual cases. For evaluating MDIS against well-known AIM saliencymethod, simulations are needed and described in details with severalinteresting conclusions, drawn for further research directions.
arxiv-2400-280 | A game-theoretic framework for classifier ensembles using weighted majority voting with local accuracy estimates | http://arxiv.org/pdf/1302.0540v1.pdf | author:Harris V. Georgiou, Michael E. Mavroforakis category:cs.LG published:2013-02-03 summary:In this paper, a novel approach for the optimal combination of binaryclassifiers is proposed. The classifier combination problem is approached froma Game Theory perspective. The proposed framework of adapted weighted majorityrules (WMR) is tested against common rank-based, Bayesian and simple majoritymodels, as well as two soft-output averaging rules. Experiments with ensemblesof Support Vector Machines (SVM), Ordinary Binary Tree Classifiers (OBTC) andweighted k-nearest-neighbor (w/k-NN) models on benchmark datasets indicate thatthis new adaptive WMR model, employing local accuracy estimators and theanalytically computed optimal weights outperform all the other simplecombination rules.
arxiv-2400-281 | An application of the stationary phase method for estimating probability densities of function derivatives | http://arxiv.org/pdf/1108.1783v4.pdf | author:Karthik S. Gurumoorthy, Anand Rangarajan, Arunava Banerjee category:stat.ML math.ST stat.TH published:2011-08-08 summary:We prove a novel result wherein the density function of thegradients---corresponding to density function of the derivatives in onedimension---of a thrice differentiable function S (obtained via a randomvariable transformation of a uniformly distributed random variable) defined ona closed, bounded interval \Omega \subset R is accurately approximated by thenormalized power spectrum of \phi=exp(iS/\tau) as the free parameter \tau-->0.The result is shown using the well known stationary phase approximation andstandard integration techniques and requires proper ordering of limits.Experimental results provide anecdotal visual evidence corroborating theresult.
arxiv-2400-282 | Sparse Camera Network for Visual Surveillance -- A Comprehensive Survey | http://arxiv.org/pdf/1302.0446v1.pdf | author:Mingli Song, Dachent Tao, Stephen J. Maybank category:cs.CV published:2013-02-03 summary:Technological advances in sensor manufacture, communication, and computingare stimulating the development of new applications that are transformingtraditional vision systems into pervasive intelligent camera networks. Theanalysis of visual cues in multi-camera networks enables a wide range ofapplications, from smart home and office automation to large area surveillanceand traffic surveillance. While dense camera networks - in which most camerashave large overlapping fields of view - are well studied, we are mainlyconcerned with sparse camera networks. A sparse camera network undertakes largearea surveillance using as few cameras as possible, and most cameras havenon-overlapping fields of view with one another. The task is challenging due tothe lack of knowledge about the topological structure of the network,variations in the appearance and motion of specific tracking targets indifferent views, and the difficulties of understanding composite events in thenetwork. In this review paper, we present a comprehensive survey of recentresearch results to address the problems of intra-camera tracking, topologicalstructure learning, target appearance modeling, and global activityunderstanding in sparse camera networks. A number of current open researchissues are discussed.
arxiv-2400-283 | Factoring nonnegative matrices with linear programs | http://arxiv.org/pdf/1206.1270v2.pdf | author:Victor Bittorf, Benjamin Recht, Christopher Re, Joel A. Tropp category:math.OC cs.LG stat.ML published:2012-06-06 summary:This paper describes a new approach, based on linear programming, forcomputing nonnegative matrix factorizations (NMFs). The key idea is adata-driven model for the factorization where the most salient features in thedata are used to express the remaining features. More precisely, given a datamatrix X, the algorithm identifies a matrix C such that X approximately equalsCX and some linear constraints. The constraints are chosen to ensure that thematrix C selects features; these features can then be used to find a low-rankNMF of X. A theoretical analysis demonstrates that this approach has guaranteessimilar to those of the recent NMF algorithm of Arora et al. (2012). Incontrast with this earlier work, the proposed method extends to more generalnoise models and leads to efficient, scalable algorithms. Experiments withsynthetic and real datasets provide evidence that the new approach is alsosuperior in practice. An optimized C++ implementation can factor amultigigabyte matrix in a matter of minutes.
arxiv-2400-284 | Generalization Guarantees for a Binary Classification Framework for Two-Stage Multiple Kernel Learning | http://arxiv.org/pdf/1302.0406v1.pdf | author:Purushottam Kar category:cs.LG stat.ML published:2013-02-02 summary:We present generalization bounds for the TS-MKL framework for two stagemultiple kernel learning. We also present bounds for sparse kernel learningformulations within the TS-MKL framework.
arxiv-2400-285 | Lambek vs. Lambek: Functorial Vector Space Semantics and String Diagrams for Lambek Calculus | http://arxiv.org/pdf/1302.0393v1.pdf | author:Bob Coecke, Edward Grefenstette, Mehrnoosh Sadrzadeh category:math.LO cs.CL math.CT published:2013-02-02 summary:The Distributional Compositional Categorical (DisCoCat) model is amathematical framework that provides compositional semantics for meanings ofnatural language sentences. It consists of a computational procedure forconstructing meanings of sentences, given their grammatical structure in termsof compositional type-logic, and given the empirically derived meanings oftheir words. For the particular case that the meaning of words is modelledwithin a distributional vector space model, its experimental predictions,derived from real large scale data, have outperformed other empiricallyvalidated methods that could build vectors for a full sentence. This successcan be attributed to a conceptually motivated mathematical underpinning, byintegrating qualitative compositional type-logic and quantitative modelling ofmeaning within a category-theoretic mathematical framework. The type-logic used in the DisCoCat model is Lambek's pregroup grammar.Pregroup types form a posetal compact closed category, which can be passed, ina functorial manner, on to the compact closed structure of vector spaces,linear maps and tensor product. The diagrammatic versions of the equationalreasoning in compact closed categories can be interpreted as the flow of wordmeanings within sentences. Pregroups simplify Lambek's previous type-logic, theLambek calculus, which has been extensively used to formalise and reason aboutvarious linguistic phenomena. The apparent reliance of the DisCoCat onpregroups has been seen as a shortcoming. This paper addresses this concern, bypointing out that one may as well realise a functorial passage from theoriginal type-logic of Lambek, a monoidal bi-closed category, to vector spaces,or to any other model of meaning organised within a monoidal bi-closedcategory. The corresponding string diagram calculus, due to Baez and Stay, nowdepicts the flow of word meanings.
arxiv-2400-286 | Fast Damage Recovery in Robotics with the T-Resilience Algorithm | http://arxiv.org/pdf/1302.0386v1.pdf | author:Sylvain Koos, Antoine Cully, Jean-Baptiste Mouret category:cs.RO cs.AI cs.LG published:2013-02-02 summary:Damage recovery is critical for autonomous robots that need to operate for along time without assistance. Most current methods are complex and costlybecause they require anticipating each potential damage in order to have acontingency plan ready. As an alternative, we introduce the T-resiliencealgorithm, a new algorithm that allows robots to quickly and autonomouslydiscover compensatory behaviors in unanticipated situations. This algorithmequips the robot with a self-model and discovers new behaviors by learning toavoid those that perform differently in the self-model and in reality. Ouralgorithm thus does not identify the damaged parts but it implicitly searchesfor efficient behaviors that do not use them. We evaluate the T-Resiliencealgorithm on a hexapod robot that needs to adapt to leg removal, broken legsand motor failures; we compare it to stochastic local search, policy gradientand the self-modeling algorithm proposed by Bongard et al. The behavior of therobot is assessed on-board thanks to a RGB-D sensor and a SLAM algorithm. Usingonly 25 tests on the robot and an overall running time of 20 minutes,T-Resilience consistently leads to substantially better results than the otherapproaches.
arxiv-2400-287 | Hessian Schatten-Norm Regularization for Linear Inverse Problems | http://arxiv.org/pdf/1209.3318v3.pdf | author:Stamatios Lefkimmiatis, John Paul Ward, Michael Unser category:math.OC cs.CV cs.NA published:2012-09-14 summary:We introduce a novel family of invariant, convex, and non-quadraticfunctionals that we employ to derive regularized solutions of ill-posed linearinverse imaging problems. The proposed regularizers involve the Schatten normsof the Hessian matrix, computed at every pixel of the image. They can be viewedas second-order extensions of the popular total-variation (TV) semi-norm sincethey satisfy the same invariance properties. Meanwhile, by taking advantage ofsecond-order derivatives, they avoid the staircase effect, a common artifact ofTV-based reconstructions, and perform well for a wide range of applications. Tosolve the corresponding optimization problems, we propose an algorithm that isbased on a primal-dual formulation. A fundamental ingredient of this algorithmis the projection of matrices onto Schatten norm balls of arbitrary radius.This operation is performed efficiently based on a direct link we providebetween vector projections onto $\ell_q$ norm balls and matrix projections ontoSchatten norm balls. Finally, we demonstrate the effectiveness of the proposedmethods through experimental results on several inverse imaging problems withreal and simulated data.
arxiv-2400-288 | An Algorithmic Solution to the Five-Point Pose Problem Based on the Cayley Representation of Rotations | http://arxiv.org/pdf/1105.3828v2.pdf | author:Evgeniy Martyushev category:cs.CV published:2011-05-19 summary:We give a new algorithmic solution to the well-known five-point relative poseproblem. Our approach does not deal with the famous cubic constraint on anessential matrix. Instead, we use the Cayley representation of rotations inorder to obtain a polynomial system from epipolar constraints. Solving thatsystem, we directly get relative rotation and translation parameters of thecameras in terms of roots of a 10th degree polynomial.
arxiv-2400-289 | A New Constructive Method to Optimize Neural Network Architecture and Generalization | http://arxiv.org/pdf/1302.0324v1.pdf | author:Hou Muzhou, Moon Ho Lee category:cs.NE 41A99, 65D15 published:2013-02-02 summary:In this paper, after analyzing the reasons of poor generalization andoverfitting in neural networks, we consider some noise data as a singular valueof a continuous function - jump discontinuity point. The continuous part can beapproximated with the simplest neural networks, which have good generalizationperformance and optimal network architecture, by traditional algorithms such asconstructive algorithm for feed-forward neural networks with incrementaltraining, BP algorithm, ELM algorithm, various constructive algorithm, RBFapproximation and SVM. At the same time, we will construct RBF neural networksto fit the singular value with every error in, and we prove that a functionwith jumping discontinuity points can be approximated by the simplest neuralnetworks with a decay RBF neural networks in by each error, and a function withjumping discontinuity point can be constructively approximated by a decay RBFneural networks in by each error and the constructive part have nogeneralization influence to the whole machine learning system which willoptimize neural network architecture and generalization performance, reduce theoverfitting phenomenon by avoid fitting the noisy data.
arxiv-2400-290 | Sparse Multiple Kernel Learning with Geometric Convergence Rate | http://arxiv.org/pdf/1302.0315v1.pdf | author:Rong Jin, Tianbao Yang, Mehrdad Mahdavi category:cs.LG stat.ML published:2013-02-01 summary:In this paper, we study the problem of sparse multiple kernel learning (MKL),where the goal is to efficiently learn a combination of a fixed small number ofkernels from a large pool that could lead to a kernel classifier with a smallprediction error. We develop an efficient algorithm based on the greedycoordinate descent algorithm, that is able to achieve a geometric convergencerate under appropriate conditions. The convergence rate is achieved bymeasuring the size of functional gradients by an empirical $\ell_2$ norm thatdepends on the empirical data distribution. This is in contrast to previousalgorithms that use a functional norm to measure the size of gradients, whichis independent from the data samples. We also establish a generalization errorbound of the learned sparse kernel classifier using the technique of localRademacher complexity.
arxiv-2400-291 | Regression shrinkage and grouping of highly correlated predictors with HORSES | http://arxiv.org/pdf/1302.0256v1.pdf | author:Woncheol Jang, Johan Lim, Nicole A. Lazar, Ji Meng Loh, Donghyeon Yu category:stat.ML 62J07, 62P10 published:2013-02-01 summary:Identifying homogeneous subgroups of variables can be challenging in highdimensional data analysis with highly correlated predictors. We propose a newmethod called Hexagonal Operator for Regression with Shrinkage and EqualitySelection, HORSES for short, that simultaneously selects positively correlatedvariables and identifies them as predictive clusters. This is achieved via aconstrained least-squares problem with regularization that consists of a linearcombination of an L_1 penalty for the coefficients and another L_1 penalty forpairwise differences of the coefficients. This specification of the penaltyfunction encourages grouping of positively correlated predictors combined witha sparsity solution. We construct an efficient algorithm to implement theHORSES procedure. We show via simulation that the proposed method outperformsother variable selection methods in terms of prediction error and parsimony.The technique is demonstrated on two data sets, a small data set from analysisof soil in Appalachia, and a high dimensional data set from a near infrared(NIR) spectroscopy study, showing the flexibility of the methodology.
arxiv-2400-292 | Pairwise MRF Calibration by Perturbation of the Bethe Reference Point | http://arxiv.org/pdf/1210.5338v2.pdf | author:Cyril Furtlehner, Yufei Han, Jean-Marc Lasgouttes, Victorin Martin category:cs.LG stat.ML published:2012-10-19 summary:We investigate different ways of generating approximate solutions to thepairwise Markov random field (MRF) selection problem. We focus mainly on theinverse Ising problem, but discuss also the somewhat related inverse Gaussianproblem because both types of MRF are suitable for inference tasks with thebelief propagation algorithm (BP) under certain conditions. Our approachconsists in to take a Bethe mean-field solution obtained with a maximumspanning tree (MST) of pairwise mutual information, referred to as the\emph{Bethe reference point}, for further perturbation procedures. We considerthree different ways following this idea: in the first one, we select andcalibrate iteratively the optimal links to be added starting from the Bethereference point; the second one is based on the observation that the naturalgradient can be computed analytically at the Bethe point; in the third one,assuming no local field and using low temperature expansion we develop a dualloop joint model based on a well chosen fundamental cycle basis. We indeedidentify a subclass of planar models, which we refer to as \emph{Bethe-dualgraph models}, having possibly many loops, but characterized by a singlyconnected dual factor graph, for which the partition function and the linearresponse can be computed exactly in respectively O(N) and $O(N^2)$ operations,thanks to a dual weight propagation (DWP) message passing procedure that we setup. When restricted to this subclass of models, the inverse Ising problem beingconvex, becomes tractable at any temperature. Experimental tests on variousdatasets with refined $L_0$ or $L_1$ regularization procedures indicate thatthese approaches may be competitive and useful alternatives to existing ones.
arxiv-2400-293 | Eléments pour une théorie des réseaux en phase d'apprentissage | http://arxiv.org/pdf/1301.2959v2.pdf | author:Jean Piniello category:nlin.AO cs.NE nlin.CD published:2013-01-14 summary:This study deals with the evolution of the so called intelligent networks(insect society without leader, cells of an organism, brain...) during theirapprenticeship period. The used formalism draws one's inspiration from the oneof the Quantum field theory (Principle of stationary action, gauge fields,invariance by symmetry transformations...). After a recall of some definitions,we consider at first the free network, that is to say which does not exchangeany information with outside. Then we study the evolution of the networkconnected with its environment, that is to say immersed into an informationfield created by this environment which so dictates to it the apprenticeshipconstraints. At that time, we obtain Lagrange equations which solutionsdescribe the network evolution during the whole apprenticeship period. Finally,while proceeding with the same formalism inspiration, we suggest other studyways capable of evolving the knowledge in the considered scope.
arxiv-2400-294 | Genetic Algorithm for Mulicriteria Optimization of a Multi-Pickup and Delivery Problem with Time Windows | http://arxiv.org/pdf/1010.0771v2.pdf | author:Imen Harbaoui Dridi, Ryan Kammarti, Mekki Ksouri, Pierre Borne category:cs.NE published:2010-10-05 summary:In This paper we present a genetic algorithm for mulicriteria optimization ofa multipickup and delivery problem with time windows (m-PDPTW). The m-PDPTW isan optimization vehicles routing problem which must meet requests for transportbetween suppliers and customers satisfying precedence, capacity and timeconstraints. This paper purposes a brief literature review of the PDPTW,present an approach based on genetic algorithms and Pareto dominance method togive a set of satisfying solutions to the m-PDPTW minimizing total travel cost,total tardiness time and the vehicles number.
arxiv-2400-295 | Distribution-Free Distribution Regression | http://arxiv.org/pdf/1302.0082v1.pdf | author:Barnabas Poczos, Alessandro Rinaldo, Aarti Singh, Larry Wasserman category:stat.ML cs.LG math.ST stat.TH published:2013-02-01 summary:`Distribution regression' refers to the situation where a response Y dependson a covariate P where P is a probability distribution. The model is Y=f(P) +mu where f is an unknown regression function and mu is a random error.Typically, we do not observe P directly, but rather, we observe a sample fromP. In this paper we develop theory and methods for distribution-free versionsof distribution regression. This means that we do not make distributionalassumptions about the error term mu and covariate P. We prove that when theeffective dimension is small enough (as measured by the doubling dimension),then the excess prediction risk converges to zero with a polynomial rate.
arxiv-2400-296 | Sparse MRI for motion correction | http://arxiv.org/pdf/1302.0077v1.pdf | author:Zai Yang, Cishen Zhang, Lihua Xie category:cs.CV physics.bio-ph physics.med-ph published:2013-02-01 summary:MR image sparsity/compressibility has been widely exploited for imagingacceleration with the development of compressed sensing. A sparsity-basedapproach to rigid-body motion correction is presented for the first time inthis paper. A motion is sought after such that the compensated MR image ismaximally sparse/compressible among the infinite candidates. Iterativealgorithms are proposed that jointly estimate the motion and the image content.The proposed method has a lot of merits, such as no need of additional data andloose requirement for the sampling sequence. Promising results are presented todemonstrate its performance.
arxiv-2400-297 | Equitability, mutual information, and the maximal information coefficient | http://arxiv.org/pdf/1301.7745v1.pdf | author:Justin B. Kinney, Gurinder S. Atwal category:q-bio.QM math.ST stat.ME stat.ML stat.TH published:2013-01-31 summary:Reshef et al. recently proposed a new statistical measure, the "maximalinformation coefficient" (MIC), for quantifying arbitrary dependencies betweenpairs of stochastic quantities. MIC is based on mutual information, afundamental quantity in information theory that is widely understood to servethis need. MIC, however, is not an estimate of mutual information. Indeed, itwas claimed that MIC possesses a desirable mathematical property called"equitability" that mutual information lacks. This was not proven; instead itwas argued solely through the analysis of simulated data. Here we show thatthis claim, in fact, is incorrect. First we offer mathematical proof that no(non-trivial) dependence measure satisfies the definition of equitabilityproposed by Reshef et al.. We then propose a self-consistent and more generaldefinition of equitability that follows naturally from the Data ProcessingInequality. Mutual information satisfies this new definition of equitabilitywhile MIC does not. Finally, we show that the simulation evidence offered byReshef et al. was artifactual. We conclude that estimating mutual informationis not only practical for many real-world applications, but also provides anatural solution to the problem of quantifying associations in large data sets.
arxiv-2400-298 | Fast non parametric entropy estimation for spatial-temporal saliency method | http://arxiv.org/pdf/1301.7661v1.pdf | author:Anh Cat Le Ngo, Guoping Qiu, Geoff Underwood, Kenneth Li-Minn Ang, Jasmine Kah-Phooi Seng category:cs.CV published:2013-01-31 summary:This paper formulates bottom-up visual saliency as center surroundconditional entropy and presents a fast and efficient technique for thecomputation of such a saliency map. It is shown that the new saliencyformulation is consistent with self-information based saliency,decision-theoretic saliency and Bayesian definition of surprises but also facesthe same significant computational challenge of estimating probability densityin very high dimensional spaces with limited samples. We have developed a fastand efficient nonparametric method to make the practical implementation ofthese types of saliency maps possible. By aligning pixels from the center andsurround regions and treating their location coordinates as random variables,we use a k-d partitioning method to efficiently estimating the center surroundconditional entropy. We present experimental results on two publicly availableeye tracking still image databases and show that the new technique iscompetitive with state of the art bottom-up saliency computational methods. Wehave also extended the technique to compute spatiotemporal visual saliency ofvideo and evaluate the bottom-up spatiotemporal saliency against eye trackingdata on a video taken onboard a moving vehicle with the driver's eye beingtracked by a head mounted eye-tracker.
arxiv-2400-299 | Rank regularization and Bayesian inference for tensor completion and extrapolation | http://arxiv.org/pdf/1301.7619v1.pdf | author:Juan Andres Bazerque, Gonzalo Mateos, Georgios B. Giannakis category:cs.IT cs.LG math.IT stat.ML published:2013-01-31 summary:A novel regularizer of the PARAFAC decomposition factors capturing thetensor's rank is proposed in this paper, as the key enabler for completion ofthree-way data arrays with missing entries. Set in a Bayesian framework, thetensor completion method incorporates prior information to enhance itssmoothing and prediction capabilities. This probabilistic approach cannaturally accommodate general models for the data distribution, lending itselfto various fitting criteria that yield optimum estimates in themaximum-a-posteriori sense. In particular, two algorithms are devised forGaussian- and Poisson-distributed data, that minimize the rank-regularizedleast-squares error and Kullback-Leibler divergence, respectively. The proposedtechnique is able to recover the "ground-truth'' tensor rank when tested onsynthetic data, and to complete brain imaging and yeast gene expressiondatasets with 50% and 15% of missing entries respectively, resulting inrecovery errors at -10dB and -15dB.
arxiv-2400-300 | Fully Automatic Expression-Invariant Face Correspondence | http://arxiv.org/pdf/1202.1444v2.pdf | author:Augusto Salazar, Stefanie Wuhrer, Chang Shu, Flavio Prieto category:cs.CV cs.GR published:2012-02-07 summary:We consider the problem of computing accurate point-to-point correspondencesamong a set of human face scans with varying expressions. Our fully automaticapproach does not require any manually placed markers on the scan. Instead, theapproach learns the locations of a set of landmarks present in a database anduses this knowledge to automatically predict the locations of these landmarkson a newly available scan. The predicted landmarks are then used to computepoint-to-point correspondences between a template model and the newly availablescan. To accurately fit the expression of the template to the expression of thescan, we use as template a blendshape model. Our algorithm was tested on adatabase of human faces of different ethnic groups with strongly varyingexpressions. Experimental results show that the obtained point-to-pointcorrespondence is both highly accurate and consistent for most of the tested 3Dface models.
