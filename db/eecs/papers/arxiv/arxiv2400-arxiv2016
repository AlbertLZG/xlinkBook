arxiv-2400-1 | Tangent Bundle Manifold Learning via Grassmann&Stiefel Eigenmaps | http://arxiv.org/abs/1212.6031 | author:Alexander V. Bernstein, Alexander P. Kuleshov category:cs.LG 68T05 published:2012-12-25 summary:One of the ultimate goals of Manifold Learning (ML) is to reconstruct anunknown nonlinear low-dimensional manifold embedded in a high-dimensionalobservation space by a given set of data points from the manifold. We derive alocal lower bound for the maximum reconstruction error in a small neighborhoodof an arbitrary point. The lower bound is defined in terms of the distancebetween tangent spaces to the original manifold and the estimated manifold atthe considered point and reconstructed point, respectively. We propose anamplification of the ML, called Tangent Bundle ML, in which the proximity notonly between the original manifold and its estimator but also between theirtangent spaces is required. We present a new algorithm that solves this problemand gives a new solution for the ML also.
arxiv-2400-2 | Exponentially Weighted Moving Average Charts for Detecting Concept Drift | http://arxiv.org/abs/1212.6018 | author:Gordon J. Ross, Niall M. Adams, Dimitris K. Tasoulis, David J. Hand category:stat.ML cs.LG stat.AP published:2012-12-25 summary:Classifying streaming data requires the development of methods which arecomputationally efficient and able to cope with changes in the underlyingdistribution of the stream, a phenomenon known in the literature as conceptdrift. We propose a new method for detecting concept drift which uses anExponentially Weighted Moving Average (EWMA) chart to monitor themisclassification rate of an streaming classifier. Our approach is modular andcan hence be run in parallel with any underlying classifier to provide anadditional layer of concept drift detection. Moreover our method iscomputationally efficient with overhead O(1) and works in a fully online mannerwith no need to store data points in memory. Unlike many existing approaches toconcept drift detection, our method allows the rate of false positivedetections to be controlled and kept constant over time.
arxiv-2400-3 | Large Scale Strongly Supervised Ensemble Metric Learning, with Applications to Face Verification and Retrieval | http://arxiv.org/abs/1212.6094 | author:Chang Huang, Shenghuo Zhu, Kai Yu category:cs.CV published:2012-12-25 summary:Learning Mahanalobis distance metrics in a high- dimensional feature space isvery difficult especially when structural sparsity and low rank are enforced toimprove com- putational efficiency in testing phase. This paper addresses bothaspects by an ensemble metric learning approach that consists of sparse blockdiagonal metric ensembling and join- t metric learning as two consecutivesteps. The former step pursues a highly sparse block diagonal metric byselecting effective feature groups while the latter one further exploitscorrelations between selected feature groups to obtain an accurate and low rankmetric. Our algorithm considers all pairwise or triplet constraints generatedfrom training samples with explicit class labels, and possesses good scala-bility with respect to increasing feature dimensionality and growing datavolumes. Its applications to face verification and retrieval outperformexisting state-of-the-art methods in accuracy while retaining high efficiency.
arxiv-2400-4 | Fully scalable online-preprocessing algorithm for short oligonucleotide microarray atlases | http://arxiv.org/abs/1212.5932 | author:Leo Lahti, Aurora Torrente, Laura L. Elo, Alvis Brazma, Johan Rung category:q-bio.QM cs.CE cs.LG q-bio.GN stat.AP stat.ML published:2012-12-24 summary:Accumulation of standardized data collections is opening up novelopportunities for holistic characterization of genome function. The limitedscalability of current preprocessing techniques has, however, formed abottleneck for full utilization of contemporary microarray collections. Whileshort oligonucleotide arrays constitute a major source of genome-wide profilingdata, scalable probe-level preprocessing algorithms have been available onlyfor few measurement platforms based on pre-calculated model parameters fromrestricted reference training sets. To overcome these key limitations, weintroduce a fully scalable online-learning algorithm that provides tools toprocess large microarray atlases including tens of thousands of arrays. Unlikethe alternatives, the proposed algorithm scales up in linear time with respectto sample size and is readily applicable to all short oligonucleotideplatforms. This is the only available preprocessing algorithm that can learnprobe-level parameters based on sequential hyperparameter updates at small,consecutive batches of data, thus circumventing the extensive memoryrequirements of the standard approaches and opening up novel opportunities totake full advantage of contemporary microarray data collections. Moreover,using the most comprehensive data collections to estimate probe-level effectscan assist in pinpointing individual probes affected by various biases andprovide new tools to guide array design and quality control. The implementationis freely available in R/Bioconductor athttp://www.bioconductor.org/packages/devel/bioc/html/RPA.html
arxiv-2400-5 | A short note on the tail bound of Wishart distribution | http://arxiv.org/abs/1212.5860 | author:Shenghuo Zhu category:math.ST cs.LG stat.TH published:2012-12-24 summary:We study the tail bound of the emperical covariance of multivariate normaldistribution. Following the work of (Gittens & Tropp, 2011), we provide a tailbound with a small constant.
arxiv-2400-6 | Blinking Molecule Tracking | http://arxiv.org/abs/1212.5877 | author:Andreas Karrenbauer, Dominik Wöll category:cs.CV cs.DM published:2012-12-24 summary:We discuss a method for tracking individual molecules which globallyoptimizes the likelihood of the connections between molecule positions fast andwith high reliability even for high spot densities and blinking molecules. Ourmethod works with cost functions which can be freely chosen to combine costsfor distances between spots in space and time and which can account for thereliability of positioning a molecule. To this end, we describe a top-downpolyhedral approach to the problem of tracking many individual molecules. Thisimmediately yields an effective implementation using standard linearprogramming solvers. Our method can be applied to 2D and 3D tracking.
arxiv-2400-7 | Reconstructing Self Organizing Maps as Spider Graphs for better visual interpretation of large unstructured datasets | http://arxiv.org/abs/1301.0289 | author:Aaditya Prakash category:cs.GR stat.ML published:2012-12-24 summary:Self-Organizing Maps (SOM) are popular unsupervised artificial neural networkused to reduce dimensions and visualize data. Visual interpretation fromSelf-Organizing Maps (SOM) has been limited due to grid approach of datarepresentation, which makes inter-scenario analysis impossible. The paperproposes a new way to structure SOM. This model reconstructs SOM to showstrength between variables as the threads of a cobweb and illuminateinter-scenario analysis. While Radar Graphs are very crude representation ofspider web, this model uses more lively and realistic cobweb representation totake into account the difference in strength and length of threads. This modelallows for visualization of highly unstructured dataset with large number ofdimensions, common in Bigdata sources.
arxiv-2400-8 | Distributed optimization of deeply nested systems | http://arxiv.org/abs/1212.5921 | author:Miguel Á. Carreira-Perpiñán, Weiran Wang category:cs.LG cs.NE math.OC stat.ML published:2012-12-24 summary:In science and engineering, intelligent processing of complex signals such asimages, sound or language is often performed by a parameterized hierarchy ofnonlinear processing layers, sometimes biologically inspired. Hierarchicalsystems (or, more generally, nested systems) offer a way to generate complexmappings using simple stages. Each layer performs a different operation andachieves an ever more sophisticated representation of the input, as, forexample, in an deep artificial neural network, an object recognition cascade incomputer vision or a speech front-end processing. Joint estimation of theparameters of all the layers and selection of an optimal architecture is widelyconsidered to be a difficult numerical nonconvex optimization problem,difficult to parallelize for execution in a distributed computationenvironment, and requiring significant human expert effort, which leads tosuboptimal systems in practice. We describe a general mathematical strategy tolearn the parameters and, to some extent, the architecture of nested systems,called the method of auxiliary coordinates (MAC). This replaces the originalproblem involving a deeply nested function with a constrained problem involvinga different function in an augmented space without nesting. The constrainedproblem may be solved with penalty-based methods using alternating optimizationover the parameters and the auxiliary coordinates. MAC has provableconvergence, is easy to implement reusing existing algorithms for singlelayers, can be parallelized trivially and massively, applies even whenparameter derivatives are not available or not desirable, and is competitivewith state-of-the-art nonlinear optimizers even in the serial computationsetting, often providing reasonable models within a few iterations.
arxiv-2400-9 | Mixture Model Averaging for Clustering | http://arxiv.org/abs/1212.5760 | author:Yuhong Wei, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2012-12-23 summary:In mixture model-based clustering applications, it is common to fit severalmodels from a family and report clustering results from only the `best' one. Insuch circumstances, selection of this best model is achieved using a modelselection criterion, most often the Bayesian information criterion. Rather thanthrow away all but the best model, we average multiple models that are in somesense close to the best one, thereby producing a weighted average of clusteringresults. Two (weighted) averaging approaches are considered: averaging thecomponent membership probabilities and averaging models. In both cases, Occam'swindow is used to determine closeness to the best model and weights arecomputed within a Bayesian model averaging paradigm. In some cases, we need tomerge components before averaging; we introduce a method for merging mixturecomponents based on the adjusted Rand index. The effectiveness of ourmodel-based clustering averaging approaches is illustrated using a family ofGaussian mixture models on real and simulated data.
arxiv-2400-10 | Data complexity measured by principal graphs | http://arxiv.org/abs/1212.5841 | author:Andrei Zinovyev, Evgeny Mirkes category:cs.LG cs.IT math.IT published:2012-12-23 summary:How to measure the complexity of a finite set of vectors embedded in amultidimensional space? This is a non-trivial question which can be approachedin many different ways. Here we suggest a set of data complexity measures usinguniversal approximators, principal cubic complexes. Principal cubic complexesgeneralise the notion of principal manifolds for datasets with non-trivialtopologies. The type of the principal cubic complex is determined by itsdimension and a grammar of elementary graph transformations. The simplestgrammar produces principal trees. We introduce three natural types of data complexity: 1) geometric (deviationof the data's approximator from some "idealized" configuration, such asdeviation from harmonicity); 2) structural (how many elements of a principalgraph are needed to approximate the data), and 3) construction complexity (howmany applications of elementary graph transformations are needed to constructthe principal object starting from the simplest one). We compute these measures for several simulated and real-life datadistributions and show them in the "accuracy-complexity" plots, helping tooptimize the accuracy/complexity ratio. We discuss various issues connectedwith measuring data complexity. Software for computing data complexity measuresfrom principal cubic complexes is provided as well.
arxiv-2400-11 | Collaborating Robotics Using Nature-Inspired Meta-Heuristics | http://arxiv.org/abs/1212.5777 | author:M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny category:cs.NE cs.RO published:2012-12-23 summary:This paper introduces collaborating robots which provide the possibility ofenhanced task performance, high reliability and decreased. Collaborating-botsare a collection of mobile robots able to self-assemble and to self-organize inorder to solve problems that cannot be solved by a single robot. These robotscombine the power of swarm intelligence with the flexibility ofself-reconfiguration as aggregate Collaborating-bots can dynamically changetheir structure to match environmental variations. Collaborating robots aremore than just networks of independent agents, they are potentiallyreconfigurable networks of communicating agents capable of coordinated sensingand interaction with the environment. Robots are going to be an important partof the future. Collaborating robots are limited in individual capability, butrobots deployed in large numbers can represent a strong force similar to acolony of ants or swarm of bees. We present a mechanism for collaboratingrobots based on swarm intelligence such as Ant colony optimization and Particleswarm Optimization
arxiv-2400-12 | Normalized Compression Distance of Multisets with Applications | http://arxiv.org/abs/1212.5711 | author:Andrew R. Cohen, Paul M. B. Vitanyi category:cs.CV cs.IT math.IT published:2012-12-22 summary:Normalized compression distance (NCD) is a parameter-free, feature-free,alignment-free, similarity measure between a pair of finite objects based oncompression. However, it is not sufficient for all applications. We propose anNCD of finite multisets (a.k.a. multiples) of finite objects that is also ametric. Previously, attempts to obtain such an NCD failed. We cover the entiretrajectory from theoretical underpinning to feasible practice. The new NCD formultisets is applied to retinal progenitor cell classification questions and torelated synthetically generated data that were earlier treated with thepairwise NCD. With the new method we achieved significantly better results.Similarly for questions about axonal organelle transport. We also applied thenew NCD to handwritten digit recognition and improved classification accuracysignificantly over that of pairwise NCD by incorporating both the pairwise andNCD for multisets. In the analysis we use the incomputable Kolmogorovcomplexity that for practical purposes is approximated from above by the lengthof the compressed version of the file involved, using a real-world compressionprogram. Index Terms--- Normalized compression distance, multisets or multiples,pattern recognition, data mining, similarity, classification, Kolmogorovcomplexity, retinal progenitor cells, synthetic data, organelle transport,handwritten character recognition
arxiv-2400-13 | High-precision camera distortion measurements with a "calibration harp" | http://arxiv.org/abs/1212.5656 | author:Zhongwei Tang, Rafael Grompone von Gioi, Pascal Monasse, Jean-Michel Morel category:cs.CV published:2012-12-22 summary:This paper addresses the high precision measurement of the distortion of adigital camera from photographs. Traditionally, this distortion is measuredfrom photographs of a flat pattern which contains aligned elements.Nevertheless, it is nearly impossible to fabricate a very flat pattern and tovalidate its flatness. This fact limits the attainable measurable precisions.In contrast, it is much easier to obtain physically very precise straight linesby tightly stretching good quality strings on a frame. Taking literally"plumb-line methods", we built a "calibration harp" instead of the classic flatpatterns to obtain a high precision measurement tool, demonstrably reaching2/100 pixel precisions. The harp is complemented with the algorithms computingautomatically from harp photographs two different and complementary lensdistortion measurements. The precision of the method is evaluated on imagescorrected by state-of-the-art distortion correction algorithms, and by popularsoftware. Three applications are shown: first an objective and reliablemeasurement of the result of any distortion correction. Second, the harppermits to control state-of-the art global camera calibration algorithms: Itpermits to select the right distortion model, thus avoiding internalcompensation errors inherent to these methods. Third, the method replacesmanual procedures in other distortion correction methods, makes them fullyautomatic, and increases their reliability and precision.
arxiv-2400-14 | Hierarchical Graphical Models for Multigroup Shape Analysis using Expectation Maximization with Sampling in Kendall's Shape Space | http://arxiv.org/abs/1212.5720 | author:Yen-Yun Yu, P. Thomas Fletcher, Suyash P. Awate category:cs.CV published:2012-12-22 summary:This paper proposes a novel framework for multi-group shape analysis relyingon a hierarchical graphical statistical model on shapes within a population.Theframework represents individual shapes as point setsmodulo translation,rotation, and scale, following the notion in Kendall shape space.Whileindividual shapes are derived from their group shape model, each group shapemodel is derived from a single population shape model. The hierarchical modelfollows the natural organization of population data and the top level in thehierarchy provides a common frame of reference for multigroup shape analysis,e.g. classification and hypothesis testing. Unlike typical shape-modelingapproaches, the proposed model is a generative model that defines a jointdistribution of object-boundary data and the shape-model variables.Furthermore, it naturally enforces optimal correspondences during the processof model fitting and thereby subsumes the so-called correspondence problem. Theproposed inference scheme employs an expectation maximization (EM) algorithmthat treats the individual and group shape variables as hidden random variablesand integrates them out before estimating the parameters (population mean andvariance and the group variances). The underpinning of the EM algorithm is thesampling of pointsets, in Kendall shape space, from their posteriordistribution, for which we exploit a highly-efficient scheme based onHamiltonian Monte Carlo simulation. Experiments in this paper use the fittedhierarchical model to perform (1) hypothesis testing for comparison betweenpairs of groups using permutation testing and (2) classification for imageretrieval. The paper validates the proposed framework on simulated data anddemonstrates results on real data.
arxiv-2400-15 | ADADELTA: An Adaptive Learning Rate Method | http://arxiv.org/abs/1212.5701 | author:Matthew D. Zeiler category:cs.LG published:2012-12-22 summary:We present a novel per-dimension learning rate method for gradient descentcalled ADADELTA. The method dynamically adapts over time using only first orderinformation and has minimal computational overhead beyond vanilla stochasticgradient descent. The method requires no manual tuning of a learning rate andappears robust to noisy gradient information, different model architecturechoices, various data modalities and selection of hyperparameters. We showpromising results compared to other methods on the MNIST digit classificationtask using a single machine and on a large scale voice dataset in a distributedcluster environment.
arxiv-2400-16 | Optimal classification in sparse Gaussian graphic model | http://arxiv.org/abs/1212.5332 | author:Yingying Fan, Jiashun Jin, Zhigang Yao category:stat.ML math.ST stat.TH published:2012-12-21 summary:Consider a two-class classification problem where the number of features ismuch larger than the sample size. The features are masked by Gaussian noisewith mean zero and covariance matrix $\Sigma$, where the precision matrix$\Omega=\Sigma^{-1}$ is unknown but is presumably sparse. The useful features,also unknown, are sparse and each contributes weakly (i.e., rare and weak) tothe classification decision. By obtaining a reasonably good estimate of$\Omega$, we formulate the setting as a linear regression model. We propose atwo-stage classification method where we first select features by the method ofInnovated Thresholding (IT), and then use the retained features and Fisher'sLDA for classification. In this approach, a crucial problem is how to set thethreshold of IT. We approach this problem by adapting the recent innovation ofHigher Criticism Thresholding (HCT). We find that when useful features are rareand weak, the limiting behavior of HCT is essentially just as good as thelimiting behavior of ideal threshold, the threshold one would choose if theunderlying distribution of the signals is known (if only). Somewhatsurprisingly, when $\Omega$ is sufficiently sparse, its off-diagonalcoordinates usually do not have a major influence over the classificationdecision. Compared to recent work in the case where $\Omega$ is the identitymatrix [Proc. Natl. Acad. Sci. USA 105 (2008) 14790-14795; Philos. Trans. R.Soc. Lond. Ser. A Math. Phys. Eng. Sci. 367 (2009) 4449-4470], the currentsetting is much more general, which needs a new approach and much moresophisticated analysis. One key component of the analysis is the intimaterelationship between HCT and Fisher's separation. Another key component is thetight large-deviation bounds for empirical processes for data withunconventional correlation structures, where graph theory on vertex coloringplays an important role.
arxiv-2400-17 | Soft Set Based Feature Selection Approach for Lung Cancer Images | http://arxiv.org/abs/1212.5391 | author:G. Jothi, H. Hannah Inbarani category:cs.LG cs.CE published:2012-12-21 summary:Lung cancer is the deadliest type of cancer for both men and women. Featureselection plays a vital role in cancer classification. This paper investigatesthe feature selection process in Computed Tomographic (CT) lung cancer imagesusing soft set theory. We propose a new soft set based unsupervised featureselection algorithm. Nineteen features are extracted from the segmented lungimages using gray level co-occurence matrix (GLCM) and gray level differentmatrix (GLDM). In this paper, an efficient Unsupervised Soft Set based QuickReduct (SSUSQR) algorithm is presented. This method is used to select featuresfrom the data set and compared with existing rough set based unsupervisedfeature selection methods. Then K-Means and Self Organizing Map (SOM)clustering algorithms are used to cluster the data. The performance of thefeature selection algorithms is evaluated based on performance of clusteringtechniques. The results show that the proposed method effectively removesredundant features.
arxiv-2400-18 | Black box modelling of HVAC system : improving the performances of neural networks | http://arxiv.org/abs/1212.5594 | author:Eric Fock, Thierry Alex Mara, Alfred Jean Philippe Lauret, Harry Boyer category:cs.NE cs.CE published:2012-12-21 summary:This paper deals with neural networks modelling of HVAC systems. In order toincrease the neural networks performances, a method based on sensitivityanalysis is applied. The same technique is also used to compute the relevanceof each input. To avoid the prediction errors in dry coil conditions, ametamodel for each capacity is derived from the neural networks. The regressioncoefficients of the polynomial forms are identified through the use of spectralanalysis. These methods based on sensitivity and spectral analysis lead to anoptimized neural network model, as regard to its architecture and predictions.
arxiv-2400-19 | Topic Extraction and Bundling of Related Scientific Articles | http://arxiv.org/abs/1212.5423 | author:Shameem A Puthiya Parambath category:cs.IR cs.DL stat.ML published:2012-12-21 summary:Automatic classification of scientific articles based on commoncharacteristics is an interesting problem with many applications in digitallibrary and information retrieval systems. Properly organized articles can beuseful for automatic generation of taxonomies in scientific writings, textualsummarization, efficient information retrieval etc. Generating article bundlesfrom a large number of input articles, based on the associated features of thearticles is tedious and computationally expensive task. In this report wepropose an automatic two-step approach for topic extraction and bundling ofrelated articles from a set of scientific articles in real-time. For topicextraction, we make use of Latent Dirichlet Allocation (LDA) topic modelingtechniques and for bundling, we make use of hierarchical agglomerativeclustering techniques. We run experiments to validate our bundling semantics and compare it withexisting models in use. We make use of an online crowdsourcing marketplaceprovided by Amazon called Amazon Mechanical Turk to carry out experiments. Weexplain our experimental setup and empirical results in detail and show thatour method is advantageous over existing ones.
arxiv-2400-20 | In Vivo Quantification of Clot Formation in Extracorporeal Circuits | http://arxiv.org/abs/1212.5454 | author:Omid David, Rabin Gerrah category:cs.CV physics.med-ph published:2012-12-21 summary:Clot formation is a common complication in extracorporeal circuits. In thispaper we describe a novel method for clot formation analysis using imageprocessing. We assembled a closed extracorporeal circuit and circulated bloodat varying speeds. Blood filters were placed in downstream of the flow, andclotting agents were added to the circuit. Digital images of the filter weresubsequently taken, and image analysis was applied to calculate the density ofthe clot. Our results show a significant correlation between the cumulativesize of the clots, the density measure of the clot based on image analysis, andflow duration in the system.
arxiv-2400-21 | Reinforcement learning for port-Hamiltonian systems | http://arxiv.org/abs/1212.5524 | author:Olivier Sprangers, Gabriel A. D. Lopes, Robert Babuska category:cs.SY cs.LG published:2012-12-21 summary:Passivity-based control (PBC) for port-Hamiltonian systems provides anintuitive way of achieving stabilization by rendering a system passive withrespect to a desired storage function. However, in most instances the controllaw is obtained without any performance considerations and it has to becalculated by solving a complex partial differential equation (PDE). In orderto address these issues we introduce a reinforcement learning approach into theenergy-balancing passivity-based control (EB-PBC) method, which is a form ofPBC in which the closed-loop energy is equal to the difference between thestored and supplied energies. We propose a technique to parameterize EB-PBCthat preserves the systems's PDE matching conditions, does not require thespecification of a global desired Hamiltonian, includes performance criteria,and is robust to extra non-linearities such as control input saturation. Theparameters of the control law are found using actor-critic reinforcementlearning, enabling learning near-optimal control policies satisfying a desiredclosed-loop energy landscape. The advantages are that near-optimal controllerscan be generated using standard energy shaping techniques and that thesolutions learned can be interpreted in terms of energy shaping and dampinginjection, which makes it possible to numerically assess stability usingpassivity theory. From the reinforcement learning perspective, our proposalallows for the class of port-Hamiltonian systems to be incorporated in theactor-critic framework, speeding up the learning thanks to the resultingparameterization of the policy. The method has been successfully applied to thependulum swing-up problem in simulations and real-life experiments.
arxiv-2400-22 | On the Adaptability of Neural Network Image Super-Resolution | http://arxiv.org/abs/1212.5352 | author:Kah Keong Chua, Yong Haur Tay category:cs.CV published:2012-12-21 summary:In this paper, we described and developed a framework for MultilayerPerceptron (MLP) to work on low level image processing, where MLP will be usedto perform image super-resolution. Meanwhile, MLP are trained with differenttypes of images from various categories, hence analyse the behaviour andperformance of the neural network. The tests are carried out using qualitativetest, in which Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR) andStructural Similarity Index (SSIM). The results showed that MLP trained withsingle image category can perform reasonably well compared to methods proposedby other researchers.
arxiv-2400-23 | Fuzzy soft rough K-Means clustering approach for gene expression data | http://arxiv.org/abs/1212.5359 | author:K. Dhanalakshmi, H. Hannah Inbarani category:cs.LG cs.CE published:2012-12-21 summary:Clustering is one of the widely used data mining techniques for medicaldiagnosis. Clustering can be considered as the most important unsupervisedlearning technique. Most of the clustering methods group data based on distanceand few methods cluster data based on similarity. The clustering algorithmsclassify gene expression data into clusters and the functionally related genesare grouped together in an efficient manner. The groupings are constructed suchthat the degree of relationship is strong among members of the same cluster andweak among members of different clusters. In this work, we focus on asimilarity relationship among genes with similar expression patterns so that aconsequential and simple analytical decision can be made from the proposedFuzzy Soft Rough K-Means algorithm. The algorithm is developed based on FuzzySoft sets and Rough sets. Comparative analysis of the proposed work is madewith bench mark algorithms like K-Means and Rough K-Means and efficiency of theproposed algorithm is illustrated in this work by using various clustervalidity measures such as DB index and Xie-Beni index.
arxiv-2400-24 | Random Spanning Trees and the Prediction of Weighted Graphs | http://arxiv.org/abs/1212.5637 | author:Nicolo' Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG stat.ML published:2012-12-21 summary:We investigate the problem of sequentially predicting the binary labels onthe nodes of an arbitrary weighted graph. We show that, under a suitableparametrization of the problem, the optimal number of prediction mistakes canbe characterized (up to logarithmic factors) by the cutsize of a randomspanning tree of the graph. The cutsize is induced by the unknown adversariallabeling of the graph nodes. In deriving our characterization, we obtain asimple randomized algorithm achieving in expectation the optimal mistake boundon any polynomially connected weighted graph. Our algorithm draws a randomspanning tree of the original graph and then predicts the nodes of this tree inconstant expected amortized time and linear space. Experiments on real-worlddatasets show that our method compares well to both global (Perceptron) andlocal (label propagation) methods, while being generally faster in practice.
arxiv-2400-25 | The Twitter of Babel: Mapping World Languages through Microblogging Platforms | http://arxiv.org/abs/1212.5238 | author:Delia Mocanu, Andrea Baronchelli, Bruno Gonçalves, Nicola Perra, Alessandro Vespignani category:physics.soc-ph cs.CL cs.SI published:2012-12-20 summary:Large scale analysis and statistics of socio-technical systems that just afew short years ago would have required the use of consistent economic andhuman resources can nowadays be conveniently performed by mining the enormousamount of digital data produced by human activities. Although acharacterization of several aspects of our societies is emerging from the datarevolution, a number of questions concerning the reliability and the biasesinherent to the big data "proxies" of social life are still open. Here, wesurvey worldwide linguistic indicators and trends through the analysis of alarge-scale dataset of microblogging posts. We show that available data allowfor the study of language geography at scales ranging from country-levelaggregation to specific city neighborhoods. The high resolution and coverage ofthe data allows us to investigate different indicators such as the linguistichomogeneity of different countries, the touristic seasonal patterns withincountries and the geographical distribution of different languages inmultilingual regions. This work highlights the potential of geolocalizedstudies of open data sources to improve current analysis and develop indicatorsfor major social phenomena in specific communities.
arxiv-2400-26 | Towards the Evolution of Novel Vertical-Axis Wind Turbines | http://arxiv.org/abs/1212.5271 | author:Richard J. Preen, Larry Bull category:cs.NE cs.AI published:2012-12-20 summary:Renewable and sustainable energy is one of the most important challengescurrently facing mankind. Wind has made an increasing contribution to theworld's energy supply mix, but still remains a long way from reaching its fullpotential. In this paper, we investigate the use of artificial evolution todesign vertical-axis wind turbine prototypes that are physically instantiatedand evaluated under approximated wind tunnel conditions. An artificial neuralnetwork is used as a surrogate model to assist learning and found to reduce thenumber of fabrications required to reach a higher aerodynamic efficiency,resulting in an important cost reduction. Unlike in other approaches, such ascomputational fluid dynamics simulations, no mathematical formulations are usedand no model assumptions are made.
arxiv-2400-27 | A Neural Network Approach to ECG Denoising | http://arxiv.org/abs/1212.5217 | author:Rui Rodrigues, Paula Couto category:cs.CE cs.NE published:2012-12-20 summary:We propose an ECG denoising method based on a feed forward neural networkwith three hidden layers. Particulary useful for very noisy signals, thisapproach uses the available ECG channels to reconstruct a noisy channel. Wetested the method, on all the records from Physionet MIT-BIH ArrhythmiaDatabase, adding electrode motion artifact noise. This denoising methodimproved the perfomance of publicly available ECG analysis programs on noisyECG signals. This is an offline method that can be used to remove noise fromvery corrupted Holter records.
arxiv-2400-28 | SMML estimators for 1-dimensional continuous data | http://arxiv.org/abs/1212.4906 | author:James G. Dowty category:cs.IT math.IT math.ST stat.ML stat.TH published:2012-12-20 summary:A method is given for calculating the strict minimum message length (SMML)estimator for 1-dimensional exponential families with continuous sufficientstatistics. A set of $n$ equations are found that the $n$ cut-points of theSMML estimator must satisfy. These equations can be solved using Newton'smethod and this approach is used to produce new results and to replicateresults that C. S. Wallace obtained using his boundary rules for the SMMLestimator. A rigorous proof is also given that, despite being composed of stepfunctions, the posterior probability corresponding to the SMML estimator is acontinuous function of the data.
arxiv-2400-29 | Automatic landmark annotation and dense correspondence registration for 3D human facial images | http://arxiv.org/abs/1212.4920 | author:Jianya Guo, Xi Mei, Kun Tang category:cs.CV q-bio.QM published:2012-12-20 summary:Dense surface registration of three-dimensional (3D) human facial imagesholds great potential for studies of human trait diversity, disease genetics,and forensics. Non-rigid registration is particularly useful for establishingdense anatomical correspondences between faces. Here we describe a novelnon-rigid registration method for fully automatic 3D facial image mapping. Thismethod comprises two steps: first, seventeen facial landmarks are automaticallyannotated, mainly via PCA-based feature recognition following 3D-to-2D datatransformation. Second, an efficient thin-plate spline (TPS) protocol is usedto establish the dense anatomical correspondence between facial images, underthe guidance of the predefined landmarks. We demonstrate that this method isrobust and highly accurate, even for different ethnicities. The average face iscalculated for individuals of Han Chinese and Uyghur origins. While fullyautomatic and computationally efficient, this method enables high-throughputanalysis of human facial feature variation.
arxiv-2400-30 | Hybrid Fuzzy-ART based K-Means Clustering Methodology to Cellular Manufacturing Using Operational Time | http://arxiv.org/abs/1212.5101 | author:Sourav Sengupta, Tamal Ghosh, Pranab K Dan, Manojit Chattopadhyay category:cs.LG published:2012-12-20 summary:This paper presents a new hybrid Fuzzy-ART based K-Means Clustering techniqueto solve the part machine grouping problem in cellular manufacturing systemsconsidering operational time. The performance of the proposed technique istested with problems from open literature and the results are compared to theexisting clustering models such as simple K-means algorithm and modified ART1algorithm using an efficient modified performance measure known as modifiedgrouping efficiency (MGE) as found in the literature. The results support thebetter performance of the proposed algorithm. The Novelty of this study lies inthe simple and efficient methodology to produce quick solutions for shop floormanagers with least computational efforts and time.
arxiv-2400-31 | An Experiment with Hierarchical Bayesian Record Linkage | http://arxiv.org/abs/1212.5203 | author:Michael D. Larsen category:math.ST stat.AP stat.CO stat.ME stat.ML stat.TH G.3.0 published:2012-12-20 summary:In record linkage (RL), or exact file matching, the goal is to identify thelinks between entities with information on two or more files. RL is animportant activity in areas including counting the population, enhancing surveyframes and data, and conducting epidemiological and follow-up studies. RL ischallenging when files are very large, no accurate personal identification (ID)number is present on all files for all units, and some information is recordedwith error. Without an unique ID number one must rely on comparisons of names,addresses, dates, and other information to find the links. Latent class modelscan be used to automatically score the value of information for determiningmatch status. Data for fitting models come from comparisons made within groupsof units that pass initial file blocking requirements. Data distributions canvary across blocks. This article examines the use of prior information andhierarchical latent class models in the context of RL.
arxiv-2400-32 | Nonparametric ridge estimation | http://arxiv.org/abs/1212.5156 | author:Christopher R. Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH published:2012-12-20 summary:We study the problem of estimating the ridges of a density function. Ridgeestimation is an extension of mode finding and is useful for understanding thestructure of a density. It can also be used to find hidden structure in pointcloud data. We show that, under mild regularity conditions, the ridges of thekernel density estimator consistently estimate the ridges of the true density.When the data are noisy measurements of a manifold, we show that the ridges areclose and topologically similar to the hidden manifold. To find the estimatedridges in practice, we adapt the modified mean-shift algorithm proposed byOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numericalexperiments verify that the algorithm is accurate.
arxiv-2400-33 | Parsimonious module inference in large networks | http://arxiv.org/abs/1212.4794 | author:Tiago P. Peixoto category:physics.soc-ph stat.ML published:2012-12-19 summary:We investigate the detectability of modules in large networks when the numberof modules is not known in advance. We employ the minimum description length(MDL) principle which seeks to minimize the total amount of informationrequired to describe the network, and avoid overfitting. According to thiscriterion, we obtain general bounds on the detectability of any prescribedblock structure, given the number of nodes and edges in the sampled network. Wealso obtain that the maximum number of detectable blocks scales as $\sqrt{N}$,where $N$ is the number of nodes in the network, for a fixed average degree$<k>$. We also show that the simplicity of the MDL approach yields an efficientmultilevel Monte Carlo inference algorithm with a complexity of $O(\tau N\logN)$, if the number of blocks is unknown, and $O(\tau N)$ if it is known, where$\tau$ is the mixing time of the Markov chain. We illustrate the application ofthe method on a large network of actors and films with over $10^6$ edges, and adissortative, bipartite block structure.
arxiv-2400-34 | A complexity analysis of statistical learning algorithms | http://arxiv.org/abs/1212.4562 | author:Mark A. Kon category:stat.ML published:2012-12-19 summary:We apply information-based complexity analysis to support vector machine(SVM) algorithms, with the goal of a comprehensive continuous algorithmicanalysis of such algorithms. This involves complexity measures in which somehigher order operations (e.g., certain optimizations) are considered primitivefor the purposes of measuring complexity. We consider classes of informationoperators and algorithms made up of scaled families, and investigate theutility of scaling the complexities to minimize error. We look at the divisionof statistical learning into information and algorithmic components, at thecomplexities of each, and at applications to support vector machine (SVM) andmore general machine learning algorithms. We give applications to SVMalgorithms graded into linear and higher order components, and give an examplein biomedical informatics.
arxiv-2400-35 | Perceptually Motivated Shape Context Which Uses Shape Interiors | http://arxiv.org/abs/1212.4608 | author:Vittal Premachandran, Ramakrishna Kakarala category:cs.CV published:2012-12-19 summary:In this paper, we identify some of the limitations of current-day shapematching techniques. We provide examples of how contour-based shape matchingtechniques cannot provide a good match for certain visually similar shapes. Toovercome this limitation, we propose a perceptually motivated variant of thewell-known shape context descriptor. We identify that the interior propertiesof the shape play an important role in object recognition and develop adescriptor that captures these interior properties. We show that our method caneasily be augmented with any other shape matching algorithm. We also show fromour experiments that the use of our descriptor can significantly improve theretrieval rates.
arxiv-2400-36 | Natural Language Understanding Based on Semantic Relations between Sentences | http://arxiv.org/abs/1212.4674 | author:Hyeok Kong category:cs.CL published:2012-12-19 summary:In this paper, we define event expression over sentences of natural languageand semantic relations between events. Based on this definition, we formallyconsider text understanding process having events as basic unit.
arxiv-2400-37 | Role Mining with Probabilistic Models | http://arxiv.org/abs/1212.4775 | author:Mario Frank, Joachim M. Buhmann, David Basin category:cs.CR cs.LG stat.ML published:2012-12-19 summary:Role mining tackles the problem of finding a role-based access control (RBAC)configuration, given an access-control matrix assigning users to accesspermissions as input. Most role mining approaches work by constructing a largeset of candidate roles and use a greedy selection strategy to iteratively picka small subset such that the differences between the resulting RBACconfiguration and the access control matrix are minimized. In this paper, weadvocate an alternative approach that recasts role mining as an inferenceproblem rather than a lossy compression problem. Instead of using combinatorialalgorithms to minimize the number of roles needed to represent theaccess-control matrix, we derive probabilistic models to learn the RBACconfiguration that most likely underlies the given matrix. Our models are generative in that they reflect the way that permissions areassigned to users in a given RBAC configuration. We additionally model howuser-permission assignments that conflict with an RBAC configuration emerge andwe investigate the influence of constraints on role hierarchies and on thenumber of assignments. In experiments with access-control matrices fromreal-world enterprises, we compare our proposed models with other role miningmethods. Our results show that our probabilistic models infer roles thatgeneralize well to new system users for a wide variety of data, while othermodels' generalization abilities depend on the dataset given.
arxiv-2400-38 | Automatic post-picking using MAPPOS improves particle image detection from Cryo-EM micrographs | http://arxiv.org/abs/1212.4871 | author:Ramin Norousi, Stephan Wickles, Christoph Leidig, Thomas Becker, Volker J. Schmid, Roland Beckmann, Achim Tresch category:stat.ML cs.CV published:2012-12-19 summary:Cryo-electron microscopy (cryo-EM) studies using single particlereconstruction are extensively used to reveal structural information onmacromolecular complexes. Aiming at the highest achievable resolution, state ofthe art electron microscopes automatically acquire thousands of high-qualitymicrographs. Particles are detected on and boxed out from each micrograph usingfully- or semi-automated approaches. However, the obtained particles stillrequire laborious manual post-picking classification, which is one majorbottleneck for single particle analysis of large datasets. We introduce MAPPOS,a supervised post-picking strategy for the classification of boxed particleimages, as additional strategy adding to the already efficient automatedparticle picking routines. MAPPOS employs machine learning techniques to traina robust classifier from a small number of characteristic image features. Inorder to accurately quantify the performance of MAPPOS we used simulatedparticle and non-particle images. In addition, we verified our method byapplying it to an experimental cryo-EM dataset and comparing the results to themanual classification of the same dataset. Comparisons between MAPPOS andmanual post-picking classification by several human experts demonstrated thatmerely a few hundred sample images are sufficient for MAPPOS to classify anentire dataset with a human-like performance. MAPPOS was shown to greatlyaccelerate the throughput of large datasets by reducing the manual workload byorders of magnitude while maintaining a reliable identification of non-particleimages.
arxiv-2400-39 | A Practical Algorithm for Topic Modeling with Provable Guarantees | http://arxiv.org/abs/1212.4777 | author:Sanjeev Arora, Rong Ge, Yoni Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, Michael Zhu category:cs.LG cs.DS stat.ML published:2012-12-19 summary:Topic models provide a useful method for dimensionality reduction andexploratory data analysis in large text corpora. Most approaches to topic modelinference have been based on a maximum likelihood objective. Efficientalgorithms exist that approximate this objective, but they have no provableguarantees. Recently, algorithms have been introduced that provide provablebounds, but these algorithms are not practical because they are inefficient andnot robust to violations of model assumptions. In this paper we present analgorithm for topic model inference that is both provable and practical. Thealgorithm produces results comparable to the best MCMC implementations whilerunning orders of magnitude faster.
arxiv-2400-40 | Maximally Informative Observables and Categorical Perception | http://arxiv.org/abs/1212.5091 | author:Elaine Tsiang category:cs.LG cs.SD published:2012-12-19 summary:We formulate the problem of perception in the framework of informationtheory, and prove that categorical perception is equivalent to the existence ofan observable that has the maximum possible information on the target ofperception. We call such an observable maximally informative. Regardlesswhether categorical perception is real, maximally informative observables canform the basis of a theory of perception. We conclude with the implications ofsuch a theory for the problem of speech perception.
arxiv-2400-41 | Feature vector regularization in machine learning | http://arxiv.org/abs/1212.4569 | author:Yue Fan, Louise Raphael, Mark Kon category:stat.ML published:2012-12-19 summary:Problems in machine learning (ML) can involve noisy input data, and MLclassification methods have reached limiting accuracies when based on standardML data sets consisting of feature vectors and their classes. Greater accuracywill require incorporation of prior structural information on data intolearning. We study methods to regularize feature vectors (unsupervisedregularization methods), analogous to supervised regularization for estimatingfunctions in ML. We study regularization (denoising) of ML feature vectorsusing Tikhonov and other regularization methods for functions on ${\bf R}^n$. Afeature vector ${\bf x}=(x_1,\ldots,x_n)=\{x_q\}_{q=1}^n$ is viewed as afunction of its index $q$, and smoothed using prior information on itsstructure. This can involve a penalty functional on feature vectors analogousto those in statistical learning, or use of proximity (e.g. graph) structure onthe set of indices. Such feature vector regularization inherits a property fromfunction denoising on ${\bf R}^n$, in that accuracy is non-monotonic in thedenoising (regularization) parameter $\alpha$. Under some assumptions about thenoise level and the data structure, we show that the best reconstructionaccuracy also occurs at a finite positive $\alpha$ in index spaces with graphstructures. We adapt two standard function denoising methods used on ${\bfR}^n$, local averaging and kernel regression. In general the index space can beany discrete set with a notion of proximity, e.g. a metric space, a subset of${\bf R}^n$, or a graph/network, with feature vectors as functions with somenotion of continuity. We show this improves feature vector recovery, and thusthe subsequent classification or regression done on them. We give an example ingene expression analysis for cancer classification with the genome as an indexspace and network structure based protein-protein interactions.
arxiv-2400-42 | Towards common-sense reasoning via conditional simulation: legacies of Turing in Artificial Intelligence | http://arxiv.org/abs/1212.4799 | author:Cameron E. Freer, Daniel M. Roy, Joshua B. Tenenbaum category:cs.AI math.LO stat.ML published:2012-12-19 summary:The problem of replicating the flexibility of human common-sense reasoninghas captured the imagination of computer scientists since the early days ofAlan Turing's foundational work on computation and the philosophy of artificialintelligence. In the intervening years, the idea of cognition as computationhas emerged as a fundamental tenet of Artificial Intelligence (AI) andcognitive science. But what kind of computation is cognition? We describe a computational formalism centered around a probabilistic Turingmachine called QUERY, which captures the operation of probabilisticconditioning via conditional simulation. Through several examples and analyses,we demonstrate how the QUERY abstraction can be used to cast common-sensereasoning as probabilistic inference in a statistical model of our observationsand the uncertain structure of the world that generated that experience. Thisformulation is a recent synthesis of several research programs in AI andcognitive science, but it also represents a surprising convergence of severalof Turing's pioneering insights in AI, the foundations of computation, andstatistics.
arxiv-2400-43 | A genetic algorithm applied to the validation of building thermal models | http://arxiv.org/abs/1212.5250 | author:Alfred Jean Philippe Lauret, Harry Boyer, Carine Riviere, Alain Bastide category:cs.NE published:2012-12-18 summary:This paper presents the coupling of a building thermal simulation code withgenetic algorithms (GAs). GAs are randomized search algorithms that are basedon the mechanisms of natural selection and genetics. We show that this couplingallows the location of defective sub-models of a building thermal model i.e.parts of model that are responsible for the disagreements between measurementsand model predictions. The method first of all is checked and validated on thebasis of a numerical model of a building taken as reference. It is then appliedto a real building case. The results show that the method could constitute anefficient tool when checking the model validity.
arxiv-2400-44 | Variational Optimization | http://arxiv.org/abs/1212.4507 | author:Joe Staines, David Barber category:stat.ML cs.LG cs.NA 65K10 G.1.6 published:2012-12-18 summary:We discuss a general technique that can be used to form a differentiablebound on the optima of non-differentiable or discrete objective functions. Weform a unified description of these methods and consider under whichcircumstances the bound is concave. In particular we consider two concreteapplications of the method, namely sparse learning and support vectorclassification.
arxiv-2400-45 | Assessing Sentiment Strength in Words Prior Polarities | http://arxiv.org/abs/1212.4315 | author:Lorenzo Gatti, Marco Guerini category:cs.CL published:2012-12-18 summary:Many approaches to sentiment analysis rely on lexica where words are taggedwith their prior polarity - i.e. if a word out of context evokes somethingpositive or something negative. In particular, broad-coverage resources likeSentiWordNet provide polarities for (almost) every word. Since words can havemultiple senses, we address the problem of how to compute the prior polarity ofa word starting from the polarity of each sense and returning its polaritystrength as an index between -1 and 1. We compare 14 such formulae that appearin the literature, and assess which one best approximates the human judgementof prior polarities, with both regression and classification models.
arxiv-2400-46 | Sketch-to-Design: Context-based Part Assembly | http://arxiv.org/abs/1212.4490 | author:Xiaohua Xie, Kai Xu, Niloy J. Mitra, Daniel Cohen-Or, Baoquan Chen category:cs.GR cs.CV published:2012-12-18 summary:Designing 3D objects from scratch is difficult, especially when the userintent is fuzzy without a clear target form. In the spirit ofmodeling-by-example, we facilitate design by providing reference andinspiration from existing model contexts. We rethink model design as navigatingthrough different possible combinations of part assemblies based on a largecollection of pre-segmented 3D models. We propose an interactivesketch-to-design system, where the user sketches prominent features of parts tocombine. The sketched strokes are analyzed individually and in context with theother parts to generate relevant shape suggestions via a design galleryinterface. As the session progresses and more parts get selected, contextualcues becomes increasingly dominant and the system quickly converges to a finaldesign. As a key enabler, we use pre-learned part-based contextual informationto allow the user to quickly explore different combinations of parts. Ourexperiments demonstrate the effectiveness of our approach for efficientlydesigning new variations from existing shapes.
arxiv-2400-47 | Accelerated Time-of-Flight Mass Spectrometry | http://arxiv.org/abs/1212.4269 | author:Morteza Ibrahimi, Andrea Montanari, George S Moore category:math.OC cs.CE stat.ML published:2012-12-18 summary:We study a simple modification to the conventional time of flight massspectrometry (TOFMS) where a \emph{variable} and (pseudo)-\emph{random} pulsingrate is used which allows for traces from different pulses to overlap. Thismodification requires little alteration to the currently employed hardware.However, it requires a reconstruction method to recover the spectrum fromhighly aliased traces. We propose and demonstrate an efficient algorithm thatcan process massive TOFMS data using computational resources that can beconsidered modest with today's standards. This approach can be used to improveduty cycle, speed, and mass resolving power of TOFMS at the same time. Weexpect this to extend the applicability of TOFMS to new domains.
arxiv-2400-48 | A Multi-View Embedding Space for Modeling Internet Images, Tags, and their Semantics | http://arxiv.org/abs/1212.4522 | author:Yunchao Gong, Qifa Ke, Michael Isard, Svetlana Lazebnik category:cs.CV cs.IR cs.LG cs.MM published:2012-12-18 summary:This paper investigates the problem of modeling Internet images andassociated text or tags for tasks such as image-to-image search, tag-to-imagesearch, and image-to-tag search (image annotation). We start with canonicalcorrelation analysis (CCA), a popular and successful approach for mappingvisual and textual features to the same latent space, and incorporate a thirdview capturing high-level image semantics, represented either by a singlecategory or multiple non-mutually-exclusive concepts. We present two ways totrain the three-view embedding: supervised, with the third view coming fromground-truth labels or search keywords; and unsupervised, with semantic themesautomatically obtained by clustering the tags. To ensure high accuracy forretrieval tasks while keeping the learning process scalable, we combinemultiple strong visual features and use explicit nonlinear kernel mappings toefficiently approximate kernel CCA. To perform retrieval, we use a speciallydesigned similarity function in the embedded space, which substantiallyoutperforms the Euclidean distance. The resulting system produces compellingqualitative results and outperforms a number of two-view baselines on retrievaltasks on three large-scale Internet image datasets.
arxiv-2400-49 | GMM-Based Hidden Markov Random Field for Color Image and 3D Volume Segmentation | http://arxiv.org/abs/1212.4527 | author:Quan Wang category:cs.CV published:2012-12-18 summary:In this project, we first study the Gaussian-based hidden Markov random field(HMRF) model and its expectation-maximization (EM) algorithm. Then wegeneralize it to Gaussian mixture model-based hidden Markov random field. Thealgorithm is implemented in MATLAB. We also apply this algorithm to color imagesegmentation problems and 3D volume segmentation problems.
arxiv-2400-50 | Analysis of Large-scale Traffic Dynamics using Non-negative Tensor Factorization | http://arxiv.org/abs/1212.4675 | author:Yufei Han, Fabien Moutarde category:cs.LG published:2012-12-18 summary:In this paper, we present our work on clustering and prediction of temporaldynamics of global congestion configurations in large-scale road networks.Instead of looking into temporal traffic state variation of individual links,or of small areas, we focus on spatial congestion configurations of the wholenetwork. In our work, we aim at describing the typical temporal dynamicpatterns of this network-level traffic state and achieving long-term predictionof the large-scale traffic dynamics, in a unified data-mining framework. Tothis end, we formulate this joint task using Non-negative Tensor Factorization(NTF), which has been shown to be a useful decomposition tools for multivariatedata sequences. Clustering and prediction are performed based on the compacttensor factorization results. Experiments on large-scale simulated dataillustrate the interest of our method with promising results for long-termforecast of traffic evolution.
arxiv-2400-51 | Bayesian Group Nonnegative Matrix Factorization for EEG Analysis | http://arxiv.org/abs/1212.4347 | author:Bonggun Shin, Alice Oh category:cs.LG stat.ML published:2012-12-18 summary:We propose a generative model of a group EEG analysis, based on appropriatekernel assumptions on EEG data. We derive the variational inference update ruleusing various approximation techniques. The proposed model outperforms thecurrent state-of-the-art algorithms in terms of common pattern extraction. Thevalidity of the proposed model is tested on the BCI competition dataset.
arxiv-2400-52 | Co-clustering separately exchangeable network data | http://arxiv.org/abs/1212.4093 | author:David Choi, Patrick J. Wolfe category:math.ST cs.SI math.CO stat.ML stat.TH published:2012-12-17 summary:This article establishes the performance of stochastic blockmodels inaddressing the co-clustering problem of partitioning a binary array intosubsets, assuming only that the data are generated by a nonparametric processsatisfying the condition of separate exchangeability. We provide oracleinequalities with rate of convergence $\mathcal{O}_P(n^{-1/4})$ correspondingto profile likelihood maximization and mean-square error minimization, and showthat the blockmodel can be interpreted in this setting as an optimalpiecewise-constant approximation to the generative nonparametric model. We alsoshow for large sample sizes that the detection of co-clusters in such dataindicates with high probability the existence of co-clusters of equal size andasymptotically equivalent connectivity in the underlying generative process.
arxiv-2400-53 | Learning Markov Decision Processes for Model Checking | http://arxiv.org/abs/1212.3873 | author:Hua Mao, Yingke Chen, Manfred Jaeger, Thomas D. Nielsen, Kim G. Larsen, Brian Nielsen category:cs.LG cs.LO cs.SE published:2012-12-17 summary:Constructing an accurate system model for formal model verification can beboth resource demanding and time-consuming. To alleviate this shortcoming,algorithms have been proposed for automatically learning system models based onobserved system behaviors. In this paper we extend the algorithm on learningprobabilistic automata to reactive systems, where the observed system behavioris in the form of alternating sequences of inputs and outputs. We propose analgorithm for automatically learning a deterministic labeled Markov decisionprocess model from the observed behavior of a reactive system. The proposedlearning algorithm is adapted from algorithms for learning deterministicprobabilistic finite automata, and extended to include both probabilistic andnondeterministic transitions. The algorithm is empirically analyzed andevaluated by learning system models of slot machines. The evaluation isperformed by analyzing the probabilistic linear temporal logic properties ofthe system as well as by analyzing the schedulers, in particular the optimalschedulers, induced by the learned models.
arxiv-2400-54 | Group Component Analysis for Multi-block Data: Common and Individual Feature Extraction | http://arxiv.org/abs/1212.3913 | author:Guoxu Zhou, Andrzej Cichocki, Yu Zhang, Danilo Mandic category:cs.CV cs.LG published:2012-12-17 summary:Very often data we encounter in practice is a collection of matrices ratherthan a single matrix. These multi-block data are naturally linked and henceoften share some common features and at the same time they have their ownindividual features, due to the background in which they are measured andcollected. In this study we proposed a new scheme of common and individualfeature analysis (CIFA) that processes multi-block data in a linked way aimingat discovering and separating their common and individual features. Accordingto whether the number of common features is given or not, two efficientalgorithms were proposed to extract the common basis which is shared by alldata. Then feature extraction is performed on the common and the individualspaces separately by incorporating the techniques such as dimensionalityreduction and blind source separation. We also discussed how the proposed CIFAcan significantly improve the performance of classification and clusteringtasks by exploiting common and individual features of samples respectively. Ourexperimental results show some encouraging features of the proposed methods incomparison to the state-of-the-art methods on synthetic and real data.
arxiv-2400-55 | Feature Clustering for Accelerating Parallel Coordinate Descent | http://arxiv.org/abs/1212.4174 | author:Chad Scherrer, Ambuj Tewari, Mahantesh Halappanavar, David Haglin category:stat.ML cs.DC cs.LG math.OC published:2012-12-17 summary:Large-scale L1-regularized loss minimization problems arise inhigh-dimensional applications such as compressed sensing and high-dimensionalsupervised learning, including classification and regression problems.High-performance algorithms and implementations are critical to efficientlysolving these problems. Building upon previous work on coordinate descentalgorithms for L1-regularized problems, we introduce a novel family ofalgorithms called block-greedy coordinate descent that includes, as specialcases, several existing algorithms such as SCD, Greedy CD, Shotgun, andThread-Greedy. We give a unified convergence analysis for the family ofblock-greedy algorithms. The analysis suggests that block-greedy coordinatedescent can better exploit parallelism if features are clustered so that themaximum inner product between features in different blocks is small. Ourtheoretical convergence analysis is supported with experimental re- sults usingdata from diverse real-world applications. We hope that algorithmic approachesand convergence analysis we provide will not only advance the field, but willalso encourage researchers to systematically explore the design space ofalgorithms for solving large-scale L1-regularization problems.
arxiv-2400-56 | Alternating Maximization: Unifying Framework for 8 Sparse PCA Formulations and Efficient Parallel Codes | http://arxiv.org/abs/1212.4137 | author:Peter Richtárik, Martin Takáč, Selin Damla Ahipaşaoğlu category:stat.ML cs.LG math.OC published:2012-12-17 summary:Given a multivariate data set, sparse principal component analysis (SPCA)aims to extract several linear combinations of the variables that togetherexplain the variance in the data as much as possible, while controlling thenumber of nonzero loadings in these combinations. In this paper we consider 8different optimization formulations for computing a single sparse loadingvector; these are obtained by combining the following factors: we employ twonorms for measuring variance (L2, L1) and two sparsity-inducing norms (L0, L1),which are used in two different ways (constraint, penalty). Three of ourformulations, notably the one with L0 constraint and L1 variance, have not beenconsidered in the literature. We give a unifying reformulation which we proposeto solve via a natural alternating maximization (AM) method. We show the the AMmethod is nontrivially equivalent to GPower (Journ\'{e}e et al; JMLR11:517--553, 2010) for all our formulations. Besides this, we provide 24efficient parallel SPCA implementations: 3 codes (multi-core, GPU and cluster)for each of the 8 problems. Parallelism in the methods is aimed at i) speedingup computations (our GPU code can be 100 times faster than an efficient serialcode written in C++), ii) obtaining solutions explaining more variance and iii)dealing with big data problems (our cluster code is able to solve a 357 GBproblem in about a minute).
arxiv-2400-57 | A Tutorial on Probabilistic Latent Semantic Analysis | http://arxiv.org/abs/1212.3900 | author:Liangjie Hong category:stat.ML cs.LG published:2012-12-17 summary:In this tutorial, I will discuss the details about how Probabilistic LatentSemantic Analysis (PLSA) is formalized and how different learning algorithmsare proposed to learn the model.
arxiv-2400-58 | Visual Objects Classification with Sliding Spatial Pyramid Matching | http://arxiv.org/abs/1212.3767 | author:Hao Wooi Lim, Yong Haur Tay category:cs.CV published:2012-12-16 summary:We present a method for visual object classification using only a singlefeature, transformed color SIFT with a variant of Spatial Pyramid Matching(SPM) that we called Sliding Spatial Pyramid Matching (SSPM), trained with anensemble of linear regression (provided by LINEAR) to obtained state of the artresult on Caltech-101 of 83.46%. SSPM is a special version of SPM where insteadof dividing an image into K number of regions, a subwindow of fixed size isslide around the image with a fixed step size. For each subwindow, a histogramof visual words is generated. To obtained the visual vocabulary, instead ofperforming K-means clustering, we randomly pick N exemplars from the trainingset and encode them with a soft non-linear mapping method. We then trained 15models, each with a different visual word size with linear regression. All 15models are then averaged together to form a single strong model.
arxiv-2400-59 | Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees | http://arxiv.org/abs/1212.3850 | author:Nima Noorshams, Martin J. Wainwright category:cs.IT cs.LG math.IT stat.ML published:2012-12-16 summary:The sum-product or belief propagation (BP) algorithm is a widely usedmessage-passing technique for computing approximate marginals in graphicalmodels. We introduce a new technique, called stochastic orthogonal seriesmessage-passing (SOSMP), for computing the BP fixed point in models withcontinuous random variables. It is based on a deterministic approximation ofthe messages via orthogonal series expansion, and a stochastic approximationvia Monte Carlo estimates of the integral updates of the basis coefficients. Weprove that the SOSMP iterates converge to a \delta-neighborhood of the uniqueBP fixed point for any tree-structured graph, and for any graphs with cycles inwhich the BP updates satisfy a contractivity condition. In addition, wedemonstrate how to choose the number of basis coefficients as a function of thedesired approximation accuracy \delta and smoothness of the compatibilityfunctions. We illustrate our theory with both simulated examples and inapplication to optical flow estimation.
arxiv-2400-60 | Biologically Inspired Spiking Neurons : Piecewise Linear Models and Digital Implementation | http://arxiv.org/abs/1212.3765 | author:Hamid Soleimani, Arash Ahmadi, Mohammad Bavandpour category:cs.LG cs.NE q-bio.NC published:2012-12-16 summary:There has been a strong push recently to examine biological scale simulationsof neuromorphic algorithms to achieve stronger inference capabilities. Thispaper presents a set of piecewise linear spiking neuron models, which canreproduce different behaviors, similar to the biological neuron, both for asingle neuron as well as a network of neurons. The proposed models areinvestigated, in terms of digital implementation feasibility and costs,targeting large scale hardware implementation. Hardware synthesis and physicalimplementations on FPGA show that the proposed models can produce preciseneural behaviors with higher performance and considerably lower implementationcosts compared with the original model. Accordingly, a compact structure of themodels which can be trained with supervised and unsupervised learningalgorithms has been developed. Using this structure and based on a spike ratecoding, a character recognition case study has been implemented and tested.
arxiv-2400-61 | A metric for software vulnerabilities classification | http://arxiv.org/abs/1212.3669 | author:Gabriele Modena category:cs.SE cs.LG published:2012-12-15 summary:Vulnerability discovery and exploits detection are two wide areas of study insoftware engineering. This preliminary work tries to combine existing methodswith machine learning techniques to define a metric classification ofvulnerable computer programs. First a feature set has been defined and latertwo models have been tested against real world vulnerabilities. A relationbetween the classifier choice and the features has also been outlined.
arxiv-2400-62 | Machine Learning in Proof General: Interfacing Interfaces | http://arxiv.org/abs/1212.3618 | author:Ekaterina Komendantskaya, Jónathan Heras, Gudmund Grov category:cs.AI cs.LG cs.LO I.2.6; F.4.1 published:2012-12-14 summary:We present ML4PG - a machine learning extension for Proof General. It allowsusers to gather proof statistics related to shapes of goals, sequences ofapplied tactics, and proof tree structures from the libraries of interactivehigher-order proofs written in Coq and SSReflect. The gathered data isclustered using the state-of-the-art machine learning algorithms available inMATLAB and Weka. ML4PG provides automated interfacing between Proof General andMATLAB/Weka. The results of clustering are used by ML4PG to provide proof hintsin the process of interactive proof development.
arxiv-2400-63 | A Multi-Orientation Analysis Approach to Retinal Vessel Tracking | http://arxiv.org/abs/1212.3530 | author:Erik Bekkers, Remco Duits, Tos Berendschot, Bart ter Haar Romeny category:cs.CV published:2012-12-14 summary:This paper presents a method for retinal vasculature extraction based onbiologically inspired multi-orientation analysis. We apply multi-orientationanalysis via so-called invertible orientation scores, modeling the corticalcolumns in the visual system of higher mammals. This allows us to genericallydeal with many hitherto complex problems inherent to vessel tracking, such ascrossings, bifurcations, parallel vessels, vessels of varying widths andvessels with high curvature. Our approach applies tracking in invertibleorientation scores via a novel geometrical principle for curve optimization inthe Euclidean motion group SE(2). The method runs fully automatically andprovides a detailed model of the retinal vasculature, which is crucial as asound basis for further quantitative analysis of the retina, especially inscreening applications.
arxiv-2400-64 | Sentence Compression in Spanish driven by Discourse Segmentation and Language Models | http://arxiv.org/abs/1212.3493 | author:Alejandro Molina, Juan-Manuel Torres-Moreno, Iria da Cunha, Eric SanJuan, Gerardo Sierra category:cs.CL cs.IR published:2012-12-14 summary:Previous works demonstrated that Automatic Text Summarization (ATS) bysentences extraction may be improved using sentence compression. In this workwe present a sentence compressions approach guided by level-sentence discoursesegmentation and probabilistic language models (LM). The results presented hereshow that the proposed solution is able to generate coherent summaries withgrammatical compressed sentences. The approach is simple enough to betransposed into other languages.
arxiv-2400-65 | A Novel Directional Weighted Minimum Deviation (DWMD) Based Filter for Removal of Random Valued Impulse Noise | http://arxiv.org/abs/1212.3373 | author:J. K. Mandal, Somnath Mukhopadhyay category:cs.CV published:2012-12-14 summary:The most median-based de noising methods works fine for restoring the imagescorrupted by Randomn Valued Impulse Noise with low noise level but very poorwith highly corrupted images. In this paper a directional weighted minimumdeviation (DWMD) based filter has been proposed for removal of high randomvalued impulse noise (RVIN). The proposed approach based on Standard Deviation(SD) works in two phases. The first phase detects the contaminated pixels bydifferencing between the test pixel and its neighbor pixels aligned with fourmain directions. The second phase filters only those pixels keeping othersintact. The filtering scheme is based on minimum standard deviation of the fourdirectional pixels. Extensive simulations show that the proposed filter notonly provide better performance of de noising RVIN but can preserve moredetails features even thin lines or dots. This technique shows betterperformance in terms of PSNR, Image Fidelity and Computational Cost compared tothe existing filters.
arxiv-2400-66 | Know Your Personalization: Learning Topic level Personalization in Online Services | http://arxiv.org/abs/1212.3390 | author:Anirban Majumder, Nisheeth Shrivastava category:cs.LG cs.IR published:2012-12-14 summary:Online service platforms (OSPs), such as search engines, news-websites,ad-providers, etc., serve highly pe rsonalized content to the user, based onthe profile extracted from his history with the OSP. Although personalization(generally) leads to a better user experience, it also raises privacy concernsfor the user---he does not know what is present in his profile and moreimportantly, what is being used to per sonalize content for him. In this paper,we capture OSP's personalization for an user in a new data structure called theperson alization vector ($\eta$), which is a weighted vector over a set oftopics, and present techniques to compute it for users of an OSP. Our approachtreats OSPs as black-boxes, and extracts $\eta$ by mining only their output,specifical ly, the personalized (for an user) and vanilla (without any userinformation) contents served, and the differences in these content. Weformulate a new model called Latent Topic Personalization (LTP) that capturesthe personalization vector into a learning framework and present efficientinference algorithms for it. We do extensive experiments for search resultpersonalization using both data from real Google users and synthetic datasets.Our results show high accuracy (R-pre = 84%) of LTP in finding personalizedtopics. For Google data, our qualitative results show how LTP can alsoidentifies evidences---queries for results on a topic with high $\eta$ valuewere re-ranked. Finally, we show how our approach can be used to build a newPrivacy evaluation framework focused at end-user privacy on commercial OSPs.
arxiv-2400-67 | Approximating rational Bezier curves by constrained Bezier curves of arbitrary degree | http://arxiv.org/abs/1212.3385 | author:Mao Shi, Jiansong Deng category:math.NA cs.CV published:2012-12-14 summary:In this paper, we propose a method to obtain a constrained approximation of arational B\'{e}zier curve by a polynomial B\'{e}zier curve. This problem isreformulated as an approximation problem between two polynomial B\'{e}ziercurves based on weighted least-squares method, where weight functions$\rho(t)=\omega(t)$ and $\rho(t)=\omega(t)^{2}$ are studied respectively. Theefficiency of the proposed method is tested using some examples.
arxiv-2400-68 | Evolution of Plastic Learning in Spiking Networks via Memristive Connections | http://arxiv.org/abs/1212.3441 | author:Gerard Howard, Ella Gale, Larry Bull, Ben de Lacy Costello, Andy Adamatzky category:cs.ET cs.NE published:2012-12-14 summary:This article presents a spiking neuroevolutionary system which implementsmemristors as plastic connections, i.e. whose weights can vary during a trial.The evolutionary design process exploits parameter self-adaptation and variabletopologies, allowing the number of neurons, connection weights, andinter-neural connectivity pattern to emerge. By comparing two phenomenologicalreal-world memristor implementations with networks comprised of (i) linearresistors (ii) constant-valued connections, we demonstrate that this approachallows the evolution of networks of appropriate complexity to emerge whilstexploiting the memristive properties of the connections to reduce learningtime. We extend this approach to allow for heterogeneous mixtures of memristorswithin the networks; our approach provides an in-depth analysis of networkstructure. Our networks are evaluated on simulated robotic navigation tasks;results demonstrate that memristive plasticity enables higher performance thanconstant-weighted connections in both static and dynamic reward scenarios, andthat mixtures of memristive elements provide performance advantages whencompared to homogeneous memristive networks.
arxiv-2400-69 | Proceedings Quantities in Formal Methods | http://arxiv.org/abs/1212.3454 | author:Uli Fahrenberg, Axel Legay, Claus Thrane category:cs.LO cs.FL cs.LG cs.SE published:2012-12-14 summary:This volume contains the proceedings of the Workshop on Quantities in FormalMethods, QFM 2012, held in Paris, France on 28 August 2012. The workshop wasaffiliated with the 18th Symposium on Formal Methods, FM 2012. The focus of theworkshop was on quantities in modeling, verification, and synthesis. Modernapplications of formal methods require to reason formally on quantities such astime, resources, or probabilities. Standard formal methods and tools havegotten very good at modeling (and verifying) qualitative properties: whether ornot certain events will occur. During the last years, these methods and toolshave been extended to also cover quantitative aspects, notably leading to toolslike e.g. UPPAAL (for real-time systems), PRISM (for probabilistic systems),and PHAVer (for hybrid systems). A lot of work remains to be done howeverbefore these tools can be used in the industrial applications at which they areaiming.
arxiv-2400-70 | A comparative study of root-based and stem-based approaches for measuring the similarity between arabic words for arabic text mining applications | http://arxiv.org/abs/1212.3634 | author:Hanane Froud, Abdelmonaim Lachkar, Said Alaoui Ouatik category:cs.CL cs.IR published:2012-12-14 summary:Representation of semantic information contained in the words is needed forany Arabic Text Mining applications. More precisely, the purpose is to bettertake into account the semantic dependencies between words expressed by theco-occurrence frequencies of these words. There have been many proposals tocompute similarities between words based on their distributions in contexts. Inthis paper, we compare and contrast the effect of two preprocessing techniquesapplied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming)approaches for measuring the similarity between Arabic words with the wellknown abstractive model -Latent Semantic Analysis (LSA)- with a wide variety ofdistance functions and similarity measures, such as the Euclidean Distance,Cosine Similarity, Jaccard Coefficient, and the Pearson CorrelationCoefficient. The obtained results show that, on the one hand, the variety ofthe corpus produces more accurate results; on the other hand, the Stem-basedapproach outperformed the Root-based one because this latter affects the wordsmeanings.
arxiv-2400-71 | Learning efficient sparse and low rank models | http://arxiv.org/abs/1212.3631 | author:Pablo Sprechmann, Alex M. Bronstein, Guillermo Sapiro category:cs.LG published:2012-12-14 summary:Parsimony, including sparsity and low rank, has been shown to successfullymodel data in numerous machine learning and signal processing tasks.Traditionally, such modeling approaches rely on an iterative algorithm thatminimizes an objective function with parsimony-promoting terms. The inherentlysequential structure and data-dependent complexity and latency of iterativeoptimization constitute a major limitation in many applications requiringreal-time performance or involving large-scale data. Another limitationencountered by these modeling techniques is the difficulty of their inclusionin discriminative learning scenarios. In this work, we propose to move theemphasis from the model to the pursuit algorithm, and develop a process-centricview of parsimonious modeling, in which a learned deterministicfixed-complexity pursuit process is used in lieu of iterative optimization. Weshow a principled way to construct learnable pursuit process architectures forstructured sparse and robust low rank models, derived from the iteration ofproximal descent algorithms. These architectures learn to approximate the exactparsimonious representation at a fraction of the complexity of the standardoptimization methods. We also show that appropriate training regimes allow tonaturally extend parsimonious models to discriminative settings.State-of-the-art results are demonstrated on several challenging problems inimage and audio processing with several orders of magnitude speedup compared tothe exact optimization algorithms.
arxiv-2400-72 | Identification of Nonlinear Systems From the Knowledge Around Different Operating Conditions: A Feed-Forward Multi-Layer ANN Based Approach | http://arxiv.org/abs/1212.3225 | author:Sayan Saha, Saptarshi Das, Anish Acharya, Abhishek Kumar, Sumit Mukherjee, Indranil Pan, Amitava Gupta category:cs.SY cs.NE published:2012-12-13 summary:The paper investigates nonlinear system identification using system outputdata at various linearized operating points. A feed-forward multi-layerArtificial Neural Network (ANN) based approach is used for this purpose andtested for two target applications i.e. nuclear reactor power level monitoringand an AC servo position control system. Various configurations of ANN usingdifferent activation functions, number of hidden layers and neurons in eachlayer are trained and tested to find out the best configuration. The trainingis carried out multiple times to check for consistency and the mean andstandard deviation of the root mean square errors (RMSE) are reported for eachconfiguration.
arxiv-2400-73 | Identifying Metaphoric Antonyms in a Corpus Analysis of Finance Articles | http://arxiv.org/abs/1212.3139 | author:Aaron Gerow, Mark Keane category:cs.CL published:2012-12-13 summary:Using a corpus of 17,000+ financial news reports (involving over 10M words),we perform an analysis of the argument-distributions of the UP and DOWN verbsused to describe movements of indices, stocks and shares. In Study 1participants identified antonyms of these verbs in a free-response task and amatching task from which the most commonly identified antonyms were compiled.In Study 2, we determined whether the argument-distributions for the verbs inthese antonym-pairs were sufficiently similar to predict the mostfrequently-identified antonym. Cosine similarity correlates moderately with theproportions of antonym-pairs identified by people (r = 0.31). Moreimpressively, 87% of the time the most frequently-identified antonym is eitherthe first- or second-most similar pair in the set of alternatives. Theimplications of these results for distributional approaches to determiningmetaphoric knowledge are discussed.
arxiv-2400-74 | Robust image reconstruction from multi-view measurements | http://arxiv.org/abs/1212.3268 | author:Gilles Puy, Pierre Vandergheynst category:cs.CV published:2012-12-13 summary:We propose a novel method to accurately reconstruct a set of imagesrepresenting a single scene from few linear multi-view measurements. Eachobserved image is modeled as the sum of a background image and a foregroundone. The background image is common to all observed images but undergoesgeometric transformations, as the scene is observed from different viewpoints.In this paper, we assume that these geometric transformations are representedby a few parameters, e.g., translations, rotations, affine transformations,etc.. The foreground images differ from one observed image to another, and areused to model possible occlusions of the scene. The proposed reconstructionalgorithm estimates jointly the images and the transformation parameters fromthe available multi-view measurements. The ideal solution of this multi-viewimaging problem minimizes a non-convex functional, and the reconstructiontechnique is an alternating descent method built to minimize this functional.The convergence of the proposed algorithm is studied, and conditions underwhich the sequence of estimated images and parameters converges to a criticalpoint of the non-convex functional are provided. Finally, the efficiency of thealgorithm is demonstrated using numerical simulations for applications such ascompressed sensing or super-resolution.
arxiv-2400-75 | Cost-Sensitive Feature Selection of Data with Errors | http://arxiv.org/abs/1212.3185 | author:Hong Zhao, Fan Min, William Zhu category:cs.LG published:2012-12-13 summary:In data mining applications, feature selection is an essential process sinceit reduces a model's complexity. The cost of obtaining the feature values mustbe taken into consideration in many domains. In this paper, we study thecost-sensitive feature selection problem on numerical data with measurementerrors, test costs and misclassification costs. The major contributions of thispaper are four-fold. First, a new data model is built to address test costs andmisclassification costs as well as error boundaries. Second, a covering-basedrough set with measurement errors is constructed. Given a confidence interval,the neighborhood is an ellipse in a two-dimension space, or an ellipsoidal in athree-dimension space, etc. Third, a new cost-sensitive feature selectionproblem is defined on this covering-based rough set. Fourth, both backtrackingand heuristic algorithms are proposed to deal with this new problem. Thealgorithms are tested on six UCI (University of California - Irvine) data sets.Experimental results show that (1) the pruning techniques of the backtrackingalgorithm help reducing the number of operations significantly, and (2) theheuristic algorithm usually obtains optimal results. This study is a steptoward realistic applications of cost-sensitive learning.
arxiv-2400-76 | Integrating Prior Knowledge Into Prognostic Biomarker Discovery based on Network Structure | http://arxiv.org/abs/1212.3214 | author:Yupeng Cun, Holger Fröhlich category:q-bio.GN stat.ML published:2012-12-13 summary:Background: Predictive, stable and interpretable gene signatures aregenerally seen as an important step towards a better personalized medicine.During the last decade various methods have been proposed for that purpose.However, one important obstacle for making gene signatures a standard tool inclinics is the typical low reproducibility of these signatures combined withthe difficulty to achieve a clear biological interpretation. For that purposein the last years there has been a growing interest in approaches that try tointegrate information from molecular interaction networks. Results: We proposea novel algorithm, called FrSVM, which integrates protein-protein interactionnetwork information into gene selection for prognostic biomarker discovery. Ourmethod is a simple filter based approach, which focuses on central genes withlarge differences in their expression. Compared to several other competingmethods our algorithm reveals a significantly better prediction performance andhigher signature stability. More- over, obtained gene lists are highly enrichedwith known disease genes and drug targets. We extendd our approach further byintegrating information on candidate disease genes and targets of diseaseassociated Transcript Factors (TFs).
arxiv-2400-77 | Learning Sparse Low-Threshold Linear Classifiers | http://arxiv.org/abs/1212.3276 | author:Sivan Sabato, Shai Shalev-Shwartz, Nathan Srebro, Daniel Hsu, Tong Zhang category:stat.ML cs.LG published:2012-12-13 summary:We consider the problem of learning a non-negative linear classifier with a$1$-norm of at most $k$, and a fixed threshold, under the hinge-loss. Thisproblem generalizes the problem of learning a $k$-monotone disjunction. Weprove that we can learn efficiently in this setting, at a rate which is linearin both $k$ and the size of the threshold, and that this is the best possiblerate. We provide an efficient online learning algorithm that achieves theoptimal rate, and show that in the batch case, empirical risk minimizationachieves this rate as well. The rates we show are tighter than the uniformconvergence rate, which grows with $k^2$.
arxiv-2400-78 | Identifying Metaphor Hierarchies in a Corpus Analysis of Finance Articles | http://arxiv.org/abs/1212.3138 | author:Aaron Georw, Mark Keane category:cs.CL published:2012-12-13 summary:Using a corpus of over 17,000 financial news reports (involving over 10Mwords), we perform an analysis of the argument-distributions of the UP- andDOWN-verbs used to describe movements of indices, stocks, and shares. Usingmeasures of the overlap in the argument distributions of these verbs andk-means clustering of their distributions, we advance evidence for the proposalthat the metaphors referred to by these verbs are organised into hierarchicalstructures of superordinate and subordinate groups.
arxiv-2400-79 | Multi-target tracking algorithms in 3D | http://arxiv.org/abs/1212.3034 | author:Rastislav Telgarsky category:cs.CV cs.DM 65D18, 68W05 published:2012-12-13 summary:Ladars provide a unique capability for identification of objects and motionsin scenes with fixed 3D field of view (FOV). This paper describes algorithmsfor multi-target tracking in 3D scenes including the preprocessing(mathematical morphology and Parzen windows), labeling of connected components,sorting of targets by selectable attributes (size, length of track, velocity),and handling of target states (acquired, coasting, re-acquired and tracked) inorder to assemble the target trajectories. This paper is derived from workingalgorithms coded in Matlab, which were tested and reviewed by others, and doesnot speculate about usage of general formulas or frameworks.
arxiv-2400-80 | Multifractal analysis of sentence lengths in English literary texts | http://arxiv.org/abs/1212.3171 | author:Iwona Grabska-Gradzińska, Andrzej Kulig, Jarosław Kwapień, Paweł Oświęcimka, Stanisław Drożdż category:cs.CL physics.soc-ph published:2012-12-13 summary:This paper presents analysis of 30 literary texts written in English bydifferent authors. For each text, there were created time series representinglength of sentences in words and analyzed its fractal properties using twomethods of multifractal analysis: MFDFA and WTMM. Both methods showed thatthere are texts which can be considered multifractal in this representation buta majority of texts are not multifractal or even not fractal at all. Out of 30books, only a few have so-correlated lengths of consecutive sentences that theanalyzed signals can be interpreted as real multifractals. An interestingdirection for future investigations would be identifying what are the specificfeatures which cause certain texts to be multifractal and other to bemonofractal or even not fractal at all.
arxiv-2400-81 | Keyword Extraction for Identifying Social Actors | http://arxiv.org/abs/1212.3023 | author:Mahyuddin K. M. Nasution, Shahrul Azman Mohd Noah category:cs.IR cs.CL published:2012-12-13 summary:Identifying the social actor has become one of tasks in ArtificialIntelligence, whereby extracting keyword from Web snippets depend on the use ofweb is steadily gaining ground in this research. We develop therefore anapproach based on overlap principle for utilizing a collection of features inweb snippets, where use of keyword will eliminate the un-relevant web pages.
arxiv-2400-82 | Diachronic Variation in Grammatical Relations | http://arxiv.org/abs/1212.3162 | author:Aaron Gerow, Khurshid Ahmad category:cs.CL published:2012-12-13 summary:We present a method of finding and analyzing shifts in grammatical relationsfound in diachronic corpora. Inspired by the econometric technique of measuringreturn and volatility instead of relative frequencies, we propose them as a wayto better characterize changes in grammatical patterns like nominalization,modification and comparison. To exemplify the use of these techniques, weexamine a corpus of NIPS papers and report trends which manifest at the token,part-of-speech and grammatical levels. Building up from frequency observationsto a second-order analysis, we show that shifts in frequencies overlook deepertrends in language, even when part-of-speech information is included. Examiningtoken, POS and grammatical levels of variation enables a summary view ofdiachronic text as a whole. We conclude with a discussion about how thesemethods can inform intuitions about specialist domains as well as changes inlanguage use as a whole.
arxiv-2400-83 | Dictionary Subselection Using an Overcomplete Joint Sparsity Model | http://arxiv.org/abs/1212.2834 | author:Mehrdad Yaghoobi, Laurent Daudet, Michael E. Davies category:cs.LG math.OC stat.ML published:2012-12-12 summary:Many natural signals exhibit a sparse representation, whenever a suitabledescribing model is given. Here, a linear generative model is considered, wheremany sparsity-based signal processing techniques rely on such a simplifiedmodel. As this model is often unknown for many classes of the signals, we needto select such a model based on the domain knowledge or using some exemplarsignals. This paper presents a new exemplar based approach for the linear model(called the dictionary) selection, for such sparse inverse problems. Theproblem of dictionary selection, which has also been called the dictionarylearning in this setting, is first reformulated as a joint sparsity model. Thejoint sparsity model here differs from the standard joint sparsity model as itconsiders an overcompleteness in the representation of each signal, within therange of selected subspaces. The new dictionary selection paradigm is examinedwith some synthetic and realistic simulations.
arxiv-2400-84 | An MDP-based Recommender System | http://arxiv.org/abs/1301.0600 | author:Guy Shani, Ronen I. Brafman, David Heckerman category:cs.LG cs.AI cs.IR published:2012-12-12 summary:Typical Recommender systems adopt a static view of the recommendation processand treat it as a prediction problem. We argue that it is more appropriate toview the problem of generating recommendations as a sequential decision problemand, consequently, that Markov decision processes (MDP) provide a moreappropriate model for Recommender systems. MDPs introduce two benefits: theytake into account the long-term effects of each recommendation, and they takeinto account the expected value of each recommendation. To succeed in practice,an MDP-based Recommender system must employ a strong initial model; and thebulk of this paper is concerned with the generation of such a model. Inparticular, we suggest the use of an n-gram predictive model for generating theinitial MDP. Our n-gram model induces a Markov-chain model of user behaviorwhose predictive accuracy is greater than that of existing predictive models.We describe our predictive model in detail and evaluate its performance on realdata. In addition, we show how the model can be used in an MDP-basedRecommender system.
arxiv-2400-85 | Pituitary Adenoma Volumetry with 3D Slicer | http://arxiv.org/abs/1212.2860 | author:Jan Egger, Tina Kapur, Christopher Nimsky, Ron Kikinis category:cs.CV published:2012-12-12 summary:In this study, we present pituitary adenoma volumetry using the free and opensource medical image computing platform for biomedical research: (3D) Slicer.Volumetric changes in cerebral pathologies like pituitary adenomas are acritical factor in treatment decisions by physicians and in general the volumeis acquired manually. Therefore, manual slice-by-slice segmentations inmagnetic resonance imaging (MRI) data, which have been obtained at regularintervals, are performed. In contrast to this manual time consumingslice-by-slice segmentation process Slicer is an alternative which can besignificantly faster and less user intensive. In this contribution, we comparepure manual segmentations of ten pituitary adenomas with semi-automaticsegmentations under Slicer. Thus, physicians drew the boundaries completelymanually on a slice-by-slice basis and performed a Slicer-enhanced segmentationusing the competitive region-growing based module of Slicer named GrowCut.Results showed that the time and user effort required for GrowCut-basedsegmentations were on average about thirty percent less than the pure manualsegmentations. Furthermore, we calculated the Dice Similarity Coefficient (DSC)between the manual and the Slicer-based segmentations to proof that the two arecomparable yielding an average DSC of 81.97\pm3.39%.
arxiv-2400-86 | Optimal Time Bounds for Approximate Clustering | http://arxiv.org/abs/1301.0587 | author:Ramgopal Mettu, Greg Plaxton category:cs.DS cs.LG stat.ML published:2012-12-12 summary:Clustering is a fundamental problem in unsupervised learning, and has beenstudied widely both as a problem of learning mixture models and as anoptimization problem. In this paper, we study clustering with respect theemph{k-median} objective function, a natural formulation of clustering in whichwe attempt to minimize the average distance to cluster centers. One of the maincontributions of this paper is a simple but powerful sampling technique that wecall emph{successive sampling} that could be of independent interest. We showthat our sampling procedure can rapidly identify a small set of points (of sizejust O(klog{n/k})) that summarize the input points for the purpose ofclustering. Using successive sampling, we develop an algorithm for the k-medianproblem that runs in O(nk) time for a wide range of values of k and isguaranteed, with high probability, to return a solution with cost at most aconstant factor times optimal. We also establish a lower bound of Omega(nk) onany randomized constant-factor approximation algorithm for the k-median problemthat succeeds with even a negligible (say 1/100) probability. Thus we establisha tight time bound of Theta(nk) for the k-median problem for a wide range ofvalues of k. The best previous upper bound for the problem was O(nk), where theO-notation hides polylogarithmic factors in n and k. The best previous lowerbound of O(nk) applied only to deterministic k-median algorithms. While wefocus our presentation on the k-median objective, all our upper bounds arevalid for the k-means objective as well. In this context our algorithm comparesfavorably to the widely used k-means heuristic, which requires O(nk) time forjust one iteration and provides no useful approximation guarantees.
arxiv-2400-87 | Accelerating Inference: towards a full Language, Compiler and Hardware stack | http://arxiv.org/abs/1212.2991 | author:Shawn Hershey, Jeff Bernstein, Bill Bradley, Andrew Schweitzer, Noah Stein, Theo Weber, Ben Vigoda category:cs.SE cs.AI stat.ML published:2012-12-12 summary:We introduce Dimple, a fully open-source API for probabilistic modeling.Dimple allows the user to specify probabilistic models in the form of graphicalmodels, Bayesian networks, or factor graphs, and performs inference (byautomatically deriving an inference engine from a variety of algorithms) on themodel. Dimple also serves as a compiler for GP5, a hardware accelerator forinference.
arxiv-2400-88 | Discriminative Probabilistic Models for Relational Data | http://arxiv.org/abs/1301.0604 | author:Ben Taskar, Pieter Abbeel, Daphne Koller category:cs.LG cs.AI stat.ML published:2012-12-12 summary:In many supervised learning tasks, the entities to be labeled are related toeach other in complex ways and their labels are not independent. For example,in hypertext classification, the labels of linked pages are highly correlated.A standard approach is to classify each entity independently, ignoring thecorrelations between them. Recently, Probabilistic Relational Models, arelational version of Bayesian networks, were used to define a jointprobabilistic model for a collection of related entities. In this paper, wepresent an alternative framework that builds on (conditional) Markov networksand addresses two limitations of the previous approach. First, undirectedmodels do not impose the acyclicity constraint that hinders representation ofmany important relational dependencies in directed models. Second, undirectedmodels are well suited for discriminative training, where we optimize theconditional likelihood of the labels given the features, which generallyimproves classification accuracy. We show how to train these modelseffectively, and how to use approximate probabilistic inference over thelearned model for collective classification of multiple related entities. Weprovide experimental results on a webpage classification task, showing thataccuracy can be significantly improved by modeling relational dependencies.
arxiv-2400-89 | IPF for Discrete Chain Factor Graphs | http://arxiv.org/abs/1301.0613 | author:Wim Wiegerinck, Tom Heskes category:cs.LG cs.AI stat.ML published:2012-12-12 summary:Iterative Proportional Fitting (IPF), combined with EM, is commonly used asan algorithm for likelihood maximization in undirected graphical models. Inthis paper, we present two iterative algorithms that generalize upon IPF. Thefirst one is for likelihood maximization in discrete chain factor graphs, whichwe define as a wide class of discrete variable models including undirectedgraphical models and Bayesian networks, but also chain graphs and sigmoidbelief networks. The second one is for conditional likelihood maximization instandard undirected models and Bayesian networks. In both algorithms, theiteration steps are expressed in closed form. Numerical simulations show thatthe algorithms are competitive with state of the art methods.
arxiv-2400-90 | Unsupervised Active Learning in Large Domains | http://arxiv.org/abs/1301.0602 | author:Harald Steck, Tommi S. Jaakkola category:cs.LG stat.ML published:2012-12-12 summary:Active learning is a powerful approach to analyzing data effectively. We showthat the feasibility of active learning depends crucially on the choice ofmeasure with respect to which the query is being optimized. The standardinformation gain, for example, does not permit an accurate evaluation with asmall committee, a representative subset of the model space. We propose asurrogate measure requiring only a small committee and discuss the propertiesof this new measure. We devise, in addition, a bootstrap approach for committeeselection. The advantages of this approach are illustrated in the context ofrecovering (regulatory) network models.
arxiv-2400-91 | Adaptive Foreground and Shadow Detection inImage Sequences | http://arxiv.org/abs/1301.0612 | author:Yang Wang, Tele Tan category:cs.CV published:2012-12-12 summary:This paper presents a novel method of foreground segmentation thatdistinguishes moving objects from their moving cast shadows in monocular imagesequences. The models of background, edge information, and shadow are set upand adaptively updated. A Bayesian belief network is proposed to describe therelationships among the segmentation label, background, intensity, and edgeinformation. The notion of Markov random field is used to encourage the spatialconnectivity of the segmented regions. The solution is obtained by maximizingthe posterior possibility density of the segmentation field.
arxiv-2400-92 | Reinforcement Learning with Partially Known World Dynamics | http://arxiv.org/abs/1301.0601 | author:Christian R. Shelton category:cs.LG stat.ML published:2012-12-12 summary:Reinforcement learning would enjoy better success on real-world problems ifdomain knowledge could be imparted to the algorithm by the modelers. Mostproblems have both hidden state and unknown dynamics. Partially observableMarkov decision processes (POMDPs) allow for the modeling of both.Unfortunately, they do not provide a natural framework in which to specifyknowledge about the domain dynamics. The designer must either admit to knowingnothing about the dynamics or completely specify the dynamics (thereby turningit into a planning problem). We propose a new framework called a partiallyknown Markov decision process (PKMDP) which allows the designer to specifyknown dynamics while still leaving portions of the environment s dynamicsunknown.The model represents NOT ONLY the environment dynamics but also theagents knowledge of the dynamics. We present a reinforcement learning algorithmfor this model based on importance sampling. The algorithm incorporatesplanning based on the known dynamics and learning about the unknown dynamics.Our results clearly demonstrate the ability to add domain knowledge and theresulting benefits for learning.
arxiv-2400-93 | Advances in Boosting (Invited Talk) | http://arxiv.org/abs/1301.0599 | author:Robert E. Schapire category:cs.LG stat.ML published:2012-12-12 summary:Boosting is a general method of generating many simple classification rulesand combining them into a single, highly accurate rule. In this talk, I willreview the AdaBoost boosting algorithm and some of its underlying theory, andthen look at how this theory has helped us to face some of the challenges ofapplying AdaBoost in two domains: In the first of these, we used boosting forpredicting and modeling the uncertainty of prices in complicated, interactingauctions. The second application was to the classification of caller utterancesin a telephone spoken-dialogue system where we faced two challenges: the needto incorporate prior knowledge to compensate for initially insufficient data;and a later need to filter the large stream of unlabeled examples beingcollected to select the ones whose labels are likely to be most informative.
arxiv-2400-94 | Staged Mixture Modelling and Boosting | http://arxiv.org/abs/1301.0586 | author:Christopher Meek, Bo Thiesson, David Heckerman category:cs.LG stat.ML published:2012-12-12 summary:In this paper, we introduce and evaluate a data-driven staged mixturemodeling technique for building density, regression, and classification models.Our basic approach is to sequentially add components to a finite mixture modelusing the structural expectation maximization (SEM) algorithm. We show that ourtechnique is qualitatively similar to boosting. This correspondence is anatural byproduct of the fact that we use the SEM algorithm to sequentially fitthe mixture model. Finally, in our experimental evaluation, we demonstrate theeffectiveness of our approach on a variety of prediction and density estimationtasks using real-world data.
arxiv-2400-95 | Asymptotic Model Selection for Naive Bayesian Networks | http://arxiv.org/abs/1301.0598 | author:Dmitry Rusakov, Dan Geiger category:cs.AI cs.LG published:2012-12-12 summary:We develop a closed form asymptotic formula to compute the marginallikelihood of data given a naive Bayesian network model with two hidden statesand binary features. This formula deviates from the standard BIC score. Ourwork provides a concrete example that the BIC score is generally not valid forstatistical models that belong to a stratified exponential family. This standsin contrast to linear and curved exponential families, where the BIC score hasbeen proven to provide a correct approximation for the marginal likelihood.
arxiv-2400-96 | Decayed MCMC Filtering | http://arxiv.org/abs/1301.0584 | author:Bhaskara Marthi, Hanna Pasula, Stuart Russell, Yuval Peres category:cs.AI cs.LG cs.SY published:2012-12-12 summary:Filtering---estimating the state of a partially observable Markov processfrom a sequence of observations---is one of the most widely studied problems incontrol theory, AI, and computational statistics. Exact computation of theposterior distribution is generally intractable for large discrete systems andfor nonlinear continuous systems, so a good deal of effort has gone intodeveloping robust approximation algorithms. This paper describes a simplestochastic approximation algorithm for filtering called {em decayed MCMC}. Thealgorithm applies Markov chain Monte Carlo sampling to the space of statetrajectories using a proposal distribution that favours flips of more recentstate variables. The formal analysis of the algorithm involves a generalizationof standard coupling arguments for MCMC convergence. We prove that for anyergodic underlying Markov process, the convergence time of decayed MCMC withinverse-polynomial decay remains bounded as the length of the observationsequence grows. We show experimentally that decayed MCMC is at leastcompetitive with other approximation algorithms such as particle filtering.
arxiv-2400-97 | Almost-everywhere algorithmic stability and generalization error | http://arxiv.org/abs/1301.0579 | author:Samuel Kutin, Partha Niyogi category:cs.LG stat.ML published:2012-12-12 summary:We explore in some detail the notion of algorithmic stability as a viableframework for analyzing the generalization error of learning algorithms. Weintroduce the new notion of training stability of a learning algorithm and showthat, in a general setting, it is sufficient for good bounds on generalizationerror. In the PAC setting, training stability is both necessary and sufficientfor learnability.\ The approach based on training stability makes no referenceto VC dimension or VC entropy. There is no need to prove uniform convergence,and generalization error is bounded directly via an extended McDiarmidinequality. As a result it potentially allows us to deal with a broader classof learning algorithms than Empirical Risk Minimization. \ We also explore therelationships among VC dimension, generalization error, and various notions ofstability. Several examples of learning algorithms are considered.
arxiv-2400-98 | Dimension Correction for Hierarchical Latent Class Models | http://arxiv.org/abs/1301.0578 | author:Tomas Kocka, Nevin Lianwen Zhang category:cs.LG stat.ML published:2012-12-12 summary:Model complexity is an important factor to consider when selecting amonggraphical models. When all variables are observed, the complexity of a modelcan be measured by its standard dimension, i.e. the number of independentparameters. When hidden variables are present, however, standard dimensionmight no longer be appropriate. One should instead use effective dimension(Geiger et al. 1996). This paper is concerned with the computation of effectivedimension. First we present an upper bound on the effective dimension of alatent class (LC) model. This bound is tight and its computation is easy. Wethen consider a generalization of LC models called hierarchical latent class(HLC) models (Zhang 2002). We show that the effective dimension of an HLC modelcan be obtained from the effective dimensions of some related LC models. Wealso demonstrate empirically that using effective dimension in place ofstandard dimension improves the quality of models learned from data.
arxiv-2400-99 | Reduction of Maximum Entropy Models to Hidden Markov Models | http://arxiv.org/abs/1301.0570 | author:Joshua Goodman category:cs.AI cs.CL published:2012-12-12 summary:We show that maximum entropy (maxent) models can be modeled with certainkinds of HMMs, allowing us to construct maxent models with hidden variables,hidden state sequences, or other characteristics. The models can be trainedusing the forward-backward algorithm. While the results are primarily oftheoretical interest, unifying apparently unrelated concepts, we also giveexperimental results for a maxent model with a hidden variable on a worddisambiguation task; the model outperforms standard techniques.
arxiv-2400-100 | The Thing That We Tried Didn't Work Very Well : Deictic Representation in Reinforcement Learning | http://arxiv.org/abs/1301.0567 | author:Sarah Finney, Natalia Gardiol, Leslie Pack Kaelbling, Tim Oates category:cs.LG cs.AI published:2012-12-12 summary:Most reinforcement learning methods operate on propositional representationsof the world state. Such representations are often intractably large andgeneralize poorly. Using a deictic representation is believed to be a viablealternative: they promise generalization while allowing the use of existingreinforcement-learning methods. Yet, there are few experiments on learning withdeictic representations reported in the literature. In this paper we explorethe effectiveness of two forms of deictic representation and a na\"{i}vepropositional representation in a simple blocks-world domain. We find,empirically, that the deictic representations actually worsen learningperformance. We conclude with a discussion of possible causes of these resultsand strategies for more effective learning in domains with objects.
arxiv-2400-101 | An Information-Theoretic External Cluster-Validity Measure | http://arxiv.org/abs/1301.0565 | author:Byron E Dom category:cs.LG stat.ML published:2012-12-12 summary:In this paper we propose a measure of clustering quality or accuracy that isappropriate in situations where it is desirable to evaluate a clusteringalgorithm by somehow comparing the clusters it produces with ``ground truth'consisting of classes assigned to the patterns by manual means or some othermeans in whose veracity there is confidence. Such measures are refered to as``external'. Our measure also has the characteristic of allowing clusteringswith different numbers of clusters to be compared in a quantitative andprincipled way. Our evaluation scheme quantitatively measures how useful thecluster labels of the patterns are as predictors of their class labels. Incases where all clusterings to be compared have the same number of clusters,the measure is equivalent to the mutual information between the cluster labelsand the class labels. In cases where the numbers of clusters are different,however, it computes the reduction in the number of bits that would be requiredto encode (compress) the class labels if both the encoder and decoder have freeacccess to the cluster labels. To achieve this encoding the estimatedconditional probabilities of the class labels given the cluster labels mustalso be encoded. These estimated probabilities can be seen as a model for theclass labels and their associated code length as a model cost.
arxiv-2400-102 | Bayesian Network Classifiers in a High Dimensional Framework | http://arxiv.org/abs/1301.0593 | author:Tatjana Pavlenko, Dietrich von Rosen category:cs.LG stat.ML published:2012-12-12 summary:We present a growing dimension asymptotic formalism. The perspective in thispaper is classification theory and we show that it can accommodateprobabilistic networks classifiers, including naive Bayes model and itsaugmented version. When represented as a Bayesian network these classifiershave an important advantage: The corresponding discriminant function turns outto be a specialized case of a generalized additive model, which makes itpossible to get closed form expressions for the asymptotic misclassificationprobabilities used here as a measure of classification accuracy. Moreover, inthis paper we propose a new quantity for assessing the discriminative power ofa set of features which is then used to elaborate the augmented naive Bayesclassifier. The result is a weighted form of the augmented naive Bayes thatdistributes weights among the sets of features according to theirdiscriminative power. We derive the asymptotic distribution of the sample baseddiscriminative power and show that it is seriously overestimated in a highdimensional case. We then apply this result to find the optimal, in a sense ofminimum misclassification probability, type of weighting.
arxiv-2400-103 | Expectation-Propogation for the Generative Aspect Model | http://arxiv.org/abs/1301.0588 | author:Thomas P. Minka, John Lafferty category:cs.LG cs.IR stat.ML published:2012-12-12 summary:The generative aspect model is an extension of the multinomial model for textthat allows word probabilities to vary stochastically across documents.Previous results with aspect models have been promising, but hindered by thecomputational difficulty of carrying out inference and learning. This paperdemonstrates that the simple variational methods of Blei et al (2001) can leadto inaccurate inferences and biased learning for the generative aspect model.We develop an alternative approach that leads to higher accuracy at comparablecost. An extension of Expectation-Propagation is used for inference and thenembedded in an EM algorithm for learning. Experimental results are presentedfor both synthetic and real data sets.
arxiv-2400-104 | Interpolating Conditional Density Trees | http://arxiv.org/abs/1301.0563 | author:Scott Davies, Andrew Moore category:cs.LG cs.AI stat.ML published:2012-12-12 summary:Joint distributions over many variables are frequently modeled by decomposingthem into products of simpler, lower-dimensional conditional distributions,such as in sparsely connected Bayesian networks. However, automaticallylearning such models can be very computationally expensive when there are manydatapoints and many continuous variables with complex nonlinear relationships,particularly when no good ways of decomposing the joint distribution are knowna priori. In such situations, previous research has generally focused on theuse of discretization techniques in which each continuous variable has a singlediscretization that is used throughout the entire network. \ In this paper, wepresent and compare a wide variety of tree-based algorithms for learning andevaluating conditional density estimates over continuous variables. These treescan be thought of as discretizations that vary according to the particularinteractions being modeled; however, the density within a given leaf of thetree need not be assumed constant, and we show that such nonuniform leafdensities lead to more accurate density estimation. We have developed Bayesiannetwork structure-learning algorithms that employ these tree-based conditionaldensity representations, and we show that they can be used to practically learncomplex joint probability models over dozens of continuous variables fromthousands of datapoints. We focus on finding models that are simultaneouslyaccurate, fast to learn, and fast to evaluate once they are learned.
arxiv-2400-105 | Continuation Methods for Mixing Heterogenous Sources | http://arxiv.org/abs/1301.0562 | author:Adrian Corduneanu, Tommi S. Jaakkola category:cs.LG stat.ML published:2012-12-12 summary:A number of modern learning tasks involve estimation from heterogeneousinformation sources. This includes classification with labeled and unlabeleddata as well as other problems with analogous structure such as competitive(game theoretic) problems. The associated estimation problems can be typicallyreduced to solving a set of fixed point equations (consistency conditions). Weintroduce a general method for combining a preferred information source withanother in this setting by evolving continuous paths of fixed points atintermediate allocations. We explicitly identify critical points along theunique paths to either increase the stability of estimation or to ensure asignificant departure from the initial source. The homotopy continuationapproach is guaranteed to terminate at the second source, and involves nocombinatorial effort. We illustrate the power of these ideas both inclassification tasks with labeled and unlabeled data, as well as in the contextof a competitive (min-max) formulation of DNA sequence motif discovery.
arxiv-2400-106 | Learning with Scope, with Application to Information Extraction and Classification | http://arxiv.org/abs/1301.0556 | author:David Blei, J Andrew Bagnell, Andrew McCallum category:cs.LG cs.IR stat.ML published:2012-12-12 summary:In probabilistic approaches to classification and information extraction, onetypically builds a statistical model of words under the assumption that futuredata will exhibit the same regularities as the training data. In many datasets, however, there are scope-limited features whose predictive power is onlyapplicable to a certain subset of the data. For example, in informationextraction from web pages, word formatting may be indicative of extractioncategory in different ways on different web pages. The difficulty with usingsuch features is capturing and exploiting the new regularities encountered inpreviously unseen data. In this paper, we propose a hierarchical probabilisticmodel that uses both local/scope-limited features, such as word formatting, andglobal features, such as word content. The local regularities are modeled as anunobserved random parameter which is drawn once for each local data set. Thisrandom parameter is estimated during the inference process and then used toperform classification with both the local and global features--- a procedurewhich is akin to automatically retuning the classifier to the localregularities on each newly encountered web page. Exact inference is intractableand we present approximations via point estimates and variational methods.Empirical results on large collections of web data demonstrate that this methodsignificantly improves performance from traditional models of global featuresalone.
arxiv-2400-107 | Tree-dependent Component Analysis | http://arxiv.org/abs/1301.0554 | author:Francis R. Bach, Michael I. Jordan category:cs.LG stat.ML published:2012-12-12 summary:We present a generalization of independent component analysis (ICA), whereinstead of looking for a linear transform that makes the data componentsindependent, we look for a transform that makes the data components well fit bya tree-structured graphical model. Treating the problem as a semiparametricstatistical problem, we show that the optimal transform is found by minimizinga contrast function based on mutual information, a function that directlyextends the contrast function used for classical ICA. We provide twoapproximations of this contrast function, one using kernel density estimation,and another using kernel generalized variance. This tree-dependent componentanalysis framework leads naturally to an efficient general multivariate densityestimation technique where only bivariate density estimation needs to beperformed.
arxiv-2400-108 | A New Class of Upper Bounds on the Log Partition Function | http://arxiv.org/abs/1301.0610 | author:Martin Wainwright, Tommi S. Jaakkola, Alan Willsky category:cs.LG stat.ML published:2012-12-12 summary:Bounds on the log partition function are important in a variety of contexts,including approximate inference, model fitting, decision theory, and largedeviations analysis. We introduce a new class of upper bounds on the logpartition function, based on convex combinations of distributions in theexponential domain, that is applicable to an arbitrary undirected graphicalmodel. In the special case of convex combinations of tree-structureddistributions, we obtain a family of variational problems, similar to the Bethefree energy, but distinguished by the following desirable properties: i. theyare cnvex, and have a unique global minimum; and ii. the global minimum givesan upper bound on the log partition function. The global minimum is defined bystationary conditions very similar to those defining fixed points of beliefpropagation or tree-based reparameterization Wainwright et al., 2001. As withBP fixed points, the elements of the minimizing argument can be used asapproximations to the marginals of the original model. The analysis describedhere can be extended to structures of higher treewidth e.g., hypertrees,thereby making connections with more advanced approximations e.g., Kikuchi andvariants Yedidia et al., 2001; Minka, 2001.
arxiv-2400-109 | Joint Training of Deep Boltzmann Machines | http://arxiv.org/abs/1212.2686 | author:Ian Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG published:2012-12-12 summary:We introduce a new method for training deep Boltzmann machines jointly. Priormethods require an initial learning pass that trains the deep Boltzmann machinegreedily, one layer at a time, or do not perform well on classifi- cationtasks.
arxiv-2400-110 | Enhanced skin colour classifier using RGB Ratio model | http://arxiv.org/abs/1212.2692 | author:Ghazali Osman, Muhammad Suzuri Hitam, Mohd Nasir Ismail category:cs.CV 68T10 published:2012-12-12 summary:Skin colour detection is frequently been used for searching people, facedetection, pornographic filtering and hand tracking. The presence of skin ornon-skin in digital image can be determined by manipulating pixels colour orpixels texture. The main problem in skin colour detection is to represent theskin colour distribution model that is invariant or least sensitive to changesin illumination condition. Another problem comes from the fact that manyobjects in the real world may possess almost similar skin-tone colour such aswood, leather, skin-coloured clothing, hair and sand. Moreover, skin colour isdifferent between races and can be different from a person to another, evenwith people of the same ethnicity. Finally, skin colour will appear a littledifferent when different types of camera are used to capture the object orscene. The objective in this study is to develop a skin colour classifier basedon pixel-based using RGB ratio model. The RGB ratio model is a newly proposedmethod that belongs under the category of an explicitly defined skin regionmodel. This skin classifier was tested with SIdb dataset and two benchmarkdatasets; UChile and TDSD datasets to measure classifier performance. Theperformance of skin classifier was measured based on true positive (TF) andfalse positive (FP) indicator. This newly proposed model was compared withKovac, Saleh and Swift models. The experimental results showed that the RGBratio model outperformed all the other models in term of detection rate. TheRGB ratio model is able to reduce FP detection that caused by reddish objectscolour as well as be able to detect darkened skin and skin covered by shadow.
arxiv-2400-111 | Bayesian one-mode projection for dynamic bipartite graphs | http://arxiv.org/abs/1212.2767 | author:Ioannis Psorakis, Iead Rezek, Zach Frankel, Stephen J. Roberts category:stat.ML cs.LG published:2012-12-12 summary:We propose a Bayesian methodology for one-mode projecting a bipartite networkthat is being observed across a series of discrete time steps. The resultingone mode network captures the uncertainty over the presence/absence of eachlink and provides a probability distribution over its possible weight values.Additionally, the incorporation of prior knowledge over previous states makesthe resulting network less sensitive to noise and missing observations thatusually take place during the data collection process. The methodology consistsof computationally inexpensive update rules and is scalable to large problems,via an appropriate distributed implementation.
arxiv-2400-112 | Clustering of functional boxplots for multiple streaming time series | http://arxiv.org/abs/1212.2784 | author:Elvira Romano, Antonio Balzanella category:stat.ME stat.ML published:2012-12-12 summary:In this paper we introduce a micro-clustering strategy for FunctionalBoxplots. The aim is to summarize a set of streaming time series splitted innon overlapping windows. It is a two step strategy which performs at first, anon-line summarization by means of functional data structures, named FunctionalBoxplot micro-clusters; then it reveals the final summarization by processing,off-line, the functional data structures. Our main contribute consists inproviding a new definition of micro-cluster based on Functional Boxplots and,in defining a proximity measure which allows to compare and update them. Thisallows to get a finer graphical summarization of the streaming time series byfive functional basic statistics of data. The obtained synthesis will be ableto keep track of the dynamic evolution of the multiple streams.
arxiv-2400-113 | Tracking Revisited using RGBD Camera: Baseline and Benchmark | http://arxiv.org/abs/1212.2823 | author:Shuran Song, Jianxiong Xiao category:cs.CV published:2012-12-12 summary:Although there has been significant progress in the past decade,tracking isstill a very challenging computer vision task, due to problems such asocclusion and model drift.Recently, the increased popularity of depth sensorse.g. Microsoft Kinect has made it easy to obtain depth data at low cost.Thismay be a game changer for tracking, since depth information can be used toprevent model drift and handle occlusion.In this paper, we construct abenchmark dataset of 100 RGBD videos with high diversity, including deformableobjects, various occlusion conditions and moving cameras. We propose a verysimple but strong baseline model for RGBD tracking, and present a quantitativecomparison of several state-of-the-art tracking algorithms.Experimental resultsshow that including depth information and reasoning about occlusionsignificantly improves tracking performance. The datasets, evaluation details,source code for the baseline algorithm, and instructions for submitting newmodels will be made available online after acceptance.
arxiv-2400-114 | Learning Hierarchical Object Maps Of Non-Stationary Environments with mobile robots | http://arxiv.org/abs/1301.0551 | author:Dragomir Anguelov, Rahul Biswas, Daphne Koller, Benson Limketkai, Sebastian Thrun category:cs.LG cs.RO stat.ML published:2012-12-12 summary:Building models, or maps, of robot environments is a highly active researcharea; however, most existing techniques construct unstructured maps and assumestatic environments. In this paper, we present an algorithm for learning objectmodels of non-stationary objects found in office-type environments. Ouralgorithm exploits the fact that many objects found in office environments lookalike (e.g., chairs, recycling bins). It does so through a two-levelhierarchical representation, which links individual objects with generic shapetemplates of object classes. We derive an approximate EM algorithm for learningshape parameters at both levels of the hierarchy, using local occupancy gridmaps for representing shape. Additionally, we develop a Bayesian modelselection algorithm that enables the robot to estimate the total number ofobjects and object templates in the environment. Experimental results using areal robot equipped with a laser range finder indicate that our approachperforms well at learning object-based maps of simple office environments. Theapproach outperforms a previously developed non-hierarchical algorithm thatmodels objects but lacks class templates.
arxiv-2400-115 | Mining the Web for the Voice of the Herd to Track Stock Market Bubbles | http://arxiv.org/abs/1212.2676 | author:Aaron Gerow, Mark Keane category:cs.CL cs.IR physics.soc-ph q-fin.GN published:2012-12-11 summary:We show that power-law analyses of financial commentaries from newspaperweb-sites can be used to identify stock market bubbles, supplementingtraditional volatility analyses. Using a four-year corpus of 17,713 online,finance-related articles (10M+ words) from the Financial Times, the New YorkTimes, and the BBC, we show that week-to-week changes in power-lawdistributions reflect market movements of the Dow Jones Industrial Average(DJI), the FTSE-100, and the NIKKEI-225. Notably, the statistical regularitiesin language track the 2007 stock market bubble, showing emerging structure inthe language of commentators, as progressively greater agreement arose in theirpositive perceptions of the market. Furthermore, during the bubble period, amarked divergence in positive language occurs as revealed by a Kullback-Leibleranalysis.
arxiv-2400-116 | Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on support vector machine classification of RT-QuIC data | http://arxiv.org/abs/1212.2617 | author:William Hulme, Peter Richtárik, Lynne McGuire, Alison Green category:q-bio.QM cs.LG stat.AP published:2012-12-11 summary:In this work we study numerical construction of optimal clinical diagnostictests for detecting sporadic Creutzfeldt-Jakob disease (sCJD). A cerebrospinalfluid sample (CSF) from a suspected sCJD patient is subjected to a processwhich initiates the aggregation of a protein present only in cases of sCJD.This aggregation is indirectly observed in real-time at regular intervals, sothat a longitudinal set of data is constructed that is then analysed forevidence of this aggregation. The best existing test is based solely on thefinal value of this set of data, which is compared against a threshold toconclude whether or not aggregation, and thus sCJD, is present. This testcriterion was decided upon by analysing data from a total of 108 sCJD andnon-sCJD samples, but this was done subjectively and there is no supportingmathematical analysis declaring this criterion to be exploiting the availabledata optimally. This paper addresses this deficiency, seeking to validate orimprove the test primarily via support vector machine (SVM) classification.Besides this, we address a number of additional issues such as i) earlystopping of the measurement process, ii) the possibility of detecting theparticular type of sCJD and iii) the incorporation of additional patient datasuch as age, sex, disease duration and timing of CSF sampling into theconstruction of the test.
arxiv-2400-117 | Languages cool as they expand: Allometric scaling and the decreasing need for new words | http://arxiv.org/abs/1212.2616 | author:Alexander M. Petersen, Joel N. Tenenbaum, Shlomo Havlin, H. Eugene Stanley, Matjaz Perc category:physics.soc-ph cs.CL stat.AP published:2012-12-11 summary:We analyze the occurrence frequencies of over 15 million words recorded inmillions of books published during the past two centuries in seven differentlanguages. For all languages and chronological subsets of the data we confirmthat two scaling regimes characterize the word frequency distributions, withonly the more common words obeying the classic Zipf law. Using corpora ofunprecedented size, we test the allometric scaling relation between the corpussize and the vocabulary size of growing languages to demonstrate a decreasingmarginal need for new words, a feature that is likely related to the underlyingcorrelations between words. We calculate the annual growth fluctuations of worduse which has a decreasing trend as the corpus size increases, indicating aslowdown in linguistic evolution following language expansion. This "coolingpattern" forms the basis of a third statistical regularity, which unlike theZipf and the Heaps law, is dynamical in nature.
arxiv-2400-118 | Language Without Words: A Pointillist Model for Natural Language Processing | http://arxiv.org/abs/1212.3228 | author:Peiyou Song, Anhei Shu, David Phipps, Dan Wallach, Mohit Tiwari, Jedidiah Crandall, George Luger category:cs.CL cs.IR cs.SI published:2012-12-11 summary:This paper explores two separate questions: Can we perform natural languageprocessing tasks without a lexicon?; and, Should we? Existing natural languageprocessing techniques are either based on words as units or use units such asgrams only for basic classification tasks. How close can a machine come toreasoning about the meanings of words and phrases in a corpus without using anylexicon, based only on grams? Our own motivation for posing this question is based on our efforts to findpopular trends in words and phrases from online Chinese social media. This formof written Chinese uses so many neologisms, creative character placements, andcombinations of writing systems that it has been dubbed the "Martian Language."Readers must often use visual queues, audible queues from reading out loud, andtheir knowledge and understanding of current events to understand a post. Foranalysis of popular trends, the specific problem is that it is difficult tobuild a lexicon when the invention of new ways to refer to a word or concept iseasy and common. For natural language processing in general, we argue in thispaper that new uses of language in social media will challenge machines'abilities to operate with words as the basic unit of understanding, not only inChinese but potentially in other languages.
arxiv-2400-119 | Convex Relaxations for Learning Bounded Treewidth Decomposable Graphs | http://arxiv.org/abs/1212.2573 | author:K. S. Sesh Kumar, Francis Bach category:cs.LG cs.DS stat.ML published:2012-12-11 summary:We consider the problem of learning the structure of undirected graphicalmodels with bounded treewidth, within the maximum likelihood framework. This isan NP-hard problem and most approaches consider local search techniques. Inthis paper, we pose it as a combinatorial optimization problem, which is thenrelaxed to a convex optimization problem that involves searching over theforest and hyperforest polytopes with special structures, independently. Asupergradient method is used to solve the dual problem, with a run-timecomplexity of $O(k^3 n^{k+2} \log n)$ for each iteration, where $n$ is thenumber of variables and $k$ is a bound on the treewidth. We compare ourapproach to state-of-the-art methods on synthetic datasets and classicalbenchmarks, showing the gains of the novel convex approach.
arxiv-2400-120 | A Learning Framework for Morphological Operators using Counter-Harmonic Mean | http://arxiv.org/abs/1212.2546 | author:Jonathan Masci, Jesús Angulo, Jürgen Schmidhuber category:cs.CV published:2012-12-11 summary:We present a novel framework for learning morphological operators usingcounter-harmonic mean. It combines concepts from morphology and convolutionalneural networks. A thorough experimental validation analyzes basicmorphological operators dilation and erosion, opening and closing, as well asthe much more complex top-hat transform, for which we report a real-worldapplication from the steel industry. Using online learning and stochasticgradient descent, our system learns both the structuring element and thecomposition of operators. It scales well to large datasets and online settings.
arxiv-2400-121 | On The Delays In Spiking Neural P Systems | http://arxiv.org/abs/1212.2529 | author:Francis George C. Cabarle, Kelvin C. Buño, Henry N. Adorna category:cs.NE cs.DC cs.ET 97P20 F.4.1 published:2012-12-11 summary:In this work we extend and improve the results done in a previous work onsimulating Spiking Neural P systems (SNP systems in short) with delays usingSNP systems without delays. We simulate the former with the latter oversequential, iteration, join, and split routing. Our results provideconstructions so that both systems halt at exactly the same time, start withonly one spike, and produce the same number of spikes to the environment afterhalting.
arxiv-2400-122 | Robust Face Recognition using Local Illumination Normalization and Discriminant Feature Point Selection | http://arxiv.org/abs/1212.2415 | author:Song Han, Jinsong Kim, Cholhun Kim, Jongchol Jo, Sunam Han category:cs.LG cs.CV published:2012-12-11 summary:Face recognition systems must be robust to the variation of various factorssuch as facial expression, illumination, head pose and aging. Especially, therobustness against illumination variation is one of the most important problemsto be solved for the practical use of face recognition systems. Gabor waveletis widely used in face detection and recognition because it gives thepossibility to simulate the function of human visual system. In this paper, wepropose a method for extracting Gabor wavelet features which is stable underthe variation of local illumination and show experiment results demonstratingits effectiveness.
arxiv-2400-123 | Mining Techniques in Network Security to Enhance Intrusion Detection Systems | http://arxiv.org/abs/1212.2414 | author:Maher Salem, Ulrich Buehler category:cs.CR cs.LG published:2012-12-11 summary:In intrusion detection systems, classifiers still suffer from severaldrawbacks such as data dimensionality and dominance, different network featuretypes, and data impact on the classification. In this paper two significantenhancements are presented to solve these drawbacks. The first enhancement isan improved feature selection using sequential backward search and informationgain. This, in turn, extracts valuable features that enhance positively thedetection rate and reduce the false positive rate. The second enhancement istransferring nominal network features to numeric ones by exploiting thediscrete random variable and the probability mass function to solve the problemof different feature types, the problem of data dominance, and data impact onthe classification. The latter is combined to known normalization methods toachieve a significant hybrid normalization approach. Finally, an intensive andcomparative study approves the efficiency of these enhancements and showsbetter performance comparing to other proposed methods.
arxiv-2400-124 | On the complexity of learning a language: An improvement of Block's algorithm | http://arxiv.org/abs/1212.2390 | author:Eric Werner category:cs.CL cs.LG published:2012-12-11 summary:Language learning is thought to be a highly complex process. One of thehurdles in learning a language is to learn the rules of syntax of the language.Rules of syntax are often ordered in that before one rule can applied one mustapply another. It has been thought that to learn the order of n rules one mustgo through all n! permutations. Thus to learn the order of 27 rules wouldrequire 27! steps or 1.08889x10^{28} steps. This number is much greater thanthe number of seconds since the beginning of the universe! In an insightfulanalysis the linguist Block ([Block 86], pp. 62-63, p.238) showed that with theassumption of transitivity this vast number of learning steps reduces to a mere377 steps. We present a mathematical analysis of the complexity of Block'salgorithm. The algorithm has a complexity of order n^2 given n rules. Inaddition, we improve Block's results exponentially, by introducing an algorithmthat has complexity of order less than n log n.
arxiv-2400-125 | PAC-Bayesian Learning and Domain Adaptation | http://arxiv.org/abs/1212.2340 | author:Pascal Germain, Amaury Habrard, François Laviolette, Emilie Morvant category:stat.ML cs.LG published:2012-12-11 summary:In machine learning, Domain Adaptation (DA) arises when the distribution gen-erating the test (target) data differs from the one generating the learning(source) data. It is well known that DA is an hard task even under strongassumptions, among which the covariate-shift where the source and targetdistributions diverge only in their marginals, i.e. they have the same labelingfunction. Another popular approach is to consider an hypothesis class thatmoves closer the two distributions while implying a low-error for both tasks.This is a VC-dim approach that restricts the complexity of an hypothesis classin order to get good generalization. Instead, we propose a PAC-Bayesianapproach that seeks for suitable weights to be given to each hypothesis inorder to build a majority vote. We prove a new DA bound in the PAC-Bayesiancontext. This leads us to design the first DA-PAC-Bayesian algorithm based onthe minimization of the proposed bound. Doing so, we seek for a \rho-weightedmajority vote that takes into account a trade-off between three quantities. Thefirst two quantities being, as usual in the PAC-Bayesian approach, (a) thecomplexity of the majority vote (measured by a Kullback-Leibler divergence) and(b) its empirical risk (measured by the \rho-average errors on the sourcesample). The third quantity is (c) the capacity of the majority vote todistinguish some structural difference between the source and target samples.
arxiv-2400-126 | Bag-of-Words Representation for Biomedical Time Series Classification | http://arxiv.org/abs/1212.2262 | author:Jin Wang, Ping Liu, Mary F. H. She, Saeid Nahavandi, and Abbas Kouzani category:cs.LG cs.AI published:2012-12-11 summary:Automatic analysis of biomedical time series such as electroencephalogram(EEG) and electrocardiographic (ECG) signals has attracted great interest inthe community of biomedical engineering due to its important applications inmedicine. In this work, a simple yet effective bag-of-words representation thatis able to capture both local and global structure similarity information isproposed for biomedical time series representation. In particular, similar tothe bag-of-words model used in text document domain, the proposed method treatsa time series as a text document and extracts local segments from the timeseries as words. The biomedical time series is then represented as a histogramof codewords, each entry of which is the count of a codeword appeared in thetime series. Although the temporal order of the local segments is ignored, thebag-of-words representation is able to capture high-level structuralinformation because both local and global structural information are wellutilized. The performance of the bag-of-words model is validated on threedatasets extracted from real EEG and ECG signals. The experimental resultsdemonstrate that the proposed method is not only insensitive to parameters ofthe bag-of-words model such as local segment length and codebook size, but alsorobust to noise.
arxiv-2400-127 | Runtime Optimizations for Prediction with Tree-Based Models | http://arxiv.org/abs/1212.2287 | author:Nima Asadi, Jimmy Lin, Arjen P. de Vries category:cs.DB cs.IR cs.LG published:2012-12-11 summary:Tree-based models have proven to be an effective solution for web ranking aswell as other problems in diverse domains. This paper focuses on optimizing theruntime performance of applying such models to make predictions, given analready-trained model. Although exceedingly simple conceptually, mostimplementations of tree-based models do not efficiently utilize modernsuperscalar processor architectures. By laying out data structures in memory ina more cache-conscious fashion, removing branches from the execution flow usinga technique called predication, and micro-batching predictions using atechnique called vectorization, we are able to better exploit modern processorarchitectures and significantly improve the speed of tree-based models overhard-coded if-else blocks. Our work contributes to the exploration ofarchitecture-conscious runtime implementations of machine learning algorithms.
arxiv-2400-128 | Inverting and Visualizing Features for Object Detection | http://arxiv.org/abs/1212.2278 | author:Carl Vondrick, Aditya Khosla, Tomasz Malisiewicz, Antonio Torralba category:cs.CV published:2012-12-11 summary:We introduce algorithms to visualize feature spaces used by object detectors.The tools in this paper allow a human to put on `HOG goggles' and perceive thevisual world as a HOG based object detector sees it. We found that thesevisualizations allow us to analyze object detection systems in new ways andgain new insight into the detector's failures. For example, when we visualizethe features for high scoring false alarms, we discovered that, although theyare clearly wrong in image space, they do look deceptively similar to truepositives in feature space. This result suggests that many of these falsealarms are caused by our choice of feature space, and indicates that creating abetter learning algorithm or building bigger datasets is unlikely to correctthese errors. By visualizing feature spaces, we can gain a more intuitiveunderstanding of our detection systems.
arxiv-2400-129 | MAD-Bayes: MAP-based Asymptotic Derivations from Bayes | http://arxiv.org/abs/1212.2126 | author:Tamara Broderick, Brian Kulis, Michael I. Jordan category:stat.ML published:2012-12-10 summary:The classical mixture of Gaussians model is related to K-means viasmall-variance asymptotics: as the covariances of the Gaussians tend to zero,the negative log-likelihood of the mixture of Gaussians model approaches theK-means objective, and the EM algorithm approaches the K-means algorithm. Kulis& Jordan (2012) used this observation to obtain a novel K-means-like algorithmfrom a Gibbs sampler for the Dirichlet process (DP) mixture. We insteadconsider applying small-variance asymptotics directly to the posterior inBayesian nonparametric models. This framework is independent of any specificBayesian inference algorithm, and it has the major advantage that itgeneralizes immediately to a range of models beyond the DP mixture. Toillustrate, we apply our framework to the feature learning setting, where thebeta process and Indian buffet process provide an appropriate Bayesiannonparametric prior. We obtain a novel objective function that goes beyondclustering to learn (and penalize new) groupings for which we relax the mutualexclusivity and exhaustivity assumptions of clustering. We demonstrate severalother algorithms, all of which are scalable and simple to implement. Empiricalresults demonstrate the benefits of the new framework.
arxiv-2400-130 | A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization | http://arxiv.org/abs/1212.2006 | author:Jiwei Li, Sujian Li category:cs.CL cs.IR published:2012-12-10 summary:Both supervised learning methods and LDA based topic model have beensuccessfully applied in the field of query focused multi-documentsummarization. In this paper, we propose a novel supervised approach that canincorporate rich sentence features into Bayesian topic models in a principledway, thus taking advantages of both topic model and feature based supervisedlearning methods. Experiments on TAC2008 and TAC2009 demonstrate theeffectiveness of our approach.
arxiv-2400-131 | Fast and Robust Linear Motion Deblurring | http://arxiv.org/abs/1212.2245 | author:Martin Welk, Patrik Raudaschl, Thomas Schwarzbauer, Martin Erler, Martin Läuter category:cs.CV I.4.4; G.1.9 published:2012-12-10 summary:We investigate efficient algorithmic realisations for robust deconvolution ofgrey-value images with known space-invariant point-spread function, withemphasis on 1D motion blur scenarios. The goal is to make deconvolutionsuitable as preprocessing step in automated image processing environments withtight time constraints. Candidate deconvolution methods are selected for theirrestoration quality, robustness and efficiency. Evaluation of restorationquality and robustness on synthetic and real-world test images leads us tofocus on a combination of Wiener filtering with few iterations of robust andregularised Richardson-Lucy deconvolution. We discuss algorithmic optimisationsfor specific scenarios. In the case of uniform linear motion blur in coordinatedirection, it is possible to achieve real-time performance (less than 50 ms) insingle-threaded CPU computation on images of $256\times256$ pixels. For moregeneral space-invariant blur settings, still favourable computation times areobtained. Exemplary parallel implementations demonstrate that the proposedmethod also achieves real-time performance for general 1D motion blurs in amulti-threaded CPU setting, and for general 2D blurs on a GPU.
arxiv-2400-132 | A Scale-Space Theory for Text | http://arxiv.org/abs/1212.2145 | author:Shuang-Hong Yang category:cs.IR cs.CL published:2012-12-10 summary:Scale-space theory has been established primarily by the computer vision andsignal processing communities as a well-founded and promising framework formulti-scale processing of signals (e.g., images). By embedding an originalsignal into a family of gradually coarsen signals parameterized with acontinuous scale parameter, it provides a formal framework to capture thestructure of a signal at different scales in a consistent way. In this paper,we present a scale space theory for text by integrating semantic and spatialfilters, and demonstrate how natural language documents can be understood,processed and analyzed at multiple resolutions, and how this scale-spacerepresentation can be used to facilitate a variety of NLP and text analysistasks.
arxiv-2400-133 | A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method | http://arxiv.org/abs/1212.2002 | author:Simon Lacoste-Julien, Mark Schmidt, Francis Bach category:cs.LG math.OC stat.ML G.1.6; I.2.6 published:2012-12-10 summary:In this note, we present a new averaging technique for the projectedstochastic subgradient method. By using a weighted average with a weight of t+1for each iterate w_t at iteration t, we obtain the convergence rate of O(1/t)with both an easy proof and an easy implementation. The new scheme is comparedempirically to existing techniques, with similar performance behavior.
arxiv-2400-134 | Query-focused Multi-document Summarization: Combining a Novel Topic Model with Graph-based Semi-supervised Learning | http://arxiv.org/abs/1212.2036 | author:Jiwei Li, Sujian Li category:cs.CL cs.IR published:2012-12-10 summary:Graph-based semi-supervised learning has proven to be an effective approachfor query-focused multi-document summarization. The problem of previoussemi-supervised learning is that sentences are ranked without considering thehigher level information beyond sentence level. Researches on generalsummarization illustrated that the addition of topic level can effectivelyimprove the summary quality. Inspired by previous researches, we propose atwo-layer (i.e. sentence layer and topic layer) graph-based semi-supervisedlearning approach. At the same time, we propose a novel topic model which makesfull use of the dependence between sentences and words. Experimental results onDUC and TAC data sets demonstrate the effectiveness of our proposed approach.
arxiv-2400-135 | Macro-Economic Time Series Modeling and Interaction Networks | http://arxiv.org/abs/1212.2044 | author:Gabriel Kronberger, Stefan Fink, Michael Kommenda, Michael Affenzeller category:cs.NE stat.AP published:2012-12-10 summary:Macro-economic models describe the dynamics of economic quantities. Theestimations and forecasts produced by such models play a substantial role forfinancial and political decisions. In this contribution we describe an approachbased on genetic programming and symbolic regression to identify variableinteractions in large datasets. In the proposed approach multiple symbolicregression runs are executed for each variable of the dataset to findpotentially interesting models. The result is a variable interaction networkthat describes which variables are most relevant for the approximation of eachvariable of the dataset. This approach is applied to a macro-economic datasetwith monthly observations of important economic indicators in order to identifypotentially interesting dependencies of these indicators. The resultinginteraction network of macro-economic indicators is briefly discussed and twoof the identified models are presented in detail. The two models approximatethe help wanted index and the CPI inflation in the US.
arxiv-2400-136 | A class of random fields on complete graphs with tractable partition function | http://arxiv.org/abs/1212.2136 | author:Boris Flach category:cs.LG stat.ML published:2012-12-10 summary:The aim of this short note is to draw attention to a method by which thepartition function and marginal probabilities for a certain class of randomfields on complete graphs can be computed in polynomial time. This classincludes Ising models with homogeneous pairwise potentials but arbitrary(inhomogeneous) unary potentials. Similarly, the partition function andmarginal probabilities can be computed in polynomial time for random fields oncomplete bipartite graphs, provided they have homogeneous pairwise potentials.We expect that these tractable classes of large scale random fields can be veryuseful for the evaluation of approximation algorithms by providing exact errorestimates.
arxiv-2400-137 | High-dimensional sequence transduction | http://arxiv.org/abs/1212.1936 | author:Nicolas Boulanger-Lewandowski, Yoshua Bengio, Pascal Vincent category:cs.LG published:2012-12-09 summary:We investigate the problem of transforming an input sequence into ahigh-dimensional output sequence in order to transcribe polyphonic audio musicinto symbolic notation. We introduce a probabilistic model based on a recurrentneural network that is able to learn realistic output distributions given theinput and we devise an efficient algorithm to search for the global mode ofthat distribution. The resulting method produces musically plausibletranscriptions even under high levels of noise and drastically outperformsprevious state-of-the-art approaches on five datasets of synthesized sounds andreal recordings, approximately halving the test error rate.
arxiv-2400-138 | Condensés de textes par des méthodes numériques | http://arxiv.org/abs/1212.1918 | author:Juan-Manuel Torres-Moreno, Patricia Velázquez-Morales, Jean-Guy Meunier category:cs.IR cs.CL published:2012-12-09 summary:Since information in electronic form is already a standard, and that thevariety and the quantity of information become increasingly large, the methodsof summarizing or automatic condensation of texts is a critical phase of theanalysis of texts. This article describes CORTEX a system based on numericalmethods, which allows obtaining a condensation of a text, which is independentof the topic and of the length of the text. The structure of the system enablesit to find the abstracts in French or Spanish in very short times.
arxiv-2400-139 | Self Authentication of image through Daubechies Transform technique (SADT) | http://arxiv.org/abs/1212.1863 | author:Madhumita Sengupta, J. K. Mandal category:cs.CR cs.CV published:2012-12-09 summary:In this paper a 4 x 4 Daubechies transform based authentication techniquetermed as SADT has been proposed to authenticate gray scale images. The coverimage is transformed into the frequency domain using 4 x 4 mask in a row majororder using Daubechies transform technique, resulting four frequency subbandsAF, HF, VF and DF. One byte of every band in a mask is embedding with two orfour bits of secret information. Experimental results are computed and comparedwith the existing authentication techniques like Li s method [5], SCDFT [6],Region-Based method [7] and other similar techniques based on Mean Square Error(MSE), Peak Signal to Noise Ratio (PSNR) and Image Fidelity (IF), which showsbetter performance in SADT.
arxiv-2400-140 | Computational Capabilities of Random Automata Networks for Reservoir Computing | http://arxiv.org/abs/1212.1744 | author:David Snyder, Alireza Goudarzi, Christof Teuscher category:nlin.AO cs.NE published:2012-12-08 summary:This paper underscores the conjecture that intrinsic computation is maximalin systems at the "edge of chaos." We study the relationship between dynamicsand computational capability in Random Boolean Networks (RBN) for ReservoirComputing (RC). RC is a computational paradigm in which a trained readout layerinterprets the dynamics of an excitable component (called the reservoir) thatis perturbed by external input. The reservoir is often implemented as ahomogeneous recurrent neural network, but there has been little investigationinto the properties of reservoirs that are discrete and heterogeneous. RandomBoolean networks are generic and heterogeneous dynamical systems and here weuse them as the reservoir. An RBN is typically a closed system; to use it as areservoir we extend it with an input layer. As a consequence of perturbation,the RBN does not necessarily fall into an attractor. Computational capabilityin RC arises from a trade-off between separability and fading memory of inputs.We find the balance of these properties predictive of classification power andoptimal at critical connectivity. These results are relevant to theconstruction of devices which exploit the intrinsic dynamics of complexheterogeneous systems, such as biomolecular substrates.
arxiv-2400-141 | Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes | http://arxiv.org/abs/1212.1824 | author:Ohad Shamir, Tong Zhang category:cs.LG math.OC stat.ML published:2012-12-08 summary:Stochastic Gradient Descent (SGD) is one of the simplest and most popularstochastic optimization methods. While it has already been theoreticallystudied for decades, the classical analysis usually required non-trivialsmoothness assumptions, which do not apply to many modern applications of SGDwith non-smooth objective functions such as support vector machines. In thispaper, we investigate the performance of SGD without such smoothnessassumptions, as well as a running average scheme to convert the SGD iterates toa solution with optimal optimization accuracy. In this framework, we prove thatafter T rounds, the suboptimality of the last SGD iterate scales asO(log(T)/\sqrt{T}) for non-smooth convex objective functions, and O(log(T)/T)in the non-smooth strongly convex case. To the best of our knowledge, these arethe first bounds of this kind, and almost match the minimax-optimal ratesobtainable by appropriate averaging schemes. We also propose a new and simpleaveraging scheme, which not only attains optimal rates, but can also be easilycomputed on-the-fly (in contrast, the suffix averaging scheme proposed inRakhlin et al. (2011) is not as simple to implement). Finally, we provide someexperimental illustrations.
arxiv-2400-142 | An Empirical Comparison of V-fold Penalisation and Cross Validation for Model Selection in Distribution-Free Regression | http://arxiv.org/abs/1212.1780 | author:Charanpal Dhanjal, Nicolas Baskiotis, Stéphan Clémençon, Nicolas Usunier category:stat.ML published:2012-12-08 summary:Model selection is a crucial issue in machine-learning and a wide variety ofpenalisation methods (with possibly data dependent complexity penalties) haverecently been introduced for this purpose. However their empirical performanceis generally not well documented in the literature. It is the goal of thispaper to investigate to which extent such recent techniques can be successfullyused for the tuning of both the regularisation and kernel parameters in supportvector regression (SVR) and the complexity measure in regression trees (CART).This task is traditionally solved via V-fold cross-validation (VFCV), whichgives efficient results for a reasonable computational cost. A disadvantagehowever of VFCV is that the procedure is known to provide an asymptoticallysuboptimal risk estimate as the number of examples tends to infinity. Recently,a penalisation procedure called V-fold penalisation has been proposed toimprove on VFCV, supported by theoretical arguments. Here we report on anextensive set of experiments comparing V-fold penalisation and VFCV forSVR/CART calibration on several benchmark datasets. We highlight cases in whichVFCV and V-fold penalisation provide poor estimates of the risk respectivelyand introduce a modified penalisation technique to reduce the estimation error.
arxiv-2400-143 | A fair comparison of many max-tree computation algorithms (Extended version of the paper submitted to ISMM 2013 | http://arxiv.org/abs/1212.1819 | author:Edwin Carlinet, Thierry Géraud category:cs.CV published:2012-12-08 summary:With the development of connected filters for the last decade, manyalgorithms have been proposed to compute the max-tree. Max-tree allows tocompute the most advanced connected operators in a simple way. However, no faircomparison of algorithms has been proposed yet and the choice of an algorithmover an other depends on many parameters. Since the need of fast algorithms isobvious for production code, we present an in depth comparison of fivealgorithms and some variations of them in a unique framework. Finally, adecision tree will be proposed to help user in choosing the right algorithmwith respect to their data.
arxiv-2400-144 | Hybrid Optimized Back propagation Learning Algorithm For Multi-layer Perceptron | http://arxiv.org/abs/1212.1752 | author:Mriganka Chakraborty, Arka Ghosh category:cs.NE published:2012-12-08 summary:Standard neural network based on general back propagation learning usingdelta method or gradient descent method has some great faults like pooroptimization of error-weight objective function, low learning rate, instability.This paper introduces a hybrid supervised back propagation learning algorithmwhich uses trust-region method of unconstrained optimization of the errorobjective function by using quasi-newton method .This optimization leads tomore accurate weight update system for minimizing the learning error duringlearning phase of multi-layer perceptron.[13][14][15] In this paper augmentedline search is used for finding points which satisfies Wolfe condition. In thispaper, This hybrid back propagation algorithm has strong global convergenceproperties & is robust & efficient in practice.
arxiv-2400-145 | Similarity of Polygonal Curves in the Presence of Outliers | http://arxiv.org/abs/1212.1617 | author:Jean-Lou De Carufel, Amin Gheibi, Anil Maheshwari, Jörg-Rüdiger Sack, Christian Scheffer category:cs.CG cs.CV cs.GR published:2012-12-07 summary:The Fr\'{e}chet distance is a well studied and commonly used measure tocapture the similarity of polygonal curves. Unfortunately, it exhibits a highsensitivity to the presence of outliers. Since the presence of outliers is afrequently occurring phenomenon in practice, a robust variant of Fr\'{e}chetdistance is required which absorbs outliers. We study such a variant here. Inthis modified variant, our objective is to minimize the length of subcurves oftwo polygonal curves that need to be ignored (MinEx problem), or alternately,maximize the length of subcurves that are preserved (MaxIn problem), to achievea given Fr\'{e}chet distance. An exact solution to one problem would imply anexact solution to the other problem. However, we show that these problems arenot solvable by radicals over $\mathbb{Q}$ and that the degree of thepolynomial equations involved is unbounded in general. This motivates thesearch for approximate solutions. We present an algorithm, which approximates,for a given input parameter $\delta$, optimal solutions for the \MinEx\ and\MaxIn\ problems up to an additive approximation error $\delta$ times thelength of the input curves. The resulting running time is upper bounded by$\mathcal{O} \left(\frac{n^3}{\delta} \log \left(\frac{n}{\delta}\right)\right)$, where $n$ is the complexity of the input polygonal curves.
arxiv-2400-146 | Evolution of the most common English words and phrases over the centuries | http://arxiv.org/abs/1212.1709 | author:Matjaz Perc category:physics.soc-ph cs.CL cs.DL published:2012-12-07 summary:By determining which were the most common English words and phrases since thebeginning of the 16th century, we obtain a unique large-scale view of theevolution of written text. We find that the most common words and phrases inany given year had a much shorter popularity lifespan in the 16th than they hadin the 20th century. By measuring how their usage propagated across the years,we show that for the past two centuries the process has been governed by linearpreferential attachment. Along with the steady growth of the English lexicon,this provides an empirical explanation for the ubiquity of the Zipf's law inlanguage statistics and confirms that writing, although undoubtedly anexpression of art and skill, is not immune to the same influences ofself-organization that are known to regulate processes as diverse as the makingof new friends and World Wide Web growth.
arxiv-2400-147 | Lossy Compression via Sparse Linear Regression: Computationally Efficient Encoding and Decoding | http://arxiv.org/abs/1212.1707 | author:Ramji Venkataramanan, Tuhin Sarkar, Sekhar Tatikonda category:cs.IT math.IT stat.ML published:2012-12-07 summary:We propose computationally efficient encoders and decoders for lossycompression using a Sparse Regression Code. The codebook is defined by a designmatrix and codewords are structured linear combinations of columns of thismatrix. The proposed encoding algorithm sequentially chooses columns of thedesign matrix to successively approximate the source sequence. It is shown toachieve the optimal distortion-rate function for i.i.d Gaussian sources underthe squared-error distortion criterion. For a given rate, the parameters of thedesign matrix can be varied to trade off distortion performance with encodingcomplexity. An example of such a trade-off as a function of the block length nis the following. With computational resource (space or time) per source sampleof O((n/\log n)^2), for a fixed distortion-level above the Gaussiandistortion-rate function, the probability of excess distortion decaysexponentially in n. The Sparse Regression Code is robust in the followingsense: for any ergodic source, the proposed encoder achieves the optimaldistortion-rate function of an i.i.d Gaussian source with the same variance.Simulations show that the encoder has good empirical performance, especially atlow and moderate rates.
arxiv-2400-148 | Spike and Tyke, the Quantized Neuron Model | http://arxiv.org/abs/1212.2958 | author:M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny category:cs.NE cs.AI published:2012-12-07 summary:Modeling spike firing assumes that spiking statistics are Poisson, but realdata violates this assumption. To capture non-Poissonian features, in order tofix the inevitable inherent irregularity, researchers rescale the time axiswith tedious computational overhead instead of searching for anotherdistribution. Spikes or action potentials are precisely-timed changes in theionic transport through synapses adjusting the synaptic weight, successfullymodeled and developed as a memristor. Memristance value is multiples of initialresistance. This reminds us with the foundations of quantum mechanics. We tryto quantize potential and resistance, as done with energy. After reviewingPlanck curve for blackbody radiation, we propose the quantization equations. Weintroduce and prove a theorem that quantizes the resistance. Then we define thetyke showing its basic characteristics. Finally we give the basictransformations to model spiking and link an energy quantum to a tyke.Investigation shows how this perfectly models the neuron spiking, with over 97%match.
arxiv-2400-149 | Layer-wise learning of deep generative models | http://arxiv.org/abs/1212.1524 | author:Ludovic Arnold, Yann Ollivier category:cs.NE cs.LG stat.ML published:2012-12-07 summary:When using deep, multi-layered architectures to build generative models ofdata, it is difficult to train all layers at once. We propose a layer-wisetraining procedure admitting a performance guarantee compared to the globaloptimum. It is based on an optimistic proxy of future performance, the bestlatent marginal. We interpret auto-encoders in this setting as generativemodels, by showing that they train a lower bound of this criterion. We test thenew learning procedure against a state of the art method (stacked RBMs), andfind it to improve performance. Both theory and experiments highlight theimportance, when training deep architectures, of using an inference model (fromdata to hidden variables) richer than the generative model (from hiddenvariables to data).
arxiv-2400-150 | Developments in the theory of randomized shortest paths with a comparison of graph node distances | http://arxiv.org/abs/1212.1666 | author:Ilkka Kivimäki, Masashi Shimbo, Marco Saerens category:stat.ML published:2012-12-07 summary:There have lately been several suggestions for parametrized distances on agraph that generalize the shortest path distance and the commute time orresistance distance. The need for developing such distances has risen from theobservation that the above-mentioned common distances in many situations failto take into account the global structure of the graph. In this article, wedevelop the theory of one family of graph node distances, known as therandomized shortest path dissimilarity, which has its foundation in statisticalphysics. We show that the randomized shortest path dissimilarity can be easilycomputed in closed form for all pairs of nodes of a graph. Moreover, we come upwith a new definition of a distance measure that we call the free energydistance. The free energy distance can be seen as an upgrade of the randomizedshortest path dissimilarity as it defines a metric, in addition to which itsatisfies the graph-geodetic property. The derivation and computation of thefree energy distance are also straightforward. We then make a comparisonbetween a set of generalized distances that interpolate between the shortestpath distance and the commute time, or resistance distance. This comparisonfocuses on the applicability of the distances in graph node clustering andclassification. The comparison, in general, shows that the parametrizeddistances perform well in the tasks. In particular, we see that the resultsobtained with the free energy distance are among the best in all theexperiments.
arxiv-2400-151 | Learning Mixtures of Arbitrary Distributions over Large Discrete Domains | http://arxiv.org/abs/1212.1527 | author:Yuval Rabani, Leonard Schulman, Chaitanya Swamy category:cs.LG cs.DS published:2012-12-07 summary:We give an algorithm for learning a mixture of {\em unstructured}distributions. This problem arises in various unsupervised learning scenarios,for example in learning {\em topic models} from a corpus of documents spanningseveral topics. We show how to learn the constituents of a mixture of $k$arbitrary distributions over a large discrete domain $[n]=\{1,2,\dots,n\}$ andthe mixture weights, using $O(n\polylog n)$ samples. (In the topic-modellearning setting, the mixture constituents correspond to the topicdistributions.) This task is information-theoretically impossible for $k>1$under the usual sampling process from a mixture distribution. However, thereare situations (such as the above-mentioned topic model case) in which eachsample point consists of several observations from the same mixtureconstituent. This number of observations, which we call the {\em "samplingaperture"}, is a crucial parameter of the problem. We obtain the {\em first}bounds for this mixture-learning problem {\em without imposing any assumptionson the mixture constituents.} We show that efficient learning is possibleexactly at the information-theoretically least-possible aperture of $2k-1$.Thus, we achieve near-optimal dependence on $n$ and optimal aperture. While thesample-size required by our algorithm depends exponentially on $k$, we provethat such a dependence is {\em unavoidable} when one considers generalmixtures. A sequence of tools contribute to the algorithm, such asconcentration results for random matrices, dimension reduction, momentestimations, and sensitivity analysis.
arxiv-2400-152 | Excess risk bounds for multitask learning with trace norm regularization | http://arxiv.org/abs/1212.1496 | author:Andreas Maurer, Massimiliano Pontil category:stat.ML cs.LG published:2012-12-06 summary:Trace norm regularization is a popular method of multitask learning. We giveexcess risk bounds with explicit dependence on the number of tasks, the numberof examples per task and properties of the data distribution. The bounds areindependent of the dimension of the input space, which may be infinite as inthe case of reproducing kernel Hilbert spaces. A byproduct of the proof arebounds on the expected norm of sums of random positive semidefinite matriceswith subexponential moments.
arxiv-2400-153 | Clusters and water flows: a novel approach to modal clustering through Morse theory | http://arxiv.org/abs/1212.1384 | author:José E. Chacón category:math.ST math.DG stat.ML stat.TH published:2012-12-06 summary:The problem of finding groups in data (cluster analysis) has been extensivelystudied by researchers from the fields of Statistics and Computer Science,among others. However, despite its popularity it is widely recognized that theinvestigation of some theoretical aspects of clustering has been relativelysparse. One of the main reasons for this lack of theoretical results is surelythe fact that, unlike the situation with other statistical problems asregression or classification, for some of the cluster methodologies it is quitedifficult to specify a population goal to which the data-based clusteringalgorithms should try to get close. This paper aims to provide some insightinto the theoretical foundations of the usual nonparametric approach toclustering, which understands clusters as regions of high density, bypresenting an explicit formulation for the ideal population clustering.
arxiv-2400-154 | The Clustering of Author's Texts of English Fiction in the Vector Space of Semantic Fields | http://arxiv.org/abs/1212.1478 | author:Bohdan Pavlyshenko category:cs.CL cs.DL cs.IR published:2012-12-06 summary:The clustering of text documents in the vector space of semantic fields andin the semantic space with orthogonal basis has been analysed. It is shown thatusing the vector space model with the basis of semantic fields is effective inthe cluster analysis algorithms of author's texts in English fiction. Theanalysis of the author's texts distribution in cluster structure showed thepresence of the areas of semantic space that represent the author's ideolectsof individual authors. SVD factorization of the semantic fields matrix makes itpossible to reduce significantly the dimension of the semantic space in thecluster analysis of author's texts.
arxiv-2400-155 | Automatic Detection of Texture Defects Using Texture-Periodicity and Gabor Wavelets | http://arxiv.org/abs/1212.1329 | author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV I.0; I.2.10 published:2012-12-06 summary:In this paper, we propose a machine vision algorithm for automaticallydetecting defects in textures belonging to 16 out of 17 wallpaper groups usingtexture-periodicity and a family of Gabor wavelets. Input defective images aresubjected to Gabor wavelet transformation in multi-scales andmulti-orientations and a resultant image is obtained in L2 norm. The resultantimage is split into several periodic blocks and energy of each block is used asa feature space to automatically identify defective and defect-free blocksusing Ward's hierarchical clustering. Experiments on defective fabric images ofthree major wallpaper groups, namely, pmm, p2 and p4m, show that the proposedmethod is robust in finding fabric defects without human intervention and canbe used for automatic defect detection in fabric industries.
arxiv-2400-156 | On the probabilistic continuous complexity conjecture | http://arxiv.org/abs/1212.1263 | author:Mark A. Kon category:stat.ML published:2012-12-06 summary:In this paper we prove the probabilistic continuous complexity conjecture. Incontinuous complexity theory, this states that the complexity of solving acontinuous problem with probability approaching 1 converges (in this limit) tothe complexity of solving the same problem in its worst case. We prove theconjecture holds if and only if space of problem elements is uniformly convex.The non-uniformly convex case has a striking counterexample in the problem ofidentifying a Brownian path in Wiener space, where it is shown thatprobabilistic complexity converges to only half of the worst case complexity inthis limit.
arxiv-2400-157 | Stochastic model for the vocabulary growth in natural languages | http://arxiv.org/abs/1212.1362 | author:Martin Gerlach, Eduardo G. Altmann category:physics.soc-ph cs.CL published:2012-12-06 summary:We propose a stochastic model for the number of different words in a givendatabase which incorporates the dependence on the database size and historicalchanges. The main feature of our model is the existence of two differentclasses of words: (i) a finite number of core-words which have higher frequencyand do not affect the probability of a new word to be used; and (ii) theremaining virtually infinite number of noncore-words which have lower frequencyand once used reduce the probability of a new word to be used in the future.Our model relies on a careful analysis of the google-ngram database of bookspublished in the last centuries and its main consequence is the generalizationof Zipf's and Heaps' law to two scaling regimes. We confirm that thesegeneralizations yield the best simple description of the data among genericdescriptive models and that the two free parameters depend only on the languagebut not on the database. From the point of view of our model the main change onhistorical time scales is the composition of the specific words included in thefinite list of core-words, which we observe to decay exponentially in time witha rate of approximately 30 words per year for English.
arxiv-2400-158 | Distributed Adaptive Networks: A Graphical Evolutionary Game-Theoretic View | http://arxiv.org/abs/1212.1245 | author:Chunxiao Jiang, Yan Chen, K. J. Ray Liu category:cs.GT cs.LG published:2012-12-06 summary:Distributed adaptive filtering has been considered as an effective approachfor data processing and estimation over distributed networks. Most existingdistributed adaptive filtering algorithms focus on designing differentinformation diffusion rules, regardless of the nature evolutionarycharacteristic of a distributed network. In this paper, we study the adaptivenetwork from the game theoretic perspective and formulate the distributedadaptive filtering problem as a graphical evolutionary game. With the proposedformulation, the nodes in the network are regarded as players and the localcombiner of estimation information from different neighbors is regarded asdifferent strategies selection. We show that this graphical evolutionary gameframework is very general and can unify the existing adaptive networkalgorithms. Based on this framework, as examples, we further propose twoerror-aware adaptive filtering algorithms. Moreover, we use graphicalevolutionary game theory to analyze the information diffusion process over theadaptive networks and evolutionarily stable strategy of the system. Finally,simulation results are shown to verify the effectiveness of our analysis andproposed methods.
arxiv-2400-159 | Autonomous Navigation by Robust Scan Matching Technique | http://arxiv.org/abs/1212.1313 | author:Debajyoti Banerji, Ranjit Ray, Jhankar Basu, Indrajit Basak category:cs.CV cs.AI published:2012-12-06 summary:For effective autonomous navigation,estimation of the pose of the robot isessential at every sampling time. For computing an accurateestimation,odometric error needs to be reduced with the help of data fromexternal sensor. In this work, a technique has been developed for accurate poseestimation of mobile robot by using Laser Range data. The technique is robustto noisy data, which may contain considerable amount of outliers. A grey imageis formed from laser range data and the key points from this image areextracted by Harris corner detector. The matching of the key points fromconsecutive data sets have been done while outliers have been rejected byRANSAC method. Robot state is measured by the correspondence between the twosets of keypoints. Finally, optimal robot state is estimated by Extended KalmanFilter. The technique has been applied to an operational robot in thelaboratory environment to show the robustness of the technique in presence ofnoisy sensor data. The performance of this new technique has been compared withthat of conventional ICP method. Through this method, effective and accuratenavigation has been achieved even in presence of substantial noise in thesensor data at the cost of a small amount of additional computationalcomplexity.
arxiv-2400-160 | Compiling Relational Database Schemata into Probabilistic Graphical Models | http://arxiv.org/abs/1212.0967 | author:Sameer Singh, Thore Graepel category:cs.AI cs.DB cs.LG stat.ML published:2012-12-05 summary:Instead of requiring a domain expert to specify the probabilisticdependencies of the data, in this work we present an approach that uses therelational DB schema to automatically construct a Bayesian graphical model fora database. This resulting model contains customized distributions for columns,latent variables that cluster the data, and factors that reflect and representthe foreign key links. Experiments demonstrate the accuracy of the model andthe scalability of inference on synthetic and real-world data.
arxiv-2400-161 | Making Early Predictions of the Accuracy of Machine Learning Applications | http://arxiv.org/abs/1212.1100 | author:J. E. Smith, P. Caleb-Solly, M. A. Tahir, D. Sannen, H. van-Brussel category:cs.LG cs.AI stat.ML I.2.6; I.5.2 published:2012-12-05 summary:The accuracy of machine learning systems is a widely studied research topic.Established techniques such as cross-validation predict the accuracy on unseendata of the classifier produced by applying a given learning method to a giventraining data set. However, they do not predict whether incurring the cost ofobtaining more data and undergoing further training will lead to higheraccuracy. In this paper we investigate techniques for making such earlypredictions. We note that when a machine learning algorithm is presented with atraining set the classifier produced, and hence its error, will depend on thecharacteristics of the algorithm, on training set's size, and also on itsspecific composition. In particular we hypothesise that if a number ofclassifiers are produced, and their observed error is decomposed into bias andvariance terms, then although these components may behave differently, theirbehaviour may be predictable. We test our hypothesis by building models that, given a measurement takenfrom the classifier created from a limited number of samples, predict thevalues that would be measured from the classifier produced when the full dataset is presented. We create separate models for bias, variance and total error.Our models are built from the results of applying ten different machinelearning algorithms to a range of data sets, and tested with "unseen"algorithms and datasets. We analyse the results for various numbers of initialtraining samples, and total dataset sizes. Results show that our predictionsare very highly correlated with the values observed after undertaking the extratraining. Finally we consider the more complex case where an ensemble ofheterogeneous classifiers is trained, and show how we can accurately estimatean upper bound on the accuracy achievable after further training.
arxiv-2400-162 | Evaluating Classifiers Without Expert Labels | http://arxiv.org/abs/1212.0960 | author:Hyun Joon Jung, Matthew Lease category:cs.LG cs.IR stat.ML published:2012-12-05 summary:This paper considers the challenge of evaluating a set of classifiers, asdone in shared task evaluations like the KDD Cup or NIST TREC, without expertlabels. While expert labels provide the traditional cornerstone for evaluatingstatistical learners, limited or expensive access to experts represents apractical bottleneck. Instead, we seek methodology for estimating performanceof the classifiers which is more scalable than expert labeling yet preserveshigh correlation with evaluation based on expert labels. We consider both: 1)using only labels automatically generated by the classifiers (blindevaluation); and 2) using labels obtained via crowdsourcing. Whilecrowdsourcing methods are lauded for scalability, using such data forevaluation raises serious concerns given the prevalence of label noise. Inregard to blind evaluation, two broad strategies are investigated: combine &score and score & combine methods infer a single pseudo-gold label set byaggregating classifier labels; classifiers are then evaluated based on thissingle pseudo-gold label set. On the other hand, score & combine methods: 1)sample multiple label sets from classifier outputs, 2) evaluate classifiers oneach label set, and 3) average classifier performance across label sets. Whenadditional crowd labels are also collected, we investigate two alternativeavenues for exploiting them: 1) direct evaluation of classifiers; or 2)supervision of combine & score methods. To assess generality of our techniques,classifier performance is measured using four common classification metrics,with statistical significance tests. Finally, we measure both score and rankcorrelations between estimated classifier performance vs. actual performanceaccording to expert judgments. Rigorous evaluation of classifiers from the TREC2011 Crowdsourcing Track shows reliable evaluation can be achieved withoutreliance on expert labels.
arxiv-2400-163 | Multiclass Diffuse Interface Models for Semi-Supervised Learning on Graphs | http://arxiv.org/abs/1212.0945 | author:Cristina Garcia-Cardona, Arjuna Flenner, Allon G. Percus category:stat.ML cs.LG math.ST stat.TH I.5.3 published:2012-12-05 summary:We present a graph-based variational algorithm for multiclass classificationof high-dimensional data, motivated by total variation techniques. The energyfunctional is based on a diffuse interface model with a periodic potential. Weaugment the model by introducing an alternative measure of smoothness thatpreserves symmetry among the class labels. Through this modification of thestandard Laplacian, we construct an efficient multiclass method that allows forsharp transitions between classes. The experimental results demonstrate thatour approach is competitive with the state of the art among other graph-basedalgorithms.
arxiv-2400-164 | Using Wikipedia to Boost SVD Recommender Systems | http://arxiv.org/abs/1212.1131 | author:Gilad Katz, Guy Shani, Bracha Shapira, Lior Rokach category:cs.LG cs.IR stat.ML published:2012-12-05 summary:Singular Value Decomposition (SVD) has been used successfully in recent yearsin the area of recommender systems. In this paper we present how this model canbe extended to consider both user ratings and information from Wikipedia. Bymapping items to Wikipedia pages and quantifying their similarity, we are ableto use this information in order to improve recommendation accuracy, especiallywhen the sparsity is high. Another advantage of the proposed approach is thefact that it can be easily integrated into any other SVD implementation,regardless of additional parameters that may have been added to it. Preliminaryexperimental results on the MovieLens dataset are encouraging.
arxiv-2400-165 | Cost-Sensitive Support Vector Machines | http://arxiv.org/abs/1212.0975 | author:Hamed Masnadi-Shirazi, Nuno Vasconcelos, Arya Iranmehr category:cs.LG stat.ML published:2012-12-05 summary:A new procedure for learning cost-sensitive SVM(CS-SVM) classifiers isproposed. The SVM hinge loss is extended to the cost sensitive setting, and theCS-SVM is derived as the minimizer of the associated risk. The extension of thehinge loss draws on recent connections between risk minimization andprobability elicitation. These connections are generalized to cost-sensitiveclassification, in a manner that guarantees consistency with the cost-sensitiveBayes risk, and associated Bayes decision rule. This ensures that optimaldecision rules, under the new hinge loss, implement the Bayes-optimalcost-sensitive classification boundary. Minimization of the new hinge loss isshown to be a generalization of the classic SVM optimization problem, and canbe solved by identical procedures. The dual problem of CS-SVM is carefullyscrutinized by means of regularization theory and sensitivity analysis and theCS-SVM algorithm is substantiated. The proposed algorithm is also extended tocost-sensitive learning with example dependent costs. The minimum costsensitive risk is proposed as the performance measure and is connected to ROCanalysis through vector optimization. The resulting algorithm avoids theshortcomings of previous approaches to cost-sensitive SVM design, and is shownto have superior experimental performance on a large number of cost sensitiveand imbalanced datasets.
arxiv-2400-166 | Multiscale Markov Decision Problems: Compression, Solution, and Transfer Learning | http://arxiv.org/abs/1212.1143 | author:Jake Bouvrie, Mauro Maggioni category:cs.AI cs.SY math.OC stat.ML published:2012-12-05 summary:Many problems in sequential decision making and stochastic control often havenatural multiscale structure: sub-tasks are assembled together to accomplishcomplex goals. Systematically inferring and leveraging hierarchical structure,particularly beyond a single level of abstraction, has remained a longstandingchallenge. We describe a fast multiscale procedure for repeatedly compressing,or homogenizing, Markov decision processes (MDPs), wherein a hierarchy ofsub-problems at different scales is automatically determined. Coarsened MDPsare themselves independent, deterministic MDPs, and may be solved usingexisting algorithms. The multiscale representation delivered by this proceduredecouples sub-tasks from each other and can lead to substantial improvements inconvergence rates both locally within sub-problems and globally acrosssub-problems, yielding significant computational savings. A second fundamentalaspect of this work is that these multiscale decompositions yield new transferopportunities across different problems, where solutions of sub-tasks atdifferent levels of the hierarchy may be amenable to transfer to new problems.Localized transfer of policies and potential operators at arbitrary scales isemphasized. Finally, we demonstrate compression and transfer in a collection ofillustrative domains, including examples involving discrete and continuousstatespaces.
arxiv-2400-167 | On Some Integrated Approaches to Inference | http://arxiv.org/abs/1212.1180 | author:Mark A. Kon, Leszek Plaskota category:stat.ML cs.LG published:2012-12-05 summary:We present arguments for the formulation of unified approach to differentstandard continuous inference methods from partial information. It is claimedthat an explicit partition of information into a priori (prior knowledge) and aposteriori information (data) is an important way of standardizing inferenceapproaches so that they can be compared on a normative scale, and so thatnotions of optimal algorithms become farther-reaching. The inference methodsconsidered include neural network approaches, information-based complexity, andMonte Carlo, spline, and regularization methods. The model is an extension ofcurrently used continuous complexity models, with a class of algorithms in theform of optimization methods, in which an optimization functional (involvingthe data) is minimized. This extends the family of current approaches incontinuous complexity theory, which include the use of interpolatory algorithmsin worst and average case settings.
arxiv-2400-168 | Sparse seismic imaging using variable projection | http://arxiv.org/abs/1212.0912 | author:Aleksandr Y. Aravkin, Tristan van Leeuwen, Ning Tu category:math.OC stat.ML published:2012-12-05 summary:We consider an important class of signal processing problems where the signalof interest is known to be sparse, and can be recovered from data givenauxiliary information about how the data was generated. For example, a sparseGreen's function may be recovered from seismic experimental data using sparsityoptimization when the source signature is known. Unfortunately, in practicethis information is often missing, and must be recovered from data along withthe signal using deconvolution techniques. In this paper, we present a novel methodology to simultaneously solve for thesparse signal and auxiliary parameters using a recently proposed variableprojection technique. Our main contribution is to combine variable projectionwith sparsity promoting optimization, obtaining an efficient algorithm forlarge-scale sparse deconvolution problems. We demonstrate the algorithm on aseismic imaging example.
arxiv-2400-169 | Kernel Estimation from Salient Structure for Robust Motion Deblurring | http://arxiv.org/abs/1212.1073 | author:Jinshan Pan, Risheng Liu, Zhixun Su, Xianfeng Gu category:cs.CV published:2012-12-05 summary:Blind image deblurring algorithms have been improving steadily in the pastyears. Most state-of-the-art algorithms, however, still cannot performperfectly in challenging cases, especially in large blur setting. In thispaper, we focus on how to estimate a good kernel estimate from a single blurredimage based on the image structure. We found that image details caused byblurring could adversely affect the kernel estimation, especially when the blurkernel is large. One effective way to eliminate these details is to apply imagedenoising model based on the Total Variation (TV). First, we developed a novelmethod for computing image structures based on TV model, such that thestructures undermining the kernel estimation will be removed. Second, tomitigate the possible adverse effect of salient edges and improve therobustness of kernel estimation, we applied a gradient selection method. Third,we proposed a novel kernel estimation method, which is capable of preservingthe continuity and sparsity of the kernel and reducing the noises. Finally, wedeveloped an adaptive weighted spatial prior, for the purpose of preservingsharp edges in latent image restoration. The effectiveness of our method isdemonstrated by experiments on various kinds of challenging examples.
arxiv-2400-170 | On the Convergence Properties of Optimal AdaBoost | http://arxiv.org/abs/1212.1108 | author:Joshua Belanich, Luis E. Ortiz category:cs.LG cs.AI stat.ML I.2.6 published:2012-12-05 summary:AdaBoost is one of the most popular machine-learning algorithms. It is simpleto implement and often found very effective by practitioners, while still beingmathematically elegant and theoretically sound. AdaBoost's behavior inpractice, and in particular the test-error behavior, has puzzled many eminentresearchers for over a decade: It seems to defy our general intuition inmachine learning regarding the fundamental trade-off between model complexityand generalization performance. In this paper, we establish the convergence of"Optimal AdaBoost," a term coined by Rudin, Daubechies, and Schapire in 2004.We prove the convergence, with the number of rounds, of the classifier itself,its generalization error, and its resulting margins for fixed data sets, undercertain reasonable conditions. More generally, we prove that the time/per-roundaverage of almost any function of the example weights converges. Our approachis to frame AdaBoost as a dynamical system, to provide sufficient conditionsfor the existence of an invariant measure, and to employ tools from ergodictheory. Unlike previous work, we do not assume AdaBoost cycles; actually, wepresent empirical evidence against it on real-world datasets. Our maintheoretical results hold under a weaker condition. We show sufficient empiricalevidence that Optimal AdaBoost always met the condition on every real-worlddataset we tried. Our results formally ground future convergence-rate analyses,and may even provide opportunities for slight algorithmic modifications tooptimize the generalization ability of AdaBoost classifiers, thus reducing apractitioner's burden of deciding how long to run the algorithm.
arxiv-2400-171 | Two Algorithms for Finding $k$ Shortest Paths of a Weighted Pushdown Automaton | http://arxiv.org/abs/1212.0927 | author:Ke Wu, Philip Resnik category:cs.CL cs.DS cs.FL published:2012-12-05 summary:We introduce efficient algorithms for finding the $k$ shortest paths of aweighted pushdown automaton (WPDA), a compact representation of a weighted setof strings with potential applications in parsing and machine translation. Bothof our algorithms are derived from the same weighted deductive logicdescription of the execution of a WPDA using different search strategies.Experimental results show our Algorithm 2 adds very little overhead vs. thesingle shortest path algorithm, even with a large $k$.
arxiv-2400-172 | Computing Consensus Curves | http://arxiv.org/abs/1212.0935 | author:Livio De La Cruz, Stephen Kobourov, Sergey Pupyrev, Paul Shen, Sankar Veeramoni category:cs.CG cs.CV cs.GT cs.MA published:2012-12-05 summary:We consider the problem of extracting accurate average ant trajectories frommany (possibly inaccurate) input trajectories contributed by citizenscientists. Although there are many generic software tools for motion trackingand specific ones for insect tracking, even untrained humans are much better atthis task, provided a robust method to computing the average trajectories. Weimplemented and tested several local (one ant at a time) and global (all antstogether) method. Our best performing algorithm uses a novel global method,based on finding edge-disjoint paths in an ant-interaction graph constructedfrom the input trajectories. The underlying optimization problem is a new andinteresting variant of network flow. Even though the problem is NP-hard, weimplemented two heuristics, which work very well in practice, outperforming allother approaches, including the best automated system.
arxiv-2400-173 | Universally consistent vertex classification for latent positions graphs | http://arxiv.org/abs/1212.1182 | author:Minh Tang, Daniel L. Sussman, Carey E. Priebe category:stat.ML math.ST stat.TH published:2012-12-05 summary:In this work we show that, using the eigen-decomposition of the adjacencymatrix, we can consistently estimate feature maps for latent position graphswith positive definite link function $\kappa$, provided that the latentpositions are i.i.d. from some distribution F. We then consider theexploitation task of vertex classification where the link function $\kappa$belongs to the class of universal kernels and class labels are observed for anumber of vertices tending to infinity and that the remaining vertices are tobe classified. We show that minimization of the empirical $\varphi$-risk forsome convex surrogate $\varphi$ of 0-1 loss over a class of linear classifierswith increasing complexities yields a universally consistent classifier, thatis, a classification rule with error converging to Bayes optimal for anydistribution F.
arxiv-2400-174 | Using external sources of bilingual information for on-the-fly word alignment | http://arxiv.org/abs/1212.1192 | author:Miquel Esplà-Gomis, Felipe Sánchez-Martínez, Mikel L. Forcada category:cs.CL I.2.7 published:2012-12-05 summary:In this paper we present a new and simple language-independent method forword-alignment based on the use of external sources of bilingual informationsuch as machine translation systems. We show that the few parameters of thealigner can be trained on a very small corpus, which leads to resultscomparable to those obtained by the state-of-the-art tool GIZA++ in terms ofprecision. Regarding other metrics, such as alignment error rate or F-measure,the parametric aligner, when trained on a very small gold-standard (450 pairsof sentences), provides results comparable to those produced by GIZA++ whentrained on an in-domain corpus of around 10,000 pairs of sentences.Furthermore, the results obtained indicate that the training isdomain-independent, which enables the use of the trained aligner 'on the fly'on any new pair of sentences.
arxiv-2400-175 | Training Support Vector Machines Using Frank-Wolfe Optimization Methods | http://arxiv.org/abs/1212.0695 | author:Emanuele Frandi, Ricardo Nanculef, Maria Grazia Gasparo, Stefano Lodi, Claudio Sartori category:cs.LG cs.CV math.OC stat.ML published:2012-12-04 summary:Training a Support Vector Machine (SVM) requires the solution of a quadraticprogramming problem (QP) whose computational complexity becomes prohibitivelyexpensive for large scale datasets. Traditional optimization methods cannot bedirectly applied in these cases, mainly due to memory restrictions. By adopting a slightly different objective function and under mild conditionson the kernel used within the model, efficient algorithms to train SVMs havebeen devised under the name of Core Vector Machines (CVMs). This frameworkexploits the equivalence of the resulting learning problem with the task ofbuilding a Minimal Enclosing Ball (MEB) problem in a feature space, where datais implicitly embedded by a kernel function. In this paper, we improve on the CVM approach by proposing two novel methodsto build SVMs based on the Frank-Wolfe algorithm, recently revisited as a fastmethod to approximate the solution of a MEB problem. In contrast to CVMs, ouralgorithms do not require to compute the solutions of a sequence ofincreasingly complex QPs and are defined by using only analytic optimizationsteps. Experiments on a large collection of datasets show that our methodsscale better than CVMs in most cases, sometimes at the price of a slightlylower accuracy. As CVMs, the proposed methods can be easily extended to machinelearning problems other than binary classification. However, effectiveclassifiers are also obtained using kernels which do not satisfy the conditionrequired by CVMs and can thus be used for a wider set of problems.
arxiv-2400-176 | Evaluation of Particle Swarm Optimization Algorithms for Weighted Max-Sat Problem: Technical Report | http://arxiv.org/abs/1212.0639 | author:Osama Khalil category:cs.NE published:2012-12-04 summary:An experimental evaluation is conducted to asses the performance of 4different Particle Swarm Optimization neighborhood structures in solvingMax-Sat problem. The experiment has shown that none of the algorithms achievesstatistically significant performance over the others under confidence level of0.05.
arxiv-2400-177 | Unmixing of Hyperspectral Data Using Robust Statistics-based NMF | http://arxiv.org/abs/1212.0888 | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2012-12-04 summary:Mixed pixels are presented in hyperspectral images due to low spatialresolution of hyperspectral sensors. Spectral unmixing decomposes mixed pixelsspectra into endmembers spectra and abundance fractions. In this paper using ofrobust statistics-based nonnegative matrix factorization (RNMF) for spectralunmixing of hyperspectral data is investigated. RNMF uses a robust costfunction and iterative updating procedure, so is not sensitive to outliers.This method has been applied to simulated data using USGS spectral library,AVIRIS and ROSIS datasets. Unmixing results are compared to traditional NMFmethod based on SAD and AAD measures. Results demonstrate that this method canbe used efficiently for hyperspectral unmixing purposes.
arxiv-2400-178 | Advances in Optimizing Recurrent Networks | http://arxiv.org/abs/1212.0901 | author:Yoshua Bengio, Nicolas Boulanger-Lewandowski, Razvan Pascanu category:cs.LG published:2012-12-04 summary:After a more than decade-long period of relatively little research activityin the area of recurrent neural networks, several new developments will bereviewed here that have allowed substantial progress both in understanding andin technical solutions towards more efficient training of recurrent networks.These advances have been motivated by and related to the optimization issuessurrounding deep learning. Although recurrent networks are extremely powerfulin what they can in principle represent in terms of modelling sequences,theirtraining is plagued by two aspects of the same issue regarding the learning oflong-term dependencies. Experiments reported here evaluate the use of clippinggradients, spanning longer time ranges with leaky integration, advancedmomentum techniques, using more powerful output probability models, andencouraging sparser gradients to help symmetry breaking and credit assignment.The experiments are performed on text and music data and show off the combinedeffects of these techniques in generally improving both training and testerror.
arxiv-2400-179 | G-invariant Persistent Homology | http://arxiv.org/abs/1212.0655 | author:Patrizio Frosini category:math.AT cs.CG cs.CV I.4.7; I.5.1 published:2012-12-04 summary:Classical persistent homology is a powerful mathematical tool for shapecomparison. Unfortunately, it is not tailored to study the action oftransformation groups that are different from the group Homeo(X) of allself-homeomorphisms of a topological space X. This fact restricts its use inapplications. In order to obtain better lower bounds for the naturalpseudo-distance d_G associated with a subgroup G of Homeo(X), we need to adaptpersistent homology and consider G-invariant persistent homology. Roughlyspeaking, the main idea consists in defining persistent homology by means of aset of chains that is invariant under the action of G. In this paper weformalize this idea, and prove the stability of the persistent Betti numberfunctions in G-invariant persistent homology with respect to the naturalpseudo-distance d_G. We also show how G-invariant persistent homology could beused in applications concerning shape comparison, when the invariance group isa proper subgroup of the group of all self-homeomorphisms of a topologicalspace. In this paper we will assume that the space X is triangulable, in orderto guarantee that the persistent Betti number functions are finite withoutusing any tameness assumption.
arxiv-2400-180 | Better subset regression | http://arxiv.org/abs/1212.0634 | author:Shifeng Xiong category:stat.ME math.ST stat.CO stat.ML stat.TH 62J07 D.2.2 published:2012-12-04 summary:To find efficient screening methods for high dimensional linear regressionmodels, this paper studies the relationship between model fitting and screeningperformance. Under a sparsity assumption, we show that a subset that includesthe true submodel always yields smaller residual sum of squares (i.e., hasbetter model fitting) than all that do not in a general asymptotic setting.This indicates that, for screening important variables, we could follow a"better fitting, better screening" rule, i.e., pick a "better" subset that hasbetter model fitting. To seek such a better subset, we consider theoptimization problem associated with best subset regression. An EM algorithm,called orthogonalizing subset screening, and its accelerating version areproposed for searching for the best subset. Although the two algorithms cannotguarantee that a subset they yield is the best, their monotonicity propertymakes the subset have better model fitting than initial subsets generated bypopular screening methods, and thus the subset can have better screeningperformance asymptotically. Simulation results show that our methods are verycompetitive in high dimensional variable screening even for finite samplesizes.
arxiv-2400-181 | A Topological Code for Plane Images | http://arxiv.org/abs/1212.0819 | author:Evgeny Shchepin category:cs.CV math.GT published:2012-12-04 summary:It is proposed a new code for contours of plane images. This code was appliedfor optical character recognition of printed and handwritten characters. Onecan apply it to recognition of any visual images.
arxiv-2400-182 | Parallel Coordinate Descent Methods for Big Data Optimization | http://arxiv.org/abs/1212.0873 | author:Peter Richtárik, Martin Takáč category:math.OC cs.AI stat.ML published:2012-12-04 summary:In this work we show that randomized (block) coordinate descent methods canbe accelerated by parallelization when applied to the problem of minimizing thesum of a partially separable smooth convex function and a simple separableconvex function. The theoretical speedup, as compared to the serial method, andreferring to the number of iterations needed to approximately solve the problemwith high probability, is a simple expression depending on the number ofparallel processors and a natural and easily computable measure of separabilityof the smooth component of the objective function. In the worst case, when nodegree of separability is present, there may be no speedup; in the best case,when the problem is separable, the speedup is equal to the number ofprocessors. Our analysis also works in the mode when the number of blocks beingupdated at each iteration is random, which allows for modeling situations withbusy or unreliable processors. We show that our algorithm is able to solve aLASSO problem involving a matrix with 20 billion nonzeros in 2 hours on a largememory node with 24 cores.
arxiv-2400-183 | An Empirical Evaluation of Portfolios Approaches for solving CSPs | http://arxiv.org/abs/1212.0692 | author:Roberto Amadini, Maurizio Gabbrielli, Jacopo Mauro category:cs.AI cs.LG published:2012-12-04 summary:Recent research in areas such as SAT solving and Integer Linear Programminghas shown that the performances of a single arbitrarily efficient solver can besignificantly outperformed by a portfolio of possibly slower on-averagesolvers. We report an empirical evaluation and comparison of portfolioapproaches applied to Constraint Satisfaction Problems (CSPs). We comparedmodels developed on top of off-the-shelf machine learning algorithms withrespect to approaches used in the SAT field and adapted for CSPs, consideringdifferent portfolio sizes and using as evaluation metrics the number of solvedproblems and the time taken to solve them. Results indicate that the best SATapproaches have top performances also in the CSP field and are slightly morecompetitive than simple models built on top of classification algorithms.
arxiv-2400-184 | Semi-blind Source Separation via Sparse Representations and Online Dictionary Learning | http://arxiv.org/abs/1212.0451 | author:Sirisha Rambhatla, Jarvis D. Haupt category:cs.SD stat.AP stat.ML published:2012-12-03 summary:This work examines a semi-blind single-channel source separation problem. Ourspecific aim is to separate one source whose local structure is approximatelyknown, from another a priori unspecified background source, given only a singlelinear combination of the two sources. We propose a separation technique basedon local sparse approximations along the lines of recent efforts in sparserepresentations and dictionary learning. A key feature of our procedure is theonline learning of dictionaries (using only the data itself) to sparsely modelthe background source, which facilitates its separation from thepartially-known source. Our approach is applicable to source separationproblems in various application domains; here, we demonstrate the performanceof our proposed approach via simulation on a stylized audio source separationtask.
arxiv-2400-185 | Machine learning prediction of cancer cell sensitivity to drugs based on genomic and chemical properties | http://arxiv.org/abs/1212.0504 | author:Michael P. Menden, Francesco Iorio, Mathew Garnett, Ultan McDermott, Cyril Benes, Pedro J. Ballester, Julio Saez-Rodriguez category:q-bio.GN cs.CE cs.LG q-bio.CB published:2012-12-03 summary:Predicting the response of a specific cancer to a therapy is a major goal inmodern oncology that should ultimately lead to a personalised treatment.High-throughput screenings of potentially active compounds against a panel ofgenomically heterogeneous cancer cell lines have unveiled multiplerelationships between genomic alterations and drug responses. Variouscomputational approaches have been proposed to predict sensitivity based ongenomic features, while others have used the chemical properties of the drugsto ascertain their effect. In an effort to integrate these complementaryapproaches, we developed machine learning models to predict the response ofcancer cell lines to drug treatment, quantified through IC50 values, based onboth the genomic features of the cell lines and the chemical properties of theconsidered drugs. Models predicted IC50 values in a 8-fold cross-validation andan independent blind test with coefficient of determination R2 of 0.72 and 0.64respectively. Furthermore, models were able to predict with comparable accuracy(R2 of 0.61) IC50s of cell lines from a tissue not used in the training stage.Our in silico models can be used to optimise the experimental design ofdrug-cell screenings by estimating a large proportion of missing IC50 valuesrather than experimentally measure them. The implications of our results gobeyond virtual drug screening design: potentially thousands of drugs could beprobed in silico to systematically test their potential efficacy as anti-tumouragents based on their structure, thus providing a computational framework toidentify new drug repositioning opportunities as well as ultimately be usefulfor personalized medicine by linking the genomic traits of patients to drugsensitivity.
arxiv-2400-186 | Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses | http://arxiv.org/abs/1212.0478 | author:Po-Ling Loh, Martin J. Wainwright category:stat.ML math.ST stat.TH published:2012-12-03 summary:We investigate the relationship between the structure of a discrete graphicalmodel and the support of the inverse of a generalized covariance matrix. Weshow that for certain graph structures, the support of the inverse covariancematrix of indicator variables on the vertices of a graph reflects theconditional independence structure of the graph. Our work extends results thathave previously been established only in the context of multivariate Gaussiangraphical models, thereby addressing an open question about the significance ofthe inverse covariance matrix of a non-Gaussian distribution. The proofexploits a combination of ideas from the geometry of exponential families,junction tree theory and convex analysis. These population-level results havevarious consequences for graph selection methods, both known and novel,including a novel method for structure estimation for missing or corruptedobservations. We provide nonasymptotic guarantees for such methods andillustrate the sharpness of these predictions via simulations.
arxiv-2400-187 | Time series forecasting: model evaluation and selection using nonparametric risk bounds | http://arxiv.org/abs/1212.0463 | author:Daniel J. McDonald, Cosma Rohilla Shalizi, Mark Schervish category:math.ST cs.LG stat.ML stat.TH published:2012-12-03 summary:We derive generalization error bounds --- bounds on the expected inaccuracyof the predictions --- for traditional time series forecasting models. Ourresults hold for many standard forecasting tools including autoregressivemodels, moving average models, and, more generally, linear state-space models.These bounds allow forecasters to select among competing models and toguarantee that with high probability, their chosen model will perform wellwithout making strong assumptions about the data generating process orappealing to asymptotic theory. We motivate our techniques with and apply themto standard economic and financial forecasting tools --- a GARCH model forpredicting equity volatility and a dynamic stochastic general equilibrium model(DSGE), the standard tool in macroeconomic forecasting. We demonstrate inparticular how our techniques can aid forecasters and policy makers in choosingmodels which behave well under uncertainty and mis-specification.
arxiv-2400-188 | Compressive Schlieren Deflectometry | http://arxiv.org/abs/1212.0433 | author:Prasad Sudhakar, Laurent Jacques, Xavier Dubois, Philippe Antoine, Luc Joannes category:cs.CV published:2012-12-03 summary:Schlieren deflectometry aims at characterizing the deflections undergone byrefracted incident light rays at any surface point of a transparent object. Forsmooth surfaces, each surface location is actually associated with a sparsedeflection map (or spectrum). This paper presents a novel method tocompressively acquire and reconstruct such spectra. This is achieved byaltering the way deflection information is captured in a common SchlierenDeflectometer, i.e., the deflection spectra are indirectly observed by theprinciple of spread spectrum compressed sensing. These observations arerealized optically using a 2-D Spatial Light Modulator (SLM) adjusted to thecorresponding sensing basis and whose modulations encode the light deviationsubsequently recorded by a CCD camera. The efficiency of this approach isdemonstrated experimentally on the observation of few test objects. Further,using a simple parametrization of the deflection spectra we show that relevantkey parameters can be directly computed using the measurements, avoiding fullreconstruction.
arxiv-2400-189 | UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild | http://arxiv.org/abs/1212.0402 | author:Khurram Soomro, Amir Roshan Zamir, Mubarak Shah category:cs.CV published:2012-12-03 summary:We introduce UCF101 which is currently the largest dataset of human actions.It consists of 101 action classes, over 13k clips and 27 hours of video data.The database consists of realistic user uploaded videos containing cameramotion and cluttered background. Additionally, we provide baseline actionrecognition results on this new dataset using standard bag of words approachwith overall performance of 44.5%. To the best of our knowledge, UCF101 iscurrently the most challenging dataset of actions due to its large number ofclasses, large number of clips and also unconstrained nature of such clips.
arxiv-2400-190 | Hypergraph and protein function prediction with gene expression data | http://arxiv.org/abs/1212.0388 | author:Loc Tran category:stat.ML cs.LG q-bio.QM G.2.2 published:2012-12-03 summary:Most network-based protein (or gene) function prediction methods are based onthe assumption that the labels of two adjacent proteins in the network arelikely to be the same. However, assuming the pairwise relationship betweenproteins or genes is not complete, the information a group of genes that showvery similar patterns of expression and tend to have similar functions (i.e.the functional modules) is missed. The natural way overcoming the informationloss of the above assumption is to represent the gene expression data as thehypergraph. Thus, in this paper, the three un-normalized, random walk, andsymmetric normalized hypergraph Laplacian based semi-supervised learningmethods applied to hypergraph constructed from the gene expression data inorder to predict the functions of yeast proteins are introduced. Experimentresults show that the average accuracy performance measures of these threehypergraph Laplacian based semi-supervised learning methods are the same.However, their average accuracy performance measures of these three methods aremuch greater than the average accuracy performance measures of un-normalizedgraph Laplacian based semi-supervised learning method (i.e. the baseline methodof this paper) applied to gene co-expression network created from the geneexpression data.
arxiv-2400-191 | GLCM-based chi-square histogram distance for automatic detection of defects on patterned textures | http://arxiv.org/abs/1212.0383 | author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV I.0; I.2.10 published:2012-12-03 summary:Chi-square histogram distance is one of the distance measures that can beused to find dissimilarity between two histograms. Motivated by the fact thattexture discrimination by human vision system is based on second-orderstatistics, we make use of histogram of gray-level co-occurrence matrix (GLCM)that is based on second-order statistics and propose a new machine visionalgorithm for automatic defect detection on patterned textures. Input defectiveimages are split into several periodic blocks and GLCMs are computed afterquantizing the gray levels from 0-255 to 0-63 to keep the size of GLCM compactand to reduce computation time. Dissimilarity matrix derived from chi-squaredistances of the GLCMs is subjected to hierarchical clustering to automaticallyidentify defective and defect-free blocks. Effectiveness of the proposed methodis demonstrated through experiments on defective real-fabric images of 2 majorwallpaper groups (pmm and p4m groups).
arxiv-2400-192 | Low-rank Matrix Completion using Alternating Minimization | http://arxiv.org/abs/1212.0467 | author:Prateek Jain, Praneeth Netrapalli, Sujay Sanghavi category:stat.ML cs.LG math.OC published:2012-12-03 summary:Alternating minimization represents a widely applicable and empiricallysuccessful approach for finding low-rank matrices that best fit the given data.For example, for the problem of low-rank matrix completion, this method isbelieved to be one of the most accurate and efficient, and formed a majorcomponent of the winning entry in the Netflix Challenge. In the alternating minimization approach, the low-rank target matrix iswritten in a bi-linear form, i.e. $X = UV^\dag$; the algorithm then alternatesbetween finding the best $U$ and the best $V$. Typically, each alternating stepin isolation is convex and tractable. However the overall problem becomesnon-convex and there has been almost no theoretical understanding of when thisapproach yields a good result. In this paper we present first theoretical analysis of the performance ofalternating minimization for matrix completion, and the related problem ofmatrix sensing. For both these problems, celebrated recent results have shownthat they become well-posed and tractable once certain (now standard)conditions are imposed on the problem. We show that alternating minimizationalso succeeds under similar conditions. Moreover, compared to existing results,our paper shows that alternating minimization guarantees faster (in particular,geometric) convergence to the true matrix, while allowing a simpler analysis.
arxiv-2400-193 | Dynamic recommender system : using cluster-based biases to improve the accuracy of the predictions | http://arxiv.org/abs/1212.0763 | author:Modou Gueye, Talel Abdessalem, Hubert Naacke category:cs.LG cs.DB cs.IR H.2.8; H.3.3 published:2012-12-03 summary:It is today accepted that matrix factorization models allow a high quality ofrating prediction in recommender systems. However, a major drawback of matrixfactorization is its static nature that results in a progressive declining ofthe accuracy of the predictions after each factorization. This is due to thefact that the new obtained ratings are not taken into account until a newfactorization is computed, which can not be done very often because of the highcost of matrix factorization. In this paper, aiming at improving the accuracy of recommender systems, wepropose a cluster-based matrix factorization technique that enables onlineintegration of new ratings. Thus, we significantly enhance the obtainedpredictions between two matrix factorizations. We use finer-grained user biasesby clustering similar items into groups, and allocating in these groups a biasto each user. The experiments we did on large datasets demonstrated theefficiency of our approach.
arxiv-2400-194 | Comparison of Fuzzy and Neuro Fuzzy Image Fusion Techniques and its Applications | http://arxiv.org/abs/1212.0318 | author:D. Srinivasa Rao, M. Seetha, M. H. M. Krishna Prasad category:cs.CV published:2012-12-03 summary:Image fusion is the process of integrating multiple images of the same sceneinto a single fused image to reduce uncertainty and minimizing redundancy whileextracting all the useful information from the source images. Image fusionprocess is required for different applications like medical imaging, remotesensing, medical imaging, machine vision, biometrics and military applicationswhere quality and critical information is required. In this paper, image fusionusing fuzzy and neuro fuzzy logic approaches utilized to fuse images fromdifferent sensors, in order to enhance visualization. The proposed work furtherexplores comparison between fuzzy based image fusion and neuro fuzzy fusiontechnique along with quality evaluation indices for image fusion like imagequality index, mutual information measure, fusion factor, fusion symmetry,fusion index, root mean square error, peak signal to noise ratio, entropy,correlation coefficient and spatial frequency. Experimental results obtainedfrom fusion process prove that the use of the neuro fuzzy based image fusionapproach shows better performance in first two test cases while in the thirdtest case fuzzy based image fusion technique gives better results.
arxiv-2400-195 | An Image Based Technique for Enhancement of Underwater Images | http://arxiv.org/abs/1212.0291 | author:C. J. Prabhakar, P. U. Praveen Kumar category:cs.CV published:2012-12-03 summary:The underwater images usually suffers from non-uniform lighting, lowcontrast, blur and diminished colors. In this paper, we proposed an image basedpreprocessing technique to enhance the quality of the underwater images. Theproposed technique comprises a combination of four filters such as homomorphicfiltering, wavelet denoising, bilateral filter and contrast equalization. Thesefilters are applied sequentially on degraded underwater images. The literaturesurvey reveals that image based preprocessing algorithms uses standard filtertechniques with various combinations. For smoothing the image, the image basedpreprocessing algorithms uses the anisotropic filter. The main drawback of theanisotropic filter is that iterative in nature and computation time is highcompared to bilateral filter. In the proposed technique, in addition to otherthree filters, we employ a bilateral filter for smoothing the image. Theexperimentation is carried out in two stages. In the first stage, we haveconducted various experiments on captured images and estimated optimalparameters for bilateral filter. Similarly, optimal filter bank and optimalwavelet shrinkage function are estimated for wavelet denoising. In the secondstage, we conducted the experiments using estimated optimal parameters, optimalfilter bank and optimal wavelet shrinkage function for evaluating the proposedtechnique. We evaluated the technique using quantitative based criteria such asa gradient magnitude histogram and Peak Signal to Noise Ratio (PSNR). Further,the results are qualitatively evaluated based on edge detection results. Theproposed technique enhances the quality of the underwater images and can beemployed prior to apply computer vision techniques.
arxiv-2400-196 | Artificial Neural Network for Performance Modeling and Optimization of CMOS Analog Circuits | http://arxiv.org/abs/1212.0215 | author:Mriganka Chakraborty category:cs.NE published:2012-12-02 summary:This paper presents an implementation of multilayer feed forward neuralnetworks (NN) to optimize CMOS analog circuits. For modeling and designrecently neural network computational modules have got acceptance as anunorthodox and useful tool. To achieve high performance of active or passivecircuit component neural network can be trained accordingly. A well trainedneural network can produce more accurate outcome depending on its learningcapability. Neural network model can replace empirical modeling solutionslimited by range and accuracy.[2] Neural network models are easy to obtain fornew circuits or devices which can replace analytical methods. Numericalmodeling methods can also be replaced by neural network model due to theircomputationally expansive behavior.[2][10][20]. The pro- posed implementationis aimed at reducing resource requirement, without much compromise on thespeed. The NN ensures proper functioning by assigning the appropriate inputs,weights, biases, and excitation function of the layer that is currently beingcomputed. The concept used is shown to be very effective in reducing resourcerequirements and enhancing speed.
arxiv-2400-197 | Metaheuristic Optimization: Algorithm Analysis and Open Problems | http://arxiv.org/abs/1212.0220 | author:Xin-She Yang category:math.OC cs.NE 90C26 published:2012-12-02 summary:Metaheuristic algorithms are becoming an important part of modernoptimization. A wide range of metaheuristic algorithms have emerged over thelast two decades, and many metaheuristics such as particle swarm optimizationare becoming increasingly popular. Despite their popularity, mathematicalanalysis of these algorithms lacks behind. Convergence analysis still remainsunsolved for the majority of metaheuristic algorithms, while efficiencyanalysis is equally challenging. In this paper, we intend to provide anoverview of convergence and efficiency studies of metaheuristics, and try toprovide a framework for analyzing metaheuristics in terms of convergence andefficiency. This can form a basis for analyzing other algorithms. We alsooutline some open questions as further research topics.
arxiv-2400-198 | Simplification and integration in computing and cognition: the SP theory and the multiple alignment concept | http://arxiv.org/abs/1212.0229 | author:James Gerard Wolff category:cs.AI cs.CL published:2012-12-02 summary:The main purpose of this article is to describe potential benefits andapplications of the SP theory, a unique attempt to simplify and integrate ideasacross artificial intelligence, mainstream computing and human cognition, withinformation compression as a unifying theme. The theory, including a concept ofmultiple alignment, combines conceptual simplicity with descriptive andexplanatory power in several areas including representation of knowledge,natural language processing, pattern recognition, several kinds of reasoning,the storage and retrieval of information, planning and problem solving,unsupervised learning, information compression, and human perception andcognition. In the SP machine -- an expression of the SP theory which iscurrently realised in the form of computer models -- there is potential for anoverall simplification of computing systems, including software. As a theorywith a broad base of support, the SP theory promises useful insights in manyareas and the integration of structures and functions, both within a given areaand amongst different areas. There are potential benefits in natural languageprocessing (with potential for the understanding and translation of naturallanguages), the need for a versatile intelligence in autonomous robots,computer vision, intelligent databases, maintaining multiple versions ofdocuments or web pages, software engineering, criminal investigations, themanagement of big data and gaining benefits from it, the semantic web, medicaldiagnosis, the detection of computer viruses, the economical transmission ofdata, and data fusion. Further development of these ideas would be facilitatedby the creation of a high-parallel, web-based, open-source version of the SPmachine, with a good user interface. This would provide a means for researchersto explore what can be done with the system and to refine it.
arxiv-2400-199 | Message-Passing Algorithms for Quadratic Minimization | http://arxiv.org/abs/1212.0171 | author:Nicholas Ruozzi, Sekhar Tatikonda category:cs.IT cs.LG math.IT stat.ML published:2012-12-02 summary:Gaussian belief propagation (GaBP) is an iterative algorithm for computingthe mean of a multivariate Gaussian distribution, or equivalently, the minimumof a multivariate positive definite quadratic function. Sufficient conditions,such as walk-summability, that guarantee the convergence and correctness ofGaBP are known, but GaBP may fail to converge to the correct solution given anarbitrary positive definite quadratic function. As was observed in previouswork, the GaBP algorithm fails to converge if the computation trees produced bythe algorithm are not positive definite. In this work, we will show that thefailure modes of the GaBP algorithm can be understood via graph covers, and weprove that a parameterized generalization of the min-sum algorithm can be usedto ensure that the computation trees remain positive definite whenever theinput matrix is positive definite. We demonstrate that the resulting algorithmis closely related to other iterative schemes for quadratic minimization suchas the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically,that there always exists a choice of parameters such that the abovegeneralization of the GaBP algorithm converges.
arxiv-2400-200 | Pedestrian Detection with Unsupervised Multi-Stage Feature Learning | http://arxiv.org/abs/1212.0142 | author:Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann LeCun category:cs.CV cs.LG published:2012-12-01 summary:Pedestrian detection is a problem of considerable practical interest. Addingto the list of successful applications of deep learning methods to vision, wereport state-of-the-art and competitive results on all major pedestriandatasets with a convolutional network model. The model uses a few new twists,such as multi-stage features, connections that skip layers to integrate globalshape information with local distinctive motif information, and an unsupervisedmethod based on convolutional sparse coding to pre-train the filters at eachstage.
arxiv-2400-201 | An Evolution Strategy Approach toward Rule-set Generation for Network Intrusion Detection Systems (IDS) | http://arxiv.org/abs/1212.0170 | author:Herve Kabamba Mbikayi category:cs.CR cs.NE published:2012-12-01 summary:With the increasing number of intrusions in system and networkinfrastructures, Intrusion Detection Systems (IDS) have become an active areaof research to develop reliable and effective solutions to detect and counterthem. The use of Evolutionary Algorithms in IDS has proved its maturity overthe times. Although most of the research works have been based on the use ofgenetic algorithms in IDS, this paper presents an approach toward thegeneration of rules for the identification of anomalous connections usingevolution strategies . The emphasis is given on how the problem can be modeledinto ES primitives and how the fitness of the population can be evaluated inorder to find the local optima, therefore resulting in optimal rules that canbe used for detecting intrusions in intrusion detection systems.
arxiv-2400-202 | Cumulative Step-size Adaptation on Linear Functions | http://arxiv.org/abs/1212.0139 | author:Alexandre Chotard, Anne Auger, Nikolaus Hansen category:cs.LG stat.ML published:2012-12-01 summary:The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation,where the step size is adapted measuring the length of a so-called cumulativepath. The cumulative path is a combination of the previous steps realized bythe algorithm, where the importance of each step decreases with time. Thisarticle studies the CSA-ES on composites of strictly increasing functions withaffine linear functions through the investigation of its underlying Markovchains. Rigorous results on the change and the variation of the step size arederived with and without cumulation. The step-size diverges geometrically fastin most cases. Furthermore, the influence of the cumulation parameter isstudied.
arxiv-2400-203 | Fingertip Detection: A Fast Method with Natural Hand | http://arxiv.org/abs/1212.0134 | author:J. L. Raheja, Karen Das, Ankit Chaudhary category:cs.CV published:2012-12-01 summary:Many vision based applications have used fingertips to track or manipulategestures in their applications. Gesture identification is a natural way to passthe signals to the machine, as the human express its feelings most of the timewith hand expressions. Here a novel time efficient algorithm has been describedfor fingertip detection. This method is invariant to hand direction and inpreprocessing it cuts only hand part from the full image, hence furthercomputation would be much faster than processing full image. Binary silhouetteof the input image is generated using HSV color space based skin filter andhand cropping done based on intensity histogram of the hand image
arxiv-2400-204 | From the decoding of cortical activities to the control of a JACO robotic arm: a whole processing chain | http://arxiv.org/abs/1212.0083 | author:Laurent Bougrain, Olivier Rochel, Octave Boussaton, Lionel Havet category:cs.NE cs.HC cs.RO q-bio.NC published:2012-12-01 summary:This paper presents a complete processing chain for decoding intracranialdata recorded in the cortex of a monkey and replicates the associated movementson a JACO robotic arm by Kinova. We developed specific modules inside theOpenViBE platform in order to build a Brain-Machine Interface able to read thedata, compute the position of the robotic finger and send this position to therobotic arm. More pre- cisely, two client/server protocols have been tested totransfer the finger positions: VRPN and a light protocol based on TCP/IPsockets. According to the requested finger position, the server calls theassoci- ated functions of an API by Kinova to move the fin- gers properly.Finally, we monitor the gap between the requested and actual fingers positions.This chain can be generalized to any movement of the arm or wrist.
arxiv-2400-205 | Challenges in Kurdish Text Processing | http://arxiv.org/abs/1212.0074 | author:Kyumars Sheykh Esmaili category:cs.IR cs.CL published:2012-12-01 summary:Despite having a large number of speakers, the Kurdish language is among theless-resourced languages. In this work we highlight the challenges and problemsin providing the required tools and techniques for processing texts written inKurdish. From a high-level perspective, the main challenges are: the inherentdiversity of the language, standardization and segmentation issues, and thelack of language resources.
arxiv-2400-206 | Artificial Neural Network Fuzzy Inference System (ANFIS) For Brain Tumor Detection | http://arxiv.org/abs/1212.0059 | author:Minakshi Sharma category:cs.CV cs.AI published:2012-12-01 summary:Detection and segmentation of Brain tumor is very important because itprovides anatomical information of normal and abnormal tissues which helps intreatment planning and patient follow-up. There are number of techniques forimage segmentation. Proposed research work uses ANFIS (Artificial NeuralNetwork Fuzzy Inference System) for image classification and then compares theresults with FCM (Fuzzy C means) and K-NN (K-nearest neighbor). ANFIS includesbenefits of both ANN and the fuzzy logic systems. A comprehensive feature setand fuzzy rules are selected to classify an abnormal image to the correspondingtumor type. Experimental results illustrate promising results in terms ofclassification accuracy. A comparative analysis is performed with the FCM andK-NN to show the superior nature of ANFIS systems.
arxiv-2400-207 | Genetic braid optimization: A heuristic approach to compute quasiparticle braids | http://arxiv.org/abs/1211.7359 | author:Ross B. McDonald, Helmut G. Katzgraber category:quant-ph cs.NE published:2012-11-30 summary:In topologically-protected quantum computation, quantum gates can be carriedout by adiabatically braiding two-dimensional quasiparticles, reminiscent ofentangled world lines. Bonesteel et al. [Phys. Rev. Lett. 95, 140503 (2005)],as well as Leijnse and Flensberg [Phys. Rev. B 86, 104511 (2012)] recentlyprovided schemes for computing quantum gates from quasiparticle braids.Mathematically, the problem of executing a gate becomes that of finding aproduct of the generators (matrices) in that set that approximates the gatebest, up to an error. To date, efficient methods to compute these gates onlystrive to optimize for accuracy. We explore the possibility of using a genericapproach applicable to a variety of braiding problems based on evolutionary(genetic) algorithms. The method efficiently finds optimal braids whileallowing the user to optimize for the relative utilities of accuracy and/orlength. Furthermore, when optimizing for error only, the method can quicklyproduce efficient braids.
arxiv-2400-208 | Multislice Modularity Optimization in Community Detection and Image Segmentation | http://arxiv.org/abs/1211.7180 | author:Huiyi Hu, Yves van Gennip, Blake Hunter, Mason A. Porter, Andrea L. Bertozzi category:cs.SI cs.CV physics.soc-ph published:2012-11-30 summary:Because networks can be used to represent many complex systems, they haveattracted considerable attention in physics, computer science, sociology, andmany other disciplines. One of the most important areas of network science isthe algorithmic detection of cohesive groups (i.e., "communities") of nodes. Inthis paper, we algorithmically detect communities in social networks and imagedata by optimizing multislice modularity. A key advantage of modularityoptimization is that it does not require prior knowledge of the number or sizesof communities, and it is capable of finding network partitions that arecomposed of communities of different sizes. By optimizing multislice modularityand subsequently calculating diagnostics on the resulting network partitions,it is thereby possible to obtain information about network structure acrossmultiple system scales. We illustrate this method on data from both socialnetworks and images, and we find that optimization of multislice modularityperforms well on these two tasks without the need for extensiveproblem-specific adaptation. However, improving the computational speed of thismethod remains a challenging open problem.
arxiv-2400-209 | Erratum: Simplified Drift Analysis for Proving Lower Bounds in Evolutionary Computation | http://arxiv.org/abs/1211.7184 | author:Pietro S. Oliveto, Carsten Witt category:cs.NE F.2.0 published:2012-11-30 summary:This erratum points out an error in the simplified drift theorem (SDT)[Algorithmica 59(3), 369-386, 2011]. It is also shown that a minor modificationof one of its conditions is sufficient to establish a valid result. In manyrespects, the new theorem is more general than before. We no longer assume aMarkov process nor a finite search space. Furthermore, the proof of the theoremis more compact than the previous ones. Finally, previous applications of theSDT are revisited. It turns out that all of these either meet the modifiedcondition directly or by means of few additional arguments.
arxiv-2400-210 | Using Differential Evolution for the Graph Coloring | http://arxiv.org/abs/1211.7200 | author:Iztok Fister, Janez Brest category:math.CO cs.NE published:2012-11-30 summary:Differential evolution was developed for reliable and versatile functionoptimization. It has also become interesting for other domains because of itsease to use. In this paper, we posed the question of whether differentialevolution can also be used by solving of the combinatorial optimizationproblems, and in particular, for the graph coloring problem. Therefore, ahybrid self-adaptive differential evolution algorithm for graph coloring wasproposed that is comparable with the best heuristics for graph coloring today,i.e. Tabucol of Hertz and de Werra and the hybrid evolutionary algorithm ofGalinier and Hao. We have focused on the graph 3-coloring. Therefore, theevolutionary algorithm with method SAW of Eiben et al., which achievedexcellent results for this kind of graphs, was also incorporated into thisstudy. The extensive experiments show that the differential evolution couldbecome a competitive tool for the solving of graph coloring problem in thefuture.
arxiv-2400-211 | Secure voice based authentication for mobile devices: Vaulted Voice Verification | http://arxiv.org/abs/1212.0042 | author:R. C. Johnson, Walter J. Scheirer, Terrance E. Boult category:cs.CR cs.CV published:2012-11-30 summary:As the use of biometrics becomes more wide-spread, the privacy concerns thatstem from the use of biometrics are becoming more apparent. As the usage ofmobile devices grows, so does the desire to implement biometric identificationinto such devices. A large majority of mobile devices being used are mobilephones. While work is being done to implement different types of biometricsinto mobile phones, such as photo based biometrics, voice is a more naturalchoice. The idea of voice as a biometric identifier has been around a longtime. One of the major concerns with using voice as an identifier is theinstability of voice. We have developed a protocol that addresses thoseinstabilities and preserves privacy. This paper describes a novel protocol thatallows a user to authenticate using voice on a mobile/remote device withoutcompromising their privacy. We first discuss the \vv protocol, which hasrecently been introduced in research literature, and then describe itslimitations. We then introduce a novel adaptation and extension of the vaultedverification protocol to voice, dubbed $V^3$. Following that we show aperformance evaluation and then conclude with a discussion of security andfuture work.
arxiv-2400-212 | A recursive divide-and-conquer approach for sparse principal component analysis | http://arxiv.org/abs/1211.7219 | author:Qian Zhao, Deyu Meng, Zongben Xu category:cs.CV cs.LG stat.ML 62H25, 68T10 I.5.0; I.5.1 published:2012-11-30 summary:In this paper, a new method is proposed for sparse PCA based on the recursivedivide-and-conquer methodology. The main idea is to separate the originalsparse PCA problem into a series of much simpler sub-problems, each having aclosed-form solution. By recursively solving these sub-problems in ananalytical way, an efficient algorithm is constructed to solve the sparse PCAproblem. The algorithm only involves simple computations and is thus easy toimplement. The proposed method can also be very easily extended to other sparsePCA problems with certain constraints, such as the nonnegative sparse PCAproblem. Furthermore, we have shown that the proposed algorithm converges to astationary point of the problem, and its computational complexity isapproximately linear in both data size and dimensionality. The effectiveness ofthe proposed method is substantiated by extensive experiments implemented on aseries of synthetic and real data in both reconstruction-error-minimization anddata-variance-maximization viewpoints.
arxiv-2400-213 | Viewpoint Invariant Object Detector | http://arxiv.org/abs/1212.0030 | author:Osama Khalil, Andrew Habib category:cs.CV published:2012-11-30 summary:Object Detection is the task of identifying the existence of an object classinstance and locating it within an image. Difficulties in handling highintra-class variations constitute major obstacles to achieving high performanceon standard benchmark datasets (scale, viewpoint, lighting conditions andorientation variations provide good examples). Suggested model aims atproviding more robustness to detecting objects suffering severe distortion dueto < 60{\deg} viewpoint changes. In addition, several model computationalbottlenecks have been resolved leading to a significant increase in the modelperformance (speed and space) without compromising the resulting accuracy.Finally, we produced two illustrative applications showing the potential of theobject detection technology being deployed in real life applications; namelycontent-based image search and content-based video search.
arxiv-2400-214 | Approximate Rank-Detecting Factorization of Low-Rank Tensors | http://arxiv.org/abs/1211.7369 | author:Franz J. Király, Andreas Ziehe category:stat.ML cs.LG math.NA published:2012-11-30 summary:We present an algorithm, AROFAC2, which detects the (CP-)rank of a degree 3tensor and calculates its factorization into rank-one components. We providegenerative conditions for the algorithm to work and demonstrate on bothsynthetic and real world data that AROFAC2 is a potentially outperformingalternative to the gold standard PARAFAC over which it has the advantages thatit can intrinsically detect the true rank, avoids spurious components, and isstable with respect to outliers and non-Gaussian noise.
arxiv-2400-215 | Scalable Spectral Algorithms for Community Detection in Directed Networks | http://arxiv.org/abs/1211.6807 | author:Sungmin Kim, Tao Shi category:cs.SI physics.soc-ph stat.ML published:2012-11-29 summary:Community detection has been one of the central problems in network studiesand directed network is particularly challenging due to asymmetry among itslinks. In this paper, we found that incorporating the direction of linksreveals new perspectives on communities regarding to two different roles,source and terminal, that a node plays in each community. Intriguingly, suchcommunities appear to be connected with unique spectral property of the graphLaplacian of the adjacency matrix and we exploit this connection by usingregularized SVD methods. We propose harvesting algorithms, coupled withregularized SVDs, that are linearly scalable for efficient identification ofcommunities in huge directed networks. The proposed algorithm shows greatperformance and scalability on benchmark networks in simulations andsuccessfully recovers communities in real network applications.
arxiv-2400-216 | Dynamic Network Cartography | http://arxiv.org/abs/1211.6950 | author:Gonzalo Mateos, Ketan Rajawat category:cs.NI cs.IT cs.MA math.IT stat.ML published:2012-11-29 summary:Communication networks have evolved from specialized, research and tacticaltransmission systems to large-scale and highly complex interconnections ofintelligent devices, increasingly becoming more commercial, consumer-oriented,and heterogeneous. Propelled by emergent social networking services andhigh-definition streaming platforms, network traffic has grown explosivelythanks to the advances in processing speed and storage capacity ofstate-of-the-art communication technologies. As "netizens" demand a seamlessnetworking experience that entails not only higher speeds, but also resilienceand robustness to failures and malicious cyber-attacks, ample opportunities forsignal processing (SP) research arise. The vision is for ubiquitous smartnetwork devices to enable data-driven statistical learning algorithms fordistributed, robust, and online network operation and management, adaptable tothe dynamically-evolving network landscape with minimal need for humanintervention. The present paper aims at delineating the analytical backgroundand the relevance of SP tools to dynamic network monitoring, introducing the SPreadership to the concept of dynamic network cartography -- a framework toconstruct maps of the dynamic network state in an efficient and scalable mannertailored to large-scale heterogeneous networks.
arxiv-2400-217 | Learning-Assisted Automated Reasoning with Flyspeck | http://arxiv.org/abs/1211.7012 | author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO published:2012-11-29 summary:The considerable mathematical knowledge encoded by the Flyspeck project iscombined with external automated theorem provers (ATPs) and machine-learningpremise selection methods trained on the proofs, producing an AI system capableof answering a wide range of mathematical queries automatically. Theperformance of this architecture is evaluated in a bootstrapping scenarioemulating the development of Flyspeck from axioms to the last theorem, eachtime using only the previous theorems and proofs. It is shown that 39% of the14185 theorems could be proved in a push-button mode (without any high-leveladvice and user interaction) in 30 seconds of real time on a fourteen-CPUworkstation. The necessary work involves: (i) an implementation of soundtranslations of the HOL Light logic to ATP formalisms: untyped first-order,polymorphic typed first-order, and typed higher-order, (ii) export of thedependency information from HOL Light and ATP proofs for the machine learners,and (iii) choice of suitable representations and methods for learning fromprevious proofs, and their integration as advisors with HOL Light. This work isdescribed and discussed here, and an initial analysis of the body of proofsthat were found fully automatically is provided.
arxiv-2400-218 | Orientation Determination from Cryo-EM images Using Least Unsquared Deviation | http://arxiv.org/abs/1211.7045 | author:Lanhui Wang, Amit Singer, Zaiwen Wen category:cs.LG math.NA math.OC q-bio.BM published:2012-11-29 summary:A major challenge in single particle reconstruction from cryo-electronmicroscopy is to establish a reliable ab-initio three-dimensional model usingtwo-dimensional projection images with unknown orientations. Common-lines basedmethods estimate the orientations without additional geometric information.However, such methods fail when the detection rate of common-lines is too lowdue to the high level of noise in the images. An approximation to the leastsquares global self consistency error was obtained using convex relaxation bysemidefinite programming. In this paper we introduce a more robust global selfconsistency error and show that the corresponding optimization problem can besolved via semidefinite relaxation. In order to prevent artificial clusteringof the estimated viewing directions, we further introduce a spectral norm termthat is added as a constraint or as a regularization term to the relaxedminimization problem. The resulted problems are solved by using either thealternating direction method of multipliers or an iteratively reweighted leastsquares procedure. Numerical experiments with both simulated and real imagesdemonstrate that the proposed methods significantly reduce the orientationestimation error when the detection rate of common-lines is low.
arxiv-2400-219 | On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes | http://arxiv.org/abs/1211.6898 | author:Bruno Scherrer, Boris Lesner category:cs.LG cs.AI published:2012-11-29 summary:We consider infinite-horizon stationary $\gamma$-discounted Markov DecisionProcesses, for which it is known that there exists a stationary optimal policy.Using Value and Policy Iteration with some error $\epsilon$ at each iteration,it is well-known that one can compute stationary policies that are$\frac{2\gamma}{(1-\gamma)^2}\epsilon$-optimal. After arguing that thisguarantee is tight, we develop variations of Value and Policy Iteration forcomputing non-stationary policies that can be up to$\frac{2\gamma}{1-\gamma}\epsilon$-optimal, which constitutes a significantimprovement in the usual situation when $\gamma$ is close to 1. Surprisingly,this shows that the problem of "computing near-optimal non-stationary policies"is much simpler than that of "computing near-optimal stationary policies".
arxiv-2400-220 | Automating rule generation for grammar checkers | http://arxiv.org/abs/1211.6887 | author:Marcin Miłkowski category:cs.CL cs.LG published:2012-11-29 summary:In this paper, I describe several approaches to automatic or semi-automaticdevelopment of symbolic rules for grammar checkers from the informationcontained in corpora. The rules obtained this way are an important addition tomanually-created rules that seem to dominate in rule-based checkers. However,the manual process of creation of rules is costly, time-consuming anderror-prone. It seems therefore advisable to use machine-learning algorithms tocreate the rules automatically or semi-automatically. The results obtained seemto corroborate my initial hypothesis that symbolic machine learning algorithmscan be useful for acquiring new rules for grammar checking. It turns out,however, that for practical uses, error corpora cannot be the sole source ofinformation used in grammar checking. I suggest therefore that only by usingdifferent approaches, grammar-checkers, or more generally, computer-aidedproofreading tools, will be able to cover most frequent and severe mistakes andavoid false alarms that seem to distract users.
arxiv-2400-221 | Overlapping clustering based on kernel similarity metric | http://arxiv.org/abs/1211.6859 | author:Chiheb-Eddine Ben N'Cir, Nadia Essoussi, Patrice Bertrand category:stat.ML cs.LG stat.ME published:2012-11-29 summary:Producing overlapping schemes is a major issue in clustering. Recent proposedoverlapping methods relies on the search of an optimal covering and are basedon different metrics, such as Euclidean distance and I-Divergence, used tomeasure closeness between observations. In this paper, we propose the use ofanother measure for overlapping clustering based on a kernel similarity metric.We also estimate the number of overlapped clusters using the Gram matrix.Experiments on both Iris and EachMovie datasets show the correctness of theestimation of number of clusters and show that measure based on kernelsimilarity metric improves the precision, recall and f-measure in overlappingclustering.
arxiv-2400-222 | Classification Recouvrante Basée sur les Méthodes à Noyau | http://arxiv.org/abs/1211.6851 | author:Chiheb-Eddine Ben N'Cir, Nadia Essoussi category:cs.LG stat.CO stat.ME stat.ML published:2012-11-29 summary:Overlapping clustering problem is an important learning issue in whichclusters are not mutually exclusive and each object may belongs simultaneouslyto several clusters. This paper presents a kernel based method that producesoverlapping clusters on a high feature space using mercer kernel techniques toimprove separability of input patterns. The proposed method, calledOKM-K(Overlapping $k$-means based kernel method), extends OKM (Overlapping$k$-means) method to produce overlapping schemes. Experiments are performed onoverlapping dataset and empirical results obtained with OKM-K outperformresults obtained with OKM.
arxiv-2400-223 | Letter counting: a stem cell for Cryptology, Quantitative Linguistics, and Statistics | http://arxiv.org/abs/1211.6847 | author:Bernard Ycart category:math.HO cs.CL cs.CR published:2012-11-29 summary:Counting letters in written texts is a very ancient practice. It hasaccompanied the development of Cryptology, Quantitative Linguistics, andStatistics. In Cryptology, counting frequencies of the different characters inan encrypted message is the basis of the so called frequency analysis method.In Quantitative Linguistics, the proportion of vowels to consonants indifferent languages was studied long before authorship attribution. InStatistics, the alternation vowel-consonants was the only example that Markovever gave of his theory of chained events. A short history of letter countingis presented. The three domains, Cryptology, Quantitative Linguistics, andStatistics, are then examined, focusing on the interactions with the other twofields through letter counting. As a conclusion, the eclectism of pastcenturies scholars, their background in humanities, and their familiarity withcryptograms, are identified as contributing factors to the mutual enrichmentprocess which is described here.
arxiv-2400-224 | A New Automatic Method to Adjust Parameters for Object Recognition | http://arxiv.org/abs/1211.6971 | author:Issam Qaffou, Mohamed Sadgal, Aziz Elfazziki category:cs.CV cs.AI published:2012-11-29 summary:To recognize an object in an image, the user must apply a combination ofoperators, where each operator has a set of parameters. These parameters mustbe well adjusted in order to reach good results. Usually, this adjustment ismade manually by the user. In this paper we propose a new method to automatethe process of parameter adjustment for an object recognition task. Our methodis based on reinforcement learning, we use two types of agents: User Agent thatgives the necessary information and Parameter Agent that adjusts the parametersof each operator. Due to the nature of reinforcement learning the results donot depend only on the system characteristics but also on the user favoritechoices.
arxiv-2400-225 | On unbiased performance evaluation for protein inference | http://arxiv.org/abs/1211.6834 | author:Zengyou He, Ting Huang, Peijun Zhu category:stat.AP cs.LG q-bio.QM published:2012-11-29 summary:This letter is a response to the comments of Serang (2012) on Huang and He(2012) in Bioinformatics. Serang (2012) claimed that the parameters for theFido algorithm should be specified using the grid search method in Serang etal. (2010) so as to generate a deserved accuracy in performance comparison. Itseems that it is an argument on parameter tuning. However, it is indeed theissue of how to conduct an unbiased performance evaluation for comparingdifferent protein inference algorithms. In this letter, we would explain why wedon't use the grid search for parameter selection in Huang and He (2012) andshow that this procedure may result in an over-estimated performance that isunfair to competing algorithms. In fact, this issue has also been pointed outby Li and Radivojac (2012).
arxiv-2400-226 | SVD Based Image Processing Applications: State of The Art, Contributions and Research Challenges | http://arxiv.org/abs/1211.7102 | author:Rowayda A. Sadek category:cs.CV cs.MM published:2012-11-29 summary:Singular Value Decomposition (SVD) has recently emerged as a new paradigm forprocessing different types of images. SVD is an attractive algebraic transformfor image processing applications. The paper proposes an experimental surveyfor the SVD as an efficient transform in image processing applications. Despitethe well-known fact that SVD offers attractive properties in imaging, theexploring of using its properties in various image applications is currently atits infancy. Since the SVD has many attractive properties have not beenutilized, this paper contributes in using these generous properties in newlyimage applications and gives a highly recommendation for more researchchallenges. In this paper, the SVD properties for images are experimentallypresented to be utilized in developing new SVD-based image processingapplications. The paper offers survey on the developed SVD based imageapplications. The paper also proposes some new contributions that wereoriginated from SVD properties analysis in different image processing. The aimof this paper is to provide a better understanding of the SVD in imageprocessing and identify important various applications and open researchdirections in this increasingly important area; SVD based image processing inthe future research.
arxiv-2400-227 | Exact and Efficient Parallel Inference for Nonparametric Mixture Models | http://arxiv.org/abs/1211.7120 | author:Sinead A. Williamson, Avinava Dubey, Eric P. Xing category:stat.ML published:2012-11-29 summary:Nonparametric mixture models based on the Dirichlet process are an elegantalternative to finite models when the number of underlying components isunknown, but inference in such models can be slow. Existing attempts toparallelize inference in such models have relied on introducing approximations,which can lead to inaccuracies in the posterior estimate. In this paper, wedescribe auxiliary variable representations for the Dirichlet process and thehierarchical Dirichlet process that allow us to sample from the true posteriorin a distributed manner. We show that our approach allows scalable inferencewithout the deterioration in estimate quality that accompanies existingmethods.
arxiv-2400-228 | TACT: A Transfer Actor-Critic Learning Framework for Energy Saving in Cellular Radio Access Networks | http://arxiv.org/abs/1211.6616 | author:Rongpeng Li, Zhifeng Zhao, Xianfu Chen, Jacques Palicot, Honggang Zhang category:cs.NI cs.AI cs.IT cs.LG math.IT published:2012-11-28 summary:Recent works have validated the possibility of improving energy efficiency inradio access networks (RANs), achieved by dynamically turning on/off some basestations (BSs). In this paper, we extend the research over BS switchingoperations, which should match up with traffic load variations. Instead ofdepending on the dynamic traffic loads which are still quite challenging toprecisely forecast, we firstly formulate the traffic variations as a Markovdecision process. Afterwards, in order to foresightedly minimize the energyconsumption of RANs, we design a reinforcement learning framework based BSswitching operation scheme. Furthermore, to avoid the underlying curse ofdimensionality in reinforcement learning, a transfer actor-critic algorithm(TACT), which utilizes the transferred learning expertise in historical periodsor neighboring regions, is proposed and provably converges. In the end, weevaluate our proposed scheme by extensive simulations under various practicalconfigurations and show that the proposed TACT algorithm contributes to aperformance jumpstart and demonstrates the feasibility of significant energyefficiency improvement at the expense of tolerable delay performance.
arxiv-2400-229 | Nonparametric Bayesian Mixed-effect Model: a Sparse Gaussian Process Approach | http://arxiv.org/abs/1211.6653 | author:Yuyang Wang, Roni Khardon category:cs.LG stat.ML published:2012-11-28 summary:Multi-task learning models using Gaussian processes (GP) have been developedand successfully applied in various applications. The main difficulty with thisapproach is the computational cost of inference using the union of examplesfrom all tasks. Therefore sparse solutions, that avoid using the entire datadirectly and instead use a set of informative "representatives" are desirable.The paper investigates this problem for the grouped mixed-effect GP model whereeach individual response is given by a fixed-effect, taken from one of a set ofunknown groups, plus a random individual effect function that capturesvariations among individuals. Such models have been widely used in previouswork but no sparse solutions have been developed. The paper presents the firstsparse solution for such problems, showing how the sparse approximation can beobtained by maximizing a variational lower bound on the marginal likelihood,generalizing ideas from single-task Gaussian processes to handle themixed-effect model as well as grouping. Experiments using artificial and realdata validate the approach showing that it can recover the performance ofinference with the full sample, that it outperforms baseline methods, and thatit outperforms state of the art sparse solutions for other multi-task GPformulations.
arxiv-2400-230 | Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs | http://arxiv.org/abs/1211.6581 | author:Eleftherios Spyromitros-Xioufis, Grigorios Tsoumakas, William Groves, Ioannis Vlahavas category:cs.LG published:2012-11-28 summary:In many practical applications of supervised learning the task involves theprediction of multiple target variables from a common set of input variables.When the prediction targets are binary the task is called multi-labelclassification, while when the targets are continuous the task is calledmulti-target regression. In both tasks, target variables often exhibitstatistical dependencies and exploiting them in order to improve predictiveaccuracy is a core challenge. A family of multi-label classification methodsaddress this challenge by building a separate model for each target on anexpanded input space where other targets are treated as additional inputvariables. Despite the success of these methods in the multi-labelclassification domain, their applicability and effectiveness in multi-targetregression has not been studied until now. In this paper, we introduce two newmethods for multi-target regression, called Stacked Single-Target and Ensembleof Regressor Chains, by adapting two popular multi-label classification methodsof this family. Furthermore, we highlight an inherent problem of these methods- a discrepancy of the values of the additional input variables betweentraining and prediction - and develop extensions that use out-of-sampleestimates of the target variables during training in order to tackle thisproblem. The results of an extensive experimental evaluation carried out on alarge and diverse collection of datasets show that, when the discrepancy isappropriately mitigated, the proposed methods attain consistent improvementsover the independent regressions baseline. Moreover, two versions of Ensembleof Regression Chains perform significantly better than four state-of-the-artmethods including regularization-based multi-task learning methods and amulti-objective random forest approach.
arxiv-2400-231 | Robustness Analysis of Hottopixx, a Linear Programming Model for Factoring Nonnegative Matrices | http://arxiv.org/abs/1211.6687 | author:Nicolas Gillis category:stat.ML cs.LG cs.NA math.OC published:2012-11-28 summary:Although nonnegative matrix factorization (NMF) is NP-hard in general, it hasbeen shown very recently that it is tractable under the assumption that theinput nonnegative data matrix is close to being separable (separabilityrequires that all columns of the input matrix belongs to the cone spanned by asmall subset of these columns). Since then, several algorithms have beendesigned to handle this subclass of NMF problems. In particular, Bittorf,Recht, R\'e and Tropp (`Factoring nonnegative matrices with linear programs',NIPS 2012) proposed a linear programming model, referred to as Hottopixx. Inthis paper, we provide a new and more general robustness analysis of theirmethod. In particular, we design a provably more robust variant using apost-processing strategy which allows us to deal with duplicates and nearduplicates in the dataset.
arxiv-2400-232 | Nature-Inspired Mateheuristic Algorithms: Success and New Challenges | http://arxiv.org/abs/1211.6658 | author:Xin-She Yang category:math.OC cs.NE 90C26 published:2012-11-28 summary:Despite the increasing popularity of metaheuristics, many crucially importantquestions remain unanswered. There are two important issues: theoreticalframework and the gap between theory and applications. At the moment, thepractice of metaheuristics is like heuristic itself, to some extent, by trialand error. Mathematical analysis lags far behind, apart from a few, limited,studies on convergence analysis and stability, there is no theoreticalframework for analyzing metaheuristic algorithms. I believe mathematical andstatistical methods using Markov chains and dynamical systems can be veryuseful in the future work. There is no doubt that any theoretical progress willprovide potentially huge insightful into meteheuristic algorithms.
arxiv-2400-233 | Nonlinear Dynamic Field Embedding: On Hyperspectral Scene Visualization | http://arxiv.org/abs/1211.6675 | author:Dalton Lunga 'and' Okan Ersoy category:cs.CV cs.CE stat.ML published:2012-11-28 summary:Graph embedding techniques are useful to characterize spectral signaturerelations for hyperspectral images. However, such images consists of disjointclasses due to spatial details that are often ignored by existing graphcomputing tools. Robust parameter estimation is a challenge for kernelfunctions that compute such graphs. Finding a corresponding high qualitycoordinate system to map signature relations remains an open research question.We answer positively on these challenges by first proposing a kernel functionof spatial and spectral information in computing neighborhood graphs. Secondly,the study exploits the force field interpretation from mechanics and devise aunifying nonlinear graph embedding framework. The generalized framework leadsto novel unsupervised multidimensional artificial field embedding techniquesthat rely on the simple additive assumption of pair-dependent attraction andrepulsion functions. The formulations capture long range and short rangedistance related effects often associated with living organisms and help toestablish algorithmic properties that mimic mutual behavior for the purpose ofdimensionality reduction. The main benefits from the proposed models includesthe ability to preserve the local topology of data and produce qualityvisualizations i.e. maintaining disjoint meaningful neighborhoods. As part ofevaluation, visualization, gradient field trajectories, and semisupervisedclassification experiments are conducted for image scenes acquired by multiplesensors at various spatial resolutions over different types of objects. Theresults demonstrate the superiority of the proposed embedding framework overvarious widely used methods.
arxiv-2400-234 | Graph Laplacians on Singular Manifolds: Toward understanding complex spaces: graph Laplacians on manifolds with singularities and boundaries | http://arxiv.org/abs/1211.6727 | author:Mikhail Belkin, Qichao Que, Yusu Wang, Xueyuan Zhou category:cs.AI cs.CG cs.LG published:2012-11-28 summary:Recently, much of the existing work in manifold learning has been done underthe assumption that the data is sampled from a manifold without boundaries andsingularities or that the functions of interest are evaluated away from suchpoints. At the same time, it can be argued that singularities and boundariesare an important aspect of the geometry of realistic data. In this paper we consider the behavior of graph Laplacians at points at ornear boundaries and two main types of other singularities: intersections, wheredifferent manifolds come together and sharp "edges", where a manifold sharplychanges direction. We show that the behavior of graph Laplacian near thesesingularities is quite different from that in the interior of the manifolds. Infact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis ofFourier series, can be observed in the behavior of graph Laplacian near suchpoints. Unlike in the interior of the domain, where graph Laplacian convergesto the Laplace-Beltrami operator, near singularities graph Laplacian tends to afirst-order differential operator, which exhibits different scaling behavior asa function of the kernel width. One important implication is that while pointsnear the singularities occupy only a small part of the total volume, thedifference in scaling results in a disproportionately large contribution to thetotal behavior. Another significant finding is that while the scaling behaviorof the operator is the same near different types of singularities, they arevery distinct at a more refined level of analysis. We believe that a comprehensive understanding of these structures in additionto the standard case of a smooth manifold can take us a long way toward bettermethods for analysis of complex non-linear data and can lead to significantprogress in algorithm design.
arxiv-2400-235 | A LASSO-Penalized BIC for Mixture Model Selection | http://arxiv.org/abs/1211.6451 | author:Sakyajit Bhattacharya, Paul D. McNicholas category:stat.ME math.ST stat.CO stat.ML stat.TH published:2012-11-27 summary:The efficacy of family-based approaches to mixture model-based clustering andclassification depends on the selection of parsimonious models. Current wisdomsuggests the Bayesian information criterion (BIC) for mixture model selection.However, the BIC has well-known limitations, including a tendency tooverestimate the number of components as well as a proclivity for, oftendrastically, underestimating the number of components in higher dimensions.While the former problem might be soluble through merging components, thelatter is impossible to mitigate in clustering and classification applications.In this paper, a LASSO-penalized BIC (LPBIC) is introduced to overcome thisproblem. This approach is illustrated based on applications of extensions ofmixtures of factor analyzers, where the LPBIC is used to select both the numberof components and the number of latent factors. The LPBIC is shown to match oroutperform the BIC in several situations.
arxiv-2400-236 | Neuro-Fuzzy Computing System with the Capacity of Implementation on Memristor-Crossbar and Optimization-Free Hardware Training | http://arxiv.org/abs/1211.6205 | author:Farnood Merrikh-Bayat, Farshad Merrikh-Bayat, Saeed Bagheri Shouraki category:cs.NE cs.AI published:2012-11-27 summary:In this paper, first we present a new explanation for the relation betweenlogical circuits and artificial neural networks, logical circuits and fuzzylogic, and artificial neural networks and fuzzy inference systems. Then, basedon these results, we propose a new neuro-fuzzy computing system which caneffectively be implemented on the memristor-crossbar structure. One importantfeature of the proposed system is that its hardware can directly be trainedusing the Hebbian learning rule and without the need to any optimization. Thesystem also has a very good capability to deal with huge number of input-outtraining data without facing problems like overtraining.
arxiv-2400-237 | A simple non-parametric Topic Mixture for Authors and Documents | http://arxiv.org/abs/1211.6248 | author:Arnim Bleier category:cs.LG stat.ML published:2012-11-27 summary:This article reviews the Author-Topic Model and presents a new non-parametricextension based on the Hierarchical Dirichlet Process. The extension isespecially suitable when no prior information about the number of componentsnecessary is available. A blocked Gibbs sampler is described and focus put onstaying as close as possible to the original model with only the minimum oftheoretical and implementation overhead necessary.
arxiv-2400-238 | Duality between subgradient and conditional gradient methods | http://arxiv.org/abs/1211.6302 | author:Francis Bach category:cs.LG math.OC stat.ML published:2012-11-27 summary:Given a convex optimization problem and its dual, there are many possiblefirst-order algorithms. In this paper, we show the equivalence between mirrordescent algorithms and algorithms generalizing the conditional gradient method.This is done through convex duality, and implies notably that for certainproblems, such as for supervised machine learning problems with non-smoothlosses or problems regularized by non-smooth regularizers, the primalsubgradient method and the dual conditional gradient method are formallyequivalent. The dual interpretation leads to a form of line search for mirrordescent, as well as guarantees of convergence for primal-dual certificates.
arxiv-2400-239 | Random Projections for Linear Support Vector Machines | http://arxiv.org/abs/1211.6085 | author:Saurabh Paul, Christos Boutsidis, Malik Magdon-Ismail, Petros Drineas category:cs.LG stat.ML published:2012-11-26 summary:Let X be a data matrix of rank \rho, whose rows represent n points ind-dimensional space. The linear support vector machine constructs a hyperplaneseparator that maximizes the 1-norm soft margin. We develop a new obliviousdimension reduction technique which is precomputed and can be applied to anyinput matrix X. We prove that, with high probability, the margin and minimumenclosing ball in the feature space are preserved to within \epsilon-relativeerror, ensuring comparable generalization as in the original space in the caseof classification. For regression, we show that the margin is preserved to\epsilon-relative error with high probability. We present extensive experimentswith real and synthetic data to support our theory.
arxiv-2400-240 | Bayesian learning of noisy Markov decision processes | http://arxiv.org/abs/1211.5901 | author:Sumeetpal S. Singh, Nicolas Chopin, Nick Whiteley category:stat.ML cs.LG stat.CO published:2012-11-26 summary:We consider the inverse reinforcement learning problem, that is, the problemof learning from, and then predicting or mimicking a controller based onstate/action data. We propose a statistical model for such data, derived fromthe structure of a Markov decision process. Adopting a Bayesian approach toinference, we show how latent variables of the model can be estimated, and howpredictions about actions can be made, in a unified framework. A new Markovchain Monte Carlo (MCMC) sampler is devised for simulation from the posteriordistribution. This step includes a parameter expansion step, which is shown tobe essential for good convergence properties of the MCMC sampler. As anillustration, the method is applied to learning a human controller.
arxiv-2400-241 | Efficient algorithms for robust recovery of images from compressed data | http://arxiv.org/abs/1211.7276 | author:Duc Son Pham, Svetha Venkatesh category:cs.IT cs.LG math.IT stat.ML published:2012-11-26 summary:Compressed sensing (CS) is an important theory for sub-Nyquist sampling andrecovery of compressible data. Recently, it has been extended by Pham andVenkatesh to cope with the case where corruption to the CS data is modeled asimpulsive noise. The new formulation, termed as robust CS, combines robuststatistics and CS into a single framework to suppress outliers in the CSrecovery. To solve the newly formulated robust CS problem, Pham and Venkateshsuggested a scheme that iteratively solves a number of CS problems, thesolutions from which converge to the true robust compressed sensing solution.However, this scheme is rather inefficient as it has to use existing CS solversas a proxy. To overcome limitation with the original robust CS algorithm, wepropose to solve the robust CS problem directly in this paper and drive morecomputationally efficient algorithms by following latest advances inlarge-scale convex optimization for non-smooth regularization. Furthermore, wealso extend the robust CS formulation to various settings, including additionalaffine constraints, $\ell_1$-norm loss function, mixed-norm regularization, andmulti-tasking, so as to further improve robust CS. We also derive simple buteffective algorithms to solve these extensions. We demonstrate that the newalgorithms provide much better computational advantage over the original robustCS formulation, and effectively solve more sophisticated extensions where theoriginal methods simply cannot. We demonstrate the usefulness of the extensionson several CS imaging tasks.
arxiv-2400-242 | An Automatic Algorithm for Object Recognition and Detection Based on ASIFT Keypoints | http://arxiv.org/abs/1211.5829 | author:Reza Oji category:cs.AI cs.CV published:2012-11-26 summary:Object recognition is an important task in image processing and computervision. This paper presents a perfect method for object recognition with fullboundary detection by combining affine scale invariant feature transform(ASIFT) and a region merging algorithm. ASIFT is a fully affine invariantalgorithm that means features are invariant to six affine parameters namelytranslation (2 parameters), zoom, rotation and two camera axis orientations.The features are very reliable and give us strong keypoints that can be usedfor matching between different images of an object. We trained an object inseveral images with different aspects for finding best keypoints of it. Then, arobust region merging algorithm is used to recognize and detect the object withfull boundary in the other images based on ASIFT keypoints and a similaritymeasure for merging regions in the image. Experimental results show that thepresented method is very efficient and powerful to recognize the object anddetect it with high accuracy.
arxiv-2400-243 | The Interplay Between Stability and Regret in Online Learning | http://arxiv.org/abs/1211.6158 | author:Ankan Saha, Prateek Jain, Ambuj Tewari category:cs.LG stat.ML published:2012-11-26 summary:This paper considers the stability of online learning algorithms and itsimplications for learnability (bounded regret). We introduce a novel quantitycalled {\em forward regret} that intuitively measures how good an onlinelearning algorithm is if it is allowed a one-step look-ahead into the future.We show that given stability, bounded forward regret is equivalent to boundedregret. We also show that the existence of an algorithm with bounded regretimplies the existence of a stable algorithm with bounded regret and boundedforward regret. The equivalence results apply to general, possibly non-convexproblems. To the best of our knowledge, our analysis provides the first generalconnection between stability and regret in the online setting that is notrestricted to a particular class of algorithms. Our stability-regret connectionprovides a simple recipe for analyzing regret incurred by any online learningalgorithm. Using our framework, we analyze several existing online learningalgorithms as well as the "approximate" versions of algorithms like RDA thatsolve an optimization problem at each iteration. Our proofs are simpler thanexisting analysis for the respective algorithms, show a clear trade-off betweenstability and forward regret, and provide tighter regret bounds in some cases.Furthermore, using our recipe, we analyze "approximate" versions of severalalgorithms such as follow-the-regularized-leader (FTRL) that requires solvingan optimization problem at each step.
arxiv-2400-244 | Online Stochastic Optimization with Multiple Objectives | http://arxiv.org/abs/1211.6013 | author:Mehrdad Mahdavi, Tianbao Yang, Rong Jin category:cs.LG math.OC published:2012-11-26 summary:In this paper we propose a general framework to characterize and solve thestochastic optimization problems with multiple objectives underlying many realworld learning applications. We first propose a projection based algorithmwhich attains an $O(T^{-1/3})$ convergence rate. Then, by leveraging on thetheory of Lagrangian in constrained optimization, we devise a novel primal-dualstochastic approximation algorithm which attains the optimal convergence rateof $O(T^{-1/2})$ for general Lipschitz continuous objectives.
arxiv-2400-245 | Texture Modeling with Convolutional Spike-and-Slab RBMs and Deep Extensions | http://arxiv.org/abs/1211.5687 | author:Heng Luo, Pierre Luc Carrier, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML published:2012-11-24 summary:We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texturemodeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves orsurpasses the state-of-the-art on texture synthesis and inpainting byparametric models. We also develop a novel RBM model with a spike-and-slabvisible layer and binary variables in the hidden layer. This model is designedto be stacked on top of the TssRBM. We show the resulting deep belief network(DBN) is a powerful generative model that improves on single-layer models andis capable of modeling not only single high-resolution and challenging texturesbut also multiple textures.
arxiv-2400-246 | New Hoopoe Heuristic Optimization | http://arxiv.org/abs/1211.6410 | author:Mohammed El-Dosuky, Ahmed EL-Bassiouny, Taher Hamza, Magdy Rashad category:cs.NE cs.AI published:2012-11-24 summary:Most optimization problems in real life applications are often highlynonlinear. Local optimization algorithms do not give the desired performance.So, only global optimization algorithms should be used to obtain optimalsolutions. This paper introduces a new nature-inspired metaheuristicoptimization algorithm, called Hoopoe Heuristic (HH). In this paper, we willstudy HH and validate it against some test functions. Investigations show thatit is very promising and could be seen as an optimization of the powerfulalgorithm of cuckoo search. Finally, we discuss the features of HoopoeHeuristic and propose topics for further studies.
arxiv-2400-247 | Detection of elliptical shapes via cross-entropy clustering | http://arxiv.org/abs/1211.5712 | author:Jacek Tabor, Krzysztof Misztal category:cs.CV published:2012-11-24 summary:The problem of finding elliptical shapes in an image will be considered. Wediscuss the solution which uses cross-entropy clustering. The proposed methodallows the search for ellipses with predefined sizes and position in the space.Moreover, it works well for search of ellipsoids in higher dimensions.
arxiv-2400-248 | Theano: new features and speed improvements | http://arxiv.org/abs/1211.5590 | author:Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio category:cs.SC cs.LG published:2012-11-23 summary:Theano is a linear algebra compiler that optimizes a user'ssymbolically-specified mathematical computations to produce efficient low-levelimplementations. In this paper, we present new features and efficiencyimprovements to Theano, and benchmarks demonstrating Theano's performancerelative to Torch7, a recently introduced machine learning library, and toRNNLM, a C++ library targeted at recurrent neural networks.
arxiv-2400-249 | Improving Perceptual Color Difference using Basic Color Terms | http://arxiv.org/abs/1211.5556 | author:Ofir Pele, Michael Werman category:cs.CV cs.GR published:2012-11-23 summary:We suggest a new color distance based on two observations. First, perceptualcolor differences were designed to be used to compare very similar colors. Theydo not capture human perception for medium and large color differences well.Thresholding was proposed to solve the problem for large color differences,i.e. two totally different colors are always the same distance apart. We showthat thresholding alone cannot improve medium color differences. We suggest toalleviate this problem using basic color terms. Second, when a color distanceis used for edge detection, many small distances around the just noticeabledifference may account for false edges. We suggest to reduce the effect ofsmall distances.
arxiv-2400-250 | Ecosystem-Oriented Distributed Evolutionary Computing | http://arxiv.org/abs/1211.5400 | author:Gerard Briscoe, Philippe De Wilde category:cs.NE published:2012-11-23 summary:We create a novel optimisation technique inspired by natural ecosystems,where the optimisation works at two levels: a first optimisation, migration ofgenes which are distributed in a peer-to-peer network, operating continuouslyin time; this process feeds a second optimisation based on evolutionarycomputing that operates locally on single peers and is aimed at findingsolutions to satisfy locally relevant constraints. We consider from the domainof computer science distributed evolutionary computing, with the relevanttheory from the domain of theoretical biology, including the fields ofevolutionary and ecological theory, the topological structure of ecosystems,and evolutionary processes within distributed environments. We then defineecosystem- oriented distributed evolutionary computing, imbibed with theproperties of self-organisation, scalability and sustainability from naturalecosystems, including a novel form of distributed evolu- tionary computing.Finally, we conclude with a discussion of the apparent compromises resultingfrom the hybrid model created, such as the network topology.
arxiv-2400-251 | Genetic Algorithm Modeling with GPU Parallel Computing Technology | http://arxiv.org/abs/1211.5481 | author:Stefano Cavuoti, Mauro Garofalo, Massimo Brescia, Antonio Pescapé, Giuseppe Longo, Giorgio Ventre category:astro-ph.IM cs.DC cs.NE published:2012-11-23 summary:We present a multi-purpose genetic algorithm, designed and implemented withGPGPU / CUDA parallel computing technology. The model was derived from amulti-core CPU serial implementation, named GAME, already scientificallysuccessfully tested and validated on astrophysical massive data classificationproblems, through a web application resource (DAMEWARE), specialized in datamining based on Machine Learning paradigms. Since genetic algorithms areinherently parallel, the GPGPU computing paradigm has provided an exploit ofthe internal training features of the model, permitting a strong optimizationin terms of processing performances and scalability.
arxiv-2400-252 | A Hash based Approach for Secure Keyless Steganography in Lossless RGB Images | http://arxiv.org/abs/1211.5614 | author:Ankit Chaudhary, J. Vasavada, J. L. Raheja, S. Kumar, M. Sharma category:cs.CR cs.CV cs.MM published:2012-11-23 summary:This paper proposes an improved steganography approach for hiding textmessages in lossless RGB images. The objective of this work is to increase thesecurity level and to improve the storage capacity with compression techniques.The security level is increased by randomly distributing the text message overthe entire image instead of clustering within specific image portions. Storagecapacity is increased by utilizing all the color channels for storinginformation and providing the source text message compression. The degradationof the images can be minimized by changing only one least significant bit percolor channel for hiding the message, incurring a very little change in theoriginal image. Using steganography alone with simple LSB has a potentialproblem that the secret message is easily detectable from the histogramanalysis method. To improve the security as well as the image embeddingcapacity indirectly, a compression based scheme is introduced. Various testshave been done to check the storage capacity and message distribution. Thesetestes show the superiority of the proposed approach with respect to otherexisting approaches.
arxiv-2400-253 | Analysis of a randomized approximation scheme for matrix multiplication | http://arxiv.org/abs/1211.5414 | author:Daniel Hsu, Sham M. Kakade, Tong Zhang category:cs.DS cs.LG cs.NA stat.ML published:2012-11-23 summary:This note gives a simple analysis of a randomized approximation scheme formatrix multiplication proposed by Sarlos (2006) based on a random rotationfollowed by uniform column sampling. The result follows from a matrix versionof Bernstein's inequality and a tail inequality for quadratic forms insubgaussian random vectors.
arxiv-2400-254 | Service Composition Design Pattern for Autonomic Computing Systems using Association Rule based Learning and Service-Oriented Architecture | http://arxiv.org/abs/1211.5227 | author:Vishnuvardhan Mannava, T. Ramesh category:cs.SE cs.DC cs.LG published:2012-11-22 summary:In this paper we present a Service Injection and composition Design Patternfor Unstructured Peer-to-Peer networks, which is designed with Aspect-orienteddesign patterns, and amalgamation of the Strategy, Worker Object, andCheck-List Design Patterns used to design the Self-Adaptive Systems. It willapply self reconfiguration planes dynamically without the interruption orintervention of the administrator for handling service failures at the servers.When a client requests for a complex service, Service Composition should bedone to fulfil the request. If a service is not available in the memory, itwill be injected as Aspectual Feature Module code. We used Service OrientedArchitecture (SOA) with Web Services in Java to Implement the composite DesignPattern. As far as we know, there are no studies on composition of designpatterns for Peer-to-peer computing domain. The pattern is described using ajava-like notation for the classes and interfaces. A simple UML class andSequence diagrams are depicted.
arxiv-2400-255 | Optimally fuzzy temporal memory | http://arxiv.org/abs/1211.5189 | author:Karthik H. Shankar, Marc W. Howard category:cs.AI cs.LG published:2012-11-22 summary:Any learner with the ability to predict the future of a structuredtime-varying signal must maintain a memory of the recent past. If the signalhas a characteristic timescale relevant to future prediction, the memory can bea simple shift register---a moving window extending into the past, requiringstorage resources that linearly grows with the timescale to be represented.However, an independent general purpose learner cannot a priori know thecharacteristic prediction-relevant timescale of the signal. Moreover, manynaturally occurring signals show scale-free long range correlations implyingthat the natural prediction-relevant timescale is essentially unbounded. Hencethe learner should maintain information from the longest possible timescaleallowed by resource availability. Here we construct a fuzzy memory system thatoptimally sacrifices the temporal accuracy of information in a scale-freefashion in order to represent prediction-relevant information fromexponentially long timescales. Using several illustrative examples, wedemonstrate the advantage of the fuzzy memory system over a shift register intime series forecasting of natural signals. When the available storageresources are limited, we suggest that a general purpose learner would bebetter off committing to such a fuzzy memory system.
arxiv-2400-256 | On pattern recovery of the fused Lasso | http://arxiv.org/abs/1211.5194 | author:Junyang Qian, Jinzhu Jia category:stat.ML math.ST stat.TH published:2012-11-22 summary:We study the property of the Fused Lasso Signal Approximator (FLSA) forestimating a blocky signal sequence with additive noise. We transform the FLSAto an ordinary Lasso problem. By studying the property of the design matrix inthe transformed Lasso problem, we find that the irrepresentable condition mightnot hold, in which case we show that the FLSA might not be able to recover thesignal pattern. We then apply the newly developed preconditioning method --Puffer Transformation [Jia and Rohe, 2012] on the transformed Lasso problem. Wecall the new method the preconditioned fused Lasso and we give non-asymptoticresults for this method. Results show that when the signal jump strength(signal difference between two neighboring groups) is big and the noise levelis small, our preconditioned fused Lasso estimator gives the correct patternwith high probability. Theoretical results give insight on what controls thesignal pattern recovery ability -- it is the noise level {instead of} thelength of the sequence. Simulations confirm our theorems and show significantimprovement of the preconditioned fused Lasso estimator over the vanilla FLSA.
arxiv-2400-257 | Cobb Angle Measurement of Scoliosis with Reduced Variability | http://arxiv.org/abs/1211.5355 | author:Raka Kundu, Amlan Chakrabarti, Prasanna K. Lenka category:cs.CV published:2012-11-22 summary:Cobb angle, which is a measure of spinal curvature is the standard method forquantifying the magnitude of Scoliosis related to spinal deformity inorthopedics. Determining the Cobb angle through manual process is subject tohuman errors. In this work, we propose a methodology to measure the magnitudeof Cobb angle, which appreciably reduces the variability related to itsmeasurement compared to the related works. The proposed methodology isfacilitated by using a suitable new improved version of Non-Local Means forimage denoisation and Otsus automatic threshold selection for Canny edgedetection. We have selected NLM for preprocessing of the image as it is one ofthe fine states of art for image denoisation and helps in retaining the imagequality. Trimmedmean, median are more robust to outliners than mean andfollowing this concept we observed that NLM denoising quality performance canbe enhanced by using Euclidean trimmed-mean replacing the mean. To prove thebetter performance of the Non-Local Euclidean Trimmed-mean denoising filter, wehave provided some comparative study results of the proposed denoisingtechnique with traditional NLM and NonLocal Euclidean Medians. The experimentalresults for Cobb angle measurement over intra observer and inter observerexperimental data reveals the better performance and superiority of theproposed approach compared to the related works. MATLAB2009b image processingtoolbox was used for the purpose of simulation and verification of the proposedmethodology.
arxiv-2400-258 | A Hybrid Bacterial Foraging Algorithm For Solving Job Shop Scheduling Problems | http://arxiv.org/abs/1211.4971 | author:S. Narendhar, T. Amudha category:cs.NE published:2012-11-21 summary:Bio-Inspired computing is the subset of Nature-Inspired computing. Job ShopScheduling Problem is categorized under popular scheduling problems. In thisresearch work, Bacterial Foraging Optimization was hybridized with Ant ColonyOptimization and a new technique Hybrid Bacterial Foraging Optimization forsolving Job Shop Scheduling Problem was proposed. The optimal solutionsobtained by proposed Hybrid Bacterial Foraging Optimization algorithms are muchbetter when compared with the solutions obtained by Bacterial ForagingOptimization algorithm for well-known test problems of different sizes. Fromthe implementation of this research work, it could be observed that theproposed Hybrid Bacterial Foraging Optimization was effective than BacterialForaging Optimization algorithm in solving Job Shop Scheduling Problems. HybridBacterial Foraging Optimization is used to implement real world Job ShopScheduling Problems.
arxiv-2400-259 | Summarizing Reviews with Variable-length Syntactic Patterns and Topic Models | http://arxiv.org/abs/1211.4929 | author:Trung V. Nguyen, Alice H. Oh category:cs.IR cs.CL published:2012-11-21 summary:We present a novel summarization framework for reviews of products andservices by selecting informative and concise text segments from the reviews.Our method consists of two major steps. First, we identify five frequentlyoccurring variable-length syntactic patterns and use them to extract candidatesegments. Then we use the output of a joint generative sentiment topic model tofilter out the non-informative segments. We verify the proposed method withquantitative and qualitative experiments. In a quantitative study, our approachoutperforms previous methods in producing informative segments and summariesthat capture aspects of products and services as expressed in theuser-generated pros and cons lists. Our user study with ninety users resonateswith this result: individual segments extracted and filtered by our method arerated as more useful by users compared to previous approaches by users.
arxiv-2400-260 | Fast Marginalized Block Sparse Bayesian Learning Algorithm | http://arxiv.org/abs/1211.4909 | author:Benyuan Liu, Zhilin Zhang, Hongqi Fan, Qiang Fu category:cs.IT cs.LG math.IT stat.ML published:2012-11-21 summary:The performance of sparse signal recovery from noise corrupted,underdetermined measurements can be improved if both sparsity and correlationstructure of signals are exploited. One typical correlation structure is theintra-block correlation in block sparse signals. To exploit this structure, aframework, called block sparse Bayesian learning (BSBL), has been proposedrecently. Algorithms derived from this framework showed superior performancebut they are not very fast, which limits their applications. This work derivesan efficient algorithm from this framework, using a marginalized likelihoodmaximization method. Compared to existing BSBL algorithms, it has closerecovery performance but is much faster. Therefore, it is more suitable forlarge scale datasets and applications requiring real-time implementation.
arxiv-2400-261 | Bayesian nonparametric Plackett-Luce models for the analysis of preferences for college degree programmes | http://arxiv.org/abs/1211.5037 | author:François Caron, Yee Whye Teh, Thomas Brendan Murphy category:stat.ML cs.LG stat.ME published:2012-11-21 summary:In this paper we propose a Bayesian nonparametric model for clusteringpartial ranking data. We start by developing a Bayesian nonparametric extensionof the popular Plackett-Luce choice model that can handle an infinite number ofchoice items. Our framework is based on the theory of random atomic measures,with the prior specified by a completely random measure. We characterise theposterior distribution given data, and derive a simple and effective Gibbssampler for posterior simulation. We then develop a Dirichlet process mixtureextension of our model and apply it to investigate the clustering ofpreferences for college degree programmes amongst Irish secondary schoolgraduates. The existence of clusters of applicants who have similar preferencesfor degree programmes is established and we determine that subject matter andgeographical location of the third level institution characterise theseclusters.
arxiv-2400-262 | Mahotas: Open source software for scriptable computer vision | http://arxiv.org/abs/1211.4907 | author:Luis Pedro Coelho category:cs.CV cs.SE published:2012-11-21 summary:Mahotas is a computer vision library for Python. It contains traditionalimage processing functionality such as filtering and morphological operationsas well as more modern computer vision functions for feature computation,including interest point detection and local descriptors. The interface is in Python, a dynamic programming language, which is veryappropriate for fast development, but the algorithms are implemented in C++ andare tuned for speed. The library is designed to fit in with the scientificsoftware ecosystem in this language and can leverage the existinginfrastructure developed in that language. Mahotas is released under a liberal open source license (MIT License) and isavailable from (http://github.com/luispedro/mahotas) and from the PythonPackage Index (http://pypi.python.org/pypi/mahotas).
arxiv-2400-263 | On the difficulty of training Recurrent Neural Networks | http://arxiv.org/abs/1211.5063 | author:Razvan Pascanu, Tomas Mikolov, Yoshua Bengio category:cs.LG published:2012-11-21 summary:There are two widely known issues with properly training Recurrent NeuralNetworks, the vanishing and the exploding gradient problems detailed in Bengioet al. (1994). In this paper we attempt to improve the understanding of theunderlying issues by exploring these problems from an analytical, a geometricand a dynamical systems perspective. Our analysis is used to justify a simpleyet effective solution. We propose a gradient norm clipping strategy to dealwith exploding gradients and a soft constraint for the vanishing gradientsproblem. We validate empirically our hypothesis and proposed solutions in theexperimental section.
arxiv-2400-264 | Scaling Genetic Programming for Source Code Modification | http://arxiv.org/abs/1211.5098 | author:Brendan Cody-Kenny, Stephen Barrett category:cs.NE cs.SE published:2012-11-21 summary:In Search Based Software Engineering, Genetic Programming has been used forbug fixing, performance improvement and parallelisation of programs through themodification of source code. Where an evolutionary computation algorithm, suchas Genetic Programming, is to be applied to similar code manipulation tasks,the complexity and size of source code for real-world software poses ascalability problem. To address this, we intend to inspect how the SoftwareEngineering concepts of modularity, granularity and localisation of change canbe reformulated as additional mechanisms within a Genetic Programmingalgorithm.
arxiv-2400-265 | Forest Sparsity for Multi-channel Compressive Sensing | http://arxiv.org/abs/1211.4657 | author:Chen Chen, Yeqing Li, Junzhou Huang category:cs.LG cs.CV cs.IT math.IT stat.ML published:2012-11-20 summary:In this paper, we investigate a new compressive sensing model formulti-channel sparse data where each channel can be represented as ahierarchical tree and different channels are highly correlated. Therefore, thefull data could follow the forest structure and we call this property as\emph{forest sparsity}. It exploits both intra- and inter- channel correlationsand enriches the family of existing model-based compressive sensing theories.The proposed theory indicates that only $\mathcal{O}(Tk+\log(N/k))$measurements are required for multi-channel data with forest sparsity, where$T$ is the number of channels, $N$ and $k$ are the length and sparsity numberof each channel respectively. This result is much better than$\mathcal{O}(Tk+T\log(N/k))$ of tree sparsity, $\mathcal{O}(Tk+k\log(N/k))$ ofjoint sparsity, and far better than $\mathcal{O}(Tk+Tk\log(N/k))$ of standardsparsity. In addition, we extend the forest sparsity theory to the multiplemeasurement vectors problem, where the measurement matrix is a block-diagonalmatrix. The result shows that the required measurement bound can be the same asthat for dense random measurement matrix, when the data shares equal energy ineach channel. A new algorithm is developed and applied on four exampleapplications to validate the benefit of the proposed model. Extensiveexperiments demonstrate the effectiveness and efficiency of the proposed theoryand algorithm.
arxiv-2400-266 | Content based video retrieval | http://arxiv.org/abs/1211.4683 | author:B. V. Patel, B. B. Meshram category:cs.MM cs.CV published:2012-11-20 summary:Content based video retrieval is an approach for facilitating the searchingand browsing of large image collections over World Wide Web. In this approach,video analysis is conducted on low level visual properties extracted from videoframe. We believed that in order to create an effective video retrieval system,visual perception must be taken into account. We conjectured that a techniquewhich employs multiple features for indexing and retrieval would be moreeffective in the discrimination and search tasks of videos. In order tovalidate this claim, content based indexing and retrieval systems wereimplemented using color histogram, various texture features and otherapproaches. Videos were stored in Oracle 9i Database and a user study measuredcorrectness of response.
arxiv-2400-267 | Random Input Sampling for Complex Models Using Markov Chain Monte Carlo | http://arxiv.org/abs/1211.4706 | author:A. Gokcen Mahmutoglu, Alper T. Erdogan, Alper Demir category:stat.ML published:2012-11-20 summary:Many random processes can be simulated as the output of a deterministic modelaccepting random inputs. Such a model usually describes a complex mathematicalor physical stochastic system and the randomness is introduced in the inputvariables of the model. When the statistics of the output event are known,these input variables have to be chosen in a specific way for the output tohave the prescribed statistics. Because the probability distribution of theinput random variables is not directly known but dictated implicitly by thestatistics of the output random variables, this problem is usually intractablefor classical sampling methods. Based on Markov Chain Monte Carlo we propose anovel method to sample random inputs to such models by introducing amodification to the standard Metropolis-Hastings algorithm. As an example weconsider a system described by a stochastic differential equation (sde) anddemonstrate how sample paths of a random process satisfying this sde can begenerated with our technique.
arxiv-2400-268 | A Brief Review of Data Mining Application Involving Protein Sequence Classification | http://arxiv.org/abs/1211.4866 | author:Suprativ Saha, Rituparna Chaki category:cs.DB cs.NE published:2012-11-20 summary:Data mining techniques have been used by researchers for analyzing proteinsequences. In protein analysis, especially in protein sequence classification,selection of feature is most important. Popular protein sequence classificationtechniques involve extraction of specific features from the sequences.Researchers apply some well-known classification techniques like neuralnetworks, Genetic algorithm, Fuzzy ARTMAP, Rough Set Classifier etc foraccurate classification. This paper presents a review is with three differentclassification models such as neural network model, fuzzy ARTMAP model andRough set classifier model. A new technique for classifying protein sequenceshave been proposed in the end. The proposed technique tries to reduce thecomputational overheads encountered by earlier approaches and increase theaccuracy of classification.
arxiv-2400-269 | An Effective Method for Fingerprint Classification | http://arxiv.org/abs/1211.4658 | author:Monowar H. Bhuyan, Sarat Saharia, Dhruba Kr Bhattacharyya category:cs.CV cs.CR 68U35 I.5.3 published:2012-11-20 summary:This paper presents an effective method for fingerprint classification usingdata mining approach. Initially, it generates a numeric code sequence for eachfingerprint image based on the ridge flow patterns. Then for each class, a seedis selected by using a frequent itemsets generation technique. These seeds aresubsequently used for clustering the fingerprint images. The proposed methodwas tested and evaluated in terms of several real-life datasets and asignificant improvement in reducing the misclassification errors has beennoticed in comparison to its other counterparts.
arxiv-2400-270 | A unifying representation for a class of dependent random measures | http://arxiv.org/abs/1211.4753 | author:Nicholas J. Foti, Joseph D. Futoma, Daniel N. Rockmore, Sinead Williamson category:stat.ML cs.LG published:2012-11-20 summary:We present a general construction for dependent random measures based onthinning Poisson processes on an augmented space. The framework is notrestricted to dependent versions of a specific nonparametric model, but can beapplied to all models that can be represented using completely random measures.Several existing dependent random measures can be seen as specific cases ofthis framework. Interesting properties of the resulting measures are derivedand the efficacy of the framework is demonstrated by constructing acovariate-dependent latent feature model and topic model that obtain superiorpredictive performance.
arxiv-2400-271 | A Traveling Salesman Learns Bayesian Networks | http://arxiv.org/abs/1211.4888 | author:Tuhin Sahai, Stefan Klus, Michael Dellnitz category:cs.LG stat.ML published:2012-11-20 summary:Structure learning of Bayesian networks is an important problem that arisesin numerous machine learning applications. In this work, we present a novelapproach for learning the structure of Bayesian networks using the solution ofan appropriately constructed traveling salesman problem. In our approach, onecomputes an optimal ordering (partially ordered set) of random variables usingmethods for the traveling salesman problem. This ordering significantly reducesthe search space for the subsequent greedy optimization that computes the finalstructure of the Bayesian network. We demonstrate our approach of learningBayesian networks on real world census and weather datasets. In both cases, wedemonstrate that the approach very accurately captures dependencies betweenrandom variables. We check the accuracy of the predictions based on independentstudies in both application domains.
arxiv-2400-272 | Domain Adaptations for Computer Vision Applications | http://arxiv.org/abs/1211.4860 | author:Oscar Beijbom category:cs.CV cs.LG stat.ML published:2012-11-20 summary:A basic assumption of statistical learning theory is that train and test dataare drawn from the same underlying distribution. Unfortunately, this assumptiondoesn't hold in many applications. Instead, ample labeled data might exist in aparticular `source' domain while inference is needed in another, `target'domain. Domain adaptation methods leverage labeled data from both domains toimprove classification on unseen data in the target domain. In this work wesurvey domain transfer learning methods for various application domains withfocus on recent work in Computer Vision.
arxiv-2400-273 | A survey of non-exchangeable priors for Bayesian nonparametric models | http://arxiv.org/abs/1211.4798 | author:Nicholas J. Foti, Sinead Williamson category:stat.ML cs.LG published:2012-11-20 summary:Dependent nonparametric processes extend distributions over measures, such asthe Dirichlet process and the beta process, to give distributions overcollections of measures, typically indexed by values in some covariate space.Such models are appropriate priors when exchangeability assumptions do nothold, and instead we want our model to vary fluidly with some set ofcovariates. Since the concept of dependent nonparametric processes wasformalized by MacEachern [1], there have been a number of models proposed andused in the statistics and machine learning literatures. Many of these modelsexhibit underlying similarities, an understanding of which, we hope, will helpin selecting an appropriate prior, developing new models, and leveraginginference techniques.
arxiv-2400-274 | Matching Through Features and Features Through Matching | http://arxiv.org/abs/1211.4771 | author:Ganesh Sundaramoorthi, Yanchao Yang category:cs.CV published:2012-11-20 summary:This paper addresses how to construct features for the problem of imagecorrespondence, in particular, the paper addresses how to construct features soas to maintain the right level of invariance versus discriminability. We showthat without additional prior knowledge of the 3D scene, the right tradeoffcannot be established in a pre-processing step of the images as is typicallydone in most feature-based matching methods. However, given knowledge of thesecond image to match, the tradeoff between invariance and discriminability offeatures in the first image is less ambiguous. This suggests to setup theproblem of feature extraction and matching as a joint estimation problem. Wedevelop a possible mathematical framework, a possible computational algorithm,and we give example demonstration on finding correspondence on images relatedby a scene that undergoes large 3D deformation of non-planar objects and cameraviewpoint change.
arxiv-2400-275 | Application of three graph Laplacian based semi-supervised learning methods to protein function prediction problem | http://arxiv.org/abs/1211.4289 | author:Loc Tran category:cs.LG cs.CE q-bio.QM stat.ML H.2.8 published:2012-11-19 summary:Protein function prediction is the important problem in modern biology. Inthis paper, the un-normalized, symmetric normalized, and random walk graphLaplacian based semi-supervised learning methods will be applied to theintegrated network combined from multiple networks to predict the functions ofall yeast proteins in these multiple networks. These multiple networks arenetwork created from Pfam domain structure, co-participation in a proteincomplex, protein-protein interaction network, genetic interaction network, andnetwork created from cell cycle gene expression measurements. Multiple networksare combined with fixed weights instead of using convex optimization todetermine the combination weights due to high time complexity of convexoptimization method. This simple combination method will not affect theaccuracy performance measures of the three semi-supervised learning methods.Experiment results show that the un-normalized and symmetric normalized graphLaplacian based methods perform slightly better than random walk graphLaplacian based method for integrated network. Moreover, the accuracyperformance measures of these three semi-supervised learning methods forintegrated network are much better than the best accuracy performance measuresof these three methods for the individual network.
arxiv-2400-276 | Storing cycles in Hopfield-type networks with pseudoinverse learning rule: admissibility and network topology | http://arxiv.org/abs/1211.4520 | author:Chuan Zhang, Gerhard Dangelmayr, Iuliana Oprea category:cs.NE 15A04 published:2012-11-19 summary:Cyclic patterns of neuronal activity are ubiquitous in animal nervoussystems, and partially responsible for generating and controlling rhythmicmovements such as locomotion, respiration, swallowing and so on. Clarifying therole of the network connectivities for generating cyclic patterns isfundamental for understanding the generation of rhythmic movements. In thispaper, the storage of binary cycles in neural networks is investigated. We calla cycle $\Sigma$ admissible if a connectivity matrix satisfying the cycle'stransition conditions exists, and construct it using the pseudoinverse learningrule. Our main focus is on the structural features of admissible cycles andcorresponding network topology. We show that $\Sigma$ is admissible if and onlyif its discrete Fourier transform contains exactly $r={rank}(\Sigma)$ nonzerocolumns. Based on the decomposition of the rows of $\Sigma$ into loops, where aloop is the set of all cyclic permutations of a row, cycles are classified assimple cycles, separable or inseparable composite cycles. Simple cycles containrows from one loop only, and the network topology is a feedforward chain withfeedback to one neuron if the loop-vectors in $\Sigma$ are cyclic permutationsof each other. Composite cycles contain rows from at least two disjoint loops,and the neurons corresponding to the rows in $\Sigma$ from the same loop areidentified with a cluster. Networks constructed from separable composite cyclesdecompose into completely isolated clusters. For inseparable composite cyclesat least two clusters are connected, and the cluster-connectivity is related tothe intersections of the spaces spanned by the loop-vectors of the clusters.Simulations showing successfully retrieved cycles in continuous-timeHopfield-type networks and in networks of spiking neurons are presented.
arxiv-2400-277 | Smoothing Dynamic Systems with State-Dependent Covariance Matrices | http://arxiv.org/abs/1211.4601 | author:Aleksandr Y. Aravkin, James V. Burke category:math.OC stat.CO stat.ML 62F35, 65K10 published:2012-11-19 summary:Kalman filtering and smoothing algorithms are used in many areas, includingtracking and navigation, medical applications, and financial trend filtering.One of the basic assumptions required to apply the Kalman smoothing frameworkis that error covariance matrices are known and given. In this paper, we studya general class of inference problems where covariance matrices can dependfunctionally on unknown parameters. In the Kalman framework, this allowsmodeling situations where covariance matrices may depend functionally on thestate sequence being estimated. We present an extended formulation andgeneralized Gauss-Newton (GGN) algorithm for inference in this context. Whenapplied to dynamic systems inference, we show the algorithm can be implementedto preserve the computational efficiency of the classic Kalman smoother. Thenew approach is illustrated with a synthetic numerical example.
arxiv-2400-278 | Hypothesis Testing in Feedforward Networks with Broadcast Failures | http://arxiv.org/abs/1211.4518 | author:Zhenliang Zhang, Edwin K. P. Chong, Ali Pezeshki, William Moran category:cs.IT cs.LG math.IT published:2012-11-19 summary:Consider a countably infinite set of nodes, which sequentially make decisionsbetween two given hypotheses. Each node takes a measurement of the underlyingtruth, observes the decisions from some immediate predecessors, and makes adecision between the given hypotheses. We consider two classes of broadcastfailures: 1) each node broadcasts a decision to the other nodes, subject torandom erasure in the form of a binary erasure channel; 2) each node broadcastsa randomly flipped decision to the other nodes in the form of a binarysymmetric channel. We are interested in whether there exists a decisionstrategy consisting of a sequence of likelihood ratio tests such that the nodedecisions converge in probability to the underlying truth. In both cases, weshow that if each node only learns from a bounded number of immediatepredecessors, then there does not exist a decision strategy such that thedecisions converge in probability to the underlying truth. However, in case 1,we show that if each node learns from an unboundedly growing number ofpredecessors, then the decisions converge in probability to the underlyingtruth, even when the erasure probabilities converge to 1. We also derive theconvergence rate of the error probability. In case 2, we show that if each nodelearns from all of its previous predecessors, then the decisions converge inprobability to the underlying truth when the flipping probabilities of thebinary symmetric channels are bounded away from 1/2. In the case where theflipping probabilities converge to 1/2, we derive a necessary condition on theconvergence rate of the flipping probabilities such that the decisions stillconverge to the underlying truth. We also explicitly characterize therelationship between the convergence rate of the error probability and theconvergence rate of the flipping probabilities.
arxiv-2400-279 | Mixture Gaussian Process Conditional Heteroscedasticity | http://arxiv.org/abs/1211.4410 | author:Emmanouil A. Platanios, Sotirios P. Chatzis category:cs.LG stat.ML published:2012-11-19 summary:Generalized autoregressive conditional heteroscedasticity (GARCH) models havelong been considered as one of the most successful families of approaches forvolatility modeling in financial return series. In this paper, we propose analternative approach based on methodologies widely used in the field ofstatistical machine learning. Specifically, we propose a novel nonparametricBayesian mixture of Gaussian process regression models, each component of whichmodels the noise variance process that contaminates the observed data as aseparate latent Gaussian process driven by the observed data. This way, weessentially obtain a mixture Gaussian process conditional heteroscedasticity(MGPCH) model for volatility modeling in financial return series. We impose anonparametric prior with power-law nature over the distribution of the modelmixture components, namely the Pitman-Yor process prior, to allow for bettercapturing modeled data distributions with heavy tails and skewness. Finally, weprovide a copula- based approach for obtaining a predictive posterior for thecovariances over the asset returns modeled by means of a postulated MGPCHmodel. We evaluate the efficacy of our approach in a number of benchmarkscenarios, and compare its performance to state-of-the-art methodologies.
arxiv-2400-280 | Five Modulus Method For Image Compression | http://arxiv.org/abs/1211.4591 | author:Firas A. Jassim, Hind E. Qassim category:cs.CV cs.MM published:2012-11-19 summary:Data is compressed by reducing its redundancy, but this also makes the dataless reliable, more prone to errors. In this paper a novel approach of imagecompression based on a new method that has been created for image compressionwhich is called Five Modulus Method (FMM). The new method consists ofconverting each pixel value in an 8-by-8 block into a multiple of 5 for each ofthe R, G and B arrays. After that, the new values could be divided by 5 to getnew values which are 6-bit length for each pixel and it is less in storagespace than the original value which is 8-bits. Also, a new protocol forcompression of the new values as a stream of bits has been presented that givesthe opportunity to store and transfer the new compressed image easily.
arxiv-2400-281 | Applying Dynamic Model for Multiple Manoeuvring Target Tracking Using Particle Filtering | http://arxiv.org/abs/1211.4524 | author:Mohammad Javad Parseh, Saeid Pashazadeh category:cs.CV cs.AI published:2012-11-19 summary:In this paper, we applied a dynamic model for manoeuvring targets in SIRparticle filter algorithm for improving tracking accuracy of multiplemanoeuvring targets. In our proposed approach, a color distribution model isused to detect changes of target's model . Our proposed approach controlsdeformation of target's model. If deformation of target's model is larger thana predetermined threshold, then the model will be updated. Global NearestNeighbor (GNN) algorithm is used as data association algorithm. We named ourproposed method as Deformation Detection Particle Filter (DDPF) . DDPF approachis compared with basic SIR-PF algorithm on real airshow videos. Comparisonsresults show that, the basic SIR-PF algorithm is not able to track themanoeuvring targets when the rotation or scaling is occurred in target' smodel. However, DDPF approach updates target's model when the rotation orscaling is occurred. Thus, the proposed approach is able to track themanoeuvring targets more efficiently and accurately.
arxiv-2400-282 | An Effective Fingerprint Classification and Search Method | http://arxiv.org/abs/1211.4503 | author:Monowar H. Bhuyan, D. K. Bhattacharyya category:cs.CV cs.CR 68U35 I.5.3 published:2012-11-19 summary:This paper presents an effective fingerprint classification method designedbased on a hierarchical agglomerative clustering technique. The performance ofthe technique was evaluated in terms of several real-life datasets and asignificant improvement in reducing the misclassification error has beennoticed. This paper also presents a query based faster fingerprint searchmethod over the clustered fingerprint databases. The retrieval accuracy of thesearch method has been found effective in light of several real-life databases.
arxiv-2400-283 | Rate-Distortion Analysis of Multiview Coding in a DIBR Framework | http://arxiv.org/abs/1211.4499 | author:Boshra Rajaei, Thomas Maugey, Hamid-Reza Pourreza, Pascal Frossard category:cs.CV published:2012-11-19 summary:Depth image based rendering techniques for multiview applications have beenrecently introduced for efficient view generation at arbitrary camerapositions. Encoding rate control has thus to consider both texture and depthdata. Due to different structures of depth and texture images and theirdifferent roles on the rendered views, distributing the available bit budgetbetween them however requires a careful analysis. Information loss due totexture coding affects the value of pixels in synthesized views while errors indepth information lead to shift in objects or unexpected patterns at theirboundaries. In this paper, we address the problem of efficient bit allocationbetween textures and depth data of multiview video sequences. We adopt arate-distortion framework based on a simplified model of depth and textureimages. Our model preserves the main features of depth and texture images.Unlike most recent solutions, our method permits to avoid rendering at encodingtime for distortion estimation so that the encoding complexity is notaugmented. In addition to this, our model is independent of the underlyinginpainting method that is used at decoder. Experiments confirm our theoreticalresults and the efficiency of our rate allocation strategy.
arxiv-2400-284 | A Rule-Based Approach For Aligning Japanese-Spanish Sentences From A Comparable Corpora | http://arxiv.org/abs/1211.4488 | author:Jessica C. Ramírez, Yuji Matsumoto category:cs.CL cs.AI published:2012-11-19 summary:The performance of a Statistical Machine Translation System (SMT) system isproportionally directed to the quality and length of the parallel corpus ituses. However for some pair of languages there is a considerable lack of them.The long term goal is to construct a Japanese-Spanish parallel corpus to beused for SMT, whereas, there are a lack of useful Japanese-Spanish parallelCorpus. To address this problem, In this study we proposed a method forextracting Japanese-Spanish Parallel Sentences from Wikipedia using POS taggingand Rule-Based approach. The main focus of this approach is the syntacticfeatures of both languages. Human evaluation was performed over a sample andshows promising results, in comparison with the baseline.
arxiv-2400-285 | Artificial Neural Network Based Optical Character Recognition | http://arxiv.org/abs/1211.4385 | author:Vivek Shrivastava, Navdeep Sharma category:cs.CV cs.NE published:2012-11-19 summary:Optical Character Recognition deals in recognition and classification ofcharacters from an image. For the recognition to be accurate, certaintopological and geometrical properties are calculated, based on which acharacter is classified and recognized. Also, the Human psychology perceivescharacters by its overall shape and features such as strokes, curves,protrusions, enclosures etc. These properties, also called Features areextracted from the image by means of spatial pixel-based calculation. Acollection of such features, called Vectors, help in defining a characteruniquely, by means of an Artificial Neural Network that uses these FeatureVectors.
arxiv-2400-286 | A Sensing Policy Based on Confidence Bounds and a Restless Multi-Armed Bandit Model | http://arxiv.org/abs/1211.4384 | author:Jan Oksanen, Visa Koivunen, H. Vincent Poor category:cs.IT cs.LG math.IT published:2012-11-19 summary:A sensing policy for the restless multi-armed bandit problem with stationarybut unknown reward distributions is proposed. The work is presented in thecontext of cognitive radios in which the bandit problem arises when decidingwhich parts of the spectrum to sense and exploit. It is shown that the proposedpolicy attains asymptotically logarithmic weak regret rate when the rewards arebounded independent and identically distributed or finite state Markovian.Simulation results verifying uniformly logarithmic weak regret are alsopresented. The proposed policy is a centrally coordinated index policy, inwhich the index of a frequency band is comprised of a sample mean term and aconfidence term. The sample mean term promotes spectrum exploitation whereasthe confidence term encourages exploration. The confidence term is designedsuch that the time interval between consecutive sensing instances of anysuboptimal band grows exponentially. This exponential growth between suboptimalsensing time instances leads to logarithmically growing weak regret. Simulationresults demonstrate that the proposed policy performs better than other similarmethods in the literature.
arxiv-2400-287 | Bayesian nonparametric models for ranked data | http://arxiv.org/abs/1211.4321 | author:Francois Caron, Yee Whye Teh category:stat.ML cs.LG stat.ME published:2012-11-19 summary:We develop a Bayesian nonparametric extension of the popular Plackett-Lucechoice model that can handle an infinite number of choice items. Our frameworkis based on the theory of random atomic measures, with the prior specified by agamma process. We derive a posterior characterization and a simple andeffective Gibbs sampler for posterior simulation. We develop a time-varyingextension of our model, and apply it to the New York Times lists of weeklybestselling books.
arxiv-2400-288 | Efficient Superimposition Recovering Algorithm | http://arxiv.org/abs/1211.4307 | author:Han Li, Kun Gai, Pinghua Gong, Changshui Zhang category:cs.CV published:2012-11-19 summary:In this article, we address the issue of recovering latent transparent layersfrom superimposition images. Here, we assume we have the estimatedtransformations and extracted gradients of latent layers. To rapidly recoverhigh-quality image layers, we propose an Efficient Superimposition RecoveringAlgorithm (ESRA) by extending the framework of accelerated gradient method. Inaddition, a key building block (in each iteration) in our proposed method isthe proximal operator calculating. Here we propose to employ a dual approachand present our Parallel Algorithm with Constrained Total Variation (PACTV)method. Our recovering method not only reconstructs high-quality layers withoutcolor-bias problem, but also theoretically guarantees good convergenceperformance.
arxiv-2400-289 | Non-Local Patch Regression: Robust Image Denoising in Patch Space | http://arxiv.org/abs/1211.4264 | author:Kunal N. Chaudhury, Amit Singer category:cs.CV published:2012-11-18 summary:It was recently demonstrated in [Chaudhury et al.,Non-Local EuclideanMedians,2012] that the denoising performance of Non-Local Means (NLM) can beimproved at large noise levels by replacing the mean by the robust Euclideanmedian. Numerical experiments on synthetic and natural images showed that thelatter consistently performed better than NLM beyond a certain noise level, andsignificantly so for images with sharp edges. The Euclidean mean and median canbe put into a common regression (on the patch space) framework, in which thel_2 norm of the residuals is considered in the former, while the l_1 norm isconsidered in the latter. The natural question then is what happens if weconsider l_p (0<p<1) regression? We investigate this possibility in this paper.
arxiv-2400-290 | What Regularized Auto-Encoders Learn from the Data Generating Distribution | http://arxiv.org/abs/1211.4246 | author:Guillaume Alain, Yoshua Bengio category:cs.LG stat.ML published:2012-11-18 summary:What do auto-encoders learn about the underlying data generatingdistribution? Recent work suggests that some auto-encoder variants do a goodjob of capturing the local manifold structure of data. This paper clarifiessome of these previous observations by showing that minimizing a particularform of regularized reconstruction error yields a reconstruction function thatlocally characterizes the shape of the data generating density. We show thatthe auto-encoder captures the score (derivative of the log-density with respectto the input). It contradicts previous interpretations of reconstruction erroras an energy function. Unlike previous results, the theorems provided here arecompletely generic and do not depend on the parametrization of theauto-encoder: they show what the auto-encoder would tend to if given enoughcapacity and examples. These results are for a contractive training criterionwe show to be similar to the denoising auto-encoder training criterion withsmall corruption noise, but with contraction applied on the wholereconstruction function rather than just encoder. Similarly to score matching,one can consider the proposed training criterion as a convenient alternative tomaximum likelihood because it does not involve a partition function. Finally,we show how an approximate Metropolis-Hastings MCMC can be setup to recoversamples from the estimated distribution, and this is confirmed in samplingexperiments.
arxiv-2400-291 | Efficiently Learning from Revealed Preference | http://arxiv.org/abs/1211.4150 | author:Morteza Zadimoghaddam, Aaron Roth category:cs.GT cs.DS cs.LG published:2012-11-17 summary:In this paper, we consider the revealed preferences problem from a learningperspective. Every day, a price vector and a budget is drawn from an unknowndistribution, and a rational agent buys his most preferred bundle according tosome unknown utility function, subject to the given prices and budgetconstraint. We wish not only to find a utility function which rationalizes afinite set of observations, but to produce a hypothesis valuation functionwhich accurately predicts the behavior of the agent in the future. We giveefficient algorithms with polynomial sample-complexity for agents with linearvaluation functions, as well as for agents with linearly separable, concavevaluation functions with bounded second derivative.
arxiv-2400-292 | Data Clustering via Principal Direction Gap Partitioning | http://arxiv.org/abs/1211.4142 | author:Ralph Abbey, Jeremy Diepenbrock, Amy Langville, Carl Meyer, Shaina Race, Dexin Zhou category:stat.ML cs.LG published:2012-11-17 summary:We explore the geometrical interpretation of the PCA based clusteringalgorithm Principal Direction Divisive Partitioning (PDDP). We give severalexamples where this algorithm breaks down, and suggest a new method, gappartitioning, which takes into account natural gaps in the data betweenclusters. Geometric features of the PCA space are derived and illustrated andexperimental results are given which show our method is comparable on thedatasets used in the original paper on PDDP.
arxiv-2400-293 | Semantic Polarity of Adjectival Predicates in Online Reviews | http://arxiv.org/abs/1211.4161 | author:Ae-Lim Ahn, Éric Laporte, Jee-Sun Nam category:cs.CL published:2012-11-17 summary:Web users produce more and more documents expressing opinions. Because thesehave become important resources for customers and manufacturers, many havefocused on them. Opinions are often expressed through adjectives with positiveor negative semantic values. In extracting information from users' opinion inonline reviews, exact recognition of the semantic polarity of adjectives is oneof the most important requirements. Since adjectives have different semanticorientations according to contexts, it is not satisfying to extract opinioninformation without considering the semantic and lexical relations between theadjectives and the feature nouns appropriate to a given domain. In this paper,we present a classification of adjectives by polarity, and we analyzeadjectives that are undetermined in the absence of contexts. Our researchshould be useful for accurately predicting semantic orientations of opinionsentences, and should be taken into account before relying on an automaticmethods.
arxiv-2400-294 | The Algebraic Combinatorial Approach for Low-Rank Matrix Completion | http://arxiv.org/abs/1211.4116 | author:Franz J. Király, Louis Theran, Ryota Tomioka category:cs.LG cs.NA math.AG math.CO stat.ML published:2012-11-17 summary:We present a novel algebraic combinatorial view on low-rank matrix completionbased on studying relations between a few entries with tools from algebraicgeometry and matroid theory. The intrinsic locality of the approach allows forthe treatment of single entries in a closed theoretical and practicalframework. More specifically, apart from introducing an algebraic combinatorialtheory of low-rank matrix completion, we present probability-one algorithms todecide whether a particular entry of the matrix can be completed. We alsodescribe methods to complete that entry from a few others, and to estimate theerror which is incurred by any method completing that entry. Furthermore, weshow how known results on matrix completion and their sampling assumptions canbe related to our new perspective and interpreted in terms of a completabilityphase transition.
arxiv-2400-295 | A Bayesian Interpretation of the Particle Swarm Optimization and Its Kernel Extension | http://arxiv.org/abs/1211.3845 | author:Peter Andras category:cs.NE published:2012-11-16 summary:Particle swarm optimization is a popular method for solving difficultoptimization problems. There have been attempts to formulate the method informal probabilistic or stochastic terms (e.g. bare bones particle swarm) withthe aim to achieve more generality and explain the practical behavior of themethod. Here we present a Bayesian interpretation of the particle swarmoptimization. This interpretation provides a formal framework for incorporationof prior knowledge about the problem that is being solved. Furthermore, it alsoallows to extend the particle optimization method through the use of kernelfunctions that represent the intermediary transformation of the data into adifferent space where the optimization problem is expected to be easier to beresolved, such transformation can be seen as a form of prior knowledge aboutthe nature of the optimization problem. We derive from the general Bayesianformulation the commonly used particle swarm methods as particular cases.
arxiv-2400-296 | On Calibrated Predictions for Auction Selection Mechanisms | http://arxiv.org/abs/1211.3955 | author:H. Brendan McMahan, Omkar Muralidharan category:cs.GT cs.LG published:2012-11-16 summary:Calibration is a basic property for prediction systems, and algorithms forachieving it are well-studied in both statistics and machine learning. In manyapplications, however, the predictions are used to make decisions that selectwhich observations are made. This makes calibration difficult, as adjustingpredictions to achieve calibration changes future data. We focus onclick-through-rate (CTR) prediction for search ad auctions. Here, CTRpredictions are used by an auction that determines which ads are shown, and wewant to maximize the value generated by the auction. We show that certain natural notions of calibration can be impossible toachieve, depending on the details of the auction. We also show that it can beimpossible to maximize auction efficiency while using calibrated predictions.Finally, we give conditions under which calibration is achievable andsimultaneously maximizes auction efficiency: roughly speaking, bids and queriesmust not contain information about CTRs that is not already captured by thepredictions.
arxiv-2400-297 | Lasso Screening Rules via Dual Polytope Projection | http://arxiv.org/abs/1211.3966 | author:Jie Wang, Peter Wonka, Jieping Ye category:cs.LG stat.ML published:2012-11-16 summary:Lasso is a widely used regression technique to find sparse representations.When the dimension of the feature space and the number of samples are extremelylarge, solving the Lasso problem remains challenging. To improve the efficiencyof solving large-scale Lasso problems, El Ghaoui and his colleagues haveproposed the SAFE rules which are able to quickly identify the inactivepredictors, i.e., predictors that have $0$ components in the solution vector.Then, the inactive predictors or features can be removed from the optimizationproblem to reduce its scale. By transforming the standard Lasso to its dualform, it can be shown that the inactive predictors include the set of inactiveconstraints on the optimal dual solution. In this paper, we propose anefficient and effective screening rule via Dual Polytope Projections (DPP),which is mainly based on the uniqueness and nonexpansiveness of the optimaldual solution due to the fact that the feasible set in the dual space is aconvex and closed polytope. Moreover, we show that our screening rule can beextended to identify inactive groups in group Lasso. To the best of ourknowledge, there is currently no "exact" screening rule for group Lasso. Wehave evaluated our screening rule using synthetic and real data sets. Resultsshow that our rule is more effective in identifying inactive predictors thanexisting state-of-the-art screening rules for Lasso.
arxiv-2400-298 | Visual Recognition of Isolated Swedish Sign Language Signs | http://arxiv.org/abs/1211.3901 | author:Saad Akram, Jonas Beskow, Hedvig Kjellstrom category:cs.CV published:2012-11-16 summary:We present a method for recognition of isolated Swedish Sign Language signs.The method will be used in a game intended to help children training signing athome, as a complement to training with a teacher. The target group is notprimarily deaf children, but children with language disorders. Using signlanguage as a support in conversation has been shown to greatly stimulate thespeech development of such children. The signer is captured with an RGB-D(Kinect) sensor, which has three advantages over a regular RGB camera. Firstly,it allows complex backgrounds to be removed easily. We segment the hands andface based on skin color and depth information. Secondly, it helps with theresolution of hand over face occlusion. Thirdly, signs take place in 3D; someaspects of the signs are defined by hand motion vertically to the image plane.This motion can be estimated if the depth is observable. The 3D motion of thehands relative to the torso are used as a cue together with the hand shape, andHMMs trained with this input are used for classification. To obtain higherrobustness towards differences across signers, Fisher Linear DiscriminantAnalysis is used to find the combinations of features that are most descriptivefor each sign, regardless of signer. Experiments show that the system candistinguish signs from a challenging 94 word vocabulary with a precision of upto 94% in the signer dependent case and up to 47% in the signer independentcase.
arxiv-2400-299 | Distance Majorization and Its Applications | http://arxiv.org/abs/1211.3907 | author:Eric C. Chi, Hua Zhou, Kenneth Lange category:math.OC stat.CO stat.ML published:2012-11-16 summary:The problem of minimizing a continuously differentiable convex function overan intersection of closed convex sets is ubiquitous in applied mathematics. Itis particularly interesting when it is easy to project onto each separate set,but nontrivial to project onto their intersection. Algorithms based on Newton'smethod such as the interior point method are viable for small to medium-scaleproblems. However, modern applications in statistics, engineering, and machinelearning are posing problems with potentially tens of thousands of parametersor more. We revisit this convex programming problem and propose an algorithmthat scales well with dimensionality. Our proposal is an instance of asequential unconstrained minimization technique and revolves around threeideas: the majorization-minimization (MM) principle, the classical penaltymethod for constrained optimization, and quasi-Newton acceleration offixed-point algorithms. The performance of our distance majorization algorithmsis illustrated in several applications.
arxiv-2400-300 | Objective Improvement in Information-Geometric Optimization | http://arxiv.org/abs/1211.3831 | author:Youhei Akimoto, Yann Ollivier category:cs.LG cs.AI math.OC stat.ML published:2012-11-16 summary:Information-Geometric Optimization (IGO) is a unified framework of stochasticalgorithms for optimization problems. Given a family of probabilitydistributions, IGO turns the original optimization problem into a newmaximization problem on the parameter space of the probability distributions.IGO updates the parameter of the probability distribution along the naturalgradient, taken with respect to the Fisher metric on the parameter manifold,aiming at maximizing an adaptive transform of the objective function. IGOrecovers several known algorithms as particular instances: for the family ofBernoulli distributions IGO recovers PBIL, for the family of Gaussiandistributions the pure rank-mu CMA-ES update is recovered, and for exponentialfamilies in expectation parametrization the cross-entropy/ML method isrecovered. This article provides a theoretical justification for the IGOframework, by proving that any step size not greater than 1 guarantees monotoneimprovement over the course of optimization, in terms of q-quantile values ofthe objective function f. The range of admissible step sizes is independent off and its domain. We extend the result to cover the case of different stepsizes for blocks of the parameters in the IGO algorithm. Moreover, we provethat expected fitness improves over time when fitness-proportional selection isapplied, in which case the RPP algorithm is recovered.
