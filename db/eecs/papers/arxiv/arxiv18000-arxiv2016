arxiv-1605-02540 | Exact ICL maximization in a non-stationary temporal extension of the stochastic block model for dynamic networks |  http://arxiv.org/abs/1605.02540  | author:Marco Corneli, Pierre Latouche, Fabrice Rossi category:stat.ML stat.AP published:2016-05-09 summary:The stochastic block model (SBM) is a flexible probabilistic tool that can beused to model interactions between clusters of nodes in a network. However, itdoes not account for interactions of time varying intensity between clusters.The extension of the SBM developed in this paper addresses this shortcomingthrough a temporal partition: assuming interactions between nodes are recordedon fixed-length time intervals, the inference procedure associated with themodel we propose allows to cluster simultaneously the nodes of the network andthe time intervals. The number of clusters of nodes and of time intervals, aswell as the memberships to clusters, are obtained by maximizing an exactintegrated complete-data likelihood, relying on a greedy search approach.Experiments on simulated and real data are carried out in order to assess theproposed methodology.
arxiv-1605-02536 | Random Fourier Features for Operator-Valued Kernels |  http://arxiv.org/abs/1605.02536  | author:Romain Brault, Florence d'Alché-Buc, Markus Heinonen category:cs.LG published:2016-05-09 summary:Devoted to multi-task learning and structured output learning,operator-valued kernels provide a flexible tool to build vector-valuedfunctions in the context of Reproducing Kernel Hilbert Spaces. To scale upthese methods, we extend the celebrated Random Fourier Feature methodology toget an approximation of operator-valued kernels. We propose a general principlefor Operator-valued Random Fourier Feature construction relying on ageneralization of Bochner's theorem for translation-invariant operator-valuedMercer kernels. We prove the uniform convergence of the kernel approximationfor bounded and unbounded operator random Fourier features using appropriateBernstein matrix concentration inequality. An experimental proof-of-conceptshows the quality of the approximation and the efficiency of the correspondinglinear models on example datasets.
arxiv-1605-02531 | Clustering Time Series and the Surprising Robustness of HMMs |  http://arxiv.org/abs/1605.02531  | author:Mark Kozdoba, Shie Mannor category:cs.IT cs.LG math.IT stat.ML published:2016-05-09 summary:Suppose that you are given a time series where consecutive samples arebelieved to come from a probabilistic source, and that the source changes fromtime to time. Your objective is to learn the distribution of each source and tocluster the samples according to the source that generated them. A standardapproach to this problem is to model the data as a hidden Markov model (HMM).However, due to the Markov property and stationarity of HMMs, simple examplescan be given where this approach yields poor results for the clustering. Wepropose a more general, non-stationary model of the data, where the onlyrestriction is that the sources can not change too often. Even though the modelgoverning the sources may not be Markovian, we show that that a maximumlikelihood HMM estimator can still be used. Specifically, we show that amaximum-likelihood HMM estimator produces the correct second moment of thedata, and the results can be extended to higher moments. In contrast to theexisting consistency and misspecification results involving maximum likelihoodfor HMMs, our approach yields bounds for finite sample sizes.
arxiv-1605-02674 | Why (and How) Avoid Orthogonal Procrustes in Regularized Multivariate Analysis |  http://arxiv.org/abs/1605.02674  | author:Sergio Muñoz-Romero, Vanessa Gómez-Verdejo, Jerónimo Arenas-García category:stat.ML published:2016-05-09 summary:Multivariate Analysis (MVA) comprises a family of well-known methods forfeature extraction that exploit correlations among input variables of the datarepresentation. One important property that is enjoyed by most such methods isuncorrelation among the extracted features. Recently, regularized versions ofMVA methods have appeared in the literature, mainly with the goal to gaininterpretability of the solution. In these cases, the solutions can no longerbe obtained in a closed manner, and it is frequent to recur to the iteration oftwo steps, one of them being an orthogonal Procrustes problem. This lettershows that the Procrustes solution is not optimal from the perspective of theoverall MVA method, and proposes an alternative approach based on the solutionof an eigenvalue problem. Our method ensures the preservation of severalproperties of the original methods, most notably the uncorrelation of theextracted features, as demonstrated theoretically and through a collection ofselected experiments.
arxiv-1605-02633 | Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering |  http://arxiv.org/abs/1605.02633  | author:Chong You, Chun-Guang Li, Daniel P. Robinson, Rene Vidal category:cs.LG cs.CV stat.ML published:2016-05-09 summary:State-of-the-art subspace clustering methods are based on expressing eachdata point as a linear combination of other data points while regularizing thematrix of coefficients with $\ell_1$, $\ell_2$ or nuclear norms. $\ell_1$regularization is guaranteed to give a subspace-preserving affinity (i.e.,there are no connections between points from different subspaces) under broadtheoretical conditions, but the clusters may not be connected. $\ell_2$ andnuclear norm regularization often improve connectivity, but give asubspace-preserving affinity only for independent subspaces. Mixed $\ell_1$,$\ell_2$ and nuclear norm regularizations offer a balance between thesubspace-preserving and connectedness properties, but this comes at the cost ofincreased computational complexity. This paper studies the geometry of theelastic net regularizer (a mixture of the $\ell_1$ and $\ell_2$ norms) and usesit to derive a provably correct and scalable active set method for finding theoptimal coefficients. Our geometric analysis also provides a theoreticaljustification and a geometric interpretation for the balance between theconnectedness (due to $\ell_2$ regularization) and subspace-preserving (due to$\ell_1$ regularization) properties for elastic net subspace clustering. Ourexperiments show that the proposed active set method not only achievesstate-of-the-art clustering performance, but also efficiently handleslarge-scale datasets.
arxiv-1605-02486 | Efficiency Evaluation of Character-level RNN Training Schedules |  http://arxiv.org/abs/1605.02486  | author:Cedric De Boom, Sam Leroux, Steven Bohez, Pieter Simoens, Thomas Demeester, Bart Dhoedt category:cs.NE published:2016-05-09 summary:We present four training and prediction schedules from the samecharacter-level recurrent neural network. The efficiency of these schedules istested in terms of model effectiveness as a function of training time andamount of training data seen. We show that the choice of training andprediction schedule potentially has a considerable impact on the predictioneffectiveness for a given training budget.
arxiv-1605-02470 | Randomized Kaczmarz for Rank Aggregation from Pairwise Comparisons |  http://arxiv.org/abs/1605.02470  | author:Vivek S. Borkar, Nikhil Karamchandani, Sharad Mirani category:cs.LG stat.ML published:2016-05-09 summary:We revisit the problem of inferring the overall ranking among entities in theframework of Bradley-Terry-Luce (BTL) model, based on available empirical dataon pairwise preferences. By a simple transformation, we can cast the problem asthat of solving a noisy linear system, for which a ready algorithm is availablein the form of the randomized Kaczmarz method. This scheme is provablyconvergent, has excellent empirical performance, and is amenable to on-line,distributed and asynchronous variants. Convergence, convergence rate, and erroranalysis of the proposed algorithm are presented and several numericalexperiments are conducted whose results validate our theoretical findings.
arxiv-1605-02464 | Orientation Driven Bag of Appearances for Person Re-identification |  http://arxiv.org/abs/1605.02464  | author:Liqian Ma, Hong Liu, Liang Hu, Can Wang, Qianru Sun category:cs.CV published:2016-05-09 summary:Person re-identification (re-id) consists of associating individual acrosscamera network, which is valuable for intelligent video surveillance and hasdrawn wide attention. Although person re-identification research is makingprogress, it still faces some challenges such as varying poses, illuminationand viewpoints. For feature representation in re-identification, existing worksusually use low-level descriptors which do not take full advantage of bodystructure information, resulting in low representation ability.%discrimination. To solve this problem, this paper proposes the mid-levelbody-structure based feature representation (BSFR) which introduces bodystructure pyramid for codebook learning and feature pooling in the verticaldirection of human body. Besides, varying viewpoints in the horizontaldirection of human body usually causes the data missing problem, $i.e.$, theappearances obtained in different orientations of the identical person couldvary significantly. To address this problem, the orientation driven bag ofappearances (ODBoA) is proposed to utilize person orientation informationextracted by orientation estimation technic. To properly evaluate the proposedapproach, we introduce a new re-identification dataset (Market-1203) based onthe Market-1501 dataset and propose a new re-identification dataset (PKU-Reid).Both datasets contain multiple images captured in different body orientationsfor each person. Experimental results on three public datasets and two proposeddatasets demonstrate the superiority of the proposed approach, indicating theeffectiveness of body structure and orientation information for improvingre-identification performance.
arxiv-1605-02408 | Structured Nonconvex and Nonsmooth Optimization: Algorithms and Iteration Complexity Analysis |  http://arxiv.org/abs/1605.02408  | author:Bo Jiang, Tianyi Lin, Shiqian Ma, Shuzhong Zhang category:math.OC cs.LG stat.ML published:2016-05-09 summary:Nonconvex optimization problems are frequently encountered in much ofstatistics, business, science and engineering, but they are not yet widelyrecognized as a technology. A reason for this relatively low degree ofpopularity is the lack of a well developed system of theory and algorithms tosupport the applications, as is the case for its convex counterpart. This paperaims to take one step in the direction of disciplined nonconvex optimization.In particular, we consider in this paper some constrained nonconvexoptimization models in block decision variables, with or without coupled affineconstraints. In the case of no coupled constraints, we show a sublinear rate ofconvergence to an $\epsilon$-stationary solution in the form of variationalinequality for a generalized conditional gradient method, where the convergencerate is shown to be dependent on the H\"olderian continuity of the gradient ofthe smooth part of the objective. For the model with coupled affineconstraints, we introduce corresponding $\epsilon$-stationarity conditions, andpropose two proximal-type variants of the ADMM to solve such a model, assumingthe proximal ADMM updates can be implemented for all the block variables exceptfor the last block, for which either a gradient step or amajorization-minimization step is implemented. We show an iteration complexitybound of $O(1/\epsilon^2)$ to reach an $\epsilon$-stationary solution for bothalgorithms. Moreover, we show that the same iteration complexity of a proximalBCD method follows immediately. Numerical results are provided to illustratethe efficacy of the proposed algorithms for tensor robust PCA.
arxiv-1605-02424 | Learning Discriminative Features with Class Encoder |  http://arxiv.org/abs/1605.02424  | author:Hailin Shi, Xiangyu Zhu, Zhen Lei, Shengcai Liao, Stan Z. Li category:cs.CV published:2016-05-09 summary:Deep neural networks usually benefit from unsupervised pre-training, e.g.auto-encoders. However, the classifier further needs supervised fine-tuningmethods for good discrimination. Besides, due to the limits of full-connection,the application of auto-encoders is usually limited to small, well alignedimages. In this paper, we incorporate the supervised information to propose anovel formulation, namely class-encoder, whose training objective is toreconstruct a sample from another one of which the labels are identical.Class-encoder aims to minimize the intra-class variations in the feature space,and to learn a good discriminative manifolds on a class scale. We impose theclass-encoder as a constraint into the softmax for better supervised training,and extend the reconstruction on feature-level to tackle the parameter sizeissue and translation issue. The experiments show that the class-encoder helpsto improve the performance on benchmarks of classification and facerecognition. This could also be a promising direction for fast training of facerecognition models.
arxiv-1605-02442 | Machine Learning Techniques with Ontology for Subjective Answer Evaluation |  http://arxiv.org/abs/1605.02442  | author:M. Syamala Devi, Himani Mittal category:cs.AI cs.CL cs.IR I.2.7 published:2016-05-09 summary:Computerized Evaluation of English Essays is performed using Machine learningtechniques like Latent Semantic Analysis (LSA), Generalized LSA, BilingualEvaluation Understudy and Maximum Entropy. Ontology, a concept map of domainknowledge, can enhance the performance of these techniques. Use of Ontologymakes the evaluation process holistic as presence of keywords, synonyms, theright word combination and coverage of concepts can be checked. In this paper,the above mentioned techniques are implemented both with and without Ontologyand tested on common input data consisting of technical answers of ComputerScience. Domain Ontology of Computer Graphics is designed and developed. Thesoftware used for implementation includes Java Programming Language and toolssuch as MATLAB, Prot\'eg\'e, etc. Ten questions from Computer Graphics withsixty answers for each question are used for testing. The results are analyzedand it is concluded that the results are more accurate with use of Ontology.
arxiv-1605-02457 | The Controlled Natural Language of Randall Munroe's Thing Explainer |  http://arxiv.org/abs/1605.02457  | author:Tobias Kuhn category:cs.CL published:2016-05-09 summary:It is rare that texts or entire books written in a Controlled NaturalLanguage (CNL) become very popular, but exactly this has happened with a bookthat has been published last year. Randall Munroe's Thing Explainer uses onlythe 1'000 most often used words of the English language together with drawnpictures to explain complicated things such as nuclear reactors, jet engines,the solar system, and dishwashers. This restricted language is a veryinteresting new case for the CNL community. I describe here its place in thecontext of existing approaches on Controlled Natural Languages, and I provide afirst analysis from a scientific perspective, covering the word productionrules and word distributions.
arxiv-1605-02460 | Fuzzy Clustering Based Segmentation Of Vertebrae in T1-Weighted Spinal MR Images |  http://arxiv.org/abs/1605.02460  | author:Jiyo. S. Athertya, G. Saravana Kumar category:cs.CV published:2016-05-09 summary:Image segmentation in the medical domain is a challenging field owing to poorresolution and limited contrast. The predominantly used conventionalsegmentation techniques and the thresholding methods suffer from limitationsbecause of heavy dependence on user interactions. Uncertainties prevalent in animage cannot be captured by these techniques. The performance furtherdeteriorates when the images are corrupted by noise, outliers and otherartifacts. The objective of this paper is to develop an effective robust fuzzyC- means clustering for segmenting vertebral body from magnetic resonance imageowing to its unsupervised form of learning. The motivation for this work isdetection of spine geometry and proper localisation and labelling will enhancethe diagnostic output of a physician. The method is compared with Otsuthresholding and K-means clustering to illustrate the robustness.The referencestandard for validation was the annotated images from the radiologist, and theDice coefficient and Hausdorff distance measures were used to evaluate thesegmentation.
arxiv-1605-02697 | Ask Your Neurons: A Deep Learning Approach to Visual Question Answering |  http://arxiv.org/abs/1605.02697  | author:Mateusz Malinowski, Marcus Rohrbach, Mario Fritz category:cs.CV cs.AI cs.CL published:2016-05-09 summary:We address a question answering task on real-world images that is set up as aVisual Turing Test. By combining latest advances in image representation andnatural language processing, we propose Ask Your Neurons, a scalable, jointlytrained, end-to-end formulation to this problem. In contrast to previous efforts, we are facing a multi-modal problem wherethe language output (answer) is conditioned on visual and natural languageinputs (image and question). We provide additional insights into the problem byanalyzing how much information is contained only in the language part for whichwe provide a new human baseline. To study human consensus, which is related tothe ambiguities inherent in this challenging task, we propose two novel metricsand collect additional answers which extend the original DAQUAR dataset toDAQUAR-Consensus. Moreover, we also extend our analysis to VQA, a large-scale questionanswering about images dataset, where we investigate some particular designchoices and show the importance of stronger visual models. At the same time, weachieve strong performance of our model that still uses a global imagerepresentation. Finally, based on such analysis, we refine our Ask Your Neuronson DAQUAR, which also leads to a better performance on this challenging task.
arxiv-1605-02677 | Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark |  http://arxiv.org/abs/1605.02677  | author:Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang category:cs.AI cs.CV published:2016-05-09 summary:Psychological research results have confirmed that people can have differentemotional reactions to different visual stimuli. Several papers have beenpublished on the problem of visual emotion analysis. In particular, attemptshave been made to analyze and predict people's emotional reaction towardsimages. To this end, different kinds of hand-tuned features are proposed. Theresults reported on several carefully selected and labeled small image datasets have confirmed the promise of such features. While the recent successes ofmany computer vision related tasks are due to the adoption of ConvolutionalNeural Networks (CNNs), visual emotion analysis has not achieved the same levelof success. This may be primarily due to the unavailability of confidentlylabeled and relatively large image data sets for visual emotion analysis. Inthis work, we introduce a new data set, which started from 3+ million weaklylabeled images of different emotions and ended up 30 times as large as thecurrent largest publicly available visual emotion data set. We hope that thisdata set encourages further research on visual emotion analysis. We alsoperform extensive benchmarking analyses on this large data set using the stateof the art methods including CNNs.
arxiv-1605-02559 | Robust imaging of hippocampal inner structure at 7T: in vivo acquisition protocol and methodological choices |  http://arxiv.org/abs/1605.02559  | author:Linda Marrakchi-Kacem, Alexandre Vignaud, Julien Sein, Johanne Germain, Thomas R Henry, Cyril Poupon, Lucie Hertz-Pannier, Stéphane Lehéricy, Olivier Colliot, Pierre-François Van de Moortele, Marie Chupin category:cs.CV published:2016-05-09 summary:OBJECTIVE:Motion-robust multi-slab imaging of hippocampal inner structure invivo at 7T.MATERIALS AND METHODS:Motion is a crucial issue for ultra-highresolution imaging, such as can be achieved with 7T MRI. An acquisitionprotocol was designed for imaging hippocampal inner structure at 7T. It relieson a compromise between anatomical details visibility and robustness to motion.In order to reduce acquisition time and motion artifacts, the full slabcovering the hippocampus was split into separate slabs with lower acquisitiontime. A robust registration approach was implemented to combine the acquiredslabs within a final 3D-consistent high-resolution slab covering the wholehippocampus. Evaluation was performed on 50 subjects overall, made of threegroups of subjects acquired using three acquisition settings; it focused onthree issues: visibility of hippocampal inner structure, robustness to motionartifacts and registration procedure performance.RESULTS:Overall, T2-weightedacquisitions with interleaved slabs proved robust. Multi-slab registrationyielded high quality datasets in 96 % of the subjects, thus compatible withfurther analyses of hippocampal inner structure.CONCLUSION:Multi-slabacquisition and registration setting is efficient for reducing acquisition timeand consequently motion artifacts for ultra-high resolution imaging of theinner structure of the hippocampus.
arxiv-1605-02560 | Studying the brain from adolescence to adulthood through sparse multi-view matrix factorisations |  http://arxiv.org/abs/1605.02560  | author:Zi Wang, Vyacheslav Karolis, Chiara Nosarti, Giovanni Montana category:stat.AP cs.CV q-bio.NC published:2016-05-09 summary:Men and women differ in specific cognitive abilities and in the expression ofseveral neuropsychiatric conditions. Such findings could be attributed to sexhormones, brain differences, as well as a number of environmental variables.Existing research on identifying sex-related differences in brain structurehave predominantly used cross-sectional studies to investigate, for instance,differences in average gray matter volumes (GMVs). In this article we explorethe potential of a recently proposed multi-view matrix factorisation (MVMF)methodology to study structural brain changes in men and women that occur fromadolescence to adulthood. MVMF is a multivariate variance decompositiontechnique that extends principal component analysis to "multi-view" datasets,i.e. where multiple and related groups of observations are available. In thisapplication, each view represents a different age group. MVMF identifies latentfactors explaining shared and age-specific contributions to the observedoverall variability in GMVs over time. These latent factors can be used toproduce low-dimensional visualisations of the data that emphasise age-specificeffects once the shared effects have been accounted for. The analysis of twodatasets consisting of individuals born prematurely as well as healthy controlsprovides evidence to suggest that the separation between males and femalesbecomes increasingly larger as the brain transitions from adolescence toadulthood. We report on specific brain regions associated to these varianceeffects.
arxiv-1605-02592 | GLEU Without Tuning |  http://arxiv.org/abs/1605.02592  | author:Courtney Napoles, Keisuke Sakaguchi, Matt Post, Joel Tetreault category:cs.CL published:2016-05-09 summary:The GLEU metric was proposed for evaluating grammatical error correctionsusing n-gram overlap with a set of reference sentences, as opposed toprecision/recall of specific annotated errors (Napoles et al., 2015). Thispaper describes improvements made to the GLEU metric that address problems thatarise when using an increasing number of reference sets. Unlike the originallypresented metric, the modified metric does not require tuning. We recommendthat this version be used instead of the original version.
arxiv-1605-02609 | Dynamic Decomposition of Spatiotemporal Neural Signals |  http://arxiv.org/abs/1605.02609  | author:Luca Ambrogioni, Marcel A. J. van Gerven, Eric Maris category:q-bio.NC stat.ML published:2016-05-09 summary:Neural signals are characterized by rich temporal and spatiotemporal dynamicsthat reflect the organization of cortical networks. Theoretical research hasshown how neural networks can operate at different dynamic ranges thatcorrespond to specific types of information processing. Here we present a dataanalysis framework that uses a linearized model of these dynamic states inorder to decompose the measured neural signal into a series of components thatcapture both rhythmic and non-rhythmic neural activity. The method is based onstochastic differential equations and Gaussian process regression. Throughcomputer simulations and analysis of magnetoencephalographic data, wedemonstrate the efficacy of the method in identifying meaningful modulations ofoscillatory signals corrupted by structured temporal and spatiotemporal noise.These results suggest that the method is particularly suitable for the analysisand interpretation of complex temporal and spatiotemporal neural signals.
arxiv-1605-02619 | On the Emergence of Shortest Paths by Reinforced Random Walks |  http://arxiv.org/abs/1605.02619  | author:Daniel R. Figueiredo, Michele Garetto category:cs.NE physics.bio-ph published:2016-05-09 summary:The co-evolution between network structure and functional performance is afundamental and challenging problem whose complexity emerges from the intrinsicinterdependent nature of structure and function. Within this context, weinvestigate the interplay between the efficiency of network navigation (i.e.,path lengths) and network structure (i.e., edge weights). We propose a simpleand tractable model based on iterative biased random walks where edge weightsincrease over time as function of the traversed path length. Under mildassumptions, we prove that biased random walks will eventually only traverseshortest paths in their journey towards the destination. We furthercharacterize the transient regime proving that the probability to traversenon-shortest paths decays according to a power-law. We also highlight variousproperties in this dynamic, such as the trade-off between exploration andconvergence, and preservation of initial network plasticity. We believe theproposed model and results can be of interest to various domains where biasedrandom walks and decentralized navigation have been applied.
arxiv-1605-02688 | Theano: A Python framework for fast computation of mathematical expressions |  http://arxiv.org/abs/1605.02688  | category:cs.SC cs.LG cs.MS published:2016-05-09 summary:Theano is a Python library that allows to define, optimize, and evaluatemathematical expressions involving multi-dimensional arrays efficiently. Sinceits introduction, it has been one of the most used CPU and GPU mathematicalcompilers - especially in the machine learning community - and has shown steadyperformance improvements. Theano is being actively and continuously developedsince 2008, multiple frameworks have been built on top of it and it has beenused to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overviewof the Theano software and its community. Section II presents the principalfeatures of Theano and how to use them, and compares them with other similarprojects. Section III focuses on recently-introduced functionalities andimprovements. Section IV compares the performance of Theano against Torch7 andTensorFlow on several machine learning models. Section V discusses currentlimitations of Theano and potential ways of improving it.
arxiv-1605-02693 | Inference of High-dimensional Autoregressive Generalized Linear Models |  http://arxiv.org/abs/1605.02693  | author:Eric C. Hall, Garvesh Raskutti, Rebecca Willett category:stat.ML cs.IT math.IT math.ST stat.TH published:2016-05-09 summary:Vector autoregressive models characterize a variety of time series in whichlinear combinations of current and past observations can be used to accuratelypredict future observations. For instance, each element of an observationvector could correspond to a different node in a network, and the parameters ofan autoregressive model would correspond to the impact of the network structureon the time series evolution. Often these models are used successfully inpractice to learn the structure of social, epidemiological, financial, orbiological neural networks. However, little is known about statisticalguarantees of estimates of such models in non-Gaussian settings. This paperaddresses the inference of the autoregressive parameters and associated networkstructure within a generalized linear model framework that includes Poisson andBernoulli autoregressive processes. At the heart of this analysis is asparsity-regularized maximum likelihood estimator. Whilesparsity-regularization is well-studied in the statistics and machine learningcommunities, those analysis methods cannot be applied to autoregressivegeneralized linear models because of the correlations and potentialheteroscedasticity inherent in the observations. Sample complexity bounds arederived using a combination of martingale concentration inequalities andmodified covering techniques originally proposed for high-dimensional linearregression analysis. These bounds, which are supported by several simulationstudies, characterize the impact of various network parameters on estimatorperformance.
arxiv-1605-02266 | Robust and Low-Rank Representation for Fast Face Identification with Occlusions |  http://arxiv.org/abs/1605.02266  | author:Michael Iliadis, Haohong Wang, Rafael Molina, Aggelos K. Katsaggelos category:cs.CV published:2016-05-08 summary:In this paper we propose an iterative method to address the faceidentification problem with block occlusions. Our approach utilizes a robustrepresentation based on two characteristics in order to model contiguous errors(e.g., block occlusion) effectively. The first fits to the errors adistribution described by a tailored loss function. The second describes theerror image as having a specific structure (resulting in low-rank). We willshow that this joint characterization is effective for describing errors withspatial continuity. Our approach is computationally efficient due to theutilization of the Alternating Direction Method of Multipliers (ADMM). Aspecial case of our fast iterative algorithm leads to the robust representationmethod which is normally used to handle non-contiguous errors (e.g., pixelcorruption). Extensive results on representative face databases document theeffectiveness of our method over existing robust representation methods withrespect to both identification rates and computational time. Code is available at Github, where you can find implementations of theF-LR-IRNNLS and F-IRNNLS (fast version of the RRC) :\url{https://github.com/miliadis/FIRC}
arxiv-1605-02264 | Laplacian Reconstruction and Refinement for Semantic Segmentation |  http://arxiv.org/abs/1605.02264  | author:Golnaz Ghiasi, Charless Fowlkes category:cs.CV published:2016-05-08 summary:CNN architectures have terrific recognition performance but rely on spatialpooling which makes it difficult to adapt them to tasks that require densepixel-accurate labeling. This paper makes two contributions: (1) We demonstratethat while the apparent spatial resolution of convolutional feature maps islow, the high-dimensional feature representation contains significant sub-pixellocalization information. (2) We describe a multi-resolution reconstructionarchitecture, akin to a Laplacian pyramid, that uses skip connections fromhigher resolution feature maps to successively refine segment boundariesreconstructed from lower resolution maps. This approach yields state-of-the-artsemantic segmentation results on PASCAL without resorting to more complex CRFor detection driven architectures.
arxiv-1605-02260 | Deeply Exploit Depth Information for Object Detection |  http://arxiv.org/abs/1605.02260  | author:Saihui Hou, Zilei Wang, Feng Wu category:cs.CV published:2016-05-08 summary:This paper addresses the issue on how to more effectively coordinate thedepth with RGB aiming at boosting the performance of RGB-D object detection.Particularly, we investigate two primary ideas under the CNN model: propertyderivation and property fusion. Firstly, we propose that the depth can beutilized not only as a type of extra information besides RGB but also to derivemore visual properties for comprehensively describing the objects of interest.So a two-stage learning framework consisting of property derivation and fusionis constructed. Here the properties can be derived either from the providedcolor/depth or their pairs (e.g. the geometry contour adopted in this paper).Secondly, we explore the fusion method of different properties in featurelearning, which is boiled down to, under the CNN model, from which layer theproperties should be fused together. The analysis shows that different semanticproperties should be learned separately and combined before passing into thefinal classifier. Actually, such a detection way is in accordance with themechanism of the primary neural cortex (V1) in brain. We experimentallyevaluate the proposed method on the challenging dataset, and have achievedstate-of-the-art performance.
arxiv-1605-02257 | A corpus of preposition supersenses in English web reviews |  http://arxiv.org/abs/1605.02257  | author:Nathan Schneider, Jena D. Hwang, Vivek Srikumar, Meredith Green, Kathryn Conger, Tim O'Gorman, Martha Palmer category:cs.CL published:2016-05-08 summary:We present the first corpus annotated with preposition supersenses,unlexicalized categories for semantic functions that can be marked by Englishprepositions (Schneider et al., 2015). That scheme improves upon itspredecessors to better facilitate comprehensive manual annotation. Moreover,unlike the previous schemes, the preposition supersenses are organizedhierarchically. Our data will be publicly released on the web upon publication.
arxiv-1605-02268 | Rate-Distortion Bounds on Bayes Risk in Supervised Learning |  http://arxiv.org/abs/1605.02268  | author:Matthew Nokleby, Ahmad Beirami, Robert Calderbank category:cs.IT cs.LG math.IT stat.ML published:2016-05-08 summary:An information-theoretic framework is presented for estimating the number oflabeled samples needed to train a classifier in a parametric Bayesian setting.Ideas from rate-distortion theory are used to derive bounds on the average$L_1$ or $L_\infty$ distance between the learned classifier and the truemaximum a posteriori classifier---which are well-established surrogates for theexcess classification error due to imperfect learning---in terms of thedifferential entropy of the posterior distribution, the Fisher information ofthe parametric family, and the number of training samples available. Themaximum {\em a posteriori} classifier is viewed as a random source, labeledtraining data are viewed as a finite-rate encoding of the source, and the $L_1$or $L_\infty$ Bayes risk is viewed as the average distortion. The result is acomplementary framework to the well-known probably approximately correct (PAC)framework. PAC bounds characterize worst-case learning performance of a familyof classifiers whose complexity is captured by the Vapnik-Chervonenkis (VC)dimension. The rate-distortion framework, on the other hand, characterizes theaverage-case performance of a family of data distributions in terms of aquantity called the interpolation dimension, which represents the complexity ofthe family of data distributions. The resulting bounds do not suffer from thepessimism typical of the PAC framework, particularly when the training set issmall. The framework also naturally accommodates multi-class settings.Furthermore, Monte Carlo methods provide accurate estimates of the bounds evenfor complicated distributions. The effectiveness of this framework isdemonstrated in both a binary and multi-class Gaussian setting.
arxiv-1605-02269 | Predicting Performance on MOOC Assessments using Multi-Regression Models |  http://arxiv.org/abs/1605.02269  | author:Zhiyun Ren, Huzefa Rangwala, Aditya Johri category:cs.CY cs.LG published:2016-05-08 summary:The past few years has seen the rapid growth of data min- ing approaches forthe analysis of data obtained from Mas- sive Open Online Courses (MOOCs). Theobjectives of this study are to develop approaches to predict the scores a stu-dent may achieve on a given grade-related assessment based on information,considered as prior performance or prior ac- tivity in the course. We develop apersonalized linear mul- tiple regression (PLMR) model to predict the grade fora student, prior to attempting the assessment activity. The developed model isreal-time and tracks the participation of a student within a MOOC (viaclick-stream server logs) and predicts the performance of a student on the nextas- sessment within the course offering. We perform a com- prehensive set ofexperiments on data obtained from three openEdX MOOCs via a Stanford Universityinitiative. Our experimental results show the promise of the proposed ap-proach in comparison to baseline approaches and also helps in identification ofkey features that are associated with the study habits and learning behaviorsof students.
arxiv-1605-02276 | Problems With Evaluation of Word Embeddings Using Word Similarity Tasks |  http://arxiv.org/abs/1605.02276  | author:Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer category:cs.CL published:2016-05-08 summary:Lacking standardized extrinsic evaluation methods for vector representationsof words, the NLP community has relied heavily on word similarity tasks as aproxy for intrinsic evaluation of word vectors. Word similarity evaluation,which correlates the distance between vectors and human judgments of semanticsimilarity is attractive, because it is computationally inexpensive and fast.In this paper we present several problems associated with the evaluation ofword vectors on word similarity datasets, and summarize existing solutions. Ourstudy suggests that the use of word similarity tasks for evaluation of wordvectors is not sustainable and calls for further research on evaluationmethods.
arxiv-1605-02277 | On-Average KL-Privacy and its equivalence to Generalization for Max-Entropy Mechanisms |  http://arxiv.org/abs/1605.02277  | author:Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg category:stat.ML cs.CR published:2016-05-08 summary:We define On-Average KL-Privacy and present its properties and connections todifferential privacy, generalization and information-theoretic quantitiesincluding max-information and mutual information. The new definitionsignificantly weakens differential privacy, while preserving its minimalisticdesign features such as composition over small group and multiple queries aswell as closeness to post-processing. Moreover, we show that On-AverageKL-Privacy is **equivalent** to generalization for a large class ofcommonly-used tools in statistics and machine learning that samples from Gibbsdistributions---a class of distributions that arises naturally from the maximumentropy principle. In addition, a byproduct of our analysis yields a lowerbound for generalization error in terms of mutual information which reveals aninteresting interplay with known upper bounds that use the same quantity.
arxiv-1605-02289 | Detecting Ground Control Points via Convolutional Neural Network for Stereo Matching |  http://arxiv.org/abs/1605.02289  | author:Zhun Zhong, Songzhi Su, Donglin Cao, Shaozi Li category:cs.CV published:2016-05-08 summary:In this paper, we present a novel approach to detect ground control points(GCPs) for stereo matching problem. First of all, we train a convolutionalneural network (CNN) on a large stereo set, and compute the matching confidenceof each pixel by using the trained CNN model. Secondly, we present a groundcontrol points selection scheme according to the maximum matching confidence ofeach pixel. Finally, the selected GCPs are used to refine the matching costs,and we apply the new matching costs to perform optimization with semi-globalmatching algorithm for improving the final disparity maps. We evaluate ourapproach on the KITTI 2012 stereo benchmark dataset. Our experiments show thatthe proposed approach significantly improves the accuracy of disparity maps.
arxiv-1605-02305 | Estimating Depth from Monocular Images as Classification Using Deep Fully Convolutional Residual Networks |  http://arxiv.org/abs/1605.02305  | author:Yuanzhouhan Cao, Zifeng Wu, Chunhua Shen category:cs.CV published:2016-05-08 summary:Depth estimation from single monocular images is a key component of sceneunderstanding and has benefited largely from deep convolutional neural networks(CNN) recently. In this article, we take advantage of the recent deep residualnetworks and propose a simple yet effective approach to this problem. Weformulate depth estimation as a pixel-wise classification task. Specifically,we first discretize the continuous depth values into multiple bins and labelthe bins according to their depth range. Then we train fully convolutional deepresidual networks to predict the depth label of each pixel. Performing discretedepth label classification instead of continuous depth value regression allowsus to predict a confidence in the form of probability distribution. We furtherapply fully-connected conditional random fields (CRF) as a post processing stepto enforce local smoothness interactions, which improves the results. Weevaluate our approach on the NYUDepth v2 dataset and achieve state-of-the-artperformance.
arxiv-1605-02315 | Information Recovery in Shuffled Graphs via Graph Matching |  http://arxiv.org/abs/1605.02315  | author:Vince Lyzinski category:stat.ML cs.IT math.CO math.IT published:2016-05-08 summary:In a number of methodologies for joint inference across graphs, it is assumedthat an explicit vertex correspondence is a priori known across the vertex setsof the graphs. While this assumption is often reasonable, in practice thesecorrespondences may be unobserved and/or errorfully observed, and graphmatching---aligning a pair of graphs to minimize their edge disagreements---isused to align the graphs before performing subsequent inference. Herein, weexplore the duality between the loss of mutual information due to an errorfullyobserved vertex correspondence and the ability of graph matching algorithms torecover the true correspondence across graphs. We then demonstrate thepractical effect that graph shuffling---and matching---can have on subsequentinference, with examples from two sample graph hypothesis testing and jointgraph clustering.
arxiv-1605-02346 | Chained Predictions Using Convolutional Neural Networks |  http://arxiv.org/abs/1605.02346  | author:Georgia Gkioxari, Alexander Toshev, Navdeep Jaitly category:cs.CV published:2016-05-08 summary:In this paper, we present an adaptation of the sequence-to-sequence model forstructured output prediction in vision tasks. In this model the outputvariables for a given input are predicted sequentially using neural networks.The prediction for each output variable depends not only on the input but alsoon the previously predicted output variables. The model is applied to spatiallocalization tasks and uses convolutional neural networks (CNNs) for processinginput images and a multi-scale deconvolutional architecture for making spatialpredictions at each time step. We explore the impact of weight sharing with arecurrent connection matrix between consecutive predictions, and compare it toa formulation where these weights are not tied. Untied weights are particularlysuited for problems with a fixed sized structure, where different classes ofoutput are predicted in different steps. We show that chained predictionsachieve top performing results on human pose estimation from single images andvideos.
arxiv-1605-02372 | Active Learning for Community Detection in Stochastic Block Models |  http://arxiv.org/abs/1605.02372  | author:Akshay Gadde, Eyal En Gad, Salman Avestimehr, Antonio Ortega category:cs.LG cs.SI math.PR published:2016-05-08 summary:The stochastic block model (SBM) is an important generative model for randomgraphs in network science and machine learning, useful for benchmarkingcommunity detection (or clustering) algorithms. The symmetric SBM generates agraph with $2n$ nodes which cluster into two equally sized communities. Nodesconnect with probability $p$ within a community and $q$ across differentcommunities. We consider the case of $p=a\ln (n)/n$ and $q=b\ln (n)/n$. In thiscase, it was recently shown that recovering the community membership (or label)of every node with high probability (w.h.p.) using only the graph is possibleif and only if the Chernoff-Hellinger (CH) divergence$D(a,b)=(\sqrt{a}-\sqrt{b})^2 \geq 1$. In this work, we study if, and by howmuch, community detection below the clustering threshold (i.e. $D(a,b)<1$) ispossible by querying the labels of a limited number of chosen nodes (i.e.,active learning). Our main result is to show that, under certain conditions,sampling the labels of a vanishingly small fraction of nodes (a numbersub-linear in $n$) is sufficient for exact community detection even when$D(a,b)<1$. Furthermore, we provide an efficient learning algorithm whichrecovers the community memberships of all nodes w.h.p. as long as the number ofsampled points meets the sufficient condition. We also show that recovery isnot possible if the number of observed labels is less than $n^{1-D(a,b)}$. Thevalidity of our results is demonstrated through numerical experiments.
arxiv-1605-02150 | On Improving Informativity and Grammaticality for Multi-Sentence Compression |  http://arxiv.org/abs/1605.02150  | author:Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond Wong, Fang Chen category:cs.CL published:2016-05-07 summary:Multi Sentence Compression (MSC) is of great value to many real worldapplications, such as guided microblog summarization, opinion summarization andnewswire summarization. Recently, word graph-based approaches have beenproposed and become popular in MSC. Their key assumption is that redundancyamong a set of related sentences provides a reliable way to generateinformative and grammatical sentences. In this paper, we propose an effectiveapproach to enhance the word graph-based MSC and tackle the issue that most ofthe state-of-the-art MSC approaches are confronted with: i.e., improving bothinformativity and grammaticality at the same time. Our approach consists ofthree main components: (1) a merging method based on Multiword Expressions(MWE); (2) a mapping strategy based on synonymy between words; (3) a re-rankingstep to identify the best compression candidates generated using a POS-basedlanguage model (POS-LM). We demonstrate the effectiveness of this novelapproach using a dataset made of clusters of English newswire sentences. Theobserved improvements on informativity and grammaticality of the generatedcompressions show that our approach is superior to state-of-the-art MSCmethods.
arxiv-1605-02140 | Matrix Factorization-Based Clustering Of Image Features For Bandwidth-Constrained Information Retrieval |  http://arxiv.org/abs/1605.02140  | author:Jacob Chakareski, Immanuel Manohar, Shantanu Rane category:cs.CV 62h25, I.4 published:2016-05-07 summary:We consider the problem of accurately and efficiently querying a remoteserver to retrieve information about images captured by a mobile device. Inaddition to reduced transmission overhead and computational complexity, theretrieval protocol should be robust to variations in the image acquisitionprocess, such as translation, rotation, scaling, and sensor-relateddifferences. We propose to extract scale-invariant image features and thenperform clustering to reduce the number of features needed for image matching.Principal Component Analysis (PCA) and Non-negative Matrix Factorization (NMF)are investigated as candidate clustering approaches. The image matchingcomplexity at the database server is quadratic in the (small) number ofclusters, not in the (very large) number of image features. We employ animage-dependent information content metric to approximate the model order,i.e., the number of clusters, needed for accurate matching, which is preferableto setting the model order using trial and error. We show how to combine thehypotheses provided by PCA and NMF factor loadings, thereby obtaining moreaccurate retrieval than using either approach alone. In experiments on adatabase of urban images, we obtain a top-1 retrieval accuracy of 89% and atop-3 accuracy of 92.5%.
arxiv-1605-02130 | Robust Dialog State Tracking for Large Ontologies |  http://arxiv.org/abs/1605.02130  | author:Franck Dernoncourt, Ji Young Lee, Trung H. Bui, Hung H. Bui category:cs.CL cs.AI cs.LG published:2016-05-07 summary:The Dialog State Tracking Challenge 4 (DSTC 4) differentiates itself from theprevious three editions as follows: the number of slot-value pairs present inthe ontology is much larger, no spoken language understanding output is given,and utterances are labeled at the subdialog level. This paper describes a noveldialog state tracking method designed to work robustly under these conditions,using elaborate string matching, coreference resolution tailored for dialogsand a few other improvements. The method can correctly identify many valuesthat are not explicitly present in the utterance. On the final evaluation, ourmethod came in first among 7 competing teams and 24 entries. The F1-scoreachieved by our method was 9 and 7 percentage points higher than that of therunner-up for the utterance-level evaluation and for the subdialog-levelevaluation, respectively.
arxiv-1605-02178 | Fast and High-Quality Bilateral Filtering Using Gauss-Chebyshev Approximation |  http://arxiv.org/abs/1605.02178  | author:Sanjay Ghosh, Kunal N. Chaudhury category:cs.CV published:2016-05-07 summary:The bilateral filter is an edge-preserving smoother that has diverseapplications in image processing, computer vision, computer graphics, andcomputational photography. The filter uses a spatial kernel along with a rangekernel to perform edge-preserving smoothing. In this paper, we consider theGaussian bilateral filter where both the kernels are Gaussian. A directimplementation of the Gaussian bilateral filter requires $O(\sigma_s^2)$operations per pixel, where $\sigma_s$ is the standard deviation of the spatialGaussian. In fact, it is well-known that the direct implementation is slow inpractice. We present an approximation of the Gaussian bilateral filter, wherebywe can cut down the number of operations to $O(1)$ per pixel for any arbitrary$\sigma_s$, and yet achieve very high-quality filtering that is almostindistinguishable from the output of the original filter. We demonstrate thatthe proposed approximation is few orders faster in practice compared to thedirect implementation. We also demonstrate that the approximation iscompetitive with existing fast algorithms in terms of speed and accuracy.
arxiv-1605-02164 | Fast Bilateral Filtering of Vector-Valued Images |  http://arxiv.org/abs/1605.02164  | author:Sanjay Ghosh, Kunal N. Chaudhury category:cs.CV published:2016-05-07 summary:In this paper, we consider a natural extension of the edge-preservingbilateral filter for vector-valued images. The direct computation of thisnon-linear filter is slow in practice. We demonstrate how a fast algorithm canbe obtained by first approximating the Gaussian kernel of the bilateral filterusing raised-cosines, and then using Monte Carlo sampling. We presentsimulation results on color images to demonstrate the accuracy of the algorithmand the speedup over the direct implementation.
arxiv-1605-02190 | Matching models across abstraction levels with Gaussian Processes |  http://arxiv.org/abs/1605.02190  | author:Giulio Caravagna, Luca Bortolussi, Guido Sanguinetti category:stat.ML published:2016-05-07 summary:Biological systems are often modelled at different levels of abstractiondepending on the particular aims/resources of a study. Such different modelsoften provide qualitatively concordant predictions over specificparametrisations, but it is generally unclear whether model predictions arequantitatively in agreement, and whether such agreement holds for differentparametrisations. Here we present a generally applicable statistical machinelearning methodology to automatically reconcile the predictions of differentmodels across abstraction levels. Our approach is based on defining acorrection map, a random function which modifies the output of a model in orderto match the statistics of the output of a different model of the same system.We use two biological examples to give a proof-of-principle demonstration ofthe methodology, and discuss its advantages and potential further applications.
arxiv-1605-02196 | All Weather Perception: Joint Data Association, Tracking, and Classification for Autonomous Ground Vehicles |  http://arxiv.org/abs/1605.02196  | author:Peter Radecki, Mark Campbell, Kevin Matzen category:cs.SY cs.CV cs.LG cs.RO published:2016-05-07 summary:A novel probabilistic perception algorithm is presented as a real-time jointsolution to data association, object tracking, and object classification for anautonomous ground vehicle in all-weather conditions. The presented algorithmextends a Rao-Blackwellized Particle Filter originally built with a particlefilter for data association and a Kalman filter for multi-object tracking(Miller et al. 2011a) to now also include multiple model tracking forclassification. Additionally a state-of-the-art vision detection algorithm thatincludes heading information for autonomous ground vehicle (AGV) applicationswas implemented. Cornell's AGV from the DARPA Urban Challenge was upgraded andused to experimentally examine if and how state-of-the-art vision algorithmscan complement or replace lidar and radar sensors. Sensor and algorithmperformance in adverse weather and lighting conditions is tested. Experimentalevaluation demonstrates robust all-weather data association, tracking, andclassification where camera, lidar, and radar sensors complement each otherinside the joint probabilistic perception algorithm.
arxiv-1605-02216 | Distributed stochastic optimization for deep learning (thesis) |  http://arxiv.org/abs/1605.02216  | author:Sixin Zhang category:cs.LG published:2016-05-07 summary:We study the problem of how to distribute the training of large-scale deeplearning models in the parallel computing environment. We propose a newdistributed stochastic optimization method called Elastic Averaging SGD(EASGD). We analyze the convergence rate of the EASGD method in the synchronousscenario and compare its stability condition with the existing ADMM method inthe round-robin scheme. An asynchronous and momentum variant of the EASGDmethod is applied to train deep convolutional neural networks for imageclassification on the CIFAR and ImageNet datasets. Our approach accelerates thetraining and furthermore achieves better test accuracy. It also requires a muchsmaller amount of communication than other common baseline approaches such asthe DOWNPOUR method. We then investigate the limit in speedup of the initial and the asymptoticphase of the mini-batch SGD, the momentum SGD, and the EASGD methods. We findthat the spread of the input data distribution has a big impact on theirinitial convergence rate and stability region. We also find a surprisingconnection between the momentum SGD and the EASGD method with a negative movingaverage rate. A non-convex case is also studied to understand when EASGD canget trapped by a saddle point. Finally, we scale up the EASGD method by using a tree structured networktopology. We show empirically its advantage and challenge. We also establish aconnection between the EASGD and the DOWNPOUR method with the classical Jacobiand the Gauss-Seidel method, thus unifying a class of distributed stochasticoptimization methods.
arxiv-1605-02234 | A Bayesian Group Sparse Multi-Task Regression Model for Imaging Genetics |  http://arxiv.org/abs/1605.02234  | author:Keelin Greenlaw, Elena Szefer, Jinko Graham, Mary Lesperance, Farouk S. Nathoo category:stat.ME stat.AP stat.ML published:2016-05-07 summary:Motivation: Recent advances in technology for brain imaging andhigh-throughput genotyping have motivated studies examining the influence ofgenetic variation on brain structure. Wang et al. (Bioinformatics, 2012) havedeveloped an approach for the analysis of imaging genomic studies usingpenalized multi-task regression with regularization based on a novel group$l_{2,1}$-norm penalty which encourages structured sparsity at both the genelevel and SNP level. While incorporating a number of useful features, theproposed method only furnishes a point estimate of the regression coefficients;techniques for conducting statistical inference are not provided. A newBayesian method is proposed here to overcome this limitation. Results: We develop a Bayesian hierarchical modeling formulation where theposterior mode corresponds to the estimator proposed by Wang et al.(\textit{Bioinformatics}, 2012), and an approach that allows for full posteriorinference including the construction of interval estimates for the regressionparameters. We show that the proposed hierarchical model can be expressed as athree-level Gaussian scale mixture and this representation facilitates the useof a Gibbs sampling algorithm for posterior simulation. Simulation studiesdemonstrate that the interval estimates obtained using our approach achieveadequate coverage probabilities that outperform those obtained from thenonparametric bootstrap. Our proposed methodology is applied to the analysis ofneuroimaging and genetic data collected as part of the Alzheimer's DiseaseNeuroimaging Initiative (ADNI), and this analysis of the ADNI cohortdemonstrates clearly the value added of incorporating interval estimationbeyond only point estimation when relating SNPs to brain imagingendophenotypes.
arxiv-1605-02240 | On Image segmentation using Fractional Gradients-Learning Model Parameters using Approximate Marginal Inference |  http://arxiv.org/abs/1605.02240  | author:Anish Acharya, Uddipan Mukherjee, Charless Fowlkes category:cs.CV published:2016-05-07 summary:Estimates of image gradients play a ubiquitous role in image segmentation andclassification problems since gradients directly relate to the boundaries orthe edges of a scene. This paper proposes an unified approach to gradientestimation based on fractional calculus that is computationally cheap andreadily applicable to any existing algorithm that relies on image gradients. Weshow experiments on edge detection and image segmentation on the StanfordBackgrounds Dataset where these improved local gradients outperforms state ofthe art, achieving a performance of 79.2% average accuracy.
arxiv-1605-02129 | Adobe-MIT submission to the DSTC 4 Spoken Language Understanding pilot task |  http://arxiv.org/abs/1605.02129  | author:Franck Dernoncourt, Ji Young Lee, Trung H. Bui, Hung H. Bui category:cs.CL cs.AI cs.LG published:2016-05-07 summary:The Dialog State Tracking Challenge 4 (DSTC 4) proposes several pilot tasks.In this paper, we focus on the spoken language understanding pilot task, whichconsists of tagging a given utterance with speech acts and semantic slots. Wecompare different classifiers: the best system obtains 0.52 and 0.67 F1-scoreson the test set for speech act recognition for the tourist and the guiderespectively, and 0.52 F1-score for semantic tagging for both the guide and thetourist.
arxiv-1605-02226 | Neural Autoregressive Distribution Estimation |  http://arxiv.org/abs/1605.02226  | author:Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, Hugo Larochelle category:cs.LG published:2016-05-07 summary:We present Neural Autoregressive Distribution Estimation (NADE) models, whichare neural network architectures applied to the problem of unsuperviseddistribution and density estimation. They leverage the probability product ruleand a weight sharing scheme inspired from restricted Boltzmann machines, toyield an estimator that is both tractable and has good generalizationperformance. We discuss how they achieve competitive performance in modelingboth binary and real-valued observations. We also present how deep NADE modelscan be trained to be agnostic to the ordering of input dimensions used by theautoregressive product rule decomposition. Finally, we also show how to exploitthe topological structure of pixels in images using a deep convolutionalarchitecture for NADE.
arxiv-1605-02134 | Neural Recovery Machine for Chinese Dropped Pronoun |  http://arxiv.org/abs/1605.02134  | author:Wei-Nan Zhang, Ting Liu, Qingyu Yin, Yu Zhang category:cs.CL published:2016-05-07 summary:Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese,Japanese etc. Previous work mainly focused on painstakingly exploring theempirical features for DPs recovery. In this paper, we propose a neuralrecovery machine (NRM) to model and recover DPs in Chinese, so that to avoidthe non-trivial feature engineering process. The experimental results show thatthe proposed NRM significantly outperforms the state-of-the-art approaches onboth two heterogeneous datasets. Further experiment results of Chinese zeropronoun (ZP) resolution show that the performance of ZP resolution can also beimproved by recovering the ZPs to DPs.
arxiv-1605-01843 | Perceptually Consistent Color-to-Gray Image Conversion |  http://arxiv.org/abs/1605.01843  | author:Shaodi You, Nick Barnes, Janine Walker category:cs.CV published:2016-05-06 summary:In this paper, we propose a color to grayscale image conversion algorithm(C2G) that aims to preserve the perceptual properties of the color image asmuch as possible. To this end, we propose measures for two perceptualproperties based on contemporary research in vision science: brightness andmulti-scale contrast. The brightness measurement is based on the idea that thebrightness of a grayscale image will affect the perception of the probabilityof color information. The color contrast measurement is based on the idea thatthe contrast of a given pixel to its surroundings can be measured as a linearcombination of color contrast at different scales. Based on these measures wepropose a graph based optimization framework to balance the brightness andcontrast measurements. To solve the optimization, an $\ell_1$-norm based methodis provided which converts color discontinuities to brightness discontinuities.To validate our methods, we evaluate against the existing \cadik and Color250datasets, and against NeoColor, a new dataset that improves over existing C2Gdatasets. NeoColor contains around 300 images from typical C2G scenarios,including: commercial photograph, printing, books, magazines, masterpieceartworks and computer designed graphics. We show improvements in metrics ofperformance, and further through a user study, we validate the performance ofboth the algorithm and the metric.
arxiv-1605-01845 | Detecting Context Dependence in Exercise Item Candidates Selected from Corpora |  http://arxiv.org/abs/1605.01845  | author:Ildikó Pilán category:cs.CL published:2016-05-06 summary:We explore the factors influencing the dependence of single sentences ontheir larger textual context in order to automatically identify candidatesentences for language learning exercises from corpora which are presentable inisolation. An in-depth investigation of this question has not been previouslycarried out. Understanding this aspect can contribute to a more efficientselection of candidate sentences which, besides reducing the time required foritem writing, can also ensure a higher degree of variability and authenticity.We present a set of relevant aspects collected based on the qualitativeanalysis of a smaller set of context-dependent corpus example sentences.Furthermore, we implemented a rule-based algorithm using these criteria whichachieved an average precision of 0.76 for the identification of differentissues related to context dependence. The method has also been evaluatedempirically where 80% of the sentences in which our system did not detectcontext-dependent elements were also considered context-independent by humanraters.
arxiv-1605-01855 | Resource allocation using metaheuristic search |  http://arxiv.org/abs/1605.01855  | author:Andy M. Connor, Amit Shah category:cs.NE published:2016-05-06 summary:This research is focused on solving problems in the area of software projectmanagement using metaheuristic search algorithms and as such is research in thefield of search based software engineering. The main aim of this research is toevaluate the performance of different metaheuristic search techniques inresource allocation and scheduling problems that would be typical of softwaredevelopment projects. This paper reports a set of experiments which evaluatethe performance of three algorithms, namely simulated annealing, tabu searchand genetic algorithms. The experimental results indicate that all of themetaheuristics search techniques can be used to solve problems in resourceallocation and scheduling within a software project. Finally, a comparativeanalysis suggests that overall the genetic algorithm had performed better thansimulated annealing and tabu search.
arxiv-1605-01919 | User Reviews and Language: How Language Influences Ratings |  http://arxiv.org/abs/1605.01919  | author:Scott A. Hale category:cs.HC cs.CL cs.CY H.5.m; H.3.5 published:2016-05-06 summary:The number of user reviews of tourist attractions, restaurants, mobile apps,etc. is increasing for all languages; yet, research is lacking on how reviewsin multiple languages should be aggregated and displayed. Speakers of differentlanguages may have consistently different experiences, e.g., differentinformation available in different languages at tourist attractions ordifferent user experiences with software due tointernationalization/localization choices. This paper assesses the similarityin the ratings given by speakers of different languages to London touristattractions on TripAdvisor. The correlations between different languages aregenerally high, but some language pairs are more correlated than others. Theresults question the common practice of computing average ratings from reviewsin many languages.
arxiv-1605-01923 | UAV-based Autonomous Image Acquisition with Multi-View Stereo Quality Assurance by Confidence Prediction |  http://arxiv.org/abs/1605.01923  | author:Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, Horst Bischof category:cs.CV cs.RO published:2016-05-06 summary:In this paper we present an autonomous system for acquiring close-rangehigh-resolution images that maximize the quality of a later-on 3Dreconstruction with respect to coverage, ground resolution and 3D uncertainty.In contrast to previous work, our system uses the already acquired images topredict the confidence in the output of a dense multi-view stereo approachwithout executing it. This confidence encodes the likelihood of a successfulreconstruction with respect to the observed scene and potential cameraconstellations. Our prediction module runs in real-time and can be trainedwithout any externally recorded ground truth. We use the confidence predictionfor on-site quality assurance and for planning further views that are tailoredfor a specific multi-view stereo approach with respect to the given scene. Wedemonstrate the capabilities of our approach with an autonomous Unmanned AerialVehicle (UAV) in a challenging outdoor scenario.
arxiv-1605-01939 | Energy Disaggregation for Real-Time Building Flexibility Detection |  http://arxiv.org/abs/1605.01939  | author:Elena Mocanu, Phuong H. Nguyen, Madeleine Gibescu category:stat.ML cs.AI cs.LG published:2016-05-06 summary:Energy is a limited resource which has to be managed wisely, taking intoaccount both supply-demand matching and capacity constraints in thedistribution grid. One aspect of the smart energy management at the buildinglevel is given by the problem of real-time detection of flexible demandavailable. In this paper we propose the use of energy disaggregation techniquesto perform this task. Firstly, we investigate the use of existingclassification methods to perform energy disaggregation. A comparison isperformed between four classifiers, namely Naive Bayes, k-Nearest Neighbors,Support Vector Machine and AdaBoost. Secondly, we propose the use of RestrictedBoltzmann Machine to automatically perform feature extraction. The extractedfeatures are then used as inputs to the four classifiers and consequently shownto improve their accuracy. The efficiency of our approach is demonstrated on areal database consisting of detailed appliance-level measurements with hightemporal resolution, which has been used for energy disaggregation in previousstudies, namely the REDD. The results show robustness and good generalizationcapabilities to newly presented buildings with at least 96% accuracy.
arxiv-1605-01950 | Automatic LQR Tuning Based on Gaussian Process Global Optimization |  http://arxiv.org/abs/1605.01950  | author:Alonso Marco, Philipp Hennig, Jeannette Bohg, Stefan Schaal, Sebastian Trimpe category:cs.RO cs.LG cs.SY published:2016-05-06 summary:This paper proposes an automatic controller tuning framework based on linearoptimal control combined with Bayesian optimization. With this framework, aninitial set of controller gains is automatically improved according to apre-defined performance objective evaluated from experimental data. Theunderlying Bayesian optimization algorithm is Entropy Search, which representsthe latent objective as a Gaussian process and constructs an explicit beliefover the location of the objective minimum. This is used to maximize theinformation gain from each experimental evaluation. Thus, this framework shallyield improved controllers with fewer evaluations compared to alternativeapproaches. A seven-degree-of-freedom robot arm balancing an inverted pole isused as the experimental demonstrator. Results of a two- and four-dimensionaltuning problems highlight the method's potential for automatic controllertuning on robotic platforms.
arxiv-1605-01999 | Visual Saliency Based on Scale-Space Analysis in the Frequency Domain |  http://arxiv.org/abs/1605.01999  | author:Jian Li, Martin Levine, Xiangjing An, Xin Xu, Hangen He category:cs.CV published:2016-05-06 summary:We address the issue of visual saliency from three perspectives. First, weconsider saliency detection as a frequency domain analysis problem. Second, weachieve this by employing the concept of {\it non-saliency}. Third, wesimultaneously consider the detection of salient regions of different size. Thepaper proposes a new bottom-up paradigm for detecting visual saliency,characterized by a scale-space analysis of the amplitude spectrum of naturalimages. We show that the convolution of the {\it image amplitude spectrum} witha low-pass Gaussian kernel of an appropriate scale is equivalent to such animage saliency detector. The saliency map is obtained by reconstructing the 2-Dsignal using the original phase and the amplitude spectrum, filtered at a scaleselected by minimizing saliency map entropy. A Hypercomplex Fourier Transformperforms the analysis in the frequency domain. Using available databases, wedemonstrate experimentally that the proposed model can predict human fixationdata. We also introduce a new image database and use it to show that thesaliency detector can highlight both small and large salient regions, as wellas inhibit repeated distractors in cluttered images. In addition, we show thatit is able to predict salient regions on which people focus their attention.
arxiv-1605-02019 | Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec |  http://arxiv.org/abs/1605.02019  | author:Christopher E Moody category:cs.CL published:2016-05-06 summary:Distributed dense word vectors have been shown to be effective at capturingtoken-level semantic and syntactic regularities in language, while topic modelscan form interpretable representations over documents. In this work, wedescribe lda2vec, a model that learns dense word vectors jointly withDirichlet-distributed latent document-level mixtures of topic vectors. Incontrast to continuous dense document representations, this formulationproduces sparse, interpretable document mixtures through a non-negative simplexconstraint. Our method is simple to incorporate into existing automaticdifferentiation frameworks and allows for unsupervised document representationsgeared for use by scientists while simultaneously learning word vectors and thelinear relationships between them.
arxiv-1605-02026 | Training Neural Networks Without Gradients: A Scalable ADMM Approach |  http://arxiv.org/abs/1605.02026  | author:Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, Tom Goldstein category:cs.LG published:2016-05-06 summary:With the growing importance of large network models and enormous trainingdatasets, GPUs have become increasingly necessary to train neural networks.This is largely because conventional optimization algorithms rely on stochasticgradient methods that don't scale well to large numbers of cores in a clustersetting. Furthermore, the convergence of all gradient methods, including batchmethods, suffers from common problems like saturation effects, poorconditioning, and saddle points. This paper explores an unconventional trainingmethod that uses alternating direction methods and Bregman iteration to trainnetworks without gradient descent steps. The proposed method reduces thenetwork training problem to a sequence of minimization sub-steps that can eachbe solved globally in closed form. The proposed method is advantageous becauseit avoids many of the caveats that make gradient methods slow on highlynon-convex problems. The method exhibits strong scaling in the distributedsetting, yielding linear speedups even when split over thousands of cores.
arxiv-1605-02046 | Low-Complexity Stochastic Generalized Belief Propagation |  http://arxiv.org/abs/1605.02046  | author:Farzin Haddadpour, Mahdi Jafari Siavoshani, Morteza Noshad category:cs.LG cs.AI cs.IT math.IT published:2016-05-06 summary:The generalized belief propagation (GBP), introduced by Yedidia et al., is anextension of the belief propagation (BP) algorithm, which is widely used indifferent problems involved in calculating exact or approximate marginals ofprobability distributions. In many problems, it has been observed that theaccuracy of GBP considerably outperforms that of BP. However, because ingeneral the computational complexity of GBP is higher than BP, its applicationis limited in practice. In this paper, we introduce a stochastic version of GBP called stochasticgeneralized belief propagation (SGBP) that can be considered as an extension tothe stochastic BP (SBP) algorithm introduced by Noorshams et al. They haveshown that SBP reduces the complexity per iteration of BP by an order ofmagnitude in alphabet size. In contrast to SBP, SGBP can reduce the computationcomplexity if certain topological conditions are met by the region graphassociated to a graphical model. However, this reduction can be larger thanonly one order of magnitude in alphabet size. In this paper, we characterizethese conditions and the amount of computation gain that we can obtain by usingSGBP. Finally, using similar proof techniques employed by Noorshams et al., forgeneral graphical models satisfy contraction conditions, we prove theasymptotic convergence of SGBP to the unique GBP fixed point, as well asproviding non-asymptotic upper bounds on the mean square error and on the highprobability error.
arxiv-1605-02060 | Deformably Registering and Annotating Whole CLARITY Brains to an Atlas via Masked LDDMM |  http://arxiv.org/abs/1605.02060  | author:Kwame S. Kutten, Joshua T. Vogelstein, Nicolas Charon, Li Ye, Karl Deisseroth, Michael I. Miller category:q-bio.QM cs.CV published:2016-05-06 summary:The CLARITY method renders brains optically transparent to enablehigh-resolution imaging in the structurally intact brain. Anatomicallyannotating CLARITY brains is necessary for discovering which regions containsignals of interest. Manually annotating whole-brain, terabyte CLARITY imagesis difficult, time-consuming, subjective, and error-prone. Automaticallyregistering CLARITY images to a pre-annotated brain atlas offers a solution,but is difficult for several reasons. Removal of the brain from the skull andsubsequent storage and processing cause variable non-rigid deformations, thuscompounding inter-subject anatomical variability. Additionally, the signal inCLARITY images arises from various biochemical contrast agents which onlysparsely label brain structures. This sparse labeling challenges the mostcommonly used registration algorithms that need to match image histogramstatistics to the more densely labeled histological brain atlases. The standardmethod is a multiscale Mutual Information B-spline algorithm that dynamicallygenerates an average template as an intermediate registration target. Wedetermined that this method performs poorly when registering CLARITY brains tothe Allen Institute's Mouse Reference Atlas (ARA), because the image histogramstatistics are poorly matched. Therefore, we developed a method (Mask-LDDMM)for registering CLARITY images, that automatically find the brain boundary andlearns the optimal deformation between the brain and atlas masks. UsingMask-LDDMM without an average template provided better results than thestandard approach when registering CLARITY brains to the ARA. The LDDMMpipelines developed here provide a fast automated way to anatomically annotateCLARITY images. Our code is available as open source software athttp://NeuroData.io.
arxiv-1605-02065 | Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds |  http://arxiv.org/abs/1605.02065  | author:Mark Bun, Thomas Steinke category:cs.CR cs.DS cs.IT cs.LG math.IT published:2016-05-06 summary:"Concentrated differential privacy" was recently introduced by Dwork andRothblum as a relaxation of differential privacy, which permits sharperanalyses of many privacy-preserving computations. We present an alternativeformulation of the concept of concentrated differential privacy in terms of theRenyi divergence between the distributions obtained by running an algorithm onneighboring inputs. With this reformulation in hand, we prove sharperquantitative results, establish lower bounds, and raise a few new questions. Wealso unify this approach with approximate differential privacy by giving anappropriate definition of "approximate concentrated differential privacy."
arxiv-1605-02077 | Function-Specific Mixing Times and Concentration Away from Equilibrium |  http://arxiv.org/abs/1605.02077  | author:Maxim Rabinovich, Aaditya Ramdas, Michael I. Jordan, Martin J. Wainwright category:math.ST cs.LG math.PR stat.TH published:2016-05-06 summary:Slow mixing is the central hurdle when working with Markov chains, especiallythose used for Monte Carlo approximations (MCMC). In many applications, it isonly of interest to to estimate the stationary expectations of a small set offunctions, and so the usual definition of mixing based on total variationconvergence may be too conservative. Accordingly, we introducefunction-specific analogs of mixing times and spectral gaps, and use them toprove Hoeffding-like function-specific concentration inequalities. Theseresults show that it is possible for empirical expectations of functions toconcentrate long before the underlying chain has mixed in the classical sense.We use our techniques to derive confidence intervals that are sharper thanthose implied by both classical Markov chain Hoeffding bounds andBerry-Esseen-corrected CLT bounds. For applications that require testing,rather than point estimation, we show similar improvements over recentsequential testing results for MCMC. We conclude by applying our framework toreal data examples of MCMC, providing evidence that our theory is both accurateand relevant to practice.
arxiv-1605-02113 | Likelihood Inflating Sampling Algorithm |  http://arxiv.org/abs/1605.02113  | author:Reihaneh Entezari, Radu V. Craiu, Jeffrey S. Rosenthal category:stat.ML stat.CO published:2016-05-06 summary:Markov Chain Monte Carlo (MCMC) sampling from a posterior distributioncorresponding to a massive data set can be computationally prohibitive sinceproducing one sample requires a number of operations that is linear in the datasize. In this paper, we introduce a new communication-free parallel method, theLikelihood Inflating Sampling Algorithm (LISA), that significantly reducescomputational costs by randomly splitting the dataset into smaller subsets andrunning MCMC methods independently and in parallel on each subset usingdifferent processors. Each processor will draw sub-samples from sub-posteriordistributions that are defined by "inflating" the likelihood function and thesub-samples are then combined using the importance re-sampling method toperform approximate full-data posterior samples. We test our method on severalexamples including the important case of Bayesian Additive Regression Trees(BART) using both simulated and real datasets. The method we propose showssignificant efficiency gains over the existing Consensus Monte Carlo of Scottet al. (2013).
arxiv-1605-02112 | Attribute And-Or Grammar for Joint Parsing of Human Attributes, Part and Pose |  http://arxiv.org/abs/1605.02112  | author:Seyoung Park, Bruce Xiaohan Nie, Song-Chun Zhu category:cs.CV published:2016-05-06 summary:This paper presents an attribute and-or grammar (A-AOG) model for jointlyinferring human body pose and human attributes in a parse graph with attributesaugmented to nodes in the hierarchical representation. In contrast to otherpopular methods in the current literature that train separate classifiers forposes and individual attributes, our method explicitly represents thedecomposition and articulation of body parts, and account for the correlationsbetween poses and attributes. The A-AOG model is an amalgamation of threetraditional grammar formulations: (i) Phrase structure grammar representing thehierarchical decomposition of the human body from whole to parts; (ii)Dependency grammar modeling the geometric articulation by a kinematic tree ofthe body pose; and (iii) Attribute grammar accounting for the compatibilityrelations between different parts in the hierarchy so that their appearancesfollow a consistent style. The parse graph outputs human detection, poseestimation, and attribute prediction simultaneously, which are intuitive andinterpretable. We conduct experiments on two tasks on two datasets, andexperimental results demonstrate the advantage of joint modeling in comparisonwith computing poses and attributes independently. Furthermore, our modelobtains better performance over existing methods for both pose estimation andattribute prediction tasks.
arxiv-1605-02105 | Distributed Learning with Infinitely Many Hypotheses |  http://arxiv.org/abs/1605.02105  | author:Angelia Nedić, Alex Olshevsky, César Uribe category:math.OC cs.LG stat.ML published:2016-05-06 summary:We consider a distributed learning setup where a network of agentssequentially access realizations of a set of random variables with unknowndistributions. The network objective is to find a parametrized distributionthat best describes their joint observations in the sense of theKullback-Leibler divergence. Apart from recent efforts in the literature, weanalyze the case of countably many hypotheses and the case of a continuum ofhypotheses. We provide non-asymptotic bounds for the concentration rate of theagents' beliefs around the correct hypothesis in terms of the number of agents,the network parameters, and the learning abilities of the agents. Additionally,we provide a novel motivation for a general set of distributed Non-Bayesianupdate rules as instances of the distributed stochastic mirror descentalgorithm.
arxiv-1605-02099 | Some Simulation Results for Emphatic Temporal-Difference Learning Algorithms |  http://arxiv.org/abs/1605.02099  | author:Huizhen Yu category:cs.LG published:2016-05-06 summary:This is a companion note to our recent study of the weak convergenceproperties of constrained emphatic temporal-difference learning (ETD)algorithms from a theoretic perspective. It supplements the latter analysiswith simulation results and illustrates the behavior of some of the ETDalgorithms using three example problems.
arxiv-1605-01839 | Beyond Local Search: Tracking Objects Everywhere with Instance-Specific Proposals |  http://arxiv.org/abs/1605.01839  | author:Gao Zhu, Fatih Porikli, Hongdong Li category:cs.CV published:2016-05-06 summary:Most tracking-by-detection methods employ a local search window around thepredicted object location in the current frame assuming the previous locationis accurate, the trajectory is smooth, and the computational capacity permits asearch radius that can accommodate the maximum speed yet small enough to reducemismatches. These, however, may not be valid always, in particular for fast andirregularly moving objects. Here, we present an object tracker that is notlimited to a local search window and has ability to probe efficiently theentire frame. Our method generates a small number of "high-quality" proposalsby a novel instance-specific objectness measure and evaluates them against theobject model that can be adopted from an existing tracking-by-detectionapproach as a core tracker. During the tracking process, we update the objectmodel concentrating on hard false-positives supplied by the proposals, whichhelp suppressing distractors caused by difficult background clutters, and learnhow to re-rank proposals according to the object model. Since we reducesignificantly the number of hypotheses the core tracker evaluates, we can usericher object descriptors and stronger detector. Our method outperforms mostrecent state-of-the-art trackers on popular tracking benchmarks, and providesimproved robustness for fast moving objects as well as for ultra low-frame-ratevideos.
arxiv-1605-01838 | DeepPicker: a Deep Learning Approach for Fully Automated Particle Picking in Cryo-EM |  http://arxiv.org/abs/1605.01838  | author:Feng Wang, Huichao Gong, Gaochao liu, Meijing Li, Chuangye Yan, Tian Xia, Xueming Li, Jianyang Zeng category:q-bio.QM cs.LG published:2016-05-06 summary:Particle picking is a time-consuming step in single-particle analysis andoften requires significant interventions from users, which has become abottleneck for future automated electron cryo-microscopy (cryo-EM). Here wereport a deep learning framework, called DeepPicker, to address this problemand fill the current gaps toward a fully automated cryo-EM pipeline. DeepPickeremploys a novel cross-molecule training strategy to capture common features ofparticles from previously-analyzed micrographs, and thus does not require anyhuman intervention during particle picking. Tests on the recently-publishedcryo-EM data of three complexes have demonstrated that our deep learning basedscheme can successfully accomplish the human-level particle picking process andidentify a sufficient number of particles that are comparable to those manuallyby human experts. These results indicate that DeepPicker can provide apractically useful tool to significantly reduce the time and manual effortspent in single-particle analysis and thus greatly facilitate high-resolutioncryo-EM structure determination.
arxiv-1605-01832 | Cross-Graph Learning of Multi-Relational Associations |  http://arxiv.org/abs/1605.01832  | author:Hanxiao Liu, Yiming Yang category:cs.LG published:2016-05-06 summary:Cross-graph Relational Learning (CGRL) refers to the problem of predictingthe strengths or labels of multi-relational tuples of heterogeneous objecttypes, through the joint inference over multiple graphs which specify theinternal connections among each type of objects. CGRL is an open challenge inmachine learning due to the daunting number of all possible tuples to deal withwhen the numbers of nodes in multiple graphs are large, and because the labeledtraining instances are extremely sparse as typical. Existing methods such astensor factorization or tensor-kernel machines do not work well because of thelack of convex formulation for the optimization of CGRL models, the poorscalability of the algorithms in handling combinatorial numbers of tuples,and/or the non-transductive nature of the learning methods which limits theirability to leverage unlabeled data in training. This paper proposes a novelframework which formulates CGRL as a convex optimization problem, enablestransductive learning using both labeled and unlabeled tuples, and offers ascalable algorithm that guarantees the optimal solution and enjoys a lineartime complexity with respect to the sizes of input graphs. In our experimentswith a subset of DBLP publication records and an Enzyme multi-source dataset,the proposed method successfully scaled to the large cross-graph inferenceproblem, and outperformed other representative approaches significantly.
arxiv-1605-01825 | Robust Optical Flow Estimation of Double-Layer Images under Transparency or Reflection |  http://arxiv.org/abs/1605.01825  | author:Jiaolong Yang, Hongdong Li, Yuchao Dai, Robby T. Tan category:cs.CV published:2016-05-06 summary:This paper deals with a challenging, frequently encountered, yet not properlyinvestigated problem in two-frame optical flow estimation. That is, the inputframes are compounds of two imaging layers -- one desired background layer ofthe scene, and one distracting, possibly moving layer due to transparency orreflection. In this situation, the conventional brightness constancy constraint-- the cornerstone of most existing optical flow methods -- will no longer bevalid. In this paper, we propose a robust solution to this problem. Theproposed method performs both optical flow estimation, and image layerseparation. It exploits a generalized double-layer brightness consistencyconstraint connecting these two tasks, and utilizes the priors for both ofthem. Experiments on both synthetic data and real images have confirmed theefficacy of the proposed method. To the best of our knowledge, this is thefirst attempt towards handling generic optical flow fields of two-frame imagescontaining transparency or reflection.
arxiv-1605-01813 | Estimating Sparse Signals with Smooth Support via Convex Programming and Block Sparsity |  http://arxiv.org/abs/1605.01813  | author:Sohil Shah, Tom Goldstein, Christoph Studer category:cs.CV published:2016-05-06 summary:Conventional algorithms for sparse signal recovery and sparse representationrely on $l_1$-norm regularized variational methods. However, when applied tothe reconstruction of $\textit{sparse images}$, i.e., images where only a fewpixels are non-zero, simple $l_1$-norm-based methods ignore potentialcorrelations in the support between adjacent pixels. In a number ofapplications, one is interested in images that are not only sparse, but alsohave a support with smooth (or contiguous) boundaries. Existing algorithms thattake into account such a support structure mostly rely on non-convex methodsand---as a consequence---do not scale well to high-dimensional problems and/ordo not converge to global optima. In this paper, we explore the use of newblock $l_1$-norm regularizers, which enforce image sparsity whilesimultaneously promoting smooth support structure. By exploiting the convexityof our regularizers, we develop new computationally-efficient recoveryalgorithms that guarantee global optimality. We demonstrate the efficacy of ourregularizers on a variety of imaging tasks including compressive imagerecovery, image restoration, and robust PCA.
arxiv-1605-02066 | Shape from Mixed Polarization |  http://arxiv.org/abs/1605.02066  | author:Vage Taamazyan, Achuta Kadambi, Ramesh Raskar category:cs.CV published:2016-05-06 summary:Shape from Polarization (SfP) estimates surface normals using photos capturedat different polarizer rotations. Fundamentally, the SfP model assumes thatlight is reflected either diffusely or specularly. However, this model is notvalid for many real-world surfaces exhibiting a mixture of diffuse and specularproperties. To address this challenge, previous methods have used a sequentialsolution: first, use an existing algorithm to separate the scene into diffuseand specular components, then apply the appropriate SfP model. In this paper,we propose a new method that jointly uses viewpoint and polarization data toholistically separate diffuse and specular components, recover refractiveindex, and ultimately recover 3D shape. By involving the physics ofpolarization in the separation process, we demonstrate competitive results witha benchmark method, while recovering additional information (e.g. refractiveindex).
arxiv-1605-02057 | Robust Bayesian Method for Simultaneous Block Sparse Signal Recovery with Applications to Face Recognition |  http://arxiv.org/abs/1605.02057  | author:Igor Fedorov, Ritwik Giri, Bhaskar D. Rao, Truong Q. Nguyen category:cs.CV published:2016-05-06 summary:In this paper, we present a novel Bayesian approach to recover simultaneouslyblock sparse signals in the presence of outliers. The key advantage of ourproposed method is the ability to handle non-stationary outliers, i.e. outlierswhich have time varying support. We validate our approach with empiricalresults showing the superiority of the proposed method over competingapproaches in synthetic data experiments as well as the multiple measurementface recognition problem.
arxiv-1605-02097 | ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning |  http://arxiv.org/abs/1605.02097  | author:Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Jaśkowski category:cs.LG cs.AI cs.CV published:2016-05-06 summary:The recent advances in deep neural networks have led to effectivevision-based reinforcement learning methods that have been employed to obtainhuman-level controllers in Atari 2600 games from pixel data. Atari 2600 games,however, do not resemble real-world tasks since they involve non-realistic 2Denvironments and the third-person perspective. Here, we propose a noveltest-bed platform for reinforcement learning research from raw visualinformation which employs the first-person perspective in a semi-realistic 3Dworld. The software, called ViZDoom, is based on the classical first-personshooter video game, Doom. It allows developing bots that play the game usingthe screen buffer. ViZDoom is lightweight, fast, and highly customizable via aconvenient mechanism of user scenarios. In the experimental part, we test theenvironment by trying to learn bots for two scenarios: a basic move-and-shoottask and a more complex maze-navigation problem. Using convolutional deepneural networks with Q-learning and experience replay, for both scenarios, wewere able to train competent bots, which exhibit human-like behaviors. Theresults confirm the utility of ViZDoom as an AI research platform and implythat visual reinforcement learning in 3D realistic first-person perspectiveenvironments is feasible.
arxiv-1605-01988 | LSTM with Working Memory |  http://arxiv.org/abs/1605.01988  | author:Andrew Pulver, Siwei Lyu category:cs.NE published:2016-05-06 summary:LSTM is arguably the most successful RNN architecture for many tasks thatinvolve sequential information. In the past few years there have been severalproposed improvements to LSTM. We propose an improvement to LSTM which allowscommunication between memory cells in different blocks and allows an LSTM layerto carry out internal computation within its memory.
arxiv-1605-01775 | Adversarial Diversity and Hard Positive Generation |  http://arxiv.org/abs/1605.01775  | author:Andras Rozsa, Ethan M. Rudd, Terrance E. Boult category:cs.CV published:2016-05-05 summary:State-of-the-art deep neural networks suffer from a fundamental problem -they misclassify adversarial examples formed by applying small perturbations toinputs. In this paper, we present a new psychometric perceptual adversarialsimilarity score (PASS) measure for quantifying adversarial images, introducethe notion of hard positive generation, and use a diverse set of adversarialperturbations - not just the closest ones - for data augmentation. We introducea novel hot/cold approach for adversarial example generation, which providesmultiple possible adversarial perturbations for every single image. Theperturbations generated by our novel approach often correspond to semanticallymeaningful image structures, and allow greater flexibility to scaleperturbation-amplitudes, which yields an increased diversity of adversarialimages. We present adversarial images on several network topologies anddatasets, including LeNet on the MNIST dataset, and GoogLeNet and ResidualNeton the ImageNet dataset. Finally, we demonstrate on LeNet and GoogLeNet thatfine-tuning with a diverse set of hard positives improves the robustness ofthese networks compared to training with prior methods of generatingadversarial images.
arxiv-1605-01478 | Modeling Rich Contexts for Sentiment Classification with LSTM |  http://arxiv.org/abs/1605.01478  | author:Minlie Huang, Yujie Cao, Chao Dong category:cs.CL cs.IR cs.SI published:2016-05-05 summary:Sentiment analysis on social media data such as tweets and weibo has become avery important and challenging task. Due to the intrinsic properties of suchdata, tweets are short, noisy, and of divergent topics, and sentimentclassification on these data requires to modeling various contexts such as theretweet/reply history of a tweet, and the social context about authors andrelationships. While few prior study has approached the issue of modelingcontexts in tweet, this paper proposes to use a hierarchical LSTM to model richcontexts in tweet, particularly long-range context. Experimental results showthat contexts can help us to perform sentiment classification remarkablybetter.
arxiv-1605-01600 | AVEC 2016 - Depression, Mood, and Emotion Recognition Workshop and Challenge |  http://arxiv.org/abs/1605.01600  | author:Michel Valstar, Jonathan Gratch, Bjorn Schuller, Fabien Ringeval, Denis Lalanne, Mercedes Torres Torres, Stefan Scherer, Guiota Stratou, Roddy Cowie, Maja Pantic category:cs.CV cs.HC cs.MM published:2016-05-05 summary:The Audio/Visual Emotion Challenge and Workshop (AVEC 2016) "Depression, Moodand Emotion" will be the sixth competition event aimed at comparison ofmultimedia processing and machine learning methods for automatic audio, visualand physiological depression and emotion analysis, with all participantscompeting under strictly the same conditions. The goal of the Challenge is toprovide a common benchmark test set for multi-modal information processing andto bring together the depression and emotion recognition communities, as wellas the audio, video and physiological processing communities, to compare therelative merits of the various approaches to depression and emotion recognitionunder well-defined and strictly comparable conditions and establish to whatextent fusion of the approaches is possible and beneficial. This paper presentsthe challenge guidelines, the common data used, and the performance of thebaseline system on the two tasks.
arxiv-1605-01779 | Clustering on the Edge: Learning Structure in Graphs |  http://arxiv.org/abs/1605.01779  | author:Matt Barnes, Artur Dubrawski category:stat.ML published:2016-05-05 summary:With the recent popularity of graphical clustering methods, there has been anincreased focus on the information between samples. We show how learningcluster structure using edge features naturally and simultaneously determinesthe most likely number of clusters and addresses data scale issues. Theseresults are particularly useful in instances where (a) there are a large numberof clusters and (b) we have some labeled edges. Applications in this domaininclude image segmentation, community discovery and entity resolution. Ourmodel is an extension of the planted partition model and our solution usesresults of correlation clustering, which achieves a partition O(log(n))-closeto the log-likelihood of the true clustering.
arxiv-1605-01576 | Patch-based Texture Synthesis for Image Inpainting |  http://arxiv.org/abs/1605.01576  | author:Tao Zhou, Brian Johnson, Rui Li category:cs.CV published:2016-05-05 summary:Image inpaiting is an important task in image processing and vision. In thispaper, we develop a general method for patch-based image inpainting bysynthesizing new textures from existing one. A novel framework is introduced tofind several optimal candidate patches and generate a new texture patch in theprocess. We form it as an optimization problem that identifies the potentialpatches for synthesis from an coarse-to-fine manner. We use the texturedescriptor as a clue in searching for matching patches from the known region.To ensure the structure faithful to the original image, a geometric constraintmetric is formally defined that is applied directly to the patch synthesisprocedure. We extensively conducted our experiments on a wide range of testingimages on various scenarios and contents by arbitrarily specifying the targetthe regions for inference followed by using existing evaluation metrics toverify its texture coherency and structural consistency. Our resultsdemonstrate the high accuracy and desirable output that can be potentially usedfor numerous applications: object removal, background subtraction, and imageretrieval.
arxiv-1605-01655 | Stance and Sentiment in Tweets |  http://arxiv.org/abs/1605.01655  | author:Saif M. Mohammad, Parinaz Sobhani, Svetlana Kiritchenko category:cs.CL published:2016-05-05 summary:We can often detect from a person's utterances whether he/she is in favor ofor against a given target entity -- their stance towards the target. However, aperson may express the same stance towards a target by using negative orpositive language. Here for the first time we present a dataset oftweet--target pairs annotated for both stance and sentiment. The targets may ormay not be referred to in the tweets, and they may or may not be the target ofopinion in the tweets. Partitions of this dataset were used as training andtest sets in a SemEval-2016 shared task competition. We propose a simple stancedetection system that outperforms submissions from all 19 teams thatparticipated in the shared task. Additionally, access to both stance andsentiment annotations allows us to explore several research questions. We showthat while knowing the sentiment expressed by a tweet is beneficial for stanceclassification, it alone is not sufficient. Finally, we use additionalunlabeled data through distant supervision techniques and word embeddings tofurther improve stance classification.
arxiv-1605-01679 | Learning Action Maps of Large Environments via First-Person Vision |  http://arxiv.org/abs/1605.01679  | author:Nicholas Rhinehart, Kris M. Kitani category:cs.CV published:2016-05-05 summary:When people observe and interact with physical spaces, they are able toassociate functionality to regions in the environment. Our goal is to automatedense functional understanding of large spaces by leveraging sparse activitydemonstrations recorded from an ego-centric viewpoint. The method we describeenables functionality estimation in large scenes where people have behaved, aswell as novel scenes where no behaviors are observed. Our method learns andpredicts "Action Maps", which encode the ability for a user to performactivities at various locations. With the usage of an egocentric camera toobserve human activities, our method scales with the size of the scene withoutthe need for mounting multiple static surveillance cameras and is well-suitedto the task of observing activities up-close. We demonstrate that by capturingappearance-based attributes of the environment and associating these attributeswith activity demonstrations, our proposed mathematical framework allows forthe prediction of Action Maps in new environments. Additionally, we offer apreliminary glance of the applicability of Action Maps by demonstrating aproof-of-concept application in which they are used in concert with activitydetections to perform localization.
arxiv-1605-01656 | A Tight Bound of Hard Thresholding |  http://arxiv.org/abs/1605.01656  | author:Jie Shen, Ping Li category:stat.ML cs.IT math.IT published:2016-05-05 summary:This paper is concerned with the hard thresholding technique which sets allbut the $k$ largest absolute elements to zero. We establish a tight bound thatquantitatively characterizes the deviation of the thresholded solution from agiven signal. Our theoretical result is universal in the sense that it holdsfor all choices of parameters, and the underlying analysis only depends onfundamental arguments in mathematical optimization. We discuss the implicationsfor the literature: Compressed Sensing. On account of the crucial estimate, we bridge theconnection between restricted isometry property (RIP) and the sparsityparameter of $k$ for a vast volume of hard thresholding based algorithms, whichrenders an improvement on the RIP condition especially when the true sparsityis unknown. This suggests that in essence, many more kinds of sensing matricesor fewer measurements are admissible for the data acquisition procedure. Machine Learning. In terms of large-scale machine learning, a significant yetchallenging problem is producing sparse solutions in online setting. In starkcontrast to prior works that attempted the $\ell_1$ relaxation for promotingsparsity, we present a novel algorithm which performs hard thresholding in eachiteration to ensure such parsimonious solutions. Equipped with the developedbound for hard thresholding, we prove global linear convergence for a number ofprevalent statistical models under mild assumptions, even though the problemturns out to be non-convex.
arxiv-1605-01713 | Not Just a Black Box: Learning Important Features Through Propagating Activation Differences |  http://arxiv.org/abs/1605.01713  | author:Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, Anshul Kundaje category:cs.LG cs.CV cs.NE published:2016-05-05 summary:The purported "black box" nature of neural networks is a barrier to adoptionin applications where interpretability is essential. Here we present DeepLIFT(Learning Important FeaTures), an efficient and effective method for computingimportance scores in a neural network. DeepLIFT compares the activation of eachneuron to its 'reference activation' and assigns contribution scores accordingto the difference. We apply DeepLIFT to models trained on natural images andgenomic data, and show significant advantages over gradient-based methods.
arxiv-1605-01677 | Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm |  http://arxiv.org/abs/1605.01677  | author:Junpei Komiyama, Junya Honda, Hiroshi Nakagawa category:stat.ML cs.LG published:2016-05-05 summary:We study the K-armed dueling bandit problem, a variation of the standardstochastic bandit problem where the feedback is limited to relative comparisonsof a pair of arms. The hardness of recommending Copeland winners, the arms thatbeat the greatest number of other arms, is characterized by deriving anasymptotic regret bound. We propose Copeland Winners Relative Minimum EmpiricalDivergence (CW-RMED) and derive an asymptotically optimal regret bound for it.However, it is not known whether the algorithm can be efficiently computed ornot. To address this issue, we devise an efficient version (ECW-RMED) andderive its asymptotic regret bound. Experimental comparisons of dueling banditalgorithms show that ECW-RMED significantly outperforms existing ones.
arxiv-1605-01514 | Fitness-based Adaptive Control of Parameters in Genetic Programming: Adaptive Value Setting of Mutation Rate and Flood Mechanisms |  http://arxiv.org/abs/1605.01514  | author:Michal Gregor, Juraj Spalek category:cs.NE published:2016-05-05 summary:This paper concerns applications of genetic algorithms and geneticprogramming to tasks for which it is difficult to find a representation thatdoes not map to a highly complex and discontinuous fitness landscape. In suchcases the standard algorithm is prone to getting trapped in local extremes. Thepaper proposes several adaptive mechanisms that are useful in preventing thesearch from getting trapped.
arxiv-1605-01661 | Parallels of human language in the behavior of bottlenose dolphins |  http://arxiv.org/abs/1605.01661  | author:R. Ferrer-i-Cancho, D. Lusseau, B. McCowan category:q-bio.NC cs.CL published:2016-05-05 summary:A short review of similarities between dolphins and humans with the help ofquantitative linguistics and information theory.
arxiv-1605-01559 | Sampling from strongly log-concave distributions with the Unadjusted Langevin Algorithm |  http://arxiv.org/abs/1605.01559  | author:Alain Durmus, Eric Moulines category:math.ST stat.ME stat.ML stat.TH published:2016-05-05 summary:We consider in this paper the problem of sampling a probability distribution$\pi$ having a density with respect to the Lebesgue measure on $\mathbb{R}^d$,known up to a normalisation factor $x \mapsto\mathrm{e}^{-U(x)}/\int_{\mathbb{R}^d} \mathrm{e}^{-U(y)} \mathrm{d} y$. Underthe assumption that $U$ is continuously differentiable, $\nabla U$ is globallyLipshitz and $U$ is strongly convex, we obtain non-asymptotic bounds for theconvergence to stationarity in Wasserstein distances of the sampling methodbased on the Euler discretization of the Langevin stochastic differentialequation for both constant and decreasing step sizes. The dependence on thedimension of the state space of the obtained bounds is studied to demonstratethe applicability of this method in the high dimensional setting. Theconvergence of an appropriately weighted empirical measure is also investigatedand bounds for the mean square error and exponential deviation inequality forLipschitz functions are reported. Some numerical results are presented toillustrate our findings.
arxiv-1605-01569 | Classification of Human Whole-Body Motion using Hidden Markov Models |  http://arxiv.org/abs/1605.01569  | author:Matthias Plappert category:cs.LG cs.CV published:2016-05-05 summary:Human motion plays an important role in many fields. Large databases existthat store and make available recordings of human motions. However, annotatingeach motion with multiple labels is a cumbersome and error-prone process. Thisbachelor's thesis presents different approaches to solve the multi-labelclassification problem using Hidden Markov Models (HMMs). First, differentfeatures that can be directly obtained from the raw data are introduced. Next,additional features are derived to improve classification performance. Thesefeatures are then used to perform the multi-label classification using twodifferent approaches. The first approach simply transforms the multi-labelproblem into a multi-class problem. The second, novel approach solves the sameproblem without the need to construct a transformation by predicting the labelsdirectly from the likelihood scores. The second approach scales linearly withthe number of labels whereas the first approach is subject to combinatorialexplosion. All aspects of the classification process are evaluated on a dataset that consists of 454 motions. System 1 achieves an accuracy of 98.02% andsystem 2 an accuracy of 93.39% on the test set.
arxiv-1605-01790 | Robust SAR STAP via Kronecker Decomposition |  http://arxiv.org/abs/1605.01790  | author:Kristjan Greenewald, Edmund Zelnio, Alfred Hero category:cs.CV published:2016-05-05 summary:This paper proposes a spatio-temporal decomposition for the detection ofmoving targets in multiantenna SAR. As a high resolution radar imagingmodality, SAR detects and localizes non-moving targets accurately, giving it anadvantage over lower resolution GMTI radars. Moving target detection is morechallenging due to target smearing and masking by clutter. Space-time adaptiveprocessing (STAP) is often used to remove the stationary clutter and enhancethe moving targets. In this work, it is shown that the performance of STAP canbe improved by modeling the clutter covariance as a space vs. time Kroneckerproduct with low rank factors. Based on this model, a low-rank Kroneckerproduct covariance estimation algorithm is proposed, and a novel separableclutter cancelation filter based on the Kronecker covariance estimate isintroduced. The proposed method provides orders of magnitude reduction in therequired number of training samples, as well as improved robustness tocorruption of the training data. Simulation results and experiments using theGotcha SAR GMTI challenge dataset are presented that confirm the advantages ofour approach relative to existing techniques.
arxiv-1605-01573 | Observational-Interventional Priors for Dose-Response Learning |  http://arxiv.org/abs/1605.01573  | author:Ricardo Silva category:stat.ML published:2016-05-05 summary:Controlled interventions provide the most direct source of information forlearning causal effects. In particular, a dose-response curve can be learned byvarying the treatment level and observing the corresponding outcomes. However,interventions can be expensive and time-consuming. Observational data, wherethe treatment is not controlled by a known mechanism, is sometimes available.Under some strong assumptions, observational data allows for the estimation ofdose-response curves. Estimating such curves nonparametrically is hard: samplesizes for controlled interventions may be small, while in the observationalcase a large number of measured confounders may need to be marginalized. Inthis paper, we introduce a hierarchical Gaussian process prior that constructsa distribution over the dose-response curve by learning from observationaldata, and reshapes the distribution with a nonparametric affine transformlearned from controlled interventions. This function composition from differentsources is shown to speed-up learning, which we demonstrate with a thoroughsensitivity analysis and an application to modeling the effect of therapy oncognitive skills of premature infants.
arxiv-1605-01623 | On the Convergence of A Family of Robust Losses for Stochastic Gradient Descent |  http://arxiv.org/abs/1605.01623  | author:Bo Han, Ivor W. Tsang, Ling Chen category:cs.LG published:2016-05-05 summary:The convergence of Stochastic Gradient Descent (SGD) using convex lossfunctions has been widely studied. However, vanilla SGD methods using convexlosses cannot perform well with noisy labels, which adversely affect the updateof the primal variable in SGD methods. Unfortunately, noisy labels areubiquitous in real world applications such as crowdsourcing. To handle noisylabels, in this paper, we present a family of robust losses for SGD methods. Byemploying our robust losses, SGD methods successfully reduce negative effectscaused by noisy labels on each update of the primal variable. We not onlyreveal that the convergence rate is O(1/T) for SGD methods using robust losses,but also provide the robustness analysis on two representative robust losses.Comprehensive experimental results on six real-world datasets show that SGDmethods using robust losses are obviously more robust than other baselinemethods in most situations with fast convergence.
arxiv-1605-01703 | A note on adjusting $R^2$ for using with cross-validation |  http://arxiv.org/abs/1605.01703  | author:Indre Zliobaite, Nikolaj Tatti category:cs.LG cs.AI stat.ML published:2016-05-05 summary:We show how to adjust the coefficient of determination ($R^2$) when used formeasuring predictive accuracy via leave-one-out cross-validation.
arxiv-1605-01710 | Plug-and-Play ADMM for Image Restoration: Fixed Point Convergence and Applications |  http://arxiv.org/abs/1605.01710  | author:Stanley H. Chan, Xiran Wang, Omar A. Elgendy category:cs.CV published:2016-05-05 summary:Alternating direction method of multiplier (ADMM) is a widely used algorithmfor solving constrained optimization problems in image restoration. Among manyuseful features, one critical feature of the ADMM algorithm is its modularstructure which allows one to plug in any off-the-shelf image denoisingalgorithm for a subproblem in the ADMM algorithm. Because of the plug-innature, this type of ADMM algorithms is coined the name "Plug-and-Play ADMM".Plug-and-Play ADMM has demonstrated promising empirical results in a number ofrecent papers. However, it is unclear under what conditions and for whatdenoising algorithms would it guarantee convergence. Also, it is unclear towhat extent would Plug-and-Play ADMM be compared to existing methods for commonGaussian and Poissonian image restoration problems. In this paper, we propose a Plug-and-Play ADMM algorithm with provable fixedpoint convergence. We show that for any denoising algorithm satisfying aboundedness criteria, called bounded denoisers, Plug-and-Play ADMM converges toa fixed point under a continuation scheme. We demonstrate applications ofPlug-and-Play ADMM on two image restoration problems including single imagesuper-resolution and quantized Poisson image recovery for single-photonimaging. We compare Plug-and-Play ADMM with state-of-the-art algorithms in eachproblem type, and demonstrate promising experimental results of the algorithm.
arxiv-1605-01635 | The IBM Speaker Recognition System: Recent Advances and Error Analysis |  http://arxiv.org/abs/1605.01635  | author:Seyed Omid Sadjadi, Jason Pelecanos, Sriram Ganapathy category:cs.CL cs.SD stat.ML published:2016-05-05 summary:We present the recent advances along with an error analysis of the IBMspeaker recognition system for conversational speech. Some of the keyadvancements that contribute to our system include: a nearest-neighbordiscriminant analysis (NDA) approach (as opposed to LDA) for intersessionvariability compensation in the i-vector space, the application of speaker andchannel-adapted features derived from an automatic speech recognition (ASR)system for speaker recognition, and the use of a DNN acoustic model with a verylarge number of output units (~10k senones) to compute the frame-level softalignments required in the i-vector estimation process. We evaluate thesetechniques on the NIST 2010 SRE extended core conditions (C1-C9), as well asthe 10sec-10sec condition. To our knowledge, results achieved by our systemrepresent the best performances published to date on these conditions. Forexample, on the extended tel-tel condition (C5) the system achieves an EER of0.59%. To garner further understanding of the remaining errors (on C5), weexamine the recordings associated with the low scoring target trials, wherevarious issues are identified for the problematic recordings/trials.Interestingly, it is observed that correcting the pathological recordings notonly improves the scores for the target trials but also for the nontargettrials.
arxiv-1605-01744 | Improving Automated Patent Claim Parsing: Dataset, System, and Experiments |  http://arxiv.org/abs/1605.01744  | author:Mengke Hu, David Cinciruk, John MacLaren Walsh category:cs.CL published:2016-05-05 summary:Off-the-shelf natural language processing software performs poorly whenparsing patent claims owing to their use of irregular language relative to thecorpora built from news articles and the web typically utilized to train thissoftware. Stopping short of the extensive and expensive process of accumulatinga large enough dataset to completely retrain parsers for patent claims, amethod of adapting existing natural language processing software towards patentclaims via forced part of speech tag correction is proposed. An AmazonMechanical Turk collection campaign organized to generate a public corpus totrain such an improved claim parsing system is discussed, identifying lessonslearned during the campaign that can be of use in future NLP dataset collectioncampaigns with AMT. Experiments utilizing this corpus and other patent claimsets measure the parsing performance improvement garnered via the claim parsingsystem. Finally, the utility of the improved claim parsing system within otherpatent processing applications is demonstrated via experiments showing improvedautomated patent subject classification when the new claim parsing system isutilized to generate the features.
arxiv-1605-01636 | Maximal Sparsity with Deep Networks? |  http://arxiv.org/abs/1605.01636  | author:Bo Xin, Yizhou Wang, Wen Gao, David Wipf category:cs.LG published:2016-05-05 summary:The iterations of many sparse estimation algorithms are comprised of a fixedlinear filter cascaded with a thresholding nonlinearity, which collectivelyresemble a typical neural network layer. Consequently, a lengthy sequence ofalgorithm iterations can be viewed as a deep network with shared, hand-craftedlayer weights. It is therefore quite natural to examine the degree to which alearned network model might act as a viable surrogate for traditional sparseestimation in domains where ample training data is available. While thepossibility of a reduced computational budget is readily apparent when aceiling is imposed on the number of layers, our work primarily focuses onestimation accuracy. In particular, it is well-known that when a signaldictionary has coherent columns, as quantified by a large RIP constant, thenmost tractable iterative algorithms are unable to find maximally sparserepresentations. In contrast, we demonstrate both theoretically and empiricallythe potential for a trained deep network to recover minimal $\ell_0$-normrepresentations in regimes where existing methods fail. The resulting system isdeployed on a practical photometric stereo estimation problem, where the goalis to remove sparse outliers that can disrupt the estimation of surface normalsfrom a 3D scene.
arxiv-1605-01652 | LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues |  http://arxiv.org/abs/1605.01652  | author:Phong Le, Marc Dymetman, Jean-Michel Renders category:cs.AI cs.CL published:2016-05-05 summary:We introduce an LSTM-based method for dynamically integrating severalword-prediction experts to obtain a conditional language model which can begood simultaneously at several subtasks. We illustrate this general approachwith an application to dialogue where we integrate a neural chat model, good atconversational aspects, with a neural question-answering model, good atretrieving precise information from a knowledge-base, and show how theintegration combines the strengths of the independent components. We hope thatthis focused contribution will attract attention on the benefits of using suchmixtures of experts in NLP.
arxiv-1605-01746 | Biobjective Performance Assessment with the COCO Platform |  http://arxiv.org/abs/1605.01746  | author:Dimo Brockhoff, Tea Tušar, Dejan Tušar, Tobias Wagner, Nikolaus Hansen, Anne Auger category:cs.NE published:2016-05-05 summary:This document details the rationales behind assessing the performance ofnumerical black-box optimizers on multi-objective problems within the COCOplatform and in particular on the biobjective test suite bbob-biobj. Theevaluation is based on a hypervolume of all non-dominated solutions in thearchive of candidate solutions and measures the runtime until the hypervolumevalue succeeds prescribed target values.
arxiv-1605-01749 | Rank Ordered Autoencoders |  http://arxiv.org/abs/1605.01749  | author:Paul Bertens category:cs.LG stat.ML published:2016-05-05 summary:A new method for the unsupervised learning of sparse representations usingautoencoders is proposed and implemented by ordering the output of the hiddenunits by their activation value and progressively reconstructing the input inthis order. This can be done efficiently in parallel with the use of cumulativesums and sorting only slightly increasing the computational costs. Minimizingthe difference of this progressive reconstruction with respect to the input canbe seen as minimizing the number of active output units required for thereconstruction of the input. The model thus learns to reconstruct optimallyusing the least number of active output units. This leads to high sparsitywithout the need for extra hyperparameters, the amount of sparsity is insteadimplicitly learned by minimizing this progressive reconstruction error. Resultsof the trained model are given for patches of the CIFAR10 dataset, showingrapid convergence of features and extremely sparse output activations whilemaintaining a minimal reconstruction error and showing extreme robustness tooverfitting. Additionally the reconstruction as function of number of activeunits is presented which shows the autoencoder learns a rank order code overthe input where the highest ranked units correspond to the highest decrease inreconstruction error.
arxiv-1605-01194 | IISCNLP at SemEval-2016 Task 2: Interpretable STS with ILP based Multiple Chunk Aligner |  http://arxiv.org/abs/1605.01194  | author:Lavanya Sita Tekumalla, Sharmistha category:cs.CL stat.ML published:2016-05-04 summary:Interpretable semantic textual similarity (iSTS) task adds a crucialexplanatory layer to pairwise sentence similarity. We address variouscomponents of this task: chunk level semantic alignment along with assignmentof similarity type and score for aligned chunks with a novel system presentedin this paper. We propose an algorithm, iMATCH, for the alignment of multiplenon-contiguous chunks based on Integer Linear Programming (ILP). Similaritytype and score assignment for pairs of chunks is done using a supervisedmulticlass classification technique based on Random Forrest Classifier. Resultsshow that our algorithm iMATCH has low execution time and outperforms mostother participating systems in terms of alignment score. Of the three datasets,we are top ranked for answer- students dataset in terms of overall score andhave top alignment score for headlines dataset in the gold chunks track.
arxiv-1605-01189 | A Generic Method for Automatic Ground Truth Generation of Camera-captured Documents |  http://arxiv.org/abs/1605.01189  | author:Sheraz Ahmed, Muhammad Imran Malik, Muhammad Zeshan Afzal, Koichi Kise, Masakazu Iwamura, Andreas Dengel, Marcus Liwicki category:cs.CV published:2016-05-04 summary:The contribution of this paper is fourfold. The first contribution is anovel, generic method for automatic ground truth generation of camera-captureddocument images (books, magazines, articles, invoices, etc.). It enables us tobuild large-scale (i.e., millions of images) labeled camera-captured/scanneddocuments datasets, without any human intervention. The method is generic,language independent and can be used for generation of labeled documentsdatasets (both scanned and cameracaptured) in any cursive and non-cursivelanguage, e.g., English, Russian, Arabic, Urdu, etc. To assess theeffectiveness of the presented method, two different datasets in English andRussian are generated using the presented method. Evaluation of samples fromthe two datasets shows that 99:98% of the images were correctly labeled. Thesecond contribution is a large dataset (called C3Wi) of camera-capturedcharacters and words images, comprising 1 million word images (10 millioncharacter images), captured in a real camera-based acquisition. This datasetcan be used for training as well as testing of character recognition systems oncamera-captured documents. The third contribution is a novel method for therecognition of cameracaptured document images. The proposed method is based onLong Short-Term Memory and outperforms the state-of-the-art methods for camerabased OCRs. As a fourth contribution, various benchmark tests are performed touncover the behavior of commercial (ABBYY), open source (Tesseract), and thepresented camera-based OCR using the presented C3Wi dataset. Evaluation resultsreveal that the existing OCRs, which already get very high accuracies onscanned documents, have limited performance on camera-captured document images;where ABBYY has an accuracy of 75%, Tesseract an accuracy of 50.22%, while thepresented character recognition system has an accuracy of 95.10%.
arxiv-1605-01141 | Texture Synthesis Through Convolutional Neural Networks and Spectrum Constraints |  http://arxiv.org/abs/1605.01141  | author:Gang Liu, Yann Gousseau, Gui-Song Xia category:cs.CV published:2016-05-04 summary:This paper presents a significant improvement for the synthesis of textureimages using convolutional neural networks (CNNs), making use of constraints onthe Fourier spectrum of the results. More precisely, the texture synthesis isregarded as a constrained optimization problem, with constraints conditioningboth the Fourier spectrum and statistical features learned by CNNs. In contrastwith existing methods, the presented method inherits from previous CNNapproaches the ability to depict local structures and fine scale details, andat the same time yields coherent large scale structures, even in the case ofquasi-periodic images. This is done at no extra computational cost. Synthesisexperiments on various images show a clear improvement compared to a recentstate-of-the art method relying on CNN constraints only.
arxiv-1605-01288 | From exp-concavity to variance control: High probability O(1/n) rates and high probability online-to-batch conversion |  http://arxiv.org/abs/1605.01288  | author:Nishant A. Mehta category:cs.LG published:2016-05-04 summary:We present an algorithm for the statistical learning setting with a boundedexp-concave loss in $d$ dimensions that obtains excess risk $O(d / n)$ withhigh probability: the dependence on the confidence parameter $\delta$ ispolylogarithmic in $1/\delta$. The core technique is to boost the confidence ofrecent $O(d / n)$ bounds, without sacrificing the rate, by leveraging aBernstein-type condition which holds due to exp-concavity. This Bernstein-typecondition implies that the variance of excess loss random variables arecontrolled in terms of their excess risk. Using this variance control, wefurther show that a regret bound for any online learner in this settingtranslates to a high probability excess risk bound for the correspondingonline-to-batch conversion of the online learner.
arxiv-1605-01326 | Compression and the origins of Zipf's law for word frequencies |  http://arxiv.org/abs/1605.01326  | author:Ramon Ferrer-i-Cancho category:cs.CL physics.soc-ph q-bio.NC published:2016-05-04 summary:Here we sketch a new derivation of Zipf's law for word frequencies based onoptimal coding. The structure of the derivation is reminiscent of Mandelbrot'srandom typing model but it has multiple advantages over random typing: (1) itdeparts from realistic cognitive pressures (2) it does not require fine tuningof parameters and (3) it sheds light on the origins of other statistical lawsof language and thus can lead to a compact theory of linguistic laws. Ourfindings suggest that the recurrence of Zipf's law in human languages couldoriginate from pressure for easy and fast communication.
arxiv-1605-01185 | Linear Bandit algorithms using the Bootstrap |  http://arxiv.org/abs/1605.01185  | author:Nandan Sudarsanam, Balaraman Ravindran category:stat.ML cs.LG published:2016-05-04 summary:This study presents two new algorithms for solving linear stochastic banditproblems. The proposed methods use an approach from non-parametric statisticscalled bootstrapping to create confidence bounds. This is achieved withoutmaking any assumptions about the distribution of noise in the underlyingsystem. We present the X-Random and X-Fixed bootstrap bandits which correspondto the two well-known approaches for conducting bootstraps on models, in theliterature. The proposed methods are compared to other popular solutions forlinear stochastic bandit problems, namely, OFUL, LinUCB and Thompson Sampling.The comparisons are carried out using a simulation study on a hierarchicalprobability meta-model, built from published data of experiments, which are runon real systems. The model representing the response surfaces is conceptualizedas a Bayesian Network which is presented with varying degrees of noise for thesimulations. One of the proposed methods, X-Random bootstrap, performs betterthan the baselines in-terms of cumulative regret across various degrees ofnoise and different number of trials. In certain settings the cumulative regretof this method is less than half of the best baseline. The X-Fixed bootstrapperforms comparably in most situations and particularly well when the number oftrials is low. The study concludes that these algorithms could be a preferredalternative for solving linear bandit problems, especially when thedistribution of the noise in the system is unknown.
arxiv-1605-01451 | Boltzmann meets Nash: Energy-efficient routing in optical networks under uncertainty |  http://arxiv.org/abs/1605.01451  | author:Panayotis Mertikopoulos, Aris L. Moustakas, Anna Tzanakaki category:cs.NI cs.GT cs.LG published:2016-05-04 summary:Motivated by the massive deployment of power-hungry data centers for serviceprovisioning, we examine the problem of routing in optical networks with theaim of minimizing traffic-driven power consumption. To tackle this issue,routing must take into account energy efficiency as well as capacityconsiderations; moreover, in rapidly-varying network environments, this must beaccomplished in a real-time, distributed manner that remains robust in thepresence of random disturbances and noise. In view of this, we derive a pricingscheme whose Nash equilibria coincide with the network's socially optimumstates, and we propose a distributed learning method based on the Boltzmanndistribution of statistical mechanics. Using tools from stochastic calculus, weshow that the resulting Boltzmann routing scheme exhibits remarkableconvergence properties under uncertainty: specifically, the long-term averageof the network's power consumption converges within $\varepsilon$ of itsminimum value in time which is at most $\tilde O(1/\varepsilon^2)$,irrespective of the fluctuations' magnitude; additionally, if the networkadmits a strict, non-mixing optimum state, the algorithm converges to it -again, no matter the noise level. Our analysis is supplemented by extensivenumerical simulations which show that Boltzmann routing can lead to asignificant decrease in power consumption over basic, shortest-path routingschemes in realistic network conditions.
arxiv-1605-01329 | Single Channel Speech Enhancement Using Outlier Detection |  http://arxiv.org/abs/1605.01329  | author:Eunjoon Cho, Bowon Lee, Ronald Schafer, Bernard Widrow category:cs.SD cs.LG published:2016-05-04 summary:Distortion of the underlying speech is a common problem for single-channelspeech enhancement algorithms, and hinders such methods from being used moreextensively. A dictionary based speech enhancement method that emphasizespreserving the underlying speech is proposed. Spectral patches of clean speechare sampled and clustered to train a dictionary. Given a noisy speech spectralpatch, the best matching dictionary entry is selected and used to estimate thenoise power at each time-frequency bin. The noise estimation step is formulatedas an outlier detection problem, where the noise at each bin is assumed presentonly if it is an outlier to the corresponding bin of the best matchingdictionary entry. This framework assigns higher priority in removing spectralelements that strongly deviate from a typical spoken unit stored in the traineddictionary. Even without the aid of a separate noise model, this method canachieve significant noise reduction for various non-stationary noises, whileeffectively preserving the underlying speech in more challenging noisyenvironments.
arxiv-1605-01177 | A metric on the space of finite sets of trajectories for evaluation of multi-target tracking algorithms |  http://arxiv.org/abs/1605.01177  | author:Abu Sajana Rahmathullah, Ángel F. García-Fernández, Lennart Svensson category:cs.CV cs.SY published:2016-05-04 summary:In this paper, we propose a metric on the space of finite sets oftrajectories for assessing multi-target tracking algorithms in a mathematicallysound way. The metric can be used, e.g., to compare estimates from algorithmswith the ground truth. It includes intuitive costs associated to localization,missed and false targets and track switches. The metric computation is based onmulti-dimensional assignments, which is an NP hard problem. Therefore, we alsopropose a lower bound for the metric, which is also a metric for sets oftrajectories and is computable in polynomial time using linear programming(LP). The LP metric can be implemented using alternating direction method ofmultipliers such that the complexity scales linearly with the length of thetrajectories.
arxiv-1605-01335 | Learning from the memory of Atari 2600 |  http://arxiv.org/abs/1605.01335  | author:Jakub Sygnowski, Henryk Michalewski category:cs.LG cs.AI published:2016-05-04 summary:We train a number of neural networks to play games Bowling, Breakout andSeaquest using information stored in the memory of a video game console Atari2600. We consider four models of neural networks which differ in size andarchitecture: two networks which use only information contained in the RAM andtwo mixed networks which use both information in the RAM and information fromthe screen. As the benchmark we used the convolutional model proposed in NIPSand received comparable results in all considered games. Quite surprisingly, inthe case of Seaquest we were able to train RAM-only agents which behave betterthan the benchmark screen-only agent. Mixing screen and RAM did not lead to animproved performance comparing to screen-only and RAM-only agents.
arxiv-1605-01379 | Leveraging Visual Question Answering for Image-Caption Ranking |  http://arxiv.org/abs/1605.01379  | author:Xiao Lin, Devi Parikh category:cs.CV published:2016-05-04 summary:Visual Question Answering (VQA) is the task of taking as input an image and afree-form natural language question about the image, and producing an accurateanswer. In this work we view VQA as a "feature extraction" module to extractimage and caption representations. We employ these representations for the taskof image-caption ranking. Each feature dimension captures (imagines) whether afact (question-answer pair) could plausibly be true for the image and caption.This allows the model to interpret images and captions from a wide variety ofperspectives. We propose score-level and representation-level fusion models toincorporate VQA knowledge in an existing state-of-the-art VQA-agnosticimage-caption ranking model. We find that incorporating and reasoning aboutconsistency between images and captions significantly improves performance.Concretely, our model improves state-of-the-art on caption retrieval by 7.1%and on image retrieval by 4.4% on the MSCOCO dataset.
arxiv-1605-01130 | Mining Discriminative Triplets of Patches for Fine-Grained Classification |  http://arxiv.org/abs/1605.01130  | author:Yaming Wang, Jonghyun Choi, Vlad I. Morariu, Larry S. Davis category:cs.CV published:2016-05-04 summary:Fine-grained classification involves distinguishing between similarsub-categories based on subtle differences in highly localized regions;therefore, accurate localization of discriminative regions remains a majorchallenge. We describe a patch-based framework to address this problem. Weintroduce triplets of patches with geometric constraints to improve theaccuracy of patch localization, and automatically mine discriminativegeometrically-constrained triplets for classification. The resulting approachonly requires object bounding boxes. Its effectiveness is demonstrated usingfour publicly available fine-grained datasets, on which it outperforms orachieves comparable performance to the state-of-the-art in classification.
arxiv-1605-01133 | Deep Motif: Visualizing Genomic Sequence Classifications |  http://arxiv.org/abs/1605.01133  | author:Jack Lanchantin, Ritambhara Singh, Zeming Lin, Yanjun Qi category:cs.LG published:2016-05-04 summary:This paper applies a deep convolutional/highway MLP framework to classifygenomic sequences on the transcription factor binding site task. To make themodel understandable, we propose an optimization driven strategy to extract"motifs", or symbolic patterns which visualize the positive class learned bythe network. We show that our system, Deep Motif (DeMo), extracts motifs thatare similar to, and in some cases outperform the current well known motifs. Inaddition, we find that a deeper model consisting of multiple convolutional andhighway layers can outperform a single convolutional and fully connected layerin the previous state-of-the-art.
arxiv-1605-01156 | Application of Deep Convolutional Neural Networks for Detecting Extreme Weather in Climate Datasets |  http://arxiv.org/abs/1605.01156  | author:Yunjie Liu, Evan Racah, Prabhat, Joaquin Correa, Amir Khosrowshahi, David Lavers, Kenneth Kunkel, Michael Wehner, William Collins category:cs.CV published:2016-05-04 summary:Detecting extreme events in large datasets is a major challenge in climatescience research. Current algorithms for extreme event detection are build uponhuman expertise in defining events based on subjective thresholds of relevantphysical variables. Often, multiple competing methods produce vastly differentresults on the same dataset. Accurate characterization of extreme events inclimate simulations and observational data archives is critical forunderstanding the trends and potential impacts of such events in a climatechange content. This study presents the first application of Deep Learningtechniques as alternative methodology for climate extreme events detection.Deep neural networks are able to learn high-level representations of a broadclass of patterns from labeled data. In this work, we developed deepConvolutional Neural Network (CNN) classification system and demonstrated theusefulness of Deep Learning technique for tackling climate pattern detectionproblems. Coupled with Bayesian based hyper-parameter optimization scheme, ourdeep CNN system achieves 89\%-99\% of accuracy in detecting extreme events(Tropical Cyclones, Atmospheric Rivers and Weather Fronts
arxiv-1605-01138 | A Comparative Evaluation of Approximate Probabilistic Simulation and Deep Neural Networks as Accounts of Human Physical Scene Understanding |  http://arxiv.org/abs/1605.01138  | author:Renqiao Zhang, Jiajun Wu, Chengkai Zhang, William T. Freeman, Joshua B. Tenenbaum category:cs.AI cs.CV q-bio.NC published:2016-05-04 summary:Humans demonstrate remarkable abilities to predict physical events in complexscenes. Two classes of models for physical scene understanding have recentlybeen proposed: "Intuitive Physics Engines", or IPEs, which posit that peoplemake predictions by running approximate probabilistic simulations in causalmental models similar in nature to video-game physics engines, and memory-basedmodels, which make judgments based on analogies to stored experiences ofpreviously encountered scenes and physical outcomes. Versions of the latterhave recently been instantiated in convolutional neural network (CNN)architectures. Here we report four experiments that, to our knowledge, are thefirst rigorous comparisons of simulation-based and CNN-based models, where bothapproaches are concretely instantiated in algorithms that can run on raw imageinputs and produce as outputs physical judgments such as whether a stack ofblocks will fall. Both approaches can achieve super-human accuracy levels andcan quantitatively predict human judgments to a similar degree, but only thesimulation-based models generalize to novel situations in ways that people do,and are qualitatively consistent with systematic perceptual illusions andjudgment asymmetries that people show.
arxiv-1605-01384 | Multi Level Monte Carlo methods for a class of ergodic stochastic differential equations |  http://arxiv.org/abs/1605.01384  | author:Lukasz Szpruch, Sebastian Vollmer, Konstantinos Zygalakis, Michael B. Giles category:math.NA stat.ME stat.ML published:2016-05-04 summary:We develop a framework that allows the use of the multi-level Monte Carlo(MLMC) methodology (Giles 2015) to calculate expectations with respect to theinvariant measures of ergodic SDEs. In that context, we study the (over-damped)Langevin equations with strongly convex potential. We show that, whenappropriate contracting couplings for the numerical integrators are available,one can obtain a time-uniform estimates of the MLMC variance in stark contrastto the majority of the results in the MLMC literature. As a consequence, onecan approximate expectations with respect to the invariant measure in anunbiased way without the need of a Metropolis- Hastings step. In addition, aroot mean square error of $\mathcal{O}(\epsilon)$ is achieved with$\mathcal{O}(\epsilon^{-2})$ complexity on par with Markov Chain Monte Carlo(MCMC) methods, which however can be computationally intensive when applied tolarge data sets. Finally, we present a multilevel version of the recentlyintroduced Stochastic Gradient Langevin (SGLD) method (Welling and Teh, 2011)built for large datasets applications. We show that this is the firststochastic gradient MCMC method with complexity $\mathcal{O}(\epsilon^{-2}\log{\epsilon}^{3})$, which is asymptotically an order $\epsilon$ lower than the $\mathcal{O}(\epsilon^{-3})$ complexity of all stochastic gradient MCMC methodsthat are currently available. Numerical experiments confirm our theoreticalfindings.
arxiv-1605-01368 | Unsupervised Total Variation Loss for Semi-supervised Deep Learning of Semantic Segmentation |  http://arxiv.org/abs/1605.01368  | author:Mehran Javanmardi, Mehdi Sajjadi, Ting Liu, Tolga Tasdizen category:cs.CV published:2016-05-04 summary:We introduce a novel unsupervised loss function for learning semanticsegmentation with deep convolutional neural nets (ConvNet) when densely labeledtraining images are not available. More specifically, the proposed lossfunction penalizes the L1-norm of the gradient of the label probability vectorimage , i.e. total variation, produced by the ConvNet. This can be seen as aregularization term that promotes piecewise smoothness of the label probabilityvector image produced by the ConvNet during learning. The unsupervised lossfunction is combined with a supervised loss in a semi-supervised setting tolearn ConvNets that can achieve high semantic segmentation accuracy even whenonly a tiny percentage of the pixels in the training images are labeled. Wedemonstrate significant improvements over the purely supervised setting in theWeizmann horse, Stanford background and Sift Flow datasets. Furthermore, weshow that using the proposed piecewise smoothness constraint in the learningphase significantly outperforms post-processing results from a purelysupervised approach with Markov Random Fields (MRF). Finally, we note that theframework we introduce is general and can be used to learn to label other typesof structures such as curvilinear structures by modifying the unsupervised lossfunction accordingly.
arxiv-1605-01224 | Learning Covariant Feature Detectors |  http://arxiv.org/abs/1605.01224  | author:Karel Lenc, Andrea Vedaldi category:cs.CV published:2016-05-04 summary:Local covariant feature detection, namely the problem of extracting viewpointinvariant features from images, has so far largely resisted the application ofmachine learning techniques. In this paper, we propose the first fully generalformulation for learning local covariant feature detectors. We propose to castdetection as a regression problem, enabling the use of powerful regressors suchas deep neural networks. We then derive a covariance constraint that can beused to automatically learn which visual structures provide stable anchors forlocal feature detection. We support these ideas theoretically, proposing anovel analysis of local features in term of geometric transformations, and weshow that all common and many uncommon detectors can be derived in thisframework. Finally, we present empirical results on a variety of detector typesand on standard feature benchmarks, showing the power and flexibility of theframework.
arxiv-1605-01436 | Sampling Requirements for Stable Autoregressive Estimation |  http://arxiv.org/abs/1605.01436  | author:Abbas Kazemipour, Sina Miran, Piya Pal, Behtash Babadi, Min Wu category:cs.IT cs.DM math.IT math.OC stat.ME stat.ML published:2016-05-04 summary:We consider the problem of estimating the parameters of a linearautoregressive model with sub-Gaussian innovations from a limited sequence ofconsecutive observations. Assuming that the parameters are compressible, weanalyze the performance of the $\ell_1$-regularized least squares as well as agreedy estimator of the parameters and characterize the sampling trade-offsrequired for stable recovery in the non-asymptotic regime. Our results extendthose of compressed sensing for linear models where the covariates are i.i.d.and independent of the observation history to autoregressive processes withhighly inter-dependent covariates. We also derive sufficient conditions on thesparsity level that guarantee the minimax optimality of the$\ell_1$-regularized least squares estimate. Applying these techniques tosimulated data as well as real-world datasets from crude oil prices and trafficspeed data confirm our predicted theoretical performance gains in terms ofestimation accuracy and model selection.
arxiv-1605-01278 | A Bayesian Approach to Policy Recognition and State Representation Learning |  http://arxiv.org/abs/1605.01278  | author:Adrian Šošić, Abdelhak M. Zoubir, Heinz Koeppl category:stat.ML cs.LG cs.SY math.DS math.PR published:2016-05-04 summary:Learning from demonstration (LfD) is the process of building behavioralmodels of a task from demonstrations provided by an expert. These models can beused e.g. for system control by generalizing the expert demonstrations topreviously unencountered situations. Most LfD methods, however, make strongassumptions about the expert behavior, e.g. they assume the existence of adeterministic optimal ground truth policy or require direct monitoring of theexpert's controls, which limits their practical use as part of a general systemidentification framework. In this work, we consider the LfD problem in a moregeneral setting where we allow for arbitrary stochastic expert policies,without reasoning about the quality of the demonstrations. In particular, wefocus on the problem of policy recognition, which is to extract a system'slatent control policy from observed system behavior. Following a Bayesianmethodology allows us to consider various sources of uncertainty about theexpert behavior, including the latent expert controls, to model the fullposterior distribution of expert controllers. Further, we show that the samemethodology can be applied in a nonparametric context to reason about thecomplexity of the state representation used by the expert and to learntask-appropriate partitionings of the system state space.
arxiv-1605-01242 | Hierarchical Modeling of Multidimensional Data in Regularly Decomposed Spaces: Applications in Image Analysis |  http://arxiv.org/abs/1605.01242  | author:Olivier Guye category:cs.CV published:2016-05-04 summary:This last document is showing the gradual introduction of hierarchicalmodeling techniques in image analysis. The first chapter is dealing with thefirst works carried out in the field of industrial applications of patternrecognition. The second chapter is focusing on the usage of these techniques insatellite imagery and on the development of a satellite data archiving systemin the aim of using it in digital geography. The third chapter is about facerecognition based on planar image analysis and about the recognition ofpartially hidden patterns. The present publication is ending with thedescription of a future system of self-descriptive coding of still or movingpictures in relation with the current video coding standards. As in theprevious documents, it will be found in annex algorithms targeted on imageanalysis according two complementary approaches: - boundary-based approach forthe industrial applications of artificial vision; - region-based approach forsatellite image analysis.
arxiv-1605-01397 | Skin Lesion Analysis toward Melanoma Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC) |  http://arxiv.org/abs/1605.01397  | author:David Gutman, Noel C. F. Codella, Emre Celebi, Brian Helba, Michael Marchetti, Nabin Mishra, Allan Halpern category:cs.CV published:2016-05-04 summary:In this article, we describe the design and implementation of a publiclyaccessible dermatology image analysis benchmark challenge. The goal of thechallenge is to sup- port research and development of algorithms for automateddiagnosis of melanoma, a lethal form of skin cancer, from dermoscopic images.The challenge was divided into sub-challenges for each task involved in imageanalysis, including lesion segmentation, dermoscopic feature detection within alesion, and classification of melanoma. Training data included 900 images. Aseparate test dataset of 379 images was provided to measure resultantperformance of systems developed with the training data. Ground truth for bothtraining and test sets was generated by a panel of dermoscopic experts. Intotal, there were 79 submissions from a group of 38 participants, making thisthe largest standardized and comparative study for melanoma diagnosis indermoscopic images to date. While the official challenge duration and rankingof participants has concluded, the datasets remain available for furtherresearch and development.
arxiv-1605-01369 | Accelerating Deep Learning with Shrinkage and Recall |  http://arxiv.org/abs/1605.01369  | author:Shuai Zheng, Abhinav Vishnu, Chris Ding category:cs.LG cs.CV cs.NE published:2016-05-04 summary:Deep Learning is a very powerful machine learning model. Deep Learning trainsa large number of parameters for multiple layers and is very slow when data isin large scale and the architecture size is large. Inspired from the shrinkingtechnique used in accelerating computation of Support Vector Machines (SVM)algorithm and screening technique used in LASSO, we propose a shrinking DeepLearning with recall (sDLr) approach to speed up deep learning computation. Weexperiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 datasets. Results show that the speedup using shrinking Deep Learning with recall(sDLr) can reach more than 2.0 while still giving competitive classificationperformance.
arxiv-1605-01643 | The embedding dimension of Laplacian eigenfunction maps |  http://arxiv.org/abs/1605.01643  | author:Jonathan Bates category:stat.ML cs.CV math.DG published:2016-05-04 summary:Any closed, connected Riemannian manifold $M$ can be smoothly embedded by itsLaplacian eigenfunction maps into $\mathbb{R}^m$ for some $m$. We call thesmallest such $m$ the maximal embedding dimension of $M$. We show that themaximal embedding dimension of $M$ is bounded from above by a constantdepending only on the dimension of $M$, a lower bound for injectivity radius, alower bound for Ricci curvature, and a volume bound. We interpret this resultfor the case of surfaces isometrically immersed in $\mathbb{R}^3$, showing thatthe maximal embedding dimension only depends on bounds for the Gaussiancurvature, mean curvature, and surface area. Furthermore, we consider therelevance of these results for shape registration.
arxiv-1605-01029 | Online Machine Learning Techniques for Predicting Operator Performance |  http://arxiv.org/abs/1605.01029  | author:Ahmet Anil Pala category:cs.LG published:2016-05-03 summary:This thesis explores a number of online machine learning algorithms. From atheoret- ical perspective, it assesses their employability for a particularfunction approximation problem where the analytical models fall short.Furthermore, it discusses the applica- tion of theoretically suitable learningalgorithms to the function approximation problem at hand through an efficientimplementation that exploits various computational and mathematical shortcuts.Finally, this thesis work evaluates the implemented learning algorithmsaccording to various evaluation criteria through rigorous testing.
arxiv-1605-00961 | Hierarchical Modeling of Multidimensional Data in Regularly Decomposed Spaces: Main Principles |  http://arxiv.org/abs/1605.00961  | author:Olivier Guye category:cs.CV published:2016-05-03 summary:The described works have been carried out in the framework of a mid-termstudy initiated by the Centre Electronique de l'Armement and led by ADERSA, aFrench company of research under contract. The aim was to study the techniquesof regular dividing of numerical data sets so as to provide tools for problemsolving enabling to model multidimensional numerical objects and to be used incomputer-aided design and manufacturing, in robotics, in image analysis andsynthesis, in pattern recognition, in decision making, in cartography andnumerical data base management. These tools are relying on the principle ofregular hierarchical decomposition and led to the implementation of amultidimensional generalization of quaternary and octernary trees: the trees oforder 2**k or 2**k-trees mapped in binary trees. This first tome, dedicated tothe hierarchical modeling of multidimensional numerical data, describes theprinciples used for building, transforming, analyzing and recognizing patternson which is relying the development of the associated algorithms. The whole sodeveloped algorithms are detailed in pseudo-code at the end of this document.The present publication especially describes: - a building method adapteddisordered and overcrowded data streams ; - its extension in inductive limits ;- the computation of the homographic transformation of a tree ; - the attributecalculus based on generalized moments and the provision of Eigen trees ; -perception procedures of objects without any covering in affine geometry ; -several supervised and unsupervised pattern recognition methods.
arxiv-1605-01101 | WEPSAM: Weakly Pre-Learnt Saliency Model |  http://arxiv.org/abs/1605.01101  | author:Avisek Lahiri, Sourya Roy, Anirban Santara, Pabitra Mitra, Prabir Kumar Biswas category:cs.CV published:2016-05-03 summary:Visual saliency detection tries to mimic human vision psychology whichconcentrates on sparse, important areas in natural image. Saliency predictionresearch has been traditionally based on low level features such as contrast,edge, etc. Recent thrust in saliency prediction research is to learn high levelsemantics using ground truth eye fixation datasets. In this paper we present,WEPSAM : Weakly Pre-Learnt Saliency Model as a pioneering effort of usingdomain specific pre-learing on ImageNet for saliency prediction using a lightweight CNN architecture. The paper proposes a two step hierarchical learning,in which the first step is to develop a framework for weakly pre-training on alarge scale dataset such as ImageNet which is void of human eye fixation maps.The second step refines the pre-trained model on a limited set of ground truthfixations. Analysis of loss on iSUN and SALICON datasets reveal thatpre-trained network converges much faster compared to randomly initializednetwork. WEPSAM also outperforms some recent state-of-the-art saliencyprediction models on the challenging MIT300 dataset.
arxiv-1605-00779 | Temporal Clustering of Time Series via Threshold Autoregressive Models: Application to Commodity Prices |  http://arxiv.org/abs/1605.00779  | author:Sipan Aslan, Ceylan Yozgatligil, Cem Iyigun category:stat.ML stat.AP stat.ME published:2016-05-03 summary:This study aimed to find temporal clusters for several commodity prices usingthe threshold non-linear autoregressive model. It is expected that the processof determining the commodity groups that are time-dependent will advance thecurrent knowledge about the dynamics of co-moving and coherent prices, and canserve as a basis for multivariate time series analyses. The clustering ofcommodity prices was examined using the proposed clustering approach based ontime series models to incorporate the time varying properties of price seriesinto the clustering scheme. Accordingly, the primary aim in this study wasgrouping time series according to the similarity between their Data GeneratingMechanisms (DGMs) rather than comparing pattern similarities in the time seriestraces. The approximation to the DGM of each series was accomplished usingthreshold autoregressive models, which are recognized for their ability torepresent nonlinear features in time series, such as abrupt changes,time-irreversibility and regime-shifting behavior. Through the use of theproposed approach, one can determine and monitor the set of co-moving timeseries variables across the time dimension. Furthermore, generating a timevarying commodity price index and sub-indexes can become possible.Consequently, we conducted a simulation study to assess the effectiveness ofthe proposed clustering approach and the results are presented for both thesimulated and real data sets.
arxiv-1605-00763 | Automatic Identification of Retinal Arteries and Veins in Fundus Images using Local Binary Patterns |  http://arxiv.org/abs/1605.00763  | author:Nima Hatami, Michael Goldbaum category:cs.CV published:2016-05-03 summary:Artery and vein (AV) classification of retinal images is a key to necessarytasks, such as automated measurement of arteriolar-to-venular diameter ratio(AVR). This paper comprehensively reviews the state-of-the art in AVclassification methods. To improve on previous methods, a new Local Bi- naryPattern-based method (LBP) is proposed. Beside its simplicity, LBP is robustagainst low contrast and low quality fundus images; and it helps the process byincluding additional AV texture and shape information. Experimental resultscompare the performance of the new method with the state-of-the art; and alsomethods with different feature extraction and classification schemas.
arxiv-1605-00972 | Phase 2: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Machine Learning Detection Algorithms |  http://arxiv.org/abs/1605.00972  | author:Peter J. Dugan, Christopher W. Clark, Yann André LeCun, Sofie M. Van Parijs category:cs.CV published:2016-05-03 summary:Overarching goals for this work aim to advance the state of the art fordetection, classification and localization (DCL) in the field of bioacoustics.This goal is primarily achieved by building a generic framework fordetection-classification (DC) using a fast, efficient and scalablearchitecture, demonstrating the capabilities of this system using on a varietyof low-frequency mid-frequency cetacean sounds. Two primary goals are todevelop transferable technologies for detection and classification in, one: thearea of advanced algorithms, such as deep learning and other methods; and two:advanced systems, capable of real-time and archival processing. For each keyarea, we will focus on producing publications from this work and providingtools and software to the community where/when possible. Currently massiveamounts of acoustic data are being collected by various institutions,corporations and national defense agencies. The long-term goal is to providetechnical capability to analyze the data using automatic algorithms for (DC)based on machine intelligence. The goal of the automation is to provideeffective and efficient mechanisms by which to process large acoustic datasetsfor understanding the bioacoustic behaviors of marine mammals. This capabilitywill provide insights into the potential ecological impacts and influences ofanthropogenic ocean sounds. This work focuses on building technologies using amaturity model based on DARPA 6.1 and 6.2 processes, for basic and appliedresearch, respectively.
arxiv-1605-00758 | Efficient Distributed Estimation of Inverse Covariance Matrices |  http://arxiv.org/abs/1605.00758  | author:Jesús Arroyo, Elizabeth Hou category:stat.ME stat.ML published:2016-05-03 summary:In distributed systems, communication is a major concern due to issues suchas its vulnerability or efficiency. In this paper, we are interested inestimating sparse inverse covariance matrices when samples are distributed intodifferent machines. We address communication efficiency by proposing a methodwhere, in a single round of communication, each machine transfers a smallsubset of the entries of the inverse covariance matrix. We show that, with thisefficient distributed method, the error rates can be comparable with estimationin a non-distributed setting, and correct model selection is still possible.Practical performance is shown through simulations.
arxiv-1605-00743 | Learning Attributes Equals Multi-Source Domain Generalization |  http://arxiv.org/abs/1605.00743  | author:Chuang Gan, Tianbao Yang, Boqing Gong category:cs.CV published:2016-05-03 summary:Attributes possess appealing properties and benefit many computer visionproblems, such as object recognition, learning with humans in the loop, andimage retrieval. Whereas the existing work mainly pursues utilizing attributesfor various computer vision problems, we contend that the most basicproblem---how to accurately and robustly detect attributes from images---hasbeen left under explored. Especially, the existing work rarely explicitlytackles the need that attribute detectors should generalize well acrossdifferent categories, including those previously unseen. Noting that this isanalogous to the objective of multi-source domain generalization, if we treateach category as a domain, we provide a novel perspective to attributedetection and propose to gear the techniques in multi-source domaingeneralization for the purpose of learning cross-category generalizableattribute detectors. We validate our understanding and approach with extensiveexperiments on four challenging datasets and three different problems.
arxiv-1605-00740 | VLSI Extreme Learning Machine: A Design Space Exploration |  http://arxiv.org/abs/1605.00740  | author:Enyi Yao, Arindam Basu category:cs.LG cs.ET published:2016-05-03 summary:In this paper, we describe a compact low-power, high performance hardwareimplementation of the extreme learning machine (ELM) for machine learningapplications. Mismatch in current mirrors are used to perform the vector-matrixmultiplication that forms the first stage of this classifier and is the mostcomputationally intensive. Both regression and classification (on UCI datasets) are demonstrated and a design space trade-off between speed, power andaccuracy is explored. Our results indicate that for a wide set of problems,$\sigma V_T$ in the range of $15-25$mV gives optimal results. An input weightmatrix rotation method to extend the input dimension and hidden layer sizebeyond the physical limits imposed by the chip is also described. This allowsus to overcome a major limit imposed on most hardware machine learners. Thechip is implemented in a $0.35 \mu$m CMOS process and occupies a die area ofaround 5 mm $\times$ 5 mm. Operating from a $1$ V power supply, it achieves anenergy efficiency of $0.47$ pJ/MAC at a classification rate of $31.6$ kHz.
arxiv-1605-00732 | A propagation matting method based on the Local Sampling and KNN Classification with adaptive feature space |  http://arxiv.org/abs/1605.00732  | author:Xiao Chen, Fazhi He category:cs.CV published:2016-05-03 summary:Closed Form is a propagation based matting algorithm, functioning well onimages with good propagation . The deficiency of the Closed Form method is thatfor complex areas with poor image propagation , such as hole areas or areas oflong and narrow structures. The right results are usually hard to get. On theseareas, if certain flags are provided, it can improve the effects of matting. Inthis paper, we design a matting algorithm by local sampling and the KNNclassifier propagation based matting algorithm. First of all, build thecorresponding features space according to the different components of imagecolors to reduce the influence of overlapping between the foreground andbackground, and to improve the classification accuracy of KNN classifier.Second, adaptively use local sampling or using local KNN classifier forprocessing based on the pros and cons of the sample performance of unknownimage areas. Finally, based on different treatment methods for the unknownareas, we will use different weight for augmenting constraints to make thetreatment more effective. In this paper, by combining qualitative observationand quantitative analysis, we will make evaluation of the experimental resultsthrough online standard set of evaluation tests. It shows that on images withgood propagation , this method is as effective as the Closed Form method, whileon images in complex regions, it can perform even better than Closed Form.
arxiv-1605-00716 | Radio Transformer Networks: Attention Models for Learning to Synchronize in Wireless Systems |  http://arxiv.org/abs/1605.00716  | author:Timothy J O'Shea, Latha Pemula, Dhruv Batra, T. Charles Clancy category:cs.LG cs.NI cs.SY published:2016-05-03 summary:We introduce learned attention models into the radio machine learning domainfor the task of modulation recognition by leveraging spatial transformernetworks and introducing new radio domain appropriate transformations. Thisattention model allows the network to learn a localization network capable ofsynchronizing and normalizing a radio signal blindly with zero knowledge of thesignals structure based on optimization of the network for classificationaccuracy, sparse representation, and regularization. Using this architecture weare able to outperform our prior results in accuracy vs signal to noise ratioagainst an identical system without attention, however we believe such anattention model has implication far beyond the task of modulation recognition.
arxiv-1605-01116 | An evaluation of randomized machine learning methods for redundant data: Predicting short and medium-term suicide risk from administrative records and risk assessments |  http://arxiv.org/abs/1605.01116  | author:Thuong Nguyen, Truyen Tran, Shivapratap Gopakumar, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG published:2016-05-03 summary:Accurate prediction of suicide risk in mental health patients remains an openproblem. Existing methods including clinician judgments have acceptablesensitivity, but yield many false positives. Exploiting administrative data hasa great potential, but the data has high dimensionality and redundancies in therecording processes. We investigate the efficacy of three most effectiverandomized machine learning techniques random forests, gradient boostingmachines, and deep neural nets with dropout in predicting suicide risk. Using acohort of mental health patients from a regional Australian hospital, wecompare the predictive performance with popular traditional approachesclinician judgments based on a checklist, sparse logistic regression anddecision trees. The randomized methods demonstrated robustness against dataredundancies and superior predictive performance on AUC and F-measure.
arxiv-1605-01014 | Deep Deformation Network for Object Landmark Localization |  http://arxiv.org/abs/1605.01014  | author:Xiang Yu, Feng Zhou, Manmohan Chandraker category:cs.CV published:2016-05-03 summary:We propose a novel cascaded framework, called deep deformation network (DDN),for localizing landmarks in non-rigid objects. The hallmarks of DDN are itsincorporation of geometric constraints within a convolutional neural network(CNN) framework, ease and efficiency of training, as well as generality ofapplication. A novel shape basis network (SBN) forms the first stage of thecascade, whereby landmarks are initialized by combining the benefits of CNNfeatures and a learned shape basis to reduce the complexity of the highlynonlinear pose manifold. In the second stage, a point transformer network (PTN)estimates local deformations parameterized as thin-plate spline transformationsfor a finer refinement. Our framework does not incorporate either handcraftedfeatures or part connectivity, which enables an end-to-end shape predictionpipeline during both training and testing. In contrast to prior cascadednetworks for landmark localization that learn a mapping from feature space tolandmark locations, we demonstrate that the regularization induced throughgeometric priors in the DDN makes it easier to train, yet produces superiorresults. The efficacy and generality of the architecture is demonstratedthrough state-of-the-art performances on several benchmarks for multiple taskssuch as facial landmark localization, human body pose estimation and bird partlocalization.
arxiv-1605-01115 | MARLow: A Joint Multiplanar Autoregressive and Low-Rank Approach for Image Completion |  http://arxiv.org/abs/1605.01115  | author:Madingg Li, Jiaying Liu, Zhiwei Xiong, Xiaoyan Sun, Zongming Guo category:cs.CV cs.MM published:2016-05-03 summary:In this paper, we propose a novel multiplanar autoregressive (AR) model toexploit the correlation in cross-dimensional planes of a similar patch groupcollected in an image, which has long been neglected by previous AR models. Onthat basis, we then present a joint multiplanar AR and low-rank based approach(MARLow) for image completion from random sampling, which exploits the nonlocalself-similarity within natural images more effectively. Specifically, themultiplanar AR model constraints the local stationarity in differentcross-sections of the patch group, while the low-rank minimization captures theintrinsic coherence of nonlocal patches. The proposed approach can be readilyextended to multichannel images (e.g. color images), by simultaneouslyconsidering the correlation in different channels. Experimental resultsdemonstrate that the proposed approach significantly outperformsstate-of-the-art methods, even if the pixel missing rate is as high as 90%.
arxiv-1605-00775 | Spatially Aware Dictionary Learning and Coding for Fossil Pollen Identification |  http://arxiv.org/abs/1605.00775  | author:Shu Kong, Surangi Punyasena, Charless Fowlkes category:cs.CV q-bio.PE q-bio.QM published:2016-05-03 summary:We propose a robust approach for performing automatic species-levelrecognition of fossil pollen grains in microscopy images that exploits bothglobal shape and local texture characteristics in a patch-based matchingmethodology. We introduce a novel criteria for selecting meaningful anddiscriminative exemplar patches. We optimize this function during trainingusing a greedy submodular function optimization framework that gives anear-optimal solution with bounded approximation error. We use these selectedexemplars as a dictionary basis and propose a spatially-aware sparse codingmethod to match testing images for identification while maintaining globalshape correspondence. To accelerate the coding process for fast matching, weintroduce a relaxed form that uses spatially-aware soft-thresholding duringcoding. Finally, we carry out an experimental study that demonstrates theeffectiveness and efficiency of our exemplar selection and classificationmechanisms, achieving $86.13\%$ accuracy on a difficult fine-grained speciesclassification task distinguishing three types of fossil spruce pollen.
arxiv-1605-01107 | Decentralized Dynamic Discriminative Dictionary Learning |  http://arxiv.org/abs/1605.01107  | author:Alec Koppel, Garrett Warnell, Ethan Stump, Alejandro Ribeiro category:stat.ML cs.LG published:2016-05-03 summary:We consider discriminative dictionary learning in a distributed onlinesetting, where a network of agents aims to learn a common set of dictionaryelements of a feature space and model parameters while sequentially receivingobservations. We formulate this problem as a distributed stochastic programwith a non-convex objective and present a block variant of the Arrow-Hurwiczsaddle point algorithm to solve it. Using Lagrange multipliers to penalize thediscrepancy between them, only neighboring nodes exchange model information. Weshow that decisions made with this saddle point algorithm asymptoticallyachieve a first-order stationarity condition on average.
arxiv-1605-00959 | Personalized Risk Scoring for Critical Care Patients using Mixtures of Gaussian Process Experts |  http://arxiv.org/abs/1605.00959  | author:Ahmed M. Alaa, Jinsung Yoon, Scott Hu, Mihaela van der Schaar category:cs.LG stat.ML published:2016-05-03 summary:We develop a personalized real time risk scoring algorithm that providestimely and granular assessments for the clinical acuity of ward patients basedon their (temporal) lab tests and vital signs. Heterogeneity of the patientspopulation is captured via a hierarchical latent class model. The proposedalgorithm aims to discover the number of latent classes in the patientspopulation, and train a mixture of Gaussian Process (GP) experts, where eachexpert models the physiological data streams associated with a specific class.Self-taught transfer learning is used to transfer the knowledge of latentclasses learned from the domain of clinically stable patients to the domain ofclinically deteriorating patients. For new patients, the posterior beliefs ofall GP experts about the patient's clinical status given her physiological datastream are computed, and a personalized risk score is evaluated as a weightedaverage of those beliefs, where the weights are learned from the patient'shospital admission information. Experiments on a heterogeneous cohort of 6,313patients admitted to Ronald Regan UCLA medical center show that our risk scoreoutperforms the currently deployed risk scores, such as MEWS and Rothmanscores.
arxiv-1605-01046 | Logarithmic proximity measures outperform plain ones in graph nodes clustering |  http://arxiv.org/abs/1605.01046  | author:Vladimir Ivashkin, Pavel Chebotarev category:cs.LG cs.DM published:2016-05-03 summary:We consider a number of graph kernels and proximity measures: commute timekernel, regularized Laplacian kernel, heat kernel, communicability, etc., andthe corresponding distances as applied to clustering nodes in random graphs.The model of generating graphs involves edge probabilities for the pairs ofnodes that belong to the same class or different classes. It turns out that inmost cases, logarithmic measures (i.e., measures resulting after takinglogarithm of the proximities) perform much better while distinguishing classesthan the "plain" measures. A direct comparison of inter-class and intra-classdistances confirms this conclusion. A possible explanation of this fact is thatmost kernels have a multiplicative nature, while the nature of distances usedin cluster algorithms is an additive one (cf. the triangle inequality). Thelogarithmic transformation is just a tool to transform one nature to another.Moreover, some distances corresponding to the logarithmic measures possess ameaningful cutpoint additivity property. In our experiments, the leader is theso-called logarithmic communicability measure, which distinctly outperforms theother measures under study.
arxiv-1605-00942 | TheanoLM - An Extensible Toolkit for Neural Network Language Modeling |  http://arxiv.org/abs/1605.00942  | author:Seppo Enarvi, Mikko Kurimo category:cs.CL cs.NE published:2016-05-03 summary:We present a new tool for training neural network language models (NNLMs),scoring sentences, and generating text. The tool has been written using Pythonlibrary Theano, which allows researcher to easily extend it and tune any aspectof the training process. Regardless of the flexibility, Theano is able togenerate extremely fast native code that can utilize a GPU or multiple CPUcores in order to parallelize the heavy numerical computations. The tool hasbeen evaluated in difficult Finnish and English conversational speechrecognition tasks, and significant improvement was obtained over our bestback-off n-gram models. The results that we obtained in the Finnish task werecompared to those from existing RNNLM and RWTHLM toolkits, and found to be asgood or better, while training times were an order of magnitude shorter.
arxiv-1605-00788 | Online Learning of Commission Avoidant Portfolio Ensembles |  http://arxiv.org/abs/1605.00788  | author:Guy Uziel, Ran El-Yaniv category:cs.AI cs.LG published:2016-05-03 summary:We present a novel online ensemble learning strategy for portfolio selection.The new strategy controls and exploits any set of commission-obliviousportfolio selection algorithms. The strategy handles transaction costs using anovel commission avoidance mechanism. We prove a logarithmic regret bound forour strategy with respect to optimal mixtures of the base algorithms. Numericalexamples validate the viability of our method and show significant improvementover the state-of-the-art.
arxiv-1605-00751 | Learning from Binary Labels with Instance-Dependent Corruption |  http://arxiv.org/abs/1605.00751  | author:Aditya Krishna Menon, Brendan van Rooyen, Nagarajan Natarajan category:cs.LG published:2016-05-03 summary:Suppose we have a sample of instances paired with binary labels corrupted byarbitrary instance- and label-dependent noise. With sufficiently many suchsamples, can we optimally classify and rank instances with respect to thenoise-free distribution? We provide a theoretical analysis of this question,with three main contributions. First, we prove that for instance-dependentnoise, any algorithm that is consistent for classification on the noisydistribution is also consistent on the clean distribution. Second, we provethat for a broad class of instance- and label-dependent noise, a similarconsistency result holds for the area under the ROC curve. Third, for thelatter noise model, when the noise-free class-probability function belongs tothe generalised linear model family, we show that the Isotron can efficientlyand provably learn from the corrupted sample.
arxiv-1605-00855 | Improving Image Captioning by Concept-based Sentence Reranking |  http://arxiv.org/abs/1605.00855  | author:Xirong Li, Qin Jin category:cs.CV cs.CL published:2016-05-03 summary:This paper describes our winning entry in the ImageCLEF 2015 image sentencegeneration task. We improve Google's CNN-LSTM model by introducingconcept-based sentence reranking, a data-driven approach which exploits thelarge amounts of concept-level annotations on Flickr. Different from previoususage of concept detection that is tailored to specific image captioningmodels, the propose approach reranks predicted sentences in terms of theirmatches with detected concepts, essentially treating the underlying model as ablack box. This property makes the approach applicable to a number of existingsolutions. We also experiment with fine tuning on the deep language model,which improves the performance further. Scoring METEOR of 0.1875 on theImageCLEF 2015 test set, our system outperforms the runner-up (METEOR of0.1687) with a clear margin.
arxiv-1605-01042 | Hierarchical Bayesian Noise Inference for Robust Real-time Probabilistic Object Classification |  http://arxiv.org/abs/1605.01042  | author:Shayegan Omidshafiei, Brett T. Lopez, Jonathan P. How, John Vian category:cs.CV published:2016-05-03 summary:Robust environment perception is essential for decision-making on robotsoperating in complex domains. Principled treatment of uncertainty sources in arobot's observation model is necessary for accurate mapping and objectdetection. This is important not only for low-level observations (e.g.,accelerometer data), but for high-level observations such as semantic objectlabels as well. This paper presents an approach for filtering sequences ofobject classification probabilities using online modeling of the noisecharacteristics of the classifier outputs. A hierarchical Bayesian approach isused to model per-class noise distributions, while simultaneously allowingsharing of high-level noise characteristics between classes. The proposedfiltering scheme, called Hierarchical Bayesian Noise Inference (HBNI), is shownto outperform classification accuracy of existing methods. The paper alsopresents real-time filtered classification hardware experiments running fullyonboard a moving quadrotor, where the proposed approach is demonstrated to workin a challenging domain where noise-agnostic filtering fails.
arxiv-1605-00894 | Recurrent Convolutional Neural Network Regression for Continuous Pain Intensity Estimation in Video |  http://arxiv.org/abs/1605.00894  | author:Jing Zhou, Xiaopeng Hong, Fei Su, Guoying Zhao category:cs.CV published:2016-05-03 summary:Automatic pain intensity estimation possesses a significant position inhealthcare and medical field. Traditional static methods prefer to extractfeatures from frames separately in a video, which would result in unstablechanges and peaks among adjacent frames. To overcome this problem, we propose areal-time regression framework based on the recurrent convolutional neuralnetwork for automatic frame-level pain intensity estimation. Given vectorsequences of AAM-warped facial images, we used a sliding-window strategy toobtain fixed-length input samples for the recurrent network. We then carefullydesign the architecture of the recurrent network to output continuous-valuedpain intensity. The proposed end-to-end pain intensity regression framework canpredict the pain intensity of each frame by considering a sufficiently largehistorical frames while limiting the scale of the parameters within the model.Our method achieves promising results regarding both accuracy and running speedon the published UNBC-McMaster Shoulder Pain Expression Archive Database.
arxiv-1605-00937 | Dictionary Learning for Massive Matrix Factorization |  http://arxiv.org/abs/1605.00937  | author:Arthur Mensch, Julien Mairal, Bertrand Thirion, Gaël Varoquaux category:stat.ML cs.LG published:2016-05-03 summary:Sparse matrix factorization is a popular tool to obtain interpretable datadecompositions, which are also effective to perform data completion ordenoising. Its applicability to large datasets has been addressed with onlineand randomized methods, that reduce the complexity in one of the matrixdimension, but not in both of them. In this paper, we tackle very largematrices in both dimensions. We propose a new factoriza-tion method that scalesgracefully to terabyte-scale datasets, that could not be processed by previousalgorithms in a reasonable amount of time. We demonstrate the efficiency of ourapproach on massive functional Magnetic Resonance Imaging (fMRI) data, and onmatrix completion problems for recommender systems, where we obtain significantspeed-ups compared to state-of-the art coordinate descent methods.
arxiv-1605-00561 | Parallel Wavelet Schemes for Images |  http://arxiv.org/abs/1605.00561  | author:David Barina, Michal Kula, Pavel Zemcik category:cs.CV published:2016-05-02 summary:In this paper, we introduce several new schemes for calculation of discretewavelet transforms of images. These schemes reduce the number of steps and, asa consequence, allow to reduce the number of synchronizations on parallelarchitectures. As an additional useful property, the proposed schemes canreduce also the number of arithmetic operations. The schemes are primarilydemonstrated on CDF 5/3 and CDF 9/7 wavelets employed in JPEG 2000 imagecompression standard. However, the presented method is general and it can beapplied on any wavelet transform. As a result, our scheme requires only twomemory barriers for 2-D CDF 5/3 transform compared to four barriers in theoriginal separable form or three barriers in the non-separable scheme recentlypublished. Our reasoning is supported by exhaustive experiments on high-endgraphics cards.
arxiv-1605-00329 | Some Insights into the Geometry and Training of Neural Networks |  http://arxiv.org/abs/1605.00329  | author:Ewout van den Berg category:cs.LG published:2016-05-02 summary:Neural networks have been successfully used for classification tasks in arapidly growing number of practical applications. Despite their popularity andwidespread use, there are still many aspects of training and classificationthat are not well understood. In this paper we aim to provide some new insightsinto training and classification by analyzing neural networks from afeature-space perspective. We review and explain the formation of decisionregions and study some of their combinatorial aspects. We place a particularemphasis on the connections between the neural network weight and bias termsand properties of decision boundaries and other regions that exhibit varyinglevels of classification confidence. We show how the error backpropagates inthese regions and emphasize the important role they have in the formation ofgradients. These findings expose the connections between scaling of the weightparameters and the density of the training samples. This sheds more light onthe vanishing gradient problem, explains the need for regularization, andsuggests an approach for subsampling training data to improve performance.
arxiv-1605-00391 | Recovery of non-linear cause-effect relationships from linearly mixed neuroimaging data |  http://arxiv.org/abs/1605.00391  | author:Sebastian Weichwald, Arthur Gretton, Bernhard Schölkopf, Moritz Grosse-Wentrup category:stat.ME cs.LG stat.AP stat.ML published:2016-05-02 summary:Causal inference concerns the identification of cause-effect relationshipsbetween variables. However, often only linear combinations of variablesconstitute meaningful causal variables. For example, recovering the signal of acortical source from electroencephalography requires a well-tuned combinationof signals recorded at multiple electrodes. We recently introduced the MERLiN(Mixture Effect Recovery in Linear Networks) algorithm that is able to recover,from an observed linear mixture, a causal variable that is a linear effect ofanother given variable. Here we relax the assumption of this cause-effectrelationship being linear and present an extended algorithm that can pick upnon-linear cause-effect relationships. Thus, the main contribution is analgorithm (and ready to use code) that has broader applicability and allows fora richer model class. Furthermore, a comparative analysis indicates that theassumption of linear cause-effect relationships is not restrictive in analysingelectroencephalographic data.
arxiv-1605-00529 | Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning |  http://arxiv.org/abs/1605.00529  | author:Mario Lucic, Mesrob I. Ohannessian, Amin Karbasi, Andreas Krause category:stat.ML cs.LG published:2016-05-02 summary:Faced with massive data, is it possible to trade off (statistical) risk, and(computational) space and time? This challenge lies at the heart of large-scalemachine learning. Using k-means clustering as a prototypical unsupervisedlearning problem, we show how we can strategically summarize the data (controlspace) in order to trade off risk and time when data is generated by aprobabilistic model. Our summarization is based on coreset constructions fromcomputational geometry. We also develop an algorithm, TRAM, to navigate thespace/time/data/risk tradeoff in practice. In particular, we show that for afixed risk (or data size), as the data size increases (resp. risk increases)the running time of TRAM decreases. Our extensive experiments on real data setsdemonstrate the existence and practical utility of such tradeoffs, not only fork-means but also for Gaussian Mixture Models.
arxiv-1605-00519 | Linear-time Outlier Detection via Sensitivity |  http://arxiv.org/abs/1605.00519  | author:Mario Lucic, Olivier Bachem, Andreas Krause category:stat.ML cs.LG published:2016-05-02 summary:Outliers are ubiquitous in modern data sets. Distance-based techniques are apopular non-parametric approach to outlier detection as they require no priorassumptions on the data generating distribution and are simple to implement.Scaling these techniques to massive data sets without sacrificing accuracy is achallenging task. We propose a novel algorithm based on the intuition thatoutliers have a significant influence on the quality of divergence-basedclustering solutions. We propose sensitivity - the worst-case impact of a datapoint on the clustering objective - as a measure of outlierness. We then provethat influence, a (non-trivial) upper-bound on the sensitivity, can be computedby a simple linear time algorithm. To scale beyond a single machine, we proposea communication efficient distributed algorithm. In an extensive experimentalevaluation, we demonstrate the effectiveness and establish the statisticalsignificance of the proposed approach. In particular, it outperforms the mostpopular distance-based approaches while being several orders of magnitudefaster.
arxiv-1605-00513 | Fuzzy clustering of distribution-valued data using adaptive L2 Wasserstein distances |  http://arxiv.org/abs/1605.00513  | author:Antonio Irpino, Francisco De Carvalho, Rosanna Verde category:stat.ML published:2016-05-02 summary:Distributional (or distribution-valued) data are a new type of data arisingfrom several sources and are considered as realizations of distributionalvariables. A new set of fuzzy c-means algorithms for data described bydistributional variables is proposed. The algorithms use the $L2$ Wasserstein distance between distributions asdissimilarity measures. Beside the extension of the fuzzy c-means algorithm fordistributional data, and considering a decomposition of the squared $L2$Wasserstein distance, we propose a set of algorithms using different automaticway to compute the weights associated with the variables as well as with theircomponents, globally or cluster-wise. The relevance weights are computed in theclustering process introducing product-to-one constraints. The relevance weights induce adaptive distances expressing the importance ofeach variable or of each component in the clustering process, acting also as avariable selection method in clustering. We have tested the proposed algorithmson artificial and real-world data. Results confirm that the proposed methodsare able to better take into account the cluster structure of the data withrespect to the standard fuzzy c-means, with non-adaptive distances.
arxiv-1605-00392 | Revisiting Human Action Recognition: Personalization vs. Generalization |  http://arxiv.org/abs/1605.00392  | author:Andrea Zunino, Jacopo Cavazza, Vittorio Murino category:cs.CV published:2016-05-02 summary:By thoroughly revisiting the classic human action recognition paradigm, thispaper aims at proposing a new approach for the design of effective actionclassification systems. Taking as testbed publicly available three-dimensional(MoCap) action/activity datasets, we analyzed and validated differenttraining/testing strategies. In particular, considering that each human actionin the datasets is performed several times by different subjects, we were ableto precisely quantify the effect of inter- and intra-subject variability, so asto figure out the impact of several learning approaches in terms ofclassification performance. The net result is that standard testing strategiesconsisting in cross-validating the algorithm using typical splits of the data(holdout, k-fold, or one-subject-out) is always outperformed by a"personalization" strategy which learns how a subject is performing an action.In other words, it is advantageous to customize (i.e., personalize) the methodto learn the actions carried out by each subject, rather than trying togeneralize the actions executions across subjects. Consequently, we finallypropose an action recognition framework consisting of a two-stageclassification approach where, given a test action, the subject is firstidentified before the actual recognition of the action takes place. Despite thebasic, off-the-shelf descriptors and standard classifiers adopted, we noted arelevant increase in performance with respect to standard state-of-the-artalgorithms, so motivating the usage of personalized approaches for designingeffective action recognition systems.
arxiv-1605-00507 | Methods for Sparse and Low-Rank Recovery under Simplex Constraints |  http://arxiv.org/abs/1605.00507  | author:Ping Li, Syama Sundar Rangapuram, Martin Slawski category:stat.ME cs.LG published:2016-05-02 summary:The de-facto standard approach of promoting sparsity by means of$\ell_1$-regularization becomes ineffective in the presence of simplexconstraints, i.e.,~the target is known to have non-negative entries summing upto a given constant. The situation is analogous for the use of nuclear normregularization for low-rank recovery of Hermitian positive semidefinitematrices with given trace. In the present paper, we discuss several strategiesto deal with this situation, from simple to more complex. As a starting point,we consider empirical risk minimization (ERM). It follows from existing theorythat ERM enjoys better theoretical properties w.r.t.~prediction and$\ell_2$-estimation error than $\ell_1$-regularization. In light of this, weargue that ERM combined with a subsequent sparsification step like thresholdingis superior to the heuristic of using $\ell_1$-regularization after droppingthe sum constraint and subsequent normalization. At the next level, we show that any sparsity-promoting regularizer undersimplex constraints cannot be convex. A novel sparsity-promoting regularizationscheme based on the inverse or negative of the squared $\ell_2$-norm isproposed, which avoids shortcomings of various alternative methods from theliterature. Our approach naturally extends to Hermitian positive semidefinitematrices with given trace. Numerical studies concerning compressed sensing,sparse mixture density estimation, portfolio optimization and quantum statetomography are used to illustrate the key points of the paper.
arxiv-1605-00482 | Compositional Sentence Representation from Character within Large Context Text |  http://arxiv.org/abs/1605.00482  | author:Geonmin Kim, Hwaran Lee, Jisu Choi, Soo-young Lee category:cs.CL published:2016-05-02 summary:In this work, we targeted two problems of representing a sentence on thebasis of a constituent word sequence: a data-sparsity problem innon-compositional word embedding, and no usage of inter-sentence dependency. Toimprove these two problems, we propose a Hierarchical Composition RecurrentNetwork (HCRN), which consists of a hierarchy with 3 levels of compositionalmodels: character, word and sentence. In HCRN, word representations are builtfrom characters, thus resolving the data-sparsity problem. Moreover, aninter-sentence dependency is embedded into the sentence representation at thelevel of sentence composition. In order to alleviate optimization difficulty ofend-to-end learning for the HCRN, we adopt a hierarchy-wise learning scheme.The HCRN was evaluated on a dialogue act classification task quantitatively andqualitatively. Especially, sentence representations with an inter-sentencedependency significantly improved the performance by capturing both implicitand explicit semantics of sentence. In classifying dialogue act on theSWBD-DAMSL database, our HCRN achieved state-of-the-art performance with a testerror rate of 22.7%.
arxiv-1605-00475 | Rolling Shutter Camera Relative Pose: Generalized Epipolar Geometry |  http://arxiv.org/abs/1605.00475  | author:Yuchao Dai, Hongdong Li, Laurent Kneip category:cs.CV published:2016-05-02 summary:The vast majority of modern consumer-grade cameras employ a rolling shuttermechanism. In dynamic geometric computer vision applications such as visualSLAM, the so-called rolling shutter effect therefore needs to be properly takeninto account. A dedicated relative pose solver appears to be the first problemto solve, as it is of eminent importance to bootstrap any derivation ofmulti-view geometry. However, despite its significance, it has receivedinadequate attention to date. This paper presents a detailed investigation of the geometry of the rollingshutter relative pose problem. We introduce the rolling shutter essentialmatrix, and establish its link to existing models such as the push-broomcameras, summarized in a clean hierarchy of multi-perspective cameras. Thegeneralization of well-established concepts from epipolar geometry is completedby a definition of the Sampson distance in the rolling shutter case. The workis concluded with a careful investigation of the introduced epipolar geometryfor rolling shutter cameras on several dedicated benchmarks.
arxiv-1605-00459 | Multi30K: Multilingual English-German Image Descriptions |  http://arxiv.org/abs/1605.00459  | author:Desmond Elliott, Stella Frank, Khalil Sima'an, Lucia Specia category:cs.CL cs.CV published:2016-05-02 summary:We introduce the Multi30K dataset to stimulate multilingual multimodalresearch. Recent advances in image description have been demonstrated onEnglish-language datasets almost exclusively, but image description should notbe limited to English. This dataset extends the Flickr30K dataset with i)German translations created by professional translators over a subset of theEnglish descriptions, and ii) descriptions crowdsourced independently of theoriginal English descriptions. We outline how the data can be used formultilingual image description and multimodal machine translation, but weanticipate the data will be useful for a broader range of tasks.
arxiv-1605-00404 | Simple2Complex: Global Optimization by Gradient Descent |  http://arxiv.org/abs/1605.00404  | author:Ming Li category:cs.LG cs.NE published:2016-05-02 summary:A method named simple2complex for modeling and training deep neural networksis proposed. Simple2complex train deep neural networks by smoothly adding moreand more layers to the shallow networks, as the learning procedure going on,the network is just like growing. Compared with learning by end2end,simple2complex is with less possibility trapping into local minimal, namely,owning ability for global optimization. Cifar10 is used for verifying thesuperiority of simple2complex.
arxiv-1605-00452 | Fourier Analysis and q-Gaussian Functions: Analytical and Numerical Results |  http://arxiv.org/abs/1605.00452  | author:Paulo Sérgio Silva Rodrigues, Gilson Antonio Giraldi category:cs.CV published:2016-05-02 summary:It is a consensus in signal processing that the Gaussian kernel and itspartial derivatives enable the development of robust algorithms for featuredetection. Fourier analysis and convolution theory have central role in suchdevelopment. In this paper we collect theoretical elements to follow thisavenue but using the q-Gaussian kernel that is a nonextensive generalization ofthe Gaussian one. Firstly, we review some theoretical elements behind theone-dimensional q-Gaussian and its Fourier transform. Then, we consider thetwo-dimensional q-Gaussian and we highlight the issues behind its analyticalFourier transform computation. We analyze the q-Gaussian kernel in the spaceand Fourier domains using the concepts of space window, cut-off frequency, andthe Heisenberg inequality.
arxiv-1605-00420 | An Enhanced Harmony Search Method for Bangla Handwritten Character Recognition Using Region Sampling |  http://arxiv.org/abs/1605.00420  | author:Ritesh Sarkhel, Amit K Saha, Nibaran Das category:cs.CV published:2016-05-02 summary:Identification of minimum number of local regions of a handwritten characterimage, containing well-defined discriminating features which are sufficient fora minimal but complete description of the character is a challenging task. Anew region selection technique based on the idea of an enhanced Harmony Searchmethodology has been proposed here. The powerful framework of Harmony Searchhas been utilized to search the region space and detect only the mostinformative regions for correctly recognizing the handwritten character. Theproposed method has been tested on handwritten samples of Bangla Basic,Compound and mixed (Basic and Compound characters) characters separately withSVM based classifier using a longest run based feature-set obtained from theimage subregions formed by a CG based quad-tree partitioning approach. Applyingthis methodology on the above mentioned three types of datasets, respectively43.75%, 12.5% and 37.5% gains have been achieved in terms of region reductionand 2.3%, 0.6% and 1.2% gains have been achieved in terms of recognitionaccuracy. The results show a sizeable reduction in the minimal number ofdescriptive regions as well a significant increase in recognition accuracy forall the datasets using the proposed technique. Thus the time and cost relatedto feature extraction is decreased without dampening the correspondingrecognition accuracy.
arxiv-1605-00405 | Gradient Descent Converges to Minimizers: The Case of Non-Isolated Critical Points |  http://arxiv.org/abs/1605.00405  | author:Ioannis Panageas, Georgios Piliouras category:math.DS cs.LG published:2016-05-02 summary:We prove that the set of initial conditions so that gradient descentconverges to strict saddle points has (Lebesgue) measure zero, even fornon-isolated critical points, answering an open question in [Lee, Simchowitz,Jordan, Recht, COLT2016].
arxiv-1605-00572 | Comparison of Optimization Methods in Optical Flow Estimation |  http://arxiv.org/abs/1605.00572  | author:Noranart Vesdapunt, Utkarsh Sinha category:cs.CV published:2016-05-02 summary:Optical flow estimation is a widely known problem in computer visionintroduced by Gibson, J.J(1950) to describe the visual perception of human bystimulus objects. Estimation of optical flow model can be achieved by solvingfor the motion vectors from region of interest in the the different timeline.In this paper, we assumed slightly uniform change of velocity between twonearby frames, and solve the optical flow problem by traditional method,Lucas-Kanade(1981). This method performs minimization of errors betweentemplate and target frame warped back onto the template. Solving minimizationsteps requires optimization methods which have diverse convergence rate anderror. We explored first and second order optimization methods, and comparetheir results with Gauss-Newton method in Lucas-Kanade. We generated 105 videoswith 10,500 frames by synthetic objects, and 10 videos with 1,000 frames fromreal world footage. Our experimental results could be used as tuning parametersfor Lucas-Kanade method.
arxiv-1605-00591 | The geometry of learning |  http://arxiv.org/abs/1605.00591  | author:Gianluca Calcagni category:q-bio.QM cs.NE published:2016-05-02 summary:We establish a correspondence between classical conditioning processes andfractals. The association strength at a given training trial corresponds to apoint in a disconnected set at a given iteration level. In this way, one canrepresent a training process as a hopping on a fractal set, instead of thetraditional learning curve as a function of the trial. The main advantage ofthis novel perspective is to provide an elegant classification of associativetheories in terms of the geometric features of fractal sets. In particular, thedimension of fractals is a parameter that can both measure the efficiency of agiven conditioning model (in terms of the characteristics of the stimuli) andcompare the efficiency of different models. We illustrate the correspondencewith the examples of the Hull, Rescorla-Wagner, and Mackintosh models and showthat they are equivalent to a Cantor set. In doing so, we approximate theMackintosh model with a new formulation in terms of a nonlinear recursiveequation for the strength of association.
arxiv-1605-00596 | Graph Clustering Bandits for Recommendation |  http://arxiv.org/abs/1605.00596  | author:Shuai Li, Claudio Gentile, Alexandros Karatzoglou category:stat.ML cs.AI cs.IR cs.LG published:2016-05-02 summary:We investigate an efficient context-dependent clustering technique forrecommender systems based on exploration-exploitation strategies throughmulti-armed bandits over multiple users. Our algorithm dynamically groups usersbased on their observed behavioral similarity during a sequence of loggedactivities. In doing so, the algorithm reacts to the currently served user byshaping clusters around him/her but, at the same time, it explores thegeneration of clusters over users which are not currently engaged. We motivatethe effectiveness of this clustering policy, and provide an extensive empiricalanalysis on real-world datasets, showing scalability and improved predictionperformance over state-of-the-art methods for sequential clustering of users inmulti-armed bandit scenarios.
arxiv-1605-00388 | Highly Accurate Prediction of Jobs Runtime Classes |  http://arxiv.org/abs/1605.00388  | author:Anat Reiner-Benaim, Anna Grabarnick, Edi Shmueli category:stat.ML published:2016-05-02 summary:Separating the short jobs from the long is a known technique to improvescheduling performance. In this paper we describe a method we developed foraccurately predicting the runtimes classes of the jobs to enable thisseparation. Our method uses the fact that the runtimes can be represented as amixture of overlapping Gaussian distributions, in order to train a CARTclassifier to provide the prediction. The threshold that separates the shortjobs from the long jobs is determined during the evaluation of the classifierto maximize prediction accuracy. Our results indicate overall accuracy of 90%for the data set used in our study, with sensitivity and specificity both above90%.
arxiv-1605-00609 | Algorithms for Learning Sparse Additive Models with Interactions in High Dimensions |  http://arxiv.org/abs/1605.00609  | author:Hemant Tyagi, Anastasios Kyrillidis, Bernd Gärtner, Andreas Krause category:cs.LG cs.IT math.IT math.NA stat.ML published:2016-05-02 summary:A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a Sparse AdditiveModel (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in\mathcal{S}}\phi_{l}(x_l)$ where $\mathcal{S} \subset [d]$, $\mathcal{S} \lld$. Assuming $\phi$'s, $\mathcal{S}$ to be unknown, there exists extensive workfor estimating $f$ from its samples. In this work, we consider a generalizedversion of SPAMs, that also allows for the presence of a sparse number ofsecond order interaction terms. For some $\mathcal{S}_1 \subset [d],\mathcal{S}_2 \subset {[d] \choose 2}$, with $\mathcal{S}_1 \ll d,\mathcal{S}_2 \ll d^2$, the function $f$ is now assumed to be of the form:$\sum_{p \in \mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in\mathcal{S}_2}\phi_{(l,l^{\prime})} (x_l,x_{l^{\prime}})$. Assuming we have thefreedom to query $f$ anywhere in its domain, we derive efficient algorithmsthat provably recover $\mathcal{S}_1,\mathcal{S}_2$ with finite sample bounds.Our analysis covers the noiseless setting where exact samples of $f$ areobtained, and also extends to the noisy setting where the queries are corruptedwith noise. For the noisy setting in particular, we consider two noise modelsnamely: i.i.d Gaussian noise and arbitrary but bounded noise. Our main methodsfor identification of $\mathcal{S}_2$ essentially rely on estimation of sparseHessian matrices, for which we provide two novel compressed sensing basedschemes. Once $\mathcal{S}_1, \mathcal{S}_2$ are known, we show how theindividual components $\phi_p$, $\phi_{(l,l^{\prime})}$ can be estimated viaadditional queries of $f$, with uniform error bounds. Lastly, we providesimulation results on synthetic data that validate our theoretical findings.
arxiv-1605-00659 | Predicting online extremism, content adopters, and interaction reciprocity |  http://arxiv.org/abs/1605.00659  | author:Emilio Ferrara, Wen-Qiang Wang, Onur Varol, Alessandro Flammini, Aram Galstyan category:cs.SI cs.LG physics.soc-ph published:2016-05-02 summary:We present a machine learning framework that leverages a mixture of metadata,network, and temporal features to detect extremist users, and predict contentadopters and interaction reciprocity in social media. We exploit a uniquedataset containing millions of tweets generated by more than 25 thousand userswho have been manually identified, reported, and suspended by Twitter due totheir involvement with extremist campaigns. We also leverage millions of tweetsgenerated by a random sample of 25 thousand regular users who were exposed to,or consumed, extremist content. We carry out three forecasting tasks, (i) todetect extremist users, (ii) to estimate whether regular users will adoptextremist content, and finally (iii) to predict whether users will reciprocatecontacts initiated by extremists. All forecasting tasks are set up in twoscenarios: a post hoc (time independent) prediction task on aggregated data,and a simulated real-time prediction task. The performance of our framework isextremely promising, yielding in the different forecasting scenarios up to 93%AUC for extremist user detection, up to 80% AUC for content adoptionprediction, and finally up to 72% AUC for interaction reciprocity forecasting.We conclude by providing a thorough feature analysis that helps determine whichare the emerging signals that provide predictive power in different scenarios.
arxiv-1605-00707 | Discovering Useful Parts for Pose Estimation in Sparsely Annotated Datasets |  http://arxiv.org/abs/1605.00707  | author:Mikhail Breslav, Tyson L. Hedrick, Stan Sclaroff, Margrit Betke category:cs.CV published:2016-05-02 summary:Our work introduces a novel way to increase pose estimation accuracy bydiscovering parts from unannotated regions of training images. Discovered partsare used to generate more accurate appearance likelihoods for traditionalpart-based models like Pictorial Structures [13] and its derivatives. Ourexperiments on images of a hawkmoth in flight show that our proposed approachsignificantly improves over existing work [27] for this application, while alsobeing more generally applicable. Our proposed approach localizes landmarks atleast twice as accurately as a baseline based on a Mixture of PictorialStructures (MPS) model. Our unique High-Resolution Moth Flight (HRMF) datasetis made publicly available with annotations.
arxiv-1605-00366 | Compression Artifacts Removal Using Convolutional Neural Networks |  http://arxiv.org/abs/1605.00366  | author:Pavel Svoboda, Michal Hradis, David Barina, Pavel Zemcik category:cs.CV published:2016-05-02 summary:This paper shows that it is possible to train large and deep convolutionalneural networks (CNN) for JPEG compression artifacts reduction, and that suchnetworks can provide significantly better reconstruction quality compared topreviously used smaller networks as well as to any other state-of-the-artmethods. We were able to train networks with 8 layers in a single step and inrelatively short time by combining residual learning, skip architecture, andsymmetric weight initialization. We provide further insights into convolutionnetworks for JPEG artifact reduction by evaluating three different objectives,generalization with respect to training dataset size, and generalization withrespect to JPEG quality level.
arxiv-1605-00355 | Contrastive Structured Anomaly Detection for Gaussian Graphical Models |  http://arxiv.org/abs/1605.00355  | author:Abhinav Maurya, Mark Cheung category:stat.ML published:2016-05-02 summary:Gaussian graphical models (GGMs) are probabilistic tools of choice foranalyzing conditional dependencies between variables in complex systems.Finding changepoints in the structural evolution of a GGM is thereforeessential to detecting anomalies in the underlying system modeled by the GGM.In order to detect structural anomalies in a GGM, we consider the problem ofestimating changes in the precision matrix of the corresponding Gaussiandistribution. We take a two-step approach to solving this problem:- (i)estimating a background precision matrix using system observations from thepast without any anomalies, and (ii) estimating a foreground precision matrixusing a sliding temporal window during anomaly monitoring. Our primarycontribution is in estimating the foreground precision using a novelcontrastive inverse covariance estimation procedure. In order to accuratelylearn only the structural changes to the GGM, we maximize a penalizedlog-likelihood where the penalty is the $l_1$ norm of difference between theforeground precision being estimated and the already learned backgroundprecision. We modify the alternating direction method of multipliers (ADMM)algorithm for sparse inverse covariance estimation to perform contrastiveestimation of the foreground precision matrix. Our results on simulated GGMdata show significant improvement in precision and recall for detectingstructural changes to the GGM, compared to a non-contrastive sliding windowbaseline.
arxiv-1605-00223 | Text-mining the NeuroSynth corpus using Deep Boltzmann Machines |  http://arxiv.org/abs/1605.00223  | author:Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana category:cs.LG cs.CL q-bio.NC stat.ML published:2016-05-01 summary:Large-scale automated meta-analysis of neuroimaging data has recentlyestablished itself as an important tool in advancing our understanding of humanbrain function. This research has been pioneered by NeuroSynth, a databasecollecting both brain activation coordinates and associated text across a largecohort of neuroimaging research papers. One of the fundamental aspects of suchmeta-analysis is text-mining. To date, word counts and more sophisticatedmethods such as Latent Dirichlet Allocation have been proposed. In this work wepresent an unsupervised study of the NeuroSynth text corpus using DeepBoltzmann Machines (DBMs). The use of DBMs yields several advantages over theaforementioned methods, principal among which is the fact that it yields bothword and document embeddings in a high-dimensional vector space. Suchembeddings serve to facilitate the use of traditional machine learningtechniques on the text corpus. The proposed DBM model is shown to learnembeddings with a clear semantic structure.
arxiv-1605-00287 | Detecting Burnscar from Hyperspectral Imagery via Sparse Representation with Low-Rank Interference |  http://arxiv.org/abs/1605.00287  | author:Minh Dao, Xiang Xiang, Bulent Ayhan, Chiman Kwan, Trac D. Tran category:cs.CV published:2016-05-01 summary:In this paper, we propose a burnscar detection model for hyperspectralimaging (HSI) data. The proposed model contains two-processing steps in whichthe first step separate and then suppress the cloud information presenting inthe data set using an RPCA algorithm and the second step detect the burnscararea in the low-rank component output of the first step. Experiments areconducted on the public MODIS dataset available at NASA official website.
arxiv-1605-00201 | Further properties of the forward-backward envelope with applications to difference-of-convex programming |  http://arxiv.org/abs/1605.00201  | author:Tianxiang Liu, Ting Kei Pong category:math.OC stat.ML published:2016-05-01 summary:In this paper, we further study the forward-backward envelope firstintroduced in [27] and [29] for problems whose objective is the sum of a properclosed convex function and a smooth possibly nonconvex function with Lipschitzcontinuous gradient. We derive sufficient conditions on the original problemfor the corresponding forward-backward envelope to be a level-bounded andKurdyka-{\L}ojasiewicz function with an exponent of $\frac12$; these resultsare important for the efficient minimization of the forward-backward envelopeby classical optimization algorithms. In addition, we demonstrate how tominimize some difference-of-convex regularized least squares problems byminimizing a suitably constructed forward-backward envelope. Our preliminarynumerical results on randomly generated instances of large-scale $\ell_{1-2}$regularized least squares problems [36] illustrate that an implementation ofthis approach with a limited-memory BFGS scheme outperforms some standardfirst-order methods such as the nonmonotone proximal gradient method in [34].
arxiv-1605-00241 | Common-Description Learning: A Framework for Learning Algorithms and Generating Subproblems from Few Examples |  http://arxiv.org/abs/1605.00241  | author:Basem G. El-Barashy category:cs.AI cs.LG published:2016-05-01 summary:Current learning algorithms face many difficulties in learning simplepatterns and using them to learn more complex ones. They also require moreexamples than humans do to learn the same pattern, assuming no prior knowledge.In this paper, a new learning framework is introduced that is calledcommon-description learning (CDL). This framework has been tested on 32 smallmulti-task datasets, and the results show that it was able to learn complexalgorithms from a few number of examples. The final model is perfectlyinterpretable and its depth depends on the question. What is meant by depthhere is that whenever needed, the model learns to break down the problem intosimpler subproblems and solves them using previously learned models. Finally,we explain the capabilities of our framework in discovering complex relationsin data and how it can help in improving language understanding in machines.
arxiv-1605-00251 | A vector-contraction inequality for Rademacher complexities |  http://arxiv.org/abs/1605.00251  | author:Andreas Maurer category:cs.LG stat.ML published:2016-05-01 summary:The contraction inequality for Rademacher averages is extended to Lipschitzfunctions with vector-valued domains, and it is also shown that in the boundingexpression the Rademacher variables can be replaced by arbitrary iid symmetricand sub-gaussian variables. Example applications are given for multi-categorylearning, K-means clustering and learning-to-learn.
arxiv-1605-00252 | Fast Rates with Unbounded Losses |  http://arxiv.org/abs/1605.00252  | author:Peter D. Grünwald, Nishant A. Mehta category:cs.LG stat.ML published:2016-05-01 summary:We present new excess risk bounds for randomized and deterministic estimatorsfor general unbounded loss functions including log loss and squared loss. Ourbounds are expressed in terms of the information complexity and hold under therecently introduced $v$-central condition, allowing for high-probabilitybounds, and its weakening, the $v$-pseudoprobability convexity condition,allowing for bounds in expectation even under heavy-tailed distributions. Theparameter $v$ determines the achievable rate and is akin to the exponent in theTsybakov margin condition and the Bernstein condition for bounded losses, whichthe $v$-conditions generalize; favorable $v$ in combination with smallinformation complexity leads to $\tilde{O}(1/n)$ rates. While these fast rateconditions control the lower tail of the excess loss, the upper tail iscontrolled by a new type of witness-of-badness condition which allows us toconnect the excess risk to a generalized R\'enyi divergence, generalizingprevious results connecting Hellinger distance to KL divergence.
arxiv-1605-00324 | Dominant Codewords Selection with Topic Model for Action Recognition |  http://arxiv.org/abs/1605.00324  | author:Hirokatsu Kataoka, Masaki Hayashi, Kenji Iwata, Yutaka Satoh, Yoshimitsu Aoki, Slobodan Ilic category:cs.CV published:2016-05-01 summary:In this paper, we propose a framework for recognizing human activities thatuses only in-topic dominant codewords and a mixture of intertopic vectors.Latent Dirichlet allocation (LDA) is used to develop approximations of humanmotion primitives; these are mid-level representations, and they adaptivelyintegrate dominant vectors when classifying human activities. In LDA topicmodeling, action videos (documents) are represented by a bag-of-words (inputfrom a dictionary), and these are based on improved dense trajectories. Theoutput topics correspond to human motion primitives, such as finger moving orsubtle leg motion. We eliminate the impurities, such as missed tracking orchanging light conditions, in each motion primitive. The assembled vector ofmotion primitives is an improved representation of the action. We demonstrateour method on four different datasets.
arxiv-1605-00316 | Directional Statistics in Machine Learning: a Brief Review |  http://arxiv.org/abs/1605.00316  | author:Suvrit Sra category:stat.ML published:2016-05-01 summary:The modern data analyst must cope with data encoded in various forms,vectors, matrices, strings, graphs, or more. Consequently, statistical andmachine learning models tailored to different data encodings are important. Wefocus on data encoded as normalized vectors, so that their "direction" is moreimportant than their magnitude. Specifically, we consider high-dimensionalvectors that lie either on the surface of the unit hypersphere or on the realprojective plane. For such data, we briefly review common mathematical modelsprevalent in machine learning, while also outlining some technical aspects,software, applications, and open mathematical challenges.
arxiv-1605-00278 | Particle Smoothing for Hidden Diffusion Processes: Adaptive Path Integral Smoother |  http://arxiv.org/abs/1605.00278  | author:H. -Ch. Ruiz, H. J. Kappen category:cs.LG stat.CO published:2016-05-01 summary:Particle smoothing methods are used for inference of stochastic processesbased on noisy observations. Typically, the estimation of the marginalposterior distribution given all observations is cumbersome and computationalintensive. In this paper, we propose a simple algorithm based on path integralcontrol theory to estimate the smoothing distribution of continuous-timediffusion processes with partial observations. In particular, we use anadaptive importance sampling method to improve the effective sampling size ofthe posterior over processes given the observations and the reliability of theestimation of the marginals. This is achieved by estimating a feedbackcontroller to sample efficiently from the joint smoothing distributions. Wecompare the results with estimations obtained from the standard ForwardFilter/Backward Simulator for two diffusion processes of different complexity.We show that the proposed method gives more reliable estimations than thestandard FFBSi when the smoothing distribution is poorly represented by thefilter distribution.
arxiv-1605-00286 | Multidimensional Scaling on Multiple Input Distance Matrices |  http://arxiv.org/abs/1605.00286  | author:Song Bai, Xiang Bai, Longin Jan Latecki, Qi Tian category:cs.CV published:2016-05-01 summary:Multidimensional Scaling (MDS) is a classic technique that seeks vectorialrepresentations for data points, given the pairwise distances between them.However, in recent years, data are usually collected from diverse sources orhave multiple heterogeneous representations. How to do multidimensional scalingon multiple input distance matrices is still unsolved to our best knowledge. Inthis paper, we first define this new task formally. Then, we propose a newalgorithm called Multi-View Multidimensional Scaling (MVMDS) by consideringeach input distance matrix as one view. Our algorithm is able to learn theweights of views (i.e., distance matrices) automatically by exploring theconsensus information and complementary nature of views. Experimental resultson synthetic as well as real datasets demonstrate the effectiveness of MVMDS.We hope that our work encourages a wider consideration in many domains whereMDS is needed.
arxiv-1605-02029 | Shaping the Future through Innovations: From Medical Imaging to Precision Medicine |  http://arxiv.org/abs/1605.02029  | author:Dorin Comaniciu, Klaus Engel, Bogdan Georgescu, Tommaso Mansi category:cs.CV cs.CE published:2016-05-01 summary:Medical images constitute a source of information essential for diseasediagnosis, treatment and follow-up. In addition, due to its patient-specificnature, imaging information represents a critical component required foradvancing precision medicine into clinical practice. This manuscript describesrecently developed technologies for better handling of image information:photorealistic visualization of medical images with Cinematic Rendering,artificial agents for in-depth image understanding, support for minimallyinvasive procedures, and patient-specific computational models with enhancedpredictive power. Throughout the manuscript we will analyze the capabilities ofsuch technologies and extrapolate on their potential impact to advance thequality of medical care, while reducing its cost.
arxiv-1605-00129 | 3D Keypoint Detection Based on Deep Neural Network with Sparse Autoencoder |  http://arxiv.org/abs/1605.00129  | author:Xinyu Lin, Ce Zhu, Qian Zhang, Yipeng Liu category:cs.CV published:2016-04-30 summary:Researchers have proposed various methods to extract 3D keypoints from thesurface of 3D mesh models over the last decades, but most of them are based ongeometric methods, which lack enough flexibility to meet the requirements forvarious applications. In this paper, we propose a new method on the basis ofdeep learning by formulating the 3D keypoint detection as a regression problemusing deep neural network (DNN) with sparse autoencoder (SAE) as our regressionmodel. Both local information and global information of a 3D mesh model inmulti-scale space are fully utilized to detect whether a vertex is a keypointor not. SAE can effectively extract the internal structure of these two kindsof information and formulate high-level features for them, which is beneficialto the regression model. Three SAEs are used to formulate the hidden layers ofthe DNN and then a logistic regression layer is trained to process thehigh-level features produced in the third SAE. Numerical experiments show thatthe proposed DNN based 3D keypoint detection algorithm outperforms current fivestate-of-the-art methods for various 3D mesh models.
arxiv-1605-00155 | Kernel Balancing: A flexible non-parametric weighting procedure for estimating causal effects |  http://arxiv.org/abs/1605.00155  | author:Chad Hazlett category:stat.ME math.ST stat.AP stat.ML stat.TH published:2016-04-30 summary:In the absence of unobserved confounders, matching and weighting methods arewidely used to estimate causal quantities including the Average TreatmentEffect on the Treated (ATT). Unfortunately, these methods do not necessarilyachieve their goal of making the multivariate distribution of covariates forthe control group identical to that of the treated, leaving some (potentiallymultivariate) functions of the covariates with different means between the twogroups. When these "imbalanced" functions influence the non-treatment potentialoutcome, the conditioning on observed covariates fails, and ATT estimates maybe biased. Kernel balancing, introduced here, targets a weaker requirement forunbiased ATT estimation, specifically, that the expected non-treatmentpotential outcome for the treatment and control groups are equal. Theconditional expectation of the non-treatment potential outcome is assumed tofall in the space of functions associated with a choice of kernel, implying aset of basis functions in which this regression surface is linear. Weights arethen chosen on the control units such that the treated and control group haveequal means on these basis functions. As a result, the expectation of thenon-treatment potential outcome must also be equal for the treated and controlgroups after weighting, allowing unbiased ATT estimation by subsequentdifference in means or an outcome model using these weights. Moreover, theweights produced are (1) precisely those that equalize a particularkernel-based approximation of the multivariate distribution of covariates forthe treated and control, and (2) equivalent to a form of stabilized inversepropensity score weighting, though it does not require assuming any model ofthe treatment assignment mechanism. An R package, KBAL, is provided toimplement this approach.
arxiv-1605-00164 | Look-ahead before you leap: end-to-end active recognition by forecasting the effect of motion |  http://arxiv.org/abs/1605.00164  | author:Dinesh Jayaraman, Kristen Grauman category:cs.CV cs.AI cs.LG cs.RO published:2016-04-30 summary:Visual recognition systems mounted on autonomous moving agents face thechallenge of unconstrained data, but simultaneously have the opportunity toimprove their performance by moving to acquire new views of test data. In thiswork, we first show how a recurrent neural network-based system may be trainedto perform end-to-end learning of motion policies suited for the "activerecognition" setting. Further, we hypothesize that active vision requires anagent to have the capacity to reason about the effects of its motions on itsview of the world. To verify this hypothesis, we attempt to induce thiscapacity in our active recognition pipeline, by simultaneously learning toforecast the effects of the agent's motions on its internal representation ofits cumulative knowledge obtained from all past views. Results across twochallenging datasets confirm both that our end-to-end system successfullylearns meaningful policies for active recognition, and that "learning to lookahead" further boosts recognition performance.
arxiv-1605-00170 | Enforcing Template Representability and Temporal Consistency for Adaptive Sparse Tracking |  http://arxiv.org/abs/1605.00170  | author:Xue Yang, Fei Han, Hua Wang, Hao Zhang category:cs.CV published:2016-04-30 summary:Sparse representation has been widely studied in visual tracking, which hasshown promising tracking performance. Despite a lot of progress, the visualtracking problem is still a challenging task due to appearance variations overtime. In this paper, we propose a novel sparse tracking algorithm that welladdresses temporal appearance changes, by enforcing template representabilityand temporal consistency (TRAC). By modeling temporal consistency, ouralgorithm addresses the issue of drifting away from a tracking target. Byexploring the templates' long-term-short-term representability, the proposedmethod adaptively updates the dictionary using the most descriptive templates,which significantly improves the robustness to target appearance changes. Wecompare our TRAC algorithm against the state-of-the-art approaches on 12challenging benchmark image sequences. Both qualitative and quantitativeresults demonstrate that our algorithm significantly outperforms previousstate-of-the-art trackers.
arxiv-1605-00176 | Stochastic Contextual Bandits with Known Reward Functions |  http://arxiv.org/abs/1605.00176  | author:Pranav Sakulkar, Bhaskar Krishnamachari category:cs.LG published:2016-04-30 summary:Many sequential decision-making problems in communication networks can bemodeled as contextual bandit problems, which are natural extensions of thewell-known multi-armed bandit problem. In contextual bandit problems, at eachtime, an agent observes some side information or context, pulls one arm andreceives the reward for that arm. We consider a stochastic formulation wherethe context-reward tuples are independently drawn from an unknown distributionin each trial. Motivated by networking applications, we analyze a setting wherethe reward is a known non-linear function of the context and the chosen arm'scurrent state. We first consider the case of discrete and finite context-spacesand propose DCB($\epsilon$), an algorithm that we prove, through a carefulanalysis, yields regret (cumulative reward gap compared to a distribution-awaregenie) scaling logarithmically in time and linearly in the number of arms thatare not optimal for any context, improving over existing algorithms where theregret scales linearly in the total number of arms. We then study continuouscontext-spaces with Lipschitz reward functions and propose CCB($\epsilon,\delta$), an algorithm that uses DCB($\epsilon$) as a subroutine.CCB($\epsilon, \delta$) reveals a novel regret-storage trade-off that isparametrized by $\delta$. Tuning $\delta$ to the time horizon allows us toobtain sub-linear regret bounds, while requiring sub-linear storage. Byexploiting joint learning for all contexts we get regret bounds forCCB($\epsilon, \delta$) that are unachievable by any existing contextual banditalgorithm for continuous context-spaces. We also show similar performancebounds for the unknown horizon case.
arxiv-1605-00097 | Application of artificial neural networks and genetic algorithms for crude fractional distillation process modeling |  http://arxiv.org/abs/1605.00097  | author:Lukasz Pater category:cs.NE published:2016-04-30 summary:This work presents the application of the artificial neural networks, trainedand structurally optimized by genetic algorithms, for modeling of crudedistillation process at PKN ORLEN S.A. refinery. Models for the mainfractionator distillation column products were developed using historical data.Quality of the fractions were predicted based on several chosen processvariables. The performance of the model was validated using test data. Neuralnetworks used in companion with genetic algorithms proved that they canaccurately predict fractions quality shifts, reproducing the results of thestandard laboratory analysis. Simple knowledge extraction method from neuralnetwork model built was also performed. Genetic algorithms can be successfullyutilized in efficient training of large neural networks and finding theiroptimal structures.
arxiv-1605-00090 | Topic Augmented Neural Network for Short Text Conversation |  http://arxiv.org/abs/1605.00090  | author:Yu Wu, Wei Wu, Zhoujun Li, Ming Zhou category:cs.CL published:2016-04-30 summary:We consider matching input messages with proper responses for short-textconversation. The matching should be performed not only by the messages and theresponses but also by the topics of the messages. To this end, we propose atopic augmented neural network which consists of a sentence embedding layer, atopic embedding layer, and a matching layer. The sentence embedding layerembeds an input message and a response into a vector space, while the topicembedding layer forms a topic vector by a linear combination of the embeddingof the topic words whose weights are determined by both themselves and themessage vector. The message vector, the response vector, and the topic vectorare then fed to the matching layer to calculate the matching score between themessage and the response. Empirical study on large scale annotated data showsthat our model can significantly outperform state of the art matching models.
arxiv-1605-00079 | Constructive neural network learning |  http://arxiv.org/abs/1605.00079  | author:Shaobo Lin, Jinshan Zeng, Xiaoqin Zhang category:cs.LG published:2016-04-30 summary:In this paper, we aim at developing scalable neural network-type learningsystems. Motivated by the idea of "constructive neural networks" inapproximation theory, we focus on "constructing" rather than "training"feed-forward neural networks (FNNs) for learning, and propose a novel FNNslearning system called the constructive feed-forward neural network (CFN).Theoretically, we prove that the proposed method not only overcomes theclassical saturation problem for FNN approximation, but also reaches theoptimal learning rate when the regression function is smooth, while thestate-of-the-art learning rates established for traditional FNNs are only nearoptimal (up to a logarithmic factor). A series of numerical simulations areprovided to show the efficiency and feasibility of CFN via comparing with thewell-known regularized least squares (RLS) with Gaussian kernel and extremelearning machine (ELM).
arxiv-1605-00075 | Deep Colorization |  http://arxiv.org/abs/1605.00075  | author:Zezhou Cheng, Qingxiong Yang, Bin Sheng category:cs.CV published:2016-04-30 summary:This paper investigates into the colorization problem which converts agrayscale image to a colorful version. This is a very difficult problem andnormally requires manual adjustment to achieve artifact-free quality. Forinstance, it normally requires human-labelled color scribbles on the grayscaletarget image or a careful selection of colorful reference images (e.g.,capturing the same scene in the grayscale target image). Unlike the previousmethods, this paper aims at a high-quality fully-automatic colorization method.With the assumption of a perfect patch matching technique, the use of anextremely large-scale reference database (that contains sufficient colorimages) is the most reliable solution to the colorization problem. However,patch matching noise will increase with respect to the size of the referencedatabase in practice. Inspired by the recent success in deep learningtechniques which provide amazing modeling of large-scale data, this paperre-formulates the colorization problem so that deep learning techniques can bedirectly employed. To ensure artifact-free quality, a joint bilateral filteringbased post-processing step is proposed. We further develop an adaptive imageclustering technique to incorporate the global image information. Numerousexperiments demonstrate that our method outperforms the state-of-art algorithmsboth in terms of quality and speed.
arxiv-1605-00064 | Higher Order Recurrent Neural Networks |  http://arxiv.org/abs/1605.00064  | author:Rohollah Soltani, Hui Jiang category:cs.NE cs.AI published:2016-04-30 summary:In this paper, we study novel neural network structures to better model longterm dependency in sequential data. We propose to use more memory units to keeptrack of more preceding states in recurrent neural networks (RNNs), which areall recurrently fed to the hidden layers as feedback through different weightedpaths. By extending the popular recurrent structure in RNNs, we provide themodels with better short-term memory mechanism to learn long term dependency insequences. Analogous to digital filters in signal processing, we call thesestructures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also belearned using the back-propagation through time method. HORNNs are generallyapplicable to a variety of sequence modelling tasks. In this work, we haveexamined HORNNs for the language modeling task using two popular data sets,namely the Penn Treebank (PTB) and English text8 data sets. Experimentalresults have shown that the proposed HORNNs yield the state-of-the-artperformance on both data sets, significantly outperforming the regular RNNs aswell as the popular LSTMs.
arxiv-1605-00057 | Distributed Cell Association for Energy Harvesting IoT Devices in Dense Small Cell Networks: A Mean-Field Multi-Armed Bandit Approach |  http://arxiv.org/abs/1605.00057  | author:Setareh Maghsudi, Ekram Hossain category:cs.NI cs.LG cs.MA published:2016-04-30 summary:The emerging Internet of Things (IoT)-driven ultra-dense small cell networks(UD-SCNs) will need to combat a variety of challenges. On one hand, massivenumber of devices sharing the limited wireless resources will rendercentralized control mechanisms infeasible due to the excessive cost ofinformation acquisition and computations. On the other hand, to reduce energyconsumption from fixed power grid and/or battery, many IoT devices may need todepend on the energy harvested from the ambient environment (e.g., from RFtransmissions, environmental sources). However, due to the opportunistic natureof energy harvesting, this will introduce uncertainty in the network operation.In this article, we study the distributed cell association problem for energyharvesting IoT devices in UD-SCNs. After reviewing the state-of-the-artresearch on the cell association problem in small cell networks, we outline themajor challenges for distributed cell association in IoT-driven UD-SCNs wherethe IoT devices will need to perform cell association in a distributed mannerin presence of uncertainty (e.g., limited knowledge on channel/network) andlimited computational capabilities. To this end, we propose an approach basedon mean-field multi-armed bandit games to solve the uplink cell associationproblem for energy harvesting IoT devices in a UD-SCN. This approach isparticularly suitable to analyze large multi-agent systems under uncertaintyand lack of information. We provide some theoretical results as well aspreliminary performance evaluation results for the proposed approach.
arxiv-1605-00055 | DisturbLabel: Regularizing CNN on the Loss Layer |  http://arxiv.org/abs/1605.00055  | author:Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, Qi Tian category:cs.CV published:2016-04-30 summary:During a long period of time we are combating over-fitting in the CNNtraining process with model regularization, including weight decay, modelaveraging, data augmentation, etc. In this paper, we present DisturbLabel, anextremely simple algorithm which randomly replaces a part of labels asincorrect values in each iteration. Although it seems weird to intentionallygenerate incorrect training labels, we show that DisturbLabel prevents thenetwork training from over-fitting by implicitly averaging over exponentiallymany networks which are trained with different label sets. To the best of ourknowledge, DisturbLabel serves as the first work which adds noises on the losslayer. Meanwhile, DisturbLabel cooperates well with Dropout to providecomplementary regularization functions. Experiments demonstrate competitiverecognition results on several popular image recognition datasets.
arxiv-1605-00052 | InterActive: Inter-Layer Activeness Propagation |  http://arxiv.org/abs/1605.00052  | author:Lingxi Xie, Liang Zheng, Jingdong Wang, Alan Yuille, Qi Tian category:cs.CV published:2016-04-30 summary:An increasing number of computer vision tasks can be tackled with deepfeatures, which are the intermediate outputs of a pre-trained ConvolutionalNeural Network. Despite the astonishing performance, deep features extractedfrom low-level neurons are still below satisfaction, arguably because theycannot access the spatial context contained in the higher layers. In thispaper, we present InterActive, a novel algorithm which computes the activenessof neurons and network connections. Activeness is propagated through a neuralnetwork in a top-down manner, carrying high-level context and improving thedescriptive power of low-level and mid-level neurons. Visualization indicatesthat neuron activeness can be interpreted as spatial-weighted neuron responses.We achieve state-of-the-art classification performance on a wide range of imagedatasets.
arxiv-1605-00003 | Predicting the direction of stock market prices using random forest |  http://arxiv.org/abs/1605.00003  | author:Luckyson Khaidem, Snehanshu Saha, Sudeepa Roy Dey category:cs.LG cs.CE published:2016-04-29 summary:Predicting trends in stock market prices has been an area of interest forresearchers for many years due to its complex and dynamic nature. Intrinsicvolatility in stock market across the globe makes the task of predictionchallenging. Forecasting and diffusion modeling, although effective can't bethe panacea to the diverse range of problems encountered in prediction,short-term or otherwise. Market risk, strongly correlated with forecastingerrors, needs to be minimized to ensure minimal risk in investment. The authorspropose to minimize forecasting error by treating the forecasting problem as aclassification problem, a popular suite of algorithms in Machine learning. Inthis paper, we propose a novel way to minimize the risk of investment in stockmarket by predicting the returns of a stock using a class of powerful machinelearning algorithms known as ensemble learning. Some of the technicalindicators such as Relative Strength Index (RSI), stochastic oscillator etc areused as inputs to train our model. The learning model used is an ensemble ofmultiple decision trees. The algorithm is shown to outperform existing algo-rithms found in the literature. Out of Bag (OOB) error estimates have beenfound to be encouraging. Key Words: Random Forest Classifier, stock priceforecasting, Exponential smoothing, feature extraction, OOB error andconvergence.
arxiv-1604-08934 | An efficient and expressive similarity measure for relational clustering using neighbourhood trees |  http://arxiv.org/abs/1604.08934  | author:Sebastijan Dumancic, Hendrik Blockeel category:stat.ML cs.AI cs.LG published:2016-04-29 summary:Clustering is an underspecified task: there are no universal criteria forwhat makes a good clustering. This is especially true for relational data,where similarity can be based on the features of individuals, the relationshipsbetween them, or a mix of both. Existing methods for relational clustering havestrong and often implicit biases in this respect. In this paper, we introduce anovel similarity measure for relational data. It is the first measure toincorporate a wide variety of types of similarity, including similarity ofattributes, similarity of relational context, and proximity in a hypergraph. Weexperimentally evaluate how using this similarity affects the quality ofclustering on very different types of datasets. The experiments demonstratethat (a) using this similarity in standard clustering methods consistentlygives good results, whereas other measures work well only on datasets thatmatch their bias; and (b) on most datasets, the novel similarity outperformseven the best among the existing ones.
arxiv-1605-00017 | deepMiRGene: Deep Neural Network based Precursor microRNA Prediction |  http://arxiv.org/abs/1605.00017  | author:Seunghyun Park, Seonwoo Min, Hyunsoo Choi, Sungroh Yoon category:cs.LG q-bio.QM published:2016-04-29 summary:Since microRNAs (miRNAs) play a crucial role in post-transcriptional generegulation, miRNA identification is one of the most essential problems incomputational biology. miRNAs are usually short in length ranging between 20and 23 base pairs. It is thus often difficult to distinguish miRNA-encodingsequences from other non-coding RNAs and pseudo miRNAs that have a similarlength, and most previous studies have recommended using precursor miRNAsinstead of mature miRNAs for robust detection. A great number of conventionalmachine-learning-based classification methods have been proposed, but theyoften have the serious disadvantage of requiring manual feature engineering,and their performance is limited as well. In this paper, we propose a novelmiRNA precursor prediction algorithm, deepMiRGene, based on recurrent neuralnetworks, specifically long short-term memory networks. deepMiRGeneautomatically learns suitable features from the data themselves without manualfeature engineering and constructs a model that can successfully reflectstructural characteristics of precursor miRNAs. For the performance evaluationof our approach, we have employed several widely used evaluation metrics onthree recent benchmark datasets and verified that deepMiRGene deliveredcomparable performance among the current state-of-the-art tools.
arxiv-1604-08723 | Music transcription modelling and composition using deep learning |  http://arxiv.org/abs/1604.08723  | author:Bob L. Sturm, João Felipe Santos, Oded Ben-Tal, Iryna Korshunova category:cs.SD cs.LG published:2016-04-29 summary:We apply deep learning methods, specifically long short-term memory (LSTM)networks, to music transcription modelling and composition. We build and trainLSTM networks using approximately 23,000 music transcriptions expressed with ahigh-level vocabulary (ABC notation), and use them to generate newtranscriptions. Our practical aim is to create music transcription modelsuseful in particular contexts of music composition. We present results fromthree perspectives: 1) at the population level, comparing descriptivestatistics of the set of training transcriptions and generated transcriptions;2) at the individual level, examining how a generated transcription reflectsthe conventions of a music practice in the training transcriptions (Celticfolk); 3) at the application level, using the system for idea generation inmusic composition. We make our datasets, software and sound examples open andavailable: \url{https://github.com/IraKorshunova/folk-rnn}.
arxiv-1604-08880 | Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables |  http://arxiv.org/abs/1604.08880  | author:Nils Y. Hammerla, Shane Halloran, Thomas Ploetz category:cs.LG cs.AI cs.HC stat.ML published:2016-04-29 summary:Human activity recognition (HAR) in ubiquitous computing is beginning toadopt deep learning to substitute for well-established analysis techniques thatrely on hand-crafted feature extraction and classification techniques. Fromthese isolated applications of custom deep architectures it is, however,difficult to gain an overview of their suitability for problems ranging fromthe recognition of manipulative gestures to the segmentation and identificationof physical activities like running or ascending stairs. In this paper werigorously explore deep, convolutional, and recurrent approaches across threerepresentative datasets that contain movement data captured with wearablesensors. We describe how to train recurrent approaches in this setting,introduce a novel regularisation approach, and illustrate how they outperformthe state-of-the-art on a large benchmark dataset. Across thousands ofrecognition experiments with randomly sampled model configurations weinvestigate the suitability of each model for different tasks in HAR, explorethe impact of hyperparameters using the fANOVA framework, and provideguidelines for the practitioner who wants to apply deep learning in theirproblem setting.
arxiv-1604-08740 | MetaGrad: Faster Convergence Without Curvature in Online Convex Optimization |  http://arxiv.org/abs/1604.08740  | author:Wouter M. Koolen, Tim van Erven category:cs.LG published:2016-04-29 summary:In online convex optimization it is well known that objective functions withcurvature are much easier than arbitrary convex functions. Here we show thatthe regret can be significantly reduced even without curvature, in cases wherethere is a stable optimum to converge to. More precisely, the regret ofexisting methods is determined by the norms of the encountered gradients, andmatching worst-case performance lower bounds tell us that this cannot beimproved uniformly. Yet we argue that this is a rather pessimistic assessmentof the complexity of the problem. We introduce a new parameter-free algorithm,called MetaGrad, for which the gradient norms in the regret are scaled down bythe distance to the (unknown) optimum. So when the optimum is reasonably stableover time, making the algorithm converge, this new scaling leads to orders ofmagnitude smaller regret even when the gradients themselves do not vanish. MetaGrad does not require any manual tuning, but instead tunes a learningrate parameter automatically for the data. Unlike all previous methods withprovable guarantees, its learning rates are not monotonically decreasing overtime, but instead are based on a novel aggregation technique. We provide twoversions of MetaGrad. The first maintains a full covariance matrix to guaranteethe sharpest bounds for problems where we can afford update time quadratic inthe dimension. The second version maintains only the diagonal. Its linear costin the dimension makes it suitable for large-scale problems.
arxiv-1604-08772 | Towards Conceptual Compression |  http://arxiv.org/abs/1604.08772  | author:Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, Daan Wierstra category:stat.ML cs.CV cs.LG published:2016-04-29 summary:We introduce a simple recurrent variational auto-encoder architecture thatsignificantly improves image modeling. The system represents thestate-of-the-art in latent variable models for both the ImageNet and Omniglotdatasets. We show that it naturally separates global conceptual informationfrom lower level details, thus addressing one of the fundamentally desiredproperties of unsupervised learning. Furthermore, the possibility ofrestricting ourselves to storing only global information about an image allowsus to achieve high quality 'conceptual compression'.
arxiv-1604-08865 | Convolutional Neural Networks for Facial Attribute-based Active Authentication on Mobile Devices |  http://arxiv.org/abs/1604.08865  | author:Pouya Samangouei, Rama Chellappa category:cs.CV published:2016-04-29 summary:We present Deep Convolutional Neural Network (DCNN) architectures for thetask of continuous authentication on mobile devices by learning intermediatefeatures to reduce the complexity of the networks. The intermediate featuresfor face images are attributes like gender, and hair color. We present amulti-task, part-based DCNN architecture for attributes detection are betterthan or comparable to state-of-the-art methods in terms of accuracy. As abyproduct of the proposed architecture, we explore the embedding space of theattributes extracted from different facial parts, such as mouth and eyes. Weshow that it is possible to discover new attributes by performing subspaceclustering of the embedded features. Furthermore, through extensiveexperimentation, we show that the attribute features extracted by our methodperforms better than previously attribute-based authentication method and thebaseline LBP method. Lastly, we deploy our architecture on a mobile device anddemonstrate the effectiveness of the proposed method.
arxiv-1604-08859 | The Z-loss: a shift and scale invariant classification loss belonging to the Spherical Family |  http://arxiv.org/abs/1604.08859  | author:Alexandre de Brébisson, Pascal Vincent category:cs.LG cs.AI stat.ML published:2016-04-29 summary:Despite being the standard loss function to train multi-class neuralnetworks, the log-softmax has two potential limitations. First, it involvescomputations that scale linearly with the number of output classes, which canrestrict the size of problems we are able to tackle with current hardware.Second, it remains unclear how close it matches the task loss such as the top-kerror rate or other non-differentiable evaluation metrics which we aim tooptimize ultimately. In this paper, we introduce an alternative classificationloss function, the Z-loss, which is designed to address these two issues.Unlike the log-softmax, it has the desirable property of belonging to thespherical loss family (Vincent et al., 2015), a class of loss functions forwhich training can be performed very efficiently with a complexity independentof the number of output classes. We show experimentally that it significantlyoutperforms the other spherical loss functions previously investigated.Furthermore, we show on a word language modeling task that it also outperformsthe log-softmax with respect to certain ranking scores, such as top-k scores,suggesting that the Z-loss has the flexibility to better match the task loss.These qualities thus makes the Z-loss an appealing candidate to train veryefficiently large output networks such as word-language models or other extremeclassification problems. On the One Billion Word (Chelba et al., 2014) dataset,we are able to train a model with the Z-loss 40 times faster than thelog-softmax and more than 4 times faster than the hierarchical softmax.
arxiv-1605-00029 | Multi-Atlas Segmentation using Partially Annotated Data: Methods and Annotation Strategies |  http://arxiv.org/abs/1605.00029  | author:Lisa M. Koch, Martin Rajchl, Wenjia Bai, Christian F. Baumgartner, Tong Tong, Jonathan Passerat-Palmbach, Paul Aljabar, Daniel Rueckert category:cs.CV published:2016-04-29 summary:Multi-atlas segmentation is a widely used tool in medical image analysis,providing robust and accurate results by learning from annotated atlasdatasets. However, the availability of fully annotated atlas images fortraining is limited due to the time required for the labelling task.Segmentation methods requiring only a proportion of each atlas image to belabelled could therefore reduce the workload on expert raters tasked withannotating atlas images. To address this issue, we first re-examine thelabelling problem common in many existing approaches and formulate its solutionin terms of a Markov Random Field energy minimisation problem on a graphconnecting atlases and the target image. This provides a unifying framework formulti-atlas segmentation. We then show how modifications in the graphconfiguration of the proposed framework enable the use of partially annotatedatlas images and investigate different partial annotation strategies. Theproposed method was evaluated on two Magnetic Resonance Imaging (MRI) datasetsfor hippocampal and cardiac segmentation. Experiments were performed aimed at(1) recreating existing segmentation techniques with the proposed framework and(2) demonstrating the potential of employing sparsely annotated atlas data formulti-atlas segmentation.
arxiv-1604-08781 | Teaching natural language to computers |  http://arxiv.org/abs/1604.08781  | author:Joseph Corneli, Miriam Corneli category:cs.CL cs.AI published:2016-04-29 summary:"Natural Language," whether spoken and attended to by humans, or processedand generated by computers, requires a series of structures and networks thatreflect creative processes in semantic, syntactic, phonetic, linguistic,social, emotional, and cultural modules. Being able to produce novel and usefulbehavior following repeated practice gets to the root of both artificialintelligence and human language. This paper investigates current modalitiesinvolved in language-like applications that computers -- and programmers -- areengaged with, and seeks ways of fine tuning the questions we ask, to betteraccount for context, self-awareness, and embodiment.
arxiv-1605-00031 | Deep Convolutional Neural Networks on Cartoon Functions |  http://arxiv.org/abs/1605.00031  | author:Philipp Grohs, Thomas Wiatowski, Helmut Bölcskei category:cs.LG cs.CV math.NA stat.ML published:2016-04-29 summary:Wiatowski and B\"olcskei, 2015, proved that deformation stability andvertical translation invariance of deep convolutional neural network-basedfeature extractors are guaranteed by the network structure per se rather thanthe specific convolution kernels and non-linearities. While the translationinvariance result applies to square-integrable functions, the deformationstability bound holds for band-limited functions only. Many signals ofpractical relevance (such as natural images) exhibit, however, sharp and curveddiscontinuities and are hence not band-limited. The main contribution of thispaper is a deformation stability result that takes these structural propertiesinto account. Specifically, we establish deformation stability bounds for theclass of cartoon functions introduced by Donoho, 2001.
arxiv-1604-08789 | Effective Backscatter Approximation for Photometry in Murky Water |  http://arxiv.org/abs/1604.08789  | author:Chourmouzios Tsiotsios, Maria E. Angelopoulou, Andrew J. Davison, Tae-Kyun Kim category:cs.CV published:2016-04-29 summary:Shading-based approaches like Photometric Stereo assume that the imageformation model can be effectively optimized for the scene normals. However, inmurky water this is a very challenging problem. The light from artificialsources is not only reflected by the scene but it is also scattered by themedium particles, yielding the backscatter component. Backscatter correspondsto a complex term with several unknown variables, and makes the problem ofnormal estimation hard. In this work, we show that instead of trying tooptimize the complex backscatter model or use previous unrealisticsimplifications, we can approximate the per-pixel backscatter signal directlyfrom the captured images. Our method is based on the observation thatbackscatter is saturated beyond a certain distance, i.e. it becomes scene-depthindependent, and finally corresponds to a smoothly varying signal which dependsstrongly on the light position with respect to each pixel. Our backscatterapproximation method facilitates imaging and scene reconstruction in murkywater when the illumination is artificial as in Photometric Stereo.Specifically, we show that it allows accurate scene normal estimation andoffers potentials like single image restoration. We evaluate our approach usingnumerical simulations and real experiments within both the controlledenvironment of a big water-tank and real murky port-waters.
arxiv-1604-08806 | 3D Interest Point Detection Based on Geometric Measures and Sparse Refinement |  http://arxiv.org/abs/1604.08806  | author:Xinyu Lin, Ce Zhu, Qian Zhang, Yipeng Liu category:cs.CV published:2016-04-29 summary:3-dimensional (3D) interest point detection plays a fundamental role incomputer vision. In this paper, we introduce a new method for detecting 3Dinterest points of the surface based on geometric measures and sparserefinement (GMSR). The key idea of our approach is to analyze the geometricproperties of local surface on mesh models in scale-space and utilize theseproperties to calculate the 3D saliency map. Those points with local maxima of3D saliency measure are selected as the candidates of 3D interest points.Finally, we utilize an $l_0$ norm based optimization method to refine thecandidates of 3D interest points by constraining the number of 3D interestpoints. Numerical experiments show that the proposed GMSR based 3D interestpoint detector outperforms current 6 state-of-the-art methods for differentkinds of mesh models.
arxiv-1605-00042 | Improved Sparse Low-Rank Matrix Estimation |  http://arxiv.org/abs/1605.00042  | author:Ankit Parekh, Ivan W. Selesnick category:math.OC cs.LG stat.ML published:2016-04-29 summary:This paper addresses the problem of estimating a sparse low-rank matrix fromits noisy observation. We propose a convex objective function consisting of adata-fidelity term and two parameterized non-convex penalty functions. Thenon-convex penalty functions induce sparsity of the singular values and theentries of the matrix to be estimated. We show how to set the parameters of thenon-convex penalty functions, in order to ensure that the objective function isstrictly convex. The proposed objective function better estimates sparselow-rank matrices than the convex method which utilizes the sum of the nuclearnorm and the $\ell_1$ norm. We derive an algorithm (as an instance of ADMM) tosolve the proposed problem, and guarantee its convergence provided the scalaraugmented Lagrangian parameter is set appropriately.
arxiv-1604-08852 | Joint Sound Source Separation and Speaker Recognition |  http://arxiv.org/abs/1604.08852  | author:Jeroen Zegers, Hugo Van hamme category:cs.SD cs.LG published:2016-04-29 summary:Non-negative Matrix Factorization (NMF) has already been applied to learnspeaker characterizations from single or non-simultaneous speech for speakerrecognition applications. It is also known for its good performance in (blind)source separation for simultaneous speech. This paper explains how NMF can beused to jointly solve the two problems in a multichannel speaker recognizer forsimultaneous speech. It is shown how state-of-the-art multichannel NMF forblind source separation can be easily extended to incorporate speakerrecognition. Experiments on the CHiME corpus show that this method outperformsthe sequential approach of first applying source separation, followed byspeaker recognition that uses state-of-the-art i-vector techniques.
arxiv-1604-08716 | Learning Compact Structural Representations for Audio Events Using Regressor Banks |  http://arxiv.org/abs/1604.08716  | author:Huy Phan, Marco Maass, Lars Hertel, Radoslaw Mazur, Ian McLoughlin, Alfred Mertins category:cs.SD cs.LG stat.ML published:2016-04-29 summary:We introduce a new learned descriptor for audio signals which is efficientfor event representation. The entries of the descriptor are produced byevaluating a set of regressors on the input signal. The regressors areclass-specific and trained using the random regression forests framework. Givenan input signal, each regressor estimates the onset and offset positions of thetarget event. The estimation confidence scores output by a regressor are thenused to quantify how the target event aligns with the temporal structure of thecorresponding category. Our proposed descriptor has two advantages. First, itis compact, i.e. the dimensionality of the descriptor is equal to the number ofevent classes. Second, we show that even simple linear classification models,trained on our descriptor, yield better accuracies on audio eventclassification task than not only the nonlinear baselines but also thestate-of-the-art results.
arxiv-1604-08697 | Sparse Generalized Eigenvalue Problem: Optimal Statistical Rates via Truncated Rayleigh Flow |  http://arxiv.org/abs/1604.08697  | author:Kean Ming Tan, Zhaoran Wang, Han Liu, Tong Zhang category:stat.ML published:2016-04-29 summary:Sparse generalized eigenvalue problem plays a pivotal role in a large familyof high-dimensional learning tasks, including sparse Fisher's discriminantanalysis, canonical correlation analysis, and sufficient dimension reduction.However, the theory of sparse generalized eigenvalue problem remains largelyunexplored. In this paper, we exploit a non-convex optimization perspective tostudy this problem. In particular, we propose the truncated Rayleigh flowmethod (Rifle) to estimate the leading generalized eigenvector and show that itconverges linearly to a solution with the optimal statistical rate ofconvergence. Our theory involves two key ingredients: (i) a new analysis of thegradient descent method on non-convex objective functions, as well as (ii) afine-grained characterization of the evolution of sparsity patterns along thesolution path. Thorough numerical studies are provided to back up our theory.Finally, we apply our proposed method in the context of sparse sufficientdimension reduction to two gene expression data sets.
arxiv-1604-08685 | Single Image 3D Interpreter Network |  http://arxiv.org/abs/1604.08685  | author:Jiajun Wu, Tianfan Xue, Joseph J. Lim, Yuandong Tian, Joshua B. Tenenbaum, Antonio Torralba, William T. Freeman category:cs.CV published:2016-04-29 summary:Understanding 3D object structure from a single image is an important butdifficult task in computer vision, mostly due to the lack of 3D objectannotations in real images. Previous work tackles this problem by eithersolving an optimization task given 2D keypoint positions, or training onsynthetic data with ground truth 3D information. In this work, we propose 3DINterpreter Network (3D-INN), an end-to-end framework which sequentiallyestimates 2D keypoint heatmaps and 3D object structure, trained on both real2D-annotated images and synthetic 3D data. This is made possible mainly by twotechnical innovations. First, we propose a Projection Layer, which projectsestimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3Dstructural parameters supervised by 2D annotations on real images. Second,heatmaps of keypoints serve as an intermediate representation connecting realand synthetic data, enabling 3D-INN to benefit from the variation and abundanceof synthetic 3D objects, without suffering from the difference between thestatistics of real and synthesized images due to imperfect rendering. Thenetwork achieves state-of-the-art performance on both 2D keypoint estimationand 3D structure recovery. We also show that the recovered 3D information canbe used in other vision applications, such as 3D rendering and image retrieval.
arxiv-1604-08683 | Top-push Video-based Person Re-identification |  http://arxiv.org/abs/1604.08683  | author:Jinjie You, Ancong Wu, Xiang Li, Wei-Shi Zheng category:cs.CV published:2016-04-29 summary:Most existing person re-identification (re-id) models focus on matching stillperson images across disjoint camera views. Since only limited information canbe exploited from still images, it is hard (if not impossible) to overcome theocclusion, pose and camera-view change, and lighting variation problems. Incomparison, video-based re-id methods can utilize extra space-time information,which contains much more rich cues for matching to overcome the mentionedproblems. However, we find that when using video-based representation, someinter-class difference can be much more obscure than the one when usingstill-image based representation, because different people could not only havesimilar appearance but also have similar motions and actions which are hard toalign. To solve this problem, we propose a top-push distance learning model(TDL), in which we integrate a top-push constrain for matching video featuresof persons. The top-push constraint enforces the optimization on top-rankmatching in re-id, so as to make the matching model more effective towardsselecting more discriminative features to distinguish different persons. Ourexperiments show that the proposed video-based re-id framework outperforms thestate-of-the-art video-based re-id methods.
arxiv-1604-08660 | Crowd Counting via Weighted VLAD on Dense Attribute Feature Maps |  http://arxiv.org/abs/1604.08660  | author:Biyun Sheng, Chunhua Shen, Guosheng Lin, Jun Li, Wankou Yang, Changyin Sun category:cs.CV published:2016-04-29 summary:Crowd counting is an important task in computer vision, which has manyapplications in video surveillance. Although the regression-based framework hasachieved great improvements for crowd counting, how to improve thediscriminative power of image representation is still an open problem.Conventional holistic features used in crowd counting often fail to capturesemantic attributes and spatial cues of the image. In this paper, we proposeintegrating semantic information into learning locality-aware feature sets foraccurate crowd counting. First, with the help of convolutional neural network(CNN), the original pixel space is mapped onto a dense attribute feature map,where each dimension of the pixel-wise feature indicates the probabilisticstrength of a certain semantic class. Then, locality-aware features (LAF) builton the idea of spatial pyramids on neighboring patches are proposed to exploremore spatial context and local information. Finally, the traditional VLADencoding method is extended to a more generalized form in which diversecoefficient weights are taken into consideration. Experimental results validatethe effectiveness of our presented method.
arxiv-1604-08893 | Faster R-CNN Features for Instance Search |  http://arxiv.org/abs/1604.08893  | author:Amaia Salvador, Xavier Giro-i-Nieto, Ferran Marques, Shin'ichi Satoh category:cs.CV published:2016-04-29 summary:Image representations derived from pre-trained Convolutional Neural Networks(CNNs) have become the new state of the art in computer vision tasks such asinstance retrieval. This work explores the suitability for instance retrievalof image- and region-wise representations pooled from an object detection CNNsuch as Faster R-CNN. We take advantage of the object proposals learned by aRegion Proposal Network (RPN) and their associated CNN features to build aninstance search pipeline composed of a first filtering stage followed by aspatial reranking. We further investigate the suitability of Faster R-CNNfeatures when the network is fine-tuned for the same objects one wants toretrieve. We assess the performance of our proposed system with the OxfordBuildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013,achieving competitive results.
arxiv-1604-08826 | Improved Dense Trajectory with Cross Streams |  http://arxiv.org/abs/1604.08826  | author:Katsunori Ohnishi, Masatoshi Hidaka, Tatsuya Harada category:cs.CV published:2016-04-29 summary:Improved dense trajectories (iDT) have shown great performance in actionrecognition, and their combination with the two-stream approach has achievedstate-of-the-art performance. It is, however, difficult for iDT to completelyremove background trajectories from video with camera shaking. Trajectories inless discriminative regions should be given modest weights in order to createmore discriminative local descriptors for action recognition. In addition, thetwo-stream approach, which learns appearance and motion information separately,cannot focus on motion in important regions when extracting features fromspatial convolutional layers of the appearance network, and vice versa. Inorder to address the above mentioned problems, we propose a new localdescriptor that pools a new convolutional layer obtained from crossing twonetworks along iDT. This new descriptor is calculated by applyingdiscriminative weights learned from one network to a convolutional layer of theother network. Our method has achieved state-of-the-art performance on ordinalaction recognition datasets, 92.3% on UCF101, and 66.2% on HMDB51.
arxiv-1604-08671 | Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution |  http://arxiv.org/abs/1604.08671  | author:Wenhan Yang, Jiashi Feng, Jianchao Yang, Fang Zhao, Jiaying Liu, Zongming Guo, Shuicheng Yan category:cs.CV published:2016-04-29 summary:In this work, we consider the image super-resolution (SR) problem. The mainchallenge of image SR is to recover high-frequency details of a low-resolution(LR) image that are important for human perception. To address this essentiallyill-posed problem, we introduce a Deep Edge Guided REcurrent rEsidual~(DEGREE)network to progressively recover the high-frequency details. Different frommost of existing methods that aim at predicting high-resolution (HR) imagesdirectly, DEGREE investigates an alternative route to recover the differencebetween a pair of LR and HR images by recurrent residual learning. DEGREEfurther augments the SR process with edge-preserving capability, namely the LRimage and its edge map can jointly infer the sharp edge details of the HR imageduring the recurrent recovery process. To speed up its training convergencerate, by-pass connections across multiple layers of DEGREE are constructed. Inaddition, we offer an understanding on DEGREE from the view-point of sub-bandfrequency decomposition on image signal and experimentally demonstrate howDEGREE can recover different frequency bands separately. Extensive experimentson three benchmark datasets clearly demonstrate the superiority of DEGREE overwell-established baselines and DEGREE also provides new state-of-the-arts onthese datasets.
arxiv-1604-08672 | Attention-Based Deep Distance Metric Learning for Aspect-Phrase Grouping |  http://arxiv.org/abs/1604.08672  | author:Shufeng Xiong, Donghong Ji category:cs.CL published:2016-04-29 summary:Aspect phrase grouping is an important task for aspect finding inaspect-level sentiment analysis and it is a challenging problem due to polysemyand its context dependency. In this paper we propose an Attention-based DeepDistance Metric Learning (ADDML) method, which is more beneficial forclustering by considering aspect phrase representation as well as contextrepresentation and their combination. First, we feed word embeddings of aspectphrases and its contexts into an attention-based network to learn featurerepresentation of contexts. Then, both of aspect phrase embedding and contextembedding as the input of a multilayer perceptron which is used to learn deepfeature subspace, under which the distance of each intra-group pair is smallerand that of each inter-group pair is bigger, respectively. After obtaining thelearned representations, we use K-means to cluster them. Experiments on fourdomain review datasets shows that the proposed method outperforms strongbaseline methods.
arxiv-1604-08561 | Comparing Fifty Natural Languages and Twelve Genetic Languages Using Word Embedding Language Divergence (WELD) as a Quantitative Measure of Language Distance |  http://arxiv.org/abs/1604.08561  | author:Ehsaneddin Asgari, Mohammad R. K. Mofrad category:cs.CL published:2016-04-28 summary:We introduce a new measure of distance between languages based on wordembedding, called word embedding language divergence (WELD). WELD is defined asdivergence between unified similarity distribution of words between languages.Using such a measure, we perform language comparison for fifty naturallanguages and twelve genetic languages. Our natural language dataset is acollection of sentence-aligned parallel corpora from bible translations forfifty languages spanning a variety of language families. Although we useparallel corpora, which guarantees having the same content in all languages,interestingly in many cases languages within the same family cluster together.In addition to natural languages, we perform language comparison for the codingregions in the genomes of 12 different organisms (4 plants, 6 animals, and twohuman subjects). Our result confirms a significant high-level difference in thegenetic language model of humans/animals versus plants. The proposed method isa step toward defining a quantitative measure of similarity between languages,with applications in languages classification, genre identification, dialectidentification, and evaluation of translations.
arxiv-1604-08382 | Convolutional Neural Networks For Automatic State-Time Feature Extraction in Reinforcement Learning Applied to Residential Load Control |  http://arxiv.org/abs/1604.08382  | author:Bert J. Claessens, Peter Vrancx, Frederik Ruelens category:cs.LG cs.SY published:2016-04-28 summary:Direct load control of a heterogeneous cluster of residential demandflexibility sources is a high-dimensional control problem with partialobservability. This work proposes a novel approach that uses a convolutionalneural network to extract hidden state-time features to mitigate the curse ofpartial observability. More specific, a convolutional neural network is used asa function approximator to estimate the state-action value function orQ-function in the supervised learning step of fitted Q-iteration. The approachis evaluated in a qualitative simulation, comprising a cluster ofthermostatically controlled loads that only share their air temperature, whilsttheir envelope temperature remains hidden. The simulation results show that thepresented approach is able to capture the underlying hidden features andsuccessfully reduce the electricity cost the cluster.
arxiv-1604-08524 | A Probabilistic Adaptive Search System for Exploring the Face Space |  http://arxiv.org/abs/1604.08524  | author:Andres G. Abad, Luis I. Reyes Castro category:stat.ML cs.CV published:2016-04-28 summary:Face recall is a basic human cognitive process performed routinely, e.g.,when meeting someone and determining if we have met that person before.Assisting a subject during face recall by suggesting candidate faces can bechallenging. One of the reasons is that the search space - the face space - isquite large and lacks structure. A commercial application of face recall isfacial composite systems - such as Identikit, PhotoFIT, and CD-FIT - where awitness searches for an image of a face that resembles his memory of aparticular offender. The inherent uncertainty and cost in the evaluation of theobjective function, the large size and lack of structure of the search space,and the unavailability of the gradient concept makes this problem inappropriatefor traditional optimization methods. In this paper we propose a novelevolutionary approach for searching the face space that can be used as a facialcomposite system. The approach is inspired by methods of Bayesian optimizationand differs from other applications in the use of the skew-normal distributionas its acquisition function. This choice of acquisition function providesgreater granularity, with regularized, conservative, and realistic results.
arxiv-1604-08504 | Detecting "Smart" Spammers On Social Network: A Topic Model Approach |  http://arxiv.org/abs/1604.08504  | author:Linqing Liu, Yao Lu, Ye Luo, Renxian Zhang, Laurent Itti, Jianwei Lu category:cs.CL cs.SI published:2016-04-28 summary:Spammer detection on social network is a challenging problem. The rigidanti-spam rules have resulted in emergence of "smart" spammers. They resemblelegitimate users who are difficult to identify. In this paper, we present anovel spammer classification approach based on Latent DirichletAllocation(LDA), a topic model. Our approach extracts both the local and theglobal information of topic distribution patterns, which capture the essence ofspamming. Tested on one benchmark dataset and one self-collected dataset, ourproposed method outperforms other state-of-the-art methods in terms of averagedF1-score.
arxiv-1604-08402 | Two Differentially Private Rating Collection Mechanisms for Recommender Systems |  http://arxiv.org/abs/1604.08402  | author:Wenjie Zheng category:stat.ML cs.CR cs.IR published:2016-04-28 summary:We design two mechanisms for the recommender system to collect user ratings.One is modified Laplace mechanism, and the other is randomized responsemechanism. We prove that they are both differentially private and preserve thedata utility.
arxiv-1604-08610 | Artistic style transfer for videos |  http://arxiv.org/abs/1604.08610  | author:Manuel Ruder, Alexey Dosovitskiy, Thomas Brox category:cs.CV published:2016-04-28 summary:In the past, manually re-drawing an image in a certain artistic stylerequired a professional artist and a long time. Doing this for a video sequencesingle-handed was beyond imagination. Nowadays computers provide newpossibilities. We present an approach that transfers the style from one image(for example, a painting) to a whole video sequence. We make use of recentadvances in style transfer in still images and propose new initializations andloss functions applicable to videos. This allows us to generate consistent andstable stylized video sequences, even in cases with large motion and strongocclusion. We show that the proposed method clearly outperforms simplerbaselines both qualitatively and quantitatively.
arxiv-1604-08352 | Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition |  http://arxiv.org/abs/1604.08352  | author:Théodore Bluche category:cs.CV cs.LG cs.NE published:2016-04-28 summary:Offline handwriting recognition systems require cropped text line images forboth training and recognition. On the one hand, the annotation of position andtranscript at line level is costly to obtain. On the other hand, automatic linesegmentation algorithms are prone to errors, compromising the subsequentrecognition. In this paper, we propose a modification of the popular andefficient multi-dimensional long short-term memory recurrent neural networks(MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. Moreparticularly, we replace the collapse layer transforming the two-dimensionalrepresentation into a sequence of predictions by a recurrent version which canrecognize one line at a time. In the proposed model, a neural network performsa kind of implicit line segmentation by computing attention weights on theimage representation. The experiments on paragraphs of Rimes and IAM databaseyield results that are competitive with those of networks trained at linelevel, and constitute a significant step towards end-to-end transcription offull documents.
arxiv-1604-08633 | Word Ordering Without Syntax |  http://arxiv.org/abs/1604.08633  | author:Allen Schmaltz, Alexander M. Rush, Stuart M. Shieber category:cs.CL published:2016-04-28 summary:Recent work on word ordering has argued that syntactic structure isimportant, or even required, for effectively recovering the order of asentence. We find that, in fact, an n-gram language model with a simpleheuristic gives strong results on this task. Furthermore, we show that a longshort-term memory (LSTM) language model is comparatively effective atrecovering order, with our basic model outperforming a state-of-the-artsyntactic model by 11.5 BLEU points. Additional data and larger beams yieldfurther gains, at the expense of training and search time.
arxiv-1604-08634 | Optimal Transport vs. Fisher-Rao distance between Copulas for Clustering Multivariate Time Series |  http://arxiv.org/abs/1604.08634  | author:Gautier Marti, Sébastien Andler, Frank Nielsen, Philippe Donnat category:stat.ML published:2016-04-28 summary:We present a methodology for clustering N objects which are described bymultivariate time series, i.e. several sequences of real-valued randomvariables. This clustering methodology leverages copulas which aredistributions encoding the dependence structure between several randomvariables. To take fully into account the dependence information whileclustering, we need a distance between copulas. In this work, we comparerenowned distances between distributions: the Fisher-Rao geodesic distance,related divergences and optimal transport, and discuss their advantages anddisadvantages. Applications of such methodology can be found in the clusteringof financial assets. A tutorial, experiments and implementation forreproducible research can be found at www.datagrapple.com/Tech.
arxiv-1605-01755 | DCTNet and PCANet for acoustic signal feature extraction |  http://arxiv.org/abs/1605.01755  | author:Yin Xian, Andrew Thompson, Xiaobai Sun, Douglas Nowacek, Loren Nolte category:cs.SD cs.LG published:2016-04-28 summary:We introduce the use of DCTNet, an efficient approximation and alternative toPCANet, for acoustic signal classification. In PCANet, the eigenfunctions ofthe local sample covariance matrix (PCA) are used as filterbanks forconvolution and feature extraction. When the eigenfunctions are wellapproximated by the Discrete Cosine Transform (DCT) functions, each layer of ofPCANet and DCTNet is essentially a time-frequency representation. We relateDCTNet to spectral feature representation methods, such as the the short timeFourier transform (STFT), spectrogram and linear frequency spectralcoefficients (LFSC). Experimental results on whale vocalization data show thatDCTNet improves classification rate, demonstrating DCTNet's applicability tosignal processing problems such as underwater acoustics.
arxiv-1604-08642 | On the representation and embedding of knowledge bases beyond binary relations |  http://arxiv.org/abs/1604.08642  | author:Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, Richong Zhang category:cs.LG cs.AI published:2016-04-28 summary:The models developed to date for knowledge base embedding are all based onthe assumption that the relations contained in knowledge bases are binary. Forthe training and testing of these embedding models, multi-fold (or n-ary)relational data are converted to triples (e.g., in FB15K dataset) andinterpreted as instances of binary relations. This paper presents a canonicalrepresentation of knowledge bases containing multi-fold relations. We show thatthe existing embedding models on the popular FB15K datasets correspond to asub-optimal modelling framework, resulting in a loss of structural information.We advocate a novel modelling framework, which models multi-fold relationsdirectly using this canonical representation. Using this framework, theexisting TransH model is generalized to a new model, m-TransH. We demonstrateexperimentally that m-TransH outperforms TransH by a large margin, therebyestablishing a new state of the art.
arxiv-1604-08291 | Streaming View Learning |  http://arxiv.org/abs/1604.08291  | author:Chang Xu, Dacheng Tao, Chao Xu category:stat.ML cs.LG published:2016-04-28 summary:An underlying assumption in conventional multi-view learning algorithms isthat all views can be simultaneously accessed. However, due to various factorswhen collecting and pre-processing data from different views, the streamingview setting, in which views arrive in a streaming manner, is becoming morecommon. By assuming that the subspaces of a multi-view model trained over pastviews are stable, here we fine tune their combination weights such that thewell-trained multi-view model is compatible with new views. This largelyovercomes the burden of learning new view functions and updating past viewfunctions. We theoretically examine convergence issues and the influence ofstreaming views in the proposed algorithm. Experimental results on real-worlddatasets suggest that studying the streaming views problem in multi-viewlearning is significant and that the proposed algorithm can effectively handlestreaming views in different applications.
arxiv-1604-08275 | Crafting Adversarial Input Sequences for Recurrent Neural Networks |  http://arxiv.org/abs/1604.08275  | author:Nicolas Papernot, Patrick McDaniel, Ananthram Swami, Richard Harang category:cs.CR cs.LG cs.NE published:2016-04-28 summary:Machine learning models are frequently used to solve complex securityproblems, as well as to make decisions in sensitive situations like guidingautonomous vehicles or predicting financial market behaviors. Previous effortshave shown that numerous machine learning models were vulnerable to adversarialmanipulations of their inputs taking the form of adversarial samples. Suchinputs are crafted by adding carefully selected perturbations to legitimateinputs so as to force the machine learning model to misbehave, for instance byoutputting a wrong class if the machine learning task of interest isclassification. In fact, to the best of our knowledge, all previous work onadversarial samples crafting for neural network considered models used to solveclassification tasks, most frequently in computer vision applications. In thispaper, we contribute to the field of adversarial machine learning byinvestigating adversarial input sequences for recurrent neural networksprocessing sequential data. We show that the classes of algorithms introducedpreviously to craft adversarial samples misclassified by feed-forward neuralnetworks can be adapted to recurrent neural networks. In a experiment, we showthat adversaries can craft adversarial sequences misleading both categoricaland sequential recurrent neural networks.
arxiv-1604-08320 | Sequential Bayesian optimal experimental design via approximate dynamic programming |  http://arxiv.org/abs/1604.08320  | author:Xun Huan, Youssef M. Marzouk category:stat.ME math.OC stat.CO stat.ML published:2016-04-28 summary:The design of multiple experiments is commonly undertaken via suboptimalstrategies, such as batch (open-loop) design that omits feedback or greedy(myopic) design that does not account for future effects. This paper introducesnew strategies for the optimal design of sequential experiments. First, werigorously formulate the general sequential optimal experimental design (sOED)problem as a dynamic program. Batch and greedy designs are shown to result fromspecial cases of this formulation. We then focus on sOED for parameterinference, adopting a Bayesian formulation with an information theoretic designobjective. To make the problem tractable, we develop new numerical approachesfor nonlinear design with continuous parameter, design, and observation spaces.We approximate the optimal policy by using backward induction with regressionto construct and refine value function approximations in the dynamic program.The proposed algorithm iteratively generates trajectories via exploration andexploitation to improve approximation accuracy in frequently visited regions ofthe state space. Numerical results are verified against analytical solutions ina linear-Gaussian setting. Advantages over batch and greedy design are thendemonstrated on a nonlinear source inversion problem where we seek an optimalpolicy for sequential sensing.
arxiv-1604-07952 | Zero-shot object prediction and context modeling using semantic scene knowledge |  http://arxiv.org/abs/1604.07952  | author:Rene Grzeszick, Gernot A. Fink category:cs.CV published:2016-04-27 summary:This work will focus on the semantic relations between scenes and objects forvisual object recognition. Semantic knowledge can be a powerful source ofinformation especially in scenarios with less or no annotated training samples.These scenarios are referred to as zero-shot recognition and often build onvisual attributes. Here, instead of attributes a more direct way is pursued,relating scenes and objects. The contribution of this paper is two-fold: First,it will be shown that scene knowledge can be an important cue for predictingobjects in an unsupervised manner. This is especially useful in clutteredscenes where visual recognition may be difficult. Second, it will be shown thatthis information can easily be integrated as a context model for objectdetection in a supervised setting.
arxiv-1604-07948 | Graph Laplacian Regularization for Inverse Imaging: Analysis in the Continuous Domain |  http://arxiv.org/abs/1604.07948  | author:Jiahao Pang, Gene Cheung category:cs.CV published:2016-04-27 summary:Inverse imaging problems are inherently under-determined, and hence it isimportant to employ appropriate image priors for regularization. One recentpopular prior---the graph Laplacian regularizer---assumes that the target pixelpatch is smooth with respect to an appropriately chosen graph. However, themechanisms and implications of imposing the graph Laplacian regularizer on theoriginal inverse problem are not well understood. To address this problem, inthis paper we interpret neighborhood graphs of pixel patches as discretecounterparts of Riemannian manifolds and perform analysis in the continuousdomain, providing insights into several fundamental aspects of graph Laplacianregularization. Specifically, we first show the convergence of the graphLaplacian regularizer to a continuous-domain functional, integrating a normmeasured in a locally adaptive metric space. Focusing on image denoising, wederive an optimal metric space assuming nonlocal self-similarity of pixelpatches, leading to an optimal graph Laplacian regularizer for denoising in thediscrete domain. We then interpret graph Laplacian regularization as ananisotropic diffusion scheme to explain its behavior during iterations, e.g.,its tendency to promote piecewise smooth signals under certain settings. Toverify our analysis, an iterative image denoising algorithm is developed.Experimental results show that our algorithm performs competitively withstate-of-the-art denoising methods such as BM3D for natural images, andoutperforms them significantly for piecewise smooth images.
arxiv-1604-07944 | DASC: Robust Dense Descriptor for Multi-modal and Multi-spectral Correspondence Estimation |  http://arxiv.org/abs/1604.07944  | author:Seungryong Kim, Dongbo Min, Bumsub Ham, Minh N. Do, Kwanghoon Sohn category:cs.CV published:2016-04-27 summary:Establishing dense correspondences between multiple images is a fundamentaltask in many applications. However, finding a reliable correspondence inmulti-modal or multi-spectral images still remains unsolved due to theirchallenging photometric and geometric variations. In this paper, we propose anovel dense descriptor, called dense adaptive self-correlation (DASC), toestimate multi-modal and multi-spectral dense correspondences. Based on anobservation that self-similarity existing within images is robust to imagingmodality variations, we define the descriptor with a series of an adaptiveself-correlation similarity measure between patches sampled by a randomizedreceptive field pooling, in which a sampling pattern is obtained using adiscriminative learning. The computational redundancy of dense descriptors isdramatically reduced by applying fast edge-aware filtering. Furthermore, inorder to address geometric variations including scale and rotation, we proposea geometry-invariant DASC (GI-DASC) descriptor that effectively leverages theDASC through a superpixel-based representation. For a quantitative evaluationof the GI-DASC, we build a novel multi-modal benchmark as varying photometricand geometric conditions. Experimental results demonstrate the outstandingperformance of the DASC and GI-DASC in many cases of multi-modal andmulti-spectral dense correspondences.
arxiv-1604-08182 | Unsupervised Classification in Hyperspectral Imagery with Nonlocal Total Variation and Primal-Dual Hybrid Gradient Algorithm |  http://arxiv.org/abs/1604.08182  | author:Wei Zhu, Victoria Chayes, Alexandre Tiard, Stephanie Sanchez, Devin Dahlberg, Da Kuang, Andrea L. Bertozzi, Stanley Osher, Dominique Zosso category:cs.CV published:2016-04-27 summary:We propose a graph-based nonlocal total variation method (NLTV) forunsupervised classification of hyperspectral images (HSI). The variationproblem is solved by the primal-dual hybrid gradient (PDHG) algorithm. Bysquaring the labeling function and using a stable simplex clustering routine,we can implement an unsupervised clustering method with random initialization.Finally, we speed up the calculation using a $k$-d tree and approximate nearestneighbor search algorithm for calculation of the weight matrix for distancesbetween pixel signatures. The effectiveness of this proposed algorithm isillustrated on both synthetic and real-world HSI, and numerical results showthat our algorithm outperform other standard unsupervised clustering methodssuch as spherical K-means, nonnegative matrix factorization (NMF), and thegraph-based Merriman-Bence-Osher (MBO) scheme.
arxiv-1604-07928 | Distributed Flexible Nonlinear Tensor Factorization |  http://arxiv.org/abs/1604.07928  | author:Shandian Zhe, Pengyuan Wang, Kuang-chih Lee, Zenglin Xu, Jian Yang, Youngja Park, Yuan Qi category:cs.LG cs.AI cs.DC stat.ML I.5.1; I.5.4 published:2016-04-27 summary:Tensor factorization is an important approach to multiway data analysis.Compared with popular multilinear methods, nonlinear tensor factorizationmodels are able to capture more complex relationships in data. However, theyare computationally expensive and incapable of exploiting the data sparsity. Toovercome these limitations, we propose a new tensor factorization model. Themodel employs a Gaussian process (GP) to capture the complex nonlinearrelationships. The GP can be projected to arbitrary sets of tensor elements,and thus can avoid the expensive computation of the Kronecker product and isable to flexibly incorporate meaningful entries for training. Furthermore, toscale up the model to large data, we develop a distributed variationalinference algorithm in MapReduce framework. To this end, we derive a tractableand tight variational evidence lower bound (ELBO) that enables efficientparallel computations and high quality inferences. In addition, we design anon-key-value Map-Reduce scheme that can prevent the costly data shuffling andfully use the memory-cache mechanism in fast MapReduce systems such as SPARK.Experiments demonstrate the advantages of our method over existing approachesin terms of both predictive performance and computational efficiency. Moreover,our approach shows a promising potential in the application ofClick-Through-Rate (CTR) prediction for online advertising.
arxiv-1604-07904 | Image Colorization Using a Deep Convolutional Neural Network |  http://arxiv.org/abs/1604.07904  | author:Tung Nguyen, Kazuki Mori, Ruck Thawonmas category:cs.CV cs.LG cs.NE published:2016-04-27 summary:In this paper, we present a novel approach that uses deep learning techniquesfor colorizing grayscale images. By utilizing a pre-trained convolutionalneural network, which is originally designed for image classification, we areable to separate content and style of different images and recombine them intoa single image. We then propose a method that can add colors to a grayscaleimage by combining its content with style of a color image having semanticsimilarity with the grayscale one. As an application, to our knowledge thefirst of its kind, we use the proposed method to colorize images of ukiyo-e agenre of Japanese painting?and obtain interesting results, showing thepotential of this method in the growing field of computer assisted art.
arxiv-1604-08201 | Interpretable Deep Neural Networks for Single-Trial EEG Classification |  http://arxiv.org/abs/1604.08201  | author:Irene Sturm, Sebastian Bach, Wojciech Samek, Klaus-Robert Müller category:cs.NE stat.ML published:2016-04-27 summary:Background: In cognitive neuroscience the potential of Deep Neural Networks(DNNs) for solving complex classification tasks is yet to be fully exploited.The most limiting factor is that DNNs as notorious 'black boxes' do not provideinsight into neurophysiological phenomena underlying a decision. Layer-wiseRelevance Propagation (LRP) has been introduced as a novel method to explainindividual network decisions. New Method: We propose the application of DNNswith LRP for the first time for EEG data analysis. Through LRP the single-trialDNN decisions are transformed into heatmaps indicating each data point'srelevance for the outcome of the decision. Results: DNN achieves classificationaccuracies comparable to those of CSP-LDA. In subjects with low performancesubject-to-subject transfer of trained DNNs can improve the results. Thesingle-trial LRP heatmaps reveal neurophysiologically plausible patterns,resembling CSP-derived scalp maps. Critically, while CSP patterns representclass-wise aggregated information, LRP heatmaps pinpoint neural patterns tosingle time points in single trials. Comparison with Existing Method(s): Wecompare the classification performance of DNNs to that of linear CSP-LDA on twodata sets related to motor-imaginery BCI. Conclusion: We have demonstrated thatDNN is a powerful non-linear tool for EEG analysis. With LRP a new quality ofhigh-resolution assessment of neural activity can be reached. LRP is apotential remedy for the lack of interpretability of DNNs that has limitedtheir utility in neuroscientific applications. The extreme specificity of theLRP-derived heatmaps opens up new avenues for investigating neural activityunderlying complex perception or decision-related processes.
arxiv-1604-08500 | Detection of epileptic seizure in EEG signals using linear least squares preprocessing |  http://arxiv.org/abs/1604.08500  | author:Z. Roshan Zamir category:cs.LG math.OC published:2016-04-27 summary:An epileptic seizure is a transient event of abnormal excessive neuronaldischarge in the brain. This unwanted event can be obstructed by detection ofelectrical changes in the brain that happen before the seizure takes place. Theautomatic detection of seizures is necessary since the visual screening of EEGrecordings is a time consuming task and requires experts to improve thediagnosis. Four linear least squares-based preprocessing models are proposed toextract key features of an EEG signal in order to detect seizures. The firsttwo models are newly developed. The original signal (EEG) is approximated by asinusoidal curve. Its amplitude is formed by a polynomial function and comparedwith the pre developed spline function.Different statistical measures namelyclassification accuracy, true positive and negative rates, false positive andnegative rates and precision are utilized to assess the performance of theproposed models. These metrics are derived from confusion matrices obtainedfrom classifiers. Different classifiers are used over the original dataset andthe set of extracted features. The proposed models significantly reduce thedimension of the classification problem and the computational time while theclassification accuracy is improved in most cases. The first and third modelsare promising feature extraction methods. Logistic, LazyIB1, LazyIB5 and J48are the best classifiers. Their true positive and negative rates are $1$ whilefalse positive and negative rates are zero and the corresponding precisionvalues are $1$. Numerical results suggest that these models are robust andefficient for detecting epileptic seizure.
arxiv-1604-08202 | Amodal Instance Segmentation |  http://arxiv.org/abs/1604.08202  | author:Ke Li, Jitendra Malik category:cs.CV published:2016-04-27 summary:We consider the problem of amodal instance segmentation, the objective ofwhich is to predict the region encompassing both visible and occluded parts ofeach object. Thus far, the lack of publicly available amodal segmentationannotations has stymied the development of amodal segmentation methods. In thispaper, we sidestep this issue by relying solely on standard modal instancesegmentation annotations to train our model. The result is a new method foramodal instance segmentation, which represents the first such method to thebest of our knowledge. We demonstrate the proposed method's effectiveness bothqualitatively and quantitatively.
arxiv-1604-08220 | Diving deeper into mentee networks |  http://arxiv.org/abs/1604.08220  | author:Ragav Venkatesan, Baoxin Li category:cs.LG cs.CV cs.NE published:2016-04-27 summary:Modern computer vision is all about the possession of powerful imagerepresentations. Deeper and deeper convolutional neural networks have beenbuilt using larger and larger datasets and are made publicly available. A largeswath of computer vision scientists use these pre-trained networks with varyingdegrees of successes in various tasks. Even though there is tremendous successin copying these networks, the representational space is not learnt from thetarget dataset in a traditional manner. One of the reasons for opting to use apre-trained network over a network learnt from scratch is that small datasetsprovide less supervision and require meticulous regularization, smaller andcareful tweaking of learning rates to even achieve stable learning withoutweight explosion. It is often the case that large deep networks are notportable, which necessitates the ability to learn mid-sized networks fromscratch. In this article, we dive deeper into training these mid-sized networks onsmall datasets from scratch by drawing additional supervision from a largepre-trained network. Such learning also provides better generalizationaccuracies than networks trained with common regularization techniques such asl2, l1 and dropouts. We show that features learnt thus, are more general thanthose learnt independently. We studied various characteristics of such networksand found some interesting behaviors.
arxiv-1604-08242 | The IBM 2016 English Conversational Telephone Speech Recognition System |  http://arxiv.org/abs/1604.08242  | author:George Saon, Tom Sercu, Steven Rennie, Hong-Kwang J. Kuo category:cs.CL published:2016-04-27 summary:We describe a collection of acoustic and language modeling techniques thatlowered the word error rate of our English conversational telephone LVCSRsystem to a record 6.9% on the Switchboard subset of the Hub5 2000 evaluationtestset. On the acoustic side, we use a score fusion of three strong models:recurrent nets with maxout activations, very deep convolutional nets with 3x3kernels, and bidirectional long-short term memory nets which operate onbottleneck features. On the language modeling side, we use an updated model "M"and hierarchical neural network LMs.
arxiv-1604-08256 | Multiview Differential Geometry of Curves |  http://arxiv.org/abs/1604.08256  | author:Ricardo Fabbri, Benjamin Kimia category:cs.CV cs.CG cs.GR math.DG I.4.8; I.3.5 published:2016-04-27 summary:The field of multiple view geometry has seen tremendous progress inreconstruction and calibration due to methods for extracting reliable pointfeatures and key developments in projective geometry. Point features, however,are not available in certain applications and result in unstructured pointcloud reconstructions. General image curves provide a complementary featurewhen keypoints are scarce, and result in 3D curve geometry, but face challengesnot addressed by the usual projective geometry of points and algebraic curves.We address these challenges by laying the theoretical foundations of aframework based on the differential geometry of general curves, includingstationary curves, occluding contours, and non-rigid curves, aiming at stereocorrespondence, camera estimation (including calibration, pose, and multiviewepipolar geometry), and 3D reconstruction given measured image curves. Bygathering previous results into a cohesive theory, novel results were madepossible, yielding three contributions. First we derive the differentialgeometry of an image curve (tangent, curvature, curvature derivative) from thatof the underlying space curve (tangent, curvature, curvature derivative,torsion). Second, we derive the differential geometry of a space curve fromthat of two corresponding image curves. Third, the differential motion of animage curve is derived from camera motion and the differential geometry andmotion of the space curve. The availability of such a theory enables novelcurve-based multiview reconstruction and camera estimation systems to augmentexisting point-based approaches. This theory has been used to reconstruct a "3Dcurve sketch", to determine camera pose from local curve geometry, andtracking; other developments are underway.
arxiv-1604-08269 | Efficient Optimization for Rank-based Loss Functions |  http://arxiv.org/abs/1604.08269  | author:Pritish Mohapatra, Michal Rolinek, C. V. Jawahar, Vladimir Kolmogorov, M. Pawan Kumar category:cs.CV published:2016-04-27 summary:The accuracy of information retrieval systems is often measured using complexnon-decomposable loss functions such as the average precision (AP) or thenormalized discounted cumulative gain (NDCG). Given a set of positive(relevant) and negative (non-relevant) samples, the parameters of a retrievalsystem can be estimated using a rank SVM framework, which minimizes aregularized convex upper bound on the empirical loss. However, the highcomputational complexity of loss-augmented inference, which is required tolearn a rank SVM, prohibits its use in large training datasets. To alleviatethis de?ciency, we present a novel quicksort avored algorithm for a large classof nondecomposable loss functions. We provide a complete characterization ofthe loss functions that are amenable to our algorithm. Furthermore, we provethat no comparison based algorithm can improve upon the computationalcomplexity of our approach asymptotically. We demonstrate that it is possibleto reduce the constant factors of the complexity by exploiting the specialstructure of the AP loss. Using the PASCAL VOC action recognition and objectdetection datasets, we show that our approach provides signi?cantly betterresults than baseline methods that use a simpler decomposable loss incomparable runtime.
arxiv-1604-08153 | Classifying Options for Deep Reinforcement Learning |  http://arxiv.org/abs/1604.08153  | author:Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, Anil Anthony Bharath category:cs.LG cs.AI stat.ML published:2016-04-27 summary:Deep reinforcement learning is the learning of multiple levels ofhierarchical representations for reinforcement learning. Hierarchicalreinforcement learning focuses on temporal abstractions in planning andlearning, allowing temporally-extended actions to be transferred between tasks.In this paper we combine one method for hierarchical reinforcement learning -the options framework - with deep Q-networks (DQNs) through the use ofdifferent "option heads" on the policy network, and a supervisory network forchoosing between the different options. We show that in a domain where we haveprior knowledge of the mapping between states and options, our augmented DQNachieves a policy competitive with that of a standard DQN, but with much lowersample complexity. This is achieved through a straightforward architecturaladjustment to the DQN, as well as an additional supervised neural network.
arxiv-1604-08145 | Laser light-field fusion for wide-field lensfree on-chip phase contrast nanoscopy |  http://arxiv.org/abs/1604.08145  | author:Farnoud Kazemzadeh, Alexander Wong category:physics.optics cs.CV published:2016-04-27 summary:Wide-field lensfree on-chip microscopy, which leverages holography principlesto capture interferometric light-field encodings without lenses, is an emergingimaging modality with widespread interest given the large field-of-viewcompared to lens-based techniques. Nanoscopy is often synonymous with highequipment costs and limited FOV. In this study, we introduce the idea of laserlight-field fusion for lensfree on-chip phase contrast nanoscopy, whereinterferometric laser light-field encodings acquired using an on-chip setupwith laser pulsations at different wavelengths are fused to produce marker-freephase contrast images with resolving power below the pixel pitch of the sensorarray as well as the wavelength of the probing light source, beyond thediffraction limit. Experimental results demonstrate, for the first time, alensfree on-chip instrument successfully detecting 500 nm nanoparticles withoutany specialized or intricate sample preparation or the use of syntheticaperture- or lateral shift-based techniques.
arxiv-1604-07953 | Simultaneous Food Localization and Recognition |  http://arxiv.org/abs/1604.07953  | author:Marc Bolaños, Petia Radeva category:cs.CV published:2016-04-27 summary:The development of automatic nutrition diaries, which would allow to keeptrack objectively of everything we eat, could enable a whole new world ofpossibilities for people concerned about their nutrition patterns. With thispurpose, in this paper we propose the first method for simultaneous foodlocalization and recognition. Our method is based on two main steps, whichconsist in, first, produce a food activation map on the input image (i.e. heatmap of probabilities) for generating bounding boxes proposals and, second,recognize each of the food types or food-related objects present in eachbounding box. We demonstrate that our proposal, compared to the most similarproblem nowadays - object localization, is able to obtain high precision andreasonable recall levels with only a few bounding boxes. Furthermore, we showthat it is applicable to both conventional and egocentric images.
arxiv-1604-08010 | Deep Learning for Saliency Prediction in Natural Video |  http://arxiv.org/abs/1604.08010  | author:Souad Chaabouni, Jenny Benois-Pineau, Ofer Hadar, Chokri Ben Amar category:cs.CV published:2016-04-27 summary:The purpose of this paper is the detection of salient areas in natural videoby using the new deep learning techniques. Salient patches in video frames arepredicted first. Then the predicted visual fixation maps are built upon them.We design the deep architecture on the basis of CaffeNet implemented with Caffetoolkit. We show that changing the way of data selection for optimisation ofnetwork parameters, we can save computation cost up to 12 times. We extend deeplearning approaches for saliency prediction in still images with RGB values tospecificity of video using the sensitivity of the human visual system toresidual motion. Furthermore, we complete primary colour pixel values bycontrast features proposed in classical visual attention prediction models. Theexperiments are conducted on two publicly available datasets. The first isIRCCYN video database containing 31 videos with an overall amount of 7300frames and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, theaccuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction ofsaliency of patches show the improvement up to 2% with regard to RGB use only.The resulting accuracy of 76, 6% is obtained. The AUC metric in comparison ofpredicted saliency maps with visual fixation maps shows the increase up to 16%on a sample of video clips from this dataset.
arxiv-1604-08079 | UBL: an R package for Utility-based Learning |  http://arxiv.org/abs/1604.08079  | author:Paula Branco, Rita P. Ribeiro, Luis Torgo category:cs.MS cs.LG stat.ML published:2016-04-27 summary:This document describes the R package UBL that allows the use of severalmethods for handling utility-based learning problems. Classification andregression problems that assume non-uniform costs and/or benefits pose seriouschallenges to predictive analytics tasks. In the context of meteorology,finance, medicine, ecology, among many other, specific domain informationconcerning the preference bias of the users must be taken into account toenhance the models predictive performance. To deal with this problem, a largenumber of techniques was proposed by the research community for bothclassification and regression tasks. The main goal of UBL package is tofacilitate the utility-based predictive analytics task by providing a set ofmethods to deal with this type of problems in the R environment. It is aversatile tool that provides mechanisms to handle both regression andclassification (binary and multiclass) tasks. Moreover, UBL package allows theuser to specify his domain preferences, but it also provides some automaticmethods that try to infer those preference bias from the domain, consideringsome common known settings.
arxiv-1604-08088 | Detecting Violence in Video using Subclasses |  http://arxiv.org/abs/1604.08088  | author:Xirong Li, Yujia Huo, Jieping Xu, Qin Jin category:cs.MM cs.CV published:2016-04-27 summary:This paper attacks the challenging problem of violence detection in videos.Different from existing works focusing on combining multi-modal features, we goone step further by adding and exploiting subclasses visually related toviolence. We enrich the MediaEval 2015 violence dataset by \emph{manually}labeling violence videos with respect to the subclasses. Such fine-grainedannotations not only help understand what have impeded previous efforts onlearning to fuse the multi-modal features, but also enhance the generalizationability of the learned fusion to novel test data. The new subclass basedsolution, with AP of 0.303 and P100 of 0.55 on the MediaEval 2015 test set,outperforms several state-of-the-art alternatives. Notice that our solutiondoes not require fine-grained annotations on the test set, so it can bedirectly applied on novel and fully unlabeled videos. Interestingly, our studyshows that motion related features, though being essential part in previoussystems, are dispensable.
arxiv-1604-08098 | Local Uncertainty Sampling for Large-Scale Multi-Class Logistic Regression |  http://arxiv.org/abs/1604.08098  | author:Lei Han, Ting Yang, Tong Zhang category:stat.CO cs.LG stat.ML published:2016-04-27 summary:A major challenge for building statistical models in the big data era is thatthe available data volume may exceed the computational capability. A commonapproach to solve this problem is to employ a subsampled dataset that can behandled by the available computational resources. In this paper, we propose ageneral subsampling scheme for large-scale multi-class logistic regression, andexamine the variance of the resulting estimator. We show that asymptotically,the proposed method always achieves a smaller variance than that of the uniformrandom sampling. Moreover, when the classes are conditional imbalanced,significant improvement over uniform sampling can be achieved. Empiricalperformance of the proposed method is compared to other methods on bothsimulated and real-world datasets, and these results confirm our theoreticalanalysis.
arxiv-1604-08102 | An ABC interpretation of the multiple auxiliary variable method |  http://arxiv.org/abs/1604.08102  | author:Dennis Prangle, Richard G. Everitt category:stat.CO stat.ML published:2016-04-27 summary:We show that the auxiliary variable method (M{\o}ller et al., 2006; Murray etal., 2006) for inference of Markov random fields can be viewed as anapproximate Bayesian computation method for likelihood estimation.
arxiv-1604-08120 | Extracting Temporal and Causal Relations between Events |  http://arxiv.org/abs/1604.08120  | author:Paramita Mirza category:cs.CL published:2016-04-27 summary:Structured information resulting from temporal information processing iscrucial for a variety of natural language processing tasks, for instance togenerate timeline summarization of events from news documents, or to answertemporal/causal-related questions about some events. In this thesis we presenta framework for an integrated temporal and causal relation extraction system.We first develop a robust extraction component for each type of relations, i.e.temporal order and causality. We then combine the two extraction componentsinto an integrated relation extraction system, CATENA---CAusal and Temporalrelation Extraction from NAtural language texts---, by utilizing thepresumption about event precedence in causality, that causing events musthappened BEFORE resulting events. Several resources and techniques to improveour relation extraction systems are also discussed, including word embeddingsand training data expansion. Finally, we report our adaptation efforts oftemporal information processing for languages other than English, namelyItalian and Indonesian.
arxiv-1604-07809 | Entities as topic labels: Improving topic interpretability and evaluability combining Entity Linking and Labeled LDA |  http://arxiv.org/abs/1604.07809  | author:Federico Nanni, Pablo Ruiz Fabo category:cs.CL published:2016-04-26 summary:In order to create a corpus exploration method providing topics that areeasier to interpret than standard LDA topic models, here we propose combiningtwo techniques called Entity linking and Labeled LDA. Our method identifies inan ontology a series of descriptive labels for each document in a corpus. Thenit generates a specific topic for each label. Having a direct relation betweentopics and labels makes interpretation easier; using an ontology as backgroundknowledge limits label ambiguity. As our topics are described with a limitednumber of clear-cut labels, they promote interpretability, and this may helpquantitative evaluation. We illustrate the potential of the approach byapplying it in order to define the most relevant topics addressed by each partyin the European Parliament's fifth mandate (1999-2004).
arxiv-1604-07872 | Are Face and Object Recognition Independent? A Neurocomputational Modeling Exploration |  http://arxiv.org/abs/1604.07872  | author:Panqu Wang, Isabel Gauthier, Garrison Cottrell category:q-bio.NC cs.CV published:2016-04-26 summary:Are face and object recognition abilities independent? Although it iscommonly believed that they are, Gauthier et al.(2014) recently showed thatthese abilities become more correlated as experience with nonface categoriesincreases. They argued that there is a single underlying visual ability, v,that is expressed in performance with both face and nonface categories asexperience grows. Using the Cambridge Face Memory Test and the VanderbiltExpertise Test, they showed that the shared variance between Cambridge FaceMemory Test and Vanderbilt Expertise Test performance increases monotonicallyas experience increases. Here, we address why a shared resource acrossdifferent visual domains does not lead to competition and to an inversecorrelation in abilities? We explain this conundrum using ourneurocomputational model of face and object processing (The Model, TM). Ourresults show that, as in the behavioral data, the correlation betweensubordinate level face and object recognition accuracy increases as experiencegrows. We suggest that different domains do not compete for resources becausethe relevant features are shared between faces and objects. The essential powerof experience is to generate a "spreading transform" for faces that generalizesto objects that must be individuated. Interestingly, when the task of thenetwork is basic level categorization, no increase in the correlation betweendomains is observed. Hence, our model predicts that it is the type ofexperience that matters and that the source of the correlation is in thefusiform face area, rather than in cortical areas that subserve basic levelcategorization. This result is consistent with our previous modelingelucidating why the FFA is recruited for novel domains of expertise (Tong etal., 2008).
arxiv-1604-07878 | Evaluating the effect of topic consideration in identifying communities of rating-based social networks |  http://arxiv.org/abs/1604.07878  | author:Ali Reihanian, Behrouz Minaei-Bidgoli, Muhammad Yousefnezhad category:cs.SI cs.LG stat.ML published:2016-04-26 summary:Finding meaningful communities in social network has attracted the attentionsof many researchers. The community structure of complex networks reveals boththeir organization and hidden relations among their constituents. Most of theresearches in the field of community detection mainly focus on the topologicalstructure of the network without performing any content analysis. Nowadays,real world social networks are containing a vast range of information includingshared objects, comments, following information, etc. In recent years, a numberof researches have proposed approaches which consider both the contents thatare interchanged in the networks and the topological structures of the networksin order to find more meaningful communities. In this research, the effect oftopic analysis in finding more meaningful communities in social networkingsites in which the users express their feelings toward different objects (likemovies) by the means of rating is demonstrated by performing extensiveexperiments.
arxiv-1604-07711 | Condorcet's Jury Theorem for Consensus Clustering |  http://arxiv.org/abs/1604.07711  | author:Brijnesh J. Jain category:stat.ML cs.LG published:2016-04-26 summary:The goal of consensus clustering is to improve the quality of clustering bycombining a sample of partitions of a dataset to a single consensus partition.This contribution extends Condorcet's Jury Theorem to the mean partitionapproach of consensus clustering. As a consequence of the proposed result, wechallenge and reappraise the role of diversity in consensus clustering.
arxiv-1604-07513 | Semantic Change Detection with Hypermaps |  http://arxiv.org/abs/1604.07513  | author:Hirokatsu Kataoka, Soma Shirakabe, Yudai Miyashita, Akio Nakamura, Kenji Iwata, Yutaka Satoh category:cs.CV cs.AI published:2016-04-26 summary:Change detection is the study of detecting changes between two differentimages of a scene taken at different times. This paper proposes the concept ofsemantic change detection, which involves intuitively inserting semanticmeaning into detected change areas. The problem to be solved consists of twoparts, semantic segmentation and change detection. In order to solve thisproblem and obtain a high-level of performance, we propose an improvement tothe hypercolumns representation, hereafter known as hypermaps, whicheffectively uses convolutional maps obtained from convolutional neural networks(CNNs). We also employ multi-scale feature representation captured by differentimage patches. We applied our method to the TSUNAMI Panoramic Change Detectiondataset, and re-annotated the changed areas of the dataset via semanticclasses. The results show that our multi-scale hypermaps provided outstandingperformance on the re-annotated TSUNAMI dataset.
arxiv-1604-07706 | Distributed Clustering of Linear Bandits in Peer to Peer Networks |  http://arxiv.org/abs/1604.07706  | author:Nathan Korda, Balazs Szorenyi, Shuai Li category:cs.LG cs.AI stat.ML published:2016-04-26 summary:We provide two distributed confidence ball algorithms for solving linearbandit problems in peer to peer networks with limited communicationcapabilities. For the first, we assume that all the peers are solving the samelinear bandit problem, and prove that our algorithm achieves the optimalasymptotic regret rate of any centralised algorithm that can instantlycommunicate information between the peers. For the second, we assume that thereare clusters of peers solving the same bandit problem within each cluster, andwe prove that our algorithm discovers these clusters, while achieving theoptimal asymptotic regret rate within each one. Through experiments on severalreal-world datasets, we demonstrate the performance of proposed algorithmscompared to the state-of-the-art.
arxiv-1604-07759 | F-measure Maximization in Multi-Label Classification with Conditionally Independent Label Subsets |  http://arxiv.org/abs/1604.07759  | author:Maxime Gasse, Alex AUssem category:cs.LG published:2016-04-26 summary:We discuss a method to improve the exact F-measure maximization algorithmcalled GFM, proposed in (Dembczynski et al. 2011) for multi-labelclassification, assuming the label set can be can partitioned intoconditionally independent subsets given the input features. If the labels wereall independent, the estimation of only $m$ parameters ($m$ denoting the numberof labels) would suffice to derive Bayes-optimal predictions in $O(m^2)$operations. In the general case, $m^2+1$ parameters are required by GFM, tosolve the problem in $O(m^3)$ operations. In this work, we show that the numberof parameters can be reduced further to $m^2/n$, in the best case, assuming thelabel set can be partitioned into $n$ conditionally independent subsets. Asthis label partition needs to be estimated from the data beforehand, we usefirst the procedure proposed in (Gasse et al. 2015) that finds such partitionand then infer the required parameters locally in each label subset. The latterare aggregated and serve as input to GFM to form the Bayes-optimal prediction.We show on a synthetic experiment that the reduction in the number ofparameters brings about significant benefits in terms of performance.
arxiv-1604-07547 | Towards Miss Universe Automatic Prediction: The Evening Gown Competition |  http://arxiv.org/abs/1604.07547  | author:Johanna Carvajal, Arnold Wiliem, Conrad Sanderson, Brian Lovell category:cs.CV cs.CY cs.MM 68T45 published:2016-04-26 summary:Can we predict the winner of Miss Universe after watching how they strodedown the catwalk during the evening gown competition? Fashion gurus say theycan! In our work, we study this question from the perspective of computervision. In particular, we want to understand whether existing computer visionapproaches can be used to automatically extract the qualities exhibited by theMiss Universe winners during their catwalk. This study could pave the waytowards new vision-based applications for the fashion industry. To this end, wepropose a novel video dataset, called the Miss Universe dataset, comprising 10years of the evening gown competition selected between 1996-2010. We furtherpropose two ranking-related problems: (1) the Miss Universe Listwise Rankingand (2) the Miss Universe Pairwise Ranking problems. In addition, we alsodevelop an approach that simultaneously addresses the two proposed problems. Todescribe the videos we employ the recently proposed Stacked Fisher Vectors inconjunction with robust local spatio-temporal features. From our evaluation wefound that although the addressed problems are extremely challenging, theproposed system is able to rank the winner in the top 3 best predicted scoresfor 5 out of 10 Miss Universe competitions.
arxiv-1604-07866 | Learning by tracking: Siamese CNN for robust target association |  http://arxiv.org/abs/1604.07866  | author:Laura Leal-Taixé, Cristian Canton-Ferrer, Konrad Schindler category:cs.LG cs.CV published:2016-04-26 summary:This paper introduces a novel approach to the task of data association withinthe context of pedestrian tracking, by introducing a two-stage learning schemeto match pairs of detections. First, a Siamese convolutional neural network(CNN) is trained to learn descriptors encoding local spatio-temporal structuresbetween the two input image patches, aggregating pixel values and optical flowinformation. Second, a set of contextual features derived from the position andsize of the compared input patches are combined with the CNN output by means ofa gradient boosting classifier to generate the final matching probability. Thislearning approach is validated by using a linear programming based multi-persontracker showing that even a simple and efficient tracker may outperform muchmore complex models when fed with our learned matching probabilities. Resultson publicly available sequences show that our method meets state-of-the-artstandards in multiple people tracking.
arxiv-1604-07554 | A New Approach in Persian Handwritten Letters Recognition Using Error Correcting Output Coding |  http://arxiv.org/abs/1604.07554  | author:Maziar Kazemi, Muhammad Yousefnezhad, Saber Nourian category:cs.CV cs.LG stat.ML published:2016-04-26 summary:Classification Ensemble, which uses the weighed polling of outputs, is theart of combining a set of basic classifiers for generating high-performance,robust and more stable results. This study aims to improve the results ofidentifying the Persian handwritten letters using Error Correcting OutputCoding (ECOC) ensemble method. Furthermore, the feature selection is used toreduce the costs of errors in our proposed method. ECOC is a method fordecomposing a multi-way classification problem into many binary classificationtasks; and then combining the results of the subtasks into a hypothesizedsolution to the original problem. Firstly, the image features are extracted byPrincipal Components Analysis (PCA). After that, ECOC is used foridentification the Persian handwritten letters which it uses Support VectorMachine (SVM) as the base classifier. The empirical results of applying thisensemble method using 10 real-world data sets of Persian handwritten lettersindicate that this method has better results in identifying the Persianhandwritten letters than other ensemble methods and also singleclassifications. Moreover, by testing a number of different features, thispaper found that we can reduce the additional cost in feature selection stageby using this method.
arxiv-1604-07751 | Compressive phase-only filtering - pattern recognition at extreme compression rates |  http://arxiv.org/abs/1604.07751  | author:David Pastor-Calle, Anna Pastuszczak, Michal Mikolajczyk, Rafal Kotynski category:cs.CV physics.optics published:2016-04-26 summary:We introduce a compressive pattern recognition method for non-adaptiveWalsh-Hadamard or discrete noiselet-based compressive measurements and showthat images measured at extremely high compression rates may still containsufficient information for pattern recognition and target localization. Wereport on a compressive pattern recognition experiment with a single-pixeldetector with which we validate the proposed method. The correlation signalsproduced with the phase-only matched filter or with the pure-phase correlationare obtained from the compressive measurements through lasso optimizationwithout the need to reconstruct the original image. This is possible owing tothe two properties of phase-only filtering: such filtering is a unitarycirculant transform, and the correlation plane it produces in patternrecognition applications is usually sparse.
arxiv-1604-07602 | Spot On: Action Localization from Pointly-Supervised Proposals |  http://arxiv.org/abs/1604.07602  | author:Pascal Mettes, Jan C. van Gemert, Cees G. M. Snoek category:cs.CV published:2016-04-26 summary:We strive for spatio-temporal localization of actions in videos. Thestate-of-the-art relies on action proposals at test time and selects the bestone with a classifier demanding carefully annotated box annotations at traintime. Annotating action boxes in video is cumbersome, tedious, and error prone.Rather than annotating boxes, we propose to annotate actions in video withpoints on a sparse subset of frames only. We introduce an overlap measurebetween action proposals and points and incorporate them all into the objectiveof a non-convex Multiple Instance Learning optimization. Experimentalevaluation on the UCF Sports and UCF 101 datasets shows that (i)spatio-temporal proposals can be used to train classifiers while retaining thelocalization performance, (ii) point annotations yield results comparable tobox annotations while being significantly faster to annotate, (iii) with aminimum amount of supervision our approach is competitive to thestate-of-the-art. Finally, we introduce spatio-temporal action annotations onthe train and test videos of Hollywood2, resulting in Hollywood2Tubes,available at tinyurl.com/hollywood2tubes.
arxiv-1604-07638 | Online Influence Maximization in Non-Stationary Social Networks |  http://arxiv.org/abs/1604.07638  | author:Yixin Bao, Xiaoke Wang, Zhi Wang, Chuan Wu, Francis C. M. Lau category:cs.SI cs.DS cs.LG published:2016-04-26 summary:Social networks have been popular platforms for information propagation. Animportant use case is viral marketing: given a promotion budget, an advertisercan choose some influential users as the seed set and provide them free ordiscounted sample products; in this way, the advertiser hopes to increase thepopularity of the product in the users' friend circles by the world-of-moutheffect, and thus maximizes the number of users that information of theproduction can reach. There has been a body of literature studying theinfluence maximization problem. Nevertheless, the existing studies mostlyinvestigate the problem on a one-off basis, assuming fixed known influenceprobabilities among users, or the knowledge of the exact social networktopology. In practice, the social network topology and the influenceprobabilities are typically unknown to the advertiser, which can be varyingover time, i.e., in cases of newly established, strengthened or weakened socialties. In this paper, we focus on a dynamic non-stationary social network anddesign a randomized algorithm, RSB, based on multi-armed bandit optimization,to maximize influence propagation over time. The algorithm produces a sequenceof online decisions and calibrates its explore-exploit strategy utilizingoutcomes of previous decisions. It is rigorously proven to achieve anupper-bounded regret in reward and applicable to large-scale social networks.Practical effectiveness of the algorithm is evaluated using both synthetic andreal-world datasets, which demonstrates that our algorithm outperforms previousstationary methods under non-stationary conditions.
arxiv-1604-07666 | $\ell_p$-Box ADMM: A Versatile Framework for Integer Programming |  http://arxiv.org/abs/1604.07666  | author:Baoyuan Wu, Bernard Ghanem category:cs.CV cs.DS published:2016-04-26 summary:This paper revisits the integer programming (IP) problem, which plays afundamental role in many computer vision and machine learning applications. Theliterature abounds with many seminal works that address this problem, somefocusing on continuous approaches (e.g. linear program relaxation) while otherson discrete ones (e.g., min-cut). However, a limited number of them aredesigned to handle the general IP form and even these methods cannot adequatelysatisfy the simultaneous requirements of accuracy, feasibility, andscalability. To this end, we propose a novel and versatile framework called$\ell_p$-box ADMM, which is based on two parts. (1) The discrete constraint isequivalently replaced by the intersection of a box and a $(n-1)$-dimensionalsphere (defined through the $\ell_p$ norm). (2) We infuse this equivalence intothe ADMM (Alternating Direction Method of Multipliers) framework to handlethese continuous constraints separately and to harness its attractiveproperties. More importantly, the ADMM update steps can lead to manageablesub-problems in the continuous domain. To demonstrate its efficacy, we consideran instance of the framework, namely $\ell_2$-box ADMM applied to binaryquadratic programming (BQP). Here, the ADMM steps are simple, computationallyefficient, and theoretically guaranteed to converge to a KKT point. Wedemonstrate the applicability of $\ell_2$-box ADMM on three importantapplications: MRF energy minimization, graph matching, and clustering. Resultsclearly show that it significantly outperforms existing generic IP solvers bothin runtime and objective. It also achieves very competitive performance vs.state-of-the-art methods specific to these applications.
arxiv-1604-07669 | Real-time Action Recognition with Enhanced Motion Vector CNNs |  http://arxiv.org/abs/1604.07669  | author:Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, Hanli Wang category:cs.CV published:2016-04-26 summary:The deep two-stream architecture exhibited excellent performance on videobased action recognition. The most computationally expensive step in thisapproach comes from the calculation of optical flow which prevents it to bereal-time. This paper accelerates this architecture by replacing optical flowwith motion vector which can be obtained directly from compressed videoswithout extra calculation. However, motion vector lacks fine structures, andcontains noisy and inaccurate motion patterns, leading to the evidentdegradation of recognition performance. Our key insight for relieving thisproblem is that optical flow and motion vector are inherent correlated.Transferring the knowledge learned with optical flow CNN to motion vector CNNcan significantly boost the performance of the latter. Specifically, weintroduce three strategies for this, initialization transfer, supervisiontransfer and their combination. Experimental results show that our methodachieves comparable recognition performance to the state-of-the-art, while ourmethod can process 390.7 frames per second, which is 27 times faster than theoriginal two-stream method.
arxiv-1604-07681 | Efficient Splitting-based Method for Global Image Smoothing |  http://arxiv.org/abs/1604.07681  | author:Youngjung Kim, Dongbo Min, Bumsub Ham, Kwanghoon Sohn category:cs.CV published:2016-04-26 summary:Edge-preserving smoothing (EPS) can be formulated as minimizing an objectivefunction that consists of data and prior terms. This global EPS approach showsbetter smoothing performance than a local one that typically has a form ofweighted averaging, at the price of high computational cost. In this paper, weintroduce a highly efficient splitting-based method for global EPS thatminimizes the objective function of ${l_2}$ data and prior terms (possiblynon-smooth and non-convex) in linear time. Different from previoussplitting-based methods that require solving a large linear system, ourapproach solves an equivalent constrained optimization problem, resulting in asequence of 1D sub-problems. This enables linear time solvers forweighted-least squares and -total variation problems. Our solver convergesquickly, and its runtime is even comparable to state-of-the-art local EPSapproaches. We also propose a family of fast iteratively re-weighted algorithmsusing a non-convex prior term. Experimental results demonstrate theeffectiveness and flexibility of our approach in a range of computer vision andimage processing tasks.
arxiv-1604-07704 | Tournament selection in zeroth-level classifier systems based on average reward reinforcement learning |  http://arxiv.org/abs/1604.07704  | author:Zhaoxiang Zang, Zhao Li, Junying Wang, Zhiping Dan category:cs.AI cs.NE I.2 published:2016-04-26 summary:As a genetics-based machine learning technique, zeroth-level classifiersystem (ZCS) is based on a discounted reward reinforcement learning algorithm,bucket-brigade algorithm, which optimizes the discounted total reward receivedby an agent but is not suitable for all multi-step problems, especiallylarge-size ones. There are some undiscounted reinforcement learning methodsavailable, such as R-learning, which optimize the average reward per time step.In this paper, R-learning is used as the reinforcement learning employed byZCS, to replace its discounted reward reinforcement learning approach, andtournament selection is used to replace roulette wheel selection in ZCS. Themodification results in classifier systems that can support long action chains,and thus is able to solve large multi-step problems.
arxiv-1604-07807 | An Enhanced Deep Feature Representation for Person Re-identification |  http://arxiv.org/abs/1604.07807  | author:Shangxuan Wu, Ying-Cong Chen, Xiang Li, An-Cong Wu, Jin-Jie You, Wei-Shi Zheng category:cs.CV published:2016-04-26 summary:Feature representation and metric learning are two critical components inperson re-identification models. In this paper, we focus on the featurerepresentation and claim that hand-crafted histogram features can becomplementary to Convolutional Neural Network (CNN) features. We propose anovel feature extraction model called Feature Fusion Net (FFN) for pedestrianimage representation. In FFN, back propagation makes CNN features constrainedby the handcrafted features. Utilizing color histogram features (RGB, HSV,YCbCr, Lab and YIQ) and texture features (multi-scale and multi-orientationGabor features), we get a new deep feature representation that is morediscriminative and compact. Experiments on three challenging datasets (VIPeR,CUHK01, PRID450s) validates the effectiveness of our proposal.
arxiv-1604-07484 | Deep Multi-fidelity Gaussian Processes |  http://arxiv.org/abs/1604.07484  | author:Maziar Raissi, George Karniadakis category:cs.LG stat.ML published:2016-04-26 summary:We develop a novel multi-fidelity framework that goes far beyond theclassical AR(1) Co-kriging scheme of Kennedy and O'Hagan (2000). Our method canhandle general discontinuous cross-correlations among systems with differentlevels of fidelity. A combination of multi-fidelity Gaussian Processes (AR(1)Co-kriging) and deep neural networks enables us to construct a method that isimmune to discontinuities. We demonstrate the effectiveness of the newtechnology using standard benchmark problems designed to resemble the outputsof complicated high- and low-fidelity codes.
arxiv-1604-07796 | Scale Normalization |  http://arxiv.org/abs/1604.07796  | author:Henry Z. Lo, Kevin Amaral, Wei Ding category:cs.NE cs.LG stat.ML published:2016-04-26 summary:One of the difficulties of training deep neural networks is caused byimproper scaling between layers. Scaling issues introduce exploding / gradientproblems, and have typically been addressed by careful scale-preservinginitialization. We investigate the value of preserving scale, or isometry,beyond the initial weights. We propose two methods of maintaing isometry, oneexact and one stochastic. Preliminary experiments show that for bothdeterminant and scale-normalization effectively speeds up learning. Resultssuggest that isometry is important in the beginning of learning, andmaintaining it leads to faster learning.
arxiv-1604-07788 | A Framework for Human Pose Estimation in Videos |  http://arxiv.org/abs/1604.07788  | author:Dong Zhang, Mubarak Shah category:cs.CV published:2016-04-26 summary:In this paper, we present a method to estimate a sequence of human poses inunconstrained videos. We aim to demonstrate that by using temporal information,the human pose estimation results can be improved over image based poseestimation methods. In contrast to the commonly employed graph optimizationformulation, which is NP-hard and needs approximate solutions, we formulatethis problem into a unified two stage tree-based optimization problem for whichan efficient and exact solution exists. Although the proposed method finds anexact solution, it does not sacrifice the ability to model the spatial andtemporal constraints between body parts in the frames; in fact it models the{\em symmetric} parts better than the existing methods. The proposed method isbased on two main ideas: `Abstraction' and `Association' to enforce the intra-and inter-frame body part constraints without inducing extra computationalcomplexity to the polynomial time solution. Using the idea of `Abstraction', anew concept of `abstract body part' is introduced to conceptually combine thesymmetric body parts and model them in the tree based body part structure.Using the idea of `Association', the optimal tracklets are generated for eachabstract body part, in order to enforce the spatiotemporal constraints betweenbody parts in adjacent frames. A sequence of the best poses is inferred fromthe abstract body part tracklets through the tree-based optimization. Finally,the poses are refined by limb alignment and refinement schemes. We evaluatedthe proposed method on three publicly available video based human poseestimation datasets, and obtained dramatically improved performance compared tothe state-of-the-art methods.
arxiv-1604-07499 | Modern Physiognomy: An Investigation on Predicting Personality Traits and Intelligence from the Human Face |  http://arxiv.org/abs/1604.07499  | author:Rizhen Qin, Wei Gao, Huarong Xu, Zhanyi Hu category:cs.CV published:2016-04-26 summary:The human behavior of evaluating other individuals with respect to theirpersonality traits and intelligence by evaluating their faces plays a crucialrole in human relations. These trait judgments might influence important socialoutcomes in our lives such as elections and court sentences. Previous studieshave reported that human can make valid inferences for at least fourpersonality traits. In addition, some studies have demonstrated that facialtrait evaluation can be learned using machine learning methods accurately. Inthis work, we experimentally explore whether self-reported personality traitsand intelligence can be predicted reliably from a facial image. Morespecifically, the prediction problem is separately cast in two parts: aclassification task and a regression task. A facial structural feature isconstructed from the relations among facial salient points, and an appearancefeature is built by five texture descriptors. In addition, a minutia-basedfingerprint feature from a fingerprint image is also explored. Theclassification results show that the personality traits "Rule-consciousness"and "Vigilance" can be predicted reliably, and that the traits of females canbe predicted more accurately than those of male. However, the regressionexperiments show that it is difficult to predict scores for individualpersonality traits and intelligence. The residual plots and the correlationresults indicate no evident linear correlation between the measured scores andthe predicted scores. Both the classification and the regression results revealthat "Rule-consciousness" and "Tension" can be reliably predicted from thefacial features, while "Social boldness" gets the worst prediction results. Theexperiments results show that it is difficult to predict intelligence fromeither the facial features or the fingerprint feature, a finding that is inagreement with previous studies.
arxiv-1604-07528 | Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification |  http://arxiv.org/abs/1604.07528  | author:Tong Xiao, Hongsheng Li, Wanli Ouyang, Xiaogang Wang category:cs.CV published:2016-04-26 summary:Learning generic and robust feature representations with data from multipledomains for the same problem is of great value, especially for the problemsthat have multiple datasets but none of them are large enough to provideabundant data variations. In this work, we present a pipeline for learning deepfeature representations from multiple domains with Convolutional NeuralNetworks (CNNs). When training a CNN with data from all the domains, someneurons learn representations shared across several domains, while some othersare effective only for a specific one. Based on this important observation, wepropose a Domain Guided Dropout algorithm to improve the feature learningprocedure. Experiments show the effectiveness of our pipeline and the proposedalgorithm. Our methods on the person re-identification problem outperformstate-of-the-art methods on multiple datasets by large margins.
arxiv-1604-07507 | Once for All: a Two-flow Convolutional Neural Network for Visual Tracking |  http://arxiv.org/abs/1604.07507  | author:Kai Chen, Wenbing Tao category:cs.CV published:2016-04-26 summary:One of the main challenges of visual object tracking comes from the arbitraryappearance of objects. Most existing algorithms try to resolve this problem asan object-specific task, i.e., the model is trained to regenerate or classify aspecific object. As a result, the model need to be initialized and retrainedfor different objects. In this paper, we propose a more generic approachutilizing a novel two-flow convolutional neural network (named YCNN). The YCNNtakes two inputs (one is object image patch, the other is search image patch),then outputs a response map which predicts how likely the object appears in aspecific location. Unlike those object-specific approach, the YCNN is trainedto measure the similarity between two image patches. Thus it will not beconfined to any specific object. Furthermore the network can be end-to-endtrained to extract both shallow and deep convolutional features which arededicated for visual tracking. And once properly trained, the YCNN can beapplied to track all kinds of objects without further training and updating.Benefiting from the once-for-all model, our algorithm is able to run at a veryhigh speed of 45 frames-per-second. The experiments on 51 sequences also showthat our algorithm achieves an outstanding performance.
arxiv-1604-07741 | EgoSampling: Wide View Hyperlapse from Single and Multiple Egocentric Videos |  http://arxiv.org/abs/1604.07741  | author:Tavi Halperin, Yair Poleg, Chetan Arora, Shmuel Peleg category:cs.CV cs.MM published:2016-04-26 summary:The possibility of sharing one's point of view makes use of wearable camerascompelling. These videos are often long, boring and coupled with extreme shakeas the camera is worn on a moving person. Fast forwarding (i.e. frame sampling)is a natural choice for faster video browsing. However, this accentuates theshake caused by natural head motion in an egocentric video, making the fastforwarded video useless. We propose EgoSampling, an adaptive frame samplingthat gives more stable, fast forwarded, hyperlapse videos. Adaptive framesampling is formulated as energy minimization, whose optimal solution can befound in polynomial time. We further turn the camera shake from a drawback intoa feature, enabling the increase of the field-of-view. This is obtained wheneach output frame is mosaiced from several input frames. Stitching multipleframes also enables the generation of a single hyperlapse video from multipleegocentric videos, allowing even faster video consumption.
arxiv-1604-07806 | Using Indirect Encoding of Multiple Brains to Produce Multimodal Behavior |  http://arxiv.org/abs/1604.07806  | author:Jacob Schrum, Joel Lehman, Sebastian Risi category:cs.AI cs.NE published:2016-04-26 summary:An important challenge in neuroevolution is to evolve complex neural networkswith multiple modes of behavior. Indirect encodings can potentially answer thischallenge. Yet in practice, indirect encodings do not yield effectivemultimodal controllers. Thus, this paper introduces novel multimodal extensionsto HyperNEAT, a popular indirect encoding. A previous multimodal HyperNEATapproach called situational policy geometry assumes that multiple brainsbenefit from being embedded within an explicit geometric space. However,experiments here illustrate that this assumption unnecessarily constrainsevolution, resulting in lower performance. Specifically, this paper introducesHyperNEAT extensions for evolving many brains without assuming geometricrelationships between them. The resulting Multi-Brain HyperNEAT can exploithuman-specified task divisions to decide when each brain controls the agent, orcan automatically discover when brains should be used, by means of preferenceneurons. A further extension called module mutation allows evolution todiscover the number of brains, enabling multimodal behavior with even lessexpert knowledge. Experiments in several multimodal domains highlight thatmulti-brain approaches are more effective than HyperNEAT without multimodalextensions, and show that brains without a geometric relation to each otheroutperform situational policy geometry. The conclusion is that Multi-BrainHyperNEAT provides several promising techniques for evolving complex multimodalbehavior.
arxiv-1604-07407 | Conversational Markers of Constructive Discussions |  http://arxiv.org/abs/1604.07407  | author:Vlad Niculae, Cristian Danescu-Niculescu-Mizil category:cs.CL cs.AI cs.SI physics.soc-ph stat.ML published:2016-04-25 summary:Group discussions are essential for organizing every aspect of modern life,from faculty meetings to senate debates, from grant review panels to papalconclaves. While costly in terms of time and organization effort, groupdiscussions are commonly seen as a way of reaching better decisions compared tosolutions that do not require coordination between the individuals (e.g.voting)---through discussion, the sum becomes greater than the parts. However,this assumption is not irrefutable: anecdotal evidence of wasteful discussionsabounds, and in our own experiments we find that over 30% of discussions areunproductive. We propose a framework for analyzing conversational dynamics in order todetermine whether a given task-oriented discussion is worth having or not. Weexploit conversational patterns reflecting the flow of ideas and the balancebetween the participants, as well as their linguistic choices. We apply thisframework to conversations naturally occurring in an online collaborative worldexploration game developed and deployed to support this research. Using thissetting, we show that linguistic cues and conversational patterns extractedfrom the first 20 seconds of a team discussion are predictive of whether itwill be a wasteful or a productive one.
arxiv-1604-07209 | Unbiased Comparative Evaluation of Ranking Functions |  http://arxiv.org/abs/1604.07209  | author:Tobias Schnabel, Adith Swaminathan, Peter Frazier, Thorsten Joachims category:cs.IR cs.LG published:2016-04-25 summary:Eliciting relevance judgments for ranking evaluation is labor-intensive andcostly, motivating careful selection of which documents to judge. Unliketraditional approaches that make this selection deterministically,probabilistic sampling has shown intriguing promise since it enables the designof estimators that are provably unbiased even when reusing data with missingjudgments. In this paper, we first unify and extend these sampling approachesby viewing the evaluation problem as a Monte Carlo estimation task that appliesto a large number of common IR metrics. Drawing on the theoretical clarity thatthis view offers, we tackle three practical evaluation scenarios: comparing twosystems, comparing $k$ systems against a baseline, and ranking $k$ systems. Foreach scenario, we derive an estimator and a variance-optimizing samplingdistribution while retaining the strengths of sampling-based evaluation,including unbiasedness, reusability despite missing data, and ease of use inpractice. In addition to the theoretical contribution, we empirically evaluateour methods against previously used sampling heuristics and find that theygenerally cut the number of required relevance judgments at least in half.
arxiv-1604-07316 | End to End Learning for Self-Driving Cars |  http://arxiv.org/abs/1604.07316  | author:Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba category:cs.CV cs.LG cs.NE published:2016-04-25 summary:We trained a convolutional neural network (CNN) to map raw pixels from asingle front-facing camera directly to steering commands. This end-to-endapproach proved surprisingly powerful. With minimum training data from humansthe system learns to drive in traffic on local roads with or without lanemarkings and on highways. It also operates in areas with unclear visualguidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessaryprocessing steps such as detecting useful road features with only the humansteering angle as the training signal. We never explicitly trained it todetect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane markingdetection, path planning, and control, our end-to-end system optimizes allprocessing steps simultaneously. We argue that this will eventually lead tobetter performance and smaller systems. Better performance will result becausethe internal components self-optimize to maximize overall system performance,instead of optimizing human-selected intermediate criteria, e.g., lanedetection. Such criteria understandably are selected for ease of humaninterpretation which doesn't automatically guarantee maximum systemperformance. Smaller networks are possible because the system learns to solvethe problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PXself-driving car computer also running Torch 7 for determining where to drive.The system operates at 30 frames per second (FPS).
arxiv-1604-07319 | Semi-supervised Dictionary Learning Based on Hilbert-Schmidt Independence Criterion |  http://arxiv.org/abs/1604.07319  | author:Mehrdad J. Gangeh, Safaa M. A. Bedawi, Ali Ghodsi, Fakhri Karray category:cs.CV published:2016-04-25 summary:In this paper, a novel semi-supervised dictionary learning and sparserepresentation (SS-DLSR) is proposed. The proposed method benefits from thesupervisory information by learning the dictionary in a space where thedependency between the data and class labels is maximized. This maximization isperformed using Hilbert-Schmidt independence criterion (HSIC). On the otherhand, the global distribution of the underlying manifolds were learned from theunlabeled data by minimizing the distances between the unlabeled data and thecorresponding nearest labeled data in the space of the dictionary learned. Theproposed SS-DLSR algorithm has closed-form solutions for both the dictionaryand sparse coefficients, and therefore does not have to learn the twoiteratively and alternately as is common in the literature of the DLSR. Thismakes the solution for the proposed algorithm very fast. The experimentsconfirm the improvement in classification performance on benchmark datasets byincluding the information from both labeled and unlabeled data, particularlywhen there are many unlabeled data.
arxiv-1604-07335 | Scalable Gaussian Processes for Supervised Hashing |  http://arxiv.org/abs/1604.07335  | author:Bahadir Ozdemir, Larry S. Davis category:cs.CV published:2016-04-25 summary:We propose a flexible procedure for large-scale image search by hashfunctions with kernels. Our method treats binary codes and pairwise semanticsimilarity as latent and observed variables, respectively, in a probabilisticmodel based on Gaussian processes for binary classification. We present anefficient inference algorithm with the sparse pseudo-input Gaussian process(SPGP) model and parallelization. Experiments on three large-scale imagedataset demonstrate the effectiveness of the proposed hashing method, GaussianProcess Hashing (GPH), for short binary codes and the datasets withoutpredefined classes in comparison to the state-of-the-art supervised hashingmethods.
arxiv-1604-07243 | Expectation Maximization for Sum-Product Networks as Exponential Family Mixture Models |  http://arxiv.org/abs/1604.07243  | author:Mattia Desana, Christoph Schnörr category:cs.LG published:2016-04-25 summary:Sum-Product Networks (SPNs) are a recent class of probabilistic models whichencode very large mixtures compactly by exploiting efficient reuse ofcomputation in inference. Crucially, in SPNs the cost of inference scaleslinearly with the number of edges $E$ but the encoded mixture size $C$ can beexponentially larger than $E$. In this paper we obtain an efficient ($O(E)$)implementation of Expectation Maximization (EM) for SPNs which is the first toinclude EM updates both on mixture coefficients (corresponding to SPN weights)and mixture components (corresponding to SPN leaves). In particular, the updateon mixture components translates to a weighted maximum likelihood problem onleaf distributions, and can be solved exactly when leaves are in theexponential family. This opens new application areas for SPNs, such as learninglarge mixtures of tree graphical models. We validate the algorithm on asynthetic but non trivial "soft-parity" distribution with $2^{n}$ modes encodedby a SPN with only $O(n)$ edges.
arxiv-1604-07342 | Incremental Hashing with Kernels |  http://arxiv.org/abs/1604.07342  | author:Bahadir Ozdemir, Mahyar Najibi, Larry S. Davis category:cs.CV published:2016-04-25 summary:We propose an incremental strategy for learning hash functions with kernelsfor large-scale image search. Our method is based on a two-stage classificationframework that treats binary codes as intermediate variables between thefeature space and the semantic space. In the first stage of classification,binary codes are considered as class labels by a set of binary SVMs; eachcorresponds to one bit. In the second stage, binary codes become the inputspace of a multi-class SVM. Hash functions are learned by an efficientalgorithm where the NP-hard problem of finding optimal binary codes is solvedvia cyclic coordinate descent and SVMs are trained in a parallelizedincremental manner. For modifications like adding images from an unseen class,we describe an incremental procedure for effective and efficient updates to theprevious hash functions. Experiments on three large-scale image datasetsdemonstrate the effectiveness of the proposed hashing method, SVM-based Hashing(SVM-Hash), over the state-of-the-art supervised hashing methods.
arxiv-1604-07356 | Fast nonlinear embeddings via structured matrices |  http://arxiv.org/abs/1604.07356  | author:Krzysztof Choromanski, Francois Fagan category:stat.ML cs.LG G.3 published:2016-04-25 summary:We present a new paradigm for speeding up randomized computations of severalfrequently used functions in machine learning. In particular, our paradigm canbe applied for improving computations of kernels based on random embeddings.Above that, the presented framework covers multivariate randomized functions.As a byproduct, we propose an algorithmic approach that also leads to asignificant reduction of space complexity. Our method is based on carefulrecycling of Gaussian vectors into structured matrices that share properties offully random matrices. The quality of the proposed structured approach followsfrom combinatorial properties of the graphs encoding correlations between rowsof these structured matrices. Our framework covers as special cases alreadyknown structured approaches such as the Fast Johnson-Lindenstrauss Transform,but is much more general since it can be applied also to highly nonlinearembeddings. We provide strong concentration results showing the quality of thepresented paradigm.
arxiv-1604-07360 | Attributes for Improved Attributes: A Multi-Task Network for Attribute Classification |  http://arxiv.org/abs/1604.07360  | author:Emily M. Hand, Rama Chellappa category:cs.CV published:2016-04-25 summary:Attributes, or semantic features, have gained popularity in the past fewyears in domains ranging from activity recognition in video to faceverification. Improving the accuracy of attribute classifiers is an importantfirst step in any application which uses these attributes. In most works todate, attributes have been considered to be independent. However, we know thisnot to be the case. Many attributes are very strongly related, such as heavymakeup and wearing lipstick. We propose to take advantage of attributerelationships in three ways: by using a multi-task deep convolutional neuralnetwork (MCNN) sharing the lowest layers amongst all attributes, sharing thehigher layers for related attributes, and by building an auxiliary network ontop of the MCNN which utilizes the scores from all attributes to improve thefinal classification of each attribute. We demonstrate the effectiveness of ourmethod by producing results on two challenging publicly available datasets.
arxiv-1604-07236 | Towards Real-Time, Country-Level Location Classification of Worldwide Tweets |  http://arxiv.org/abs/1604.07236  | author:Arkaitz Zubiaga, Alex Voss, Rob Procter, Maria Liakata, Bo Wang, Adam Tsakalidis category:cs.IR cs.CL cs.SI published:2016-04-25 summary:With the increase of interest in using social media as a source for research,many have tackled the task of automatically geolocating tweets, motivated bythe lack of explicit location information in the majority of tweets. Whileothers have focused on state- or city-level classification of tweets restrictedto a specific country, here we undertake the task in a broader context byclassifying global tweets at the country level, so far unexplored in areal-time scenario. We analyse the extent to which a tweet's country of origincan be determined by making use of eight tweet-inherent features forclassification using Support Vector Machines. Furthermore, we use two datasets,collected a year apart from each other, to analyse the extent to which a modeltrained from historical tweets can still be leveraged for classification of newtweets. With classification experiments on all 217 countries in our datasets,as well as on the top 25 countries, we offer some insights into the best use oftweet-inherent features for an accurate country-level classification of tweets.Among the features inherent in a tweet, we observe that the validity ofhistorical tweet content fades over time, and other metadata associated withthe tweet, such as the language of the tweet, the name of the user, or the timezone in which the user is located, lead to more accurate classification. Whileno feature set is optimal for all countries, and each country needs to betreated differently, we show that remarkably high performance values above 0.9in terms of F1 score can be achieved for countries with unique characteristicssuch as those having a language that is not spoken in many other countries or aunique time zone. However, the difficulty of achieving an accurateclassification increases for countries with multiple commonalities, especiallyfor English and Spanish speaking countries.
arxiv-1604-07451 | Learning Local Dependence In Ordered Data |  http://arxiv.org/abs/1604.07451  | author:Guo Yu, Jacob Bien category:math.ST stat.CO stat.ME stat.ML stat.TH published:2016-04-25 summary:In many applications, data comes with a natural ordering. This ordering canoften induce local dependence among nearby variables. However, in complex data,the width of this dependence may vary, making simple assumptions such as aconstant neighborhood size unrealistic. We propose a framework for learningthis local dependence based on estimating the inverse of the Cholesky factor ofthe covariance matrix. Penalized maximum likelihood estimation of this matrixyields a simple regression interpretation for local dependence in whichvariables are predicted by their neighbors. Our proposed method involvessolving a convex, penalized Gaussian likelihood problem with a hierarchicalgroup lasso penalty. The problem decomposes into independent subproblems whichcan be solved efficiently in parallel using first-order methods. Our methodyields a sparse, symmetric, positive definite estimator of the precisionmatrix, encoding a Gaussian graphical model. We derive theoretical results notfound in existing methods attaining this structure. In particular, ourconditions for signed support recovery and estimation consistency rates inmultiple norms are as mild as those in a regression problem. Empirical resultsshow our method performing favorably compared to existing methods. We apply ourmethod to genomic data to flexibly model linkage disequilibrium.
arxiv-1604-07457 | Modeling the Contribution of Central Versus Peripheral Vision in Scene, Object, and Face Recognition |  http://arxiv.org/abs/1604.07457  | author:Panqu Wang, Garrison Cottrell category:q-bio.NC cs.CV published:2016-04-25 summary:It is commonly believed that the central visual field is important forrecognizing objects and faces, and the peripheral region is useful for scenerecognition. However, the relative importance of central versus peripheralinformation for object, scene, and face recognition is unclear. In a behavioralstudy, Larson and Loschky (2009) investigated this question by measuring thescene recognition accuracy as a function of visual angle, and demonstrated thatperipheral vision was indeed more useful in recognizing scenes than centralvision. In this work, we modeled and replicated the result of Larson andLoschky (2009), using deep convolutional neural networks. Having fit the datafor scenes, we used the model to predict future data for large-scale scenerecognition as well as for objects and faces. Our results suggest that therelative order of importance of using central visual field information is facerecognition>object recognition>scene recognition, and vice-versa for peripheralinformation.
arxiv-1604-07102 | Makeup like a superstar: Deep Localized Makeup Transfer Network |  http://arxiv.org/abs/1604.07102  | author:Si Liu, Xinyu Ou, Ruihe Qian, Wei Wang, Xiaochun Cao category:cs.CV cs.AI published:2016-04-25 summary:In this paper, we propose a novel Deep Localized Makeup Transfer Network toautomatically recommend the most suitable makeup for a female and synthesis themakeup on her face. Given a before-makeup face, her most suitable makeup isdetermined automatically. Then, both the beforemakeup and the reference facesare fed into the proposed Deep Transfer Network to generate the after-makeupface. Our end-to-end makeup transfer network have several nice propertiesincluding: (1) with complete functions: including foundation, lip gloss, andeye shadow transfer; (2) cosmetic specific: different cosmetics are transferredin different manners; (3) localized: different cosmetics are applied ondifferent facial regions; (4) producing naturally looking results withoutobvious artifacts; (5) controllable makeup lightness: various results fromlight makeup to heavy makeup can be generated. Qualitative and quantitativeexperiments show that our network performs much better than the methods of [Guoand Sim, 2009] and two variants of NerualStyle [Gatys et al., 2015a].
arxiv-1604-07463 | Dynamic Pricing with Demand Covariates |  http://arxiv.org/abs/1604.07463  | author:Sheng Qiang, Mohsen Bayati category:stat.ML published:2016-04-25 summary:We consider a firm that sells products over $T$ periods without knowing thedemand function. The firm sequentially sets prices to earn revenue and to learnthe underlying demand function simultaneously. A natural heuristic for thisproblem, commonly used in practice, is greedy iterative least squares (GILS).At each time period, GILS estimates the demand as a linear function of theprice by applying least squares to the set of prior prices and realizeddemands. Then a price that maximizes the revenue, given the estimated demandfunction, is used for the next time period. The performance is measured by theregret, which is the expected revenue loss from the optimal (oracle) pricingpolicy when the demand function is known. Recently, den Boer and Zwart (2014)and Keskin and Zeevi (2014) demonstrated that GILS is sub-optimal. Theyintroduced algorithms which integrate forced price dispersion with GILS andachieve asymptotically optimal performance. In this paper, we consider this dynamic pricing problem in a data-richenvironment. In particular, we assume that the firm knows the expected demandunder a particular price from historical data, and in each period, beforesetting the price, the firm has access to extra information (demand covariates)which may be predictive of the demand. We prove that in this setting GILSachieves asymptotically optimal regret of order $\log(T)$. We also show thefollowing surprising result: in the original dynamic pricing problem of denBoer and Zwart (2014) and Keskin and Zeevi (2014), inclusion of any set ofcovariates in GILS as potential demand covariates (even though they could carryno information) would make GILS asymptotically optimal. We validate our resultsvia extensive numerical simulations on synthetic and real data sets.
arxiv-1604-07180 | Observing and Recommending from a Social Web with Biases |  http://arxiv.org/abs/1604.07180  | author:Steffen Staab, Sophie Stalla-Bourdillon, Laura Carmichael category:cs.DB cs.LG K.5.0; H.2.8 published:2016-04-25 summary:The research question this report addresses is: how, and to what extent,those directly involved with the design, development and employment of aspecific black box algorithm can be certain that it is not unlawfullydiscriminating (directly and/or indirectly) against particular persons withprotected characteristics (e.g. gender, race and ethnicity)?
arxiv-1604-07379 | Context Encoders: Feature Learning by Inpainting |  http://arxiv.org/abs/1604.07379  | author:Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros category:cs.CV cs.AI cs.GR cs.LG published:2016-04-25 summary:We present an unsupervised visual feature learning algorithm driven bycontext-based pixel prediction. By analogy with auto-encoders, we proposeContext Encoders -- a convolutional neural network trained to generate thecontents of an arbitrary image region conditioned on its surroundings. In orderto succeed at this task, context encoders need to both understand the contentof the entire image, as well as produce a plausible hypothesis for the missingpart(s). When training context encoders, we have experimented with both astandard pixel-wise reconstruction loss, as well as a reconstruction plus anadversarial loss. The latter produces much sharper results because it canbetter handle multiple modes in the output. We found that a context encoderlearns a representation that captures not just appearance but also thesemantics of visual structures. We quantitatively demonstrate the effectivenessof our learned features for CNN pre-training on classification, detection, andsegmentation tasks. Furthermore, context encoders can be used for semanticinpainting tasks, either stand-alone or as initialization for non-parametricmethods.
