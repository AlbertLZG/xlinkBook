arxiv-18000-1 | Gearbox Fault Detection through PSO Exact Wavelet Analysis and SVM Classifier | http://arxiv.org/pdf/1605.04874v1.pdf | author:Amir Hosein Zamanian, Abdolreza Ohadi category:cs.LG published:2016-05-12 summary:Time-frequency methods for vibration-based gearbox faults detection have beenconsidered the most efficient method. Among these methods, continuous wavelettransform (CWT) as one of the best time-frequency method has been used for bothstationary and transitory signals. Some deficiencies of CWT are problem ofoverlapping and distortion ofsignals. In this condition, a large amount ofredundant information exists so that it may cause false alarm ormisinterpretation of the operator. In this paper a modified method called ExactWavelet Analysis is used to minimize the effects of overlapping and distortionin case of gearbox faults. To implement exact wavelet analysis, Particle SwarmOptimization (PSO) algorithm has been used for this purpose. This method havebeen implemented for the acceleration signals from 2D acceleration sensoracquired by Advantech PCI-1710 card from a gearbox test setup in AmirkabirUniversity of Technology. Gearbox has been considered in both healthy andchipped tooth gears conditions. Kernelized Support Vector Machine (SVM) withradial basis functions has used the extracted features from exact waveletanalysis for classification. The efficiency of this classifier is thenevaluated with the other signals acquired from the setup test. The results showthat in comparison of CWT, PSO Exact Wavelet Transform has better ability infeature extraction in price of more computational effort. In addition, PSOexact wavelet has better speed comparing to Genetic Algorithm (GA) exactwavelet in condition of equal population because of factoring mutation andcrossover in PSO algorithm. SVM classifier with the extracted features ingearbox shows very good results and its ability has been proved.
arxiv-18000-2 | A Gaussian Mixture MRF for Model-Based Iterative Reconstruction with Applications to Low-Dose X-ray CT | http://arxiv.org/pdf/1605.04006v1.pdf | author:Ruoqiao Zhang, Dong Hye Ye, Debashish Pal, Jean-Baptiste Thibault, Ken D. Sauer, Charles A. Bouman category:cs.CV math.OC physics.med-ph published:2016-05-12 summary:Markov random fields (MRFs) have been widely used as prior models in variousinverse problems such as tomographic reconstruction. While MRFs provide asimple and often effective way to model the spatial dependencies in images,they suffer from the fact that parameter estimation is difficult. In practice,this means that MRFs typically have very simple structure that cannotcompletely capture the subtle characteristics of complex images. In this paper, we present a novel Gaussian mixture Markov random field model(GM-MRF) that can be used as a very expressive prior model for inverse problemssuch as denoising and reconstruction. The GM-MRF forms a global image model bymerging together individual Gaussian-mixture models (GMMs) for image patches.In addition, we present a novel analytical framework for computing MAPestimates using the GM-MRF prior model through the construction of surrogatefunctions that result in a sequence of quadratic optimizations. We alsointroduce a simple but effective method to adjust the GM-MRF so as to controlthe sharpness in low- and high-contrast regions of the reconstructionseparately. We demonstrate the value of the model with experiments includingimage denoising and low-dose CT reconstruction.
arxiv-18000-3 | Which Learning Algorithms Can Generalize Identity-Based Rules to Novel Inputs? | http://arxiv.org/pdf/1605.04002v1.pdf | author:Paul Tupper, Bobak Shahriari category:cs.CL published:2016-05-12 summary:We propose a novel framework for the analysis of learning algorithms thatallows us to say when such algorithms can and cannot generalize certainpatterns from training data to test data. In particular we focus on situationswhere the rule that must be learned concerns two components of a stimulus beingidentical. We call such a basis for discrimination an identity-based rule.Identity-based rules have proven to be difficult or impossible for certaintypes of learning algorithms to acquire from limited datasets. This is incontrast to human behaviour on similar tasks. Here we provide a framework forrigorously establishing which learning algorithms will fail at generalizingidentity-based rules to novel stimuli. We use this framework to show that suchalgorithms are unable to generalize identity-based rules to novel inputs unlesstrained on virtually all possible inputs. We demonstrate these resultscomputationally with a multilayer feedforward neural network.
arxiv-18000-4 | Generating Sentences from a Continuous Space | http://arxiv.org/pdf/1511.06349v4.pdf | author:Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio category:cs.LG cs.CL published:2015-11-19 summary:The standard recurrent neural network language model (RNNLM) generatessentences one word at a time and does not work from an explicit global sentencerepresentation. In this work, we introduce and study an RNN-based variationalautoencoder generative model that incorporates distributed latentrepresentations of entire sentences. This factorization allows it to explicitlymodel holistic properties of sentences such as style, topic, and high-levelsyntactic features. Samples from the prior over these sentence representationsremarkably produce diverse and well-formed sentences through simpledeterministic decoding. By examining paths through this latent space, we areable to generate coherent novel sentences that interpolate between knownsentences. We present techniques for solving the difficult learning problempresented by this model, demonstrate its effectiveness in imputing missingwords, explore many interesting properties of the model's latent sentencespace, and present negative results on the use of the model in languagemodeling.
arxiv-18000-5 | Do Prices Coordinate Markets? | http://arxiv.org/pdf/1511.00925v4.pdf | author:Justin Hsu, Jamie Morgenstern, Ryan Rogers, Aaron Roth, Rakesh Vohra category:cs.GT cs.LG published:2015-11-03 summary:Walrasian equilibrium prices can be said to coordinate markets: They supporta welfare optimal allocation in which each buyer is buying bundle of goods thatis individually most preferred. However, this clean story has two caveats.First, the prices alone are not sufficient to coordinate the market, and buyersmay need to select among their most preferred bundles in a coordinated way tofind a feasible allocation. Second, we don't in practice expect to encounterexact equilibrium prices tailored to the market, but instead only approximateprices, somehow encoding "distributional" information about the market. Howwell do prices work to coordinate markets when tie-breaking is not coordinated,and they encode only distributional information? We answer this question. First, we provide a genericity condition such thatfor buyers with Matroid Based Valuations, overdemand with respect toequilibrium prices is at most 1, independent of the supply of goods, even whentie-breaking is done in an uncoordinated fashion. Second, we providelearning-theoretic results that show that such prices are robust to changingthe buyers in the market, so long as all buyers are sampled from the same(unknown) distribution.
arxiv-18000-6 | On the Convergent Properties of Word Embedding Methods | http://arxiv.org/pdf/1605.03956v1.pdf | author:Yingtao Tian, Vivek Kulkarni, Bryan Perozzi, Steven Skiena category:cs.CL published:2016-05-12 summary:Do word embeddings converge to learn similar things over differentinitializations? How repeatable are experiments with word embeddings? Are allword embedding techniques equally reliable? In this paper we propose evaluatingmethods for learning word representations by their consistency acrossinitializations. We propose a measure to quantify the similarity of the learnedword representations under this setting (where they are subject to differentrandom initializations). Our preliminary results illustrate that our metric notonly measures a intrinsic property of word embedding methods but alsocorrelates well with other evaluation metrics on downstream tasks. We believeour methods are is useful in characterizing robustness -- an important propertyto consider when developing new word embedding methods.
arxiv-18000-7 | Competitive analysis of the top-K ranking problem | http://arxiv.org/pdf/1605.03933v1.pdf | author:Xi Chen, Sivakanth Gopi, Jieming Mao, Jon Schneider category:cs.DS cs.IT cs.LG math.IT stat.ML published:2016-05-12 summary:Motivated by applications in recommender systems, web search, social choiceand crowdsourcing, we consider the problem of identifying the set of top $K$items from noisy pairwise comparisons. In our setting, we are non-activelygiven $r$ pairwise comparisons between each pair of $n$ items, where eachcomparison has noise constrained by a very general noise model called thestrong stochastic transitivity (SST) model. We analyze the competitive ratio ofalgorithms for the top-$K$ problem. In particular, we present a linear timealgorithm for the top-$K$ problem which has a competitive ratio of$\tilde{O}(\sqrt{n})$; i.e. to solve any instance of top-$K$, our algorithmneeds at most $\tilde{O}(\sqrt{n})$ times as many samples needed as the bestpossible algorithm for that instance (in contrast, all previous knownalgorithms for the top-$K$ problem have competitive ratios of$\tilde{\Omega}(n)$ or worse). We further show that this is tight: anyalgorithm for the top-$K$ problem has competitive ratio at least$\tilde{\Omega}(\sqrt{n})$.
arxiv-18000-8 | Empirical Similarity for Absent Data Generation in Imbalanced Classification | http://arxiv.org/pdf/1508.01235v2.pdf | author:Arash Pourhabib category:stat.ML cs.LG published:2015-08-05 summary:When the training data in a two-class classification problem is overwhelmedby one class, most classification techniques fail to correctly identify thedata points belonging to the underrepresented class. We proposeSimilarity-based Imbalanced Classification (SBIC) that learns patterns in thetraining data based on an empirical similarity function. To take the imbalancedstructure of the training data into account, SBIC utilizes the concept ofabsent data, i.e. data from the minority class which can help better find theboundary between the two classes. SBIC simultaneously optimizes the weights ofthe empirical similarity function and finds the locations of absent datapoints. As such, SBIC uses an embedded mechanism for synthetic data generationwhich does not modify the training dataset, but alters the algorithm to suitimbalanced datasets. Therefore, SBIC uses the ideas of both major schools ofthoughts in imbalanced classification: Like cost-sensitive approaches SBICoperates on an algorithm level to handle imbalanced structures; and similar tosynthetic data generation approaches, it utilizes the properties of unobserveddata points from the minority class. The application of SBIC to imbalanceddatasets suggests it is comparable to, and in some cases outperforms, othercommonly used classification techniques for imbalanced datasets.
arxiv-18000-9 | An Empirical-Bayes Score for Discrete Bayesian Networks | http://arxiv.org/pdf/1605.03884v1.pdf | author:Marco Scutari category:stat.ML stat.ME published:2016-05-12 summary:Bayesian network structure learning is often performed in a Bayesian setting,by evaluating candidate structures using their posterior probabilities for agiven data set. Score-based algorithms then use those posterior probabilitiesas an objective function and return the maximum a posteriori network as thelearned model. For discrete Bayesian networks, the canonical choice for aposterior score is the Bayesian Dirichlet equivalent uniform (BDeu) marginallikelihood with a uniform (U) graph prior (Heckerman et al., 1995). Itsfavourable theoretical properties descend from assuming a uniform prior both onthe space of the network structures and on the space of the parameters of thenetwork. In this paper, we revisit the limitations of these assumptions; and weintroduce an alternative set of assumptions and the resulting score: theBayesian Dirichlet sparse (BDs) empirical Bayes marginal likelihood with amarginal uniform (MU) graph prior. We evaluate its performance in an extensivesimulation study, showing that MU+BDs is more accurate than U+BDeu both inlearning the structure of the network and in predicting new observations, whilenot being computationally more complex to estimate.
arxiv-18000-10 | Trainlets: Dictionary Learning in High Dimensions | http://arxiv.org/pdf/1602.00212v4.pdf | author:Jeremias Sulam, Boaz Ophir, Michael Zibulevsky, Michael Elad category:cs.CV published:2016-01-31 summary:Sparse representations has shown to be a very powerful model for real worldsignals, and has enabled the development of applications with notableperformance. Combined with the ability to learn a dictionary from signalexamples, sparsity-inspired algorithms are often achieving state-of-the-artresults in a wide variety of tasks. Yet, these methods have traditionally beenrestricted to small dimensions mainly due to the computational constraints thatthe dictionary learning problem entails. In the context of image processing,this implies handling small image patches. In this work we show how toefficiently handle bigger dimensions and go beyond the small patches insparsity-based signal and image processing methods. We build our approach basedon a new cropped wavelet decomposition, which enables a multi-scale analysiswith virtually no border effects. We then employ this as the base dictionarywithin a double sparsity model to enable the training of adaptive dictionaries.To cope with the increase of training data, while at the same time improvingthe training performance, we present an Online Sparse Dictionary Learning(OSDL) algorithm to train this model effectively, enabling it to handlemillions of examples. This work shows that dictionary learning can be up-scaledto tackle a new level of signal dimensions, obtaining large adaptable atomsthat we call trainlets.
arxiv-18000-11 | A New Manifold Distance Measure for Visual Object Categorization | http://arxiv.org/pdf/1605.03865v1.pdf | author:Fengfu Li, Xiayuan Huang, Hong Qiao, Bo Zhang category:cs.CV published:2016-05-12 summary:Manifold distances are very effective tools for visual object recognition.However, most of the traditional manifold distances between images are based onthe pixel-level comparison and thus easily affected by image rotations andtranslations. In this paper, we propose a new manifold distance to model thedissimilarities between visual objects based on the Complex Wavelet StructuralSimilarity (CW-SSIM) index. The proposed distance is more robust to rotationsand translations of images than the traditional manifold distance and theCW-SSIM index based distance. In addition, the proposed distance is combinedwith the $k$-medoids clustering method to derive a new clustering method forvisual object categorization. Experiments on Coil-20, Coil-100 and OlivettiFace Databases show that the proposed distance measure is better for visualobject categorization than both the traditional manifold distances and theCW-SSIM index based distances.
arxiv-18000-12 | An End-to-End System for Unconstrained Face Verification with Deep Convolutional Neural Networks | http://arxiv.org/pdf/1605.02686v2.pdf | author:Jun-Cheng Chen, Rajeev Ranjan, Swami Sankaranarayanan, Amit Kumar, Ching-Hui Chen, Vishal M. Patel, Carlos D. Castillo, Rama Chellappa category:cs.CV published:2016-05-09 summary:Over the last four years, methods based on Deep Convolutional Neural Networks(DCNNs) have shown impressive performance improvements for object detection andrecognition problems. This has been made possible due to the availability oflarge annotated datasets, a better understanding of the non-linear mappingbetween input images and class labels as well as the affordability of GPUs. Inthis paper, we present the design details of a deep learning system forend-to-end unconstrained face verification/recognition. The quantitativeperformance evaluation is conducted using the newly released IARPA JanusBenchmark A (IJB-A), the JANUS Challenge Set 2 (JANUS CS2), and the LFWdataset. The IJB-A dataset includes real-world unconstrained faces of 500subjects with significant pose and illumination variations which are muchharder than the Labeled Faces in the Wild (LFW) and Youtube Face (YTF)datasets. JANUS CS2 is the extended version of IJB-A which contains not onlyall the images/frames of IJB-A but also includes the original videos forevaluating video-based face verification system. Some open issues regardingDCNNs for object recognition problems are then discussed.
arxiv-18000-13 | Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning | http://arxiv.org/pdf/1605.03852v1.pdf | author:Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Chris Dyer category:cs.CL published:2016-05-12 summary:We use Bayesian optimization to learn curricula for word representationlearning, optimizing performance on downstream tasks that depend on the learnedrepresentations as features. The curricula are modeled by a linear rankingfunction which is the scalar product of a learned weight vector and anengineered feature vector that characterizes the different aspects of thecomplexity of each instance in the training corpus. We show that learning thecurriculum improves performance on a variety of downstream tasks over randomorders and in comparison to the natural corpus order.
arxiv-18000-14 | Context-dependent feature analysis with random forests | http://arxiv.org/pdf/1605.03848v1.pdf | author:Antonio Sutera, Gilles Louppe, Vân Anh Huynh-Thu, Louis Wehenkel, Pierre Geurts category:stat.ML cs.LG published:2016-05-12 summary:In many cases, feature selection is often more complicated than identifying asingle subset of input variables that would together explain the output. Theremay be interactions that depend on contextual information, i.e., variables thatreveal to be relevant only in some specific circumstances. In this setting, thecontribution of this paper is to extend the random forest variable importancesframework in order (i) to identify variables whose relevance iscontext-dependent and (ii) to characterize as precisely as possible the effectof contextual information on these variables. The usage and the relevance ofour framework for highlighting context-dependent variables is illustrated onboth artificial and real datasets.
arxiv-18000-15 | Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model | http://arxiv.org/pdf/1605.03835v1.pdf | author:Kyunghyun Cho category:cs.CL cs.LG stat.ML published:2016-05-12 summary:Recent advances in conditional recurrent language modelling have mainlyfocused on network architectures (e.g., attention mechanism), learningalgorithms (e.g., scheduled sampling and sequence-level training) and novelapplications (e.g., image/video description generation, speech recognition,etc.) On the other hand, we notice that decoding algorithms/strategies have notbeen investigated as much, and it has become standard to use greedy or beamsearch. In this paper, we propose a novel decoding strategy motivated by anearlier observation that nonlinear hidden layers of a deep neural networkstretch the data manifold. The proposed strategy is embarrassinglyparallelizable without any communication overhead, while improving an existingdecoding algorithm. We extensively evaluate it with attention-based neuralmachine translation on the task of En->Cz translation.
arxiv-18000-16 | Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning | http://arxiv.org/pdf/1605.03832v1.pdf | author:Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick Littell, David Mortensen, Alan W Black, Lori Levin, Chris Dyer category:cs.CL published:2016-05-12 summary:We introduce polyglot language models, recurrent neural network modelstrained to predict symbol sequences in many different languages using sharedrepresentations of symbols and conditioning on typological information aboutthe language to be predicted. We apply these to the problem of modeling phonesequences---a domain in which universal symbol inventories andcross-linguistically shared feature representations are a natural fit.Intrinsic evaluation on held-out perplexity, qualitative analysis of thelearned representations, and extrinsic evaluation in two downstreamapplications that make use of phonetic features show (i) that polyglot modelsbetter generalize to held-out data than comparable monolingual models and (ii)that polyglot phonetic feature representations are of higher quality than thoselearned monolingually.
arxiv-18000-17 | Crowd Pedestrian Counting Considering Network Flow Constraints in Videos | http://arxiv.org/pdf/1605.03821v1.pdf | author:Liqing Gao, Yanzhang Wang, Xin Ye, Jian Wang category:cs.CV published:2016-05-12 summary:A quadratic programming method with network flow constraints is proposed toimprove crowd pedestrian counting in video surveillance. Most of the existingapproaches estimate the number of pedestrians within one frame, which result ininconsistent predictions in temporal domain. In this paper, firstly, we segmentthe foreground of each frame into different groups, each of which containsseveral pedestrians. Then we train a regression-based map from low levelfeatures of each group to its person number. Secondly, we construct a directedgraph to simulate people flow, whose vertices represent groups of each frameand edges represent people moving from one group to another. Then, the peopleflow can be viewed as an integer flow in the constructed directed graph.Finally, by solving a quadratic programming problem with network flowconstraints in the directed graph, we obtain a consistent pedestrian counting.The experimental results show that our method can improve the crowd countingaccuracy significantly.
arxiv-18000-18 | A Mid-level Video Representation based on Binary Descriptors: A Case Study for Pornography Detection | http://arxiv.org/pdf/1605.03804v1.pdf | author:Carlos Caetano, Sandra Avila, William Robson Schwartz, Silvio Jamil F. Guimarães, Arnaldo de A. Araújo category:cs.CV published:2016-05-12 summary:With the growing amount of inappropriate content on the Internet, such aspornography, arises the need to detect and filter such material. The reason forthis is given by the fact that such content is often prohibited in certainenvironments (e.g., schools and workplaces) or for certain publics (e.g.,children). In recent years, many works have been mainly focused on detectingpornographic images and videos based on visual content, particularly on thedetection of skin color. Although these approaches provide good results, theygenerally have the disadvantage of a high false positive rate since not allimages with large areas of skin exposure are necessarily pornographic images,such as people wearing swimsuits or images related to sports. Local featurebased approaches with Bag-of-Words models (BoW) have been successfully appliedto visual recognition tasks in the context of pornography detection. Eventhough existing methods provide promising results, they use local featuredescriptors that require a high computational processing time yieldinghigh-dimensional vectors. In this work, we propose an approach for pornographydetection based on local binary feature extraction and BossaNova imagerepresentation, a BoW model extension that preserves more richly the visualinformation. Moreover, we propose two approaches for video description based onthe combination of mid-level representations namely BossaNova Video Descriptor(BNVD) and BoW Video Descriptor (BoW-VD). The proposed techniques arepromising, achieving an accuracy of 92.40%, thus reducing the classificationerror by 16% over the current state-of-the-art local features approach on thePornography dataset.
arxiv-18000-19 | Tensor Train polynomial models via Riemannian optimization | http://arxiv.org/pdf/1605.03795v1.pdf | author:Alexander Novikov, Mikhail Trofimov, Ivan Oseledets category:stat.ML cs.LG published:2016-05-12 summary:Modeling interactions between features improves the performance of machinelearning solutions in many domains (e.g. recommender systems or sentimentanalysis). In this paper, we introduce Exponential Machines (ExM), a predictorthat models all interactions of every order. The key idea is to represent anexponentially large tensor of parameters in a factorized format called TensorTrain (TT). The Tensor Train format regularizes the model and lets you controlthe number of underlying parameters. To train the model, we develop astochastic version of Riemannian optimization, which allows us to fit tensorswith $2^{30}$ entries. We show that the model achieves state-of-the-artperformance on synthetic data with high-order interactions.
arxiv-18000-20 | Direct Method for Training Feed-forward Neural Networks using Batch Extended Kalman Filter for Multi-Step-Ahead Predictions | http://arxiv.org/pdf/1605.03764v1.pdf | author:Artem Chernodub category:cs.NE published:2016-05-12 summary:This paper is dedicated to the long-term, or multi-step-ahead, time seriesprediction problem. We propose a novel method for training feed-forward neuralnetworks, such as multilayer perceptrons, with tapped delay lines. Specialbatch calculation of derivatives called Forecasted Propagation Through Time andbatch modification of the Extended Kalman Filter are introduced. Experimentswere carried out on well-known time series benchmarks, the Mackey-Glass chaoticprocess and the Santa Fe Laser Data Series. Recurrent and feed-forward neuralnetworks were evaluated.
arxiv-18000-21 | Fast Graph-Based Object Segmentation for RGB-D Images | http://arxiv.org/pdf/1605.03746v1.pdf | author:Giorgio Toscana, Stefano Rosa category:cs.CV cs.RO published:2016-05-12 summary:Object segmentation is an important capability for robotic systems, inparticular for grasping. We present a graph- based approach for thesegmentation of simple objects from RGB-D images. We are interested insegmenting objects with large variety in appearance, from lack of texture tostrong textures, for the task of robotic grasping. The algorithm does not relyon image features or machine learning. We propose a modified Canny edgedetector for extracting robust edges by using depth information and two simplecost functions for combining color and depth cues. The cost functions are usedto build an undirected graph, which is partitioned using the concept ofinternal and external differences between graph regions. The partitioning isfast with O(NlogN) complexity. We also discuss ways to deal with missing depthinformation. We test the approach on different publicly available RGB-D objectdatasets, such as the Rutgers APC RGB-D dataset and the RGB-D Object Dataset,and compare the results with other existing methods.
arxiv-18000-22 | Real-time 3D Tracking of Articulated Tools for Robotic Surgery | http://arxiv.org/pdf/1605.03483v2.pdf | author:Menglong Ye, Lin Zhang, Stamatia Giannarou, Guang-Zhong Yang category:cs.CV cs.RO published:2016-05-11 summary:In robotic surgery, tool tracking is important for providing safe tool-tissueinteraction and facilitating surgical skills assessment. Despite recentadvances in tool tracking, existing approaches are faced with majordifficulties in real-time tracking of articulated tools. Most algorithms aretailored for offline processing with pre-recorded videos. In this paper, wepropose a real-time 3D tracking method for articulated tools in roboticsurgery. The proposed method is based on the CAD model of the tools as well asrobot kinematics to generate online part-based templates for efficient 2Dmatching and 3D pose estimation. A robust verification approach is incorporatedto reject outliers in 2D detections, which is then followed by fusing inlierswith robot kinematic readings for 3D pose estimation of the tool. The proposedmethod has been validated with phantom data, as well as ex vivo and in vivoexperiments. The results derived clearly demonstrate the performance advantageof the proposed method when compared to the state-of-the-art.
arxiv-18000-23 | Real-time Robust Manhattan Frame Estimation: Global Optimality and Applications | http://arxiv.org/pdf/1605.03730v1.pdf | author:Kyungdon Joo, Tae-Hyun Oh, Junsik Kim, In So Kweon category:cs.CV cs.RO published:2016-05-12 summary:Most man-made environments, such as urban and indoor scenes, consist of a setof parallel and orthogonal planar structures. These structures are approximatedby Manhattan world assumption and be referred to Manhattan Frame (MF). Given aset of inputs such as surface normals or vanishing points, we pose an MFestimation problem as a consensus set maximization that maximizes the number ofinliers over the rotation search space. Conventionally this problem can besolved by a branch-and-bound framework which mathematically guarantees globaloptimality. However, the computational time of the conventionalbranch-and-bound algorithms is rather far from real-time performance. In thispaper, we propose a novel bound computation method on an efficient measurementdomain for MF estimation, i.e., the extended Gaussian image (EGI). By relaxingthe original problem, we can compute the bounds in real-time performance, whilepreserving global optimality. Furthermore, we quantitatively and qualitativelydemonstrate the performance of the proposed method for various synthetic andreal-world data. We also show the versatility of our approach through threedifferent applications: extension to multiple MF estimation, videostabilization and line clustering.
arxiv-18000-24 | Deformable Parts Correlation Filters for Robust Visual Tracking | http://arxiv.org/pdf/1605.03720v1.pdf | author:Alan Lukežič, Luka Čehovin, Matej Kristan category:cs.CV published:2016-05-12 summary:Deformable parts models show a great potential in tracking by principallyaddressing non-rigid object deformations and self occlusions, but according torecent benchmarks, they often lag behind the holistic approaches. The reason isthat potentially large number of degrees of freedom have to be estimated forobject localization and simplifications of the constellation topology are oftenassumed to make the inference tractable. We present a new formulation of theconstellation model with correlation filters that treats the geometric andvisual constraints within a single convex cost function and derive a highlyefficient optimization for MAP inference of a fully-connected constellation. Wepropose a tracker that models the object at two levels of detail. The coarselevel corresponds a root correlation filter and a novel color model forapproximate object localization, while the mid-level representation is composedof the new deformable constellation of correlation filters that refine theobject location. The resulting tracker is rigorously analyzed on a highlychallenging OTB, VOT2014 and VOT2015 benchmarks, exhibits a state-of-the-artperformance and runs in real-time.
arxiv-18000-25 | Improved Image Boundaries for Better Video Segmentation | http://arxiv.org/pdf/1605.03718v1.pdf | author:Anna Khoreva, Rodrigo Benenson, Fabio Galasso, Matthias Hein, Bernt Schiele category:cs.CV published:2016-05-12 summary:Graph-based video segmentation methods rely on superpixels as starting point.While most previous work has focused on the construction of the graph edges andweights as well as solving the graph partitioning problem, this paper focuseson better superpixels for video segmentation. We demonstrate by a comparativeanalysis that superpixels extracted from boundaries perform best, and show thatboundary estimation can be significantly improved via image and time domaincues. With superpixels generated from our better boundaries we observeconsistent improvement for two video segmentation methods in two differentdatasets.
arxiv-18000-26 | Movie Description | http://arxiv.org/pdf/1605.03705v1.pdf | author:Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, Bernt Schiele category:cs.CV cs.CL published:2016-05-12 summary:Audio Description (AD) provides linguistic descriptions of movies and allowsvisually impaired people to follow a movie along with their peers. Suchdescriptions are by design mainly visual and thus naturally form an interestingdata source for computer vision and computational linguistics. In this work wepropose a novel dataset which contains transcribed ADs, which are temporallyaligned to full length movies. In addition we also collected and aligned moviescripts used in prior work and compare the two sources of descriptions. Intotal the Large Scale Movie Description Challenge (LSMDC) contains a parallelcorpus of 118,114 sentences and video clips from 202 movies. First wecharacterize the dataset by benchmarking different approaches for generatingvideo descriptions. Comparing ADs to scripts, we find that ADs are indeed morevisual and describe precisely what is shown rather than what should happenaccording to the scripts created prior to movie production. Furthermore, wepresent and compare the results of several teams who participated in achallenge organized in the context of the workshop "Describing andUnderstanding Video & The Large Scale Movie Description Challenge (LSMDC)", atICCV 2015.
arxiv-18000-27 | Robust and Efficient Relative Pose with a Multi-camera System for Autonomous Vehicle in Highly Dynamic Environments | http://arxiv.org/pdf/1605.03689v1.pdf | author:Liu Liu, Hongdong Li, Yuchao Dai category:cs.RO cs.CV published:2016-05-12 summary:This paper studies the relative pose problem for autonomous vehicle drivingin highly dynamic and possibly cluttered environments. This is a challengingscenario due to the existence of multiple, large, and independently movingobjects in the environment, which often leads to excessive portion of outliersand results in erroneous motion estimation. Existing algorithms cannot copewith such situations well. This paper proposes a new algorithm for relativepose using a multi-camera system with multiple non-overlapping individualcameras. The method works robustly even when the numbers of outliers areoverwhelming. By exploiting specific prior knowledge of driving scene we havedeveloped an efficient 4-point algorithm for multi-camera relative pose, whichadmits analytic solutions by solving a polynomial root-finding equation, andruns extremely fast (at about 0.5$u$s per root). When the solver is used incombination with RANSAC, we are able to quickly prune unpromising hypotheses,significantly improve the chance of finding inliers. Experiments on syntheticdata have validated the performance of the proposed algorithm. Tests on realdata further confirm the method's practical relevance.
arxiv-18000-28 | Going Deeper into First-Person Activity Recognition | http://arxiv.org/pdf/1605.03688v1.pdf | author:Minghuang Ma, Haoqi Fan, Kris M. Kitani category:cs.CV published:2016-05-12 summary:We bring together ideas from recent work on feature design for egocentricaction recognition under one framework by exploring the use of deepconvolutional neural networks (CNN). Recent work has shown that features suchas hand appearance, object attributes, local hand motion and camera ego-motionare important for characterizing first-person actions. To integrate these ideasunder one framework, we propose a twin stream network architecture, where onestream analyzes appearance information and the other stream analyzes motioninformation. Our appearance stream encodes prior knowledge of the egocentricparadigm by explicitly training the network to segment hands and localizeobjects. By visualizing certain neuron activation of our network, we show thatour proposed architecture naturally learns features that capture objectattributes and hand-object configurations. Our extensive experiments onbenchmark egocentric action datasets show that our deep architecture enablesrecognition rates that significantly outperform state-of-the-art techniques --an average $6.6\%$ increase in accuracy over all datasets. Furthermore, bylearning to recognize objects, actions and activities jointly, the performanceof individual recognition tasks also increase by $30\%$ (actions) and $14\%$(objects). We also include the results of extensive ablative analysis tohighlight the importance of network design decisions..
arxiv-18000-29 | Real-Time Web Scale Event Summarization Using Sequential Decision Making | http://arxiv.org/pdf/1605.03664v1.pdf | author:Chris Kedzie, Fernando Diaz, Kathleen McKeown category:cs.CL published:2016-05-12 summary:We present a system based on sequential decision making for the onlinesummarization of massive document streams, such as those found on the web.Given an event of interest (e.g. "Boston marathon bombing"), our system is ableto filter the stream for relevance and produce a series of short text updatesdescribing the event as it unfolds over time. Unlike previous work, ourapproach is able to jointly model the relevance, comprehensiveness, novelty,and timeliness required by time-sensitive queries. We demonstrate a 28.3%improvement in summary F1 and a 43.8% improvement in time-sensitive F1 metrics.
arxiv-18000-30 | Item Popularity Prediction in E-commerce Using Image Quality Feature Vectors | http://arxiv.org/pdf/1605.03663v1.pdf | author:Stephen Zakrewsky, Kamelia Aryafar, Ali Shokoufandeh category:cs.CV published:2016-05-12 summary:Online retail is a visual experience- Shoppers often use images as firstorder information to decide if an item matches their personal style. Imagecharacteristics such as color, simplicity, scene composition, texture, style,aesthetics and overall quality play a crucial role in making a purchasedecision, clicking on or liking a product listing. In this paper we use a setof image features that indicate quality to predict product listing popularityon a major e-commerce website, Etsy. We first define listing popularity throughsearch clicks, favoriting and purchase activity. Next, we infer listing qualityfrom the pixel-level information of listed images as quality features. We thencompare our findings to text-only models for popularity prediction. Our initialresults indicate that a combined image and text modeling of product listingsoutperforms text-only models in popularity prediction.
arxiv-18000-31 | Subspace Perspective on Canonical Correlation Analysis: Dimension Reduction and Minimax Rates | http://arxiv.org/pdf/1605.03662v1.pdf | author:Zhuang Ma, Xiaodong Li category:math.ST stat.ML stat.TH published:2016-05-12 summary:Canonical correlation analysis (CCA) is a fundamental statistical tool forexploring the correlation structure between two sets of random variables. Inthis paper, motivated by recent success of applying CCA to learn lowdimensional representations of high dimensional objects, we propose to quantifythe estimation loss of CCA by the excess prediction loss defined through aprediction-after-dimension-reduction framework. Such framework suggests viewingCCA estimation as estimating the subspaces spanned by the canonical variates.Interestedly, the proposed error metrics derived from the excess predictionloss turn out to be closely related to the principal angles between thesubspaces spanned by the population and sample canonical variates respectively. We characterize the non-asymptotic minimax rates under the proposed metrics,especially the dependency of the minimax rates on the key quantities includingthe dimensions, the condition number of the covariance matrices, the canonicalcorrelations and the eigen-gap, with minimal assumptions on the jointcovariance matrix. To the best of our knowledge, this is the first finitesample result that captures the effect of the canonical correlations on theminimax rates.
arxiv-18000-32 | Learning Representations for Counterfactual Inference | http://arxiv.org/pdf/1605.03661v1.pdf | author:Fredrik D. Johansson, Uri Shalit, David Sontag category:stat.ML cs.AI cs.LG published:2016-05-12 summary:Observational studies are rising in importance due to the widespreadaccumulation of data in fields such as healthcare, education, employment andecology. We consider the task of answering counterfactual questions such as,"Would this patient have lower blood sugar had she received a differentmedication?". We propose a new algorithmic framework for counterfactualinference which brings together ideas from domain adaptation and representationlearning. In addition to a theoretical justification, we perform an empiricalcomparison with previous approaches to causal inference from observationaldata. Our deep learning algorithm significantly outperforms the previousstate-of-the-art.
arxiv-18000-33 | HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition | http://arxiv.org/pdf/1603.01249v2.pdf | author:Rajeev Ranjan, Vishal M. Patel, Rama Chellappa category:cs.CV published:2016-03-03 summary:We present an algorithm for simultaneous face detection, landmarkslocalization, pose estimation and gender recognition using deep convolutionalneural networks (CNN). The proposed method called, Hyperface, fuses theintermediate layers of a deep CNN using a separate CNN and trains multi-taskloss on the fused features. It exploits the synergy among the tasks whichboosts up their individual performances. Extensive experiments show that theproposed method is able to capture both global and local information of facesand performs significantly better than many competitive algorithms for each ofthese four tasks.
arxiv-18000-34 | EEF: Exponentially Embedded Families with Class-Specific Features for Classification | http://arxiv.org/pdf/1605.03631v1.pdf | author:Bo Tang, Steven Kay, Haibo He, Paul M. Baggenstoss category:stat.ML cs.LG published:2016-05-11 summary:In this letter, we present a novel exponentially embedded families (EEF)based classification method, in which the probability density function (PDF) onraw data is estimated from the PDF on features. With the PDF construction, weshow that class-specific features can be used in the proposed classificationmethod, instead of a common feature subset for all classes as used inconventional approaches. We apply the proposed EEF classifier for textcategorization as a case study and derive an optimal Bayesian classificationrule with class-specific feature selection based on the Information Gain (IG)score. The promising performance on real-life data sets demonstrates theeffectiveness of the proposed approach and indicates its wide potentialapplications.
arxiv-18000-35 | Distributed Compressive Sensing: A Deep Learning Approach | http://arxiv.org/pdf/1508.04924v3.pdf | author:Hamid Palangi, Rabab Ward, Li Deng category:cs.LG cs.CV published:2015-08-20 summary:Various studies that address the compressed sensing problem with MultipleMeasurement Vectors (MMVs) have been recently carried. These studies assume thevectors of the different channels to be jointly sparse. In this paper, we relaxthis condition. Instead we assume that these sparse vectors depend on eachother but that this dependency is unknown. We capture this dependency bycomputing the conditional probability of each entry in each vector beingnon-zero, given the "residuals" of all previous vectors. To estimate theseprobabilities, we propose the use of the Long Short-Term Memory (LSTM)[1], adata driven model for sequence modelling that is deep in time. To calculate themodel parameters, we minimize a cross entropy cost function. To reconstruct thesparse vectors at the decoder, we propose a greedy solver that uses the abovemodel to estimate the conditional probabilities. By performing extensiveexperiments on two real world datasets, we show that the proposed methodsignificantly outperforms the general MMV solver (the Simultaneous OrthogonalMatching Pursuit (SOMP)) and a number of the model-based Bayesian methods. Theproposed method does not add any complexity to the general compressive sensingencoder. The trained model is used just at the decoder. As the proposed methodis a data driven method, it is only applicable when training data is available.In many applications however, training data is indeed available, e.g. inrecorded images and videos.
arxiv-18000-36 | A Survey of Semantic Segmentation | http://arxiv.org/pdf/1602.06541v2.pdf | author:Martin Thoma category:cs.CV published:2016-02-21 summary:This survey gives an overview over different techniques used for pixel-levelsemantic segmentation. Metrics and datasets for the evaluation of segmentationalgorithms and traditional approaches for segmentation such as unsupervisedmethods, Decision Forests and SVMs are described and pointers to the relevantpapers are given. Recently published approaches with convolutional neuralnetworks are mentioned and typical problematic situations for segmentationalgorithms are examined. A taxonomy of segmentation algorithms is given.
arxiv-18000-37 | Peak Criterion for Choosing Gaussian Kernel Bandwidth in Support Vector Data Description | http://arxiv.org/pdf/1602.05257v2.pdf | author:Deovrat Kakde, Arin Chaudhuri, Seunghyun Kong, Maria Jahja, Hansi Jiang, Jorge Silva category:cs.LG published:2016-02-17 summary:Support Vector Data Description (SVDD) is a machine-learning technique usedfor single class classification and outlier detection. SVDD formulation withkernel function provides a flexible boundary around data. The value of kernelfunction parameters affects the nature of the data boundary. For example, it isobserved that with a Gaussian kernel, as the value of kernel bandwidth islowered, the data boundary changes from spherical to wiggly. The spherical databoundary leads to underfitting, and an extremely wiggly data boundary leads tooverfitting. In this paper, we propose empirical criterion to obtain goodvalues of the Gaussian kernel bandwidth parameter. This criterion provides asmooth boundary that captures the essential geometric features of the data.
arxiv-18000-38 | Blind image separation based on exponentiated transmuted Weibull distribution | http://arxiv.org/pdf/1605.03624v1.pdf | author:A. M. Adam, R. M. Farouk, M. E. Abd El-aziz category:cs.CV published:2016-05-11 summary:In recent years the processing of blind image separation has beeninvestigated. As a result, a number of feature extraction algorithms for directapplication of such image structures have been developed. For example,separation of mixed fingerprints found in any crime scene, in which a mixtureof two or more fingerprints may be obtained, for identification, we have toseparate them. In this paper, we have proposed a new technique for separating amultiple mixed images based on exponentiated transmuted Weibull distribution.To adaptively estimate the parameters of such score functions, an efficientmethod based on maximum likelihood and genetic algorithm will be used. We alsocalculate the accuracy of this proposed distribution and compare thealgorithmic performance using the efficient approach with other previousgeneralized distributions. We find from the numerical results that the proposeddistribution has flexibility and an efficient result
arxiv-18000-39 | ASP Vision: Optically Computing the First Layer of Convolutional Neural Networks using Angle Sensitive Pixels | http://arxiv.org/pdf/1605.03621v1.pdf | author:Huaijin Chen, Suren Jayasuriya, Jiyue Yang, Judy Stephen, Sriram Sivaramakrishnan, Ashok Veeraraghavan, Alyosha Molnar category:cs.CV published:2016-05-11 summary:Deep learning using convolutional neural networks (CNNs) is quickly becomingthe state-of-the-art for challenging computer vision applications. However,deep learning's power consumption and bandwidth requirements currently limitits application in embedded and mobile systems with tight energy budgets. Inthis paper, we explore the energy savings of optically computing the firstlayer of CNNs. To do so, we utilize bio-inspired Angle Sensitive Pixels (ASPs),custom CMOS diffractive image sensors which act similar to Gabor filter banksin the V1 layer of the human visual cortex. ASPs replace both image sensing andthe first layer of a conventional CNN by directly performing optical edgefiltering, saving sensing energy, data bandwidth, and CNN FLOPS to compute. Ourexperimental results (both on synthetic data and a hardware prototype) for avariety of vision tasks such as digit recognition, object recognition, and faceidentification demonstrate 97% reduction in image sensor power consumption and90% reduction in data bandwidth from sensor to CPU, while achieving similarperformance compared to traditional deep learning pipelines.
arxiv-18000-40 | Asymptotic sequential Rademacher complexity of a finite function class | http://arxiv.org/pdf/1605.03843v1.pdf | author:Dmitry B. Rokhlin category:cs.LG stat.ML 68Q32, 60F05 published:2016-05-11 summary:For a finite function class we describe the large sample limit of thesequential Rademacher complexity in terms of the viscosity solution of a$G$-heat equation. In the language of Peng's sublinear expectation theory, thesame quantity equals to the expected value of the largest order statistics of amultidimensional $G$-normal random variable. We illustrate this result byderiving upper and lower bounds for the asymptotic sequential Rademachercomplexity.
arxiv-18000-41 | COCO: Performance Assessment | http://arxiv.org/pdf/1605.03560v1.pdf | author:Nikolaus Hansen, Anne Auger, Dimo Brockhoff, Dejan Tušar, Tea Tušar category:cs.NE published:2016-05-11 summary:We present an any-time performance assessment for benchmarking numericaloptimization algorithms in a black-box scenario, applied within the COCObenchmarking platform. The performance assessment is based on runtimes measuredin number of objective function evaluations to reach one or several qualityindicator target values. We argue that runtime is the only available measurewith a generic, meaningful, and quantitative interpretation. We discuss thechoice of the target values, runlength-based targets, and the aggregation ofresults by using simulated restarts, averages, and empirical distributionfunctions.
arxiv-18000-42 | View Synthesis by Appearance Flow | http://arxiv.org/pdf/1605.03557v1.pdf | author:Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, Alexei A. Efros category:cs.CV published:2016-05-11 summary:Given one or more images of an object (or a scene), is it possible tosynthesize a new image of the same instance observed from an arbitraryviewpoint? In this paper, we attempt to tackle this problem, known as novelview synthesis, by re-formulating it as a pixel copying task that avoids thenotorious difficulties of generating pixels from scratch. Our approach is builton the observation that the visual appearance of different views of the sameinstance is highly correlated. Such correlation could be explicitly learned bytraining a convolutional neural network (CNN) to predict appearance flows --2-D coordinate vectors specifying which pixels in the input view could be usedto reconstruct the target view. We show that for both objects and scenes, ourapproach is able to generate higher-quality synthesized views with crisptexture and boundaries than previous CNN-based techniques.
arxiv-18000-43 | Linear Shape Deformation Models with Local Support Using Graph-based Structured Matrix Factorisation | http://arxiv.org/pdf/1510.08291v2.pdf | author:Florian Bernard, Peter Gemmar, Frank Hertel, Jorge Goncalves, Johan Thunberg category:cs.CV math.OC stat.ML published:2015-10-28 summary:Representing 3D shape deformations by linear models in high-dimensional spacehas many applications in computer vision and medical imaging, such asshape-based interpolation or segmentation. Commonly, using Principal ComponentsAnalysis a low-dimensional (affine) subspace of the high-dimensional shapespace is determined. However, the resulting factors (the most dominanteigenvectors of the covariance matrix) have global support, i.e. changing thecoefficient of a single factor deforms the entire shape. In this paper, amethod to obtain deformation factors with local support is presented. Thebenefits of such models include better flexibility and interpretability as wellas the possibility of interactively deforming shapes locally. For that, basedon a well-grounded theoretical motivation, we formulate a matrix factorisationproblem employing sparsity and graph-based regularisation terms. We demonstratethat for brain shapes our method outperforms the state of the art in localsupport models with respect to generalisation ability and sparse shapereconstruction, whereas for human body shapes our method gives more realisticdeformations.
arxiv-18000-44 | Common Variable Learning and Invariant Representation Learning using Siamese Neural Networks | http://arxiv.org/pdf/1512.08806v3.pdf | author:Uri Shaham, Roy Lederman category:stat.ML cs.LG cs.NE published:2015-12-29 summary:We consider the statistical problem of learning common source of variabilityin data which are synchronously captured by multiple sensors, and demonstratethat Siamese neural networks can be naturally applied to this problem. Thisapproach is useful in particular in exploratory, data-driven applications,where neither a model nor label information is available. In recent years, manyresearchers have successfully applied Siamese neural networks to obtain anembedding of data which corresponds to a "semantic similarity". We present aninterpretation of this "semantic similarity" as learning of equivalenceclasses. We discuss properties of the embedding obtained by Siamese networksand provide empirical results that demonstrate the ability of Siamese networksto learn common variability.
arxiv-18000-45 | On the Iteration Complexity of Oblivious First-Order Optimization Algorithms | http://arxiv.org/pdf/1605.03529v1.pdf | author:Yossi Arjevani, Ohad Shamir category:math.OC cs.LG published:2016-05-11 summary:We consider a broad class of first-order optimization algorithms which are\emph{oblivious}, in the sense that their step sizes are scheduled regardlessof the function under consideration, except for limited side-information suchas smoothness or strong convexity parameters. With the knowledge of these twoparameters, we show that any such algorithm attains an iteration complexitylower bound of $\Omega(\sqrt{L/\epsilon})$ for $L$-smooth convex functions, and$\tilde{\Omega}(\sqrt{L/\mu}\ln(1/\epsilon))$ for $L$-smooth $\mu$-stronglyconvex functions. These lower bounds are stronger than those in the traditionaloracle model, as they hold independently of the dimension. To attain these, weabandon the oracle model in favor of a structure-based approach which buildsupon a framework recently proposed in (Arjevani et al., 2015). We further showthat without knowing the strong convexity parameter, it is impossible to attainan iteration complexity better than$\tilde{\Omega}\left((L/\mu)\ln(1/\epsilon)\right)$. This result is then usedto formalize an observation regarding $L$-smooth convex functions, namely, thatthe iteration complexity of algorithms employing time-invariant step sizes mustbe at least $\Omega(L/\epsilon)$.
arxiv-18000-46 | The Yahoo Query Treebank, V. 1.0 | http://arxiv.org/pdf/1605.02945v2.pdf | author:Yuval Pinter, Roi Reichart, Idan Szpektor category:cs.CL cs.IR published:2016-05-10 summary:A description and annotation guidelines for the Yahoo Webscope release ofQuery Treebank, Version 1.0, May 2016.
arxiv-18000-47 | When Do Luxury Cars Hit the Road? Findings by A Big Data Approach | http://arxiv.org/pdf/1605.02827v2.pdf | author:Yang Feng, Jiebo Luo category:cs.CY cs.CV published:2016-05-10 summary:In this paper, we focus on studying the appearing time of different kinds ofcars on the road. This information will enable us to infer the life style ofthe car owners. The results can further be used to guide marketing towards carowners. Conventionally, this kind of study is carried out by sending outquestionnaires, which is limited in scale and diversity. To solve this problem,we propose a fully automatic method to carry out this study. Our study is basedon publicly available surveillance camera data. To make the results reliable,we only use the high resolution cameras (i.e. resolution greater than $1280\times 720$). Images from the public cameras are downloaded every minute. Afterobtaining 50,000 images, we apply faster R-CNN (region-based convoluntionalneural network) to detect the cars in the downloaded images and a fine-tunedVGG16 model is used to recognize the car makes. Based on the recognitionresults, we present a data-driven analysis on the relationship between carmakes and their appearing times, with implications on lifestyles.
arxiv-18000-48 | Deep Neural Networks Under Stress | http://arxiv.org/pdf/1605.03498v1.pdf | author:Micael Carvalho, Matthieu Cord, Sandra Avila, Nicolas Thome, Eduardo Valle category:cs.CV cs.AI published:2016-05-11 summary:In recent years, deep architectures have been used for transfer learning withstate-of-the-art performance in many datasets. The properties of their featuresremain, however, largely unstudied under the transfer perspective. In thiswork, we present an extensive analysis of the resiliency of feature vectorsextracted from deep models, with special focus on the trade-off betweenperformance and compression rate. By introducing perturbations to imagedescriptions extracted from a deep convolutional neural network, we changetheir precision and number of dimensions, measuring how it affects the finalscore. We show that deep features are more robust to these disturbances whencompared to classical approaches, achieving a compression rate of 98.4%, whilelosing only 0.88% of their original score for Pascal VOC 2007.
arxiv-18000-49 | On-the-fly Network Pruning for Object Detection | http://arxiv.org/pdf/1605.03477v1.pdf | author:Marc Masana, Joost van de Weijer, Andrew D. Bagdanov category:cs.CV published:2016-05-11 summary:Object detection with deep neural networks is often performed by passing afew thousand candidate bounding boxes through a deep neural network for eachimage. These bounding boxes are highly correlated since they originate from thesame image. In this paper we investigate how to exploit feature occurrence atthe image scale to prune the neural network which is subsequently applied toall bounding boxes. We show that removing units which have near-zero activationin the image allows us to significantly reduce the number of parameters in thenetwork. Results on the PASCAL 2007 Object Detection Challenge demonstrate thatup to 40% of units in some fully-connected layers can be entirely eliminatedwith little change in the detection result.
arxiv-18000-50 | Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning | http://arxiv.org/pdf/1510.08906v3.pdf | author:Christoph Dann, Emma Brunskill category:stat.ML cs.AI cs.LG published:2015-10-29 summary:Recently, there has been significant progress in understanding reinforcementlearning in discounted infinite-horizon Markov decision processes (MDPs) byderiving tight sample complexity bounds. However, in many real-worldapplications, an interactive learning agent operates for a fixed or boundedperiod of time, for example tutoring students for exams or handling customerservice requests. Such scenarios can often be better treated as episodicfixed-horizon MDPs, for which only looser bounds on the sample complexityexist. A natural notion of sample complexity in this setting is the number ofepisodes required to guarantee a certain performance with high probability (PACguarantee). In this paper, we derive an upper PAC bound $\tildeO(\frac{\mathcal S^2 \mathcal A H^2}{\epsilon^2} \ln\frac 1 \delta)$ and alower PAC bound $\tilde \Omega(\frac{\mathcal S \mathcal A H^2}{\epsilon^2}\ln \frac 1 {\delta + c})$ that match up to log-terms and an additional lineardependency on the number of states $\mathcal S$. The lower bound is the firstof its kind for this setting. Our upper bound leverages Bernstein's inequalityto improve on previous bounds for episodic finite-horizon MDPs which have atime-horizon dependency of at least $H^3$.
arxiv-18000-51 | Collaborative Filtering Bandits | http://arxiv.org/pdf/1502.03473v6.pdf | author:Shuai Li, Alexandros Karatzoglou, Claudio Gentile category:cs.LG cs.AI stat.ML published:2015-02-11 summary:Classical collaborative filtering, and content-based filtering methods try tolearn a static recommendation model given training data. These approaches arefar from ideal in highly dynamic recommendation domains such as newsrecommendation and computational advertisement, where the set of items andusers is very fluid. In this work, we investigate an adaptive clusteringtechnique for content recommendation based on exploration-exploitationstrategies in contextual multi-armed bandit settings. Our algorithm takes intoaccount the collaborative effects that arise due to the interaction of theusers with the items, by dynamically grouping users based on the items underconsideration and, at the same time, grouping items based on the similarity ofthe clusterings induced over the users. The resulting algorithm thus takesadvantage of preference patterns in the data in a way akin to collaborativefiltering methods. We provide an empirical analysis on medium-size real-worlddatasets, showing scalability and increased prediction performance (as measuredby click-through rate) over state-of-the-art methods for clustering bandits. Wealso provide a regret analysis within a standard linear stochastic noisesetting.
arxiv-18000-52 | A constrained L1 minimization approach for estimating multiple Sparse Gaussian or Nonparanormal Graphical Models | http://arxiv.org/pdf/1605.03468v1.pdf | author:Beilun Wang, Ritambhara Singh, Yanjun Qi category:cs.LG stat.ML published:2016-05-11 summary:The flood of multi-context measurement data from many scientific domains havecreated an urgent need to reconstruct context-specific variable networks, thatcan significantly simplify network-driven studies. Computationally, thisproblem can be formulated as jointly estimating multiple different, butrelated, sparse Undirected Graphical Models (UGM) from samples aggregatedacross several tasks. Previous joint-UGM studies could not fully address thechallenge since they mostly focus on Gaussian Graphical Models (GGM) and haveused likelihood-based formulations to push multiple estimated networks toward acommon pattern. Differently, we propose a novel approach, SIMULE (learningShared and Individual parts of MULtiple graphs Explicitly) to solve multi-taskUGM using a l1 constrained optimization. SIMULE can handle both multivariateGaussian and multivariate Nonparanormal data (greatly relaxing the normalityassumption most real data do not follow). SIMULE is cast as independentsubproblems of linear programming that can be solved efficiently. Itautomatically infers specific dependencies that are unique to each context aswell as shared substructures preserved among all the contexts. Theoretically weprove that SIMULE achieves a consistent estimation at rate O(log(Kp)/ntot) (notbeen proved before). On four synthetic datasets and two real datasets, SIMULEshows significant improvements over state-of-the-art multi-sGGM and single-UGMbaselines.
arxiv-18000-53 | Image-level Classification in Hyperspectral Images using Feature Descriptors, with Application to Face Recognition | http://arxiv.org/pdf/1605.03428v1.pdf | author:Vivek Sharma, Luc Van Gool category:cs.CV published:2016-05-11 summary:In this paper, we proposed a novel pipeline for image-level classification inthe hyperspectral images. By doing this, we show that the discriminativespectral information at image-level features lead to significantly improvedperformance in a face recognition task. We also explored the potential oftraditional feature descriptors in the hyperspectral images. From ourevaluations, we observe that SIFT features outperform the state-of-the-arthyperspectral face recognition methods, and also the other descriptors. Withthe increasing deployment of hyperspectral sensors in a multitude ofapplications, we believe that our approach can effectively exploit the spectralinformation in hyperspectral images, thus beneficial to more accurateclassification.
arxiv-18000-54 | Neural Autoregressive Distribution Estimation | http://arxiv.org/pdf/1605.02226v2.pdf | author:Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, Hugo Larochelle category:cs.LG published:2016-05-07 summary:We present Neural Autoregressive Distribution Estimation (NADE) models, whichare neural network architectures applied to the problem of unsuperviseddistribution and density estimation. They leverage the probability product ruleand a weight sharing scheme inspired from restricted Boltzmann machines, toyield an estimator that is both tractable and has good generalizationperformance. We discuss how they achieve competitive performance in modelingboth binary and real-valued observations. We also present how deep NADE modelscan be trained to be agnostic to the ordering of input dimensions used by theautoregressive product rule decomposition. Finally, we also show how to exploitthe topological structure of pixels in images using a deep convolutionalarchitecture for NADE.
arxiv-18000-55 | Random forests for survival analysis using maximally selected rank statistics | http://arxiv.org/pdf/1605.03391v1.pdf | author:Marvin N. Wright, Theresa Dankowski, Andreas Ziegler category:stat.ML cs.LG published:2016-05-11 summary:The most popular approach for analyzing survival data is the Cox regressionmodel. The Cox model may, however, be misspecified, and its proportionalityassumption is not always fulfilled. An alternative approach is random forestsfor survival outcomes. The standard split criterion for random survival forestsis the log-rank test statistics, which favors splitting variables with manypossible split points. Conditional inference forests avoid this split pointselection bias. However, linear rank statistics are utilized in currentsoftware for conditional inference forests to select the optimal splittingvariable, which cannot detect non-linear effects in the independent variables.We therefore use maximally selected rank statistics for split point selectionin random forests for survival analysis. As in conditional inference forests,p-values for association between split points and survival time are minimized.We describe several p-value approximations and the implementation of theproposed random forest approach. A simulation study demonstrates that unbiasedsplit point selection is possible. However, there is a trade-off betweenunbiased split point selection and runtime. In benchmark studies of predictionperformance on simulated and real datasets the new method performs better thanrandom survival forests if informative dichotomous variables are combined withuninformative variables with more categories and better than conditionalinference forests if non-linear covariate effects are included. In a runtimecomparison the method proves to be computationally faster than bothalternatives, if a simple p-value approximation is used.
arxiv-18000-56 | Efficiently Creating 3D Training Data for Fine Hand Pose Estimation | http://arxiv.org/pdf/1605.03389v1.pdf | author:Markus Oberweger, Gernot Riegler, Paul Wohlhart, Vincent Lepetit category:cs.CV cs.HC published:2016-05-11 summary:While many recent hand pose estimation methods critically rely on a trainingset of labelled frames, the creation of such a dataset is a challenging taskthat has been overlooked so far. As a result, existing datasets are limited toa few sequences and individuals, with limited accuracy, and this prevents thesemethods from delivering their full potential. We propose a semi-automatedmethod for efficiently and accurately labeling each frame of a hand depth videowith the corresponding 3D locations of the joints: The user is asked to provideonly an estimate of the 2D reprojections of the visible joints in somereference frames, which are automatically selected to minimize the labelingwork by efficiently optimizing a sub-modular loss function. We then exploitspatial, temporal, and appearance constraints to retrieve the full 3D poses ofthe hand over the complete sequence. We show that this data can be used totrain a recent state-of-the-art hand pose estimation method, leading toincreased accuracy. The code and dataset can be found on our websitehttps://cvarlab.icg.tugraz.at/projects/hand_detection/
arxiv-18000-57 | Active Uncertainty Calibration in Bayesian ODE Solvers | http://arxiv.org/pdf/1605.03364v1.pdf | author:Hans Kersting, Philipp Hennig category:cs.NA cs.LG math.NA stat.ML published:2016-05-11 summary:There is resurging interest, in statistics and machine learning, in solversfor ordinary differential equations (ODEs) that return probability measuresinstead of point estimates. Recently, Conrad et al. introduced a sampling-basedclass of methods that are 'well-calibrated' in a specific sense. But thecomputational cost of these methods is significantly above that of classicmethods. On the other hand, Schober et al. pointed out a precise connectionbetween classic Runge-Kutta ODE solvers and Gaussian filters, which gives onlya rough probabilistic calibration, but at negligible cost overhead. Byformulating the solution of ODEs as approximate inference in linear GaussianSDEs, we investigate a range of probabilistic ODE solvers, that bridge thetrade-off between computational cost and probabilistic calibration, andidentify the inaccurate gradient measurement as the crucial source ofuncertainty. We propose the novel filtering-based method Bayesian Quadraturefiltering (BQF) which uses Bayesian quadrature to actively learn theimprecision in the gradient measurement by collecting multiple gradientevaluations.
arxiv-18000-58 | Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration | http://arxiv.org/pdf/1605.03344v1.pdf | author:Jiaolong Yang, Hongdong Li, Dylan Campbell, Yunde Jia category:cs.CV published:2016-05-11 summary:The Iterative Closest Point (ICP) algorithm is one of the most widely usedmethods for point-set registration. However, being based on local iterativeoptimization, ICP is known to be susceptible to local minima. Its performancecritically relies on the quality of the initialization and only localoptimality is guaranteed. This paper presents the first globally optimalalgorithm, named Go-ICP, for Euclidean (rigid) registration of two 3Dpoint-sets under the L2 error metric defined in ICP. The Go-ICP method is basedon a branch-and-bound (BnB) scheme that searches the entire 3D motion spaceSE(3). By exploiting the special structure of SE(3) geometry, we derive novelupper and lower bounds for the registration error function. Local ICP isintegrated into the BnB scheme, which speeds up the new method whileguaranteeing global optimality. We also discuss extensions, addressing theissue of outlier robustness. The evaluation demonstrates that the proposedmethod is able to produce reliable registration results regardless of theinitialization. Go-ICP can be applied in scenarios where an optimal solution isdesirable or where a good initialization is not always available.
arxiv-18000-59 | Asymptotic properties for combined $L_1$ and concave regularization | http://arxiv.org/pdf/1605.03335v1.pdf | author:Yingying Fan, Jinchi Lv category:stat.ME stat.ML published:2016-05-11 summary:Two important goals of high-dimensional modeling are prediction and variableselection. In this article, we consider regularization with combined $L_1$ andconcave penalties, and study the sampling properties of the global optimum ofthe suggested method in ultra-high dimensional settings. The $L_1$-penaltyprovides the minimum regularization needed for removing noise variables inorder to achieve oracle prediction risk, while concave penalty imposesadditional regularization to control model sparsity. In the linear modelsetting, we prove that the global optimum of our method enjoys the same oracleinequalities as the lasso estimator and admits an explicit bound on the falsesign rate, which can be asymptotically vanishing. Moreover, we establish oraclerisk inequalities for the method and the sampling properties of computablesolutions. Numerical studies suggest that our method yields more stableestimates than using a concave penalty alone.
arxiv-18000-60 | A robust particle detection algorithm based on symmetry | http://arxiv.org/pdf/1605.03328v1.pdf | author:Alvaro Rodriguez, Hanqing Zhang, Krister Wiklund, Tomas Brodin, Jonatan Klaminder, Patrik Andersson, Magnus Andersson category:cs.CV published:2016-05-11 summary:Particle tracking is common in many biophysical, ecological, andmicro-fluidic applications. Reliable tracking information is heavily dependenton of the system under study and algorithms that correctly determines particleposition between images. However, in a real environmental context with thepresence of noise including particular or dissolved matter in water, and lowand fluctuating light conditions, many algorithms fail to obtain reliableinformation. We propose a new algorithm, the Circular Symmetry algorithm(C-Sym), for detecting the position of a circular particle with high accuracyand precision in noisy conditions. The algorithm takes advantage of the spatialsymmetry of the particle allowing for subpixel accuracy. We compare theproposed algorithm with four different methods using both synthetic andexperimental datasets. The results show that C-Sym is the most accurate andprecise algorithm when tracking micro-particles in all tested conditions and ithas the potential for use in applications including tracking biota in theirenvironment.
arxiv-18000-61 | Unsupervised Semantic Action Discovery from Video Collections | http://arxiv.org/pdf/1605.03324v1.pdf | author:Ozan Sener, Amir Roshan Zamir, Chenxia Wu, Silvio Savarese, Ashutosh Saxena category:cs.CV cs.RO stat.ML published:2016-05-11 summary:Human communication takes many forms, including speech, text andinstructional videos. It typically has an underlying structure, with a startingpoint, ending, and certain objective steps between them. In this paper, weconsider instructional videos where there are tens of millions of them on theInternet. We propose a method for parsing a video into such semantic steps in anunsupervised way. Our method is capable of providing a semantic "storyline" ofthe video composed of its objective steps. We accomplish this using both visualand language cues in a joint generative model. Our method can also provide atextual description for each of the identified semantic steps and videosegments. We evaluate our method on a large number of complex YouTube videosand show that our method discovers semantically correct instructions for avariety of tasks.
arxiv-18000-62 | Tuning parameter selection in high dimensional penalized likelihood | http://arxiv.org/pdf/1605.03321v1.pdf | author:Yingying Fan, Cheng Yong Tang category:stat.ME stat.ML published:2016-05-11 summary:Determining how to appropriately select the tuning parameter is essential inpenalized likelihood methods for high-dimensional data analysis. We examinethis problem in the setting of penalized likelihood methods for generalizedlinear models, where the dimensionality of covariates p is allowed to increaseexponentially with the sample size n. We propose to select the tuning parameterby optimizing the generalized information criterion (GIC) with an appropriatemodel complexity penalty. To ensure that we consistently identify the truemodel, a range for the model complexity penalty is identified in GIC. We findthat this model complexity penalty should diverge at the rate of some power of$\log p$ depending on the tail probability behavior of the response variables.This reveals that using the AIC or BIC to select the tuning parameter may notbe adequate for consistently identifying the true model. Based on ourtheoretical study, we propose a uniform choice of the model complexity penaltyand show that the proposed approach consistently identifies the true modelamong candidate models with asymptotic probability one. We justify theperformance of the proposed procedure by numerical simulations and a geneexpression data analysis.
arxiv-18000-63 | Interaction pursuit in high-dimensional multi-response regression via distance correlation | http://arxiv.org/pdf/1605.03315v1.pdf | author:Yinfei Kong, Daoji Li, Yingying Fan, Jinchi Lv category:stat.ME stat.ML published:2016-05-11 summary:Feature interactions can contribute to a large proportion of variation inmany prediction models. In the era of big data, the coexistence of highdimensionality in both responses and covariates poses unprecedented challengesin identifying important interactions. In this paper, we suggest a two-stageinteraction identification method, called the interaction pursuit via distancecorrelation (IPDC), in the setting of high-dimensional multi-responseinteraction models that exploits feature screening applied to transformedvariables with distance correlation followed by feature selection. Such aprocedure is computationally efficient, generally applicable beyond theheredity assumption, and effective even when the number of responses divergeswith the sample size. Under mild regularity conditions, we show that thismethod enjoys nice theoretical properties including the sure screeningproperty, support union recovery, and oracle inequalities in prediction andestimation for both interactions and main effects. The advantages of our methodare supported by several simulation studies and real data analysis.
arxiv-18000-64 | Innovated scalable efficient estimation in ultra-large Gaussian graphical models | http://arxiv.org/pdf/1605.03313v1.pdf | author:Yingying Fan, Jinchi Lv category:stat.ME stat.ML published:2016-05-11 summary:Large-scale precision matrix estimation is of fundamental importance yetchallenging in many contemporary applications for recovering Gaussian graphicalmodels. In this paper, we suggest a new approach of innovated scalableefficient estimation (ISEE) for estimating large precision matrix. Motivated bythe innovated transformation, we convert the original problem into that oflarge covariance matrix estimation. The suggested method combines the strengthsof recent advances in high-dimensional sparse modeling and large covariancematrix estimation. Compared to existing approaches, our method is scalable andcan deal with much larger precision matrices with simple tuning. Under mildregularity conditions, we establish that this procedure can recover theunderlying graphical structure with significant probability and provideefficient estimation of link strengths. Both computational and theoreticaladvantages of the procedure are evidenced through simulation and real dataexamples.
arxiv-18000-65 | The constrained Dantzig selector with enhanced consistency | http://arxiv.org/pdf/1605.03311v1.pdf | author:Yinfei Kong, Zemin Zheng, Jinchi Lv category:stat.ME stat.ML published:2016-05-11 summary:The Dantzig selector has received popularity for many applications such ascompressed sensing and sparse modeling, thanks to its computational efficiencyas a linear programming problem and its nice sampling properties. Existingresults show that it can recover sparse signals mimicking the accuracy of theideal procedure, up to a logarithmic factor of the dimensionality. Such afactor has been shown to hold for many regularization methods. An importantquestion is whether this factor can be reduced to a logarithmic factor of thesample size in ultra-high dimensions under mild regularity conditions. Toprovide an affirmative answer, in this paper we suggest the constrained Dantzigselector, which has more flexible constraints and parameter space. We provethat the suggested method can achieve convergence rates within a logarithmicfactor of the sample size of the oracle rates and improved sparsity, under afairly weak assumption on the signal strength. Such improvement is significantin ultra-high dimensions. This method can be implemented efficiently throughsequential linear programming. Numerical studies confirm that the sample sizeneeded for a certain level of accuracy in these problems can be much reduced.
arxiv-18000-66 | Asymptotic equivalence of regularization methods in thresholded parameter space | http://arxiv.org/pdf/1605.03310v1.pdf | author:Yingying Fan, Jinchi Lv category:stat.ME math.ST stat.ML stat.TH published:2016-05-11 summary:High-dimensional data analysis has motivated a spectrum of regularizationmethods for variable selection and sparse modeling, with two popular classes ofconvex ones and concave ones. A long debate has been on whether one classdominates the other, an important question both in theory and to practitioners.In this paper, we characterize the asymptotic equivalence of regularizationmethods, with general penalty functions, in a thresholded parameter space underthe generalized linear model setting, where the dimensionality can grow up toexponentially with the sample size. To assess their performance, we establishthe oracle inequalities, as in Bickel, Ritov and Tsybakov (2009), of the globalminimizer for these methods under various prediction and variable selectionlosses. These results reveal an interesting phase transition phenomenon. Forpolynomially growing dimensionality, the $L_1$-regularization method of Lassoand concave methods are asymptotically equivalent, having the same convergencerates in the oracle inequalities. For exponentially growing dimensionality,concave methods are asymptotically equivalent but have faster convergence ratesthan the Lasso. We also establish a stronger property of the oracle riskinequalities of the regularization methods, as well as the sampling propertiesof computable solutions. Our new theoretical results are illustrated andjustified by simulation and real data examples.
arxiv-18000-67 | High dimensional thresholded regression and shrinkage effect | http://arxiv.org/pdf/1605.03306v1.pdf | author:Zemin Zheng, Yingying Fan, Jinchi Lv category:stat.ME stat.ML published:2016-05-11 summary:High-dimensional sparse modeling via regularization provides a powerful toolfor analyzing large-scale data sets and obtaining meaningful, interpretablemodels. The use of nonconvex penalty functions shows advantage in selectingimportant features in high dimensions, but the global optimality of suchmethods still demands more understanding. In this paper, we consider sparseregression with hard-thresholding penalty, which we show to give rise tothresholded regression. This approach is motivated by its close connection withthe $L_0$-regularization, which can be unrealistic to implement in practice butof appealing sampling properties, and its computational advantage. Under somemild regularity conditions allowing possibly exponentially growingdimensionality, we establish the oracle inequalities of the resultingregularized estimator, as the global minimizer, under various prediction andvariable selection losses, as well as the oracle risk inequalities of thehard-thresholded estimator followed by a further $L_2$-regularization. The riskproperties exhibit interesting shrinkage effects under both estimation andprediction losses. We identify the optimal choice of the ridge parameter, whichis shown to have simultaneous advantages to both the $L_2$-loss and predictionloss. These new results and phenomena are evidenced by simulation and real dataexamples.
arxiv-18000-68 | Minimizing Finite Sums with the Stochastic Average Gradient | http://arxiv.org/pdf/1309.2388v2.pdf | author:Mark Schmidt, Nicolas Le Roux, Francis Bach category:math.OC cs.LG stat.CO stat.ML published:2013-09-10 summary:We propose the stochastic average gradient (SAG) method for optimizing thesum of a finite number of smooth convex functions. Like stochastic gradient(SG) methods, the SAG method's iteration cost is independent of the number ofterms in the sum. However, by incorporating a memory of previous gradientvalues the SAG method achieves a faster convergence rate than black-box SGmethods. The convergence rate is improved from O(1/k^{1/2}) to O(1/k) ingeneral, and when the sum is strongly-convex the convergence rate is improvedfrom the sub-linear O(1/k) to a linear convergence rate of the form O(p^k) forp \textless{} 1. Further, in many cases the convergence rate of the new methodis also faster than black-box deterministic gradient methods, in terms of thenumber of gradient evaluations. Numerical experiments indicate that the newalgorithm often dramatically outperforms existing SG and deterministic gradientmethods, and that the performance may be further improved through the use ofnon-uniform sampling strategies.
arxiv-18000-69 | Multi-Scale Convolutional Neural Networks for Time Series Classification | http://arxiv.org/pdf/1603.06995v4.pdf | author:Zhicheng Cui, Wenlin Chen, Yixin Chen category:cs.CV published:2016-03-22 summary:Time series classification (TSC), the problem of predicting class labels oftime series, has been around for decades within the community of data miningand machine learning, and found many important applications such as biomedicalengineering and clinical prediction. However, it still remains challenging andfalls short of classification accuracy and efficiency. Traditional approachestypically involve extracting discriminative features from the original timeseries using dynamic time warping (DTW) or shapelet transformation, based onwhich an off-the-shelf classifier can be applied. These methods are ad-hoc andseparate the feature extraction part with the classification part, which limitstheir accuracy performance. Plus, most existing methods fail to take intoaccount the fact that time series often have features at different time scales.To address these problems, we propose a novel end-to-end neural network model,Multi-Scale Convolutional Neural Networks (MCNN), which incorporates featureextraction and classification in a single framework. Leveraging a novelmulti-branch layer and learnable convolutional layers, MCNN automaticallyextracts features at different scales and frequencies, leading to superiorfeature representation. MCNN is also computationally efficient, as it naturallyleverages GPU computing. We conduct comprehensive empirical evaluation withvarious existing methods on a large number of benchmark datasets, and show thatMCNN advances the state-of-the-art by achieving superior accuracy performancethan other leading methods.
arxiv-18000-70 | Generalized Sparse Precision Matrix Selection for Fitting Multivariate Gaussian Random Fields to Large Data Sets | http://arxiv.org/pdf/1605.03267v1.pdf | author:Sam Davanloo Tajbakhsh, Necdet Serhat Aybat, Enrique del Castillo category:stat.ML published:2016-05-11 summary:This paper generalizes the Sparse Precision matrix Selection (SPS) algorithm,proposed by Davanloo et al. (2015) for estimating scalar Gaussian Random Field(GRF) models, to the multivariate, second-order stationary case under aseparable covariance function. Theoretical convergence rates for the estimatedcovariance matrix and for the estimated parameters of the correlation functionare established. Numerical simulation results validate our theoreticalfindings. Data segmentation is used to handle large data sets.
arxiv-18000-71 | Learning to decompose for object detection and instance segmentation | http://arxiv.org/pdf/1511.06449v3.pdf | author:Eunbyung Park, Alexander C. Berg category:cs.CV cs.LG published:2015-11-19 summary:Although deep convolutional neural networks(CNNs) have achieved remarkableresults on object detection and segmentation, pre- and post-processing stepssuch as region proposals and non-maximum suppression(NMS), have been required.These steps result in high computational complexity and sensitivity tohyperparameters, e.g. thresholds for NMS. In this work, we propose a novelend-to-end trainable deep neural network architecture, which consists ofconvolutional and recurrent layers, that generates the correct number of objectinstances and their bounding boxes (or segmentation masks) given an image,using only a single network evaluation without any pre- or post-processingsteps. We have tested on detecting digits in multi-digit images synthesizedusing MNIST, automatically segmenting digits in these images, and detectingcars in the KITTI benchmark dataset. The proposed approach outperforms a strongCNN baseline on the synthesized digits datasets and shows promising results onKITTI car detection.
arxiv-18000-72 | Sensorimotor Input as a Language Generalisation Tool: A Neurorobotics Model for Generation and Generalisation of Noun-Verb Combinations with Sensorimotor Inputs | http://arxiv.org/pdf/1605.03261v1.pdf | author:Junpei Zhong, Martin Peniak, Jun Tani, Tetsuya Ogata, Angelo Cangelosi category:cs.RO cs.CL published:2016-05-11 summary:The paper presents a neurorobotics cognitive model to explain theunderstanding and generalisation of nouns and verbs combinations when a vocalcommand consisting of a verb-noun sentence is provided to a humanoid robot.This generalisation process is done via the grounding process: differentobjects are being interacted, and associated, with different motor behaviours,following a learning approach inspired by developmental language acquisition ininfants. This cognitive model is based on Multiple Time-scale Recurrent NeuralNetworks (MTRNN).With the data obtained from object manipulation tasks with ahumanoid robot platform, the robotic agent implemented with this model canground the primitive embodied structure of verbs through training withverb-noun combination samples. Moreover, we show that a functional hierarchicalarchitecture, based on MTRNN, is able to generalise and produce novelcombinations of noun-verb sentences. Further analyses of the learned networkdynamics and representations also demonstrate how the generalisation ispossible via the exploitation of this functional hierarchical recurrentnetwork.
arxiv-18000-73 | Deep Attributes Driven Multi-Camera Person Re-identification | http://arxiv.org/pdf/1605.03259v1.pdf | author:Chi Su, Shiliang Zhang, Junliang Xing, Wen Gao, Qi Tian category:cs.CV published:2016-05-11 summary:The visual appearance of a person is easily affected by many factors likepose variations, viewpoint changes and camera parameter differences. This makesperson Re-Identification (ReID) among multiple cameras a very challenging task.This work is motivated to learn mid-level human attributes which are robust tosuch visual appearance variations. And we propose a semi-supervised attributelearning framework which progressively boosts the accuracy of attributes onlyusing a limited number of labeled data. Specifically, this framework involves athree-stage training. A deep Convolutional Neural Network (dCNN) is firsttrained on an independent dataset labeled with attributes. Then it isfine-tuned on another dataset only labeled with person IDs using our definedtriplet loss. Finally, the updated dCNN predicts attribute labels for thetarget dataset, which is combined with the independent dataset for the finalround of fine-tuning. The predicted attributes, namely \emph{deep attributes}exhibit superior generalization ability across different datasets. By directlyusing the deep attributes with simple Cosine distance, we have obtainedsurprisingly good accuracy on four person ReID datasets. Experiments also showthat a simple metric learning modular further boosts our method, making itsignificantly outperform many recent works.
arxiv-18000-74 | Labeled Multi-Bernoulli Tracking for Industrial Mobile Platform Safety | http://arxiv.org/pdf/1604.05966v2.pdf | author:Tharindu Rathnayake, Reza Hoseinnezhad, Ruwan Tennakoon, Alireza Bab-Hadiashar category:cs.CV published:2016-04-20 summary:This paper presents a track-before-detect labeled multi-Bernoulli filtertailored for industrial mobile platform safety applications. We derive twoapplication specific separable likelihood functions that capture the geometricshape and colour information of the human targets who are wearing a highvisible vest. These likelihoods are then used in a labeled multi-Bernoullifilter with a novel two step Bayesian update. Preliminary simulation resultsshow that the proposed solution can successfully track human workers wearing aluminous yellow colour vest in an industrial environment.
arxiv-18000-75 | Performance Bounds for Sparse Signal Reconstruction with Multiple Side Information | http://arxiv.org/pdf/1605.03234v1.pdf | author:Huynh Van Luong, Jurgen Seiler, Andre Kaup, Soren Forchhammer, Nikos Deligiannis category:cs.IT cs.CV math.IT math.OC published:2016-05-10 summary:In the context of compressive sensing (CS), this paper considers the problemof reconstructing sparse signals with the aid of other given correlated sourcesas multiple side information (SI). To address this problem, we propose areconstruction algorithm with multiple SI (RAMSI) that solves a generalweighted $n$-$\ell_{1}$ norm minimization. The proposed RAMSI algorithm takesadvantage of both CS and the $n$-$\ell_{1}$ minimization by adaptivelycomputing optimal weights among SI signals at every reconstructed iteration. Inaddition, we establish theoretical performance bounds on the number ofmeasurements that are required to successfully reconstruct the original sparsesource using RAMSI under arbitrary support SI conditions. The analyses of theestablished bounds reveal that RAMSI can achieve sharper bounds and significantperformance improvements compared to classical CS. We evaluate experimentallythe proposed algorithm and the established bounds using synthetic sparsesignals as well as correlated feature histograms, extracted from a multiviewimage database for object recognition. The obtained results show clearly thatthe proposed RAMSI algorithm outperforms classical CS and CS with single SI interms of both the theoretical bounds and the practical performance.
arxiv-18000-76 | Exploring the Space of Adversarial Images | http://arxiv.org/pdf/1510.05328v4.pdf | author:Pedro Tabacof, Eduardo Valle category:cs.NE published:2015-10-19 summary:Adversarial examples have raised questions regarding the robustness andsecurity of deep neural networks. In this work we formalize the problem ofadversarial images given a pretrained classifier, showing that even in thelinear case the resulting optimization problem is nonconvex. We generateadversarial images using shallow and deep classifiers on the MNIST and ImageNetdatasets. We probe the pixel space of adversarial images using noise of varyingintensity and distribution. We bring novel visualizations that showcase thephenomenon and its high variability. We show that adversarial images appear inlarge regions in the pixel space, but that, for the same task, a shallowclassifier seems more robust to adversarial images than a deep convolutionalnetwork.
arxiv-18000-77 | Action Recognition in Video Using Sparse Coding and Relative Features | http://arxiv.org/pdf/1605.03222v1.pdf | author:Anali Alfaro, Domingo Mery, Alvaro Soto category:cs.CV published:2016-05-10 summary:This work presents an approach to category-based action recognition in videousing sparse coding techniques. The proposed approach includes two maincontributions: i) A new method to handle intra-class variations by decomposingeach video into a reduced set of representative atomic action acts orkey-sequences, and ii) A new video descriptor, ITRA: Inter-Temporal RelationalAct Descriptor, that exploits the power of comparative reasoning to capturerelative similarity relations among key-sequences. In terms of the method toobtain key-sequences, we introduce a loss function that, for each video, leadsto the identification of a sparse set of representative key-frames capturingboth, relevant particularities arising in the input video, as well as relevantgeneralities arising in the complete class collection. In terms of the methodto obtain the ITRA descriptor, we introduce a novel scheme to quantify relativeintra and inter-class similarities among local temporal patterns arising in thevideos. The resulting ITRA descriptor demonstrates to be highly effective todiscriminate among action categories. As a result, the proposed approachreaches remarkable action recognition performance on several popular benchmarkdatasets, outperforming alternative state-of-the-art techniques by a largemargin.
arxiv-18000-78 | Robust Bayesian Method for Simultaneous Block Sparse Signal Recovery with Applications to Face Recognition | http://arxiv.org/pdf/1605.02057v2.pdf | author:Igor Fedorov, Ritwik Giri, Bhaskar D. Rao, Truong Q. Nguyen category:cs.CV published:2016-05-06 summary:In this paper, we present a novel Bayesian approach to recover simultaneouslyblock sparse signals in the presence of outliers. The key advantage of ourproposed method is the ability to handle non-stationary outliers, i.e. outlierswhich have time varying support. We validate our approach with empiricalresults showing the superiority of the proposed method over competingapproaches in synthetic data experiments as well as the multiple measurementface recognition problem.
arxiv-18000-79 | Vocabulary Manipulation for Neural Machine Translation | http://arxiv.org/pdf/1605.03209v1.pdf | author:Haitao Mi, Zhiguo Wang, Abe Ittycheriah category:cs.CL published:2016-05-10 summary:In order to capture rich language phenomena, neural machine translationmodels have to use a large vocabulary size, which requires high computing timeand large memory usage. In this paper, we alleviate this issue by introducing asentence-level or batch-level vocabulary, which is only a very small sub-set ofthe full output vocabulary. For each sentence or batch, we only predict thetarget words in its sentence-level or batch-level vocabulary. Thus, we reduceboth the computing time and the memory usage. Our method simply takes intoaccount the translation options of each word or phrase in the source sentence,and picks a very small target vocabulary for each sentence based on aword-to-word translation model or a bilingual phrase library learned from atraditional machine translation model. Experimental results on the large-scaleEnglish-to-French task show that our method achieves better translationperformance by 1 BLEU point over the large vocabulary neural machinetranslation system of Jean et al. (2015).
arxiv-18000-80 | DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model | http://arxiv.org/pdf/1605.03170v1.pdf | author:Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, Bernt Schiele category:cs.CV published:2016-05-10 summary:The goal of this paper is to advance the state-of-the-art of articulated poseestimation in scenes with multiple people. To that end we contribute on threefronts. We propose (1) improved body part detectors that generate effectivebottom-up proposals for body parts; (2) novel image-conditioned pairwise termsthat allow to assemble the proposals into a variable number of consistent bodypart configurations; and (3) an incremental optimization strategy that exploresthe search space more efficiently thus leading both to better performance andsignificant speed-up factors. We evaluate our approach on two single-person andtwo multi-person pose estimation benchmarks. The proposed approachsignificantly outperforms best known multi-person pose estimation results whiledemonstrating competitive performance on the task of single person poseestimation. Models and code available at http://pose.mpi-inf.mpg.de
arxiv-18000-81 | Clustering subgaussian mixtures by semidefinite programming | http://arxiv.org/pdf/1602.06612v2.pdf | author:Dustin G. Mixon, Soledad Villar, Rachel Ward category:stat.ML cs.DS cs.IT cs.LG math.IT math.ST stat.TH published:2016-02-22 summary:We introduce a model-free relax-and-round algorithm for k-means clusteringbased on a semidefinite relaxation due to Peng and Wei. The algorithminterprets the SDP output as a denoised version of the original data and thenrounds this output to a hard clustering. We provide a generic method forproving performance guarantees for this algorithm, and we analyze the algorithmin the context of subgaussian mixture models. We also study the fundamentallimits of estimating Gaussian centers by k-means clustering in order to compareour approximation guarantee to the theoretically optimal k-means clusteringsolution.
arxiv-18000-82 | Road Detection through Supervised Classification | http://arxiv.org/pdf/1605.03150v1.pdf | author:Yasamin Alkhorshid, Kamelia Aryafar, Sven Bauer, Gerd Wanielik category:cs.CV published:2016-05-10 summary:Autonomous driving is a rapidly evolving technology. Autonomous vehicles arecapable of sensing their environment and navigating without human input throughsensory information such as radar, lidar, GNSS, vehicle odometry, and computervision. This sensory input provides a rich dataset that can be used incombination with machine learning models to tackle multiple problems insupervised settings. In this paper we focus on road detection throughgray-scale images as the sole sensory input. Our contributions are twofold:first, we introduce an annotated dataset of urban roads for machine learningtasks; second, we introduce a road detection framework on this dataset throughsupervised classification and hand-crafted feature vectors.
arxiv-18000-83 | A Coverage Embedding Model for Neural Machine Translation | http://arxiv.org/pdf/1605.03148v1.pdf | author:Haitao Mi, Baskaran Sankaran, Zhiguo Wang, Abe Ittycheriah category:cs.CL published:2016-05-10 summary:In this paper, we enhance the attention-based neural machine translation byadding an explicit coverage embedding model to alleviate issues of repeatingand dropping translations in NMT. For each source word, our model starts with afull coverage embedding vector, and then keeps updating it with a gatedrecurrent unit as the translation goes. All the initialized coverage embeddingsand updating matrix are learned in the training procedure. Experiments on thelarge-scale Chinese-to-English task show that our enhanced model improves thetranslation quality significantly on various test sets over the strong largevocabulary NMT system.
arxiv-18000-84 | Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize | http://arxiv.org/pdf/1511.07471v2.pdf | author:Huizhen Yu category:cs.LG published:2015-11-23 summary:We consider the emphatic temporal-difference (TD) algorithm, ETD($\lambda$),for learning the value functions of stationary policies in a discounted, finitestate and action Markov decision process. The ETD($\lambda$) algorithm wasrecently proposed by Sutton, Mahmood, and White to solve a long-standingdivergence problem of the standard TD algorithm when it is applied tooff-policy training, where data from an exploratory policy are used to evaluateother policies of interest. The almost sure convergence of ETD($\lambda$) hasbeen proved in our recent work under general off-policy training conditions,but for a narrow range of diminishing stepsize. In this paper we presentconvergence results for constrained versions of ETD($\lambda$) with constantstepsize and with diminishing stepsize from a broad range. Our resultscharacterize the asymptotic behavior of the trajectory of iterates produced bythose algorithms, and are derived by combining key properties of ETD($\lambda$)with powerful convergence theorems from the weak convergence methods instochastic approximation theory. For the case of constant stepsize, in additionto analyzing the behavior of the algorithms in the limit as the stepsizeparameter approaches zero, we also analyze their behavior for a fixed stepsizeand bound the deviations of their averaged iterates from the desired solution.These results are obtained by exploiting the weak Feller property of the Markovchains associated with the algorithms, and by using ergodic theorems for weakFeller Markov chains, in conjunction with the convergence results we get fromthe weak convergence methods. Besides ETD($\lambda$), our analysis also appliesto the off-policy TD($\lambda$) algorithm, when the divergence issue is avoidedby setting $\lambda$ sufficiently large.
arxiv-18000-85 | PARAPH: Presentation Attack Rejection by Analyzing Polarization Hypotheses | http://arxiv.org/pdf/1605.03124v1.pdf | author:Ethan M. Rudd, Manuel Gunther, Terrance E. Boult category:cs.CV published:2016-05-10 summary:For applications such as airport border control, biometric technologies thatcan process many capture subjects quickly, efficiently, with weak supervision,and with minimal discomfort are desirable. Facial recognition is particularlyappealing because it is minimally invasive yet offers relatively goodrecognition performance. Unfortunately, the combination of weak supervision andminimal invasiveness makes even highly accurate facial recognition systemssusceptible to spoofing via presentation attacks. Thus, there is great demandfor an effective and low cost system capable of rejecting such attacks.To thisend we introduce PARAPH -- a novel hardware extension that exploits differentmeasurements of light polarization to yield an image space in whichpresentation media are readily discernible from Bona Fide facialcharacteristics. The PARAPH system is inexpensive with an added cost of lessthan 10 US dollars. The system makes two polarization measurements in rapidsuccession, allowing them to be approximately pixel-aligned, with a frame ratelimited by the camera, not the system. There are no moving parts above themolecular level, due to the efficient use of twisted nematic liquid crystals.We present evaluation images using three presentation attack media next to anactual face -- high quality photos on glossy and matte paper and a video of theface on an LCD. In each case, the actual face in the image generated by PARAPHis structurally discernible from the presentations, which appear either asnoise (print attacks) or saturated images (replay attacks).
arxiv-18000-86 | Kernel-Based Structural Equation Models for Topology Identification of Directed Networks | http://arxiv.org/pdf/1605.03122v1.pdf | author:Yanning Shen, Brian Baingana, Georgios B. Giannakis category:stat.ML published:2016-05-10 summary:Structural equation models (SEMs) have been widely adopted for inference ofcausal interactions in complex networks. Recent examples include unveilingtopologies of hidden causal networks over which processes such as spreadingdiseases, or rumors propagate. The appeal of SEMs in these settings stems fromtheir simplicity and tractability, since they typically assume lineardependencies among observable variables. Acknowledging the limitations inherentto adopting linear models, the present paper advocates nonlinear SEMs, whichaccount for (possible) nonlinear dependencies among network nodes. Theadvocated approach leverages kernels as a powerful encompassing framework fornonlinear modeling, and an efficient estimator with affordable tradeoffs is putforth. Interestingly, pursuit of the novel kernel-based approach yields aconvex regularized estimator that promotes edge sparsity, and is amenable toproximal-splitting optimization methods. To this end, solvers withcomplementary merits are developed by leveraging the alternating directionmethod of multipliers, and proximal gradient iterations. Experiments conductedon simulated data demonstrate that the novel approach outperforms linear SEMswith respect to edge detection errors. Furthermore, tests on a real geneexpression dataset unveil interesting new edges that were not revealed bylinear SEMs, which could shed more light on regulatory behavior of human genes.
arxiv-18000-87 | Hardware-oriented Approximation of Convolutional Neural Networks | http://arxiv.org/pdf/1604.03168v2.pdf | author:Philipp Gysel, Mohammad Motamedi, Soheil Ghiasi category:cs.CV published:2016-04-11 summary:High computational complexity hinders the widespread usage of ConvolutionalNeural Networks (CNNs), especially in mobile devices. Hardware accelerators arearguably the most promising approach for reducing both execution time and powerconsumption. One of the most important steps in accelerator development ishardware-oriented model approximation. In this paper we present Ristretto, amodel approximation framework that analyzes a given CNN with respect tonumerical resolution used in representing weights and outputs of convolutionaland fully connected layers. Ristretto can condense models by using fixed pointarithmetic and representation instead of floating point. Moreover, Ristrettofine-tunes the resulting fixed point network. Given a maximum error toleranceof 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit.The code for Ristretto is available.
arxiv-18000-88 | Semi-Supervised Representation Learning based on Probabilistic Labeling | http://arxiv.org/pdf/1605.03072v1.pdf | author:Ershad Banijamali, Ali Ghodsi category:cs.LG published:2016-05-10 summary:In this paper we present a new way of semi-supervised representationlearning. The algorithm is based on assigning class probabilities to unlabeleddata. The approach will use Hilber-Schmidt Independence Criterion (HSIC) tofind a mapping which takes the data to a lower dimensional space. We call thisalgorithm SSRL-PL. Use of unlabeled data for learning is not always beneficialand there is no algorithm which deterministically guarantee the improvement ofthe performance by using unlabeled data. Therefore, we also propose a bound onthe performance of the algorithm which can be used to determine theeffectiveness of using the structure of unlabeled data in the algorithm.
arxiv-18000-89 | A note on the statistical view of matrix completion | http://arxiv.org/pdf/1605.03040v1.pdf | author:Tianxi Li category:stat.ML published:2016-05-10 summary:A very simple interpretation of matrix completion problem is introduced basedon statistical models. Combined with the well-known results from missing dataanalysis, such interpretation indicates that matrix completion is still a validand principled estimation procedure even without the missing completely atrandom (MCAR) assumption, which almost all of the current theoretical studiesof matrix completion assume.
arxiv-18000-90 | Destination Prediction by Trajectory Distribution Based Model | http://arxiv.org/pdf/1605.03027v1.pdf | author:Philippe C. Besse, Brendan Guillouet, Jean-Michel Loubes, Francois Royer category:stat.ML published:2016-05-10 summary:In this paper we propose a new method to predict the final destination ofvehicle trips based on their initial partial trajectories. We first review howwe obtained clustering of trajectories that describes user behaviour. Then, weexplain how we model main traffic flow patterns by a mixture of 2d Gaussiandistributions. This yielded a density based clustering of locations, whichproduces a data driven grid of similar points within each pattern. We presenthow this model can be used to predict the final destination of a new trajectorybased on their first locations using a two step procedure: We first assign thenew trajectory to the clusters it mot likely belongs. Secondly, we usecharacteristics from trajectories inside these clusters to predict the finaldestination. Finally, we present experimental results of our methods forclassification of trajectories and final destination prediction on datasets oftimestamped GPS-Location of taxi trips. We test our methods on two differentdatasets, to assess the capacity of our method to adapt automatically todifferent subsets.
arxiv-18000-91 | Automatic 3D liver location and segmentation via convolutional neural networks and graph cut | http://arxiv.org/pdf/1605.03012v1.pdf | author:Fang Lu, Fa Wu, Peijun Hu, Zhiyi Peng, Dexing Kong category:cs.CV published:2016-05-10 summary:Purpose Segmentation of the liver from abdominal computed tomography (CT)image is an essential step in some computer assisted clinical interventions,such as surgery planning for living donor liver transplant (LDLT), radiotherapyand volume measurement. In this work, we develop a deep learning algorithm withgraph cut refinement to automatically segment liver in CT scans. Methods Theproposed method consists of two main steps: (i) simultaneously liver detectionand probabilistic segmentation using 3D convolutional neural networks (CNNs);(ii) accuracy refinement of initial segmentation with graph cut and thepreviously learned probability map. Results The proposed approach was validatedon forty CT volumes taken from two public databases MICCAI-Sliver07 and3Dircadb. For the MICCAI-Sliver07 test set, the calculated mean ratios ofvolumetric overlap error (VOE), relative volume difference (RVD), averagesymmetric surface distance (ASD), root mean square symmetric surface distance(RMSD) and maximum symmetric surface distance (MSD) are 5.9%, 2.7%, 0.91%, 1.88mm, and 18.94 mm, respectively. In the case of 20 3Dircadb data, the calculatedmean ratios of VOE, RVD, ASD, RMSD and MSD are 9.36%, 0.97%, 1.89%, 4.15 mm and33.14 mm, respectively. Conclusion The proposed method is fully automaticwithout any user interaction. Quantitative results reveal that the proposedapproach is efficient and accurate for hepatic volume estimation in a clinicalsetup. The high correlation between the automatic and manual references showsthat the proposed method can be good enough to replace the time-consuming andnon-reproducible manual segmentation method.
arxiv-18000-92 | MUST-CNN: A Multilayer Shift-and-Stitch Deep Convolutional Architecture for Sequence-based Protein Structure Prediction | http://arxiv.org/pdf/1605.03004v1.pdf | author:Zeming Lin, Jack Lanchantin, Yanjun Qi category:cs.LG published:2016-05-10 summary:Predicting protein properties such as solvent accessibility and secondarystructure from its primary amino acid sequence is an important task inbioinformatics. Recently, a few deep learning models have surpassed thetraditional window based multilayer perceptron. Taking inspiration from theimage classification domain we propose a deep convolutional neural networkarchitecture, MUST-CNN, to predict protein properties. This architecture uses anovel multilayer shift-and-stitch (MUST) technique to generate fully denseper-position predictions on protein sequences. Our model is significantlysimpler than the state-of-the-art, yet achieves better results. By combiningMUST and the efficient convolution operation, we can consider far moreparameters while retaining very fast prediction speeds. We beat thestate-of-the-art performance on two large protein property prediction datasets.
arxiv-18000-93 | An efficient K-means algorithm for Massive Data | http://arxiv.org/pdf/1605.02989v1.pdf | author:Marco Capó, Aritz Pérez, José Antonio Lozano category:stat.ML cs.LG published:2016-05-10 summary:Due to the progressive growth of the amount of data available in a widevariety of scientific fields, it has become more difficult to ma- nipulate andanalyze such information. Even though datasets have grown in size, the K-meansalgorithm remains as one of the most popular clustering methods, in spite ofits dependency on the initial settings and high computational cost, especiallyin terms of distance computations. In this work, we propose an efficientapproximation to the K-means problem intended for massive data. Our approachrecursively partitions the entire dataset into a small number of sub- sets,each of which is characterized by its representative (center of mass) andweight (cardinality), afterwards a weighted version of the K-means algorithm isapplied over such local representation, which can drastically reduce the numberof distances computed. In addition to some theoretical properties, experimentalresults indicate that our method outperforms well-known approaches, such as theK-means++ and the minibatch K-means, in terms of the relation between number ofdistance computations and the quality of the approximation.
arxiv-18000-94 | Weakly Supervised Learning of Affordances | http://arxiv.org/pdf/1605.02964v1.pdf | author:Abhilash Srikantha, Juergen Gall category:cs.CV published:2016-05-10 summary:Localizing functional regions of objects or affordances is an importantaspect of scene understanding. In this work, we cast the problem of affordancesegmentation as that of semantic image segmentation. In order to explorevarious levels of supervision, we introduce a pixel-annotated affordancedataset of 3090 images containing 9916 object instances with rich contextualinformation in terms of human-object interactions. We use a deep convolutionalneural network within an expectation maximization framework to take advantageof weakly labeled data like image level annotations or keypoint annotations. Weshow that a further reduction in supervision is possible with a minimal loss inperformance when human pose is used as context.
arxiv-18000-95 | A Bayesian Approach to Biomedical Text Summarization | http://arxiv.org/pdf/1605.02948v1.pdf | author:Milad Moradi, Nasser Ghadiri category:cs.CL cs.IR I.2.7; J.3 published:2016-05-10 summary:Many biomedical researchers and clinicians are faced with the informationoverload problem. Attaining desirable information from the ever-increasing bodyof knowledge is a difficult task without using automatic text summarizationtools that help them to acquire the intended information in shorter time andwith less effort. Although many text summarization methods have been proposed,developing domain-specific methods for the biomedical texts is a challengingtask. In this paper, we propose a biomedical text summarization method, basedon concept extraction technique and a novel sentence classification approach.We incorporate domain knowledge by utilizing the UMLS knowledge source and thena\"ive Bayes classifier to build our text summarizer. Unlike many existingmethods, the system learns to classify the sentences without the need fortraining data, and selects them for the summary according to the distributionof essential concepts within the original text. We show that the use ofcritical concepts to represent the sentences as vectors of features, andclassifying the sentences based on the distribution of those concepts, willimprove the performance of automatic summarization. An extensive evaluation isperformed on a collection of scientific articles in biomedical domain. Theresults show that our proposed method outperforms several well-knownresearch-based, commercial and baseline summarizers according to the mostcommonly used ROUGE evaluation metrics.
arxiv-18000-96 | Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine | http://arxiv.org/pdf/1605.02917v1.pdf | author:Seyed Hamid Reza Mohammadi, Mohammad Ali Zare Chahooki category:cs.IR cs.LG published:2016-05-10 summary:Search engines are the most important tools for web data acquisition. Webpages are crawled and indexed by search Engines. Users typically locate usefulweb pages by querying a search engine. One of the challenges in search enginesadministration is spam pages which waste search engine resources. These pagesby deception of search engine ranking algorithms try to be showed in the firstpage of results. There are many approaches to web spam pages detection such asmeasurement of HTML code style similarity, pages linguistic pattern analysisand machine learning algorithm on page content features. One of the famousalgorithms has been used in machine learning approach is Support Vector Machine(SVM) classifier. Recently basic structure of SVM has been changed by newextensions to increase robustness and classification accuracy. In this paper weimproved accuracy of web spam detection by using two nonlinear kernels intoTwin SVM (TSVM) as an improved extension of SVM. The classifier ability to dataseparation has been increased by using two separated kernels for each class ofdata. Effectiveness of new proposed method has been experimented with twopublicly used spam datasets called UK-2007 and UK-2006. Results show theeffectiveness of proposed kernelized version of TSVM in web spam pagedetection.
arxiv-18000-97 | Grammatical Case Based IS-A Relation Extraction with Boosting for Polish | http://arxiv.org/pdf/1605.02916v1.pdf | author:Paweł Łoziński, Dariusz Czerski, Mieczysław A. Kłopotek category:cs.CL cs.IR H.3.1 published:2016-05-10 summary:Pattern-based methods of IS-A relation extraction rely heavily on so calledHearst patterns. These are ways of expressing instance enumerations of a classin natural language. While these lexico-syntactic patterns prove quite useful,they may not capture all taxonomical relations expressed in text. Therefore inthis paper we describe a novel method of IS-A relation extraction frompatterns, which uses morpho-syntactical annotations along with grammatical caseof noun phrases that constitute entities participating in IS-A relation. Wealso describe a method for increasing the number of extracted relations that wecall pseudo-subclass boosting which has potential application in anypattern-based relation extraction method. Experiments were conducted on acorpus of about 0.5 billion web documents in Polish language.
arxiv-18000-98 | Recurrent Human Pose Estimation | http://arxiv.org/pdf/1605.02914v1.pdf | author:Vasileios Belagiannis, Andrew Zisserman category:cs.CV cs.NE published:2016-05-10 summary:We propose a novel ConvNet model for predicting 2D human body poses in animage. The model regresses a heatmap representation for each body keypoint, andis able to learn and represent both the part appearances and the context of thepart configuration. We make the following three contributions: (i) anarchitecture combining a feed forward module with a recurrent module, where therecurrent module can be run iteratively to improve the performance; (ii) themodel can be trained end-to-end and from scratch, with auxiliary lossesincorporated to improve performance; (iii) we investigate whether keypointvisibility can also be predicted. The model is evaluated on two benchmarkdatasets. The result is a simple architecture that achieves performance on parwith the state of the art, but without the complexity of a graphical modelstage (or layers).
arxiv-18000-99 | Compact Hash Codes for Efficient Visual Descriptors Retrieval in Large Scale Databases | http://arxiv.org/pdf/1605.02892v1.pdf | author:Simone Ercoli, Marco Bertini, Alberto Del Bimbo category:cs.CV published:2016-05-10 summary:In this paper we present an efficient method for visual descriptors retrievalbased on compact hash codes computed using a multiple k-means assignment. Themethod has been applied to the problem of approximate nearest neighbor (ANN)search of local and global visual content descriptors, and it has been testedon different datasets: three large scale public datasets of up to one billiondescriptors (BIGANN) and, supported by recent progress in convolutional neuralnetworks (CNNs), also on the CIFAR-10 and MNIST datasets. Experimental resultsshow that, despite its simplicity, the proposed method obtains a very highperformance that makes it superior to more complex state-of-the-art methods.
arxiv-18000-100 | Learning theory estimates with observations from general stationary stochastic processes | http://arxiv.org/pdf/1605.02887v1.pdf | author:Hanyuan Hang, Yunlong Feng, Ingo Steinwart, Johan A. K. Suykens category:stat.ML cs.LG published:2016-05-10 summary:This paper investigates the supervised learning problem with observationsdrawn from certain general stationary stochastic processes. Here by\emph{general}, we mean that many stationary stochastic processes can beincluded. We show that when the stochastic processes satisfy a generalizedBernstein-type inequality, a unified treatment on analyzing the learningschemes with various mixing processes can be conducted and a sharp oracleinequality for generic regularized empirical risk minimization schemes can beestablished. The obtained oracle inequality is then applied to deriveconvergence rates for several learning schemes such as empirical riskminimization (ERM), least squares support vector machines (LS-SVMs) using givengeneric kernels, and SVMs using Gaussian kernels for both least squares andquantile regression. It turns out that for i.i.d.~processes, our learning ratesfor ERM recover the optimal rates. On the other hand, for non-i.i.d.~processesincluding geometrically $\alpha$-mixing Markov processes, geometrically$\alpha$-mixing processes with restricted decay, $\phi$-mixing processes, and(time-reversed) geometrically $\mathcal{C}$-mixing processes, our learningrates for SVMs with Gaussian kernels match, up to some arbitrarily small extraterm in the exponent, the optimal rates. For the remaining cases, our rates areat least close to the optimal rates. As a by-product, the assumed generalizedBernstein-type inequality also provides an interpretation of the so-called"effective number of observations" for various mixing processes.
arxiv-18000-101 | Adaptive Combination of l0 LMS Adaptive Filters for Sparse System Identification in Fluctuating Noise Power | http://arxiv.org/pdf/1605.02878v1.pdf | author:Bijit Kumar Das, Mrityunjoy Chakraborty category:cs.IT cs.LG math.IT published:2016-05-10 summary:Recently, the l0-least mean square (l0-LMS) algorithm has been proposed toidentify sparse linear systems by employing a sparsity-promoting continuousfunction as an approximation of l0 pseudonorm penalty. However, the performanceof this algorithm is sensitive to the appropriate choice of the some parameterresponsible for the zero-attracting intensity. The optimum choice for thisparameter depends on the signal-to-noise ratio (SNR) prevailing in the system.Thus, it becomes difficult to fix a suitable value for this parameter,particularly in a situation where SNR fluctuates over time. In this work, wepropose several adaptive combinations of differently parameterized l0-LMS toget an overall satisfactory performance independent of the SNR, and discusssome issues relevant to these combination structures. We also demonstrate anefficient partial update scheme which not only reduces the number ofcomputations per iteration, but also achieves some interesting performance gaincompared with the full update case. Then, we propose a new recursive leastsquares (RLS)-type rule to update the combining parameter more efficiently.Finally, we extend the combination of two filters to a combination of M numberadaptive filters, which manifests further improvement for M > 2.
arxiv-18000-102 | Performance Analysis of the Gradient Comparator LMS Algorithm | http://arxiv.org/pdf/1605.02877v1.pdf | author:Bijit Kumar Das, Mrityunjoy Chakraborty category:cs.IT cs.LG math.IT published:2016-05-10 summary:The sparsity-aware zero attractor least mean square (ZA-LMS) algorithmmanifests much lower misadjustment in strongly sparse environment than itssparsity-agnostic counterpart, the least mean square (LMS), but is shown toperform worse than the LMS when sparsity of the impulse response decreases. Thereweighted variant of the ZA-LMS, namely RZA-LMS shows robustness against thisvariation in sparsity, but at the price of increased computational complexity.The other variants such as the l 0 -LMS and the improved proportionatenormalized LMS (IPNLMS), though perform satisfactorily, are alsocomputationally intensive. The gradient comparator LMS (GC-LMS) is a practicalsolution of this trade-off when hardware constraint is to be considered. Inthis paper, we analyse the mean and the mean square convergence performance ofthe GC-LMS algorithm in detail. The analyses satisfactorily match with thesimulation results.
arxiv-18000-103 | Modeling Short Over-Dispersed Spike-Train Data: A Hierarchical Parametric Empirical Bayes Framework | http://arxiv.org/pdf/1605.02869v1.pdf | author:Qi She, Beth Jelfs, Rosa H. M. Chan category:q-bio.QM q-bio.NC stat.ML published:2016-05-10 summary:In this letter, a Hierarchical Parametric Empirical Bayes (HPEB) model isproposed to fit spike count data. We have integrated Generalized Linear Modelsand empirical Bayes theory to simultaneously solve three problems: (1)over-dispersion of spike count values; (2) biased estimation of the maximumlikelihood method and (3) difficulty in sampling from high-dimensional datawith fully Bayes estimators. We apply the model to study both simulated dataand experimental neural data from the retina. The simulation results indicatethat the new model can estimate both the weights of connections among neuralpopulations and the output firing rates efficiently and accurately. The resultsfrom the retinal datasets show that the proposed model outperforms bothstandard Poisson and Negative Binomial Generalized Linear Models in terms ofthe prediction log-likelihood of held-out datasets.
arxiv-18000-104 | Maximal Sparsity with Deep Networks? | http://arxiv.org/pdf/1605.01636v2.pdf | author:Bo Xin, Yizhou Wang, Wen Gao, David Wipf category:cs.LG published:2016-05-05 summary:The iterations of many sparse estimation algorithms are comprised of a fixedlinear filter cascaded with a thresholding nonlinearity, which collectivelyresemble a typical neural network layer. Consequently, a lengthy sequence ofalgorithm iterations can be viewed as a deep network with shared, hand-craftedlayer weights. It is therefore quite natural to examine the degree to which alearned network model might act as a viable surrogate for traditional sparseestimation in domains where ample training data is available. While thepossibility of a reduced computational budget is readily apparent when aceiling is imposed on the number of layers, our work primarily focuses onestimation accuracy. In particular, it is well-known that when a signaldictionary has coherent columns, as quantified by a large RIP constant, thenmost tractable iterative algorithms are unable to find maximally sparserepresentations. In contrast, we demonstrate both theoretically and empiricallythe potential for a trained deep network to recover minimal $\ell_0$-normrepresentations in regimes where existing methods fail. The resulting system isdeployed on a practical photometric stereo estimation problem, where the goalis to remove sparse outliers that can disrupt the estimation of surface normalsfrom a 3D scene.
arxiv-18000-105 | Fast and High-Quality Bilateral Filtering Using Gauss-Chebyshev Approximation | http://arxiv.org/pdf/1605.02178v2.pdf | author:Sanjay Ghosh, Kunal N. Chaudhury category:cs.CV published:2016-05-07 summary:The bilateral filter is an edge-preserving smoother that has diverseapplications in image processing, computer vision, computer graphics, andcomputational photography. The filter uses a spatial kernel along with a rangekernel to perform edge-preserving smoothing. In this paper, we consider theGaussian bilateral filter where both the kernels are Gaussian. A directimplementation of the Gaussian bilateral filter requires $O(\sigma_s^2)$operations per pixel, where $\sigma_s$ is the standard deviation of the spatialGaussian. In fact, it is well-known that the direct implementation is slow inpractice. We present an approximation of the Gaussian bilateral filter, wherebywe can cut down the number of operations to $O(1)$ per pixel for any arbitrary$\sigma_s$, and yet achieve very high-quality filtering that is almostindistinguishable from the output of the original filter. We demonstrate thatthe proposed approximation is few orders faster in practice compared to thedirect implementation. We also demonstrate that the approximation iscompetitive with existing fast algorithms in terms of speed and accuracy.
arxiv-18000-106 | Decoding Stacked Denoising Autoencoders | http://arxiv.org/pdf/1605.02832v1.pdf | author:Sho Sonoda, Noboru Murata category:cs.LG stat.ML published:2016-05-10 summary:Data representation in a stacked denoising autoencoder is investigated.Decoding is a simple technique for translating a stacked denoising autoencoderinto a composition of denoising autoencoders in the ground space. In theinfinitesimal limit, a composition of denoising autoencoders is reduced to acontinuous denoising autoencoder, which is rich in analytic properties andgeometric interpretation. For example, the continuous denoising autoencodersolves the backward heat equation and transports each data point so as todecrease entropy of the data distribution. Together with ridgelet analysis, anintegral representation of a stacked denoising autoencoder is derived.
arxiv-18000-107 | A Selection of Giant Radio Sources from NVSS | http://arxiv.org/pdf/1603.06895v2.pdf | author:D. D. Proctor category:astro-ph.GA cs.CV stat.ML published:2016-03-22 summary:Results of the application of pattern recognition techniques to the problemof identifying Giant Radio Sources (GRS) from the data in the NVSS catalog arepresented and issues affecting the process are explored. Decision-tree patternrecognition software was applied to training set source pairs developed fromknown NVSS large angular size radio galaxies. The full training set consistedof 51,195 source pairs, 48 of which were known GRS for which each lobe wasprimarily represented by a single catalog component. The source pairs had amaximum separation of 20 arc minutes and a minimum component area of 1.87square arc minutes at the 1.4 mJy level. The importance of comparing resultingprobability distributions of the training and application sets for cases ofunknown class ratio is demonstrated. The probability of correctly ranking arandomly selected (GRS, non-GRS) pair from the best of the tested classifierswas determined to be 97.8 +/- 1.5%. The best classifiers were applied to theover 870,000 candidate pairs from the entire catalog. Images of higher rankedsources were visually screened and a table of over sixteen hundred candidates,including morphological annotation, is presented. These systems include doublesand triples, Wide-Angle Tail (WAT) and Narrow-Angle Tail (NAT), S- or Z-shapedsystems, and core-jets and resolved cores. While some resolved lobe systems arerecovered with this technique, generally it is expected that such systems wouldrequire a different approach.
arxiv-18000-108 | Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality | http://arxiv.org/pdf/1506.07216v3.pdf | author:Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, David P. Woodruff category:cs.LG cs.CC cs.IT math.IT stat.ML published:2015-06-24 summary:We study the tradeoff between the statistical error and communication cost ofdistributed statistical estimation problems in high dimensions. In thedistributed sparse Gaussian mean estimation problem, each of the $m$ machinesreceives $n$ data points from a $d$-dimensional Gaussian distribution withunknown mean $\theta$ which is promised to be $k$-sparse. The machinescommunicate by message passing and aim to estimate the mean $\theta$. Weprovide a tight (up to logarithmic factors) tradeoff between the estimationerror and the number of bits communicated between the machines. This directlyleads to a lower bound for the distributed \textit{sparse linear regression}problem: to achieve the statistical minimax error, the total communication isat least $\Omega(\min\{n,d\}m)$, where $n$ is the number of observations thateach machine receives and $d$ is the ambient dimension. These lower resultsimprove upon [Sha14,SD'14] by allowing multi-round iterative communicationmodel. We also give the first optimal simultaneous protocol in the dense casefor mean estimation. As our main technique, we prove a \textit{distributed data processinginequality}, as a generalization of usual data processing inequalities, whichmight be of independent interest and useful for other problems.
arxiv-18000-109 | You Only Look Once: Unified, Real-Time Object Detection | http://arxiv.org/pdf/1506.02640v5.pdf | author:Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi category:cs.CV published:2015-06-08 summary:We present YOLO, a new approach to object detection. Prior work on objectdetection repurposes classifiers to perform detection. Instead, we frame objectdetection as a regression problem to spatially separated bounding boxes andassociated class probabilities. A single neural network predicts bounding boxesand class probabilities directly from full images in one evaluation. Since thewhole detection pipeline is a single network, it can be optimized end-to-enddirectly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processesimages in real-time at 45 frames per second. A smaller version of the network,Fast YOLO, processes an astounding 155 frames per second while still achievingdouble the mAP of other real-time detectors. Compared to state-of-the-artdetection systems, YOLO makes more localization errors but is far less likelyto predict false detections where nothing exists. Finally, YOLO learns verygeneral representations of objects. It outperforms all other detection methods,including DPM and R-CNN, by a wide margin when generalizing from natural imagesto artwork on both the Picasso Dataset and the People-Art Dataset.
arxiv-18000-110 | XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks | http://arxiv.org/pdf/1603.05279v3.pdf | author:Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi category:cs.CV published:2016-03-16 summary:We propose two efficient approximations to standard convolutional neuralnetworks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks,the filters are approximated with binary values resulting in 32x memory saving.In XNOR-Networks, both the filters and the input to convolutional layers arebinary. XNOR-Networks approximate convolutions using primarily binaryoperations. This results in 58x faster convolutional operations and 32x memorysavings. XNOR-Nets offer the possibility of running state-of-the-art networkson CPUs (rather than GPUs) in real-time. Our binary networks are simple,accurate, efficient, and work on challenging visual tasks. We evaluate ourapproach on the ImageNet classification task. The classification accuracy witha Binary-Weight-Network version of AlexNet is only 2.9% less than thefull-precision AlexNet (in top-1 measure). We compare our method with recentnetwork binarization methods, BinaryConnect and BinaryNets, and outperformthese methods by large margins on ImageNet, more than 16% in top-1 accuracy.
arxiv-18000-111 | Finding Common Characteristics Among NBA Playoff and Championship Teams: A Machine Learning Approach | http://arxiv.org/pdf/1604.05266v3.pdf | author:Ikjyot Singh Kohli category:stat.ML stat.AP published:2016-04-18 summary:In this paper, we employ machine learning techniques to analyze sixteenseasons of NBA regular season data from every team to determine the commoncharacteristics among NBA playoff teams. Each team was characterized by 42predictor variables and one binary response variable taking on a value of"TRUE" if a team had made the playoffs, and value of "FALSE" if a team hadmissed the playoffs. After fitting an initial classification tree to thisproblem, this tree was then pruned which decreased the test error rate. Furtherto this, a random forest of classification trees was grown which provided avery accurate model from which a variable importance plot was generated todetermine which predictor variables had the greatest influence on the responsevariable. The result of this work was the conclusion that the most importantfactors in characterizing a team's playoff eligibility are the opponent fieldgoal percentage and the opponent points per game. This seems to suggest that\emph{defensive} factors as opposed to offensive factors are the most importantcharacteristics shared among NBA playoff teams. We also perform aclassification analysis to determine common characteristics among NBAchampionship teams. Using an artificial neural network structure, we show thatchampionship teams must be able to have very strong defensive characteristics,in particular, strong perimeter defense characteristics in combination with aneffective half-court offense that generates high-percentage two-point shots. Akey part of this offensive strategy must also be the ability to draw fouls.This analysis will hopefully dispel the rising notion that an offense gearedtowards shooting many three point shots is a sufficient and necessary conditionfor an NBA team to be successful in qualifying for the playoffs and winning achampionship.
arxiv-18000-112 | Computer Vision Approach for Low Cost, High Precision Measurement of Grapevine Trunk Diameter in Outdoor Conditions | http://arxiv.org/pdf/1406.4845v2.pdf | author:Diego Sebastián Pérez, Facundo Bromberg, Francisco Gonzalez Antivilo category:cs.CV published:2014-05-16 summary:Trunk diameter is a variable of agricultural interest, used mainly in theprediction of fruit trees production. It is correlated with leaf area andbiomass of trees, and consequently gives a good estimate of the potentialproduction of the plants. This work presents a low cost, high precision methodfor the measurement of trunk diameter of grapevines based on Computer Visiontechniques. Several methods based on Computer Vision and other techniques areintroduced in the literature. These methods present different advantages forcrop management: they are amenable to be operated by unknowledgeable personnel,with lower operational costs; they result in lower stress levels toknowledgeable personnel, avoiding the deterioration of the measurement qualityover time; and they make the measurement process amenable to be embedded inlarger autonomous systems, allowing more measurements to be taken withequivalent costs. To date, all existing autonomous methods are either of lowprecision, or have a prohibitive cost for massive agricultural adoption,leaving the manual Vernier caliper or tape measure as the only choice in mostsituations. In this work we present a semi-autonomous measurement method thatis susceptible to be fully automated, cost effective for mass adoption, and itsprecision is competitive (with slight improvements) over the caliper manualmethod.
arxiv-18000-113 | Identification of refugee influx patterns in Greece via model-theoretic analysis of daily arrivals | http://arxiv.org/pdf/1605.02784v1.pdf | author:Harris V. Georgiou category:stat.ML cs.CY published:2016-05-09 summary:The refugee crisis is perhaps the single most challenging problem for Europetoday. Hundreds of thousands of people have already traveled across dangeroussea passages from Turkish shores to Greek islands, resulting in thousands ofdead and missing, despite the best rescue efforts from both sides. One of themain reasons is the total lack of any early warning-alerting system, whichcould provide some preparation time for the prompt and effective deployment ofresources at the hot zones. This work is such an attempt for a systemicanalysis of the refugee influx in Greece, aiming at (a) the statistical andsignal-level characterization of the smuggling networks and (b) the formulationand preliminary assessment of such models for predictive purposes, i.e., as thebasis of such an early warning-alerting protocol. To our knowledge, this is thefirst-ever attempt to design such a system, since this refugee crisis itselfand its geographical properties are unique (intense event handling, little orno warning). The analysis employs a wide range of statistical, signal-based andmatrix factorization (decomposition) techniques, including linear &linear-cosine regression, spectral analysis, ARMA, SVD, Probabilistic PCA, ICA,K-SVD for Dictionary Learning, as well as fractal dimension analysis. It isestablished that the behavioral patterns of the smuggling networks closelymatch (as expected) the regular burst and pause periods of store-and-forwardnetworks in digital communications. There are also major periodic trends in therange of 6.2-6.5 days and strong correlations in lags of four or more days,with distinct preference in the Sunday-Monday 48-hour time frame. These resultsshow that such models can be used successfully for short-term forecasting ofthe influx intensity, producing an invaluable operational asset for planners,decision-makers and first-responders.
arxiv-18000-114 | Arm muscular effort estimation from images using Computer Vision and Machine Learning | http://arxiv.org/pdf/1605.02783v1.pdf | author:Leandro Abraham, Facundo Bromberg, Raymundo Forradellas category:cs.CV published:2016-05-09 summary:A problem of great interest in biomechanics is the estimation of internalmuscle forces and joint torques. Direct measurement of these variables is notpossible, so it is addressed in practice using mechanical models that takes asinput kinematics and muscle activity. Input data for these models is capturedusing impractical, intrusive and expensive devices. In this work we presentfirsts steps towards capturing muscle activity of the arm biceps through a lessintrusive and more economic process. First, we consider an isometriccontraction setup for which muscle activity directly correlates with externalweight supported by the muscle. Then, using Computer Vision feature extractionalgorithms (Local Binary Patterns and Color Histograms) and Machine Learningalgorithms we learn a classifier (e.g., Support Vector Machines and RandomForests) for inferring the weight held by the muscle. We consider two similarproblems: discriminating between two weights and discriminating between fourtarget weights. Also, we consider the use of calibration information to bettergeneralize over unseen subjects. We obtained promising results showingaccuracies of $79.78\%$ and $42.34\%$ for the cases with calibration of two andfour levels respectively. The main difficulty of this approach is to generalizethe learned model over unseen subjects, due to the great differences existingamong human subjects arms. Considering the difficulty in the requirement ofgeneralization, and the simplicity of our approach, it is enlightening toachieve over random results, suggesting that it is possible to extractmeaningful information for the predictive task.
arxiv-18000-115 | Image Classification of Grapevine Buds using Scale-Invariant Features Transform, Bag of Features and Support Vector Machines | http://arxiv.org/pdf/1605.02775v1.pdf | author:Diego Sebastián Pérez, Facundo Bromberg, Carlos Ariel Diaz category:cs.CV published:2016-05-09 summary:In viticulture, there are several applications where bud detection invineyard images is a necessary task, susceptible of being automated through theuse of computer vision methods. A common and effective family of visualdetection algorithms are the scanning-window type, that slide a (usually) fixedsize window along the original image, classifying each resulting windowed-patchas containing or not containing the target object. The simplicity of thesealgorithms finds its most challenging aspect in the classification stage.Interested in grapevine buds detection in natural field conditions, this paperpresents a classification method for images of grapevine buds ranging 100 to1600 pixels in diameter, captured in outdoor, under natural field conditions,in winter (i.e., no grape bunches, very few leaves, and dormant buds), withoutartificial background, and with minimum equipment requirements. The proposedmethod uses well-known computer vision technologies: Scale-Invariant FeatureTransform for calculating low-level features, Bag of Features for building animage descriptor, and Support Vector Machines for training a classifier. Whenevaluated over images containing buds of at least 100 pixels in diameter, theapproach achieves a recall higher than 0.9 and a precision of 0.86 over allwindowed-patches covering the whole bud and down to 60% of it, and scaled up towindow patches containing a proportion of 20%-80% of bud versus backgroundpixels. This robustness on the position and size of the window demonstrates itsviability for use as the classification stage in a scanning-window detectionalgorithms.
arxiv-18000-116 | Anytime Bi-Objective Optimization with a Hybrid Multi-Objective CMA-ES (HMO-CMA-ES) | http://arxiv.org/pdf/1605.02720v1.pdf | author:Ilya Loshchilov, Tobias Glasmachers category:cs.NE published:2016-05-09 summary:We propose a multi-objective optimization algorithm aimed at achieving goodanytime performance over a wide range of problems. Performance is assessed interms of the hypervolume metric. The algorithm called HMO-CMA-ES represents ahybrid of several old and new variants of CMA-ES, complemented by BOBYQA as awarm start. We benchmark HMO-CMA-ES on the recently introduced bi-objectiveproblem suite of the COCO framework (COmparing Continuous Optimizers),consisting of 55 scalable continuous optimization problems, which is used bythe Black-Box Optimization Benchmarking (BBOB) Workshop 2016.
arxiv-18000-117 | Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning | http://arxiv.org/pdf/1605.02711v1.pdf | author:Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, Jarvis Haupt category:cs.LG math.OC stat.ML published:2016-05-09 summary:We propose a stochastic variance reduced optimization algorithm for solving aclass of large-scale nonconvex optimization problems with cardinalityconstraints, and provide sufficient conditions under which the proposedalgorithm enjoys strong linear convergence guarantees and optimal estimationaccuracy in high dimensions. We further extend our analysis to an asynchronousvariant of the approach, and demonstrate a near linear speedup in sparsesettings. Numerical experiments demonstrate the efficiency of our method interms of both parameter estimation and computational performance.
arxiv-18000-118 | A Theoretical Analysis of Deep Neural Networks for Texture Classification | http://arxiv.org/pdf/1605.02699v1.pdf | author:Saikat Basu, Manohar Karki, Robert DiBiano, Supratik Mukhopadhyay, Sangram Ganguly, Ramakrishna Nemani, Shreekant Gayaka category:cs.CV cs.LG stat.ML published:2016-05-09 summary:We investigate the use of Deep Neural Networks for the classification ofimage datasets where texture features are important for generatingclass-conditional discriminative representations. To this end, we first derivethe size of the feature space for some standard textural features extractedfrom the input dataset and then use the theory of Vapnik-Chervonenkis dimensionto show that hand-crafted feature extraction creates low-dimensionalrepresentations which help in reducing the overall excess error rate. As acorollary to this analysis, we derive for the first time upper bounds on the VCdimension of Convolutional Neural Network as well as Dropout and Dropconnectnetworks and the relation between excess error rate of Dropout and Dropconnectnetworks. The concept of intrinsic dimension is used to validate the intuitionthat texture-based datasets are inherently higher dimensional as compared tohandwritten digits or other object recognition datasets and hence moredifficult to be shattered by neural networks. We then derive the mean distancefrom the centroid to the nearest and farthest sampling points in ann-dimensional manifold and show that the Relative Contrast of the sample datavanishes as dimensionality of the underlying vector space tends to infinity.
arxiv-18000-119 | Ask Your Neurons: A Deep Learning Approach to Visual Question Answering | http://arxiv.org/pdf/1605.02697v1.pdf | author:Mateusz Malinowski, Marcus Rohrbach, Mario Fritz category:cs.CV cs.AI cs.CL published:2016-05-09 summary:We address a question answering task on real-world images that is set up as aVisual Turing Test. By combining latest advances in image representation andnatural language processing, we propose Ask Your Neurons, a scalable, jointlytrained, end-to-end formulation to this problem. In contrast to previous efforts, we are facing a multi-modal problem wherethe language output (answer) is conditioned on visual and natural languageinputs (image and question). We provide additional insights into the problem byanalyzing how much information is contained only in the language part for whichwe provide a new human baseline. To study human consensus, which is related tothe ambiguities inherent in this challenging task, we propose two novel metricsand collect additional answers which extend the original DAQUAR dataset toDAQUAR-Consensus. Moreover, we also extend our analysis to VQA, a large-scale questionanswering about images dataset, where we investigate some particular designchoices and show the importance of stronger visual models. At the same time, weachieve strong performance of our model that still uses a global imagerepresentation. Finally, based on such analysis, we refine our Ask Your Neuronson DAQUAR, which also leads to a better performance on this challenging task.
arxiv-18000-120 | Inference of High-dimensional Autoregressive Generalized Linear Models | http://arxiv.org/pdf/1605.02693v1.pdf | author:Eric C. Hall, Garvesh Raskutti, Rebecca Willett category:stat.ML cs.IT math.IT math.ST stat.TH published:2016-05-09 summary:Vector autoregressive models characterize a variety of time series in whichlinear combinations of current and past observations can be used to accuratelypredict future observations. For instance, each element of an observationvector could correspond to a different node in a network, and the parameters ofan autoregressive model would correspond to the impact of the network structureon the time series evolution. Often these models are used successfully inpractice to learn the structure of social, epidemiological, financial, orbiological neural networks. However, little is known about statisticalguarantees of estimates of such models in non-Gaussian settings. This paperaddresses the inference of the autoregressive parameters and associated networkstructure within a generalized linear model framework that includes Poisson andBernoulli autoregressive processes. At the heart of this analysis is asparsity-regularized maximum likelihood estimator. Whilesparsity-regularization is well-studied in the statistics and machine learningcommunities, those analysis methods cannot be applied to autoregressivegeneralized linear models because of the correlations and potentialheteroscedasticity inherent in the observations. Sample complexity bounds arederived using a combination of martingale concentration inequalities andmodified covering techniques originally proposed for high-dimensional linearregression analysis. These bounds, which are supported by several simulationstudies, characterize the impact of various network parameters on estimatorperformance.
arxiv-18000-121 | Theano: A Python framework for fast computation of mathematical expressions | http://arxiv.org/pdf/1605.02688v1.pdf | author:The Theano Development Team, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Frédéric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, Yoshua Bengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, Nicolas Bouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Brébisson, Olivier Breuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooijmans, Marc-Alexandre Côté, Myriam Côté, Aaron Courville, Yann N. Dauphin, Olivier Delalleau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, Mélanie Ducoffe, Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, Mathieu Germain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, Iban Harlouchet, Jean-Philippe Heng, Balázs Hidasi, Sina Honari, Arjun Jain, Sébastien Jean, Kai Jia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, César Laurent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas Léonard, Zhouhan Lin, Jesse A. Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropietro, Robert T. McGibbon, Roland Memisevic, Bart van Merriënboer, Vincent Michalski, Mehdi Mirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel, Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Salvatier, François Savard, Jan Schlüter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban, Dmitriy Serdyuk, Samira Shabanian, Étienne Simon, Sigurd Spieckermann, S. Ramana Subramanyam, Jakub Sygnowski, Jérémie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban, Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J. Webb, Matthew Willson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, Ying Zhang category:cs.SC cs.LG cs.MS published:2016-05-09 summary:Theano is a Python library that allows to define, optimize, and evaluatemathematical expressions involving multi-dimensional arrays efficiently. Sinceits introduction, it has been one of the most used CPU and GPU mathematicalcompilers - especially in the machine learning community - and has shown steadyperformance improvements. Theano is being actively and continuously developedsince 2008, multiple frameworks have been built on top of it and it has beenused to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overviewof the Theano software and its community. Section II presents the principalfeatures of Theano and how to use them, and compares them with other similarprojects. Section III focuses on recently-introduced functionalities andimprovements. Section IV compares the performance of Theano against Torch7 andTensorFlow on several machine learning models. Section V discusses currentlimitations of Theano and potential ways of improving it.
arxiv-18000-122 | Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark | http://arxiv.org/pdf/1605.02677v1.pdf | author:Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang category:cs.AI cs.CV published:2016-05-09 summary:Psychological research results have confirmed that people can have differentemotional reactions to different visual stimuli. Several papers have beenpublished on the problem of visual emotion analysis. In particular, attemptshave been made to analyze and predict people's emotional reaction towardsimages. To this end, different kinds of hand-tuned features are proposed. Theresults reported on several carefully selected and labeled small image datasets have confirmed the promise of such features. While the recent successes ofmany computer vision related tasks are due to the adoption of ConvolutionalNeural Networks (CNNs), visual emotion analysis has not achieved the same levelof success. This may be primarily due to the unavailability of confidentlylabeled and relatively large image data sets for visual emotion analysis. Inthis work, we introduce a new data set, which started from 3+ million weaklylabeled images of different emotions and ended up 30 times as large as thecurrent largest publicly available visual emotion data set. We hope that thisdata set encourages further research on visual emotion analysis. We alsoperform extensive benchmarking analyses on this large data set using the stateof the art methods including CNNs.
arxiv-18000-123 | A Game-Theoretic Approach to Multi-Pedestrian Activity Forecasting | http://arxiv.org/pdf/1604.01431v2.pdf | author:Wei-Chiu Ma, De-An Huang, Namhoon Lee, Kris M. Kitani category:cs.CV published:2016-04-05 summary:We develop predictive models of pedestrian dynamics by encoding the couplednature of multi-pedestrian interaction using game theory, and deeplearning-based visual analysis to estimate person-specific behavior parameters.Building predictive models for multi-pedestrian interactions however, is verychallenging due to two reasons: (1) the dynamics of interaction are complexinterdependent processes, where the predicted behavior of one pedestrian canaffect the actions taken by others and (2) dynamics are variable depending onan individuals physical characteristics (e.g., an older person may walk slowlywhile the younger person may walk faster). To address these challenges, we (1)utilize concepts from game theory to model the interdependent decision makingprocess of multiple pedestrians and (2) use visual classifiers to learn amapping from pedestrian appearance to behavior parameters. We evaluate ourproposed model on several public multiple pedestrian interaction videodatasets. Results show that our strategic planning model explains humaninteractions 25% better when compared to state-of-the-art methods.
arxiv-18000-124 | Why (and How) Avoid Orthogonal Procrustes in Regularized Multivariate Analysis | http://arxiv.org/pdf/1605.02674v1.pdf | author:Sergio Muñoz-Romero, Vanessa Gómez-Verdejo, Jerónimo Arenas-García category:stat.ML published:2016-05-09 summary:Multivariate Analysis (MVA) comprises a family of well-known methods forfeature extraction that exploit correlations among input variables of the datarepresentation. One important property that is enjoyed by most such methods isuncorrelation among the extracted features. Recently, regularized versions ofMVA methods have appeared in the literature, mainly with the goal to gaininterpretability of the solution. In these cases, the solutions can no longerbe obtained in a closed manner, and it is frequent to recur to the iteration oftwo steps, one of them being an orthogonal Procrustes problem. This lettershows that the Procrustes solution is not optimal from the perspective of theoverall MVA method, and proposes an alternative approach based on the solutionof an eigenvalue problem. Our method ensures the preservation of severalproperties of the original methods, most notably the uncorrelation of theextracted features, as demonstrated theoretically and through a collection ofselected experiments.
arxiv-18000-125 | Population fluctuation promotes cooperation in networks | http://arxiv.org/pdf/1407.8032v4.pdf | author:Steve Miller, Joshua Knowles category:cs.GT cs.NE physics.soc-ph published:2014-07-30 summary:We consider the problem of explaining the emergence and evolution ofcooperation in dynamic network-structured populations. Building on seminal workby Poncela et al, which shows how cooperation (in one-shot prisoner's dilemma)is supported in growing populations by an evolutionary preferential attachment(EPA) model, we investigate the effect of fluctuations in the population size.We find that the fluctuating model is more robust than Poncela et al's in thatcooperation flourishes for a wide variety of initial conditions. In terms ofboth the temptation to defect, and the types of strategies present in thefounder network, the fluctuating population is found to lead more securely tocooperation. Further, we find that this model will also support the emergenceof cooperation from pre-existing non-cooperative random networks. This model,like Poncela et al's, does not require agents to have memory, recognition ofother agents, or other cognitive abilities, and so may suggest a more generalexplanation of the emergence of cooperation in early evolutionary transitions,than mechanisms such as kin selection, direct and indirect reciprocity.
arxiv-18000-126 | Learning the kernel matrix via predictive low-rank approximations | http://arxiv.org/pdf/1601.04366v2.pdf | author:Martin Stražar, Tomaž Curk category:cs.LG stat.ML published:2016-01-17 summary:Efficient and accurate low-rank approximations of multiple data sources areessential in the era of big data. The scaling of kernel-based learningalgorithms to large datasets is limited by the O(n^2) computation and storagecomplexity of the full kernel matrix, which is required by most of the recentkernel learning algorithms. We present the Mklaren algorithm to approximate multiple kernel matriceslearn a regression model, which is entirely based on geometrical concepts. Thealgorithm does not require access to full kernel matrices yet it accounts forthe correlations between all kernels. It uses Incomplete Choleskydecomposition, where pivot selection is based on least-angle regression in thecombined, low-dimensional feature space. The algorithm has linear complexity inthe number of data points and kernels. When explicit feature space induced bythe kernel can be constructed, a mapping from the dual to the primal Ridgeregression weights is used for model interpretation. The Mklaren algorithm was tested on eight standard regression datasets. Itoutperforms contemporary kernel matrix approximation approaches when learningwith multiple kernels. It identifies relevant kernels, achieving highestexplained variance than other multiple kernel learning methods for the samenumber of iterations. Test accuracy, equivalent to the one using full kernelmatrices, was achieved with at significantly lower approximation ranks. Adifference in run times of two orders of magnitude was observed when either thenumber of samples or kernels exceeds 3000.
arxiv-18000-127 | Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering | http://arxiv.org/pdf/1605.02633v1.pdf | author:Chong You, Chun-Guang Li, Daniel P. Robinson, Rene Vidal category:cs.LG cs.CV stat.ML published:2016-05-09 summary:State-of-the-art subspace clustering methods are based on expressing eachdata point as a linear combination of other data points while regularizing thematrix of coefficients with $\ell_1$, $\ell_2$ or nuclear norms. $\ell_1$regularization is guaranteed to give a subspace-preserving affinity (i.e.,there are no connections between points from different subspaces) under broadtheoretical conditions, but the clusters may not be connected. $\ell_2$ andnuclear norm regularization often improve connectivity, but give asubspace-preserving affinity only for independent subspaces. Mixed $\ell_1$,$\ell_2$ and nuclear norm regularizations offer a balance between thesubspace-preserving and connectedness properties, but this comes at the cost ofincreased computational complexity. This paper studies the geometry of theelastic net regularizer (a mixture of the $\ell_1$ and $\ell_2$ norms) and usesit to derive a provably correct and scalable active set method for finding theoptimal coefficients. Our geometric analysis also provides a theoreticaljustification and a geometric interpretation for the balance between theconnectedness (due to $\ell_2$ regularization) and subspace-preserving (due to$\ell_1$ regularization) properties for elastic net subspace clustering. Ourexperiments show that the proposed active set method not only achievesstate-of-the-art clustering performance, but also efficiently handleslarge-scale datasets.
arxiv-18000-128 | On the Emergence of Shortest Paths by Reinforced Random Walks | http://arxiv.org/pdf/1605.02619v1.pdf | author:Daniel R. Figueiredo, Michele Garetto category:cs.NE physics.bio-ph published:2016-05-09 summary:The co-evolution between network structure and functional performance is afundamental and challenging problem whose complexity emerges from the intrinsicinterdependent nature of structure and function. Within this context, weinvestigate the interplay between the efficiency of network navigation (i.e.,path lengths) and network structure (i.e., edge weights). We propose a simpleand tractable model based on iterative biased random walks where edge weightsincrease over time as function of the traversed path length. Under mildassumptions, we prove that biased random walks will eventually only traverseshortest paths in their journey towards the destination. We furthercharacterize the transient regime proving that the probability to traversenon-shortest paths decays according to a power-law. We also highlight variousproperties in this dynamic, such as the trade-off between exploration andconvergence, and preservation of initial network plasticity. We believe theproposed model and results can be of interest to various domains where biasedrandom walks and decentralized navigation have been applied.
arxiv-18000-129 | Dynamic Decomposition of Spatiotemporal Neural Signals | http://arxiv.org/pdf/1605.02609v1.pdf | author:Luca Ambrogioni, Marcel A. J. van Gerven, Eric Maris category:q-bio.NC stat.ML published:2016-05-09 summary:Neural signals are characterized by rich temporal and spatiotemporal dynamicsthat reflect the organization of cortical networks. Theoretical research hasshown how neural networks can operate at different dynamic ranges thatcorrespond to specific types of information processing. Here we present a dataanalysis framework that uses a linearized model of these dynamic states inorder to decompose the measured neural signal into a series of components thatcapture both rhythmic and non-rhythmic neural activity. The method is based onstochastic differential equations and Gaussian process regression. Throughcomputer simulations and analysis of magnetoencephalographic data, wedemonstrate the efficacy of the method in identifying meaningful modulations ofoscillatory signals corrupted by structured temporal and spatiotemporal noise.These results suggest that the method is particularly suitable for the analysisand interpretation of complex temporal and spatiotemporal neural signals.
arxiv-18000-130 | Multitask Learning for Sequence Labeling Tasks | http://arxiv.org/pdf/1404.6580v2.pdf | author:Arvind Agarwal, Saurabh Kataria category:cs.LG published:2014-04-25 summary:In this paper, we present a learning method for sequence labeling tasks inwhich each example sequence has multiple label sequences. Our method learnsmultiple models, one model for each label sequence. Each model computes thejoint probability of all label sequences given the example sequence. Althougheach model considers all label sequences, its primary focus is only one labelsequence, and therefore, each model becomes a task-specific model, for the taskbelonging to that primary label. Such multiple models are learned {\itsimultaneously} by facilitating the learning transfer among models through {\itexplicit parameter sharing}. We experiment the proposed method on twoapplications and show that our method significantly outperforms thestate-of-the-art method.
arxiv-18000-131 | GLEU Without Tuning | http://arxiv.org/pdf/1605.02592v1.pdf | author:Courtney Napoles, Keisuke Sakaguchi, Matt Post, Joel Tetreault category:cs.CL published:2016-05-09 summary:The GLEU metric was proposed for evaluating grammatical error correctionsusing n-gram overlap with a set of reference sentences, as opposed toprecision/recall of specific annotated errors (Napoles et al., 2015). Thispaper describes improvements made to the GLEU metric that address problems thatarise when using an increasing number of reference sets. Unlike the originallypresented metric, the modified metric does not require tuning. We recommendthat this version be used instead of the original version.
arxiv-18000-132 | Studying the brain from adolescence to adulthood through sparse multi-view matrix factorisations | http://arxiv.org/pdf/1605.02560v1.pdf | author:Zi Wang, Vyacheslav Karolis, Chiara Nosarti, Giovanni Montana category:stat.AP cs.CV q-bio.NC published:2016-05-09 summary:Men and women differ in specific cognitive abilities and in the expression ofseveral neuropsychiatric conditions. Such findings could be attributed to sexhormones, brain differences, as well as a number of environmental variables.Existing research on identifying sex-related differences in brain structurehave predominantly used cross-sectional studies to investigate, for instance,differences in average gray matter volumes (GMVs). In this article we explorethe potential of a recently proposed multi-view matrix factorisation (MVMF)methodology to study structural brain changes in men and women that occur fromadolescence to adulthood. MVMF is a multivariate variance decompositiontechnique that extends principal component analysis to "multi-view" datasets,i.e. where multiple and related groups of observations are available. In thisapplication, each view represents a different age group. MVMF identifies latentfactors explaining shared and age-specific contributions to the observedoverall variability in GMVs over time. These latent factors can be used toproduce low-dimensional visualisations of the data that emphasise age-specificeffects once the shared effects have been accounted for. The analysis of twodatasets consisting of individuals born prematurely as well as healthy controlsprovides evidence to suggest that the separation between males and femalesbecomes increasingly larger as the brain transitions from adolescence toadulthood. We report on specific brain regions associated to these varianceeffects.
arxiv-18000-133 | Robust imaging of hippocampal inner structure at 7T: in vivo acquisition protocol and methodological choices | http://arxiv.org/pdf/1605.02559v1.pdf | author:Linda Marrakchi-Kacem, Alexandre Vignaud, Julien Sein, Johanne Germain, Thomas R Henry, Cyril Poupon, Lucie Hertz-Pannier, Stéphane Lehéricy, Olivier Colliot, Pierre-François Van de Moortele, Marie Chupin category:cs.CV published:2016-05-09 summary:OBJECTIVE:Motion-robust multi-slab imaging of hippocampal inner structure invivo at 7T.MATERIALS AND METHODS:Motion is a crucial issue for ultra-highresolution imaging, such as can be achieved with 7T MRI. An acquisitionprotocol was designed for imaging hippocampal inner structure at 7T. It relieson a compromise between anatomical details visibility and robustness to motion.In order to reduce acquisition time and motion artifacts, the full slabcovering the hippocampus was split into separate slabs with lower acquisitiontime. A robust registration approach was implemented to combine the acquiredslabs within a final 3D-consistent high-resolution slab covering the wholehippocampus. Evaluation was performed on 50 subjects overall, made of threegroups of subjects acquired using three acquisition settings; it focused onthree issues: visibility of hippocampal inner structure, robustness to motionartifacts and registration procedure performance.RESULTS:Overall, T2-weightedacquisitions with interleaved slabs proved robust. Multi-slab registrationyielded high quality datasets in 96 % of the subjects, thus compatible withfurther analyses of hippocampal inner structure.CONCLUSION:Multi-slabacquisition and registration setting is efficient for reducing acquisition timeand consequently motion artifacts for ultra-high resolution imaging of theinner structure of the hippocampus.
arxiv-18000-134 | Mean Absolute Percentage Error for regression models | http://arxiv.org/pdf/1605.02541v1.pdf | author:Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, Fabrice Rossi category:stat.ML published:2016-05-09 summary:We study in this paper the consequences of using the Mean Absolute PercentageError (MAPE) as a measure of quality for regression models. We prove theexistence of an optimal MAPE model and we show the universal consistency ofEmpirical Risk Minimization based on the MAPE. We also show that finding thebest model under the MAPE is equivalent to doing weighted Mean Absolute Error(MAE) regression, and we apply this weighting strategy to kernel regression.The behavior of the MAPE kernel regression is illustrated on simulated data.
arxiv-18000-135 | Exact ICL maximization in a non-stationary temporal extension of the stochastic block model for dynamic networks | http://arxiv.org/pdf/1605.02540v1.pdf | author:Marco Corneli, Pierre Latouche, Fabrice Rossi category:stat.ML stat.AP published:2016-05-09 summary:The stochastic block model (SBM) is a flexible probabilistic tool that can beused to model interactions between clusters of nodes in a network. However, itdoes not account for interactions of time varying intensity between clusters.The extension of the SBM developed in this paper addresses this shortcomingthrough a temporal partition: assuming interactions between nodes are recordedon fixed-length time intervals, the inference procedure associated with themodel we propose allows to cluster simultaneously the nodes of the network andthe time intervals. The number of clusters of nodes and of time intervals, aswell as the memberships to clusters, are obtained by maximizing an exactintegrated complete-data likelihood, relying on a greedy search approach.Experiments on simulated and real data are carried out in order to assess theproposed methodology.
arxiv-18000-136 | Random Fourier Features for Operator-Valued Kernels | http://arxiv.org/pdf/1605.02536v1.pdf | author:Romain Brault, Florence d'Alché-Buc, Markus Heinonen category:cs.LG published:2016-05-09 summary:Devoted to multi-task learning and structured output learning,operator-valued kernels provide a flexible tool to build vector-valuedfunctions in the context of Reproducing Kernel Hilbert Spaces. To scale upthese methods, we extend the celebrated Random Fourier Feature methodology toget an approximation of operator-valued kernels. We propose a general principlefor Operator-valued Random Fourier Feature construction relying on ageneralization of Bochner's theorem for translation-invariant operator-valuedMercer kernels. We prove the uniform convergence of the kernel approximationfor bounded and unbounded operator random Fourier features using appropriateBernstein matrix concentration inequality. An experimental proof-of-conceptshows the quality of the approximation and the efficiency of the correspondinglinear models on example datasets.
arxiv-18000-137 | Clustering Time Series and the Surprising Robustness of HMMs | http://arxiv.org/pdf/1605.02531v1.pdf | author:Mark Kozdoba, Shie Mannor category:cs.IT cs.LG math.IT stat.ML published:2016-05-09 summary:Suppose that you are given a time series where consecutive samples arebelieved to come from a probabilistic source, and that the source changes fromtime to time. Your objective is to learn the distribution of each source and tocluster the samples according to the source that generated them. A standardapproach to this problem is to model the data as a hidden Markov model (HMM).However, due to the Markov property and stationarity of HMMs, simple examplescan be given where this approach yields poor results for the clustering. Wepropose a more general, non-stationary model of the data, where the onlyrestriction is that the sources can not change too often. Even though the modelgoverning the sources may not be Markovian, we show that that a maximumlikelihood HMM estimator can still be used. Specifically, we show that amaximum-likelihood HMM estimator produces the correct second moment of thedata, and the results can be extended to higher moments. In contrast to theexisting consistency and misspecification results involving maximum likelihoodfor HMMs, our approach yields bounds for finite sample sizes.
arxiv-18000-138 | Efficiency Evaluation of Character-level RNN Training Schedules | http://arxiv.org/pdf/1605.02486v1.pdf | author:Cedric De Boom, Sam Leroux, Steven Bohez, Pieter Simoens, Thomas Demeester, Bart Dhoedt category:cs.NE published:2016-05-09 summary:We present four training and prediction schedules from the samecharacter-level recurrent neural network. The efficiency of these schedules istested in terms of model effectiveness as a function of training time andamount of training data seen. We show that the choice of training andprediction schedule potentially has a considerable impact on the predictioneffectiveness for a given training budget.
arxiv-18000-139 | Visual Instance Retrieval with Deep Convolutional Networks | http://arxiv.org/pdf/1412.6574v4.pdf | author:Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson, Atsuto Maki category:cs.CV published:2014-12-20 summary:This paper provides an extensive study on the availability of imagerepresentations based on convolutional networks (ConvNets) for the task ofvisual instance retrieval. Besides the choice of convolutional layers, wepresent an efficient pipeline exploiting multi-scale schemes to extract localfeatures, in particular, by taking geometric invariance into explicit account,i.e. positions, scales and spatial consistency. In our experiments using fivestandard image retrieval datasets, we demonstrate that generic ConvNet imagerepresentations can outperform other state-of-the-art methods if they areextracted appropriately.
arxiv-18000-140 | Randomized Kaczmarz for Rank Aggregation from Pairwise Comparisons | http://arxiv.org/pdf/1605.02470v1.pdf | author:Vivek S. Borkar, Nikhil Karamchandani, Sharad Mirani category:cs.LG stat.ML published:2016-05-09 summary:We revisit the problem of inferring the overall ranking among entities in theframework of Bradley-Terry-Luce (BTL) model, based on available empirical dataon pairwise preferences. By a simple transformation, we can cast the problem asthat of solving a noisy linear system, for which a ready algorithm is availablein the form of the randomized Kaczmarz method. This scheme is provablyconvergent, has excellent empirical performance, and is amenable to on-line,distributed and asynchronous variants. Convergence, convergence rate, and erroranalysis of the proposed algorithm are presented and several numericalexperiments are conducted whose results validate our theoretical findings.
arxiv-18000-141 | Orientation Driven Bag of Appearances for Person Re-identification | http://arxiv.org/pdf/1605.02464v1.pdf | author:Liqian Ma, Hong Liu, Liang Hu, Can Wang, Qianru Sun category:cs.CV published:2016-05-09 summary:Person re-identification (re-id) consists of associating individual acrosscamera network, which is valuable for intelligent video surveillance and hasdrawn wide attention. Although person re-identification research is makingprogress, it still faces some challenges such as varying poses, illuminationand viewpoints. For feature representation in re-identification, existing worksusually use low-level descriptors which do not take full advantage of bodystructure information, resulting in low representation ability.%discrimination. To solve this problem, this paper proposes the mid-levelbody-structure based feature representation (BSFR) which introduces bodystructure pyramid for codebook learning and feature pooling in the verticaldirection of human body. Besides, varying viewpoints in the horizontaldirection of human body usually causes the data missing problem, $i.e.$, theappearances obtained in different orientations of the identical person couldvary significantly. To address this problem, the orientation driven bag ofappearances (ODBoA) is proposed to utilize person orientation informationextracted by orientation estimation technic. To properly evaluate the proposedapproach, we introduce a new re-identification dataset (Market-1203) based onthe Market-1501 dataset and propose a new re-identification dataset (PKU-Reid).Both datasets contain multiple images captured in different body orientationsfor each person. Experimental results on three public datasets and two proposeddatasets demonstrate the superiority of the proposed approach, indicating theeffectiveness of body structure and orientation information for improvingre-identification performance.
arxiv-18000-142 | Fuzzy Clustering Based Segmentation Of Vertebrae in T1-Weighted Spinal MR Images | http://arxiv.org/pdf/1605.02460v1.pdf | author:Jiyo. S. Athertya, G. Saravana Kumar category:cs.CV published:2016-05-09 summary:Image segmentation in the medical domain is a challenging field owing to poorresolution and limited contrast. The predominantly used conventionalsegmentation techniques and the thresholding methods suffer from limitationsbecause of heavy dependence on user interactions. Uncertainties prevalent in animage cannot be captured by these techniques. The performance furtherdeteriorates when the images are corrupted by noise, outliers and otherartifacts. The objective of this paper is to develop an effective robust fuzzyC- means clustering for segmenting vertebral body from magnetic resonance imageowing to its unsupervised form of learning. The motivation for this work isdetection of spine geometry and proper localisation and labelling will enhancethe diagnostic output of a physician. The method is compared with Otsuthresholding and K-means clustering to illustrate the robustness.The referencestandard for validation was the annotated images from the radiologist, and theDice coefficient and Hausdorff distance measures were used to evaluate thesegmentation.
arxiv-18000-143 | The Controlled Natural Language of Randall Munroe's Thing Explainer | http://arxiv.org/pdf/1605.02457v1.pdf | author:Tobias Kuhn category:cs.CL published:2016-05-09 summary:It is rare that texts or entire books written in a Controlled NaturalLanguage (CNL) become very popular, but exactly this has happened with a bookthat has been published last year. Randall Munroe's Thing Explainer uses onlythe 1'000 most often used words of the English language together with drawnpictures to explain complicated things such as nuclear reactors, jet engines,the solar system, and dishwashers. This restricted language is a veryinteresting new case for the CNL community. I describe here its place in thecontext of existing approaches on Controlled Natural Languages, and I provide afirst analysis from a scientific perspective, covering the word productionrules and word distributions.
arxiv-18000-144 | A Bayesian approach to constrained single- and multi-objective optimization | http://arxiv.org/pdf/1510.00503v3.pdf | author:Paul Feliot, Julien Bect, Emmanuel Vazquez category:stat.CO stat.ML published:2015-10-02 summary:This article addresses the problem of derivative-free (single- ormulti-objective) optimization subject to multiple inequality constraints. Boththe objective and constraint functions are assumed to be smooth, non-linear andexpensive to evaluate. As a consequence, the number of evaluations that can beused to carry out the optimization is very limited, as in complex industrialdesign optimization problems. The method we propose to overcome this difficultyhas its roots in both the Bayesian and the multi-objective optimizationliteratures. More specifically, an extended domination rule is used to handleobjectives and constraints in a unified way, and a corresponding expectedhyper-volume improvement sampling criterion is proposed. This new criterion isnaturally adapted to the search of a feasible point when none is available, andreduces to existing Bayesian sampling criteria---the classical ExpectedImprovement (EI) criterion and some of its constrained/multi-objectiveextensions---as soon as at least one feasible point is available. Thecalculation and optimization of the criterion are performed using SequentialMonte Carlo techniques. In particular, an algorithm similar to the subsetsimulation method, which is well known in the field of structural reliability,is used to estimate the criterion. The method, which we call BMOO (for BayesianMulti-Objective Optimization), is compared to state-of-the-art algorithms forsingle- and multi-objective constrained optimization.
arxiv-18000-145 | Machine Learning Techniques with Ontology for Subjective Answer Evaluation | http://arxiv.org/pdf/1605.02442v1.pdf | author:M. Syamala Devi, Himani Mittal category:cs.AI cs.CL cs.IR I.2.7 published:2016-05-09 summary:Computerized Evaluation of English Essays is performed using Machine learningtechniques like Latent Semantic Analysis (LSA), Generalized LSA, BilingualEvaluation Understudy and Maximum Entropy. Ontology, a concept map of domainknowledge, can enhance the performance of these techniques. Use of Ontologymakes the evaluation process holistic as presence of keywords, synonyms, theright word combination and coverage of concepts can be checked. In this paper,the above mentioned techniques are implemented both with and without Ontologyand tested on common input data consisting of technical answers of ComputerScience. Domain Ontology of Computer Graphics is designed and developed. Thesoftware used for implementation includes Java Programming Language and toolssuch as MATLAB, Prot\'eg\'e, etc. Ten questions from Computer Graphics withsixty answers for each question are used for testing. The results are analyzedand it is concluded that the results are more accurate with use of Ontology.
arxiv-18000-146 | Learning Discriminative Features with Class Encoder | http://arxiv.org/pdf/1605.02424v1.pdf | author:Hailin Shi, Xiangyu Zhu, Zhen Lei, Shengcai Liao, Stan Z. Li category:cs.CV published:2016-05-09 summary:Deep neural networks usually benefit from unsupervised pre-training, e.g.auto-encoders. However, the classifier further needs supervised fine-tuningmethods for good discrimination. Besides, due to the limits of full-connection,the application of auto-encoders is usually limited to small, well alignedimages. In this paper, we incorporate the supervised information to propose anovel formulation, namely class-encoder, whose training objective is toreconstruct a sample from another one of which the labels are identical.Class-encoder aims to minimize the intra-class variations in the feature space,and to learn a good discriminative manifolds on a class scale. We impose theclass-encoder as a constraint into the softmax for better supervised training,and extend the reconstruction on feature-level to tackle the parameter sizeissue and translation issue. The experiments show that the class-encoder helpsto improve the performance on benchmarks of classification and facerecognition. This could also be a promising direction for fast training of facerecognition models.
arxiv-18000-147 | Structured Nonconvex and Nonsmooth Optimization: Algorithms and Iteration Complexity Analysis | http://arxiv.org/pdf/1605.02408v1.pdf | author:Bo Jiang, Tianyi Lin, Shiqian Ma, Shuzhong Zhang category:math.OC cs.LG stat.ML published:2016-05-09 summary:Nonconvex optimization problems are frequently encountered in much ofstatistics, business, science and engineering, but they are not yet widelyrecognized as a technology. A reason for this relatively low degree ofpopularity is the lack of a well developed system of theory and algorithms tosupport the applications, as is the case for its convex counterpart. This paperaims to take one step in the direction of disciplined nonconvex optimization.In particular, we consider in this paper some constrained nonconvexoptimization models in block decision variables, with or without coupled affineconstraints. In the case of no coupled constraints, we show a sublinear rate ofconvergence to an $\epsilon$-stationary solution in the form of variationalinequality for a generalized conditional gradient method, where the convergencerate is shown to be dependent on the H\"olderian continuity of the gradient ofthe smooth part of the objective. For the model with coupled affineconstraints, we introduce corresponding $\epsilon$-stationarity conditions, andpropose two proximal-type variants of the ADMM to solve such a model, assumingthe proximal ADMM updates can be implemented for all the block variables exceptfor the last block, for which either a gradient step or amajorization-minimization step is implemented. We show an iteration complexitybound of $O(1/\epsilon^2)$ to reach an $\epsilon$-stationary solution for bothalgorithms. Moreover, we show that the same iteration complexity of a proximalBCD method follows immediately. Numerical results are provided to illustratethe efficacy of the proposed algorithms for tensor robust PCA.
arxiv-18000-148 | The Power of Depth for Feedforward Neural Networks | http://arxiv.org/pdf/1512.03965v4.pdf | author:Ronen Eldan, Ohad Shamir category:cs.LG cs.NE stat.ML published:2015-12-12 summary:We show that there is a simple (approximately radial) function on $\reals^d$,expressible by a small 3-layer feedforward neural networks, which cannot beapproximated by any 2-layer network, to more than a certain constant accuracy,unless its width is exponential in the dimension. The result holds forvirtually all known activation functions, including rectified linear units,sigmoids and thresholds, and formally demonstrates that depth -- even ifincreased by 1 -- can be exponentially more valuable than width for standardfeedforward neural networks. Moreover, compared to related results in thecontext of Boolean functions, our result requires fewer assumptions, and theproof techniques and construction are very different.
arxiv-18000-149 | Fast Discrete Distribution Clustering Using Wasserstein Barycenter with Sparse Support | http://arxiv.org/pdf/1510.00012v2.pdf | author:Jianbo Ye, Panruo Wu, James Z. Wang, Jia Li category:stat.CO cs.LG stat.ML published:2015-09-30 summary:In a variety of research areas, the bag of weighted vectors and the histogramare widely used descriptors for complex objects. Both can be expressed asdiscrete distributions. D2-clustering pursues the minimum total within-clustervariation for a set of discrete distributions subject to theKantorovich-Wasserstein metric. D2-clustering has a severe scalability issue,the bottleneck being the computation of a centroid distribution, calledWasserstein barycenter, that minimizes its sum of squared distances to thecluster members. In this paper, we develop a modified Bregman ADMM approach forcomputing the approximate discrete Wasserstein barycenter of large clusters. Inthe case when the support points of the barycenters are unknown and of lowcardinality, our method achieves high accuracy empirically at a much reducedcomputational cost. The strengths and weaknesses of our method and itsalternatives are examined through experiments; and scenarios for theirrespective usage are recommended. Moreover, we develop both serial andparallelized versions of the algorithm. By experimenting with large-scale data,we demonstrate the computational efficiency of the new methods and investigatetheir convergence properties and numerical stability. The clustering resultsobtained on several datasets in different domains are highly competitive incomparison with some widely used methods' in the corresponding areas.
arxiv-18000-150 | Active Learning for Community Detection in Stochastic Block Models | http://arxiv.org/pdf/1605.02372v1.pdf | author:Akshay Gadde, Eyal En Gad, Salman Avestimehr, Antonio Ortega category:cs.LG cs.SI math.PR published:2016-05-08 summary:The stochastic block model (SBM) is an important generative model for randomgraphs in network science and machine learning, useful for benchmarkingcommunity detection (or clustering) algorithms. The symmetric SBM generates agraph with $2n$ nodes which cluster into two equally sized communities. Nodesconnect with probability $p$ within a community and $q$ across differentcommunities. We consider the case of $p=a\ln (n)/n$ and $q=b\ln (n)/n$. In thiscase, it was recently shown that recovering the community membership (or label)of every node with high probability (w.h.p.) using only the graph is possibleif and only if the Chernoff-Hellinger (CH) divergence$D(a,b)=(\sqrt{a}-\sqrt{b})^2 \geq 1$. In this work, we study if, and by howmuch, community detection below the clustering threshold (i.e. $D(a,b)<1$) ispossible by querying the labels of a limited number of chosen nodes (i.e.,active learning). Our main result is to show that, under certain conditions,sampling the labels of a vanishingly small fraction of nodes (a numbersub-linear in $n$) is sufficient for exact community detection even when$D(a,b)<1$. Furthermore, we provide an efficient learning algorithm whichrecovers the community memberships of all nodes w.h.p. as long as the number ofsampled points meets the sufficient condition. We also show that recovery isnot possible if the number of observed labels is less than $n^{1-D(a,b)}$. Thevalidity of our results is demonstrated through numerical experiments.
arxiv-18000-151 | Not Just a Black Box: Learning Important Features Through Propagating Activation Differences | http://arxiv.org/pdf/1605.01713v2.pdf | author:Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, Anshul Kundaje category:cs.LG cs.CV cs.NE published:2016-05-05 summary:The purported "black box" nature of neural networks is a barrier to adoptionin applications where interpretability is essential. Here we present DeepLIFT(Learning Important FeaTures), an efficient and effective method for computingimportance scores in a neural network. DeepLIFT compares the activation of eachneuron to its 'reference activation' and assigns contribution scores accordingto the difference. We apply DeepLIFT to models trained on natural images andgenomic data, and show significant advantages over gradient-based methods.
arxiv-18000-152 | Vanishing point attracts gaze in free-viewing and visual search tasks | http://arxiv.org/pdf/1512.01722v2.pdf | author:Ali Borji, Mengyang Feng category:cs.CV published:2015-12-06 summary:To investigate whether the vanishing point (VP) plays a significant role ingaze guidance, we ran two experiments. In the first one, we recorded fixationsof 10 observers (4 female; mean age 22; SD=0.84) freely viewing 532 images, outof which 319 had VP (shuffled presentation; each image for 4 secs). We foundthat the average number of fixations at a local region (80x80 pixels) centeredat the VP is significantly higher than the average fixations at randomlocations (t-test; n=319; p=1.8e-35). To address the confounding factor ofsaliency, we learned a combined model of bottom-up saliency and VP. AUC scoreof our model (0.85; SD=0.01) is significantly higher than the original saliencymodel (e.g., 0.8 using AIM model by Bruce & Tsotsos (2009), t-test; p=3.14e-16) and the VP-only model (0.64, t-test; p= 4.02e-22). In the secondexperiment, we asked 14 subjects (4 female, mean age 23.07, SD=1.26) to searchfor a target character (T or L) placed randomly on a 3x3 imaginary gridoverlaid on top of an image. Subjects reported their answers by pressing one oftwo keys. Stimuli consisted of 270 color images (180 with a single VP, 90without). The target happened with equal probability inside each cell (15 timesL, 15 times T). We found that subjects were significantly faster (and moreaccurate) when target happened inside the cell containing the VP compared tocells without VP (median across 14 subjects 1.34 sec vs. 1.96; Wilcoxonrank-sum test; p = 0.0014). Response time at VP cells were also significantlylower than response time on images without VP (median 2.37; p= 4.77e-05). Thesefindings support the hypothesis that vanishing point, similar to face and text(Cerf et al., 2009) as well as gaze direction (Borji et al., 2014) attractsattention in free-viewing and visual search.
arxiv-18000-153 | Efficient Reinforcement Learning in Deterministic Systems with Value Function Generalization | http://arxiv.org/pdf/1307.4847v3.pdf | author:Zheng Wen, Benjamin Van Roy category:cs.LG cs.AI cs.SY stat.ML published:2013-07-18 summary:We consider the problem of reinforcement learning over episodes of afinite-horizon deterministic system and as a solution propose optimisticconstraint propagation (OCP), an algorithm designed to synthesize efficientexploration and value function generalization. We establish that when the truevalue function lies within a given hypothesis class, OCP selects optimalactions over all but at most K episodes, where K is the eluder dimension of thegiven hypothesis class. We establish further efficiency and asymptoticperformance guarantees that apply even if the true value function does not liein the given hypothesis class, for the special case where the hypothesis classis the span of pre-specified indicator functions over disjoint sets. We alsodiscuss the computational complexity of OCP and present computational resultsinvolving two illustrative examples.
arxiv-18000-154 | Learning from Rational Behavior: Predicting Solutions to Unknown Linear Programs | http://arxiv.org/pdf/1506.02162v2.pdf | author:Shahin Jabbari, Ryan Rogers, Aaron Roth, Zhiwei Steven Wu category:cs.DS cs.GT cs.LG published:2015-06-06 summary:We define and study the problem of predicting the solution to a linearprogram, given only partial information about its objective and constraints.This generalizes the problem of learning to predict the purchasing behavior ofa rational agent who has an unknown objective function, which has been studiedunder the name "Learning from Revealed Preferences". We give mistake boundlearning algorithms in two settings: in the first, the objective of the linearprogram is known to the learner, but there is an arbitrary, fixed set ofconstraints which are unknown. Each example given to the learner is defined byan additional, known constraint, and the goal of the learner is to predict theoptimal solution of the linear program given the union of the known and unknownconstraints. This models, among other things, the problem of predicting thebehavior of a rational agent whose goals are known, but whose resources areunknown. In the second setting, the objective of the linear program is unknown,and changing in a controlled way. The constraints of the linear program mayalso change every day, but are known. An example is given by a set ofconstraints and partial information about the objective, and the task of thelearner is again to predict the optimal solution of the partially known linearprogram.
arxiv-18000-155 | Chained Predictions Using Convolutional Neural Networks | http://arxiv.org/pdf/1605.02346v1.pdf | author:Georgia Gkioxari, Alexander Toshev, Navdeep Jaitly category:cs.CV published:2016-05-08 summary:In this paper, we present an adaptation of the sequence-to-sequence model forstructured output prediction in vision tasks. In this model the outputvariables for a given input are predicted sequentially using neural networks.The prediction for each output variable depends not only on the input but alsoon the previously predicted output variables. The model is applied to spatiallocalization tasks and uses convolutional neural networks (CNNs) for processinginput images and a multi-scale deconvolutional architecture for making spatialpredictions at each time step. We explore the impact of weight sharing with arecurrent connection matrix between consecutive predictions, and compare it toa formulation where these weights are not tied. Untied weights are particularlysuited for problems with a fixed sized structure, where different classes ofoutput are predicted in different steps. We show that chained predictionsachieve top performing results on human pose estimation from single images andvideos.
arxiv-18000-156 | Triplet Probabilistic Embedding for Face Verification and Clustering | http://arxiv.org/pdf/1604.05417v2.pdf | author:Swami Sankaranarayanan, Azadeh Alavi, Carlos Castillo, Rama Chellappa category:cs.CV cs.LG stat.ML published:2016-04-19 summary:Despite significant progress made over the past twenty five years,unconstrained face verification remains a challenging problem. This paperproposes an approach that couples a deep CNN-based approach with alow-dimensional discriminative embedding learned using triplet probabilityconstraints to solve the unconstrained face verification problem. Aside fromyielding performance improvements, this embedding provides significantadvantages in terms of memory and for post-processing operations like subjectspecific clustering. Experiments on the challenging IJB-A dataset show that theproposed algorithm performs comparably or better than the state of the artmethods in verification and identification metrics, while requiring much lesstraining data and training time. The superior performance of the proposedmethod on the CFP dataset shows that the representation learned by our deep CNNis robust to extreme pose variation. Furthermore, we demonstrate the robustnessof the deep features to challenges including age, pose, blur and clutter byperforming simple clustering experiments on both IJB-A and LFW datasets.
arxiv-18000-157 | $M$-Statistic for Kernel Change-Point Detection | http://arxiv.org/pdf/1507.01279v3.pdf | author:Shuang Li, Yao Xie, Hanjun Dai, Le Song category:cs.LG math.ST stat.ML stat.TH published:2015-07-05 summary:Detecting the emergence of an abrupt change-point is a classic problem instatistics and machine learning. Kernel-based nonparametric statistics havebeen proposed for this task which make fewer assumptions on the distributionsthan traditional parametric approach. However, none of the existing kernelstatistics has provided a computationally efficient way to characterize theextremal behavior of the statistic. Such characterization is crucial forsetting the detection threshold, to control the significance level in theoffline case as well as the false alarm rate (captured by the average runlength) in the online case. In this paper we focus on the scenario when theamount of background data is large, and propose two related computationallyefficient kernel-based statistics for change-point detection, which we call"$M$-statistics". A novel theoretical result of the paper is thecharacterization of the tail probability of these statistics using a newtechnique based on change-of-measure. Such characterization provides usaccurate detection thresholds for both offline and online cases incomputationally efficient manner, without the need to resort to the moreexpensive simulations such as bootstrapping. Moreover, our $M$-statistic can beapplied to high-dimensional data by choosing a proper kernel. We show that ourmethods perform well in both synthetic and real world data.
arxiv-18000-158 | Recurrent Neural Network Training with Dark Knowledge Transfer | http://arxiv.org/pdf/1505.04630v5.pdf | author:Zhiyuan Tang, Dong Wang, Zhiyong Zhang category:stat.ML cs.CL cs.LG cs.NE published:2015-05-18 summary:Recurrent neural networks (RNNs), particularly long short-term memory (LSTM),have gained much attention in automatic speech recognition (ASR). Although somesuccessful stories have been reported, training RNNs remains highlychallenging, especially with limited training data. Recent research found thata well-trained model can be used as a teacher to train other child models, byusing the predictions generated by the teacher model as supervision. Thisknowledge transfer learning has been employed to train simple neural nets witha complex one, so that the final performance can reach a level that isinfeasible to obtain by regular training. In this paper, we employ theknowledge transfer learning approach to train RNNs (precisely LSTM) using adeep neural network (DNN) model as the teacher. This is different from most ofthe existing research on knowledge transfer learning, since the teacher (DNN)is assumed to be weaker than the child (RNN); however, our experiments on anASR task showed that it works fairly well: without applying any tricks on thelearning scheme, this approach can train RNNs successfully even with limitedtraining data.
arxiv-18000-159 | Information Recovery in Shuffled Graphs via Graph Matching | http://arxiv.org/pdf/1605.02315v1.pdf | author:Vince Lyzinski category:stat.ML cs.IT math.CO math.IT published:2016-05-08 summary:In a number of methodologies for joint inference across graphs, it is assumedthat an explicit vertex correspondence is a priori known across the vertex setsof the graphs. While this assumption is often reasonable, in practice thesecorrespondences may be unobserved and/or errorfully observed, and graphmatching---aligning a pair of graphs to minimize their edge disagreements---isused to align the graphs before performing subsequent inference. Herein, weexplore the duality between the loss of mutual information due to an errorfullyobserved vertex correspondence and the ability of graph matching algorithms torecover the true correspondence across graphs. We then demonstrate thepractical effect that graph shuffling---and matching---can have on subsequentinference, with examples from two sample graph hypothesis testing and jointgraph clustering.
arxiv-18000-160 | Estimating Depth from Monocular Images as Classification Using Deep Fully Convolutional Residual Networks | http://arxiv.org/pdf/1605.02305v1.pdf | author:Yuanzhouhan Cao, Zifeng Wu, Chunhua Shen category:cs.CV published:2016-05-08 summary:Depth estimation from single monocular images is a key component of sceneunderstanding and has benefited largely from deep convolutional neural networks(CNN) recently. In this article, we take advantage of the recent deep residualnetworks and propose a simple yet effective approach to this problem. Weformulate depth estimation as a pixel-wise classification task. Specifically,we first discretize the continuous depth values into multiple bins and labelthe bins according to their depth range. Then we train fully convolutional deepresidual networks to predict the depth label of each pixel. Performing discretedepth label classification instead of continuous depth value regression allowsus to predict a confidence in the form of probability distribution. We furtherapply fully-connected conditional random fields (CRF) as a post processing stepto enforce local smoothness interactions, which improves the results. Weevaluate our approach on the NYUDepth v2 dataset and achieve state-of-the-artperformance.
arxiv-18000-161 | Deep Learning with Eigenvalue Decay Regularizer | http://arxiv.org/pdf/1604.06985v3.pdf | author:Oswaldo Ludwig category:cs.LG published:2016-04-24 summary:This paper extends our previous work on regularization of neural networksusing Eigenvalue Decay by employing a soft approximation of the dominanteigenvalue in order to enable the calculation of its derivatives in relation tothe synaptic weights, and therefore the application of back-propagation, whichis a primary demand for deep learning. Moreover, we extend our previoustheoretical analysis to deep neural networks and multiclass classificationproblems. Our method is implemented as an additional regularizer in Keras, amodular neural networks library written in Python, and evaluated in thebenchmark data sets Reuters Newswire Topics Classification, IMDB database forbinary sentiment classification, MNIST database of handwritten digits andCIFAR-10 data set for image classification.
arxiv-18000-162 | Ultradense Word Embeddings by Orthogonal Transformation | http://arxiv.org/pdf/1602.07572v2.pdf | author:Sascha Rothe, Sebastian Ebert, Hinrich Schütze category:cs.CL published:2016-02-24 summary:Embeddings are generic representations that are useful for many NLP tasks. Inthis paper, we introduce DENSIFIER, a method that learns an orthogonaltransformation of the embedding space that focuses the information relevant fora task in an ultradense subspace of a dimensionality that is smaller by afactor of 100 than the original space. We show that ultradense embeddingsgenerated by DENSIFIER reach state of the art on a lexicon creation task inwhich words are annotated with three types of lexical information - sentiment,concreteness and frequency. On the SemEval2015 10B sentiment analysis task weshow that no information is lost when the ultradense subspace is used, buttraining is an order of magnitude more efficient due to the compactness of theultradense space.
arxiv-18000-163 | Detecting Ground Control Points via Convolutional Neural Network for Stereo Matching | http://arxiv.org/pdf/1605.02289v1.pdf | author:Zhun Zhong, Songzhi Su, Donglin Cao, Shaozi Li category:cs.CV published:2016-05-08 summary:In this paper, we present a novel approach to detect ground control points(GCPs) for stereo matching problem. First of all, we train a convolutionalneural network (CNN) on a large stereo set, and compute the matching confidenceof each pixel by using the trained CNN model. Secondly, we present a groundcontrol points selection scheme according to the maximum matching confidence ofeach pixel. Finally, the selected GCPs are used to refine the matching costs,and we apply the new matching costs to perform optimization with semi-globalmatching algorithm for improving the final disparity maps. We evaluate ourapproach on the KITTI 2012 stereo benchmark dataset. Our experiments show thatthe proposed approach significantly improves the accuracy of disparity maps.
arxiv-18000-164 | On-Average KL-Privacy and its equivalence to Generalization for Max-Entropy Mechanisms | http://arxiv.org/pdf/1605.02277v1.pdf | author:Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg category:stat.ML cs.CR published:2016-05-08 summary:We define On-Average KL-Privacy and present its properties and connections todifferential privacy, generalization and information-theoretic quantitiesincluding max-information and mutual information. The new definitionsignificantly weakens differential privacy, while preserving its minimalisticdesign features such as composition over small group and multiple queries aswell as closeness to post-processing. Moreover, we show that On-AverageKL-Privacy is **equivalent** to generalization for a large class ofcommonly-used tools in statistics and machine learning that samples from Gibbsdistributions---a class of distributions that arises naturally from the maximumentropy principle. In addition, a byproduct of our analysis yields a lowerbound for generalization error in terms of mutual information which reveals aninteresting interplay with known upper bounds that use the same quantity.
arxiv-18000-165 | Problems With Evaluation of Word Embeddings Using Word Similarity Tasks | http://arxiv.org/pdf/1605.02276v1.pdf | author:Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer category:cs.CL published:2016-05-08 summary:Lacking standardized extrinsic evaluation methods for vector representationsof words, the NLP community has relied heavily on word similarity tasks as aproxy for intrinsic evaluation of word vectors. Word similarity evaluation,which correlates the distance between vectors and human judgments of semanticsimilarity is attractive, because it is computationally inexpensive and fast.In this paper we present several problems associated with the evaluation ofword vectors on word similarity datasets, and summarize existing solutions. Ourstudy suggests that the use of word similarity tasks for evaluation of wordvectors is not sustainable and calls for further research on evaluationmethods.
arxiv-18000-166 | Predicting Performance on MOOC Assessments using Multi-Regression Models | http://arxiv.org/pdf/1605.02269v1.pdf | author:Zhiyun Ren, Huzefa Rangwala, Aditya Johri category:cs.CY cs.LG published:2016-05-08 summary:The past few years has seen the rapid growth of data min- ing approaches forthe analysis of data obtained from Mas- sive Open Online Courses (MOOCs). Theobjectives of this study are to develop approaches to predict the scores a stu-dent may achieve on a given grade-related assessment based on information,considered as prior performance or prior ac- tivity in the course. We develop apersonalized linear mul- tiple regression (PLMR) model to predict the grade fora student, prior to attempting the assessment activity. The developed model isreal-time and tracks the participation of a student within a MOOC (viaclick-stream server logs) and predicts the performance of a student on the nextas- sessment within the course offering. We perform a com- prehensive set ofexperiments on data obtained from three openEdX MOOCs via a Stanford Universityinitiative. Our experimental results show the promise of the proposed ap-proach in comparison to baseline approaches and also helps in identification ofkey features that are associated with the study habits and learning behaviorsof students.
arxiv-18000-167 | Rate-Distortion Bounds on Bayes Risk in Supervised Learning | http://arxiv.org/pdf/1605.02268v1.pdf | author:Matthew Nokleby, Ahmad Beirami, Robert Calderbank category:cs.IT cs.LG math.IT stat.ML published:2016-05-08 summary:An information-theoretic framework is presented for estimating the number oflabeled samples needed to train a classifier in a parametric Bayesian setting.Ideas from rate-distortion theory are used to derive bounds on the average$L_1$ or $L_\infty$ distance between the learned classifier and the truemaximum a posteriori classifier---which are well-established surrogates for theexcess classification error due to imperfect learning---in terms of thedifferential entropy of the posterior distribution, the Fisher information ofthe parametric family, and the number of training samples available. Themaximum {\em a posteriori} classifier is viewed as a random source, labeledtraining data are viewed as a finite-rate encoding of the source, and the $L_1$or $L_\infty$ Bayes risk is viewed as the average distortion. The result is acomplementary framework to the well-known probably approximately correct (PAC)framework. PAC bounds characterize worst-case learning performance of a familyof classifiers whose complexity is captured by the Vapnik-Chervonenkis (VC)dimension. The rate-distortion framework, on the other hand, characterizes theaverage-case performance of a family of data distributions in terms of aquantity called the interpolation dimension, which represents the complexity ofthe family of data distributions. The resulting bounds do not suffer from thepessimism typical of the PAC framework, particularly when the training set issmall. The framework also naturally accommodates multi-class settings.Furthermore, Monte Carlo methods provide accurate estimates of the bounds evenfor complicated distributions. The effectiveness of this framework isdemonstrated in both a binary and multi-class Gaussian setting.
arxiv-18000-168 | Robust and Low-Rank Representation for Fast Face Identification with Occlusions | http://arxiv.org/pdf/1605.02266v1.pdf | author:Michael Iliadis, Haohong Wang, Rafael Molina, Aggelos K. Katsaggelos category:cs.CV published:2016-05-08 summary:In this paper we propose an iterative method to address the faceidentification problem with block occlusions. Our approach utilizes a robustrepresentation based on two characteristics in order to model contiguous errors(e.g., block occlusion) effectively. The first fits to the errors adistribution described by a tailored loss function. The second describes theerror image as having a specific structure (resulting in low-rank). We willshow that this joint characterization is effective for describing errors withspatial continuity. Our approach is computationally efficient due to theutilization of the Alternating Direction Method of Multipliers (ADMM). Aspecial case of our fast iterative algorithm leads to the robust representationmethod which is normally used to handle non-contiguous errors (e.g., pixelcorruption). Extensive results on representative face databases document theeffectiveness of our method over existing robust representation methods withrespect to both identification rates and computational time. Code is available at Github, where you can find implementations of theF-LR-IRNNLS and F-IRNNLS (fast version of the RRC) :\url{https://github.com/miliadis/FIRC}
arxiv-18000-169 | Laplacian Reconstruction and Refinement for Semantic Segmentation | http://arxiv.org/pdf/1605.02264v1.pdf | author:Golnaz Ghiasi, Charless Fowlkes category:cs.CV published:2016-05-08 summary:CNN architectures have terrific recognition performance but rely on spatialpooling which makes it difficult to adapt them to tasks that require densepixel-accurate labeling. This paper makes two contributions: (1) We demonstratethat while the apparent spatial resolution of convolutional feature maps islow, the high-dimensional feature representation contains significant sub-pixellocalization information. (2) We describe a multi-resolution reconstructionarchitecture, akin to a Laplacian pyramid, that uses skip connections fromhigher resolution feature maps to successively refine segment boundariesreconstructed from lower resolution maps. This approach yields state-of-the-artsemantic segmentation results on PASCAL without resorting to more complex CRFor detection driven architectures.
arxiv-18000-170 | Deeply Exploit Depth Information for Object Detection | http://arxiv.org/pdf/1605.02260v1.pdf | author:Saihui Hou, Zilei Wang, Feng Wu category:cs.CV published:2016-05-08 summary:This paper addresses the issue on how to more effectively coordinate thedepth with RGB aiming at boosting the performance of RGB-D object detection.Particularly, we investigate two primary ideas under the CNN model: propertyderivation and property fusion. Firstly, we propose that the depth can beutilized not only as a type of extra information besides RGB but also to derivemore visual properties for comprehensively describing the objects of interest.So a two-stage learning framework consisting of property derivation and fusionis constructed. Here the properties can be derived either from the providedcolor/depth or their pairs (e.g. the geometry contour adopted in this paper).Secondly, we explore the fusion method of different properties in featurelearning, which is boiled down to, under the CNN model, from which layer theproperties should be fused together. The analysis shows that different semanticproperties should be learned separately and combined before passing into thefinal classifier. Actually, such a detection way is in accordance with themechanism of the primary neural cortex (V1) in brain. We experimentallyevaluate the proposed method on the challenging dataset, and have achievedstate-of-the-art performance.
arxiv-18000-171 | A corpus of preposition supersenses in English web reviews | http://arxiv.org/pdf/1605.02257v1.pdf | author:Nathan Schneider, Jena D. Hwang, Vivek Srikumar, Meredith Green, Kathryn Conger, Tim O'Gorman, Martha Palmer category:cs.CL published:2016-05-08 summary:We present the first corpus annotated with preposition supersenses,unlexicalized categories for semantic functions that can be marked by Englishprepositions (Schneider et al., 2015). That scheme improves upon itspredecessors to better facilitate comprehensive manual annotation. Moreover,unlike the previous schemes, the preposition supersenses are organizedhierarchically. Our data will be publicly released on the web upon publication.
arxiv-18000-172 | On Image segmentation using Fractional Gradients-Learning Model Parameters using Approximate Marginal Inference | http://arxiv.org/pdf/1605.02240v1.pdf | author:Anish Acharya, Uddipan Mukherjee, Charless Fowlkes category:cs.CV published:2016-05-07 summary:Estimates of image gradients play a ubiquitous role in image segmentation andclassification problems since gradients directly relate to the boundaries orthe edges of a scene. This paper proposes an unified approach to gradientestimation based on fractional calculus that is computationally cheap andreadily applicable to any existing algorithm that relies on image gradients. Weshow experiments on edge detection and image segmentation on the StanfordBackgrounds Dataset where these improved local gradients outperforms state ofthe art, achieving a performance of 79.2% average accuracy.
arxiv-18000-173 | A Bayesian Group Sparse Multi-Task Regression Model for Imaging Genetics | http://arxiv.org/pdf/1605.02234v1.pdf | author:Keelin Greenlaw, Elena Szefer, Jinko Graham, Mary Lesperance, Farouk S. Nathoo category:stat.ME stat.AP stat.ML published:2016-05-07 summary:Motivation: Recent advances in technology for brain imaging andhigh-throughput genotyping have motivated studies examining the influence ofgenetic variation on brain structure. Wang et al. (Bioinformatics, 2012) havedeveloped an approach for the analysis of imaging genomic studies usingpenalized multi-task regression with regularization based on a novel group$l_{2,1}$-norm penalty which encourages structured sparsity at both the genelevel and SNP level. While incorporating a number of useful features, theproposed method only furnishes a point estimate of the regression coefficients;techniques for conducting statistical inference are not provided. A newBayesian method is proposed here to overcome this limitation. Results: We develop a Bayesian hierarchical modeling formulation where theposterior mode corresponds to the estimator proposed by Wang et al.(\textit{Bioinformatics}, 2012), and an approach that allows for full posteriorinference including the construction of interval estimates for the regressionparameters. We show that the proposed hierarchical model can be expressed as athree-level Gaussian scale mixture and this representation facilitates the useof a Gibbs sampling algorithm for posterior simulation. Simulation studiesdemonstrate that the interval estimates obtained using our approach achieveadequate coverage probabilities that outperform those obtained from thenonparametric bootstrap. Our proposed methodology is applied to the analysis ofneuroimaging and genetic data collected as part of the Alzheimer's DiseaseNeuroimaging Initiative (ADNI), and this analysis of the ADNI cohortdemonstrates clearly the value added of incorporating interval estimationbeyond only point estimation when relating SNPs to brain imagingendophenotypes.
arxiv-18000-174 | Building Machines That Learn and Think Like People | http://arxiv.org/pdf/1604.00289v2.pdf | author:Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman category:cs.AI cs.CV cs.LG cs.NE stat.ML published:2016-04-01 summary:Recent progress in artificial intelligence (AI) has renewed interest inbuilding systems that learn and think like people. Many advances have come fromusing deep neural networks trained end-to-end in tasks such as objectrecognition, video games, and board games, achieving performance that equals oreven beats humans in some respects. Despite their biological inspiration andperformance achievements, these systems differ from human intelligence incrucial ways. We review progress in cognitive science suggesting that trulyhuman-like learning and thinking machines will have to reach beyond currentengineering trends in both what they learn, and how they learn it.Specifically, we argue that these machines should (a) build causal models ofthe world that support explanation and understanding, rather than merelysolving pattern recognition problems; (b) ground learning in intuitive theoriesof physics and psychology, to support and enrich the knowledge that is learned;and (c) harness compositionality and learning-to-learn to rapidly acquire andgeneralize knowledge to new tasks and situations. We suggest concretechallenges and promising routes towards these goals that can combine thestrengths of recent neural network advances with more structured cognitivemodels.
arxiv-18000-175 | Distributed stochastic optimization for deep learning (thesis) | http://arxiv.org/pdf/1605.02216v1.pdf | author:Sixin Zhang category:cs.LG published:2016-05-07 summary:We study the problem of how to distribute the training of large-scale deeplearning models in the parallel computing environment. We propose a newdistributed stochastic optimization method called Elastic Averaging SGD(EASGD). We analyze the convergence rate of the EASGD method in the synchronousscenario and compare its stability condition with the existing ADMM method inthe round-robin scheme. An asynchronous and momentum variant of the EASGDmethod is applied to train deep convolutional neural networks for imageclassification on the CIFAR and ImageNet datasets. Our approach accelerates thetraining and furthermore achieves better test accuracy. It also requires a muchsmaller amount of communication than other common baseline approaches such asthe DOWNPOUR method. We then investigate the limit in speedup of the initial and the asymptoticphase of the mini-batch SGD, the momentum SGD, and the EASGD methods. We findthat the spread of the input data distribution has a big impact on theirinitial convergence rate and stability region. We also find a surprisingconnection between the momentum SGD and the EASGD method with a negative movingaverage rate. A non-convex case is also studied to understand when EASGD canget trapped by a saddle point. Finally, we scale up the EASGD method by using a tree structured networktopology. We show empirically its advantage and challenge. We also establish aconnection between the EASGD and the DOWNPOUR method with the classical Jacobiand the Gauss-Seidel method, thus unifying a class of distributed stochasticoptimization methods.
arxiv-18000-176 | All Weather Perception: Joint Data Association, Tracking, and Classification for Autonomous Ground Vehicles | http://arxiv.org/pdf/1605.02196v1.pdf | author:Peter Radecki, Mark Campbell, Kevin Matzen category:cs.SY cs.CV cs.LG cs.RO published:2016-05-07 summary:A novel probabilistic perception algorithm is presented as a real-time jointsolution to data association, object tracking, and object classification for anautonomous ground vehicle in all-weather conditions. The presented algorithmextends a Rao-Blackwellized Particle Filter originally built with a particlefilter for data association and a Kalman filter for multi-object tracking(Miller et al. 2011a) to now also include multiple model tracking forclassification. Additionally a state-of-the-art vision detection algorithm thatincludes heading information for autonomous ground vehicle (AGV) applicationswas implemented. Cornell's AGV from the DARPA Urban Challenge was upgraded andused to experimentally examine if and how state-of-the-art vision algorithmscan complement or replace lidar and radar sensors. Sensor and algorithmperformance in adverse weather and lighting conditions is tested. Experimentalevaluation demonstrates robust all-weather data association, tracking, andclassification where camera, lidar, and radar sensors complement each otherinside the joint probabilistic perception algorithm.
arxiv-18000-177 | Matching models across abstraction levels with Gaussian Processes | http://arxiv.org/pdf/1605.02190v1.pdf | author:Giulio Caravagna, Luca Bortolussi, Guido Sanguinetti category:stat.ML published:2016-05-07 summary:Biological systems are often modelled at different levels of abstractiondepending on the particular aims/resources of a study. Such different modelsoften provide qualitatively concordant predictions over specificparametrisations, but it is generally unclear whether model predictions arequantitatively in agreement, and whether such agreement holds for differentparametrisations. Here we present a generally applicable statistical machinelearning methodology to automatically reconcile the predictions of differentmodels across abstraction levels. Our approach is based on defining acorrection map, a random function which modifies the output of a model in orderto match the statistics of the output of a different model of the same system.We use two biological examples to give a proof-of-principle demonstration ofthe methodology, and discuss its advantages and potential further applications.
arxiv-18000-178 | Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences | http://arxiv.org/pdf/1602.01208v3.pdf | author:Akira Taniguchi, Tadahiro Taniguchi, Tetsunari Inamura category:cs.AI cs.CL cs.RO published:2016-02-03 summary:In this paper, we propose a novel unsupervised learning method for thelexical acquisition of words related to places visited by robots, from humancontinuous speech signals. We address the problem of learning novel words by arobot that has no prior knowledge of these words except for a primitiveacoustic model. Further, we propose a method that allows a robot to effectivelyuse the learned words and their meanings for self-localization tasks. Theproposed method is nonparametric Bayesian spatial concept acquisition method(SpCoA) that integrates the generative model for self-localization and theunsupervised word segmentation in uttered sentences via latent variablesrelated to the spatial concept. We implemented the proposed method SpCoA onSIGVerse, which is a simulation environment, and TurtleBot2, which is a mobilerobot in a real environment. Further, we conducted experiments for evaluatingthe performance of SpCoA. The experimental results showed that SpCoA enabledthe robot to acquire the names of places from speech sentences. They alsorevealed that the robot could effectively utilize the acquired spatial conceptsand reduce the uncertainty in self-localization.
arxiv-18000-179 | Fast Bilateral Filtering of Vector-Valued Images | http://arxiv.org/pdf/1605.02164v1.pdf | author:Sanjay Ghosh, Kunal N. Chaudhury category:cs.CV published:2016-05-07 summary:In this paper, we consider a natural extension of the edge-preservingbilateral filter for vector-valued images. The direct computation of thisnon-linear filter is slow in practice. We demonstrate how a fast algorithm canbe obtained by first approximating the Gaussian kernel of the bilateral filterusing raised-cosines, and then using Monte Carlo sampling. We presentsimulation results on color images to demonstrate the accuracy of the algorithmand the speedup over the direct implementation.
arxiv-18000-180 | Learning with Memory Embeddings | http://arxiv.org/pdf/1511.07972v9.pdf | author:Volker Tresp, Cristóbal Esteban, Yinchong Yang, Stephan Baier, Denis Krompaß category:cs.AI cs.CL cs.LG published:2015-11-25 summary:Embedding learning, a.k.a. representation learning, has been shown to be ableto model large-scale semantic knowledge graphs. A key concept is a mapping ofthe knowledge graph to a tensor representation whose entries are predicted bymodels using latent representations of generalized entities. Latent variablemodels are well suited to deal with the high dimensionality and sparsity oftypical knowledge graphs. In recent publications the embedding models wereextended to also consider time evolutions, time patterns and subsymbolicrepresentations. In this paper we map embedding models, which were developedpurely as solutions to technical problems for modelling temporal knowledgegraphs, to various cognitive memory functions, in particular to semantic andconcept memory, episodic memory, sensory memory, short-term memory, and workingmemory. We discuss learning, query answering, the path from sensory input tosemantic decoding, and the relationship between episodic memory and semanticmemory. We introduce a number of hypotheses on human memory that can be derivedfrom the developed mathematical models.
arxiv-18000-181 | On Improving Informativity and Grammaticality for Multi-Sentence Compression | http://arxiv.org/pdf/1605.02150v1.pdf | author:Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond Wong, Fang Chen category:cs.CL published:2016-05-07 summary:Multi Sentence Compression (MSC) is of great value to many real worldapplications, such as guided microblog summarization, opinion summarization andnewswire summarization. Recently, word graph-based approaches have beenproposed and become popular in MSC. Their key assumption is that redundancyamong a set of related sentences provides a reliable way to generateinformative and grammatical sentences. In this paper, we propose an effectiveapproach to enhance the word graph-based MSC and tackle the issue that most ofthe state-of-the-art MSC approaches are confronted with: i.e., improving bothinformativity and grammaticality at the same time. Our approach consists ofthree main components: (1) a merging method based on Multiword Expressions(MWE); (2) a mapping strategy based on synonymy between words; (3) a re-rankingstep to identify the best compression candidates generated using a POS-basedlanguage model (POS-LM). We demonstrate the effectiveness of this novelapproach using a dataset made of clusters of English newswire sentences. Theobserved improvements on informativity and grammaticality of the generatedcompressions show that our approach is superior to state-of-the-art MSCmethods.
arxiv-18000-182 | Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks | http://arxiv.org/pdf/1602.03616v2.pdf | author:Anh Nguyen, Jason Yosinski, Jeff Clune category:cs.NE cs.CV published:2016-02-11 summary:We can better understand deep neural networks by identifying which featureseach of their neurons have learned to detect. To do so, researchers havecreated Deep Visualization techniques including activation maximization, whichsynthetically generates inputs (e.g. images) that maximally activate eachneuron. A limitation of current techniques is that they assume each neurondetects only one type of feature, but we know that neurons can be multifaceted,in that they fire in response to many different types of features: for example,a grocery store class neuron must activate either for rows of produce or for astorefront. Previous activation maximization techniques constructed imageswithout regard for the multiple different facets of a neuron, creatinginappropriate mixes of colors, parts of objects, scales, orientations, etc.Here, we introduce an algorithm that explicitly uncovers the multiple facets ofeach neuron by producing a synthetic visualization of each of the types ofimages that activate a neuron. We also introduce regularization methods thatproduce state-of-the-art results in terms of the interpretability of imagesobtained by activation maximization. By separately synthesizing each type ofimage a neuron fires in response to, the visualizations have more appropriatecolors and coherent global structure. Multifaceted feature visualization thusprovides a clearer and more comprehensive description of the role of eachneuron.
arxiv-18000-183 | Matrix Factorization-Based Clustering Of Image Features For Bandwidth-Constrained Information Retrieval | http://arxiv.org/pdf/1605.02140v1.pdf | author:Jacob Chakareski, Immanuel Manohar, Shantanu Rane category:cs.CV 62h25, I.4 published:2016-05-07 summary:We consider the problem of accurately and efficiently querying a remoteserver to retrieve information about images captured by a mobile device. Inaddition to reduced transmission overhead and computational complexity, theretrieval protocol should be robust to variations in the image acquisitionprocess, such as translation, rotation, scaling, and sensor-relateddifferences. We propose to extract scale-invariant image features and thenperform clustering to reduce the number of features needed for image matching.Principal Component Analysis (PCA) and Non-negative Matrix Factorization (NMF)are investigated as candidate clustering approaches. The image matchingcomplexity at the database server is quadratic in the (small) number ofclusters, not in the (very large) number of image features. We employ animage-dependent information content metric to approximate the model order,i.e., the number of clusters, needed for accurate matching, which is preferableto setting the model order using trial and error. We show how to combine thehypotheses provided by PCA and NMF factor loadings, thereby obtaining moreaccurate retrieval than using either approach alone. In experiments on adatabase of urban images, we obtain a top-1 retrieval accuracy of 89% and atop-3 accuracy of 92.5%.
arxiv-18000-184 | Neural Recovery Machine for Chinese Dropped Pronoun | http://arxiv.org/pdf/1605.02134v1.pdf | author:Wei-Nan Zhang, Ting Liu, Qingyu Yin, Yu Zhang category:cs.CL published:2016-05-07 summary:Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese,Japanese etc. Previous work mainly focused on painstakingly exploring theempirical features for DPs recovery. In this paper, we propose a neuralrecovery machine (NRM) to model and recover DPs in Chinese, so that to avoidthe non-trivial feature engineering process. The experimental results show thatthe proposed NRM significantly outperforms the state-of-the-art approaches onboth two heterogeneous datasets. Further experiment results of Chinese zeropronoun (ZP) resolution show that the performance of ZP resolution can also beimproved by recovering the ZPs to DPs.
arxiv-18000-185 | Robust Dialog State Tracking for Large Ontologies | http://arxiv.org/pdf/1605.02130v1.pdf | author:Franck Dernoncourt, Ji Young Lee, Trung H. Bui, Hung H. Bui category:cs.CL cs.AI cs.LG published:2016-05-07 summary:The Dialog State Tracking Challenge 4 (DSTC 4) differentiates itself from theprevious three editions as follows: the number of slot-value pairs present inthe ontology is much larger, no spoken language understanding output is given,and utterances are labeled at the subdialog level. This paper describes a noveldialog state tracking method designed to work robustly under these conditions,using elaborate string matching, coreference resolution tailored for dialogsand a few other improvements. The method can correctly identify many valuesthat are not explicitly present in the utterance. On the final evaluation, ourmethod came in first among 7 competing teams and 24 entries. The F1-scoreachieved by our method was 9 and 7 percentage points higher than that of therunner-up for the utterance-level evaluation and for the subdialog-levelevaluation, respectively.
arxiv-18000-186 | Adobe-MIT submission to the DSTC 4 Spoken Language Understanding pilot task | http://arxiv.org/pdf/1605.02129v1.pdf | author:Franck Dernoncourt, Ji Young Lee, Trung H. Bui, Hung H. Bui category:cs.CL cs.AI cs.LG published:2016-05-07 summary:The Dialog State Tracking Challenge 4 (DSTC 4) proposes several pilot tasks.In this paper, we focus on the spoken language understanding pilot task, whichconsists of tagging a given utterance with speech acts and semantic slots. Wecompare different classifiers: the best system obtains 0.52 and 0.67 F1-scoreson the test set for speech act recognition for the tourist and the guiderespectively, and 0.52 F1-score for semantic tagging for both the guide and thetourist.
arxiv-18000-187 | Clustering on Multiple Incomplete Datasets via Collective Kernel Learning | http://arxiv.org/pdf/1310.1177v2.pdf | author:Weixiang Shao, Xiaoxiao Shi, Philip S. Yu category:cs.LG H.2.8; I.5.3 published:2013-10-04 summary:Multiple datasets containing different types of features may be available fora given task. For instance, users' profiles can be used to group users forrecommendation systems. In addition, a model can also use users' historicalbehaviors and credit history to group users. Each dataset contains differentinformation and suffices for learning. A number of clustering algorithms onmultiple datasets were proposed during the past few years. These algorithmsassume that at least one dataset is complete. So far as we know, all theprevious methods will not be applicable if there is no complete datasetavailable. However, in reality, there are many situations where no dataset iscomplete. As in building a recommendation system, some new users may not have aprofile or historical behaviors, while some may not have a credit history.Hence, no available dataset is complete. In order to solve this problem, wepropose an approach called Collective Kernel Learning to infer hidden samplesimilarity from multiple incomplete datasets. The idea is to collectivelycompletes the kernel matrices of incomplete datasets by optimizing thealignment of the shared instances of the datasets. Furthermore, a clusteringalgorithm is proposed based on the kernel matrix. The experiments on bothsynthetic and real datasets demonstrate the effectiveness of the proposedapproach. The proposed clustering algorithm outperforms the comparisonalgorithms by as much as two times in normalized mutual information.
arxiv-18000-188 | Likelihood Inflating Sampling Algorithm | http://arxiv.org/pdf/1605.02113v1.pdf | author:Reihaneh Entezari, Radu V. Craiu, Jeffrey S. Rosenthal category:stat.ML stat.CO published:2016-05-06 summary:Markov Chain Monte Carlo (MCMC) sampling from a posterior distributioncorresponding to a massive data set can be computationally prohibitive sinceproducing one sample requires a number of operations that is linear in the datasize. In this paper, we introduce a new communication-free parallel method, theLikelihood Inflating Sampling Algorithm (LISA), that significantly reducescomputational costs by randomly splitting the dataset into smaller subsets andrunning MCMC methods independently and in parallel on each subset usingdifferent processors. Each processor will draw sub-samples from sub-posteriordistributions that are defined by "inflating" the likelihood function and thesub-samples are then combined using the importance re-sampling method toperform approximate full-data posterior samples. We test our method on severalexamples including the important case of Bayesian Additive Regression Trees(BART) using both simulated and real datasets. The method we propose showssignificant efficiency gains over the existing Consensus Monte Carlo of Scottet al. (2013).
arxiv-18000-189 | Attribute And-Or Grammar for Joint Parsing of Human Attributes, Part and Pose | http://arxiv.org/pdf/1605.02112v1.pdf | author:Seyoung Park, Bruce Xiaohan Nie, Song-Chun Zhu category:cs.CV published:2016-05-06 summary:This paper presents an attribute and-or grammar (A-AOG) model for jointlyinferring human body pose and human attributes in a parse graph with attributesaugmented to nodes in the hierarchical representation. In contrast to otherpopular methods in the current literature that train separate classifiers forposes and individual attributes, our method explicitly represents thedecomposition and articulation of body parts, and account for the correlationsbetween poses and attributes. The A-AOG model is an amalgamation of threetraditional grammar formulations: (i) Phrase structure grammar representing thehierarchical decomposition of the human body from whole to parts; (ii)Dependency grammar modeling the geometric articulation by a kinematic tree ofthe body pose; and (iii) Attribute grammar accounting for the compatibilityrelations between different parts in the hierarchy so that their appearancesfollow a consistent style. The parse graph outputs human detection, poseestimation, and attribute prediction simultaneously, which are intuitive andinterpretable. We conduct experiments on two tasks on two datasets, andexperimental results demonstrate the advantage of joint modeling in comparisonwith computing poses and attributes independently. Furthermore, our modelobtains better performance over existing methods for both pose estimation andattribute prediction tasks.
arxiv-18000-190 | Distributed Learning with Infinitely Many Hypotheses | http://arxiv.org/pdf/1605.02105v1.pdf | author:Angelia Nedić, Alex Olshevsky, César Uribe category:math.OC cs.LG stat.ML published:2016-05-06 summary:We consider a distributed learning setup where a network of agentssequentially access realizations of a set of random variables with unknowndistributions. The network objective is to find a parametrized distributionthat best describes their joint observations in the sense of theKullback-Leibler divergence. Apart from recent efforts in the literature, weanalyze the case of countably many hypotheses and the case of a continuum ofhypotheses. We provide non-asymptotic bounds for the concentration rate of theagents' beliefs around the correct hypothesis in terms of the number of agents,the network parameters, and the learning abilities of the agents. Additionally,we provide a novel motivation for a general set of distributed Non-Bayesianupdate rules as instances of the distributed stochastic mirror descentalgorithm.
arxiv-18000-191 | Sequence Level Training with Recurrent Neural Networks | http://arxiv.org/pdf/1511.06732v7.pdf | author:Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba category:cs.LG cs.CL published:2015-11-20 summary:Many natural language processing applications use language models to generatetext. These models are typically trained to predict the next word in asequence, given the previous words and some context such as an image. However,at test time the model is expected to generate the entire sequence fromscratch. This discrepancy makes generation brittle, as errors may accumulatealong the way. We address this issue by proposing a novel sequence leveltraining algorithm that directly optimizes the metric used at test time, suchas BLEU or ROUGE. On three different tasks, our approach outperforms severalstrong baselines for greedy generation. The method is also competitive whenthese baselines employ beam search, while being several times faster.
arxiv-18000-192 | Towards Automated Melanoma Screening: Proper Computer Vision & Reliable Results | http://arxiv.org/pdf/1604.04024v3.pdf | author:Michel Fornaciali, Micael Carvalho, Flávia Vasques Bittencourt, Sandra Avila, Eduardo Valle category:cs.CV published:2016-04-14 summary:In this paper we survey, analyze and criticize current art on automatedmelanoma screening, reimplementing a baseline technique, and proposing twonovel ones. Melanoma, although highly curable when detected early, ends as oneof the most dangerous types of cancer, due to delayed diagnosis and treatment.Its incidence is soaring, much faster than the number of trained professionalsable to diagnose it. Automated screening appears as an alternative to make themost of those professionals, focusing their time on the patients at risk whilesafely discharging the other patients. However, the potential of automatedmelanoma diagnosis is currently unfulfilled, due to the emphasis of currentliterature on outdated computer vision models. Even more problematic is theirreproducibility of current art. We show how streamlined pipelines based uponcurrent Computer Vision outperform conventional models - a model based on anadvanced bags of words reaches an AUC of 84.6%, and a model based on deepneural networks reaches 89.3%, while the baseline (a classical bag of words)stays at 81.2%. We also initiate a dialog to improve reproducibility in ourcommunity
arxiv-18000-193 | Some Simulation Results for Emphatic Temporal-Difference Learning Algorithms | http://arxiv.org/pdf/1605.02099v1.pdf | author:Huizhen Yu category:cs.LG published:2016-05-06 summary:This is a companion note to our recent study of the weak convergenceproperties of constrained emphatic temporal-difference learning (ETD)algorithms from a theoretic perspective. It supplements the latter analysiswith simulation results and illustrates the behavior of some of the ETDalgorithms using three example problems.
arxiv-18000-194 | ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning | http://arxiv.org/pdf/1605.02097v1.pdf | author:Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Jaśkowski category:cs.LG cs.AI cs.CV published:2016-05-06 summary:The recent advances in deep neural networks have led to effectivevision-based reinforcement learning methods that have been employed to obtainhuman-level controllers in Atari 2600 games from pixel data. Atari 2600 games,however, do not resemble real-world tasks since they involve non-realistic 2Denvironments and the third-person perspective. Here, we propose a noveltest-bed platform for reinforcement learning research from raw visualinformation which employs the first-person perspective in a semi-realistic 3Dworld. The software, called ViZDoom, is based on the classical first-personshooter video game, Doom. It allows developing bots that play the game usingthe screen buffer. ViZDoom is lightweight, fast, and highly customizable via aconvenient mechanism of user scenarios. In the experimental part, we test theenvironment by trying to learn bots for two scenarios: a basic move-and-shoottask and a more complex maze-navigation problem. Using convolutional deepneural networks with Q-learning and experience replay, for both scenarios, wewere able to train competent bots, which exhibit human-like behaviors. Theresults confirm the utility of ViZDoom as an AI research platform and implythat visual reinforcement learning in 3D realistic first-person perspectiveenvironments is feasible.
arxiv-18000-195 | Stochastic Contextual Bandits with Known Reward Functions | http://arxiv.org/pdf/1605.00176v2.pdf | author:Pranav Sakulkar, Bhaskar Krishnamachari category:cs.LG published:2016-04-30 summary:Many sequential decision-making problems in communication networks can bemodeled as contextual bandit problems, which are natural extensions of thewell-known multi-armed bandit problem. In contextual bandit problems, at eachtime, an agent observes some side information or context, pulls one arm andreceives the reward for that arm. We consider a stochastic formulation wherethe context-reward tuples are independently drawn from an unknown distributionin each trial. Motivated by networking applications, we analyze a setting wherethe reward is a known non-linear function of the context and the chosen arm'scurrent state. We first consider the case of discrete and finite context-spacesand propose DCB($\epsilon$), an algorithm that we prove, through a carefulanalysis, yields regret (cumulative reward gap compared to a distribution-awaregenie) scaling logarithmically in time and linearly in the number of arms thatare not optimal for any context, improving over existing algorithms where theregret scales linearly in the total number of arms. We then study continuouscontext-spaces with Lipschitz reward functions and propose CCB($\epsilon,\delta$), an algorithm that uses DCB($\epsilon$) as a subroutine.CCB($\epsilon, \delta$) reveals a novel regret-storage trade-off that isparametrized by $\delta$. Tuning $\delta$ to the time horizon allows us toobtain sub-linear regret bounds, while requiring sub-linear storage. Byexploiting joint learning for all contexts we get regret bounds forCCB($\epsilon, \delta$) that are unachievable by any existing contextual banditalgorithm for continuous context-spaces. We also show similar performancebounds for the unknown horizon case.
arxiv-18000-196 | Function-Specific Mixing Times and Concentration Away from Equilibrium | http://arxiv.org/pdf/1605.02077v1.pdf | author:Maxim Rabinovich, Aaditya Ramdas, Michael I. Jordan, Martin J. Wainwright category:math.ST cs.LG math.PR stat.TH published:2016-05-06 summary:Slow mixing is the central hurdle when working with Markov chains, especiallythose used for Monte Carlo approximations (MCMC). In many applications, it isonly of interest to to estimate the stationary expectations of a small set offunctions, and so the usual definition of mixing based on total variationconvergence may be too conservative. Accordingly, we introducefunction-specific analogs of mixing times and spectral gaps, and use them toprove Hoeffding-like function-specific concentration inequalities. Theseresults show that it is possible for empirical expectations of functions toconcentrate long before the underlying chain has mixed in the classical sense.We use our techniques to derive confidence intervals that are sharper thanthose implied by both classical Markov chain Hoeffding bounds andBerry-Esseen-corrected CLT bounds. For applications that require testing,rather than point estimation, we show similar improvements over recentsequential testing results for MCMC. We conclude by applying our framework toreal data examples of MCMC, providing evidence that our theory is both accurateand relevant to practice.
arxiv-18000-197 | Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds | http://arxiv.org/pdf/1605.02065v1.pdf | author:Mark Bun, Thomas Steinke category:cs.CR cs.DS cs.IT cs.LG math.IT published:2016-05-06 summary:"Concentrated differential privacy" was recently introduced by Dwork andRothblum as a relaxation of differential privacy, which permits sharperanalyses of many privacy-preserving computations. We present an alternativeformulation of the concept of concentrated differential privacy in terms of theRenyi divergence between the distributions obtained by running an algorithm onneighboring inputs. With this reformulation in hand, we prove sharperquantitative results, establish lower bounds, and raise a few new questions. Wealso unify this approach with approximate differential privacy by giving anappropriate definition of "approximate concentrated differential privacy."
arxiv-18000-198 | Deformably Registering and Annotating Whole CLARITY Brains to an Atlas via Masked LDDMM | http://arxiv.org/pdf/1605.02060v1.pdf | author:Kwame S. Kutten, Joshua T. Vogelstein, Nicolas Charon, Li Ye, Karl Deisseroth, Michael I. Miller category:q-bio.QM cs.CV published:2016-05-06 summary:The CLARITY method renders brains optically transparent to enablehigh-resolution imaging in the structurally intact brain. Anatomicallyannotating CLARITY brains is necessary for discovering which regions containsignals of interest. Manually annotating whole-brain, terabyte CLARITY imagesis difficult, time-consuming, subjective, and error-prone. Automaticallyregistering CLARITY images to a pre-annotated brain atlas offers a solution,but is difficult for several reasons. Removal of the brain from the skull andsubsequent storage and processing cause variable non-rigid deformations, thuscompounding inter-subject anatomical variability. Additionally, the signal inCLARITY images arises from various biochemical contrast agents which onlysparsely label brain structures. This sparse labeling challenges the mostcommonly used registration algorithms that need to match image histogramstatistics to the more densely labeled histological brain atlases. The standardmethod is a multiscale Mutual Information B-spline algorithm that dynamicallygenerates an average template as an intermediate registration target. Wedetermined that this method performs poorly when registering CLARITY brains tothe Allen Institute's Mouse Reference Atlas (ARA), because the image histogramstatistics are poorly matched. Therefore, we developed a method (Mask-LDDMM)for registering CLARITY images, that automatically find the brain boundary andlearns the optimal deformation between the brain and atlas masks. UsingMask-LDDMM without an average template provided better results than thestandard approach when registering CLARITY brains to the ARA. The LDDMMpipelines developed here provide a fast automated way to anatomically annotateCLARITY images. Our code is available as open source software athttp://NeuroData.io.
arxiv-18000-199 | Low-Complexity Stochastic Generalized Belief Propagation | http://arxiv.org/pdf/1605.02046v1.pdf | author:Farzin Haddadpour, Mahdi Jafari Siavoshani, Morteza Noshad category:cs.LG cs.AI cs.IT math.IT published:2016-05-06 summary:The generalized belief propagation (GBP), introduced by Yedidia et al., is anextension of the belief propagation (BP) algorithm, which is widely used indifferent problems involved in calculating exact or approximate marginals ofprobability distributions. In many problems, it has been observed that theaccuracy of GBP considerably outperforms that of BP. However, because ingeneral the computational complexity of GBP is higher than BP, its applicationis limited in practice. In this paper, we introduce a stochastic version of GBP called stochasticgeneralized belief propagation (SGBP) that can be considered as an extension tothe stochastic BP (SBP) algorithm introduced by Noorshams et al. They haveshown that SBP reduces the complexity per iteration of BP by an order ofmagnitude in alphabet size. In contrast to SBP, SGBP can reduce the computationcomplexity if certain topological conditions are met by the region graphassociated to a graphical model. However, this reduction can be larger thanonly one order of magnitude in alphabet size. In this paper, we characterizethese conditions and the amount of computation gain that we can obtain by usingSGBP. Finally, using similar proof techniques employed by Noorshams et al., forgeneral graphical models satisfy contraction conditions, we prove theasymptotic convergence of SGBP to the unique GBP fixed point, as well asproviding non-asymptotic upper bounds on the mean square error and on the highprobability error.
arxiv-18000-200 | Regret Analysis of the Anytime Optimally Confident UCB Algorithm | http://arxiv.org/pdf/1603.08661v2.pdf | author:Tor Lattimore category:cs.LG math.ST stat.ML stat.TH published:2016-03-29 summary:I introduce and analyse an anytime version of the Optimally Confident UCB(OCUCB) algorithm designed for minimising the cumulative regret in finite-armedstochastic bandits with subgaussian noise. The new algorithm is simple,intuitive (in hindsight) and comes with the strongest finite-time regretguarantees for a horizon-free algorithm so far. I also show a finite-time lowerbound that nearly matches the upper bound.
arxiv-18000-201 | Training Neural Networks Without Gradients: A Scalable ADMM Approach | http://arxiv.org/pdf/1605.02026v1.pdf | author:Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, Tom Goldstein category:cs.LG published:2016-05-06 summary:With the growing importance of large network models and enormous trainingdatasets, GPUs have become increasingly necessary to train neural networks.This is largely because conventional optimization algorithms rely on stochasticgradient methods that don't scale well to large numbers of cores in a clustersetting. Furthermore, the convergence of all gradient methods, including batchmethods, suffers from common problems like saturation effects, poorconditioning, and saddle points. This paper explores an unconventional trainingmethod that uses alternating direction methods and Bregman iteration to trainnetworks without gradient descent steps. The proposed method reduces thenetwork training problem to a sequence of minimization sub-steps that can eachbe solved globally in closed form. The proposed method is advantageous becauseit avoids many of the caveats that make gradient methods slow on highlynon-convex problems. The method exhibits strong scaling in the distributedsetting, yielding linear speedups even when split over thousands of cores.
arxiv-18000-202 | Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec | http://arxiv.org/pdf/1605.02019v1.pdf | author:Christopher E Moody category:cs.CL published:2016-05-06 summary:Distributed dense word vectors have been shown to be effective at capturingtoken-level semantic and syntactic regularities in language, while topic modelscan form interpretable representations over documents. In this work, wedescribe lda2vec, a model that learns dense word vectors jointly withDirichlet-distributed latent document-level mixtures of topic vectors. Incontrast to continuous dense document representations, this formulationproduces sparse, interpretable document mixtures through a non-negative simplexconstraint. Our method is simple to incorporate into existing automaticdifferentiation frameworks and allows for unsupervised document representationsgeared for use by scientists while simultaneously learning word vectors and thelinear relationships between them.
arxiv-18000-203 | Device and System Level Design Considerations for Analog-Non-Volatile-Memory Based Neuromorphic Architectures | http://arxiv.org/pdf/1512.08030v2.pdf | author:Sukru Burc Eryilmaz, Duygu Kuzum, Shimeng Yu, H. -S. Philip Wong category:cs.NE cs.AI published:2015-12-25 summary:This paper gives an overview of recent progress in the brain inspiredcomputing field with a focus on implementation using emerging memories aselectronic synapses. Design considerations and challenges such as requirementsand design targets on multilevel states, device variability, programmingenergy, array-level connectivity, fan-in/fanout, wire energy, and IR drop arepresented. Wires are increasingly important in design decisions, especially forlarge systems, and cycle-to-cycle variations have large impact on learningperformance.
arxiv-18000-204 | Visual Saliency Based on Scale-Space Analysis in the Frequency Domain | http://arxiv.org/pdf/1605.01999v1.pdf | author:Jian Li, Martin Levine, Xiangjing An, Xin Xu, Hangen He category:cs.CV published:2016-05-06 summary:We address the issue of visual saliency from three perspectives. First, weconsider saliency detection as a frequency domain analysis problem. Second, weachieve this by employing the concept of {\it non-saliency}. Third, wesimultaneously consider the detection of salient regions of different size. Thepaper proposes a new bottom-up paradigm for detecting visual saliency,characterized by a scale-space analysis of the amplitude spectrum of naturalimages. We show that the convolution of the {\it image amplitude spectrum} witha low-pass Gaussian kernel of an appropriate scale is equivalent to such animage saliency detector. The saliency map is obtained by reconstructing the 2-Dsignal using the original phase and the amplitude spectrum, filtered at a scaleselected by minimizing saliency map entropy. A Hypercomplex Fourier Transformperforms the analysis in the frequency domain. Using available databases, wedemonstrate experimentally that the proposed model can predict human fixationdata. We also introduce a new image database and use it to show that thesaliency detector can highlight both small and large salient regions, as wellas inhibit repeated distractors in cluttered images. In addition, we show thatit is able to predict salient regions on which people focus their attention.
arxiv-18000-205 | Interpretable Classification Models for Recidivism Prediction | http://arxiv.org/pdf/1503.07810v4.pdf | author:Jiaming Zeng, Berk Ustun, Cynthia Rudin category:stat.ML stat.AP published:2015-03-26 summary:We investigate a long-debated question, which is how to create predictivemodels of recidivism that are sufficiently accurate, transparent, andinterpretable to use for decision-making. This question is complicated as thesemodels are used to support different decisions, from sentencing, to determiningrelease on probation, to allocating preventative social services. Each use casemight have an objective other than classification accuracy, such as a desiredtrue positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair isa point on the receiver operator characteristic (ROC) curve. We use popularmachine learning methods to create models along the full ROC curve on a widerange of recidivism prediction problems. We show that many methods (SVM, RidgeRegression) produce equally accurate models along the full ROC curve. However,methods that designed for interpretability (CART, C5.0) cannot be tuned toproduce models that are accurate and/or interpretable. To handle thisshortcoming, we use a new method known as SLIM (Supersparse Linear IntegerModels) to produce accurate, transparent, and interpretable models along thefull ROC curve. These models can be used for decision-making for many differentuse cases, since they are just as accurate as the most powerful black-boxmachine learning models, but completely transparent, and highly interpretable.
arxiv-18000-206 | Automatic LQR Tuning Based on Gaussian Process Global Optimization | http://arxiv.org/pdf/1605.01950v1.pdf | author:Alonso Marco, Philipp Hennig, Jeannette Bohg, Stefan Schaal, Sebastian Trimpe category:cs.RO cs.LG cs.SY published:2016-05-06 summary:This paper proposes an automatic controller tuning framework based on linearoptimal control combined with Bayesian optimization. With this framework, aninitial set of controller gains is automatically improved according to apre-defined performance objective evaluated from experimental data. Theunderlying Bayesian optimization algorithm is Entropy Search, which representsthe latent objective as a Gaussian process and constructs an explicit beliefover the location of the objective minimum. This is used to maximize theinformation gain from each experimental evaluation. Thus, this framework shallyield improved controllers with fewer evaluations compared to alternativeapproaches. A seven-degree-of-freedom robot arm balancing an inverted pole isused as the experimental demonstrator. Results of a two- and four-dimensionaltuning problems highlight the method's potential for automatic controllertuning on robotic platforms.
arxiv-18000-207 | Energy Disaggregation for Real-Time Building Flexibility Detection | http://arxiv.org/pdf/1605.01939v1.pdf | author:Elena Mocanu, Phuong H. Nguyen, Madeleine Gibescu category:stat.ML cs.AI cs.LG published:2016-05-06 summary:Energy is a limited resource which has to be managed wisely, taking intoaccount both supply-demand matching and capacity constraints in thedistribution grid. One aspect of the smart energy management at the buildinglevel is given by the problem of real-time detection of flexible demandavailable. In this paper we propose the use of energy disaggregation techniquesto perform this task. Firstly, we investigate the use of existingclassification methods to perform energy disaggregation. A comparison isperformed between four classifiers, namely Naive Bayes, k-Nearest Neighbors,Support Vector Machine and AdaBoost. Secondly, we propose the use of RestrictedBoltzmann Machine to automatically perform feature extraction. The extractedfeatures are then used as inputs to the four classifiers and consequently shownto improve their accuracy. The efficiency of our approach is demonstrated on areal database consisting of detailed appliance-level measurements with hightemporal resolution, which has been used for energy disaggregation in previousstudies, namely the REDD. The results show robustness and good generalizationcapabilities to newly presented buildings with at least 96% accuracy.
arxiv-18000-208 | UAV-based Autonomous Image Acquisition with Multi-View Stereo Quality Assurance by Confidence Prediction | http://arxiv.org/pdf/1605.01923v1.pdf | author:Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, Horst Bischof category:cs.CV cs.RO published:2016-05-06 summary:In this paper we present an autonomous system for acquiring close-rangehigh-resolution images that maximize the quality of a later-on 3Dreconstruction with respect to coverage, ground resolution and 3D uncertainty.In contrast to previous work, our system uses the already acquired images topredict the confidence in the output of a dense multi-view stereo approachwithout executing it. This confidence encodes the likelihood of a successfulreconstruction with respect to the observed scene and potential cameraconstellations. Our prediction module runs in real-time and can be trainedwithout any externally recorded ground truth. We use the confidence predictionfor on-site quality assurance and for planning further views that are tailoredfor a specific multi-view stereo approach with respect to the given scene. Wedemonstrate the capabilities of our approach with an autonomous Unmanned AerialVehicle (UAV) in a challenging outdoor scenario.
arxiv-18000-209 | User Reviews and Language: How Language Influences Ratings | http://arxiv.org/pdf/1605.01919v1.pdf | author:Scott A. Hale category:cs.HC cs.CL cs.CY H.5.m; H.3.5 published:2016-05-06 summary:The number of user reviews of tourist attractions, restaurants, mobile apps,etc. is increasing for all languages; yet, research is lacking on how reviewsin multiple languages should be aggregated and displayed. Speakers of differentlanguages may have consistently different experiences, e.g., differentinformation available in different languages at tourist attractions ordifferent user experiences with software due tointernationalization/localization choices. This paper assesses the similarityin the ratings given by speakers of different languages to London touristattractions on TripAdvisor. The correlations between different languages aregenerally high, but some language pairs are more correlated than others. Theresults question the common practice of computing average ratings from reviewsin many languages.
arxiv-18000-210 | AVEC 2016 - Depression, Mood, and Emotion Recognition Workshop and Challenge | http://arxiv.org/pdf/1605.01600v2.pdf | author:Michel Valstar, Jonathan Gratch, Bjorn Schuller, Fabien Ringeval, Denis Lalanne, Mercedes Torres Torres, Stefan Scherer, Guiota Stratou, Roddy Cowie, Maja Pantic category:cs.CV cs.HC cs.MM published:2016-05-05 summary:The Audio/Visual Emotion Challenge and Workshop (AVEC 2016) "Depression, Moodand Emotion" will be the sixth competition event aimed at comparison ofmultimedia processing and machine learning methods for automatic audio, visualand physiological depression and emotion analysis, with all participantscompeting under strictly the same conditions. The goal of the Challenge is toprovide a common benchmark test set for multi-modal information processing andto bring together the depression and emotion recognition communities, as wellas the audio, video and physiological processing communities, to compare therelative merits of the various approaches to depression and emotion recognitionunder well-defined and strictly comparable conditions and establish to whatextent fusion of the approaches is possible and beneficial. This paper presentsthe challenge guidelines, the common data used, and the performance of thebaseline system on the two tasks.
arxiv-18000-211 | Market forecasting using Hidden Markov Models | http://arxiv.org/pdf/1504.07829v2.pdf | author:Sara Rebagliati, Emanuela Sasso, Samuele Soraggi category:stat.ML cs.LG 91B84 published:2015-04-29 summary:Working on the daily closing prices and logreturns, in this paper we dealwith the use of Hidden Markov Models (HMMs) to forecast the price of theEUR/USD Futures. The aim of our work is to understand how the HMMs describedifferent financial time series depending on their structure. Subsequently, weanalyse the forecasting methods exposed in the previous literature, putting onevidence their pros and cons.
arxiv-18000-212 | Notes on Low-rank Matrix Factorization | http://arxiv.org/pdf/1507.00333v3.pdf | author:Yuan Lu, Jie Yang category:cs.NA cs.IR cs.LG published:2015-06-30 summary:Low-rank matrix factorization (MF) is an important technique in data science.The key idea of MF is that there exists latent structures in the data, byuncovering which we could obtain a compressed representation of the data. Byfactorizing an original matrix to low-rank matrices, MF provides a unifiedmethod for dimension reduction, clustering, and matrix completion. In thisarticle we review several important variants of MF, including: Basic MF,Non-negative MF, Orthogonal non-negative MF. As can be told from their names,non-negative MF and orthogonal non-negative MF are variants of basic MF withnon-negativity and/or orthogonality constraints. Such constraints are useful inspecific senarios. In the first part of this article, we introduce, for each ofthese models, the application scenarios, the distinctive properties, and theoptimizing method. By properly adapting MF, we can go beyond the problem ofclustering and matrix completion. In the second part of this article, we willextend MF to sparse matrix compeletion, enhance matrix compeletion usingvarious regularization methods, and make use of MF for (semi-)supervisedlearning by introducing latent space reinforcement and transformation. We willsee that MF is not only a useful model but also as a flexible framework that isapplicable for various prediction problems.
arxiv-18000-213 | Resource allocation using metaheuristic search | http://arxiv.org/pdf/1605.01855v1.pdf | author:Andy M. Connor, Amit Shah category:cs.NE published:2016-05-06 summary:This research is focused on solving problems in the area of software projectmanagement using metaheuristic search algorithms and as such is research in thefield of search based software engineering. The main aim of this research is toevaluate the performance of different metaheuristic search techniques inresource allocation and scheduling problems that would be typical of softwaredevelopment projects. This paper reports a set of experiments which evaluatethe performance of three algorithms, namely simulated annealing, tabu searchand genetic algorithms. The experimental results indicate that all of themetaheuristics search techniques can be used to solve problems in resourceallocation and scheduling within a software project. Finally, a comparativeanalysis suggests that overall the genetic algorithm had performed better thansimulated annealing and tabu search.
arxiv-18000-214 | Detecting Context Dependence in Exercise Item Candidates Selected from Corpora | http://arxiv.org/pdf/1605.01845v1.pdf | author:Ildikó Pilán category:cs.CL published:2016-05-06 summary:We explore the factors influencing the dependence of single sentences ontheir larger textual context in order to automatically identify candidatesentences for language learning exercises from corpora which are presentable inisolation. An in-depth investigation of this question has not been previouslycarried out. Understanding this aspect can contribute to a more efficientselection of candidate sentences which, besides reducing the time required foritem writing, can also ensure a higher degree of variability and authenticity.We present a set of relevant aspects collected based on the qualitativeanalysis of a smaller set of context-dependent corpus example sentences.Furthermore, we implemented a rule-based algorithm using these criteria whichachieved an average precision of 0.76 for the identification of differentissues related to context dependence. The method has also been evaluatedempirically where 80% of the sentences in which our system did not detectcontext-dependent elements were also considered context-independent by humanraters.
arxiv-18000-215 | Convergence of Contrastive Divergence Algorithm in Exponential Family | http://arxiv.org/pdf/1603.05729v2.pdf | author:Tung-Yu Wu, Bai Jiang, Yifan Jin, Wing H. Wong category:stat.ML published:2016-03-17 summary:This paper studies the convergence properties of contrastive divergencealgorithm for parameter inference in exponential family, by relating it toMarkov chain theory and stochastic stability literature. We prove that, undermild conditions and given a finite data sample $X_1,\dots,X_n \simp_{\theta^*}$ i.i.d. in an event with probability approaching to 1, thesequence $\{\theta_t\}_{t \ge 0}$ generated by CD algorithm is a positiveHarris recurrent chain, and thus processes an unique invariant distribution$\pi_n$. The invariant distribution concentrates around the Maximum LikelihoodEstimate at a speed arbitrarily slower than $\sqrt{n}$, and the number of stepsin Markov Chain Monte Carlo only affects the coefficient factor of theconcentration rate. Finally we conclude that as $n \to \infty$, $$\limsup_{t\to \infty} \left\Vert \frac{1}{t} \sum_{s=1}^t \theta_s - \theta^*\right\Vert\overset{p}{\to} 0.$$
arxiv-18000-216 | Perceptually Consistent Color-to-Gray Image Conversion | http://arxiv.org/pdf/1605.01843v1.pdf | author:Shaodi You, Nick Barnes, Janine Walker category:cs.CV published:2016-05-06 summary:In this paper, we propose a color to grayscale image conversion algorithm(C2G) that aims to preserve the perceptual properties of the color image asmuch as possible. To this end, we propose measures for two perceptualproperties based on contemporary research in vision science: brightness andmulti-scale contrast. The brightness measurement is based on the idea that thebrightness of a grayscale image will affect the perception of the probabilityof color information. The color contrast measurement is based on the idea thatthe contrast of a given pixel to its surroundings can be measured as a linearcombination of color contrast at different scales. Based on these measures wepropose a graph based optimization framework to balance the brightness andcontrast measurements. To solve the optimization, an $\ell_1$-norm based methodis provided which converts color discontinuities to brightness discontinuities.To validate our methods, we evaluate against the existing \cadik and Color250datasets, and against NeoColor, a new dataset that improves over existing C2Gdatasets. NeoColor contains around 300 images from typical C2G scenarios,including: commercial photograph, printing, books, magazines, masterpieceartworks and computer designed graphics. We show improvements in metrics ofperformance, and further through a user study, we validate the performance ofboth the algorithm and the metric.
arxiv-18000-217 | Beyond Local Search: Tracking Objects Everywhere with Instance-Specific Proposals | http://arxiv.org/pdf/1605.01839v1.pdf | author:Gao Zhu, Fatih Porikli, Hongdong Li category:cs.CV published:2016-05-06 summary:Most tracking-by-detection methods employ a local search window around thepredicted object location in the current frame assuming the previous locationis accurate, the trajectory is smooth, and the computational capacity permits asearch radius that can accommodate the maximum speed yet small enough to reducemismatches. These, however, may not be valid always, in particular for fast andirregularly moving objects. Here, we present an object tracker that is notlimited to a local search window and has ability to probe efficiently theentire frame. Our method generates a small number of "high-quality" proposalsby a novel instance-specific objectness measure and evaluates them against theobject model that can be adopted from an existing tracking-by-detectionapproach as a core tracker. During the tracking process, we update the objectmodel concentrating on hard false-positives supplied by the proposals, whichhelp suppressing distractors caused by difficult background clutters, and learnhow to re-rank proposals according to the object model. Since we reducesignificantly the number of hypotheses the core tracker evaluates, we can usericher object descriptors and stronger detector. Our method outperforms mostrecent state-of-the-art trackers on popular tracking benchmarks, and providesimproved robustness for fast moving objects as well as for ultra low-frame-ratevideos.
arxiv-18000-218 | DeepPicker: a Deep Learning Approach for Fully Automated Particle Picking in Cryo-EM | http://arxiv.org/pdf/1605.01838v1.pdf | author:Feng Wang, Huichao Gong, Gaochao liu, Meijing Li, Chuangye Yan, Tian Xia, Xueming Li, Jianyang Zeng category:q-bio.QM cs.LG published:2016-05-06 summary:Particle picking is a time-consuming step in single-particle analysis andoften requires significant interventions from users, which has become abottleneck for future automated electron cryo-microscopy (cryo-EM). Here wereport a deep learning framework, called DeepPicker, to address this problemand fill the current gaps toward a fully automated cryo-EM pipeline. DeepPickeremploys a novel cross-molecule training strategy to capture common features ofparticles from previously-analyzed micrographs, and thus does not require anyhuman intervention during particle picking. Tests on the recently-publishedcryo-EM data of three complexes have demonstrated that our deep learning basedscheme can successfully accomplish the human-level particle picking process andidentify a sufficient number of particles that are comparable to those manuallyby human experts. These results indicate that DeepPicker can provide apractically useful tool to significantly reduce the time and manual effortspent in single-particle analysis and thus greatly facilitate high-resolutioncryo-EM structure determination.
arxiv-18000-219 | Cross-Graph Learning of Multi-Relational Associations | http://arxiv.org/pdf/1605.01832v1.pdf | author:Hanxiao Liu, Yiming Yang category:cs.LG published:2016-05-06 summary:Cross-graph Relational Learning (CGRL) refers to the problem of predictingthe strengths or labels of multi-relational tuples of heterogeneous objecttypes, through the joint inference over multiple graphs which specify theinternal connections among each type of objects. CGRL is an open challenge inmachine learning due to the daunting number of all possible tuples to deal withwhen the numbers of nodes in multiple graphs are large, and because the labeledtraining instances are extremely sparse as typical. Existing methods such astensor factorization or tensor-kernel machines do not work well because of thelack of convex formulation for the optimization of CGRL models, the poorscalability of the algorithms in handling combinatorial numbers of tuples,and/or the non-transductive nature of the learning methods which limits theirability to leverage unlabeled data in training. This paper proposes a novelframework which formulates CGRL as a convex optimization problem, enablestransductive learning using both labeled and unlabeled tuples, and offers ascalable algorithm that guarantees the optimal solution and enjoys a lineartime complexity with respect to the sizes of input graphs. In our experimentswith a subset of DBLP publication records and an Enzyme multi-source dataset,the proposed method successfully scaled to the large cross-graph inferenceproblem, and outperformed other representative approaches significantly.
arxiv-18000-220 | Robust Optical Flow Estimation of Double-Layer Images under Transparency or Reflection | http://arxiv.org/pdf/1605.01825v1.pdf | author:Jiaolong Yang, Hongdong Li, Yuchao Dai, Robby T. Tan category:cs.CV published:2016-05-06 summary:This paper deals with a challenging, frequently encountered, yet not properlyinvestigated problem in two-frame optical flow estimation. That is, the inputframes are compounds of two imaging layers -- one desired background layer ofthe scene, and one distracting, possibly moving layer due to transparency orreflection. In this situation, the conventional brightness constancy constraint-- the cornerstone of most existing optical flow methods -- will no longer bevalid. In this paper, we propose a robust solution to this problem. Theproposed method performs both optical flow estimation, and image layerseparation. It exploits a generalized double-layer brightness consistencyconstraint connecting these two tasks, and utilizes the priors for both ofthem. Experiments on both synthetic data and real images have confirmed theefficacy of the proposed method. To the best of our knowledge, this is thefirst attempt towards handling generic optical flow fields of two-frame imagescontaining transparency or reflection.
arxiv-18000-221 | Weighted SGD for $\ell_p$ Regression with Randomized Preconditioning | http://arxiv.org/pdf/1502.03571v3.pdf | author:Jiyan Yang, Yin-Lam Chow, Christopher Ré, Michael W. Mahoney category:math.OC stat.ML published:2015-02-12 summary:In recent years, stochastic gradient descent (SGD) methods and randomizedlinear algebra (RLA) algorithms have been applied to many large-scale problemsin machine learning and data analysis. We aim to bridge the gap between thesetwo methods in solving constrained overdetermined linear regressionproblems---e.g., $\ell_2$ and $\ell_1$ regression problems. We propose a hybridalgorithm named pwSGD that uses RLA techniques for preconditioning andconstructing an importance sampling distribution, and then performs an SGD-likeiterative process with weighted sampling on the preconditioned system. We provethat pwSGD inherits faster convergence rates that only depend on the lowerdimension of the linear system, while maintaining low computation complexity.Particularly, when solving $\ell_1$ regression with size $n$ by $d$, pwSGDreturns an approximate solution with $\epsilon$ relative error in the objectivevalue in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d)/\epsilon^2)$time. This complexity is uniformly better than that of RLA methods in terms ofboth $\epsilon$ and $d$ when the problem is unconstrained. For $\ell_2$regression, pwSGD returns an approximate solution with $\epsilon$ relativeerror in the objective value and the solution vector measured in predictionnorm in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d)\log(1/\epsilon) /\epsilon)$ time. We also provide lower bounds on the coresetcomplexity for more general regression problems, indicating that still newideas will be needed to extend similar RLA preconditioning ideas to weightedSGD algorithms for more general regression problems. Finally, the effectivenessof such algorithms is illustrated numerically on both synthetic and realdatasets.
arxiv-18000-222 | Estimating Sparse Signals with Smooth Support via Convex Programming and Block Sparsity | http://arxiv.org/pdf/1605.01813v1.pdf | author:Sohil Shah, Tom Goldstein, Christoph Studer category:cs.CV published:2016-05-06 summary:Conventional algorithms for sparse signal recovery and sparse representationrely on $l_1$-norm regularized variational methods. However, when applied tothe reconstruction of $\textit{sparse images}$, i.e., images where only a fewpixels are non-zero, simple $l_1$-norm-based methods ignore potentialcorrelations in the support between adjacent pixels. In a number ofapplications, one is interested in images that are not only sparse, but alsohave a support with smooth (or contiguous) boundaries. Existing algorithms thattake into account such a support structure mostly rely on non-convex methodsand---as a consequence---do not scale well to high-dimensional problems and/ordo not converge to global optima. In this paper, we explore the use of newblock $l_1$-norm regularizers, which enforce image sparsity whilesimultaneously promoting smooth support structure. By exploiting the convexityof our regularizers, we develop new computationally-efficient recoveryalgorithms that guarantee global optimality. We demonstrate the efficacy of ourregularizers on a variety of imaging tasks including compressive imagerecovery, image restoration, and robust PCA.
arxiv-18000-223 | Information Recovery from Pairwise Measurements | http://arxiv.org/pdf/1504.01369v4.pdf | author:Yuxin Chen, Changho Suh, Andrea J. Goldsmith category:cs.IT cs.DM cs.LG math.IT math.ST stat.ML stat.TH published:2015-04-06 summary:This paper is concerned with jointly recovering $n$ node-variables $\left\{x_{i}\right\}_{1\leq i\leq n}$ from a collection of pairwise differencemeasurements. Imagine we acquire a few observations taking the form of$x_{i}-x_{j}$; the observation pattern is represented by a measurement graph$\mathcal{G}$ with an edge set $\mathcal{E}$ such that $x_{i}-x_{j}$ isobserved if and only if $(i,j)\in\mathcal{E}$. To account for noisymeasurements in a general manner, we model the data acquisition process by aset of channels with given input/output transition measures. Employinginformation-theoretic tools applied to channel decoding problems, we develop a\emph{unified} framework to characterize the fundamental recovery criterion,which accommodates general graph structures, alphabet sizes, and channeltransition measures. In particular, our results isolate a family of\emph{minimum} \emph{channel divergence measures} to characterize the degree ofmeasurement corruption, which together with the size of the minimum cut of$\mathcal{G}$ dictates the feasibility of exact information recovery. Forvarious homogeneous graphs, the recovery condition depends almost only on theedge sparsity of the measurement graph irrespective of other graphical metrics;alternatively, the minimum sample complexity required for these graphs scaleslike \[ \text{minimum sample complexity }\asymp\frac{n\logn}{\mathsf{Hel}_{1/2}^{\min}} \] for certain information metric$\mathsf{Hel}_{1/2}^{\min}$ defined in the main text, as long as the alphabetsize is not super-polynomial in $n$. We apply our general theory to threeconcrete applications, including the stochastic block model, the outlier model,and the haplotype assembly problem. Our theory leads to order-wise tightrecovery conditions for all these scenarios.
arxiv-18000-224 | Shape from Mixed Polarization | http://arxiv.org/pdf/1605.02066v1.pdf | author:Vage Taamazyan, Achuta Kadambi, Ramesh Raskar category:cs.CV published:2016-05-06 summary:Shape from Polarization (SfP) estimates surface normals using photos capturedat different polarizer rotations. Fundamentally, the SfP model assumes thatlight is reflected either diffusely or specularly. However, this model is notvalid for many real-world surfaces exhibiting a mixture of diffuse and specularproperties. To address this challenge, previous methods have used a sequentialsolution: first, use an existing algorithm to separate the scene into diffuseand specular components, then apply the appropriate SfP model. In this paper,we propose a new method that jointly uses viewpoint and polarization data toholistically separate diffuse and specular components, recover refractiveindex, and ultimately recover 3D shape. By involving the physics ofpolarization in the separation process, we demonstrate competitive results witha benchmark method, while recovering additional information (e.g. refractiveindex).
arxiv-18000-225 | Robust SAR STAP via Kronecker Decomposition | http://arxiv.org/pdf/1605.01790v1.pdf | author:Kristjan Greenewald, Edmund Zelnio, Alfred Hero category:cs.CV published:2016-05-05 summary:This paper proposes a spatio-temporal decomposition for the detection ofmoving targets in multiantenna SAR. As a high resolution radar imagingmodality, SAR detects and localizes non-moving targets accurately, giving it anadvantage over lower resolution GMTI radars. Moving target detection is morechallenging due to target smearing and masking by clutter. Space-time adaptiveprocessing (STAP) is often used to remove the stationary clutter and enhancethe moving targets. In this work, it is shown that the performance of STAP canbe improved by modeling the clutter covariance as a space vs. time Kroneckerproduct with low rank factors. Based on this model, a low-rank Kroneckerproduct covariance estimation algorithm is proposed, and a novel separableclutter cancelation filter based on the Kronecker covariance estimate isintroduced. The proposed method provides orders of magnitude reduction in therequired number of training samples, as well as improved robustness tocorruption of the training data. Simulation results and experiments using theGotcha SAR GMTI challenge dataset are presented that confirm the advantages ofour approach relative to existing techniques.
arxiv-18000-226 | Adaptive coherence estimator (ACE) for explosive hazard detection using wideband electromagnetic induction (WEMI) | http://arxiv.org/pdf/1603.06140v3.pdf | author:Brendan Alvey, Alina Zare, Matthew Cook, Dominic K. Ho category:cs.CV published:2016-03-19 summary:The adaptive coherence estimator (ACE) estimates the squared cosine of theangle between a known target vector and a sample vector in a whitenedcoordinate space. The space is whitened according to an estimation of thebackground statistics, which directly effects the performance of the statisticas a target detector. In this paper, the ACE detection statistic is used todetect buried explosive hazards with data from a Wideband ElectromagneticInduction (WEMI) sensor. Target signatures are based on a dictionary definedusing a Discrete Spectrum of Relaxation Frequencies (DSRF) model. Results aresummarized as a receiver operator curve (ROC) and compared to other leadingmethods.
arxiv-18000-227 | Provable Bayesian Inference via Particle Mirror Descent | http://arxiv.org/pdf/1506.03101v3.pdf | author:Bo Dai, Niao He, Hanjun Dai, Le Song category:cs.LG stat.CO stat.ML published:2015-06-09 summary:Bayesian methods are appealing in their flexibility in modeling complex dataand ability in capturing uncertainty in parameters. However, when Bayes' ruledoes not result in tractable closed-form, most approximate inference algorithmslack either scalability or rigorous guarantees. To tackle this challenge, wepropose a simple yet provable algorithm, \emph{Particle Mirror Descent} (PMD),to iteratively approximate the posterior density. PMD is inspired by stochasticfunctional mirror descent where one descends in the density space using a smallbatch of data points at each iteration, and by particle filtering where oneuses samples to approximate a function. We prove result of the first kind that,with $m$ particles, PMD provides a posterior density estimator that convergesin terms of $KL$-divergence to the true posterior in rate $O(1/\sqrt{m})$. Wedemonstrate competitive empirical performances of PMD compared to severalapproximate inference algorithms in mixture models, logistic regression, sparseGaussian processes and latent Dirichlet allocation on large scale datasets.
arxiv-18000-228 | Clustering on the Edge: Learning Structure in Graphs | http://arxiv.org/pdf/1605.01779v1.pdf | author:Matt Barnes, Artur Dubrawski category:stat.ML published:2016-05-05 summary:With the recent popularity of graphical clustering methods, there has been anincreased focus on the information between samples. We show how learningcluster structure using edge features naturally and simultaneously determinesthe most likely number of clusters and addresses data scale issues. Theseresults are particularly useful in instances where (a) there are a large numberof clusters and (b) we have some labeled edges. Applications in this domaininclude image segmentation, community discovery and entity resolution. Ourmodel is an extension of the planted partition model and our solution usesresults of correlation clustering, which achieves a partition O(log(n))-closeto the log-likelihood of the true clustering.
arxiv-18000-229 | We Are Humor Beings: Understanding and Predicting Visual Humor | http://arxiv.org/pdf/1512.04407v4.pdf | author:Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh category:cs.CV cs.CL cs.LG published:2015-12-14 summary:Humor is an integral part of human lives. Despite being tremendouslyimpactful, it is perhaps surprising that we do not have a detailedunderstanding of humor yet. As interactions between humans and AI systemsincrease, it is imperative that these systems are taught to understandsubtleties of human expressions such as humor. In this work, we are interestedin the question - what content in a scene causes it to be funny? As a firststep towards understanding visual humor, we analyze the humor manifested inabstract scenes and design computational models for them. We collect twodatasets of abstract scenes that facilitate the study of humor at both thescene-level and the object-level. We analyze the funny scenes and explore thedifferent types of humor depicted in them via human studies. We model two tasksthat we believe demonstrate an understanding of some aspects of visual humor.The tasks involve predicting the funniness of a scene and altering thefunniness of a scene. We show that our models perform well quantitatively, andqualitatively through human studies. Our datasets are publicly available.
arxiv-18000-230 | Rank Ordered Autoencoders | http://arxiv.org/pdf/1605.01749v1.pdf | author:Paul Bertens category:cs.LG stat.ML published:2016-05-05 summary:A new method for the unsupervised learning of sparse representations usingautoencoders is proposed and implemented by ordering the output of the hiddenunits by their activation value and progressively reconstructing the input inthis order. This can be done efficiently in parallel with the use of cumulativesums and sorting only slightly increasing the computational costs. Minimizingthe difference of this progressive reconstruction with respect to the input canbe seen as minimizing the number of active output units required for thereconstruction of the input. The model thus learns to reconstruct optimallyusing the least number of active output units. This leads to high sparsitywithout the need for extra hyperparameters, the amount of sparsity is insteadimplicitly learned by minimizing this progressive reconstruction error. Resultsof the trained model are given for patches of the CIFAR10 dataset, showingrapid convergence of features and extremely sparse output activations whilemaintaining a minimal reconstruction error and showing extreme robustness tooverfitting. Additionally the reconstruction as function of number of activeunits is presented which shows the autoencoder learns a rank order code overthe input where the highest ranked units correspond to the highest decrease inreconstruction error.
arxiv-18000-231 | Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit | http://arxiv.org/pdf/1507.01238v3.pdf | author:Chong You, Daniel P. Robinson, Rene Vidal category:cs.CV cs.LG stat.ML published:2015-07-05 summary:Subspace clustering methods based on $\ell_1$, $\ell_2$ or nuclear normregularization have become very popular due to their simplicity, theoreticalguarantees and empirical success. However, the choice of the regularizer cangreatly impact both theory and practice. For instance, $\ell_1$ regularizationis guaranteed to give a subspace-preserving affinity (i.e., there are noconnections between points from different subspaces) under broad conditions(e.g., arbitrary subspaces and corrupted data). However, it requires solving alarge scale convex optimization problem. On the other hand, $\ell_2$ andnuclear norm regularization provide efficient closed form solutions, butrequire very strong assumptions to guarantee a subspace-preserving affinity,e.g., independent subspaces and uncorrupted data. In this paper we study asubspace clustering method based on orthogonal matching pursuit. We show thatthe method is both computationally efficient and guaranteed to give asubspace-preserving affinity under broad conditions. Experiments on syntheticdata verify our theoretical analysis, and applications in handwritten digit andface clustering show that our approach achieves the best trade off betweenaccuracy and efficiency.
arxiv-18000-232 | Biobjective Performance Assessment with the COCO Platform | http://arxiv.org/pdf/1605.01746v1.pdf | author:Dimo Brockhoff, Tea Tušar, Dejan Tušar, Tobias Wagner, Nikolaus Hansen, Anne Auger category:cs.NE published:2016-05-05 summary:This document details the rationales behind assessing the performance ofnumerical black-box optimizers on multi-objective problems within the COCOplatform and in particular on the biobjective test suite bbob-biobj. Theevaluation is based on a hypervolume of all non-dominated solutions in thearchive of candidate solutions and measures the runtime until the hypervolumevalue succeeds prescribed target values.
arxiv-18000-233 | Improving Automated Patent Claim Parsing: Dataset, System, and Experiments | http://arxiv.org/pdf/1605.01744v1.pdf | author:Mengke Hu, David Cinciruk, John MacLaren Walsh category:cs.CL published:2016-05-05 summary:Off-the-shelf natural language processing software performs poorly whenparsing patent claims owing to their use of irregular language relative to thecorpora built from news articles and the web typically utilized to train thissoftware. Stopping short of the extensive and expensive process of accumulatinga large enough dataset to completely retrain parsers for patent claims, amethod of adapting existing natural language processing software towards patentclaims via forced part of speech tag correction is proposed. An AmazonMechanical Turk collection campaign organized to generate a public corpus totrain such an improved claim parsing system is discussed, identifying lessonslearned during the campaign that can be of use in future NLP dataset collectioncampaigns with AMT. Experiments utilizing this corpus and other patent claimsets measure the parsing performance improvement garnered via the claim parsingsystem. Finally, the utility of the improved claim parsing system within otherpatent processing applications is demonstrated via experiments showing improvedautomated patent subject classification when the new claim parsing system isutilized to generate the features.
arxiv-18000-234 | Plug-and-Play ADMM for Image Restoration: Fixed Point Convergence and Applications | http://arxiv.org/pdf/1605.01710v1.pdf | author:Stanley H. Chan, Xiran Wang, Omar A. Elgendy category:cs.CV published:2016-05-05 summary:Alternating direction method of multiplier (ADMM) is a widely used algorithmfor solving constrained optimization problems in image restoration. Among manyuseful features, one critical feature of the ADMM algorithm is its modularstructure which allows one to plug in any off-the-shelf image denoisingalgorithm for a subproblem in the ADMM algorithm. Because of the plug-innature, this type of ADMM algorithms is coined the name "Plug-and-Play ADMM".Plug-and-Play ADMM has demonstrated promising empirical results in a number ofrecent papers. However, it is unclear under what conditions and for whatdenoising algorithms would it guarantee convergence. Also, it is unclear towhat extent would Plug-and-Play ADMM be compared to existing methods for commonGaussian and Poissonian image restoration problems. In this paper, we propose a Plug-and-Play ADMM algorithm with provable fixedpoint convergence. We show that for any denoising algorithm satisfying aboundedness criteria, called bounded denoisers, Plug-and-Play ADMM converges toa fixed point under a continuation scheme. We demonstrate applications ofPlug-and-Play ADMM on two image restoration problems including single imagesuper-resolution and quantized Poisson image recovery for single-photonimaging. We compare Plug-and-Play ADMM with state-of-the-art algorithms in eachproblem type, and demonstrate promising experimental results of the algorithm.
arxiv-18000-235 | A note on adjusting $R^2$ for using with cross-validation | http://arxiv.org/pdf/1605.01703v1.pdf | author:Indre Zliobaite, Nikolaj Tatti category:cs.LG cs.AI stat.ML published:2016-05-05 summary:We show how to adjust the coefficient of determination ($R^2$) when used formeasuring predictive accuracy via leave-one-out cross-validation.
arxiv-18000-236 | Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes | http://arxiv.org/pdf/1604.02125v2.pdf | author:Gordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol, Yash Goyal, Kevin Kochersberger, Dhruv Batra category:cs.CV cs.CL cs.LG published:2016-04-07 summary:We present an approach to simultaneously perform semantic segmentation andprepositional phrase attachment resolution for captioned images. The motivationfor this work comes from the fact that some ambiguities in language simplycannot be resolved without simultaneously reasoning about an associated image.If we consider the sentence "I shot an elephant in my pajamas", looking at thelanguage alone (and not reasoning about common sense), it is unclear if it isthe person or the elephant that is wearing the pajamas or both. Our approachinvolves producing a diverse set of plausible hypotheses for both semanticsegmentation and prepositional phrase attachment resolution that are thenjointly re-ranked to select the most consistent pair. We show that our semanticsegmentation and prepositional phrase attachment resolution modules havecomplementary strengths, and that joint reasoning produces more accurateresults than any module operating in isolation. We also show that multiplehypotheses are crucial to improved multiple-module reasoning. Our vision andlanguage approach significantly outperforms a state-of-the-art NLP system(Stanford Parser [16,27]) by 17.91% (28.69% relative) in one experiment, and by12.83% (25.28% relative) in another. We also make small improvements over astate-of-the-art vision system (DeepLab-CRF [13]).
arxiv-18000-237 | Phase 2: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Machine Learning Detection Algorithms | http://arxiv.org/pdf/1605.00972v2.pdf | author:Peter J. Dugan, Christopher W. Clark, Yann André LeCun, Sofie M. Van Parijs category:cs.CV published:2016-05-03 summary:Overarching goals for this work aim to advance the state of the art fordetection, classification and localization (DCL) in the field of bioacoustics.This goal is primarily achieved by building a generic framework fordetection-classification (DC) using a fast, efficient and scalablearchitecture, demonstrating the capabilities of this system using on a varietyof low-frequency mid-frequency cetacean sounds. Two primary goals are todevelop transferable technologies for detection and classification in, one: thearea of advanced algorithms, such as deep learning and other methods; and two:advanced systems, capable of real-time and archival processing. For each keyarea, we will focus on producing publications from this work and providingtools and software to the community where/when possible. Currently massiveamounts of acoustic data are being collected by various institutions,corporations and national defense agencies. The long-term goal is to providetechnical capability to analyze the data using automatic algorithms for (DC)based on machine intelligence. The goal of the automation is to provideeffective and efficient mechanisms by which to process large acoustic datasetsfor understanding the bioacoustic behaviors of marine mammals. This capabilitywill provide insights into the potential ecological impacts and influences ofanthropogenic ocean sounds. This work focuses on building technologies using amaturity model based on DARPA 6.1 and 6.2 processes, for basic and appliedresearch, respectively.
arxiv-18000-238 | Learning Action Maps of Large Environments via First-Person Vision | http://arxiv.org/pdf/1605.01679v1.pdf | author:Nicholas Rhinehart, Kris M. Kitani category:cs.CV published:2016-05-05 summary:When people observe and interact with physical spaces, they are able toassociate functionality to regions in the environment. Our goal is to automatedense functional understanding of large spaces by leveraging sparse activitydemonstrations recorded from an ego-centric viewpoint. The method we describeenables functionality estimation in large scenes where people have behaved, aswell as novel scenes where no behaviors are observed. Our method learns andpredicts "Action Maps", which encode the ability for a user to performactivities at various locations. With the usage of an egocentric camera toobserve human activities, our method scales with the size of the scene withoutthe need for mounting multiple static surveillance cameras and is well-suitedto the task of observing activities up-close. We demonstrate that by capturingappearance-based attributes of the environment and associating these attributeswith activity demonstrations, our proposed mathematical framework allows forthe prediction of Action Maps in new environments. Additionally, we offer apreliminary glance of the applicability of Action Maps by demonstrating aproof-of-concept application in which they are used in concert with activitydetections to perform localization.
arxiv-18000-239 | Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm | http://arxiv.org/pdf/1605.01677v1.pdf | author:Junpei Komiyama, Junya Honda, Hiroshi Nakagawa category:stat.ML cs.LG published:2016-05-05 summary:We study the K-armed dueling bandit problem, a variation of the standardstochastic bandit problem where the feedback is limited to relative comparisonsof a pair of arms. The hardness of recommending Copeland winners, the arms thatbeat the greatest number of other arms, is characterized by deriving anasymptotic regret bound. We propose Copeland Winners Relative Minimum EmpiricalDivergence (CW-RMED) and derive an asymptotically optimal regret bound for it.However, it is not known whether the algorithm can be efficiently computed ornot. To address this issue, we devise an efficient version (ECW-RMED) andderive its asymptotic regret bound. Experimental comparisons of dueling banditalgorithms show that ECW-RMED significantly outperforms existing ones.
arxiv-18000-240 | Parallels of human language in the behavior of bottlenose dolphins | http://arxiv.org/pdf/1605.01661v1.pdf | author:R. Ferrer-i-Cancho, D. Lusseau, B. McCowan category:q-bio.NC cs.CL published:2016-05-05 summary:A short review of similarities between dolphins and humans with the help ofquantitative linguistics and information theory.
arxiv-18000-241 | A Tight Bound of Hard Thresholding | http://arxiv.org/pdf/1605.01656v1.pdf | author:Jie Shen, Ping Li category:stat.ML cs.IT math.IT published:2016-05-05 summary:This paper is concerned with the hard thresholding technique which sets allbut the $k$ largest absolute elements to zero. We establish a tight bound thatquantitatively characterizes the deviation of the thresholded solution from agiven signal. Our theoretical result is universal in the sense that it holdsfor all choices of parameters, and the underlying analysis only depends onfundamental arguments in mathematical optimization. We discuss the implicationsfor the literature: Compressed Sensing. On account of the crucial estimate, we bridge theconnection between restricted isometry property (RIP) and the sparsityparameter of $k$ for a vast volume of hard thresholding based algorithms, whichrenders an improvement on the RIP condition especially when the true sparsityis unknown. This suggests that in essence, many more kinds of sensing matricesor fewer measurements are admissible for the data acquisition procedure. Machine Learning. In terms of large-scale machine learning, a significant yetchallenging problem is producing sparse solutions in online setting. In starkcontrast to prior works that attempted the $\ell_1$ relaxation for promotingsparsity, we present a novel algorithm which performs hard thresholding in eachiteration to ensure such parsimonious solutions. Equipped with the developedbound for hard thresholding, we prove global linear convergence for a number ofprevalent statistical models under mild assumptions, even though the problemturns out to be non-convex.
arxiv-18000-242 | Stance and Sentiment in Tweets | http://arxiv.org/pdf/1605.01655v1.pdf | author:Saif M. Mohammad, Parinaz Sobhani, Svetlana Kiritchenko category:cs.CL published:2016-05-05 summary:We can often detect from a person's utterances whether he/she is in favor ofor against a given target entity -- their stance towards the target. However, aperson may express the same stance towards a target by using negative orpositive language. Here for the first time we present a dataset oftweet--target pairs annotated for both stance and sentiment. The targets may ormay not be referred to in the tweets, and they may or may not be the target ofopinion in the tweets. Partitions of this dataset were used as training andtest sets in a SemEval-2016 shared task competition. We propose a simple stancedetection system that outperforms submissions from all 19 teams thatparticipated in the shared task. Additionally, access to both stance andsentiment annotations allows us to explore several research questions. We showthat while knowing the sentiment expressed by a tweet is beneficial for stanceclassification, it alone is not sufficient. Finally, we use additionalunlabeled data through distant supervision techniques and word embeddings tofurther improve stance classification.
arxiv-18000-243 | LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues | http://arxiv.org/pdf/1605.01652v1.pdf | author:Phong Le, Marc Dymetman, Jean-Michel Renders category:cs.AI cs.CL published:2016-05-05 summary:We introduce an LSTM-based method for dynamically integrating severalword-prediction experts to obtain a conditional language model which can begood simultaneously at several subtasks. We illustrate this general approachwith an application to dialogue where we integrate a neural chat model, good atconversational aspects, with a neural question-answering model, good atretrieving precise information from a knowledge-base, and show how theintegration combines the strengths of the independent components. We hope thatthis focused contribution will attract attention on the benefits of using suchmixtures of experts in NLP.
arxiv-18000-244 | The IBM Speaker Recognition System: Recent Advances and Error Analysis | http://arxiv.org/pdf/1605.01635v1.pdf | author:Seyed Omid Sadjadi, Jason Pelecanos, Sriram Ganapathy category:cs.CL cs.SD stat.ML published:2016-05-05 summary:We present the recent advances along with an error analysis of the IBMspeaker recognition system for conversational speech. Some of the keyadvancements that contribute to our system include: a nearest-neighbordiscriminant analysis (NDA) approach (as opposed to LDA) for intersessionvariability compensation in the i-vector space, the application of speaker andchannel-adapted features derived from an automatic speech recognition (ASR)system for speaker recognition, and the use of a DNN acoustic model with a verylarge number of output units (~10k senones) to compute the frame-level softalignments required in the i-vector estimation process. We evaluate thesetechniques on the NIST 2010 SRE extended core conditions (C1-C9), as well asthe 10sec-10sec condition. To our knowledge, results achieved by our systemrepresent the best performances published to date on these conditions. Forexample, on the extended tel-tel condition (C5) the system achieves an EER of0.59%. To garner further understanding of the remaining errors (on C5), weexamine the recordings associated with the low scoring target trials, wherevarious issues are identified for the problematic recordings/trials.Interestingly, it is observed that correcting the pathological recordings notonly improves the scores for the target trials but also for the nontargettrials.
arxiv-18000-245 | On the Convergence of A Family of Robust Losses for Stochastic Gradient Descent | http://arxiv.org/pdf/1605.01623v1.pdf | author:Bo Han, Ivor W. Tsang, Ling Chen category:cs.LG published:2016-05-05 summary:The convergence of Stochastic Gradient Descent (SGD) using convex lossfunctions has been widely studied. However, vanilla SGD methods using convexlosses cannot perform well with noisy labels, which adversely affect the updateof the primal variable in SGD methods. Unfortunately, noisy labels areubiquitous in real world applications such as crowdsourcing. To handle noisylabels, in this paper, we present a family of robust losses for SGD methods. Byemploying our robust losses, SGD methods successfully reduce negative effectscaused by noisy labels on each update of the primal variable. We not onlyreveal that the convergence rate is O(1/T) for SGD methods using robust losses,but also provide the robustness analysis on two representative robust losses.Comprehensive experimental results on six real-world datasets show that SGDmethods using robust losses are obviously more robust than other baselinemethods in most situations with fast convergence.
arxiv-18000-246 | Large Scale Deep Convolutional Neural Network Features Search with Lucene | http://arxiv.org/pdf/1603.09687v3.pdf | author:Claudio Gennaro category:cs.CV cs.IR published:2016-03-31 summary:In this work, we propose an approach to index Deep Convolutional NeuralNetwork Features to support efficient content-based retrieval on large imagedatabases. To this aim, we have converted the these features into a textualform, to index them into an inverted index by means of Lucene. In this way, wewere able to set up a robust retrieval system that combines full-text searchwith content-based image retrieval capabilities. We evaluated differentstrategies of textual representation in order to optimize the index occupationand the query response time. In order to show that our approach is able tohandle large datasets, we have developed a web-based prototype that provides aninterface for combined textual and visual searching into a dataset of about 100million of images.
arxiv-18000-247 | MatConvNet - Convolutional Neural Networks for MATLAB | http://arxiv.org/pdf/1412.4564v3.pdf | author:Andrea Vedaldi, Karel Lenc category:cs.CV cs.LG cs.MS cs.NE published:2014-12-15 summary:MatConvNet is an implementation of Convolutional Neural Networks (CNNs) forMATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.It exposes the building blocks of CNNs as easy-to-use MATLAB functions,providing routines for computing linear convolutions with filter banks, featurepooling, and many more. In this manner, MatConvNet allows fast prototyping ofnew CNN architectures; at the same time, it supports efficient computation onCPU and GPU allowing to train complex models on large datasets such as ImageNetILSVRC. This document provides an overview of CNNs and how they are implementedin MatConvNet and gives the technical details of each computational block inthe toolbox.
arxiv-18000-248 | Patch-based Texture Synthesis for Image Inpainting | http://arxiv.org/pdf/1605.01576v1.pdf | author:Tao Zhou, Brian Johnson, Rui Li category:cs.CV published:2016-05-05 summary:Image inpaiting is an important task in image processing and vision. In thispaper, we develop a general method for patch-based image inpainting bysynthesizing new textures from existing one. A novel framework is introduced tofind several optimal candidate patches and generate a new texture patch in theprocess. We form it as an optimization problem that identifies the potentialpatches for synthesis from an coarse-to-fine manner. We use the texturedescriptor as a clue in searching for matching patches from the known region.To ensure the structure faithful to the original image, a geometric constraintmetric is formally defined that is applied directly to the patch synthesisprocedure. We extensively conducted our experiments on a wide range of testingimages on various scenarios and contents by arbitrarily specifying the targetthe regions for inference followed by using existing evaluation metrics toverify its texture coherency and structural consistency. Our resultsdemonstrate the high accuracy and desirable output that can be potentially usedfor numerous applications: object removal, background subtraction, and imageretrieval.
arxiv-18000-249 | Observational-Interventional Priors for Dose-Response Learning | http://arxiv.org/pdf/1605.01573v1.pdf | author:Ricardo Silva category:stat.ML published:2016-05-05 summary:Controlled interventions provide the most direct source of information forlearning causal effects. In particular, a dose-response curve can be learned byvarying the treatment level and observing the corresponding outcomes. However,interventions can be expensive and time-consuming. Observational data, wherethe treatment is not controlled by a known mechanism, is sometimes available.Under some strong assumptions, observational data allows for the estimation ofdose-response curves. Estimating such curves nonparametrically is hard: samplesizes for controlled interventions may be small, while in the observationalcase a large number of measured confounders may need to be marginalized. Inthis paper, we introduce a hierarchical Gaussian process prior that constructsa distribution over the dose-response curve by learning from observationaldata, and reshapes the distribution with a nonparametric affine transformlearned from controlled interventions. This function composition from differentsources is shown to speed-up learning, which we demonstrate with a thoroughsensitivity analysis and an application to modeling the effect of therapy oncognitive skills of premature infants.
arxiv-18000-250 | Classification of Human Whole-Body Motion using Hidden Markov Models | http://arxiv.org/pdf/1605.01569v1.pdf | author:Matthias Plappert category:cs.LG cs.CV published:2016-05-05 summary:Human motion plays an important role in many fields. Large databases existthat store and make available recordings of human motions. However, annotatingeach motion with multiple labels is a cumbersome and error-prone process. Thisbachelor's thesis presents different approaches to solve the multi-labelclassification problem using Hidden Markov Models (HMMs). First, differentfeatures that can be directly obtained from the raw data are introduced. Next,additional features are derived to improve classification performance. Thesefeatures are then used to perform the multi-label classification using twodifferent approaches. The first approach simply transforms the multi-labelproblem into a multi-class problem. The second, novel approach solves the sameproblem without the need to construct a transformation by predicting the labelsdirectly from the likelihood scores. The second approach scales linearly withthe number of labels whereas the first approach is subject to combinatorialexplosion. All aspects of the classification process are evaluated on a dataset that consists of 454 motions. System 1 achieves an accuracy of 98.02% andsystem 2 an accuracy of 93.39% on the test set.
arxiv-18000-251 | Sampling from strongly log-concave distributions with the Unadjusted Langevin Algorithm | http://arxiv.org/pdf/1605.01559v1.pdf | author:Alain Durmus, Eric Moulines category:math.ST stat.ME stat.ML stat.TH published:2016-05-05 summary:We consider in this paper the problem of sampling a probability distribution$\pi$ having a density with respect to the Lebesgue measure on $\mathbb{R}^d$,known up to a normalisation factor $x \mapsto\mathrm{e}^{-U(x)}/\int_{\mathbb{R}^d} \mathrm{e}^{-U(y)} \mathrm{d} y$. Underthe assumption that $U$ is continuously differentiable, $\nabla U$ is globallyLipshitz and $U$ is strongly convex, we obtain non-asymptotic bounds for theconvergence to stationarity in Wasserstein distances of the sampling methodbased on the Euler discretization of the Langevin stochastic differentialequation for both constant and decreasing step sizes. The dependence on thedimension of the state space of the obtained bounds is studied to demonstratethe applicability of this method in the high dimensional setting. Theconvergence of an appropriately weighted empirical measure is also investigatedand bounds for the mean square error and exponential deviation inequality forLipschitz functions are reported. Some numerical results are presented toillustrate our findings.
arxiv-18000-252 | Hierarchical Modeling of Multidimensional Data in Regularly Decomposed Spaces: Main Principles | http://arxiv.org/pdf/1605.00961v2.pdf | author:Olivier Guye category:cs.CV published:2016-05-03 summary:The described works have been carried out in the framework of a mid-termstudy initiated by the Centre Electronique de l'Armement and led by ADERSA, aFrench company of research under contract. The aim was to study the techniquesof regular dividing of numerical data sets so as to provide tools for problemsolving enabling to model multidimensional numerical objects and to be used incomputer-aided design and manufacturing, in robotics, in image analysis andsynthesis, in pattern recognition, in decision making, in cartography andnumerical data base management. These tools are relying on the principle ofregular hierarchical decomposition and led to the implementation of amultidimensional generalization of quaternary and octernary trees: the trees oforder 2**k or 2**k-trees mapped in binary trees. This first tome, dedicated tothe hierarchical modeling of multidimensional numerical data, describes theprinciples used for building, transforming, analyzing and recognizing patternson which is relying the development of the associated algorithms. The whole sodeveloped algorithms are detailed in pseudo-code at the end of this document.The present publication especially describes: - a building method adapteddisordered and overcrowded data streams ; - its extension in inductive limits ;- the computation of the homographic transformation of a tree ; - the attributecalculus based on generalized moments and the provision of Eigen trees ; -perception procedures of objects without any covering in affine geometry ; -several supervised and unsupervised pattern recognition methods.
arxiv-18000-253 | Fitness-based Adaptive Control of Parameters in Genetic Programming: Adaptive Value Setting of Mutation Rate and Flood Mechanisms | http://arxiv.org/pdf/1605.01514v1.pdf | author:Michal Gregor, Juraj Spalek category:cs.NE published:2016-05-05 summary:This paper concerns applications of genetic algorithms and geneticprogramming to tasks for which it is difficult to find a representation thatdoes not map to a highly complex and discontinuous fitness landscape. In suchcases the standard algorithm is prone to getting trapped in local extremes. Thepaper proposes several adaptive mechanisms that are useful in preventing thesearch from getting trapped.
arxiv-18000-254 | Multilingual Twitter Sentiment Classification: The Role of Human Annotators | http://arxiv.org/pdf/1602.07563v2.pdf | author:Igor Mozetic, Miha Grcar, Jasmina Smailovic category:cs.CL cs.AI published:2016-02-24 summary:What are the limits of automated Twitter sentiment classification? We analyzea large set of manually labeled tweets in different languages, use them astraining data, and construct automated classification models. It turns out thatthe quality of classification models depends much more on the quality and sizeof training data than on the type of the model trained. Experimental resultsindicate that there is no statistically significant difference between theperformance of the top classification models. We quantify the quality oftraining data by applying various annotator agreement measures, and identifythe weakest points of different datasets. We show that the model performanceapproaches the inter-annotator agreement when the size of the training set issufficiently large. However, it is crucial to regularly monitor the self- andinter-annotator agreements since this improves the training datasets andconsequently the model performance. Finally, we show that there is strongevidence that humans perceive the sentiment classes (negative, neutral, andpositive) as ordered.
arxiv-18000-255 | A Large Dataset of Object Scans | http://arxiv.org/pdf/1602.02481v3.pdf | author:Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, Vladlen Koltun category:cs.CV cs.GR published:2016-02-08 summary:We have created a dataset of more than ten thousand 3D scans of real objects.To create the dataset, we recruited 70 operators, equipped them withconsumer-grade mobile 3D scanning setups, and paid them to scan objects intheir environments. The operators scanned objects of their choosing, outsidethe laboratory and without direct supervision by computer vision professionals.The result is a large and diverse collection of object scans: from shoes, mugs,and toys to grand pianos, construction vehicles, and large outdoor sculptures.We worked with an attorney to ensure that data acquisition did not violateprivacy constraints. The acquired data was irrevocably placed in the publicdomain and is available freely at http://redwood-data.org/3dscan .
arxiv-18000-256 | Modeling Rich Contexts for Sentiment Classification with LSTM | http://arxiv.org/pdf/1605.01478v1.pdf | author:Minlie Huang, Yujie Cao, Chao Dong category:cs.CL cs.IR cs.SI published:2016-05-05 summary:Sentiment analysis on social media data such as tweets and weibo has become avery important and challenging task. Due to the intrinsic properties of suchdata, tweets are short, noisy, and of divergent topics, and sentimentclassification on these data requires to modeling various contexts such as theretweet/reply history of a tweet, and the social context about authors andrelationships. While few prior study has approached the issue of modelingcontexts in tweet, this paper proposes to use a hierarchical LSTM to model richcontexts in tweet, particularly long-range context. Experimental results showthat contexts can help us to perform sentiment classification remarkablybetter.
arxiv-18000-257 | Hamiltonian Monte Carlo Acceleration Using Surrogate Functions with Random Bases | http://arxiv.org/pdf/1506.05555v4.pdf | author:Cheng Zhang, Babak Shahbaba, Hongkai Zhao category:stat.CO stat.ML published:2015-06-18 summary:For big data analysis, high computational cost for Bayesian methods oftenlimits their applications in practice. In recent years, there have been manyattempts to improve computational efficiency of Bayesian inference. Here wepropose an efficient and scalable computational technique for astate-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, HamiltonianMonte Carlo (HMC). The key idea is to explore and exploit the structure andregularity in parameter space for the underlying probabilistic model toconstruct an effective approximation of its geometric properties. To this end,we build a surrogate function to approximate the target distribution usingproperly chosen random bases and an efficient optimization process. Theresulting method provides a flexible, scalable, and efficient samplingalgorithm, which converges to the correct target distribution. We show that bychoosing the basis functions and optimization process differently, our methodcan be related to other approaches for the construction of surrogate functionssuch as generalized additive models or Gaussian process models. Experimentsbased on simulated and real data show that our approach leads to substantiallymore efficient sampling algorithms compared to existing state-of-the artmethods.
arxiv-18000-258 | Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization | http://arxiv.org/pdf/1604.03257v2.pdf | author:Tianbao Yang, Qihang Lin, Zhe Li category:math.OC stat.ML published:2016-04-12 summary:Recently, {\it stochastic momentum} methods have been widely adopted intraining deep neural networks. However, their convergence analysis is stillunderexplored at the moment, in particular for non-convex optimization. Thispaper fills the gap between practice and theory by developing a basicconvergence analysis of two stochastic momentum methods, namely stochasticheavy-ball method and the stochastic variant of Nesterov's accelerated gradientmethod. We hope that the basic convergence results developed in this paper canserve the reference to the convergence of stochastic momentum methods and alsoserve the baselines for comparison in future development of stochastic momentummethods. The novelty of convergence analysis presented in this paper is aunified framework, revealing more insights about the similarities anddifferences between different stochastic momentum methods and stochasticgradient method. The unified framework exhibits a continuous change from thegradient method to Nesterov's accelerated gradient method and finally theheavy-ball method incurred by a free parameter, which can help explain asimilar change observed in the testing error convergence behavior for deeplearning. Furthermore, our empirical results for optimizing deep neuralnetworks demonstrate that the stochastic variant of Nesterov's acceleratedgradient method achieves a good tradeoff (between speed of convergence intraining error and robustness of convergence in testing error) among the threestochastic methods.
arxiv-18000-259 | Boltzmann meets Nash: Energy-efficient routing in optical networks under uncertainty | http://arxiv.org/pdf/1605.01451v1.pdf | author:Panayotis Mertikopoulos, Aris L. Moustakas, Anna Tzanakaki category:cs.NI cs.GT cs.LG published:2016-05-04 summary:Motivated by the massive deployment of power-hungry data centers for serviceprovisioning, we examine the problem of routing in optical networks with theaim of minimizing traffic-driven power consumption. To tackle this issue,routing must take into account energy efficiency as well as capacityconsiderations; moreover, in rapidly-varying network environments, this must beaccomplished in a real-time, distributed manner that remains robust in thepresence of random disturbances and noise. In view of this, we derive a pricingscheme whose Nash equilibria coincide with the network's socially optimumstates, and we propose a distributed learning method based on the Boltzmanndistribution of statistical mechanics. Using tools from stochastic calculus, weshow that the resulting Boltzmann routing scheme exhibits remarkableconvergence properties under uncertainty: specifically, the long-term averageof the network's power consumption converges within $\varepsilon$ of itsminimum value in time which is at most $\tilde O(1/\varepsilon^2)$,irrespective of the fluctuations' magnitude; additionally, if the networkadmits a strict, non-mixing optimum state, the algorithm converges to it -again, no matter the noise level. Our analysis is supplemented by extensivenumerical simulations which show that Boltzmann routing can lead to asignificant decrease in power consumption over basic, shortest-path routingschemes in realistic network conditions.
arxiv-18000-260 | Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians | http://arxiv.org/pdf/1507.05699v5.pdf | author:Peiyun Hu, Deva Ramanan category:cs.CV published:2015-07-21 summary:Convolutional neural nets (CNNs) have demonstrated remarkable performance inrecent history. Such approaches tend to work in a unidirectional bottom-upfeed-forward fashion. However, practical experience and biological evidencetells us that feedback plays a crucial role, particularly for detailed spatialunderstanding tasks. This work explores bidirectional architectures that alsoreason with top-down feedback: neural units are influenced by both lower andhigher-level units. We do so by treating units as rectified latent variables in a quadraticenergy function, which can be seen as a hierarchical Rectified Gaussian model(RGs). We show that RGs can be optimized with a quadratic program (QP), thatcan in turn be optimized with a recurrent neural network (with rectified linearunits). This allows RGs to be trained with GPU-optimized gradient descent. Froma theoretical perspective, RGs help establish a connection between CNNs andhierarchical probabilistic models. From a practical perspective, RGs are wellsuited for detailed spatial tasks that can benefit from top-down reasoning. Weillustrate them on the challenging task of keypoint localization underocclusions, where local bottom-up evidence may be misleading. We demonstratestate-of-the-art results on challenging benchmarks.
arxiv-18000-261 | Sampling Requirements for Stable Autoregressive Estimation | http://arxiv.org/pdf/1605.01436v1.pdf | author:Abbas Kazemipour, Sina Miran, Piya Pal, Behtash Babadi, Min Wu category:cs.IT cs.DM math.IT math.OC stat.ME stat.ML published:2016-05-04 summary:We consider the problem of estimating the parameters of a linearautoregressive model with sub-Gaussian innovations from a limited sequence ofconsecutive observations. Assuming that the parameters are compressible, weanalyze the performance of the $\ell_1$-regularized least squares as well as agreedy estimator of the parameters and characterize the sampling trade-offsrequired for stable recovery in the non-asymptotic regime. Our results extendthose of compressed sensing for linear models where the covariates are i.i.d.and independent of the observation history to autoregressive processes withhighly inter-dependent covariates. We also derive sufficient conditions on thesparsity level that guarantee the minimax optimality of the$\ell_1$-regularized least squares estimate. Applying these techniques tosimulated data as well as real-world datasets from crude oil prices and trafficspeed data confirm our predicted theoretical performance gains in terms ofestimation accuracy and model selection.
arxiv-18000-262 | Skin Lesion Analysis toward Melanoma Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC) | http://arxiv.org/pdf/1605.01397v1.pdf | author:David Gutman, Noel C. F. Codella, Emre Celebi, Brian Helba, Michael Marchetti, Nabin Mishra, Allan Halpern category:cs.CV published:2016-05-04 summary:In this article, we describe the design and implementation of a publiclyaccessible dermatology image analysis benchmark challenge. The goal of thechallenge is to sup- port research and development of algorithms for automateddiagnosis of melanoma, a lethal form of skin cancer, from dermoscopic images.The challenge was divided into sub-challenges for each task involved in imageanalysis, including lesion segmentation, dermoscopic feature detection within alesion, and classification of melanoma. Training data included 900 images. Aseparate test dataset of 379 images was provided to measure resultantperformance of systems developed with the training data. Ground truth for bothtraining and test sets was generated by a panel of dermoscopic experts. Intotal, there were 79 submissions from a group of 38 participants, making thisthe largest standardized and comparative study for melanoma diagnosis indermoscopic images to date. While the official challenge duration and rankingof participants has concluded, the datasets remain available for furtherresearch and development.
arxiv-18000-263 | The embedding dimension of Laplacian eigenfunction maps | http://arxiv.org/pdf/1605.01643v1.pdf | author:Jonathan Bates category:stat.ML cs.CV math.DG published:2016-05-04 summary:Any closed, connected Riemannian manifold $M$ can be smoothly embedded by itsLaplacian eigenfunction maps into $\mathbb{R}^m$ for some $m$. We call thesmallest such $m$ the maximal embedding dimension of $M$. We show that themaximal embedding dimension of $M$ is bounded from above by a constantdepending only on the dimension of $M$, a lower bound for injectivity radius, alower bound for Ricci curvature, and a volume bound. We interpret this resultfor the case of surfaces isometrically immersed in $\mathbb{R}^3$, showing thatthe maximal embedding dimension only depends on bounds for the Gaussiancurvature, mean curvature, and surface area. Furthermore, we consider therelevance of these results for shape registration.
arxiv-18000-264 | Multi Level Monte Carlo methods for a class of ergodic stochastic differential equations | http://arxiv.org/pdf/1605.01384v1.pdf | author:Lukasz Szpruch, Sebastian Vollmer, Konstantinos Zygalakis, Michael B. Giles category:math.NA stat.ME stat.ML published:2016-05-04 summary:We develop a framework that allows the use of the multi-level Monte Carlo(MLMC) methodology (Giles 2015) to calculate expectations with respect to theinvariant measures of ergodic SDEs. In that context, we study the (over-damped)Langevin equations with strongly convex potential. We show that, whenappropriate contracting couplings for the numerical integrators are available,one can obtain a time-uniform estimates of the MLMC variance in stark contrastto the majority of the results in the MLMC literature. As a consequence, onecan approximate expectations with respect to the invariant measure in anunbiased way without the need of a Metropolis- Hastings step. In addition, aroot mean square error of $\mathcal{O}(\epsilon)$ is achieved with$\mathcal{O}(\epsilon^{-2})$ complexity on par with Markov Chain Monte Carlo(MCMC) methods, which however can be computationally intensive when applied tolarge data sets. Finally, we present a multilevel version of the recentlyintroduced Stochastic Gradient Langevin (SGLD) method (Welling and Teh, 2011)built for large datasets applications. We show that this is the firststochastic gradient MCMC method with complexity $\mathcal{O}(\epsilon^{-2}\log{\epsilon}^{3})$, which is asymptotically an order $\epsilon$ lower than the $\mathcal{O}(\epsilon^{-3})$ complexity of all stochastic gradient MCMC methodsthat are currently available. Numerical experiments confirm our theoreticalfindings.
arxiv-18000-265 | Leveraging Visual Question Answering for Image-Caption Ranking | http://arxiv.org/pdf/1605.01379v1.pdf | author:Xiao Lin, Devi Parikh category:cs.CV published:2016-05-04 summary:Visual Question Answering (VQA) is the task of taking as input an image and afree-form natural language question about the image, and producing an accurateanswer. In this work we view VQA as a "feature extraction" module to extractimage and caption representations. We employ these representations for the taskof image-caption ranking. Each feature dimension captures (imagines) whether afact (question-answer pair) could plausibly be true for the image and caption.This allows the model to interpret images and captions from a wide variety ofperspectives. We propose score-level and representation-level fusion models toincorporate VQA knowledge in an existing state-of-the-art VQA-agnosticimage-caption ranking model. We find that incorporating and reasoning aboutconsistency between images and captions significantly improves performance.Concretely, our model improves state-of-the-art on caption retrieval by 7.1%and on image retrieval by 4.4% on the MSCOCO dataset.
arxiv-18000-266 | Adaptive Penalized Estimation of Directed Acyclic Graphs From Categorical Data | http://arxiv.org/pdf/1403.2310v3.pdf | author:Jiaying Gu, Fei Fu, Qing Zhou category:stat.ME stat.ML published:2014-03-10 summary:We develop in this article a penalized likelihood method to estimate sparseBayesian networks from categorical data. The structure of a Bayesian network isrepresented by a directed acyclic graph (DAG). We model the conditionaldistribution of a node given its parents by multi-logit regression and estimatethe structure of a DAG via maximizing a regularized likelihood. The adaptivegroup Lasso penalty is employed to encourage sparsity by selecting groupeddummy variables encoding the level of a factor. We develop a blockwisecoordinate descent algorithm to solve the penalized likelihood problem subjectto the acyclicity constraint of a DAG. When intervention data are available,our method may construct a causal network, in which a directed edge representsa causal relation. We apply our method to various simulated networks and a realbiological network. The results show that our method is very competitive,compared to other existing methods, in DAG estimation from both interventionaland high-dimensional observational data. We also establish consistency inparameter and structure estimation for our method when the number of nodes isfixed.
arxiv-18000-267 | Accelerating Deep Learning with Shrinkage and Recall | http://arxiv.org/pdf/1605.01369v1.pdf | author:Shuai Zheng, Abhinav Vishnu, Chris Ding category:cs.LG cs.CV cs.NE published:2016-05-04 summary:Deep Learning is a very powerful machine learning model. Deep Learning trainsa large number of parameters for multiple layers and is very slow when data isin large scale and the architecture size is large. Inspired from the shrinkingtechnique used in accelerating computation of Support Vector Machines (SVM)algorithm and screening technique used in LASSO, we propose a shrinking DeepLearning with recall (sDLr) approach to speed up deep learning computation. Weexperiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 datasets. Results show that the speedup using shrinking Deep Learning with recall(sDLr) can reach more than 2.0 while still giving competitive classificationperformance.
arxiv-18000-268 | Randomer Forests | http://arxiv.org/pdf/1506.03410v2.pdf | author:Tyler M. Tomita, Mauro Maggioni, Joshua T. Vogelstein category:stat.ML cs.LG 68T10 I.5.2 published:2015-06-10 summary:Random forests (RF) is a popular general purpose classifier that has beenshown to outperform many other classifiers on a variety of datasets. Thewidespread use of random forests can be attributed to several factors, some ofwhich include its excellent empirical performance, scale and unit invariance,robustness to outliers, time and space complexity, and interpretability. WhileRF has many desirable qualities, one drawback is its sensitivity to rotationsand other operations that "mix" variables. In this work, we establish ageneralized forest building scheme, linear threshold forests. Random forestsand many other currently existing decision forest algorithms can be viewed asspecial cases of this scheme. With this scheme in mind, we propose a fewspecial cases which we call randomer forests (RerFs). RerFs are linearthreshold forest that exhibit all of the nice properties of RF, in addition toapproximate affine invariance. In simulated datasets designed for RF to dowell, we demonstrate that RerF outperforms RF. We also demonstrate that oneparticular variant of RerF is approximately affine invariant. Lastly, in anevaluation on 121 benchmark datasets, we observe that RerF outperforms RF. Wetherefore putatively propose that RerF be considered a replacement for RF asthe general purpose classifier of choice. Open source code is available athttp://ttomita.github.io/RandomerForest/.
arxiv-18000-269 | The Hidden Convexity of Spectral Clustering | http://arxiv.org/pdf/1403.0667v3.pdf | author:James Voss, Mikhail Belkin, Luis Rademacher category:cs.LG stat.ML published:2014-03-04 summary:In recent years, spectral clustering has become a standard method for dataanalysis used in a broad range of applications. In this paper we propose a newclass of algorithms for multiway spectral clustering based on optimization of acertain "contrast function" over the unit sphere. These algorithms, partlyinspired by certain Independent Component Analysis techniques, are simple, easyto implement and efficient. Geometrically, the proposed algorithms can be interpreted as hidden basisrecovery by means of function optimization. We give a complete characterizationof the contrast functions admissible for provable basis recovery. We show howthese conditions can be interpreted as a "hidden convexity" of our optimizationproblem on the sphere; interestingly, we use efficient convex maximizationrather than the more common convex minimization. We also show encouragingexperimental results on real and simulated data.
arxiv-18000-270 | Learning from the memory of Atari 2600 | http://arxiv.org/pdf/1605.01335v1.pdf | author:Jakub Sygnowski, Henryk Michalewski category:cs.LG cs.AI published:2016-05-04 summary:We train a number of neural networks to play games Bowling, Breakout andSeaquest using information stored in the memory of a video game console Atari2600. We consider four models of neural networks which differ in size andarchitecture: two networks which use only information contained in the RAM andtwo mixed networks which use both information in the RAM and information fromthe screen. As the benchmark we used the convolutional model proposed in NIPSand received comparable results in all considered games. Quite surprisingly, inthe case of Seaquest we were able to train RAM-only agents which behave betterthan the benchmark screen-only agent. Mixing screen and RAM did not lead to animproved performance comparing to screen-only and RAM-only agents.
arxiv-18000-271 | Single Channel Speech Enhancement Using Outlier Detection | http://arxiv.org/pdf/1605.01329v1.pdf | author:Eunjoon Cho, Bowon Lee, Ronald Schafer, Bernard Widrow category:cs.SD cs.LG published:2016-05-04 summary:Distortion of the underlying speech is a common problem for single-channelspeech enhancement algorithms, and hinders such methods from being used moreextensively. A dictionary based speech enhancement method that emphasizespreserving the underlying speech is proposed. Spectral patches of clean speechare sampled and clustered to train a dictionary. Given a noisy speech spectralpatch, the best matching dictionary entry is selected and used to estimate thenoise power at each time-frequency bin. The noise estimation step is formulatedas an outlier detection problem, where the noise at each bin is assumed presentonly if it is an outlier to the corresponding bin of the best matchingdictionary entry. This framework assigns higher priority in removing spectralelements that strongly deviate from a typical spoken unit stored in the traineddictionary. Even without the aid of a separate noise model, this method canachieve significant noise reduction for various non-stationary noises, whileeffectively preserving the underlying speech in more challenging noisyenvironments.
arxiv-18000-272 | Compression and the origins of Zipf's law for word frequencies | http://arxiv.org/pdf/1605.01326v1.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL physics.soc-ph q-bio.NC published:2016-05-04 summary:Here we sketch a new derivation of Zipf's law for word frequencies based onoptimal coding. The structure of the derivation is reminiscent of Mandelbrot'srandom typing model but it has multiple advantages over random typing: (1) itdeparts from realistic cognitive pressures (2) it does not require fine tuningof parameters and (3) it sheds light on the origins of other statistical lawsof language and thus can lead to a compact theory of linguistic laws. Ourfindings suggest that the recurrence of Zipf's law in human languages couldoriginate from pressure for easy and fast communication.
arxiv-18000-273 | Compression and the origins of Zipf's law of abbreviation | http://arxiv.org/pdf/1504.04884v3.pdf | author:R. Ferrer-i-Cancho, C. Bentz, C. Seguin category:cs.IT cs.CL cs.SI math.IT published:2015-04-19 summary:Languages across the world exhibit Zipf's law of abbreviation, namely morefrequent words tend to be shorter. The generalized version of the law - aninverse relationship between the frequency of a unit and its magnitude - holdsalso for the behaviours of other species and the genetic code. The apparentuniversality of this pattern in human language and its ubiquity in otherdomains calls for a theoretical understanding of its origins. To this end, wegeneralize the information theoretic concept of mean code length as a meanenergetic cost function over the probability and the magnitude of the types ofthe repertoire. We show that the minimization of that cost function and anegative correlation between probability and the magnitude of types areintimately related.
arxiv-18000-274 | From exp-concavity to variance control: High probability O(1/n) rates and high probability online-to-batch conversion | http://arxiv.org/pdf/1605.01288v1.pdf | author:Nishant A. Mehta category:cs.LG published:2016-05-04 summary:We present an algorithm for the statistical learning setting with a boundedexp-concave loss in $d$ dimensions that obtains excess risk $O(d / n)$ withhigh probability: the dependence on the confidence parameter $\delta$ ispolylogarithmic in $1/\delta$. The core technique is to boost the confidence ofrecent $O(d / n)$ bounds, without sacrificing the rate, by leveraging aBernstein-type condition which holds due to exp-concavity. This Bernstein-typecondition implies that the variance of excess loss random variables arecontrolled in terms of their excess risk. Using this variance control, wefurther show that a regret bound for any online learner in this settingtranslates to a high probability excess risk bound for the correspondingonline-to-batch conversion of the online learner.
arxiv-18000-275 | Video (language) modeling: a baseline for generative models of natural videos | http://arxiv.org/pdf/1412.6604v5.pdf | author:MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, Sumit Chopra category:cs.LG cs.CV published:2014-12-20 summary:We propose a strong baseline model for unsupervised feature learning usingvideo data. By learning to predict missing frames or extrapolate future framesfrom an input video sequence, the model discovers both spatial and temporalcorrelations which are useful to represent complex deformations and motionpatterns. The models we propose are largely borrowed from the language modelingliterature, and adapted to the vision domain by quantizing the space of imagepatches into a large dictionary. We demonstrate the approach on both a fillingand a generation task. For the first time, we show that, after training onnatural videos, such a model can predict non-trivial motions over short videosequences.
arxiv-18000-276 | A Bayesian Approach to Policy Recognition and State Representation Learning | http://arxiv.org/pdf/1605.01278v1.pdf | author:Adrian Šošić, Abdelhak M. Zoubir, Heinz Koeppl category:stat.ML cs.LG cs.SY math.DS math.PR published:2016-05-04 summary:Learning from demonstration (LfD) is the process of building behavioralmodels of a task from demonstrations provided by an expert. These models can beused e.g. for system control by generalizing the expert demonstrations topreviously unencountered situations. Most LfD methods, however, make strongassumptions about the expert behavior, e.g. they assume the existence of adeterministic optimal ground truth policy or require direct monitoring of theexpert's controls, which limits their practical use as part of a general systemidentification framework. In this work, we consider the LfD problem in a moregeneral setting where we allow for arbitrary stochastic expert policies,without reasoning about the quality of the demonstrations. In particular, wefocus on the problem of policy recognition, which is to extract a system'slatent control policy from observed system behavior. Following a Bayesianmethodology allows us to consider various sources of uncertainty about theexpert behavior, including the latent expert controls, to model the fullposterior distribution of expert controllers. Further, we show that the samemethodology can be applied in a nonparametric context to reason about thecomplexity of the state representation used by the expert and to learntask-appropriate partitionings of the system state space.
arxiv-18000-277 | Hierarchical Modeling of Multidimensional Data in Regularly Decomposed Spaces: Applications in Image Analysis | http://arxiv.org/pdf/1605.01242v1.pdf | author:Olivier Guye category:cs.CV published:2016-05-04 summary:This last document is showing the gradual introduction of hierarchicalmodeling techniques in image analysis. The first chapter is dealing with thefirst works carried out in the field of industrial applications of patternrecognition. The second chapter is focusing on the usage of these techniques insatellite imagery and on the development of a satellite data archiving systemin the aim of using it in digital geography. The third chapter is about facerecognition based on planar image analysis and about the recognition ofpartially hidden patterns. The present publication is ending with thedescription of a future system of self-descriptive coding of still or movingpictures in relation with the current video coding standards. As in theprevious documents, it will be found in annex algorithms targeted on imageanalysis according two complementary approaches: - boundary-based approach forthe industrial applications of artificial vision; - region-based approach forsatellite image analysis.
arxiv-18000-278 | Classical Statistics and Statistical Learning in Imaging Neuroscience | http://arxiv.org/pdf/1603.01857v2.pdf | author:Danilo Bzdok category:stat.ML q-bio.NC published:2016-03-06 summary:Neuroimaging research has predominantly drawn conclusions based on classicalstatistics, including null-hypothesis testing, t-tests, and ANOVA. Throughoutrecent years, statistical learning methods enjoy increasing popularity,including cross-validation, pattern classification, and sparsity-inducingregression. These two methodological families used for neuroimaging dataanalysis can be viewed as two extremes of a continuum. Yet, they originatedfrom different historical contexts, build on different theories, rest ondifferent assumptions, evaluate different outcome metrics, and permit differentconclusions. This paper portrays commonalities and differences betweenclassical statistics and statistical learning with their relation toneuroimaging research. The conceptual implications are illustrated in threecommon analysis scenarios. It is thus tried to resolve possible confusionbetween classical hypothesis testing and data-guided model estimation bydiscussing their ramifications for the neuroimaging access to neurobiology.
arxiv-18000-279 | Learning Covariant Feature Detectors | http://arxiv.org/pdf/1605.01224v1.pdf | author:Karel Lenc, Andrea Vedaldi category:cs.CV published:2016-05-04 summary:Local covariant feature detection, namely the problem of extracting viewpointinvariant features from images, has so far largely resisted the application ofmachine learning techniques. In this paper, we propose the first fully generalformulation for learning local covariant feature detectors. We propose to castdetection as a regression problem, enabling the use of powerful regressors suchas deep neural networks. We then derive a covariance constraint that can beused to automatically learn which visual structures provide stable anchors forlocal feature detection. We support these ideas theoretically, proposing anovel analysis of local features in term of geometric transformations, and weshow that all common and many uncommon detectors can be derived in thisframework. Finally, we present empirical results on a variety of detector typesand on standard feature benchmarks, showing the power and flexibility of theframework.
arxiv-18000-280 | IISCNLP at SemEval-2016 Task 2: Interpretable STS with ILP based Multiple Chunk Aligner | http://arxiv.org/pdf/1605.01194v1.pdf | author:Lavanya Sita Tekumalla, Sharmistha category:cs.CL stat.ML published:2016-05-04 summary:Interpretable semantic textual similarity (iSTS) task adds a crucialexplanatory layer to pairwise sentence similarity. We address variouscomponents of this task: chunk level semantic alignment along with assignmentof similarity type and score for aligned chunks with a novel system presentedin this paper. We propose an algorithm, iMATCH, for the alignment of multiplenon-contiguous chunks based on Integer Linear Programming (ILP). Similaritytype and score assignment for pairs of chunks is done using a supervisedmulticlass classification technique based on Random Forrest Classifier. Resultsshow that our algorithm iMATCH has low execution time and outperforms mostother participating systems in terms of alignment score. Of the three datasets,we are top ranked for answer- students dataset in terms of overall score andhave top alignment score for headlines dataset in the gold chunks track.
arxiv-18000-281 | A Generic Method for Automatic Ground Truth Generation of Camera-captured Documents | http://arxiv.org/pdf/1605.01189v1.pdf | author:Sheraz Ahmed, Muhammad Imran Malik, Muhammad Zeshan Afzal, Koichi Kise, Masakazu Iwamura, Andreas Dengel, Marcus Liwicki category:cs.CV published:2016-05-04 summary:The contribution of this paper is fourfold. The first contribution is anovel, generic method for automatic ground truth generation of camera-captureddocument images (books, magazines, articles, invoices, etc.). It enables us tobuild large-scale (i.e., millions of images) labeled camera-captured/scanneddocuments datasets, without any human intervention. The method is generic,language independent and can be used for generation of labeled documentsdatasets (both scanned and cameracaptured) in any cursive and non-cursivelanguage, e.g., English, Russian, Arabic, Urdu, etc. To assess theeffectiveness of the presented method, two different datasets in English andRussian are generated using the presented method. Evaluation of samples fromthe two datasets shows that 99:98% of the images were correctly labeled. Thesecond contribution is a large dataset (called C3Wi) of camera-capturedcharacters and words images, comprising 1 million word images (10 millioncharacter images), captured in a real camera-based acquisition. This datasetcan be used for training as well as testing of character recognition systems oncamera-captured documents. The third contribution is a novel method for therecognition of cameracaptured document images. The proposed method is based onLong Short-Term Memory and outperforms the state-of-the-art methods for camerabased OCRs. As a fourth contribution, various benchmark tests are performed touncover the behavior of commercial (ABBYY), open source (Tesseract), and thepresented camera-based OCR using the presented C3Wi dataset. Evaluation resultsreveal that the existing OCRs, which already get very high accuracies onscanned documents, have limited performance on camera-captured document images;where ABBYY has an accuracy of 75%, Tesseract an accuracy of 50.22%, while thepresented character recognition system has an accuracy of 95.10%.
arxiv-18000-282 | Linear Bandit algorithms using the Bootstrap | http://arxiv.org/pdf/1605.01185v1.pdf | author:Nandan Sudarsanam, Balaraman Ravindran category:stat.ML cs.LG published:2016-05-04 summary:This study presents two new algorithms for solving linear stochastic banditproblems. The proposed methods use an approach from non-parametric statisticscalled bootstrapping to create confidence bounds. This is achieved withoutmaking any assumptions about the distribution of noise in the underlyingsystem. We present the X-Random and X-Fixed bootstrap bandits which correspondto the two well-known approaches for conducting bootstraps on models, in theliterature. The proposed methods are compared to other popular solutions forlinear stochastic bandit problems, namely, OFUL, LinUCB and Thompson Sampling.The comparisons are carried out using a simulation study on a hierarchicalprobability meta-model, built from published data of experiments, which are runon real systems. The model representing the response surfaces is conceptualizedas a Bayesian Network which is presented with varying degrees of noise for thesimulations. One of the proposed methods, X-Random bootstrap, performs betterthan the baselines in-terms of cumulative regret across various degrees ofnoise and different number of trials. In certain settings the cumulative regretof this method is less than half of the best baseline. The X-Fixed bootstrapperforms comparably in most situations and particularly well when the number oftrials is low. The study concludes that these algorithms could be a preferredalternative for solving linear bandit problems, especially when thedistribution of the noise in the system is unknown.
arxiv-18000-283 | A metric on the space of finite sets of trajectories for evaluation of multi-target tracking algorithms | http://arxiv.org/pdf/1605.01177v1.pdf | author:Abu Sajana Rahmathullah, Ángel F. García-Fernández, Lennart Svensson category:cs.CV cs.SY published:2016-05-04 summary:In this paper, we propose a metric on the space of finite sets oftrajectories for assessing multi-target tracking algorithms in a mathematicallysound way. The metric can be used, e.g., to compare estimates from algorithmswith the ground truth. It includes intuitive costs associated to localization,missed and false targets and track switches. The metric computation is based onmulti-dimensional assignments, which is an NP hard problem. Therefore, we alsopropose a lower bound for the metric, which is also a metric for sets oftrajectories and is computable in polynomial time using linear programming(LP). The LP metric can be implemented using alternating direction method ofmultipliers such that the complexity scales linearly with the length of thetrajectories.
arxiv-18000-284 | Towards Better Analysis of Deep Convolutional Neural Networks | http://arxiv.org/pdf/1604.07043v3.pdf | author:Mengchen Liu, Jiaxin Shi, Zhen Li, Chongxuan Li, Jun Zhu, Shixia Liu category:cs.CV published:2016-04-24 summary:Deep convolutional neural networks (CNNs) have achieved breakthroughperformance in many pattern recognition tasks such as image classification.However, the development of high-quality deep models typically relies on asubstantial amount of trial-and-error, as there is still no clear understandingof when and why a deep model works. In this paper, we present a visualanalytics approach for better understanding, diagnosing, and refining deepCNNs. We formulate a deep CNN as a directed acyclic graph. Based on thisformulation, a hybrid visualization is developed to disclose the multiplefacets of each neuron and the interactions between them. In particular, weintroduce a hierarchical rectangle packing algorithm and a matrix reorderingalgorithm to show the derived features of a neuron cluster. We also propose abiclustering-based edge bundling method to reduce visual clutter caused by alarge number of connections between neurons. We evaluated our method on a setof CNNs and the results are generally favorable.
arxiv-18000-285 | Application of Deep Convolutional Neural Networks for Detecting Extreme Weather in Climate Datasets | http://arxiv.org/pdf/1605.01156v1.pdf | author:Yunjie Liu, Evan Racah, Prabhat, Joaquin Correa, Amir Khosrowshahi, David Lavers, Kenneth Kunkel, Michael Wehner, William Collins category:cs.CV published:2016-05-04 summary:Detecting extreme events in large datasets is a major challenge in climatescience research. Current algorithms for extreme event detection are build uponhuman expertise in defining events based on subjective thresholds of relevantphysical variables. Often, multiple competing methods produce vastly differentresults on the same dataset. Accurate characterization of extreme events inclimate simulations and observational data archives is critical forunderstanding the trends and potential impacts of such events in a climatechange content. This study presents the first application of Deep Learningtechniques as alternative methodology for climate extreme events detection.Deep neural networks are able to learn high-level representations of a broadclass of patterns from labeled data. In this work, we developed deepConvolutional Neural Network (CNN) classification system and demonstrated theusefulness of Deep Learning technique for tackling climate pattern detectionproblems. Coupled with Bayesian based hyper-parameter optimization scheme, ourdeep CNN system achieves 89\%-99\% of accuracy in detecting extreme events(Tropical Cyclones, Atmospheric Rivers and Weather Fronts
arxiv-18000-286 | Ethnicity sensitive author disambiguation using semi-supervised learning | http://arxiv.org/pdf/1508.07744v2.pdf | author:Gilles Louppe, Hussein Al-Natsheh, Mateusz Susik, Eamonn Maguire category:cs.DL cs.IR stat.ML published:2015-08-31 summary:Author name disambiguation in bibliographic databases is the problem ofgrouping together scientific publications written by the same person,accounting for potential homonyms and/or synonyms. Among solutions to thisproblem, digital libraries are increasingly offering tools for authors tomanually curate their publications and claim those that are theirs. Indirectly,these tools allow for the inexpensive collection of large annotated trainingdata, which can be further leveraged to build a complementary automateddisambiguation system capable of inferring patterns for identifyingpublications written by the same person. Building on more than 1 millionpublicly released crowdsourced annotations, we propose an automated authordisambiguation solution exploiting this data (i) to learn an accurateclassifier for identifying coreferring authors and (ii) to guide the clusteringof scientific publications by distinct authors in a semi-supervised way. To thebest of our knowledge, our analysis is the first to be carried out on data ofthis size and coverage. With respect to the state of the art, we validate thegeneral pipeline used in most existing solutions, and improve by: (i) proposingphonetic-based blocking strategies, thereby increasing recall; and (ii) addingstrong ethnicity-sensitive features for learning a linkage function, therebytailoring disambiguation to non-Western author names whenever necessary.
arxiv-18000-287 | Learning from Binary Labels with Instance-Dependent Corruption | http://arxiv.org/pdf/1605.00751v2.pdf | author:Aditya Krishna Menon, Brendan van Rooyen, Nagarajan Natarajan category:cs.LG published:2016-05-03 summary:Suppose we have a sample of instances paired with binary labels corrupted byarbitrary instance- and label-dependent noise. With sufficiently many suchsamples, can we optimally classify and rank instances with respect to thenoise-free distribution? We provide a theoretical analysis of this question,with three main contributions. First, we prove that for instance-dependentnoise, any algorithm that is consistent for classification on the noisydistribution is also consistent on the clean distribution. Second, we provethat for a broad class of instance- and label-dependent noise, a similarconsistency result holds for the area under the ROC curve. Third, for thelatter noise model, when the noise-free class-probability function belongs tothe generalised linear model family, we show that the Isotron can efficientlyand provably learn from the corrupted sample.
arxiv-18000-288 | A Comparative Evaluation of Approximate Probabilistic Simulation and Deep Neural Networks as Accounts of Human Physical Scene Understanding | http://arxiv.org/pdf/1605.01138v1.pdf | author:Renqiao Zhang, Jiajun Wu, Chengkai Zhang, William T. Freeman, Joshua B. Tenenbaum category:cs.AI cs.CV q-bio.NC published:2016-05-04 summary:Humans demonstrate remarkable abilities to predict physical events in complexscenes. Two classes of models for physical scene understanding have recentlybeen proposed: "Intuitive Physics Engines", or IPEs, which posit that peoplemake predictions by running approximate probabilistic simulations in causalmental models similar in nature to video-game physics engines, and memory-basedmodels, which make judgments based on analogies to stored experiences ofpreviously encountered scenes and physical outcomes. Versions of the latterhave recently been instantiated in convolutional neural network (CNN)architectures. Here we report four experiments that, to our knowledge, are thefirst rigorous comparisons of simulation-based and CNN-based models, where bothapproaches are concretely instantiated in algorithms that can run on raw imageinputs and produce as outputs physical judgments such as whether a stack ofblocks will fall. Both approaches can achieve super-human accuracy levels andcan quantitatively predict human judgments to a similar degree, but only thesimulation-based models generalize to novel situations in ways that people do,and are qualitatively consistent with systematic perceptual illusions andjudgment asymmetries that people show.
arxiv-18000-289 | RSG: Beating SG without Smoothness and/or Strong Convexity | http://arxiv.org/pdf/1512.03107v9.pdf | author:Tianbao Yang, Qihang Lin category:math.OC stat.ML published:2015-12-09 summary:In this paper, we propose novel deterministic and stochastic {\bf R}estarted{\bf S}ub{\bf G}radient (RSG) methods that can find an $\epsilon$-optimalsolution for a broad class of non-smooth and/or non-strongly convexoptimization problems faster than the vanilla deterministic or stochasticsubgradient method (SG). We show that for non-smooth and non-strongly convexoptimization, RSG can reduce the dependence of SG's iteration complexity on thedistance to the optimal set of the initial solution to that of points on the$\epsilon$-level set. For a special family of non-smooth and non-stronglyconvex optimization problems whose epigraph is a polyhedron, we further showthat RSG could converge linearly. In addition, RSG has an$O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity forproblems with a much weaker notion of strong convexity, namely locallysemi-strongly convexity. For a family of non-smooth optimization problems thatadmit a local Kurdyka-\L ojasiewicz property with a power constant of$\beta\in(0,1)$, RSG has an$O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity,which is better than that of SG for such optimization problems whose iterationcomplexity is $O(\frac{1}{\epsilon^2})$. The novelty of our analysis lies atexploiting the lower bound of the subgradient of the objective function at the$\epsilon$-level set. It is this novelty that allows us to explore the localproperties of functions (e.g., local semi-strong convexity, local Kurdyka-\Lojasiewicz property, more generally local error bounds) to develop improvedconvergence of RSG.
arxiv-18000-290 | Deep Motif: Visualizing Genomic Sequence Classifications | http://arxiv.org/pdf/1605.01133v1.pdf | author:Jack Lanchantin, Ritambhara Singh, Zeming Lin, Yanjun Qi category:cs.LG published:2016-05-04 summary:This paper applies a deep convolutional/highway MLP framework to classifygenomic sequences on the transcription factor binding site task. To make themodel understandable, we propose an optimization driven strategy to extract"motifs", or symbolic patterns which visualize the positive class learned bythe network. We show that our system, Deep Motif (DeMo), extracts motifs thatare similar to, and in some cases outperform the current well known motifs. Inaddition, we find that a deeper model consisting of multiple convolutional andhighway layers can outperform a single convolutional and fully connected layerin the previous state-of-the-art.
arxiv-18000-291 | Mining Discriminative Triplets of Patches for Fine-Grained Classification | http://arxiv.org/pdf/1605.01130v1.pdf | author:Yaming Wang, Jonghyun Choi, Vlad I. Morariu, Larry S. Davis category:cs.CV published:2016-05-04 summary:Fine-grained classification involves distinguishing between similarsub-categories based on subtle differences in highly localized regions;therefore, accurate localization of discriminative regions remains a majorchallenge. We describe a patch-based framework to address this problem. Weintroduce triplets of patches with geometric constraints to improve theaccuracy of patch localization, and automatically mine discriminativegeometrically-constrained triplets for classification. The resulting approachonly requires object bounding boxes. Its effectiveness is demonstrated usingfour publicly available fine-grained datasets, on which it outperforms orachieves comparable performance to the state-of-the-art in classification.
arxiv-18000-292 | Persistence Lenses: Segmentation, Simplification, Vectorization, Scale Space and Fractal Analysis of Images | http://arxiv.org/pdf/1604.07361v2.pdf | author:Martin Brooks category:cs.CV cs.CG math.GN published:2016-04-25 summary:A persistence lens is a hierarchy of disjoint upper and lower level sets of acontinuous luminance image's Reeb graph. The boundary components of apersistence lens's interior components are Jordan curves that serve as ahierarchical segmentation of the image, and may be rendered as vector graphics.A persistence lens determines a varilet basis for the luminance image, in whichimage simplification is a realized by subspace projection. Image scale space,and image fractal analysis, result from applying a scale measure to each basisfunction.
arxiv-18000-293 | Optimizing Neural Networks with Kronecker-factored Approximate Curvature | http://arxiv.org/pdf/1503.05671v6.pdf | author:James Martens, Roger Grosse category:cs.LG cs.NE stat.ML published:2015-03-19 summary:We propose an efficient method for approximating natural gradient descent inneural networks which we call Kronecker-Factored Approximate Curvature (K-FAC).K-FAC is based on an efficiently invertible approximation of a neural network'sFisher information matrix which is neither diagonal nor low-rank, and in somecases is completely non-sparse. It is derived by approximating various largeblocks of the Fisher (corresponding to entire layers) as being the Kroneckerproduct of two much smaller matrices. While only several times more expensiveto compute than the plain stochastic gradient, the updates produced by K-FACmake much more progress optimizing the objective, which results in an algorithmthat can be much faster than stochastic gradient descent with momentum inpractice. And unlike some previously proposed approximatenatural-gradient/Newton methods which use high-quality non-diagonal curvaturematrices (such as Hessian-free optimization), K-FAC works very well in highlystochastic optimization regimes. This is because the cost of storing andinverting K-FAC's approximation to the curvature matrix does not depend on theamount of data used to estimate it, which is a feature typically associatedonly with diagonal or low-rank approximations to the curvature matrix.
arxiv-18000-294 | Asymptotic Theory for Random Forests | http://arxiv.org/pdf/1405.0352v2.pdf | author:Stefan Wager category:math.ST stat.ML stat.TH published:2014-05-02 summary:Random forests have proven to be reliable predictive algorithms in manyapplication areas. Not much is known, however, about the statistical propertiesof random forests. Several authors have established conditions under whichtheir predictions are consistent, but these results do not provide practicalestimates of random forest errors. In this paper, we analyze a random forestmodel based on subsampling, and show that random forest predictions areasymptotically normal provided that the subsample size s scales as s(n)/n =o(log(n)^{-d}), where n is the number of training examples and d is the numberof features. Moreover, we show that the asymptotic variance can consistently beestimated using an infinitesimal jackknife for bagged ensembles recentlyproposed by Efron (2014). In other words, our results let us both characterizeand estimate the error-distribution of random forest predictions, thus taking astep towards making random forests tools for statistical inference instead ofjust black-box predictive algorithms.
arxiv-18000-295 | MOT16: A Benchmark for Multi-Object Tracking | http://arxiv.org/pdf/1603.00831v2.pdf | author:Anton Milan, Laura Leal-Taixe, Ian Reid, Stefan Roth, Konrad Schindler category:cs.CV published:2016-03-02 summary:Standardized benchmarks are crucial for the majority of computer visionapplications. Although leaderboards and ranking tables should not beover-claimed, benchmarks often provide the most objective measure ofperformance and are therefore important guides for reseach. Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, waslaunched with the goal of collecting existing and new data and creating aframework for the standardized evaluation of multiple object tracking methods.The first release of the benchmark focuses on multiple people tracking, sincepedestrians are by far the most studied object in the tracking community. Thispaper accompanies a new release of the MOTChallenge benchmark. Unlike theinitial release, all videos of MOT16 have been carefully annotated following aconsistent protocol. Moreover, it not only offers a significant increase in thenumber of labeled boxes, but also provides multiple object classes besidepedestrians and the level of visibility for every single object of interest.
arxiv-18000-296 | An evaluation of randomized machine learning methods for redundant data: Predicting short and medium-term suicide risk from administrative records and risk assessments | http://arxiv.org/pdf/1605.01116v1.pdf | author:Thuong Nguyen, Truyen Tran, Shivapratap Gopakumar, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG published:2016-05-03 summary:Accurate prediction of suicide risk in mental health patients remains an openproblem. Existing methods including clinician judgments have acceptablesensitivity, but yield many false positives. Exploiting administrative data hasa great potential, but the data has high dimensionality and redundancies in therecording processes. We investigate the efficacy of three most effectiverandomized machine learning techniques random forests, gradient boostingmachines, and deep neural nets with dropout in predicting suicide risk. Using acohort of mental health patients from a regional Australian hospital, wecompare the predictive performance with popular traditional approachesclinician judgments based on a checklist, sparse logistic regression anddecision trees. The randomized methods demonstrated robustness against dataredundancies and superior predictive performance on AUC and F-measure.
arxiv-18000-297 | New insights and perspectives on the natural gradient method | http://arxiv.org/pdf/1412.1193v6.pdf | author:James Martens category:cs.LG stat.ML published:2014-12-03 summary:Natural gradient descent is an optimization method traditionally motivatedfrom the perspective of information geometry, and works well for manyapplications as an alternative to stochastic gradient descent. In this paper wecritically analyze this method and its properties, and show how it can beviewed as a type of approximate 2nd-order optimization method, where the Fisherinformation matrix used to compute the natural gradient direction can be viewedas an approximation of the Hessian. This perspective turns out to havesignificant implications for how to design a practical and robust version ofthe method. Among our various other contributions is a thorough analysis of theconvergence speed of natural gradient descent and more general stochasticmethods, a critical examination of the oft-used "empirical" approximation ofthe Fisher matrix, and an analysis of the (approximate) parameterizationinvariance property possessed by the method, which we show still holds forcertain other choices of the curvature matrix, but notably not the Hessian.
arxiv-18000-298 | MARLow: A Joint Multiplanar Autoregressive and Low-Rank Approach for Image Completion | http://arxiv.org/pdf/1605.01115v1.pdf | author:Madingg Li, Jiaying Liu, Zhiwei Xiong, Xiaoyan Sun, Zongming Guo category:cs.CV cs.MM published:2016-05-03 summary:In this paper, we propose a novel multiplanar autoregressive (AR) model toexploit the correlation in cross-dimensional planes of a similar patch groupcollected in an image, which has long been neglected by previous AR models. Onthat basis, we then present a joint multiplanar AR and low-rank based approach(MARLow) for image completion from random sampling, which exploits the nonlocalself-similarity within natural images more effectively. Specifically, themultiplanar AR model constraints the local stationarity in differentcross-sections of the patch group, while the low-rank minimization captures theintrinsic coherence of nonlocal patches. The proposed approach can be readilyextended to multichannel images (e.g. color images), by simultaneouslyconsidering the correlation in different channels. Experimental resultsdemonstrate that the proposed approach significantly outperformsstate-of-the-art methods, even if the pixel missing rate is as high as 90%.
arxiv-18000-299 | Decentralized Dynamic Discriminative Dictionary Learning | http://arxiv.org/pdf/1605.01107v1.pdf | author:Alec Koppel, Garrett Warnell, Ethan Stump, Alejandro Ribeiro category:stat.ML cs.LG published:2016-05-03 summary:We consider discriminative dictionary learning in a distributed onlinesetting, where a network of agents aims to learn a common set of dictionaryelements of a feature space and model parameters while sequentially receivingobservations. We formulate this problem as a distributed stochastic programwith a non-convex objective and present a block variant of the Arrow-Hurwiczsaddle point algorithm to solve it. Using Lagrange multipliers to penalize thediscrepancy between them, only neighboring nodes exchange model information. Weshow that decisions made with this saddle point algorithm asymptoticallyachieve a first-order stationarity condition on average.
arxiv-18000-300 | Gated Graph Sequence Neural Networks | http://arxiv.org/pdf/1511.05493v3.pdf | author:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel category:cs.LG cs.AI cs.NE stat.ML published:2015-11-17 summary:Graph-structured data appears frequently in domains including chemistry,natural language semantics, social networks, and knowledge bases. In this work,we study feature learning techniques for graph-structured inputs. Our startingpoint is previous work on Graph Neural Networks (Scarselli et al., 2009), whichwe modify to use gated recurrent units and modern optimization techniques andthen extend to output sequences. The result is a flexible and broadly usefulclass of neural network models that has favorable inductive biases relative topurely sequence-based models (e.g., LSTMs) when the problem isgraph-structured. We demonstrate the capabilities on some simple AI (bAbI) andgraph algorithm learning tasks. We then show it achieves state-of-the-artperformance on a problem from program verification, in which subgraphs need tobe matched to abstract data structures.
