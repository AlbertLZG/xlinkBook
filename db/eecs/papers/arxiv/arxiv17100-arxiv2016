arxiv-17100-1 | Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion | http://arxiv.org/abs/1603.09725 | author:Israel D. Gebru, Silèye Ba, Xiaofei Li, Radu Horaud category:cs.CV cs.SD published:2016-03-31 summary:Speaker diarization consists of assigning speech signals to speakers engagedin dialog. An audio-visual spatiotemporal diarization model is proposed. Themodel is well suited for challenging scenarios that consist of severalparticipants engaged in multi-party dialog while they move around and turntheir heads towards the other participants rather than facing the cameras andthe microphones. Multiple-person visual tracking is combined with multiplespeech-source localization in order to tackle the person-to-speech associationproblem. The latter is solved within a novel audio-visual fusion method on thefollowing grounds: binaural spectral features are first extracted from amicrophone pair, then a supervised audio-visual alignment technique maps thesefeatures onto an image, and finally a semi-supervised clustering method assignsbinaural spectral features to visible persons. The main advantage of thismethod over previous work is that it processes in a principled way speechsignals uttered simultaneously by multiple persons. The diarization itself iscast into a latent-variable temporal graphical model that infers speakeridentities and speech turns, based on the output of the audio-visualassociation process available at each time slice, and on the dynamics of thediarization variable itself. The proposed formulation yields an efficient exactinference procedure. A novel dataset, that contains audio-visual training dataas well as a number of scenarios involving several participants engaged informal and informal dialog, is introduced. The proposed method is thoroughlytested and benchmarked with respect to several state-of-the art diarizationalgorithms.
arxiv-17100-2 | Data Collection for Interactive Learning through the Dialog | http://arxiv.org/abs/1603.09631 | author:Miroslav Vodolán, Filip Jurčíček category:cs.CL cs.LG published:2016-03-31 summary:This paper presents a dataset collected from natural dialogs which enables totest the ability of dialog systems to learn new facts from user utterancesthroughout the dialog. This interactive learning will help with one of the mostprevailing problems of open domain dialog system, which is the sparsity offacts a dialog system can reason about. The proposed dataset, consisting of1900 collected dialogs, allows simulation of an interactive gaining ofdenotations and questions explanations from users which can be used for theinteractive learning.
arxiv-17100-3 | Minimal Gated Unit for Recurrent Neural Networks | http://arxiv.org/abs/1603.09420 | author:Guo-Bing Zhou, Jianxin Wu, Chen-Lin Zhang, Zhi-Hua Zhou category:cs.NE cs.LG published:2016-03-31 summary:Recently recurrent neural networks (RNN) has been very successful in handlingsequence data. However, understanding RNN and finding the best practices forRNN is a difficult task, partly because there are many competing and complexhidden units (such as LSTM and GRU). We propose a gated unit for RNN, named asMinimal Gated Unit (MGU), since it only contains one gate, which is a minimaldesign among all gated hidden units. The design of MGU benefits from evaluationresults on LSTM and GRU in the literature. Experiments on various sequence datashow that MGU has comparable accuracy with GRU, but has a simpler structure,fewer parameters, and faster training. Hence, MGU is suitable in RNN'sapplications. Its simple architecture also means that it is easier to evaluateand tune, and in principle it is easier to study MGU's properties theoreticallyand empirically.
arxiv-17100-4 | Neural Language Correction with Character-Based Attention | http://arxiv.org/abs/1603.09727 | author:Ziang Xie, Anand Avati, Naveen Arivazhagan, Dan Jurafsky, Andrew Y. Ng category:cs.CL cs.AI published:2016-03-31 summary:Natural language correction has the potential to help language learnersimprove their writing skills. While approaches with separate classifiers fordifferent error types have high precision, they do not flexibly handle errorssuch as redundancy or non-idiomatic phrasing. On the other hand, word andphrase-based machine translation methods are not designed to cope withorthographic errors, and have recently been outpaced by neural models.Motivated by these issues, we present a neural network-based approach tolanguage correction. The core component of our method is an encoder-decoderrecurrent neural network with an attention mechanism. By operating at thecharacter level, the network avoids the problem of out-of-vocabulary words. Weillustrate the flexibility of our approach on dataset of noisy, user-generatedtext collected from an English learner forum. When combined with a languagemodel, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014Shared Task. We further demonstrate that training the network on additionaldata with synthesized errors can improve performance.
arxiv-17100-5 | Learning Multiscale Features Directly From Waveforms | http://arxiv.org/abs/1603.09509 | author:Zhenyao Zhu, Jesse H. Engel, Awni Hannun category:cs.CL cs.LG cs.NE cs.SD published:2016-03-31 summary:Deep learning has dramatically improved the performance of speech recognitionsystems through learning hierarchies of features optimized for the task athand. However, true end-to-end learning, where features are learned directlyfrom waveforms, has only recently reached the performance of hand-tailoredrepresentations based on the Fourier transform. In this paper, we detail anapproach to use convolutional filters to push past the inherent tradeoff oftemporal and frequency resolution that exists for spectral representations. Atincreased computational cost, we show that increasing temporal resolution viareduced stride and increasing frequency resolution via additional filtersdelivers significant performance improvements. Further, we find more efficientrepresentations by simultaneously learning at multiple scales, leading to anoverall decrease in word error rate on a difficult internal speech test set by20.7% relative to networks with the same number of parameters trained onspectrograms.
arxiv-17100-6 | Robust Head-Pose Estimation Based on Partially-Latent Mixture of Linear Regression | http://arxiv.org/abs/1603.09732 | author:Vincent Drouard, Radu Horaud, Antoine Deleforge, Silèye Ba, Georgios Evangelidis category:cs.CV published:2016-03-31 summary:Head-pose estimation has many applications, such as social-event analysis,human-robot and human-computer interaction, driving assistance, and so forth.Head-pose estimation is challenging because it must cope with changingillumination conditions, face orientation and appearance variabilities, partialocclusions of facial landmarks, as well as bounding-box-to-face alignmentproblems. We propose a mixture of linear regression method that learns how tomap high-dimensional feature vectors (extracted from bounding-boxes of faces)onto both head-pose parameters and bounding-box shifts, such that at runtimethey are simultaneously predicted. We describe in detail the mapping methodthat combines the merits of manifold learning and of mixture of linearregression. We validate our method with three publicly available datasets andwe thoroughly benchmark four variants of the proposed algorithm with severalstate-of-the-art head-pose estimation methods.
arxiv-17100-7 | Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders | http://arxiv.org/abs/1603.09128 | author:Simon Šuster, Ivan Titov, Gertjan van Noord category:cs.CL cs.LG stat.ML published:2016-03-30 summary:We present an approach to learning multi-sense word embeddings relying bothon monolingual and bilingual information. Our model consists of an encoder,which uses monolingual and bilingual context (i.e. a parallel sentence) tochoose a sense for a given word, and a decoder which predicts context wordsbased on the chosen sense. The two components are estimated jointly. We observethat the word representations induced from bilingual data outperform themonolingual counterparts across a range of evaluation tasks, even thoughcrosslingual information is not available at test time.
arxiv-17100-8 | Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings | http://arxiv.org/abs/1603.09188 | author:Spandana Gella, Mirella Lapata, Frank Keller category:cs.CL cs.CV published:2016-03-30 summary:We introduce a new task, visual sense disambiguation for verbs: given animage and a verb, assign the correct sense of the verb, i.e., the one thatdescribes the action depicted in the image. Just as textual word sensedisambiguation is useful for a wide range of NLP tasks, visual sensedisambiguation can be useful for multimodal tasks such as image retrieval,image description, and text illustration. We introduce VerSe, a new datasetthat augments existing multimodal datasets (COCO and TUHOI) with sense labels.We propose an unsupervised algorithm based on Lesk which performs visual sensedisambiguation using textual, visual, or multimodal embeddings. We find thattextual embeddings perform well when gold-standard textual annotations (objectlabels and image descriptions) are available, while multimodal embeddingsperform well on unannotated images. We also verify our findings by using thetextual and multimodal embeddings as features in a supervised setting andanalyse the performance of visual sense disambiguation task. VerSe is madepublicly available and can be downloaded at:https://github.com/spandanagella/verse.
arxiv-17100-9 | deepTarget: End-to-end Learning Framework for microRNA Target Prediction using Deep Recurrent Neural Networks | http://arxiv.org/abs/1603.09123 | author:Byunghan Lee, Junghwan Baek, Seunghyun Park, Sungroh Yoon category:cs.LG q-bio.GN published:2016-03-30 summary:MicroRNAs (miRNAs) are short sequences of ribonucleic acids that control theexpression of target messenger RNAs (mRNAs) by binding them. Robust predictionof miRNA-mRNA pairs is of utmost importance in deciphering gene regulations buthas been challenging because of high false positive rates, despite a deluge ofcomputational tools that normally require laborious manual feature extraction.This paper presents an end-to-end machine learning framework for miRNA targetprediction. Leveraged by deep recurrent neural networks-based auto-encoding andsequence-sequence interaction learning, our approach not only delivers anunprecedented level of accuracy but also eliminates the need for manual featureextraction. The performance gap between the proposed method and existingalternatives is substantial (over 25% increase in F-measure), and deepTargetdelivers a quantum leap in the long-standing challenge of robust miRNA targetprediction.
arxiv-17100-10 | LIFT: Learned Invariant Feature Transform | http://arxiv.org/abs/1603.09114 | author:Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, Pascal Fua category:cs.CV published:2016-03-30 summary:We introduce a novel Deep Network architecture that implements the fullfeature point handling pipeline, that is, detection, orientation estimation,and feature description. While previous works have successfully tackled eachone of these problems individually, we show how to learn to do all three in aunified manner while preserving end-to-end differentiability. We thendemonstrate that our Deep pipeline outperforms state-of-the-art methods on anumber of benchmark datasets, without the need of retraining.
arxiv-17100-11 | Unsupervised Understanding of Location and Illumination Changes in Egocentric Videos | http://arxiv.org/abs/1603.09200 | author:Alejandro Betancourt, Natalia Díaz-Rodríguez, Emilia Barakova, Lucio Marcenaro, Matthias Rauterberg, Carlo Regazzoni category:cs.CV published:2016-03-30 summary:Wearable cameras stand out as one of the most promising devices for thecoming years, and as a consequence, the demand of computer algorithms toautomatically understand these videos has been increasing quickly. An automaticunderstanding of these videos is not an easy task, and its mobile natureimplies important challenges to be faced, such as the changing light conditionsand the unrestricted locations recorded. This paper proposes an unsupervisedstrategy based on global features and manifold learning to endow wearablecameras with contextual information regarding the light conditions and thelocation recorded. Results show that non-linear manifold methods can capturecontextual patterns from global features without compromising largecomputational resources. As an application case, the proposed unsupervisedstrategy is used as a switching mechanism to improve the hand-detection problemin egocentric videos under a multi-model approach.
arxiv-17100-12 | Structured Feature Learning for Pose Estimation | http://arxiv.org/abs/1603.09065 | author:Xiao Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang category:cs.CV published:2016-03-30 summary:In this paper, we propose a structured feature learning framework to reasonthe correlations among body joints at the feature level in human poseestimation. Different from existing approaches of modelling structures on scoremaps or predicted labels, feature maps preserve substantially richerdescriptions of body joints. The relationships between feature maps of jointsare captured with the introduced geometrical transform kernels, which can beeasily implemented with a convolution layer. Features and their relationshipsare jointly learned in an end-to-end learning system. A bi-directional treestructured model is proposed, so that the feature channels at a body joint canwell receive information from other joints. The proposed framework improvesfeature learning substantially. With very simple post processing, it reachesthe best mean PCP on the LSP and FLIC datasets. Compared with the baseline oflearning features at each joint separately with ConvNet, the mean PCP has beenimproved by 18% on FLIC. The code is released to the public.
arxiv-17100-13 | Semi-Supervised Learning for Asymmetric Graphs through Reach and Distance Diffusion | http://arxiv.org/abs/1603.09064 | author:Edith Cohen category:cs.LG published:2016-03-30 summary:Semi-supervised learning algorithms are an indispensable tool when labeledexamples are scarce and there are many unlabeled examples [Blum and Chawla2001, Zhu et. al. 2003]. With graph-based methods, entities (examples)correspond to nodes in a graph and edges correspond to related entities. Thegraph structure is used to infer implicit pairwise affinity values (kernel)which are used to compute the learned labels. Two powerful techniques to define such a kernel are "symmetric" spectralmethods and Personalized Page Rank (PPR). With spectral methods, labels can bescalably learned using Jacobi iterations, but an inherent limiting issue isthat they are applicable to (undirected) graphs, whereas often, such as withlike, follow, or hyperlinks, relations between entities are inherentlyasymmetric. PPR naturally works with directed graphs but even with state of theart techniques does not scale when we want to learn billions of labels. Aiming at both high scalability and handling of directed relations, wepropose here and kernels. Our design is inspired by models for influencediffusion in social networks, formalized and spawned from the seminal work of[Kempe, Kleinberg, and Tardos 2003]. These models apply with directedinteractions and are naturally suited for asymmetry. We tailor these models todefine a natural asymmetric "kernel" and design highly scalable algorithms forparameter setting and label learning.
arxiv-17100-14 | Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index | http://arxiv.org/abs/1603.09326 | author:Susan Athey, Raj Chetty, Guido Imbens, Hyunseung Kang category:stat.ME stat.ML published:2016-03-30 summary:Estimating the long-term effects of treatments is of interest in many fields.A common challenge in estimating such treatment effects is that long-termoutcomes are unobserved in the time frame needed to make policy decisions. Oneapproach to overcome this missing data problem is to analyze treatments effectson an intermediate outcome, often called a statistical surrogate, if itsatisfies the condition that treatment and outcome are independent conditionalon the statistical surrogate. The validity of the surrogacy condition is oftencontroversial. Here we exploit that fact that in modern datasets, researchersoften observe a large number, possibly hundreds or thousands, of intermediateoutcomes, thought to lie on or close to the causal chain between the treatmentand the long-term outcome of interest. Even if none of the individual proxiessatisfies the statistical surrogacy criterion by itself, using multiple proxiescan be useful in causal inference. We focus primarily on a setting with twosamples, an experimental sample containing data about the treatment indicatorand the surrogates and an observational sample containing information about thesurrogates and the primary outcome. We state assumptions under which theaverage treatment effect be identified and estimated with a high-dimensionalvector of proxies that collectively satisfy the surrogacy assumption, andderive the bias from violations of the surrogacy assumption, and show that evenif the primary outcome is also observed in the experimental sample, there isstill information to be gained from using surrogates.
arxiv-17100-15 | Image Denoising Using Very Deep Fully Convolutional Encoder-Decoder Networks with Symmetric Skip Connections | http://arxiv.org/abs/1603.09056 | author:Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang category:cs.CV published:2016-03-30 summary:Image denoising is a long-standing problem in computer vision and imageprocessing, as well as a test bed for low-level image modeling algorithms. Inthis paper, we propose a very deep encoding-decoding framework for imagedenoising. Instead of using image priors, the proposed framework learnsend-to-end fully convolutional mappings from noisy images to the clean ones.The network is composed of multiple layers of convolution and de-convolutionoperators. With the observation that deeper networks improve denoisingperformance, we propose to use significantly deeper networks than thoseemployed previously for low-level image processing tasks such as denoising. Wepropose to symmetrically link convolutional and de-convolutional layers withskip-layer connections, with which the training converges much faster andattains a higher-quality local optimum. From the image processing point ofview, those symmetric connections help preserve image details.
arxiv-17100-16 | Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence and Vector Cosine in VSMs | http://arxiv.org/abs/1603.09054 | author:Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, Chu-Ren Huang category:cs.CL published:2016-03-30 summary:In this paper, we claim that vector cosine, which is generally consideredamong the most efficient unsupervised measures for identifying word similarityin Vector Space Models, can be outperformed by an unsupervised measure thatcalculates the extent of the intersection among the most mutually dependentcontexts of the target words. To prove it, we describe and evaluate APSyn, avariant of the Average Precision that, without any optimization, outperformsthe vector cosine and the co-occurrence on the standard ESL test set, with animprovement ranging between +9.00% and +17.98%, depending on the number ofchosen top contexts.
arxiv-17100-17 | Phoenix: A Self-Optimizing Chess Engine | http://arxiv.org/abs/1603.09051 | author:Rahul A R, G Srinivasaraghavan category:cs.AI cs.NE published:2016-03-30 summary:Since the advent of computers, many tasks which required humans to spend alot of time and energy have been trivialized by the computers' ability toperform repetitive tasks extremely quickly. However there are still many areasin which humans excel in comparison with the machines. One such area is chess.Even with great advances in the speed and computational power of modernmachines, Grandmasters often beat the best chess programs in the world withrelative ease. This may be due to the fact that a game of chess cannot be wonby pure calculation. There is more to the goodness of a chess position thansome numerical value which apparently can be seen only by the human brain. Herean effort has been made to improve current chess engines by letting themselvesevolve over a period of time. Firstly, the problem of learning is reduced intoan optimization problem by defining Position Evaluation in terms of PositionalValue Tables (PVTs). Next, the PVTs are optimized using Multi-Niche Crowdingwhich successfully identifies the optima in a multimodal function, therebyarriving at distinctly different solutions which are close to the globaloptimum.
arxiv-17100-18 | Robustness of Bayesian Pool-based Active Learning Against Prior Misspecification | http://arxiv.org/abs/1603.09050 | author:Nguyen Viet Cuong, Nan Ye, Wee Sun Lee category:cs.LG stat.ML published:2016-03-30 summary:We study the robustness of active learning (AL) algorithms against priormisspecification: whether an algorithm achieves similar performance using aperturbed prior as compared to using the true prior. In both the average andworst cases of the maximum coverage setting, we prove that all$\alpha$-approximate algorithms are robust (i.e., near $\alpha$-approximate) ifthe utility is Lipschitz continuous in the prior. We further show thatrobustness may not be achieved if the utility is non-Lipschitz. This suggestswe should use a Lipschitz utility for AL if robustness is required. For theminimum cost setting, we can also obtain a robustness result for approximate ALalgorithms. Our results imply that many commonly used AL algorithms are robustagainst perturbed priors. We then propose the use of a mixture prior toalleviate the problem of prior misspecification. We analyze the robustness ofthe uniform mixture prior and show experimentally that it performs reasonablywell in practice.
arxiv-17100-19 | Confidence driven TGV fusion | http://arxiv.org/abs/1603.09302 | author:Valsamis Ntouskos, Fiora Pirri category:cs.CV published:2016-03-30 summary:We introduce a novel model for spatially varying variational data fusion,driven by point-wise confidence values. The proposed model allows for the jointestimation of the data and the confidence values based on the spatial coherenceof the data. We discuss the main properties of the introduced model as well assuitable algorithms for estimating the solution of the corresponding biconvexminimization problem and their convergence. The performance of the proposedmodel is evaluated considering the problem of depth image fusion by using bothsynthetic and real data from publicly available datasets.
arxiv-17100-20 | Partial Face Detection for Continuous Authentication | http://arxiv.org/abs/1603.09364 | author:Upal Mahbub, Vishal M. Patel, Deepak Chandra, Brandon Barbello, Rama Chellappa category:cs.CV published:2016-03-30 summary:In this paper, a part-based technique for real time detection of users' faceson mobile devices is proposed. This method is specifically designed fordetecting partially cropped and occluded faces captured using a smartphone'sfront-facing camera for continuous authentication. The key idea is to detectfacial segments in the frame and cluster the results to obtain the region whichis most likely to contain a face. Extensive experimentation on a mobile datasetof 50 users shows that our method performs better than many state-of-the-artface detection methods in terms of accuracy and processing speed.
arxiv-17100-21 | Optimal Recommendation to Users that React: Online Learning for a Class of POMDPs | http://arxiv.org/abs/1603.09233 | author:Rahul Meshram, Aditya Gopalan, D. Manjunath category:cs.LG published:2016-03-30 summary:We describe and study a model for an Automated Online Recommendation System(AORS) in which a user's preferences can be time-dependent and can also dependon the history of past recommendations and play-outs. The three key features ofthe model that makes it more realistic compared to existing models forrecommendation systems are (1) user preference is inherently latent, (2)current recommendations can affect future preferences, and (3) it allows forthe development of learning algorithms with provable performance guarantees.The problem is cast as an average-cost restless multi-armed bandit for a givenuser, with an independent partially observable Markov decision process (POMDP)for each item of content. We analyze the POMDP for a single arm, describe itsstructural properties, and characterize its optimal policy. We then develop aThompson sampling-based online reinforcement learning algorithm to learn theparameters of the model and optimize utility from the binary responses of theusers to continuous recommendations. We then analyze the performance of thelearning algorithm and characterize the regret. Illustrative numerical resultsand directions for extension to the restless hidden Markov multi-armed banditproblem are also presented.
arxiv-17100-22 | Dense Image Representation with Spatial Pyramid VLAD Coding of CNN for Locally Robust Captioning | http://arxiv.org/abs/1603.09046 | author:Andrew Shin, Masataka Yamaguchi, Katsunori Ohnishi, Tatsuya Harada category:cs.CV published:2016-03-30 summary:The workflow of extracting features from images using convolutional neuralnetworks (CNN) and generating captions with recurrent neural networks (RNN) hasbecome a de-facto standard for image captioning task. However, since CNNfeatures are originally designed for classification task, it is mostlyconcerned with the main conspicuous element of the image, and often fails tocorrectly convey information on local, secondary elements. We propose toincorporate coding with vector of locally aggregated descriptors (VLAD) onspatial pyramid for CNN features of sub-regions in order to generate imagerepresentations that better reflect the local information of the images. Ourresults show that our method of compact VLAD coding can match CNN features withas little as 3% of dimensionality and, when combined with spatial pyramid, itresults in image captions that more accurately take local elements intoaccount.
arxiv-17100-23 | Performance of a community detection algorithm based on semidefinite programming | http://arxiv.org/abs/1603.09045 | author:Adel Javanmard, Andrea Montanari, Federico Ricci-Tersenghi category:stat.ML cs.SI physics.soc-ph published:2016-03-30 summary:The problem of detecting communities in a graph is maybe one the most studiedinference problems, given its simplicity and widespread diffusion among severaldisciplines. A very common benchmark for this problem is the stochastic blockmodel or planted partition problem, where a phase transition takes place in thedetection of the planted partition by changing the signal-to-noise ratio.Optimal algorithms for the detection exist which are based on spectral methods,but we show these are extremely sensible to slight modification in thegenerative model. Recently Javanmard, Montanari and Ricci-Tersenghi(arXiv:1511.08769) have used statistical physics arguments, and numericalsimulations to show that finding communities in the stochastic block model viasemidefinite programming is quasi optimal. Further, the resulting semidefiniterelaxation can be solved efficiently, and is very robust with respect tochanges in the generative model. In this paper we study in detail severalpractical aspects of this new algorithm based on semidefinite programming forthe detection of the planted partition. The algorithm turns out to be veryfast, allowing the solution of problems with $O(10^5)$ variables in few secondon a laptop computer.
arxiv-17100-24 | Möbius invariants of shapes and images | http://arxiv.org/abs/1603.09335 | author:Stephen Marsland, Robert McLachlan category:cs.CV math.MG published:2016-03-30 summary:Identifying when different images are of the same object despite changescaused by imaging technologies, or processes such as growth, has manyapplications in fields such as computer vision and biological image analysis.One approach to this problem is to identify the group of possibletransformations of the object and to find invariants to the action of thatgroup, meaning that the object has the same values of the invariants despitethe action of the group. In this paper we study the invariants of planar shapesand images under the M\"obius group $\mathrm{PSL}(2,\mathbb{C})$, which arisesin the conformal camera model of vision and may also correspond to neurologicalaspects of vision, such as grouping of lines and circles. We survey thecomputational requirements of an invariant, and the known M\"obius invariants,and then develop an algorithm by which shapes can be recognised that isM\"obius- and parametrization-invariant, numerically stable, and robust tonoise. We demonstrate the efficacy of this new invariant approach on sets ofcurves, and then develop a M\"obius-invariant signature of grey-scale images.
arxiv-17100-25 | Binary Quadratic Programing for Online Tracking of Hundreds of People in Extremely Crowded Scenes | http://arxiv.org/abs/1603.09240 | author:Afshin Dehghan, Mubarak Shah category:cs.CV cs.RO published:2016-03-30 summary:Multi-object tracking has been studied for decades. However, when it comes totracking pedestrians in extremely crowded scenes, we are limited to only fewworks. This is an important problem which gives rise to several challenges.Pre-trained object detectors fail to localize targets in crowded sequences.This consequently limits the use of data-association based multi-targettracking methods which rely on the outcome of an object detector. Additionally,the small apparent target size makes it challenging to extract features todiscriminate targets from their surroundings. Finally, the large number oftargets greatly increases computational complexity which in turn makes it hardto extend existing multi-target tracking approaches to high-density crowdscenarios. In this paper, we propose a tracker that addresses theaforementioned problems and is capable of tracking hundreds of peopleefficiently. We formulate online crowd tracking as Binary Quadratic Programing.Our formulation employs target's individual information in the form ofappearance and motion as well as contextual cues in the form of neighborhoodmotion, spatial proximity and grouping constraints, and solves detection anddata association simultaneously. In order to solve the proposed quadraticoptimization efficiently, where state-of art commercial quadratic programingsolvers fail to find the answer in a reasonable amount of time, we propose touse the most recent version of the Modified Frank Wolfe algorithm, which takesadvantage of SWAP-steps to speed up the optimization. We show that the proposedformulation can track hundreds of targets efficiently and improves state-of-artresults by significant margins on eleven challenging high density crowdsequences.
arxiv-17100-26 | Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles | http://arxiv.org/abs/1603.09246 | author:Mehdi Noroozi, Paolo Favaro category:cs.CV published:2016-03-30 summary:In this paper we study the problem of image representation learning withouthuman annotation. Following the principles of self-supervision, we build aconvolutional neural network (CNN) that can be trained to solve Jigsaw puzzlesas a pretext task, which requires no manual labeling, and then later repurposedto solve object classification and detection. To maintain the compatibilityacross tasks we introduce the context-free network (CFN), a Siamese-ennead CNN.The CFN takes image tiles as input and explicitly limits the receptive field(or context) of its early processing units to one tile at a time. We show thatthe CFN is a more compact version of AlexNet, but with the same semanticlearning capabilities. By training the CFN to solve Jigsaw puzzles, we learnboth a feature mapping of object parts as well as their cor-rect spatialarrangement. Our experimental evaluations show that the learned featurescapture semantically relevant content. The performance in object detection offeatures extracted from the CFN is the highest (51.8%) among unsupervisedlytrained features, and very close to that of supervisedly trained features(56.5%). In object classification the CFN features achieve also the bestaccuracy (38.1%) among unsupervisedly trained features on the ImageNet 2012dataset.
arxiv-17100-27 | Vector Quantization for Machine Vision | http://arxiv.org/abs/1603.09037 | author:Vincenzo Liguori category:cs.CV published:2016-03-30 summary:This paper shows how to reduce the computational cost for a variety of commonmachine vision tasks by operating directly in the compressed domain,particularly in the context of hardware acceleration. Pyramid VectorQuantization (PVQ) is the compression technique of choice and its propertiesare exploited to simplify Support Vector Machines (SVM), Convolutional NeuralNetworks(CNNs), Histogram of Oriented Gradients (HOG) features, interest pointsmatching and other algorithms.
arxiv-17100-28 | Towards Geo-Distributed Machine Learning | http://arxiv.org/abs/1603.09035 | author:Ignacio Cano, Markus Weimer, Dhruv Mahajan, Carlo Curino, Giovanni Matteo Fumarola category:cs.LG cs.DC stat.ML published:2016-03-30 summary:Latency to end-users and regulatory requirements push large companies tobuild data centers all around the world. The resulting data is "born"geographically distributed. On the other hand, many machine learningapplications require a global view of such data in order to achieve the bestresults. These types of applications form a new class of learning problems,which we call Geo-Distributed Machine Learning (GDML). Such applications needto cope with: 1) scarce and expensive cross-data center bandwidth, and 2)growing privacy concerns that are pushing for stricter data sovereigntyregulations. Current solutions to learning from geo-distributed data sourcesrevolve around the idea of first centralizing the data in one data center, andthen training locally. As machine learning algorithms arecommunication-intensive, the cost of centralizing the data is thought to beoffset by the lower cost of intra-data center communication during training. Inthis work, we show that the current centralized practice can be far fromoptimal, and propose a system for doing geo-distributed training. Furthermore,we argue that the geo-distributed approach is structurally more amenable todealing with regulatory constraints, as raw data never leaves the source datacenter. Our empirical evaluation on three real datasets confirms the generalvalidity of our approach, and shows that GDML is not only possible but alsoadvisable in many scenarios.
arxiv-17100-29 | A latent-observed dissimilarity measure | http://arxiv.org/abs/1603.09254 | author:Yasushi Terazono category:stat.ML published:2016-03-30 summary:Quantitatively assessing relationships between latent variables and observedvariables is important for understanding and developing generative models andrepresentation learning. In this paper, we propose latent-observeddissimilarity (LOD) to evaluate the dissimilarity between the probabilisticcharacteristics of latent and observed variables. We also define four essentialtypes of generative models with different independence/conditional independenceconfigurations. Experiments using tractable real-world data show that LOD caneffectively capture the differences between models and reflect the capabilityfor higher layer learning. They also show that the conditional independence oflatent variables given observed variables contributes to improving thetransmission of information and characteristics from lower layers to higherlayers.
arxiv-17100-30 | Clinical Information Extraction via Convolutional Neural Network | http://arxiv.org/abs/1603.09381 | author:Peng Li, Heng Huang category:cs.LG cs.CL cs.NE published:2016-03-30 summary:We report an implementation of a clinical information extraction tool thatleverages deep neural network to annotate event spans and their attributes fromraw clinical notes and pathology reports. Our approach uses context words andtheir part-of-speech tags and shape information as features. Then we hiretemporal (1D) convolutional neural network to learn hidden featurerepresentations. Finally, we use Multilayer Perceptron (MLP) to predict eventspans. The empirical evaluation demonstrates that our approach significantlyoutperforms baselines.
arxiv-17100-31 | Maximize Pointwise Cost-sensitively Submodular Functions With Budget Constraint | http://arxiv.org/abs/1603.09029 | author:Nguyen Viet Cuong, Huan Xu category:cs.AI cs.DM math.OC stat.ML published:2016-03-30 summary:We study the worst-case adaptive optimization problem with budget constraint.Unlike previous works, we consider the general setting where the cost is a setfunction on sets of decisions. For this setting, we investigate thenear-optimality of greedy policies when the utility function satisfies a novelproperty called pointwise cost-sensitive submodularity. This property is anextension of cost-sensitive submodularity, which in turn is a generalization ofsubmodularity to general cost functions. We prove that two simple greedypolicies for the problem are not near-optimal but the best between them isnear-optimal. With this result, we propose a combined policy that isnear-optimal with respect to the optimal worst-case policy that uses half ofthe budget. We discuss applications of our theoretical results and also reportexperimental results comparing the greedy policies on the active learningproblem.
arxiv-17100-32 | Degrees of Freedom in Deep Neural Networks | http://arxiv.org/abs/1603.09260 | author:Tianxiang Gao, Vladimir Jojic category:cs.LG stat.ML published:2016-03-30 summary:In this paper, we explore degrees of freedom in deep sigmoidal neuralnetworks. We show that the degrees of freedom in these models is related to theexpected optimism, which is the expected difference between test error andtraining error. We provide an efficient Monte-Carlo method to estimate thedegrees of freedom for multi-class classification methods. We show degrees offreedom are lower than the parameter count in a simple XOR network. We extendthese results to neural nets trained on synthetic and real data, andinvestigate impact of network's architecture and different regularizationchoices. The degrees of freedom in deep networks are dramatically smaller thanthe number of parameters, in some real datasets several orders of magnitude.Further, we observe that for fixed number of parameters, deeper networks haveless degrees of freedom exhibiting a regularization-by-depth.
arxiv-17100-33 | Palmprint Recognition Using Deep Scattering Convolutional Network | http://arxiv.org/abs/1603.09027 | author:Shervin Minaee, Yao Wang category:cs.CV published:2016-03-30 summary:Palmprint recognition has drawn a lot of attention during the recent years.Many algorithms have been proposed for palmprint recognition in the past,majority of them being based on features extracted from the transform domain.Many of these transform domain features are not translation or rotationinvariant, and therefore a great deal of preprocessing is needed to align theimages. In this paper, a powerful image representation, called scatteringnetwork/transform, is used for palmprint recognition. Scattering network is aconvolutional network where its architecture and filters are predefined wavelettransforms. The first layer of scattering network captures similar features toSIFT descriptors and the higher-layer features capture higher-frequency contentof the signal which are lost in SIFT and other similar descriptors. Afterextraction of the scattering features, their dimensionality is reduced byapplying principal component analysis (PCA) which reduces the computationalcomplexity of the recognition task. Two different classifiers are used forrecognition: multi-class SVM and minimum-distance classifier. The proposedscheme has been tested on a well-known palmprint database and achieved accuracyrate of 99.95% and 100% using minimum distance classifier and SVM respectively.
arxiv-17100-34 | Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding | http://arxiv.org/abs/1603.09405 | author:Peng Li, Heng Huang category:cs.CL cs.AI cs.NE published:2016-03-30 summary:Neural network based approaches for sentence relation modeling automaticallygenerate hidden matching features from raw sentence pairs. However, the qualityof matching feature representation may not be satisfied due to complex semanticrelations such as entailment or contradiction. To address this challenge, wepropose a new deep neural network architecture that jointly leveragepre-trained word embedding and auxiliary character embedding to learn sentencemeanings. The two kinds of word sequence representations as inputs intomulti-layer bidirectional LSTM to learn enhanced sentence representation. Afterthat, we construct matching features followed by another temporal CNN to learnhigh-level hidden matching feature representations. Experimental resultsdemonstrate that our approach consistently outperforms the existing methods onstandard evaluation datasets.
arxiv-17100-35 | Rich Image Captioning in the Wild | http://arxiv.org/abs/1603.09016 | author:Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cornelia Carapcea, Chris Thrasher, Chris Buehler, Chris Sienkiewicz category:cs.CV published:2016-03-30 summary:We present an image caption system that addresses new challenges ofautomatically describing images in the wild. The challenges include highquality caption quality with respect to human judgments, out-of-domain datahandling, and low latency required in many applications. Built on top of astate-of-the-art framework, we developed a deep vision model that detects abroad range of visual concepts, an entity recognition model that identifiescelebrities and landmarks, and a confidence model for the caption output.Experimental results show that our caption engine outperforms previousstate-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO)and out of-domain datasets.
arxiv-17100-36 | On the Geometry of Message Passing Algorithms for Gaussian Reciprocal Processes | http://arxiv.org/abs/1603.09279 | author:Francesca Paola Carli category:stat.ML math.OC published:2016-03-30 summary:Reciprocal processes are acausal generalizations of Markov processesintroduced by Bernstein in 1932. In the literature, a significant amount ofattention has been focused on developing dynamical models for reciprocalprocesses. Recently, probabilistic graphical models for reciprocal processeshave been provided. This opens the way to the application of efficientinference algorithms in the machine learning literature to solve the smoothingproblem for reciprocal processes. Such algorithms are known to converge if theunderlying graph is a tree. This is not the case for a reciprocal process,whose associated graphical model is a single loop network. The contribution ofthis paper is twofold. First, we introduce belief propagation for Gaussianreciprocal processes. Second, we establish a link between convergence analysisof belief propagation for Gaussian reciprocal processes and stability theoryfor differentially positive systems.
arxiv-17100-37 | Bayesian inference in hierarchical models by combining independent posteriors | http://arxiv.org/abs/1603.09272 | author:Ritabrata Dutta, Paul Blomstedt, Samuel Kaski category:stat.CO stat.ME stat.ML published:2016-03-30 summary:Hierarchical models are versatile tools for joint modeling of data setsarising from different, but related, sources. Fully Bayesian inference may,however, become computationally prohibitive if the source-specific data modelsare complex, or if the number of sources is very large. To facilitatecomputation, we propose an approach, where inference is first madeindependently for the parameters of each data set, whereupon the obtainedposterior samples are used as observed data in a substitute hierarchical model,based on a scaled likelihood function. Compared to direct inference in a fullhierarchical model, the approach has the advantage of being able to speed upconvergence by breaking down the initial large inference problem into smallerindividual subproblems with better convergence properties. Moreover it enablesparallel processing of the possibly complex inferences of the source-specificparameters, which may otherwise create a computational bottleneck if processedjointly as part of a hierarchical model. The approach is illustrated with bothsimulated and real data.
arxiv-17100-38 | Cost-sensitive Label Embedding for Multi-label Classification | http://arxiv.org/abs/1603.09048 | author:Kuan-Hao Huang, Hsuan-Tien Lin category:cs.LG stat.ML published:2016-03-30 summary:Label embedding (LE) is an important family of multi-label classificationalgorithms that digest the label information jointly for better performance.Different real-world applications evaluate performance by different costfunctions of interest. Current LE algorithms often aim to optimize one specificcost function, but they can suffer from bad performance with respect to othercost functions. In this paper, we resolve the performance issue by proposing anovel cost-sensitive LE algorithm that takes the cost function of interest intoaccount. The proposed algorithm is based on using distances of the embeddedvectors to approximate the cost information, and takes the classic manifoldlearning approach to compute the embedded vectors. The algorithm can deal withboth symmetric and asymmetric cost functions, and effectively makescost-sensitive decisions by nearest-neighbor decoding within the embeddedvectors. Extensive experimental results justify that the proposed algorithm issignificantly better than a wide spectrum of existing LE algorithms acrossdifferent cost functions.
arxiv-17100-39 | Deep Networks with Stochastic Depth | http://arxiv.org/abs/1603.09382 | author:Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger category:cs.LG cs.CV cs.NE published:2016-03-30 summary:Very deep convolutional networks with hundreds or more layers have lead tosignificant reductions in error on competitive benchmarks like the ImageNet orCOCO tasks. Although the unmatched expressiveness of the many deep layers canbe highly desirable at test time, training very deep networks comes with itsown set of challenges. The gradients can vanish, the forward flow oftendiminishes and the training time can be painfully slow even on moderncomputers. In this paper we propose stochastic depth, a training procedure thatenables the seemingly contradictory setup to train short networks and obtaindeep networks. We start with very deep networks but during training, for eachmini-batch, randomly drop a subset of layers and bypass them with the identityfunction. The resulting networks are short (in expectation) during training anddeep during testing. Training Residual Networks with stochastic depth iscompellingly simple to implement, yet effective. We show that this approachsuccessfully addresses the training difficulties of deep networks andcomplements the recent success of Residual and Highway Networks. It reducestraining time substantially and improves the test errors on almost all datasets significantly (CIFAR-10, CIFAR-100, SVHN). Intriguingly, with stochasticdepth we can increase the depth of residual networks even beyond 1200 layersand still yield meaningful improvements in test error (4.91%) on CIFAR-10.
arxiv-17100-40 | Recurrent Batch Normalization | http://arxiv.org/abs/1603.09025 | author:Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, Aaron Courville category:cs.LG published:2016-03-30 summary:We propose a reparameterization of LSTM that brings the benefits of batchnormalization to recurrent neural networks. Whereas previous works only applybatch normalization to the input-to-hidden transformation of RNNs, wedemonstrate that it is both possible and beneficial to batch-normalize thehidden-to-hidden transition, thereby reducing internal covariate shift betweentime steps. We evaluate our proposal on various sequential problems such assequence classification, language modeling and question answering. Ourempirical results show that our batch-normalized LSTM consistently leads tofaster convergence and improved generalization.
arxiv-17100-41 | Improving and Scaling Trans-dimensional Random Field Language Models | http://arxiv.org/abs/1603.09170 | author:Bin Wang, Zhijian Ou, Yong He, Akinori Kawamura category:cs.CL cs.LG stat.ML published:2016-03-30 summary:The dominant language models (LMs) such as n-gram and neural network (NN)models represent sentence probabilities in terms of conditionals. In contrast,a new trans-dimensional random field (TRF) LM has been recently introduced toshow superior performances, where the whole sentence is modeled as a randomfield. In this paper, we further develop the TDF LMs with two technicalimprovements, which are a new method of exploiting Hessian information inparameter optimization to further enhance the convergence of the trainingalgorithm and an enabling method for training the TRF LMs on large corpus whichmay contain rare very long sentences. Experiments show that the TRF LMs canscale to using training data of up to 32 million words, consistently achieve10% relative perplexity reductions over 5-gram LMs, and perform as good as NNLMs but with much faster speed in calculating sentence probabilities.
arxiv-17100-42 | Exploiting Facial Landmarks for Emotion Recognition in the Wild | http://arxiv.org/abs/1603.09129 | author:Matthew Day category:cs.CV I.2.10 published:2016-03-30 summary:In this paper, we describe an entry to the third Emotion Recognition in theWild Challenge, EmotiW2015. We detail the associated experiments and show that,through more accurately locating the facial landmarks, and considering only thedistances between them, we can achieve a surprising level of performance. Theresulting system is not only more accurate than the challenge baseline, butalso much simpler.
arxiv-17100-43 | Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs | http://arxiv.org/abs/1603.09320 | author:Yu. A. Malkov, D. A. Yashunin category:cs.DS cs.CV cs.IR cs.SI published:2016-03-30 summary:We present a new algorithm for the approximate nearest neighbor search basedon navigable small world graphs with controllable hierarchy (Hierarchical NSW)admitting simple insertion, deletion and K-nearest neighbor queries. TheHierarchical NSW is a fully graph-based approach without a need for additionalsearch structures (such as kd-trees or Cartesian concatenation) typically usedat coarse search stage of the most proximity graph techniques. The algorithmincrementally builds a layered structure consisting from hierarchical set ofproximity graphs (layers) for nested subsets of the stored elements. Themaximum layer in which an element is present is selected randomly withexponentially decaying probability distribution. This allows producing graphssimilar to the previously studied Navigable Small World (NSW) structures whileadditionally having the links separated by their characteristic distancescales. Starting search from the upper layer instead of random seeds togetherwith utilizing the scale separation boosts the performance compared to the NSWand allows a logarithmic complexity scaling. Additional employment of a simpleheuristic for selecting proximity graph neighbors increases performance at highrecall and in case of highly clustered data. Performance evaluation on a largenumber of datasets has demonstrated that the proposed general metric spacemethod is able to strongly outperform many previous state-of-art vector-onlyapproaches such as FLANN, FALCONN and Annoy. Similarity of the algorithm to awell-known 1D skip list structure allows straightforward efficient and balanceddistributed implementation.
arxiv-17100-44 | Learning Local Descriptors by Optimizing the Keypoint-Correspondence Criterion | http://arxiv.org/abs/1603.09095 | author:Nenad Markuš, Igor S. Pandžić, Jörgen Ahlberg category:cs.CV published:2016-03-30 summary:Current best local descriptors are learned on a large dataset of matching andnon-matching keypoint pairs. However, data of this kind is not always availablesince detailed keypoint correspondences can be hard to establish. On the otherhand, we can often obtain labels for pairs of keypoint bags. For example,keypoint bags extracted from two images of the same object under differentviews form a matching pair, and keypoint bags extracted from images ofdifferent objects form a non-matching pair. On average, matching pairs shouldcontain more corresponding keypoints than non-matching pairs. We describe anend-to-end differentiable architecture that enables the learning of localkeypoint descriptors from such weakly-labeled data.
arxiv-17100-45 | Dataflow Matrix Machines as a Generalization of Recurrent Neural Networks | http://arxiv.org/abs/1603.09002 | author:Michael Bukatin, Steve Matthews, Andrey Radul category:cs.NE published:2016-03-29 summary:Dataflow matrix machines are a powerful generalization of recurrent neuralnetworks. They work with multiple types of arbitrary linear streams, multipletypes of powerful neurons, and allow to incorporate higher-order constructions.We expect them to be useful in machine learning and probabilistic programming,and in the synthesis of dynamic systems and of deterministic and probabilisticprograms.
arxiv-17100-46 | COCO: The Experimental Procedure | http://arxiv.org/abs/1603.08776 | author:Nikolaus Hansen, Tea Tusar, Olaf Mersmann, Anne Auger, Dimo Brockhoff category:cs.AI cs.NE published:2016-03-29 summary:We present a budget-free experimental setup and procedure for benchmarkingnumericaloptimization algorithms in a black-box scenario. This procedure can beapplied with the COCO benchmarking platform. We describe initialization of andinput to the algorithm and touch upon therelevance of termination and restarts.
arxiv-17100-47 | Towards Understanding Sparse Filtering: A Theoretical Perspective | http://arxiv.org/abs/1603.08831 | author:Fabio Massimo Zennaro, Ke Chen category:cs.LG published:2016-03-29 summary:In this paper we present our study on a recent and effective algorithm forunsupervised learning, that is, sparse filtering. The aim of this research isnot to show whether or how well sparse filtering works, but to understand whyand when sparse filtering does work. We provide a thorough study of thisalgorithm through a conceptual evaluation of feature distribution learning, atheoretical analysis of the properties of sparse filtering, and an experimentalvalidation of our conclusions. We argue that sparse filtering works byexplicitly maximizing the informativeness of the learned representation throughthe maximization of the proxy of sparsity, and by implicitly preservinginformation conveyed by the distribution of the original data through theconstraint of structure preservation. In particular, we prove that sparsefiltering preserves the cosine neighborhoodness of the data. We validate ourstatements on artificial and real data sets by applying our theoreticalunderstanding to the explanation of the success of sparse filtering onreal-world problems. Our work provides a strong theoretical framework forunderstanding sparse filtering, it highlights assumptions and conditions forsuccess behind the algorithm, and it provides a fresh insight into developingnew feature distribution learning algorithms.
arxiv-17100-48 | Spectral M-estimation with Applications to Hidden Markov Models | http://arxiv.org/abs/1603.08815 | author:Dustin Tran, Minjae Kim, Finale Doshi-Velez category:stat.CO cs.LG stat.ME published:2016-03-29 summary:Method of moment estimators exhibit appealing statistical properties, such asasymptotic unbiasedness, for nonconvex problems. However, they typicallyrequire a large number of samples and are extremely sensitive to modelmisspecification. In this paper, we apply the framework of M-estimation todevelop both a generalized method of moments procedure and a principled methodfor regularization. Our proposed M-estimator obtains optimal sample efficiencyrates (in the class of moment-based estimators) and the same well-known rateson prediction accuracy as other spectral estimators. It also makes itstraightforward to incorporate regularization into the sample momentconditions. We demonstrate empirically the gains in sample efficiency from ourapproach on hidden Markov models.
arxiv-17100-49 | Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community | http://arxiv.org/abs/1603.08832 | author:Ethan Fast, Tina Vachovsky, Michael S. Bernstein category:cs.CL cs.SI published:2016-03-29 summary:Imagine a princess asleep in a castle, waiting for her prince to slay thedragon and rescue her. Tales like the famous Sleeping Beauty clearly divide upgender roles. But what about more modern stories, borne of a generationincreasingly aware of social constructs like sexism and racism? Do thesestories tend to reinforce gender stereotypes, or counter them? In this paper,we present a technique that combines natural language processing with acrowdsourced lexicon of stereotypes to capture gender biases in fiction. Weapply this technique across 1.8 billion words of fiction from the Wattpadonline writing community, investigating gender representation in stories, howmale and female characters behave and are described, and how authors' use ofgender stereotypes is associated with the community's ratings. We find thatmale over-representation and traditional gender stereotypes (e.g., dominant menand submissive women) are common throughout nearly every genre in our corpus.However, only some of these stereotypes, like sexual or violent men, areassociated with highly rated stories. Finally, despite women often being thetarget of negative stereotypes, female authors are equally likely to write suchstereotypes as men.
arxiv-17100-50 | Online Rules for Control of False Discovery Rate and False Discovery Exceedance | http://arxiv.org/abs/1603.09000 | author:Adel Javanmard, Andrea Montanari category:math.ST cs.LG stat.AP stat.ME stat.ML stat.TH published:2016-03-29 summary:Multiple hypothesis testing is a core problem in statistical inference andarises in almost every scientific field. Given a set of null hypotheses$\mathcal{H}(n) = (H_1,\dotsc, H_n)$, Benjamini and Hochberg introduced thefalse discovery rate (FDR), which is the expected proportion of false positivesamong rejected null hypotheses, and proposed a testing procedure that controlsFDR below a pre-assigned significance level. Nowadays FDR is the criterion ofchoice for large scale multiple hypothesis testing. In this paper we considerthe problem of controlling FDR in an "online manner". Concretely, we consideran ordered --possibly infinite-- sequence of null hypotheses $\mathcal{H} =(H_1,H_2,H_3,\dots )$ where, at each step $i$, the statistician must decidewhether to reject hypothesis $H_i$ having access only to the previousdecisions. This model was introduced by Foster and Stine. We study a class of"generalized alpha-investing" procedures and prove that any rule in this classcontrols online FDR, provided $p$-values corresponding to true nulls areindependent from the other $p$-values. (Earlier work only established mFDRcontrol.) Next, we obtain conditions under which generalized alpha-investingcontrols FDR in the presence of general $p$-values dependencies. Finally, wedevelop a modified set of procedures that also allow to control the falsediscovery exceedance (the tail of the proportion of false discoveries).Numerical simulations and analytical results indicate that online procedures donot incur a large loss in statistical power with respect to offline approaches,such as Benjamini-Hochberg.
arxiv-17100-51 | Revisiting Semi-Supervised Learning with Graph Embeddings | http://arxiv.org/abs/1603.08861 | author:Zhilin Yang, William Cohen, Ruslan Salakhutdinov category:cs.LG published:2016-03-29 summary:We present a semi-supervised learning framework based on graph embeddings.Given a graph between instances, we train an embedding for each instance tojointly predict the class label and the neighborhood context in the graph. Wedevelop both transductive and inductive variants of our method. In thetransductive variant of our method, the class labels are determined by both thelearned embeddings and input feature vectors, while in the inductive variant,the embeddings are defined as a parametric function of the feature vectors, sopredictions can be made on instances not seen during training. On a large anddiverse set of benchmark tasks, including text classification, distantlysupervised entity extraction, and entity classification, we show improvedperformance over many of the existing models.
arxiv-17100-52 | Locally Epistatic Models for Genome-wide Prediction and Association by Importance Sampling | http://arxiv.org/abs/1603.08813 | author:Deniz Akdemir, Jean-Luc Jannink category:stat.AP q-bio.QM stat.ML published:2016-03-29 summary:In statistical genetics an important task involves building predictive modelsfor the genotype-phenotype relationships and thus attribute a proportion of thetotal phenotypic variance to the variation in genotypes. Numerous models havebeen proposed to incorporate additive genetic effects into models forprediction or association. However, there is a scarcity of models that canadequately account for gene by gene or other forms of genetical interactions.In addition, there is an increased interest in using marker annotations ingenome-wide prediction and association. In this paper, we discuss an hybridmodeling methodology which combines the parametric mixed modeling approach andthe non-parametric rule ensembles. This approach gives us a flexible class ofmodels that can be used to capture additive, locally epistatic genetic effects,gene x background interactions and allows us to incorporate one or moreannotations into the genomic selection or association models. We use benchmarkdata sets covering a range of organisms and traits in addition to simulateddata sets to illustrate the strengths of this approach. The improvement ofmodel accuracies and association results suggest that a part of the "missingheritability" in complex traits can be captured by modeling local epistasis.
arxiv-17100-53 | Scalable Solution for Approximate Nearest Subspace Search | http://arxiv.org/abs/1603.08810 | author:Masakazu Iwamura, Masataka Konishi, Koichi Kise category:cs.CV published:2016-03-29 summary:Finding the nearest subspace is a fundamental problem and influential to manyapplications. In particular, a scalable solution that is fast and accurate fora large problem has a great impact. The existing methods for the problem are,however, useless in a large-scale problem with a large number of subspaces andhigh dimensionality of the feature space. A cause is that they are designedbased on the traditional idea to represent a subspace by a single point. Inthis paper, we propose a scalable solution for the approximate nearest subspacesearch (ANSS) problem. Intuitively, the proposed method represents a subspaceby multiple points unlike the existing methods. This makes a large-scale ANSSproblem tractable. In the experiment with 3036 subspaces in the1024-dimensional space, we confirmed that the proposed method was 7.3 timesfaster than the previous state-of-the-art without loss of accuracy.
arxiv-17100-54 | COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Setting | http://arxiv.org/abs/1603.08785 | author:Nikolaus Hansen, Anne Auger, Olaf Mersmann, Tea Tusar, Dimo Brockhoff category:cs.AI stat.ML published:2016-03-29 summary:COCO is a platform for Comparing Continuous Optimizers in a black-boxsetting. It aims at automatizing the tedious and repetitive task ofbenchmarking numerical optimization algorithms to the greatest possible extent.We present the rationals behind the development of the platform as a generalproposition for a guideline towards better benchmarking. We detail underlyingfundamental concepts of COCO such as its definition of a problem, the idea ofinstances, the relevance of target values, and runtime as central performancemeasure. Finally, we give a quick overview of the basic code structure and theavailable test suites.
arxiv-17100-55 | Regret Analysis of the Anytime Optimally Confident UCB Algorithm | http://arxiv.org/abs/1603.08661 | author:Tor Lattimore category:cs.LG math.ST stat.ML stat.TH published:2016-03-29 summary:I introduce and analyse an anytime version of the Optimally Confident UCB(OCUCB) algorithm designed for minimising the cumulative regret in finite-armedstochastic bandits with subgaussian noise. The new algorithm is simple,intuitive (in hindsight) and comes with the strongest finite-time regretguarantees for a horizon-free algorithm so far. I also show a finite-time lowerbound that nearly matches the upper bound.
arxiv-17100-56 | Machine Learning and Cloud Computing: Survey of Distributed and SaaS Solutions | http://arxiv.org/abs/1603.08767 | author:Daniel Pop category:cs.DC cs.LG published:2016-03-29 summary:Applying popular machine learning algorithms to large amounts of data raisednew challenges for the ML practitioners. Traditional ML libraries does notsupport well processing of huge datasets, so that new approaches were needed.Parallelization using modern parallel computing frameworks, such as MapReduce,CUDA, or Dryad gained in popularity and acceptance, resulting in new MLlibraries developed on top of these frameworks. We will briefly introduce themost prominent industrial and academic outcomes, such as Apache Mahout,GraphLab or Jubatus. We will investigate how cloud computing paradigm impacted the field of ML.First direction is of popular statistics tools and libraries (R system, Python)deployed in the cloud. A second line of products is augmenting existing toolswith plugins that allow users to create a Hadoop cluster in the cloud and runjobs on it. Next on the list are libraries of distributed implementations forML algorithms, and on-premise deployments of complex systems for data analyticsand data mining. Last approach on the radar of this survey is ML asSoftware-as-a-Service, several BigData start-ups (and large companies as well)already opening their solutions to the market.
arxiv-17100-57 | Towards Practical Bayesian Parameter and State Estimation | http://arxiv.org/abs/1603.08988 | author:Yusuf Bugra Erol, Yi Wu, Lei Li, Stuart Russell category:cs.AI cs.LG stat.ML published:2016-03-29 summary:Joint state and parameter estimation is a core problem for dynamic Bayesiannetworks. Although modern probabilistic inference toolkits make it relativelyeasy to specify large and practically relevant probabilistic models, the silverbullet---an efficient and general online inference algorithm for suchproblems---remains elusive, forcing users to write special-purpose code foreach application. We propose a novel blackbox algorithm -- a hybrid of particlefiltering for state variables and assumed density filtering for parametervariables. It has following advantages: (a) it is efficient due to its onlinenature, and (b) it is applicable to both discrete and continuous parameterspaces . On a variety of toy and real models, our system is able to generatemore accurate results within a fixed computation budget. This preliminaryevidence indicates that the proposed approach is likely to be of practical use.
arxiv-17100-58 | Compilation as a Typed EDSL-to-EDSL Transformation | http://arxiv.org/abs/1603.08865 | author:Emil Axelsson category:cs.CL published:2016-03-29 summary:This article is about an implementation and compilation technique that isused in RAW-Feldspar which is a complete rewrite of the Feldspar embeddeddomain-specific language (EDSL) (Axelsson et al. 2010). Feldspar is high-levelfunctional language that generates efficient C code to run on embedded targets.The gist of the technique presented in this post is the following: ratherwriting a back end that converts pure Feldspar expressions directly to C, wetranslate them to a low-level monadic EDSL. From the low-level EDSL, C code isthen generated. This approach has several advantages: 1. The translation is simpler to write than a complete C back end. 2. The translation is between two typed EDSLs, which rules out many potentialerrors. 3. The low-level EDSL is reusable and can be shared between severalhigh-level EDSLs. Although the article contains a lot of code, most of it is in fact reusable.As mentioned in Discussion, we can write the same implementation in less than50 lines of code using generic libraries that we have developed to supportFeldspar.
arxiv-17100-59 | Multi-Cue Zero-Shot Learning with Strong Supervision | http://arxiv.org/abs/1603.08754 | author:Zeynep Akata, Mateusz Malinowski, Mario Fritz, Bernt Schiele category:cs.CV published:2016-03-29 summary:Scaling up visual category recognition to large numbers of classes remainschallenging. A promising research direction is zero-shot learning, which doesnot require any training data to recognize new classes, but rather relies onsome form of auxiliary information describing the new classes. Ultimately, thismay allow to use textbook knowledge that humans employ to learn about newclasses by transferring knowledge from classes they know well. The mostsuccessful zero-shot learning approaches currently require a particular type ofauxiliary information -- namely attribute annotations performed by humans --that is not readily available for most classes. Our goal is to circumvent thisbottleneck by substituting such annotations by extracting multiple pieces ofinformation from multiple unstructured text sources readily available on theweb. To compensate for the weaker form of auxiliary information, we incorporatestronger supervision in the form of semantic part annotations on the classesfrom which we transfer knowledge. We achieve our goal by a joint embeddingframework that maps multiple text parts as well as multiple semantic parts intoa common space. Our results consistently and significantly improve on thestate-of-the-art in zero-short recognition and retrieval.
arxiv-17100-60 | Face Image Analysis using AAM, Gabor, LBP and WD features for Gender, Age, Expression and Ethnicity Classification | http://arxiv.org/abs/1604.01684 | author:N. S. Lakshmiprabha category:cs.CV published:2016-03-29 summary:The growth in electronic transactions and human machine interactions rely onthe information such as gender, age, expression and ethnicity provided by theface image. In order to obtain these information, feature extraction plays amajor role. In this paper, retrieval of age, gender, expression and raceinformation from an individual face image is analysed using different featureextraction methods. The performance of four major feature extraction methodssuch as Active Appearance Model (AAM), Gabor wavelets, Local Binary Pattern(LBP) and Wavelet Decomposition (WD) are analyzed for gender recognition, ageestimation, expression recognition and racial recognition in terms of accuracy(recognition rate), time for feature extraction, neural training and time totest an image. Each of this recognition system is compared with four featureextractors on same dataset (training and validation set) to get a betterunderstanding in its performance. Experiments carried out on FG-NET,Cohn-Kanade, PAL face database shows that each method has its own merits anddemerits. Hence it is practically impossible to define a method which is bestat all circumstances with less computational complexity. Further, a detailedcomparison of age estimation and age estimation using gender information isprovided along with a solution to overcome aging effect in case of genderrecognition. An attempt has been made in obtaining all (i.e. gender, age range,expression and ethnicity) information from a test image in a single go.
arxiv-17100-61 | Multi-Band Image Fusion Based on Spectral Unmixing | http://arxiv.org/abs/1603.08720 | author:Qi Wei, Jose Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret, Marcus Chen, Simon Godsill category:cs.CV published:2016-03-29 summary:This paper presents a multi-band image fusion algorithm based on unsupervisedspectral unmixing for combining a high-spatial low-spectral resolution imageand a low-spatial high-spectral resolution image. The widely used linearobservation model (with additive Gaussian noise) is combined with the linearspectral mixture model to form the likelihoods of the observations. Thenon-negativity and sum-to-one constraints resulting from the intrinsic physicalproperties of the abundances are introduced as prior information to regularizethis ill-posed problem. The joint fusion and unmixing problem is thenformulated as maximizing the joint posterior distribution with respect to theendmember signatures and abundance maps, This optimization problem is attackedwith an alternating optimization strategy. The two resulting sub-problems areconvex and are solved efficiently using the alternating direction method ofmultipliers. Experiments are conducted for both synthetic and semi-real data.Simulation results show that the proposed unmixing based fusion scheme improvesboth the abundance and endmember estimation comparing with the state-of-the-artjoint fusion and unmixing algorithms.
arxiv-17100-62 | Unified View of Matrix Completion under General Structural Constraints | http://arxiv.org/abs/1603.08708 | author:Suriya Gunasekar, Arindam Banerjee, Joydeep Ghosh category:stat.ML published:2016-03-29 summary:In this paper, we present a unified analysis of matrix completion undergeneral low-dimensional structural constraints induced by {\em any} normregularization. We consider two estimators for the general problem ofstructured matrix completion, and provide unified upper bounds on the samplecomplexity and the estimation error. Our analysis relies on results fromgeneric chaining, and we establish two intermediate results of independentinterest: (a) in characterizing the size or complexity of low dimensionalsubsets in high dimensional ambient space, a certain partial complexity measureencountered in the analysis of matrix completion problems is characterized interms of a well understood complexity measure of Gaussian widths, and (b) it isshown that a form of restricted strong convexity holds for matrix completionproblems under general norm regularization. Further, we provide severalnon-trivial examples of structures included in our framework, notably therecently proposed spectral $k$-support norm.
arxiv-17100-63 | ROOT13: Spotting Hypernyms, Co-Hyponyms and Randoms | http://arxiv.org/abs/1603.08705 | author:Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, Chu-Ren Huang category:cs.CL published:2016-03-29 summary:In this paper, we describe ROOT13, a supervised system for the classificationof hypernyms, co-hyponyms and random words. The system relies on a RandomForest algorithm and 13 unsupervised corpus-based features. We evaluate it witha 10-fold cross validation on 9,600 pairs, equally distributed among the threeclasses and involving several Parts-Of-Speech (i.e. adjectives, nouns andverbs). When all the classes are present, ROOT13 achieves an F1 score of 88.3%,against a baseline of 57.6% (vector cosine). When the classification is binary,ROOT13 achieves the following results: hypernyms-co-hyponyms (93.4% vs. 60.2%),hypernymsrandom (92.3% vs. 65.5%) and co-hyponyms-random (97.3% vs. 81.5%). Ourresults are competitive with stateof-the-art models.
arxiv-17100-64 | A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity | http://arxiv.org/abs/1603.08868 | author:Ildikó Pilán, Sowmya Vajjala, Elena Volodina category:cs.CL published:2016-03-29 summary:Corpora and web texts can become a rich language learning resource if we havea means of assessing whether they are linguistically appropriate for learnersat a given proficiency level. In this paper, we aim at addressing this issue bypresenting the first approach for predicting linguistic complexity for Swedishsecond language learning material on a 5-point scale. After showing that thetraditional Swedish readability measure, L\"asbarhetsindex (LIX), is notsuitable for this task, we propose a supervised machine learning model, basedon a range of linguistic features, that can reliably classify texts accordingto their difficulty level. Our model obtained an accuracy of 81.3% and anF-score of 0.8, which is comparable to the state of the art in English and isconsiderably higher than previously reported results for other languages. Wefurther studied the utility of our features with single sentences instead offull texts since sentences are a common linguistic unit in language learningexercises. We trained a separate model on sentence-level data with fiveclasses, which yielded 63.4% accuracy. Although this is lower than the documentlevel performance, we achieved an adjacent accuracy of 92%. Furthermore, wefound that using a combination of different features, compared to using lexicalfeatures alone, resulted in 7% improvement in classification accuracy at thesentence level, whereas at the document level, lexical features were moredominant. Our models are intended for use in a freely accessible web-basedlanguage learning platform for the automatic generation of exercises.
arxiv-17100-65 | SMASH: Data-driven Reconstruction of Physically Valid Collisions | http://arxiv.org/abs/1603.08984 | author:Aron Monszpart, Nils Thuerey, Niloy J. Mitra category:cs.GR cs.CV published:2016-03-29 summary:Collision sequences are commonly used in games and entertainment to add dramaand excitement. Authoring even two body collisions in real world can bedifficult as one has to get timing and the object trajectories to be correctlysynchronized. After trial-and-error iterations, when objects can actually bemade to collide, then they are difficult to acquire in 3D. In contrast,synthetically generating plausible collisions is difficult as it requiresadjusting different collision parameters (e.g., object mass ratio, coefficientof restitution, etc.) and appropriate initial parameters. We present SMASH todirectly `read off' appropriate collision parameters simply based on inputvideo recordings. Specifically, we describe how to use laws of rigid bodycollision to regularize the problem of lifting 2D annotated poses to 3Dreconstruction of collision sequences. The reconstructed sequences can then bemodified and combined to easily author novel and plausible collision sequences.We demonstrate the system on various complex collision sequences.
arxiv-17100-66 | Interpretability of Multivariate Brain Maps in Brain Decoding: Definition and Quantification | http://arxiv.org/abs/1603.08704 | author:Seyed Mostafa Kia category:stat.ML cs.LG published:2016-03-29 summary:Brain decoding is a popular multivariate approach for hypothesis testing inneuroimaging. It is well known that the brain maps derived from weights oflinear classifiers are hard to interpret because of high correlations betweenpredictors, low signal to noise ratios, and the high dimensionality ofneuroimaging data. Therefore, improving the interpretability of brain decodingapproaches is of primary interest in many neuroimaging studies. Despiteextensive studies of this type, at present, there is no formal definition forinterpretability of multivariate brain maps. As a consequence, there is noquantitative measure for evaluating the interpretability of different braindecoding methods. In this paper, first, we present a theoretical definition ofinterpretability in brain decoding; we show that the interpretability ofmultivariate brain maps can be decomposed into their reproducibility andrepresentativeness. Second, as an application of the proposed theoreticaldefinition, we formalize a heuristic method for approximating theinterpretability of multivariate brain maps in a binary magnetoencephalography(MEG) decoding scenario. Third, we propose to combine the approximatedinterpretability and the performance of the brain decoding model into a newmulti-objective criterion for model selection. Our results for the MEG datashow that optimizing the hyper-parameters of the regularized linear classifierbased on the proposed criterion results in more informative multivariate brainmaps. More importantly, the presented definition provides the theoreticalbackground for quantitative evaluation of interpretability, and hence,facilitates the development of more effective brain decoding algorithms in thefuture.
arxiv-17100-67 | Nine Features in a Random Forest to Learn Taxonomical Semantic Relations | http://arxiv.org/abs/1603.08702 | author:Enrico Santus, Alessandro Lenci, Tin-Shing Chiu, Qin Lu, Chu-Ren Huang category:cs.CL published:2016-03-29 summary:ROOT9 is a supervised system for the classification of hypernyms, co-hyponymsand random words that is derived from the already introduced ROOT13 (Santus etal., 2016). It relies on a Random Forest algorithm and nine unsupervisedcorpus-based features. We evaluate it with a 10-fold cross validation on 9,600pairs, equally distributed among the three classes and involving severalParts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes arepresent, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2%(vector cosine). When the classification is binary, ROOT9 achieves thefollowing results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%,hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. Inorder to compare the performance with the state-of-the-art, we have alsoevaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that itis in fact competitive. Finally, we investigated whether the system learns thesemantic relation or it simply learns the prototypical hypernyms, as claimed byLevy et al. (2015). The second possibility seems to be the most likely, eventhough ROOT9 can be trained on negative examples (i.e., switched hypernyms) todrastically reduce this bias.
arxiv-17100-68 | What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL Datasets | http://arxiv.org/abs/1603.08701 | author:Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, Chu-Ren Huang category:cs.CL published:2016-03-29 summary:In this paper, we claim that Vector Cosine, which is generally considered oneof the most efficient unsupervised measures for identifying word similarity inVector Space Models, can be outperformed by a completely unsupervised measurethat evaluates the extent of the intersection among the most associatedcontexts of two target words, weighting such intersection according to the rankof the shared contexts in the dependency ranked lists. This claim comes fromthe hypothesis that similar words do not simply occur in similar contexts, butthey share a larger portion of their most relevant contexts compared to otherrelated words. To prove it, we describe and evaluate APSyn, a variant ofAverage Precision that, independently of the adopted parameters, outperformsthe Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In thebest setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracyin the TOEFL dataset, beating therefore the non-English US college applicants(whose average, as reported in the literature, is 64.50%) and severalstate-of-the-art approaches.
arxiv-17100-69 | Fusing Face and Periocular biometrics using Canonical correlation analysis | http://arxiv.org/abs/1604.01683 | author:N. S. Lakshmiprabha category:cs.CV published:2016-03-29 summary:This paper presents a novel face and periocular biometric fusion at featurelevel using canonical correlation analysis. Face recognition itself haslimitations such as illumination, pose, expression, occlusion etc. Also,periocular biometrics has spectacles, head angle, hair and expression as itslimitations. Unimodal biometrics cannot surmount all these limitations. Therecognition accuracy can be increased by fusing dual information (face andperiocular) from a single source (face image) using canonical correlationanalysis (CCA). This work also proposes a new wavelet decomposed local binarypattern (WD-LBP) feature extractor which provides sufficient features forfusion. A detailed analysis on face and periocular biometrics shows that WD-LBPfeatures are more accurate and faster than local binary pattern (LBP) and gaborwavelet. The experimental results using Muct face database reveals that theproposed multimodal biometrics performs better than the unimodal biometrics.
arxiv-17100-70 | Learning to Refine Object Segments | http://arxiv.org/abs/1603.08695 | author:Pedro O. Pinheiro, Tsung-Yi Lin, Ronan Collobert, Piotr Dollàr category:cs.CV published:2016-03-29 summary:Object segmentation requires both object-level information and low-levelpixel data. This presents a challenge for feedforward networks: lower layers inconvolutional nets capture rich spatial information, while upper layers encodeobject-level knowledge but are invariant to factors such as pose andappearance. In this work we propose to augment feedforward nets for objectsegmentation with a novel top-down refinement approach. The resultingbottom-up/top-down architecture is capable of efficiently generatinghigh-fidelity object masks. Similarly to skip connections, our approachleverages features at all layers of the net. Unlike skip connections, ourapproach does not attempt to output independent predictions at each layer.Instead, we first output a coarse `mask encoding' in a feedforward pass, thenrefine this mask encoding in a top-down pass utilizing features at successivelylower layers. The approach is simple, fast, and effective. Building on therecent DeepMask network for generating object proposals, we show accuracyimprovements of 10-20% in average recall for various setups and for smallobjects we improve recall by nearly 2 times. Additionally, by optimizing theoverall network architecture, our approach, which we call SharpMask, is 50\%faster than the original DeepMask network (under .8s per image).
arxiv-17100-71 | Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments | http://arxiv.org/abs/1603.08594 | author:Geetanjali Rakshit, Sagar Sontakke, Pushpak Bhattacharyya, Gholamreza Haffari category:cs.CL published:2016-03-29 summary:In this paper, we attempt to solve the problem of Prepositional Phrase (PP)attachments in English. The motivation for the work comes from NLP applicationslike Machine Translation, for which, getting the correct attachment ofprepositions is very crucial. The idea is to correct the PP-attachments for asentence with the help of alignments from parallel data in another language.The novelty of our work lies in the formulation of the problem into a dualdecomposition based algorithm that enforces agreement between the parse treesfrom two languages as a constraint. Experiments were performed on theEnglish-Hindi language pair and the performance improved by 10% over thebaseline, where the baseline is the attachment predicted by the MSTParser modeltrained for English.
arxiv-17100-72 | The Conditional Lucas & Kanade Algorithm | http://arxiv.org/abs/1603.08597 | author:Chen-Hsuan Lin, Rui Zhu, Simon Lucey category:cs.CV published:2016-03-29 summary:The Lucas & Kanade (LK) algorithm is the method of choice for efficient denseimage and object alignment. The approach is efficient as it attempts to modelthe connection between appearance and geometric displacement through a linearrelationship that assumes independence across pixel coordinates. A drawback ofthe approach, however, is its generative nature. Specifically, its performanceis tightly coupled with how well the linear model can synthesize appearancefrom geometric displacement, even though the alignment task itself isassociated with the inverse problem. In this paper, we present a new approach,referred to as the Conditional LK algorithm, which: (i) directly learns linearmodels that predict geometric displacement as a function of appearance, and(ii) employs a novel strategy for ensuring that the generative pixelindependence assumption can still be taken advantage of. We demonstrate thatour approach exhibits superior performance to classical generative forms of theLK algorithm. Furthermore, we demonstrate its comparable performance tostate-of-the-art methods such as the Supervised Descent Method withsubstantially less training examples, as well as the unique ability to "swap"geometric warp functions without having to retrain from scratch. Finally, froma theoretical perspective, our approach hints at possible redundancies thatexist in current state-of-the-art methods for alignment that could be leveragedin vision systems of the future.
arxiv-17100-73 | Classiffication-based Financial Markets Prediction using Deep Neural Networks | http://arxiv.org/abs/1603.08604 | author:Matthew Dixon, Diego Klabjan, Jin Hoon Bang category:cs.LG cs.CE published:2016-03-29 summary:Deep neural networks (DNNs) are powerful types of artificial neural networks(ANNs) that use several hidden layers. They have recently gained considerableattention in the speech transcription and image recognition community(Krizhevsky et al., 2012) for their superior predictive properties includingrobustness to overfitting. However their application to algorithmic trading hasnot been previously researched, partly because of their computationalcomplexity. This paper describes the application of DNNs to predictingfinancial market movement directions. In particular we describe theconfiguration and training approach and then demonstrate their application tobacktesting a simple trading strategy over 43 different Commodity and FX futuremid-prices at 5-minute intervals. All results in this paper are generated usinga C++ implementation on the Intel Xeon Phi co-processor which is 11.4x fasterthan the serial version and a Python strategy backtesting environment both ofwhich are available as open source code written by the authors.
arxiv-17100-74 | Adaptive Computation Time for Recurrent Neural Networks | http://arxiv.org/abs/1603.08983 | author:Alex Graves category:cs.NE published:2016-03-29 summary:This paper introduces Adaptive Computation Time (ACT), an algorithm thatallows recurrent neural networks to learn how many computational steps to takebetween receiving an input and emitting an output. ACT requires minimal changesto the network architecture, is deterministic and differentiable, and does notadd any noise to the parameter gradients. Experimental results are provided forfour synthetic problems: determining the parity of binary vectors, applyingbinary logic operations, adding integers, and sorting real numbers. Overall,performance is dramatically improved by the use of ACT, which successfullyadapts the number of computational steps to the requirements of the problem. Wealso present character-level language modelling results on the Hutter prizeWikipedia dataset. In this case ACT does not yield large gains in performance;however it does provide intriguing insight into the structure of the data, withmore computation allocated to harder-to-predict transitions, such as spacesbetween words and ends of sentences. This suggests that ACT or other adaptivecomputation methods could provide a generic method for inferring segmentboundaries in sequence data.
arxiv-17100-75 | A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data | http://arxiv.org/abs/1603.08884 | author:Adam Trischler, Zheng Ye, Xingdi Yuan, Jing He, Phillip Bachman, Kaheer Suleman category:cs.CL I.2.7 published:2016-03-29 summary:Understanding unstructured text is a major goal within natural languageprocessing. Comprehension tests pose questions based on short text passages toevaluate such understanding. In this work, we investigate machine comprehensionon the challenging {\it MCTest} benchmark. Partly because of its limited size,prior work on {\it MCTest} has focused mainly on engineering better features.We tackle the dataset with a neural approach, harnessing simple neural networksarranged in a parallel hierarchy. The parallel hierarchy enables our model tocompare the passage, question, and answer from a variety of trainableperspectives, as opposed to using a manually designed, rigid feature set.Perspectives range from the word level to sentence fragments to sequences ofsentences; the networks operate only on word-embedding representations of text.When trained with a methodology designed to help cope with limited trainingdata, our Parallel-Hierarchical model sets a new state of the art for {\itMCTest}, outperforming previous feature-engineered approaches slightly andprevious neural approaches by a significant margin (over 15\% absolute).
arxiv-17100-76 | Submodular Variational Inference for Network Reconstruction | http://arxiv.org/abs/1603.08616 | author:Lin Chen, Amin Karbasi, Forrest W Crawford category:cs.LG cs.DS cs.SI stat.ML published:2016-03-29 summary:In real-world and online social networks, individuals receive and transmitinformation in real time. Cascading information transmissions --- phone calls,text messages, social media posts --- may be understood as a realization of adiffusion process operating on the network, and its branching path can berepresented by a directed tree. One important feature of dynamic real-worlddiffusion processes is that the process may not traverse every edge in thenetwork on which it operates. When the network itself is unknown, the path ofthe diffusion process may reveal some, but not all, of the edges connectingnodes that have received the diffusing information. The networkreconstruction/inference problem is to estimate connections that are notrevealed by the diffusion processes. This problem naturally arises in a manydisciplines. Most of existing works on network reconstruction study thisproblem by deriving a likelihood function for the realized diffusion processgiven full knowledge of the network on which it operates, and attempting tofind the network topology that maximizes this likelihood. The major challengein this work is the intractability of the optimization problem. In this paper,we focus on the network reconstruction problem for a broad class of real-worlddiffusion processes, exemplified by a network diffusion scheme calledrespondent-driven sampling (RDS) that is widely used in epidemiology. We provethat under a reasonable model of network diffusion, the likelihood of anobserved RDS realization is a Bayesian log-submodular model. We propose anovel, accurate, and computationally efficient variational inference algorithmfor the network reconstruction problem under this model. In this algorithm, weallow for more flexibility for the possible deviation of the subjects' reportedtotal degrees in the underlying graphical structure from the true ones.
arxiv-17100-77 | Detecting weak changes in dynamic events over networks | http://arxiv.org/abs/1603.08981 | author:Shuang Li, Yao Xie, Mehrdad Farajtabar, Le Song category:cs.LG stat.ML published:2016-03-29 summary:Large volume of event data are becoming increasingly available in a widevariety of applications, such as social network analysis, Internet trafficmonitoring and healthcare analytics. Event data are observed irregularly incontinuous time, and the precise time interval between two events carries agreat deal of information about the dynamics of the underlying systems. How todetect changes in these systems as quickly as possible based on such eventdata? In this paper, we present a novel online detection algorithm for highdimensional event data over networks. Our method is based on a likelihood ratiotest for point processes, and achieve weak signal detection by aggregatinglocal statistics over time and networks. We also design an online algorithm forefficiently updating the statistics using an EM-like algorithm, and derivehighly accurate theoretical characterization of the false-alarm-rate. Wedemonstrate the good performance of our algorithm via numerical examples andreal-world twitter and memetracker datasets.
arxiv-17100-78 | Classification of Alzheimer's Disease using fMRI Data and Deep Learning Convolutional Neural Networks | http://arxiv.org/abs/1603.08631 | author:Saman Sarraf, Ghassem Tofighi category:cs.CV published:2016-03-29 summary:Over the past decade, machine learning techniques especially predictivemodeling and pattern recognition in biomedical sciences from drug deliverysystem to medical imaging has become one of the important methods which areassisting researchers to have deeper understanding of entire issue and to solvecomplex medical problems. Deep learning is power learning machine learningalgorithm in classification while extracting high-level features. In thispaper, we used convolutional neural network to classify Alzheimer's brain fromnormal healthy brain. The importance of classifying this kind of medical datais to potentially develop a predict model or system in order to recognize thetype disease from normal subjects or to estimate the stage of the disease.Classification of clinical data such as Alzheimer's disease has been alwayschallenging and most problematic part has been always selecting the mostdiscriminative features. Using Convolutional Neural Network (CNN) and thefamous architecture LeNet-5, we successfully classified functional MRI data ofAlzheimer's subjects from normal controls where the accuracy of test data ontrained data reached 96.85%. This experiment suggests us the shift and scaleinvariant features extracted by CNN followed by deep learning classification ismost powerful method to distinguish clinical data from healthy data in fMRI.This approach also enables us to expand our methodology to predict morecomplicated systems.
arxiv-17100-79 | Towards an Automated Requirements-driven Development of Smart Cyber-Physical Systems | http://arxiv.org/abs/1603.08636 | author:Jiri Vinarek, Petr Hnetynka category:cs.SE cs.CL published:2016-03-29 summary:The Invariant Refinement Method for Self Adaptation (IRM-SA) is a designmethod targeting development of smart Cyber-Physical Systems (sCPS). It allowsfor a systematic translation of the system requirements into the systemarchitecture expressed as an ensemble-based component system (EBCS). However,since the requirements are captured using natural language, there exists thedanger of their misinterpretation due to natural language requirements'ambiguity, which could eventually lead to design errors. Thus, automation andvalidation of the design process is desirable. In this paper, we (i) analyzethe translation process of natural language requirements into the IRM-SA model,(ii) identify individual steps that can be automated and/or validated usingnatural language processing techniques, and (iii) propose suitable methods.
arxiv-17100-80 | Learning a Predictable and Generative Vector Representation for Objects | http://arxiv.org/abs/1603.08637 | author:Rohit Girdhar, David F. Fouhey, Mikel Rodriguez, Abhinav Gupta category:cs.CV published:2016-03-29 summary:What is a good vector representation of an object? We believe that it shouldbe generative in 3D, in the sense that it can produce new 3D objects; as wellas be predictable from 2D, in the sense that it can be perceived from 2Dimages. We propose a novel architecture, called the TL-embedding network, tolearn an embedding space with these properties. The network consists of twocomponents: (a) an autoencoder that ensures the representation is generative;and (b) a convolutional network that ensures the representation is predictable.This enables tackling a number of tasks including voxel prediction from 2Dimages and 3D model retrieval. Extensive experimental analysis demonstrates theusefulness and versatility of this embedding.
arxiv-17100-81 | FAST: Free Adaptive Super-Resolution via Transfer for Compressed Videos | http://arxiv.org/abs/1603.08968 | author:Zhengdong Zhang, Vivienne Sze category:cs.CV published:2016-03-29 summary:High resolution displays are increasingly popular, requiring most of theexisting video content to be adapted to higher resolution. State-of-the-artsuper-resolution algorithms mainly address the visual quality of the outputinstead of real-time throughput. This paper introduces FAST, a framework toaccelerate any image based super-resolution algorithm running on compressedvideos. FAST leverages the similarity between adjacent frames in a video. Giventhe output of a super-resolution algorithm on one frame, the techniqueadaptively transfers it to the adjacent frames and skips running thesuper-resolution algorithm. The transferring process has negligible computationcost because the required information, including motion vectors, block size,and prediction residual, are embedded in the compressed video for free. In thiswork, we show that FAST accelerates state-of-the-art super-resolutionalgorithms by up to an order of magnitude with acceptable quality loss of up to0.2 dB. Thus, we believe that the FAST framework is an important step towardsenabling real-time super-resolution algorithms that upsample streamed videosfor large displays.
arxiv-17100-82 | Instance-sensitive Fully Convolutional Networks | http://arxiv.org/abs/1603.08678 | author:Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun category:cs.CV published:2016-03-29 summary:Fully convolutional networks (FCNs) have been proven very successful forsemantic segmentation, but the FCN outputs are unaware of object instances. Inthis paper, we develop FCNs that are capable of proposing instance-levelsegment candidates. In contrast to the previous FCN that generates one scoremap, our FCN is designed to compute a small set of instance-sensitive scoremaps, each of which is the outcome of a pixel-wise classifier of a relativeposition to instances. On top of these instance-sensitive score maps, a simpleassembling module is able to output instance candidate at each position. Incontrast to the recent DeepMask method for segmenting instances, our methoddoes not have any high-dimensional layer related to the mask resolution, butinstead exploits image local coherence for estimating instances. We presentcompetitive results of instance segment proposal on both PASCAL VOC and MSCOCO.
arxiv-17100-83 | Latent Embeddings for Zero-shot Classification | http://arxiv.org/abs/1603.08895 | author:Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, Bernt Schiele category:cs.CV published:2016-03-29 summary:We present a novel latent embedding model for learning a compatibilityfunction between image and class embeddings, in the context of zero-shotclassification. The proposed method augments the state-of-the-art bilinearcompatibility model by incorporating latent variables. Instead of learning asingle bilinear map, it learns a collection of maps with the selection, ofwhich map to use, being a latent variable for the current image-class pair. Wetrain the model with a ranking based objective function which penalizesincorrect rankings of the true class for a given image. We empiricallydemonstrate that our model improves the state-of-the-art for various classembeddings consistently on three challenging publicly available datasets forthe zero-shot setting. Moreover, our method leads to visually highlyinterpretable results with clear clusters of different fine-grained objectproperties that correspond to different latent variable maps.
arxiv-17100-84 | Sweep Distortion Removal from THz Images via Blind Demodulation | http://arxiv.org/abs/1604.03426 | author:Alireza Aghasi, Barmak Heshmat, Albert Redo-Sanchez, Justin Romberg, Ramesh Raskar category:cs.CV physics.optics published:2016-03-29 summary:Heavy sweep distortion induced by alignments and inter-reflections of layersof a sample is a major burden in recovering 2D and 3D information in timeresolved spectral imaging. This problem cannot be addressed by conventionaldenoising and signal processing techniques as it heavily depends on the physicsof the acquisition. Here we propose and implement an algorithmic frameworkbased on low-rank matrix recovery and alternating minimization that exploitsthe forward model for THz acquisition. The method allows recovering theoriginal signal in spite of the presence of temporal-spatial distortions. Weaddress a blind-demodulation problem, where based on several observations ofthe sample texture modulated by an undesired sweep pattern, the two classes ofsignals are separated. The performance of the method is examined in bothsynthetic and experimental data, and the successful reconstructions aredemonstrated. The proposed general scheme can be implemented to advanceinspection and imaging applications in THz and other time-resolved sensingmodalities.
arxiv-17100-85 | Cross-modal Supervision for Learning Active Speaker Detection in Video | http://arxiv.org/abs/1603.08907 | author:Punarjay Chakravarty, Tinne Tuytelaars category:cs.CV published:2016-03-29 summary:In this paper, we show how to use audio to supervise the learning of activespeaker detection in video. Voice Activity Detection (VAD) guides the learningof the vision-based classifier in a weakly supervised manner. The classifieruses spatio-temporal features to encode upper body motion - facial expressionsand gesticulations associated with speaking. We further improve a generic modelfor active speaker detection by learning person specific models. Finally, wedemonstrate the online adaptation of generic models learnt on one dataset, topreviously unseen people in a new dataset, again using audio (VAD) for weaksupervision. The use of temporal continuity overcomes the lack of cleantraining data. We are the first to present an active speaker detection systemthat learns on one audio-visual dataset and automatically adapts to speakers ina new dataset. This work can be seen as an example of how the availability ofmulti-modal data allows us to learn a model without the need for supervision,by transferring knowledge from one modality to another.
arxiv-17100-86 | Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints | http://arxiv.org/abs/1603.08887 | author:Greg Durrett, Taylor Berg-Kirkpatrick, Dan Klein category:cs.CL published:2016-03-29 summary:We present a discriminative model for single-document summarization thatintegrally combines compression and anaphoricity constraints. Our model selectstextual units to include in the summary based on a rich set of sparse featureswhose weights are learned on a large corpus. We allow for the deletion ofcontent within a sentence when that deletion is licensed by compression rules;in our framework, these are implemented as dependencies between subsententialunits of text. Anaphoricity constraints then improve cross-sentence coherenceby guaranteeing that, for each pronoun included in the summary, the pronoun'santecedent is included as well or the pronoun is rewritten as a full mention.When trained end-to-end, our final system outperforms prior work on both ROUGEas well as on human judgments of linguistic quality.
arxiv-17100-87 | Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs | http://arxiv.org/abs/1603.08358 | author:Siddhartha Chandra, Iasonas Kokkinos category:cs.CV cs.LG published:2016-03-28 summary:In this work we propose a combination of the Gaussian Conditional RandomField (G-CRF) with Deep Learning for the task of structured prediction. Ourmethod inherits several virtues of G-CRF and Deep Learning: (a) the structuredprediction task has a unique global optimum that is obtained exactly from thesolution of a linear system (b) structured prediction can be jointly trained inan end-to-end setting in general architectures and with arbitrary lossfunctions, (c) the pairwise terms do not have to be simple hand-craftedexpressions, as in the line of works building on the DenseCRF, but can ratherbe `discovered' from data through deep architectures - in particular we usefully convolutional networks to obtain the unary and pairwise terms of ourG-CRF. Building on standard tools from numerical analysis we develop veryefficient algorithms for inference and learning. This efficiency allows us toexplore more sophisticated architectures for structured prediction in deeplearning: we introduce multi-resolution architectures to couple informationacross scales in a joint optimization framework, yielding systematicimprovement. We demonstrate the utility of our approach on the challenging VOCPASCAL 2012 image segmentation benchmark, where an extensive ablation studyindicates substantial improvements over strong baselines.
arxiv-17100-88 | Hybrid Ant Colony Optimization in solving Multi-Skill Resource-Constrained Project Scheduling Problem | http://arxiv.org/abs/1603.08538 | author:Paweł B. Myszkowski, Marek E. Skowroński, Łukasz P. Olech, Krzysztof Oślizło category:cs.NE published:2016-03-28 summary:In this paper Hybrid Ant Colony Optimization (HAntCO) approach in solvingMulti--Skill Resource Constrained Project Scheduling Problem (MS--RCPSP) hasbeen presented. We have proposed hybrid approach that links classical heuristicpriority rules for project scheduling with Ant Colony Optimization (ACO).Furthermore, a novel approach for updating pheromone value has been proposed,based on both the best and worst solutions stored by ants. The objective ofthis paper is to research the usability and robustness of ACO and its hybridswith priority rules in solving MS--RCPSP. Experiments have been performed usingartificially created dataset instances, based on real--world ones. We publishedthose instances that can be used as a benchmark. Presented results show thatACO--based hybrid method is an efficient approach. More directed search processby hybrids makes this approach more stable and provides mostly better resultsthan classical ACO.
arxiv-17100-89 | Exploring Local Context for Multi-target Tracking in Wide Area Aerial Surveillance | http://arxiv.org/abs/1603.08592 | author:Bor-Jeng Chen, Gerard Medioni category:cs.CV published:2016-03-28 summary:Tracking many vehicles in wide coverage aerial imagery is crucial forunderstanding events in a large field of view. Most approaches aim to associatedetections from frame differencing into tracks. However, slow or stoppedvehicles result in long-term missing detections and further cause trackingdiscontinuities. Relying merely on appearance clue to recover missingdetections is difficult as targets are extremely small and in grayscale. Inthis paper, we address the limitations of detection association methods bycoupling it with a local context tracker (LCT), which does not rely on motiondetections. On one hand, our LCT learns neighboring spatial relation and trackseach target in consecutive frames using graph optimization. It takes theadvantage of context constraints to avoid drifting to nearby targets. Wegenerate hypotheses from sparse and dense flow efficiently to keep solutionstractable. On the other hand, we use detection association strategy to extractshort tracks in batch processing. We explicitly handle merged detections bygenerating additional hypotheses from them. Our evaluation on wide area aerialimagery sequences shows significant improvement over state-of-the-art methods.
arxiv-17100-90 | Generalized Exponential Concentration Inequality for Rényi Divergence Estimation | http://arxiv.org/abs/1603.08589 | author:Shashank Singh, Barnabás Póczos category:cs.IT math.IT math.ST stat.ML stat.TH published:2016-03-28 summary:Estimating divergences in a consistent way is of great importance in manymachine learning tasks. Although this is a fundamental problem in nonparametricstatistics, to the best of our knowledge there has been no finite sampleexponential inequality convergence bound derived for any divergence estimators.The main contribution of our work is to provide such a bound for an estimatorof R\'enyi-$\alpha$ divergence for a smooth H\"older class of densities on the$d$-dimensional unit cube $[0, 1]^d$. We also illustrate our theoreticalresults with a numerical experiment.
arxiv-17100-91 | Longitudinal Analysis of Discussion Topics in an Online Breast Cancer Community using Convolutional Neural Networks | http://arxiv.org/abs/1603.08458 | author:Shaodian Zhang, Edouard Grave, Elizabeth Sklar, Noemie Elhadad category:cs.CL cs.CY cs.SI published:2016-03-28 summary:Identifying topics of discussions in online health communities (OHC) iscritical to various applications, but can be difficult because topics of OHCcontent are usually heterogeneous and domain-dependent. In this paper, weprovide a multi-class schema, an annotated dataset, and supervised classifiersbased on convolutional neural network (CNN) and other models for the task ofclassifying discussion topics. We apply the CNN classifier to the most popularbreast cancer online community, and carry out a longitudinal analysis to showtopic distributions and topic changes throughout members' participation. Ourexperimental results suggest that CNN outperforms other classifiers in the taskof topic classification, and that certain trajectories can be detected withrespect to topic changes.
arxiv-17100-92 | Exponential Concentration of a Density Functional Estimator | http://arxiv.org/abs/1603.08584 | author:Shashank Singh, Barnabás P óczos category:math.ST cs.IT math.IT stat.ML stat.TH published:2016-03-28 summary:We analyze a plug-in estimator for a large class of integral functionals ofone or more continuous probability densities. This class includes importantfamilies of entropy, divergence, mutual information, and their conditionalversions. For densities on the $d$-dimensional unit cube $[0,1]^d$ that lie ina $\beta$-H\"older smoothness class, we prove our estimator converges at therate $O \left( n^{-\frac{\beta}{\beta + d}} \right)$. Furthermore, we prove theestimator is exponentially concentrated about its mean, whereas most previousrelated results have proven only expected error bounds on estimators.
arxiv-17100-93 | Exclusivity Regularized Machine | http://arxiv.org/abs/1603.08318 | author:Xiaojie Guo category:cs.LG published:2016-03-28 summary:It has been recognized that the diversity of base learners is of utmostimportance to a good ensemble. This paper defines a novel measurement ofdiversity, termed as exclusivity. With the designed exclusivity, we furtherpropose an ensemble model, namely Exclusivity Regularized Machine (ERM), tojointly suppress the training error of ensemble and enhance the diversitybetween bases. Moreover, an Augmented Lagrange Multiplier based algorithm iscustomized to effectively and efficiently seek the optimal solution of ERM.Theoretical analysis on convergence and global optimality of the proposedalgorithm, as well as experiments are provided to reveal the efficacy of ourmethod and show its superiority over state-of-the-art alternatives in terms ofaccuracy and efficiency.
arxiv-17100-94 | Analysis of k-Nearest Neighbor Distances with Application to Entropy Estimation | http://arxiv.org/abs/1603.08578 | author:Shashank Singh, Barnabás Póczos category:math.ST cs.IT math.IT stat.ML stat.TH published:2016-03-28 summary:Estimating entropy and mutual information consistently is important for manymachine learning applications. The Kozachenko-Leonenko (KL) estimator(Kozachenko & Leonenko, 1987) is a widely used nonparametric estimator for theentropy of multivariate continuous random variables, as well as the basis ofthe mutual information estimator of Kraskov et al. (2004), perhaps the mostwidely used estimator of mutual information in this setting. Despite thepractical importance of these estimators, major theoretical questions regardingtheir finite-sample behavior remain open. This paper proves finite-samplebounds on the bias and variance of the KL estimator, showing that it achievesthe minimax convergence rate for certain classes of smooth functions. Inproving these bounds, we analyze finite-sample behavior of k-nearest neighbors(k-NN) distance statistics (on which the KL estimator is based). We deriveconcentration inequalities for k-NN distances and a general expectation boundfor statistics of k-NN distances, which may be useful for other analyses ofk-NN methods.
arxiv-17100-95 | Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation | http://arxiv.org/abs/1603.08564 | author:Satrajit Mukherjee, Bodhisattwa Prasad Majumder, Aritran Piplai, Swagatam Das category:cs.CV stat.ML published:2016-03-28 summary:The paper proposes a novel Kernelized image segmentation scheme for noisyimages that utilizes the concept of Smallest Univalue Segment AssimilatingNucleus (SUSAN) and incorporates spatial constraints by computing circularcolour map induced weights. Fuzzy damping coefficients are obtained for eachnucleus or center pixel on the basis of the corresponding weighted SUSAN areavalues, the weights being equal to the inverse of the number of horizontal andvertical moves required to reach a neighborhood pixel from the center pixel.These weights are used to vary the contributions of the different nuclei in theKernel based framework. The paper also presents an edge quality metric obtainedby fuzzy decision based edge candidate selection and final computation of theblurriness of the edges after their selection. The inability of existingalgorithms to preserve edge information and structural details in theirsegmented maps necessitates the computation of the edge quality factor (EQF)for all the competing algorithms. Qualitative and quantitative analysis havebeen rendered with respect to state-of-the-art algorithms and for images riddenwith varying types of noises. Speckle noise ridden SAR images and Rician noiseridden Magnetic Resonance Images have also been considered for evaluating theeffectiveness of the proposed algorithm in extracting important segmentationinformation.
arxiv-17100-96 | Unsupervised Learning using Sequential Verification for Action Recognition | http://arxiv.org/abs/1603.08561 | author:Ishan Misra, C. Lawrence Zitnick, Martial Hebert category:cs.CV cs.AI cs.LG published:2016-03-28 summary:In this paper, we consider the problem of learning a visual representationfrom the raw spatiotemporal signals in videos for use in action recognition.Our representation is learned without supervision from semantic labels. Weformulate it as an unsupervised sequential verification task, i.e., wedetermine whether a sequence of frames from a video is in the correct temporalorder. With this simple task and no semantic labels, we learn a powerfulunsupervised representation using a Convolutional Neural Network (CNN). Therepresentation contains complementary information to that learned fromsupervised image datasets like ImageNet. Qualitative results show that ourmethod captures information that is temporally varying, such as human pose.When used as pre-training for action recognition, our method gives significantgains over learning without external data on benchmark datasets like UCF101 andHMDB51. Our method can also be combined with supervised representations toprovide an additional boost in accuracy for action recognition. Finally, toquantify its sensitivity to human pose, we show results for human poseestimation on the FLIC dataset that are competitive with approaches usingsignificantly more supervised training data.
arxiv-17100-97 | Genetic cellular neural networks for generating three-dimensional geometry | http://arxiv.org/abs/1603.08551 | author:Hugo Martay category:cs.NE cs.GR 92B20 published:2016-03-28 summary:There are a number of ways to procedurally generate interestingthree-dimensional shapes, and a method where a cellular neural network iscombined with a mesh growth algorithm is presented here. The aim is to create ashape from a genetic code in such a way that a crude search can findinteresting shapes. Identical neural networks are placed at each vertex of amesh which can communicate with neural networks on neighboring vertices. Theoutput of the neural networks determine how the mesh grows, allowinginteresting shapes to be produced emergently, mimicking some of the complexityof biological organism development. Since the neural networks' parameters canbe freely mutated, the approach is amenable for use in a genetic algorithm.
arxiv-17100-98 | Colorful Image Colorization | http://arxiv.org/abs/1603.08511 | author:Richard Zhang, Phillip Isola, Alexei A. Efros category:cs.CV published:2016-03-28 summary:Given a grayscale photograph as input, this paper attacks the problem ofhallucinating a {\em plausible} color version of the photograph. This problemis clearly underconstrained, so previous approaches have either relied onsignificant user interaction or resulted in desaturated colorizations. Wepropose a fully automatic approach that produces vibrant and realisticcolorizations. We embrace the underlying uncertainty of the problem by posingit as a classification task and explore using class-rebalancing at trainingtime to increase the diversity of colors in the result. The system isimplemented as a feed-forward operation in a CNN at test time and is trained onover a million color images. We evaluate our algorithm using a "colorizationTuring test", asking human subjects to choose between a generated and groundtruth color image. Our method successfully fools humans 20\% of the time,significantly higher than previous methods.
arxiv-17100-99 | Generating Visual Explanations | http://arxiv.org/abs/1603.08507 | author:Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, Trevor Darrell category:cs.CV cs.AI cs.CL published:2016-03-28 summary:Clearly explaining a rationale for a classification decision to an end-usercan be as important as the decision itself. Existing approaches for deep visualrecognition are generally opaque and do not output any justification text;contemporary vision-language models can describe image content but fail to takeinto account class-discriminative image aspects which justify visualpredictions. We propose a new model that focuses on the discriminatingproperties of the visible object, jointly predicts a class label, and explainswhy the predicted label is appropriate for the image. We propose a novel lossfunction based on sampling and reinforcement learning that learns to generatesentences that realize a global sentence property, such as class specificity.Our results on a fine-grained bird species classification dataset show that ourmodel is able to generate explanations which are not only consistent with animage but also more discriminative than descriptions produced by existingcaptioning methods.
arxiv-17100-100 | Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation | http://arxiv.org/abs/1603.08486 | author:Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina Demner-Fushman, Jianhua Yao, Ronald M Summers category:cs.CV published:2016-03-28 summary:Despite the recent advances in automatically describing image contents, theirapplications have been mostly limited to image caption datasets containingnatural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deeplearning model to efficiently detect a disease from an image and annotate itscontexts (e.g., location, severity and the affected organs). We employ apublicly available radiology dataset of chest x-rays and their reports, and useits image annotations to mine disease names to train convolutional neuralnetworks (CNNs). In doing so, we adopt various regularization techniques tocircumvent the large normal-vs-diseased cases bias. Recurrent neural networks(RNNs) are then trained to describe the contexts of a detected disease, basedon the deep CNN features. Moreover, we introduce a novel approach to use theweights of the already trained pair of CNN/RNN on the domain-specificimage/text dataset, to infer the joint image/text contexts for composite imagelabeling. Significantly improved image annotation results are demonstratedusing the recurrent neural cascade model by taking the joint image/textcontexts into account.
arxiv-17100-101 | Estimating Mixture Models via Mixtures of Polynomials | http://arxiv.org/abs/1603.08482 | author:Sida I. Wang, Arun Tejasvi Chaganty, Percy Liang category:stat.ML cs.LG published:2016-03-28 summary:Mixture modeling is a general technique for making any simple model moreexpressive through weighted combination. This generality and simplicity in partexplains the success of the Expectation Maximization (EM) algorithm, in whichupdates are easy to derive for a wide class of mixture models. However, thelikelihood of a mixture model is non-convex, so EM has no known globalconvergence guarantees. Recently, method of moments approaches offer globalguarantees for some mixture models, but they do not extend easily to the rangeof mixture models that exist. In this work, we present Polymom, an unifyingframework based on method of moments in which estimation procedures are easilyderivable, just as in EM. Polymom is applicable when the moments of a singlemixture component are polynomials of the parameters. Our key observation isthat the moments of the mixture model are a mixture of these polynomials, whichallows us to cast estimation as a Generalized Moment Problem. We solve itsrelaxations using semidefinite optimization, and then extract parameters usingideas from computer algebra. This framework allows us to draw insights andapply tools from convex optimization, computer algebra and the theory ofmoments to study problems in statistical estimation.
arxiv-17100-102 | Audio Visual Emotion Recognition with Temporal Alignment and Perception Attention | http://arxiv.org/abs/1603.08321 | author:Linlin Chao, Jianhua Tao, Minghao Yang, Ya Li, Zhengqi Wen category:cs.CV cs.CL cs.LG published:2016-03-28 summary:This paper focuses on two key problems for audio-visual emotion recognitionin the video. One is the audio and visual streams temporal alignment forfeature level fusion. The other one is locating and re-weighting the perceptionattentions in the whole audio-visual stream for better recognition. The LongShort Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the mainclassification architecture. Firstly, soft attention mechanism aligns the audioand visual streams. Secondly, seven emotion embedding vectors, which arecorresponding to each classification emotion type, are added to locate theperception attentions. The locating and re-weighting process is also based onthe soft attention mechanism. The experiment results on EmotiW2015 dataset andthe qualitative analysis show the efficiency of the proposed two techniques.
arxiv-17100-103 | Deep Embedding for Spatial Role Labeling | http://arxiv.org/abs/1603.08474 | author:Oswaldo Ludwig, Xiao Liu, Parisa Kordjamshidi, Marie-Francine Moens category:cs.CL cs.CV cs.LG cs.NE published:2016-03-28 summary:This paper introduces the visually informed embedding of word (VIEW), acontinuous vector representation for a word extracted from a deep neural modeltrained using the Microsoft COCO data set to forecast the spatial arrangementsbetween visual objects, given a textual description. The model is composed of adeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory(LSTM) network, the latter being preceded by an embedding layer. The VIEW isapplied to transferring multimodal background knowledge to Spatial RoleLabeling (SpRL) algorithms, which recognize spatial relations between objectsmentioned in the text. This work also contributes with a new method to selectcomplementary features and a fine-tuning method for MLP that improves the $F1$measure in classifying the words into spatial roles. The VIEW is evaluated withthe Task 3 of SemEval-2013 benchmark data set, SpaceEval.
arxiv-17100-104 | Sparse Activity and Sparse Connectivity in Supervised Learning | http://arxiv.org/abs/1603.08367 | author:Markus Thom, Günther Palm category:cs.LG cs.CG cs.CV cs.NE published:2016-03-28 summary:Sparseness is a useful regularizer for learning in a wide range ofapplications, in particular in neural networks. This paper proposes a modeltargeted at classification tasks, where sparse activity and sparse connectivityare used to enhance classification capabilities. The tool for achieving this isa sparseness-enforcing projection operator which finds the closest vector witha pre-defined sparseness for any given vector. In the theoretical part of thispaper, a comprehensive theory for such a projection is developed. Inconclusion, it is shown that the projection is differentiable almost everywhereand can thus be implemented as a smooth neuronal transfer function. The entiremodel can hence be tuned end-to-end using gradient-based methods. Experimentson the MNIST database of handwritten digits show that classificationperformance can be boosted by sparse activity or sparse connectivity. With acombination of both, performance can be significantly better compared toclassical non-sparse approaches.
arxiv-17100-105 | Hierarchical Gaussian Mixture Model with Objects Attached to Terminal and Non-terminal Dendrogram Nodes | http://arxiv.org/abs/1603.08342 | author:Łukasz P. Olech, Mariusz Paradowski category:cs.LG cs.CV published:2016-03-28 summary:A hierarchical clustering algorithm based on Gaussian mixture model ispresented. The key difference to regular hierarchical mixture models is theability to store objects in both terminal and nonterminal nodes. Upper levelsof the hierarchy contain sparsely distributed objects, while lower levelscontain densely represented ones. As it was shown by experiments, this abilityhelps in noise detection (modelling). Furthermore, compared to regularhierarchical mixture model, the presented method generates more compactdendrograms with higher quality measured by adopted F-measure.
arxiv-17100-106 | Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing | http://arxiv.org/abs/1603.08270 | author:Steven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy, Alexander Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, Dharmendra S. Modha category:cs.NE published:2016-03-28 summary:Deep networks are now able to achieve human-level performance on a broadspectrum of recognition tasks. Independently, neuromorphic computing has nowdemonstrated unprecedented energy-efficiency through a new chip architecturebased on spiking neurons, low precision synapses, and a scalable communicationnetwork. Here, we demonstrate that neuromorphic computing, despite its novelarchitectural primitives, can implement deep convolution networks that i)approach state-of-the-art classification accuracy across 8 standard datasets,encompassing vision and speech, ii) perform inference while preserving thehardware's underlying energy-efficiency and high throughput, running on theaforementioned datasets at between 1100 and 2300 frames per second and usingbetween 25 and 325 mW (effectively > 5000 frames / sec / W) and iii) can bespecified and trained using backpropagation with the same ease-of-use ascontemporary deep learning. For the first time, the algorithmic power of deeplearning can be merged with the efficiency of neuromorphic processors, bringingthe promise of embedded, intelligent, brain-inspired computing one step closer.
arxiv-17100-107 | Continuous Stereo Matching using Local Expansion Moves | http://arxiv.org/abs/1603.08328 | author:Tatsunori Taniai, Yasuyuki Matsushita, Yoichi Sato, Takeshi Naemura category:cs.CV published:2016-03-28 summary:We present an accurate and efficient stereo matching method using localexpansion moves, a new move making scheme using graph cuts. The local expansionmoves are presented as many alpha-expansions defined for small grid regions.The local expansion moves extend the traditional expansion moves by two ways:localization and spatial propagation. By localization, we use differentcandidate alpha-labels according to the locations of local alpha-expansions. Byspatial propagation, we design our local alpha-expansions to propagatecurrently assigned labels for nearby regions. With this localization andspatial propagation, our method can efficiently infer Markov random fieldmodels with a huge or continuous label space using a randomized search scheme.Our local expansion move method has several advantages over previous approachesthat are based on fusion moves or belief propagation; it produces submodularmoves deriving a subproblem optimality; it helps find good, smooth, piecewiselinear disparity maps; it is suitable for parallelization; it can usecost-volume filtering techniques for accelerating the matching costcomputations. Our method is evaluated using the Middlebury stereo benchmark andshown to have the best performance in sub-pixel accuracy.
arxiv-17100-108 | Non-Greedy L21-Norm Maximization for Principal Component Analysis | http://arxiv.org/abs/1603.08293 | author:Feiping Nie, Heng Huang category:cs.LG published:2016-03-28 summary:Principal Component Analysis (PCA) is one of the most important unsupervisedmethods to handle high-dimensional data. However, due to the high computationalcomplexity of its eigen decomposition solution, it hard to apply PCA to thelarge-scale data with high dimensionality. Meanwhile, the squared L2-norm basedobjective makes it sensitive to data outliers. In recent research, the L1-normmaximization based PCA method was proposed for efficient computation and beingrobust to outliers. However, this work used a greedy strategy to solve theeigen vectors. Moreover, the L1-norm maximization based objective may not bethe correct robust PCA formulation, because it loses the theoretical connectionto the minimization of data reconstruction error, which is one of the mostimportant intuitions and goals of PCA. In this paper, we propose to maximizethe L21-norm based robust PCA objective, which is theoretically connected tothe minimization of reconstruction error. More importantly, we propose theefficient non-greedy optimization algorithms to solve our objective and themore general L21-norm maximization problem with theoretically guaranteedconvergence. Experimental results on real world data sets show theeffectiveness of the proposed method for principal component analysis.
arxiv-17100-109 | Hierarchy of Groups Evaluation Using Different F-score Variants | http://arxiv.org/abs/1603.08323 | author:Michał Spytkowski, Łukasz P. Olech, Halina Kwaśnicka category:cs.CV published:2016-03-28 summary:The paper presents a cursory examination of clustering, focusing on a rarelyexplored field of hierarchy of clusters. Based on this, a short discussion ofclustering quality measures is presented and the F-score measure is examinedmore deeply. As there are no attempts to assess the quality for hierarchies ofclusters, three variants of the F-Score based index are presented: classic,hierarchical and partial order. The partial order index is the authors'approach to the subject. Conducted experiments show the properties of theconsidered measures. In conclusions, the strong and weak sides of each variantare presented.
arxiv-17100-110 | Attend, Infer, Repeat: Fast Scene Understanding with Generative Models | http://arxiv.org/abs/1603.08575 | author:S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, Koray Kavukcuoglu, Geoffrey E. Hinton category:cs.CV cs.LG published:2016-03-28 summary:We present a framework for efficient inference in structured image modelsthat explicitly reason about objects. We achieve this by performingprobabilistic inference using a recurrent neural network that attends to sceneelements and processes them one at a time. Crucially, the model itself learnsto choose the appropriate number of inference steps. We use this scheme tolearn to perform inference in partially specified 2D models (variable-sizedvariational auto-encoders) and fully specified 3D models (probabilisticrenderers). We show that such models learn to identify multiple objects -counting, locating and classifying the elements of a scene - without anysupervision, e.g., decomposing 3D images with various numbers of objects in asingle forward pass of a neural network. We further show that the networksproduce accurate inferences when compared to supervised counterparts, and thattheir structure leads to improved generalization.
arxiv-17100-111 | Negative Learning Rates and P-Learning | http://arxiv.org/abs/1603.08253 | author:Devon Merrill category:cs.AI cs.LG published:2016-03-27 summary:We present a method of training a differentiable function approximator for aregression task using negative examples. We effect this training using negativelearning rates. We also show how this method can be used to perform directpolicy learning in a reinforcement learning setting.
arxiv-17100-112 | Regularization Parameter Selection for a Bayesian Multi-Level Group Lasso Regression Model with Application to Imaging Genomics | http://arxiv.org/abs/1603.08163 | author:Farouk S. Nathoo, Keelin Greenlaw, Mary Lesperance category:stat.ML stat.AP stat.CO published:2016-03-27 summary:We investigate the choice of tuning parameters for a Bayesian multi-levelgroup lasso model developed for the joint analysis of neuroimaging and geneticdata. The regression model we consider relates multivariate phenotypesconsisting of brain summary measures (volumetric and cortical thickness values)to single nucleotide polymorphism (SNPs) data and imposes penalization at twonested levels, the first corresponding to genes and the second corresponding toSNPs. Associated with each level in the penalty is a tuning parameter whichcorresponds to a hyperparameter in the hierarchical Bayesian formulation.Following previous work on Bayesian lassos we consider the estimation of tuningparameters through either hierarchical Bayes based on hyperpriors and Gibbssampling or through empirical Bayes based on maximizing the marginal likelihoodusing a Monte Carlo EM algorithm. For the specific model under consideration wefind that these approaches can lead to severe overshrinkage of the regressionparameter estimates in the high-dimensional setting or when the genetic effectsare weak. We demonstrate these problems through simulation examples and studyan approximation to the marginal likelihood which sheds light on the cause ofthis problem. We then suggest an alternative approach based on the widelyapplicable information criterion (WAIC), an asymptotic approximation toleave-one-out cross-validation that can be computed conveniently within an MCMCframework.
arxiv-17100-113 | 3DMatch: Learning the Matching of Local 3D Geometry in Range Scans | http://arxiv.org/abs/1603.08182 | author:Andy Zeng, Shuran Song, Matthias Nießner, Matthew Fisher, Jianxiong Xiao category:cs.CV published:2016-03-27 summary:Establishing correspondences between 3D geometries is essential to a largevariety of graphics and vision applications, including 3D reconstruction,localization, and shape matching. Despite significant progress, geometricmatching on real-world 3D data is still a challenging task due to the noisy,low-resolution, and incomplete nature of scanning data. These difficultieslimit the performance of current state-of-art methods which are typically basedon histograms over geometric properties. In this paper, we introduce 3DMatch, adata-driven local feature learner that jointly learns a geometric featurerepresentation and an associated metric function from a large collection ofreal-world scanning data. We represent 3D geometry using accumulated distancefields around key-point locations. This representation is suited to handlenoisy and partial scanning data, and concurrently supports deep learning withconvolutional neural networks directly in 3D. To train the networks, we proposea way to automatically generate correspondence labels for deep learning byleveraging existing RGB-D reconstruction algorithms. In our results, wedemonstrate that we are able to outperform state-of-the-art approaches by asignificant margin. In addition, we show the robustness of our descriptor in apurely geometric sparse bundle adjustment pipeline for 3D reconstruction.
arxiv-17100-114 | VolumeDeform: Real-time Volumetric Non-rigid Reconstruction | http://arxiv.org/abs/1603.08161 | author:Matthias Innmann, Michael Zollhöfer, Matthias Nießner, Christian Theobalt, Marc Stamminger category:cs.CV published:2016-03-27 summary:We present a novel approach for the reconstruction of dynamic geometricshapes using a single hand-held consumer-grade RGB-D sensor at real-time rates.Our method does not require a pre-defined shape template to start with andbuilds up the scene model from scratch during the scanning process. Geometryand motion are parameterized in a unified manner by a volumetric representationthat encodes a distance field of the surface geometry as well as the non-rigidspace deformation. Motion tracking is based on a set of extracted sparse colorfeatures in combination with a dense depth-based constraint formulation. Thisenables accurate tracking and drastically reduces drift inherent to standardmodel-to-depth alignment. We cast finding the optimal deformation of space as anon-linear regularized variational optimization problem by enforcing localsmoothness and proximity to the input constraints. The problem is tackled inreal-time at the camera's capture rate using a data-parallel flip-flopoptimization strategy. Our results demonstrate robust tracking even for fastmotion and scenes that lack geometric features.
arxiv-17100-115 | Recurrent Mixture Density Network for Spatiotemporal Visual Attention | http://arxiv.org/abs/1603.08199 | author:Loris Bazzani, Hugo Larochelle, Lorenzo Torresani category:cs.CV published:2016-03-27 summary:The high-dimensional and redundant nature of video have pushed researchers toseek the design of attentional models that can dynamically focus computationson the spatiotemporal volumes that are most relevant. Specifically, thesemodels have been used to eliminate or down-weight background pixels that arenot important for the task at hand. In order to deal with this problem, wepropose an attentional model that learns where to look in a video directly fromhuman fixation data. The proposed model leverages deep 3D convolutionalfeatures to represent clip segments in videos. This clip-level representationis aggregated over time by a long short-term memory network that connects intoa mixture density network model of the likely positions of fixations in eachframe. The resulting model is trained end to end using backpropagation. Ourexperiments show state-of-the-art performance on saliency prediction forvideos. Experiments on Hollywood2 and UCF101 also show that the saliency can beused to improve classification accuracy on action recognition tasks.
arxiv-17100-116 | Perceptual Losses for Real-Time Style Transfer and Super-Resolution | http://arxiv.org/abs/1603.08155 | author:Justin Johnson, Alexandre Alahi, Li Fei-Fei category:cs.CV cs.LG published:2016-03-27 summary:We consider image transformation problems, where an input image istransformed into an output image. Recent methods for such problems typicallytrain feed-forward convolutional neural networks using a \emph{per-pixel} lossbetween the output and ground-truth images. Parallel work has shown thathigh-quality images can be generated by defining and optimizing\emph{perceptual} loss functions based on high-level features extracted frompretrained networks. We combine the benefits of both approaches, and proposethe use of perceptual loss functions for training feed-forward networks forimage transformation tasks. We show results on image style transfer, where afeed-forward network is trained to solve the optimization problem proposed byGatys et al in real-time. Compared to the optimization-based method, ournetwork gives similar qualitative results but is three orders of magnitudefaster. We also experiment with single-image super-resolution, where replacinga per-pixel loss with a perceptual loss gives visually pleasing results.
arxiv-17100-117 | Human Pose Estimation using Deep Consensus Voting | http://arxiv.org/abs/1603.08212 | author:Ita Lifshitz, Ethan Fetaya, Shimon Ullman category:cs.CV cs.LG published:2016-03-27 summary:In this paper we consider the problem of human pose estimation from a singlestill image. We propose a novel approach where each location in the image votesfor the position of each keypoint using a convolutional neural net. The votingscheme allows us to utilize information from the whole image, rather than relyon a sparse set of keypoint locations. Using dense, multi-target votes, notonly produces good keypoint predictions, but also enables us to computeimage-dependent joint keypoint probabilities by looking at consensus voting.This differs from most previous methods where joint probabilities are learnedfrom relative keypoint locations and are independent of the image. We finallycombine the keypoints votes and joint probabilities in order to identify theoptimal pose configuration. We show our competitive performance on the MPIIHuman Pose and Leeds Sports Pose datasets.
arxiv-17100-118 | Towards Machine Intelligence | http://arxiv.org/abs/1603.08262 | author:Kamil Rocki category:cs.AI cs.LG cs.NE published:2016-03-27 summary:There exists a theory of a single general-purpose learning algorithm whichcould explain the principles of its operation. This theory assumes that thebrain has some initial rough architecture, a small library of simple innatecircuits which are prewired at birth and proposes that all significant mentalalgorithms can be learned. Given current understanding and observations, thispaper reviews and lists the ingredients of such an algorithm from botharchitectural and functional perspectives.
arxiv-17100-119 | DeLight-Net: Decomposing Reflectance Maps into Specular Materials and Natural Illumination | http://arxiv.org/abs/1603.08240 | author:Stamatios Georgoulis, Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Luc Van Gool, Tinne Tuytelaars category:cs.CV published:2016-03-27 summary:In this paper we are extracting surface reflectance and natural environmentalillumination from a reflectance map, i.e. from a single 2D image of a sphere ofone material under one illumination. This is a notoriously difficult problem,yet key to various re-rendering applications. With the recent advances inestimating reflectance maps from 2D images their further decomposition hasbecome increasingly relevant. To this end, we propose a Convolutional Neural Network (CNN) architecture toreconstruct both material parameters (i.e. Phong) as well as illumination (i.e.high-resolution spherical illumination maps), that is solely trained onsynthetic data. We demonstrate that decomposition of synthetic as well as realphotographs of reflectance maps, both in High Dynamic Range (HDR), and, for thefirst time, on Low Dynamic Range (LDR) as well. Results are compared toprevious approaches quantitatively as well as qualitatively in terms ofre-renderings where illumination, material, view or shape are changed.
arxiv-17100-120 | Exact Subsampling MCMC | http://arxiv.org/abs/1603.08232 | author:Matias Quiroz, Mattias Villani, Robert Kohn category:stat.CO stat.ME stat.ML published:2016-03-27 summary:Speeding up Markov Chain Monte Carlo (MCMC) for datasets with manyobservations by data subsampling has recently received considerable attentionin the literature. Most of the proposed methods are approximate, and the onlyexact solution has been documented to be highly inefficient. We propose asimulation consistent subsampling method for estimating expectations of anyfunction of the parameters using a combination of MCMC subsampling and theimportance sampling correction for occasionally negative likelihood estimatesin Lyne et al. (2015). Our algorithm is based on first obtaining an unbiasedbut not necessarily positive estimate of the likelihood. The estimator uses asoft lower bound such that the likelihood estimate is positive with a highprobability, and computationally cheap control variables to lower variability.Second, we carry out a correlated pseudo marginal MCMC on the absolute value ofthe likelihood estimate. Third, the sign of the likelihood is corrected usingan importance sampling step that has low variance by construction. Weillustrate the usefulness of the method with two examples.
arxiv-17100-121 | Evolution of active categorical image classification via saccadic eye movement | http://arxiv.org/abs/1603.08233 | author:Randal S. Olson, Jason H. Moore, Christoph Adami category:cs.CV cs.LG cs.NE published:2016-03-27 summary:Pattern recognition and classification is a central concern for moderninformation processing systems. In particular, one key challenge to image andvideo classification has been that the computational cost of image processingscales linearly with the number of pixels in the image or video. Here wepresent an intelligent machine (the "active categorical classifier," or ACC)that is inspired by the saccadic movements of the eye, and is capable ofclassifying images by selectively scanning only a portion of the image. Weharness evolutionary computation to optimize the ACC on the MNIST hand-writtendigit classification task, and provide a proof-of-concept that the ACC works onnoisy multi-class data. We further analyze the ACC and demonstrate its abilityto classify images after viewing only a fraction of the pixels, and provideinsight on future research paths to further improve upon the ACC presentedhere.
arxiv-17100-122 | Do You See What I Mean? Visual Resolution of Linguistic Ambiguities | http://arxiv.org/abs/1603.08079 | author:Yevgeni Berzak, Andrei Barbu, Daniel Harari, Boris Katz, Shimon Ullman category:cs.CV cs.AI cs.CL published:2016-03-26 summary:Understanding language goes hand in hand with the ability to integratecomplex contextual information obtained via perception. In this work, wepresent a novel task for grounded language understanding: disambiguating asentence given a visual scene which depicts one of the possible interpretationsof that sentence. To this end, we introduce a new multimodal corpus containingambiguous sentences, representing a wide range of syntactic, semantic anddiscourse ambiguities, coupled with videos that visualize the differentinterpretations for each sentence. We address this task by extending a visionmodel which determines if a sentence is depicted by a video. We demonstrate howsuch a model can be adjusted to recognize different interpretations of the sameunderlying sentence, allowing to disambiguate sentences in a unified fashionacross the different ambiguity types.
arxiv-17100-123 | Classification of Large-Scale Fundus Image Data Sets: A Cloud-Computing Framework | http://arxiv.org/abs/1603.08071 | author:Sohini Roychowdhury category:cs.CV published:2016-03-26 summary:Large medical image data sets with high dimensionality require substantialamount of computation time for data creation and data processing. This paperpresents a novel generalized method that finds optimal image-based feature setsthat reduce computational time complexity while maximizing overallclassification accuracy for detection of diabetic retinopathy (DR). First,region-based and pixel-based features are extracted from fundus images forclassification of DR lesions and vessel-like structures. Next, feature rankingstrategies are used to distinguish the optimal classification feature sets. DRlesion and vessel classification accuracies are computed using the boosteddecision tree and decision forest classifiers in the Microsoft Azure MachineLearning Studio platform, respectively. For images from the DIARETDB1 data set,40 of its highest-ranked features are used to classify four DR lesion typeswith an average classification accuracy of 90.1% in 792 seconds. Also, forclassification of red lesion regions and hemorrhages from microaneurysms,accuracies of 85% and 72% are observed, respectively. For images from STAREdata set, 40 high-ranked features can classify minor blood vessels with anaccuracy of 83.5% in 326 seconds. Such cloud-based fundus image analysissystems can significantly enhance the borderline classification performances inautomated screening systems.
arxiv-17100-124 | Pointing the Unknown Words | http://arxiv.org/abs/1603.08148 | author:Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, Yoshua Bengio category:cs.CL cs.LG cs.NE published:2016-03-26 summary:The problem of rare and unknown words is an important issue that canpotentially effect the performance of many NLP systems, including bothtraditional count based and deep learning models. We propose a novel way todeal with the rare and unseen words for the neural network models withattention. Our model uses two softmax layers in order to predict the next wordin conditional language models: one of the softmax layers predicts the locationof a word in the source sentence, and the other softmax layer predicts a wordin the shortlist vocabulary. The decision of which softmax layer to use at eachtimestep is adaptively made by an MLP which is conditioned on the context. Wemotivate this work from a psychological evidence that humans naturally have atendency to point towards objects in the context or the environment when thename of an object is not known. Using our proposed model, we observeimprovements in two tasks, neural machine translation on the Europarl Englishto French parallel corpora and text summarization on the Gigaword dataset.
arxiv-17100-125 | Recognizing Car Fluents from Video | http://arxiv.org/abs/1603.08067 | author:Bo Li, Tianfu Wu, Caiming Xiong, Song-Chun Zhu category:cs.CV published:2016-03-26 summary:Physical fluents, a term originally used by Newton [40], refers totime-varying object states in dynamic scenes. In this paper, we are interestedin inferring the fluents of vehicles from video. For example, a door (hood,trunk) is open or closed through various actions, light is blinking to turn.Recognizing these fluents has broad applications, yet have received scantattention in the computer vision literature. Car fluent recognition entails aunified framework for car detection, car part localization and part statusrecognition, which is made difficult by large structural and appearancevariations, low resolutions and occlusions. This paper learns aspatial-temporal And-Or hierarchical model to represent car fluents. Thelearning of this model is formulated under the latent structural SVM framework.Since there are no publicly related dataset, we collect and annotate a carfluent dataset consisting of car videos with diverse fluents. In experiments,the proposed method outperforms several highly related baseline methods interms of car fluent recognition and car part localization.
arxiv-17100-126 | A generalized flow for multi-class and binary classification tasks: An Azure ML approach | http://arxiv.org/abs/1603.08070 | author:Matthew Bihis, Sohini Roychowdhury category:cs.CV published:2016-03-26 summary:The constant growth in the present day real-world databases posecomputational challenges for a single computer. Cloud-based platforms, on theother hand, are capable of handling large volumes of information manipulationtasks, thereby necessitating their use for large real-world data setcomputations. This work focuses on creating a novel Generalized Flow within thecloud-based computing platform: Microsoft Azure Machine Learning Studio (MAMLS)that accepts multi-class and binary classification data sets alike andprocesses them to maximize the overall classification accuracy. First, eachdata set is split into training and testing data sets, respectively. Then,linear and nonlinear classification model parameters are estimated using thetraining data set. Data dimensionality reduction is then performed to maximizeclassification accuracy. For multi-class data sets, data centric information isused to further improve overall classification accuracy by reducing themulti-class classification to a series of hierarchical binary classificationtasks. Finally, the performance of optimized classification model thus achievedis evaluated and scored on the testing data set. The classificationcharacteristics of the proposed flow are comparatively evaluated on 3 publicdata sets and a local data set with respect to existing state-of-the-artmethods. On the 3 public data sets, the proposed flow achieves 78-97.5%classification accuracy. Also, the local data set, created using theinformation regarding presence of Diabetic Retinopathy lesions in fundusimages, results in 85.3-95.7% average classification accuracy, which is higherthan the existing methods. Thus, the proposed generalized flow can be usefulfor a wide range of application-oriented "big data sets".
arxiv-17100-127 | On Fast Bilateral Filtering using Fourier Kernels | http://arxiv.org/abs/1603.08081 | author:Sanjay Ghosh, Kunal N. Chaudhury category:cs.CV published:2016-03-26 summary:It was demonstrated in earlier work that, by approximating its range kernelusing shiftable functions, the non-linear bilateral filter can be computedusing a series of fast convolutions. Previous approaches based on shiftableapproximation have, however, been restricted to Gaussian range kernels. In thiswork, we propose a novel approximation that can be applied to any range kernel,provided it has a pointwise-convergent Fourier series. More specifically, wepropose to approximate the Gaussian range kernel of the bilateral filter usinga Fourier basis, where the coefficients of the basis are obtained by solving aseries of least-squares problems. The coefficients can be efficiently computedusing a recursive form of the QR decomposition. By controlling the cardinalityof the Fourier basis, we can obtain a good tradeoff between the run-time andthe filtering accuracy. In particular, we are able to guarantee sub-pixelaccuracy for the overall filtering, which is not provided by most existingmethods for fast bilateral filtering. We present simulation results todemonstrate the speed and accuracy of the proposed algorithm.
arxiv-17100-128 | Online shopping behavior study based on multi-granularity opinion mining: China vs. America | http://arxiv.org/abs/1603.08089 | author:Qingqing Zhou, Rui Xia, Chengzhi Zhang category:cs.CY cs.CL cs.HC published:2016-03-26 summary:With the development of e-commerce, many products are now being soldworldwide, and manufacturers are eager to obtain a better understanding ofcustomer behavior in various regions. To achieve this goal, most previousefforts have focused mainly on questionnaires, which are time-consuming andcostly. The tremendous volume of product reviews on e-commerce websites hasseen a new trend emerge, whereby manufacturers attempt to understand userpreferences by analyzing online reviews. Following this trend, this paperaddresses the problem of studying customer behavior by exploiting recentlydeveloped opinion mining techniques. This work is novel for three reasons.First, questionnaire-based investigation is automatically enabled by employingalgorithms for template-based question generation and opinion mining-basedanswer extraction. Using this system, manufacturers are able to obtain reportsof customer behavior featuring a much larger sample size, more directinformation, a higher degree of automation, and a lower cost. Second,international customer behavior study is made easier by integrating tools formultilingual opinion mining. Third, this is the ?rst time an automaticquestionnaire investigation has been conducted to compare customer behavior inChina and America, where product reviews are written and read in Chinese andEnglish, respectively. Our study on digital cameras, smartphones, and tabletcomputers yields three ?ndings. First, Chinese customers follow the Doctrine ofthe Mean, and often use euphemistic expressions, while American customersexpress their opinions more directly. Second, Chinese customers care more aboutgeneral feelings, while American customers pay more attention to productdetails. Third, Chinese customers focus on external features, while Americancustomers care more about the internal features of products.
arxiv-17100-129 | Measuring Book Impact Based on the Multi-granularity Online Review Mining | http://arxiv.org/abs/1603.08091 | author:Qingqing Zhou, Chengzhi Zhang, Star X. Zhao, Bikun Chen category:cs.DL cs.CL published:2016-03-26 summary:As with articles and journals, the customary methods for measuring books'academic impact mainly involve citations, which is easy but limited tointerrogating traditional citation databases and scholarly book reviews,Researchers have attempted to use other metrics, such as Google Books,libcitation, and publisher prestige. However, these approaches lackcontent-level information and cannot determine the citation intentions ofusers. Meanwhile, the abundant online review resources concerning academicbooks can be used to mine deeper information and content utilizing altmetricperspectives. In this study, we measure the impacts of academic books bymulti-granularity mining online reviews, and we identify factors that affect abook's impact. First, online reviews of a sample of academic books on Amazon.cnare crawled and processed. Then, multi-granularity review mining is conductedto identify review sentiment polarities and aspects' sentiment values. Lastly,the numbers of positive reviews and negative reviews, aspect sentiment values,star values, and information regarding helpfulness are integrated via theentropy method, and lead to the calculation of the final book impact scores.The results of a correlation analysis of book impact scores obtained via ourmethod versus traditional book citations show that, although there aresubstantial differences between subject areas, online book reviews tend toreflect the academic impact. Thus, we infer that online reviews represent apromising source for mining book impact within the altmetric perspective and atthe multi-granularity content level. Moreover, our proposed method might alsobe a means by which to measure other books besides academic publications.
arxiv-17100-130 | Learning Hough Regression Models via Bridge Partial Least Squares for Object Detection | http://arxiv.org/abs/1603.08092 | author:Jianyu Tang, Hanzi Wang, Yan Yan category:cs.CV published:2016-03-26 summary:Popular Hough Transform-based object detection approaches usually constructan appearance codebook by clustering local image features. However, how tochoose appropriate values for the parameters used in the clustering stepremains an open problem. Moreover, some popular histogram features extractedfrom overlapping image blocks may cause a high degree of redundancy andmulticollinearity. In this paper, we propose a novel Hough Transform-basedobject detection approach. First, to address the above issues, we exploit aBridge Partial Least Squares (BPLS) technique to establish context-encodedHough Regression Models (HRMs), which are linear regression models that castprobabilistic Hough votes to predict object locations. BPLS is an efficientvariant of Partial Least Squares (PLS). PLS-based regression techniques(including BPLS) can reduce the redundancy and eliminate the multicollinearityof a feature set. And the appropriate value of the only parameter used in PLS(i.e., the number of latent components) can be determined by using across-validation procedure. Second, to efficiently handle object scale changes,we propose a novel multi-scale voting scheme. In this scheme, multiple Houghimages corresponding to multiple object scales can be obtained simultaneously.Third, an object in a test image may correspond to multiple true and falsepositive hypotheses at different scales. Based on the proposed multi-scalevoting scheme, a principled strategy is proposed to fuse hypotheses to reducefalse positives by evaluating normalized pointwise mutual information betweenhypotheses. In the experiments, we also compare the proposed HRM approach withits several variants to evaluate the influences of its components on itsperformance. Experimental results show that the proposed HRM approach hasachieved desirable performances on popular benchmark datasets.
arxiv-17100-131 | Blind signal separation and identification of mixtures of images | http://arxiv.org/abs/1603.08095 | author:Felipe P. do Carmo, Joaquim T. de Assis, Vania V. Estrela, Alessandra M. Coelho category:cs.CV published:2016-03-26 summary:In this paper, a fresh procedure to handle image mixtures by means of blindsignal separation relying on a combination of second order and higher orderstatistics techniques are introduced. The problem of blind signal separation isreassigned to the wavelet domain. The key idea behind this method is that theimage mixture can be decomposed into the sum of uncorrelated and/or independentsub-bands using wavelet transform. Initially, the observed image ispre-whitened in the space domain. Afterwards, an initial separation matrix isestimated from the second order statistics de-correlation model in the waveletdomain. Later, this matrix will be used as an initial separation matrix for thehigher order statistics stage in order to find the best separation matrix. Thesuggested algorithm was tested using natural images.Experiments have confirmedthat the use of the proposed process provides promising outcomes in identifyingan image from noisy mixtures of images.
arxiv-17100-132 | Unsupervised Domain Adaptation in the Wild: Dealing with Asymmetric Label Sets | http://arxiv.org/abs/1603.08105 | author:Ayush Mittal, Anant Raj, Vinay P. Namboodiri, Tinne Tuytelaars category:cs.CV published:2016-03-26 summary:The goal of domain adaptation is to adapt models learned on a source domainto a particular target domain. Most methods for unsupervised domain adaptationproposed in the literature to date, assume that the set of classes present inthe target domain is identical to the set of classes present in the sourcedomain. This is a restrictive assumption that limits the practicalapplicability of unsupervised domain adaptation techniques in real worldsettings ("in the wild"). Therefore, we relax this constraint and propose atechnique that allows the set of target classes to be a subset of the sourceclasses. This way, large publicly available annotated datasets with a widevariety of classes can be used as source, even if the actual set of classes intarget can be more limited and, maybe most importantly, unknown beforehand. To this end, we propose an algorithm that orders a set of source subspacesthat are relevant to the target classification problem. Our method then choosesa restricted set from this ordered set of source subspaces. As an extension,even starting from multiple source datasets with varied sets of categories,this method automatically selects an appropriate subset of source categoriesrelevant to a target dataset. Empirical analysis on a number of source andtarget domain datasets shows that restricting the source subspace to only asubset of categories does indeed substantially improve the eventual targetclassification accuracy over the baseline that considers all source classes.
arxiv-17100-133 | Support Driven Wavelet Frame-based Image Deblurring | http://arxiv.org/abs/1603.08108 | author:Liangtian He, Yilun Wang, Zhaoyin Xiang category:cs.CV 90C26 I.4.3 published:2016-03-26 summary:The wavelet frame systems have been playing an active role in imagerestoration and many other image processing fields over the past decades, owingto the good capability of sparsely approximating piece-wise smooth functionssuch as images. In this paper, we propose a novel wavelet frame based sparserecovery model called \textit{Support Driven Sparse Regularization} (SDSR) forimage deblurring, where the partial support information of frame coefficientsis attained via a self-learning strategy and exploited via the proposedtruncated $\ell_0$ regularization. Moreover, the state-of-the-art imagerestoration methods can be naturally incorporated into our proposed waveletframe based sparse recovery framework. In particular, in order to achievereliable support estimation of the frame coefficients, we make use of thestate-of-the-art image restoration result such as that from the IDD-BM3D methodas the initial reference image for support estimation. Our extensiveexperimental results have shown convincing improvements over existingstate-of-the-art deblurring methods.
arxiv-17100-134 | Fast and Provably Accurate Bilateral Filtering | http://arxiv.org/abs/1603.08109 | author:Kunal N. Chaudhury, Swapnil D. Dabhade category:cs.CV published:2016-03-26 summary:The bilateral filter is a non-linear filter that uses a range filter alongwith a spatial filter to perform edge-preserving smoothing of images. A directcomputation of the bilateral filter requires $O(S)$ operations per pixel, where$S$ is the size of the support of the spatial filter. In this paper, we presenta fast and provably accurate algorithm for approximating the bilateral filterwhen the range kernel is Gaussian. In particular, for box and Gaussian spatialfilters, the proposed algorithm can cut down the complexity to $O(1)$ per pixelfor any arbitrary $S$. The algorithm has a simple implementation involving$N+1$ spatial filterings, where $N$ is the approximation order. We give adetailed analysis of the filtering accuracy that can be achieved by theproposed approximation in relation to the target bilateral filter. This allowsus to to estimate the order $N$ required to obtain a given accuracy. We alsopresent comprehensive numerical results to demonstrate that the proposedalgorithm is competitive with state-of-the-art methods in terms of speed andaccuracy.
arxiv-17100-135 | Reconstructing undirected graphs from eigenspaces | http://arxiv.org/abs/1603.08113 | author:Yohann De Castro, Thibault Espinasse, Paul Rochet category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH published:2016-03-26 summary:In this paper, we aim at recovering an undirected weighted graph of $N$vertices from the knowledge of a perturbed version of the eigenspaces of itsadjacency matrix $W$. Our approach is based on minimizing a cost function givenby the Frobenius norm of the commutator $\mathsf{A} \mathsf{B}-\mathsf{B}\mathsf{A}$ between symmetric matrices $\mathsf{A}$ and $\mathsf{B}$. In the Erd\H{o}s-R\'enyi model with no self-loops, we show thatidentifiability (i.e. the ability to reconstruct $W$ from the knowledge of itseigenspaces) follows a sharp phase transition on the expected number of edgeswith threshold function $N\log N/2$. Given an estimation of the eigenspaces based on a $n$-sample, we providebackward-type support selection procedures from theoretical and practical pointof views. In particular, deleting an edge from the active support, our studyunveils that the empirical contrast is of the order of $\mathcal O(1/n)$ whenwe overestimate the true support and lower bounded by a positive constant whenthe estimated support is smaller than the true support. This feature leads to apowerful practical support estimation procedure when properly thresholding theempirical contrast. Simulated and real life numerical experiments assert ournew methodology.
arxiv-17100-136 | Dense Nonrigid Ground Truth for Optical Flow in Real-World Scenes | http://arxiv.org/abs/1603.08120 | author:Wenbin Li, Darren Cosker, Zhihan Lv, Matthew Brown category:cs.CV published:2016-03-26 summary:In this paper we present the first ground truth dataset of nonrigidlydeforming real-world scenes (both long and short video sequences) in order toquantitatively evaluate RGB based tracking and registration methods. Toconstruct ground truth for the RGB sequences, we simultaneously captureNear-Infrared (NIR) image sequences where dense markers - visible only in NIR -represent ground truth positions. This allows for comparison with automaticallytracked RGB positions and the formation of error metrics. Most previousdatasets containing nonrigidly deforming sequences are based on synthetic data.Our capture protocol enables us to acquire real-world deforming objects withrealistic photometric effects - such as blur and illumination change - as wellas occlusion and complex deformations. A public evaluation website isconstructed to allow for ranking of RGB image based optical flow and otherdense tracking algorithms, with various statistical measures. Furthermore, wepresent the first RGB-NIR multispectral optical flow model allowing for energyoptimization by combining information from both the RGB and the complementaryNIR channels. In our experiments we evaluate eight existing RGB based opticalflow methods on our new dataset. We also evaluate our multispectral opticalflow algorithm in real-world scenes by varying the input channels across RGB,NIR and RGB-NIR.
arxiv-17100-137 | Video Interpolation using Optical Flow and Laplacian Smoothness | http://arxiv.org/abs/1603.08124 | author:Wenbin Li, Darren Cosker category:cs.CV published:2016-03-26 summary:Non-rigid video interpolation is a common computer vision task. In this paperwe present an optical flow approach which adopts a Laplacian Cotangent Meshconstraint to enhance the local smoothness. Similar to Li et al., our approachadopts a mesh to the image with a resolution up to one vertex per pixel anduses angle constraints to ensure sensible local deformations between imagepairs. The Laplacian Mesh constraints are expressed wholly inside the opticalflow optimization, and can be applied in a straightforward manner to a widerange of image tracking and registration problems. We evaluate our approach bytesting on several benchmark datasets, including the Middlebury and Garg et al.datasets. In addition, we show application of our method for constructing 3DMorphable Facial Models from dynamic 3D data.
arxiv-17100-138 | A Draft Memory Model on Spiking Neural Assemblies | http://arxiv.org/abs/1603.08146 | author:João Ranhel, João H. Albuquerque, Bruno P. M. Azevedo, Nathalia M. Cunha, Pedro J. Ishimaru category:cs.NE published:2016-03-26 summary:A draft memory model (DM) for neural networks with spike propagation delay(SNNwD) is described. Novelty in this approach are that the DM learnsimmediately, with stimuli presented once, without synaptic weight changes, andwithout external learning algorithm. Basal on this model is to trap spikeswithin neural loops. In order to construct the DM we developed two functionalblocks, also described herein. The decoder block receives input from a singlespikes source and connect it to one among many outputs. The selector blockoperates in the opposite direction, receiving many spikes sources andconnecting one of them to a single output. We realized conceptual proofs bytesting the DM in the prime numbers classifying task. This activation-basedmemory can be used as immediate and short-term memory.
arxiv-17100-139 | Data-Driven Dynamic Decision Models | http://arxiv.org/abs/1603.08150 | author:John J. Nay, Jonathan M. Gilligan category:stat.ML cs.GT cs.MA cs.NE published:2016-03-26 summary:This article outlines a method for automatically generating models of dynamicdecision-making that both have strong predictive power and are interpretable inhuman terms. This is useful for designing empirically grounded agent-basedsimulations and for gaining direct insight into observed dynamic processes. Weuse an efficient model representation and a genetic algorithm-based estimationprocess to generate simple approximations that explain most of the structure ofcomplex stochastic processes. This method, implemented in C++ and R, scaleswell to large data sets. We apply our methods to empirical data from humansubjects game experiments and international relations. We also demonstrate themethod's ability to recover known data-generating processes by simulating datawith agent-based models and correctly deriving the underlying decision modelsfor multiple agent models and degrees of stochasticity.
arxiv-17100-140 | How useful is photo-realistic rendering for visual learning? | http://arxiv.org/abs/1603.08152 | author:Yair Movshovitz-Attias, Takeo Kanade, Yaser Sheikh category:cs.CV published:2016-03-26 summary:Data seems cheap to get, and in many ways it is, but the process of creatinga high quality labeled dataset from a mass of data is time-consuming andexpensive. With the advent of rich 3D repositories, photo-realistic rendering systemsoffer the opportunity to provide nearly limitless data. Yet, their primaryvalue for visual learning may be the quality of the data they can providerather than the quantity. Rendering engines offer the promise of perfect labelsin addition to the data: what the precise camera pose is; what the preciselighting location, temperature, and distribution is; what the geometry of theobject is. In this work we focus on semi-automating dataset creation through use ofsynthetic data and apply this method to an important task -- object viewpointestimation. Using state-of-the-art rendering software we generate a largelabeled dataset of cars rendered densely in viewpoint space. We investigate theeffect of rendering parameters on estimation performance and show realism isimportant. We show that generalizing from synthetic data is not harder than thedomain adaptation required between two real-image datasets and that combiningsynthetic images with a small amount of real data improves estimation accuracy.
arxiv-17100-141 | "Did I Say Something Wrong?" A Word-Level Analysis of Wikipedia Articles for Deletion Discussions | http://arxiv.org/abs/1603.08048 | author:Michael Ruster category:cs.CL cs.SI stat.ML published:2016-03-25 summary:This thesis focuses on gaining linguistic insights into textual discussionson a word level. It was of special interest to distinguish messages thatconstructively contribute to a discussion from those that are detrimental tothem. Thereby, we wanted to determine whether "I"- and "You"-messages areindicators for either of the two discussion styles. These messages are nowadaysoften used in guidelines for successful communication. Although their effectshave been successfully evaluated multiple times, a large-scale analysis hasnever been conducted. Thus, we used Wikipedia Articles for Deletion (short: AfD) discussionstogether with the records of blocked users and developed a fully automatedcreation of an annotated data set. In this data set, messages were labelledeither constructive or disruptive. We applied binary classifiers to the data todetermine characteristic words for both discussion styles. Thereby, we alsoinvestigated whether function words like pronouns and conjunctions play animportant role in distinguishing the two. We found that "You"-messages were a strong indicator for disruptive messageswhich matches their attributed effects on communication. However, we found"I"-messages to be indicative for disruptive messages as well which is contraryto their attributed effects. The importance of function words could neither beconfirmed nor refuted. Other characteristic words for either communicationstyle were not found. Yet, the results suggest that a different model mightrepresent disruptive and constructive messages in textual discussions better.
arxiv-17100-142 | An Empirical Study of Dimensional Reduction Techniques for Facial Action Units Detection | http://arxiv.org/abs/1603.08039 | author:Zhuo Hui, Wen-Sheng Chu category:cs.CV published:2016-03-25 summary:Biologically inspired features, such as Gabor filters, result in very highdimensional measurement. Does reducing the dimensionality of the feature spaceafford advantages beyond computational efficiency? Do some approaches todimensionality reduction (DR) yield improved action unit detection? To answerthese questions, we compared DR approaches in two relatively large databases ofspontaneous facial behavior (45 participants in total with over 2 minutes ofFACS-coded video per participant). Facial features were tracked and alignedusing active appearance models (AAM). SIFT and Gabor features were extractedfrom local facial regions. We compared linear (PCA and KPCA), manifold (LPP andLLE), supervised (LDA and KDA) and hybrid approaches (LSDA) to DR with respectto AU detection. For further comparison, a no-DR control condition was includedas well. Linear support vector machine classifiers with independent train andtest sets were used for AU detection. AU detection was quantified using areaunder the ROC curve and F1. Baseline results for PCA with Gabor features werecomparable with previous research. With some notable exceptions, DR improved AUdetection relative to no-DR. Locality embedding approaches proved vulnerable to\emph{out-of-sample} problems. Gradient-based SIFT lead to better AU detectionthan the filter-based Gabor features. For area under the curve, few differenceswere found between linear and other DR approaches. For F1, results were mixed.For both metrics, the pattern of results varied among action units. Thesefindings suggest that action unit detection may be optimized by using specificDR for specific action units. PCA and LDA were the most efficient approaches;KDA was the least efficient.
arxiv-17100-143 | On the Detection of Mixture Distributions with applications to the Most Biased Coin Problem | http://arxiv.org/abs/1603.08037 | author:Kevin Jamieson, Daniel Haas, Ben Recht category:cs.LG published:2016-03-25 summary:This paper studies the trade-off between two different kinds of pureexploration: breadth versus depth. The most biased coin problem asks how manytotal coin flips are required to identify a "heavy" coin from an infinite bagcontaining both "heavy" coins with mean $\theta_1 \in (0,1)$, and "light" coinswith mean $\theta_0 \in (0,\theta_1)$, where heavy coins are drawn from the bagwith probability $\alpha \in (0,1/2)$. The key difficulty of this problem liesin distinguishing whether the two kinds of coins have very similar means, orwhether heavy coins are just extremely rare. This problem has applications incrowdsourcing, anomaly detection, and radio spectrum search. Chandrasekaran et.al. (2014) recently introduced a solution to this problem but it requiredperfect knowledge of $\theta_0,\theta_1,\alpha$. In contrast, we derivealgorithms that are adaptive to partial or absent knowledge of the problemparameters. Moreover, our techniques generalize beyond coins to more generalinstances of infinitely many armed bandit problems. We also prove lower boundsthat show our algorithm's upper bounds are tight up to $\log$ factors, and onthe way characterize the sample complexity of differentiating between a singleparametric distribution and a mixture of two such distributions. As a result,these bounds have surprising implications both for solutions to the most biasedcoin problem and for anomaly detection when only partial information about theparameters is known.
arxiv-17100-144 | Universality of Mallows' and degeneracy of Kendall's kernels for rankings | http://arxiv.org/abs/1603.08035 | author:Horia Mania, Aaditya Ramdas, Martin J. Wainwright, Michael I. Jordan, Benjamin Recht category:stat.ML cs.DM cs.LG published:2016-03-25 summary:Kernel methods provide an attractive framework for aggregating and learningfrom ranking data, and so understanding the fundamental properties of kernelsover permutations is a question of broad interest. We provide a detailedanalysis of the Fourier spectra of the standard Kendall and Mallows kernels,and a new class of polynomial-type kernels. We prove that the Kendall kernelhas exactly two irreducible representations at which the Fourier transform isnon-zero, and moreover, the associated matrices are rank one. This implies thatthe Kendall kernel is nearly degenerate, with limited expressive anddiscriminative power. In sharp contrast, we prove that the Fourier transform ofthe Mallows kernel is a strictly positive definite matrix at all irreduciblerepresentations. This property guarantees that the Mallows kernel is bothcharacteristic and universal. We introduce a family of normalized polynomialkernels of degree p that interpolates between the Kendall (degree one) andMallows (infinite degree) kernels, and show that for d-dimensionalpermutations, the p-th degree kernel is characteristic when p is greater orequal than d - 1, unlike the Euclidean case in which no finite-degreepolynomial kernel is characteristic.
arxiv-17100-145 | Resnet in Resnet: Generalizing Residual Architectures | http://arxiv.org/abs/1603.08029 | author:Sasha Targ, Diogo Almeida, Kevin Lyman category:cs.LG cs.CV cs.NE stat.ML published:2016-03-25 summary:Residual networks (ResNets) have recently achieved state-of-the-art onchallenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deepdual-stream architecture that generalizes ResNets and standard CNNs and iseasily implemented with no computational overhead. RiR consistently improvesperformance over ResNets, outperforms architectures with similar amounts ofaugmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.
arxiv-17100-146 | On the Simultaneous Preservation of Privacy and Community Structure in Anonymized Networks | http://arxiv.org/abs/1603.08028 | author:Daniel Cullina, Kushagra Singhal, Negar Kiyavash, Prateek Mittal category:cs.LG cs.CR cs.SI published:2016-03-25 summary:We consider the problem of performing community detection on a network, whilemaintaining privacy, assuming that the adversary has access to an auxiliarycorrelated network. We ask the question "Does there exist a regime where thenetwork cannot be deanonymized perfectly, yet the community structure could belearned?." To answer this question, we derive information theoretic conversesfor the perfect deanonymization problem using the Stochastic Block Model andedge sub-sampling. We also provide an almost tight achievability result forperfect deanonymization. We also evaluate the performance of percolation based deanonymizationalgorithm on Stochastic Block Model data-sets that satisfy the conditions ofour converse. Although our converse applies to exact deanonymization, thealgorithm fails drastically when the conditions of the converse are met.Additionally, we study the effect of edge sub-sampling on the communitystructure of a real world dataset. Results show that the dataset falls underthe purview of the idea of this paper. There results suggest that it may bepossible to prove stronger partial deanonymizability converses, which wouldenable better privacy guarantees.
arxiv-17100-147 | On the Compression of Recurrent Neural Networks with an Application to LVCSR acoustic modeling for Embedded Speech Recognition | http://arxiv.org/abs/1603.08042 | author:Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, Ian McGraw category:cs.CL cs.LG cs.NE published:2016-03-25 summary:We study the problem of compressing recurrent neural networks (RNNs). Inparticular, we focus on the compression of RNN acoustic models, which aremotivated by the goal of building compact and accurate speech recognitionsystems which can be run efficiently on mobile devices. In this work, wepresent a technique for general recurrent model compression that jointlycompresses both recurrent and non-recurrent inter-layer weight matrices. Wefind that the proposed technique allows us to reduce the size of our LongShort-Term Memory (LSTM) acoustic model to a third of its original size withnegligible loss in accuracy.
arxiv-17100-148 | How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation | http://arxiv.org/abs/1603.08023 | author:Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin, Joelle Pineau category:cs.CL cs.AI cs.LG cs.NE published:2016-03-25 summary:We investigate evaluation metrics for end-to-end dialogue systems wheresupervised labels, such as task completion, are not available. Recent works inend-to-end dialogue systems have adopted metrics from machine translation andtext summarization to compare a model's generated response to a single targetresponse. We show that these metrics correlate very weakly or not at all withhuman judgements of the response quality in both technical and non-technicaldomains. We provide quantitative and qualitative results highlighting specificweaknesses in existing metrics, and provide recommendations for futuredevelopment of better automatic evaluation metrics for dialogue systems.
arxiv-17100-149 | Classifying Syntactic Regularities for Hundreds of Languages | http://arxiv.org/abs/1603.08016 | author:Reed Coke, Ben King, Dragomir Radev category:cs.CL published:2016-03-25 summary:This paper presents a comparison of classification methods for linguistictypology for the purpose of expanding an extensive, but sparse languageresource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath,2013). We experimented with a variety of regression and nearest-neighbormethods for use in classification over a set of 325 languages and six syntacticrules drawn from WALS. To classify each rule, we consider the typologicalfeatures of the other five rules; linguistic features extracted from aword-aligned Bible in each language; and genealogical features (genus andfamily) of each language. In general, we find that propagating the majoritylabel among all languages of the same genus achieves the best accuracy in labelpre- diction. Following this, a logistic regression model that combinestypological and linguistic features offers the next best performance.Interestingly, this model actually outperforms the majority labels among alllanguages of the same family.
arxiv-17100-150 | Quadratic Projection Based Feature Extraction with Its Application to Biometric Recognition | http://arxiv.org/abs/1603.07797 | author:Yan Yan, Hanzi Wang, Si Chen, Xiaochun Cao, David Zhang category:cs.CV published:2016-03-25 summary:This paper presents a novel quadratic projection based feature extractionframework, where a set of quadratic matrices is learned to distinguish eachclass from all other classes. We formulate quadratic matrix learning (QML) as astandard semidefinite programming (SDP) problem. However, the con- ventionalinterior-point SDP solvers do not scale well to the problem of QML forhigh-dimensional data. To solve the scalability of QML, we develop an efficientalgorithm, termed DualQML, based on the Lagrange duality theory, to extractnonlinear features. To evaluate the feasibility and effectiveness of theproposed framework, we conduct extensive experiments on biometric recognition.Experimental results on three representative biometric recogni- tion tasks,including face, palmprint, and ear recognition, demonstrate the superiority ofthe DualQML-based feature extraction algorithm compared to the currentstate-of-the-art algorithms
arxiv-17100-151 | Friction from Reflectance: Deep Reflectance Codes for Predicting Physical Surface Properties from One-Shot In-Field Reflectance | http://arxiv.org/abs/1603.07998 | author:Hang Zhang, Kristin Dana, Ko Nishino category:cs.CV published:2016-03-25 summary:Images are the standard input for vision algorithms, but one-shot infieldreflectance measurements are creating new opportunities for recognition andscene understanding. In this work, we address the question of what reflectancecan reveal about materials in an efficient manner. We go beyond the question ofrecognition and labeling and ask the question: What intrinsic physicalproperties of the surface can be estimated using reflectance? We introduce aframework that enables prediction of actual friction values for surfaces usingone-shot reflectance measurements. This work is a first of its kindvision-based friction estimation. We develop a novel representation forreflectance disks that capture partial BRDF measurements instantaneously. Ourmethod of deep reflectance codes combines CNN features and fisher vectorpooling with optimal binary embedding to create codes that have sufficientdiscriminatory power and have important properties of illumination and spatialinvariance. The experimental results demonstrate that reflectance can play anew role in deciphering the underlying physical properties of real-worldscenes.
arxiv-17100-152 | Developing Quantum Annealer Driven Data Discovery | http://arxiv.org/abs/1603.07980 | author:Joseph Dulny III, Michael Kim category:quant-ph cs.LG published:2016-03-25 summary:Machine learning applications are limited by computational power. In thispaper, we gain novel insights into the application of quantum annealing (QA) tomachine learning (ML) through experiments in natural language processing (NLP),seizure prediction, and linear separability testing. These experiments areperformed on QA simulators and early-stage commercial QA hardware and comparedto an unprecedented number of traditional ML techniques. We extend QBoost, anearly implementation of a binary classifier that utilizes a quantum annealer,via resampling and ensembling of predicted probabilities to produce a morerobust class estimator. To determine the strengths and weaknesses of thisapproach, resampled QBoost (RQBoost) is tested across several datasets andcompared to QBoost and traditional ML. We show and explain how QBoost incombination with a commercial QA device are unable to perfectly separate binaryclass data which is linearly separable via logistic regression with shrinkage.We further explore the performance of RQBoost in the space of NLP and seizureprediction and find QA-enabled ML using QBoost and RQBoost is outperformed bytraditional techniques. Additionally, we provide a detailed discussion ofalgorithmic constraints and trade-offs imposed by the use of this QA hardware.Through these experiments, we provide unique insights into the state of quantumML via boosting and the use of quantum annealing hardware that are valuable toinstitutions interested in applying QA to problems in ML and beyond.
arxiv-17100-153 | An Effective Unconstrained Correlation Filter and Its Kernelization for Face Recognition | http://arxiv.org/abs/1603.07800 | author:Yan Yan, Hanzi Wang, Cuihua Li, Chenhui Yang, Bineng Zhong category:cs.CV published:2016-03-25 summary:In this paper, an effective unconstrained correlation filter called Uncon-strained Optimal Origin Tradeoff Filter (UOOTF) is presented and applied torobust face recognition. Compared with the conventional correlation filters inClass-dependence Feature Analysis (CFA), UOOTF improves the overall performancefor unseen patterns by removing the hard constraints on the origin correlationoutputs during the filter design. To handle non-linearly separabledistributions between different classes, we further develop a non- linearextension of UOOTF based on the kernel technique. The kernel ex- tension ofUOOTF allows for higher flexibility of the decision boundary due to a widerrange of non-linearity properties. Experimental results demon- strate theeffectiveness of the proposed unconstrained correlation filter and itskernelization in the task of face recognition.
arxiv-17100-154 | Mode-Seeking on Hypergraphs for Robust Geometric Model Fitting | http://arxiv.org/abs/1603.07807 | author:Hanzi Wang, Guobao Xiao, Yan Yan, David Suter category:cs.CV published:2016-03-25 summary:In this paper, we propose a novel geometric model fitting method, calledMode-Seeking on Hypergraphs (MSH),to deal with multi-structure data even in thepresence of severe outliers. The proposed method formulates geometric modelfitting as a mode seeking problem on a hypergraph in which vertices representmodel hypotheses and hyperedges denote data points. MSH intuitively detectsmodel instances by a simple and effective mode seeking algorithm. In additionto the mode seeking algorithm, MSH includes a similarity measure betweenvertices on the hypergraph and a weight-aware sampling technique. The proposedmethod not only alleviates sensitivity to the data distribution, but also isscalable to large scale problems. Experimental results further demonstrate thatthe proposed method has significant superiority over the state-of-the-artfitting methods on both synthetic data and real images.
arxiv-17100-155 | Disentangling Nonlinear Perceptual Embeddings With Multi-Query Triplet Networks | http://arxiv.org/abs/1603.07810 | author:Andreas Veit, Serge Belongie, Theofanis Karaletsos category:cs.CV cs.AI cs.LG published:2016-03-25 summary:In typical perceptual tasks, higher-order concepts are inferred from visualfeatures to assist with perceptual decision making. However, there is amultitude of visual concepts which can be inferred from a single stimulus. Whenlearning nonlinear embeddings with siamese or triplet networks fromsimilarities, we typically assume they are sourced from a single visualconcept. In this paper, we are concerned with the hypothesis that it can bepotentially harmful to ignore the heterogeneity of concepts affiliated withobserved similarities when learning these embedding networks. We demonstrateempirically that this hypothesis holds and suggest an approach that deals withthese shortcomings, by combining multiple notions of similarities in onecompact system. We propose Multi-Query Networks (MQNs) that leverage recentadvances in representation learning on factorized triplet embeddings incombination with Convolutional Networks in order to learn embeddingsdifferentiated into semantically distinct subspaces, which are learned with alatent space attention mechanism. We show that the resulting model learnsvisually relevant semantic subspaces with features that do not only outperformsingle triplet networks, but even sets of concept specific networks.
arxiv-17100-156 | Unsupervised Category Discovery via Looped Deep Pseudo-Task Optimization Using a Large Scale Radiology Image Database | http://arxiv.org/abs/1603.07965 | author:Xiaosong Wang, Le Lu, Hoo-chang Shin, Lauren Kim, Isabella Nogues, Jianhua Yao, Ronald Summers category:cs.CV published:2016-03-25 summary:Obtaining semantic labels on a large scale radiology image database (215,786key images from 61,845 unique patients) is a prerequisite yet bottleneck totrain highly effective deep convolutional neural network (CNN) models for imagerecognition. Nevertheless, conventional methods for collecting image labels(e.g., Google search followed by crowd-sourcing) are not applicable due to theformidable difficulties of medical annotation tasks for those who are notclinically trained. This type of image labeling task remains non-trivial evenfor radiologists due to uncertainty and possible drastic inter-observervariation or inconsistency. In this paper, we present a looped deep pseudo-task optimization procedurefor automatic category discovery of visually coherent and clinically semantic(concept) clusters. Our system can be initialized by domain-specific (CNNtrained on radiology images and text report derived labels) or generic(ImageNet based) CNN models. Afterwards, a sequence of pseudo-tasks areexploited by the looped deep image feature clustering (to refine image labels)and deep CNN training/classification using new labels (to obtain more taskrepresentative deep features). Our method is conceptually simple and based onthe hypothesized "convergence" of better labels leading to better trained CNNmodels which in turn feed more effective deep image features to facilitate moremeaningful clustering/labels. We have empirically validated the convergence anddemonstrated promising quantitative and qualitative results. Category labels ofsignificantly higher quality than those in previous work are discovered. Thisallows for further investigation of the hierarchical semantic nature of thegiven large-scale radiology image database.
arxiv-17100-157 | Object Recognition Based on Amounts of Unlabeled Data | http://arxiv.org/abs/1603.07957 | author:Fuqiang Liu, Fukun Bi, Liang Chen category:cs.CV published:2016-03-25 summary:This paper proposes a novel semi-supervised method on object recognition.First, based on Boost Picking, a universal algorithm, Boost Picking Teaching(BPT), is proposed to train an effective binary-classifier just using a fewlabeled data and amounts of unlabeled data. Then, an ensemble strategy isdetailed to synthesize multiple BPT-trained binary-classifiers to be ahigh-performance multi-classifier. The rationality of the strategy is alsoanalyzed in theory. Finally, the proposed method is tested on two databases,CIFAR-10 and CIFAR-100. Using 2% labeled data and 98% unlabeled data, theaccuracies of the proposed method on the two data sets are 78.39% and 50.77%respectively.
arxiv-17100-158 | Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning | http://arxiv.org/abs/1603.07954 | author:Karthik Narasimhan, Adam Yala, Regina Barzilay category:cs.CL published:2016-03-25 summary:In traditional formulations, information extraction systems operate on afixed collection of documents. In this work, we explore the task of acquiringand incorporating external evidence to improve extraction accuracy. Thisprocess entails query reformulation for search, extraction from new sources andreconciliation of extracted values, which are repeated until sufficientevidence is collected. We approach the problem using a reinforcement learningframework where our model learns to select optimal actions based on contextualinformation. We employ a deep Q-network, trained to optimize a reward functionthat reflects extraction accuracy while penalizing extra effort. Ourexperiments on a publicly available database of shooting incidents demonstratethat our system outperforms traditional extractors by 7.2% on average.
arxiv-17100-159 | A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic Extraction of Semantics, Formation of Integrated Concepts and Re-selection Features for Ambiguity | http://arxiv.org/abs/1603.07886 | author:Peijie Yin, Hong Qiao, Wei Wu, Lu Qi, YinLin Li, Shanlin Zhong, Bo Zhang category:cs.CV cs.AI cs.LG published:2016-03-25 summary:Integration between biology and information science benefits both fields.Many related models have been proposed, such as computational visual cognitionmodels, computational motor control models, integrations of both and so on. Ingeneral, the robustness and precision of recognition is one of the key problemsfor object recognition models. In this paper, inspired by features of human recognition process and theirbiological mechanisms, a new integrated and dynamic framework is proposed tomimic the semantic extraction, concept formation and feature re-selection inhuman visual processing. The main contributions of the proposed model are asfollows: (1) Semantic feature extraction: Local semantic features are learnt fromepisodic features that are extracted from raw images through a deep neuralnetwork; (2) Integrated concept formation: Concepts are formed with local semanticinformation and structural information learnt through network. (3) Feature re-selection: When ambiguity is detected during recognitionprocess, distinctive features according to the difference between ambiguouscandidates are re-selected for recognition. Experimental results on hand-written digits and facial shape dataset showthat, compared with other methods, the new proposed model exhibits higherrobustness and precision for visual recognition, especially in the conditionwhen input samples are smantic ambiguous. Meanwhile, the introduced biologicalmechanisms further strengthen the interaction between neuroscience andinformation science.
arxiv-17100-160 | Hybridization of Expectation-Maximization and K-Means Algorithms for Better Clustering Performance | http://arxiv.org/abs/1603.07879 | author:D. Raja Kishor, N. B. Venkateswarlu category:cs.LG stat.ML published:2016-03-25 summary:The present work proposes hybridization of Expectation-Maximization (EM) andK-Means techniques as an attempt to speed-up the clustering process. Thoughboth K-Means and EM techniques look into different areas, K-means can be viewedas an approximate way to obtain maximum likelihood estimates for the means.Along with the proposed algorithm for hybridization, the present work alsoexperiments with the Standard EM algorithm. Six different datasets are used forthe experiments of which three are synthetic datasets. Clustering fitness andSum of Squared Errors (SSE) are computed for measuring the clusteringperformance. In all the experiments it is observed that the proposed algorithmfor hybridization of EM and K-Means techniques is consistently taking lessexecution time with acceptable Clustering Fitness value and less SSE than thestandard EM algorithm. It is also observed that the proposed algorithm isproducing better clustering results than the Cluster package of PurdueUniversity.
arxiv-17100-161 | Exact Bayesian inference for off-line change-point detection in tree-structured graphical models | http://arxiv.org/abs/1603.07871 | author:Loïc Schwaller, Stéphane Robin category:stat.ML published:2016-03-25 summary:We consider the problem of change-point detection in multivariatetime-series. The multivariate distribution of the observations is supposed tofollow a graphical model, whose graph and parameters are affected by abruptchanges throughout time. We demonstrate that it is possible to perform exactBayesian inference whenever one considers a simple class of undirected graphscalled spanning trees as possible structures. We are then able to integrate onthe graph and segmentation spaces at the same time by combining classicaldynamic programming with algebraic results pertaining to spanning trees. Inparticular, we show that quantities such as posterior distributions forchange-points or posterior edge probabilities over time can efficiently beobtained. We illustrate our results on both synthetic and experimental dataarising from biology and neuroscience.
arxiv-17100-162 | The Asymptotic Performance of Linear Echo State Neural Networks | http://arxiv.org/abs/1603.07866 | author:Romain Couillet, Gilles Wainrib, Harry Sevi, Hafiz Tiomoko Ali category:cs.LG cs.NE math.PR published:2016-03-25 summary:In this article, a study of the mean-square error (MSE) performance of linearecho-state neural networks is performed, both for training and testing tasks.Considering the realistic setting of noise present at the network nodes, wederive deterministic equivalents for the aforementioned MSE in the limit wherethe number of input data $T$ and network size $n$ both grow large. Specializingthen the network connectivity matrix to specific random settings, we furtherobtain simple formulas that provide new insights on the performance of suchnetworks.
arxiv-17100-163 | Markov substitute processes : a new model for linguistics and beyond | http://arxiv.org/abs/1603.07850 | author:Olivier Catoni, Thomas Mainguy category:stat.ML math.ST stat.TH published:2016-03-25 summary:We introduce Markov substitute processes, a new model at the crossroad ofstatistics and formal grammars, and prove its main property : Markov substituteprocesses with a given support form an exponential family.
arxiv-17100-164 | A movie genre prediction based on Multivariate Bernoulli model and genre correlations | http://arxiv.org/abs/1604.08608 | author:Eric Makita, Artem Lenskiy category:cs.IR cs.LG published:2016-03-25 summary:Movie ratings play an important role both in determining the likelihood of apotential viewer to watch the movie and in reflecting the current viewersatisfaction with the movie. They are available in several sources like thetelevision guide, best-selling reference books, newspaper columns, andtelevision programs. Furthermore, movie ratings are crucial for recommendationengines that track the behavior of all users and utilize the information tosuggest items they might like. Movie ratings in most cases, thus, provideinformation that might be more important than movie feature-based data. It isintuitively appealing that information about the viewing preferences in moviegenres is sufficient for predicting a genre of an unlabeled movie. In order topredict movie genres, we treat ratings as a feature vector, apply the Bernoullievent model to estimate the likelihood of a movies given genre, and evaluatethe posterior probability of the genre of a given movie using the Bayes rule.The goal of the proposed technique is to efficiently use the movie ratings forthe task of predicting movie genres. In our approach we attempted to answer thequestion: "Given the set of users who watched a movie, is it possible topredict the genre of a movie based on its ratings?" Our simulation results withMovieLens 100k data demonstrated the efficiency and accuracy of our proposedtechnique, achieving 59% prediction rate for exact prediction and 69% whenincluding correlated genres.
arxiv-17100-165 | A multinomial probabilistic model for movie genre predictions | http://arxiv.org/abs/1603.07849 | author:Eric Makita, Artem Lenskiy category:cs.IR cs.LG published:2016-03-25 summary:This paper proposes a movie genre-prediction based on multinomial probabilitymodel. To the best of our knowledge, this problem has not been addressed yet inthe field of recommender system. The prediction of a movie genre has manypractical applications including complementing the items categories given byexperts and providing a surprise effect in the recommendations given to a user.We employ mulitnomial event model to estimate a likelihood of a movie givengenre and the Bayes rule to evaluate the posterior probability of a genre givena movie. Experiments with the MovieLens dataset validate our approach. Weachieved 70% prediction rate using only 15% of the whole set for training.
arxiv-17100-166 | Deep Learning At Scale and At Ease | http://arxiv.org/abs/1603.07846 | author:Wei Wang, Gang Chen, Haibo Chen, Tien Tuan Anh Dinh, Jinyang Gao, Beng Chin Ooi, Kian-Lee Tan, Sheng Wang category:cs.LG cs.DC published:2016-03-25 summary:Recently, deep learning techniques have enjoyed success in various multimediaapplications, such as image classification and multi-modal data analysis. Largedeep learning models are developed for learning rich representations of complexdata. There are two challenges to overcome before deep learning can be widelyadopted in multimedia and other applications. One is usability, namely theimplementation of different models and training algorithms must be done bynon-experts without much effort especially when the model is large and complex.The other is scalability, that is the deep learning system must be able toprovision for a huge demand of computing resources for training large modelswith massive datasets. To address these two challenges, in this paper, wedesign a distributed deep learning platform called SINGA which has an intuitiveprogramming model based on the common layer abstraction of deep learningmodels. Good scalability is achieved through flexible distributed trainingarchitecture and specific optimization techniques. SINGA runs on GPUs as wellas on CPUs, and we show that it outperforms many other state-of-the-art deeplearning systems. Our experience with developing and training deep learningmodels for real-life multimedia applications in SINGA shows that the platformis both usable and scalable.
arxiv-17100-167 | Early Detection of Combustion Instabilities using Deep Convolutional Selective Autoencoders on Hi-speed Flame Video | http://arxiv.org/abs/1603.07839 | author:Adedotun Akintayo, Kin Gwn Lore, Soumalya Sarkar, Soumik Sarkar category:cs.CV cs.LG cs.NE published:2016-03-25 summary:This paper proposes an end-to-end convolutional selective autoencoderapproach for early detection of combustion instabilities using rapidly arrivingflame image frames. The instabilities arising in combustion processes causesignificant deterioration and safety issues in various human-engineered systemssuch as land and air based gas turbine engines. These properties are describedas self-sustaining, large amplitude pressure oscillations and show varyingspatial scales periodic coherent vortex structure shedding. However, suchinstability is extremely difficult to detect before a combustion processbecomes completely unstable due to its sudden (bifurcation-type) nature. Inthis context, an autoencoder is trained to selectively mask stable flame andallow unstable flame image frames. In that process, the model learns toidentify and extract rich descriptive and explanatory flame shape features.With such a training scheme, the selective autoencoder is shown to be able todetect subtle instability features as a combustion process makes transitionfrom stable to unstable region. As a consequence, the deep learning tool-chaincan perform as an early detection framework for combustion instabilities thatwill have a transformative impact on the safety and performance of modernengines.
arxiv-17100-168 | Investigation Into The Effectiveness Of Long Short Term Memory Networks For Stock Price Prediction | http://arxiv.org/abs/1603.07893 | author:Hengjian Jia category:cs.NE cs.LG published:2016-03-25 summary:The effectiveness of long short term memory networks trained bybackpropagation through time for stock price prediction is explored in thispaper. A range of different architecture LSTM networks are constructed trainedand tested.
arxiv-17100-169 | An end-to-end convolutional selective autoencoder approach to Soybean Cyst Nematode eggs detection | http://arxiv.org/abs/1603.07834 | author:Adedotun Akintayo, Nigel Lee, Vikas Chawla, Mark Mullaney, Christopher Marett, Asheesh Singh, Arti Singh, Greg Tylka, Baskar Ganapathysubramaniam, Soumik Sarkar category:cs.CV cs.LG stat.ML published:2016-03-25 summary:This paper proposes a novel selective autoencoder approach within theframework of deep convolutional networks. The crux of the idea is to train adeep convolutional autoencoder to suppress undesired parts of an image framewhile allowing the desired parts resulting in efficient object detection. Theefficacy of the framework is demonstrated on a critical plant science problem.In the United States, approximately $1 billion is lost per annum due to anematode infection on soybean plants. Currently, plant-pathologists rely onlabor-intensive and time-consuming identification of Soybean Cyst Nematode(SCN) eggs in soil samples via manual microscopy. The proposed frameworkattempts to significantly expedite the process by using a series of manuallylabeled microscopic images for training followed by automated high-throughputegg detection. The problem is particularly difficult due to the presence of alarge population of non-egg particles (disturbances) in the image frames thatare very similar to SCN eggs in shape, pose and illumination. Therefore, theselective autoencoder is trained to learn unique features related to theinvariant shapes and sizes of the SCN eggs without handcrafting. After that, acomposite non-maximum suppression and differencing is applied at thepost-processing stage.
arxiv-17100-170 | Privacy-Preserved Big Data Analysis Based on Asymmetric Imputation Kernels | http://arxiv.org/abs/1603.07828 | author:Bo-Wei Chen category:cs.LG cs.CR published:2016-03-25 summary:This study presents an efficient approach for incomplete data classification,where the entries of samples are missing or masked due to privacy preservation.To deal with these incomplete data, a new kernel function with asymmetricintrinsic mappings is proposed in this study. Such a new kernel uses three-sidesimilarities for kernel matrix formation. The similarity between a testinstance and a training sample relies on not only their distance but also therelation between this test sample and the centroid of the class, where thetraining sample belongs. This reduces biased estimation compared with typicalmethods when only one training sample is used for kernel matrix formation.Furthermore, the proposed kernel is capable of performing data imputation byusing class-dependent averages. This enhances Fisher Discriminant Ratios anddata discriminability. Experiments on two databases were carried out forevaluating the proposed method. The result indicated that the accuracy of theproposed method was higher than that of the baseline. These findings therebydemonstrate the effectiveness of the proposed idea.
arxiv-17100-171 | Training-Free Synthesized Face Sketch Recognition Using Image Quality Assessment Metrics | http://arxiv.org/abs/1603.07823 | author:Nannan Wang, Jie Li, Leiyu Sun, Bin Song, Xinbo Gao category:cs.CV published:2016-03-25 summary:Face sketch synthesis has wide applications ranging from digitalentertainments to law enforcements. Objective image quality assessment scoresand face recognition accuracy are two mainly used tools to evaluate thesynthesis performance. In this paper, we proposed a synthesized face sketchrecognition framework based on full-reference image quality assessment metrics.Synthesized sketches generated from four state-of-the-art methods are utilizedto test the performance of the proposed recognition framework. For the imagequality assessment metrics, we employed the classical structured similarityindex metric and other three prevalent metrics: visual information fidelity,feature similarity index metric and gradient magnitude similarity deviation.Extensive experiments compared with baseline methods illustrate theeffectiveness of the proposed synthesized face sketch recognition framework.Data and implementation code in this paper are available online atwww.ihitworld.com/WNN/IQA_Sketch.zip.
arxiv-17100-172 | A Diagram Is Worth A Dozen Images | http://arxiv.org/abs/1603.07396 | author:Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi category:cs.CV cs.AI published:2016-03-24 summary:Diagrams are common tools for representing complex concepts, relationshipsand events, often when it would be difficult to portray the same informationwith natural images. Understanding natural images has been extensively studiedin computer vision, while diagram understanding has received little attention.In this paper, we study the problem of diagram interpretation and reasoning,the challenging task of identifying the structure of a diagram and thesemantics of its constituents and their relationships. We introduce DiagramParse Graphs (DPG) as our representation to model the structure of diagrams. Wedefine syntactic parsing of diagrams as learning to infer DPGs for diagrams andstudy semantic interpretation and reasoning of diagrams in the context ofdiagram question answering. We devise an LSTM-based method for syntacticparsing of diagrams and introduce a DPG-based attention model for diagramquestion answering. We compile a new dataset of diagrams with exhaustiveannotations of constituents and relationships for over 5,000 diagrams and15,000 questions and answers. Our results show the significance of our modelsfor syntactic parsing and question answering in diagrams using DPGs.
arxiv-17100-173 | A Reconfigurable Low Power High Throughput Streaming Architecture for Big Data Processing | http://arxiv.org/abs/1603.07400 | author:Raqibul Hasan, Tarek Taha, Zahangir Alom category:cs.LG cs.AR cs.DC published:2016-03-24 summary:General purpose computing systems are used for a large variety ofapplications. Extensive supports for flexibility in these systems limit theirenergy efficiencies. Given that big data applications are among the mainemerging workloads for computing systems, specialized architectures for bigdata processing are needed to enable low power and high throughput execution.Several big data applications are particularly focused on classification andclustering tasks. In this paper we propose a multicore heterogeneousarchitecture for big data processing. This system has the capability to processkey machine learning algorithms such as deep neural network, autoencoder, andk-means clustering. Memristor crossbars are utilized to provide low power highthroughput execution of neural networks. The system has both training andrecognition (evaluation of new input) capabilities. The proposed system couldbe used for classification, unsupervised clustering, dimensionality reduction,feature extraction, and anomaly detection applications. The system level areaand power benefits of the specialized architecture is compared with the NVIDIATelsa K20 GPGPU. Our experimental evaluations show that the proposedarchitecture can provide four to six orders of magnitude more energy efficiencyover GPGPUs for big data processing.
arxiv-17100-174 | Attentive Contexts for Object Detection | http://arxiv.org/abs/1603.07415 | author:Jianan Li, Yunchao Wei, Xiaodan Liang, Jian Dong, Tingfa Xu, Jiashi Feng, Shuicheng Yan category:cs.CV published:2016-03-24 summary:Modern deep neural network based object detection methods typically classifycandidate proposals using their interior features. However, global and localsurrounding contexts that are believed to be valuable for object detection arenot fully exploited by existing methods yet. In this work, we take a steptowards understanding what is a robust practice to extract and utilizecontextual information to facilitate object detection in practice.Specifically, we consider the following two questions: "how to identify usefulglobal contextual information for detecting a certain object?" and "how toexploit local context surrounding a proposal for better inferring itscontents?". We provide preliminary answers to these questions throughdeveloping a novel Attention to Context Convolution Neural Network (AC-CNN)based object detection model. AC-CNN effectively incorporates global and localcontextual information into the region-based CNN (e.g. Fast RCNN) detectionmodel and provides better object detection performance. It consists of oneattention-based global contextualized (AGC) sub-network and one multi-scalelocal contextualized (MLC) sub-network. To capture global context, the AGCsub-network recurrently generates an attention map for an input image tohighlight useful global contextual locations, through multiple stacked LongShort-Term Memory (LSTM) layers. For capturing surrounding local context, theMLC sub-network exploits both the inside and outside contextual information ofeach specific proposal at multiple scales. The global and local context arethen fused together for making the final decision for detection. Extensiveexperiments on PASCAL VOC 2007 and VOC 2012 well demonstrate the superiority ofthe proposed AC-CNN over well-established baselines. In particular, AC-CNNoutperforms the popular Fast-RCNN by 2.0% and 2.2% on VOC 2007 and VOC 2012 interms of mAP, respectively.
arxiv-17100-175 | Evaluating the Performance of Offensive Linemen in the NFL | http://arxiv.org/abs/1603.07593 | author:Nikhil Byanna, Diego Klabjan category:stat.ML published:2016-03-24 summary:How does one objectively measure the performance of an individual offensivelineman in the NFL? The existing literature proposes various measures that relyon subjective assessments of game film, but has yet to develop an objectivemethodology to evaluate performance. Using a variety of statistics related toan offensive lineman's performance, we develop a framework to objectivelyanalyze the overall performance of an individual offensive lineman anddetermine specific linemen who are overvalued or undervalued relative to theirsalary. We identify eight players across the 2013-2014 and 2014-2015 NFLseasons that are considered to be overvalued or undervalued and corroborate theresults with existing metrics that are based on subjective evaluation. To thebest of our knowledge, the techniques set forth in this work have not beenutilized in previous works to evaluate the performance of NFL players at anyposition, including offensive linemen.
arxiv-17100-176 | Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks | http://arxiv.org/abs/1603.07772 | author:Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, Li Shen, Xiaohui Xie category:cs.CV cs.LG published:2016-03-24 summary:Skeleton based action recognition distinguishes human actions using thetrajectories of skeleton joints, which provide a very good representation fordescribing actions. Considering that recurrent neural networks (RNNs) with LongShort-Term Memory (LSTM) can learn feature representations and model long-termtemporal dependencies automatically, we propose an end-to-end fully connecteddeep LSTM network for skeleton based action recognition. Inspired by theobservation that the co-occurrences of the joints intrinsically characterizehuman actions, we take the skeleton as the input at each time slot andintroduce a novel regularization scheme to learn the co-occurrence features ofskeleton joints. To train the deep LSTM network effectively, we propose a newdropout algorithm which simultaneously operates on the gates, cells, and outputresponses of the LSTM neurons. Experimental results on three human actionrecognition datasets consistently demonstrate the effectiveness of the proposedmodel.
arxiv-17100-177 | Generating Text from Structured Data with Application to the Biography Domain | http://arxiv.org/abs/1603.07771 | author:Remi Lebret, David Grangier, Michael Auli category:cs.CL published:2016-03-24 summary:This paper introduces a neural model for concept-to-text generation thatscales to large, rich domains. We experiment with a new dataset of biographiesfrom Wikipedia that is an order of magni- tude larger than existing resourceswith over 700k samples. The dataset is also vastly more diverse with a 400kvocab- ulary, compared to a few hundred words for Weathergov or Robocup. Ourmodel builds upon recent work on conditional neural language model for textgenera- tion. To deal with the large vocabulary, we extend these models to mixa fixed vocabulary with copy actions that trans- fer sample-specific words fromthe in- put database to the generated output sen- tence. Our neural modelsignificantly out- performs a classical Kneser-Ney language model adapted tothis task by nearly 15 BLEU.
arxiv-17100-178 | Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric Video | http://arxiv.org/abs/1603.07763 | author:Hao Jiang, Kristen Grauman category:cs.CV published:2016-03-24 summary:Understanding the camera wearer's activity is central to egocentric vision,yet one key facet of that activity is inherently invisible to the camera--thewearer's body pose. Prior work focuses on estimating the pose of hands and armswhen they come into view, but this 1) gives an incomplete view of the full bodyposture, and 2) prevents any pose estimate at all in many frames, since thehands are only visible in a fraction of daily life activities. We propose toinfer the "invisible pose" of a person behind the egocentric camera. Given asingle video, our efficient learning-based approach returns the full body 3Djoint positions for each frame. Our method exploits cues from the dynamicmotion signatures of the surrounding scene--which changes predictably as afunction of body pose--as well as static scene structures that reveal theviewpoint (e.g., sitting vs. standing). We further introduce a novel energyminimization scheme to infer the pose sequence. It uses soft predictions of theposes per time instant together with a non-parametric model of human posedynamics over longer windows. Our method outperforms an array of possiblealternatives, including deep learning approaches for direct pose regressionfrom images.
arxiv-17100-179 | A universal tradeoff between power, precision and speed in physical communication | http://arxiv.org/abs/1603.07758 | author:Subhaneil Lahiri, Jascha Sohl-Dickstein, Surya Ganguli category:cs.IT math.IT physics.bio-ph q-bio.NC stat.ML published:2016-03-24 summary:Maximizing the speed and precision of communication while minimizing powerdissipation is a fundamental engineering design goal. Also, biological systemsachieve remarkable speed, precision and power efficiency using poorlyunderstood physical design principles. Powerful theories like informationtheory and thermodynamics do not provide general limits on power, precision andspeed. Here we go beyond these classical theories to prove that the product ofprecision and speed is universally bounded by power dissipation in any physicalcommunication channel whose dynamics is faster than that of the signal.Moreover, our derivation involves a novel connection between friction andinformation geometry. These results may yield insight into both the engineeringdesign of communication devices and the structure and function of biologicalsignaling systems.
arxiv-17100-180 | Pathway Lasso: Estimate and Select Sparse Mediation Pathways with High Dimensional Mediators | http://arxiv.org/abs/1603.07749 | author:Yi Zhao, Xi Luo category:stat.ML stat.AP stat.ME published:2016-03-24 summary:In many scientific studies, it becomes increasingly important to delineatethe causal pathways through a large number of mediators, such as genetic andbrain mediators. Structural equation modeling (SEM) is a popular technique toestimate the pathway effects, commonly expressed as products of coefficients.However, it becomes unstable to fit such models with high dimensionalmediators, especially for a general setting where all the mediators arecausally dependent but the exact causal relationships between them are unknown.This paper proposes a sparse mediation model using a regularized SEM approach,where sparsity here means that a small number of mediators have nonzeromediation effects between a treatment and an outcome. To address the modelselection challenge, we innovate by introducing a new penalty called PathwayLasso. This penalty function is a convex relaxation of the non-convex productfunction, and it enables a computationally tractable optimization criterion toestimate and select many pathway effects simultaneously. We develop a fastADMM-type algorithm to compute the model parameters, and we show that theiterative updates can be expressed in closed form. On both simulated data and areal fMRI dataset, the proposed approach yields higher pathway selectionaccuracy and lower estimation bias than other competing methods.
arxiv-17100-181 | Coarse-to-Fine Segmentation With Shape-Tailored Scale Spaces | http://arxiv.org/abs/1603.07745 | author:Ganesh Sundaramoorthi, Naeemullah Khan, Byung-Woo Hong category:cs.CV published:2016-03-24 summary:We formulate a general energy and method for segmentation that is designed tohave preference for segmenting the coarse structure over the fine structure ofthe data, without smoothing across boundaries of regions. The energy isformulated by considering data terms at a continuum of scales from the scalespace computed from the Heat Equation within regions, and integrating theseterms over all time. We show that the energy may be approximately optimizedwithout solving for the entire scale space, but rather solving time-independentlinear equations at the native scale of the image, making the methodcomputationally feasible. We provide a multi-region scheme, and apply ourmethod to motion segmentation. Experiments on a benchmark dataset shows thatour method is less sensitive to clutter or other undesirable fine-scalestructure, and leads to better performance in motion segmentation.
arxiv-17100-182 | Skill-Based Differences in Spatio-Temporal Team Behavior in Defence of The Ancients 2 | http://arxiv.org/abs/1603.07738 | author:Anders Drachen, Matthew Yancey, John Maguire, Derrek Chu, Iris Yuhui Wang, Tobias Mahlmann, Matthias Schubert, Diego Klabjan category:stat.ML published:2016-03-24 summary:Multiplayer Online Battle Arena (MOBA) games are among the most playeddigital games in the world. In these games, teams of players fight against eachother in arena environments, and the gameplay is focused on tactical combat.Mastering MOBAs requires extensive practice, as is exemplified in the popularMOBA Defence of the Ancients 2 (DotA 2). In this paper, we present threedata-driven measures of spatio-temporal behavior in DotA 2: 1) Zone changes; 2)Distribution of team members and: 3) Time series clustering via a fuzzyapproach. We present a method for obtaining accurate positional data from DotA2. We investigate how behavior varies across these measures as a function ofthe skill level of teams, using four tiers from novice to professional players.Results indicate that spatio-temporal behavior of MOBA teams is related to teamskill, with professional teams having smaller within-team distances andconducting more zone changes than amateur teams. The temporal distribution ofthe within-team distances of professional and high-skilled teams also generallyfollows patterns distinct from lower skill ranks.
arxiv-17100-183 | Probabilistic Reasoning via Deep Learning: Neural Association Models | http://arxiv.org/abs/1603.07704 | author:Quan Liu, Hui Jiang, Zhen-Hua Ling, Si Wei, Yu Hu category:cs.AI cs.LG cs.NE published:2016-03-24 summary:In this paper, we propose a new deep learning approach, called neuralassociation model (NAM), for probabilistic reasoning in artificialintelligence. We propose to use neural networks to model association betweenany two events in a domain. Neural networks take one event as input and computea conditional probability of the other event to model how likely these twoevents are associated. The actual meaning of the conditional probabilitiesvaries between applications and depends on how the models are trained. In thiswork, as two case studies, we have investigated two NAM structures, namely deepneural networks (DNNs) and relation modulated neural nets (RMNNs), on severalprobabilistic reasoning tasks in AI, including recognizing textual entailment,triple classification in multirelational knowledge bases and common-sensereasoning. Experimental results on several popular data sets derived fromWordNet, FreeBase and ConceptNet have all demonstrated that both DNNs and RMNNsperform equally well and they can significantly outperform the conventionalmethods available for these reasoning tasks. Moreover, comparing with DNNs,RMNNs are superior in knowledge transfer, where a pre-trained model can bequickly extended to an unseen relation after observing only a few trainingsamples.
arxiv-17100-184 | Joint Projection and Dictionary Learning using Low-rank Regularization and Graph Constraints | http://arxiv.org/abs/1603.07697 | author:Homa Foroughi, Nilanjan Ray, Hong Zhang category:cs.CV published:2016-03-24 summary:In this paper, we aim at learning simultaneously a discriminative dictionaryand a robust projection matrix from noisy data. The joint learning, makes thelearned projection and dictionary a better fit for each other, so a moreaccurate classification can be obtained. However, current prevailing jointdimensionality reduction and dictionary learning methods, would fail when thetraining samples are noisy or heavily corrupted. To address this issue, wepropose a joint projection and dictionary learning using low-rankregularization and graph constraints (JPDL-LR). Specifically, thediscrimination of the dictionary is achieved by imposing Fisher criterion onthe coding coefficients. In addition, our method explicitly encodes the localstructure of data by incorporating a graph regularization term, that furtherimproves the discriminative ability of the projection matrix. Inspired byrecent advances of low-rank representation for removing outliers and noise, weenforce a low-rank constraint on sub-dictionaries of all classes to make themmore compact and robust to noise. Experimental results on several benchmarkdatasets verify the effectiveness and robustness of our method for bothdimensionality reduction and image classification, especially when the datacontains considerable noise or variations.
arxiv-17100-185 | Part-of-Speech Relevance Weights for Learning Word Embeddings | http://arxiv.org/abs/1603.07695 | author:Quan Liu, Zhen-Hua Ling, Hui Jiang, Yu Hu category:cs.CL published:2016-03-24 summary:This paper proposes a model to learn word embeddings with weighted contextsbased on part-of-speech (POS) relevance weights. POS is a fundamental elementin natural language. However, state-of-the-art word embedding models fail toconsider it. This paper proposes to use position-dependent POS relevanceweighting matrices to model the inherent syntactic relationship among wordswithin a context window. We utilize the POS relevance weights to model eachword-context pairs during the word embedding training process. The modelproposed in this paper paper jointly optimizes word vectors and the POSrelevance matrices. Experiments conducted on popular word analogy and wordsimilarity tasks all demonstrated the effectiveness of the proposed method.
arxiv-17100-186 | Predictive Analytics Using Smartphone Sensors for Depressive Episodes | http://arxiv.org/abs/1603.07692 | author:Taeheon Jeong, Diego Klabjan, Justin Starren category:cs.CY cs.HC stat.ML published:2016-03-24 summary:The behaviors of patients with depression are usually difficult to predictbecause the patients demonstrate the symptoms of a depressive episode without awarning at unexpected times. The goal of this research is to build algorithmsthat detect signals of such unusual moments so that doctors can be proactive inapproaching already diagnosed patients before they fall in depression. Eachpatient is equipped with a smartphone with the capability to track its sensors.We first find the home location of a patient, which is then augmented withother sensor data to identify sleep patterns and select communication patterns.The algorithms require two to three weeks of training data to build standardpatterns, which are considered normal behaviors; and then, the methods identifyany anomalies in day-to-day data readings of sensors. Four smartphone sensors,including the accelerometer, the gyroscope, the location probe and thecommunication log probe are used for anomaly detection in sleeping andcommunication patterns.
arxiv-17100-187 | Recursive Neural Language Architecture for Tag Prediction | http://arxiv.org/abs/1603.07646 | author:Saurabh Kataria category:cs.IR cs.CL cs.LG cs.NE published:2016-03-24 summary:We consider the problem of learning distributed representations for tags fromtheir associated content for the task of tag recommendation. Consideringtagging information is usually very sparse, effective learning from content andtag association is very crucial and challenging task. Recently, various neuralrepresentation learning models such as WSABIE and its variants show promisingperformance, mainly due to compact feature representations learned in asemantic space. However, their capacity is limited by a linear compositionalapproach for representing tags as sum of equal parts and hurt theirperformance. In this work, we propose a neural feedback relevance model forlearning tag representations with weighted feature representations. Ourexperiments on two widely used datasets show significant improvement forquality of recommendations over various baselines.
arxiv-17100-188 | Position and Vector Detection of Blind Spot motion with the Horn-Schunck Optical Flow | http://arxiv.org/abs/1603.07625 | author:Stephen Yu, Mike Wu category:cs.CV published:2016-03-24 summary:The proposed method uses live image footage which, based on calculations ofpixel motion, decides whether or not an object is in the blind-spot. If found,the driver is notified by a sensory light or noise built into the vehicle'sCPU. The new technology incorporates optical vectors and flow fields ratherthan expensive radar-waves, creating cheaper detection systems that retain theneeded accuracy while adapting to the current processor speeds.
arxiv-17100-189 | Semantic Properties of Customer Sentiment in Tweets | http://arxiv.org/abs/1603.07624 | author:Eun Hee Ko, Diego Klabjan category:cs.CL cs.IR cs.SI stat.ML published:2016-03-24 summary:An increasing number of people are using online social networking services(SNSs), and a significant amount of information related to experiences inconsumption is shared in this new media form. Text mining is an emergingtechnique for mining useful information from the web. We aim at discovering inparticular tweets semantic patterns in consumers' discussions on social media.Specifically, the purposes of this study are twofold: 1) finding similarity anddissimilarity between two sets of textual documents that include consumers'sentiment polarities, two forms of positive vs. negative opinions and 2)driving actual content from the textual data that has a semantic trend. Theconsidered tweets include consumers opinions on US retail companies (e.g.,Amazon, Walmart). Cosine similarity and K-means clustering methods are used toachieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topicmodeling algorithm, is used for the latter purpose. This is the first studywhich discover semantic properties of textual data in consumption contextbeyond sentiment analysis. In addition to major findings, we apply LDA (LatentDirichlet Allocations) to the same data and drew latent topics that representconsumers' positive opinions and negative opinions on social media.
arxiv-17100-190 | Going Out of Business: Auction House Behavior in the Massively Multi-Player Online Game | http://arxiv.org/abs/1603.07610 | author:Anders Drachen, Joseph Riley, Shawna Baskin, Diego Klabjan category:cs.CY cs.HC stat.ML published:2016-03-24 summary:The in-game economies of massively multi-player online games (MMOGs) arecomplex systems that have to be carefully designed and managed. This paperpresents the results of an analysis of auction house data from the MMOG Glitch,across a 14 month time period, the entire lifetime of the game. The datacomprise almost 3 million data points, over 20,000 unique players and more than650 products. Furthermore, an interactive visualization, based on Sankey flowdiagrams, is presented which shows the proportion of the different clustersacross each time bin, as well as the flow of players between clusters. Thediagram allows evaluation of migration of players between clusters as afunction of time, as well as churn analysis. The presented work provides atemplate analysis and visualization model for progression-based ortemporal-based analysis of player behavior broadly applicable to games.
arxiv-17100-191 | Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL | http://arxiv.org/abs/1603.07609 | author:Yevgeni Berzak, Roi Reichart, Boris Katz category:cs.CL published:2016-03-24 summary:This work examines the impact of cross-linguistic transfer on grammaticalerrors in English as Second Language (ESL) texts. Using a computationalframework that formalizes the theory of Contrastive Analysis (CA), wedemonstrate that language specific error distributions in ESL writing can bepredicted from the typological properties of the native language and theirrelation to the typology of English. Our typology driven model enables toobtain accurate estimates of such distributions without access to any ESL datafor the target languages. Furthermore, we present a strategy for adjusting ourmethod to low-resource languages that lack typological documentation using abootstrapping approach which approximates native language typology from ESLtexts. Finally, we show that our framework is instrumental for linguisticinquiry seeking to identify first language factors that contribute to a widerange of difficulties in second language acquisition.
arxiv-17100-192 | On the Powerball Method | http://arxiv.org/abs/1603.07421 | author:Ye Yuan, Mu Li, Claire J. Tomlin category:cs.SY cs.LG math.OC published:2016-03-24 summary:We propose a new method to accelerate the convergence of optimizationalgorithms. This method adds a power coefficient $\gamma\in(0,1)$ to thegradient during optimization. We call this the Powerball method after thewell-known Heavy-ball method \cite{heavyball}. We prove that the Powerballmethod can achieve $\epsilon$ accuracy for strongly convex functions by using$O\left((1-\gamma)^{-1}\epsilon^{\gamma-1}\right)$ iterations. We alsodemonstrate that the Powerball method provides a $10$-fold speed up of theconvergence of both gradient descent and L-BFGS on multiple real datasets.
arxiv-17100-193 | Multi-Subregion Based Correlation Filter Bank for Robust Face Recognition | http://arxiv.org/abs/1603.07604 | author:Yan Yan, Hanzi Wang, David Suter category:cs.CV published:2016-03-24 summary:In this paper, we propose an effective feature extraction algorithm, calledMulti-Subregion based Correlation Filter Bank (MS-CFB), for robust facerecognition. MS-CFB combines the benefits of global-based and local-basedfeature extraction algorithms, where multiple correlation filters correspond-ing to different face subregions are jointly designed to optimize the overallcorrelation outputs. Furthermore, we reduce the computational complexi- ty ofMS-CFB by designing the correlation filter bank in the spatial domain andimprove its generalization capability by capitalizing on the unconstrained formduring the filter bank design process. MS-CFB not only takes the d- ifferencesamong face subregions into account, but also effectively exploits thediscriminative information in face subregions. Experimental results on variouspublic face databases demonstrate that the proposed algorithm pro- vides abetter feature representation for classification and achieves higherrecognition rates compared with several state-of-the-art algorithms.
arxiv-17100-194 | Semantic Regularities in Document Representations | http://arxiv.org/abs/1603.07603 | author:Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, Xueqi Cheng category:cs.CL published:2016-03-24 summary:Recent work exhibited that distributed word representations are good atcapturing linguistic regularities in language. This allows vector-orientedreasoning based on simple linear algebra between words. Since many differentmethods have been proposed for learning document representations, it is naturalto ask whether there is also linear structure in these learned representationsto allow similar reasoning at document level. To answer this question, wedesign a new document analogy task for testing the semantic regularities indocument representations, and conduct empirical evaluations over severalstate-of-the-art document representation models. The results reveal that neuralembedding based document representations work better on this analogy task thanconventional methods, and we provide some preliminary explanations over theseobservations.
arxiv-17100-195 | Clustering Time-Series Energy Data from Smart Meters | http://arxiv.org/abs/1603.07602 | author:Alexander Lavin, Diego Klabjan category:stat.ML published:2016-03-24 summary:Investigations have been performed into using clustering methods in datamining time-series data from smart meters. The problem is to identify patternsand trends in energy usage profiles of commercial and industrial customers over24-hour periods, and group similar profiles. We tested our method on energyusage data provided by several U.S. power utilities. The results show accurategrouping of accounts similar in their energy usage patterns, and potential forthe method to be utilized in energy efficiency programs.
arxiv-17100-196 | Source Localization on Graphs via l1 Recovery and Spectral Graph Theory | http://arxiv.org/abs/1603.07584 | author:Rodrigo Pena, Xavier Bresson, Pierre Vandergheynst category:cs.LG published:2016-03-24 summary:We cast the problem of source localization on graphs as the simultaneousproblem of sparse recovery and diffusion ker- nel learning. An l1regularization term enforces the sparsity constraint while we recover thesources of diffusion from a single snapshot of the diffusion process. Thediffusion ker- nel is estimated by assuming the process to be as generic as thestandard heat diffusion. We show with synthetic data that we can concomitantlylearn the diffusion kernel and the sources, given an estimated initialization.We validate our model with cholera mortality and atmospheric tracer diffusiondata, showing also that the accuracy of the solution depends on theconstruction of the graph from the data points.
arxiv-17100-197 | Mapping Out Narrative Structures and Dynamics Using Networks and Textual Information | http://arxiv.org/abs/1604.03029 | author:Semi Min, Juyong Park category:cs.CL cs.SI physics.soc-ph published:2016-03-24 summary:Human communication is often executed in the form of a narrative, an accountof connected events composed of characters, actions, and settings. A coherentnarrative structure is therefore a requisite for a well-formulated narrative --be it fictional or nonfictional -- for informative and effective communication,opening up the possibility of a deeper understanding of a narrative by studyingits structural properties. In this paper we present a network-based frameworkfor modeling and analyzing the structure of a narrative, which is furtherexpanded by incorporating methods from computational linguistics to utilize thenarrative text. Modeling a narrative as a dynamically unfolding system, wecharacterize its progression via the growth patterns of the character network,and use sentiment analysis and topic modeling to represent the actual contentof the narrative in the form of interaction maps between characters withassociated sentiment values and keywords. This is a network framework advancedbeyond the simple occurrence-based one most often used until now, allowing oneto utilize the unique characteristics of a given narrative to a high degree.Given the ubiquity and importance of narratives, such advanced network-basedrepresentation and analysis framework may lead to a more systematic modelingand understanding of narratives for social interactions, expression of humansentiments, and communication.
arxiv-17100-198 | Weakly Supervised Semantic Labelling and Instance Segmentation | http://arxiv.org/abs/1603.07485 | author:Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, Bernt Schiele category:cs.CV published:2016-03-24 summary:Semantic labelling and instance segmentation are two tasks that requireparticularly costly annotations. Starting from weak supervision in the form ofbounding box detection annotations, we propose to recursively train a convnetsuch that outputs are improved after each iteration. We explore which aspectsaffect the recursive training, and which is the most suitable box-guidedsegmentation to use as initialisation. Our results improve significantly overpreviously reported ones, even when using rectangles as rough initialisation.Overall, our weak supervision approach reaches ~95% of the quality of the fullysupervised model, both for semantic labelling and instance segmentation.
arxiv-17100-199 | Fine-scale Surface Normal Estimation using a Single NIR Image | http://arxiv.org/abs/1603.07475 | author:Youngjin Yoon, Gyeongmin Choe, Namil Kim, Joon-Young Lee, In So Kweon category:cs.CV published:2016-03-24 summary:We present surface normal estimation using a single near infrared (NIR)image. We are focusing on fine-scale surface geometry captured with anuncalibrated light source. To tackle this ill-posed problem, we adopt agenerative adversarial network which is effective in recovering a sharp output,which is also essential for fine-scale surface normal estimation. Weincorporate angular error and integrability constraint into the objectivefunction of the network to make estimated normals physically meaningful. Wetrain and validate our network on a recent NIR dataset, and also evaluate thegenerality of our trained model by using new external datasets which arecaptured with a different camera under different environment.
arxiv-17100-200 | Deep Extreme Feature Extraction: New MVA Method for Searching Particles in High Energy Physics | http://arxiv.org/abs/1603.07454 | author:Chao Ma, Tianchenghou, Bin Lan, Jinhui Xu, Zhenhua Zhang category:cs.LG cs.NE published:2016-03-24 summary:In this paper, we present Deep Extreme Feature Extraction (DEFE), a newensemble MVA method for searching $\tau^{+}\tau^{-}$ channel of Higgs bosons inhigh energy physics. DEFE can be viewed as a deep ensemble learning scheme thattrains a strongly diverse set of neural feature learners without explicitlyencouraging diversity and penalizing correlations. This is achieved by adoptingan implicit neural controller (not involved in feedforward compuation) thatdirectly controls and distributes gradient flows from higher level deepprediction network. Such model-independent controller results in that everysingle local feature learned are used in the feature-to-output mapping stage,avoiding the blind averaging of features. DEFE makes the ensembles 'deep' inthe sense that it allows deep post-process of these features that tries tolearn to select and abstract the ensemble of neural feature learners. With theapplication of this model, a selection regions full of signal process can beobtained through the training of a miniature collision events set. Incomparison of the Classic Deep Neural Network, DEFE shows a state-of-the-artperformance: the error rate has decreased by about 37\%, the accuracy hasbroken through 90\% for the first time, along with the discovery significancehas reached a standard deviation of 6.0 $\sigma$. Experimental data shows that,DEFE is able to train an ensemble of discriminative feature learners thatboosts the overperformance of final prediction.
arxiv-17100-201 | Pixel-Level Domain Transfer | http://arxiv.org/abs/1603.07442 | author:Donggeun Yoo, Namil Kim, Sunggyun Park, Anthony S. Paek, In So Kweon category:cs.CV cs.AI published:2016-03-24 summary:We present an image-conditional image generation model. The model transfersan input domain to a target domain in semantic level, and generates the targetimage in pixel level. To generate realistic target images, we employ thereal/fake-discriminator in Generative Adversarial Nets, but also introduce anovel domain-discriminator to make the generated image relevant to the inputimage. We verify our model through a challenging task of generating a piece ofclothing from an input image of a dressed person. We present a high qualityclothing dataset containing the two domains, and succeed in demonstratingdecent results.
arxiv-17100-202 | Viewpoint Invariant 3D Human Pose Estimation with Recurrent Error Feedback | http://arxiv.org/abs/1603.07076 | author:Albert Haque, Boya Peng, Zelun Luo, Alexandre Alahi, Serena Yeung, Li Fei-Fei category:cs.CV published:2016-03-23 summary:We propose a viewpoint invariant model for 3D human pose estimation from asingle depth image. To achieve viewpoint invariance, our deep discriminativemodel embeds local regions into a learned viewpoint invariant feature space.Formulated as a multi-task learning problem, our model is able to selectivelypredict partial poses in the presence of noise and occlusion. Our approachleverages a convolutional and recurrent network with a top-down error feedbackmechanism to self-correct previous pose estimates in an end-to-end manner. Weevaluate our model on a previously published depth dataset and a newlycollected human pose dataset containing 100K annotated depth images fromextreme viewpoints. Experiments show that our model achieves competitiveperformance on frontal views while achieving state-of-the-art performance onalternate viewpoints.
arxiv-17100-203 | Recurrent Neural Network Encoder with Attention for Community Question Answering | http://arxiv.org/abs/1603.07044 | author:Wei-Ning Hsu, Yu Zhang, James Glass category:cs.CL cs.LG cs.NE published:2016-03-23 summary:We apply a general recurrent neural network (RNN) encoder framework tocommunity question answering (cQA) tasks. Our approach does not rely on anylinguistic processing, and can be applied to different languages or domains.Further improvements are observed when we extend the RNN encoders with a neuralattention mechanism that encourages reasoning over entire sequences. To dealwith practical issues such as data sparsity and imbalanced labels, we applyvarious techniques such as transfer learning and multitask learning. Ourexperiments on the SemEval-2016 cQA task show 10% improvement on a MAP scorecompared to an information retrieval-based approach, and achieve comparableperformance to a strong handcrafted feature-based method.
arxiv-17100-204 | Lightweight Unsupervised Domain Adaptation by Convolutional Filter Reconstruction | http://arxiv.org/abs/1603.07234 | author:Rahaf Aljundi, Tinne Tuytelaars category:cs.CV published:2016-03-23 summary:End-to-end learning methods have achieved impressive results in many areas ofcomputer vision. At the same time, these methods still suffer from adegradation in performance when testing on new datasets that stem from adifferent distribution. This is known as the domain shift effect. Recentlyproposed adaptation methods focus on retraining the network parameters.However, this requires access to all (labeled) source data, a large amount of(unlabeled) target data, and plenty of computational resources. In this work,we propose a lightweight alternative, that allows adapting to the target domainbased on a limited number of target samples in a matter of minutes rather thanhours, days or even weeks. To this end, we first analyze the output of eachconvolutional layer from a domain adaptation perspective. Surprisingly, we findthat already at the very first layer, domain shift effects pop up. We thenpropose a new domain adaptation method, where first layer convolutional filtersthat are badly affected by the domain shift are reconstructed based on lessaffected ones. This improves the performance of the deep network on variousbenchmark datasets.
arxiv-17100-205 | Cosolver2B: An Efficient Local Search Heuristic for the Travelling Thief Problem | http://arxiv.org/abs/1603.07051 | author:Mohamed El Yafrani, Belaïd Ahiod category:cs.AI cs.DS cs.NE published:2016-03-23 summary:Real-world problems are very difficult to optimize. However, many researchershave been solving benchmark problems that have been extensively investigatedfor the last decades even if they have very few direct applications. TheTraveling Thief Problem (TTP) is a NP-hard optimization problem that aims toprovide a more realistic model. TTP targets particularly routing problem underpacking/loading constraints which can be found in supply chain management andtransportation. In this paper, TTP is presented and formulated mathematically.A combined local search algorithm is proposed and compared with Random LocalSearch (RLS) and Evolutionary Algorithm (EA). The obtained results are quitepromising since new better solutions were found.
arxiv-17100-206 | Semantic Object Parsing with Graph LSTM | http://arxiv.org/abs/1603.07063 | author:Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, Shuicheng Yan category:cs.CV published:2016-03-23 summary:By taking the semantic object parsing task as an exemplar applicationscenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network,which is the generalization of LSTM from sequential data or multi-dimensionaldata to general graph-structured data. Particularly, instead of evenly andfixedly dividing an image to pixels or patches in existing multi-dimensionalLSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take eacharbitrary-shaped superpixel as a semantically consistent node, and adaptivelyconstruct an undirected graph for each image, where the spatial relations ofthe superpixels are naturally used as edges. Constructed on such an adaptivegraph topology, the Graph LSTM is more naturally aligned with the visualpatterns in the image (e.g., object boundaries or appearance similarities) andprovides a more economical information propagation route. Furthermore, for eachoptimization step over Graph LSTM, we propose to use a confidence-driven schemeto update the hidden and memory states of nodes progressively till all nodesare updated. In addition, for each node, the forgets gates are adaptivelylearned to capture different degrees of semantic correlation with neighboringnodes. Comprehensive evaluations on four diverse semantic object parsingdatasets well demonstrate the significant superiority of our Graph LSTM overother state-of-the-art solutions.
arxiv-17100-207 | Predicting Glaucoma Visual Field Loss by Hierarchically Aggregating Clustering-based Predictors | http://arxiv.org/abs/1603.07094 | author:Motohide Higaki, Kai Morino, Hiroshi Murata, Ryo Asaoka, Kenji Yamanishi category:stat.ML cs.LG published:2016-03-23 summary:This study addresses the issue of predicting the glaucomatous visual fieldloss from patient disease datasets. Our goal is to accurately predict theprogress of the disease in individual patients. As very few measurements areavailable for each patient, it is difficult to produce good predictors forindividuals. A recently proposed clustering-based method enhances the power ofprediction using patient data with similar spatiotemporal patterns. Eachpatient is categorized into a cluster of patients, and a predictive model isconstructed using all of the data in the class. Predictions are highlydependent on the quality of clustering, but it is difficult to identify thebest clustering method. Thus, we propose a method for aggregating cluster-basedpredictors to obtain better prediction accuracy than from a singlecluster-based prediction. Further, the method shows very high performances byhierarchically aggregating experts generated from several cluster-basedmethods. We use real datasets to demonstrate that our method performssignificantly better than conventional clustering-based and patient-wiseregression methods, because the hierarchical aggregating strategy has amechanism whereby good predictors in a small community can thrive.
arxiv-17100-208 | Do We Really Need to Collect Millions of Faces for Effective Face Recognition? | http://arxiv.org/abs/1603.07057 | author:Iacopo Masi, Anh Tuan Tran, Jatuporn Toy Leksut, Tal Hassner, Gerard Medioni category:cs.CV published:2016-03-23 summary:Face recognition capabilities have recently made extraordinary leaps. Thoughthis progress is at least partially due to ballooning training set sizes --huge numbers of face images downloaded and labeled for identity -- it is notclear if the formidable task of collecting so many images is truly necessary.We propose a far more accessible means of increasing training data sizes forface recognition systems. Rather than manually harvesting and labeling morefaces, we simply synthesize them. We describe novel methods of enriching anexisting dataset with important facial appearance variations by manipulatingthe faces it contains. We further apply this synthesis approach when matchingquery images represented using a standard convolutional neural network. Theeffect of training and testing with synthesized images is extensively tested onthe LFW and IJB-A (verification and identification) benchmarks and Janus CS2.The performances obtained by our approach match state of the art resultsreported by systems trained on millions of downloaded images.
arxiv-17100-209 | Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos | http://arxiv.org/abs/1603.07120 | author:Amir Shahroudy, Tian-Tsong Ng, Yihong Gong, Gang Wang category:cs.CV published:2016-03-23 summary:Single modality action recognition on RGB or depth sequences has beenextensively explored recently. It is generally accepted that each of these twomodalities has different strengths and limitations for the task of actionrecognition. Therefore, analysis of the RGB+D videos can help us to betterstudy the complementary properties of these two types of modalities and achievehigher levels of performance. In this paper, we propose a new deep autoencoderbased shared-specific feature factorization network to separate inputmultimodal signals into a hierarchy of components. Further, based on thestructure of the features, a structured sparsity learning machine is proposedwhich utilizes mixed norms to apply regularization within components and groupselection between them for better classification performance. Our experimentalresults show the effectiveness of our cross-modality feature analysis frameworkby achieving state-of-the-art accuracy for action classification on fourchallenging benchmark datasets, for which we reduce the error rate by more than40% in three datasets and saturating the benchmark with perfect accuracy forthe other one.
arxiv-17100-210 | CONDITOR1: Topic Maps and DITA labelling tool for textual documents with historical information | http://arxiv.org/abs/1603.07313 | author:Piedad Garrido, Jesus Tramullas, Manuel Coll category:cs.DL cs.CL cs.IR published:2016-03-23 summary:Conditor is a software tool which works with textual documents containinghistorical information. The purpose of this work two-fold: firstly to show thevalidity of the developed engine to correctly identify and label the entitiesof the universe of discourse with a labelled-combined XTM-DITA model. Secondlyto explain the improvements achieved in the information retrieval processthanks to the use of a object-oriented database (JPOX) as well as itsintegration into the Lucene-type database search process to not only accomplishmore accurate searches, but to also help the future development of arecommender system. We finish with a brief demo in a 3D-graph of the results ofthe aforementioned search.
arxiv-17100-211 | Robust cDNA microarray image segmentation and analysis technique based on Hough circle transform | http://arxiv.org/abs/1603.07123 | author:R. M. Farouk, M. A. SayedElahl category:cs.CV published:2016-03-23 summary:One of the most challenging tasks in microarray image analysis is spotsegmentation. A solution to this problem is to provide an algorithm than can beused to find any spot within the microarray image. Circular HoughTransformation (CHT) is a powerful feature extraction technique used in imageanalysis, computer vision, and digital image processing. CHT algorithm isapplied on the cDNA microarray images to develop the accuracy and theefficiency of the spots localization, addressing and segmentation process. Thepurpose of the applied technique is to find imperfect instances of spots withina certain class of circles by applying a voting procedure on the cDNAmicroarray images for spots localization, addressing and characterizing thepixels of each spot into foreground pixels and background simultaneously.Intensive experiments on the University of North Carolina (UNC) microarraydatabase indicate that the proposed method is superior to the K-means methodand the Support vector machine (SVM). Keywords: Hough circle transformation,cDNA microarray image analysis, cDNA microarray image segmentation, spotslocalization and addressing, spots segmentation
arxiv-17100-212 | Learning Mixtures of Plackett-Luce models | http://arxiv.org/abs/1603.07323 | author:Zhibing Zhao, Peter Piech, Lirong Xia category:cs.LG published:2016-03-23 summary:In this paper we address the identifiability and efficient learning problemsof finite mixtures of Plackett-Luce models for rank data. We prove that for any$k\geq 2$, the mixture of $k$ Plackett-Luce models for no more than $2k-1$alternatives is non-identifiable and this bound is tight for $k=2$. For genericidentifiability, we prove that the mixture of $k$ Plackett-Luce models over $m$alternatives is generically identifiable if $k\leq\lfloor\frac {m-2}2\rfloor!$. We also propose an efficient generalized method of moments (GMM) algorithm tolearn the mixture of two Plackett-Luce models and show that the algorithm isconsistent. Our experiments show that our GMM algorithm is significantly fasterthan the EMM algorithm by Gormley and Murphy (2008), while achievingcompetitive statistical efficiency.
arxiv-17100-213 | A Richly Annotated Dataset for Pedestrian Attribute Recognition | http://arxiv.org/abs/1603.07054 | author:Dangwei Li, Zhang Zhang, Xiaotang Chen, Haibin Ling, Kaiqi Huang category:cs.CV published:2016-03-23 summary:In this paper, we aim to improve the dataset foundation for pedestrianattribute recognition in real surveillance scenarios. Recognition of humanattributes, such as gender, and clothes types, has great prospects in realapplications. However, the development of suitable benchmark datasets forattribute recognition remains lagged behind. Existing human attribute datasetsare collected from various sources or an integration of pedestrianre-identification datasets. Such heterogeneous collection poses a big challengeon developing high quality fine-grained attribute recognition algorithms.Furthermore, human attribute recognition are generally severely affected byenvironmental or contextual factors, such as viewpoints, occlusions and bodyparts, while existing attribute datasets barely care about them. To tacklethese problems, we build a Richly Annotated Pedestrian (RAP) dataset from realmulti-camera surveillance scenarios with long term collection, where datasamples are annotated with not only fine-grained human attributes but alsoenvironmental and contextual factors. RAP has in total 41,585 pedestriansamples, each of which is annotated with 72 attributes as well as viewpoints,occlusions, body parts information. To our knowledge, the RAP dataset is thelargest pedestrian attribute dataset, which is expected to greatly promote thestudy of large-scale attribute recognition systems. Furthermore, we empiricallyanalyze the effects of different environmental and contextual factors onpedestrian attribute recognition. Experimental results demonstrate thatviewpoints, occlusions and body parts information could assist attributerecognition a lot in real applications.
arxiv-17100-214 | On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis | http://arxiv.org/abs/1603.07294 | author:James Foulds, Joseph Geumlek, Max Welling, Kamalika Chaudhuri category:cs.LG cs.AI cs.CR stat.ML published:2016-03-23 summary:Bayesian inference has great promise for the privacy-preserving analysis ofsensitive data, as posterior sampling automatically preserves differentialprivacy, an algorithmic notion of data privacy, under certain conditions (Wanget al., 2015). While Wang et al. (2015)'s one posterior sample (OPS) approachelegantly provides privacy "for free," it is data inefficient in the sense ofasymptotic relative efficiency (ARE). We show that a simple alternative basedon the Laplace mechanism, the workhorse technique of differential privacy, isas asymptotically efficient as non-private posterior inference, under generalassumptions. The Laplace mechanism has additional practical advantagesincluding efficient use of the privacy budget for MCMC. We demonstrate thepracticality of our approach on a time-series analysis of sensitive militaryrecords from the Afghanistan and Iraq wars disclosed by the Wikileaksorganization.
arxiv-17100-215 | Debugging Machine Learning Tasks | http://arxiv.org/abs/1603.07292 | author:Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, Deepak Vijaykeerthy category:cs.LG cs.AI cs.PL stat.ML D.2.5; I.2.3 published:2016-03-23 summary:Unlike traditional programs (such as operating systems or word processors)which have large amounts of code, machine learning tasks use programs withrelatively small amounts of code (written in machine learning libraries), butvoluminous amounts of data. Just like developers of traditional programs debugerrors in their code, developers of machine learning tasks debug and fix errorsin their data. However, algorithms and tools for debugging and fixing errors indata are less common, when compared to their counterparts for detecting andfixing errors in code. In this paper, we consider classification tasks whereerrors in training data lead to misclassifications in test points, and proposean automated method to find the root causes of such misclassifications. Ourroot cause analysis is based on Pearl's theory of causation, and uses Pearl'sPS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,encodes the computation of PS as a probabilistic program, and uses recent workon probabilistic programs and transformations on probabilistic programs (alongwith gray-box models of machine learning algorithms) to efficiently compute PS.Psi is able to identify root causes of data errors in interesting data sets.
arxiv-17100-216 | BreakingNews: Article Annotation by Image and Text Processing | http://arxiv.org/abs/1603.07141 | author:Arnau Ramisa, Fei Yan, Francesc Moreno-Noguer, Krystian Mikolajczyk category:cs.CV published:2016-03-23 summary:Building upon recent Deep Neural Network architectures, current approacheslying in the intersection of computer vision and natural language processinghave achieved unprecedented breakthroughs in tasks like automatic captioning orimage retrieval. Most of these learning methods, though, rely on large trainingsets of images associated with human annotations that specifically describe thevisual content. In this paper we propose to go a step further and explore themore complex cases where textual descriptions are loosely related to theimages. We focus on the particular domain of News articles in which the textualcontent often expresses connotative and ambiguous relations that are onlysuggested but not directly inferred from images. We introduce new deep learningmethods that address source detection, popularity prediction, articleillustration and geolocation of articles. An adaptive CNN architecture isproposed, that shares most of the structure for all the tasks, and is suitablefor multitask and transfer learning. Deep Canonical Correlation Analysis isdeployed for article illustration, and a new loss function based on GreatCircle Distance is proposed for geolocation. Furthermore, we presentBreakingNews, a novel dataset with approximately 100K news articles includingimages, text and captions, and enriched with heterogeneous meta-data (such asGPS coordinates and popularity metrics). We show this dataset to be appropriateto explore all aforementioned problems, for which we provide a baselineperformance using various Deep Learning architectures, and differentrepresentations of the textual and visual features. We report very promisingresults and bring to light several limitations of current state-of-the-art inthis kind of domain, which we hope will help spur progress in the field.
arxiv-17100-217 | The Anatomy of a Search and Mining System for Digital Archives | http://arxiv.org/abs/1603.07150 | author:Martyn Harris, Mark Levene, Dell Zhang, Dan Levene category:cs.DL cs.CL cs.IR published:2016-03-23 summary:Samtla (Search And Mining Tools with Linguistic Analysis) is a digitalhumanities system designed in collaboration with historians and linguists toassist them with their research work in quantifying the content of any textualcorpora through approximate phrase search and document comparison. Theretrieval engine uses a character-based n-gram language model rather than theconventional word-based one so as to achieve great flexibility in languageagnostic query processing. The index is implemented as a space-optimised character-based suffix treewith an accompanying database of document content and metadata. A number oftext mining tools are integrated into the system to allow researchers todiscover textual patterns, perform comparative analysis, and find out what iscurrently popular in the research community. Herein we describe the system architecture, user interface, models andalgorithms, and data storage of the Samtla system. We also present several casestudies of its usage in practice together with an evaluation of the systems'ranking performance through crowdsourcing.
arxiv-17100-218 | Enabling Cognitive Intelligence Queries in Relational Databases using Low-dimensional Word Embeddings | http://arxiv.org/abs/1603.07185 | author:Rajesh Bordawekar, Oded Shmueli category:cs.CL cs.DB published:2016-03-23 summary:We apply distributed language embedding methods from Natural LanguageProcessing to assign a vector to each database entity associated token (forexample, a token may be a word occurring in a table row, or the name of acolumn). These vectors, of typical dimension 200, capture the meaning of tokensbased on the contexts in which the tokens appear together. To form vectors, weapply a learning method to a token sequence derived from the database. Wedescribe various techniques for extracting token sequences from a database. Thetechniques differ in complexity, in the token sequences they output and in thedatabase information used (e.g., foreign keys). The vectors can be used toalgebraically quantify semantic relationships between the tokens such assimilarities and analogies. Vectors enable a dual view of the data: relationaland (meaningful rather than purely syntactical) text. We introduce and explorea new class of queries called cognitive intelligence (CI) queries that extractinformation from the database based, in part, on the relationships encoded byvectors. We have implemented a prototype system on top of Spark to exhibit thepower of CI queries. Here, CI queries are realized via SQL UDFs. This powergoes far beyond text extensions to relational systems due to the informationencoded in vectors. We also consider various extensions to the basic scheme,including using a collection of views derived from the database to focus on adomain of interest, utilizing vectors and/or text from external sources,maintaining vectors as the database evolves and exploring a database withoututilizing its schema. For the latter, we consider minimal extensions to SQL tovastly improve query expressiveness.
arxiv-17100-219 | Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices | http://arxiv.org/abs/1603.07341 | author:Tayfun Gokmen, Yurii Vlasov category:cs.LG cs.NE stat.ML published:2016-03-23 summary:In recent years, deep neural networks (DNN) have demonstrated significantbusiness impact in large scale analysis and classification tasks such as speechrecognition, visual object detection, pattern extraction, etc. Training oflarge DNNs, however, is universally considered as time consuming andcomputationally intensive task that demands datacenter-scale computationalresources recruited for many days. Here we propose a concept of resistiveprocessing unit (RPU) devices that can potentially accelerate DNN training byorders of magnitude while using much less power. The proposed RPU device canstore and update the weight values locally thus minimizing data movement duringtraining and allowing to fully exploit the locality and the parallelism of thetraining algorithm. We identify the RPU device and system specifications forimplementation of an accelerator chip for DNN training in a realisticCMOS-compatible technology. For large DNNs with about 1 billion weights thismassively parallel RPU architecture can achieve acceleration factors of 30,000Xcompared to state-of-the-art microprocessors while providing power efficiencyof 84,000 GigaOps/s/W. Problems that currently require days of training on adatacenter-size cluster with thousands of machines can be addressed withinhours on a single RPU accelerator. A system consisted of a cluster of RPUaccelerators will be able to tackle Big Data problems with trillions ofparameters that is impossible to address today like, for example, naturalspeech recognition and translation between all world languages, real-timeanalytics on large streams of business and scientific data, integration andanalysis of multimodal sensory data flows from massive number of IoT (Internetof Things) sensors.
arxiv-17100-220 | Weakly-Supervised Semantic Segmentation using Motion Cues | http://arxiv.org/abs/1603.07188 | author:Pavel Tokmakov, Karteek Alahari, Cordelia Schmid category:cs.CV published:2016-03-23 summary:Fully convolutional neural networks (FCNNs) trained on a large number ofimages with strong pixel-level annotations have become the new state of the artfor the semantic segmentation task. While there have been recent attempts tolearn FCNNs from image-level weak annotations, they need additionalconstraints, such as the size of an object, to obtain reasonable performance.To address this issue, we present motion-CNN (M-CNN), a novel FCNN frameworkwhich incorporates motion cues and is learned from video-level weakannotations. Our learning scheme to train the network uses motion segments assoft constraints, thereby handling noisy motion information. When trained onweakly-annotated videos, our method outperforms the state-of-the-art approachof Papandreou et al. by a factor of 2 on the PASCAL VOC 2012 image segmentationbenchmark. We also demonstrate that the performance of M-CNN learned with 150weak video annotations is on par with state-of-the-art weakly-supervisedmethods trained with thousands of images. Finally, M-CNN substantiallyoutperforms recent approaches in a related task of video co-localization on theYouTube-Objects dataset.
arxiv-17100-221 | A Decentralized Quasi-Newton Method for Dual Formulations of Consensus Optimization | http://arxiv.org/abs/1603.07195 | author:Mark Eisen, Aryan Mokhtari, Alejandro Ribeiro category:math.OC cs.DC cs.LG published:2016-03-23 summary:This paper considers consensus optimization problems where each node of anetwork has access to a different summand of an aggregate cost function. Nodestry to minimize the aggregate cost function, while they exchange informationonly with their neighbors. We modify the dual decomposition method toincorporate a curvature correction inspired by theBroyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method. The resulting dualD-BFGS method is a fully decentralized algorithm in which nodes approximatecurvature information of themselves and their neighbors through thesatisfaction of a secant condition. Dual D-BFGS is of interest in consensusoptimization problems that are not well conditioned, making first orderdecentralized methods ineffective, and in which second order information is notreadily available, making decentralized second order methods infeasible.Asynchronous implementation is discussed and convergence of D-BFGS isestablished formally for both synchronous and asynchronous implementations.Performance advantages relative to alternative decentralized algorithms areshown numerically.
arxiv-17100-222 | Face Recognition Using Deep Multi-Pose Representations | http://arxiv.org/abs/1603.07388 | author:Wael AbdAlmageed, Yue Wua, Stephen Rawlsa, Shai Harel, Tal Hassner, Iacopo Masi, Jongmoo Choi, Jatuporn Toy Leksut, Jungyeon Kim, Prem Natarajan, Ram Nevatia, Gerard Medioni category:cs.CV published:2016-03-23 summary:We introduce our method and system for face recognition using multiplepose-aware deep learning models. In our representation, a face image isprocessed by several pose-specific deep convolutional neural network (CNN)models to generate multiple pose-specific features. 3D rendering is used togenerate multiple face poses from the input image. Sensitivity of therecognition system to pose variations is reduced since we use an ensemble ofpose-specific CNN features. The paper presents extensive experimental resultson the effect of landmark detection, CNN layer selection and pose modelselection on the performance of the recognition pipeline. Our novelrepresentation achieves better results than the state-of-the-art on IARPA's CS2and NIST's IJB-A in both verification and identification (i.e. search) tasks.
arxiv-17100-223 | Predicting litigation likelihood and time to litigation for patents | http://arxiv.org/abs/1603.07394 | author:Papis Wongchaisuwat, Diego Klabjan, John O. McGinnis category:stat.ML published:2016-03-23 summary:Patent lawsuits are costly and time-consuming. An ability to forecast apatent litigation and time to litigation allows companies to better allocatebudget and time in managing their patent portfolios. We develop predictivemodels for estimating the likelihood of litigation for patents and the expectedtime to litigation based on both textual and non-textual features. Our workfocuses on improving the state-of-the-art by relying on a different set offeatures and employing more sophisticated algorithms with more realistic data.The rate of patent litigations is very low, which consequently makes theproblem difficult. The initial model for predicting the likelihood is furthermodified to capture a time-to-litigation perspective.
arxiv-17100-224 | A guide to convolution arithmetic for deep learning | http://arxiv.org/abs/1603.07285 | author:Vincent Dumoulin, Francesco Visin category:stat.ML cs.LG cs.NE published:2016-03-23 summary:We introduce a guide to help deep learning practitioners understand andmanipulate convolutional neural network architectures. The guide clarifies therelationship between various properties (input shape, kernel shape, zeropadding, strides and output shape) of convolutional, pooling and transposedconvolutional layers, as well as the relationship between convolutional andtransposed convolutional layers. Relationships are derived for various cases,and are illustrated in order to make them intuitive.
arxiv-17100-225 | Neural Summarization by Extracting Sentences and Words | http://arxiv.org/abs/1603.07252 | author:Jianpeng Cheng, Mirella Lapata category:cs.CL published:2016-03-23 summary:Traditional approaches to extractive summarization rely heavily onhuman-engineered features. In this work we propose a data-driven approach basedon neural networks and continuous sentence features. We develop a generalframework for single-document summarization composed of a hierarchical documentencoder and an attention-based extractor. This architecture allows us todevelop different classes of summarization models which can extract sentencesor words. We train our models on large scale corpora containing hundreds ofthousands of document-summary pairs. Experimental results on two summarizationdatasets demonstrate that our models obtain results comparable to the state ofthe art without any access to linguistic annotation.
arxiv-17100-226 | Evaluating semantic models with word-sentence relatedness | http://arxiv.org/abs/1603.07253 | author:Kimberly Glasgow, Matthew Roos, Amy Haufler, Mark Chevillet, Michael Wolmetz category:cs.CL published:2016-03-23 summary:Semantic textual similarity (STS) systems are designed to encode and evaluatethe semantic similarity between words, phrases, sentences, and documents. Onemethod for assessing the quality or authenticity of semantic informationencoded in these systems is by comparison with human judgments. A data set forevaluating semantic models was developed consisting of 775 Englishword-sentence pairs, each annotated for semantic relatedness by human ratersengaged in a Maximum Difference Scaling (MDS) task, as well as a fasteralternative task. As a sample application of this relatedness data,behavior-based relatedness was compared to the relatedness computed via fouroff-the-shelf STS models: n-gram, Latent Semantic Analysis (LSA), Word2Vec, andUMBC Ebiquity. Some STS models captured much of the variance in the humanjudgments collected, but they were not sensitive to the implicatures andentailments that were processed and considered by the participants. All textstimuli and judgment data have been made freely available.
arxiv-17100-227 | Global-Local Face Upsampling Network | http://arxiv.org/abs/1603.07235 | author:Oncel Tuzel, Yuichi Taguchi, John R. Hershey category:cs.CV cs.LG published:2016-03-23 summary:Face hallucination, which is the task of generating a high-resolution faceimage from a low-resolution input image, is a well-studied problem that isuseful in widespread application areas. Face hallucination is particularlychallenging when the input face resolution is very low (e.g., 10 x 12 pixels)and/or the image is captured in an uncontrolled setting with large pose andillumination variations. In this paper, we revisit the algorithm introduced in[1] and present a deep interpretation of this framework that achievesstate-of-the-art under such challenging scenarios. In our deep networkarchitecture the global and local constraints that define a face can beefficiently modeled and learned end-to-end using training data. Conceptuallyour network design can be partitioned into two sub-networks: the first oneimplements the holistic face reconstruction according to global constraints,and the second one enhances face-specific details and enforces local patchstatistics. We optimize the deep network using a new loss function forsuper-resolution that combines reconstruction error with a learned face qualitymeasure in adversarial setting, producing improved visual results. We conductextensive experiments in both controlled and uncontrolled setups and show thatour algorithm improves the state of the art both numerically and visually.
arxiv-17100-228 | A Tutorial on Deep Neural Networks for Intelligent Systems | http://arxiv.org/abs/1603.07249 | author:Juan C. Cuevas-Tello, Manuel Valenzuela-Rendon, Juan A. Nolazco-Flores category:cs.NE cs.LG J.4.6 published:2016-03-23 summary:Developing Intelligent Systems involves artificial intelligence approachesincluding artificial neural networks. Here, we present a tutorial of DeepNeural Networks (DNNs), and some insights about the origin of the term "deep";references to deep learning are also given. Restricted Boltzmann Machines,which are the core of DNNs, are discussed in detail. An example of a simpletwo-layer network, performing unsupervised learning for unlabeled data, isshown. Deep Belief Networks (DBNs), which are used to build networks with morethan two layers, are also described. Moreover, examples for supervised learningwith DNNs performing simple prediction and classification tasks, are presentedand explained. This tutorial includes two intelligent pattern recognitionapplications: hand- written digits (benchmark known as MNIST) and speechrecognition.
arxiv-17100-229 | Gaussian Process Morphable Models | http://arxiv.org/abs/1603.07254 | author:Marcel Lüthi, Christoph Jud, Thomas Gerig, Thomas Vetter category:cs.CV published:2016-03-23 summary:Statistical shape models (SSMs) represent a class of shapes as a normaldistribution of point variations, whose parameters are estimated from exampleshapes. Principal component analysis (PCA) is applied to obtain alow-dimensional representation of the shape variation in terms of the leadingprincipal components. In this paper, we propose a generalization of SSMs,called Gaussian Process Morphable Models (GPMMs). We model the shape variationswith a Gaussian process, which we represent using the leading components of itsKarhunen-Loeve expansion. To compute the expansion, we make use of anapproximation scheme based on the Nystrom method. The resulting model can beseen as a continuous analogon of an SSM. However, while for SSMs the shapevariation is restricted to the span of the example data, with GPMMs we candefine the shape variation using any Gaussian process. For example, we canbuild shape models that correspond to classical spline models, and thus do notrequire any example data. Furthermore, Gaussian processes make it possible tocombine different models. For example, an SSM can be extended with a splinemodel, to obtain a model that incorporates learned shape characteristics, butis flexible enough to explain shapes that cannot be represented by the SSM. Weintroduce a simple algorithm for fitting a GPMM to a surface or image. Thisresults in a non-rigid registration approach, whose regularization propertiesare defined by a GPMM. We show how we can obtain different registrationschemes,including methods for multi-scale, spatially-varying or hybridregistration, by constructing an appropriate GPMM. As our approach strictlyseparates modelling from the fitting process, this is all achieved withoutchanges to the fitting algorithm. We show the applicability and versatility ofGPMMs on a clinical use case, where the goal is the model-based segmentation of3D forearm images.
arxiv-17100-230 | Stacked Hourglass Networks for Human Pose Estimation | http://arxiv.org/abs/1603.06937 | author:Alejandro Newell, Kaiyu Yang, Jia Deng category:cs.CV published:2016-03-22 summary:This work introduces a novel Convolutional Network architecture for the taskof human pose estimation. Features are processed across all scales andconsolidated to best capture the various spatial relationships associated withthe body. We show how repeated bottom-up, top-down processing used inconjunction with intermediate supervision is critical to improving theperformance of the network. We refer to the architecture as a 'stackedhourglass' network based on the successive steps of pooling and upsampling thatare done to produce a final set of estimates. State-of-the-art results areachieved on the FLIC and MPII benchmarks outcompeting all recent methods.
arxiv-17100-231 | Inference via Message Passing on Partially Labeled Stochastic Block Models | http://arxiv.org/abs/1603.06923 | author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST stat.ML stat.TH published:2016-03-22 summary:We study the community detection and recovery problem in partially-labeledstochastic block models (SBM). We develop a fast linearized message-passingalgorithm to reconstruct labels for SBM (with $n$ nodes, $k$ blocks, $p,q$intra and inter block connectivity) when $\delta$ proportion of node labels arerevealed. The signal-to-noise ratio ${\sf SNR}(n,k,p,q,\delta)$ is shown tocharacterize the fundamental limitations of inference via local algorithms. Onthe one hand, when ${\sf SNR}>1$, the linearized message-passing algorithmprovides the statistical inference guarantee with mis-classification rate atmost $\exp(-({\sf SNR}-1)/2)$, thus interpolating smoothly between strong andweak consistency. This exponential dependence improves upon the known errorrate $({\sf SNR}-1)^{-1}$ in the literature on weak recovery. On the otherhand, when ${\sf SNR}<1$ (for $k=2$) and ${\sf SNR}<1/4$ (for general growing$k$), we prove that local algorithms suffer an error rate at least $\frac{1}{2}- \sqrt{\delta \cdot {\sf SNR}}$, which is only slightly better than randomguess for small $\delta$.
arxiv-17100-232 | Completely random measures for modeling power laws in sparse graphs | http://arxiv.org/abs/1603.06915 | author:Diana Cai, Tamara Broderick category:stat.ML math.ST stat.ME stat.TH published:2016-03-22 summary:Network data appear in a number of applications, such as online socialnetworks and biological networks, and there is growing interest in bothdeveloping models for networks as well as studying the properties of such data.Since individual network datasets continue to grow in size, it is necessary todevelop models that accurately represent the real-life scaling properties ofnetworks. One behavior of interest is having a power law in the degreedistribution. However, other types of power laws that have been observedempirically and considered for applications such as clustering and featureallocation models have not been studied as frequently in models for graph data.In this paper, we enumerate desirable asymptotic behavior that may be ofinterest for modeling graph data, including sparsity and several types of powerlaws. We outline a general framework for graph generative models usingcompletely random measures; by contrast to the pioneering work of Caron and Fox(2015), we consider instantiating more of the existing atoms of the randommeasure as the dataset size increases rather than adding new atoms to themeasure. We see that these two models can be complementary; they respectivelyyield interpretations as (1) time passing among existing members of a networkand (2) new individuals joining a network. We detail a particular instance ofthis framework and show simulated results that suggest this model exhibits somedesirable asymptotic power-law behavior.
arxiv-17100-233 | Edge-exchangeable graphs and sparsity | http://arxiv.org/abs/1603.06898 | author:Tamara Broderick, Diana Cai category:math.ST stat.ME stat.ML stat.TH published:2016-03-22 summary:A known failing of many popular random graph models is that the Aldous-HooverTheorem guarantees these graphs are dense with probability one; that is, thenumber of edges grows quadratically with the number of nodes. This behavior isconsidered unrealistic in observed graphs. We define a notion of edgeexchangeability for random graphs in contrast to the established notion ofinfinite exchangeability for random graphs --- which has traditionally reliedon exchangeability of nodes (rather than edges) in a graph. We show that,unlike node exchangeability, edge exchangeability encompasses models that areknown to provide a projective sequence of random graphs that circumvent theAldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of thenumber of edges with the number of nodes. We show how edge-exchangeability ofgraphs relates naturally to existing notions of exchangeability from clustering(a.k.a. partitions) and other familiar combinatorial structures.
arxiv-17100-234 | Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of Pairwise Comparisons | http://arxiv.org/abs/1603.06881 | author:Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright category:cs.LG cs.AI cs.IT math.IT stat.ML published:2016-03-22 summary:We study methods for aggregating pairwise comparison data in order toestimate outcome probabilities for future comparisons among a collection of nitems. Working within a flexible framework that imposes only a form of strongstochastic transitivity (SST), we introduce an adaptivity index defined by theindifference sets of the pairwise comparison probabilities. In addition tomeasuring the usual worst-case risk of an estimator, this adaptivity index alsocaptures the extent to which the estimator adapts to instance-specificdifficulty relative to an oracle estimator. We prove three main results thatinvolve this adaptivity index and different algorithms. First, we propose athree-step estimator termed Count-Randomize-Least squares (CRL), and show thatit has adaptivity index upper bounded as $\sqrt{n}$ up to logarithmic factors.We then show that that conditional on the hardness of planted clique, nocomputationally efficient estimator can achieve an adaptivity index smallerthan $\sqrt{n}$. Second, we show that a regularized least squares estimator canachieve a poly-logarithmic adaptivity index, thereby demonstrating a$\sqrt{n}$-gap between optimal and computationally achievable adaptivity.Finally, we prove that the standard least squares estimator, which is known tobe optimally adaptive in several closely related problems, fails to adapt inthe context of estimating pairwise probabilities.
arxiv-17100-235 | Trading-off variance and complexity in stochastic gradient descent | http://arxiv.org/abs/1603.06861 | author:Vatsal Shah, Megasthenis Asteris, Anastasios Kyrillidis, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT math.OC published:2016-03-22 summary:Stochastic gradient descent is the method of choice for large-scale machinelearning problems, by virtue of its light complexity per iteration. However, itlags behind its non-stochastic counterparts with respect to the convergencerate, due to high variance introduced by the stochastic updates. The popularStochastic Variance-Reduced Gradient (SVRG) method mitigates this shortcoming,introducing a new update rule which requires infrequent passes over the entireinput dataset to compute the full-gradient. In this work, we propose CheapSVRG, a stochastic variance-reductionoptimization scheme. Our algorithm is similar to SVRG but instead of the fullgradient, it uses a surrogate which can be efficiently computed on a smallsubset of the input data. It achieves a linear convergence rate ---up to someerror level, depending on the nature of the optimization problem---and featuresa trade-off between the computational complexity and the convergence rate.Empirical evaluation shows that CheapSVRG performs at least competitivelycompared to the state of the art.
arxiv-17100-236 | Multi-Scale Convolutional Neural Networks for Time Series Classification | http://arxiv.org/abs/1603.06995 | author:Zhicheng Cui, Wenlin Chen, Yixin Chen category:cs.CV published:2016-03-22 summary:Time series classification (TSC), the problem of predicting class labels oftime series, has been around for decades within the community of data miningand machine learning, and found many important applications such as biomedicalengineering and clinical prediction. However, it still remains challenging andfalls short of classification accuracy and efficiency. Traditional approachestypically involve extracting discriminative features from the original timeseries using dynamic time warping (DTW) or shapelet transformation, based onwhich an off-the-shelf classifier can be applied. These methods are ad-hoc andseparate the feature extraction part with the classification part, which limitstheir accuracy performance. Plus, most existing methods fail to take intoaccount the fact that time series often have features at different time scales.To address these problems, we propose a novel end-to-end neural network model,Multi-Scale Convolutional Neural Networks (MCNN), which incorporates featureextraction and classification in a single framework. Leveraging a novelmulti-branch layer and learnable convolutional layers, MCNN automaticallyextracts features at different scales and frequencies, leading to superiorfeature representation. MCNN is also computationally efficient, as it naturallyleverages GPU computing. We conduct comprehensive empirical evaluation withvarious existing methods on a large number of benchmark datasets, and show thatMCNN advances the state-of-the-art by achieving superior accuracy performancethan other leading methods.
arxiv-17100-237 | Knowledge Transfer for Scene-specific Motion Prediction | http://arxiv.org/abs/1603.06987 | author:Lamberto Ballan, Francesco Castaldo, Alexandre Alahi, Francesco Palmieri, Silvio Savarese category:cs.CV published:2016-03-22 summary:When given a single frame of the video, humans can not only interpret thecontent of the scene, but also they are able to forecast the near future. Thisability is mostly driven by their rich prior knowledge about the visual world,both in terms of (\emph{i}) the dynamics of moving agents, as well as(\emph{ii}) the semantic of the scene. In this work we exploit the interplaybetween these two key elements to predict scene-specific motion patterns.First, we extract patch descriptors encoding the probability of moving to theadjacent patches, and the probability of being in that particular patch orchanging behavior. Then, we introduce a Dynamic Bayesian Network which exploitsthis scene specific knowledge for trajectory prediction. Experimental resultsdemonstrate that our method is able to accurately predict trajectories andtransfer predictions to a novel scene characterized by similar elements.
arxiv-17100-238 | Enhanced perceptrons using contrastive biclusters | http://arxiv.org/abs/1603.06859 | author:André L. V. Coelho, Fabrício O. de França category:cs.NE cs.LG stat.ML published:2016-03-22 summary:Perceptrons are neuronal devices capable of fully discriminating linearlyseparable classes. Although straightforward to implement and train, theirapplicability is usually hindered by non-trivial requirements imposed byreal-world classification problems. Therefore, several approaches, such askernel perceptrons, have been conceived to counteract such difficulties. Inthis paper, we investigate an enhanced perceptron model based on the notion ofcontrastive biclusters. From this perspective, a good discriminative biclustercomprises a subset of data instances belonging to one class that show highcoherence across a subset of features and high differentiation from nearestinstances of the other class under the same features (referred to as itscontrastive bicluster). Upon each local subspace associated with a pair ofcontrastive biclusters a perceptron is trained and the model with highest areaunder the receiver operating characteristic curve (AUC) value is selected asthe final classifier. Experiments conducted on a range of data sets, includingthose related to a difficult biosignal classification problem, show that theproposed variant can be indeed very useful, prevailing in most of the casesupon standard and kernel perceptrons in terms of accuracy and AUC measures.
arxiv-17100-239 | Word Sense Disambiguation with Neural Language Models | http://arxiv.org/abs/1603.07012 | author:Dayu Yuan, Ryan Doherty, Julian Richardson, Colin Evans, Eric Altendorf category:cs.CL published:2016-03-22 summary:Determining the intended sense of words in text -- word sense disambiguation(WSD) -- is a long-standing problem in natural language processing. In thispaper, we present WSD algorithms which use neural network language models toachieve state-of-the-art precision. Each of these methods learns todisambiguate word senses using only a set of word senses, a few examplesentences for each sense taken from a licensed lexicon, and a large unlabeledtext corpus. We classify based on cosine similarity of vectors derived from thecontexts in unlabeled query and labeled example sentences. We demonstratestate-of-the-art results when using the WordNet sense inventory, andsignificantly better than baseline performance using the New Oxford AmericanDictionary inventory. The best performance was achieved by combining an LSTMlanguage model with graph label propagation.
arxiv-17100-240 | Multi-velocity neural networks for gesture recognition in videos | http://arxiv.org/abs/1603.06829 | author:Otkrist Gupta, Dan Raviv, Ramesh Raskar category:cs.CV cs.LG published:2016-03-22 summary:We present a new action recognition deep neural network which adaptivelylearns the best action velocities in addition to the classification. While deepneural networks have reached maturity for image understanding tasks, we arestill exploring network topologies and features to handle the richerenvironment of video clips. Here, we tackle the problem of multiple velocitiesin action recognition, and provide state-of-the-art results for gesturerecognition, on known and new collected datasets. We further provide thetraining steps for our semi-supervised network, suited to learn from hugeunlabeled datasets with only a fraction of labeled examples.
arxiv-17100-241 | A Self-Paced Regularization Framework for Multi-Label Learning | http://arxiv.org/abs/1603.06708 | author:Changsheng Li, Fan Wei, Junchi Yan, Weishan Dong, Qingshan Liu, Xiaoyu Zhang, Hongyuan Zha category:cs.LG published:2016-03-22 summary:In this paper, we propose a novel multi-label learning framework, calledMulti-Label Self-Paced Learning (MLSPL), in an attempt to incorporate theself-paced learning strategy into multi-label learning regime. In light of thebenefits of adopting the easy-to-hard strategy proposed by self-paced learning,the devised MLSPL aims to learn multiple labels jointly by gradually includinglabel learning tasks and instances into model training from the easy to thehard. We first introduce a self-paced function as a regularizer in themulti-label learning formulation, so as to simultaneously rank priorities ofthe label learning tasks and the instances in each learning iteration.Considering that different multi-label learning scenarios often need differentself-paced schemes during optimization, we thus propose a general way to findthe desired self-paced functions. Experimental results on three benchmarkdatasets suggest the state-of-the-art performance of our approach.
arxiv-17100-242 | Active Detection and Localization of Textureless Objects in Cluttered Environments | http://arxiv.org/abs/1603.07022 | author:Marco Imperoli, Alberto Pretto category:cs.CV cs.RO published:2016-03-22 summary:This paper introduces an active object detection and localization frameworkthat combines a robust untextured object detection and 3D pose estimationalgorithm with a novel next-best-view selection strategy. We address thedetection and localization problems by proposing an edge-based registrationalgorithm that refines the object position by minimizing a cost directlyextracted from a 3D image tensor that encodes the minimum distance to an edgepoint in a joint direction/location space. We face the next-best-view problemby exploiting a sequential decision process that, for each step, selects thenext camera position which maximizes the mutual information between the stateand the next observations. We solve the intrinsic intractability of thissolution by generating observations that represent scene realizations, i.e.combination samples of object hypothesis provided by the object detector, whilemodeling the state by means of a set of constantly resampled particles.Experiments performed on different real world, challenging datasets confirm theeffectiveness of the proposed methods.
arxiv-17100-243 | MOON: A Mixed Objective Optimization Network for the Recognition of Facial Attributes | http://arxiv.org/abs/1603.07027 | author:Ethan Rudd, Manuel Günther, Terrance Boult category:cs.CV published:2016-03-22 summary:Multi-task vision problems can often be decomposed into separate tasks andstages, e.g., separating feature extraction and model building, or trainingindependent models for each task. Joint optimization has been shown to improveperformance, but can be difficult to apply to deep convolution neural networks(DCNN), especially with unbalanced data. This paper introduces a novel mixedobjective optimization network (MOON), with a loss function which mixes errorsfrom multiple tasks and supports domain adaptation when label frequenciesdiffer between training and operational testing. Experiments demonstrate thatnot only does MOON advance the state of the art in facial attributerecognition, but it also outperforms independently trained DCNNs using the samedata.
arxiv-17100-244 | New metrics for learning and inference on sets, ontologies, and functions | http://arxiv.org/abs/1603.06846 | author:Ruiyu Yang, Yuxiang Jiang, Matthew W. Hahn, Elizabeth A. Housworth, Predrag Radivojac category:stat.ML published:2016-03-22 summary:We propose new metrics on sets, ontologies, and functions that can be used invarious stages of probabilistic modeling, including exploratory data analysis,learning, inference, and result interpretation. These new functions unify andgeneralize some of the popular metrics on sets and functions, such as theJaccard and bag distances on sets and Marczewski-Steinhaus distance onfunctions. We then introduce information-theoretic metrics on directed acyclicgraphs drawn independently according to a fixed probability distribution andshow how they can be used to calculate similarity between class labels for theobjects with hierarchical output spaces (e.g., protein function). Finally, weprovide evidence that the proposed metrics are useful by clustering speciesbased solely on functional annotations available for subsets of their genes.The functional trees resemble evolutionary trees obtained by the phylogeneticanalysis of their genomes.
arxiv-17100-245 | Information Theoretic-Learning Auto-Encoder | http://arxiv.org/abs/1603.06653 | author:Eder Santana, Matthew Emigh, Jose C Principe category:cs.LG published:2016-03-22 summary:We propose Information Theoretic-Learning (ITL) divergence measures forvariational regularization of neural networks. We also explore ITL-regularizedautoencoders as an alternative to variational autoencoding bayes, adversarialautoencoders and generative adversarial networks for randomly generating sampledata without explicitly defining a partition function. This paper alsoformalizes, generative moment matching networks under the ITL framework.
arxiv-17100-246 | Con-Patch: When a Patch Meets its Context | http://arxiv.org/abs/1603.06812 | author:Yaniv Romano, Michael Elad category:cs.CV published:2016-03-22 summary:Measuring the similarity between patches in images is a fundamental buildingblock in various tasks. Naturally, the patch-size has a major impact on thematching quality, and on the consequent application performance. Under theassumption that our patch database is sufficiently sampled, using large patches(e.g. 21-by-21) should be preferred over small ones (e.g. 7-by-7). However,this "dense-sampling" assumption is rarely true; in most cases large patchescannot find relevant nearby examples. This phenomenon is a consequence of thecurse of dimensionality, stating that the database-size should growexponentially with the patch-size to ensure proper matches. This explains thefavored choice of small patch-size in most applications. Is there a way to keep the simplicity and work with small patches whilegetting some of the benefits that large patches provide? In this work we offersuch an approach. We propose to concatenate the regular content of aconventional (small) patch with a compact representation of its (large)surroundings - its context. Therefore, with a minor increase of the dimensions(e.g. with additional 10 values to the patch representation), weimplicitly/softly describe the information of a large patch. The additionaldescriptors are computed based on a self-similarity behavior of the patchsurrounding. We show that this approach achieves better matches, compared to the use ofconventional-size patches, without the need to increase the database-size.Also, the effectiveness of the proposed method is tested on three distinctproblems: (i) External natural image denoising, (ii) Depth imagesuper-resolution, and (iii) Motion-compensated frame-rate up-conversion.
arxiv-17100-247 | Input Aggregated Network for Face Video Representation | http://arxiv.org/abs/1603.06655 | author:Zhen Dong, Su Jia, Chi Zhang, Mingtao Pei category:cs.CV published:2016-03-22 summary:Recently, deep neural network has shown promising performance in face imagerecognition. The inputs of most networks are face images, and there is hardlyany work reported in literature on network with face videos as input. Tosufficiently discover the useful information contained in face videos, wepresent a novel network architecture called input aggregated network which isable to learn fixed-length representations for variable-length face videos. Toaccomplish this goal, an aggregation unit is designed to model a face videowith various frames as a point on a Riemannian manifold, and the mapping unitaims at mapping the point into high-dimensional space where face videosbelonging to the same subject are close-by and others are distant. These twounits together with the frame representation unit build an end-to-end learningsystem which can learn representations of face videos for the specific tasks.Experiments on two public face video datasets demonstrate the effectiveness ofthe proposed network.
arxiv-17100-248 | Information Processing by Nonlinear Phase Dynamics in Locally Connected Arrays | http://arxiv.org/abs/1603.06665 | author:Richard A. Kiehl category:cs.NE cs.ET published:2016-03-22 summary:Research toward powerful information processing systems that circumvent theinterconnect bottleneck by exploiting the nonlinear evolution of multiple phasedynamics in locally connected arrays is discussed. We focus on a scheme inwhich logic states are defined by the electrical phase of a dynamic process andinformation processing is realized through interactions between the elements inthe array. Simulation results are given for networks comprised of neuron-likeintegrate-and-fire elements, which could potentially be implemented byultra-small tunnel junctions, molecules and other types of nanoscale elements.This approach could lead to powerful information processing systems due tomassive parallelism in simple, highly scalable nano-architectures. The rationalfor this approach, its advantages, simulation results, critical issues, andfuture research directions are discussed.
arxiv-17100-249 | Learning Representations for Automatic Colorization | http://arxiv.org/abs/1603.06668 | author:Gustav Larsson, Michael Maire, Gregory Shakhnarovich category:cs.CV published:2016-03-22 summary:We develop a fully automatic image colorization system. Our approachleverages recent advances in deep networks, exploiting both low-level andsemantic representations during colorization. As many scene elements naturallyappear according to multimodal color distributions, we train our model topredict per-pixel color histograms. This intermediate output can be used toautomatically generate a color image, or further manipulated prior to imageformation; our experiments consider both scenarios. On both fully and partiallyautomatic colorization tasks, our system significantly outperforms all existingmethods.
arxiv-17100-250 | Implementation of a FPGA-Based Feature Detection and Networking System for Real-time Traffic Monitoring | http://arxiv.org/abs/1603.06669 | author:Jieshi Chen, Benjamin Carrion Schafer, Ivan Wang-Hei Ho category:cs.CV published:2016-03-22 summary:With the growing demand of real-time traffic monitoring nowadays,software-based image processing can hardly meet the real-time data processingrequirement due to the serial data processing nature. In this paper, theimplementation of a hardware-based feature detection and networking systemprototype for real-time traffic monitoring as well as data transmission ispresented. The hardware architecture of the proposed system is mainly composedof three parts: data collection, feature detection, and data transmission.Overall, the presented prototype can tolerate a high data rate of about 60frames per second. By integrating the feature detection and data transmissionfunctions, the presented system can be further developed for various VANETapplication scenarios to improve road safety and traffic efficiency. Forexample, detection of vehicles that violate traffic rules, parking enforcement,etc.
arxiv-17100-251 | Learning Executable Semantic Parsers for Natural Language Understanding | http://arxiv.org/abs/1603.06677 | author:Percy Liang category:cs.CL cs.AI published:2016-03-22 summary:For building question answering systems and natural language interfaces,semantic parsing has emerged as an important and powerful paradigm. Semanticparsers map natural language into logical forms, the classic representation formany important linguistic phenomena. The modern twist is that we are interestedin learning semantic parsers from data, which introduces a new layer ofstatistical and computational issues. This article lays out the components of astatistical semantic parser, highlighting the key challenges. We will see thatsemantic parsing is a rich fusion of the logical and the statistical world, andthat this fusion will play an integral role in the future of natural languageunderstanding systems.
arxiv-17100-252 | Stitching Stabilizer: Two-frame-stitching Video Stabilization for Embedded Systems | http://arxiv.org/abs/1603.06678 | author:Masaki Satoh category:cs.CV published:2016-03-22 summary:In conventional electronic video stabilization, the stabilized frame isobtained by cropping the input frame to cancel camera shake. While a smallcropping size results in strong stabilization, it does not provide ussatisfactory results from the viewpoint of image quality, because it narrowsthe angle of view. By fusing several frames, we can effectively expand the areaof input frames, and achieve strong stabilization even with a large croppingsize. Several methods for doing so have been studied. However, theircomputational costs are too high for embedded systems such as smartphones. We propose a simple, yet surprisingly effective algorithm, called thestitching stabilizer. It stitches only two frames together with a minimalcomputational cost. It can achieve real-time processes in embedded systems, forFull HD and 30 FPS videos. To clearly show the effect, we apply it tohyperlapse. Using several clips, we show it produces more strongly stabilizedand natural results than the existing solutions from Microsoft and Instagram.
arxiv-17100-253 | Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis | http://arxiv.org/abs/1603.06679 | author:Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, Xiaokui Xiao category:cs.CL cs.IR cs.LG published:2016-03-22 summary:Aspect-based sentiment analysis has obtained substantial popularity due toits ability to extract useful information from customer reviews. In most cases,aspect terms in a review sentence have strong relations with opinion termsbecause an aspect is the target where an opinion is expressed. With thisconnection, some of the existing work focused on designing syntactic rules todouble propagate information between aspect and opinion terms. However, thesemethods require large amount of efforts and domain knowledge to design precisesyntactic rules and fail to handle uncertainty. In this paper, we propose anovel joint model that integrates recursive neural networks and conditionalrandom fields into a unified framework for aspect-based sentiment analysis. Ourtask is to extract aspect and opinion terms/phrases for each review. Theproposed model is able to learn high-level discriminative features and doublepropagate information between aspect and opinion terms simultaneously.Furthermore, it is flexible to incorporate linguistic or lexicon features intothe proposed model to further boost its performance in terms of informationextraction. Experimental results on the SemEval Challenge 2014 dataset show thesuperiority of our proposed model over several baseline methods as well as thewinning systems.
arxiv-17100-254 | Image Super-Resolution Based on Sparsity Prior via Smoothed $l_0$ Norm | http://arxiv.org/abs/1603.06680 | author:Mohammad Rostami, Zhou Wang category:cs.CV published:2016-03-22 summary:In this paper we aim to tackle the problem of reconstructing ahigh-resolution image from a single low-resolution input image, known as singleimage super-resolution. In the literature, sparse representation has been usedto address this problem, where it is assumed that both low-resolution andhigh-resolution images share the same sparse representation over a pair ofcoupled jointly trained dictionaries. This assumption enables us to use thecompressed sensing theory to find the jointly sparse representation via thelow-resolution image and then use it to recover the high-resolution image.However, sparse representation of a signal over a known dictionary is anill-posed, combinatorial optimization problem. Here we propose an algorithmthat adopts the smoothed $l_0$-norm (SL0) approach to find the jointly sparserepresentation. Improved quality of the reconstructed image is obtained formost images in terms of both peak signal-to-noise-ratio (PSNR) and structuralsimilarity (SSIM) measures.
arxiv-17100-255 | Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus | http://arxiv.org/abs/1603.06807 | author:Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, Yoshua Bengio category:cs.CL cs.AI cs.LG cs.NE published:2016-03-22 summary:Over the past decade, large-scale supervised learning corpora have enabledmachine learning researchers to make substantial advances. However, to thisdate, there are no large-scale question-answer corpora available. In this paperwe present the 30M Factoid Question-Answer Corpus, an enormous question-answerpair corpus produced by applying a novel neural network architecture on theknowledge base Freebase to transduce facts into natural language questions. Theproduced question-answer pairs are evaluated both by human evaluators and usingautomatic evaluation metrics, including well-established machine translationand sentence similarity metrics. Across all evaluation criteria thequestion-generation model outperforms the competing template-based baseline.Furthermore, when presented to human evaluators, the generated questions appearto be indistinguishable from real human-generated questions.
arxiv-17100-256 | Using real-time cluster configurations of streaming asynchronous features as online state descriptors in financial markets | http://arxiv.org/abs/1603.06805 | author:Dieter Hendricks category:q-fin.TR cs.LG q-fin.CP published:2016-03-22 summary:We present a scheme for online, unsupervised state discovery and detectionfrom streaming, multi-featured, asynchronous data in high-frequency financialmarkets. Online feature correlations are computed using an unbiased, losslessFourier estimator. A high-speed maximum likelihood clustering algorithm is thenused to find the feature cluster configuration which best explains thestructure in the correlation matrix. We conjecture that this featureconfiguration is a candidate descriptor for the temporal state of the system.Using a simple cluster configuration similarity metric, we are able toenumerate the state space based on prevailing feature configurations. Theproposed state representation removes the need for human-driven datapre-processing for state attribute specification, allowing a learning agent tofind structure in streaming data, discern changes in the system, enumerate itsperceived state space and learn suitable action-selection policies.
arxiv-17100-257 | Localized Lasso for High-Dimensional Regression | http://arxiv.org/abs/1603.06743 | author:Makoto Yamada, Koh Takeuchi, Tomoharu Iwata, John Shawe-Taylor, Samuel Kaski category:stat.ML cs.LG stat.ME published:2016-03-22 summary:We introduce the localized Lasso, which is suited for learning models thatare both interpretable and have a high predictive power in problems with highdimensionality $d$ and small sample size $n$. More specifically, we consider afunction defined by local sparse models, one at each data point. We introducesample-wise network regularization to borrow strength across the models, andsample-wise exclusive group sparsity (a.k.a., $\ell_{1,2}^2$ norm) to introducediversity into the choice of feature sets in the local models. The local modelsare interpretable in terms of similarity of their sparsity patterns. The costfunction is convex, and thus has a globally optimal solution. Moreover, wepropose a simple yet efficient iterative least-squares based optimizationprocedure for the localized Lasso, which does not need a tuning parameter, andis guaranteed to converge to a globally optimal solution. The solution isempirically shown to outperform alternatives for both simulated and genomicpersonalized medicine data.
arxiv-17100-258 | A Selection of Giant Radio Sources from NVSS | http://arxiv.org/abs/1603.06895 | author:D. D. Proctor category:astro-ph.GA cs.CV stat.ML published:2016-03-22 summary:Results of the application of pattern recognition techniques to the problemof identifying Giant Radio Sources (GRS) from the data in the NVSS catalog arepresented and issues affecting the process are explored. Decision-tree patternrecognition software was applied to training set source pairs developed fromknown NVSS large angular size radio galaxies. The full training set consistedof 51,195 source pairs, 48 of which were known GRS for which each lobe wasprimarily represented by a single catalog component. The source pairs had amaximum separation of 20 arc minutes and a minimum component area of 1.87square arc minutes at the 1.4 mJy level. The importance of comparing resultingprobability distributions of the training and application sets for cases ofunknown class ratio is demonstrated. The probability of correctly ranking arandomly selected (GRS, non-GRS) pair from the best of the tested classifierswas determined to be 97.8 +/- 1.5%. The best classifiers were applied to theover 870,000 candidate pairs from the entire catalog. Images of higher rankedsources were visually screened and a table of over sixteen hundred candidates,including morphological annotation, is presented. These systems include doublesand triples, Wide-Angle Tail (WAT) and Narrow-Angle Tail (NAT), S- or Z-shapedsystems, and core-jets and resolved cores. While some resolved lobe systems arerecovered with this technique, generally it is expected that such systems wouldrequire a different approach.
arxiv-17100-259 | Adaptive Parameter Selection in Evolutionary Algorithms by Reinforcement Learning with Dynamic Discretization of Parameter Range | http://arxiv.org/abs/1603.06788 | author:Arkady Rost, Irina Petrova, Arina Buzdalova category:cs.NE 68T05 G.1.6; I.2.6 published:2016-03-22 summary:Online parameter controllers for evolutionary algorithms adjust values ofparameters during the run of an evolutionary algorithm. Recently a newefficient parameter controller based on reinforcement learning was proposed byKarafotias et al. In this method ranges of parameters are discretized intoseveral intervals before the run. However, performing adaptive discretizationduring the run may increase efficiency of an evolutionary algorithm. Aleti etal. proposed another efficient controller with adaptive discretization. In the present paper we propose a parameter controller based on reinforcementlearning with adaptive discretization. The proposed controller is compared withthe existing parameter adjusting methods on several test problems usingdifferent configurations of an evolutionary algorithm. For the test problems,we consider four continuous functions, namely the sphere function, theRosenbrock function, the Levi function and the Rastrigin function. Results showthat the new controller outperforms the other controllers on most of theconsidered test problems.
arxiv-17100-260 | Multi-domain machine translation enhancements by parallel data extraction from comparable corpora | http://arxiv.org/abs/1603.06785 | author:Krzysztof Wołk, Emilia Rejmund, Krzysztof Marasek category:cs.CL stat.ML published:2016-03-22 summary:Parallel texts are a relatively rare language resource, however, theyconstitute a very useful research material with a wide range of applications.This study presents and analyses new methodologies we developed for obtainingsuch data from previously built comparable corpora. The methodologies areautomatic and unsupervised which makes them good for large scale research. Thetask is highly practical as non-parallel multilingual data occur much morefrequently than parallel corpora and accessing them is easy, although parallelsentences are a considerably more useful resource. In this study, we propose amethod of automatic web crawling in order to build topic-aligned comparablecorpora, e.g. based on the Wikipedia or Euronews.com. We also developed newmethods of obtaining parallel sentences from comparable data and proposedmethods of filtration of corpora capable of selecting inconsistent or onlypartially equivalent translations. Our methods are easily scalable to otherlanguages. Evaluation of the quality of the created corpora was performed byanalysing the impact of their use on statistical machine translation systems.Experiments were presented on the basis of the Polish-English language pair fortexts from different domains, i.e. lectures, phrasebooks, film dialogues,European Parliament proceedings and texts contained medicines leaflets. We alsotested a second method of creating parallel corpora based on data fromcomparable corpora which allows for automatically expanding the existing corpusof sentences about a given domain on the basis of analogies found between them.It does not require, therefore, having past parallel resources in order totrain a classifier.
arxiv-17100-261 | Doubly Random Parallel Stochastic Methods for Large Scale Learning | http://arxiv.org/abs/1603.06782 | author:Aryan Mokhtari, Alec Koppel, Alejandro Ribeiro category:cs.LG math.OC published:2016-03-22 summary:We consider learning problems over training sets in which both, the number oftraining examples and the dimension of the feature vectors, are large. To solvethese problems we propose the random parallel stochastic algorithm (RAPSA). Wecall the algorithm random parallel because it utilizes multiple processors tooperate in a randomly chosen subset of blocks of the feature vector. We callthe algorithm parallel stochastic because processors choose elements of thetraining set randomly and independently. Algorithms that are parallel in eitherof these dimensions exist, but RAPSA is the first attempt at a methodology thatis parallel in both, the selection of blocks and the selection of elements ofthe training set. In RAPSA, processors utilize the randomly chosen functions tocompute the stochastic gradient component associated with a randomly chosenblock. The technical contribution of this paper is to show that this minimallycoordinated algorithm converges to the optimal classifier when the trainingobjective is convex. In particular, we show that: (i) When using decreasingstepsizes, RAPSA converges almost surely over the random choice of blocks andfunctions. (ii) When using constant stepsizes, convergence is to a neighborhoodof optimality with a rate that is linear in expectation. RAPSA is numericallyevaluated on the MNIST digit recognition problem.
arxiv-17100-262 | Energy-Efficient ConvNets Through Approximate Computing | http://arxiv.org/abs/1603.06777 | author:Bert Moons, Bert De Brabandere, Luc Van Gool, Marian Verhelst category:cs.CV published:2016-03-22 summary:Recently ConvNets or convolutional neural networks (CNN) have come up asstate-of-the-art classification and detection algorithms, achieving near-humanperformance in visual detection. However, ConvNet algorithms are typically verycomputation and memory intensive. In order to be able to embed ConvNet-basedclassification into wearable platforms and embedded systems such as smartphonesor ubiquitous electronics for the internet-of-things, their energy consumptionshould be reduced drastically. This paper proposes methods based on approximatecomputing to reduce energy consumption in state-of-the-art ConvNetaccelerators. By combining techniques both at the system- and circuit level, wecan gain energy in the systems arithmetic: up to 30x without losingclassification accuracy and more than 100x at 99% classification accuracy,compared to the commonly used 16-bit fixed point number format.
arxiv-17100-263 | Latent Predictor Networks for Code Generation | http://arxiv.org/abs/1603.06744 | author:Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin Wang, Phil Blunsom category:cs.CL cs.NE published:2016-03-22 summary:Many language generation tasks require the production of text conditioned onboth structured and unstructured inputs. We present a novel neural networkarchitecture which generates an output sequence conditioned on an arbitrarynumber of input functions. Crucially, our approach allows both the choice ofconditioning context and the granularity of generation, for example charactersor tokens, to be marginalised, thus permitting scalable and effective training.Using this framework, we address the problem of generating programming codefrom a mixed natural language and structured specification. We create two newdata sets for this paradigm derived from the collectible trading card gamesMagic the Gathering and Hearthstone. On these, and a third preexisting corpus,we demonstrate that marginalising multiple predictors allows our model tooutperform strong benchmarks.
arxiv-17100-264 | Fully Convolutional Attention Localization Networks: Efficient Attention Localization for Fine-Grained Recognition | http://arxiv.org/abs/1603.06765 | author:Xiao Liu, Tian Xia, Jiang Wang, Yuanqing Lin category:cs.CV published:2016-03-22 summary:Fine-grained recognition is challenging mainly because the inter-classdifferences between fine-grained classes are usually local and subtle whileintra-class differences could be large due to pose variations. In order todistinguish them from intra-class variations, it is essential to zoom in onhighly discriminative local regions. In this work, we introduce a reinforcementlearning-based fully convolutional attention localization network to adaptivelyselect multiple task-driven visual attention regions. We show that zooming inon the selected attention regions significantly improves the performance offine-grained recognition. Compared to previous reinforcement learning-basedmodels, the proposed approach is noticeably more computationally efficientduring both training and testing because of its fully-convolutionalarchitecture, and it is capable of simultaneous focusing its glimpse onmultiple visual attention regions. The experiments demonstrate that theproposed method achieves notably higher classification accuracy on threebenchmark fine-grained recognition datasets: Stanford Dogs, Stanford Cars, andCUB-200-2011.
arxiv-17100-265 | Convolution in Convolution for Network in Network | http://arxiv.org/abs/1603.06759 | author:Yanwei Pang, Manli Sun, Xiaoheng Jiang, Xuelong Li category:cs.CV published:2016-03-22 summary:Network in Netwrok (NiN) is an effective instance and an important extensionof Convolutional Neural Network (CNN) consisting of alternating convolutionallayers and pooling layers. Instead of using a linear filter for convolution,NiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, toreplace the linear filter. Because of the powerfulness of MLP and $ 1\times 1 $convolutions in spatial domain, NiN has stronger ability of featurerepresentation and hence results in better recognition rate. However, MLPitself consists of fully connected layers which give rise to a large number ofparameters. In this paper, we propose to replace dense shallow MLP with sparseshallow MLP. One or more layers of the sparse shallow MLP are sparely connectedin the channel dimension or channel-spatial domain. The proposed method isimplemented by applying unshared convolution across the channel dimension andapplying shared convolution across the spatial dimension in some computationallayers. The proposed method is called CiC. Experimental results on the CIFAR10dataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate theeffectiveness of the proposed CiC method.
arxiv-17100-266 | Deep Self-Convolutional Activations Descriptor for Dense Cross-Modal Correspondence | http://arxiv.org/abs/1603.06327 | author:Seungryong Kim, Dongbo Min, Stephen Lin, Kwanghoon Sohn category:cs.CV published:2016-03-21 summary:We present a novel descriptor, called deep self-convolutional activations(DeSCA), designed for establishing dense correspondences between images takenunder different imaging modalities, such as different spectral ranges orlighting conditions. Motivated by descriptors based on local self-similarity(LSS), we formulate a novel descriptor by leveraging LSS in a deeparchitecture, leading to better discriminative power and greater robustness tonon-rigid image deformations than state-of-the-art cross-modality descriptors.The DeSCA first computes self-convolutions over a local support window forrandomly sampled patches, and then builds self-convolution activations byperforming an average pooling through a hierarchical formulation within a deepconvolutional architecture. Finally, the feature responses on theself-convolution activations are encoded through a spatial pyramid pooling in acircular configuration. In contrast to existing convolutional neural networks(CNNs) based descriptors, the DeSCA is training-free (i.e., randomly sampledpatches are utilized as the convolution kernels), is robust to cross-modalimaging, and can be densely computed in an efficient manner that significantlyreduces computational redundancy. The state-of-the-art performance of DeSCA onchallenging cases of cross-modal image pairs is demonstrated through extensiveexperiments.
arxiv-17100-267 | Frankenstein: Learning Deep Face Representations using Small Data | http://arxiv.org/abs/1603.06470 | author:Guosheng Hu, Xiaojiang Peng, Yongxin Yang, Timothy Hospedales, Jakob Verbeek category:cs.CV published:2016-03-21 summary:Deep convolutional neural networks have recently proven extremely effectivefor difficult face recognition problems in uncontrolled settings. To train suchnetworks very large training sets are needed with millions of labeled images.For some applications, such as near-infrared (NIR) face recognition, suchlarger training datasets are, however, not publicly available and verydifficult to collect. We propose a method to generate very large trainingdatasets of synthetic images by compositing real face images in a givendataset. We show that this method enables to learn models from as few as 10,000training images, which perform on par with models trained from 500,000 images.Using our approach we also improve the state-of-the-art results on the CASIANIR-VIS heterogeneous face recognition dataset.
arxiv-17100-268 | Hard-Clustering with Gaussian Mixture Models | http://arxiv.org/abs/1603.06478 | author:Johannes Blömer, Sascha Brauer, Kathrin Bujna category:cs.LG cs.DS published:2016-03-21 summary:Training the parameters of statistical models to describe a given data set isa central task in the field of data mining and machine learning. A very popularand powerful way of parameter estimation is the method of maximum likelihoodestimation (MLE). Among the most widely used families of statistical models aremixture models, especially, mixtures of Gaussian distributions. A popularhard-clustering variant of the MLE problem is the so-called complete-datamaximum likelihood estimation (CMLE) method. The standard approach to solve theCMLE problem is the Classification-Expectation-Maximization (CEM) algorithm.Unfortunately, it is only guaranteed that the algorithm converges to some(possibly arbitrarily poor) stationary point of the objective function. In this paper, we present two algorithms for a restricted version of the CMLEproblem. That is, our algorithms approximate reasonable solutions to the CMLEproblem which satisfy certain natural properties. Moreover, they computesolutions whose cost (i.e. complete-data log-likelihood values) are at most afactor $(1+\epsilon)$ worse than the cost of the solutions that we search for.Note the CMLE problem in its most general, i.e. unrestricted, form is not welldefined and allows for trivial optimal solutions that can be thought of asdegenerated solutions.
arxiv-17100-269 | Bayesian Neural Word Embedding | http://arxiv.org/abs/1603.06571 | author:Oren Barkan category:cs.CL cs.LG published:2016-03-21 summary:Recently, several works in the domain of natural language processingpresented successful methods for word embedding. Among them, the Skip-gram (SG)with negative sampling, known also as Word2Vec, advanced the state-of-the-artof various linguistics tasks. In this paper, we propose a scalable Bayesianneural word embedding algorithm that can be beneficial to general itemsimilarity tasks as well. The algorithm relies on a Variational Bayes solutionfor the SG objective and a detailed step by step description of the algorithmis provided. We present experimental results that demonstrate the performanceof the proposed algorithm and show it is competitive with the original SGmethod.
arxiv-17100-270 | Illumination-invariant image mosaic calculation based on logarithmic search | http://arxiv.org/abs/1603.06433 | author:Wolfgang Konen category:cs.CV published:2016-03-21 summary:This technical report describes an improved image mosaicking algorithm. It isbased on Jain's logarithmic search algorithm [Jain 1981] which is coupled tothe method of Kourogi (1999} for matching images in a video sequence.Logarithmic search has a better invariance against illumination changes thanthe original optical-flow-based method of Kourogi.
arxiv-17100-271 | Beyond Sharing Weights for Deep Domain Adaptation | http://arxiv.org/abs/1603.06432 | author:Artem Rozantsev, Mathieu Salzmann, Pascal Fua category:cs.CV published:2016-03-21 summary:Deep Neural Networks have demonstrated outstanding performance in manyComputer Vision tasks but typically require large amounts of labeled trainingdata to achieve it. This is a serious limitation when such data is difficult toobtain. In traditional Machine Learning, Domain Adaptation is an approach toovercoming this problem by leveraging annotated data from a source domain, inwhich it is abundant, to train a classifier to operate in a target domain, inwhich labeled data is either sparse or even lacking altogether. In the DeepLearning case, most existing methods use the same architecture with the sameweights for both source and target data, which essentially amounts to learningdomain invariant features. Here, we show that it is more effective toexplicitly model the shift from one domain to the other. To this end, weintroduce a two-stream architecture, one of which operates in the source domainand the other in the target domain. In contrast to other approaches, theweights in corresponding layers are related but not shared to account fordifferences between the two domains. We demonstrate that this both yieldshigher accuracy than state-of-the-art methods on several object recognition anddetection tasks and consistently outperforms networks with shared weights inboth supervised and unsupervised settings.
arxiv-17100-272 | Appearance Harmonization for Single Image Shadow Removal | http://arxiv.org/abs/1603.06398 | author:Liqian Ma, Jue Wang, Eli Shechtman, Kalyan Sunkavalli, Shimin Hu category:cs.CV published:2016-03-21 summary:Shadows often create unwanted artifacts in photographs, and removing them canbe very challenging. Previous shadow removal methods often produce de-shadowedregions that are visually inconsistent with the rest of the image. In this workwe propose a fully automatic shadow region harmonization approach that improvesthe appearance compatibility of the de-shadowed region as typically produced byprevious methods. It is based on a shadow-guided patch-based image synthesisapproach that reconstructs the shadow region using patches sampled fromnon-shadowed regions. The result is then refined based on the reconstructionconfidence to handle unique image patterns. Many shadow removal results andcomparisons are show the effectiveness of our improvement. Quantitativeevaluation on a benchmark dataset suggests that our automatic shadowharmonization approach effectively improves upon the state-of-the-art.
arxiv-17100-273 | Analyzing coevolutionary games with dynamic fitness landscapes | http://arxiv.org/abs/1603.06374 | author:Hendrik Richter category:q-bio.PE cs.GT cs.NE published:2016-03-21 summary:Coevolutionary games cast players that may change their strategies as well astheir networks of interaction. In this paper a framework is introduced fordescribing coevolutionary game dynamics by landscape models. It is shown thatcoevolutionary games invoke dynamic landscapes. Numerical experiments are shownfor a prisoner's dilemma (PD) and a snow drift (SD) game that both use eitherbirth-death (BD) or death-birth (DB) strategy updating. The resultinglandscapes are analyzed with respect to modality and ruggedness
arxiv-17100-274 | Harnessing Deep Neural Networks with Logic Rules | http://arxiv.org/abs/1603.06318 | author:Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric Xing category:cs.LG cs.AI cs.CL stat.ML published:2016-03-21 summary:Combining deep neural networks with structured logic rules is desirable toharness flexibility and reduce unpredictability of the neural models. Wepropose a general framework capable of enhancing various types of neuralnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.Specifically, we develop an iterative distillation method that transfers thestructured information of logic rules into the weights of neural networks. Wedeploy the framework on a CNN for sentiment analysis, and an RNN for namedentity recognition. With a few highly intuitive rules, we obtain substantialimprovements and achieve state-of-the-art or comparable results to previousbest-performing systems.
arxiv-17100-275 | Unified Depth Prediction and Intrinsic Image Decomposition from a Single Image via Joint Convolutional Neural Fields | http://arxiv.org/abs/1603.06359 | author:Seungryong Kim, Kihong Park, Kwanghoon Sohn, Stephen Lin category:cs.CV published:2016-03-21 summary:We present a method for jointly predicting a depth map and intrinsic imagesfrom single-image input. The two tasks are formulated in a synergistic mannerthrough a joint conditional random field (CRF) that is solved using a novelconvolutional neural network (CNN) architecture, called the joint convolutionalneural field (JCNF) model. Tailored to our joint estimation problem, JCNFdiffers from previous CNNs in its sharing of convolutional activations andlayers between networks for each task, its inference in the gradient domainwhere there exists greater correlation between depth and intrinsic images, andthe incorporation of a gradient scale network that learns the confidence ofestimated gradients in order to effectively balance them in the solution. Thisapproach is shown to surpass state-of-the-art methods both on single-imagedepth estimation and on intrinsic image decomposition.
arxiv-17100-276 | A Discontinuous Neural Network for Non-Negative Sparse Approximation | http://arxiv.org/abs/1603.06353 | author:Martijn Arts, Marius Cordts, Monika Gorin, Marc Spehr, Rudolf Mathar category:cs.NE math.OC q-bio.NC published:2016-03-21 summary:This paper investigates a discontinuous neural network which is used as amodel of the mammalian olfactory system and can more generally be applied tosolve non-negative sparse approximation problems. By inherently limiting thesystems integrators to having non-negative outputs, the system function becomesdiscontinuous since the integrators switch between being inactive and beingactive. It is shown that the presented network converges to equilibrium pointswhich are solutions to general non-negative least squares optimizationproblems. We specify a Caratheodory solution and prove that the network isstable, provided that the system matrix has full column-rank. Under a mildcondition on the equilibrium point, we show that the network converges to itsequilibrium within a finite number of switches. Two applications of the neuralnetwork are shown. Firstly, we apply the network as a model of the olfactorysystem and show that in principle it may be capable of performing complexsparse signal recovery tasks. Secondly, we generalize the application toinclude non-negative sparse approximation problems and compare the recoveryperformance to a classical non-negative basis pursuit denoising algorithm. Weconclude that the recovery performance differs only marginally from theclassical algorithm, while the neural network has the advantage that noperformance critical regularization parameter has to be chosen prior torecovery.
arxiv-17100-277 | Incorporating Copying Mechanism in Sequence-to-Sequence Learning | http://arxiv.org/abs/1603.06393 | author:Jiatao Gu, Zhengdong Lu, Hang Li, Victor O. K. Li category:cs.CL cs.AI cs.LG cs.NE published:2016-03-21 summary:We address an important problem in sequence-to-sequence (Seq2Seq) learningreferred to as copying, in which certain segments in the input sequence areselectively replicated in the output sequence. A similar phenomenon isobservable in human language communication. For example, humans tend to repeatentity names or even long phrases in conversation. The challenge with regard tocopying in Seq2Seq is that new machinery is needed to decide when to performthe operation. In this paper, we incorporate copying into neural network-basedSeq2Seq learning and propose a new model called CopyNet with encoder-decoderstructure. CopyNet can nicely integrate the regular way of word generation inthe decoder with the new copying mechanism which can choose sub-sequences inthe input sequence and put them at proper places in the output sequence. Ourempirical study on both synthetic data sets and real world data setsdemonstrates the efficacy of CopyNet. For example, CopyNet can outperformregular RNN-based model with remarkable margins on text summarization tasks.
arxiv-17100-278 | Data Augmentation via Levy Processes | http://arxiv.org/abs/1603.06340 | author:Stefan Wager, William Fithian, Percy Liang category:stat.ML published:2016-03-21 summary:If a document is about travel, we may expect that short snippets of thedocument should also be about travel. We introduce a general framework forincorporating these types of invariances into a discriminative classifier. Theframework imagines data as being drawn from a slice of a Levy process. If weslice the Levy process at an earlier point in time, we obtain additionalpseudo-examples, which can be used to train the classifier. We show that thisscheme has two desirable properties: it preserves the Bayes decision boundary,and it is equivalent to fitting a generative model in the limit where we rewindtime back to 0. Our construction captures popular schemes such as Gaussianfeature noising and dropout training, as well as admitting new generalizations.
arxiv-17100-279 | Online Learning with Low Rank Experts | http://arxiv.org/abs/1603.06352 | author:Elad Hazan, Tomer Koren, Roi Livni, Yishay Mansour category:cs.LG published:2016-03-21 summary:We consider the problem of prediction with expert advice when the losses ofthe experts have low-dimensional structure: they are restricted to an unknown$d$-dimensional subspace. We devise algorithms with regret bounds that areindependent of the number of experts and depends only on the rank $d$. For thestochastic model we show a tight bound of $\Theta(\sqrt{dT})$, and extend it toa setting of an approximate $d$ subspace. For the adversarial model we show anupper bound of $O(d\sqrt{T})$ and a lower bound of $\Omega(\sqrt{dT})$.
arxiv-17100-280 | Learning Dexterous Manipulation for a Soft Robotic Hand from Human Demonstration | http://arxiv.org/abs/1603.06348 | author:Abhishek Gupta, Clemens Eppner, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO published:2016-03-21 summary:Dexterous multi-fingered hands can accomplish fine manipulation behaviorsthat are infeasible with simple robotic grippers. However, sophisticatedmulti-fingered hands are often expensive and fragile. Low-cost soft hands offeran appealing alternative to more conventional devices, but present considerablechallenges in sensing and actuation, making them difficult to apply to morecomplex manipulation tasks. In this paper, we describe an approach to learningfrom demonstration that can be used to train soft robotic hands to performdexterous manipulation tasks. Our method uses object-centric demonstrations,where a human demonstrates the desired motion of manipulated objects with theirown hands, and the robot autonomously learns to imitate these demonstrationsusing reinforcement learning. We propose a novel algorithm that allows us toblend and select a subset of the most feasible demonstrations to learn toimitate on the hardware, which we use with an extension of the guided policysearch framework to use multiple demonstrations to learn generalizable neuralnetwork policies. We demonstrate our approach on the RBO Hand 2, with learnedmotor skills for turning a valve, manipulating an abacus, and grasping.
arxiv-17100-281 | A System for Probabilistic Linking of Thesauri and Classification Systems | http://arxiv.org/abs/1603.06485 | author:Lisa Posch, Philipp Schaer, Arnim Bleier, Markus Strohmaier category:cs.AI cs.CL cs.DL published:2016-03-21 summary:This paper presents a system which creates and visualizes probabilisticsemantic links between concepts in a thesaurus and classes in a classificationsystem. For creating the links, we build on the Polylingual Labeled Topic Model(PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class inthe classification system by using information from the natural language textof documents, their assigned thesaurus descriptors and their designatedclasses. The links are then presented to users of the system in an interactivevisualization, providing them with an automatically generated overview of therelations between the thesaurus and the classification system.
arxiv-17100-282 | Deep Learning in Bioinformatics | http://arxiv.org/abs/1603.06430 | author:Seonwoo Min, Byunghan Lee, Sungroh Yoon category:cs.LG q-bio.GN published:2016-03-21 summary:As we are living in the era of big data, transforming biomedical big datainto valuable knowledge has been one of the most important problems inbioinformatics. At the same time, deep learning has advanced rapidly sinceearly 2000s and is recently showing a state-of-the-art performance in variousfields. So naturally, applying deep learning in bioinformatics to gain insightsfrom data is under the spotlight of both the academia and the industry. Thisarticle reviews some research of deep learning in bioinformatics. To provide abig picture, we categorized the research by both bioinformatics domains (i.e.,omics, biomedical imaging, biomedical signal processing) and deep learningarchitectures (i.e., deep neural network, convolutional neural network,recurrent neural network, modified neural network) as well as present briefdescriptions of each work. Additionally, we introduce a few issues of deeplearning in bioinformatics such as problems of class imbalance data and suggestfuture research directions such as multimodal deep learning. We believe thatthis paper could provide valuable insights and be a starting point forresearchers to apply deep learning in their bioinformatics studies.
arxiv-17100-283 | Variational Autoencoders for Feature Detection of Magnetic Resonance Imaging Data | http://arxiv.org/abs/1603.06624 | author:R. Devon Hjelm, Sergey M. Plis, Vince C. Calhoun category:cs.LG cs.NE stat.ML published:2016-03-21 summary:Independent component analysis (ICA), as an approach to the blindsource-separation (BSS) problem, has become the de-facto standard in manymedical imaging settings. Despite successes and a large ongoing researcheffort, the limitation of ICA to square linear transformations have not beenovercome, so that general INFOMAX is still far from being realized. As analternative, we present feature analysis in medical imaging as a problem solvedby Helmholtz machines, which include dimensionality reduction andreconstruction of the raw data under the same objective, and which recentlyhave overcome major difficulties in inference and learning with deep andnonlinear configurations. We demonstrate one approach to training Helmholtzmachines, variational auto-encoders (VAE), as a viable approach toward featureextraction with magnetic resonance imaging (MRI) data.
arxiv-17100-284 | Stack-propagation: Improved Representation Learning for Syntax | http://arxiv.org/abs/1603.06598 | author:Yuan Zhang, David Weiss category:cs.CL published:2016-03-21 summary:Traditional syntax models typically leverage part-of-speech (POS) informationby constructing features from hand-tuned templates. We demonstrate that abetter approach is to utilize POS tags as a regularizer of learnedrepresentations. We propose a simple method for learning a stacked pipeline ofmodels which we call "stack-propagation". We apply this to dependency parsingand tagging, where we use the hidden layer of the tagger network as arepresentation of the input tokens for the parser. At test time, our parserdoes not require predicted POS tags. On 19 languages from the UniversalDependencies, our method is 1.3% (absolute) more accurate than astate-of-the-art graph-based approach and 2.7% more accurate than the mostcomparable greedy model.
arxiv-17100-285 | The SVM Classifier Based on the Modified Particle Swarm Optimization | http://arxiv.org/abs/1603.08296 | author:L. Demidova, E. Nikulchev, Yu. Sokolova category:cs.LG cs.NE published:2016-03-21 summary:The problem of development of the SVM classifier based on the modifiedparticle swarm optimization has been considered. This algorithm carries out thesimultaneous search of the kernel function type, values of the kernel functionparameters and value of the regularization parameter for the SVM classifier.Such SVM classifier provides the high quality of data classification. The ideaof particles' {\guillemotleft}regeneration{\guillemotright} is put on the basisof the modified particle swarm optimization algorithm. At the realization ofthis idea, some particles change their kernel function type to the one whichcorresponds to the particle with the best value of the classification accuracy.The offered particle swarm optimization algorithm allows reducing the timeexpenditures for development of the SVM classifier. The results of experimentalstudies confirm the efficiency of this algorithm.
arxiv-17100-286 | Instance Influence Estimation for Hyperspectral Target Signature Characterization using Extended Functions of Multiple Instances | http://arxiv.org/abs/1603.06496 | author:Sheng Zou, Alina Zare category:cs.CV published:2016-03-21 summary:The Extended Functions of Multiple Instances (eFUMI) algorithm is ageneralization of Multiple Instance Learning (MIL). In eFUMI, only bag level(i.e. set level) labels are needed to estimate target signatures from mixeddata. The training bags in eFUMI are labeled positive if any data point in abag contains or represents any proportion of the target signature and arelabeled as a negative bag if all data points in the bag do not represent anytarget. From these imprecise labels, eFUMI has been shown to be effective atestimating target signatures in hyperspectral subpixel target detectionproblems. One motivating scenario for the use of eFUMI is where an analystcircles objects/regions of interest in a hyperspectral scene such that thetarget signatures of these objects can be estimated and be used to determinewhether other instances of the object appear elsewhere in the image collection.The regions highlighted by the analyst serve as the imprecise labels for eFUMI.Often, an analyst may want to iteratively refine their imprecise labels. Inthis paper, we present an approach for estimating the influence on theestimated target signature if the label for a particular input data point ismodified. This "instance influence estimation" guides an analyst to focus on(re-)labeling the data points that provide the largest change in the resultingestimated target signature and, thus, reduce the amount of time an analystneeds to spend refining the labels for a hyperspectral scene. Results are shownon real hyperspectral sub-pixel target detection data sets.
arxiv-17100-287 | Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits | http://arxiv.org/abs/1603.06560 | author:Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar category:cs.LG stat.ML published:2016-03-21 summary:Performance of machine learning algorithms depends critically on identifyinga good set of hyperparameters. While current methods offer efficiencies byadaptively choosing new configurations to train, an alternative strategy is toadaptively allocate resources across the selected configurations. We formulatehyperparameter optimization as a pure-exploration non-stochastic infinitelymany armed bandit problem where allocation of additional resources to an armcorresponds to training a configuration on larger subsets of the data. Weintroduce Hyperband for this framework and analyze its theoretical properties,providing several desirable guarantees. We compare Hyperband withstate-of-the-art Bayesian optimization methods and a random search baseline ona comprehensive benchmark including 117 datasets. Our results on this benchmarkdemonstrate that while Bayesian optimization methods do not outperform randomsearch trained for twice as long, Hyperband in favorable settings offersvaluable speedups.
arxiv-17100-288 | Action-Affect Classification and Morphing using Multi-Task Representation Learning | http://arxiv.org/abs/1603.06554 | author:Timothy J. Shields, Mohamed R. Amer, Max Ehrlich, Amir Tamrakar category:cs.CV cs.AI cs.HC cs.LG published:2016-03-21 summary:Most recent work focused on affect from facial expressions, and not as muchon body. This work focuses on body affect analysis. Affect does not occur inisolation. Humans usually couple affect with an action in natural interactions;for example, a person could be talking and smiling. Recognizing body affect insequences requires efficient algorithms to capture both the micro movementsthat differentiate between happy and sad and the macro variations betweendifferent actions. We depart from traditional approaches for time-series dataanalytics by proposing a multi-task learning model that learns a sharedrepresentation that is well-suited for action-affect classification as well asgeneration. For this paper we choose Conditional Restricted Boltzmann Machinesto be our building block. We propose a new model that enhances the CRBM modelwith a factored multi-task component to become Multi-Task ConditionalRestricted Boltzmann Machines (MTCRBMs). We evaluate our approach on twopublicly available datasets, the Body Affect dataset and the Tower Gamedataset, and show superior classification performance improvement over thestate-of-the-art, as well as the generative abilities of our model.
arxiv-17100-289 | A Comparison Study of Nonlinear Kernels | http://arxiv.org/abs/1603.06541 | author:Ping Li category:stat.ML cs.LG published:2016-03-21 summary:In this paper, we compare 5 different nonlinear kernels: min-max, RBF, fRBF(folded RBF), acos, and acos-$\chi^2$, on a wide range of publicly availabledatasets. The proposed fRBF kernel performs very similarly to the RBF kernel.Both RBF and fRBF kernels require an important tuning parameter ($\gamma$).Interestingly, for a significant portion of the datasets, the min-max kerneloutperforms the best-tuned RBF/fRBF kernels. The acos kernel and acos-$\chi^2$kernel also perform well in general and in some datasets achieve the bestaccuracies. One crucial issue with the use of nonlinear kernels is the excessivecomputational and memory cost. These days, one increasingly popular strategy isto linearize the kernels through various randomization algorithms. In ourstudy, the randomization method for the min-max kernel demonstrates excellentperformance compared to the randomization methods for other types of nonlinearkernels, measured in terms of the number of nonzero terms in the transformeddataset. Our study provides evidence for supporting the use of the min-max kernel andthe corresponding randomized linearization method (i.e., the so-called "0-bitCWS"). Furthermore, the results motivate at least two directions for futureresearch: (i) To develop new (and linearizable) nonlinear kernels for betteraccuracies; and (ii) To develop better linearization algorithms for improvingthe current linearization methods for the RBF kernel, the acos kernel, and theacos-$\chi^2$ kernel. One attempt is to combine the min-max kernel with theacos kernel or the acos-$\chi^2$ kernel. The advantages of these two new andtuning-free nonlinear kernels are demonstrated vias our extensive experiments.
arxiv-17100-290 | Deep video gesture recognition using illumination invariants | http://arxiv.org/abs/1603.06531 | author:Otkrist Gupta, Dan Raviv, Ramesh Raskar category:cs.CV cs.LG published:2016-03-21 summary:In this paper we present architectures based on deep neural nets for gesturerecognition in videos, which are invariant to local scaling. We amalgamateautoencoder and predictor architectures using an adaptive weighting schemecoping with a reduced size labeled dataset, while enriching our models fromenormous unlabeled sets. We further improve robustness to lighting conditionsby introducing a new adaptive filer based on temporal local scalenormalization. We provide superior results over known methods, including recentreported approaches based on neural nets.
arxiv-17100-291 | Convex block-sparse linear regression with expanders -- provably | http://arxiv.org/abs/1603.06313 | author:Anastasios Kyrillidis, Bubacarr Bah, Rouzbeh Hasheminezhad, Quoc Tran-Dinh, Luca Baldassarre, Volkan Cevher category:cs.IT math.IT math.OC stat.ML published:2016-03-21 summary:Sparse matrices are favorable objects in machine learning and optimization.When such matrices are used, in place of dense ones, the overall complexityrequirements in optimization can be significantly reduced in practice, both interms of space and run-time. Prompted by this observation, we study a convexoptimization scheme for block-sparse recovery from linear measurements. Toobtain linear sketches, we use expander matrices, i.e., sparse matricescontaining only few non-zeros per column. Hitherto, to the best of ourknowledge, such algorithmic solutions have been only studied from a non-convexperspective. Our aim here is to theoretically characterize the performance ofconvex approaches under such setting. Our key novelty is the expression of the recovery error in terms of themodel-based norm, while assuring that solution lives in the model. To achievethis, we show that sparse model-based matrices satisfy a group version of thenull-space property. Our experimental findings on synthetic and realapplications support our claims for faster recovery in the convex setting -- asopposed to using dense sensing matrices, while showing a competitive recoveryperformance.
arxiv-17100-292 | Static and Dynamic Feature Selection in Morphosyntactic Analyzers | http://arxiv.org/abs/1603.06503 | author:Bernd Bohnet, Miguel Ballesteros, Ryan McDonald, Joakim Nivre category:cs.CL published:2016-03-21 summary:We study the use of greedy feature selection methods for morphosyntactictagging under a number of different conditions. We compare a static ordering offeatures to a dynamic ordering based on mutual information statistics, and weapply the techniques to standalone taggers as well as joint systems for taggingand parsing. Experiments on five languages show that feature selection canresult in more compact models as well as higher accuracy under all conditions,but also that a dynamic ordering works better than a static ordering and thatjoint systems benefit more than standalone taggers. We also show that the sametechniques can be used to select which morphosyntactic categories to predict inorder to maximize syntactic accuracy in a joint system. Our final resultsrepresent a substantial improvement of the state of the art for severallanguages, while at the same time reducing both the number of features and therunning time by up to 80% in some cases.
arxiv-17100-293 | Controlling Explanatory Heatmap Resolution and Semantics via Decomposition Depth | http://arxiv.org/abs/1603.06463 | author:Sebastian Bach, Alexander Binder, Klaus-Robert Müller, Wojciech Samek category:cs.CV published:2016-03-21 summary:We present an application of the Layer-wise Relevance Propagation (LRP)algorithm to state of the art deep convolutional neural networks and FisherVector classifiers to compare the image perception and prediction strategies ofboth classifiers with the use of visualized heatmaps. Layer-wise RelevancePropagation (LRP) is a method to compute scores for individual components of aninput image, denoting their contribution to the prediction of the classifierfor one particular test point. We demonstrate the impact of different choicesof decomposition cut-off points during the LRP-process, controlling theresolution and semantics of the heatmap on test images from the PASCAL VOC 2007test data set.
arxiv-17100-294 | A Survey on Object Detection in Optical Remote Sensing Images | http://arxiv.org/abs/1603.06201 | author:Gong Cheng, Junwei Han category:cs.CV published:2016-03-20 summary:Object detection in optical remote sensing images, being a fundamental butchallenging problem in the field of aerial and satellite image analysis, playsan important role for a wide range of applications and is receiving significantattention in recent years. While enormous methods exist, a deep review of theliterature concerning generic object detection is still lacking. This paperaims to provide a review of the recent progress in this field. Different fromseveral previously published surveys that focus on a specific object class suchas building and road, we concentrate on more generic object categoriesincluding, but are not limited to, road, building, tree, vehicle, ship,airport, urban-area. Covering about 270 publications we survey 1) templatematching-based object detection methods, 2) knowledge-based object detectionmethods, 3) object-based image analysis (OBIA)-based object detection methods,4) machine learning-based object detection methods, and 5) five publiclyavailable datasets and three standard evaluation metrics. We also discuss thechallenges of current studies and propose two promising research directions,namely deep learning-based feature representation and weakly supervisedlearning-based geospatial object detection. It is our hope that this surveywill be beneficial for the researchers to have better understanding of thisresearch field.
arxiv-17100-295 | Modelling Temporal Information Using Discrete Fourier Transform for Video Classification | http://arxiv.org/abs/1603.06182 | author:Haimin Zhang, Min Xu, Changsheng Xu, Ramesh Jain category:cs.CV published:2016-03-20 summary:Recently, video classification attracts intensive research efforts. However,most existing works are based on framelevel visual features, which might failto model the temporal information, e.g. characteristics accumulated along time.In order to capture video temporal information, we propose to analyse featuresin frequency domain transformed by discrete Fourier transform (DFT features).Frame-level features are firstly extract by a pre-trained deep convolutionalneural network (CNN). Then, time domain features are transformed andinterpolated into DFT features. CNN and DFT features are further encoded byusing different pooling methods and fused for video classification. In thisway, static image features extracted from a pre-trained deep CNN and temporalinformation represented by DFT features are jointly considered for videoclassification. We test our method for video emotion classification and actionrecognition. Experimental results demonstrate that combining DFT features caneffectively capture temporal information and therefore improve the performanceof both video emotion classification and action recognition. Our approach hasachieved a state-of-the-art performance on the largest video emotion dataset(VideoEmotion-8 dataset) and competitive results on UCF-101.
arxiv-17100-296 | Towards Automatic Wild Animal Monitoring: Identification of Animal Species in Camera-trap Images using Very Deep Convolutional Neural Networks | http://arxiv.org/abs/1603.06169 | author:Alexander Gomez, Augusto Salazar, Francisco Vargas category:cs.CV published:2016-03-20 summary:Non intrusive monitoring of animals in the wild is possible using cameratrapping framework, which uses cameras triggered by sensors to take a burst ofimages of animals in their habitat. However camera trapping framework producesa high volume of data (in the order on thousands or millions of images), whichmust be analyzed by a human expert. In this work, a method for animal speciesidentification in the wild using very deep convolutional neural networks ispresented. Multiple versions of the Snapshot Serengeti dataset were used inorder to probe the ability of the method to cope with different challenges thatcamera-trap images demand. The method reached 88.9% of accuracy in Top-1 and98.1% in Top-5 in the evaluation set using a residual network topology. Also,the results show that the proposed method outperforms previous approximationsand proves that recognition in camera-trap images can be automated.
arxiv-17100-297 | Collaborative prediction with expert advice | http://arxiv.org/abs/1603.06265 | author:Paul Christiano category:cs.LG published:2016-03-20 summary:Many practical learning systems aggregate data across many users, whilelearning theory traditionally considers a single learner who trusts all oftheir observations. A case in point is the foundational learning problem ofprediction with expert advice. To date, there has been no theoretical study ofthe general collaborative version of prediction with expert advice, in whichmany users face a similar problem and would like to share their experiences inorder to learn faster. A key issue in this collaborative framework isrobustness: generally algorithms that aggregate data are vulnerable tomanipulation by even a small number of dishonest users. We exhibit the first robust collaborative algorithm for prediction withexpert advice. When all users are honest and have similar tastes our algorithmmatches the performance of pooling data and using a traditional algorithm. Butour algorithm also guarantees that adding users never significantly degradesperformance, even if the additional users behave adversarially. We achievestrong guarantees even when the overwhelming majority of users behaveadversarially. As a special case, our algorithm is extremely robust tovariation amongst the users.
arxiv-17100-298 | Joint Stochastic Approximation learning of Helmholtz Machines | http://arxiv.org/abs/1603.06170 | author:Haotian Xu, Zhijian Ou category:cs.LG stat.ML published:2016-03-20 summary:Though with progress, model learning and performing posterior inference stillremains a common challenge for using deep generative models, especially forhandling discrete hidden variables. This paper is mainly concerned withalgorithms for learning Helmholz machines, which is characterized by pairingthe generative model with an auxiliary inference model. A common drawback ofprevious learning algorithms is that they indirectly optimize some bounds ofthe targeted marginal log-likelihood. In contrast, we successfully develop anew class of algorithms, based on stochastic approximation (SA) theory of theRobbins-Monro type, to directly optimize the marginal log-likelihood andsimultaneously minimize the inclusive KL-divergence. The resulting learningalgorithm is thus called joint SA (JSA). Moreover, we construct an effectiveMCMC operator for JSA. Our results on the MNIST datasets demonstrate that theJSA's performance is consistently superior to that of competing algorithms likeRWS, for learning a range of difficult models.
arxiv-17100-299 | Segmentation from Natural Language Expressions | http://arxiv.org/abs/1603.06180 | author:Ronghang Hu, Marcus Rohrbach, Trevor Darrell category:cs.CV published:2016-03-20 summary:In this paper we approach the novel problem of segmenting an image based on anatural language expression. This is different from traditional semanticsegmentation over a predefined set of semantic classes, as e.g., the phrase"two men sitting on the right bench" requires segmenting only the two people onthe right bench and no one standing or sitting on another bench. Previousapproaches suitable for this task were limited to a fixed set of categoriesand/or rectangular regions. To produce pixelwise segmentation for the languageexpression, we propose an end-to-end trainable recurrent and convolutionalnetwork model that jointly learns to process visual and linguistic information.In our model, a recurrent LSTM network is used to encode the referentialexpression into a vector representation, and a fully convolutional network isused to a extract a spatial feature map from the image and output a spatialresponse map for the target object. We demonstrate on a benchmark dataset thatour model can produce quality segmentation output from the natural languageexpression, and outperforms baseline methods by a large margin.
arxiv-17100-300 | Modelling Temporal Information Using Discrete Fourier Transform for Recognizing Emotions in User-generated Videos | http://arxiv.org/abs/1603.06568 | author:Haimin Zhang, Min Xu category:cs.CV published:2016-03-20 summary:With the widespread of user-generated Internet videos, emotion recognition inthose videos attracts increasing research efforts. However, most existing worksare based on framelevel visual features and/or audio features, which might failto model the temporal information, e.g. characteristics accumulated along time.In order to capture video temporal information, in this paper, we propose toanalyse features in frequency domain transformed by discrete Fourier transform(DFT features). Frame-level features are firstly extract by a pre-trained deepconvolutional neural network (CNN). Then, time domain features are transferredand interpolated into DFT features. CNN and DFT features are further encodedand fused for emotion classification. By this way, static image featuresextracted from a pre-trained deep CNN and temporal information represented byDFT features are jointly considered for video emotion recognition. Experimentalresults demonstrate that combining DFT features can effectively capturetemporal information and therefore improve emotion recognition performance. Ourapproach has achieved a state-of-the-art performance on the largest videoemotion dataset (VideoEmotion-8 dataset), improving accuracy from 51.1% to62.6%.
