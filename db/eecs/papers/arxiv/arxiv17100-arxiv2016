arxiv-17100-1 | Learning to Generate Genotypes with Neural Networks | http://arxiv.org/pdf/1604.04153v1.pdf | author:Alexander W. Churchill, Siddharth Sigtia, Chrisantha Fernando category:cs.NE published:2016-04-14 summary:Neural networks and evolutionary computation have a rich intertwined history.They most commonly appear together when an evolutionary algorithm optimises theparameters and topology of a neural network for reinforcement learningproblems, or when a neural network is applied as a surrogate fitness functionto aid the evolutionary optimisation of expensive fitness functions. In thispaper we take a different approach, asking the question of whether a neuralnetwork can be used to provide a mutation distribution for an evolutionaryalgorithm, and what advantages this approach may offer? Two modern neuralnetwork models are investigated, a Denoising Autoencoder modified to producestochastic outputs and the Neural Autoregressive Distribution Estimator.Results show that the neural network approach to learning genotypes is able tosolve many difficult discrete problems, such as MaxSat and HIFF, and regularlyoutperforms other evolutionary techniques.
arxiv-17100-2 | A Discrete Firefly Algorithm to Solve a Rich Vehicle Routing Problem Modelling a Newspaper Distribution System with Recycling Policy | http://arxiv.org/pdf/1604.04146v1.pdf | author:E. Osaba, Xin-She Yang, F. Diaz, E. Onieva, A. D. Masegosa, A. Perallos category:cs.NE cs.AI math.OC 78M32 published:2016-04-14 summary:A real-world newspaper distribution problem with recycling policy is tackledin this work. In order to meet all the complex restrictions contained in such aproblem, it has been modeled as a rich vehicle routing problem, which can bemore specifically considered as an asymmetric and clustered vehicle routingproblem with simultaneous pickup and deliveries, variable costs and forbiddenpaths (AC-VRP-SPDVCFP). This is the first study of such a problem in theliterature. For this reason, a benchmark composed by 15 instances has been alsoproposed. In the design of this benchmark, real geographical positions havebeen used, located in the province of Bizkaia, Spain. For the proper treatmentof this AC-VRP-SPDVCFP, a discrete firefly algorithm (DFA) has been developed.This application is the first application of the firefly algorithm to any richvehicle routing problem. To prove that the proposed DFA is a promisingtechnique, its performance has been compared with two other well-knowntechniques: an evolutionary algorithm and an evolutionary simulated annealing.Our results have shown that the DFA has outperformed these two classicmeta-heuristics.
arxiv-17100-3 | Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle | http://arxiv.org/pdf/1604.04144v1.pdf | author:Jason Kuen, Kian Ming Lim, Chin Poo Lee category:cs.CV cs.LG cs.NE published:2016-04-14 summary:Visual representation is crucial for a visual tracking method's performances.Conventionally, visual representations adopted in visual tracking rely onhand-crafted computer vision descriptors. These descriptors were developedgenerically without considering tracking-specific information. In this paper,we propose to learn complex-valued invariant representations from trackedsequential image patches, via strong temporal slowness constraint and stackedconvolutional autoencoders. The deep slow local representations are learnedoffline on unlabeled data and transferred to the observational model of ourproposed tracker. The proposed observational model retains old training samplesto alleviate drift, and collect negative samples which are coherent withtarget's motion pattern for better discriminative tracking. With the learnedrepresentation and online training samples, a logistic regression classifier isadopted to distinguish target from background, and retrained online to adapt toappearance changes. Subsequently, the observational model is integrated into aparticle filter framework to peform visual tracking. Experimental results onvarious challenging benchmark sequences demonstrate that the proposed trackerperforms favourably against several state-of-the-art trackers.
arxiv-17100-4 | On Reducing the Number of Visual Words in the Bag-of-Features Representation | http://arxiv.org/pdf/1604.04142v1.pdf | author:Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro category:cs.CV cs.IR published:2016-04-14 summary:A new class of applications based on visual search engines are emerging,especially on smart-phones that have evolved into powerful tools for processingimages and videos. The state-of-the-art algorithms for large visual contentrecognition and content based similarity search today use the "Bag of Features"(BoF) or "Bag of Words" (BoW) approach. The idea, borrowed from text retrieval,enables the use of inverted files. A very well known issue with this approachis that the query images, as well as the stored data, are described withthousands of words. This poses obvious efficiency problems when using invertedfiles to perform efficient image matching. In this paper, we propose andcompare various techniques to reduce the number of words describing an image toimprove efficiency and we study the effects of this reduction on effectivenessin landmark recognition and retrieval scenarios. We show that very relevantimprovement in performance are achievable still preserving the advantages ofthe BoF base approach.
arxiv-17100-5 | An Improved Discrete Bat Algorithm for Symmetric and Asymmetric Traveling Salesman Problems | http://arxiv.org/pdf/1604.04138v1.pdf | author:Eneko Osaba, Xin-She Yang, Fernando Diaz, Pedro Lopez-Garcia, Roberto Carballedo category:cs.NE cs.AI math.OC 78M32 published:2016-04-14 summary:Bat algorithm is a population metaheuristic proposed in 2010 which is basedon the echolocation or bio-sonar characteristics of microbats. Since its firstimplementation, the bat algorithm has been used in a wide range of fields. Inthis paper, we present a discrete version of the bat algorithm to solve thewell-known symmetric and asymmetric traveling salesman problems. In addition,we propose an improvement in the basic structure of the classic bat algorithm.To prove that our proposal is a promising approximation method, we havecompared its performance in 37 instances with the results obtained by fivedifferent techniques: evolutionary simulated annealing, genetic algorithm, anisland based distributed genetic algorithm, a discrete firefly algorithm and animperialist competitive algorithm. In order to obtain fair and rigorouscomparisons, we have conducted three different statistical tests along thepaper: the Student's $t$-test, the Holm's test, and the Friedman test. We havealso compared the convergence behaviour shown by our proposal with the onesshown by the evolutionary simulated annealing, and the discrete fireflyalgorithm. The experimentation carried out in this study has shown that thepresented improved bat algorithm outperforms significantly all the otheralternatives in most of the cases.
arxiv-17100-6 | Filling in the details: Perceiving from low fidelity images | http://arxiv.org/pdf/1604.04125v1.pdf | author:Farahnaz Ahmed Wick, Michael L. Wick, Marc Pomplun category:cs.CV cs.LG cs.NE published:2016-04-14 summary:Humans perceive their surroundings in great detail even though most of ourvisual field is reduced to low-fidelity color-deprived (e.g. dichromatic) inputby the retina. In contrast, most deep learning architectures arecomputationally wasteful in that they consider every part of the input whenperforming an image processing task. Yet, the human visual system is able toperform visual reasoning despite having only a small fovea of high visualacuity. With this in mind, we wish to understand the extent to whichconnectionist architectures are able to learn from and reason with low acuity,distorted inputs. Specifically, we train autoencoders to generate full-detailimages from low-detail "foveations" of those images and then measure theirability to reconstruct the full-detail images from the foveated versions. Byvarying the type of foveation, we can study how well the architectures can copewith various types of distortion. We find that the autoencoder compensates forlower detail by learning increasingly global feature functions. In many cases,the learnt features are suitable for reconstructing the original full-detailimage. For example, we find that the networks accurately perceive color in theperiphery, even when 75\% of the input is achromatic.
arxiv-17100-7 | Training Deep Networks with Structured Layers by Matrix Backpropagation | http://arxiv.org/pdf/1509.07838v4.pdf | author:Catalin Ionescu, Orestis Vantzos, Cristian Sminchisescu category:cs.CV cs.AI published:2015-09-25 summary:Deep neural network architectures have recently produced excellent results ina variety of areas in artificial intelligence and visual recognition, wellsurpassing traditional shallow architectures trained using hand-designedfeatures. The power of deep networks stems both from their ability to performlocal computations followed by pointwise non-linearities over increasinglylarger receptive fields, and from the simplicity and scalability of thegradient-descent training procedure based on backpropagation. An open problemis the inclusion of layers that perform global, structured matrix computationslike segmentation (e.g. normalized cuts) or higher-order pooling (e.g.log-tangent space metrics defined over the manifold of symmetric positivedefinite matrices) while preserving the validity and efficiency of anend-to-end deep training framework. In this paper we propose a soundmathematical apparatus to formally integrate global structured computation intodeep computation architectures. At the heart of our methodology is thedevelopment of the theory and practice of backpropagation that generalizes tothe calculus of adjoint matrix variations. The proposed matrix backpropagationmethodology applies broadly to a variety of problems in machine learning orcomputational perception. Here we illustrate it by performing visualsegmentation experiments using the BSDS and MSCOCO benchmarks, where we showthat deep networks relying on second-order pooling and normalized cuts layers,trained end-to-end using matrix backpropagation, outperform counterparts thatdo not take advantage of such global layers.
arxiv-17100-8 | Deep Residual Networks with Exponential Linear Unit | http://arxiv.org/pdf/1604.04112v1.pdf | author:Anish Shah, Eashan Kadam, Hena Shah, Sameer Shinde category:cs.CV published:2016-04-14 summary:Very deep convolutional neural networks introduced new problems likevanishing gradient and degradation. The recent successful contributions towardssolving these problems are Residual and Highway Networks. These networksintroduce skip connections that allow the information (from the input or thoselearned in earlier layers) to flow more into the deeper layers. These very deepmodels have lead to a considerable decrease in test errors, on benchmarks likeImageNet and COCO. In this paper, we propose the use of exponential linear unitinstead of the combination of ReLU and Batch Normalization in ResidualNetworks. We show that this not only speeds up learning in Residual Networksbut also improves the accuracy as the depth increases. It improves the testerror on almost all data sets, like CIFAR-10 and CIFAR-100
arxiv-17100-9 | DeepFool: a simple and accurate method to fool deep neural networks | http://arxiv.org/pdf/1511.04599v2.pdf | author:Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard category:cs.LG published:2015-11-14 summary:State-of-the-art deep neural networks have achieved impressive results onmany image classification tasks. However, these same architectures have beenshown to be unstable to small, well sought, perturbations of the images.Despite the importance of this phenomenon, no effective methods have beenproposed to accurately compute the robustness of state-of-the-art deepclassifiers to such perturbations on large-scale datasets. In this paper, wefill this gap and propose the DeepFool algorithm to efficiently computeperturbations that fool deep networks, and thus reliably quantify therobustness of these classifiers. Extensive experimental results show that ourapproach outperforms recent methods in the task of computing adversarialperturbations and making classifiers more robust.
arxiv-17100-10 | Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources | http://arxiv.org/pdf/1511.06973v2.pdf | author:Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, Anton van den Hengel category:cs.CV published:2015-11-22 summary:We propose a method for visual question answering which combines an internalrepresentation of the content of an image with information extracted from ageneral knowledge base to answer a broad range of image-based questions. Thisallows more complex questions to be answered using the predominant neuralnetwork-based approach than has previously been possible. It particularlyallows questions to be asked about the contents of an image, even when theimage itself does not contain the whole answer. The method constructs a textualrepresentation of the semantic content of an image, and merges it with textualinformation sourced from a knowledge base, to develop a deeper understanding ofthe scene viewed. Priming a recurrent neural network with this combinedinformation, and the submitted question, leads to a very flexible visualquestion answering approach. We are specifically able to answer questions posedin natural language, that refer to information not contained in the image. Wedemonstrate the effectiveness of our model on two publicly available datasets,Toronto COCO-QA and MS COCO-VQA and show that it produces the best reportedresults in both cases.
arxiv-17100-11 | Unconstrained Facial Landmark Localization with Backbone-Branches Fully-Convolutional Networks | http://arxiv.org/pdf/1507.03409v3.pdf | author:Zhujin Liang, Shengyong Ding, Liang Lin category:cs.CV published:2015-07-13 summary:This paper investigates how to rapidly and accurately localize faciallandmarks in unconstrained, cluttered environments rather than in the wellsegmented face images. We present a novel Backbone-Branches Fully-ConvolutionalNeural Network (BB-FCN), which produces facial landmark response maps directlyfrom raw images without relying on pre-process or sliding window approaches.BB-FCN contains one backbone and a number of network branches with eachcorresponding to one landmark type, and it operates in a progressive manner.Specifically, the backbone roughly detects the locations of facial landmarks bytaking the whole image as input, and the branches further refine thelocalizations based on a local observation from the backbone's intermediatefeature map. Moreover, our backbone-branches architecture does not containfull-connection layers for location regression, leading to efficient learningand inference. Our extensive experiments show that our model achieves superiorperformances over other state-of-the-arts under both the constrained (i.e. withface regions) and the "in the wild" scenarios.
arxiv-17100-12 | Optimal Rates For Regularization Of Statistical Inverse Learning Problems | http://arxiv.org/pdf/1604.04054v1.pdf | author:Gilles Blanchard, Nicole Mücke category:stat.ML published:2016-04-14 summary:We consider a statistical inverse learning problem, where we observe theimage of a function $f$ through a linear operator $A$ at i.i.d. random designpoints $X_i$, superposed with an additive noise. The distribution of the designpoints is unknown and can be very general. We analyze simultaneously the direct(estimation of $Af$) and the inverse (estimation of $f$) learning problems. Inthis general framework, we obtain strong and weak minimax optimal rates ofconvergence (as the number of observations $n$ grows large) for a large classof spectral regularization methods over regularity classes defined throughappropriate source conditions. This improves on or completes previous resultsobtained in related settings. The optimality of the obtained rates is shown notonly in the exponent in $n$ but also in the explicit dependency of the constantfactor in the variance of the noise and the radius of the source condition set.
arxiv-17100-13 | Object Detection from Video Tubelets with Convolutional Neural Networks | http://arxiv.org/pdf/1604.04053v1.pdf | author:Kai Kang, Wanli Ouyang, Hongsheng Li, Xiaogang Wang category:cs.CV published:2016-04-14 summary:Deep Convolution Neural Networks (CNNs) have shown impressive performance invarious vision tasks such as image classification, object detection andsemantic segmentation. For object detection, particularly in still images, theperformance has been significantly increased last year thanks to powerful deepnetworks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNNfeatures (R-CNN)). The lately introduced ImageNet task on object detection fromvideo (VID) brings the object detection task into the video domain, in whichobjects' locations at each frame are required to be annotated with boundingboxes. In this work, we introduce a complete framework for the VID task basedon still-image object detection and general object tracking. Their relationsand contributions in the VID task are thoroughly studied and evaluated. Inaddition, a temporal convolution network is proposed to incorporate temporalinformation to regularize the detection results and shows its effectiveness forthe task.
arxiv-17100-14 | Deep Feature Based Contextual Model for Object Detection | http://arxiv.org/pdf/1604.04048v1.pdf | author:Wenqing Chu, Deng Cai category:cs.CV published:2016-04-14 summary:Object detection is one of the most active areas in computer vision, whichhas made significant improvement in recent years. Current state-of-the-artobject detection methods mostly adhere to the framework of regions withconvolutional neural network (R-CNN) and only use local appearance featuresinside object bounding boxes. Since these approaches ignore the contextualinformation around the object proposals, the outcome of these detectors maygenerate a semantically incoherent interpretation of the input image. In thispaper, we propose an ensemble object detection system which incorporates thelocal appearance, the contextual information in term of relationships amongobjects and the global scene based contextual feature generated by aconvolutional neural network. The system is formulated as a fully connectedconditional random field (CRF) defined on object proposals and the contextualconstraints among object proposals are modeled as edges naturally. Furthermore,a fast mean field approximation method is utilized to inference in this CRFmodel efficiently. The experimental results demonstrate that our approachachieves a higher mean average precision (mAP) on PASCAL VOC 2007 datasetscompared to the baseline algorithm Faster R-CNN.
arxiv-17100-15 | Harnessing Deep Neural Networks with Logic Rules | http://arxiv.org/pdf/1603.06318v2.pdf | author:Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric Xing category:cs.LG cs.AI cs.CL stat.ML published:2016-03-21 summary:Combining deep neural networks with structured logic rules is desirable toharness flexibility and reduce unpredictability of the neural models. Wepropose a general framework capable of enhancing various types of neuralnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.Specifically, we develop an iterative distillation method that transfers thestructured information of logic rules into the weights of neural networks. Wedeploy the framework on a CNN for sentiment analysis, and an RNN for namedentity recognition. With a few highly intuitive rules, we obtain substantialimprovements and achieve state-of-the-art or comparable results to previousbest-performing systems.
arxiv-17100-16 | Fast Parallel Randomized Algorithm for Nonnegative Matrix Factorization with KL Divergence for Large Sparse Datasets | http://arxiv.org/pdf/1604.04026v1.pdf | author:Duy Khuong Nguyen, Tu Bao Ho category:math.OC cs.LG cs.NA published:2016-04-14 summary:Nonnegative Matrix Factorization (NMF) with Kullback-Leibler Divergence(NMF-KL) is one of the most significant NMF problems and equivalent toProbabilistic Latent Semantic Indexing (PLSI), which has been successfullyapplied in many applications. For sparse count data, a Poisson distribution andKL divergence provide sparse models and sparse representation, which describethe random variation better than a normal distribution and Frobenius norm.Specially, sparse models provide more concise understanding of the appearanceof attributes over latent components, while sparse representation providesconcise interpretability of the contribution of latent components overinstances. However, minimizing NMF with KL divergence is much more difficultthan minimizing NMF with Frobenius norm; and sparse models, sparserepresentation and fast algorithms for large sparse datasets are stillchallenges for NMF with KL divergence. In this paper, we propose a fastparallel randomized coordinate descent algorithm having fast convergence forlarge sparse datasets to archive sparse models and sparse representation. Theproposed algorithm's experimental results overperform the current studies' onesin this problem.
arxiv-17100-17 | Learning Deep Structure-Preserving Image-Text Embeddings | http://arxiv.org/pdf/1511.06078v2.pdf | author:Liwei Wang, Yin Li, Svetlana Lazebnik category:cs.CV cs.CL cs.LG published:2015-11-19 summary:This paper proposes a method for learning joint embeddings of images and textusing a two-branch neural network with multiple layers of linear projectionsfollowed by nonlinearities. The network is trained using a large marginobjective that combines cross-view ranking constraints with within-viewneighborhood structure preservation constraints inspired by metric learningliterature. Extensive experiments show that our approach gains significantimprovements in accuracy for image-to-text and text-to-image retrieval. Ourmethod achieves new state-of-the-art results on the Flickr30K and MSCOCOimage-sentence datasets and shows promise on the new task of phraselocalization on the Flickr30K Entities dataset.
arxiv-17100-18 | Simple one-pass algorithm for penalized linear regression with cross-validation on MapReduce | http://arxiv.org/pdf/1307.0048v3.pdf | author:Kun Yang category:stat.ML cs.DC cs.LG published:2013-06-28 summary:In this paper, we propose a one-pass algorithm on MapReduce for penalizedlinear regression \[f_\lambda(\alpha, \beta) = \Y - \alpha\mathbf{1} - X\beta\_2^2 +p_{\lambda}(\beta)\] where $\alpha$ is the intercept which can be omitteddepending on application; $\beta$ is the coefficients and $p_{\lambda}$ is thepenalized function with penalizing parameter $\lambda$. $f_\lambda(\alpha,\beta)$ includes interesting classes such as Lasso, Ridge regression andElastic-net. Compared to latest iterative distributed algorithms requiringmultiple MapReduce jobs, our algorithm achieves huge performance improvement;moreover, our algorithm is exact compared to the approximate algorithms such asparallel stochastic gradient decent. Moreover, what our algorithm distinguisheswith others is that it trains the model with cross validation to choose optimal$\lambda$ instead of user specified one. Key words: penalized linear regression, lasso, elastic-net, ridge, MapReduce
arxiv-17100-19 | Learnt quasi-transitive similarity for retrieval from large collections of faces | http://arxiv.org/pdf/1603.00560v2.pdf | author:Ognjen Arandjelovic category:cs.CV published:2016-03-02 summary:We are interested in identity-based retrieval of face sets from largeunlabelled collections acquired in uncontrolled environments. Given a baselinealgorithm for measuring the similarity of two face sets, the meta-algorithmintroduced in this paper seeks to leverage the structure of the data corpus tomake the best use of the available baseline. In particular, we show how partialtransitivity of inter-personal similarity can be exploited to improve theretrieval of particularly challenging sets which poorly match the query underthe baseline measure. We: (i) describe the use of proxy sets as a means ofcomputing the similarity between two sets, (ii) introduce transitivitymeta-features based on the similarity of salient modes of appearance variationbetween sets, (iii) show how quasi-transitivity can be learnt from suchfeatures without any labelling or manual intervention, and (iv) demonstrate theeffectiveness of the proposed methodology through experiments on thenotoriously challenging YouTube database and two successful baselines from theliterature.
arxiv-17100-20 | Factors in Finetuning Deep Model for object detection | http://arxiv.org/pdf/1601.05150v2.pdf | author:Wanli Ouyang, Xiaogang Wang, Cong Zhang, Xiaokang Yang category:cs.CV published:2016-01-20 summary:Finetuning from a pretrained deep model is found to yield state-of-the-artperformance for many vision tasks. This paper investigates many factors thatinfluence the performance in finetuning for object detection. There is along-tailed distribution of sample numbers for classes in object detection. Ouranalysis and empirical results show that classes with more samples have higherimpact on the feature learning. And it is better to make the sample number moreuniform across classes. Generic object detection can be considered as multipleequally important tasks. Detection of each class is a task. These classes/taskshave their individuality in discriminative visual appearance representation.Taking this individuality into account, we cluster objects into visuallysimilar class groups and learn deep representations for these groupsseparately. A hierarchical feature learning scheme is proposed. In this scheme,the knowledge from the group with large number of classes is transferred forlearning features in its sub-groups. Finetuned on the GoogLeNet model,experimental results show 4.7% absolute mAP improvement of our approach on theImageNet object detection dataset without increasing much computational cost atthe testing stage.
arxiv-17100-21 | A Linearly-Convergent Stochastic L-BFGS Algorithm | http://arxiv.org/pdf/1508.02087v2.pdf | author:Philipp Moritz, Robert Nishihara, Michael I. Jordan category:math.OC cs.LG math.NA stat.CO stat.ML published:2015-08-09 summary:We propose a new stochastic L-BFGS algorithm and prove a linear convergencerate for strongly convex and smooth functions. Our algorithm draws heavily froma recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well asa recent approach to variance reduction for stochastic gradient descent fromJohnson and Zhang (2013). We demonstrate experimentally that our algorithmperforms well on large-scale convex and non-convex optimization problems,exhibiting linear convergence and rapidly solving the optimization problems tohigh levels of precision. Furthermore, we show that our algorithm performs wellfor a wide-range of step sizes, often differing by several orders of magnitude.
arxiv-17100-22 | Conditional Random Fields as Recurrent Neural Networks | http://arxiv.org/pdf/1502.03240v3.pdf | author:Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr category:cs.CV published:2015-02-11 summary:Pixel-level labelling tasks, such as semantic segmentation, play a centralrole in image understanding. Recent approaches have attempted to harness thecapabilities of deep learning techniques for image recognition to tacklepixel-level labelling tasks. One central issue in this methodology is thelimited capacity of deep learning techniques to delineate visual objects. Tosolve this problem, we introduce a new form of convolutional neural networkthat combines the strengths of Convolutional Neural Networks (CNNs) andConditional Random Fields (CRFs)-based probabilistic graphical modelling. Tothis end, we formulate mean-field approximate inference for the ConditionalRandom Fields with Gaussian pairwise potentials as Recurrent Neural Networks.This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain adeep network that has desirable properties of both CNNs and CRFs. Importantly,our system fully integrates CRF modelling with CNNs, making it possible totrain the whole deep network end-to-end with the usual back-propagationalgorithm, avoiding offline post-processing methods for object delineation. Weapply the proposed method to the problem of semantic image segmentation,obtaining top results on the challenging Pascal VOC 2012 segmentationbenchmark.
arxiv-17100-23 | Theoretically-Grounded Policy Advice from Multiple Teachers in Reinforcement Learning Settings with Applications to Negative Transfer | http://arxiv.org/pdf/1604.03986v1.pdf | author:Yusen Zhan, Haitham Bou Ammar, Matthew E. taylor category:cs.LG published:2016-04-13 summary:Policy advice is a transfer learning method where a student agent is able tolearn faster via advice from a teacher. However, both this and otherreinforcement learning transfer methods have little theoretical analysis. Thispaper formally defines a setting where multiple teacher agents can provideadvice to a student and introduces an algorithm to leverage both autonomousexploration and teacher's advice. Our regret bounds justify the intuition thatgood teachers help while bad teachers hurt. Using our formalization, we arealso able to quantify, for the first time, when negative transfer can occurwithin such a reinforcement learning setting.
arxiv-17100-24 | Visual Storytelling | http://arxiv.org/pdf/1604.03968v1.pdf | author:Ting-Hao, Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, Margaret Mitchell category:cs.CL cs.AI cs.CV published:2016-04-13 summary:We introduce the first dataset for sequential vision-to-language, and explorehow this data may be used for the task of visual storytelling. The firstrelease of this dataset, SIND v.1, includes 81,743 unique photos in 20,211sequences, aligned to both descriptive (caption) and story language. Weestablish several strong baselines for the storytelling task, and motivate anautomatic metric to benchmark progress. Modelling concrete description as wellas figurative and social language, as provided in this dataset and thestorytelling task, has the potential to move artificial intelligence from basicunderstandings of typical visual scenes towards more and more human-likeunderstanding of grounded event structure and subjective expression.
arxiv-17100-25 | Accelerated graph-based nonlinear denoising filters | http://arxiv.org/pdf/1512.00389v2.pdf | author:Andrew Knyazev, Alexander Malyshev category:cs.CV math.NA 65F30 I.4.3; G.1.3 published:2015-12-01 summary:Denoising filters, such as bilateral, guided, and total variation filters,applied to images on general graphs may require repeated application if noiseis not small enough. We formulate two acceleration techniques of the resultediterations: conjugate gradient method and Nesterov's acceleration. Wenumerically show efficiency of the accelerated nonlinear filters for imagedenoising and demonstrate 2-12 times speed-up, i.e., the accelerationtechniques reduce the number of iterations required to reach a given peaksignal-to-noise ratio (PSNR) by the above indicated factor of 2-12.
arxiv-17100-26 | Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis | http://arxiv.org/pdf/1604.03930v1.pdf | author:Rong Ge, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, Aaron Sidford category:cs.LG math.OC stat.ML published:2016-04-13 summary:This paper considers the problem of canonical-correlation analysis (CCA)(Hotelling, 1936) and, more broadly, the generalized eigenvector problem for apair of symmetric matrices. These are two fundamental problems in data analysisand scientific computing with numerous applications in machine learning andstatistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009). We provide simple iterative algorithms, with improved runtimes, for solvingthese problems that are globally linearly convergent with moderate dependencieson the condition numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-$k$ generalized eigenvectorproblem. We solve this problem through a general framework that simply requiresblack box access to an approximate linear system solver. Instantiating thisframework with accelerated gradient descent we obtain a running time of$O(\frac{z k \sqrt{\kappa}}{\rho} \log(1/\epsilon) \log\left(k\kappa/\rho\right))$ where $z$ is the total number of nonzero entries,$\kappa$ is the condition number and $\rho$ is the relative eigenvalue gap ofthe appropriate matrices. Our algorithm is linear in the input size and the number of components $k$ upto a $\log(k)$ factor. This is essential for handling large-scale matrices thatappear in practice. To the best of our knowledge this is the first suchalgorithm with global linear convergence. We hope that our results promptfurther research and ultimately improve the practical running time forperforming these important data analysis procedures on large data sets.
arxiv-17100-27 | Partition Functions from Rao-Blackwellized Tempered Sampling | http://arxiv.org/pdf/1603.01912v2.pdf | author:David Carlson, Patrick Stinson, Ari Pakman, Liam Paninski category:stat.ML published:2016-03-07 summary:Partition functions of probability distributions are important quantities formodel evaluation and comparisons. We present a new method to compute partitionfunctions of complex and multimodal distributions. Such distributions are oftensampled using simulated tempering, which augments the target space with anauxiliary inverse temperature variable. Our method exploits the multinomialprobability law of the inverse temperatures, and provides estimates of thepartition function in terms of a simple quotient of Rao-Blackwellized marginalinverse temperature probability estimates, which are updated while sampling. Weshow that the method has interesting connections with several alternativepopular methods, and offers some significant advantages. In particular, weempirically find that the new method provides more accurate estimates thanAnnealed Importance Sampling when calculating partition functions of largeRestricted Boltzmann Machines (RBM); moreover, the method is sufficientlyaccurate to track training and validation log-likelihoods during learning ofRBMs, at minimal computational cost.
arxiv-17100-28 | Max-Information, Differential Privacy, and Post-Selection Hypothesis Testing | http://arxiv.org/pdf/1604.03924v1.pdf | author:Ryan Rogers, Aaron Roth, Adam Smith, Om Thakkar category:cs.LG published:2016-04-13 summary:In this paper, we initiate a principled study of how the generalizationproperties of approximate differential privacy can be used to perform adaptivehypothesis testing, while giving statistically valid $p$-value corrections. Wedo this by observing that the guarantees of algorithms with bounded approximatemax-information are sufficient to correct the $p$-values of adaptively chosenhypotheses, and then by proving that algorithms that satisfy$(\epsilon,\delta)$-differential privacy have bounded approximate maxinformation when their inputs are drawn from a product distribution. Thissubstantially extends the known connection between differential privacy andmax-information, which previously was only known to hold for (pure)$(\epsilon,0)$-differential privacy. It also extends our understanding ofmax-information as a partially unifying measure controlling the generalizationproperties of adaptive data analyses. We also show a lower bound, proving that(despite the strong composition properties of max-information), when data isdrawn from a product distribution, $(\epsilon,\delta)$-differentially privatealgorithms can come first in a composition with other algorithms satisfyingmax-information bounds, but not necessarily second if the composition isrequired to itself satisfy a nontrivial max-information bound. This, inparticular, implies that the connection between$(\epsilon,\delta)$-differential privacy and max-information holds only forinputs drawn from product distributions, unlike the connection between$(\epsilon,0)$-differential privacy and max-information.
arxiv-17100-29 | Removing Clouds and Recovering Ground Observations in Satellite Image Sequences via Temporally Contiguous Robust Matrix Completion | http://arxiv.org/pdf/1604.03915v1.pdf | author:Jialei Wang, Peder A. Olsen, Andrew R. Conn, Aurelie C. Lozano category:cs.CV cs.LG published:2016-04-13 summary:We consider the problem of removing and replacing clouds in satellite imagesequences, which has a wide range of applications in remote sensing. Ourapproach first detects and removes the cloud-contaminated part of the imagesequences. It then recovers the missing scenes from the clean parts using theproposed "TECROMAC" (TEmporally Contiguous RObust MAtrix Completion) objective.The objective function balances temporal smoothness with a low rank solutionwhile staying close to the original observations. The matrix whose the rows arepixels and columnsare days corresponding to the image, has low-rank because thepixels reflect land-types such as vegetation, roads and lakes and there arerelatively few variations as a result. We provide efficient optimizationalgorithms for TECROMAC, so we can exploit images containing millions ofpixels. Empirical results on real satellite image sequences, as well assimulated data, demonstrate that our approach is able to recover underlyingimages from heavily cloud-contaminated observations.
arxiv-17100-30 | Inverse Reinforcement Learning with Simultaneous Estimation of Rewards and Dynamics | http://arxiv.org/pdf/1604.03912v1.pdf | author:Michael Herman, Tobias Gindele, Jörg Wagner, Felix Schmitt, Wolfram Burgard category:cs.AI cs.LG cs.SY stat.ML published:2016-04-13 summary:Inverse Reinforcement Learning (IRL) describes the problem of learning anunknown reward function of a Markov Decision Process (MDP) from observedbehavior of an agent. Since the agent's behavior originates in its policy andMDP policies depend on both the stochastic system dynamics as well as thereward function, the solution of the inverse problem is significantlyinfluenced by both. Current IRL approaches assume that if the transition modelis unknown, additional samples from the system's dynamics are accessible, orthe observed behavior provides enough samples of the system's dynamics to solvethe inverse problem accurately. These assumptions are often not satisfied. Toovercome this, we present a gradient-based IRL approach that simultaneouslyestimates the system's dynamics. By solving the combined optimization problem,our approach takes into account the bias of the demonstrations, which stemsfrom the generating policy. The evaluation on a synthetic MDP and a transferlearning task shows improvements regarding the sample efficiency as well as theaccuracy of the estimated reward functions and transition models.
arxiv-17100-31 | Single-Image Depth Perception in the Wild | http://arxiv.org/pdf/1604.03901v1.pdf | author:Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng category:cs.CV cs.AI published:2016-04-13 summary:This paper studies single-image depth perception in the wild, i.e.,recovering depth from a single image taken in unconstrained settings. Weintroduce a new dataset "Depth in the Wild" consisting of images in the wildannotated with relative depth between pairs of random points. We also propose anew algorithm that learns to estimate metric depth using annotations ofrelative depth. Compared to the state of the art, our algorithm is simpler andperforms better. Experiments show that our algorithm, combined with existingRGB-D data and our new relative depth annotations, significantly improvessingle-image depth perception in the wild.
arxiv-17100-32 | Staple: Complementary Learners for Real-Time Tracking | http://arxiv.org/pdf/1512.01355v2.pdf | author:Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, Philip Torr category:cs.CV published:2015-12-04 summary:Correlation Filter-based trackers have recently achieved excellentperformance, showing great robustness to challenging situations exhibitingmotion blur and illumination changes. However, since the model that they learndepends strongly on the spatial layout of the tracked object, they arenotoriously sensitive to deformation. Models based on colour statistics havecomplementary traits: they cope well with variation in shape, but suffer whenillumination is not consistent throughout a sequence. Moreover, colourdistributions alone can be insufficiently discriminative. In this paper, weshow that a simple tracker combining complementary cues in a ridge regressionframework can operate faster than 80 FPS and outperform not only all entries inthe popular VOT14 competition, but also recent and far more sophisticatedtrackers according to multiple benchmarks.
arxiv-17100-33 | Algorithms for stochastic optimization with expectation constraints | http://arxiv.org/pdf/1604.03887v1.pdf | author:Guanghui Lan, Zhiqiang Zhou category:math.OC stat.ML published:2016-04-13 summary:This paper considers the problem of minimizing an expectation function over aclosed convex set, coupled with functional constraints given in the form ofexpectation. We present a new stochastic approximation (SA) type algorithm,namely the alternating mirror-descent SA (AMD-SA) algorithm, to minimize theseexpectation constrained problems, and show that it exhibits the optimal rate ofconvergence when the objective and constraint functions are convex or stronglyconvex. We also present a variant of this algorithm to solve a special class ofstructured nonconvex problems and establish its rate of convergence. It isworth noting that all theses algorithms are primal methods which do not requirethe estimation of the dual variables. We also provide some promisingpreliminary numerical results for these algorithms applied to some portfoliooptimization and machine learning problems.
arxiv-17100-34 | The Effect of Distortions on the Prediction of Visual Attention | http://arxiv.org/pdf/1604.03882v1.pdf | author:Milind S. Gide, Samuel F. Dodge, Lina J. Karam category:cs.CV published:2016-04-13 summary:Existing saliency models have been designed and evaluated for predicting thesaliency in distortion-free images. However, in practice, the image quality isaffected by a host of factors at several stages of the image processingpipeline such as acquisition, compression and transmission. Several studieshave explored the effect of distortion on human visual attention; however, noneof them have considered the performance of visual saliency models in thepresence of distortion. Furthermore, given that one potential application ofvisual saliency prediction is to aid pooling of objective visual qualitymetrics, it is important to compare the performance of existing saliency modelson distorted images. In this paper, we evaluate several state-of-the-art visualattention models over different databases consisting of distorted images withvarious types of distortions such as blur, noise and compression with varyinglevels of distortion severity. This paper also introduces new improvedperformance evaluation metrics that are shown to overcome shortcomings inexisting performance metrics. We find that the performance of most modelsimproves with moderate and high levels of distortions as compared to the neardistortion-free case. In addition, model performance is also found to decreasewith an increase in image complexity.
arxiv-17100-35 | Detangling People: Individuating Multiple Close People and Their Body Parts via Region Assembly | http://arxiv.org/pdf/1604.03880v1.pdf | author:Hao Jiang, Kristen Grauman category:cs.CV published:2016-04-13 summary:Today's person detection methods work best when people are in common uprightposes and appear reasonably well spaced out in the image. However, in many realimages, that's not what people do. People often appear quite close to eachother, e.g., with limbs linked or heads touching, and their poses are often notpedestrian-like. We propose an approach to detangle people in multi-personimages. We formulate the task as a region assembly problem. Starting from alarge set of overlapping regions from body part semantic segmentation andgeneric object proposals, our optimization approach reassembles those piecestogether into multiple person instances. It enforces that the composed bodypart regions of each person instance obey constraints on relative sizes, mutualspatial relationships, foreground coverage, and exclusive label assignmentswhen overlapping. Since optimal region assembly is a challenging combinatorialproblem, we present a Lagrangian relaxation method to accelerate the lowerbound estimation, thereby enabling a fast branch and bound solution for theglobal optimum. As output, our method produces a pixel-level map indicatingboth 1) the body part labels (arm, leg, torso, and head), and 2) which partsbelong to which individual person. Our results on three challenging datasetsshow our method is robust to clutter, occlusion, and complex poses. Itoutperforms a variety of competing methods, including existing detector CRFmethods and region CNN approaches. In addition, we demonstrate its impact on aproxemics recognition task, which demands a precise representation of "whosebody part is where" in crowded images.
arxiv-17100-36 | Hierarchical Compound Poisson Factorization | http://arxiv.org/pdf/1604.03853v1.pdf | author:Mehmet E. Basbug, Barbara Engelhardt category:cs.LG cs.AI stat.ML published:2016-04-13 summary:Non-negative matrix factorization models based on a hierarchicalGamma-Poisson structure capture user and item behavior effectively in extremelysparse data sets, making them the ideal choice for collaborative filteringapplications. Hierarchical Poisson factorization (HPF) in particular has provedsuccessful for scalable recommendation systems with extreme sparsity. HPF,however, suffers from a tight coupling of sparsity model (absence of a rating)and response model (the value of the rating), which limits the expressivenessof the latter. Here, we introduce hierarchical compound Poisson factorization(HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF tohigh-dimensional extremely sparse matrices. More importantly, HCPF decouplesthe sparsity model from the response model, allowing us to choose the mostsuitable distribution for the response. HCPF can capture binary, non-negativediscrete, non-negative continuous, and zero-inflated continuous responses. Wecompare HCPF with HPF on nine discrete and three continuous data sets andconclude that HCPF captures the relationship between sparsity and responsebetter than HPF.
arxiv-17100-37 | Reversible Image Merging for Low-level Machine Vision | http://arxiv.org/pdf/1604.03832v1.pdf | author:Mikhail Kharinov category:cs.CV published:2016-04-13 summary:In this paper a hierarchical model for pixel clustering and imagesegmentation is developed. In the model an image is hierarchically structured.The original image is treated as a set of nested images, which are capable toreversibly merge with each other. An object is defined as a structural elementof an image, so that, an image is regarded as a maximal object. The simulatingof none-hierarchical optimal pixel clustering by hierarchical clustering isstudied. To generate a hierarchy of optimized piecewise constant imageapproximations, estimated by the standard deviation of approximation from theimage, the conversion of any hierarchy of approximations into the hierarchydescribed in relation to the number of intensity levels by convex sequence oftotal squared errors is proposed.
arxiv-17100-38 | Animation and Chirplet-Based Development of a PIR Sensor Array for Intruder Classification in an Outdoor Environment | http://arxiv.org/pdf/1604.03829v1.pdf | author:Raviteja Upadrashta, Tarun Choubisa, A. Praneeth, Tony G., Aswath V. S., P. Vijay Kumar, Sripad Kowshik, Hari Prasad Gokul R, T. V. Prabhakar category:cs.LG published:2016-04-13 summary:This paper presents the development of a passive infra-red sensor towerplatform along with a classification algorithm to distinguish between humanintrusion, animal intrusion and clutter arising from wind-blown vegetativemovement in an outdoor environment. The research was aimed at exploring thepotential use of wireless sensor networks as an early-warning system to helpmitigate human-wildlife conflicts occurring at the edge of a forest. There arethree important features to the development. Firstly, the sensor platformemploys multiple sensors arranged in the form of a two-dimensional array togive it a key spatial-resolution capability that aids in classification.Secondly, given the challenges of collecting data involving animal intrusion,an Animation-based Simulation tool for Passive Infra-Red sEnsor (ASPIRE) wasdeveloped that simulates signals corresponding to human and animal intrusionand some limited models of vegetative clutter. This speeded up the process ofalgorithm development by allowing us to test different hypotheses in atime-efficient manner. Finally, a chirplet-based model for intruder signal wasdeveloped that significantly helped boost classification accuracy despitedrawing data from a smaller number of sensors. An SVM-based classifier was usedwhich made use of chirplet, energy and signal cross-correlation-based features.The average accuracy obtained for intruder detection and classification onreal-world and simulated data sets was in excess of 97%.
arxiv-17100-39 | Loss Functions for Top-k Error: Analysis and Insights | http://arxiv.org/pdf/1512.00486v2.pdf | author:Maksim Lapin, Matthias Hein, Bernt Schiele category:stat.ML cs.CV cs.LG published:2015-12-01 summary:In order to push the performance on realistic computer vision tasks, thenumber of classes in modern benchmark datasets has significantly increased inrecent years. This increase in the number of classes comes along with increasedambiguity between the class labels, raising the question if top-1 error is theright performance measure. In this paper, we provide an extensive comparisonand evaluation of established multiclass methods comparing their top-kperformance both from a practical as well as from a theoretical perspective.Moreover, we introduce novel top-k loss functions as modifications of thesoftmax and the multiclass SVM losses and provide efficient optimizationschemes for them. In the experiments, we compare on various datasets all of theproposed and established methods for top-k error optimization. An interestinginsight of this paper is that the softmax loss yields competitive top-kperformance for all k simultaneously. For a specific top-k error, our new top-klosses lead typically to further improvements while being faster to train thanthe softmax.
arxiv-17100-40 | Spectral Clustering over the Logistic Random Dot Product Graph | http://arxiv.org/pdf/1510.00850v2.pdf | author:Luke O'Connor, Muriel Médard, Soheil Feizi category:stat.ML published:2015-10-03 summary:Inference of clusters over networks is a central problem in machine learning.Commonly, it is formulated as a discrete optimization, and a continuousrelaxation is used to obtain a spectral algorithm. An alternative problemformulation arises by considering a latent space model, in which edgeprobabilities are determined by continuous latent positions. A model ofparticular interest is the Random Dot Product Graph (RDPG), which can be fitusing an efficient spectral method; however, this method is based on aheuristic that can fail, even in simple cases. In this paper, we consider aclosely related latent space model, the Logistic RDPG, which uses a logisticlink function to map from latent position inner products to edge likelihoods.Over this model, we show that asymptotically exact maximum likelihood inferenceof the latent position vectors can be achieved using a spectral method. Ourmethod involves computing the top eigenvectors of a normalized adjacency matrixand scaling the eigenvectors using a regression step. Through simulations, weshow that this method is more accurate and more robust than existing spectraland semidefinite network clustering methods. In particular, the novelregression scaling step is essential to the performance gain of the proposedmethod.
arxiv-17100-41 | A General Distributed Dual Coordinate Optimization Framework for Regularized Loss Minimization | http://arxiv.org/pdf/1604.03763v1.pdf | author:Shun Zheng, Fen Xia, Wei Xu, Tong Zhang category:cs.LG cs.DC published:2016-04-13 summary:In modern large-scale machine learning applications, the training data areoften partitioned and stored on multiple machines. It is customary to employthe "data parallelism" approach, where the aggregated training loss isminimized without moving data across machines. In this paper, we introduce anovel distributed dual formulation for regularized loss minimization problemsthat can directly handle data parallelism under the distributed computingenvironment. This formulation allows us to systematically derive dualcoordinate optimization procedures, which we refer to as DistributedAlternating Dual Maximization (DADM). The method extends earlier studies aboutdistributed SDCA algorithms and has a rigorous theoretical analysis. Based onthe new formulation, we also develop an accelerated DADM algorithm bygeneralizing the acceleration technique from the accelerated SDCA algorithm tothe distributed setting. Our empirical studies show that our novel approachsignificantly improves the previous state-of-the-art distributed dualcoordinate optimization algorithms.
arxiv-17100-42 | VConv-DAE: Deep Volumetric Shape Learning Without Object Labels | http://arxiv.org/pdf/1604.03755v1.pdf | author:Abhishek Sharma, Oliver Grau, Mario Fritz category:cs.CV cs.GR published:2016-04-13 summary:With the advent of affordable depth sensors, 3D capture becomes more and moreubiquitous and already has made its way into commercial products. Yet,capturing the geometry or complete shapes of everyday objects using scanningdevices (eg. Kinect) still comes with several challenges that result in noiseor even incomplete shapes. Recent success in deep learning has shown how tolearn complex shape distributions in a data-driven way from large scale 3D CADModel collections and to utilize them for 3D processing on volumetricrepresentations and thereby circumventing problems of topology andtessellation. Prior work has shown encouraging results on problems ranging fromshape completion to recognition. We provide an analysis of such approaches anddiscover that training as well as the resulting representation are strongly andunnecessarily tied to the notion of object labels. Furthermore, deep learningresearch argues ~\cite{Vincent08} that learning representation withover-complete model are more prone to overfitting compared to the approach thatlearns from noisy data. Thus, we investigate a full convolutional volumetricdenoising auto encoder that is trained in a unsupervised fashion. Itoutperforms prior work on recognition as well as more challenging tasks likedenoising and shape completion. In addition, our approach is atleast two orderof magnitude faster at test time and thus, provides a path to scaling up 3Ddeep learning.
arxiv-17100-43 | Variational Bayesian Inference of Line Spectra | http://arxiv.org/pdf/1604.03744v1.pdf | author:Mihai-Alin Badiu, Thomas Lundgaard Hansen, Bernard Henri Fleury category:cs.IT math.IT stat.ML published:2016-04-13 summary:In this paper, we address the fundamental problem of line spectral estimationin a Bayesian framework. We target model order and parameter estimation viavariational inference in a probabilistic model in which the frequencies arecontinuous-valued, i.e., not restricted to a grid; and the coefficients aregoverned by a Bernoulli-Gaussian prior model turning model order selection intobinary sequence detection. Unlike earlier works which retain only pointestimates of the frequencies, we undertake a more complete Bayesian treatmentby estimating the posterior probability density functions (pdfs) of thefrequencies and computing expectations over them. Thus, we additionally captureand operate with the uncertainty of the frequency estimates. Aiming to maximizethe model evidence, variational optimization provides analytic approximationsof the posterior pdfs and also gives estimates of the additional parameters. Wepropose an accurate representation of the pdfs of the frequencies by mixturesof von Mises pdfs, which yields closed-form expectations. We define thealgorithm VALSE in which the estimates of the pdfs and parameters areiteratively updated. VALSE is a gridless, convergent method, does not requireparameter tuning, can easily include prior knowledge about the frequencies andprovides approximate posterior pdfs based on which the uncertainty in linespectral estimation can be quantified. Simulation results show that accountingfor the uncertainty of frequency estimates, rather than computing just pointestimates, significantly improves the performance. The performance of VALSE issuperior to that of state-of-the-art methods and closely approaches theCram\'er-Rao bound computed for the true model order.
arxiv-17100-44 | A Differentiable Transition Between Additive and Multiplicative Neurons | http://arxiv.org/pdf/1604.03736v1.pdf | author:Wiebke Köpp, Patrick van der Smagt, Sebastian Urban category:cs.LG stat.ML published:2016-04-13 summary:Existing approaches to combine both additive and multiplicative neural unitseither use a fixed assignment of operations or require discrete optimization todetermine what function a neuron should perform. However, this leads to anextensive increase in the computational complexity of the training procedure. We present a novel, parameterizable transfer function based on themathematical concept of non-integer functional iteration that allows theoperation each neuron performs to be smoothly and, most importantly,differentiablely adjusted between addition and multiplication. This allows thedecision between addition and multiplication to be integrated into the standardbackpropagation training procedure.
arxiv-17100-45 | DENSER Cities: A System for Dense Efficient Reconstructions of Cities | http://arxiv.org/pdf/1604.03734v1.pdf | author:Michael Tanner, Pedro Pinies, Lina Maria Paz, Paul Newman category:cs.CV cs.RO published:2016-04-13 summary:This paper is about the efficient generation of dense, colored models ofcity-scale environments from range data and in particular, stereo cameras.Better maps make for better understanding; better understanding leads to betterrobots, but this comes at a cost. The computational and memory requirements oflarge dense models can be prohibitive. We provide the theory and the systemneeded to create city-scale dense reconstructions. To do so, we apply aregularizer over a compressed 3D data structure while dealing with the complexboundary conditions this induces during the data-fusion stage. We show thatonly with these considerations can we swiftly create neat, large, "wellbehaved" reconstructions. We evaluate our system using the KITTI dataset andprovide statistics for the metric errors in all surfaces created compared tothose measured with 3D laser. Our regularizer reduces the median error by 40%in 3.4 km of dense reconstructions with a median accuracy of 6 cm. Forsubjective analysis, we provide a qualitative review of 6.1 km of our densereconstructions in an attached video. These are the largest densereconstructions from a single passive camera we are aware of in the literature.
arxiv-17100-46 | A Novel Method to Study Bottom-up Visual Saliency and its Neural Mechanism | http://arxiv.org/pdf/1604.08426v1.pdf | author:Cheng Chen, Xilin Zhang, Yizhou Wang, Fang Fang category:cs.CV q-bio.NC published:2016-04-13 summary:In this study, we propose a novel method to measure bottom-up saliency mapsof natural images. In order to eliminate the influence of top-down signals,backward masking is used to make stimuli (natural images) subjectivelyinvisible to subjects, however, the bottom-up saliency can still orient thesubjects attention. To measure this orientation/attention effect, we adopt thecueing effect paradigm by deploying discrimination tasks at each location of animage, and measure the discrimination performance variation across the image asthe attentional effect of the bottom-up saliency. Such attentional effects arecombined to construct a final bottomup saliency map. Based on the proposedmethod, we introduce a new bottom-up saliency map dataset of natural images tobenchmark computational models. We compare several state-of-the-art saliencymodels on the dataset. Moreover, the proposed paradigm is applied toinvestigate the neural basis of the bottom-up visual saliency map by analyzingpsychophysical and fMRI experimental results. Our findings suggest that thebottom-up saliency maps of natural images are constructed in V1. It provides astrong scientific evidence to resolve the long standing dispute in neuroscienceabout where the bottom-up saliency map is constructed in human brain.
arxiv-17100-47 | Quantifying uncertainties on excursion sets under a Gaussian random field prior | http://arxiv.org/pdf/1501.03659v2.pdf | author:Dario Azzimonti, Julien Bect, Clément Chevalier, David Ginsbourger category:math.ST stat.CO stat.ML stat.TH published:2015-01-15 summary:We focus on the problem of estimating and quantifying uncertainties on theexcursion set of a function under a limited evaluation budget. We adopt aBayesian approach where the objective function is assumed to be a realizationof a Gaussian random field. In this setting, the posterior distribution on theobjective function gives rise to a posterior distribution on excursion sets.Several approaches exist to summarize the distribution of such sets based onrandom closed set theory. While the recently proposed Vorob'ev approachexploits analytical formulae, further notions of variability require MonteCarlo estimators relying on Gaussian random field conditional simulations. Inthe present work we propose a method to choose Monte Carlo simulation pointsand obtain quasi-realizations of the conditional field at fine designs throughaffine predictors. The points are chosen optimally in the sense that theyminimize the posterior expected distance in measure between the excursion setand its reconstruction. The proposed method reduces the computational costs dueto Monte Carlo simulations and enables the computation of quasi-realizations onfine designs in large dimensions. We apply this reconstruction approach toobtain realizations of an excursion set on a fine grid which allow us to give anew measure of uncertainty based on the distance transform of the excursionset. Finally we present a safety engineering test case where the simulationmethod is employed to compute a Monte Carlo estimate of a contour line.
arxiv-17100-48 | Bayesian inference in hierarchical models by combining independent posteriors | http://arxiv.org/pdf/1603.09272v2.pdf | author:Ritabrata Dutta, Paul Blomstedt, Samuel Kaski category:stat.CO stat.ME stat.ML published:2016-03-30 summary:Hierarchical models are versatile tools for joint modeling of data setsarising from different, but related, sources. Fully Bayesian inference may,however, become computationally prohibitive if the source-specific data modelsare complex, or if the number of sources is very large. To facilitatecomputation, we propose an approach, where inference is first madeindependently for the parameters of each data set, whereupon the obtainedposterior samples are used as observed data in a substitute hierarchical model,based on a scaled likelihood function. Compared to direct inference in a fullhierarchical model, the approach has the advantage of being able to speed upconvergence by breaking down the initial large inference problem into smallerindividual subproblems with better convergence properties. Moreover it enablesparallel processing of the possibly complex inferences of the source-specificparameters, which may otherwise create a computational bottleneck if processedjointly as part of a hierarchical model. The approach is illustrated with bothsimulated and real data.
arxiv-17100-49 | RGBD Datasets: Past, Present and Future | http://arxiv.org/pdf/1604.00999v2.pdf | author:Michael Firman category:cs.CV cs.RO published:2016-04-04 summary:Since the launch of the Microsoft Kinect, scores of RGBD datasets have beenreleased. These have propelled advances in areas from reconstruction to gesturerecognition. In this paper we explore the field, reviewing datasets acrosseight categories: semantics, object pose estimation, camera tracking, scenereconstruction, object tracking, human actions, faces and identification. Byextracting relevant information in each category we help researchers to findappropriate data for their needs, and we consider which datasets have succeededin driving computer vision forward and why. Finally, we examine the future of RGBD datasets. We identify key areas whichare currently underexplored, and suggest that future directions may includesynthetic data and dense reconstructions of static and dynamic scenes.
arxiv-17100-50 | Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks | http://arxiv.org/pdf/1604.03650v1.pdf | author:Junyuan Xie, Ross Girshick, Ali Farhadi category:cs.CV published:2016-04-13 summary:As 3D movie viewing becomes mainstream and Virtual Reality (VR) marketemerges, the demand for 3D contents is growing rapidly. Producing 3D videos,however, remains challenging. In this paper we propose to use deep neuralnetworks for automatically converting 2D videos and images to stereoscopic 3Dformat. In contrast to previous automatic 2D-to-3D conversion algorithms, whichhave separate stages and need ground truth depth map as supervision, ourapproach is trained end-to-end directly on stereo pairs extracted from 3Dmovies. This novel training scheme makes it possible to exploit orders ofmagnitude more data and significantly increases performance. Indeed, Deep3Doutperforms baselines in both quantitative and human subject evaluations.
arxiv-17100-51 | High Performance Binarized Neural Networks trained on the ImageNet Classification Task | http://arxiv.org/pdf/1604.03058v2.pdf | author:Xundong Wu category:cs.CV cs.LG cs.NE published:2016-04-11 summary:We trained Binarized Neural Networks (BNNs) on the high resolution ImageNetLSVRC-2102 dataset classification task and achieved a good performance. With amoderate size network of 10 layers, we obtained top-5 classification accuracyrate of 81 percent on validation set which is much better than previouspublished results. We expect training networks of a much better performancethrough increase network depth would be straight forward by following ourcurrent strategies. A detailed discussion on strategies used in the networktraining is included as well as preliminary analysis.
arxiv-17100-52 | Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex | http://arxiv.org/pdf/1604.03640v1.pdf | author:Qianli Liao, Tomaso Poggio category:cs.LG cs.NE published:2016-04-13 summary:We discuss relations between Residual Networks (ResNet), Recurrent NeuralNetworks (RNNs) and the primate visual cortex. We begin with the observationthat a shallow RNN is exactly equivalent to a very deep ResNet with weightsharing among the layers. A direct implementation of such a RNN, althoughhaving orders of magnitude fewer parameters, leads to a performance similar tothe corresponding ResNet. We propose 1) a generalization of both RNN and ResNetarchitectures and 2) the conjecture that a class of moderately deep RNNs is abiologically-plausible model of the ventral stream in visual cortex. Wedemonstrate the effectiveness of the architectures by testing them on theCIFAR-10 dataset.
arxiv-17100-53 | ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks | http://arxiv.org/pdf/1511.03776v3.pdf | author:Chen Sun, Manohar Paluri, Ronan Collobert, Ram Nevatia, Lubomir Bourdev category:cs.CV published:2015-11-12 summary:This paper aims to classify and locate objects accurately and efficiently,without using bounding box annotations. It is challenging as objects in thewild could appear at arbitrary locations and in different scales. In thispaper, we propose a novel classification architecture ProNet based onconvolutional neural networks. It uses computationally efficient neuralnetworks to propose image regions that are likely to contain objects, andapplies more powerful but slower networks on the proposed regions. The basicbuilding block is a multi-scale fully-convolutional network which assignsobject confidence scores to boxes at different locations and scales. We showthat such networks can be trained effectively using image-level annotations,and can be connected into cascades or trees for efficient objectclassification. ProNet outperforms previous state-of-the-art significantly onPASCAL VOC 2012 and MS COCO datasets for object classification and point-basedlocalization.
arxiv-17100-54 | Online Multi-target Tracking using Recurrent Neural Networks | http://arxiv.org/pdf/1604.03635v1.pdf | author:Anton Milan, Seyed Hamid Rezatofighi, Anthony Dick, Konrad Schindler, Ian Reid category:cs.CV published:2016-04-13 summary:We present a novel approach to online multi-target tracking based onrecurrent neural networks (RNNs). Tracking multiple objects in real-worldscenes involves many challenges, including a) an a-priori unknown andtime-varying number of targets, b) a continuous state estimation of all presenttargets, and c) a discrete combinatorial problem of data association. Mostprevious methods involve complex models that require tedious tuning ofparameters. Here, we propose for the first time, a full end-to-end learningapproach for online multi-target tracking based on deep learning. Existing deeplearning methods are not designed for the above challenges and cannot betrivially applied to the task. Our solution addresses all of the above pointsin a principled way. Experiments on both synthetic and real data showcompetitive results obtained at 300 Hz on a standard CPU, and pave the waytowards future research in this direction.
arxiv-17100-55 | Quantifying mesoscale neuroanatomy using X-ray microtomography | http://arxiv.org/pdf/1604.03629v1.pdf | author:Eva L. Dyer, William Gray Roncal, Hugo L. Fernandes, Doga Gürsoy, Xianghui Xiao, Joshua T. Vogelstein, Chris Jacobsen, Konrad P. Körding, Narayanan Kasthuri category:q-bio.QM cs.CV published:2016-04-13 summary:Common methods for imaging the 3D microstructure of the brain often requireslicing the brain, imaging these slices, and stitching the images backtogether. In contrast, X-rays allow access into centimeter-thick sampleswithout sectioning, providing an unique and largely untapped approach forproducing large 3D mesoscale brain maps. Here we demonstrate the use ofsynchrotron X-ray microtomography ($\mu$CT) for brain science and provide amuch needed toolkit for analyzing the large datasets afforded by this approach.We introduce methods for sample preparation, imaging, and analyzing theresultant neural structures. This pipeline provides methods for automated celldetection and segmentation of the vasculature, in addition to large-scaleanalyses of the spatial statistics of cells and blood vessels. We applied ourmethods to produce dense micron-scale maps of the cells and blood vessels in amillimeter-scale volume of mouse brain with $\mu$CT. Our results demonstratethat X-ray tomography promises rapid reconstructions over brain large volumes,in a way that is complementary to other brain mapping and connectomics efforts.
arxiv-17100-56 | Joint Unsupervised Learning of Deep Representations and Image Clusters | http://arxiv.org/pdf/1604.03628v1.pdf | author:Jianwei Yang, Devi Parikh, Dhruv Batra category:cs.CV cs.LG published:2016-04-13 summary:In this paper, we propose a recurrent framework for joint unsupervisedlearning of deep representations and image clusters. In our framework,successive operations in a clustering algorithm are expressed as steps in arecurrent process, stacked on top of representations output by a ConvolutionalNeural Network (CNN). During training, image clusters and representations areupdated jointly: image clustering is conducted in the forward pass, whilerepresentation learning in the backward pass. Our key idea behind thisframework is that good representations are beneficial to image clustering andclustering results provide supervisory signals to representation learning. Byintegrating two processes into a single model with a unified weighted tripletloss and optimizing it end-to-end, we can obtain not only more powerfulrepresentations, but also more precise image clusters. Extensive experimentsshow that our method outperforms the state-of-the-art on image clusteringacross a variety of image datasets. Moreover, the learned representationsgeneralize well when transferred to other tasks.
arxiv-17100-57 | Dissecting a Social Botnet: Growth, Content and Influence in Twitter | http://arxiv.org/pdf/1604.03627v1.pdf | author:Norah Abokhodair, Daisy Yoo, David W. McDonald category:cs.CY cs.CL cs.SI published:2016-04-13 summary:Social botnets have become an important phenomenon on social media. There aremany ways in which social bots can disrupt or influence online discourse, suchas, spam hashtags, scam twitter users, and astroturfing. In this paper weconsidered one specific social botnet in Twitter to understand how it growsover time, how the content of tweets by the social botnet differ from regularusers in the same dataset, and lastly, how the social botnet may haveinfluenced the relevant discussions. Our analysis is based on a qualitativecoding for approximately 3000 tweets in Arabic and English from the Syriansocial bot that was active for 35 weeks on Twitter before it was shutdown. Wefind that the growth, behavior and content of this particular botnet did notspecifically align with common conceptions of botnets. Further we identifyinteresting aspects of the botnet that distinguish it from regular users.
arxiv-17100-58 | Energy-Efficient Object Detection using Semantic Decomposition | http://arxiv.org/pdf/1509.08970v2.pdf | author:Priyadarshini Panda, Swagath Venkataramani, Abhronil Sengupta, Anand Raghunathan, Kaushik Roy category:cs.CV published:2015-09-29 summary:Machine-learning algorithms offer immense possibilities in the development ofseveral cognitive applications. In fact, large scale machine-learningclassifiers now represent the state-of-the-art in a wide range of objectdetection/classification problems. However, the network complexities oflarge-scale classifiers present them as one of the most challenging and energyintensive workloads across the computing spectrum. In this paper, we present anew approach to optimize energy efficiency of object detection tasks usingsemantic decomposition to build a hierarchical classification framework. Weobserve that certain semantic information like color/texture are common acrossvarious images in real-world datasets for object detection applications. Weexploit these common semantic features to distinguish the objects of interestfrom the remaining inputs (non-objects of interest) in a dataset at a lowercomputational effort. We propose a 2-stage hierarchical classificationframework, with increasing levels of complexity, wherein the first stage istrained to recognize the broad representative semantic features relevant to theobject of interest. The first stage rejects the input instances that do nothave the representative features and passes only the relevant instances to thesecond stage. Our methodology thus allows us to reject certain information atlower complexity and utilize the full computational effort of a network only ona smaller fraction of inputs to perform detection. We use color and texture asdistinctive traits to carry out several experiments for object detection. Ourexperiments on the Caltech101/CIFAR10 dataset show that the proposed methodyields 1.93x/1.46x improvement in average energy, respectively, over thetraditional single classifier model.
arxiv-17100-59 | Object Boundary Guided Semantic Segmentation | http://arxiv.org/pdf/1603.09742v3.pdf | author:Qin Huang, Chunyang Xia, Wenchao Zheng, Yuhang Song, Hao Xu, C. -C. Jay Kuo category:cs.CV published:2016-03-31 summary:Semantic segmentation is crucial for understanding image contents and objectlocalizations. Recent development in fully-convolutional neural network (FCN)has enabled accurate pixel-level labeling with finer results. Althoughdifferent constraints have been applied to help delineate segmentation details,previous methods majorly focus on differentiating the surface patterns ofdifferent object classes and merge those with similar properties, whichconsequently deprive the FCN based segmentation of its ability to understandand recognize the object. To tackle with this major shortage, we introduce adouble branch network, which not only learns about the object class for eachregion, but also acquires the knowledge to set up object boundaries so that amore accurate segmentation of object class and finer details could be achieved. To this end, we relabel the object contours based on ground truth of objectlabeling and use the FCN network to specially learn a three-class branch, whichis later used as a mask layer combing with original 21-class FCN. This network,called object boundary guided FCN (OBG-FCN) is then going through an end-to-endtraining, which refines the details of segmentation boundaries and classaccuracies. We apply the proposed method in the PASCAL VOC segmentation benchmark, andhave achieved state-of-the-art performance. Our pre-trained edge model hasshown to be stable and accurate even at accuracy level of FCN-2s.
arxiv-17100-60 | What do different evaluation metrics tell us about saliency models? | http://arxiv.org/pdf/1604.03605v1.pdf | author:Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, Frédo Durand category:cs.CV published:2016-04-12 summary:How best to evaluate a saliency model's ability to predict where humans lookin images is an open research question. The choice of evaluation metric dependson how saliency is defined and how the ground truth is represented. Metricsdiffer in how they rank saliency models, and this results from how falsepositives and false negatives are treated, whether viewing biases are accountedfor, whether spatial deviations are factored in, and how the saliency maps arepre-processed. In this paper, we provide an analysis of 8 different evaluationmetrics and their properties. With the help of systematic experiments andvisualizations of metric computations, we add interpretability to saliencyscores and more transparency to the evaluation of saliency models. Building offthe differences in metric properties and behaviors, we make recommendations formetric selections under specific assumptions and for specific applications.
arxiv-17100-61 | Community Detection with Node Attributes and its Generalization | http://arxiv.org/pdf/1604.03601v1.pdf | author:Yuan Li category:cs.SI physics.soc-ph stat.ML published:2016-04-12 summary:Community detection algorithms are fundamental tools to understandorganizational principles in social networks. With the increasing power ofsocial media platforms, when detecting communities there are two possi- blesources of information one can use: the structure of social network and nodeattributes. However structure of social networks and node attributes are ofteninterpreted separately in the research of community detection. When these twosources are interpreted simultaneously, one common as- sumption shared byprevious studies is that nodes attributes are correlated with communities. Inthis paper, we present a model that is capable of combining topologyinformation and nodes attributes information with- out assuming correlation.This new model can recover communities with higher accuracy even when nodeattributes and communities are uncorre- lated. We derive the detectabilitythreshold for this model and use Belief Propagation (BP) to make inference.This algorithm is optimal in the sense that it can recover community all theway down to the threshold. This new model is also with the potential to handleedge content and dynamic settings.
arxiv-17100-62 | Towards Adapting Deep Visuomotor Representations from Simulated to Real Environments | http://arxiv.org/pdf/1511.07111v3.pdf | author:Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao Peng, Sergey Levine, Kate Saenko, Trevor Darrell category:cs.CV published:2015-11-23 summary:We address the problem of adapting robotic perception from simulated toreal-world environments. For many robotic control tasks, real training imageryis expensive to obtain, but a large number of synthetic images is easy togenerate through simulation. We propose a method that adapts visualrepresentations using a small number of paired synthetic and real views of thesame scene. Our model generalizes prior approaches and combines a standardin-domain loss, a cross-domain adaptation loss, and a contrastive lossexplicitly designed to align pairs of images in feature space. We presume asynthetic dataset comprised of views that are a superset of a small number ofreal views, where the alignment may be either explicit or latent. We evaluateour approach on a manipulation task and show that by exploiting the presence ofsynthetic-real image pairs, our model is able to compensate for domain shiftmore effectively than conventional initialization techniques. Our results serveas an initial step toward pretraining deep visuomotor policies entirely insimulation, significantly reducing physical demands when learning complexpolicies.
arxiv-17100-63 | Enhanced Low-Rank Matrix Approximation | http://arxiv.org/pdf/1511.01966v4.pdf | author:Ankit Parekh, Ivan W. Selesnick category:cs.CV cs.LG math.OC published:2015-11-06 summary:This letter proposes to estimate low-rank matrices by formulating a convexoptimization problem with non-convex regularization. We employ parameterizednon-convex penalty functions to estimate the non-zero singular values moreaccurately than the nuclear norm. A closed-form solution for the global optimumof the proposed objective function (sum of data fidelity and the non-convexregularizer) is also derived. The solution reduces to singular valuethresholding method as a special case. The proposed method is demonstrated forimage denoising.
arxiv-17100-64 | Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels | http://arxiv.org/pdf/1512.06974v2.pdf | author:Ishan Misra, C. Lawrence Zitnick, Margaret Mitchell, Ross Girshick category:cs.CV published:2015-12-22 summary:When human annotators are given a choice about what to label in an image,they apply their own subjective judgments on what to ignore and what tomention. We refer to these noisy "human-centric" annotations as exhibitinghuman reporting bias. Examples of such annotations include image tags andkeywords found on photo sharing sites, or in datasets containing imagecaptions. In this paper, we use these noisy annotations for learning visuallycorrect image classifiers. Such annotations do not use consistent vocabulary,and miss a significant amount of the information present in an image; however,we demonstrate that the noise in these annotations exhibits structure and canbe modeled. We propose an algorithm to decouple the human reporting bias fromthe correct visually grounded labels. Our results are highly interpretable forreporting "what's in the image" versus "what's worth saying." We demonstratethe algorithm's efficacy along a variety of metrics and datasets, including MSCOCO and Yahoo Flickr 100M. We show significant improvements over traditionalalgorithms for both image classification and image captioning, doubling theperformance of existing methods in some cases.
arxiv-17100-65 | Training Region-based Object Detectors with Online Hard Example Mining | http://arxiv.org/pdf/1604.03540v1.pdf | author:Abhinav Shrivastava, Abhinav Gupta, Ross Girshick category:cs.CV cs.LG published:2016-04-12 summary:The field of object detection has made significant advances riding on thewave of region-based ConvNets, but their training procedure still includes manyheuristics and hyperparameters that are costly to tune. We present a simple yetsurprisingly effective online hard example mining (OHEM) algorithm for trainingregion-based ConvNet detectors. Our motivation is the same as it has alwaysbeen -- detection datasets contain an overwhelming number of easy examples anda small number of hard examples. Automatic selection of these hard examples canmake training more effective and efficient. OHEM is a simple and intuitivealgorithm that eliminates several heuristics and hyperparameters in common use.But more importantly, it yields consistent and significant boosts in detectionperformance on benchmarks like PASCAL VOC 2007 and 2012. Its effectivenessincreases as datasets become larger and more difficult, as demonstrated by theresults on the MS COCO dataset. Moreover, combined with complementary advancesin the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP onPASCAL VOC 2007 and 2012 respectively.
arxiv-17100-66 | Cross-stitch Networks for Multi-task Learning | http://arxiv.org/pdf/1604.03539v1.pdf | author:Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert category:cs.CV cs.LG published:2016-04-12 summary:Multi-task learning in Convolutional Networks has displayed remarkablesuccess in the field of recognition. This success can be largely attributed tolearning shared representations from multiple supervisory tasks. However,existing multi-task approaches rely on enumerating multiple networkarchitectures specific to the tasks at hand, that do not generalize. In thispaper, we propose a principled approach to learn shared representations inConvNets using multi-task learning. Specifically, we propose a new sharingunit: "cross-stitch" unit. These units combine the activations from multiplenetworks and can be trained end-to-end. A network with cross-stitch units canlearn an optimal combination of shared and task-specific representations. Ourproposed method generalizes across multiple tasks and shows dramaticallyimproved performance over baseline methods for categories with few trainingexamples.
arxiv-17100-67 | Recurrent Batch Normalization | http://arxiv.org/pdf/1603.09025v4.pdf | author:Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, Aaron Courville category:cs.LG published:2016-03-30 summary:We propose a reparameterization of LSTM that brings the benefits of batchnormalization to recurrent neural networks. Whereas previous works only applybatch normalization to the input-to-hidden transformation of RNNs, wedemonstrate that it is both possible and beneficial to batch-normalize thehidden-to-hidden transition, thereby reducing internal covariate shift betweentime steps. We evaluate our proposal on various sequential problems such assequence classification, language modeling and question answering. Ourempirical results show that our batch-normalized LSTM consistently leads tofaster convergence and improved generalization.
arxiv-17100-68 | Contextual Deep CNN Based Hyperspectral Classification | http://arxiv.org/pdf/1604.03519v1.pdf | author:Hyungtae Lee, Heesung Kwon category:cs.CV cs.LG published:2016-04-12 summary:In this paper, we describe a novel deep convolutional neural networks (CNN)based approach called contextual deep CNN that can jointly exploit spatial andspectral features for hyperspectral image classification. The contextual deepCNN first concurrently applies multiple 3-dimensional local convolutionalfilters with different sizes jointly exploiting spatial and spectral featuresof a hyperspectral image. The initial spatial and spectral feature mapsobtained from applying the variable size convolutional filters are thencombined together to form a joint spatio-spectral feature map. The jointfeature map representing rich spectral and spatial properties of thehyperspectral image is then fed through fully convolutional layers thateventually predict the corresponding label of each pixel vector. The proposedapproach is tested on the Indian Pines data and performance comparison showsenhanced classification performance of the proposed approach over the currentstate of the art.
arxiv-17100-69 | DTM: Deformable Template Matching | http://arxiv.org/pdf/1604.03518v1.pdf | author:Hyungtae Lee, Heesung Kwon, Ryan M. Robinson, William D. Nothwang category:cs.CV published:2016-04-12 summary:A novel template matching algorithm that can incorporate the concept ofdeformable parts, is presented in this paper. Unlike the deformable part model(DPM) employed in object recognition, the proposed template-matching approachcalled Deformable Template Matching (DTM) does not require a training step.Instead, deformation is achieved by a set of predefined basic rules (e.g. theleft sub-patch cannot pass across the right patch). Experimental evaluation ofthis new method using the PASCAL VOC 07 dataset demonstrated substantialperformance improvement over conventional template matching algorithms.Additionally, to confirm the applicability of DTM, the concept is applied tothe generation of a rotation-invariant SIFT descriptor. Experimental evaluationemploying deformable matching of SIFT features shows an increased number ofmatching features compared to a conventional SIFT matching.
arxiv-17100-70 | Fast Object Localization Using a CNN Feature Map Based Multi-Scale Search | http://arxiv.org/pdf/1604.03517v1.pdf | author:Hyungtae Lee, Heesung Kwon, Archith J. Bency, William D. Nothwang category:cs.CV published:2016-04-12 summary:Object localization is an important task in computer vision but requires alarge amount of computational power due mainly to an exhaustive multiscalesearch on the input image. In this paper, we describe a near real-timemultiscale search on a deep CNN feature map that does not use region proposals.The proposed approach effectively exploits local semantic information preservedin the feature map of the outermost convolutional layer. A multi-scale searchis performed on the feature map by processing all the sub-regions of differentsizes using separate expert units of fully connected layers. Each expert unitreceives as input local semantic features only from the correspondingsub-regions of a specific geometric shape. Therefore, it contains more nearlyoptimal parameters tailored to the corresponding shape. This multi-scale andmulti-aspect ratio scanning strategy can effectively localize a potentialobject of an arbitrary size. The proposed approach is fast and able to localizeobjects of interest with a frame rate of 4 fps while providing improveddetection performance over the state-of-the art on the PASCAL VOC 12 and MSCOCOdata sets.
arxiv-17100-71 | Full Flow: Optical Flow Estimation By Global Optimization over Regular Grids | http://arxiv.org/pdf/1604.03513v1.pdf | author:Qifeng Chen, Vladlen Koltun category:cs.CV published:2016-04-12 summary:We present a global optimization approach to optical flow estimation. Theapproach optimizes a classical optical flow objective over the full space ofmappings between discrete grids. No descriptor matching is used. The highlyregular structure of the space of mappings enables optimizations that reducethe computational complexity of the algorithm's inner loop from quadratic tolinear and support efficient matching of tens of thousands of nodes to tens ofthousands of displacements. We show that one-shot global optimization of aclassical Horn-Schunck-type objective over regular grids at a single resolutionis sufficient to initialize continuous interpolation and achievestate-of-the-art performance on challenging modern benchmarks.
arxiv-17100-72 | An Unbiased Data Collection and Content Exploitation/Exploration Strategy for Personalization | http://arxiv.org/pdf/1604.03506v1.pdf | author:Liangjie Hong, Adnan Boz category:cs.IR cs.LG published:2016-04-12 summary:One of missions for personalization systems and recommender systems is toshow content items according to users' personal interests. In order to achievesuch goal, these systems are learning user interests over time and trying topresent content items tailoring to user profiles. Recommending items accordingto users' preferences has been investigated extensively in the past few years,mainly thanks for the popularity of Netflix competition. In a real setting,users may be attracted by a subset of those items and interact with them, onlyleaving partial feedbacks to the system to learn in the next cycle, which leadsto significant biases into systems and hence results in a situation where userengagement metrics cannot be improved over time. The problem is not just forone component of the system. The data collected from users is usually used inmany different tasks, including learning ranking functions, building userprofiles and constructing content classifiers. Once the data is biased, allthese downstream use cases would be impacted as well. Therefore, it would bebeneficial to gather unbiased data through user interactions. Traditionally,unbiased data collection is done through showing items uniformly sampling fromthe content pool. However, this simple scheme is not feasible as it risks userengagement metrics and it takes long time to gather user feedbacks. In thispaper, we introduce a user-friendly unbiased data collection framework, byutilizing methods developed in the exploitation and exploration literature. Wediscuss how the framework is different from normal multi-armed bandit problemsand why such method is needed. We layout a novel Thompson sampling forBernoulli ranked-list to effectively balance user experiences and datacollection. The proposed method is validated from a real bucket test and weshow strong results comparing to old algorithms
arxiv-17100-73 | Counting Everyday Objects in Everyday Scenes | http://arxiv.org/pdf/1604.03505v1.pdf | author:Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath RS, Dhruv Batra, Devi Parikh category:cs.CV published:2016-04-12 summary:We introduce the problem of counting everyday objects in everyday scenes.While previous works have studied specific counting problems such as pedestriancounting in surveillance videos, or biological cell counting, we are interestedin counting common objects in natural scenes. We study this problem in a setupsimilar to traditional scene understanding problems. Given an image, weconsider the task of predicting the counts (or the numerosity) of categories ofinterest. We study some simple approaches and applications for this countingproblem. Our detect approach adapts an object detector to perform counting,while our glance approach regresses to ground truth counts. Our associativesubitizing (aso-sub) approach divides an image into regions and regresses tofractional object counts in each region. We create an ensemble (ens) of thesecounting methods which improves performance. We demonstrate countingperformance on the PASCAL and MS COCO datasets. We show proof-of-conceptapplications of our automatic counting methods to 1) improve object detectionperformance, and 2) visual question answering (on VQA and COCO-QA). Our codeand datasets will be publicly available.
arxiv-17100-74 | GPU-FV: Realtime Fisher Vector and Its Applications in Video Monitoring | http://arxiv.org/pdf/1604.03498v1.pdf | author:Wenying Ma, Liangliang Cao, Lei Yu, Guoping Long, Yucheng Li category:cs.CV published:2016-04-12 summary:Fisher vector has been widely used in many multimedia retrieval and visualrecognition applications with good performance. However, the computationcomplexity prevents its usage in real-time video monitoring. In this work, weproposed and implemented GPU-FV, a fast Fisher vector extraction method withthe help of modern GPUs. The challenge of implementing Fisher vector on GPUslies in the data dependency in feature extraction and expensive memory accessin Fisher vector computing. To handle these challenges, we carefully designedGPU-FV in a way that utilizes the computing power of GPU as much as possible,and applied optimizations such as loop tiling to boost the performance. GPU-FVis about 12 times faster than the CPU version, and 50\% faster than anon-optimized GPU implementation. For standard video input (320*240), GPU-FVcan process each frame within 34ms on a model GPU. Our experiments show thatGPU-FV obtains a similar recognition accuracy as traditional FV on VOC 2007 andCaltech 256 image sets. We also applied GPU-FV for realtime video monitoringtasks and found that GPU-FV outperforms a number of previous works. Especially,when the number of training examples are small, GPU-FV outperforms the recentpopular deep CNN features borrowed from ImageNet. The code can be downloadedfrom the following link https://bitbucket.org/mawenjing/gpu-fv.
arxiv-17100-75 | Structured Matrix Recovery via the Generalized Dantzig Selector | http://arxiv.org/pdf/1604.03492v1.pdf | author:Sheng Chen, Arindam Banerjee category:stat.ML published:2016-04-12 summary:In recent years, structured matrix recovery problems have gained considerableattention for its real world applications, such as recommender systems andcomputer vision. Much of the existing work has focused on matrices withlow-rank structure, and limited progress has been made matrices with othertypes of structure. In this paper we present non-asymptotic analysis forestimation of generally structured matrices via the generalized Dantzigselector under generic sub-Gaussian measurements. We show that the estimationerror can always be succinctly expressed in terms of a few geometric measuresof suitable sets which only depend on the structure of the underlying truematrix. In addition, we derive the general bounds on these geometric measuresfor structures characterized by unitarily invariant norms, which is a largefamily covering most matrix norms of practical interest. Examples are providedto illustrate the utility of our theoretical development.
arxiv-17100-76 | A C++ library for Multimodal Deep Learning | http://arxiv.org/pdf/1512.06927v4.pdf | author:Jian Jin category:cs.LG published:2015-12-22 summary:MDL, Multimodal Deep Learning Library, is a deep learning framework thatsupports multiple models, and this document explains its philosophy andfunctionality. MDL runs on Linux, Mac, and Unix platforms. It depends onOpenCV.
arxiv-17100-77 | Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition | http://arxiv.org/pdf/1511.04196v2.pdf | author:Zhiwei Deng, Arash Vahdat, Hexiang Hu, Greg Mori category:cs.CV published:2015-11-13 summary:Rich semantic relations are important in a variety of visual recognitionproblems. As a concrete example, group activity recognition involves theinteractions and relative spatial relations of a set of people in a scene.State of the art recognition methods center on deep learning approaches fortraining highly effective, complex classifiers for interpreting images.However, bridging the relatively low-level concepts output by these methods tointerpret higher-level compositional scenes remains a challenge. Graphicalmodels are a standard tool for this task. In this paper, we propose a method tointegrate graphical models and deep neural networks into a joint framework.Instead of using a traditional inference method, we use a sequential inferencemodeled by a recurrent neural network. Beyond this, the appropriate structurefor inference can be learned by imposing gates on edges between nodes.Empirical results on group activity recognition demonstrate the potential ofthis model to handle highly structured learning tasks.
arxiv-17100-78 | From Pixels to Sentiment: Fine-tuning CNNs for Visual Sentiment Prediction | http://arxiv.org/pdf/1604.03489v1.pdf | author:Victor Campos, Brendan Jou, Xavier Giro-i-Nieto category:cs.CV cs.MM published:2016-04-12 summary:Visual media have become a crucial part of our social lives. The throughputof generated multimedia content, together with its richness for conveyingsentiments and feelings, highlights the need of automated visual sentimentanalysis tools. We explore how Convolutional Neural Networks (CNNs), acomputational learning paradigm that has shown outstanding performance inseveral vision tasks, can be applied to the task of visual sentiment predictionby fine-tuning a state-of-the-art CNN. We analyze its architecture, studyingseveral performance boosting techniques, which led to a network tuned toachieve a 6.1 % absolute accuracy improvement over the previousstate-of-the-art on a dataset of images from a popular social media platform.Finally, we present visualizations of local patterns that the networkassociates to each image's sentiment.
arxiv-17100-79 | Visualizing and Understanding Deep Texture Representations | http://arxiv.org/pdf/1511.05197v2.pdf | author:Tsung-Yu Lin, Subhransu Maji category:cs.CV published:2015-11-16 summary:A number of recent approaches have used deep convolutional neural networks(CNNs) to build texture representations. Nevertheless, it is still unclear howthese models represent texture and invariances to categorical variations. Thiswork conducts a systematic evaluation of recent CNN-based texture descriptorsfor recognition and attempts to understand the nature of invariances capturedby these representations. First we show that the recently proposed bilinear CNNmodel [25] is an excellent general-purpose texture descriptor and comparesfavorably to other CNN-based descriptors on various texture and scenerecognition benchmarks. The model is translationally invariant and obtainsbetter accuracy on the ImageNet dataset without requiring spatial jittering ofdata compared to corresponding models trained with spatial jittering. Based onrecent work [13, 28] we propose a technique to visualize pre-images, providinga means for understanding categorical properties that are captured by theserepresentations. Finally, we show preliminary results on how a unifiedparametric model of texture analysis and synthesis can be used forattribute-based image manipulation, e.g. to make an image more swirly,honeycombed, or knitted. The source code and additional visualizations areavailable at http://vis-www.cs.umass.edu/texture
arxiv-17100-80 | The Matrix Generalized Inverse Gaussian Distribution: Properties and Applications | http://arxiv.org/pdf/1604.03463v1.pdf | author:Farideh Fazayeli, Arindam Banerjee category:stat.ML published:2016-04-12 summary:While the Matrix Generalized Inverse Gaussian ($\mathcal{MGIG}$) distributionarises naturally in some settings as a distribution over symmetric positivesemi-definite matrices, certain key properties of the distribution andeffective ways of sampling from the distribution have not been carefullystudied. In this paper, we show that the $\mathcal{MGIG}$ is unimodal, and themode can be obtained by solving an Algebraic Riccati Equation (ARE) equation[7]. Based on the property, we propose an importance sampling method for the$\mathcal{MGIG}$ where the mode of the proposal distribution matches that ofthe target. The proposed sampling method is more efficient than existingapproaches [32, 33], which use proposal distributions that may have the modefar from the $\mathcal{MGIG}$'s mode. Further, we illustrate that the theposterior distribution in latent factor models, such as probabilistic matrixfactorization (PMF) [25], when marginalized over one latent factor has the$\mathcal{MGIG}$ distribution. The characterization leads to a novel CollapsedMonte Carlo (CMC) inference algorithm for such latent factor models. Weillustrate that CMC has a lower log loss or perplexity than MCMC, and needsfewer samples.
arxiv-17100-81 | Multi-modal Fusion for Diabetes Mellitus and Impaired Glucose Regulation Detection | http://arxiv.org/pdf/1604.03443v1.pdf | author:Jinxing Li, David Zhang, Yongcheng Li, Jian Wu category:cs.CV published:2016-04-12 summary:Effective and accurate diagnosis of Diabetes Mellitus (DM), as well as itsearly stage Impaired Glucose Regulation (IGR), has attracted much attentionrecently. Traditional Chinese Medicine (TCM) [3], [5] etc. has proved thattongue, face and sublingual diagnosis as a noninvasive method is a reasonableway for disease detection. However, most previous works only focus on a singlemodality (tongue, face or sublingual) for diagnosis, although differentmodalities may provide complementary information for the diagnosis of DM andIGR. In this paper, we propose a novel multi-modal classification method todiscriminate between DM (or IGR) and healthy controls. Specially, the tongue,facial and sublingual images are first collected by using a non-invasivecapture device. The color, texture and geometry features of these three typesof images are then extracted, respectively. Finally, our so-called multi-modalsimilar and specific learning (MMSSL) approach is proposed to combine featuresof tongue, face and sublingual, which not only exploits the correlation butalso extracts individual components among them. Experimental results on adataset consisting of 192 Healthy, 198 DM and 114 IGR samples (all samples wereobtained from Guangdong Provincial Hospital of Traditional Chinese Medicine)substantiate the effectiveness and superiority of our proposed method for thediagnosis of DM and IGR, compared to the case of using a single modality.
arxiv-17100-82 | Video Description using Bidirectional Recurrent Neural Networks | http://arxiv.org/pdf/1604.03390v1.pdf | author:Álvaro Peris, Marc Bolaños, Petia Radeva, Francisco Casacuberta category:cs.CV cs.CL cs.LG published:2016-04-12 summary:Although traditionally used in the machine translation field, theencoder-decoder framework has been recently applied for the generation of videoand image descriptions. The combination of Convolutional and Recurrent NeuralNetworks in these models has proven to outperform the previous state of theart, obtaining more accurate video descriptions. In this work we proposepushing further this model by introducing two contributions into the encodingstage. First, producing richer image representations by combining object andlocation information from Convolutional Neural Networks and second, introducingBidirectional Recurrent Neural Networks for capturing both forward and backwardtemporal relationships in the input frames.
arxiv-17100-83 | A Convex Surrogate Operator for General Non-Modular Loss Functions | http://arxiv.org/pdf/1604.03373v1.pdf | author:Jiaqian Yu, Matthew Blaschko category:stat.ML cs.LG published:2016-04-12 summary:Empirical risk minimization frequently employs convex surrogates tounderlying discrete loss functions in order to achieve computationaltractability during optimization. However, classical convex surrogates can onlytightly bound modular loss functions, sub-modular functions or supermodularfunctions separately while maintaining polynomial time computation. In thiswork, a novel generic convex surrogate for general non-modular loss functionsis introduced, which provides for the first time a tractable solution for lossfunctions that are neither super-modular nor submodular. This convex surro-gateis based on a submodular-supermodular decomposition for which the existence anduniqueness is proven in this paper. It takes the sum of two convex surrogatesthat separately bound the supermodular component and the submodular componentusing slack-rescaling and the Lov{\'a}sz hinge, respectively. It is furtherproven that this surrogate is convex , piecewise linear, an extension of theloss function, and for which subgradient computation is polynomial time.Empirical results are reported on a non-submodular loss based on theS{{\o}}rensen-Dice difference function, and a real-world face track datasetwith tens of thousands of frames, demonstrating the improved performance,efficiency, and scalabil-ity of the novel convex surrogate.
arxiv-17100-84 | Order-Fractal transition in abstract paintings | http://arxiv.org/pdf/1510.06767v3.pdf | author:E. M. De la Calleja, F. Cervantes, J. De la Calleja category:cs.CV published:2015-10-22 summary:We report the degree of order of twenty-two Jackson Pollock's paintings using\emph{Hausdorff-Besicovitch fractal dimension}. Through the maximum value ofeach multi-fractal spectrum, the artworks are classify by the year in whichthey were painted. It has been reported that Pollock's paintings are fractaland it increased on his latest works. However our results show that fractaldimension of the paintings are on a range of fractal dimension with valuesclose to two. We identify this behavior as a fractal-order transition. Based onthe study of disorder-order transition in physical systems, we interpreted thefractal-order transition through its dark paint strokes in Pollocks' paintings,as structured lines following a power law measured by fractal dimension. Weobtain self-similarity in some specific Pollock's paintings, that reveal animportant dependence on the scale of observation. We also characterize by itsfractal spectrum, the called \emph{Teri's Find}. We obtained similar spectrumsbetween \emph{Teri's Find} and \emph{Number 5} from Pollock, suggesting thatfractal dimension cannot be completely rejected as a quantitative parameter toauthenticate this kind of artworks.
arxiv-17100-85 | Improving sentence compression by learning to predict gaze | http://arxiv.org/pdf/1604.03357v1.pdf | author:Sigrid Klerke, Yoav Goldberg, Anders Søgaard category:cs.CL published:2016-04-12 summary:We show how eye-tracking corpora can be used to improve sentence compressionmodels, presenting a novel multi-task learning algorithm based on multi-layerLSTMs. We obtain performance competitive with or better than state-of-the-artapproaches.
arxiv-17100-86 | Orientation-boosted Voxel Nets for 3D Object Recognition | http://arxiv.org/pdf/1604.03351v1.pdf | author:Nima Sedaghat, Mohammadreza Zolfaghari, Thomas Brox category:cs.CV cs.NE published:2016-04-12 summary:Recent work has shown good recognition results in 3D data using 3Dconvolutional networks. In this paper, we argue that the object orientationplays an important role in 3D recognition. To this end, we approach thecategory-level classification task as a multi-task problem, in which thenetwork is forced to predict the pose of the object in addition to the classlabel. We show that this yields significant improvements in the classificationresults. We implemented different network architectures for this purpose andtested them on different datasets representing various 3D data sources: LiDARdata, CAD models and RGBD images. We report state-of-the-art results onclassification, and analyze the effects of orientation-boosting on the dominantsignal paths in the network.
arxiv-17100-87 | Optimal Margin Distribution Machine | http://arxiv.org/pdf/1604.03348v1.pdf | author:Teng Zhang, Zhi-Hua Zhou category:cs.LG published:2016-04-12 summary:Support vector machine (SVM) has been one of the most popular learningalgorithms, with the central idea of maximizing the minimum margin, i.e., thesmallest distance from the instances to the classification boundary. Recenttheoretical results, however, disclosed that maximizing the minimum margin doesnot necessarily lead to better generalization performances, and instead, themargin distribution has been proven to be more crucial. Based on this idea, wepropose a new method, named Optimal margin Distribution Machine (ODM), whichtries to achieve a better generalization performance by optimizing the margindistribution. We characterize the margin distribution by the first- andsecond-order statistics, i.e., the margin mean and variance. The proposedmethod is a general learning approach which can be used in any place where SVMcan be applied, and their superiority is verified both theoretically andempirically in this paper.
arxiv-17100-88 | An incremental linear-time learning algorithm for the Optimum-Path Forest classifier | http://arxiv.org/pdf/1604.03346v1.pdf | author:Mateus Riva, Moacir Ponti category:cs.LG cs.CV published:2016-04-12 summary:We present a classification method with linear-time incremental capabilitiesbased on the Optimum-Path Forest (OPF) classifier. The OPF considers instancesas nodes of a fully-connected training graph, where the edges' weights are thedistances between two nodes' feature vectors. Upon this graph, a minimumspanning tree is built, and every edge connecting instances of differentclasses is removed, with those nodes becoming prototypes or roots of a tree. Anew instance is classified by discovering which tree it would conquer. In thispaper we describe a new training algorithm with incremental capabilities whilekeeping the properties of the OPF. New instances can be inserted in the modelinto one of the existing trees; substitute the prototype of a tree; or split atree. This incremental method was tested for accuracy and running time againstboth full retraining using the original OPF and an adaptation of theDifferential Image Foresting Transform. The method updates the training modelin linear-time, while keeping similar accuracies when compared with theoriginal model, which runs in quadratic-time.
arxiv-17100-89 | Loss Bounds and Time Complexity for Speed Priors | http://arxiv.org/pdf/1604.03343v1.pdf | author:Daniel Filan, Marcus Hutter, Jan Leike category:cs.LG stat.ML published:2016-04-12 summary:This paper establishes for the first time the predictive performance of speedpriors and their computational complexity. A speed prior is essentially aprobability distribution that puts low probability on strings that are notefficiently computable. We propose a variant to the original speed prior(Schmidhuber, 2002), and show that our prior can predict sequences drawn fromprobability measures that are estimable in polynomial time. Our speed prior iscomputable in doubly-exponential time, but not in polynomial time. On apolynomial time computable sequence our speed prior is computable inexponential time. We show better upper complexity bounds for Schmidhuber'sspeed prior under the same conditions, and that it predicts deterministicsequences that are computable in polynomial time; however, we also show that itis not computable in polynomial time, and the question of its predictiveproperties for stochastic sequences remains open.
arxiv-17100-90 | Typicality-Based Stability and Privacy | http://arxiv.org/pdf/1604.03336v1.pdf | author:Raef Bassily, Yoav Freund category:cs.LG cs.DS published:2016-04-12 summary:In this paper, we introduce a new notion of algorithmic stability calledtypical stability. When our goal is to release real-valued queries (statistics)computed over a dataset, this notion does not require the queries to be ofbounded sensitivity -- a condition that is generally assumed under a standardnotion of algorithmic stability known as differential privacy [DMNS06, Dwo06].Instead, typical stability requires the output of the query, when computed on adataset drawn from the underlying distribution, to be "well-concentrated"around its expected value with respect to that distribution. Typical stabilitycan also be motivated as an alternative definition for database privacy (insuch case, we call it typical privacy). Like differential privacy, this notionenjoys several important properties including robustness to post-processing andadaptive composition. We also discuss the guarantees of typical stability on the generalizationerror for a broader class of queries than that of bounded-sensitivity queries.This class contains all queries whose output distributions have a "light" tail,e.g., subgaussian and subexponential queries. In particular, we show that if atypically stable interaction with a dataset yields a query from that class,then this query when evaluated on the same dataset will have smallgeneralization error with high probability (i.e., it will not overfit to thedataset). We discuss the composition guarantees of typical stability and prove acomposition theorem that gives a characterization of the degradation of theparameters of typical stability/privacy under $k$-fold adaptive composition. Wealso give simple noise-addition algorithms that achieve this notion. Thesealgorithms are similar to their differentially private counterparts, however,the added noise is calibrated differently.
arxiv-17100-91 | Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand Pose Estimation | http://arxiv.org/pdf/1604.03334v1.pdf | author:Qi Ye, Shanxin Yuan, Tae-Kyun Kim category:cs.CV published:2016-04-12 summary:Discriminative methods often generate hand poses kinematically implausible,then generative methods are used to correct (or verify) these results in ahybrid method. Estimating 3D hand pose in a hierarchy, where thehigh-dimensional output space is decomposed into smaller ones, has been showneffective. Existing hierarchical methods mainly focus on the decomposition ofthe output space while the input space remains almost the same along thehierarchy. In this paper, a hybrid hand pose estimation method is proposed byapplying the kinematic hierarchy strategy to the input space (as well as theoutput space) of the discriminative method by a spatial attention mechanism andto the optimization of the generative method by hierarchical Particle SwarmOptimization (PSO). The spatial attention mechanism integrates cascaded andhierarchical regression into a CNN framework by transforming both the input(andfeature space) and the output space, which greatly reduces the viewpoint andarticulation variations. Between the levels in the hierarchy, the hierarchicalPSO forces the kinematic constraints to the results of the CNNs. Theexperimental results show that our method significantly outperforms fourstate-of-the-art methods and three baselines on three public benchmarks.
arxiv-17100-92 | Identity Mappings in Deep Residual Networks | http://arxiv.org/pdf/1603.05027v2.pdf | author:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun category:cs.CV cs.LG published:2016-03-16 summary:Deep residual networks have emerged as a family of extremely deeparchitectures showing compelling accuracy and nice convergence behaviors. Inthis paper, we analyze the propagation formulations behind the residualbuilding blocks, which suggest that the forward and backward signals can bedirectly propagated from one block to any other block, when using identitymappings as the skip connections and after-addition activation. A series ofablation experiments support the importance of these identity mappings. Thismotivates us to propose a new residual unit, which further makes training easyand improves generalization. We report improved results using a 1001-layerResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet onImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.
arxiv-17100-93 | Applying Ontological Modeling on Quranic Nature Domain | http://arxiv.org/pdf/1604.03318v1.pdf | author:A. B. M. Shamsuzzaman Sadi, Towfique Anam, Mohamed Abdirazak, Abdillahi Hasan Adnan, Sazid Zaman Khan, Mohamed Mahmudur Rahman, Ghassan Samara category:cs.AI cs.CL published:2016-04-12 summary:The holy Quran is the holy book of the Muslims. It contains information aboutmany domains. Often people search for particular concepts of holy Quran basedon the relations among concepts. An ontological modeling of holy Quran can beuseful in such a scenario. In this paper, we have modeled nature relatedconcepts of holy Quran using OWL (Web Ontology Language) / RDF (ResourceDescription Framework). Our methodology involves identifying nature relatedconcepts mentioned in holy Quran and identifying relations among thoseconcepts. These concepts and relations are represented as classes/instances andproperties of an OWL ontology. Later, in the result section it is shown that,using the Ontological model, SPARQL queries can retrieve verses and concepts ofinterest. Thus, this modeling helps semantic search and query on the holyQuran. In this work, we have used English translation of the holy Quran bySahih International, Protege OWL Editor and for querying we have used SPARQL.
arxiv-17100-94 | Synthesizing Training Images for Boosting Human 3D Pose Estimation | http://arxiv.org/pdf/1604.02703v2.pdf | author:Wenzheng Chen, Huan Wang, Yangyan Li, Hao Su, Changhe Tu, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen category:cs.CV published:2016-04-10 summary:Human 3D pose estimation from a single image is a challenging task withnumerous applications. Convolutional Neural Networks (CNNs) have recentlyachieved superior performance on the task of 2D pose estimation from a singleimage, by training on images with 2D annotations collected by crowd sourcing.This suggests that similar success could be achieved for direct estimation of3D poses. However, 3D poses are much harder to annotate, and the lack ofsuitable annotated training images hinders attempts towards end-to-endsolutions. To address this issue, we opt to automatically synthesize training imageswith ground truth pose annotations. We find that pose space coverage andtexture diversity are the key ingredients for the effectiveness of synthetictraining data. We present a fully automatic, scalable approach that samples thehuman pose space for guiding the synthesis procedure and extracts clothingtextures from real images. We demonstrate that CNNs trained with our syntheticimages out-perform those trained with real photos on 3D pose estimation tasks.
arxiv-17100-95 | Confidence Decision Trees via Online and Active Learning for Streaming (BIG) Data | http://arxiv.org/pdf/1604.03278v1.pdf | author:Rocco De Rosa category:stat.ML cs.LG published:2016-04-12 summary:Decision tree classifiers are a widely used tool in data stream mining. Theuse of confidence intervals to estimate the gain associated with each splitleads to very effective methods, like the popular Hoeffding tree algorithm.From a statistical viewpoint, the analysis of decision tree classifiers in astreaming setting requires knowing when enough new information has beencollected to justify splitting a leaf. Although some of the issues in thestatistical analysis of Hoeffding trees have been already clarified, a generaland rigorous study of confidence intervals for splitting criteria is missing.We fill this gap by deriving accurate confidence intervals to estimate thesplitting gain in decision tree learning with respect to three criteria:entropy, Gini index, and a third index proposed by Kearns and Mansour. Ourconfidence intervals depend in a more detailed way on the tree parameters. Wealso extend our confidence analysis to a selective sampling setting, in whichthe decision tree learner adaptively decides which labels to query in thestream. We furnish theoretical guarantee bounding the probability that theclassification is non-optimal learning the decision tree via our selectivesampling strategy. Experiments on real and synthetic data in a streamingsetting show that our trees are indeed more accurate than trees with the samenumber of leaves generated by other techniques and our active learning modulepermits to save labeling cost. In addition, comparing our labeling strategywith recent methods, we show that our approach is more robust and consistentrespect all the other techniques applied to incremental decision trees.
arxiv-17100-96 | The Right Mutation Strength for Multi-Valued Decision Variables | http://arxiv.org/pdf/1604.03277v1.pdf | author:Benjamin Doerr, Carola Doerr, Timo Kötzing category:cs.NE cs.DS F.2.2 published:2016-04-12 summary:The most common representation in evolutionary computation are bit strings.This is ideal to model binary decision variables, but less useful for variablestaking more values. With very little theoretical work existing on how to useevolutionary algorithms for such optimization problems, we study the run timeof simple evolutionary algorithms on some OneMax-like functions defined over$\Omega = \{0, 1, \dots, r-1\}^n$. More precisely, we regard a variety ofproblem classes requesting the component-wise minimization of the distance toan unknown target vector $z \in \Omega$. For such problems we see a crucialdifference in how we extend the standard-bit mutation operator to thesemulti-valued domains. While it is natural to select each position of thesolution vector to be changed independently with probability $1/n$, there arevarious ways to then change such a position. If we change each selectedposition to a random value different from the original one, we obtain anexpected run time of $\Theta(nr \log n)$. If we change each selected positionby either $+1$ or $-1$ (random choice), the optimization time reduces to$\Theta(nr + n\log n)$. If we use a random mutation strength $i \in\{0,1,\ldots,r-1\}^n$ with probability inversely proportional to $i$ and changethe selected position by either $+i$ or $-i$ (random choice), then theoptimization time becomes $\Theta(n \log(r)(\log(n)+\log(r)))$, bringing downthe dependence on $r$ from linear to polylogarithmic. One of our resultsdepends on a new variant of the lower bounding multiplicative drift theorem.
arxiv-17100-97 | Online Learning of Portfolio Ensembles with Sector Exposure Regularization | http://arxiv.org/pdf/1604.03266v1.pdf | author:Guy Uziel, Ran El-Yaniv category:cs.LG published:2016-04-12 summary:We consider online learning of ensembles of portfolio selection algorithmsand aim to regularize risk by encouraging diversification with respect to apredefined risk-driven grouping of stocks. Our procedure uses online convexoptimization to control capital allocation to underlying investment algorithmswhile encouraging non-sparsity over the given grouping. We prove a logarithmicregret for this procedure with respect to the best-in-hindsight ensemble. Weapplied the procedure with known mean-reversion portfolio selection algorithmsusing the standard GICS industry sector grouping. Empirical Experimentalresults showed an impressive percentage increase of risk-adjusted return(Sharpe ratio).
arxiv-17100-98 | Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer | http://arxiv.org/pdf/1511.03240v2.pdf | author:Jun Xie, Martin Kiefel, Ming-Ting Sun, Andreas Geiger category:cs.CV published:2015-11-10 summary:Semantic annotations are vital for training models for object recognition,semantic segmentation or scene understanding. Unfortunately, pixelwiseannotation of images at very large scale is labor-intensive and only littlelabeled data is available, particularly at instance level and for streetscenes. In this paper, we propose to tackle this problem by lifting thesemantic instance labeling task from 2D into 3D. Given reconstructions fromstereo or laser data, we annotate static 3D scene elements with rough boundingprimitives and develop a model which transfers this information into the imagedomain. We leverage our method to obtain 2D labels for a novel suburban videodataset which we have collected, resulting in 400k semantic and instance imageannotations. A comparison of our method to state-of-the-art label transferbaselines reveals that 3D information enables more efficient annotation whileat the same time resulting in improved accuracy and time-coherent labels.
arxiv-17100-99 | Attributes as Semantic Units between Natural Language and Visual Recognition | http://arxiv.org/pdf/1604.03249v1.pdf | author:Marcus Rohrbach category:cs.CV cs.CL published:2016-04-12 summary:Impressive progress has been made in the fields of computer vision andnatural language processing. However, it remains a challenge to find the bestpoint of interaction for these very different modalities. In this chapter wediscuss how attributes allow us to exchange information between the twomodalities and in this way lead to an interaction on a semantic level.Specifically we discuss how attributes allow using knowledge mined fromlanguage resources for recognizing novel visual categories, how we can generatesentence description about images and video, how we can ground natural languagein visual content, and finally, how we can answer natural language questionsabout images.
arxiv-17100-100 | The Univariate Flagging Algorithm (UFA): a Fully-Automated Approach for Identifying Optimal Thresholds in Data | http://arxiv.org/pdf/1604.03248v1.pdf | author:Mallory Sheth, Roy Welsch, Natasha Markuzon category:cs.LG stat.AP published:2016-04-12 summary:In many data classification problems, there is no linear relationship betweenan explanatory and the dependent variables. Instead, there may be ranges of theinput variable for which the observed outcome is signficantly more or lesslikely. This paper describes an algorithm for automatic detection of suchthresholds, called the Univariate Flagging Algorithm (UFA). The algorithmsearches for a separation that optimizes the difference between separated areaswhile providing the maximum support. We evaluate its performance using threeexamples and demonstrate that thresholds identified by the algorithm align wellwith visual inspection and subject matter expertise. We also introduce twoclassification approaches that use UFA and show that the performance attainedon unseen test data is equal to or better than that of more traditionalclassifiers. We demonstrate that the proposed algorithm is robust againstmissing data and noise, is scalable, and is easy to interpret and visualize. Itis also well suited for problems where incidence of the target is low.
arxiv-17100-101 | Thesis: Multiple Kernel Learning for Object Categorization | http://arxiv.org/pdf/1604.03247v1.pdf | author:Dinesh Govindaraj category:cs.CV cs.LG published:2016-04-12 summary:Object Categorization is a challenging problem, especially when the imageshave clutter background, occlusions or different lighting conditions. In thepast, many descriptors have been proposed which aid object categorization evenin such adverse conditions. Each descriptor has its own merits and de-merits.Some descriptors are invariant to transformations while the others are morediscriminative. Past research has shown that, employing multiple descriptorsrather than any single descriptor leads to better recognition. The problem oflearning the optimal combination of the available descriptors for a particularclassification task is studied. Multiple Kernel Learning (MKL) framework hasbeen developed for learning an optimal combination of descriptors for objectcategorization. Existing MKL formulations often employ block l-1 normregularization which is equivalent to selecting a single kernel from a libraryof kernels. Since essentially a single descriptor is selected, the existingformulations maybe sub- optimal for object categorization. A MKL formulationbased on block l-infinity norm regularization has been developed, which choosesan optimal combination of kernels as opposed to selecting a single kernel. AComposite Multiple Kernel Learning(CKL) formulation based on mixed l-infinityand l-1 norm regularization has been developed. These formulations end inSecond Order Cone Programs(SOCP). Other efficient alter- native algorithms forthese formulation have been implemented. Empirical results on benchmarkdatasets show significant improvement using these new MKL formulations.
arxiv-17100-102 | CRAFT Objects from Images | http://arxiv.org/pdf/1604.03239v1.pdf | author:Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li category:cs.CV published:2016-04-12 summary:Object detection is a fundamental problem in image understanding. One popularsolution is the R-CNN framework and its fast versions. They decompose theobject detection problem into two cascaded easier tasks: 1) generating objectproposals from images, 2) classifying proposals into various object categories.Despite that we are handling with two relatively easier tasks, they are notsolved perfectly and there's still room for improvement. In this paper, we pushthe "divide and conquer" solution even further by dividing each task into twosub-tasks. We call the proposed method "CRAFT" (Cascade Region-proposal-networkAnd FasT-rcnn), which tackles each task with a carefully designed networkcascade. We show that the cascade structure helps in both tasks: in proposalgeneration, it provides more compact and better localized object proposals; inobject classification, it reduces false positives (mainly between ambiguouscategories) by capturing both inter- and intra-category variances. CRAFTachieves consistent and considerable improvement over the state-of-the-art onobject detection benchmarks like PASCAL VOC 07/12 and ILSVRC.
arxiv-17100-103 | Convolutional Pose Machines | http://arxiv.org/pdf/1602.00134v4.pdf | author:Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh category:cs.CV published:2016-01-30 summary:Pose Machines provide a sequential prediction framework for learning richimplicit spatial models. In this work we show a systematic design for howconvolutional networks can be incorporated into the pose machine framework forlearning image features and image-dependent spatial models for the task of poseestimation. The contribution of this paper is to implicitly model long-rangedependencies between variables in structured prediction tasks such asarticulated pose estimation. We achieve this by designing a sequentialarchitecture composed of convolutional networks that directly operate on beliefmaps from previous stages, producing increasingly refined estimates for partlocations, without the need for explicit graphical model-style inference. Ourapproach addresses the characteristic difficulty of vanishing gradients duringtraining by providing a natural learning objective function that enforcesintermediate supervision, thereby replenishing back-propagated gradients andconditioning the learning procedure. We demonstrate state-of-the-artperformance and outperform competing methods on standard benchmarks includingthe MPII, LSP, and FLIC datasets.
arxiv-17100-104 | Recurrent Attentional Networks for Saliency Detection | http://arxiv.org/pdf/1604.03227v1.pdf | author:Jason Kuen, Zhenhua Wang, Gang Wang category:cs.CV cs.LG stat.ML published:2016-04-12 summary:Convolutional-deconvolution networks can be adopted to perform end-to-endsaliency detection. But, they do not work well with objects of multiple scales.To overcome such a limitation, in this work, we propose a recurrent attentionalconvolutional-deconvolution network (RACDNN). Using spatial transformer andrecurrent network units, RACDNN is able to iteratively attend to selected imagesub-regions to perform saliency refinement progressively. Besides tackling thescale problem, RACDNN can also learn context-aware features from pastiterations to enhance saliency refinement in future iterations. Experiments onseveral challenging saliency detection datasets validate the effectiveness ofRACDNN, and show that RACDNN outperforms state-of-the-art saliency detectionmethods.
arxiv-17100-105 | Geometric Feature-Based Facial Expression Recognition in Image Sequences Using Multi-Class AdaBoost and Support Vector Machines | http://arxiv.org/pdf/1604.03225v1.pdf | author:Deepak Ghimire, Joonwhoan Lee category:cs.CV 68T01 I.4; I.5 published:2016-04-12 summary:Facial expressions are widely used in the behavioral interpretation ofemotions, cognitive science, and social interactions. In this paper, we presenta novel method for fully automatic facial expression recognition in facialimage sequences. As the facial expression evolves over time facial landmarksare automatically tracked in consecutive video frames, using displacementsbased on elastic bunch graph matching displacement estimation. Feature vectorsfrom individual landmarks, as well as pairs of landmarks tracking results areextracted, and normalized, with respect to the first frame in the sequence. Theprototypical expression sequence for each class of facial expression is formed,by taking the median of the landmark tracking results from the training facialexpression sequences. Multi-class AdaBoost with dynamic time warping similaritydistance between the feature vector of input facial expression and prototypicalfacial expression, is used as a weak classifier to select the subset ofdiscriminative feature vectors. Finally, two methods for facial expressionrecognition are presented, either by using multi-class AdaBoost with dynamictime warping, or by using support vector machine on the boosted featurevectors. The results on the Cohn-Kanade (CK+) facial expression database show arecognition accuracy of 95.17% and 97.35% using multi-class AdaBoost andsupport vector machines, respectively.
arxiv-17100-106 | Disfluency Detection using a Bidirectional LSTM | http://arxiv.org/pdf/1604.03209v1.pdf | author:Vicky Zayats, Mari Ostendorf, Hannaneh Hajishirzi category:cs.CL published:2016-04-12 summary:We introduce a new approach for disfluency detection using a BidirectionalLong-Short Term Memory neural network (BLSTM). In addition to the wordsequence, the model takes as input pattern match features that were developedto reduce sensitivity to vocabulary size in training, which lead to improvedperformance over the word sequence alone. The BLSTM takes advantage of explicitrepair states in addition to the standard reparandum states. The final outputleverages integer linear programming to incorporate constraints of disfluencystructure. In experiments on the Switchboard corpus, the model achievesstate-of-the-art performance for both the standard disfluency detection taskand the correction detection task. Analysis shows that the model has betterdetection of non-repetition disfluencies, which tend to be much harder todetect.
arxiv-17100-107 | Compact Bilinear Pooling | http://arxiv.org/pdf/1511.06062v2.pdf | author:Yang Gao, Oscar Beijbom, Ning Zhang, Trevor Darrell category:cs.CV published:2015-11-19 summary:Bilinear models has been shown to achieve impressive performance on a widerange of visual tasks, such as semantic segmentation, fine grained recognitionand face recognition. However, bilinear features are high dimensional,typically on the order of hundreds of thousands to a few million, which makesthem impractical for subsequent analysis. We propose two compact bilinearrepresentations with the same discriminative power as the full bilinearrepresentation but with only a few thousand dimensions. Our compactrepresentations allow back-propagation of classification errors enabling anend-to-end optimization of the visual recognition system. The compact bilinearrepresentations are derived through a novel kernelized analysis of bilinearpooling which provide insights into the discriminative power of bilinearpooling, and a platform for further research in compact pooling methods.Experimentation illustrate the utility of the proposed representations forimage classification and few-shot learning across several datasets.
arxiv-17100-108 | Efficient Classification of Multi-Labelled Text Streams by Clashing | http://arxiv.org/pdf/1604.03200v1.pdf | author:Ricardo Ñanculef, Ilias Flaounas, Nello Cristianini category:cs.AI cs.LG published:2016-04-12 summary:We present a method for the classification of multi-labelled text documentsexplicitly designed for data stream applications that require to process avirtually infinite sequence of data using constant memory and constantprocessing time. Our method is composed of an online procedure used toefficiently map text into a low-dimensional feature space and a partition ofthis space into a set of regions for which the system extracts and keepsstatistics used to predict multi-label text annotations. Documents are fed intothe system as a sequence of words, mapped to a region of the partition, andannotated using the statistics computed from the labelled instances collidingin the same region. This approach is referred to as clashing. We illustrate themethod in real-world text data, comparing the results with those obtained usingother text classifiers. In addition, we provide an analysis about the effect ofthe representation space dimensionality on the predictive performance of thesystem. Our results show that the online embedding indeed approximates thegeometry of the full corpus-wise TF and TF-IDF space. The model obtainscompetitive F measures with respect to the most accurate methods, usingsignificantly fewer computational resources. In addition, the method achieves ahigher macro-averaged F measure than methods with similar running time.Furthermore, the system is able to learn faster than the other methods frompartially labelled streams.
arxiv-17100-109 | TGIF: A New Dataset and Benchmark on Animated GIF Description | http://arxiv.org/pdf/1604.02748v2.pdf | author:Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, Jiebo Luo category:cs.CV published:2016-04-10 summary:With the recent popularity of animated GIFs on social media, there is needfor ways to index them with rich metadata. To advance research on animated GIFunderstanding, we collected a new dataset, Tumblr GIF (TGIF), with 100Kanimated GIFs from Tumblr and 120K natural language descriptions obtained viacrowdsourcing. The motivation for this work is to develop a testbed for imagesequence description systems, where the task is to generate natural languagedescriptions for animated GIFs or video clips. To ensure a high qualitydataset, we developed a series of novel quality controls to validate free-formtext input from crowdworkers. We show that there is unambiguous associationbetween visual content and natural language descriptions in our dataset, makingit an ideal benchmark for the visual content captioning task. We performextensive statistical analyses to compare our dataset to existing image andvideo description datasets. Next, we provide baseline results on the animatedGIF description task, using three representative techniques: nearest neighbor,statistical machine translation, and recurrent neural networks. Finally, weshow that models fine-tuned from our animated GIF description dataset can behelpful for automatic movie description.
arxiv-17100-110 | Privacy-Preserving Egocentric Activity Recognition from Extreme Low Resolution | http://arxiv.org/pdf/1604.03196v1.pdf | author:Michael S. Ryoo, Brandon Rothrock, Charles Fleming category:cs.CV published:2016-04-12 summary:Privacy protection from video taken by wearable cameras is an importantsocietal challenge. We desire a wearable vision system that can recognize humanactivities, yet not disclose the identity of the participants. Videoanonymization is typically handled by decimating the image to a very lowresolution. Activity recognition, however, generally requires resolution highenough that features such as faces are identifiable. In this paper, we proposea new approach to address such contradicting objectives: human activityrecognition while only using extreme low-resolution (e.g., 16x12) anonymizedvideos. We introduce the paradigm of inverse super resolution (ISR), theconcept of learning the optimal set of image transformations to generatemultiple low-resolution videos from a single video. Our ISR learns differenttypes of sub-pixel transformations optimized for the activity classification,allowing the classifier to best take advantage of existing high-resolutionvideos (e.g., YouTube videos) by generating multiple LR training videostailored for the problem. We experimentally confirm that the paradigm ofinverse super resolution is able to benefit activity recognition from extremelow-resolution videos (e.g., 16x12 and 32x24), particularly in first-personscenarios.
arxiv-17100-111 | Multi-Type Activity Recognition in Robot-Centric Scenarios | http://arxiv.org/pdf/1507.02558v2.pdf | author:Ilaria Gori, J. K. Aggarwal, Larry Matthies, Michael S. Ryoo category:cs.CV published:2015-07-09 summary:Activity recognition is very useful in scenarios where robots interact with,monitor or assist humans. In the past years many types of activities -- singleactions, two persons interactions or ego-centric activities, to name a few --have been analyzed. Whereas traditional methods treat such types of activitiesseparately, an autonomous robot should be able to detect and recognize multipletypes of activities to effectively fulfill its tasks. We propose a method thatis intrinsically able to detect and recognize activities of different typesthat happen in sequence or concurrently. We present a new unified descriptor,called Relation History Image (RHI), which can be extracted from all theactivity types we are interested in. We then formulate an optimizationprocedure to detect and recognize activities of different types. We apply ourapproach to a new dataset recorded from a robot-centric perspective andsystematically evaluate its quality compared to multiple baselines. Finally, weshow the efficacy of the RHI descriptor on publicly available datasetsperforming extensive comparisons.
arxiv-17100-112 | A Unified Bayesian Framework for Sparse Non-negative Matrix Factorization | http://arxiv.org/pdf/1604.02181v2.pdf | author:Igor Fedorov, Alican Nalci, Ritwik Giri, Bhaskar D. Rao, Truong Q. Nguyen, H. Garudadri category:stat.ML published:2016-04-07 summary:In this work, we study the sparse non-negative matrix factorization (SparseNMF or S-NMF) problem. NMF and S-NMF are popular machine learning tools whichdecompose a given non-negative dataset into a dictionary and an activationmatrix, where both are constrained to be non-negative. We review how commonconcave sparsity measures from the compressed sensing literature can beextended to the S-NMF problem. Furthermore, we show that these sparsitymeasures have a Bayesian interpretation and each one corresponds to a specificprior on the activations. We present a comprehensive Sparse Bayesian Learning(SBL) framework for modeling non-negative data and provide details for Type Iand Type II inference procedures. We show that efficient multiplicative updaterules can be employed to solve the S-NMF problem for the penalty functionsdiscussed and present experimental results validating our assertions.
arxiv-17100-113 | Application of the Second-Order Statistics for Estimation of the Pure Spectra of Individual Components from the Visible Hyperspectral Images of Their Mixture | http://arxiv.org/pdf/1604.03193v1.pdf | author:Sung-Ho Jong, Yong-U Ri, Kye-Ryong Sin category:cs.CV published:2016-04-12 summary:The second-order statistics (SOS) can be applied in estimation of the purespectra of chemical components from the spectrum of their mixture, when SOSseems to be good at estimation of spectral patterns, but their peak directionsare opposite in some cases. In this paper, one method for judgment of the peakdirection of the pure spectra was proposed, where the base line of the purespectra was drawn by using their histograms and the peak directions were chosenso as to make all of the pure spectra located upwards over the base line.Results of the SOS analysis on the visible hyperspectral images of the mixturecomposed of two or three chemical components showed that the present methodoffered the reasonable shape and direction of the pure spectra of itscomponents.
arxiv-17100-114 | Deep Learning for Tactile Understanding From Visual and Haptic Data | http://arxiv.org/pdf/1511.06065v2.pdf | author:Yang Gao, Lisa Anne Hendricks, Katherine J. Kuchenbecker, Trevor Darrell category:cs.RO cs.CV cs.LG published:2015-11-19 summary:Robots which interact with the physical world will benefit from afine-grained tactile understanding of objects and surfaces. Additionally, forcertain tasks, robots may need to know the haptic properties of an objectbefore touching it. To enable better tactile understanding for robots, wepropose a method of classifying surfaces with haptic adjectives (e.g.,compressible or smooth) from both visual and physical interaction data. Humanstypically combine visual predictions and feedback from physical interactions toaccurately predict haptic properties and interact with the world. Inspired bythis cognitive pattern, we propose and explore a purely visual hapticprediction model. Purely visual models enable a robot to "feel" withoutphysical interaction. Furthermore, we demonstrate that using both visual andphysical interaction signals together yields more accurate hapticclassification. Our models take advantage of recent advances in deep neuralnetworks by employing a unified approach to learning features for physicalinteraction and visual observations. Even though we employ little domainspecific knowledge, our model still achieves better results than methods basedon hand-designed features.
arxiv-17100-115 | Learning Simple Auctions | http://arxiv.org/pdf/1604.03171v1.pdf | author:Jamie Morgenstern, Tim Roughgarden category:cs.LG cs.GT published:2016-04-11 summary:We present a general framework for proving polynomial sample complexitybounds for the problem of learning from samples the best auction in a class of"simple" auctions. Our framework captures all of the most prominent examples of"simple" auctions, including anonymous and non-anonymous item and bundlepricings, with either a single or multiple buyers. The technique we propose isto break the analysis of auctions into two natural pieces. First, one showsthat the set of allocation rules have large amounts of structure; second,fixing an allocation on a sample, one shows that the set of auctions agreeingwith this allocation on that sample have revenue functions with lowdimensionality. Our results effectively imply that whenever it's possible tocompute a near-optimal simple auction with a known prior, it is also possibleto compute such an auction with an unknown prior (given a polynomial number ofsamples).
arxiv-17100-116 | Affinity CNN: Learning Pixel-Centric Pairwise Relations for Figure/Ground Embedding | http://arxiv.org/pdf/1512.02767v2.pdf | author:Michael Maire, Takuya Narihira, Stella X. Yu category:cs.CV cs.LG cs.NE published:2015-12-09 summary:Spectral embedding provides a framework for solving perceptual organizationproblems, including image segmentation and figure/ground organization. From anaffinity matrix describing pairwise relationships between pixels, it clusterspixels into regions, and, using a complex-valued extension, orders pixelsaccording to layer. We train a convolutional neural network (CNN) to directlypredict the pairwise relationships that define this affinity matrix. Spectralembedding then resolves these predictions into a globally-consistentsegmentation and figure/ground organization of the scene. Experimentsdemonstrate significant benefit to this direct coupling compared to prior workswhich use explicit intermediate stages, such as edge detection, on the pathwayfrom image to affinities. Our results suggest spectral embedding as a powerfulalternative to the conditional random field (CRF)-based globalization schemestypically coupled to deep neural networks.
arxiv-17100-117 | Shallow Parsing Pipeline for Hindi-English Code-Mixed Social Media Text | http://arxiv.org/pdf/1604.03136v1.pdf | author:Arnav Sharma, Sakshi Gupta, Raveesh Motlani, Piyush Bansal, Manish Srivastava, Radhika Mamidi, Dipti M. Sharma category:cs.CL published:2016-04-11 summary:In this study, the problem of shallow parsing of Hindi-English code-mixedsocial media text (CSMT) has been addressed. We have annotated the data,developed a language identifier, a normalizer, a part-of-speech tagger and ashallow parser. To the best of our knowledge, we are the first to attemptshallow parsing on CSMT. The pipeline developed has been made available to theresearch community with the goal of enabling better text analysis of HindiEnglish CSMT. The pipeline is accessible at http://bit.ly/csmt-parser-api .
arxiv-17100-118 | Conversational flow in Oxford-style debates | http://arxiv.org/pdf/1604.03114v1.pdf | author:Justine Zhang, Ravi Kumar, Sujith Ravi, Cristian Danescu-Niculescu-Mizil category:cs.CL cs.AI cs.SI physics.soc-ph stat.ML published:2016-04-11 summary:Public debates are a common platform for presenting and juxtaposing divergingviews on important issues. In this work we propose a methodology for trackinghow ideas flow between participants throughout a debate. We use this approachin a case study of Oxford-style debates---a competitive format where the winneris determined by audience votes---and show how the outcome of a debate dependson aspects of conversational flow. In particular, we find that winners tend tomake better use of a debate's interactive component than losers, by activelypursuing their opponents' points rather than promoting their own ideas over thecourse of the conversation.
arxiv-17100-119 | Fully-Automatic Synapse Prediction and Validation on a Large Data Set | http://arxiv.org/pdf/1604.03075v1.pdf | author:Gary B. Huang, Louis K. Scheffer, Stephen M. Plaza category:cs.CV published:2016-04-11 summary:Extracting a connectome from an electron microscopy (EM) data set requiresidentification of neurons and determination of synapses between neurons. Asmanual extraction of this information is very time-consuming, there has beenextensive research effort to automatically segment the neurons to help guideand eventually replace manual tracing. Until recently, there has beencomparatively less research on automatically detecting the actual synapsesbetween neurons. This discrepancy can, in part, be attributed to severalfactors: obtaining neuronal shapes is a prerequisite first step in extracting aconnectome, manual tracing is much more time-consuming than annotatingsynapses, and neuronal contact area can be used as a proxy for synapses indetermining connections. However, recent research has demonstrated that contact area alone is not asufficient predictor of synaptic connection. Moreover, as segmentation hasimproved, we have observed that synapse annotation is consuming a moresignificant fraction of overall reconstruction time. This ratio will only getworse as segmentation improves, gating overall possible speed-up. Therefore, weaddress this problem by developing algorithms that automatically detectpre-synaptic neurons and their post-synaptic partners. In particular,pre-synaptic structures are detected using a Deep and Wide Multiscale RecursiveNetwork, and post-synaptic partners are detected using a MLP with featuresconditioned on the local segmentation. This work is novel because it requires minimal amount of training, leveragesadvances in image segmentation directly, and provides a complete solution forpolyadic synapse detection. We further introduce novel metrics to evaluate ouralgorithm on connectomes of meaningful size. These metrics demonstrate thatcomplete automatic prediction can be used to effectively characterize mostconnectivity correctly.
arxiv-17100-120 | Reservoir computing for spatiotemporal signal classification without trained output weights | http://arxiv.org/pdf/1604.03073v1.pdf | author:Ashley Prater category:cs.NE cs.CV cs.LG published:2016-04-11 summary:Reservoir computing is a recently introduced machine learning paradigm thathas been shown to be well-suited for the processing of spatiotemporal data.Rather than training the network node connections and weights viabackpropagation in traditional recurrent neural networks, reservoirs insteadhave fixed connections and weights among the `hidden layer' nodes, andtraditionally only the weights to the output layer of neurons are trained usinglinear regression. We claim that for signal classification tasks, one may forgothe weight training step entirely and instead use a simple supervisedclustering method. The proposed method is analyzed theoretically and exploredthrough numerical experiments on real-world data. The examples demonstrate thatthe proposed clustering method outperforms the traditional trained outputweight approach in terms of speed, accuracy, and sensitivity to reservoirparameters.
arxiv-17100-121 | Structural-RNN: Deep Learning on Spatio-Temporal Graphs | http://arxiv.org/pdf/1511.05298v3.pdf | author:Ashesh Jain, Amir R. Zamir, Silvio Savarese, Ashutosh Saxena category:cs.CV cs.LG cs.NE cs.RO published:2015-11-17 summary:Deep Recurrent Neural Network architectures, though remarkably capable atmodeling sequences, lack an intuitive high-level spatio-temporal structure.That is while many problems in computer vision inherently have an underlyinghigh-level structure and can benefit from it. Spatio-temporal graphs are apopular tool for imposing such high-level intuitions in the formulation of realworld problems. In this paper, we propose an approach for combining the powerof high-level spatio-temporal graphs and sequence learning success of RecurrentNeural Networks~(RNNs). We develop a scalable method for casting an arbitraryspatio-temporal graph as a rich RNN mixture that is feedforward, fullydifferentiable, and jointly trainable. The proposed method is generic andprincipled as it can be used for transforming any spatio-temporal graph throughemploying a certain set of well defined steps. The evaluations of the proposedapproach on a diverse set of problems, ranging from modeling human motion toobject interactions, shows improvement over the state-of-the-art with a largemargin. We expect this method to empower new approaches to problem formulationthrough high-level spatio-temporal graphs and Recurrent Neural Networks.
arxiv-17100-122 | An electronic-game framework for evaluating coevolutionary algorithms | http://arxiv.org/pdf/1604.00644v2.pdf | author:Karine da Silva Miras de Araújo, Fabrício Olivetti de França category:cs.NE cs.AI published:2016-04-03 summary:One of the common artificial intelligence applications in electronic gamesconsists of making an artificial agent learn how to execute some determinedtask successfully in a game environment. One way to perform this task isthrough machine learning algorithms capable of learning the sequence of actionsrequired to win in a given game environment. There are several supervisedlearning techniques able to learn the correct answer for a problem throughexamples. However, when learning how to play electronic games, the correctanswer might only be known by the end of the game, after all the actions werealready taken. Thus, not being possible to measure the accuracy of eachindividual action to be taken at each time step. A way for dealing with thisproblem is through Neuroevolution, a method which trains Artificial NeuralNetworks using evolutionary algorithms. In this article, we introduce aframework for testing optimization algorithms with artificial agent controllersin electronic games, called EvoMan, which is inspired in the action-platformergame Mega Man II. The environment can be configured to run in differentexperiment modes, as single evolution, coevolution and others. To demonstratesome challenges regarding the proposed platform, as initial experiments weapplied Neuroevolution using Genetic Algorithms and the NEAT algorithm, in thecontext of competitively coevolving two distinct agents in this game.
arxiv-17100-123 | No Regret Bound for Extreme Bandits | http://arxiv.org/pdf/1508.02933v3.pdf | author:Robert Nishihara, David Lopez-Paz, Léon Bottou category:stat.ML cs.LG math.OC math.ST stat.TH published:2015-08-12 summary:Algorithms for hyperparameter optimization abound, all of which work wellunder different and often unverifiable assumptions. Motivated by the generalchallenge of sequentially choosing which algorithm to use, we study the morespecific task of choosing among distributions to use for random hyperparameteroptimization. This work is naturally framed in the extreme bandit setting,which deals with sequentially choosing which distribution from a collection tosample in order to minimize (maximize) the single best cost (reward). Whereasthe distributions in the standard bandit setting are primarily characterized bytheir means, a number of subtleties arise when we care about the minimal costas opposed to the average cost. For example, there may not be a well-defined"best" distribution as there is in the standard bandit setting. The bestdistribution depends on the rewards that have been obtained and on theremaining time horizon. Whereas in the standard bandit setting, it is sensibleto compare policies with an oracle which plays the single best arm, in theextreme bandit setting, there are multiple sensible oracle models. We define asensible notion of "extreme regret" in the extreme bandit setting, whichparallels the concept of regret in the standard bandit setting. We then provethat no policy can asymptotically achieve no extreme regret.
arxiv-17100-124 | Learning Global Features for Coreference Resolution | http://arxiv.org/pdf/1604.03035v1.pdf | author:Sam Wiseman, Alexander M. Rush, Stuart M. Shieber category:cs.CL published:2016-04-11 summary:There is compelling evidence that coreference prediction would benefit frommodeling global information about entity-clusters. Yet, state-of-the-artperformance can be achieved with systems treating each mention predictionindependently, which we attribute to the inherent difficulty of craftinginformative cluster-level features. We instead propose to use recurrent neuralnetworks (RNNs) to learn latent, global representations of entity clustersdirectly from their mentions. We show that such representations are especiallyuseful for the prediction of pronominal mentions, and can be incorporated intoan end-to-end coreference system that outperforms the state of the art withoutrequiring any additional search.
arxiv-17100-125 | M3: Scaling Up Machine Learning via Memory Mapping | http://arxiv.org/pdf/1604.03034v1.pdf | author:Dezhi Fang, Duen Horng Chau category:cs.LG cs.DC published:2016-04-11 summary:To process data that do not fit in RAM, conventional wisdom would suggestusing distributed approaches. However, recent research has demonstrated virtualmemory's strong potential in scaling up graph mining algorithms on a singlemachine. We propose to use a similar approach for general machine learning. Wecontribute: (1) our latest finding that memory mapping is also a feasibletechnique for scaling up general machine learning algorithms like logisticregression and k-means, when data fits in or exceeds RAM (we tested datasets upto 190GB); (2) an approach, called M3, that enables existing machine learningalgorithms to work with out-of-core datasets through memory mapping, achievinga speed that is significantly faster than a 4-instance Spark cluster, andcomparable to an 8-instance cluster.
arxiv-17100-126 | In the mood: the dynamics of collective sentiments on Twitter | http://arxiv.org/pdf/1604.03427v1.pdf | author:Nathaniel Charlton, Colin Singleton, Danica Vukadinović Greetham category:cs.SI stat.ML published:2016-04-11 summary:We study the relationship between the sentiment levels of Twitter users andthe evolving network structure that the users created by @-mentioning eachother. We use a large dataset of tweets to which we apply three sentimentscoring algorithms, including the open source SentiStrength program.Specifically we make three contributions. Firstly we find that people who havepotentially the largest communication reach (according to a dynamic centralitymeasure) use sentiment differently than the average user: for example they usepositive sentiment more often and negative sentiment less often. Secondly wefind that when we follow structurally stable Twitter communities over a periodof months, their sentiment levels are also stable, and sudden changes incommunity sentiment from one day to the next can in most cases be traced toexternal events affecting the community. Thirdly, based on our findings, wecreate and calibrate a simple agent-based model that is capable of reproducingmeasures of emotive response comparable to those obtained from our empiricaldataset.
arxiv-17100-127 | Doubly Stochastic Primal-Dual Coordinate Method for Empirical Risk Minimization and Bilinear Saddle-Point Problem | http://arxiv.org/pdf/1508.03390v2.pdf | author:Adams Wei Yu, Qihang Lin, Tianbao Yang category:cs.LG stat.ML published:2015-08-14 summary:We proposed a doubly stochastic primal-dual coordinate (DSPDC) optimizationalgorithm for empirical risk minimization, which can be formulated as abilinear saddle-point problem. In each iteration, our method randomly samples ablock of coordinates of the primal and dual solutions to update. Theconvergence of our method is established in both the distance from the currentiterate to the optimal solution and the primal-dual objective gap. We show thatthe proposed method has a lower overall complexity than existing coordinatemethods when either the data matrix has a factorized structure or the proximalmapping on each block is computationally expensive, e.g., involves aneigenvalue decomposition. Furthermore, we give a theoretical lower bound on theiteration complexity of a general family of primal-dual (block) coordinatemethods for bilinear saddle-point problems, which also includes DSPDC.
arxiv-17100-128 | Semi-supervised learning of local structured output predictors | http://arxiv.org/pdf/1604.03010v1.pdf | author:Xin Du category:cs.LG cs.CV published:2016-04-11 summary:In this paper, we study the problem of semi-supervised structured outputprediction, which aims to learn predictors for structured outputs, such assequences, tree nodes, vectors, etc., from a set of data points of bothinput-output pairs and single inputs without outputs. The traditional methodsto solve this problem usually learns one single predictor for all the datapoints, and ignores the variety of the different data points. Different partsof the data set may have different local distributions, and requires differentoptimal local predictors. To overcome this disadvantage of existing methods, wepropose to learn different local predictors for neighborhoods of different datapoints, and the missing structured outputs simultaneously. In the neighborhoodof each data point, we proposed to learn a linear predictor by minimizing boththe complexity of the predictor and the upper bound of the structuredprediction loss. The minimization is conducted by gradient descent algorithms.Experiments over four benchmark data sets, including DDSM mammography medicalimages, SUN natural image data set, Cora research paper data set, and Spanishnews wire article sentence data set, show the advantages of the proposedmethod.
arxiv-17100-129 | Demystifying Fixed k-Nearest Neighbor Information Estimators | http://arxiv.org/pdf/1604.03006v1.pdf | author:Weihao Gao, Sewoong Oh, Pramod Viswanath category:cs.LG cs.IT math.IT stat.ML published:2016-04-11 summary:Estimating mutual information from i.i.d. samples drawn from an unknown jointdensity function is a basic statistical problem of broad interest withmultitudinous applications. The most popular estimator is one proposed byKraskov and St\"ogbauer and Grassberger (KSG) in 2004, and is nonparametric andbased on the distances of each sample to its $k^{\rm th}$ nearest neighboringsample, where $k$ is a fixed small integer. Despite its widespread use (part ofscientific software packages), theoretical properties of this estimator havebeen largely unexplored. In this paper we demonstrate that the estimator isconsistent and also identify an upper bound on the rate of convergence of thebias as a function of number of samples. We argue that the superior performancebenefits of the KSG estimator stems from a curious "correlation boosting"effect and build on this intuition to modify the KSG estimator in novel ways toconstruct a superior estimator. As a byproduct of our investigations, we obtainnearly tight rates of convergence of the $\ell_2$ error of the well known fixed$k$ nearest neighbor estimator of differential entropy by Kozachenko andLeonenko.
arxiv-17100-130 | Graph Connectivity in Noisy Sparse Subspace Clustering | http://arxiv.org/pdf/1504.01046v2.pdf | author:Yining Wang, Yu-Xiang Wang, Aarti Singh category:stat.ML cs.LG published:2015-04-04 summary:Subspace clustering is the problem of clustering data points into a union oflow-dimensional linear/affine subspaces. It is the mathematical abstraction ofmany important problems in computer vision, image processing and machinelearning. A line of recent work (4, 19, 24, 20) provided strong theoreticalguarantee for sparse subspace clustering (4), the state-of-the-art algorithmfor subspace clustering, on both noiseless and noisy data sets. It was shownthat under mild conditions, with high probability no two points from differentsubspaces are clustered together. Such guarantee, however, is not sufficientfor the clustering to be correct, due to the notorious "graph connectivityproblem" (15). In this paper, we investigate the graph connectivity problem fornoisy sparse subspace clustering and show that a simple post-processingprocedure is capable of delivering consistent clustering under certain "generalposition" or "restricted eigenvalue" assumptions. We also show that ourcondition is almost tight with adversarial noise perturbation by constructing acounter-example. These results provide the first exact clustering guarantee ofnoisy SSC for subspaces of dimension greater then 3.
arxiv-17100-131 | Using Sentence-Level LSTM Language Models for Script Inference | http://arxiv.org/pdf/1604.02993v1.pdf | author:Karl Pichotta, Raymond J. Mooney category:cs.CL published:2016-04-11 summary:There is a small but growing body of research on statistical scripts, modelsof event sequences that allow probabilistic inference of implicit events fromdocuments. These systems operate on structured verb-argument events produced byan NLP pipeline. We compare these systems with recent Recurrent Neural Netmodels that directly operate on raw tokens to predict sentences, finding thelatter to be roughly comparable to the former in terms of predicting missingevents in documents.
arxiv-17100-132 | CP-mtML: Coupled Projection multi-task Metric Learning for Large Scale Face Retrieval | http://arxiv.org/pdf/1604.02975v1.pdf | author:Binod Bhattarai, Gaurav Sharma, Frederic Jurie category:cs.CV published:2016-04-11 summary:We propose a novel Coupled Projection multi-task Metric Learning (CP-mtML)method for large scale face retrieval. In contrast to previous works which werelimited to low dimensional features and small datasets, the proposed methodscales to large datasets with high dimensional face descriptors. It utilisespairwise (dis-)similarity constraints as supervision and hence does not requireexhaustive class annotation for every training image. While, traditionally,multi-task learning methods have been validated on same dataset but differenttasks, we work on the more challenging setting with heterogeneous datasets anddifferent tasks. We show empirical validation on multiple face image datasetsof different facial traits, e.g. identity, age and expression. We use classicLocal Binary Pattern (LBP) descriptors along with the recent Deep ConvolutionalNeural Network (CNN) features. The experiments clearly demonstrate thescalability and improved performance of the proposed method on the tasks ofidentity and age based face image retrieval compared to competitive existingmethods, on the standard datasets and with the presence of a million distractorface images.
arxiv-17100-133 | Learning to Assign Orientations to Feature Points | http://arxiv.org/pdf/1511.04273v2.pdf | author:Kwang Moo Yi, Yannick Verdie, Pascal Fua, Vincent Lepetit category:cs.CV published:2015-11-13 summary:We show how to train a Convolutional Neural Network to assign a canonicalorientation to feature points given an image patch centered on the featurepoint. Our method improves feature point matching upon the state-of-the art andcan be used in conjunction with any existing rotation sensitive descriptors. Toavoid the tedious and almost impossible task of finding a target orientation tolearn, we propose to use Siamese networks which implicitly find the optimalorientations during training. We also propose a new type of activation functionfor Neural Networks that generalizes the popular ReLU, maxout, and PReLUactivation functions. This novel activation performs better for our task. Wevalidate the effectiveness of our method extensively with four existingdatasets, including two non-planar datasets, as well as our own dataset. Weshow that we outperform the state-of-the-art without the need of retraining foreach dataset.
arxiv-17100-134 | Kernel-based Sensor Fusion with Application to Audio-Visual Voice Activity Detection | http://arxiv.org/pdf/1604.02946v1.pdf | author:David Dov, Ronen Talmon, Israel Cohen category:cs.CV published:2016-04-11 summary:In this paper, we address the problem of multiple view data fusion in thepresence of noise and interferences. Recent studies have approached thisproblem using kernel methods, by relying particularly on a product of kernelsconstructed separately for each view. From a graph theory point of view, weanalyze this fusion approach in a discrete setting. More specifically, based ona statistical model for the connectivity between data points, we propose analgorithm for the selection of the kernel bandwidth, a parameter, which, as weshow, has important implications on the robustness of this fusion approach tointerferences. Then, we consider the fusion of audio-visual speech signalsmeasured by a single microphone and by a video camera pointed to the face ofthe speaker. Specifically, we address the task of voice activity detection,i.e., the detection of speech and non-speech segments, in the presence ofstructured interferences such as keyboard taps and office noise. We propose analgorithm for voice activity detection based on the audio-visual signal.Simulation results show that the proposed algorithm outperforms competingfusion and voice activity detection approaches. In addition, we demonstratethat a proper selection of the kernel bandwidth indeed leads to improvedperformance.
arxiv-17100-135 | Communicating with sentences: A multi-word naming game model | http://arxiv.org/pdf/1512.08347v3.pdf | author:Yang Lou, Guanrong Chen, Jianwei Hu category:cs.CL physics.soc-ph published:2015-12-28 summary:Naming game simulates the process of naming a single object by a single word,in which a population of communicating agents can reach global consensusasymptotically through iteratively pair-wise conversations. In this paper, wepropose an extension of the single-word naming game, to a multi-word naminggame (MWNG), which simulates the naming game process when agents name an objectby a sentence (i.e., a series of multiple words) for describing a complexobject such as an opinion or an event. We first define several categories ofwords, and then organize sentences by combining words from different wordcategories. We refer to a formatted combination of several words as a pattern.In such an MWNG, through a pair-wise conversation, it requires the hearer toachieve consensus with the speaker with respect to both every single word inthe sentence as well as the sentence pattern, so as to guarantee the correctmeaning of the saying; otherwise, they fail reaching consensus in theinteraction. We employ three typical topologies used for the underlyingcommunication network, namely random-graph, small-world and scale-freenetworks. We validate the model by using both conventional English languagepatterns and man-made test sentence patterns in performing the MWNG. Oursimulation results show that: 1) the new sentence sharing model is an extensionof the classical lexicon sharing model; 2) the propagating, learning andconverging processes are more complicated than that in the conventional naminggame; 3) the convergence time is non-decreasing as the network becomes betterconnected; 4) the agents are prone to accept short sentence patterns. These newfindings may help deepen our understanding of the human language developmentfrom a network science perspective.
arxiv-17100-136 | Improving and Scaling Trans-dimensional Random Field Language Models | http://arxiv.org/pdf/1603.09170v2.pdf | author:Bin Wang, Zhijian Ou, Yong He, Akinori Kawamura category:cs.CL cs.LG stat.ML published:2016-03-30 summary:The dominant language models (LMs) such as n-gram and neural network (NN)models represent sentence probabilities in terms of conditionals. In contrast,a new trans-dimensional random field (TRF) LM has been recently introduced toshow superior performances, where the whole sentence is modeled as a randomfield. In this paper, we further develop the TDF LMs with two technicalimprovements, which are a new method of exploiting Hessian information inparameter optimization to further enhance the convergence of the trainingalgorithm and an enabling method for training the TRF LMs on large corpus whichmay contain rare very long sentences. Experiments show that the TRF LMs canscale to using training data of up to 32 million words, consistently achieve10% relative perplexity reductions over 5-gram LMs, and perform as good as NNLMs but with much faster speed in calculating sentence probabilities.
arxiv-17100-137 | Optical Flow with Semantic Segmentation and Localized Layers | http://arxiv.org/pdf/1603.03911v2.pdf | author:Laura Sevilla-Lara, Deqing Sun, Varun Jampani, Michael J. Black category:cs.CV published:2016-03-12 summary:Existing optical flow methods make generic, spatially homogeneous,assumptions about the spatial structure of the flow. In reality, optical flowvaries across an image depending on object class. Simply put, different objectsmove differently. Here we exploit recent advances in static semantic scenesegmentation to segment the image into objects of different types. We definedifferent models of image motion in these regions depending on the type ofobject. For example, we model the motion on roads with homographies, vegetationwith spatially smooth flow, and independently moving objects like cars andplanes with affine motion plus deviations. We then pose the flow estimationproblem using a novel formulation of localized layers, which addresseslimitations of traditional layered models for dealing with complex scenemotion. Our semantic flow method achieves the lowest error of any publishedmonocular method in the KITTI-2015 flow benchmark and produces qualitativelybetter flow and segmentation than recent top methods on a wide range of naturalvideos.
arxiv-17100-138 | Statistics of RGBD Images | http://arxiv.org/pdf/1604.02902v1.pdf | author:Dan Rosenbaum, Yair Weiss category:cs.CV published:2016-04-11 summary:Cameras that can measure the depth of each pixel in addition to its colorhave become easily available and are used in many consumer products worldwide.Often the depth channel is captured at lower quality compared to the RGBchannels and different algorithms have been proposed to improve the quality ofthe D channel given the RGB channels. Typically these approaches work byassuming that edges in RGB are correlated with edges in D. In this paper we approach this problem from the standpoint of natural imagestatistics. We obtain examples of high quality RGBD images from a computergraphics generated movie (MPI-Sintel) and we use these examples to comparedifferent probabilistic generative models of RGBD image patches. We then usethe generative models together with a degradation model and obtain a BayesLeast Squares (BLS) estimator of the D channel given the RGB channels. Ourresults show that learned generative models outperform the state-of-the-art inimproving the quality of depth channels given the color channels in naturalimages even when training is performed on artificially generated images.
arxiv-17100-139 | Sparse Coding for Alpha Matting | http://arxiv.org/pdf/1604.02898v1.pdf | author:Jubin Johnson, Ehsan Shahrian Varnousfaderani, Hisham Cholakkal, Deepu Rajan category:cs.CV published:2016-04-11 summary:Existing color sampling based alpha matting methods use the compositingequation to estimate alpha at a pixel from pairs of foreground (F) andbackground (B) samples. The quality of the matte depends on the selected (F,B)pairs. In this paper, the matting problem is reinterpreted as a sparse codingof pixel features, wherein the sum of the codes gives the estimate of the alphamatte from a set of unpaired F and B samples. A non-parametric probabilisticsegmentation provides a certainty measure on the pixel belonging to foregroundor background, based on which a dictionary is formed for use in sparse coding.By removing the restriction to conform to (F,B) pairs, this method allows forbetter alpha estimation from multiple F and B samples. The same framework isextended to videos, where the requirement of temporal coherence is handledeffectively. Here, the dictionary is formed by samples from multiple frames. Amulti-frame graph model, as opposed to a single image as for image matting, isproposed that can be solved efficiently in closed form. Quantitative andqualitative evaluations on a benchmark dataset are provided to show that theproposed method outperforms current state-of-the-art in image and videomatting.
arxiv-17100-140 | A statistical learning strategy for closed-loop control of fluid flows | http://arxiv.org/pdf/1604.03392v1.pdf | author:Florimond Guéniat, Lionel Mathelin, M. Yousuff Hussaini category:stat.ML math.OC published:2016-04-11 summary:This work discusses a closed-loop control strategy for complex systemsutilizing scarce and streaming data. A discrete embedding space is first builtusing hash functions applied to the sensor measurements from which a Markovprocess model is derived, approximating the complex system's dynamics. Acontrol strategy is then learned using reinforcement learning once rewardsrelevant with respect to the control objective are identified. This method isdesigned for experimental configurations, requiring no computations nor priorknowledge of the system, and enjoys intrinsic robustness. It is illustrated ontwo systems: the control of the transitions of a Lorenz 63 dynamical system,and the control of the drag of a cylinder flow. The method is shown to performwell.
arxiv-17100-141 | Semantic 3D Reconstruction with Continuous Regularization and Ray Potentials Using a Visibility Consistency Constraint | http://arxiv.org/pdf/1604.02885v1.pdf | author:Nikolay Savinov, Christian Haene, Lubor Ladicky, Marc Pollefeys category:cs.CV published:2016-04-11 summary:We propose an approach for dense semantic 3D reconstruction which uses a dataterm that is defined as potentials over viewing rays, combined with continuoussurface area penalization. Our formulation is a convex relaxation which weaugment with a crucial non-convex constraint that ensures exact handling ofvisibility. To tackle the non-convex minimization problem, we propose amajorize-minimize type strategy which converges to a critical point. Wedemonstrate the benefits of using the non-convex constraint experimentally. Forthe geometry-only case, we set a new state of the art on two datasets of thecommonly used Middlebury multi-view stereo benchmark. Moreover, ourgeneral-purpose formulation directly reconstructs thin objects, which areusually treated with specialized algorithms. A qualitative evaluation on thedense semantic 3D reconstruction task shows that we improve significantly overprevious methods.
arxiv-17100-142 | Manifold Gaussian Processes for Regression | http://arxiv.org/pdf/1402.5876v4.pdf | author:Roberto Calandra, Jan Peters, Carl Edward Rasmussen, Marc Peter Deisenroth category:stat.ML cs.LG published:2014-02-24 summary:Off-the-shelf Gaussian Process (GP) covariance functions encode smoothnessassumptions on the structure of the function to be modeled. To model complexand non-differentiable functions, these smoothness assumptions are often toorestrictive. One way to alleviate this limitation is to find a differentrepresentation of the data by introducing a feature space. This feature spaceis often learned in an unsupervised way, which might lead to datarepresentations that are not useful for the overall regression task. In thispaper, we propose Manifold Gaussian Processes, a novel supervised method thatjointly learns a transformation of the data into a feature space and a GPregression from the feature space to observed space. The Manifold GP is a fullGP and allows to learn data representations, which are useful for the overallregression task. As a proof-of-concept, we evaluate our approach on complexnon-smooth functions where standard GPs perform poorly, such as step functionsand robotics tasks with contacts.
arxiv-17100-143 | Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks | http://arxiv.org/pdf/1604.02878v1.pdf | author:Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao category:cs.CV published:2016-04-11 summary:Face detection and alignment in unconstrained environment are challenging dueto various poses, illuminations and occlusions. Recent studies show that deeplearning approaches can achieve impressive performance on these two tasks. Inthis paper, we propose a deep cascaded multi-task framework which exploits theinherent correlation between them to boost up their performance. In particular,our framework adopts a cascaded structure with three stages of carefullydesigned deep convolutional networks that predict face and landmark location ina coarse-to-fine manner. In addition, in the learning process, we propose a newonline hard sample mining strategy that can improve the performanceautomatically without manual sample selection. Our method achieves superioraccuracy over the state-of-the-art techniques on the challenging FDDB and WIDERFACE benchmark for face detection, and AFLW benchmark for face alignment, whilekeeps real time performance.
arxiv-17100-144 | Efficient Globally Optimal 2D-to-3D Deformable Shape Matching | http://arxiv.org/pdf/1601.06070v2.pdf | author:Zorah Lähner, Emanuele Rodolà, Frank R. Schmidt, Michael M. Bronstein, Daniel Cremers category:cs.CV published:2016-01-22 summary:We propose the first algorithm for non-rigid 2D-to-3D shape matching, wherethe input is a 2D shape represented as a planar curve and a 3D shaperepresented as a surface; the output is a continuous curve on the surface. Wecast the problem as finding the shortest circular path on the prod- uct3-manifold of the surface and the curve. We prove that the optimal matching canbe computed in polynomial time with a (worst-case) complexity of$O(mn^2\log(n))$, where $m$ and $n$ denote the number of vertices on thetemplate curve and the 3D shape respectively. We also demonstrate that inpractice the runtime is essentially linear in $m\!\cdot\! n$ making it anefficient method for shape analysis and shape retrieval. Quantitativeevaluation confirms that the method provides excellent results for sketch-baseddeformable 3D shape re- trieval.
arxiv-17100-145 | Active Learning for Online Recognition of Human Activities from Streaming Videos | http://arxiv.org/pdf/1604.02855v1.pdf | author:Rocco De Rosa, Ilaria Gori, Fabio Cuzzolin, Barbara Caputo, Nicolò Cesa-Bianchi category:stat.ML cs.CV cs.LG published:2016-04-11 summary:Recognising human activities from streaming videos poses unique challenges tolearning algorithms: predictive models need to be scalable, incrementallytrainable, and must remain bounded in size even when the data stream isarbitrarily long. Furthermore, as parameter tuning is problematic in astreaming setting, suitable approaches should be parameterless, and make noassumptions on what class labels may occur in the stream. We present here anapproach to the recognition of human actions from streaming data which meetsall these requirements by: (1) incrementally learning a model which adaptivelycovers the feature space with simple local classifiers; (2) employing an activelearning strategy to reduce annotation requests; (3) achieving promisingaccuracy within a fixed model size. Extensive experiments on standardbenchmarks show that our approach is competitive with state-of-the-artnon-incremental methods, and outperforms the existing active incrementalbaselines.
arxiv-17100-146 | Probabilistic Integration: A Role for Statisticians in Numerical Analysis? | http://arxiv.org/pdf/1512.00933v4.pdf | author:François-Xavier Briol, Chris. J. Oates, Mark Girolami, Michael A. Osborne, Dino Sejdinovic category:stat.ML cs.NA math.NA math.ST stat.CO stat.TH published:2015-12-03 summary:A research frontier has emerged in scientific computation, founded on theprinciple that numerical error entails epistemic uncertainty that ought to besubjected to statistical analysis. This viewpoint raises several interestingchallenges, including the design of statistical methods that enable thecoherent propagation of probabilities through a (possibly deterministic)computational pipeline. This paper examines thoroughly the case forprobabilistic numerical methods in statistical computation and a specific casestudy is presented for Markov chain and Quasi Monte Carlo methods. Aprobabilistic integrator is equipped with a full distribution over its output,providing a measure of epistemic uncertainty that is shown to be statisticallyvalid at finite computational levels, as well as in asymptotic regimes. Theapproach is motivated by expensive integration problems, where, as in krigging,one is willing to expend, at worst, cubic computational effort in order to gainuncertainty quantification. There, probabilistic integrators enjoy the "best ofboth worlds", leveraging the sampling efficiency of Monte Carlo methods whilstproviding a principled route to assessment of the impact of numerical error onscientific conclusions. Several substantial applications are provided forillustration and critical evaluation, including examples from statisticalmodelling, computer graphics and uncertainty quantification in oil reservoirmodelling.
arxiv-17100-147 | Method of Tibetan Person Knowledge Extraction | http://arxiv.org/pdf/1604.02843v1.pdf | author:Yuan Sun, Zhen Zhu category:cs.CL published:2016-04-11 summary:Person knowledge extraction is the foundation of the Tibetan knowledge graphconstruction, which provides support for Tibetan question answering system,information retrieval, information extraction and other researches, andpromotes national unity and social stability. This paper proposes a SVM andtemplate based approach to Tibetan person knowledge extraction. Throughconstructing the training corpus, we build the templates based the shallowparsing analysis of Tibetan syntactic, semantic features and verbs. Using thetraining corpus, we design a hierarchical SVM classifier to realize the entityknowledge extraction. Finally, experimental results prove the method hasgreater improvement in Tibetan person knowledge extraction.
arxiv-17100-148 | Differential Evolution with Generalized Mutation Operator for Parameters Optimization in Gene Selection for Cancer Classification | http://arxiv.org/pdf/1510.02516v2.pdf | author:H. Sharifi Noghabi, H. Rajabi Mashhadi, K. Shojaei category:cs.NE published:2015-10-08 summary:Differential Evolution (DE) proved to be one of the most successfulevolutionary algorithms for global optimization purposes in continuousproblems. The core operator in DE is mutation which can provide the algorithmwith both exploration and exploitation. In this article, a new notation for DEis proposed which has a formula that can be utilized for generating andextracting novel mutations and by applying this new notation, four novelmutations are proposed. More importantly, by combining these novel trial vectorgeneration strategies and four other well-known ones, we proposed GeneralizedMutation Differential Evolution (GMDE) that takes advantage of two mutationpools that have both explorative and exploitative strategies inside them.Results and experimental analysis are performed on CEC2005 benchmarks and theresults stated that GMDE is surprisingly competitive and significantly improvedthe performance of this algorithm. Finally, GMDE is also applied to parametersoptimization, modification and improvement of a feature selection method forcancer classification purposes over gene expression microarray profiles.
arxiv-17100-149 | Beyond Brightness Constancy: Learning Noise Models for Optical Flow | http://arxiv.org/pdf/1604.02815v1.pdf | author:Dan Rosenbaum, Yair Weiss category:cs.CV published:2016-04-11 summary:Optical flow is typically estimated by minimizing a "data cost" and anoptional regularizer. While there has been much work on different regularizersmany modern algorithms still use a data cost that is not very different fromthe ones used over 30 years ago: a robust version of brightness constancy orgradient constancy. In this paper we leverage the recent availability ofground-truth optical flow databases in order to learn a data cost. Specificallywe take a generative approach in which the data cost models the distribution ofnoise after warping an image according to the flow and we measure the"goodness" of a data cost by how well it matches the true distribution of flowwarp error. Consistent with current practice, we find that robust versions ofgradient constancy are better models than simple brightness constancy but alearned GMM that models the density of patches of warp error gives a muchbetter fit than any existing assumption of constancy. This significantadvantage of the GMM is due to an explicit modeling of the spatial structure ofwarp errors, a feature which is missing from almost all existing data costs inoptical flow. Finally, we show how a good density model of warp error patchescan be used for optical flow estimation on whole images. We replace the datacost by the expected patch log-likelihood (EPLL), and show how this cost can beoptimized iteratively using an additional step of denoising the warp errorimage. The results of our experiments are promising and show that patch modelswith higher likelihood lead to better optical flow estimation.
arxiv-17100-150 | NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis | http://arxiv.org/pdf/1604.02808v1.pdf | author:Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang category:cs.CV published:2016-04-11 summary:Recent approaches in depth-based human activity analysis achieved outstandingperformance and proved the effectiveness of 3D representation forclassification of action classes. Currently available depth-based andRGB+D-based action recognition benchmarks have a number of limitations,including the lack of training samples, distinct class labels, camera views andvariety of subjects. In this paper we introduce a large-scale dataset for RGB+Dhuman action recognition with more than 56 thousand video samples and 4 millionframes, collected from 40 distinct subjects. Our dataset contains 60 differentaction classes including daily, mutual, and health-related actions. Inaddition, we propose a new recurrent neural network structure to model thelong-term temporal correlation of the features for each body part, and utilizethem for better action classification. Experimental results show the advantagesof applying deep learning methods over state-of-the-art hand-crafted featureson the suggested cross-subject and cross-view evaluation criteria for ourdataset. The introduction of this large scale dataset will enable the communityto apply, develop and adapt various data-hungry learning techniques for thetask of depth-based and RGB+D-based human activity analysis.
arxiv-17100-151 | Capturing Dynamic Textured Surfaces of Moving Targets | http://arxiv.org/pdf/1604.02801v1.pdf | author:Ruizhe Wang, Lingyu Wei, Etienne Vouga, Qixing Huang, Duygu Ceylan, Gerard Medioni, Hao Li category:cs.CV published:2016-04-11 summary:We present an end-to-end system for reconstructing complete watertight andtextured models of moving subjects such as clothed humans and animals, usingonly three or four handheld sensors. The heart of our framework is a newpairwise registration algorithm that minimizes, using a particle swarmstrategy, an alignment error metric based on mutual visibility and occlusion.We show that this algorithm reliably registers partial scans with as little as15% overlap without requiring any initial correspondences, and outperformsalternative global registration algorithms. This registration algorithm allowsus to reconstruct moving subjects from free-viewpoint video produced byconsumer-grade sensors, without extensive sensor calibration, constrainedcapture volume, expensive arrays of cameras, or templates of the subjectgeometry.
arxiv-17100-152 | Adaptive Object Detection Using Adjacency and Zoom Prediction | http://arxiv.org/pdf/1512.07711v2.pdf | author:Yongxi Lu, Tara Javidi, Svetlana Lazebnik category:cs.CV published:2015-12-24 summary:State-of-the-art object detection systems rely on an accurate set of regionproposals. Several recent methods use a neural network architecture tohypothesize promising object locations. While these approaches arecomputationally efficient, they rely on fixed image regions as anchors forpredictions. In this paper we propose to use a search strategy that adaptivelydirects computational resources to sub-regions likely to contain objects.Compared to methods based on fixed anchor locations, our approach naturallyadapts to cases where object instances are sparse and small. Our approach iscomparable in terms of accuracy to the state-of-the-art Faster R-CNN approachwhile using two orders of magnitude fewer anchors on average. Code is publiclyavailable.
arxiv-17100-153 | Symbolic Knowledge Extraction using Łukasiewicz Logics | http://arxiv.org/pdf/1604.03099v1.pdf | author:Carlos Leandro category:cs.AI cs.LG published:2016-04-11 summary:This work describes a methodology that combines logic-based systems andconnectionist systems. Our approach uses finite truth-valued {\L}ukasiewiczlogic, wherein every connective can be defined by a neuron in an artificialnetwork. This allowed the injection of first-order formulas into a networkarchitecture, and also simplified symbolic rule extraction. For that we traineda neural networks using the Levenderg-Marquardt algorithm, where we restrictedthe knowledge dissemination in the network structure. This procedure reducesneural network plasticity without drastically damaging the learningperformance, thus making the descriptive power of produced neural networkssimilar to the descriptive power of {\L}ukasiewicz logic language andsimplifying the translation between symbolic and connectionist structures. Weused this method for reverse engineering truth table and in extraction offormulas from real data sets.
arxiv-17100-154 | Validation of Matching | http://arxiv.org/pdf/1411.0023v2.pdf | author:Ya Le, Eric Bax, Nicola Barbieri, David Garcia Soriano, Jitesh Mehta, James Li category:cs.LG stat.ML published:2014-10-31 summary:We introduce a technique to compute probably approximately correct (PAC)bounds on precision and recall for matching algorithms. The bounds require someverified matches, but those matches may be used to develop the algorithms. Thebounds can be applied to network reconciliation or entity resolutionalgorithms, which identify nodes in different networks or values in a data setthat correspond to the same entity. For network reconciliation, the bounds donot require knowledge of the network generation process.
arxiv-17100-155 | Fine-grained Categorization and Dataset Bootstrapping using Deep Metric Learning with Humans in the Loop | http://arxiv.org/pdf/1512.05227v2.pdf | author:Yin Cui, Feng Zhou, Yuanqing Lin, Serge Belongie category:cs.CV published:2015-12-16 summary:Existing fine-grained visual categorization methods often suffer from threechallenges: lack of training data, large number of fine-grained categories, andhigh intraclass vs. low inter-class variance. In this work we propose a genericiterative framework for fine-grained categorization and dataset bootstrappingthat handles these three challenges. Using deep metric learning with humans inthe loop, we learn a low dimensional feature embedding with anchor points onmanifolds for each category. These anchor points capture intra-class variancesand remain discriminative between classes. In each round, images with highconfidence scores from our model are sent to humans for labeling. By comparingwith exemplar images, labelers mark each candidate image as either a "truepositive" or a "false positive". True positives are added into our currentdataset and false positives are regarded as "hard negatives" for our metriclearning model. Then the model is retrained with an expanded dataset and hardnegatives for the next round. To demonstrate the effectiveness of the proposedframework, we bootstrap a fine-grained flower dataset with 620 categories fromInstagram images. The proposed deep metric learning scheme is evaluated on bothour dataset and the CUB-200-2001 Birds dataset. Experimental evaluations showsignificant performance gain using dataset bootstrapping and demonstratestate-of-the-art results achieved by the proposed deep metric learning methods.
arxiv-17100-156 | Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly Detection | http://arxiv.org/pdf/1602.07807v2.pdf | author:Michael Bloodgood, Benjamin Strauss category:cs.DB cs.CL stat.ML published:2016-02-25 summary:Many important forms of data are stored digitally in XML format. Errors canoccur in the textual content of the data in the fields of the XML. Fixing theseerrors manually is time-consuming and expensive, especially for large amountsof data. There is increasing interest in the research, development, and use ofautomated techniques for assisting with data cleaning. Electronic dictionariesare an important form of data frequently stored in XML format that frequentlyhave errors introduced through a mixture of manual typographical entry errorsand optical character recognition errors. In this paper we describe methods forflagging statistical anomalies as likely errors in electronic dictionariesstored in XML format. We describe six systems based on different sources ofinformation. The systems detect errors using various signals in the dataincluding uncommon characters, text length, character-based language models,word-based language models, tied-field length ratios, and tied-fieldtransliteration models. Four of the systems detect errors based on expectationsautomatically inferred from content within elements of a single field type. Wecall these single-field systems. Two of the systems detect errors based oncorrespondence expectations automatically inferred from content within elementsof multiple related field types. We call these tied-field systems. For eachsystem, we provide an intuitive analysis of the type of error that it issuccessful at detecting. Finally, we describe two larger-scale evaluationsusing crowdsourcing with Amazon's Mechanical Turk platform and using theannotations of a domain expert. The evaluations consistently show that thesystems are useful for improving the efficiency with which errors in XMLelectronic dictionaries can be detected.
arxiv-17100-157 | Natural Language Object Retrieval | http://arxiv.org/pdf/1511.04164v3.pdf | author:Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell category:cs.CV cs.CL published:2015-11-13 summary:In this paper, we address the task of natural language object retrieval, tolocalize a target object within a given image based on a natural language queryof the object. Natural language object retrieval differs from text-based imageretrieval task as it involves spatial information about objects within thescene and global scene context. To address this issue, we propose a novelSpatial Context Recurrent ConvNet (SCRC) model as scoring function on candidateboxes for object retrieval, integrating spatial configurations and globalscene-level contextual information into the network. Our model processes querytext, local image descriptors, spatial configurations and global contextfeatures through a recurrent network, outputs the probability of the query textconditioned on each candidate box as a score for the box, and can transfervisual-linguistic knowledge from image captioning domain to our task.Experimental results demonstrate that our method effectively utilizes bothlocal and global information, outperforming previous baseline methodssignificantly on different datasets and scenarios, and can exploit large scalevision and language datasets for knowledge transfer.
arxiv-17100-158 | Do We Really Need to Collect Millions of Faces for Effective Face Recognition? | http://arxiv.org/pdf/1603.07057v2.pdf | author:Iacopo Masi, Anh Tuan Tran, Jatuporn Toy Leksut, Tal Hassner, Gerard Medioni category:cs.CV published:2016-03-23 summary:Face recognition capabilities have recently made extraordinary leaps. Thoughthis progress is at least partially due to ballooning training set sizes --huge numbers of face images downloaded and labeled for identity -- it is notclear if the formidable task of collecting so many images is truly necessary.We propose a far more accessible means of increasing training data sizes forface recognition systems. Rather than manually harvesting and labeling morefaces, we simply synthesize them. We describe novel methods of enriching anexisting dataset with important facial appearance variations by manipulatingthe faces it contains. We further apply this synthesis approach when matchingquery images represented using a standard convolutional neural network. Theeffect of training and testing with synthesized images is extensively tested onthe LFW and IJB-A (verification and identification) benchmarks and Janus CS2.The performances obtained by our approach match state of the art resultsreported by systems trained on millions of downloaded images.
arxiv-17100-159 | Visual Quality Enhancement in Optoacoustic Tomography using Active Contour Segmentation Priors | http://arxiv.org/pdf/1510.08174v3.pdf | author:Subhamoy Mandal, Xosé Luís Deán-Ben, Daniel Razansky category:physics.med-ph cs.CV physics.optics published:2015-10-28 summary:Segmentation of biomedical images is essential for studying andcharacterizing anatomical structures, detection and evaluation of pathologicaltissues. Segmentation has been further shown to enhance the reconstructionperformance in many tomographic imaging modalities by accounting forheterogeneities of the excitation field and tissue properties in the imagedregion. This is particularly relevant in optoacoustic tomography, wherediscontinuities in the optical and acoustic tissue properties, if not properlyaccounted for, may result in deterioration of the imaging performance.Efficient segmentation of optoacoustic images is often hampered by therelatively low intrinsic contrast of large anatomical structures, which isfurther impaired by the limited angular coverage of some commonly employedtomographic imaging configurations. Herein, we analyze the performance ofactive contour models for boundary segmentation in cross-sectional optoacoustictomography. The segmented mask is employed to construct a two compartment modelfor the acoustic and optical parameters of the imaged tissues, which issubsequently used to improve accuracy of the image reconstruction routines. Theperformance of the suggested segmentation and modeling approach are showcasedin tissue-mimicking phantoms and small animal imaging experiments.
arxiv-17100-160 | Reverse Engineering and Symbolic Knowledge Extraction on Łukasiewicz Fuzzy Logics using Linear Neural Networks | http://arxiv.org/pdf/1604.02774v1.pdf | author:Carlos Leandro category:cs.AI cs.NE 94D04 published:2016-04-11 summary:This work describes a methodology to combine logic-based systems andconnectionist systems. Our approach uses finite truth valued {\L}ukasiewiczlogic, where we take advantage of fact what in this type of logics everyconnective can be define by a neuron in an artificial network having byactivation function the identity truncated to zero and one. This allowed theinjection of first-order formulas in a network architecture, and alsosimplified symbolic rule extraction. Our method trains a neural network using Levenderg-Marquardt algorithm, wherewe restrict the knowledge dissemination in the network structure. We show howthis reduces neural networks plasticity without damage drastically the learningperformance. Making the descriptive power of produced neural networks similarto the descriptive power of {\L}ukasiewicz logic language, simplifying thetranslation between symbolic and connectionist structures. This method is used in the reverse engineering problem of finding the formulaused on generation of a truth table for a multi-valued {\L}ukasiewicz logic.For real data sets the method is particularly useful for attribute selection,on binary classification problems defined using nominal attribute. Afterattribute selection and possible data set completion in the resultingconnectionist model: neurons are directly representable using a disjunctive orconjunctive formulas, in the {\L}ukasiewicz logic, or neurons areinterpretations which can be approximated by symbolic rules. This fact isexemplified, extracting symbolic knowledge from connectionist models generatedfor the data set Mushroom from UCI Machine Learning Repository.
arxiv-17100-161 | Generation and Comprehension of Unambiguous Object Descriptions | http://arxiv.org/pdf/1511.02283v3.pdf | author:Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, Kevin Murphy category:cs.CV cs.CL cs.LG cs.RO published:2015-11-07 summary:We propose a method that can generate an unambiguous description (known as areferring expression) of a specific object or region in an image, and which canalso comprehend or interpret such an expression to infer which object is beingdescribed. We show that our method outperforms previous methods that generatedescriptions of objects without taking into account other potentially ambiguousobjects in the scene. Our model is inspired by recent successes of deeplearning methods for image captioning, but while image captioning is difficultto evaluate, our task allows for easy objective evaluation. We also present anew large-scale dataset for referring expressions, based on MS-COCO. We havereleased the dataset and a toolbox for visualization and evaluation, seehttps://github.com/mjhucla/Google_Refexp_toolbox
arxiv-17100-162 | Semi-supervised Learning with Encoder-Decoder Recurrent Neural Networks: Experiments with Motion Capture Sequences | http://arxiv.org/pdf/1511.06653v2.pdf | author:Félix G. Harvey, Christopher Pal category:cs.CV cs.LG published:2015-11-20 summary:Recent work on sequence to sequence translation using Recurrent NeuralNetworks (RNNs) based on Long Short Term Memory (LSTM) architectures has showngreat potential for learning useful representations of sequential data. Aone-to-many encoder-decoder(s) scheme allows for a single encoder to providerepresentations serving multiple purposes. In our case, we present an LSTMencoder network able to produce representations used by two decoders: one thatreconstructs, and one that classifies if the training sequence has anassociated label. This allows the network to learn representations that areuseful for both discriminative and reconstructive tasks at the same time. Thisparadigm is well suited for semi-supervised learning with sequences and we testour proposed approach on an action recognition task using motion capture(MOCAP) sequences. We find that semi-supervised feature learning can improvestate-of-the-art movement classification accuracy on the HDM05 action dataset.Further, we find that even when using only labeled data and a primarilydiscriminative objective the addition of a reconstructive decoder can serve asa form of regularization that reduces over-fitting and improves test setaccuracy.
arxiv-17100-163 | Performance Trade-Offs in Multi-Processor Approximate Message Passing | http://arxiv.org/pdf/1604.02752v1.pdf | author:Junan Zhu, Ahmad Beirami, Dror Baron category:cs.IT cs.DC cs.LG math.IT published:2016-04-10 summary:We consider large-scale linear inverse problems in Bayesian settings. Ourgeneral approach follows a recent line of work that applies the approximatemessage passing (AMP) framework in multi-processor (MP) computational systemsby storing and processing a subset of rows of the measurement matrix along withcorresponding measurements at each MP node. In each MP-AMP iteration, nodes ofthe MP system and its fusion center exchange lossily compressed messagespertaining to their estimates of the input. There is a trade-off between thephysical costs of the reconstruction process including computation time,communication loads, and the reconstruction quality, and it is impossible tosimultaneously minimize all the costs. We pose this minimization as amulti-objective optimization problem (MOP), and study the properties of thebest trade-offs (Pareto optimality) in this MOP. We prove that the achievableregion of this MOP is convex, and conjecture how the combined cost ofcomputation and communication scales with the desired mean squared error. Theseproperties are verified numerically.
arxiv-17100-164 | Unsupervised Learning of Edges | http://arxiv.org/pdf/1511.04166v2.pdf | author:Yin Li, Manohar Paluri, James M. Rehg, Piotr Dollár category:cs.CV published:2015-11-13 summary:Data-driven approaches for edge detection have proven effective and achievetop results on modern benchmarks. However, all current data-driven edgedetectors require manual supervision for training in the form of hand-labeledregion segments or object boundaries. Specifically, human annotators marksemantically meaningful edges which are subsequently used for training. Is thisform of strong, high-level supervision actually necessary to learn toaccurately detect edges? In this work we present a simple yet effectiveapproach for training edge detectors without human supervision. To this end weutilize motion, and more specifically, the only input to our method is noisysemi-dense matches between frames. We begin with only a rudimentary knowledgeof edges (in the form of image gradients), and alternate between improvingmotion estimation and edge detection in turn. Using a large corpus of videodata, we show that edge detectors trained using our unsupervised schemeapproach the performance of the same methods trained with full supervision(within 3-5%). Finally, we show that when using a deep network for the edgedetector, our approach provides a novel pre-training scheme for objectdetection.
arxiv-17100-165 | Correlated Equilibria for Approximate Variational Inference in MRFs | http://arxiv.org/pdf/1604.02737v1.pdf | author:Luis E. Ortiz, Ze Gong category:cs.AI cs.GT stat.ML published:2016-04-10 summary:Almost all of the work in graphical models for game theory has mirroredprevious work in probabilistic graphical models. Our work considers theopposite direction: Taking advantage of recent advances in equilibriumcomputation for belief inference. In particular, we present formulations ofinference problems in Markov random fields (MRFs) as computation of equilibriain a certain class of game-theoretic graphical models. While some previous workexplores this direction, none of that work concretely establishes the preciseconnection between variational probabilistic inference in MRFs and correlatedequilibria. There is no work that exploits recent theoretical and empiricalresults from the literature on algorithmic and computational game theory on thetractable, polynomial-time computation of exact or approximate correlatedequilibria in graphical games with arbitrary, loopy graph structure. Our workdiscusses how to design new algorithms with equally tractable guarantees forthe computation of approximate variational inference in MRFs. In addition,inspired by a previously stated game-theoretic view of state-of-the-arttree-reweighed (TRW) message-passing techniques for belief inference aszero-sum game, we propose a different, general-sum potential game to designapproximate fictitious-play techniques. We perform synthetic experimentsevaluating our proposed approximation algorithms with standard methods and TRWon several classes of classical Ising models. Our experiments show that ourglobal approach is competitive, particularly shinning in a class of Isingmodels with constant, "highly attractive" edge-weights, in which it is oftenbetter than all other alternatives we evaluated. While our local approach wasnot as effective as our global approach or TRW, almost all of the alternativesare often no better than a simple baseline: estimate the marginal probabilityto be 0.5.
arxiv-17100-166 | Evaluating the Performance of Offensive Linemen in the NFL | http://arxiv.org/pdf/1603.07593v2.pdf | author:Nikhil Byanna, Diego Klabjan category:stat.ML published:2016-03-24 summary:How does one objectively measure the performance of an individual offensivelineman in the NFL? The existing literature proposes various measures that relyon subjective assessments of game film, but has yet to develop an objectivemethodology to evaluate performance. Using a variety of statistics related toan offensive lineman's performance, we develop a framework to objectivelyanalyze the overall performance of an individual offensive lineman anddetermine specific linemen who are overvalued or undervalued relative to theirsalary. We identify eight players across the 2013-2014 and 2014-2015 NFLseasons that are considered to be overvalued or undervalued and corroborate theresults with existing metrics that are based on subjective evaluation. To thebest of our knowledge, the techniques set forth in this work have not beenutilized in previous works to evaluate the performance of NFL players at anyposition, including offensive linemen.
arxiv-17100-167 | Generating images with recurrent adversarial networks | http://arxiv.org/pdf/1602.05110v3.pdf | author:Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, Roland Memisevic category:cs.LG cs.CV published:2016-02-16 summary:Gatys et al. (2015) showed that optimizing pixels to match features in aconvolutional network with respect reference image features is a way to renderimages of high visual quality. We show that unrolling this gradient-basedoptimization yields a recurrent computation that creates images byincrementally adding onto a visual "canvas". We propose a recurrent generativemodel inspired by this view, and show that it can be trained using adversarialtraining to generate very good image samples. We also propose a way toquantitatively compare adversarial networks by having the generators anddiscriminators of these networks compete against each other.
arxiv-17100-168 | Soccer Field Localization from a Single Image | http://arxiv.org/pdf/1604.02715v1.pdf | author:Namdar Homayounfar, Sanja Fidler, Raquel Urtasun category:cs.CV published:2016-04-10 summary:In this work, we propose a novel way of efficiently localizing a soccer fieldfrom a single broadcast image of the game. Related work in this area relies onmanually annotating a few key frames and extending the localization to similarimages, or installing fixed specialized cameras in the stadium from which thelayout of the field can be obtained. In contrast, we formulate this problem asa branch and bound inference in a Markov random field where an energy functionis defined in terms of field cues such as grass, lines and circles. Moreover,our approach is fully automatic and depends only on single images from thebroadcast video of the game. We demonstrate the effectiveness of our method byapplying it to various games and obtain promising results. Finally, we positthat our approach can be applied easily to other sports such as hockey andbasketball.
arxiv-17100-169 | PatchBatch: a Batch Augmented Loss for Optical Flow | http://arxiv.org/pdf/1512.01815v2.pdf | author:David Gadot, Lior Wolf category:cs.CV published:2015-12-06 summary:We propose a new pipeline for optical flow computation, based on DeepLearning techniques. We suggest using a Siamese CNN to independently, and inparallel, compute the descriptors of both images. The learned descriptors arethen compared efficiently using the L2 norm and do not require networkprocessing of patch pairs. The success of the method is based on an innovativeloss function that computes higher moments of the loss distributions for eachtraining batch. Combined with an Approximate Nearest Neighbor patch matchingmethod and a flow interpolation technique, state of the art performance isobtained on the most challenging and competitive optical flow benchmarks.
arxiv-17100-170 | Stability and Structural Properties of Gene Regulation Networks with Coregulation Rules | http://arxiv.org/pdf/1602.08753v2.pdf | author:Jonathan H. Warrell, Musa M. Mhlanga category:stat.ML q-bio.MN q-bio.QM published:2016-02-28 summary:Coregulation of the expression of groups of genes has been extensivelydemonstrated empirically in bacterial and eukaryotic systems. Such coregulationcan arise through the use of shared regulatory motifs, which allow thecoordinated expression of modules (and module groups) of functionally relatedgenes across the genome. Coregulation can also arise through the physicalassociation of multi-gene complexes through chromosomal looping, which are thentranscribed together. We present a general formalism for modeling coregulationrules in the framework of Random Boolean Networks (RBN), and develop specificmodels for transcription factor networks with modular structure (includingmodule groups, and multi-input modules (MIM) with autoregulation) andmulti-gene complexes (including hierarchical differentiation between multi-genecomplex members). We develop a mean-field approach to analyse the stability oflarge networks incorporating coregulation, and show that autoregulated MIM andhierarchical gene-complex models can achieve greater stability than networkswithout coregulation whose rules have matching activation frequency. We providefurther analysis of the stability of small networks of both kinds throughsimulations. We also characterize several general properties of the transientsand attractors in the hierarchical coregulation model, and show usingsimulations that the steady-state distribution factorizes hierarchically as aBayesian network in a Markov Jump Process analogue of the RBN model.
arxiv-17100-171 | Generalized Statistical Tests for mRNA and Protein Subcellular Spatial Patterning against Complete Spatial Randomness | http://arxiv.org/pdf/1602.06429v3.pdf | author:Jonathan H. Warrell, Anca F. Savulescu, Robyn Brackin, Musa M. Mhlanga category:stat.ML q-bio.QM stat.AP published:2016-02-20 summary:We derive generalized estimators for a number of spatial statistics that havebeen used in the analysis of spatially resolved omics data, such as Ripley's K,H and L functions, clustering index, and degree of clustering, which allowthese statistics to be calculated on data modelled by arbitrary random measures(RMs). Our estimators generalize those typically used to calculate thesestatistics on point process data, allowing them to be calculated on RMs whichassign continuous values to spatial regions, for instance to model proteinintensity. The clustering index (H*) compares Ripley's H function calculatedempirically to its distribution under complete spatial randomness (CSR),leading us to consider CSR null hypotheses for RMs which are notpoint-processes when generalizing this statistic. We thus consider restrictedclasses of completely random measures which can be simulated directly (Gammaprocesses and Marked Poisson Processes), as well as the general class of allCSR RMs, for which we derive an exact permutation-based H* estimator. Weestablish several properties of the estimators, including bounds on theaccuracy of our general Ripley K estimator, its relationship to a previousestimator for the cross-correlation measure, and the relationship of ourgeneralized H* estimator to previous statistics. To test the ability of ourapproach to identify spatial patterning, we use Fluorescent In SituHybridization (FISH) and Immunofluorescence (IF) data to probe for mRNA andprotein subcellular localization patterns respectively in polarizing mousefibroblasts on micropattened cells. We observe correlated patterns ofclustering over time for corresponding mRNAs and proteins, suggesting adeterministic effect of mRNA localization on protein localization for severalpairs tested, including one case in which spatial patterning at the mRNA levelhas not been previously demonstrated.
arxiv-17100-172 | DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation | http://arxiv.org/pdf/1604.02677v1.pdf | author:Hao Chen, Xiaojuan Qi, Lequan Yu, Pheng-Ann Heng category:cs.CV published:2016-04-10 summary:The morphology of glands has been used routinely by pathologists to assessthe malignancy degree of adenocarcinomas. Accurate segmentation of glands fromhistology images is a crucial step to obtain reliable morphological statisticsfor quantitative diagnosis. In this paper, we proposed an efficient deepcontour-aware network (DCAN) to solve this challenging problem under a unifiedmulti-task learning framework. In the proposed network, multi-level contextualfeatures from the hierarchical architecture are explored with auxiliarysupervision for accurate gland segmentation. When incorporated with multi-taskregularization during the training, the discriminative capability ofintermediate features can be further improved. Moreover, our network can notonly output accurate probability maps of glands, but also depict clear contourssimultaneously for separating clustered objects, which further boosts the glandsegmentation performance. This unified framework can be efficient when appliedto large-scale histopathological data without resorting to additional steps togenerate contours based on low-level cues for post-separating. Our method wonthe 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams,surpassing all the other methods by a significant margin.
arxiv-17100-173 | Distance for Functional Data Clustering Based on Smoothing Parameter Commutation | http://arxiv.org/pdf/1604.02668v1.pdf | author:ShengLi Tzeng, Christian Hennig, Yu-Fen Li, Chien-Ju Lin category:stat.ME stat.AP stat.ML published:2016-04-10 summary:We propose a novel method to determine the dissimilarity between subjects forfunctional data clustering. Spline smoothing or interpolation is common to dealwith data of such type. Instead of estimating the best-representing curve foreach subject as fixed during clustering, we measure the dissimilarity betweensubjects based on varying curve estimates with commutation of smoothingparameters pair-by-pair (of subjects). The intuitions are that smoothingparameters of smoothing splines reflect inverse signal-to-noise ratios and thatapplying an identical smoothing parameter the smoothed curves for two similarsubjects are expected to be close. The effectiveness of our proposal is shownthrough simulations comparing to other dissimilarity measures. It also hasseveral pragmatic advantages. First, missing values or irregular time pointscan be handled directly, thanks to the nature of smoothing splines. Second,conventional clustering method based on dissimilarity can be employedstraightforward, and the dissimilarity also serves as a useful tool for outlierdetection. Third, the implementation is almost handy since subroutines forsmoothing splines and numerical integration are widely available. Fourth, thecomputational complexity does not increase and is parallel with that incalculating Euclidean distance between curves estimated by smoothing splines.
arxiv-17100-174 | Latent Embeddings for Zero-shot Classification | http://arxiv.org/pdf/1603.08895v2.pdf | author:Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, Bernt Schiele category:cs.CV published:2016-03-29 summary:We present a novel latent embedding model for learning a compatibilityfunction between image and class embeddings, in the context of zero-shotclassification. The proposed method augments the state-of-the-art bilinearcompatibility model by incorporating latent variables. Instead of learning asingle bilinear map, it learns a collection of maps with the selection, ofwhich map to use, being a latent variable for the current image-class pair. Wetrain the model with a ranking based objective function which penalizesincorrect rankings of the true class for a given image. We empiricallydemonstrate that our model improves the state-of-the-art for various classembeddings consistently on three challenging publicly available datasets forthe zero-shot setting. Moreover, our method leads to visually highlyinterpretable results with clear clusters of different fine-grained objectproperties that correspond to different latent variable maps.
arxiv-17100-175 | Direction matters: hand pose estimation from local surface normals | http://arxiv.org/pdf/1604.02657v1.pdf | author:Chengde Wan, Angela Yao, Luc Van Gool category:cs.CV published:2016-04-10 summary:We present a hierarchical regression framework for estimating hand jointpositions from single depth images based on local surface normals. Thehierarchical regression follows the tree structured topology of hand from wristto finger tips. We propose a conditional regression forest, i.e., the FrameConditioned Regression Forest (FCRF) which uses a new normal differencefeature. At each stage of the regression, the frame of reference is establishedfrom either the local surface normal or previously estimated hand joints. Bymaking the regression with respect to the local frame, the pose estimation ismore robust to rigid transformations. We also introduce a new efficientapproximation to estimate surface normals. We verify the effectiveness of ourmethod by conducting experiments on two challenging real-world datasets andshow consistent improvements over previous discriminative pose estimationmethods.
arxiv-17100-176 | Real-Time Facial Segmentation and Performance Capture from RGB Input | http://arxiv.org/pdf/1604.02647v1.pdf | author:Shunsuke Saito, Tianye Li, Hao Li category:cs.CV published:2016-04-10 summary:We introduce the concept of unconstrained real-time 3D facial performancecapture through explicit semantic segmentation in the RGB input. To ensurerobustness, cutting edge supervised learning approaches rely on large trainingdatasets of face images captured in the wild. While impressive tracking qualityhas been demonstrated for faces that are largely visible, any occlusion due tohair, accessories, or hand-to-face gestures would result in significant visualartifacts and loss of tracking accuracy. The modeling of occlusions has beenmostly avoided due to its immense space of appearance variability. To addressthis curse of high dimensionality, we perform tracking in unconstrained imagesassuming non-face regions can be fully masked out. Along with recentbreakthroughs in deep learning, we demonstrate that pixel-level facialsegmentation is possible in real-time by repurposing convolutional neuralnetworks designed originally for general semantic segmentation. We develop anefficient architecture based on a two-stream deconvolution network withcomplementary characteristics, and introduce carefully designed trainingsamples and data augmentation strategies for improved segmentation accuracy androbustness. We adopt a state-of-the-art regression-based facial trackingframework with segmented face images as training, and demonstrate accurate anduninterrupted facial performance capture in the presence of extreme occlusionand even side views. Furthermore, the resulting segmentation can be directlyused to composite partial 3D face models on the input images and enableseamless facial manipulation tasks, such as virtual make-up or facereplacement.
arxiv-17100-177 | Online Nonnegative Matrix Factorization with Outliers | http://arxiv.org/pdf/1604.02634v1.pdf | author:Renbo Zhao, Vincent Y. F. Tan category:stat.ML cs.LG math.OC stat.ME published:2016-04-10 summary:We propose a unified and systematic framework for performing onlinenonnegative matrix factorization in the presence of outliers that isparticularly suited to large datasets. Within this framework, we propose twosolvers based on proximal gradient descent and alternating direction method ofmultipliers. We prove that the objective function converges almost surely byappealing to the quasi-martingale convergence theorem. We also show the learnedbasis matrix converges to the set of local minimizers of the objective functionalmost surely. In addition, we extend our basic problem formulation to varioussettings with different constraints and regularizers, and adapt the solvers andanalyses to each setting. We perform extensive experiments on both syntheticand image datasets. These experiments demonstrate the efficiency and efficacyof our algorithm on tasks such as basis learning, image denoising and shadowremoval.
arxiv-17100-178 | Grid Based Nonlinear Filtering Revisited: Recursive Estimation & Asymptotic Optimality | http://arxiv.org/pdf/1604.02631v1.pdf | author:Dionysios S. Kalogerias, Athina P. Petropulu category:math.ST cs.IT math.IT math.OC stat.ME stat.ML stat.TH published:2016-04-10 summary:We revisit the development of grid based recursive approximate filtering ofgeneral Markov processes in discrete time, partially observed in conditionallyGaussian noise. The grid based filters considered rely on two types of statequantization: The \textit{Markovian} type and the \textit{marginal} type. Wepropose a set of novel, relaxed sufficient conditions, ensuring strong andfully characterized pathwise convergence of these filters to the respectiveMMSE state estimator. In particular, for marginal state quantizations, weintroduce the notion of \textit{conditional regularity of stochastic kernels},which, to the best of our knowledge, constitutes the most relaxed conditionproposed, under which asymptotic optimality of the respective grid basedfilters is guaranteed. Further, we extend our convergence results, includingfiltering of bounded and continuous functionals of the state, as well asrecursive approximate state prediction. For both Markovian and marginalquantizations, the whole development of the respective grid based filtersrelies more on linear-algebraic techniques and less on measure theoreticarguments, making the presentation considerably shorter and technicallysimpler.
arxiv-17100-179 | TextProposals: a Text-specific Selective Search Algorithm for Word Spotting in the Wild | http://arxiv.org/pdf/1604.02619v1.pdf | author:Lluis Gomez-Bigorda, Dimosthenis Karatzas category:cs.CV published:2016-04-10 summary:The use of object proposals in scene text understanding tasks is innovative.Motivated by the success of powerful while expensive techniques to recognizewords in a holistic way, object proposals techniques emerge as an alternativeto the traditional text detectors. In this paper we introduce a novel objectproposals method that is specifically designed for text. We rely on asimilarity based region grouping algorithm that generates a hierarchy of wordhypotheses. Over the nodes of this hierarchy it is possible to apply a holisticword recognition method in an efficient way. Our experiments demonstrate that the presented method is superior in itsability of producing good quality word proposals when compared withclass-independent algorithms. We show impressive recall rates with a fewthousand proposals in different standard benchmarks, including focused orincidental text datasets, and multi-language scenarios. Moreover, thecombination of our object proposals with existing whole-word recognizers showcompetitive performance in end-to-end word spotting, and, in some benchmarks,outperforms previously published results. Concretely, in the challengingICDAR2015 Incidental Text dataset, we overcome in more than 10 percent f-scorethe best-performing method in the last ICDAR Robust Reading Competition. Sourcecode of the complete end-to-end system is available athttps://github.com/lluisgomez/TextProposals.
arxiv-17100-180 | Fusing Audio, Textual and Visual Features for Sentiment Analysis of News Videos | http://arxiv.org/pdf/1604.02612v1.pdf | author:Moisés H. R. Pereira, Flávio L. C. Pádua, Adriano C. M. Pereira, Fabrício Benevenuto, Daniel H. Dalip category:cs.CL published:2016-04-09 summary:This paper presents a novel approach to perform sentiment analysis of newsvideos, based on the fusion of audio, textual and visual clues extracted fromtheir contents. The proposed approach aims at contributing to thesemiodiscoursive study regarding the construction of the ethos (identity) ofthis media universe, which has become a central part of the modern-day lives ofmillions of people. To achieve this goal, we apply state-of-the-artcomputational methods for (1) automatic emotion recognition from facialexpressions, (2) extraction of modulations in the participants' speeches and(3) sentiment analysis from the closed caption associated to the videos ofinterest. More specifically, we compute features, such as, visual intensitiesof recognized emotions, field sizes of participants, voicing probability, soundloudness, speech fundamental frequencies and the sentiment scores (polarities)from text sentences in the closed caption. Experimental results with a datasetcontaining 520 annotated news videos from three Brazilian and one Americanpopular TV newscasts show that our approach achieves an accuracy of up to 84%in the sentiments (tension levels) classification task, thus demonstrating itshigh potential to be used by media analysts in several applications,especially, in the journalistic domain.
arxiv-17100-181 | A General Retraining Framework for Scalable Adversarial Classification | http://arxiv.org/pdf/1604.02606v1.pdf | author:Bo Li, Yevgeniy Vorobeychik, Xinyun Chen category:cs.GT cs.LG stat.ML published:2016-04-09 summary:Traditional classification algorithms assume that training and test data comefrom the same or similar distribution. This assumption is violated inadversarial settings, where malicious actors modify instances to evadedetection. A number of custom methods have been developed for both adversarialevasion attacks and robust learning. We propose the first systematic andgeneral-purpose retraining framework which can: a) boost robustness of anarbitrary learning algorithm, and b) incorporate a broad class of adversarialmodels. We show that, under natural conditions, the retraining frameworkminimizes an upper bound on optimal adversarial risk, and show how to extendthis result to account for approximations of evasion attacks. We also offer avery general adversarial evasion model and algorithmic framework based oncoordinate greedy local search. Extensive experimental evaluation demonstratesthat our retraining methods are nearly indistinguishable from state-of-the-artalgorithms for optimizing adversarial risk, but far more scalable and general.The experiments also confirm that without retraining, our adversarial frameworkis extremely effective in dramatically reducing the effectiveness of learning.In contrast, retraining significantly boosts robustness to evasion attackswithout compromising much overall accuracy.
arxiv-17100-182 | $\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images | http://arxiv.org/pdf/1601.04149v3.pdf | author:Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, Thomas S. Huang category:cs.CV cs.AI cs.LG published:2016-01-16 summary:In this paper, we design a Deep Dual-Domain ($\mathbf{D^3}$) based fastrestoration model to remove artifacts of JPEG compressed images. It leveragesthe large learning capacity of deep networks, as well as the problem-specificexpertise that was hardly incorporated in the past design of deeparchitectures. For the latter, we take into consideration both the priorknowledge of the JPEG compression scheme, and the successful practice of thesparsity-based dual-domain approach. We further design the One-Step SparseInference (1-SI) module, as an efficient and light-weighted feed-forwardapproximation of sparse coding. Extensive experiments verify the superiority ofthe proposed $D^3$ model over several state-of-the-art methods. Specifically,our best model is capable of outperforming the latest deep model for around 1dB in PSNR, and is 30 times faster.
arxiv-17100-183 | Learning Compact Recurrent Neural Networks | http://arxiv.org/pdf/1604.02594v1.pdf | author:Zhiyun Lu, Vikas Sindhwani, Tara N. Sainath category:cs.LG cs.CL cs.NE published:2016-04-09 summary:Recurrent neural networks (RNNs), including long short-term memory (LSTM)RNNs, have produced state-of-the-art results on a variety of speech recognitiontasks. However, these models are often too large in size for deployment onmobile devices with memory and latency constraints. In this work, we studymechanisms for learning compact RNNs and LSTMs via low-rank factorizations andparameter sharing schemes. Our goal is to investigate redundancies in recurrentarchitectures where compression can be admitted without losing performance. Ahybrid strategy of using structured matrices in the bottom layers and sharedlow-rank factors on the top layers is found to be particularly effective,reducing the parameters of a standard LSTM by 75%, at a small cost of 0.3%increase in WER, on a 2,000-hr English Voice Search task.
arxiv-17100-184 | ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs | http://arxiv.org/pdf/1512.05193v3.pdf | author:Wenpeng Yin, Hinrich Schütze, Bing Xiang, Bowen Zhou category:cs.CL published:2015-12-16 summary:How to model a pair of sentences is a critical issue in many NLP tasks suchas answer selection (AS), paraphrase identification (PI) and textual entailment(TE). Most prior work (i) deals with one individual task by fine-tuning aspecific system; (ii) models each sentence's representation separately, rarelyconsidering the impact of the other sentence; or (iii) relies fully on manuallydesigned, task-specific linguistic features. This work presents a generalAttention Based Convolutional Neural Network (ABCNN) for modeling a pair ofsentences. We make three contributions. (i) ABCNN can be applied to a widevariety of tasks that require modeling of sentence pairs. (ii) We propose threeattention schemes that integrate mutual influence between sentences into CNN;thus, the representation of each sentence takes into consideration itscounterpart. These interdependent sentence pair representations are morepowerful than isolated sentence representations. (iii) ABCNN achievesstate-of-the-art performance on AS, PI and TE tasks.
arxiv-17100-185 | Scene-driven Retrieval in Edited Videos using Aesthetic and Semantic Deep Features | http://arxiv.org/pdf/1604.02546v1.pdf | author:Lorenzo Baraldi, Costantino Grana, Rita Cucchiara category:cs.CV cs.IR cs.MM published:2016-04-09 summary:This paper presents a novel retrieval pipeline for video collections, whichaims to retrieve the most significant parts of an edited video for a givenquery, and represent them with thumbnails which are at the same timesemantically meaningful and aesthetically remarkable. Videos are firstsegmented into coherent and story-telling scenes, then a retrieval algorithmbased on deep learning is proposed to retrieve the most significant scenes fora textual query. A ranking strategy based on deep features is finally used totackle the problem of visualizing the best thumbnail. Qualitative andquantitative experiments are conducted on a collection of edited videos todemonstrate the effectiveness of our approach.
arxiv-17100-186 | Visual7W: Grounded Question Answering in Images | http://arxiv.org/pdf/1511.03416v4.pdf | author:Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei category:cs.CV cs.LG cs.NE published:2015-11-11 summary:We have seen great progress in basic perceptual tasks such as objectrecognition and detection. However, AI models still fail to match humans inhigh-level vision tasks due to the lack of capacities for deeper reasoning.Recently the new task of visual question answering (QA) has been proposed toevaluate a model's capacity for deep image understanding. Previous works haveestablished a loose, global association between QA sentences and images.However, many questions and answers, in practice, relate to local regions inthe images. We establish a semantic link between textual descriptions and imageregions by object-level grounding. It enables a new type of QA with visualanswers, in addition to textual answers used in previous work. We study thevisual QA tasks in a grounded setting with a large collection of 7Wmultiple-choice QA pairs. Furthermore, we evaluate human performance andseveral baseline models on the QA tasks. Finally, we propose a novel LSTM modelwith spatial attention to tackle the 7W QA tasks.
arxiv-17100-187 | Person Re-identification in the Wild | http://arxiv.org/pdf/1604.02531v1.pdf | author:Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Qi Tian category:cs.CV published:2016-04-09 summary:We present a novel large-scale dataset and comprehensive baselines forend-to-end pedestrian detection and person recognition in raw video frames. Ourbaselines address three issues: the performance of various combinations ofdetectors and recognizers, mechanisms for pedestrian detection to help improveoverall re-identification accuracy and assessing the effectiveness of differentdetectors for re-identification. We make three distinct contributions. First, anew dataset, PRW, is introduced to evaluate Person Re-identification in theWild, using videos acquired through six synchronized cameras. It contains 932identities and 11,816 frames in which pedestrians are annotated with theirbounding box positions and identities. Extensive benchmarking results arepresented on this dataset. Second, we show that pedestrian detection aidsre-identification through two simple yet effective improvements: adiscriminatively trained ID-discriminative Embedding (IDE) in the personsubspace using convolutional neural network (CNN) features and a ConfidenceWeighted Similarity (CWS) metric that incorporates detection scores intosimilarity measurement. Third, we derive insights in evaluating detectorperformance for the particular scenario of accurate person re-identification.
arxiv-17100-188 | Dissimilarity-based Sparse Subset Selection | http://arxiv.org/pdf/1407.6810v2.pdf | author:Ehsan Elhamifar, Guillermo Sapiro, S. Shankar Sastry category:cs.LG stat.ML published:2014-07-25 summary:Finding an informative subset of a large collection of data points or modelsis at the center of many problems in computer vision, recommender systems,bio/health informatics as well as image and natural language processing. Givenpairwise dissimilarities between the elements of a `source set' and a `targetset,' we consider the problem of finding a subset of the source set, calledrepresentatives or exemplars, that can efficiently describe the target set. Weformulate the problem as a row-sparsity regularized trace minimization problem.Since the proposed formulation is, in general, NP-hard, we consider a convexrelaxation. The solution of our optimization finds representatives and theassignment of each element of the target set to each representative, hence,obtaining a clustering. We analyze the solution of our proposed optimization asa function of the regularization parameter. We show that when the two setsjointly partition into multiple groups, our algorithm finds representativesfrom all groups and reveals clustering of the sets. In addition, we show thatthe proposed framework can effectively deal with outliers. Our algorithm workswith arbitrary dissimilarities, which can be asymmetric or violate the triangleinequality. To efficiently implement our algorithm, we consider an AlternatingDirection Method of Multipliers (ADMM) framework, which results in quadraticcomplexity in the problem size. We show that the ADMM implementation allows toparallelize the algorithm, hence further reducing the computational time.Finally, by experiments on real-world datasets, we show that our proposedalgorithm improves the state of the art on the two problems of scenecategorization using representative images and time-series modeling andsegmentation using representative~models.
arxiv-17100-189 | Higher order features and recurrent neural networks based on Long-Short Term Memory nodes in supervised biomedical word sense disambiguation | http://arxiv.org/pdf/1604.02506v1.pdf | author:Antonio Jimeno Yepes category:cs.CL cs.LG published:2016-04-09 summary:Word sense disambiguation helps identifying the proper sense of ambiguouswords in text. With large terminologies such as the UMLS Metathesaurusambiguities appear and highly effective disambiguation methods are required.Supervised learning algorithm methods are used as one of the approaches toperform disambiguation. Features extracted from the context of an ambiguousword are used to identify the proper sense of such a word. The type of featureshave an impact on machine learning methods, thus affect disambiguationperformance. In this work, we have evaluated several types of features derivedfrom the context of the ambiguous word and we have explored as well more globalfeatures derived from MEDLINE using word embeddings. Results show that wordembeddings improve the performance of more traditional features and allow aswell using recurrent neural networks based on Long-Short Term Memory (LSTM)nodes, which further improve the disambiguation performance. The combination ofunigrams and word embeddings set a new state of the art performance with anaccuracy of 95.97 in the MSH WSD data set.
arxiv-17100-190 | Randomized Robust Subspace Recovery for High Dimensional Data Matrices | http://arxiv.org/pdf/1505.05901v2.pdf | author:Mostafa Rahmani, George Atia category:stat.ML cs.CV published:2015-05-21 summary:This paper explores and analyzes two randomized designs for robust PrincipalComponent Analysis (PCA) employing low-dimensional data sketching. In onedesign, a data sketch is constructed using random column sampling followed bylow dimensional embedding, while in the other, sketching is based on randomcolumn and row sampling. Both designs are shown to bring about substantialsavings in complexity and memory requirements for robust subspace learning overconventional approaches that use the full scale data. A characterization of thesample and computational complexity of both designs is derived in the contextof two distinct outlier models, namely, sparse and independent outlier models.The proposed randomized approach can provably recover the correct subspace withcomputational and sample complexity that are almost independent of the size ofthe data. The results of the mathematical analysis are confirmed throughnumerical simulations using both synthetic and real data.
arxiv-17100-191 | Towards universal neural nets: Gibbs machines and ACE | http://arxiv.org/pdf/1508.06585v4.pdf | author:Galin Georgiev category:cs.CV cs.LG cs.NE published:2015-08-26 summary:We study a class of neural nets - \emph{Gibbs machines} - which are a type ofvariational auto-encoders, designed for gradual learning. They offer anuniversal platform for incrementally adding newly learned features, includingphysical symmetries, and are directly connected to information geometry andthermodynamics. Combining them with classifiers, gives rise to a brand ofuniversal generative neural nets - stochastic auto-classifier-encoders (ACE).ACE have state-of-the-art performance in their class, both for classificationand density estimation for the MNIST data set.
arxiv-17100-192 | On the Sensitivity of the Lasso to the Number of Predictor Variables | http://arxiv.org/pdf/1403.4544v2.pdf | author:Cheryl J. Flynn, Clifford M. Hurvich, Jeffrey S. Simonoff category:stat.ML published:2014-03-18 summary:The Lasso is a computationally efficient procedure that can produce sparseestimators when the number of predictors (p) is large. Oracle inequalitiesprovide probability loss bounds for the Lasso estimator at a deterministicchoice of the regularization parameter. These bounds tend to zero if p isappropriately controlled, and are thus commonly cited as theoreticaljustification for the Lasso and its ability to handle high-dimensionalsettings. Unfortunately, in practice the regularization parameter is notselected to be a deterministic quantity, but is instead chosen using a random,data-dependent procedure. To address this shortcoming of previous theoreticalwork, we study the loss of the Lasso estimator when tuned optimally forprediction. Assuming orthonormal predictors and a sparse true model, we provethat the probability that the best possible predictive performance of the Lassodeteriorates as p increases can be arbitrarily close to one given asufficiently high signal to noise ratio and sufficiently large p. We furtherdemonstrate empirically that the deterioration in performance can be far worsethan is commonly suggested in the literature and provide a real data examplewhere deterioration is observed.
arxiv-17100-193 | Challenges in Bayesian Adaptive Data Analysis | http://arxiv.org/pdf/1604.02492v1.pdf | author:Sam Elder category:cs.LG stat.ML published:2016-04-08 summary:Traditional statistical analysis requires that the analysis process and dataare independent. By contrast, the new field of adaptive data analysis hopes tounderstand and provide algorithms and accuracy guarantees for research as it iscommonly performed in practice, as an iterative process of proposing hypothesesand interacting with the data set. Previous work has established a model with arather strong lower bound on sample complexity in terms of the number ofqueries, $n\sim\sqrt q$, arguing that adaptive data analysis is much harderthan static data analysis, where $n\sim\log q$ is possible. Instead, we arguethat those strong lower bounds point to a shortcoming in the model, aninformational asymmetry with no basis in applications. In its place, we propose a new Bayesian version of the problem without thisunnecessary asymmetry. The previous lower bounds are no longer valid, whichoffers the possibility for stronger results. However, we show that a largefamily of methods, including all previously proposed algorithms, cannot achievethe static dependence of $n\sim\log q$ even in this regime, establishingpolylogarithmic lower bounds with a new family of lower bounds. Thesepreliminary results suggest that adaptive data analysis is harder than staticdata analysis even without this information asymmetry, but still leave wideopen the possibility that new algorithms can be developed to work with fewersamples than the previous best known algorithms.
arxiv-17100-194 | Symmetries and control in generative neural nets | http://arxiv.org/pdf/1511.02841v3.pdf | author:Galin Georgiev category:cs.CV cs.LG published:2015-11-09 summary:We study generative nets which can control and modify observations, afterbeing trained on real-life datasets. In order to zoom-in on an object, somespatial, color and other attributes are learned by classifiers in specializedattention nets. In field-theoretical terms, these learned symmetry statisticsform the gauge group of the data set. Plugging them in the generative layers ofauto-classifiers-encoders (ACE) appears to be the most direct way tosimultaneously: i) generate new observations with arbitrary attributes, from agiven class, ii) describe the low-dimensional manifold encoding the "essence"of the data, after superfluous attributes are factored out, and iii)organically control, i.e., move or modify objects within given observations. Wedemonstrate the sharp improvement of the generative qualities of shallow ACE,with added spatial and color symmetry statistics, on the distorted MNIST andCIFAR10 datasets.
arxiv-17100-195 | Application of Multifractal Analysis to Segmentation of Water Bodies in Optical and Synthetic Aperture Radar Satellite Images | http://arxiv.org/pdf/1604.02488v1.pdf | author:Victor Manuel San Martin, Alejandra Figliola category:cs.CV published:2016-04-08 summary:A method for segmenting water bodies in optical and synthetic aperture radar(SAR) satellite images is proposed. It makes use of the textural features ofthe different regions in the image for segmentation. The method consists in amultiscale analysis of the images, which allows us to study the imagesregularity both, locally and globally. As results of the analysis, coarsemultifractal spectra of studied images and a group of images that associateseach position (pixel) with its corresponding value of local regularity (orsingularity) spectrum are obtained. Thresholds are then applied to themultifractal spectra of the images for the classification. These thresholds areselected after studying the characteristics of the spectra under the assumptionthat water bodies have larger local regularity than other soil types.Classifications obtained by the multifractal method are compared quantitativelywith those obtained by neural networks trained to classify the pixels of theimages in covered against uncovered by water. In optical images, theclassifications are also compared with those derived using the so-calledNormalized Differential Water Index (NDWI).
arxiv-17100-196 | Character-Level Question Answering with Attention | http://arxiv.org/pdf/1604.00727v3.pdf | author:David Golub, Xiaodong He category:cs.CL cs.AI cs.LG published:2016-04-04 summary:We show that an encoder-decoder framework can be successfully applied toquestion-answering with a structured knowledge base. In addition, we propose anew character-level modeling approach for this task, which we use to make ourmodel robust to unseen entities and predicates. We use our model forsingle-relation question answering, and demonstrate the effectiveness of ournovel approach on the SimpleQuestions dataset, where we improvestate-of-the-art accuracy by 2% for both Freebase2M and Freebase5M subsetsproposed. Importantly, we achieve these results even though our character-levelmodel has 16x less parameters than an equivalent word-embedding model, usessignificantly less training data than previous work which relies on dataaugmentation, and encounters only 1.18% of the entities seen during trainingwhen testing.
arxiv-17100-197 | Machine Learning for Visual Navigation of Unmanned Ground Vehicles | http://arxiv.org/pdf/1604.02485v1.pdf | author:Artem A. Lenskiy, Jong-Soo Lee category:cs.CV published:2016-04-08 summary:The use of visual information for the navigation of unmanned ground vehiclesin a cross-country environment recently received great attention. However,until now, the use of textural information has been somewhat less effectivethan color or laser range information. This manuscript reviews the recentachievements in cross-country scene segmentation and addresses theirshortcomings. It then describes a problem related to classification of highdimensional texture features. Finally, it compares three machine learningalgorithms aimed at resolving this problem. The experimental results for eachmachine learning algorithm with the discussion of comparisons are given at theend of the manuscript.
arxiv-17100-198 | Leveraging Network Dynamics for Improved Link Prediction | http://arxiv.org/pdf/1604.03221v1.pdf | author:Alireza Hajibagheri, Gita Sukthankar, Kiran Lakkaraju category:cs.SI cs.LG published:2016-04-08 summary:The aim of link prediction is to forecast connections that are most likely tooccur in the future, based on examples of previously observed links. A keyinsight is that it is useful to explicitly model network dynamics, howfrequently links are created or destroyed when doing link prediction. In thispaper, we introduce a new supervised link prediction framework, RPM (RatePrediction Model). In addition to network similarity measures, RPM uses thepredicted rate of link modifications, modeled using time series data; it isimplemented in Spark-ML and trained with the original link distribution, ratherthan a small balanced subset. We compare the use of this network dynamics modelto directly creating time series of network similarity measures. Ourexperiments show that RPM, which leverages predicted rates, outperforms the useof network similarity measures, either individually or within a time series.
arxiv-17100-199 | One-class classifiers based on entropic spanning graphs | http://arxiv.org/pdf/1604.02477v1.pdf | author:Lorenzo Livi, Cesare Alippi category:cs.LG cs.CV cs.IT math.IT published:2016-04-08 summary:One-class classifiers offer valuable tools to assess the presence of outliersin data. In this paper, we propose a design methodology for one-classclassifiers based on entropic spanning graphs. The spanning graph is learned onthe embedded input data, with the aim to generate a partition of the vertices.The final partition is derived by exploiting a criterion based on mutualinformation minimization. Here, we compute the mutual information by using aconvenient formulation provided in terms of the $\alpha$-Jensen difference.Once training is completed, in order to associate a confidence level with theclassifier decision, a graph-based fuzzy model is constructed. Thefuzzification process is based only on topological information of the verticesof the entropic spanning graph. As such, the proposed one-class classifier issuitable also for datasets with complex geometric structures. We provideexperiments on well-known benchmarking datasets containing both feature vectorsand labeled graphs. In addition, we apply the method on the problem of proteinsolubility recognition by considering several data representations for thesamples. Experimental results demonstrate the effectiveness and versatility ofthe proposed method with respect to other state-of-the-art approaches.
arxiv-17100-200 | Image segmentation of cross-country scenes captured in IR spectrum | http://arxiv.org/pdf/1604.02469v1.pdf | author:Artem Lenskiy category:cs.CV 68T10 published:2016-04-08 summary:Computer vision has become a major source of information for autonomousnavigation of robots of various types, self-driving cars, military robots andmars/lunar rovers are some examples. Nevertheless, the majority of methodsfocus on analysing images captured in visible spectrum. In this manuscript weelaborate on the problem of segmenting cross-country scenes captured in IRspectrum. For this purpose we proposed employing salient features. Salientfeatures are robust to variations in scale, brightness and view angle. Wesuggest the Speeded-Up Robust Features as a basis for our salient features fora number of reasons discussed in the paper. We also provide a comparison of twoSURF implementations. The SURF features are extracted from images of differentterrain types. For every feature we estimate a terrain class membershipfunction. The membership values are obtained by means of either the multi-layerperceptron or nearest neighbours. The features' class membership values andtheir spatial positions are then applied to estimate class membership valuesfor all pixels in the image. To decrease the effect of segmentation blinkingthat is caused by rapid switching between different terrain types and to speedup segmentation, we are tracking camera position and predict features'positions. The comparison of the multi-layer perception and the nearestneighbour classifiers is presented in the paper. The error rate of the terrainsegmentation using the nearest neighbours obtained on the testing set is16.6+-9.17%.
arxiv-17100-201 | Wavelet-Based Semantic Features for Hyperspectral Signature Discrimination | http://arxiv.org/pdf/1602.03903v2.pdf | author:Siwei Feng, Yuki Itoh, Mario Parente, Marco F. Duarte category:cs.CV cs.LG published:2016-02-11 summary:Hyperspectral signature classification is a quantitative analysis approachfor hyperspectral imagery which performs detection and classification of theconstituent materials at the pixel level in the scene. The classificationprocedure can be operated directly on hyperspectral data or performed by usingsome features extracted from the corresponding hyperspectral signaturescontaining information like the signature's energy or shape. In this paper, wedescribe a technique that applies non-homogeneous hidden Markov chain (NHMC)models to hyperspectral signature classification. The basic idea is to usestatistical models (such as NHMC) to characterize wavelet coefficients whichcapture the spectrum semantics (i.e., structural information) at multiplelevels. Experimental results show that the approach based on NHMC models canoutperform existing approaches relevant in classification tasks.
arxiv-17100-202 | CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples | http://arxiv.org/pdf/1604.02426v1.pdf | author:Filip Radenović, Giorgos Tolias, Ondřej Chum category:cs.CV published:2016-04-08 summary:Convolutional Neural Networks (CNNs) achieve state-of-the-art performance inmany computer vision tasks. However, this achievement is preceded by extrememanual annotation in order to perform either training from scratch orfine-tuning for the target task. In this work, we propose to fine-tune CNN forimage retrieval from a large collection of unordered images in a fullyautomated manner. We employ state-of-the-art retrieval andStructure-from-Motion (SfM) methods to obtain 3D models, which are used toguide the selection of the training data for CNN fine-tuning. We show that bothhard positive and hard negative examples enhance the final performance inparticular object retrieval with compact codes.
arxiv-17100-203 | Feature Selection for Regression Problems Based on the Morisita Estimator of Intrinsic Dimension | http://arxiv.org/pdf/1602.00216v5.pdf | author:Jean Golay, Michael Leuenberger, Mikhail Kanevski category:stat.ML cs.LG published:2016-01-31 summary:Data acquisition, storage and management have been improved, while the keyfactors of many phenomena are not well known. Consequently, irrelevant andredundant features artificially increase the size of datasets, whichcomplicates learning tasks, such as regression. To address this problem,feature selection methods have been proposed. This paper introduces a newsupervised filter based on the Morisita estimator of intrinsic dimension. Itcan identify relevant features and distinguish between redundant and irrelevantinformation. Besides, it offers a clear graphical representation of the resultsand it can be easily implemented in different programming languages.Comprehensive numerical experiments are conducted using simulated datasetscharacterized by different levels of complexity, sample size and noise. Thesuggested algorithm is also successfully tested on a selection of real worldapplications and compared with RReliefF using extreme learning machine. Inaddition, a new measure of feature relevance is presented and discussed.
arxiv-17100-204 | RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling | http://arxiv.org/pdf/1604.02388v1.pdf | author:Yang He, Wei-Chen Chiu, Margret Keuper, Mario Fritz category:cs.CV published:2016-04-08 summary:Beyond the success in classification, neural networks have recently shownstrong results on pixel-wise prediction tasks like image semantic segmentationon RGBD data. However, the commonly used deconvolutional layers for upsamplingintermediate representations to the full-resolution output still showsdifferent failure modes, like imprecise segmentation boundaries and labelmistakes particular on large, weakly textured objects (e.g. fridge, whiteboard,door). We attribute these errors in part to the rigid way, current networkaggregate information, that can be either too local (missing context) or tooglobal (inaccurate boundaries). Therefore we propose a data-driven poolinglayer that integrates with fully convolutional architectures and utilizesboundary detection from RGBD image segmentation approaches. We extend ourapproach to leverage region-level correspondence across images with anadditional temporal pooling stage. We evaluate our approach on the NYU-Depth-V2dataset comprised of indoor RGBD video sequences and make comparison withrespect to various state-of-the-art baselines. We improve on thestate-of-the-art and in particular in accuracy of the predicted boundaries andpreviously problematic classes.
arxiv-17100-205 | The (1+1) Elitist Black-Box Complexity of LeadingOnes | http://arxiv.org/pdf/1604.02355v1.pdf | author:Carola Doerr, Johannes Lengler category:cs.NE cs.CC F.2.2 published:2016-04-08 summary:One important goal of black-box complexity theory is the development ofcomplexity models allowing to derive meaningful lower bounds for whole classesof randomized search heuristics. Complementing classical runtime analysis,black-box models help us understand how algorithmic choices such as thepopulation size, the variation operators, or the selection rules influence theoptimization time. One example for such a result is the $\Omega(n \log n)$lower bound for unary unbiased algorithms on functions with a unique globaloptimum [Lehre/Witt, GECCO 2010], which tells us that higher arity operators orbiased sampling strategies are needed when trying to beat this bound. In lackof analyzing techniques, almost no non-trivial bounds are known for otherrestricted models. Proving such bounds therefore remains to be one of the mainchallenges in black-box complexity theory. With this paper we contribute to our technical toolbox for lower boundcomputations by proposing a new type of information-theoretic argument. Weregard the permutation- and bit-invariant version of \textsc{LeadingOnes} andprove that its (1+1) elitist black-box complexity is $\Omega(n^2)$, a boundthat is matched by (1+1)-type evolutionary algorithms. The (1+1) elitistcomplexity of \textsc{LeadingOnes} is thus considerably larger than itsunrestricted one, which is known to be of order $n\log\log n$ [Afshani et al.,2013].
arxiv-17100-206 | Bayesian Neighbourhood Component Analysis | http://arxiv.org/pdf/1604.02354v1.pdf | author:Dong Wang, Xiaoyang Tan category:cs.CV cs.LG published:2016-04-08 summary:Learning a good distance metric in feature space potentially improves theperformance of the KNN classifier and is useful in many real-worldapplications. Many metric learning algorithms are however based on the pointestimation of a quadratic optimization problem, which is time-consuming,susceptible to overfitting, and lack a natural mechanism to reason withparameter uncertainty, an important property useful especially when thetraining set is small and/or noisy. To deal with these issues, we present anovel Bayesian metric learning method, called Bayesian NCA, based on thewell-known Neighbourhood Component Analysis method, in which the metricposterior is characterized by the local label consistency constraints ofobservations, encoded with a similarity graph instead of independent pairwiseconstraints. For efficient Bayesian optimization, we explore the variationallower bound over the log-likelihood of the original NCA objective. Experimentson several publicly available datasets demonstrate that the proposed method isable to learn robust metric measures from small size dataset and/or fromchallenging training set with labels contaminated by errors. The proposedmethod is also shown to outperform a previous pairwise constrained Bayesianmetric learning method.
arxiv-17100-207 | The "Sprekend Nederland" project and its application to accent location | http://arxiv.org/pdf/1602.02499v2.pdf | author:David A. van Leeuwen, Rosemary Orr category:stat.ML cs.CL published:2016-02-08 summary:This paper describes the data collection effort that is part of the projectSprekend Nederland (The Netherlands Talking), and discusses its potential usein Automatic Accent Location. We define Automatic Accent Location as the taskto describe the accent of a speaker in terms of the location of the speaker andits history. We discuss possible ways of describing accent location, theconsequence these have for the task of automatic accent location, and potentialevaluation metrics.
arxiv-17100-208 | Back to the Basics: Bayesian extensions of IRT outperform neural networks for proficiency estimation | http://arxiv.org/pdf/1604.02336v1.pdf | author:Kevin H. Wilson, Yan Karklin, Bojian Han, Chaitanya Ekanadham category:cs.AI cs.LG published:2016-04-08 summary:Estimating student proficiency is an important task for computer-basedlearning systems. We compare a family of IRT-based proficiency estimationmethods with a recently proposed approach using recurrent neural networks(RNNs) on two publicly available and one proprietary data set, evaluating eachmodel according to how well a student's future response is predicted givenprevious responses. IRT-based methods consistently matched or outperformed theRNN-based method across all data sets at the finest level of contentgranularity that was tractable for them to be trained on. A hierarchicalextension of IRT that captured item grouping structure performed best overall.When data sets included non-trivial autocorrelations in student responsepatterns, a temporal extension of IRT improved performance over standard IRTwhile the RNN-based method did not. We conclude that IRT-based models provide asimpler, better-performing alternative to the current generation of RNN-basedmodels while also affording more interpretability and guarantees due to theirformulation as Bayesian probabilistic models.
arxiv-17100-209 | Pymanopt: A Python Toolbox for Manifold Optimization using Automatic Differentiation | http://arxiv.org/pdf/1603.03236v2.pdf | author:James Townsend, Niklas Koep, Sebastian Weichwald category:cs.MS cs.LG math.OC stat.ML published:2016-03-10 summary:Manifold optimization is a method for (non-convex) optimization of anobjective function, subject to constraints which are smooth, in the sense thatthe set of points which satisfy the constraints admits the structure of adifferentiable manifold. While many optimization problems are of the describedform, technicalities of differential geometry and the laborious calculation ofderivatives pose a significant barrier for experimenting with manifoldoptimization techniques. We introduce Pymanopt (available at https://pymanopt.github.io), a manifoldoptimization toolbox implemented in Python that - similarly to the ManoptMatlab toolbox - implements several manifold geometries and optimizationalgorithms. Moreover, we lower the barriers to users further by using automateddifferentiation for calculating derivative information, saving users time andsaving them from potential calculation and implementation errors.
arxiv-17100-210 | Free-Space Detection with Self-Supervised and Online Trained Fully Convolutional Networks | http://arxiv.org/pdf/1604.02316v1.pdf | author:Willem P. Sanberg, Gijs Dubbelman, Peter H. N. de With category:cs.CV published:2016-04-08 summary:Recently, vision-based Advanced Driver Assist Systems have gained broadinterest. In this work, we investigate free-space detection, for which wepropose to employ a Fully Convolutional Network (FCN). We show that this FCNcan be trained in a self-supervised manner and achieve similar results comparedto training on manually annotated data, thereby reducing the need for largemanually annotated training sets. To this end, our self-supervised trainingrelies on a stereo-vision disparity system, to automatically generate (weak)training labels for the color-based FCN. Additionally, our self-supervisedtraining facilitates online training of the FCN instead of offline.Consequently, given that the applied FCN is relatively small, the free-spaceanalysis becomes highly adaptive to any traffic scene that the vehicleencounters. We have validated our algorithm using publicly available data andon a new challenging benchmark dataset that is released with this paper.Experiments show that the online training boosts performance with 5% whencompared to offline training, both for Fmax and AP.
arxiv-17100-211 | Norm-preserving Orthogonal Permutation Linear Unit Activation Functions (OPLU) | http://arxiv.org/pdf/1604.02313v1.pdf | author:Artem Chernodub, Dimitri Nowicki category:cs.NE published:2016-04-08 summary:We propose a novel activation function that implements piece-wise orthogonalnon-linear mappings based on permutations. It is straightforward to implement,and very computationally efficient, also it has little memory requirements. Wetested it on two toy problems for feedforward and recurrent networks, it showssimilar performance to tanh and ReLU. OPLU activation function ensures normpreservance of the backpropagated gradients, therefore it is potentially goodfor the training of deep, extra deep, and recurrent neural networks.
arxiv-17100-212 | Block-diagonal covariance selection for high-dimensional Gaussian graphical models | http://arxiv.org/pdf/1511.04033v2.pdf | author:Emilie Devijver, Mélina Gallopin category:math.ST cs.LG stat.ML stat.TH published:2015-11-12 summary:Gaussian graphical models are widely utilized to infer and visualize networksof dependencies between continuous variables. However, inferring the graph isdifficult when the sample size is small compared to the number of variables. Toreduce the number of parameters to estimate in the model, we propose anon-asymptotic model selection procedure supported by strong theoreticalguarantees based on an oracle inequality and a minimax lower bound. Thecovariance matrix of the model is approximated by a block-diagonal matrix. Thestructure of this matrix is detected by thresholding the sample covariancematrix, where the threshold is selected using the slope heuristic. Based on theblock-diagonal structure of the covariance matrix, the estimation problem isdivided into several independent problems: subsequently, the network ofdependencies between variables is inferred using the graphical lasso algorithmin each block. The performance of the procedure is illustrated on simulateddata. An application to a real gene expression dataset with a limited samplesize is also presented: the dimension reduction allows attention to beobjectively focused on interactions among smaller subsets of genes, leading toa more parsimonious and interpretable modular network.
arxiv-17100-213 | A method for locally approximating regularized iterative tomographic reconstruction methods | http://arxiv.org/pdf/1604.02292v1.pdf | author:D. M. Pelt, K. J. Batenburg category:math.NA cs.CV published:2016-04-08 summary:In many applications of tomography, the acquired projections are eitherlimited in number or contain a significant amount of noise. In these cases,standard reconstruction methods tend to produce artifacts that can make furtheranalysis difficult. Advanced regularized iterative methods, such as totalvariation minimization, are often able to achieve a higher reconstructionquality by exploiting prior knowledge about the scanned object. In practice,however, these methods often have prohibitively long computation times or largememory requirements. Furthermore, since they are based on minimizing a globalobjective function, regularized iterative methods need to reconstruct theentire scanned object, even when one is only interested in a (small) region ofthe reconstructed image. In this paper, we present a method to approximate regularized iterativereconstruction methods inside a (small) region of the scanned object. Themethod only performs computations inside the region of interest, ensuring lowcomputational requirements. Reconstruction results for different phantom imagesand types of regularization are given, showing that reconstructions of theproposed local method are almost identical to those of the global regularizediterative methods that are approximated, even for relatively small regions ofinterest. Furthermore, we show that larger regions can be reconstructedefficiently by reconstructing several small regions in parallel and combiningthem into a single reconstruction afterwards.
arxiv-17100-214 | Well-posedness of a nonlinear integro-differential problem and its rearranged formulation | http://arxiv.org/pdf/1506.02247v2.pdf | author:Gonzalo Galiano, Emanuele Schiavi, Julián Velasco category:cs.CV published:2015-06-07 summary:We study the existence and uniqueness of solutions of a nonlinearintegro-differential problem which we reformulate introducing the notion of thedecreasing rearrangement of the solution. A dimensional reduction of theproblem is obtained and a detailed analysis of the properties of the solutionsof the model is provided. Finally, a fast numerical method is devised andimplemented to show the performance of the model when typical image processingtasks such as filtering and segmentation are performed.
arxiv-17100-215 | Manifold unwrapping using density ridges | http://arxiv.org/pdf/1604.01602v2.pdf | author:Jonas Nordhaug Myhre, Matineh Shaker, Devrim Kaba, Robert Jenssen, Deniz Erdogmus category:stat.ML published:2016-04-06 summary:Research on manifold learning within a density ridge estimation framework hasshown great potential in recent work for both estimation and de-noising ofmanifolds, building on the intuitive and well-defined notion of principalcurves and surfaces. However, the problem of unwrapping or unfolding manifoldshas received relatively little attention within the density ridge approach,despite being an integral part of manifold learning in general. This paperproposes two novel algorithms for unwrapping manifolds based on estimatedprincipal curves and surfaces for one- and multi-dimensional manifoldsrespectively. The methods of unwrapping are founded in the realization thatboth principal curves and principal surfaces will have inherent local maxima ofthe probability density function. Following this observation, coordinatesystems that follow the shape of the manifold can be computed by following theintegral curves of the gradient flow of a kernel density estimate on themanifold. Furthermore, since integral curves of the gradient flow of a kerneldensity estimate is inherently local, we propose to stitch together localcoordinate systems using parallel transport along the manifold. We providenumerical experiments on both real and synthetic data that illustrates clearand intuitive unwrapping results comparable to state-of-the-art manifoldlearning algorithms.
arxiv-17100-216 | Online Open World Recognition | http://arxiv.org/pdf/1604.02275v1.pdf | author:Rocco De Rosa, Thomas Mensink, Barbara Caputo category:cs.CV cs.LG stat.ML published:2016-04-08 summary:As we enter into the big data age and an avalanche of images have becomereadily available, recognition systems face the need to move from close, labsettings where the number of classes and training data are fixed, to dynamicscenarios where the number of categories to be recognized grows continuouslyover time, as well as new data providing useful information to update thesystem. Recent attempts, like the open world recognition framework, tried toinject dynamics into the system by detecting new unknown classes and addingthem incrementally, while at the same time continuously updating the models forthe known classes. incrementally adding new classes and detecting instancesfrom unknown classes, while at the same time continuously updating the modelsfor the known classes. In this paper we argue that to properly capture theintrinsic dynamic of open world recognition, it is necessary to add to theseaspects (a) the incremental learning of the underlying metric, (b) theincremental estimate of confidence thresholds for the unknown classes, and (c)the use of local learning to precisely describe the space of classes. We extendthree existing metric learning algorithms towards these goals by using onlinemetric learning. Experimentally we validate our approach on two large-scaledatasets in different learning scenarios. For all these scenarios our proposedmethods outperform their non-online counterparts. We conclude that local andonline learning is important to capture the full dynamics of open worldrecognition.
arxiv-17100-217 | Deep Structured Scene Parsing by Learning with Image Descriptions | http://arxiv.org/pdf/1604.02271v1.pdf | author:Liang Lin, Guangrun Wang, Rui Zhang, Ruimao Zhang, Xiaodan Liang, Wangmeng Zuo category:cs.CV 68U10 I.4.8; I.5 published:2016-04-08 summary:This paper addresses a fundamental problem of scene understanding: How toparse the scene image into a structured configuration (i.e., a semantic objecthierarchy with object interaction relations) that finely accords with humanperception. We propose a deep architecture consisting of two networks: i) aconvolutional neural network (CNN) extracting the image representation forpixelwise object labeling and ii) a recursive neural network (RNN) discoveringthe hierarchical object structure and the inter-object relations. Rather thanrelying on elaborative user annotations (e.g., manually labeling semantic mapsand relations), we train our deep model in a weakly-supervised manner byleveraging the descriptive sentences of the training images. Specifically, wedecompose each sentence into a semantic tree consisting of nouns and verbphrases, and facilitate these trees discovering the configurations of thetraining images. Once these scene configurations are determined, then theparameters of both the CNN and RNN are updated accordingly by back propagation.The entire model training is accomplished through an Expectation-Maximizationmethod. Extensive experiments suggest that our model is capable of producingmeaningful and structured scene configurations and achieving more favorablescene labeling performance on PASCAL VOC 2012 over other state-of-the-artweakly-supervised methods.
arxiv-17100-218 | Single-Molecule Protein Identification by Sub-Nanopore Sensors | http://arxiv.org/pdf/1604.02270v1.pdf | author:Mikhail Kolmogorov, Eamonn Kennedy, Zhuxin Dong, Gregory Timp, Pavel Pevzner category:q-bio.QM cs.LG published:2016-04-08 summary:Recent advances in top-down mass spectrometry enabled identification ofintact proteins, but this technology still faces challenges. For example,top-down mass spectrometry suffers from a lack of sensitivity since the ioncounts for a single fragmentation event are often low. In contrast, nanoporetechnology is exquisitely sensitive to single intact molecules, but it has onlybeen successfully applied to DNA sequencing, so far. Here, we explore thepotential of sub-nanopores for single-molecule protein identification (SMPI)and describe an algorithm for analyzing the electrical current blockade signal(nanospectrum) resulting from the translocation of a denaturated, linearlycharged protein through a sub-nanopore. We further describe the first SMPIalgorithm, compute the p-values of Protein-Nanospectrum Matches, and discussthe promise and computational limitations of the current SMPI technology.
arxiv-17100-219 | Probabilistic classifiers with low rank indefinite kernels | http://arxiv.org/pdf/1604.02264v1.pdf | author:Frank-Michael Schleif, Andrej Gisbrecht, Peter Tino category:cs.LG published:2016-04-08 summary:Indefinite similarity measures can be frequently found in bio-informatics bymeans of alignment scores, but are also common in other fields like shapemeasures in image retrieval. Lacking an underlying vector space, the data aregiven as pairwise similarities only. The few algorithms available for such datado not scale to larger datasets. Focusing on probabilistic batch classifiers,the Indefinite Kernel Fisher Discriminant (iKFD) and the ProbabilisticClassification Vector Machine (PCVM) are both effective algorithms for thistype of data but, with cubic complexity. Here we propose an extension of iKFDand PCVM such that linear runtime and memory complexity is achieved for lowrank indefinite kernels. Employing the Nystr\"om approximation for indefinitekernels, we also propose a new almost parameter free approach to identify thelandmarks, restricted to a supervised learning problem. Evaluations at severallarger similarity data from various domains show that the proposed methodsprovides similar generalization capabilities while being easier to parametrizeand substantially faster for large scale data.
arxiv-17100-220 | Weakly Supervised Deep Detection Networks | http://arxiv.org/pdf/1511.02853v3.pdf | author:Hakan Bilen, Andrea Vedaldi category:cs.CV published:2015-11-09 summary:Weakly supervised learning of object detection is an important problem inimage understanding that still does not have a satisfactory solution. In thispaper, we address this problem by exploiting the power of deep convolutionalneural networks pre-trained on large-scale image-level classification tasks. Wepropose a weakly supervised deep detection architecture that modifies one suchnetwork to operate at the level of image regions, performing simultaneouslyregion selection and classification. Trained as an image classifier, thearchitecture implicitly learns object detectors that are better thanalternative weakly supervised detection systems on the PASCAL VOC data. Themodel, which is a simple and elegant end-to-end architecture, outperformsstandard data augmentation and fine-tuning techniques for the task ofimage-level classification as well.
arxiv-17100-221 | Geometric Scene Parsing with Hierarchical LSTM | http://arxiv.org/pdf/1604.01931v2.pdf | author:Zhanglin Peng, Ruimao Zhang, Xiaodan Liang, Xiaobai Liu, Liang Lin category:cs.CV published:2016-04-07 summary:This paper addresses the problem of geometric scene parsing, i.e.simultaneously labeling geometric surfaces (e.g. sky, ground and verticalplane) and determining the interaction relations (e.g. layering, supporting,siding and affinity) between main regions. This problem is more challengingthan the traditional semantic scene labeling, as recovering geometricstructures necessarily requires the rich and diverse contextual information. Toachieve these goals, we propose a novel recurrent neural network model, namedHierarchical Long Short-Term Memory (H-LSTM). It contains two coupledsub-networks: the Pixel LSTM (P-LSTM) and the Multi-scale Super-pixel LSTM(MS-LSTM) for handling the surface labeling and relation prediction,respectively. The two sub-networks provide complementary information to eachother to exploit hierarchical scene contexts, and they are jointly optimizedfor boosting the performance. Our extensive experiments show that our model iscapable of parsing scene geometric structures and outperforming severalstate-of-the-art methods by large margins. In addition, we show promising 3Dreconstruction results from the still images based on the geometric parsing.
arxiv-17100-222 | Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves | http://arxiv.org/pdf/1604.02038v2.pdf | author:Fei Tian, Bin Gao, Di He, Tie-Yan Liu category:cs.LG cs.CL cs.IR published:2016-04-07 summary:We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic modelthat assumes the generation of each word within a sentence to depend on boththe topic of the sentence and the whole history of its preceding words in thesentence. Different from conventional topic models that largely ignore thesequential order of words or their topic coherence, SLRTM gives fullcharacterization to them by using a Recurrent Neural Networks (RNN) basedframework. Experimental results have shown that SLRTM outperforms severalstrong baselines on various tasks. Furthermore, SLRTM can automaticallygenerate sentences given a topic (i.e., topics to sentences), which is a keytechnology for real world applications such as personalized short textconversation.
arxiv-17100-223 | Feature-Based Diversity Optimization for Problem Instance Classification | http://arxiv.org/pdf/1510.08568v2.pdf | author:Wanru Gao, Samadhi Nallaperuma, Frank Neumann category:cs.NE cs.AI published:2015-10-29 summary:Understanding the behaviour of heuristic search methods is a challenge. Thiseven holds for simple local search methods such as 2-OPT for the TravelingSalesperson problem. In this paper, we present a general framework that is ableto construct a diverse set of instances that are hard or easy for a givensearch heuristic. Such a diverse set is obtained by using an evolutionaryalgorithm for constructing hard or easy instances that are diverse with respectto different features of the underlying problem. Examining the constructedinstance sets, we show that many combinations of two or three features give agood classification of the TSP instances in terms of whether they are hard tobe solved by 2-OPT.
arxiv-17100-224 | Learning Structured Inference Neural Networks with Label Relations | http://arxiv.org/pdf/1511.05616v3.pdf | author:Hexiang Hu, Guang-Tong Zhou, Zhiwei Deng, Zicheng Liao, Greg Mori category:cs.CV cs.LG published:2015-11-17 summary:Images of scenes have various objects as well as abundant attributes, anddiverse levels of visual categorization are possible. A natural image could beassigned with fine-grained labels that describe major components,coarse-grained labels that depict high level abstraction or a set of labelsthat reveal attributes. Such categorization at different concept layers can bemodeled with label graphs encoding label information. In this paper, we exploitthis rich information with a state-of-art deep learning framework, and proposea generic structured model that leverages diverse label relations to improveimage classification performance. Our approach employs a novel stacked labelprediction neural network, capturing both inter-level and intra-level labelsemantics. We evaluate our method on benchmark image datasets, and empiricalresults illustrate the efficacy of our model.
arxiv-17100-225 | UTSig: A Persian Offline Signature Dataset | http://arxiv.org/pdf/1603.03235v3.pdf | author:Amir Soleimani, Kazim Fouladi, Babak N. Araabi category:cs.CV published:2016-03-10 summary:The crucial role of datasets in signature verification systems has motivatedresearchers to collect signature samples. However, with regard to the distinctcharacteristics of Persian signature, existing offline signature datasetscannot be used in Persian systems. This paper presents a new and public Persianoffline signature dataset, UTSig, which consists of 8280 images from 115classes that each class has 27 genuine, 3 opposite-hand signatures of thegenuine signer, and 42 skilled forgeries made by 6 forgers from 230 people.Compared to the other public datasets, UTSig has larger number of samples,classes, and forgers. Meanwhile its samples were collected by consideringvariables such as signing period, writing instrument, signature box size, andnumber of observable samples for forgers. Reviewing the main characteristics ofoffline signature datasets, we statistically show that Persian signatures hasfewer number of branch points and end points. We propose and test fourdifferent training and testing setups for UTSig. Results of our experimentsshow that training genuine samples along with opposite-hand signed samples andrandom forgeries can improve the performance in terms of equal error rate andminimum cost of log likelihood ratio which is an information theoreticcriterion.
arxiv-17100-226 | Characterizing Concept Drift | http://arxiv.org/pdf/1511.03816v6.pdf | author:Geoffrey I. Webb, Roy Hyde, Hong Cao, Hai Long Nguyen, Francois Petitjean category:cs.LG cs.AI published:2015-11-12 summary:Most machine learning models are static, but the world is dynamic, andincreasing online deployment of learned models gives increasing urgency to thedevelopment of efficient and effective mechanisms to address learning in thecontext of non-stationary distributions, or as it is commonly called conceptdrift. However, the key issue of characterizing the different types of driftthat can occur has not previously been subjected to rigorous definition andanalysis. In particular, while some qualitative drift categorizations have beenproposed, few have been formally defined, and the quantitative descriptionsrequired for precise and objective understanding of learner performance havenot existed. We present the first comprehensive framework for quantitativeanalysis of drift. This supports the development of the first comprehensive setof formal definitions of types of concept drift. The formal definitions clarifyambiguities and identify gaps in previous definitions, giving rise to a newcomprehensive taxonomy of concept drift types and a solid foundation forresearch into mechanisms to detect and address concept drift.
arxiv-17100-227 | A Survey on Soft Subspace Clustering | http://arxiv.org/pdf/1409.5616v2.pdf | author:Zhaohong Deng, Kup-Sze Choi, Yizhang Jiang, Jun Wang, Shitong Wang category:cs.LG published:2014-09-19 summary:Subspace clustering (SC) is a promising clustering technology to identifyclusters based on their associations with subspaces in high dimensional spaces.SC can be classified into hard subspace clustering (HSC) and soft subspaceclustering (SSC). While HSC algorithms have been extensively studied and wellaccepted by the scientific community, SSC algorithms are relatively new butgaining more attention in recent years due to better adaptability. In thepaper, a comprehensive survey on existing SSC algorithms and the recentdevelopment are presented. The SSC algorithms are classified systematicallyinto three main categories, namely, conventional SSC (CSSC), independent SSC(ISSC) and extended SSC (XSSC). The characteristics of these algorithms arehighlighted and the potential future development of SSC is also discussed.
arxiv-17100-228 | Deep Online Convex Optimization by Putting Forecaster to Sleep | http://arxiv.org/pdf/1509.01851v2.pdf | author:David Balduzzi category:cs.LG cs.GT cs.NE published:2015-09-06 summary:Methods from convex optimization such as accelerated gradient descent arewidely used as building blocks for deep learning algorithms. However, thereasons for their empirical success are unclear, since neural networks are notconvex and standard guarantees do not apply. This paper develops the firstrigorous link between online convex optimization and error backpropagation onconvolutional networks. The first step is to introduce circadian games, a mildgeneralization of convex games with similar convergence properties. The mainresult is that error backpropagation on a convolutional network is equivalentto playing out a circadian game. It follows immediately that the waking-regretof players in the game (the units in the neural network) controls the overallrate of convergence of the network. Finally, we explore some implications ofthe results: (i) we describe the representations learned by a neural networkgame-theoretically, (ii) propose a learning setting at the level of individualunits that can be plugged into deep architectures, and (iii) propose a newapproach to adaptive model selection by applying bandit algorithms to choosewhich players to wake on each round.
arxiv-17100-229 | Collaborative prediction with expert advice | http://arxiv.org/pdf/1603.06265v3.pdf | author:Paul Christiano category:cs.LG published:2016-03-20 summary:Many practical learning systems aggregate data across many users, whilelearning theory traditionally considers a single learner who trusts all oftheir observations. A case in point is the foundational learning problem ofprediction with expert advice. To date, there has been no theoretical study ofthe general collaborative version of prediction with expert advice, in whichmany users face a similar problem and would like to share their experiences inorder to learn faster. A key issue in this collaborative framework isrobustness: generally algorithms that aggregate data are vulnerable tomanipulation by even a small number of dishonest users. We exhibit the first robust collaborative algorithm for prediction withexpert advice. When all users are honest and have similar tastes our algorithmmatches the performance of pooling data and using a traditional algorithm. Butour algorithm also guarantees that adding users never significantly degradesperformance, even if the additional users behave adversarially. We achievestrong guarantees even when the overwhelming majority of users behaveadversarially. As a special case, our algorithm is extremely robust tovariation amongst the users.
arxiv-17100-230 | Transfer Learning for Low-Resource Neural Machine Translation | http://arxiv.org/pdf/1604.02201v1.pdf | author:Barret Zoph, Deniz Yuret, Jonathan May, Kevin Knight category:cs.CL published:2016-04-08 summary:The encoder-decoder framework for neural machine translation (NMT) has beenshown effective in large data scenarios, but is much less effective forlow-resource languages. We present a transfer learning method thatsignificantly improves Bleu scores across a range of low-resource languages.Our key idea is to first train a high-resource language pair (the parentmodel), then transfer some of the learned parameters to the low-resource pair(the child model) to initialize and constrain training. Using our transferlearning method we improve baseline NMT models by an average of 5.6 Bleu onfour low-resource language pairs. Ensembling and unknown word replacement addanother 2 Bleu which brings the NMT performance on low-resource machinetranslation close to a strong syntax based machine translation (SBMT) system,exceeding its performance on one language pair. Additionally, using thetransfer learning model for re-scoring, we can improve the SBMT system by anaverage of 1.3 Bleu, improving the state-of-the-art on low-resource machinetranslation.
arxiv-17100-231 | Automatic Annotation of Structured Facts in Images | http://arxiv.org/pdf/1604.00466v3.pdf | author:Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed Elgammal category:cs.CL cs.CV published:2016-04-02 summary:Motivated by the application of fact-level image understanding, we present anautomatic method for data collection of structured visual facts from imageswith captions. Example structured facts include attributed objects (e.g.,<flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man,walking, dog>), and positional information (e.g., <vase, on, table>). Thecollected annotations are in the form of fact-image pairs (e.g.,<man, walking,dog> and an image region containing this fact). With a language approach, theproposed method is able to collect hundreds of thousands of visual factannotations with accuracy of 83% according to human judgment. Our methodautomatically collected more than 380,000 visual fact annotations and more than110,000 unique visual facts from images with captions and localized them inimages in less than one day of processing time on standard CPU platforms.
arxiv-17100-232 | Family in the Wild (FIW): A Large-scale Kinship Recognition Database | http://arxiv.org/pdf/1604.02182v1.pdf | author:Joseph P Robinson, Ming Shao, Yue Wu, Yun Fu category:cs.CV published:2016-04-07 summary:We introduce a large-scale dataset for visual kin-based problems, the Familyin the Wild (FIW) dataset. Motivated by the lack of a single, unified imagedataset available for kinship tasks, our goal is to provide a dataset thatcaptivates the interest of the research community, i.e., large enough tosupport multiple tasks for evaluation. For this, we collected and labelled thelargest set of family images to date, with only a small team and an efficientlabelling tool that was designed to optimize the process of marking complexhierarchical relationships, attributes, and local label information in familyphotos. We experimentally compare our dataset the existing kinship imagedatasets, and demonstrate the practical value of the newly collected FIWdataset. We also demonstrate that using a pre-trained convolutional neuralnetwork (CNN) as an off-the-shelf feature extractor as performing better thantraditional feature types used for kinship based tasks in the visual domain. Wealso measure human performance and show their performance does not match up tothat of machine vision algorithms.
arxiv-17100-233 | The Limitations of Optimization from Samples | http://arxiv.org/pdf/1512.06238v2.pdf | author:Eric Balkanski, Aviad Rubinstein, Yaron Singer category:cs.DS cs.DM cs.LG published:2015-12-19 summary:In this paper we consider the following question: can we optimize decisionson models learned from data and be guaranteed that we achieve desirableoutcomes? We formalize this question through a novel framework calledoptimization from samples (OPS). In the OPS framework, we are given sampledvalues of a function drawn from some distribution and the objective is tooptimize the function under some constraint. We show that there are classes of functions which have desirable learnabilityand optimizability guarantees and for which no reasonable approximation foroptimization from samples is achievable. In particular, our main result showsthat even for maximization of coverage functions under a cardinality constraint$k$, there exists a hypothesis class of functions that cannot be approximatedwithin a factor of $n^{-1/4 + \epsilon}$ (for any constant $\epsilon > 0$) ofthe optimal solution, from samples drawn from the uniform distribution over allsets of size at most $k$. In the general case of monotone submodular functions,we show an $n^{-1/3 + \epsilon}$ lower bound and an almost matching$\tilde{\Omega}(n^{-1/3})$-optimization from samples algorithm. On the positiveside, if a monotone subadditive function has bounded curvature we obtaindesirable guarantees. We also show that additive and unit-demand functions canbe optimized from samples to within arbitrarily good precision, and that budgetadditive functions can be optimized from samples to a factor of 1/2.
arxiv-17100-234 | A Kernel-Based Nonparametric Test for Anomaly Detection over Line Networks | http://arxiv.org/pdf/1404.0298v2.pdf | author:Shaofeng Zou, Yingbin Liang, H. Vincent Poor category:cs.IT math.IT stat.ML published:2014-04-01 summary:The nonparametric problem of detecting existence of an anomalous intervalover a one dimensional line network is studied. Nodes corresponding to ananomalous interval (if exists) receive samples generated by a distribution q,which is different from the distribution p that generates samples for othernodes. If anomalous interval does not exist, then all nodes receive samplesgenerated by p. It is assumed that the distributions p and q are arbitrary, andare unknown. In order to detect whether an anomalous interval exists, a test isbuilt based on mean embeddings of distributions into a reproducing kernelHilbert space (RKHS) and the metric of maximummean discrepancy (MMD). It isshown that as the network size n goes to infinity, if the minimum length ofcandidate anomalous intervals is larger than a threshold which has the orderO(log n), the proposed test is asymptotically successful, i.e., the probabilityof detection error approaches zero asymptotically. An efficient algorithm toperform the test with substantial computational complexity reduction isproposed, and is shown to be asymptotically successful if the condition on theminimum length of candidate anomalous interval is satisfied. Numerical resultsare provided, which are consistent with the theoretical results.
arxiv-17100-235 | A Semi-Lagrangian two-level preconditioned Newton-Krylov solver for constrained diffeomorphic image registration | http://arxiv.org/pdf/1604.02153v1.pdf | author:Andreas Mang, George Biros category:math.OC cs.CV published:2016-04-07 summary:We propose an efficient numerical algorithm for the solution of diffeomorphicimage registration problems. We use an optimization formulation constrained bya partial differential equation (PDE), where the constraints are a scalartransport equation. We use a pseudospectral discretization in space and second-order accuratesemi-Lagrangian time stepping scheme for the transport PDE. We solve for astationary velocity field using a preconditioned, globalized, matrix-freeNewton-Krylov scheme. We propose and test a two-level Hessian preconditioner.We consider two strategies for inverting the preconditioner on the coarse grid:a nested preconditioned conjugate gradient method (exact solve) and a nestedChebyshev iterative method (inexact solve) with a fixed number of iterations. We test the performance of our solver in different synthetic and real-worldtwo-dimensional application scenarios. We study grid convergence andcomputational efficiency of our new scheme. We compare the performance of oursolver against our initial implementation that uses the same spatialdiscretization but a standard, explicit, second-order Runge-Kutta scheme forthe numerical time integration of the transport equations and a single-levelpreconditioner. Our improved scheme delivers significant speedups over ouroriginal implementation. As a highlight, we observe a 20$\times$ speedup for atwo dimensional, real world multi-subject medical image registration problem.
arxiv-17100-236 | A MultiPath Network for Object Detection | http://arxiv.org/pdf/1604.02135v1.pdf | author:Sergey Zagoruyko, Adam Lerer, Tsung-Yi Lin, Pedro O. Pinheiro, Sam Gross, Soumith Chintala, Piotr Dollár category:cs.CV published:2016-04-07 summary:The recent MS COCO object detection dataset presents several new challengesfor object detection. In particular, it contains objects at a broad range ofscales, less prototypical images, and requires more precise localization. Toaddress these challenges, we test three modifications to the standard FastR-CNN object detector: (1) skip connections that give the detector access tofeatures at multiple network layers, (2) a foveal structure to exploit objectcontext at multiple object resolutions, and (3) an integral loss function andcorresponding network adjustment that improve localization. The result of thesemodifications is that information can flow along multiple paths in our network,including through features from multiple network layers and from multipleobject views. We refer to our modified classifier as a "MultiPath" network. Wecouple our MultiPath network with DeepMask object proposals, which are wellsuited for localization and small objects, and adapt our pipeline to predictsegmentation masks in addition to bounding boxes. The combined system improvesresults over the baseline Fast R-CNN detector with Selective Search by 66%overall and by 4x on small objects. It placed second in both the COCO 2015detection and segmentation challenges.
arxiv-17100-237 | Horizon Lines in the Wild | http://arxiv.org/pdf/1604.02129v1.pdf | author:Scott Workman, Menghua Zhai, Nathan Jacobs category:cs.CV published:2016-04-07 summary:The horizon line is an important property for a wide variety of imageunderstanding tasks. As such, many methods have been introduced to estimate thehorizon line from a single image, primarily geometric methods which assume thepresence of specific cues in the scene (e.g., vanishing points). These purelygeometric methods are limited in their real-world capability, require extensivetuning, and are tested on benchmark datasets designed to showcase theirability. We introduce a large, realistic evaluation dataset, Horizon Lines inthe Wild (HLW), containing natural images with labeled horizon lines.
arxiv-17100-238 | Multilevel Weighted Support Vector Machine for Classification on Healthcare Data with Missing Values | http://arxiv.org/pdf/1604.02123v1.pdf | author:Talayeh Razzaghi, Oleg Roderick, Ilya Safro, Nicholas Marko category:stat.ML cs.LG stat.AP published:2016-04-07 summary:This work is motivated by the needs of predictive analytics on healthcaredata as represented by Electronic Medical Records. Such data is invariablyproblematic: noisy, with missing entries, with imbalance in classes ofinterests, leading to serious bias in predictive modeling. Since standard datamining methods often produce poor performance measures, we argue fordevelopment of specialized techniques of data-preprocessing and classification.In this paper, we propose a new method to simultaneously classify largedatasets and reduce the effects of missing values. It is based on a multilevelframework of the cost-sensitive SVM and the expected maximization imputationmethod for missing values, which relies on iterated regression analyses. Wecompare classification results of multilevel SVM-based algorithms on publicbenchmark datasets with imbalanced classes and missing values as well as realdata in health applications, and show that our multilevel SVM-based methodproduces fast, and more accurate and robust classification results.
arxiv-17100-239 | A Single Model Explains both Visual and Auditory Precortical Coding | http://arxiv.org/pdf/1602.08486v2.pdf | author:Honghao Shan, Matthew H. Tong, Garrison W. Cottrell category:q-bio.NC cs.CV cs.LG cs.NE published:2016-02-26 summary:Precortical neural systems encode information collected by the senses, butthe driving principles of the encoding used have remained a subject of debate.We present a model of retinal coding that is based on three constraints:information preservation, minimization of the neural wiring, and responseequalization. The resulting novel version of sparse principal componentsanalysis successfully captures a number of known characteristics of the retinalcoding system, such as center-surround receptive fields, color opponencychannels, and spatiotemporal responses that correspond to magnocellular andparvocellular pathways. Furthermore, when trained on auditory data, the samemodel learns receptive fields well fit by gammatone filters, commonly used tomodel precortical auditory coding. This suggests that efficient coding may be aunifying principle of precortical encoding across modalities.
arxiv-17100-240 | Trajectory Aligned Features For First Person Action Recognition | http://arxiv.org/pdf/1604.02115v1.pdf | author:Suriya Singh, Chetan Arora, C. V. Jawahar category:cs.CV published:2016-04-07 summary:Egocentric videos are characterised by their ability to have the first personview. With the popularity of Google Glass and GoPro, use of egocentric videosis on the rise. Recognizing action of the wearer from egocentric videos is animportant problem. Unstructured movement of the camera due to natural headmotion of the wearer causes sharp changes in the visual field of the egocentriccamera causing many standard third person action recognition techniques toperform poorly on such videos. Objects present in the scene and hand gesturesof the wearer are the most important cues for first person action recognitionbut are difficult to segment and recognize in an egocentric video. We propose anovel representation of the first person actions derived from featuretrajectories. The features are simple to compute using standard point trackingand does not assume segmentation of hand/objects or recognizing object or handpose unlike in many previous approaches. We train a bag of words classifierwith the proposed features and report a performance improvement of more than11% on publicly available datasets. Although not designed for the particularcase, we show that our technique can also recognize wearer's actions when handsor objects are not visible.
arxiv-17100-241 | Solving Optimization Problems by the Spatial Public Goods Game | http://arxiv.org/pdf/1604.02929v1.pdf | author:Marco Alberto Javarone category:physics.soc-ph cs.GT cs.NE published:2016-04-07 summary:We introduce a method based on the spatial Public Goods Game for solvingoptimization tasks. In particular, we focus on the Traveling Salesman Problem,i.e., a problem whose search space exponentially grows increasing the number ofcities, then becoming NP-hard. The proposed method considers a population whoseagents are provided with a random solution to the given problem. Then, agentsinteract by playing the Public Goods Game using the fitness of their solutionas currency of the game. In doing so, agents with better solutions providehigher contributions, while agents with lower ones tend to imitate the solutionof richer agents to increase their fitness. Numerical simulations show that theproposed method allows to compute exact solutions, and suboptimal ones, in theconsidered search spaces. As result, beyond to propose a new heuristic forcombinatorial optimization tasks, our work aims to highlight the potentialityof evolutionary game theory outside its current horizons.
arxiv-17100-242 | A robust autoassociative memory with coupled networks of Kuramoto-type oscillators | http://arxiv.org/pdf/1604.02085v1.pdf | author:Daniel Heger, Katharina Krischer category:nlin.AO cs.CV cs.NE published:2016-04-07 summary:Uncertain recognition success, unfavorable scaling of connection complexityor dependence on complex external input impair the usefulness of currentoscillatory neural networks for pattern recognition or restrict technicalrealizations to small networks. We propose a new network architecture ofcoupled oscillators for pattern recognition which shows none of the mentionedaws. Furthermore we illustrate the recognition process with simulation resultsand analyze the new dynamics analytically: Possible output patterns areisolated attractors of the system. Additionally, simple criteria forrecognition success are derived from a lower bound on the basins of attraction.
arxiv-17100-243 | The Cityscapes Dataset for Semantic Urban Scene Understanding | http://arxiv.org/pdf/1604.01685v2.pdf | author:Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele category:cs.CV published:2016-04-06 summary:Visual understanding of complex urban street scenes is an enabling factor fora wide range of applications. Object detection has benefited enormously fromlarge-scale datasets, especially in the context of deep learning. For semanticurban scene understanding, however, no current dataset adequately captures thecomplexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scaledataset to train and test approaches for pixel-level and instance-levelsemantic labeling. Cityscapes is comprised of a large, diverse set of stereovideo sequences recorded in streets from 50 different cities. 5000 of theseimages have high quality pixel-level annotations; 20000 additional images havecoarse annotations to enable methods that leverage large volumes ofweakly-labeled data. Crucially, our effort exceeds previous attempts in termsof dataset size, annotation richness, scene variability, and complexity. Ouraccompanying empirical study provides an in-depth analysis of the datasetcharacteristics, as well as a performance evaluation of severalstate-of-the-art approaches based on our benchmark.
arxiv-17100-244 | 3-D Hand Pose Estimation from Kinect's Point Cloud Using Appearance Matching | http://arxiv.org/pdf/1604.02032v1.pdf | author:Pasquale Coscia, Francesco A. N. Palmieri, Francesco Castaldo, Alberto Cavallo category:cs.CV published:2016-04-07 summary:We present a novel appearance-based approach for pose estimation of a humanhand using the point clouds provided by the low-cost Microsoft Kinect sensor.Both the free-hand case, in which the hand is isolated from the surroundingenvironment, and the hand-object case, in which the different types ofinteractions are classified, have been considered. The hand-object case isclearly the most challenging task having to deal with multiple tracks. Theapproach proposed here belongs to the class of partial pose estimation wherethe estimated pose in a frame is used for the initialization of the next one.The pose estimation is obtained by applying a modified version of the IterativeClosest Point (ICP) algorithm to synthetic models to obtain the rigidtransformation that aligns each model with respect to the input data. Theproposed framework uses a "pure" point cloud as provided by the Kinect sensorwithout any other information such as RGB values or normal vector components.For this reason, the proposed method can also be applied to data obtained fromother types of depth sensor, or RGB-D camera.
arxiv-17100-245 | Neural Architectures for Named Entity Recognition | http://arxiv.org/pdf/1603.01360v3.pdf | author:Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer category:cs.CL published:2016-03-04 summary:State-of-the-art named entity recognition systems rely heavily onhand-crafted features and domain-specific knowledge in order to learneffectively from the small, supervised training corpora that are available. Inthis paper, we introduce two new neural architectures---one based onbidirectional LSTMs and conditional random fields, and the other thatconstructs and labels segments using a transition-based approach inspired byshift-reduce parsers. Our models rely on two sources of information aboutwords: character-based word representations learned from the supervised corpusand unsupervised word representations learned from unannotated corpora. Ourmodels obtain state-of-the-art performance in NER in four languages withoutresorting to any language-specific knowledge or resources such as gazetteers.
arxiv-17100-246 | LocNet: Improving Localization Accuracy for Object Detection | http://arxiv.org/pdf/1511.07763v2.pdf | author:Spyros Gidaris, Nikos Komodakis category:cs.CV cs.LG cs.NE published:2015-11-24 summary:We propose a novel object localization methodology with the purpose ofboosting the localization accuracy of state-of-the-art object detectionsystems. Our model, given a search region, aims at returning the bounding boxof an object of interest inside this region. To accomplish its goal, it relieson assigning conditional probabilities to each row and column of this region,where these probabilities provide useful information regarding the location ofthe boundaries of the object inside the search region and allow the accurateinference of the object bounding box under a simple probabilistic framework. For implementing our localization model, we make use of a convolutionalneural network architecture that is properly adapted for this task, calledLocNet. We show experimentally that LocNet achieves a very significantimprovement on the mAP for high IoU thresholds on PASCAL VOC2007 test set andthat it can be very easily coupled with recent state-of-the-art objectdetection systems, helping them to boost their performance. Finally, wedemonstrate that our detection approach can achieve high detection accuracyeven when it is given as input a set of sliding windows, thus proving that itis independent of box proposal methods.
arxiv-17100-247 | Edge Detection Based Shape Identification | http://arxiv.org/pdf/1604.02030v1.pdf | author:Vivek Kumar, Sumit Pandey, Amrindra Pal, Sandeep Sharma category:cs.CV published:2016-04-07 summary:Image recognition is the need of the hour. In order to be able to recognizean image, it is of immense importance that the image should be distinguishablefrom the background. In the present work, an approach is presented forautomatic detection and recognition of regular 2D shapes in low noiseenvironments. The work has a large number of direct applications in the realworld. The algorithm proposed is based on locating the edges and thus, in turncalculating the area of the object helps in identification of a specifiedshape. The results were simulated using MATLAB tool are encouraging andvalidate the proposed algorithm. Index Terms: Edge Detection, Area Calculation, Shape Detection, ObjectRecognition
arxiv-17100-248 | Longitudinal Analysis of Discussion Topics in an Online Breast Cancer Community using Convolutional Neural Networks | http://arxiv.org/pdf/1603.08458v3.pdf | author:Shaodian Zhang, Edouard Grave, Elizabeth Sklar, Noemie Elhadad category:cs.CL cs.CY cs.SI published:2016-03-28 summary:Identifying topics of discussions in online health communities (OHC) iscritical to various applications, but can be difficult because topics of OHCcontent are usually heterogeneous and domain-dependent. In this paper, weprovide a multi-class schema, an annotated dataset, and supervised classifiersbased on convolutional neural network (CNN) and other models for the task ofclassifying discussion topics. We apply the CNN classifier to the most popularbreast cancer online community, and carry out a longitudinal analysis to showtopic distributions and topic changes throughout members' participation. Ourexperimental results suggest that CNN outperforms other classifiers in the taskof topic classification, and that certain trajectories can be detected withrespect to topic changes.
arxiv-17100-249 | Combinatorial Topic Models using Small-Variance Asymptotics | http://arxiv.org/pdf/1604.02027v1.pdf | author:Ke Jiang, Suvrit Sra, Brian Kulis category:cs.LG cs.CL stat.ML published:2016-04-07 summary:Topic models have emerged as fundamental tools in unsupervised machinelearning. Most modern topic modeling algorithms take a probabilistic view andderive inference algorithms based on Latent Dirichlet Allocation (LDA) or itsvariants. In contrast, we study topic modeling as a combinatorial optimizationproblem, and derive its objective function from LDA by passing to thesmall-variance limit. We minimize the derived objective by using ideas fromcombinatorial optimization, which results in a new, fast, and high-qualitytopic modeling algorithm. In particular, we show the surprising result that ouralgorithm can outperform all major LDA-based topic modeling approaches, evenwhen the data are sampled from an LDA model and true hyper-parameters areprovided to these competitors. These results make a strong case that topicmodels need not be limited to a probabilistic view.
arxiv-17100-250 | Distributed Information-Theoretic Biclustering | http://arxiv.org/pdf/1602.04605v2.pdf | author:Georg Pichler, Pablo Piantanida, Gerald Matz category:cs.IT cs.LG math.IT published:2016-02-15 summary:We study a novel multi-terminal source coding setup motivated by thebiclustering problem. Two separate encoders observe two stationary, memorylesssources $X^n$ and $Z^n$, respectively. The goal is to find rate-limitedencodings $f(x^n)$ and $g(z^n)$ that maximize the mutual information$I(f(X^n);g(Z^n))/n$. We present non-trivial outer and inner bounds on theachievable region for this problem. These bounds are also generalized to anarbitrary collection of stationary, memoryless sources. The considered problemis intimately connected to distributed hypothesis testing against independenceunder communication constraints, and hence our results are expected to apply tothat setting as well.
arxiv-17100-251 | Convex Sparse Spectral Clustering: Single-view to Multi-view | http://arxiv.org/pdf/1511.06860v2.pdf | author:Canyi Lu, Shuicheng Yan, Zhouchen Lin category:cs.CV published:2015-11-21 summary:Spectral Clustering (SC) is one of the most widely used methods for dataclustering. It first finds a low-dimensonal embedding $\U$ of data by computingthe eigenvectors of the normalized Laplacian matrix, and then performs k-meanson $\U^\top$ to get the final clustering result. In this work, we observe that,in the ideal case, $\U\U^\top$ should be block diagonal and thus sparse.Therefore we propose the Sparse Spectral Clustering (SSC) method which extendsSC with sparse regularization on $\U\U^\top$. To address the computationalissue of the nonconvex SSC model, we propose a novel convex relaxation of SSCbased on the convex hull of the fixed rank projection matrices. Then the convexSSC model can be efficiently solved by the Alternating Direction Method of\canyi{Multipliers} (ADMM). Furthermore, we propose the Pairwise SparseSpectral Clustering (PSSC) which extends SSC to boost the clusteringperformance by using the multi-view information of data. Experimentalcomparisons with several baselines on real-world datasets testify to theefficacy of our proposed methods.
arxiv-17100-252 | Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies | http://arxiv.org/pdf/1604.01980v1.pdf | author:Emanuel Laude, Thomas Möllenhoff, Michael Moeller, Jan Lellmann, Daniel Cremers category:cs.CV math.OC published:2016-04-07 summary:Convex relaxations of nonconvex multilabel problems have been demonstrated toproduce superior (provably optimal or near-optimal) solutions to a variety ofclassical computer vision problems. Yet, they are of limited practical use asthey require a fine discretization of the label space, entailing a huge demandin memory and runtime. In this work, we propose the first sublabel accurateconvex relaxation for vectorial multilabel problems. The key idea is that weapproximate the dataterm of the vectorial labeling problem in a piecewiseconvex (rather than piecewise linear) manner. As a result we have a morefaithful approximation of the original cost function that provides a meaningfulinterpretation for the fractional solutions of the relaxed convex problem. Innumerous experiments on large-displacement optical flow estimation and on colorimage denoising we demonstrate that the computed solutions have superiorquality while requiring much lower memory and runtime.
arxiv-17100-253 | An Adaptive Resample-Move Algorithm for Estimating Normalizing Constants | http://arxiv.org/pdf/1604.01972v1.pdf | author:Marco Fraccaro, Ulrich Paquet, Ole Winther category:stat.ML published:2016-04-07 summary:The estimation of normalizing constants is a fundamental step inprobabilistic model comparison. Sequential Monte Carlo methods may be used forthis task and have the advantage of being inherently parallelizable. However,the standard choice of using a fixed number of particles at each iteration issuboptimal because some steps will contribute disproportionately to thevariance of the estimate. We introduce an adaptive version of the Resample-Movealgorithm, in which the particle set is adaptively expanded whenever a betterapproximation of an intermediate distribution is needed. The algorithm buildson the expression for the optimal number of particles and the correspondingminimum variance found under ideal conditions. Benchmark results on challengingGaussian Process Classification and Restricted Boltzmann Machine applicationsshow that Adaptive Resample-Move (ARM) estimates the normalizing constant witha smaller variance, using less computational resources, than eitherResample-Move with a fixed number of particles or Annealed Importance Sampling.A further advantage over Annealed Importance Sampling is that ARM is easier totune.
arxiv-17100-254 | Correlated and Individual Multi-Modal Deep Learning for RGB-D Object Recognition | http://arxiv.org/pdf/1604.01655v2.pdf | author:Ziyan Wang, Ruogu Lin, Jiwen Lu, Jianjiang Feng, Jie zhou category:cs.CV published:2016-04-06 summary:In this paper, we propose a new correlated and individual multi-modal deeplearning (CIMDL) method for RGB-D object recognition. Unlike most conventionalRGB-D object recognition methods which extract features from the RGB and depthchannels individually, our CIMDL jointly learns feature representations fromraw RGB-D data with a pair of deep neural networks, so that the sharable andmodal-specific information can be simultaneously exploited. Specifically, weconstruct a pair of deep convolutional neural networks (CNNs) for the RGB anddepth data, and concatenate them at the top layer of the network with a lossfunction which learns a new feature space where both correlated part and theindividual part of the RGB-D information are well modelled. The parameters ofthe whole networks are updated by using the back-propagation criterion.Experimental results on two widely used RGB-D object image benchmark datasetsclearly show that our method outperforms state-of-the-arts.
arxiv-17100-255 | On the Complexity of Inner Product Similarity Join | http://arxiv.org/pdf/1510.02824v3.pdf | author:Thomas D. Ahle, Rasmus Pagh, Ilya Razenshteyn, Francesco Silvestri category:cs.DS cs.DB cs.LG published:2015-10-09 summary:A number of tasks in classification, information retrieval, recommendationsystems, and record linkage reduce to the core problem of inner productsimilarity join (IPS join): identifying pairs of vectors in a collection thathave a sufficiently large inner product. IPS join is well understood whenvectors are normalized and some approximation of inner products is allowed.However, the general case where vectors may have any length appears much morechallenging. Recently, new upper bounds based on asymmetric locality-sensitivehashing (ALSH) and asymmetric embeddings have emerged, but little has beenknown on the lower bound side. In this paper we initiate a systematic study ofinner product similarity join, showing new lower and upper bounds. Our mainresults are: * Approximation hardness of IPS join in subquadratic time, assuming thestrong exponential time hypothesis. * New upper and lower bounds for (A)LSH-based algorithms. In particular, weshow that asymmetry can be avoided by relaxing the LSH definition to onlyconsider the collision probability of distinct elements. * A new indexing method for IPS based on linear sketches, implying that ourhardness results are not far from being tight. Our technical contributions include new asymmetric embeddings that may be ofindependent interest. At the conceptual level we strive to provide greaterclarity, for example by distinguishing among signed and unsigned variants ofIPS join and shedding new light on the effect of asymmetry.
arxiv-17100-256 | Monitoring Chinese Population Migration in Consecutive Weekly Basis from Intra-city scale to Inter-province scale by Didi's Bigdata | http://arxiv.org/pdf/1604.01955v1.pdf | author:Renyu Zhao category:stat.ML published:2016-04-07 summary:Population migration is valuable information which leads to proper decisionin urban-planning strategy, massive investment, and many other fields. Forinstance, inter-city migration is a posterior evidence to see if thegovernment's constrain of population works, and inter-community immigrationmight be a prior evidence of real estate price hike. With timely data, it isalso impossible to compare which city is more favorable for the people, supposethe cities release different new regulations, we could also compare thecustomers of different real estate development groups, where they come from,where they probably will go. Unfortunately these data was not available. In this paper, leveraging the data generated by positioning team in Didi, wepropose a novel approach that timely monitoring population migration fromcommunity scale to provincial scale. Migration can be detected as soon as in aweek. It could be faster, the setting of a week is for statistical purpose. Amonitoring system is developed, then applied nation wide in China, someobservations derived from the system will be presented in this paper. This new method of migration perception is origin from the insight thatnowadays people mostly moving with their personal Access Point (AP), also knownas WiFi hotspot. Assume that the ratio of AP moving to the migration ofpopulation is constant, analysis of comparative population migration would befeasible. More exact quantitative research would also be done with few sampleresearch and model regression. The procedures of processing data includes many steps: eliminating the impactof pseudo-migration AP, for instance pocket WiFi, and second-hand tradedrouter; distinguishing moving of population with moving of companies;identifying shifting of AP by the finger print clusters, etc..
arxiv-17100-257 | Deep Online Convex Optimization with Gated Games | http://arxiv.org/pdf/1604.01952v1.pdf | author:David Balduzzi category:cs.LG cs.GT cs.NE stat.ML published:2016-04-07 summary:Methods from convex optimization are widely used as building blocks for deeplearning algorithms. However, the reasons for their empirical success areunclear, since modern convolutional networks (convnets), incorporatingrectifier units and max-pooling, are neither smooth nor convex. Standardguarantees therefore do not apply. This paper provides the first convergencerates for gradient descent on rectifier convnets. The proof utilizes theparticular structure of rectifier networks which consists in binaryactive/inactive gates applied on top of an underlying linear network. Theapproach generalizes to max-pooling, dropout and maxout. In other words, toprecisely the neural networks that perform best empirically. The key step is tointroduce gated games, an extension of convex games with similar convergenceproperties that capture the gating function of rectifiers. The main result isthat rectifier convnets converge to a critical point at a rate controlled bythe gated-regret of the units in the network. Corollaries of the main resultinclude: (i) a game-theoretic description of the representations learned by aneural network; (ii) a logarithmic-regret algorithm for training neural nets;and (iii) a formal setting for analyzing conditional computation in neural netsthat can be applied to recently developed models of attention.
arxiv-17100-258 | Optimizing Performance of Recurrent Neural Networks on GPUs | http://arxiv.org/pdf/1604.01946v1.pdf | author:Jeremy Appleyard, Tomas Kocisky, Phil Blunsom category:cs.LG cs.NE published:2016-04-07 summary:As recurrent neural networks become larger and deeper, training times forsingle networks are rising into weeks or even months. As such there is asignificant incentive to improve the performance and scalability of thesenetworks. While GPUs have become the hardware of choice for training anddeploying recurrent models, the implementations employed often make use of onlybasic optimizations for these architectures. In this article we demonstratethat by exposing parallelism between operations within the network, an order ofmagnitude speedup across a range of network sizes can be achieved over a naiveimplementation. We describe three stages of optimization that have beenincorporated into the fifth release of NVIDIA's cuDNN: firstly optimizing asingle cell, secondly a single layer, and thirdly the entire network.
arxiv-17100-259 | Long Short-Term Memory-Networks for Machine Reading | http://arxiv.org/pdf/1601.06733v5.pdf | author:Jianpeng Cheng, Li Dong, Mirella Lapata category:cs.CL cs.NE published:2016-01-25 summary:Machine reading, the automatic understanding of text, remains a challengingtask of great value for NLP applications. We propose a machine reader whichprocesses text incrementally from left to right, while linking the current wordto previous words stored in memory and implicitly discovering lexicaldependencies facilitating understanding. The reader is equipped with a LongShort-Term Memory architecture, which differs from previous work in that it hasa memory tape (instead of a memory cell) for adaptively storing pastinformation without severe information compression. We also integrate ourreader with a new attention mechanism in encoder-decoder architecture.Experiments on language modeling, sentiment analysis, and natural languageinference show that our model matches or outperforms the state of the art.
arxiv-17100-260 | Relaxed Leverage Sampling for Low-rank Matrix Completion | http://arxiv.org/pdf/1503.06379v3.pdf | author:Abhisek Kundu category:cs.IT cs.LG math.IT stat.ML published:2015-03-22 summary:We consider the problem of exact recovery of any $m\times n$ matrix of rank$\varrho$ from a small number of observed entries via the standard nuclear normminimization framework. Such low-rank matrices have degrees of freedom$(m+n)\varrho - \varrho^2$. We show that any arbitrary low-rank matrices can berecovered exactly from a $\Theta\left(((m+n)\varrho -\varrho^2)\log^2(m+n)\right)$ randomly sampled entries, thus matching the lowerbound on the required number of entries (in terms of degrees of freedom), withan additional factor of $O(\log^2(m+n))$. To achieve this bound on sample sizewe observe each entry with probabilities proportional to the sum ofcorresponding row and column leverage scores, minus their product. We show thatthis relaxation in sampling probabilities (as opposed to sum of leverage scoresin Chen et al, 2014) can give us an $O(\varrho^2\log^2(m+n))$ additiveimprovement on the (best known) sample size obtained by Chen et al, 2014, forthe nuclear norm minimization. Experiments on real data corroborate thetheoretical improvement on sample size. Further, exact recovery of $(a)$incoherent matrices (with restricted leverage scores), and $(b)$ matrices withonly one of the row or column spaces to be incoherent, can be performed usingour relaxed leverage score sampling, via nuclear norm minimization, withoutknowing the leverage scores a priori. In such settings also we can achieveimprovement on sample size.
arxiv-17100-261 | Neural Headline Generation with Minimum Risk Training | http://arxiv.org/pdf/1604.01904v1.pdf | author:Ayana, Shiqi Shen, Zhiyuan Liu, Maosong Sun category:cs.CL published:2016-04-07 summary:Automatic headline generation is an important research area within textsummarization and sentence compression. Recently, neural headline generationmodels have been proposed to take advantage of well-trained neural networks inlearning sentence representations and mapping sequence to sequence.Nevertheless, traditional neural network encoder utilizes maximum likelihoodestimation for parameter optimization, which essentially constraints theexpected training objective within word level instead of sentence level.Moreover, the performance of model prediction significantly relies on trainingdata distribution. To overcome these drawbacks, we employ minimum risk trainingstrategy in this paper, which directly optimizes model parameters with respectto evaluation metrics and statistically leads to significant improvements forheadline generation. Experiment results show that our approach outperformsstate-of-the-art systems on both English and Chinese headline generation tasks.
arxiv-17100-262 | A Novel Scene Text Detection Algorithm Based On Convolutional Neural Network | http://arxiv.org/pdf/1604.01894v1.pdf | author:Xiaohang Ren, Kai Chen, Jun Sun category:cs.CV published:2016-04-07 summary:Candidate text region extraction plays a critical role in convolutionalneural network (CNN) based text detection from natural images. In this paper,we propose a CNN based scene text detection algorithm with a new text regionextractor. The so called candidate text region extractor I-MSER is based onMaximally Stable Extremal Region (MSER), which can improve the independency andcompleteness of the extracted candidate text regions. Design of I-MSER ismotivated by the observation that text MSERs have high similarity and are closeto each other. The independency of candidate text regions obtained by I-MSER isguaranteed by selecting the most representative regions from a MSER tree whichis generated according to the spatial overlapping relationship among the MSERs.A multi-layer CNN model is trained to score the confidence value of theextracted regions extracted by the I-MSER for text detection. The new textdetection algorithm based on I-MSER is evaluated with wide-used ICDAR 2011 and2013 datasets and shows improved detection performance compared to the existingalgorithms.
arxiv-17100-263 | Generative Concatenative Nets Jointly Learn to Write and Classify Reviews | http://arxiv.org/pdf/1511.03683v5.pdf | author:Zachary C. Lipton, Sharad Vikram, Julian McAuley category:cs.CL cs.LG published:2015-11-11 summary:A recommender system's basic task is to estimate how users will respond tounseen items. This is typically modeled in terms of how a user might rate aproduct, but here we aim to extend such approaches to model how a user wouldwrite about the product. To do so, we design a character-level Recurrent NeuralNetwork (RNN) that generates personalized product reviews. The networkconvincingly learns styles and opinions of nearly 1000 distinct authors, usinga large corpus of reviews from BeerAdvocate.com. It also tailors reviews todescribe specific items, categories, and star ratings. Using a simple inputreplication strategy, the Generative Concatenative Network (GCN) preserves thesignal of static auxiliary inputs across wide sequence intervals. Without anyadditional training, the generative model can classify reviews, identifying theauthor of the review, the product category, and the sentiment (rating), withremarkable accuracy. Our evaluation shows the GCN captures complex dynamics intext, such as the effect of negation, misspellings, slang, and largevocabularies gracefully absent any machinery explicitly dedicated to thepurpose.
arxiv-17100-264 | A CNN Based Scene Chinese Text Recognition Algorithm With Synthetic Data Engine | http://arxiv.org/pdf/1604.01891v1.pdf | author:Xiaohang Ren, Kai Chen, Jun Sun category:cs.CV published:2016-04-07 summary:Scene text recognition plays an important role in many computer visionapplications. The small size of available public available scene text datasetsis the main challenge when training a text recognition CNN model. In thispaper, we propose a CNN based Chinese text recognition algorithm. To enlargethe dataset for training the CNN model, we design a synthetic data engine forChinese scene character generation, which generates representative characterimages according to the fonts use frequency of Chinese texts. As the Chinesetext is more complex, the English text recognition CNN architecture is modifiedfor Chinese text. To ensure the small size nature character dataset and thelarge size artificial character dataset are comparable in training, the CNNmodel are trained progressively. The proposed Chinese text recognitionalgorithm is evaluated with two Chinese text datasets. The algorithm achievesbetter recognize accuracy compared to the baseline methods.
arxiv-17100-265 | Reinterpreting the Transformation Posterior in Probabilistic Image Registration | http://arxiv.org/pdf/1604.01889v1.pdf | author:Jie Luo, Karteek Popuri, Dana Cobzas, Hongyi Ding, Masashi Sugiyama category:cs.CV published:2016-04-07 summary:Probabilistic image registration methods estimate the posterior distributionof transformation. The conventional way of interpreting the transformationposterior is to use the mode as the most likely transformation and assign itscorresponding intensity to the registered voxel. Meanwhile, summary statisticsof the posterior are employed to evaluate the registration uncertainty, that isthe trustworthiness of the registered image. Despite the wide acceptance, thisconvention has never been justified. In this paper, based on illustrativeexamples, we question the correctness and usefulness of conventional methods.In order to faithfully translate the transformation posterior, we propose toencode the variability of values into a novel data type called ensemble fields.Ensemble fields can serve as a complement to the registered image and afoundation for developing advanced methods to characterize the uncertainty inregistration-based tasks. We demonstrate the potential of ensemble fields bypilot examples
arxiv-17100-266 | Towards Bayesian Deep Learning: A Survey | http://arxiv.org/pdf/1604.01662v2.pdf | author:Hao Wang, Dit-Yan Yeung category:stat.ML cs.AI cs.CV cs.LG cs.NE published:2016-04-06 summary:While perception tasks such as visual object recognition and textunderstanding play an important role in human intelligence, the subsequenttasks that involve inference, reasoning and planning require an even higherlevel of intelligence. The past few years have seen major advances in manyperception tasks using deep learning models. For higher-level inference,however, probabilistic graphical models with their Bayesian nature are stillmore powerful and flexible. To achieve integrated intelligence that involvesboth perception and inference, it is naturally desirable to tightly integratedeep learning and Bayesian models within a principled probabilistic framework,which we call Bayesian deep learning. In this unified framework, the perceptionof text or images using deep learning can boost the performance of higher-levelinference and in return, the feedback from the inference process is able toenhance the perception of text or images. This survey provides a generalintroduction to Bayesian deep learning and reviews its recent applications onrecommender systems, topic models, and control. In this survey, we also discussthe relationship and differences between Bayesian deep learning and otherrelated topics like Bayesian treatment of neural networks.
arxiv-17100-267 | Towards More Efficient SPSD Matrix Approximation and CUR Matrix Decomposition | http://arxiv.org/pdf/1503.08395v5.pdf | author:Shusen Wang, Zhihua Zhang, Tong Zhang category:cs.LG published:2015-03-29 summary:Symmetric positive semi-definite (SPSD) matrix approximation methods havebeen extensively used to speed up large-scale eigenvalue computation and kernellearning methods. The sketching based method, which we call the prototypemodel, produces relatively accurate approximations. The prototype model iscomputationally efficient on skinny matrices where one of the matrix dimensionsis relatively small, but it is inefficient on large square matrices. TheNystr\"om method is highly efficient on SPSD matrices, but can only achieve lowmatrix approximation accuracy. In this paper we propose novel model which we call the {\it faster SPSDmatrix approximation model}. The faster model is nearly as efficient as theNystr\"om method and as accurate as the prototype model. We show that thefaster model can potentially solve eigenvalue problems and kernel learningproblems in linear time with respect to the matrix size to achieve $1+\epsilon$relative-error, whereas the prototype model and the Nystr\"om method cost atleast quadratic time to attain comparable error bound. We also contribute newunderstandings of the Nystro\"om method. The Nystr\"om method is a specialinstance of our faster SPSD matrix approximation model, and it is approximationto the prototype model. Our technique can be straightforwardly applied to makethe CUR matrix decomposition more efficiently computed without much affectingthe accuracy. Empirical experiments demonstrate the effectiveness of theproposed methods.
arxiv-17100-268 | Active Learning Algorithms for Graphical Model Selection | http://arxiv.org/pdf/1602.00354v2.pdf | author:Gautam Dasarathy, Aarti Singh, Maria-Florina Balcan, Jong Hyuk Park category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2016-02-01 summary:The problem of learning the structure of a high dimensional graphical modelfrom data has received considerable attention in recent years. In manyapplications such as sensor networks and proteomics it is often expensive toobtain samples from all the variables involved simultaneously. For instance,this might involve the synchronization of a large number of sensors or thetagging of a large number of proteins. To address this important issue, weinitiate the study of a novel graphical model selection problem, where the goalis to optimize the total number of scalar samples obtained by allowing thecollection of samples from only subsets of the variables. We propose a generalparadigm for graphical model selection where feedback is used to guide thesampling to high degree vertices, while obtaining only few samples from theones with the low degrees. We instantiate this framework with two specificactive learning algorithms, one of which makes mild assumptions but iscomputationally expensive, while the other is more computationally efficientbut requires stronger (nevertheless standard) assumptions. Whereas the samplecomplexity of passive algorithms is typically a function of the maximum degreeof the graph, we show that the sample complexity of our algorithms is provablesmaller and that it depends on a novel local complexity measure that is akin tothe average degree of the graph. We finally demonstrate the efficacy of ourframework via simulations.
arxiv-17100-269 | GIFT: A Real-time and Scalable 3D Shape Search Engine | http://arxiv.org/pdf/1604.01879v1.pdf | author:Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, Longin Jan Latecki category:cs.CV published:2016-04-07 summary:Projective analysis is an important solution for 3D shape retrieval, sincehuman visual perceptions of 3D shapes rely on various 2D observations fromdifferent view points. Although multiple informative and discriminative viewsare utilized, most projection-based retrieval systems suffer from heavycomputational cost, thus cannot satisfy the basic requirement of scalabilityfor search engines. In this paper, we present a real-time 3D shape searchengine based on the projective images of 3D shapes. The real-time property ofour search engine results from the following aspects: (1) efficient projectionand view feature extraction using GPU acceleration; (2) the first invertedfile, referred as F-IF, is utilized to speed up the procedure of multi-viewmatching; (3) the second inverted file (S-IF), which captures a localdistribution of 3D shapes in the feature manifold, is adopted for efficientcontext-based re-ranking. As a result, for each query the retrieval task can befinished within one second despite the necessary cost of IO overhead. We namethe proposed 3D shape search engine, which combines GPU acceleration andInverted File Twice, as GIFT. Besides its high efficiency, GIFT alsooutperforms the state-of-the-art methods significantly in retrieval accuracy onvarious shape benchmarks and competitions.
arxiv-17100-270 | When is Nontrivial Estimation Possible for Graphons and Stochastic Block Models? | http://arxiv.org/pdf/1604.01871v1.pdf | author:Audra McMillan, Adam Smith category:math.ST cs.LG stat.TH published:2016-04-07 summary:Block graphons (also called stochastic block models) are an important andwidely-studied class of models for random networks. We provide a lower bound onthe accuracy of estimators for block graphons with a large number of blocks. Weshow that, given only the number $k$ of blocks and an upper bound $\rho$ on thevalues (connection probabilities) of the graphon, every estimator incurs errorat least on the order of $\min(\rho, \sqrt{\rho k^2/n^2})$ in the $\delta_2$metric with constant probability, in the worst case over graphons. Inparticular, our bound rules out any nontrivial estimation (that is, with$\delta_2$ error substantially less than $\rho$) when $k\geq n\sqrt{\rho}$.Combined with previous upper and lower bounds, our results characterize, up tologarithmic terms, the minimax accuracy of graphon estimation in the $\delta_2$metric. A similar lower bound to ours was obtained independently by Klopp,Tsybakov and Verzelen (2016).
arxiv-17100-271 | Building Ensembles of Adaptive Nested Dichotomies with Random-Pair Selection | http://arxiv.org/pdf/1604.01854v1.pdf | author:Tim Leathart, Bernhard Pfahringer, Eibe Frank category:stat.ML cs.LG published:2016-04-07 summary:A system of nested dichotomies is a method of decomposing a multi-classproblem into a collection of binary problems. Such a system recursively splitsthe set of classes into two subsets, and trains a binary classifier todistinguish between each subset. Even though ensembles of nested dichotomieswith random structure have been shown to perform well in practice, using a moresophisticated class subset selection method can be used to improveclassification accuracy. We investigate an approach to this problem calledrandom-pair selection, and evaluate its effectiveness compared to otherpublished methods of subset selection. We show that our method outperformsother methods in many cases, and is at least on par in almost all other cases.
arxiv-17100-272 | End-to-End Deep Learning for Person Search | http://arxiv.org/pdf/1604.01850v1.pdf | author:Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, Xiaogang Wang category:cs.CV published:2016-04-07 summary:Existing person re-identification (re-id) benchmarks and algorithms mainlyfocus on matching cropped pedestrian images between queries and candidates.However, it is different from real-world scenarios where the annotations ofpedestrian bounding boxes are unavailable and the target person needs to befound from whole images. To close the gap, we investigate how to localize andmatch query persons from the scene images without relying on the annotations ofcandidate boxes. Instead of breaking it down into two separatetasks---pedestrian detection and person re-id, we propose an end-to-end deeplearning framework to jointly handle both tasks. A random sampling softmax lossis proposed to effectively train the model under the supervision of sparse andunbalanced labels. On the other hand, existing benchmarks are small in scaleand the samples are collected from a few fixed camera views with low scenediversities. To address this issue, we collect a large-scale andscene-diversified person search dataset, which contains 18,184 images, 8,432persons, and 99,809 annotated boundingboxes\footnote{\url{http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html}}. Weevaluate our approach and other baselines on the proposed dataset, and studythe influence of various factors. Experiments show that our method achieves thebest result.
arxiv-17100-273 | Penalty methods for a class of non-Lipschitz optimization problems | http://arxiv.org/pdf/1409.2558v3.pdf | author:Xiaojun Chen, Zhaosong Lu, Ting Kei Pong category:math.OC stat.ML published:2014-09-09 summary:We consider a class of constrained optimization problems with a possiblynonconvex non-Lipschitz objective and a convex feasible set being theintersection of a polyhedron and a possibly degenerate ellipsoid. Such problemshave a wide range of applications in data science, where the objective is usedfor inducing sparsity in the solutions while the constraint set models thenoise tolerance and incorporates other prior information for data fitting. Tosolve this class of constrained optimization problems, a common approach is thepenalty method. However, there is little theory on exact penalization forproblems with nonconvex and non-Lipschitz objective functions. In this paper,we study the existence of exact penalty parameters regarding local minimizers,stationary points and $\epsilon$-minimizers under suitable assumptions.Moreover, we discuss a penalty method whose subproblems are solved via anonmonotone proximal gradient method with a suitable update scheme for thepenalty parameters, and prove the convergence of the algorithm to a KKT pointof the constrained problem. Preliminary numerical results demonstrate theefficiency of the penalty method for finding sparse solutions ofunderdetermined linear systems.
arxiv-17100-274 | VRFP: On-the-fly Video Retrieval using Web Images and Fast Fisher Vector Products | http://arxiv.org/pdf/1512.03384v2.pdf | author:Xintong Han, Bharat Singh, Vlad I. Morariu, Larry S. Davis category:cs.CV published:2015-12-10 summary:VRFP is a real-time video retrieval framework based on short text inputqueries in which weakly labeled training samples from the web are obtained,after the query is known. Our experiments show that a Fisher Vector is robustto noise present in web-images and compares favorably in terms of accuracy toother standard representations. While a Fisher Vector for a new query can beconstructed efficiently, matching against the test set is slow due to its highdimensionality. To perform matching in real-time, we present a losslessalgorithm for accelerating the computation of dot product between highdimensional Fisher Vectors. We prove that the expected number ofmultiplications required is quadratic in terms of sparsity in Fisher Vectors.We are not only able to construct and apply query models in real-time, but withthe help of a simple re-ranking scheme, we also outperform state-of-the-artautomatic retrieval methods by a significant margin on TRECVID MED13 (3.5%),MED14 (1.3%) and CCV datasets (5.2%).
arxiv-17100-275 | A Classification Leveraged Object Detector | http://arxiv.org/pdf/1604.01841v1.pdf | author:Miao Sun, Tony X. Han, Zhihai He category:cs.CV published:2016-04-07 summary:Currently, the state-of-the-art image classification algorithms outperformthe best available object detector by a big margin in terms of averageprecision. We, therefore, propose a simple yet principled approach that allowsus to leverage object detection through image classification on supportingregions specified by a preliminary object detector. Using a simple bag-of-words model based image classification algorithm, we leveraged the performanceof the deformable model objector from 35.9% to 39.5% in average precision over20 categories on standard PASCAL VOC 2007 detection dataset.
arxiv-17100-276 | Clustering Via Crowdsourcing | http://arxiv.org/pdf/1604.01839v1.pdf | author:Arya Mazumdar, Barna Saha category:cs.DS cs.IT cs.LG math.IT published:2016-04-07 summary:In recent years, crowdsourcing, aka human aided computation has emerged as aneffective platform for solving problems that are considered complex formachines alone. Using human is time-consuming and costly due to monetarycompensations. Therefore, a crowd based algorithm must judiciously use anyinformation computed through an automated process, and ask minimum number ofquestions to the crowd adaptively. One such problem which has received significant attention is {\em entityresolution}. Formally, we are given a graph $G=(V,E)$ with unknown edge set $E$where $G$ is a union of $k$ (again unknown, but typically large $O(n^\alpha)$,for $\alpha>0$) disjoint cliques $G_i(V_i, E_i)$, $i =1, \dots, k$. The goal isto retrieve the sets $V_i$s by making minimum number of pair-wise queries $V\times V\to\{\pm1\}$ to an oracle (the crowd). When the answer to each query iscorrect, e.g. via resampling, then this reduces to finding connected componentsin a graph. On the other hand, when crowd answers may be incorrect, itcorresponds to clustering over minimum number of noisy inputs. Even, withperfect answers, a simple lower and upper bound of $\Theta(nk)$ on querycomplexity can be shown. A major contribution of this paper is to reduce thequery complexity to linear or even sublinear in $n$ when mild side informationis provided by a machine, and even in presence of crowd errors which are notcorrectable via resampling. We develop new information theoretic lower boundson the query complexity of clustering with side information and errors, and ourupper bounds closely match with them. Our algorithms are naturallyparallelizable, and also give near-optimal bounds on the number of adaptiverounds required to match the query complexity.
arxiv-17100-277 | Self-organization of vocabularies under different interaction orders | http://arxiv.org/pdf/1603.05350v2.pdf | author:Javier Vera category:cs.CL physics.soc-ph published:2016-03-17 summary:Traditionally, the formation of vocabularies has been studied by agent-basedmodels (specially, the Naming Game) in which random pairs of agents negotiateword-meaning associations at each discrete time step. This paper proposes afirst approximation to a novel question: To what extent the negotiation ofword-meaning associations is influenced by the order in which the individualsinteract? Automata Networks provide the adequate mathematical framework toexplore this question. Computer simulations suggest that on two-dimensionallattices the typical features of the formation of word-meaning associations arerecovered under random schemes that update small fractions of the population atthe same time.
arxiv-17100-278 | Automata networks model for alignment and least effort on vocabulary formation | http://arxiv.org/pdf/1508.01577v2.pdf | author:Javier Vera, Felipe Urbina, Eric Goles category:cs.CL physics.soc-ph published:2015-08-07 summary:Can artificial communities of agents develop language with scaling relationsclose to the Zipf law? As a preliminary answer to this question, we propose anAutomata Networks model of the formation of a vocabulary on a population ofindividuals, under two in principle opposite strategies: the alignment and theleast effort principle. Within the previous account to the emergence oflinguistic conventions (specially, the Naming Game), we focus on modelingspeaker and hearer efforts as actions over their vocabularies and we study theimpact of these actions on the formation of a shared language. The numericalsimulations are essentially based on an energy function, that measures theamount of local agreement between the vocabularies. The results suggests thaton one dimensional lattices the best strategy to the formation of sharedlanguages is the one that minimizes the efforts of speakers on communicativetasks.
arxiv-17100-279 | Automata networks for multi-party communication in the Naming Game | http://arxiv.org/pdf/1510.02358v2.pdf | author:Javier Vera, Pedro Montealegre, Eric Goles category:cs.CL published:2015-10-08 summary:The Naming Game has been studied to explore the role of self-organization inthe development and negotiation of linguistic conventions. In this paper, wedefine an automata networks approach to the Naming Game. Two problems arefaced: (1) the definition of an automata networks for multi-party communicativeinteractions; and (2) the proof of convergence for three different orders inwhich the individuals are updated (updating schemes). Finally, computersimulations are explored in two-dimensional lattices with the purpose torecover the main features of the Naming Game and to describe the dynamics underdifferent updating schemes.
arxiv-17100-280 | Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models | http://arxiv.org/pdf/1507.04808v3.pdf | author:Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, Joelle Pineau category:cs.CL cs.AI cs.LG cs.NE I.5.1; I.2.7 published:2015-07-17 summary:We investigate the task of building open domain, conversational dialoguesystems based on large dialogue corpora using generative models. Generativemodels produce system responses that are autonomously generated word-by-word,opening up the possibility for realistic, flexible interactions. In support ofthis goal, we extend the recently proposed hierarchical recurrentencoder-decoder neural network to the dialogue domain, and demonstrate thatthis model is competitive with state-of-the-art neural language models andback-off n-gram models. We investigate the limitations of this and similarapproaches, and show how its performance can be improved by bootstrapping thelearning from a larger question-answer pair corpus and from pretrained wordembeddings.
arxiv-17100-281 | A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification | http://arxiv.org/pdf/1510.03820v4.pdf | author:Ye Zhang, Byron Wallace category:cs.CL cs.LG cs.NE published:2015-10-13 summary:Convolutional Neural Networks (CNNs) have recently achieved remarkably strongperformance on the practically important task of sentence classification (kim2014, kalchbrenner 2014, johnson 2014). However, these models requirepractitioners to specify an exact model architecture and set accompanyinghyperparameters, including the filter region size, regularization parameters,and so on. It is currently unknown how sensitive model performance is tochanges in these configurations for the task of sentence classification. Wethus conduct a sensitivity analysis of one-layer CNNs to explore the effect ofarchitecture components on model performance; our aim is to distinguish betweenimportant and comparatively inconsequential design decisions for sentenceclassification. We focus on one-layer CNNs (to the exclusion of more complexmodels) due to their comparative simplicity and strong empirical performance,which makes it a modern standard baseline method akin to Support Vector Machine(SVMs) and logistic regression. We derive practical advice from our extensiveempirical results for those interested in getting the most out of CNNs forsentence classification in real world settings.
arxiv-17100-282 | Dimensionality Reduction with Subspace Structure Preservation | http://arxiv.org/pdf/1412.2404v3.pdf | author:Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju category:cs.LG stat.ML published:2014-12-07 summary:Modeling data as being sampled from a union of independent subspaces has beenwidely applied to a number of real world applications. However, dimensionalityreduction approaches that theoretically preserve this independence assumptionhave not been well studied. Our key contribution is to show that $2K$projection vectors are sufficient for the independence preservation of any $K$class data sampled from a union of independent subspaces. It is thisnon-trivial observation that we use for designing our dimensionality reductiontechnique. In this paper, we propose a novel dimensionality reduction algorithmthat theoretically preserves this structure for a given dataset. We support ourtheoretical analysis with empirical results on both synthetic and real worlddata achieving \textit{state-of-the-art} results compared to populardimensionality reduction techniques.
arxiv-17100-283 | Differential TD Learning for Value Function Approximation | http://arxiv.org/pdf/1604.01828v1.pdf | author:Adithya M. Devraj, Sean P. Meyn category:cs.SY cs.LG math.OC published:2016-04-06 summary:Value functions arise as a component of algorithms as well as performancemetrics in statistics and engineering applications. Computation of theassociated Bellman equations is numerically challenging in all but a fewspecial cases. A popular approximation technique is known as TemporalDifference (TD) learning. The algorithm introduced in this paper is intended toresolve two well-known problems with this approach:In the discounted-costsetting, the variance of the algorithm diverges as the discount factorapproaches unity. Second, for the average cost setting, unbiased algorithmsexist only in special cases. It is shown that the gradient of any of these value functions admits arepresentation that lends itself to algorithm design. Based on this result, thenew differential TD method is obtained for Markovian models on Euclidean spacewith smooth dynamics. Numerical examples show remarkable improvements in performance. Inapplication to speed scaling, variance is reduced by two orders of magnitude.
arxiv-17100-284 | Deep Semantic Matching for Optical Flow | http://arxiv.org/pdf/1604.01827v1.pdf | author:Min Bai, Wenjie Luo, Kaustav Kundu, Raquel Urtasun category:cs.CV published:2016-04-06 summary:We tackle the problem of estimating optical flow from a monocular camera inthe context of autonomous driving. We build on the observation that the sceneis typically composed of a static background, as well as a relatively smallnumber of traffic participants which move rigidly in 3D. We propose to estimatethe traffic participants using instance-level segmentation. For each trafficparticipant, we use the epipolar constraints that govern each independentmotion for faster and more accurate estimation. Our second contribution is anew convolutional net that learns to perform flow matching, and is able toestimate the uncertainty of its matches. This is a core element of our flowestimation pipeline. We demonstrate the effectiveness of our approach in thechallenging KITTI 2015 flow benchmark, and show that our approach outperformspublished approaches by a large margin.
arxiv-17100-285 | R-FUSE: Robust Fast Fusion of Multi-Band Images Based on Solving a Sylvester Equation | http://arxiv.org/pdf/1604.01818v1.pdf | author:Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret, Jose Bioucas-Dias, Simon Godsill category:cs.CV published:2016-04-06 summary:This paper proposes a robust fast multi-band image fusion method to merge ahigh-spatial low-spectral resolution image and a low-spatial high-spectralresolution image. Following the method recently developed in [1], thegeneralized Sylvester matrix equation associated with the multi-band imagefusion problem is solved in a more robust and efficient way by exploiting theWoodbury formula, avoiding any permutation operation in the frequency domain aswell as the blurring kernel invertibility assumption required in [1]. Thanks tothis improvement, the proposed algorithm requires fewer computationaloperations and is also more robust with respect to the blurring kernel comparedwith the one in [1]. The proposed new algorithm is tested with different priorsconsidered in [1]. Our conclusion is that the proposed fusion algorithm is morerobust than the one in [1] with a reduced computational cost.
arxiv-17100-286 | Generalising the Discriminative Restricted Boltzmann Machine | http://arxiv.org/pdf/1604.01806v1.pdf | author:Srikanth Cherla, Son N Tran, Tillman Weyde, Artur d'Avila Garcez category:cs.LG published:2016-04-06 summary:We present a novel theoretical result that generalises the DiscriminativeRestricted Boltzmann Machine (DRBM). While originally the DRBM was definedassuming the {0, 1}-Bernoulli distribution in each of its hidden units, thisresult makes it possible to derive cost functions for variants of the DRBM thatutilise other distributions, including some that are often encountered in theliterature. This is illustrated with the Binomial and {-1, +1}-Bernoullidistributions here. We evaluate these two DRBM variants and compare them withthe original one on three benchmark datasets, namely the MNIST and USPS digitclassification datasets, and the 20 Newsgroups document classification dataset.Results show that each of the three compared models outperforms the remainingtwo in one of the three datasets, thus indicating that the proposed theoreticalgeneralisation of the DRBM may be valuable in practice.
arxiv-17100-287 | A Recurrent Latent Variable Model for Sequential Data | http://arxiv.org/pdf/1506.02216v6.pdf | author:Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, Yoshua Bengio category:cs.LG published:2015-06-07 summary:In this paper, we explore the inclusion of latent random variables into thedynamic hidden state of a recurrent neural network (RNN) by combining elementsof the variational autoencoder. We argue that through the use of high-levellatent random variables, the variational RNN (VRNN)1 can model the kind ofvariability observed in highly structured sequential data such as naturalspeech. We empirically evaluate the proposed model against related sequentialmodels on four speech datasets and one handwriting dataset. Our results showthe important roles that latent random variables can play in the RNN dynamichidden state.
arxiv-17100-288 | Learning to Track at 100 FPS with Deep Regression Networks | http://arxiv.org/pdf/1604.01802v1.pdf | author:David Held, Sebastian Thrun, Silvio Savarese category:cs.CV cs.AI cs.LG cs.RO published:2016-04-06 summary:Machine learning techniques are often used in computer vision due to theirability to leverage large amounts of training data to improve performance.Unfortunately, most generic object trackers are still trained from scratchonline and do not benefit from the large number of videos that are readilyavailable for offline training. We propose a method for using neural networksto track generic objects in a way that allows them to improve performance bytraining on labeled videos. Previous attempts to use neural networks fortracking are very slow to run and not practical for real-time applications. Incontrast, our tracker uses a simple feed-forward network with no onlinetraining required, allowing our tracker to run at 100 fps during test time. Ourtracker trains from both labeled video as well as a large collection of images,which helps prevent overfitting. The tracker learns generic object motion andcan be used to track novel objects that do not appear in the training set. Wetest our network on a standard tracking benchmark to demonstrate our tracker'sstate-of-the-art performance. Our network learns to track generic objects inreal-time as they move throughout the world.
arxiv-17100-289 | Advances in Very Deep Convolutional Neural Networks for LVCSR | http://arxiv.org/pdf/1604.01792v1.pdf | author:Tom Sercu, Vaibhava Goel category:cs.CL cs.LG cs.NE published:2016-04-06 summary:Very deep CNNs with small 3x3 kernels have recently been shown to achievevery strong performance as acoustic models in hybrid NN-HMM speech recognitionsystems. In this paper, we demonstrate that the accuracy gains of these deepCNNs are retained both on larger scale data, and after sequence training. Weshow this by carrying out sequence training on both the 300h switchboard-1 andthe 2000h switchboard dataset. Furthermore, we investigate how pooling andpadding in time influences performance, both in terms of word error rate andcomputational cost. We argue that designing CNNs without timepadding andwithout timepooling, though slightly suboptimal for accuracy, has twosignificant consequences. Firstly, the proposed design allows for efficientevaluation at sequence training and test (deployment) time. Secondly, thisdesign principle allows for batch normalization to be adopted to CNNs onsequence data. Our very deep CNN model sequence trained on the 2000hswitchboard dataset obtains 9.4 word error rate on the Hub5 test-set, matchingwith a single model the performance of 2015 IBM system combination, which wasthe previous best published result.
arxiv-17100-290 | A Subpath Kernel for Learning Hierarchical Image Representations | http://arxiv.org/pdf/1604.01787v1.pdf | author:Yanwei Cui, Laetitia Chapel, Sébastien Lefèvre category:cs.CV published:2016-04-06 summary:Tree kernels have demonstrated their ability to deal with hierarchical data,as the intrinsic tree structure often plays a discriminative role. While suchkernels have been successfully applied to various domains such as naturelanguage processing and bioinformatics, they mostly concentrate on orderedtrees and whose nodes are described by symbolic data. Meanwhile, hierarchicalrepresentations have gained increasing interest to describe image content. Thisis particularly true in remote sensing, where such representations allow forrevealing different objects of interest at various scales through a treestructure. However, the induced trees are unordered and the nodes are equippedwith numerical features. In this paper, we propose a new structured kernel forhierarchical image representations which is built on the concept of subpathkernel. Experimental results on both artificial and remote sensing datasetsshow that the proposed kernel manages to deal with the hierarchical nature ofthe data, leading to better classification rates.
arxiv-17100-291 | Safe Probability | http://arxiv.org/pdf/1604.01785v1.pdf | author:Peter Grünwald category:stat.ME cs.AI cs.LG math.ST stat.TH 62A01 published:2016-04-06 summary:We formalize the idea of probability distributions that lead to reliablepredictions about some, but not all aspects of a domain. The resulting notionof `safety' provides a fresh perspective on foundational issues in statistics,providing a middle ground between imprecise probability and multiple-priormodels on the one hand and strictly Bayesian approaches on the other. It alsoallows us to formalize fiducial distributions in terms of the set of randomvariables that they can safely predict, thus taking some of the sting out ofthe fiducial idea. By restricting probabilistic inference to safe uses, onealso automatically avoids paradoxes such as the Monty Hall problem. Safetycomes in a variety of degrees, such as "validity" (the strongest notion),"calibration", "confidence safety" and "unbiasedness" (almost the weakestnotion).
arxiv-17100-292 | Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding | http://arxiv.org/pdf/1604.01753v1.pdf | author:Gunnar A. Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, Abhinav Gupta category:cs.CV published:2016-04-06 summary:Computer vision has a great potential to help our daily lives by searchingfor lost keys, watering flowers or reminding us to take a pill. To succeed withsuch tasks, computer vision methods need to be trained from real and diverseexamples of our daily dynamic scenes. While most of such scenes are notparticularly exciting, they typically do not appear on YouTube, in movies or TVbroadcasts. So how do we collect sufficiently many diverse but boring samplesrepresenting our lives? We propose a novel Hollywood in Homes approach tocollect such data. Instead of shooting videos in the lab, we ensure diversityby distributing and crowdsourcing the whole process of video creation fromscript writing to video recording and annotation. Following this procedure wecollect a new dataset, Charades, with hundreds of people recording videos intheir own homes, acting out casual everyday activities. The dataset is composedof 9,850 annotated videos with an average length of 30 seconds, showingactivities of 267 people from three continents. Each video is annotated bymultiple free-text descriptions, action labels, action intervals and classes ofinteracted objects. In total, Charades provides 27,847 video descriptions,37,972 temporally localized intervals for 160 action classes and 24,623 labelsfor 40 object classes. Using this rich data, we evaluate and provide baselineresults for several tasks including action recognition and automaticdescription generation. We believe that the realism, diversity, and casualnature of this dataset will present unique challenges and new opportunities forcomputer vision community.
arxiv-17100-293 | Dynamic Capacity Networks | http://arxiv.org/pdf/1511.07838v6.pdf | author:Amjad Almahairi, Nicolas Ballas, Tim Cooijmans, Yin Zheng, Hugo Larochelle, Aaron Courville category:cs.LG cs.NE published:2015-11-24 summary:We introduce the Dynamic Capacity Network (DCN), a neural network that canadaptively assign its capacity across different portions of the input data.This is achieved by combining modules of two types: low-capacity sub-networksand high-capacity sub-networks. The low-capacity sub-networks are appliedacross most of the input, but also provide a guide to select a few portions ofthe input on which to apply the high-capacity sub-networks. The selection ismade using a novel gradient-based attention mechanism, that efficientlyidentifies input regions for which the DCN's output is most sensitive and towhich we should devote more capacity. We focus our empirical evaluation on theCluttered MNIST and SVHN image datasets. Our findings indicate that DCNs areable to drastically reduce the number of computations, compared to traditionalconvolutional neural networks, while maintaining similar or even betterperformance.
arxiv-17100-294 | Denoising and Covariance Estimation of Single Particle Cryo-EM Images | http://arxiv.org/pdf/1602.06632v3.pdf | author:Tejal Bhamre, Teng Zhang, Amit Singer category:cs.CV q-bio.BM stat.ML published:2016-02-22 summary:The problem of image restoration in cryo-EM entails correcting for theeffects of the Contrast Transfer Function (CTF) and noise. Popular methods forimage restoration include `phase flipping', which corrects only for the Fourierphases but not amplitudes, and Wiener filtering, which requires the spectralsignal to noise ratio. We propose a new image restoration method which we call`Covariance Wiener Filtering' (CWF). In CWF, the covariance matrix of theprojection images is used within the classical Wiener filtering framework forsolving the image restoration deconvolution problem. Our estimation procedurefor the covariance matrix is new and successfully corrects for the CTF. Wedemonstrate the efficacy of CWF by applying it to restore both simulated andexperimental cryo-EM images. Results with experimental datasets demonstratethat CWF provides a good way to evaluate the particle images and to see whatthe dataset contains even without 2D classification and averaging.
arxiv-17100-295 | A U-statistic Approach to Hypothesis Testing for Structure Discovery in Undirected Graphical Models | http://arxiv.org/pdf/1604.01733v1.pdf | author:Wacha Bounliphone, Matthew Blaschko category:stat.ML math.ST stat.TH published:2016-04-06 summary:Structure discovery in graphical models is the determination of the topologyof a graph that encodes conditional independence properties of the jointdistribution of all variables in the model. For some class of probabilitydistributions, an edge between two variables is present if and only if thecorresponding entry in the precision matrix is non-zero. For a finite sampleestimate of the precision matrix, entries close to zero may be due to lowsample effects, or due to an actual association between variables; these twocases are not readily distinguishable. %Fisher provided a hypothesis test basedon a parametric approximation to the distribution of an entry in the precisionmatrix of a Gaussian distribution, but this may not provide valid upper boundson $p$-values for non-Gaussian distributions. Many related works on this topicconsider potentially restrictive distributional or sparsity assumptions thatmay not apply to a data sample of interest, and direct estimation of theuncertainty of an estimate of the precision matrix for general distributionsremains challenging. Consequently, we make use of results for $U$-statisticsand apply them to the covariance matrix. By probabilistically bounding thedistortion of the covariance matrix, we can apply Weyl's theorem to bound thedistortion of the precision matrix, yielding a conservative, but sound testthreshold for a much wider class of distributions than considered in previousworks. The resulting test enables one to answer with statistical significancewhether an edge is present in the graph, and convergence results are known fora wide range of distributions. The computational complexities is linear in thesample size enabling the application of the test to large data samples forwhich computation time becomes a limiting factor. We experimentally validatethe correctness and scalability of the test on multivariate distributions forwhich the distributional assumptions of competing tests result inunderestimates of the false positive ratio. By contrast, the proposed testremains sound, promising to be a useful tool for hypothesis testing for diversereal-world problems.
arxiv-17100-296 | Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text | http://arxiv.org/pdf/1604.01729v1.pdf | author:Subhashini Venugopalan, Lisa Anne Hendricks, Raymond Mooney, Kate Saenko category:cs.CL cs.CV published:2016-04-06 summary:This paper investigates how linguistic knowledge mined from large textcorpora can aid the generation of natural language descriptions of videos.Specifically, we integrate both a neural language model and distributionalsemantics trained on large text corpora into a recent LSTM-based architecturefor video description. We evaluate our approach on a collection of Youtubevideos as well as two large movie description datasets showing significantimprovements in grammaticality while maintaining or modestly improvingdescriptive quality. Further, we show that such techniques can be beneficialfor describing unseen object classes with no paired training data (zeroshotcaptioning).
arxiv-17100-297 | Reading Between the Pixels: Photographic Steganography for Camera Display Messaging | http://arxiv.org/pdf/1604.01720v1.pdf | author:Eric Wengrowski, Kristin Dana, Marco Gruteser, Narayan Mandayam category:cs.CV cs.GR cs.MM cs.NI I.4.8 published:2016-04-06 summary:We exploit human color metamers to send light-modulated messages less visibleto the human eye, but recoverable by cameras. These messages are a keycomponent to camera-display messaging, such as handheld smartphones capturinginformation from electronic signage. Each color pixel in the display image ismodified by a particular color gradient vector. The challenge is to find thecolor gradient that maximizes camera response, while minimizing human response.The mismatch in human spectral and camera sensitivity curves creates anopportunity for hidden messaging. Our approach does not require knowledge ofthese sensitivity curves, instead we employ a data-driven method. We learn anellipsoidal partitioning of the six-dimensional space of colors and colorgradients. This partitioning creates metamer sets defined by the base color atthe display pixel and the color gradient direction for message encoding. Wesample from the resulting metamer sets to find color steps for each base colorto embed a binary message into an arbitrary image with reduced visibleartifacts. Unlike previous methods that rely on visually obtrusive intensitymodulation, we embed with color so that the message is more hidden. Ordinarydisplays and cameras are used without the need for expensive LEDs or high speeddevices. The primary contribution of this work is a framework to map the pixelsin an arbitrary image to a metamer pair for steganographic photo messaging.
arxiv-17100-298 | Learning in Auctions: Regret is Hard, Envy is Easy | http://arxiv.org/pdf/1511.01411v6.pdf | author:Constantinos Daskalakis, Vasilis Syrgkanis category:cs.GT cs.AI cs.CC cs.LG published:2015-11-04 summary:A line of recent work provides welfare guarantees of simple combinatorialauction formats, such as selling m items via simultaneous second price auctions(SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman etal. 2013). These guarantees hold even when the auctions are repeatedly executedand players use no-regret learning algorithms. Unfortunately, off-the-shelfno-regret algorithms for these auctions are computationally inefficient as thenumber of actions is exponential. We show that this obstacle is insurmountable:there are no polynomial-time no-regret algorithms for SiSPAs, unlessRP$\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raisesthe question of how good outcomes polynomially-bounded bidders may discover insuch auctions. To answer this question, we propose a novel concept of learning in auctions,termed "no-envy learning." This notion is founded upon Walrasian equilibrium,and we show that it is both efficiently implementable and results inapproximately optimal welfare, even when the bidders have fractionallysubadditive (XOS) valuations (assuming demand oracles) or coverage valuations(without demand oracles). No-envy learning outcomes are a relaxation ofno-regret outcomes, which maintain their approximate welfare optimality whileendowing them with computational tractability. Our results extend to otherauction formats that have been studied in the literature via the smoothnessparadigm. Our results for XOS valuations are enabled by a novelFollow-The-Perturbed-Leader algorithm for settings where the number of expertsis infinite, and the payoff function of the learner is non-linear. Thisalgorithm has applications outside of auction settings, such as in securitygames. Our result for coverage valuations is based on a novel use of convexrounding schemes and a reduction to online convex optimization.
arxiv-17100-299 | On the Geometry of Message Passing Algorithms for Gaussian Reciprocal Processes | http://arxiv.org/pdf/1603.09279v2.pdf | author:Francesca Paola Carli category:stat.ML math.OC published:2016-03-30 summary:Reciprocal processes are acausal generalizations of Markov processesintroduced by Bernstein in 1932. In the literature, a significant amount ofattention has been focused on developing dynamical models for reciprocalprocesses. Recently, probabilistic graphical models for reciprocal processeshave been provided. This opens the way to the application of efficientinference algorithms in the machine learning literature to solve the smoothingproblem for reciprocal processes. Such algorithms are known to converge if theunderlying graph is a tree. This is not the case for a reciprocal process,whose associated graphical model is a single loop network. The contribution ofthis paper is twofold. First, we introduce belief propagation for Gaussianreciprocal processes. Second, we establish a link between convergence analysisof belief propagation for Gaussian reciprocal processes and stability theoryfor differentially positive systems.
arxiv-17100-300 | A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories | http://arxiv.org/pdf/1604.01696v1.pdf | author:Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen category:cs.CL cs.AI published:2016-04-06 summary:Representation and learning of commonsense knowledge is one of thefoundational problems in the quest to enable deep language understanding. Thisissue is particularly challenging for understanding casual and correlationalrelationships between events. While this topic has received a lot of interestin the NLP community, research has been hindered by the lack of a properevaluation framework. This paper attempts to address this problem with a newframework for evaluating story understanding and script learning: the 'StoryCloze Test'. This test requires a system to choose the correct ending to afour-sentence story. We created a new corpus of ~50k five-sentence commonsensestories, ROCStories, to enable this evaluation. This corpus is unique in twoways: (1) it captures a rich set of causal and temporal commonsense relationsbetween daily events, and (2) it is a high quality collection of everyday lifestories that can also be used for story generation. Experimental evaluationshows that a host of baselines and state-of-the-art models based on shallowlanguage understanding struggle to achieve a high score on the Story ClozeTest. We discuss these implications for script and story learning, and offersuggestions for deeper language understanding.
