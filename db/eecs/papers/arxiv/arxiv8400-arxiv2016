arxiv-8400-1 | Gabor wavelets combined with volumetric fractal dimension applied to texture analysis | http://arxiv.org/pdf/1412.7856v1.pdf | author:Álvaro Gomez Z., João B. Florindo, Odemir M. Bruno category:cs.CV published:2014-12-25 summary:Texture analysis and classification remain as one of the biggest challengesfor the field of computer vision and pattern recognition. On this matter, Gaborwavelets has proven to be a useful technique to characterize distinctivetexture patterns. However, most of the approaches used to extract descriptorsof the Gabor magnitude space usually fail in representing adequately therichness of detail present into a unique feature vector. In this paper, wepropose a new method to enhance the Gabor wavelets process extracting a fractalsignature of the magnitude spaces. Each signature is reduced using a canonicalanalysis function and concatenated to form the final feature vector.Experiments were conducted on several texture image databases to prove thepower and effectiveness of the proposed method. Results obtained shown thatthis method outperforms other early proposed method, creating a more reliabletechnique for texture feature extraction.
arxiv-8400-2 | Joint Deep Learning for Car Detection | http://arxiv.org/pdf/1412.7854v1.pdf | author:Seyedshams Feyzabadi category:cs.CV published:2014-12-25 summary:Traditional object recognition approaches apply feature extraction, partdeformation handling, occlusion handling and classification sequentially whilethey are independent from each other. Ouyang and Wang proposed a model forjointly learning of all of the mentioned processes using one deep neuralnetwork. We utilized, and manipulated their toolbox in order to apply it in cardetection scenarios where it had not been tested. Creating a single deeparchitecture from these components, improves the interaction between them andcan enhance the performance of the whole system. We believe that the approachcan be used as a general purpose object detection toolbox. We tested thealgorithm on UIUC car dataset, and achieved a reasonable result. The accuracyof our method was 86 % while there are better results of accuracy with up to 91% and will be shown later. We strongly believe that having an experiment on alarger dataset can show the advantage of using deep models over shallow ones.
arxiv-8400-3 | Fractal descriptors based on the probability dimension: a texture analysis and classification approach | http://arxiv.org/pdf/1412.7851v1.pdf | author:João Batista Florindo, Odemir Martinez Bruno category:cs.CV published:2014-12-25 summary:In this work, we propose a novel technique for obtaining descriptors ofgray-level texture images. The descriptors are provided by applying amultiscale transform to the fractal dimension of the image estimated throughthe probability (Voss) method. The effectiveness of the descriptors is verifiedin a classification task using benchmark over texture datasets. The resultsobtained demonstrate the efficiency of the proposed method as a tool for thedescription and discrimination of texture images.
arxiv-8400-4 | Brachiaria species identification using imaging techniques based on fractal descriptors | http://arxiv.org/pdf/1412.7849v1.pdf | author:João Batista Florindo, Núbia Rosa da Silva, Liliane Maria Romualdo, Fernanda de Fátima da Silva, Pedro Henrique de Cerqueira Luz, Valdo Rodrigues Herling, Odemir Martinez Bruno category:cs.CV published:2014-12-25 summary:The use of a rapid and accurate method in diagnosis and classification ofspecies and/or cultivars of forage has practical relevance, scientific andtrade in various areas of study. Thus, leaf samples of fodder plant species\textit{Brachiaria} were previously identified, collected and scanned to betreated by means of artificial vision to make the database and be used insubsequent classifications. Forage crops used were: \textit{Brachiariadecumbens} cv. IPEAN; \textit{Brachiaria ruziziensis} Germain \& Evrard;\textit{Brachiaria Brizantha} (Hochst. ex. A. Rich.) Stapf; \textit{Brachiariaarrecta} (Hack.) Stent. and \textit{Brachiaria spp}. The images were analyzedby the fractal descriptors method, where a set of measures are obtained fromthe values of the fractal dimension at different scales. Therefore such valuesare used as inputs for a state-of-the-art classifier, the Support VectorMachine, which finally discriminates the images according to the respectivespecies.
arxiv-8400-5 | Texture analysis using volume-radius fractal dimension | http://arxiv.org/pdf/1412.7844v1.pdf | author:André R. Backes, Odemir M. Bruno category:cs.CV published:2014-12-25 summary:Texture plays an important role in computer vision. It is one of the mostimportant visual attributes used in image analysis, once it providesinformation about pixel organization at different regions of the image. Thispaper presents a novel approach for texture characterization, based oncomplexity analysis. The proposed approach expands the idea of the Mass-radiusfractal dimension, a method originally developed for shape analysis, to a setof coordinates in 3D-space that represents the texture under analysis in asignature able to characterize efficiently different texture classes in termsof complexity. An experiment using images from the Brodatz album illustratesthe method performance.
arxiv-8400-6 | FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory and Computation | http://arxiv.org/pdf/1407.6251v2.pdf | author:Philip Lenz, Andreas Geiger, Raquel Urtasun category:cs.CV published:2014-07-23 summary:One of the most popular approaches to multi-target tracking istracking-by-detection. Current min-cost flow algorithms which solve the dataassociation problem optimally have three main drawbacks: they arecomputationally expensive, they assume that the whole video is given as abatch, and they scale badly in memory and computation with the length of thevideo sequence. In this paper, we address each of these issues, resulting in acomputationally and memory-bounded solution. First, we introduce a dynamicversion of the successive shortest-path algorithm which solves the dataassociation problem optimally while reusing computation, resulting insignificantly faster inference than standard solvers. Second, we address theoptimal solution to the data association problem when dealing with an incomingstream of data (i.e., online setting). Finally, we present our maincontribution which is an approximate online solution with bounded memory andcomputation which is capable of handling videos of arbitrarily length whileperforming tracking in real time. We demonstrate the effectiveness of ouralgorithms on the KITTI and PETS2009 benchmarks and show state-of-the-artperformance, while being significantly faster than existing solvers.
arxiv-8400-7 | Plagiarism Detection on Electronic Text based Assignments using Vector Space Model (ICIAfS14) | http://arxiv.org/pdf/1412.7782v1.pdf | author:MAC Jiffriya, MAC Akmal Jahan, Roshan G. Ragel category:cs.IR cs.CL published:2014-12-25 summary:Plagiarism is known as illegal use of others' part of work or whole work asone's own in any field such as art, poetry, literature, cinema, research andother creative forms of study. Plagiarism is one of the important issues inacademic and research fields and giving more concern in academic systems. Thesituation is even worse with the availability of ample resources on the web.This paper focuses on an effective plagiarism detection tool on identifyingsuitable intra-corpal plagiarism detection for text based assignments bycomparing unigram, bigram, trigram of vector space model with cosine similaritymeasure. Manually evaluated, labelled dataset was tested using unigram, bigramand trigram vector. Even though trigram vector consumes comparatively moretime, it shows better results with the labelled data. In addition, the selectedtrigram vector space model with cosine similarity measure is compared withtri-gram sequence matching technique with Jaccard measure. In the results,cosine similarity score shows slightly higher values than the other. Because,it focuses on giving more weight for terms that do not frequently exist in thedataset and cosine similarity measure using trigram technique is morepreferable than the other. Therefore, we present our new tool and it could beused as an effective tool to evaluate text based electronic assignments andminimize the plagiarism among students.
arxiv-8400-8 | Improved Parameter Identification Method Based on Moving Rate | http://arxiv.org/pdf/1412.7774v1.pdf | author:Chol Man Ho, Son Il Gwak, Song Ho Pak, Jong Won Ha category:cs.NE published:2014-12-25 summary:To improve the problem that the parameter identification for fuzzy neuralnetwork has many time complexities in calculating, an improved T-S fuzzyinference method and an parameter identification method for fuzzy neuralnetwork are proposed. It mainly includes three parts. First, improved fuzzyinference method based on production term for T-S Fuzzy model is explained.Then, compared with existing Sugeno fuzzy inference based on Compositionalrules and type-distance fuzzy inference method, the proposed fuzzy inferencealgorithm has a less amount of complexity in calculating and the calculatingprocess is simple. Next, a parameter identification method for FNN based onproduction inference is proposed. Finally, the proposed method is applied forthe precipitation forecast and security situation prediction. Test resultsshowed that the proposed method significantly improved the effectiveness ofidentification, reduced the learning order, time complexity and learning error.
arxiv-8400-9 | Concentration for matrix martingales in continuous time and microscopic activity of social networks | http://arxiv.org/pdf/1412.7705v1.pdf | author:Emmanuel Bacry, Stéphane Gaïffas, Jean-François Muzy category:math.PR stat.ML published:2014-12-24 summary:This paper gives new concentration inequalities for the spectral norm ofmatrix martingales in continuous time. Both cases of purely discountinuous andcontinuous martingales are considered. The analysis is based on a newsupermartingale property of the trace exponential, based on tools fromstochastic calculus. Matrix martingales in continuous time are probabilisticobjects that naturally appear for statistical learning of time-dependentsystems. We focus here on the the microscopic study of (social) networks, basedon self-exciting counting processes, such as the Hawkes process, together witha low-rank prior assumption of the self-exciting component. A consequence ofthese new concentration inequalities is a push forward of the theoreticalanalysis of such models.
arxiv-8400-10 | Locating Tables in Scanned Documents for Reconstructing and Republishing (ICIAfS14) | http://arxiv.org/pdf/1412.7689v1.pdf | author:Akmal Jahan Mac, Roshan G Ragel category:cs.CV published:2014-12-24 summary:Pool of knowledge available to the mankind depends on the source of learningresources, which can vary from ancient printed documents to present electronicmaterial. The rapid conversion of material available in traditional librariesto digital form needs a significant amount of work if we are to maintain theformat and the look of the electronic documents as same as their printedcounterparts. Most of the printed documents contain not only characters and itsformatting but also some associated non text objects such as tables, charts andgraphical objects. It is challenging to detect them and to concentrate on theformat preservation of the contents while reproducing them. To address thisissue, we propose an algorithm using local thresholds for word space and lineheight to locate and extract all categories of tables from scanned documentimages. From the experiments performed on 298 documents, we conclude that ouralgorithm has an overall accuracy of about 75% in detecting tables from thescanned document images. Since the algorithm does not completely depend on rulelines, it can detect all categories of tables in a range of scanned documentswith different font types, styles and sizes to extract their formattingfeatures. Moreover, the algorithm can be applied to locate tables in multicolumn layouts with small modification in layout analysis. Treating tables withtheir existing formatting features will tremendously help the reproducing ofprinted documents for reprinting and updating purposes.
arxiv-8400-11 | A Fuzzy Based Model to Identify Printed Sinhala Characters (ICIAfS14) | http://arxiv.org/pdf/1412.7680v1.pdf | author:G. I. Gunarathna, M. A. P. Chamikara, R. G. Ragel category:cs.CV published:2014-12-24 summary:Character recognition techniques for printed documents are widely used forEnglish language. However, the systems that are implemented to recognize Asianlanguages struggle to increase the accuracy of recognition. Among other Asianlanguages (such as Arabic, Tamil, Chinese), Sinhala characters are unique,mainly because they are round in shape. This unique feature makes it achallenge to extend the prevailing techniques to improve recognition of Sinhalacharacters. Therefore, a little attention has been given to improve theaccuracy of Sinhala character recognition. A novel method, which makes use ofthis unique feature, could be advantageous over other methods. This paperdescribes the use of a fuzzy inference system to recognize Sinhala characters.Feature extraction is mainly focused on distance and intersection measurementsin different directions from the center of the letter making use of the roundshape of characters. The results showed an overall accuracy of 90.7% for 140instances of letters tested, much better than similar systems.
arxiv-8400-12 | Inference for Sparse Conditional Precision Matrices | http://arxiv.org/pdf/1412.7638v1.pdf | author:Jialei Wang, Mladen Kolar category:stat.ML published:2014-12-24 summary:Given $n$ i.i.d. observations of a random vector $(X,Z)$, where $X$ is ahigh-dimensional vector and $Z$ is a low-dimensional index variable, we studythe problem of estimating the conditional inverse covariance matrix $\Omega(z)= (E[(X-E[X \mid Z])(X-E[X \mid Z])^T \mid Z=z])^{-1}$ under the assumptionthat the set of non-zero elements is small and does not depend on the indexvariable. We develop a novel procedure that combines the ideas of the localconstant smoothing and the group Lasso for estimating the conditional inversecovariance matrix. A proximal iterative smoothing algorithm is used to solvethe corresponding convex optimization problems. We prove that our procedurerecovers the conditional independence assumptions of the distribution $X \midZ$ with high probability. This result is established by developing a uniformdeviation bound for the high-dimensional conditional covariance matrix from itspopulation counterpart, which may be of independent interest. Furthermore, wedevelop point-wise confidence intervals for individual elements of theconditional inverse covariance matrix. We perform extensive simulation studies,in which we demonstrate that our proposed procedure outperforms sensiblecompetitors. We illustrate our proposal on a S&P 500 stock price data set.
arxiv-8400-13 | Symmetry in Image Registration and Deformation Modeling | http://arxiv.org/pdf/1412.7513v2.pdf | author:Stefan Sommer, Henry O. Jacobs category:cs.CV math.DG published:2014-12-23 summary:We survey the role of symmetry in diffeomorphic registration of landmarks,curves, surfaces, images and higher-order data. The infinite dimensionalproblem of finding correspondences between objects can for a range of concretedata types be reduced resulting in compact representations of shape and spatialstructure. This reduction is possible because the available data is incompletein encoding the full deformation model. Using reduction by symmetry, wedescribe the reduced models in a common theoretical framework that draws onlinks between the registration problem and geometric mechanics. Symmetry alsoarises in reduction to the Lie algebra using particle relabeling symmetryallowing the equations of motion to be written purely in terms of Eulerianvelocity field. Reduction by symmetry has recently been applied forjet-matching and higher-order discrete approximations of the image matchingproblem. We outline these constructions and further cases where reduction bysymmetry promises new approaches to registration of complex data types.
arxiv-8400-14 | AltecOnDB: A Large-Vocabulary Arabic Online Handwriting Recognition Database | http://arxiv.org/pdf/1412.7626v1.pdf | author:Ibrahim Abdelaziz, Sherif Abdou category:cs.CV published:2014-12-24 summary:Arabic is a semitic language characterized by a complex and rich morphology.The exceptional degree of ambiguity in the writing system, the rich morphology,and the highly complex word formation process of roots and patterns allcontribute to making computational approaches to Arabic very challenging. As aresult, a practical handwriting recognition system should support largevocabulary to provide a high coverage and use the context information fordisambiguation. Several research efforts have been devoted for building onlineArabic handwriting recognition systems. Most of these methods are either usingtheir small private test data sets or a standard database with limited lexiconand coverage. A large scale handwriting database is an essential resource thatcan advance the research of online handwriting recognition. Currently, there isno online Arabic handwriting database with large lexicon, high coverage, largenumber of writers and training/testing data. In this paper, we introduce AltecOnDB, a large scale online Arabichandwriting database. AltecOnDB has 98% coverage of all the possible PAWS ofthe Arabic language. The collected samples are complete sentences that includedigits and punctuation marks. The collected data is available on sentence, wordand character levels, hence, high-level linguistic models can be used forperformance improvements. Data is collected from more than 1000 writers withdifferent backgrounds, genders and ages. Annotation and verification tools aredeveloped to facilitate the annotation and verification phases. We built anelementary recognition system to test our database and show the existingdifficulties when handling a large vocabulary and dealing with large amounts ofstyles variations in the collected data.
arxiv-8400-15 | Particle Metropolis adjusted Langevin algorithms for state space models | http://arxiv.org/pdf/1402.0694v2.pdf | author:Chris Nemeth, Paul Fearnhead category:stat.CO stat.ML published:2014-02-04 summary:Particle MCMC is a class of algorithms that can be used to analysestate-space models. They use MCMC moves to update the parameters of the models,and particle filters to propose values for the path of the state-space model.Currently the default is to use random walk Metropolis to update the parametervalues. We show that it is possible to use information from the output of theparticle filter to obtain better proposal distributions for the parameters. Inparticular it is possible to obtain estimates of the gradient of the logposterior from each run of the particle filter, and use these estimates withina Langevin-type proposal. We propose using the recent computationally efficientapproach of Nemeth et al. (2013) for obtaining such estimates. We showempirically that for a variety of state-space models this proposal is moreefficient than the standard random walk Metropolis proposal in terms of:reducing autocorrelation of the posterior samples, reducing the burn-in time ofthe MCMC sampler and increasing the squared jump distance between posteriorsamples.
arxiv-8400-16 | The Computational Theory of Intelligence: Information Entropy | http://arxiv.org/pdf/1412.7978v1.pdf | author:Daniel Kovach category:cs.AI cs.LG 68T27 I.2.1 published:2014-12-24 summary:This paper presents an information theoretic approach to the concept ofintelligence in the computational sense. We introduce a probabilistic frameworkfrom which computational intelligence is shown to be an entropy minimizingprocess at the local level. Using this new scheme, we develop a simple datadriven clustering example and discuss its applications.
arxiv-8400-17 | Crypto-Nets: Neural Networks over Encrypted Data | http://arxiv.org/pdf/1412.6181v2.pdf | author:Pengtao Xie, Misha Bilenko, Tom Finley, Ran Gilad-Bachrach, Kristin Lauter, Michael Naehrig category:cs.LG cs.CR cs.NE published:2014-12-18 summary:The problem we address is the following: how can a user employ a predictivemodel that is held by a third party, without compromising private information.For example, a hospital may wish to use a cloud service to predict thereadmission risk of a patient. However, due to regulations, the patient'smedical files cannot be revealed. The goal is to make an inference using themodel, without jeopardizing the accuracy of the prediction or the privacy ofthe data. To achieve high accuracy, we use neural networks, which have been shown tooutperform other learning models for many tasks. To achieve the privacyrequirements, we use homomorphic encryption in the following protocol: the dataowner encrypts the data and sends the ciphertexts to the third party to obtaina prediction from a trained model. The model operates on these ciphertexts andsends back the encrypted prediction. In this protocol, not only the dataremains private, even the values predicted are available only to the dataowner. Using homomorphic encryption and modifications to the activation functionsand training algorithms of neural networks, we show that it is protocol ispossible and may be feasible. This method paves the way to build a securecloud-based neural network prediction services without invading users' privacy.
arxiv-8400-18 | Differential Privacy and Machine Learning: a Survey and Review | http://arxiv.org/pdf/1412.7584v1.pdf | author:Zhanglong Ji, Zachary C. Lipton, Charles Elkan category:cs.LG cs.CR cs.DB published:2014-12-24 summary:The objective of machine learning is to extract useful information from data,while privacy is preserved by concealing information. Thus it seems hard toreconcile these competing interests. However, they frequently must be balancedwhen mining sensitive data. For example, medical research represents animportant application where it is necessary both to extract useful informationand protect patient privacy. One way to resolve the conflict is to extractgeneral characteristics of whole populations without disclosing the privateinformation of individuals. In this paper, we consider differential privacy, one of the most popular andpowerful definitions of privacy. We explore the interplay between machinelearning and differential privacy, namely privacy-preserving machine learningalgorithms and learning-based data release mechanisms. We also describe sometheoretical results that address what can be learned differentially privatelyand upper bounds of loss functions for differentially private algorithms. Finally, we present some open questions, including how to incorporate publicdata, how to deal with missing data in private datasets, and whether, as thenumber of observed samples grows arbitrarily large, differentially privatemachine learning algorithms can be achieved at no cost to utility as comparedto corresponding non-differentially private algorithms.
arxiv-8400-19 | Matching Pursuit LASSO Part II: Applications and Sparse Recovery over Batch Signals | http://arxiv.org/pdf/1302.5010v2.pdf | author:Mingkui Tan, Ivor W. Tsang, Li Wang category:cs.CV cs.LG stat.ML published:2013-02-20 summary:Matching Pursuit LASSIn Part I \cite{TanPMLPart1}, a Matching Pursuit LASSO({MPL}) algorithm has been presented for solving large-scale sparse recovery(SR) problems. In this paper, we present a subspace search to further improvethe performance of MPL, and then continue to address another major challenge ofSR -- batch SR with many signals, a consideration which is absent from most ofprevious $\ell_1$-norm methods. As a result, a batch-mode {MPL} is developed tovastly speed up sparse recovery of many signals simultaneously. Comprehensivenumerical experiments on compressive sensing and face recognition tasksdemonstrate the superior performance of MPL and BMPL over other methodsconsidered in this paper, in terms of sparse recovery ability and efficiency.In particular, BMPL is up to 400 times faster than existing $\ell_1$-normmethods considered to be state-of-the-art.O Part II: Applications and SparseRecovery over Batch Signals
arxiv-8400-20 | A Time and Space Efficient Junction Tree Architecture | http://arxiv.org/pdf/1308.0187v9.pdf | author:Stephen Pasteris category:cs.AI cs.LG published:2013-07-31 summary:The junction tree algorithm is a way of computing marginals of booleanmultivariate probability distributions that factorise over sets of randomvariables. The junction tree algorithm first constructs a tree called ajunction tree who's vertices are sets of random variables. The algorithm thenperforms a generalised version of belief propagation on the junction tree. TheShafer-Shenoy and Hugin architectures are two ways to perform this beliefpropagation that tradeoff time and space complexities in different ways: Huginpropagation is at least as fast as Shafer-Shenoy propagation and in the casesthat we have large vertices of high degree is significantly faster. However,this speed increase comes at the cost of an increased space complexity. Thispaper first introduces a simple novel architecture, ARCH-1, which has the bestof both worlds: the speed of Hugin propagation and the low space requirementsof Shafer-Shenoy propagation. A more complicated novel architecture, ARCH-2, isthen introduced which has, up to a factor only linear in the maximumcardinality of any vertex, time and space complexities at least as good asARCH-1 and in the cases that we have large vertices of high degree issignificantly faster than ARCH-1.
arxiv-8400-21 | Model Selection in High-Dimensional Misspecified Models | http://arxiv.org/pdf/1412.7468v1.pdf | author:Pallavi Basu, Yang Feng, Jinchi Lv category:math.ST stat.ME stat.ML stat.TH published:2014-12-23 summary:Model selection is indispensable to high-dimensional sparse modeling inselecting the best set of covariates among a sequence of candidate models. Mostexisting work assumes implicitly that the model is correctly specified or offixed dimensions. Yet model misspecification and high dimensionality are commonin real applications. In this paper, we investigate two classicalKullback-Leibler divergence and Bayesian principles of model selection in thesetting of high-dimensional misspecified models. Asymptotic expansions of theseprinciples reveal that the effect of model misspecification is crucial andshould be taken into account, leading to the generalized AIC and generalizedBIC in high dimensions. With a natural choice of prior probabilities, wesuggest the generalized BIC with prior probability which involves a logarithmicfactor of the dimensionality in penalizing model complexity. We furtherestablish the consistency of the covariance contrast matrix estimator in ageneral setting. Our results and new method are supported by numerical studies.
arxiv-8400-22 | Learning Temporal Dependencies in Data Using a DBN-BLSTM | http://arxiv.org/pdf/1412.6093v2.pdf | author:Kratarth Goel, Raunaq Vohra category:cs.LG cs.NE published:2014-12-18 summary:Since the advent of deep learning, it has been used to solve various problemsusing many different architectures. The application of such deep architecturesto auditory data is also not uncommon. However, these architectures do notalways adequately consider the temporal dependencies in data. We thus propose anew generic architecture called the Deep Belief Network - Bidirectional LongShort-Term Memory (DBN-BLSTM) network that models sequences by keeping track ofthe temporal information while enabling deep representations in the data. Wedemonstrate this new architecture by applying it to the task of musicgeneration and obtain state-of-the-art results.
arxiv-8400-23 | Facial Expressions recognition Based on Principal Component Analysis (PCA) | http://arxiv.org/pdf/1506.01939v1.pdf | author:Abdelmajid Hassan Mansour, Gafar Zen Alabdeen Salh, Ali Shaif Alhalemi category:cs.CV published:2014-12-23 summary:The facial expression recognition is an ocular task that can be performedwithout human discomfort, is really a speedily growing on the computer researchfield. There are many applications and programs uses facial expression toevaluate human character, judgment, feelings, and viewpoint. The process ofrecognizing facial expression is a hard task due to the several circumstancessuch as facial occlusions, face shape, illumination, face colors, and etc. Thispaper present a PCA methodology to distinguish expressions of faces underdifferent circumstances and identifying it. Relies on Eigen faces techniqueusing standard Data base images. So as to overcome the problem of difficulty tocomputers to identify the features and expressions of persons.
arxiv-8400-24 | Global registration of multiple point clouds using semidefinite programming | http://arxiv.org/pdf/1306.5226v5.pdf | author:Kunal N. Chaudhury, Yuehaw Khoo, Amit Singer category:cs.CV cs.NA math.NA math.OC published:2013-06-21 summary:Consider $N$ points in $\mathbb{R}^d$ and $M$ local coordinate systems thatare related through unknown rigid transforms. For each point we are given(possibly noisy) measurements of its local coordinates in some of thecoordinate systems. Alternatively, for each coordinate system, we observe thecoordinates of a subset of the points. The problem of estimating the globalcoordinates of the $N$ points (up to a rigid transform) from such measurementscomes up in distributed approaches to molecular conformation and sensor networklocalization, and also in computer vision and graphics. The least-squares formulation of this problem, though non-convex, has a wellknown closed-form solution when $M=2$ (based on the singular valuedecomposition). However, no closed form solution is known for $M\geq 3$. In this paper, we demonstrate how the least-squares formulation can berelaxed into a convex program, namely a semidefinite program (SDP). By settingup connections between the uniqueness of this SDP and results from rigiditytheory, we prove conditions for exact and stable recovery for the SDPrelaxation. In particular, we prove that the SDP relaxation can guaranteerecovery under more adversarial conditions compared to earlier proposedspectral relaxations, and derive error bounds for the registration errorincurred by the SDP relaxation. We also present results of numerical experiments on simulated data to confirmthe theoretical findings. We empirically demonstrate that (a) unlike thespectral relaxation, the relaxation gap is mostly zero for the semidefiniteprogram (i.e., we are able to solve the original non-convex least-squaresproblem) up to a certain noise threshold, and (b) the semidefinite programperforms significantly better than spectral and manifold-optimization methods,particularly at large noise levels.
arxiv-8400-25 | Fusing Color and Texture Cues to Categorize the Fruit Diseases from Images | http://arxiv.org/pdf/1412.7277v1.pdf | author:Shiv Ram Dubey, Anand Singh Jalal category:cs.CV published:2014-12-23 summary:The economic and production losses in agricultural industry worldwide are dueto the presence of diseases in the several kinds of fruits. In this paper, amethod for the classification of fruit diseases is proposed and experimentallyvalidated. The image processing based proposed approach is composed of thefollowing main steps; in the first step K-Means clustering technique is usedfor the defect segmentation, in the second step color and textural cues areextracted and fused from the segmented image, and finally images are classifiedinto one of the classes by using a Multi-class Support Vector Machine. We haveconsidered diseases of apple as a test case and evaluated our approach forthree types of apple diseases namely apple scab, apple blotch and apple rot andnormal apples without diseases. Our experimentation points out that theproposed fusion scheme can significantly support accurate detection andautomatic classification of fruit diseases.
arxiv-8400-26 | Text to Multi-level MindMaps: A Novel Method for Hierarchical Visual Abstraction of Natural Language Text | http://arxiv.org/pdf/1408.1031v2.pdf | author:Mohamed Elhoseiny, Ahmed Elgammal category:cs.CL cs.HC published:2014-08-01 summary:MindMapping is a well-known technique used in note taking, which encourageslearning and studying. MindMapping has been manually adopted to help presentknowledge and concepts in a visual form. Unfortunately, there is no reliableautomated approach to generate MindMaps from Natural Language text. This workfirstly introduces MindMap Multilevel Visualization concept which is to jointlyvisualize and summarize textual information. The visualization is achievedpictorially across multiple levels using semantic information (i.e. ontology),while the summarization is achieved by the information in the highest levels asthey represent abstract information in the text. This work also presents thefirst automated approach that takes a text input and generates a MindMapvisualization out of it. The approach could visualize text documents inmultilevel MindMaps, in which a high-level MindMap node could be expanded intochild MindMaps. \ignore{ As far as we know, this is the first work that viewMindMapping as a new approach to jointly summarize and visualize textualinformation.} The proposed method involves understanding of the input text andconverting it into intermediate Detailed Meaning Representation (DMR). The DMRis then visualized with two modes; Single level or Multiple levels, which isconvenient for larger text. The generated MindMaps from both approaches wereevaluated based on Human Subject experiments performed on Amazon MechanicalTurk with various parameter settings.
arxiv-8400-27 | Improved Distributed Principal Component Analysis | http://arxiv.org/pdf/1408.5823v5.pdf | author:Maria-Florina Balcan, Vandana Kanchanapally, Yingyu Liang, David Woodruff category:cs.LG published:2014-08-25 summary:We study the distributed computing setting in which there are multipleservers, each holding a set of points, who wish to compute functions on theunion of their point sets. A key task in this setting is Principal ComponentAnalysis (PCA), in which the servers would like to compute a low dimensionalsubspace capturing as much of the variance of the union of their point sets aspossible. Given a procedure for approximate PCA, one can use it toapproximately solve $\ell_2$-error fitting problems such as $k$-meansclustering and subspace clustering. The essential properties of an approximatedistributed PCA algorithm are its communication cost and computationalefficiency for a given desired accuracy in downstream applications. We give newalgorithms and analyses for distributed PCA which lead to improvedcommunication and computational costs for $k$-means clustering and relatedproblems. Our empirical study on real world data shows a speedup of orders ofmagnitude, preserving communication with only a negligible degradation insolution quality. Some of these techniques we develop, such as a generaltransformation from a constant success probability subspace embedding to a highsuccess probability subspace embedding with a dimension and sparsityindependent of the success probability, may be of independent interest.
arxiv-8400-28 | An $\{l_1,l_2,l_{\infty}\}$-Regularization Approach to High-Dimensional Errors-in-variables Models | http://arxiv.org/pdf/1412.7216v1.pdf | author:Alexandre Belloni, Mathieu Rosenbaum, Alexandre B. Tsybakov category:math.ST stat.ML stat.TH published:2014-12-22 summary:Several new estimation methods have been recently proposed for the linearregression model with observation error in the design. Different assumptions onthe data generating process have motivated different estimators and analysis.In particular, the literature considered (1) observation errors in the designuniformly bounded by some $\bar \delta$, and (2) zero mean independentobservation errors. Under the first assumption, the rates of convergence of theproposed estimators depend explicitly on $\bar \delta$, while the secondassumption has been applied when an estimator for the second moment of theobservational error is available. This work proposes and studies two newestimators which, compared to other procedures for regression models witherrors in the design, exploit an additional $l_{\infty}$-norm regularization.The first estimator is applicable when both (1) and (2) hold but does notrequire an estimator for the second moment of the observational error. Thesecond estimator is applicable under (2) and requires an estimator for thesecond moment of the observation error. Importantly, we impose no assumption onthe accuracy of this pilot estimator, in contrast to the previously knownprocedures. As the recent proposals, we allow the number of covariates to bemuch larger than the sample size. We establish the rates of convergence of theestimators and compare them with the bounds obtained for related estimators inthe literature. These comparisons show interesting insights on the interplay ofthe assumptions and the achievable rates of convergence.
arxiv-8400-29 | Online Distributed Optimization on Dynamic Networks | http://arxiv.org/pdf/1412.7215v1.pdf | author:Saghar Hosseini, Airlie Chapman, Mehran Mesbahi category:math.OC cs.DS cs.LG cs.MA cs.SY published:2014-12-22 summary:This paper presents a distributed optimization scheme over a network ofagents in the presence of cost uncertainties and over switching communicationtopologies. Inspired by recent advances in distributed convex optimization, wepropose a distributed algorithm based on a dual sub-gradient averaging. Theobjective of this algorithm is to minimize a cost function cooperatively.Furthermore, the algorithm changes the weights on the communication links inthe network to adapt to varying reliability of neighboring agents. Aconvergence rate analysis as a function of the underlying network topology isthen presented, followed by simulation results for representative classes ofsensor networks.
arxiv-8400-30 | Non-stationary Stochastic Optimization | http://arxiv.org/pdf/1307.5449v2.pdf | author:O. Besbes, Y. Gur, A. Zeevi category:math.PR cs.LG stat.ML published:2013-07-20 summary:We consider a non-stationary variant of a sequential stochastic optimizationproblem, in which the underlying cost functions may change along the horizon.We propose a measure, termed variation budget, that controls the extent of saidchange, and study how restrictions on this budget impact achievableperformance. We identify sharp conditions under which it is possible to achievelong-run-average optimality and more refined performance measures such as rateoptimality that fully characterize the complexity of such problems. In doingso, we also establish a strong connection between two rather disparate strandsof literature: adversarial online convex optimization; and the more traditionalstochastic approximation paradigm (couched in a non-stationary setting). Thisconnection is the key to deriving well performing policies in the latter, byleveraging structure of optimal policies in the former. Finally, tight boundson the minimax regret allow us to quantify the "price of non-stationarity,"which mathematically captures the added complexity embedded in a temporallychanging environment versus a stationary one.
arxiv-8400-31 | Audio Source Separation Using a Deep Autoencoder | http://arxiv.org/pdf/1412.7193v1.pdf | author:Giljin Jang, Han-Gyu Kim, Yung-Hwan Oh category:cs.SD cs.LG cs.NE published:2014-12-22 summary:This paper proposes a novel framework for unsupervised audio sourceseparation using a deep autoencoder. The characteristics of unknown sourcesignals mixed in the mixed input is automatically by properly configuredautoencoders implemented by a network with many layers, and separated byclustering the coefficient vectors in the code layer. By investigating theweight vectors to the final target, representation layer, the primitivecomponents of the audio signals in the frequency domain are observed. Byclustering the activation coefficients in the code layer, the previouslyunknown source signals are segregated. The original source sounds are thenseparated and reconstructed by using code vectors which belong to differentclusters. The restored sounds are not perfect but yield promising results forthe possibility in the success of many practical applications.
arxiv-8400-32 | Bayesian Optimisation for Machine Translation | http://arxiv.org/pdf/1412.7180v1.pdf | author:Yishu Miao, Ziyu Wang, Phil Blunsom category:cs.CL cs.LG I.2.7 published:2014-12-22 summary:This paper presents novel Bayesian optimisation algorithms for minimum errorrate training of statistical machine translation systems. We explore twoclasses of algorithms for efficiently exploring the translation space, with thefirst based on N-best lists and the second based on a hypergraph representationthat compactly represents an exponential number of translation options. Ouralgorithms exhibit faster convergence and are capable of obtaining lower errorrates than the existing translation model specific approaches, all within ageneric Bayesian optimisation framework. Further more, we also introduce arandom embedding algorithm to scale our approach to sparse high dimensionalfeature sets.
arxiv-8400-33 | Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus | http://arxiv.org/pdf/1412.4314v2.pdf | author:Joseph Chee Chang, Chu-Cheng Lin category:cs.NE cs.CL published:2014-12-14 summary:Mixed language data is one of the difficult yet less explored domains ofnatural language processing. Most research in fields like machine translationor sentiment analysis assume monolingual input. However, people who are capableof using more than one language often communicate using multiple languages atthe same time. Sociolinguists believe this "code-switching" phenomenon to besocially motivated. For example, to express solidarity or to establishauthority. Most past work depend on external tools or resources, such aspart-of-speech tagging, dictionary look-up, or named-entity recognizers toextract rich features for training machine learning models. In this paper, wetrain recurrent neural networks with only raw features, and use word embeddingto automatically learn meaningful representations. Using the samemixed-language Twitter corpus, our system is able to outperform the bestSVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% inaccuracy, or by 17% in error rate reduction.
arxiv-8400-34 | Half-CNN: A General Framework for Whole-Image Regression | http://arxiv.org/pdf/1412.6885v1.pdf | author:Jun Yuan, Bingbing Ni, Ashraf A. Kassim category:cs.CV cs.LG cs.NE published:2014-12-22 summary:The Convolutional Neural Network (CNN) has achieved great success in imageclassification. The classification model can also be utilized at image or patchlevel for many other applications, such as object detection and segmentation.In this paper, we propose a whole-image CNN regression model, by removing thefull connection layer and training the network with continuous feature maps.This is a generic regression framework that fits many applications. Wedemonstrate this method through two tasks: simultaneous face detection &segmentation, and scene saliency prediction. The result is comparable withother models in the respective fields, using only a small scale network. Sincethe regression model is trained on corresponding image / feature map pairs,there are no requirements on uniform input size as opposed to theclassification model. Our framework avoids classifier design, a process thatmay introduce too much manual intervention in model development. Yet, it ishighly correlated to the classification network and offers some in-deep reviewof CNN structures.
arxiv-8400-35 | A New Way to Factorize Linear Cameras | http://arxiv.org/pdf/1412.6847v1.pdf | author:Feng Lu, Ziqiang Chen category:cs.CV published:2014-12-22 summary:The implementation details of factorizing the 3x4 projection matrices oflinear cameras into their left matrix factors and the 4x4 homogeneouscentral(also parallel for infinite center cases) projection factors arepresented in this work. Any full row rank 3x4 real matrix can be factorizedinto such basic matrices which will be called LC factors. A further extension to multiple view midpoint triangulation, for both pinholeand affine camera cases, is also presented based on such camera factorizations.
arxiv-8400-36 | A Stable Multi-Scale Kernel for Topological Machine Learning | http://arxiv.org/pdf/1412.6821v1.pdf | author:Jan Reininghaus, Stefan Huber, Ulrich Bauer, Roland Kwitt category:stat.ML cs.CV cs.LG math.AT published:2014-12-21 summary:Topological data analysis offers a rich source of valuable information tostudy vision problems. Yet, so far we lack a theoretically sound connection topopular kernel-based learning techniques, such as kernel SVMs or kernel PCA. Inthis work, we establish such a connection by designing a multi-scale kernel forpersistence diagrams, a stable summary representation of topological featuresin data. We show that this kernel is positive definite and prove its stabilitywith respect to the 1-Wasserstein distance. Experiments on two benchmarkdatasets for 3D shape classification/retrieval and texture recognition showconsiderable performance gains of the proposed method compared to analternative approach that is based on the recently introduced persistencelandscapes.
arxiv-8400-37 | Parameter estimation in spherical symmetry groups | http://arxiv.org/pdf/1411.2540v2.pdf | author:Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Marc DeGraef, Jeffrey Simmons, Alfred Hero category:stat.ML published:2014-11-10 summary:This paper considers statistical estimation problems where the probabilitydistribution of the observed random variable is invariant with respect toactions of a finite topological group. It is shown that any such distributionmust satisfy a restricted finite mixture representation. When specialized tothe case of distributions over the sphere that are invariant to the actions ofa finite spherical symmetry group $\mathcal G$, a group-invariant extension ofthe Von Mises Fisher (VMF) distribution is obtained. The $\mathcal G$-invariantVMF is parameterized by location and scale parameters that specify thedistribution's mean orientation and its concentration about the mean,respectively. Using the restricted finite mixture representation theseparameters can be estimated using an Expectation Maximization (EM) maximumlikelihood (ML) estimation algorithm. This is illustrated for the problem ofmean crystal orientation estimation under the spherically symmetric groupassociated with the crystal form, e.g., cubic or octahedral or hexahedral.Simulations and experiments establish the advantages of the extended VMF EM-MLestimator for data acquired by Electron Backscatter Diffraction (EBSD)microscopy of a polycrystalline Nickel alloy sample.
arxiv-8400-38 | Mixture of Parts Revisited: Expressive Part Interactions for Pose Estimation | http://arxiv.org/pdf/1412.6791v1.pdf | author:Anoop Katti, Anurag Mittal category:cs.CV published:2014-12-21 summary:Part-based models with restrictive tree-structured interactions for the HumanPose Estimation problem, leaves many part interactions unhandled. Two of themost common and strong manifestations of such unhandled interactions areself-occlusion among the parts and the confusion in the localization of thenon-adjacent symmetric parts. By handling the self-occlusion in a dataefficient manner, we improve the performance of the basic Mixture of Partsmodel by a large margin, especially on uncommon poses. Through addressing theconfusion in the symmetric limb localization using a combination of twocomplementing trees, we improve the performance on all the parts by atmostdoubling the running time. Finally, we show that the combination of the twosolutions improves the results. We report results that are equivalent to thestate-of-the-art on two standard datasets. Because of maintaining thetree-structured interactions and only part-level modeling of the base Mixtureof Parts model, this is achieved in time that is much less than the bestperforming part-based model.
arxiv-8400-39 | Bi-directional Shape Correspondences (BSC): A Novel Technique for 2-d Shape Warping in Quadratic Time? | http://arxiv.org/pdf/1412.6759v1.pdf | author:Abdulrahman Oladipupo Ibraheem category:cs.CV published:2014-12-21 summary:We propose Bidirectional Shape Correspondence (BSC) as a possible improvementon the famous shape contexts (SC) framework. Our proposals derive from theobservation that the SC framework enforces a one-to-one correspondence betweensample points, and that this leads to two possible drawbacks. First, thisdenies the framework the opportunity to effect advantageous many-to-manymatching between points on the two shapes being compared. Second, this callsfor the Hungarian algorithm which unfortunately usurps cubic time. While thedynamic-space-warping dynamic programming algorithm has provided a standardsolution to the first problem above, it demands quintic time for generalmulti-contour shapes, and w times quadratic time for the special case ofsingle-contour shapes, even after an heuristic search window of width w hasbeen chosen. Therefore, in this work, we propose a simple method for computing"many-to-many" correspondences for the class of all 2-d shapes in quadratictime. Our approach is to explicitly let each point on the first shape choose abest match on the second shape, and vice versa. Along the way, we also proposethe use of data-clustering techniques for dealing with the outliers problem,and, from another viewpoint, it turns out that this clustering can be seen asan autonomous, rather than pre-computed, sampling of shape boundary.
arxiv-8400-40 | Correlation of Data Reconstruction Error and Shrinkages in Pair-wise Distances under Principal Component Analysis (PCA) | http://arxiv.org/pdf/1412.6752v1.pdf | author:Abdulrahman Oladipupo Ibraheem category:cs.LG stat.ML published:2014-12-21 summary:In this on-going work, I explore certain theoretical and empiricalimplications of data transformations under the PCA. In particular, I state andprove three theorems about PCA, which I paraphrase as follows: 1). PCA withoutdiscarding eigenvector rows is injective, but looses this injectivity wheneigenvector rows are discarded 2). PCA without discarding eigen- vector rowspreserves pair-wise distances, but tends to cause pair-wise distances to shrinkwhen eigenvector rows are discarded. 3). For any pair of points, the shrinkagein pair-wise distance is bounded above by an L1 norm reconstruction errorassociated with the points. Clearly, 3). suggests that there might exist somecorrelation between shrinkages in pair-wise distances and mean squarereconstruction error which is defined as the sum of those eigenvaluesassociated with the discarded eigenvectors. I therefore decided to performnumerical experiments to obtain the corre- lation between the sum of thoseeigenvalues and shrinkages in pair-wise distances. In addition, I have alsoperformed some experiments to check respectively the effect of the sum of thoseeigenvalues and the effect of the shrinkages on classification accuracies underthe PCA map. So far, I have obtained the following results on some publiclyavailable data from the UCI Machine Learning Repository: 1). There seems to bea strong cor- relation between the sum of those eigenvalues associated withdiscarded eigenvectors and shrinkages in pair-wise distances. 2). Neither thesum of those eigenvalues nor pair-wise distances have any strong correlationswith classification accuracies. 1
arxiv-8400-41 | SENNS: Sparse Extraction Neural NetworkS for Feature Extraction | http://arxiv.org/pdf/1412.6749v1.pdf | author:Abdulrahman Oladipupo Ibraheem category:cs.CV cs.AI cs.NE math.OC stat.ML 90-08 published:2014-12-21 summary:By drawing on ideas from optimisation theory, artificial neural networks(ANN), graph embeddings and sparse representations, I develop a noveltechnique, termed SENNS (Sparse Extraction Neural NetworkS), aimed ataddressing the feature extraction problem. The proposed method uses (preferablydeep) ANNs for projecting input attribute vectors to an output space whereinpairwise distances are maximized for vectors belonging to different classes,but minimized for those belonging to the same class, while simultaneouslyenforcing sparsity on the ANN outputs. The vectors that result from theprojection can then be used as features in any classifier of choice.Mathematically, I formulate the proposed method as the minimisation of anobjective function which can be interpreted, in the ANN output space, as anegative factor of the sum of the squares of the pair-wise distances betweenoutput vectors belonging to different classes, added to a positive factor ofthe sum of squares of the pair-wise distances between output vectors belongingto the same classes, plus sparsity and weight decay terms. To derive analgorithm for minimizing the objective function via gradient descent, I use themulti-variate version of the chain rule to obtain the partial derivatives ofthe function with respect to ANN weights and biases, and find that each of therequired partial derivatives can be expressed as a sum of six terms. As itturns out, four of those six terms can be computed using the standard backpropagation algorithm; the fifth can be computed via a slight modification ofthe standard backpropagation algorithm; while the sixth one can be computed viasimple arithmetic. Finally, I propose experiments on the ARABASE Arabic corporaof digits and letters, the CMU PIE database of faces, the MNIST digitsdatabase, and other standard machine learning databases.
arxiv-8400-42 | Locally Weighted Learning for Naive Bayes Classifier | http://arxiv.org/pdf/1412.6741v1.pdf | author:Kim-Hung Li, Cheuk Ting Li category:stat.ML cs.LG published:2014-12-21 summary:As a consequence of the strong and usually violated conditional independenceassumption (CIA) of naive Bayes (NB) classifier, the performance of NB becomesless and less favorable compared to sophisticated classifiers when the samplesize increases. We learn from this phenomenon that when the size of thetraining data is large, we should either relax the assumption or apply NB to a"reduced" data set, say for example use NB as a local model. The latterapproach trades the ignored information for the robustness to the modelassumption. In this paper, we consider using NB as a model for locally weighteddata. A special weighting function is designed so that if CIA holds for theunweighted data, it also holds for the weighted data. The new method isintuitive and capable of handling class imbalance. It is theoretically moresound than the locally weighted learners of naive Bayes that baseclassification only on the $k$ nearest neighbors. Empirical study shows thatthe new method with appropriate choice of parameter outperforms seven existingclassifiers of similar nature.
arxiv-8400-43 | Implicit Temporal Differences | http://arxiv.org/pdf/1412.6734v1.pdf | author:Aviv Tamar, Panos Toulis, Shie Mannor, Edoardo M. Airoldi category:stat.ML cs.LG published:2014-12-21 summary:In reinforcement learning, the TD($\lambda$) algorithm is a fundamentalpolicy evaluation method with an efficient online implementation that issuitable for large-scale problems. One practical drawback of TD($\lambda$) isits sensitivity to the choice of the step-size. It is an empirically well-knownfact that a large step-size leads to fast convergence, at the cost of highervariance and risk of instability. In this work, we introduce the implicitTD($\lambda$) algorithm which has the same function and computational cost asTD($\lambda$), but is significantly more stable. We provide a theoreticalexplanation of this stability and an empirical evaluation of implicitTD($\lambda$) on typical benchmark tasks. Our results show that implicitTD($\lambda$) outperforms standard TD($\lambda$) and a state-of-the-art methodthat automatically tunes the step-size, and thus shows promise for wideapplicability.
arxiv-8400-44 | Active Learning for Crowd-Sourced Databases | http://arxiv.org/pdf/1209.3686v4.pdf | author:Barzan Mozafari, Purnamrita Sarkar, Michael J. Franklin, Michael I. Jordan, Samuel Madden category:cs.LG cs.DB published:2012-09-17 summary:Crowd-sourcing has become a popular means of acquiring labeled data for awide variety of tasks where humans are more accurate than computers, e.g.,labeling images, matching objects, or analyzing sentiment. However, relyingsolely on the crowd is often impractical even for data sets with thousands ofitems, due to time and cost constraints of acquiring human input (which costpennies and minutes per label). In this paper, we propose algorithms forintegrating machine learning into crowd-sourced databases, with the goal ofallowing crowd-sourcing applications to scale, i.e., to handle larger datasetsat lower costs. The key observation is that, in many of the above tasks, humansand machine learning algorithms can be complementary, as humans are often moreaccurate but slow and expensive, while algorithms are usually less accurate,but faster and cheaper. Based on this observation, we present two new active learning algorithms tocombine humans and algorithms together in a crowd-sourced database. Ouralgorithms are based on the theory of non-parametric bootstrap, which makes ourresults applicable to a broad class of machine learning models. Our results, onthree real-life datasets collected with Amazon's Mechanical Turk, and on 15well-known UCI data sets, show that our methods on average ask humans to labelone to two orders of magnitude fewer items to achieve the same accuracy as abaseline that labels random images, and two to eight times fewer questions thanprevious active learning schemes.
arxiv-8400-45 | Surpassing Human-Level Face Verification Performance on LFW with GaussianFace | http://arxiv.org/pdf/1404.3840v3.pdf | author:Chaochao Lu, Xiaoou Tang category:cs.CV cs.LG stat.ML published:2014-04-15 summary:Face verification remains a challenging problem in very complex conditionswith large variations such as pose, illumination, expression, and occlusions.This problem is exacerbated when we rely unrealistically on a single trainingdata source, which is often insufficient to cover the intrinsically complexface variations. This paper proposes a principled multi-task learning approachbased on Discriminative Gaussian Process Latent Variable Model, namedGaussianFace, to enrich the diversity of training data. In comparison toexisting methods, our model exploits additional data from multiplesource-domains to improve the generalization performance of face verificationin an unknown target-domain. Importantly, our model can adapt automatically tocomplex data distributions, and therefore can well capture complex facevariations inherent in multiple sources. Extensive experiments demonstrate theeffectiveness of the proposed model in learning from diverse data sources andgeneralize to unseen domain. Specifically, the accuracy of our algorithmachieves an impressive accuracy rate of 98.52% on the well-known andchallenging Labeled Faces in the Wild (LFW) benchmark. For the first time, thehuman-level performance in face verification (97.53%) on LFW is surpassed.
arxiv-8400-46 | Scalable Planning and Learning for Multiagent POMDPs: Extended Version | http://arxiv.org/pdf/1404.1140v2.pdf | author:Christopher Amato, Frans A. Oliehoek category:cs.AI cs.LG published:2014-04-04 summary:Online, sample-based planning algorithms for POMDPs have shown great promisein scaling to problems with large state spaces, but they become intractable forlarge action and observation spaces. This is particularly problematic inmultiagent POMDPs where the action and observation space grows exponentiallywith the number of agents. To combat this intractability, we propose a novelscalable approach based on sample-based planning and factored value functionsthat exploits structure present in many multiagent settings. This approachapplies not only in the planning case, but also in the Bayesian reinforcementlearning setting. Experimental results show that we are able to provide highquality solutions to large multiagent planning and learning problems.
arxiv-8400-47 | On the Robustness of Learning in Games with Stochastically Perturbed Payoff Observations | http://arxiv.org/pdf/1412.6565v1.pdf | author:Mario Bravo, Panayotis Mertikopoulos category:math.OC cs.GT math.PR stat.ML published:2014-12-20 summary:We study a general class of game-theoretic learning dynamics in the presenceof random payoff disturbances and observation noise, and we provide a unifiedframework that extends several rationality properties of the (stochastic)replicator dynamics and other game dynamics. In the unilateral case, we showthat the stochastic dynamics under study lead to no regret, irrespective of thenoise level. In the multi-player case, we find that dominated strategies becomeextinct (a.s.) and strict Nash equilibria remain stochastically asymptoticallystable - again, independently of the perturbations' magnitude. Finally, weestablish an averaging principle for 2-player games and we show that theempirical distribution of play converges to Nash equilibrium in zero-sum gamesunder any noise level.
arxiv-8400-48 | Training for Fast Sequential Prediction Using Dynamic Feature Selection | http://arxiv.org/pdf/1410.8498v2.pdf | author:Emma Strubell, Luke Vilnis, Andrew McCallum category:cs.CL cs.AI published:2014-10-30 summary:We present paired learning and inference algorithms for significantlyreducing computation and increasing speed of the vector dot products in theclassifiers that are at the heart of many NLP components. This is accomplishedby partitioning the features into a sequence of templates which are orderedsuch that high confidence can often be reached using only a small fraction ofall features. Parameter estimation is arranged to maximize accuracy and earlyconfidence in this sequence. We present experiments in left-to-rightpart-of-speech tagging on WSJ, demonstrating that we can preserve accuracyabove 97% with over a five-fold reduction in run-time.
arxiv-8400-49 | Deep Speech: Scaling up end-to-end speech recognition | http://arxiv.org/pdf/1412.5567v2.pdf | author:Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, Andrew Y. Ng category:cs.CL cs.LG cs.NE published:2014-12-17 summary:We present a state-of-the-art speech recognition system developed usingend-to-end deep learning. Our architecture is significantly simpler thantraditional speech systems, which rely on laboriously engineered processingpipelines; these traditional systems also tend to perform poorly when used innoisy environments. In contrast, our system does not need hand-designedcomponents to model background noise, reverberation, or speaker variation, butinstead directly learns a function that is robust to such effects. We do notneed a phoneme dictionary, nor even the concept of a "phoneme." Key to ourapproach is a well-optimized RNN training system that uses multiple GPUs, aswell as a set of novel data synthesis techniques that allow us to efficientlyobtain a large amount of varied data for training. Our system, called DeepSpeech, outperforms previously published results on the widely studiedSwitchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speechalso handles challenging noisy environments better than widely used,state-of-the-art commercial speech systems.
arxiv-8400-50 | A Simple and Efficient Method To Generate Word Sense Representations | http://arxiv.org/pdf/1412.6045v2.pdf | author:Luis Nieto Piña, Richard Johansson category:cs.CL published:2014-12-18 summary:Distributed representations of words have boosted the performance of manyNatural Language Processing tasks. However, usually only one representation perword is obtained, not acknowledging the fact that some words have multiplemeanings. This has a negative effect on the individual word representations andthe language model as a whole. In this paper we present a simple model thatenables recent techniques for building word vectors to represent distinctsenses of polysemic words. In our assessment of this model we show that it isable to effectively discriminate between words' senses and to do so in acomputationally efficient manner.
arxiv-8400-51 | Fast Algorithm for Low-rank matrix recovery in Poisson noise | http://arxiv.org/pdf/1407.0726v2.pdf | author:Yang Cao, Yao Xie category:stat.ML cs.LG math.ST stat.TH published:2014-07-02 summary:This paper describes a fast algorithm for recovering low-rank matrices fromtheir linear measurements contaminated with Poisson noise: the Poisson noiseMaximum Likelihood Singular Value thresholding (PMLSV) algorithm. We propose aconvex optimization formulation with a cost function consisting of the sum of alikelihood function and a regularization function which the nuclear norm of thematrix. Instead of solving the optimization problem directly by semi-definiteprogram (SDP), we derive an iterative singular value thresholding algorithm byexpanding the likelihood function. We demonstrate the good performance of theproposed algorithm on recovery of solar flare images with Poisson noise: thealgorithm is more efficient than solving SDP using the interior-point algorithmand it generates a good approximate solution compared to that solved from SDP.
arxiv-8400-52 | Cauchy Principal Component Analysis | http://arxiv.org/pdf/1412.6506v1.pdf | author:Pengtao Xie, Eric Xing category:cs.LG stat.ML published:2014-12-19 summary:Principal Component Analysis (PCA) has wide applications in machine learning,text mining and computer vision. Classical PCA based on a Gaussian noise modelis fragile to noise of large magnitude. Laplace noise assumption based PCAmethods cannot deal with dense noise effectively. In this paper, we proposeCauchy Principal Component Analysis (Cauchy PCA), a very simple yet effectivePCA method which is robust to various types of noise. We utilize Cauchydistribution to model noise and derive Cauchy PCA under the maximum likelihoodestimation (MLE) framework with low rank constraint. Our method can robustlyestimate the low rank matrix regardless of whether noise is large or small,dense or sparse. We analyze the robustness of Cauchy PCA from a robuststatistics view and present an efficient singular value projection optimizationmethod. Experimental results on both simulated data and real applicationsdemonstrate the robustness of Cauchy PCA to various noise patterns.
arxiv-8400-53 | A la Carte - Learning Fast Kernels | http://arxiv.org/pdf/1412.6493v1.pdf | author:Zichao Yang, Alexander J. Smola, Le Song, Andrew Gordon Wilson category:cs.LG stat.ML published:2014-12-19 summary:Kernel methods have great promise for learning rich statisticalrepresentations of large modern datasets. However, compared to neural networks,kernel methods have been perceived as lacking in scalability and flexibility.We introduce a family of fast, flexible, lightly parametrized and generalpurpose kernel learning methods, derived from Fastfood basis functionexpansions. We provide mechanisms to learn the properties of groups of spectralfrequencies in these expansions, which require only O(mlogd) time and O(m)memory, for m basis functions and d input dimensions. We show that the proposedmethods can learn a wide class of kernels, outperforming the alternatives inaccuracy, speed, and memory consumption.
arxiv-8400-54 | Simplified firefly algorithm for 2D image key-points search | http://arxiv.org/pdf/1412.6464v1.pdf | author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana, Zbigniew Marszałek, Dawid Połap, Marcin Woźniak category:cs.NE cs.AI cs.CV published:2014-12-19 summary:In order to identify an object, human eyes firstly search the field of viewfor points or areas which have particular properties. These properties are usedto recognise an image or an object. Then this process could be taken as a modelto develop computer algorithms for images identification. This paper proposesthe idea of applying the simplified firefly algorithm to search for key-areasin 2D images. For a set of input test images the proposed version of fireflyalgorithm has been examined. Research results are presented and discussed toshow the efficiency of this evolutionary computation method.
arxiv-8400-55 | Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer | http://arxiv.org/pdf/1412.6451v1.pdf | author:Mark Wernsdorfer, Ute Schmid category:cs.LG cs.AI cs.RO published:2014-12-19 summary:Methods of deep machine learning enable to to reuse low-level representationsefficiently for generating more abstract high-level representations.Originally, deep learning has been applied passively (e.g., for classificationpurposes). Recently, it has been extended to estimate the value of actions forautonomous agents within the framework of reinforcement learning (RL). Explicitmodels of the environment can be learned to augment such a value function.Although "flat" connectionist methods have already been used for model-basedRL, up to now, only model-free variants of RL have been equipped with methodsfrom deep learning. We propose a variant of deep model-based RL that enables anagent to learn arbitrarily abstract hierarchical representations of itsenvironment. In this paper, we present research on how such hierarchicalrepresentations can be grounded in sensorimotor interaction between an agentand its environment.
arxiv-8400-56 | Py3DFreeHandUS: a library for voxel-array reconstruction using Ultrasonography and attitude sensors | http://arxiv.org/pdf/1412.6391v1.pdf | author:Davide Monari, Francesco Cenni, Erwin Aertbeliën, Kaat Desloovere category:cs.CV cs.CE published:2014-12-19 summary:In medical imaging, there is a growing interest to provide real-time imageswith good quality for large anatomical structures. To cope with this issue, wedeveloped a library that allows to replace, for some specific clinicalapplications, more robust systems such as Computer Tomography (CT) and MagneticResonance Imaging (MRI). Our python library Py3DFreeHandUS is a package forprocessing data acquired simultaneously by ultra-sonographic systems (US) andmarker-based optoelectronic systems. In particular, US data enables tovisualize subcutaneous body structures, whereas the optoelectronic system isable to collect the 3D position in space for reflective objects, that arecalled markers. By combining these two measurement devices, it is possible toreconstruct the real 3D morphology of body structures such as muscles, forrelevant clinical implications. In the present research work, the differentsteps which allow to obtain a relevant 3D data set as well as the proceduresfor calibrating the systems and for determining the quality of thereconstruction.
arxiv-8400-57 | Distributed Decision Trees | http://arxiv.org/pdf/1412.6388v1.pdf | author:Ozan İrsoy, Ethem Alpaydın category:cs.LG stat.ML published:2014-12-19 summary:Recently proposed budding tree is a decision tree algorithm in which everynode is part internal node and part leaf. This allows representing everydecision tree in a continuous parameter space, and therefore a budding tree canbe jointly trained with backpropagation, like a neural network. Even thoughthis continuity allows it to be used in hierarchical representation learning,the learned representations are local: Activation makes a soft selection amongall root-to-leaf paths in a tree. In this work we extend the budding tree andpropose the distributed tree where the children use different and independentsplits and hence multiple paths in a tree can be traversed at the same time.This ability to combine multiple paths gives the power of a distributedrepresentation, as in a traditional perceptron layer. We show that distributedtrees perform comparably or better than budding and traditional hard trees onclassification and regression tasks.
arxiv-8400-58 | Reverse Engineering Chemical Reaction Networks from Time Series Data | http://arxiv.org/pdf/1412.6346v1.pdf | author:Dominic P. Searson, Mark J. Willis, Allen Wright category:cs.NE q-bio.MN published:2014-12-19 summary:The automated inference of physically interpretable (bio)chemical reactionnetwork models from measured experimental data is a challenging problem whosesolution has significant commercial and academic ramifications. It isdemonstrated, using simulations, how sets of elementary reactions comprisingchemical reaction networks, as well as their rate coefficients, may beaccurately recovered from non-equilibrium time series concentration data, suchas that obtained from laboratory scale reactors. A variant of an evolutionaryalgorithm called differential evolution in conjunction with least squarestechniques is used to search the space of reaction networks in order to inferboth the reaction network topology and its rate parameters. Properties of thestoichiometric matrices of trial networks are used to bias the search towardsphysically realisable solutions. No other information, such as chemicalcharacterisation of the reactive species is required, although where availableit may be used to improve the search process.
arxiv-8400-59 | From dependency to causality: a machine learning approach | http://arxiv.org/pdf/1412.6285v1.pdf | author:Gianluca Bontempi, Maxime Flauder category:cs.LG cs.AI stat.ML published:2014-12-19 summary:The relationship between statistical dependency and causality lies at theheart of all statistical approaches to causal inference. Recent results in theChaLearn cause-effect pair challenge have shown that causal directionality canbe inferred with good accuracy also in Markov indistinguishable configurationsthanks to data driven approaches. This paper proposes a supervised machinelearning approach to infer the existence of a directed causal link between twovariables in multivariate settings with $n>2$ variables. The approach relies onthe asymmetry of some conditional (in)dependence relations between the membersof the Markov blankets of two variables causally connected. Our results showthat supervised learning methods may be successfully used to extract causalinformation on the basis of asymmetric statistical descriptors also for $n>2$variate distributions.
arxiv-8400-60 | Supertagging: Introduction, learning, and application | http://arxiv.org/pdf/1412.6264v1.pdf | author:Taraka Rama K category:cs.CL published:2014-12-19 summary:Supertagging is an approach originally developed by Bangalore and Joshi(1999) to improve the parsing efficiency. In the beginning, the scholars usedsmall training datasets and somewhat na\"ive smoothing techniques to learn theprobability distributions of supertags. Since its inception, the applicabilityof Supertags has been explored for TAG (tree-adjoining grammar) formalism aswell as other related yet, different formalisms such as CCG. This article willtry to summarize the various chapters, relevant to statistical parsing, fromthe most recent edited book volume (Bangalore and Joshi, 2010). The chapterswere selected so as to blend the learning of supertags, its integration intofull-scale parsing, and in semantic parsing.
arxiv-8400-61 | Gradual training of deep denoising auto encoders | http://arxiv.org/pdf/1412.6257v1.pdf | author:Alexander Kalmanovich, Gal Chechik category:cs.LG cs.NE published:2014-12-19 summary:Stacked denoising auto encoders (DAEs) are well known to learn useful deeprepresentations, which can be used to improve supervised training byinitializing a deep network. We investigate a training scheme of a deep DAE,where DAE layers are gradually added and keep adapting as additional layers areadded. We show that in the regime of mid-sized datasets, this gradual trainingprovides a small but consistent improvement over stacked training in bothreconstruction quality and classification error over stacked training on MNISTand CIFAR datasets.
arxiv-8400-62 | Fisher Kernel for Deep Neural Activations | http://arxiv.org/pdf/1412.1628v2.pdf | author:Donggeun Yoo, Sunggyun Park, Joon-Young Lee, In So Kweon category:cs.CV cs.LG cs.NE published:2014-12-04 summary:Compared to image representation based on low-level local descriptors, deepneural activations of Convolutional Neural Networks (CNNs) are richer inmid-level representation, but poorer in geometric invariance properties. Inthis paper, we present a straightforward framework for better imagerepresentation by combining the two approaches. To take advantages of bothrepresentations, we propose an efficient method to extract a fair amount ofmulti-scale dense local activations from a pre-trained CNN. We then aggregatethe activations by Fisher kernel framework, which has been modified with asimple scale-wise normalization essential to make it suitable for CNNactivations. Replacing the direct use of a single activation vector with ourrepresentation demonstrates significant performance improvements: +17.76 (Acc.)on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest thatour proposal can be used as a primary image representation for betterperformances in visual recognition tasks.
arxiv-8400-63 | Information-Theoretic Methods for Identifying Relationships among Climate Variables | http://arxiv.org/pdf/1412.6219v1.pdf | author:Kevin H. Knuth, Deniz Gençağa, William B. Rossow category:physics.ao-ph stat.ML published:2014-12-19 summary:Information-theoretic quantities, such as entropy, are used to quantify theamount of information a given variable provides. Entropies can be used togetherto compute the mutual information, which quantifies the amount of informationtwo variables share. However, accurately estimating these quantities from datais extremely challenging. We have developed a set of computational techniquesthat allow one to accurately compute marginal and joint entropies. Thesealgorithms are probabilistic in nature and thus provide information on theuncertainty in our estimates, which enable us to establish statisticalsignificance of our findings. We demonstrate these methods by identifyingrelations between cloud data from the International Satellite Cloud ClimatologyProject (ISCCP) and data from other sources, such as equatorial pacific seasurface temperatures (SST).
arxiv-8400-64 | Multiple Authors Detection: A Quantitative Analysis of Dream of the Red Chamber | http://arxiv.org/pdf/1412.6211v1.pdf | author:Xianfeng Hu, Yang Wang, Qiang Wu category:cs.LG cs.CL published:2014-12-19 summary:Inspired by the authorship controversy of Dream of the Red Chamber and theapplication of machine learning in the study of literary stylometry, we developa rigorous new method for the mathematical analysis of authorship by testingfor a so-called chrono-divide in writing styles. Our method incorporates someof the latest advances in the study of authorship attribution, particularlytechniques from support vector machines. By introducing the notion of relativefrequency as a feature ranking metric our method proves to be highly effectiveand robust. Applying our method to the Cheng-Gao version of Dream of the Red Chamber hasled to convincing if not irrefutable evidence that the first $80$ chapters andthe last $40$ chapters of the book were written by two different authors.Furthermore, our analysis has unexpectedly provided strong support to thehypothesis that Chapter 67 was not the work of Cao Xueqin either. We have also tested our method to the other three Great Classical Novels inChinese. As expected no chrono-divides have been found. This provides furtherevidence of the robustness of our method.
arxiv-8400-65 | Automated Objective Surgical Skill Assessment in the Operating Room Using Unstructured Tool Motion | http://arxiv.org/pdf/1412.6163v1.pdf | author:Piyush Poddar, Narges Ahmidi, S. Swaroop Vedula, Lisa Ishii, Gregory D. Hager, Masaru Ishii category:cs.CV published:2014-12-18 summary:Previous work on surgical skill assessment using intraoperative tool motionin the operating room (OR) has focused on highly-structured surgical tasks suchas cholecystectomy. Further, these methods only considered generic motionmetrics such as time and number of movements, which are of limited instructivevalue. In this paper, we developed and evaluated an automated approach to thesurgical skill assessment of nasal septoplasty in the OR. The obstructed fieldof view and highly unstructured nature of septoplasty precludes trainees fromefficiently learning the procedure. We propose a descriptive structure ofseptoplasty consisting of two types of activity: (1) brushing activity directedaway from the septum plane characterizing the consistency of the surgeon'swrist motion and (2) activity along the septal plane characterizing thesurgeon's coverage pattern. We derived features related to these two activitytypes that classify a surgeon's level of training with an average accuracy ofabout 72%. The features we developed provide surgeons with personalized,actionable feedback regarding their tool motion.
arxiv-8400-66 | Semantic Part Segmentation using Compositional Model combining Shape and Appearance | http://arxiv.org/pdf/1412.6124v1.pdf | author:Jianyu Wang, Alan Yuille category:cs.CV published:2014-12-18 summary:In this paper, we study the problem of semantic part segmentation foranimals. This is more challenging than standard object detection, objectsegmentation and pose estimation tasks because semantic parts of animals oftenhave similar appearance and highly varying shapes. To tackle these challenges,we build a mixture of compositional models to represent the object boundary andthe boundaries of semantic parts. And we incorporate edge, appearance, andsemantic part cues into the compositional model. Given part-level segmentationannotation, we develop a novel algorithm to learn a mixture of compositionalmodels under various poses and viewpoints for certain animal classes.Furthermore, a linear complexity algorithm is offered for efficient inferenceof the compositional model using dynamic programming. We evaluate our methodfor horse and cow using a newly annotated dataset on Pascal VOC 2010 which haspixelwise part labels. Experimental results demonstrate the effectiveness ofour method.
arxiv-8400-67 | Compressing Deep Convolutional Networks using Vector Quantization | http://arxiv.org/pdf/1412.6115v1.pdf | author:Yunchao Gong, Liu Liu, Ming Yang, Lubomir Bourdev category:cs.CV cs.LG cs.NE published:2014-12-18 summary:Deep convolutional neural networks (CNN) has become the most promising methodfor object recognition, repeatedly demonstrating record breaking results forimage classification and object detection in recent years. However, a very deepCNN generally involves many layers with millions of parameters, making thestorage of the network model to be extremely large. This prohibits the usage ofdeep CNNs on resource limited hardware, especially cell phones or otherembedded devices. In this paper, we tackle this model storage issue byinvestigating information theoretical vector quantization methods forcompressing the parameters of CNNs. In particular, we have found in terms ofcompressing the most storage demanding dense connected layers, vectorquantization methods have a clear gain over existing matrix factorizationmethods. Simply applying k-means clustering to the weights or conductingproduct quantization can lead to a very good balance between model size andrecognition accuracy. For the 1000-category classification task in the ImageNetchallenge, we are able to achieve 16-24 times compression of the network withonly 1% loss of classification accuracy using the state-of-the-art CNN.
arxiv-8400-68 | Quantized Matrix Completion for Personalized Learning | http://arxiv.org/pdf/1412.5968v1.pdf | author:Andrew S. Lan, Christoph Studer, Richard G. Baraniuk category:stat.ML cs.LG published:2014-12-18 summary:The recently proposed SPARse Factor Analysis (SPARFA) framework forpersonalized learning performs factor analysis on ordinal or binary-valued(e.g., correct/incorrect) graded learner responses to questions. The underlyingfactors are termed "concepts" (or knowledge components) and are used forlearning analytics (LA), the estimation of learner concept-knowledge profiles,and for content analytics (CA), the estimation of question-concept associationsand question difficulties. While SPARFA is a powerful tool for LA and CA, itrequires a number of algorithm parameters (including the number of concepts),which are difficult to determine in practice. In this paper, we proposeSPARFA-Lite, a convex optimization-based method for LA that builds on matrixcompletion, which only requires a single algorithm parameter and enables us toautomatically identify the required number of concepts. Using a variety ofeducational datasets, we demonstrate that SPARFALite (i) achieves comparableperformance in predicting unobserved learner responses to existing methods,including item response theory (IRT) and SPARFA, and (ii) is computationallymore efficient.
arxiv-8400-69 | Tag-Aware Ordinal Sparse Factor Analysis for Learning and Content Analytics | http://arxiv.org/pdf/1412.5967v1.pdf | author:Andrew S. Lan, Christoph Studer, Andrew E. Waters, Richard G. Baraniuk category:stat.ML cs.LG published:2014-12-18 summary:Machine learning offers novel ways and means to design personalized learningsystems wherein each student's educational experience is customized in realtime depending on their background, learning goals, and performance to date.SPARse Factor Analysis (SPARFA) is a novel framework for machine learning-basedlearning analytics, which estimates a learner's knowledge of the conceptsunderlying a domain, and content analytics, which estimates the relationshipsamong a collection of questions and those concepts. SPARFA jointly learns theassociations among the questions and the concepts, learner concept knowledgeprofiles, and the underlying question difficulties, solely based on thecorrect/incorrect graded responses of a population of learners to a collectionof questions. In this paper, we extend the SPARFA framework significantly toenable: (i) the analysis of graded responses on an ordinal scale (partialcredit) rather than a binary scale (correct/incorrect); (ii) the exploitationof tags/labels for questions that partially describe the question{conceptassociations. The resulting Ordinal SPARFA-Tag framework greatly enhances theinterpretability of the estimated concepts. We demonstrate using realeducational data that Ordinal SPARFA-Tag outperforms both SPARFA and existingcollaborative filtering techniques in predicting missing learner responses.
arxiv-8400-70 | Large Scale Distributed Distance Metric Learning | http://arxiv.org/pdf/1412.5949v1.pdf | author:Pengtao Xie, Eric Xing category:cs.LG published:2014-12-18 summary:In large scale machine learning and data mining problems with high featuredimensionality, the Euclidean distance between data points can beuninformative, and Distance Metric Learning (DML) is often desired to learn aproper similarity measure (using side information such as example data pairsbeing similar or dissimilar). However, high dimensionality and large volume ofpairwise constraints in modern big data can lead to prohibitive computationalcost for both the original DML formulation in Xing et al. (2002) and laterextensions. In this paper, we present a distributed algorithm for DML, and alarge-scale implementation on a parameter server architecture. Our approachbuilds on a parallelizable reformulation of Xing et al. (2002), and anasynchronous stochastic gradient descent optimization procedure. To ourknowledge, this is the first distributed solution to DML, and we show that, ona system with 256 CPU cores, our program is able to complete a DML task on adataset with 1 million data points, 22-thousand features, and 200 millionlabeled data pairs, in 15 hours; and the learned metric shows greateffectiveness in properly measuring distances.
arxiv-8400-71 | Deep Recurrent Neural Networks for Time Series Prediction | http://arxiv.org/pdf/1407.5949v2.pdf | author:Sharat C. Prasad, Piyush Prasad category:cs.NE published:2014-07-22 summary:Ability of deep networks to extract high level features and of recurrentnetworks to perform time-series inference have been studied. In view ofuniversality of one hidden layer network at approximating functions under weakconstraints, the benefit of multiple layers is to enlarge the space ofdynamical systems approximated or, given the space, reduce the number of unitsrequired for a certain error. Traditionally shallow networks with manuallyengineered features are used, back-propagation extent is limited to one andattempt to choose a large number of hidden units to satisfy the Markovcondition is made. In case of Markov models, it has been shown that manysystems need to be modeled as higher order. In the present work, we presentdeep recurrent networks with longer backpropagation through time extent as asolution to modeling systems that are high order and to predicting ahead. Westudy epileptic seizure suppression electro-stimulator. Extraction of manuallyengineered complex features and prediction employing them has not allowed smalllow-power implementations as, to avoid possibility of surgery, extraction ofany features that may be required has to be included. In this solution, arecurrent neural network performs both feature extraction and prediction. Weprove analytically that adding hidden layers or increasing backpropagationextent increases the rate of decrease of approximation error. A DynamicProgramming (DP) training procedure employing matrix operations is derived. DPand use of matrix operations makes the procedure efficient particularly whenusing data-parallel computing. The simulation studies show the geometry of theparameter space, that the network learns the temporal structure, thatparameters converge while model output displays same dynamic behavior as thesystem and greater than .99 Average Detection Rate on all real seizure datatried.
arxiv-8400-72 | A theoretical basis for efficient computations with noisy spiking neurons | http://arxiv.org/pdf/1412.5862v1.pdf | author:Zeno Jonke, Stefan Habenschuss, Wolfgang Maass category:cs.NE q-bio.NC 68Q10 published:2014-12-18 summary:Network of neurons in the brain apply - unlike processors in our currentgeneration of computer hardware - an event-based processing strategy, whereshort pulses (spikes) are emitted sparsely by neurons to signal the occurrenceof an event at a particular point in time. Such spike-based computationspromise to be substantially more power-efficient than traditional clockedprocessing schemes. However it turned out to be surprisingly difficult todesign networks of spiking neurons that are able to carry out demandingcomputations. We present here a new theoretical framework for organizingcomputations of networks of spiking neurons. In particular, we show that asuitable design enables them to solve hard constraint satisfaction problemsfrom the domains of planning - optimization and verification - logicalinference. The underlying design principles employ noise as a computationalresource. Nevertheless the timing of spikes (rather than just spike rates)plays an essential role in the resulting computations. Furthermore, one candemonstrate for the Traveling Salesman Problem a surprising computationaladvantage of networks of spiking neurons compared with traditional artificialneural networks and Gibbs sampling. The identification of such advantage hasbeen a well-known open problem.
arxiv-8400-73 | Contour Detection Using Contrast Formulas in the Framework of Logarithmic Models | http://arxiv.org/pdf/1412.5802v1.pdf | author:Vasile Patrascu category:cs.CV published:2014-12-18 summary:In this paper we use a new logarithmic model of image representation,developed in [1,2], for edge detection. In fact, in the framework of the newmodel we obtain the formulas for computing the "contrast of a pixel" and the"contrast" image is just the "contour" or edge image. In our setting the rangeof values is preserved and the quality of the contour is good for high as wellas for low luminosity regions. We present the comparison of our results withthe results using classical edge detection operators.
arxiv-8400-74 | Image Enhancement Using a Generalization of Homographic Function | http://arxiv.org/pdf/1412.5796v1.pdf | author:Vasile Patrascu category:cs.CV published:2014-12-18 summary:This paper presents a new method of gray level image enhancement, based onpoint transforms. In order to define the transform function, it was used ageneralization of the homographic function.
arxiv-8400-75 | Image enhancement using the mean dynamic range maximization with logarithmic operations | http://arxiv.org/pdf/1412.6092v1.pdf | author:Vasile Patrascu category:cs.CV published:2014-12-18 summary:In this paper we use a logarithmic model for gray level image enhancement. Webegin with a short presentation of the model and then, we propose a new formulafor the mean dynamic range. After that we present two image transforms: oneperforms an optimal enhancement of the mean dynamic range using the logarithmicaddition, and the other does the same for positive and negative values usingthe logarithmic scalar multiplication. We present the comparison of the resultsobtained by dynamic ranges optimization with the results obtained usingclassical image enhancement methods like gamma correction and histogramequalization.
arxiv-8400-76 | Gray Level Image Enhancement Using Polygonal Functions | http://arxiv.org/pdf/1412.5787v1.pdf | author:Vasile Patrascu category:cs.CV published:2014-12-18 summary:This paper presents a method for enhancing the gray level images. This methodtakes part from the category of point transforms and it is based oninterpolation functions. The latter have a graphic represented by polygonallines. The interpolation nodes of these functions are calculated taking intoaccount the statistics of gray levels belonging to the image.
arxiv-8400-77 | Gray level image enhancement using the Bernstein polynomials | http://arxiv.org/pdf/1412.5769v1.pdf | author:Vasile Patrascu category:cs.CV published:2014-12-18 summary:This paper presents a method for enhancing the gray level images. Thispresented method takes part from the category of point operations and it isbased on piecewise linear functions. The interpolation nodes of these functionsare calculated using the Bernstein polynomials.
arxiv-8400-78 | Image Dynamic Range Enhancement in the Context of Logarithmic Models | http://arxiv.org/pdf/1412.5764v1.pdf | author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV published:2014-12-18 summary:Images of a scene observed under a variable illumination or with a variableoptical aperture are not identical. Does a privileged representant exist? Inwhich mathematical context? How to obtain it? The authors answer to suchquestions in the context of logarithmic models for images. After a shortpresentation of the model, the paper presents two image transforms: oneperforms an optimal enhancement of the dynamic range, and the other does thesame for the mean dynamic range. Experimental results are shown.
arxiv-8400-79 | Deep Convolutional Neural Fields for Depth Estimation from a Single Image | http://arxiv.org/pdf/1411.6387v2.pdf | author:Fayao Liu, Chunhua Shen, Guosheng Lin category:cs.CV published:2014-11-24 summary:We consider the problem of depth estimation from a single monocular image inthis work. It is a challenging task as no reliable depth cues are available,e.g., stereo correspondences, motions, etc. Previous efforts have been focusingon exploiting geometric priors or additional sources of information, with allusing hand-crafted features. Recently, there is mounting evidence that featuresfrom deep convolutional neural networks (CNN) are setting new records forvarious vision applications. On the other hand, considering the continuouscharacteristic of the depth values, depth estimations can be naturallyformulated into a continuous conditional random field (CRF) learning problem.Therefore, we in this paper present a deep convolutional neural field model forestimating depths from a single image, aiming to jointly explore the capacityof deep CNN and continuous CRF. Specifically, we propose a deep structuredlearning scheme which learns the unary and pairwise potentials of continuousCRF in a unified deep CNN framework. The proposed method can be used for depth estimations of general scenes withno geometric priors nor any extra information injected. In our case, theintegral of the partition function can be analytically calculated, thus we canexactly solve the log-likelihood optimization. Moreover, solving the MAPproblem for predicting depths of a new image is highly efficient as closed-formsolutions exist. We experimentally demonstrate that the proposed methodoutperforms state-of-the-art depth estimation methods on both indoor andoutdoor scene datasets.
arxiv-8400-80 | Multiobjective Optimization of Classifiers by Means of 3-D Convex Hull Based Evolutionary Algorithm | http://arxiv.org/pdf/1412.5710v1.pdf | author:Jiaqi Zhao, Vitor Basto Fernandes, Licheng Jiao, Iryna Yevseyeva, Asep Maulana, Rui Li, Thomas Bäck, Michael T. M. Emmerich category:cs.NE cs.LG published:2014-12-18 summary:Finding a good classifier is a multiobjective optimization problem withdifferent error rates and the costs to be minimized. The receiver operatingcharacteristic is widely used in the machine learning community to analyze theperformance of parametric classifiers or sets of Pareto optimal classifiers. Inorder to directly compare two sets of classifiers the area (or volume) underthe convex hull can be used as a scalar indicator for the performance of a setof classifiers in receiver operating characteristic space. Recently, the convex hull based multiobjective genetic programming algorithmwas proposed and successfully applied to maximize the convex hull area forbinary classification problems. The contribution of this paper is to extendthis algorithm for dealing with higher dimensional problem formulations. Inparticular, we discuss problems where parsimony (or classifier complexity) isstated as a third objective and multi-class classification with three differenttrue classification rates to be maximized. The design of the algorithm proposed in this paper is inspired byindicator-based evolutionary algorithms, where first a performance indicatorfor a solution set is established and then a selection operator is designedthat complies with the performance indicator. In this case, the performanceindicator will be the volume under the convex hull. The algorithm is tested andanalyzed in a proof of concept study on different benchmarks that are designedfor measuring its capability to capture relevant parts of a convex hull. Further benchmark and application studies on email classification and featureselection round up the analysis and assess robustness and usefulness of the newalgorithm in real world settings.
arxiv-8400-81 | High Frequency Content based Stimulus for Perceptual Sharpness Assessment in Natural Images | http://arxiv.org/pdf/1412.5490v2.pdf | author:Ashirbani Saha, Q. M. Jonathan Wu category:cs.CV published:2014-12-17 summary:A blind approach to evaluate the perceptual sharpness present in a naturalimage is proposed. Though the literature demonstrates a set of variegatedvisual cues to detect or evaluate the absence or presence of sharpness, weemphasize in the current work that high frequency content and local standarddeviation can form strong features to compute perceived sharpness in anynatural image, and can be considered an able alternative for the existing cues.Unsharp areas in a natural image happen to exhibit uniform intensity or lack ofsharp changes between regions. Sharp region transitions in an image are causedby the presence of spatial high frequency content. Therefore, in the proposedapproach, we hypothesize that using the high frequency content as the principalstimulus, the perceived sharpness can be quantified in an image. When an imageis convolved with a high pass filter, higher values at any pixel locationsignify the presence of high frequency content at those locations. Consideringthese values as the stimulus, the exponent of the stimulus is weighted by localstandard deviation to impart the contribution of the local contrast within theformation of the sharpness map. The sharpness map highlights the relativelysharper regions in the image and is used to calculate the perceived sharpnessscore of the image. The advantages of the proposed method lie in its use ofsimple visual cues of high frequency content and local contrast to arrive atthe perceptual score, and requiring no training with the images. The promise ofthe proposed method is demonstrated by its ability to compute perceivedsharpness for within image and across image sharpness changes and for blindevaluation of perceptual degradation resulting due to presence of blur.Experiments conducted on several databases demonstrate improved performance ofthe proposed method over that of the state-of-the-art techniques.
arxiv-8400-82 | cuDNN: Efficient Primitives for Deep Learning | http://arxiv.org/pdf/1410.0759v3.pdf | author:Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, Evan Shelhamer category:cs.NE cs.LG cs.MS published:2014-10-03 summary:We present a library of efficient implementations of deep learningprimitives. Deep learning workloads are computationally intensive, andoptimizing their kernels is difficult and time-consuming. As parallelarchitectures evolve, kernels must be reoptimized, which makes maintainingcodebases difficult over time. Similar issues have long been addressed in theHPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS).However, there is no analogous library for deep learning. Without such alibrary, researchers implementing deep learning workloads on parallelprocessors must create and optimize their own implementations of the maincomputational kernels, and this work must be repeated as new parallelprocessors emerge. To address this problem, we have created a library similarin intent to BLAS, with optimized routines for deep learning workloads. Ourimplementation contains routines for GPUs, although similarly to the BLASlibrary, these routines could be implemented for other platforms. The libraryis easy to integrate into existing frameworks, and provides optimizedperformance and memory usage. For example, integrating cuDNN into Caffe, apopular framework for convolutional networks, improves performance by 36% on astandard model while also reducing memory consumption.
arxiv-8400-83 | Efficient Visual Coding: From Retina To V2 | http://arxiv.org/pdf/1312.6077v2.pdf | author:Honghao Shan, Garrison Cottrell category:cs.CV q-bio.NC published:2013-12-20 summary:The human visual system has a hierarchical structure consisting of layers ofprocessing, such as the retina, V1, V2, etc. Understanding the functional rolesof these visual processing layers would help to integrate thepsychophysiological and neurophysiological models into a consistent theory ofhuman vision, and would also provide insights to computer vision research. Oneclassical theory of the early visual pathway hypothesizes that it serves tocapture the statistical structure of the visual inputs by efficiently codingthe visual information in its outputs. Until recently, most computationalmodels following this theory have focused upon explaining the receptive fieldproperties of one or two visual layers. Recent work in deep networks haseliminated this concern, however, there is till the retinal layer to consider.Here we improve on a previously-described hierarchical model Recursive ICA(RICA) [1] which starts with PCA, followed by a layer of sparse coding or ICA,followed by a component-wise nonlinearity derived from considerations of thevariable distributions expected by ICA. This process is then repeated. In thiswork, we improve on this model by using a new version of sparse PCA (sPCA),which results in biologically-plausible receptive fields for both the sPCA andICA/sparse coding. When applied to natural image patches, our model learnsvisual features exhibiting the receptive field properties of retinal ganglioncells/lateral geniculate nucleus (LGN) cells, V1 simple cells, V1 complexcells, and V2 cells. Our work provides predictions for experimentalneuroscience studies. For example, our result suggests that a previousneurophysiological study improperly discarded some of their recorded neurons;we predict that their discarded neurons capture the shape contour of objects.
arxiv-8400-84 | Road Detection by One-Class Color Classification: Dataset and Experiments | http://arxiv.org/pdf/1412.3506v2.pdf | author:Jose M. Alvarez, Theo Gevers, Antonio M. Lopez category:cs.CV published:2014-12-11 summary:Detecting traversable road areas ahead a moving vehicle is a key process formodern autonomous driving systems. A common approach to road detection consistsof exploiting color features to classify pixels as road or background. Thesealgorithms reduce the effect of lighting variations and weather conditions byexploiting the discriminant/invariant properties of different colorrepresentations. Furthermore, the lack of labeled datasets has motivated thedevelopment of algorithms performing on single images based on the assumptionthat the bottom part of the image belongs to the road surface. In this paper, we first introduce a dataset of road images taken at differenttimes and in different scenarios using an onboard camera. Then, we devise asimple online algorithm and conduct an exhaustive evaluation of differentclassifiers and the effect of using different color representation tocharacterize pixels.
arxiv-8400-85 | Towards Open World Recognition | http://arxiv.org/pdf/1412.5687v1.pdf | author:Abhijit Bendale, Terrance Boult category:cs.CV published:2014-12-18 summary:With the of advent rich classification models and high computational powervisual recognition systems have found many operational applications.Recognition in the real world poses multiple challenges that are not apparentin controlled lab environments. The datasets are dynamic and novel categoriesmust be continuously detected and then added. At prediction time, a trainedsystem has to deal with myriad unseen categories. Operational systems requireminimum down time, even to learn. To handle these operational issues, wepresent the problem of Open World recognition and formally define it. We provethat thresholding sums of monotonically decreasing functions of distances inlinearly transformed feature space can balance "open space risk" and empiricalrisk. Our theory extends existing algorithms for open world recognition. Wepresent a protocol for evaluation of open world recognition systems. We presentthe Nearest Non-Outlier (NNO) algorithm which evolves model efficiently, addingobject categories incrementally while detecting outliers and managing openspace risk. We perform experiments on the ImageNet dataset with 1.2M+ images tovalidate the effectiveness of our method on large scale visual recognitiontasks. NNO consistently yields superior results on open world recognition.
arxiv-8400-86 | Optimal Triggering of Networked Control Systems | http://arxiv.org/pdf/1412.5676v1.pdf | author:Ali Heydari category:cs.SY math.OC stat.ML published:2014-12-17 summary:The problem of resource allocation of nonlinear networked control systems isinvestigated, where, unlike the well discussed case of triggering forstability, the objective is optimal triggering. An approximate dynamicprogramming approach is developed for solving problems with fixed final timesinitially and then it is extended to infinite horizon problems. Different casesincluding Zero-Order-Hold, Generalized Zero-Order-Hold, and stochastic networksare investigated. Afterwards, the developments are extended to the case ofproblems with unknown dynamics and a model-free scheme is presented forlearning the (approximate) optimal solution. After detailed analyses ofconvergence, optimality, and stability of the results, the performance of themethod is demonstrated through different numerical examples.
arxiv-8400-87 | Effective sampling for large-scale automated writing evaluation systems | http://arxiv.org/pdf/1412.5659v1.pdf | author:Nicholas Dronen, Peter W. Foltz, Kyle Habermehl category:cs.CL cs.LG published:2014-12-17 summary:Automated writing evaluation (AWE) has been shown to be an effectivemechanism for quickly providing feedback to students. It has already seen wideadoption in enterprise-scale applications and is starting to be adopted inlarge-scale contexts. Training an AWE model has historically required a singlebatch of several hundred writing examples and human scores for each of them.This requirement limits large-scale adoption of AWE since human-scoring essaysis costly. Here we evaluate algorithms for ensuring that AWE models areconsistently trained using the most informative essays. Our results show how tominimize training set sizes while maximizing predictive performance, therebyreducing cost without unduly sacrificing accuracy. We conclude with adiscussion of how to integrate this approach into large-scale AWE systems.
arxiv-8400-88 | Support recovery without incoherence: A case for nonconvex regularization | http://arxiv.org/pdf/1412.5632v1.pdf | author:Po-Ling Loh, Martin J. Wainwright category:math.ST cs.IT math.IT stat.ML stat.TH 62F12 published:2014-12-17 summary:We demonstrate that the primal-dual witness proof method may be used toestablish variable selection consistency and $\ell_\infty$-bounds for sparseregression problems, even when the loss function and/or regularizer arenonconvex. Using this method, we derive two theorems concerning supportrecovery and $\ell_\infty$-guarantees for the regression estimator in a generalsetting. Our results provide rigorous theoretical justification for the use ofnonconvex regularization: For certain nonconvex regularizers with vanishingderivative away from the origin, support recovery consistency may be guaranteedwithout requiring the typical incoherence conditions present in $\ell_1$-basedmethods. We then derive several corollaries that illustrate the wideapplicability of our method to analyzing composite objective functionsinvolving losses such as least squares, nonconvex modified least squares forerrors-in variables linear regression, the negative log likelihood forgeneralized linear models, and the graphical Lasso. We conclude with empiricalstudies to corroborate our theoretical predictions.
arxiv-8400-89 | Feature extraction from complex networks: A case of study in genomic sequences classification | http://arxiv.org/pdf/1412.5627v1.pdf | author:Bruno Mendes Moro Conque, André Yoshiaki Kashiwabara, Fabrício Martins Lopes category:cs.CE cs.LG q-bio.QM published:2014-12-17 summary:This work presents a new approach for classification of genomic sequencesfrom measurements of complex networks and information theory. For this, it isconsidered the nucleotides, dinucleotides and trinucleotides of a genomicsequence. For each of them, the entropy, sum entropy and maximum entropy valuesare calculated.For each of them is also generated a network, in which the nodesare the nucleotides, dinucleotides or trinucleotides and its edges areestimated by observing the respective adjacency among them in the genomicsequence. In this way, it is generated three networks, for which measures ofcomplex networks are extracted.These measures together with measures ofinformation theory comprise a feature vector representing a genomic sequence.Thus, the feature vector is used for classification by methods such as SVM,MultiLayer Perceptron, J48, IBK, Naive Bayes and Random Forest in order toevaluate the proposed approach.It was adopted coding sequences, intergenicsequences and TSS (Transcriptional Starter Sites) as datasets, for which thebetter results were obtained by the Random Forest with 91.2%, followed by J48with 89.1% and SVM with 84.8% of accuracy. These results indicate that the newapproach of feature extraction has its value, reaching good levels ofclassification even considering only the genomic sequences, i.e., no other apriori knowledge about them is considered.
arxiv-8400-90 | Learning from Data with Heterogeneous Noise using SGD | http://arxiv.org/pdf/1412.5617v1.pdf | author:Shuang Song, Kamalika Chaudhuri, Anand D. Sarwate category:cs.LG published:2014-12-17 summary:We consider learning from data of variable quality that may be obtained fromdifferent heterogeneous sources. Addressing learning from heterogeneous data inits full generality is a challenging problem. In this paper, we adopt instead amodel in which data is observed through heterogeneous noise, where the noiselevel reflects the quality of the data source. We study how to use stochasticgradient algorithms to learn in this model. Our study is motivated by twoconcrete examples where this problem arises naturally: learning with localdifferential privacy based on data from multiple sources with different privacyrequirements, and learning from data with labels of variable quality. The main contribution of this paper is to identify how heterogeneous noiseimpacts performance. We show that given two datasets with heterogeneous noise,the order in which to use them in standard SGD depends on the learning rate. Wepropose a method for changing the learning rate as a function of theheterogeneity, and prove new regret bounds for our method in two cases ofinterest. Experiments on real data show that our method performs better thanusing a single learning rate and using only the less noisy of the two datasetswhen the noise level is low to moderate.
arxiv-8400-91 | Towards a constructive multilayer perceptron for regression task using non-parametric clustering. A case study of Photo-Z redshift reconstruction | http://arxiv.org/pdf/1412.5513v1.pdf | author:Cyrine Arouri, Engelbert Mephu Nguifo, Sabeur Aridhi, Cécile Roucelle, Gaelle Bonnet-Loosli, Norbert Tsopzé category:cs.NE cs.AI published:2014-12-17 summary:The choice of architecture of artificial neuron network (ANN) is still achallenging task that users face every time. It greatly affects the accuracy ofthe built network. In fact there is no optimal method that is applicable tovarious implementations at the same time. In this paper we propose a method toconstruct ANN based on clustering, that resolves the problems of random and adhoc approaches for multilayer ANN architecture. Our method can be applied toregression problems. Experimental results obtained with different datasets,reveals the efficiency of our method.
arxiv-8400-92 | Full-reference image quality assessment by combining global and local distortion measures | http://arxiv.org/pdf/1412.5488v1.pdf | author:Ashirbani Saha, Q. M. Jonathan Wu category:cs.CV published:2014-12-17 summary:Full-reference image quality assessment (FR-IQA) techniques compare areference and a distorted/test image and predict the perceptual quality of thetest image in terms of a scalar value representing an objective score. Theevaluation of FR-IQA techniques is carried out by comparing the objectivescores from the techniques with the subjective scores (obtained from humanobservers) provided in the image databases used for the IQA. Hence, wereasonably assume that the goal of a human observer is to rate the distortionpresent in the test image. The goal oriented tasks are processed by the humanvisual system (HVS) through top-down processing which actively searches forlocal distortions driven by the goal. Therefore local distortion measures in animage are important for the top-down processing. At the same time, bottom-upprocessing also takes place signifying spontaneous visual functions in the HVS.To account for this, global perceptual features can be used. Therefore, wehypothesize that the resulting objective score for an image can be derived fromthe combination of local and global distortion measures calculated from thereference and test images. We calculate the local distortion by measuring thelocal correlation differences from the gradient and contrast information. Forglobal distortion, dissimilarity of the saliency maps computed from a bottom-upmodel of saliency is used. The motivation behind the proposed approach has beenthoroughly discussed, accompanied by an intuitive analysis. Finally,experiments are conducted in six benchmark databases suggesting theeffectiveness of the proposed approach that achieves competitive performancewith the state-of-the-art methods providing an improvement in the overallperformance.
arxiv-8400-93 | Computational Model to Generate Case-Inflected Forms of Masculine Nouns for Word Search in Sanskrit E-Text | http://arxiv.org/pdf/1412.5477v1.pdf | author:S V Kasmir Raja, V Rajitha, Lakshmanan Meenakshi category:cs.CL published:2014-12-17 summary:The problem of word search in Sanskrit is inseparable from complexities thatinclude those caused by euphonic conjunctions and case-inflections. Thecase-inflectional forms of a noun normally number 24 owing to the fact that inSanskrit there are eight cases and three numbers-singular, dual and plural. Thetraditional method of generating these inflectional forms is rather elaborateowing to the fact that there are differences in the forms generated betweeneven very similar words and there are subtle nuances involved. Further, itwould be a cumbersome exercise to generate and search for 24 forms of a wordduring a word search in a large text, using the currently availablecase-inflectional form generators. This study presents a new approach togenerating case-inflectional forms that is simpler to compute. Further, anoptimized model that is sufficient for generating only those word forms thatare required in a word search and is more than 80% efficient compared to thecomplete case-inflectional forms generator, is presented in this study for thefirst time.
arxiv-8400-94 | Extended Recommendation Framework: Generating the Text of a User Review as a Personalized Summary | http://arxiv.org/pdf/1412.5448v1.pdf | author:Mickaël Poussevin, Vincent Guigue, Patrick Gallinari category:cs.IR cs.CL published:2014-12-17 summary:We propose to augment rating based recommender systems by providing the userwith additional information which might help him in his choice or in theunderstanding of the recommendation. We consider here as a new task, thegeneration of personalized reviews associated to items. We use an extractivesummary formulation for generating these reviews. We also show that the twoinformation sources, ratings and items could be used both for estimatingratings and for generating summaries, leading to improved performance for eachsystem compared to the use of a single source. Besides these two contributions,we show how a personalized polarity classifier can integrate the rating andtextual aspects. Overall, the proposed system offers the user threepersonalized hints for a recommendation: rating, text and polarity. We evaluatethese three components on two datasets using appropriate measures for eachtask.
arxiv-8400-95 | Word Network Topic Model: A Simple but General Solution for Short and Imbalanced Texts | http://arxiv.org/pdf/1412.5404v1.pdf | author:Yuan Zuo, Jichang Zhao, Ke Xu category:cs.CL cs.IR published:2014-12-17 summary:The short text has been the prevalent format for information of Internet inrecent decades, especially with the development of online social media, whosemillions of users generate a vast number of short messages everyday. Althoughsophisticated signals delivered by the short text make it a promising sourcefor topic modeling, its extreme sparsity and imbalance brings unprecedentedchallenges to conventional topic models like LDA and its variants. Aiming atpresenting a simple but general solution for topic modeling in short texts, wepresent a word co-occurrence network based model named WNTM to tackle thesparsity and imbalance simultaneously. Different from previous approaches, WNTMmodels the distribution over topics for each word instead of learning topicsfor each document, which successfully enhance the semantic density of dataspace without importing too much time or space complexity. Meanwhile, the richcontextual information preserved in the word-word space also guarantees itssensitivity in identifying rare topics with convincing quality. Furthermore,employing the same Gibbs sampling with LDA makes WNTM easily to be extended tovarious application scenarios. Extensive validations on both short and normaltexts testify the outperformance of WNTM as compared to baseline methods. Andfinally we also demonstrate its potential in precisely discovering newlyemerging topics or unexpected events in Weibo at pretty early stages.
arxiv-8400-96 | Representation of Evolutionary Algorithms in FPGA Cluster for Project of Large-Scale Networks | http://arxiv.org/pdf/1412.5384v1.pdf | author:Andre B. Perina, Marcilyanne M. Gois, Paulo Matias, Joao M. P. Cardoso, Alexandre C. B. Delbem, Vanderlei Bonato category:cs.DC cs.NE published:2014-12-17 summary:Many problems are related to network projects, such as electric distribution,telecommunication and others. Most of them can be represented by graphs, whichmanipulate thousands or millions of nodes, becoming almost an impossible taskto obtain real-time solutions. Many efficient solutions use EvolutionaryAlgorithms (EA), where researches show that performance of EAs can besubstantially raised by using an appropriate representation, such as theNode-Depth Encoding (NDE). The objective of this work was to partition animplementation on single-FPGA (Field-Programmable Gate Array) based on NDE from512 nodes to a multi-FPGAs approach, expanding the system to 4096 nodes.
arxiv-8400-97 | Deep Learning for Multi-label Classification | http://arxiv.org/pdf/1502.05988v1.pdf | author:Jesse Read, Fernando Perez-Cruz category:cs.LG cs.AI published:2014-12-17 summary:In multi-label classification, the main focus has been to develop ways oflearning the underlying dependencies between labels, and to take advantage ofthis at classification time. Developing better feature-space representationshas been predominantly employed to reduce complexity, e.g., by eliminatingnon-helpful feature attributes from the input space prior to (or during)training. This is an important task, since many multi-label methods typicallycreate many different copies or views of the same input data as they transformit, and considerable memory can be saved by taking advantage of redundancy. Inthis paper, we show that a proper development of the feature space can makelabels less interdependent and easier to model and predict at inference time.For this task we use a deep learning approach with restricted Boltzmannmachines. We present a deep network that, in an empirical evaluation,outperforms a number of competitive methods from the literature
arxiv-8400-98 | The Affine Transforms for Image Enhancement in the Context of Logarithmic Models | http://arxiv.org/pdf/1412.5334v1.pdf | author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV published:2014-12-17 summary:The logarithmic model offers new tools for image processing. An efficientmethod for image enhancement is to use an affine transformation with thelogarithmic operations: addition and scalar multiplication. We define somecriteria for automatically determining the parameters of the processing andthis is done via mean and variance computed by logarithmic operations.
arxiv-8400-99 | A Mathematical Model for Logarithmic Image Processing | http://arxiv.org/pdf/1412.5328v1.pdf | author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV published:2014-12-17 summary:In this paper, we propose a new mathematical model for image processing. Itis a logarithmical one. We consider the bounded interval (-1, 1) as the set ofgray levels. Firstly, we define two operations: addition <+> and real scalarmultiplication <x>. With these operations, the set of gray levels becomes areal vector space. Then, defining the scalar product (..) and the norm ., we obtain an Euclidean space of the gray levels. Secondly, we extend theseoperations and functions for color images. We finally show the effect ofvarious simple operations on an image.
arxiv-8400-100 | Color Image Enhancement In the Framework of Logarithmic Models | http://arxiv.org/pdf/1412.5325v1.pdf | author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV published:2014-12-17 summary:In this paper, we propose a mathematical model for color image processing. Itis a logarithmical one. We consider the cube (-1,1)x(-1,1)x(-1,1) as the set ofvalues for the color space. We define two operations: addition <+> and realscalar multiplication <x>. With these operations the space of colors becomes areal vector space. Then, defining the scalar product (..) and the norm ., we obtain a (logarithmic) Euclidean space. We show how we can use thismodel for color image enhancement and we present some experimental results.
arxiv-8400-101 | Gene Similarity-based Approaches for Determining Core-Genes of Chloroplasts | http://arxiv.org/pdf/1412.5323v1.pdf | author:Bassam AlKindy, Christophe Guyeux, Jean-François Couchot, Michel Salomon, Jacques M. Bahi category:cs.NE q-bio.GN published:2014-12-17 summary:In computational biology and bioinformatics, the manner to understandevolution processes within various related organisms paid a lot of attentionthese last decades. However, accurate methodologies are still needed todiscover genes content evolution. In a previous work, two novel approachesbased on sequence similarities and genes features have been proposed. Moreprecisely, we proposed to use genes names, sequence similarities, or both,insured either from NCBI or from DOGMA annotation tools. Dogma has theadvantage to be an up-to-date accurate automatic tool specifically designed forchloroplasts, whereas NCBI possesses high quality human curated genes (togetherwith wrongly annotated ones). The key idea of the former proposal was to takethe best from these two tools. However, the first proposal was limited by namevariations and spelling errors on the NCBI side, leading to core trees of lowquality. In this paper, these flaws are fixed by improving the comparison ofNCBI and DOGMA results, and by relaxing constraints on gene names while addinga stage of post-validation on gene sequences. The two stages of similaritymeasures, on names and sequences, are thus proposed for sequence clustering.This improves results that can be obtained using either NCBI or DOGMA alone.Results obtained with this quality control test are further investigated andcompared with previously released ones, on both computational and biologicalaspects, considering a set of 99 chloroplastic genomes.
arxiv-8400-102 | An Algebraical Model for Gray Level Images | http://arxiv.org/pdf/1412.5322v1.pdf | author:Vasile Patrascu category:cs.CV published:2014-12-17 summary:In this paper we propose a new algebraical model for the gray level images.It can be used for digital image processing. The model adresses to those imageswhich are generated in improper light conditions (very low or high level). Thevector space structure is able to illustrate some features into the image usingmodified level of contrast and luminosity. Also, the defined structure could beused in image enhancement. The general approach is presented with experimentalresults to demonstrate image enhancement.
arxiv-8400-103 | Iranian cashes recognition using mobile | http://arxiv.org/pdf/1412.5275v1.pdf | author:Ismail Nojavani, Azade Rezaeezade, Amirhassan Monadjemi category:cs.CV published:2014-12-17 summary:In economical societies of today, using cash is an inseparable aspect ofhuman life. People use cashes for marketing, services, entertainments, bankoperations and so on. This huge amount of contact with cash and the necessityof knowing the monetary value of it caused one of the most challenging problemsfor visually impaired people. In this paper we propose a mobile phone basedapproach to identify monetary value of a picture taken from cashes using someimage processing and machine vision techniques. While the developed approach isvery fast, it can recognize the value of cash by average accuracy of about 95%and can overcome different challenges like rotation, scaling, collision,illumination changes, perspective, and some others.
arxiv-8400-104 | Consistency Analysis of an Empirical Minimum Error Entropy Algorithm | http://arxiv.org/pdf/1412.5272v1.pdf | author:Jun Fan, Ting Hu, Qiang Wu, Ding-Xuan Zhou category:cs.LG stat.ML published:2014-12-17 summary:In this paper we study the consistency of an empirical minimum error entropy(MEE) algorithm in a regression setting. We introduce two types of consistency.The error entropy consistency, which requires the error entropy of the learnedfunction to approximate the minimum error entropy, is shown to be always trueif the bandwidth parameter tends to 0 at an appropriate rate. The regressionconsistency, which requires the learned function to approximate the regressionfunction, however, is a complicated issue. We prove that the error entropyconsistency implies the regression consistency for homoskedastic models wherethe noise is independent of the input variable. But for heteroskedastic models,a counterexample is used to show that the two types of consistency do notcoincide. A surprising result is that the regression consistency is alwaystrue, provided that the bandwidth parameter tends to infinity at an appropriaterate. Regression consistency of two classes of special models is shown to holdwith fixed bandwidth parameter, which further illustrates the complexity ofregression consistency of MEE. Fourier transform plays crucial roles in ouranalysis.
arxiv-8400-105 | ANN Model to Predict Stock Prices at Stock Exchange Markets | http://arxiv.org/pdf/1502.06434v1.pdf | author:B. W. Wanjawa, L. Muchemi category:q-fin.ST cs.CE cs.LG cs.NE published:2014-12-17 summary:Stock exchanges are considered major players in financial sectors of manycountries. Most Stockbrokers, who execute stock trade, use technical,fundamental or time series analysis in trying to predict stock prices, so as toadvise clients. However, these strategies do not usually guarantee good returnsbecause they guide on trends and not the most likely price. It is thereforenecessary to explore improved methods of prediction. The research proposes the use of Artificial Neural Network that isfeedforward multi-layer perceptron with error backpropagation and develops amodel of configuration 5:21:21:1 with 80% training data in 130,000 cycles. Theresearch develops a prototype and tests it on 2008-2012 data from stock marketse.g. Nairobi Securities Exchange and New York Stock Exchange, where predictionresults show MAPE of between 0.71% and 2.77%. Validation done with Encog andNeuroph realized comparable results. The model is thus capable of prediction ontypical stock markets.
arxiv-8400-106 | Learning unbiased features | http://arxiv.org/pdf/1412.5244v1.pdf | author:Yujia Li, Kevin Swersky, Richard Zemel category:cs.LG cs.AI cs.NE stat.ML published:2014-12-17 summary:A key element in transfer learning is representation learning; ifrepresentations can be developed that expose the relevant factors underlyingthe data, then new tasks and domains can be learned readily based on mappingsof these salient factors. We propose that an important aim for theserepresentations are to be unbiased. Different forms of representation learningcan be derived from alternative definitions of unwanted bias, e.g., bias toparticular tasks, domains, or irrelevant underlying data dimensions. One veryuseful approach to estimating the amount of bias in a representation comes frommaximum mean discrepancy (MMD) [5], a measure of distance between probabilitydistributions. We are not the first to suggest that MMD can be a usefulcriterion in developing representations that apply across multiple domains ortasks [1]. However, in this paper we describe a number of novel applications ofthis criterion that we have devised, all based on the idea of developingunbiased representations. These formulations include: a standard domainadaptation framework; a method of learning invariant representations; anapproach based on noise-insensitive autoencoders; and a novel form ofgenerative model.
arxiv-8400-107 | The supervised hierarchical Dirichlet process | http://arxiv.org/pdf/1412.5236v1.pdf | author:Andrew M. Dai, Amos J. Storkey category:stat.ML cs.LG published:2014-12-17 summary:We propose the supervised hierarchical Dirichlet process (sHDP), anonparametric generative model for the joint distribution of a group ofobservations and a response variable directly associated with that whole group.We compare the sHDP with another leading method for regression on grouped data,the supervised latent Dirichlet allocation (sLDA) model. We evaluate our methodon two real-world classification problems and two real-world regressionproblems. Bayesian nonparametric regression models based on the Dirichletprocess, such as the Dirichlet process-generalised linear models (DP-GLM) havepreviously been explored; these models allow flexibility in modelling nonlinearrelationships. However, until now, Hierarchical Dirichlet Process (HDP)mixtures have not seen significant use in supervised problems with grouped datasince a straightforward application of the HDP on the grouped data results inlearnt clusters that are not predictive of the responses. The sHDP solves thisproblem by allowing for clusters to be learnt jointly from the group structureand from the label assigned to each group.
arxiv-8400-108 | Testing MCMC code | http://arxiv.org/pdf/1412.5218v1.pdf | author:Roger B. Grosse, David K. Duvenaud category:cs.SE cs.LG stat.ML published:2014-12-16 summary:Markov Chain Monte Carlo (MCMC) algorithms are a workhorse of probabilisticmodeling and inference, but are difficult to debug, and are prone to silentfailure if implemented naively. We outline several strategies for testing thecorrectness of MCMC algorithms. Specifically, we advocate writing code in amodular way, where conditional probability calculations are kept separate fromthe logic of the sampler. We discuss strategies for both unit testing andintegration testing. As a running example, we show how a Python implementationof Gibbs sampling for a mixture of Gaussians model can be tested.
arxiv-8400-109 | Sample Complexity Analysis for Learning Overcomplete Latent Variable Models through Tensor Methods | http://arxiv.org/pdf/1408.0553v2.pdf | author:Animashree Anandkumar, Rong Ge, Majid Janzamin category:cs.LG math.PR stat.ML published:2014-08-03 summary:We provide guarantees for learning latent variable models emphasizing on theovercomplete regime, where the dimensionality of the latent space can exceedthe observed dimensionality. In particular, we consider multiview mixtures,spherical Gaussian mixtures, ICA, and sparse coding models. We provide tightconcentration bounds for empirical moments through novel covering arguments. Weanalyze parameter recovery through a simple tensor power update algorithm. Inthe semi-supervised setting, we exploit the label or prior information to get arough estimate of the model parameters, and then refine it using the tensormethod on unlabeled samples. We establish that learning is possible when thenumber of components scales as $k=o(d^{p/2})$, where $d$ is the observeddimension, and $p$ is the order of the observed moment employed in the tensormethod. Our concentration bound analysis also leads to minimax samplecomplexity for semi-supervised learning of spherical Gaussian mixtures. In theunsupervised setting, we use a simple initialization algorithm based on SVD ofthe tensor slices, and provide guarantees under the stricter condition that$k\le \beta d$ (where constant $\beta$ can be larger than $1$), where thetensor method recovers the components under a polynomial running time (andexponential in $\beta$). Our analysis establishes that a wide range ofovercomplete latent variable models can be learned efficiently with lowcomputational and sample complexity through tensor decomposition methods.
arxiv-8400-110 | Application of Topic Models to Judgments from Public Procurement Domain | http://arxiv.org/pdf/1412.5212v1.pdf | author:Michał Łopuszyński category:cs.CL published:2014-12-16 summary:In this work, automatic analysis of themes contained in a large corpora ofjudgments from public procurement domain is performed. The employed techniqueis unsupervised latent Dirichlet allocation (LDA). In addition, it is proposed,to use LDA in conjunction with recently developed method of unsupervisedkeyword extraction. Such an approach improves the interpretability of theautomatically obtained topics and allows for better computational performance.The described analysis illustrates a potential of the method in detectingrecurring themes and discovering temporal trends in lodged contract appeals.These results may be in future applied to improve information retrieval fromrepositories of legal texts or as auxiliary material for legal analyses carriedout by human experts.
arxiv-8400-111 | Testing and Confidence Intervals for High Dimensional Proportional Hazards Model | http://arxiv.org/pdf/1412.5158v1.pdf | author:Ethan X. Fang, Yang Ning, Han Liu category:stat.ML math.ST stat.TH published:2014-12-16 summary:This paper proposes a decorrelation-based approach to test hypotheses andconstruct confidence intervals for the low dimensional component of highdimensional proportional hazards models. Motivated by the geometric projectionprinciple, we propose new decorrelated score, Wald and partial likelihood ratiostatistics. Without assuming model selection consistency, we prove theasymptotic normality of these test statistics, establish their semiparametricoptimality. We also develop new procedures for constructing pointwiseconfidence intervals for the baseline hazard function and baseline survivalfunction. Thorough numerical results are provided to back up our theory.
arxiv-8400-112 | Locally Scale-Invariant Convolutional Neural Networks | http://arxiv.org/pdf/1412.5104v1.pdf | author:Angjoo Kanazawa, Abhishek Sharma, David Jacobs category:cs.CV cs.LG cs.NE published:2014-12-16 summary:Convolutional Neural Networks (ConvNets) have shown excellent results on manyvisual classification tasks. With the exception of ImageNet, these datasets arecarefully crafted such that objects are well-aligned at similar scales.Naturally, the feature learning problem gets more challenging as the amount ofvariation in the data increases, as the models have to learn to be invariant tocertain changes in appearance. Recent results on the ImageNet dataset show thatgiven enough data, ConvNets can learn such invariances producing verydiscriminative features [1]. But could we do more: use less parameters, lessdata, learn more discriminative features, if certain invariances were builtinto the learning process? In this paper we present a simple model that allowsConvNets to learn features in a locally scale-invariant manner withoutincreasing the number of model parameters. We show on a modified MNIST datasetthat when faced with scale variation, building in scale-invariance allowsConvNets to learn more discriminative features with reduced chances ofover-fitting.
arxiv-8400-113 | Type-Driven Incremental Semantic Parsing with Polymorphism | http://arxiv.org/pdf/1411.5379v3.pdf | author:Kai Zhao, Liang Huang category:cs.CL published:2014-11-19 summary:Semantic parsing has made significant progress, but most current semanticparsers are extremely slow (CKY-based) and rather primitive in representation.We introduce three new techniques to tackle these problems. First, we designthe first linear-time incremental shift-reduce-style semantic parsing algorithmwhich is more efficient than conventional cubic-time bottom-up semanticparsers. Second, our parser, being type-driven instead of syntax-driven, usestype-checking to decide the direction of reduction, which eliminates the needfor a syntactic grammar such as CCG. Third, to fully exploit the power oftype-driven semantic parsing beyond simple types (such as entities and truthvalues), we borrow from programming language theory the concepts of subtypepolymorphism and parametric polymorphism to enrich the type system in order tobetter guide the parsing. Our system learns very accurate parses in GeoQuery,Jobs and Atis domains.
arxiv-8400-114 | Analysis of Optimal Recombination in Genetic Algorithm for a Scheduling Problem with Setups | http://arxiv.org/pdf/1412.5067v1.pdf | author:A. V. Eremeev, Ju. V. Kovalenko category:cs.NE published:2014-12-16 summary:In this paper, we perform an experimental study of optimal recombinationoperator for makespan minimization problem on single machine withsequence-dependent setup times ($1s_{vu}C_{\max}$). The computationalexperiment on benchmark problems from TSPLIB library indicates practicalapplicability of optimal recombination in crossover operator of geneticalgorithm for $1s_{vu}C_{\max}$.
arxiv-8400-115 | Binary Linear Classification and Feature Selection via Generalized Approximate Message Passing | http://arxiv.org/pdf/1401.0872v3.pdf | author:Justin Ziniel, Philip Schniter, Per Sederberg category:cs.IT math.IT stat.ML published:2014-01-05 summary:For the problem of binary linear classification and feature selection, wepropose algorithmic approaches to classifier design based on the generalizedapproximate message passing (GAMP) algorithm, recently proposed in the contextof compressive sensing. We are particularly motivated by problems where thenumber of features greatly exceeds the number of training examples, but whereonly a few features suffice for accurate classification. We show thatsum-product GAMP can be used to (approximately) minimize the classificationerror rate and max-sum GAMP can be used to minimize a wide variety ofregularized loss functions. Furthermore, we describe anexpectation-maximization (EM)-based scheme to learn the associated modelparameters online, as an alternative to cross-validation, and we show thatGAMP's state-evolution framework can be used to accurately predict themisclassification rate. Finally, we present a detailed numerical study toconfirm the accuracy, speed, and flexibility afforded by our GAMP-basedapproaches to binary linear classification and feature selection.
arxiv-8400-116 | Machine learning for ultrafast X-ray diffraction patterns on large-scale GPU clusters | http://arxiv.org/pdf/1409.4256v2.pdf | author:Tomas Ekeberg, Stefan Engblom, Jing Liu category:q-bio.BM cs.DC cs.LG physics.bio-ph q-bio.QM published:2014-09-11 summary:The classical method of determining the atomic structure of complex moleculesby analyzing diffraction patterns is currently undergoing drastic developments.Modern techniques for producing extremely bright and coherent X-ray lasersallow a beam of streaming particles to be intercepted and hit by an ultrashorthigh energy X-ray beam. Through machine learning methods the data thuscollected can be transformed into a three-dimensional volumetric intensity mapof the particle itself. The computational complexity associated with thisproblem is very high such that clusters of data parallel accelerators arerequired. We have implemented a distributed and highly efficient algorithm forinversion of large collections of diffraction patterns targeting clusters ofhundreds of GPUs. With the expected enormous amount of diffraction data to beproduced in the foreseeable future, this is the required scale to approach realtime processing of data at the beam site. Using both real and synthetic data welook at the scaling properties of the application and discuss the overallcomputational viability of this exciting and novel imaging technique.
arxiv-8400-117 | A Scalable Asynchronous Distributed Algorithm for Topic Modeling | http://arxiv.org/pdf/1412.4986v1.pdf | author:Hsiang-Fu Yu, Cho-Jui Hsieh, Hyokun Yun, S. V. N Vishwanathan, Inderjit S. Dhillon category:cs.DC cs.IR cs.LG published:2014-12-16 summary:Learning meaningful topic models with massive document collections whichcontain millions of documents and billions of tokens is challenging because oftwo reasons: First, one needs to deal with a large number of topics (typicallyin the order of thousands). Second, one needs a scalable and efficient way ofdistributing the computation across multiple machines. In this paper we presenta novel algorithm F+Nomad LDA which simultaneously tackles both these problems.In order to handle large number of topics we use an appropriately modifiedFenwick tree. This data structure allows us to sample from a multinomialdistribution over $T$ items in $O(\log T)$ time. Moreover, when topic countschange the data structure can be updated in $O(\log T)$ time. In order todistribute the computation across multiple processor we present a novelasynchronous framework inspired by the Nomad algorithm of\cite{YunYuHsietal13}. We show that F+Nomad LDA significantly outperformstate-of-the-art on massive problems which involve millions of documents,billions of words, and thousands of topics.
arxiv-8400-118 | Sparse, guided feature connections in an Abstract Deep Network | http://arxiv.org/pdf/1412.4967v1.pdf | author:Anthony Knittel, Alan Blair category:cs.NE published:2014-12-16 summary:We present a technique for developing a network of re-used features, wherethe topology is formed using a coarse learning method, that allowsgradient-descent fine tuning, known as an Abstract Deep Network (ADN). Newfeatures are built based on observed co-occurrences, and the network ismaintained using a selection process related to evolutionary algorithms. Thisallows coarse ex- ploration of the problem space, effective for irregulardomains, while gradient descent allows pre- cise solutions. Accuracy onstandard UCI and Protein-Structure Prediction problems is comparable withbenchmark SVM and optimized GBML approaches, and shows scalability foraddressing large problems. The discrete implementation is symbolic, allowinginterpretability, while the continuous method using fine-tuning shows improvedaccuracy. The binary multiplexer problem is explored, as an irregular domainthat does not support gradient descent learning, showing solution to the bench-mark 135-bit problem. A convolutional implementation is demonstrated on imageclassification, showing an error-rate of 0.79% on the MNIST problem, without apre-defined topology. The ADN system provides a method for developing a verysparse, deep feature topology, based on observed relationships betweenfeatures, that is able to find solutions in irregular domains, and initialize anetwork prior to gradient descent learning.
arxiv-8400-119 | Efficient GPU Implementation for Single Block Orthogonal Dictionary Learning | http://arxiv.org/pdf/1412.4944v1.pdf | author:Paul Irofti category:cs.CV cs.DC published:2014-12-16 summary:Dictionary training for sparse representations involves dealing with largechunks of data and complex algorithms that determine time consumingimplementations. SBO is an iterative dictionary learning algorithm based onconstructing unions of orthonormal bases via singular value decomposition, thatrepresents each data item through a single best fit orthobase. In this paper wepresent a GPGPU approach of implementing SBO in OpenCL. We provide a lock-freesolution that ensures full-occupancy of the GPU by following the map-reducemodel for the sparse-coding stage and by making use of the Partitioned GlobalAddress Space (PGAS) model for developing parallel dictionary updates. Theresulting implementation achieves a favourable trade-off between algorithmcomplexity and data representation quality compared to PAK-SVD which is thestandard overcomplete dictionary learning approach. We present and discussnumerical results showing a significant acceleration of the execution time forthe dictionary learning process.
arxiv-8400-120 | Discovering beautiful attributes for aesthetic image analysis | http://arxiv.org/pdf/1412.4940v1.pdf | author:Luca Marchesotti, Naila Murray, Florent Perronnin category:cs.CV published:2014-12-16 summary:Aesthetic image analysis is the study and assessment of the aestheticproperties of images. Current computational approaches to aesthetic imageanalysis either provide accurate or interpretable results. To obtain bothaccuracy and interpretability by humans, we advocate the use of learned andnameable visual attributes as mid-level features. For this purpose, we proposeto discover and learn the visual appearance of attributes automatically, usinga recently introduced database, called AVA, which contains more than 250,000images together with their aesthetic scores and textual comments given byphotography enthusiasts. We provide a detailed analysis of these annotations aswell as the context in which they were given. We then describe how these threekey components of AVA - images, scores, and comments - can be effectivelyleveraged to learn visual attributes. Lastly, we show that these learnedattributes can be successfully used in three applications: aesthetic qualityprediction, image tagging and retrieval.
arxiv-8400-121 | SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives | http://arxiv.org/pdf/1407.0202v3.pdf | author:Aaron Defazio, Francis Bach, Simon Lacoste-Julien category:cs.LG math.OC stat.ML published:2014-07-01 summary:In this work we introduce a new optimisation method called SAGA in the spiritof SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradientalgorithms with fast linear convergence rates. SAGA improves on the theorybehind SAG and SVRG, with better theoretical convergence rates, and has supportfor composite objectives where a proximal operator is used on the regulariser.Unlike SDCA, SAGA supports non-strongly convex problems directly, and isadaptive to any inherent strong convexity of the problem. We give experimentalresults showing the effectiveness of our method.
arxiv-8400-122 | Asynchronous Adaptation and Learning over Networks - Part III: Comparison Analysis | http://arxiv.org/pdf/1312.5439v3.pdf | author:Xiaochuan Zhao, Ali H. Sayed category:cs.SY cs.IT cs.LG math.IT math.OC published:2013-12-19 summary:In Part II [3] we carried out a detailed mean-square-error analysis of theperformance of asynchronous adaptation and learning over networks under afairly general model for asynchronous events including random topologies,random link failures, random data arrival times, and agents turning on and offrandomly. In this Part III, we compare the performance of synchronous andasynchronous networks. We also compare the performance of decentralizedadaptation against centralized stochastic-gradient (batch) solutions. Twointeresting conclusions stand out. First, the results establish that theperformance of adaptive networks is largely immune to the effect ofasynchronous events: the mean and mean-square convergence rates and theasymptotic bias values are not degraded relative to synchronous or centralizedimplementations. Only the steady-state mean-square-deviation suffers adegradation in the order of $\nu$, which represents the small step-sizeparameters used for adaptation. Second, the results show that the adaptivedistributed network matches the performance of the centralized solution. Theseconclusions highlight another critical benefit of cooperation by networkedagents: cooperation does not only enhance performance in comparison tostand-alone single-agent processing, but it also endows the network withremarkable resilience to various forms of random failure events and is able todeliver performance that is as powerful as batch solutions.
arxiv-8400-123 | Asynchronous Adaptation and Learning over Networks - Part II: Performance Analysis | http://arxiv.org/pdf/1312.5438v3.pdf | author:Xiaochuan Zhao, Ali H. Sayed category:cs.SY cs.IT cs.LG math.IT math.OC published:2013-12-19 summary:In Part I \cite{Zhao13TSPasync1}, we introduced a fairly general model forasynchronous events over adaptive networks including random topologies, randomlink failures, random data arrival times, and agents turning on and offrandomly. We performed a stability analysis and established the notable factthat the network is still able to converge in the mean-square-error sense tothe desired solution. Once stable behavior is guaranteed, it becomes importantto evaluate how fast the iterates converge and how close they get to theoptimal solution. This is a demanding task due to the various asynchronousevents and due to the fact that agents influence each other. In this Part II,we carry out a detailed analysis of the mean-square-error performance ofasynchronous strategies for solving distributed optimization and adaptationproblems over networks. We derive analytical expressions for the mean-squareconvergence rate and the steady-state mean-square-deviation. The expressionsreveal how the various parameters of the asynchronous behavior influencenetwork performance. In the process, we establish the interesting conclusionthat even under the influence of asynchronous events, all agents in theadaptive network can still reach an $O(\nu^{1 + \gamma_o'})$ near-agreementwith some $\gamma_o' > 0$ while approaching the desired solution within$O(\nu)$ accuracy, where $\nu$ is proportional to the small step-size parameterfor adaptation.
arxiv-8400-124 | Asynchronous Adaptation and Learning over Networks --- Part I: Modeling and Stability Analysis | http://arxiv.org/pdf/1312.5434v3.pdf | author:Xiaochuan Zhao, Ali H. Sayed category:cs.SY cs.IT cs.LG math.IT math.OC published:2013-12-19 summary:In this work and the supporting Parts II [2] and III [3], we provide a ratherdetailed analysis of the stability and performance of asynchronous strategiesfor solving distributed optimization and adaptation problems over networks. Weexamine asynchronous networks that are subject to fairly general sources ofuncertainties, such as changing topologies, random link failures, random dataarrival times, and agents turning on and off randomly. Under this model, agentsin the network may stop updating their solutions or may stop sending orreceiving information in a random manner and without coordination with otheragents. We establish in Part I conditions on the first and second-order momentsof the relevant parameter distributions to ensure mean-square stable behavior.We derive in Part II expressions that reveal how the various parameters of theasynchronous behavior influence network performance. We compare in Part III theperformance of asynchronous networks to the performance of both centralizedsolutions and synchronous networks. One notable conclusion is that themean-square-error performance of asynchronous networks shows a degradation onlyof the order of $O(\nu)$, where $\nu$ is a small step-size parameter, while theconvergence rate remains largely unaltered. The results provide a solidjustification for the remarkable resilience of cooperative networks in the faceof random failures at multiple levels: agents, links, data arrivals, andtopology.
arxiv-8400-125 | Combining the Best of Graphical Models and ConvNets for Semantic Segmentation | http://arxiv.org/pdf/1412.4313v2.pdf | author:Michael Cogswell, Xiao Lin, Senthil Purushwalkam, Dhruv Batra category:cs.CV published:2014-12-14 summary:We present a two-module approach to semantic segmentation that incorporatesConvolutional Networks (CNNs) and Graphical Models. Graphical models are usedto generate a small (5-30) set of diverse segmentations proposals, such thatthis set has high recall. Since the number of required proposals is so low, wecan extract fairly complex features to rank them. Our complex feature of choiceis a novel CNN called SegNet, which directly outputs a (coarse) semanticsegmentation. Importantly, SegNet is specifically trained to optimize thecorpus-level PASCAL IOU loss function. To the best of our knowledge, this isthe first CNN specifically designed for semantic segmentation. This two-moduleapproach achieves $52.5\%$ on the PASCAL 2012 segmentation challenge.
arxiv-8400-126 | A Morphological Analyzer for Japanese Nouns, Verbs and Adjectives | http://arxiv.org/pdf/1410.0291v2.pdf | author:Yanchuan Sim category:cs.CL published:2014-10-01 summary:We present an open source morphological analyzer for Japanese nouns, verbsand adjectives. The system builds upon the morphological analyzing capabilitiesof MeCab to incorporate finer details of classification such as politeness,tense, mood and voice attributes. We implemented our analyzer in the form of afinite state transducer using the open source finite state compiler FOMAtoolkit. The source code and tool is available athttps://bitbucket.org/skylander/yc-nlplab/.
arxiv-8400-127 | Learning with Pseudo-Ensembles | http://arxiv.org/pdf/1412.4864v1.pdf | author:Philip Bachman, Ouais Alsharif, Doina Precup category:stat.ML cs.LG cs.NE published:2014-12-16 summary:We formalize the notion of a pseudo-ensemble, a (possibly infinite)collection of child models spawned from a parent model by perturbing itaccording to some noise process. E.g., dropout (Hinton et. al, 2012) in a deepneural network trains a pseudo-ensemble of child subnetworks generated byrandomly masking nodes in the parent network. We present a novel regularizerbased on making the behavior of a pseudo-ensemble robust with respect to thenoise process generating it. In the fully-supervised setting, our regularizermatches the performance of dropout. But, unlike dropout, our regularizernaturally extends to the semi-supervised setting, where it producesstate-of-the-art results. We provide a case study in which we transform theRecursive Neural Tensor Network of (Socher et. al, 2013) into apseudo-ensemble, which significantly improves its performance on a real-worldsentiment analysis benchmark.
arxiv-8400-128 | Max-Margin based Discriminative Feature Learning | http://arxiv.org/pdf/1412.4863v1.pdf | author:Changsheng Li, Qingshan Liu, Weishan Dong, Xin Zhang, Lin Yang category:cs.LG published:2014-12-16 summary:In this paper, we propose a new max-margin based discriminative featurelearning method. Specifically, we aim at learning a low-dimensional featurerepresentation, so as to maximize the global margin of the data and make thesamples from the same class as close as possible. In order to enhance therobustness to noise, a $l_{2,1}$ norm constraint is introduced to make thetransformation matrix in group sparsity. In addition, for multi-classclassification tasks, we further intend to learn and leverage the correlationrelationships among multiple class tasks for assisting in learningdiscriminative features. The experimental results demonstrate the power of theproposed method against the related state-of-the-art methods.
arxiv-8400-129 | Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification | http://arxiv.org/pdf/1412.4526v2.pdf | author:Hongsheng Li, Rui Zhao, Xiaogang Wang category:cs.CV published:2014-12-15 summary:We present highly efficient algorithms for performing forward and backwardpropagation of Convolutional Neural Network (CNN) for pixelwise classificationon images. For pixelwise classification tasks, such as image segmentation andobject detection, surrounding image patches are fed into CNN for predicting theclasses of centered pixels via forward propagation and for updating CNNparameters via backward propagation. However, forward and backward propagationwas originally designed for whole-image classification. Directly applying it topixelwise classification in a patch-by-patch scanning manner is extremelyinefficient, because surrounding patches of pixels have large overlaps, whichlead to a lot of redundant computation. The proposed algorithms eliminate all the redundant computation inconvolution and pooling on images by introducing novel d-regularly sparsekernels. It generates exactly the same results as those by patch-by-patchscanning. Convolution and pooling operations with such kernels are able tocontinuously access memory and can run efficiently on GPUs. A fraction ofpatches of interest can be chosen from each training image for backwardpropagation by applying a mask to the error map at the last CNN layer. Itscomputation complexity is constant with respect to the number of patchessampled from the image. Experiments have shown that our proposed algorithmsspeed up commonly used patch-by-patch scanning over 1500 times in both forwardand backward propagation. The speedup increases with the sizes of images andpatches.
arxiv-8400-130 | Cooperative learning in multi-agent systems from intermittent measurements | http://arxiv.org/pdf/1209.2194v5.pdf | author:Naomi Ehrich Leonard, Alex Olshevsky category:math.OC cs.LG cs.MA cs.SY published:2012-09-11 summary:Motivated by the problem of tracking a direction in a decentralized way, weconsider the general problem of cooperative learning in multi-agent systemswith time-varying connectivity and intermittent measurements. We propose adistributed learning protocol capable of learning an unknown vector $\mu$ fromnoisy measurements made independently by autonomous nodes. Our protocol iscompletely distributed and able to cope with the time-varying, unpredictable,and noisy nature of inter-agent communication, and intermittent noisymeasurements of $\mu$. Our main result bounds the learning speed of ourprotocol in terms of the size and combinatorial features of the (time-varying)networks connecting the nodes.
arxiv-8400-131 | Rule-based Emotion Detection on Social Media: Putting Tweets on Plutchik's Wheel | http://arxiv.org/pdf/1412.4682v1.pdf | author:Erik Tromp, Mykola Pechenizkiy category:cs.CL published:2014-12-15 summary:We study sentiment analysis beyond the typical granularity of polarity andinstead use Plutchik's wheel of emotions model. We introduce RBEM-Emo as anextension to the Rule-Based Emission Model algorithm to deduce such emotionsfrom human-written messages. We evaluate our approach on two different datasetsand compare its performance with the current state-of-the-art techniques foremotion detection, including a recursive auto-encoder. The results of theexperimental study suggest that RBEM-Emo is a promising approach advancing thecurrent state-of-the-art in emotion detection.
arxiv-8400-132 | Kernel-based Information Criterion | http://arxiv.org/pdf/1408.5810v2.pdf | author:Somayeh Danafar, Kenji Fukumizu, Faustino Gomez category:stat.ML published:2014-08-25 summary:This paper introduces Kernel-based Information Criterion (KIC) for modelselection in regression analysis. The novel kernel-based complexity measure inKIC efficiently computes the interdependency between parameters of the modelusing a variable-wise variance and yields selection of better, more robustregressors. Experimental results show superior performance on both simulatedand real data sets compared to Leave-One-Out Cross-Validation (LOOCV),kernel-based Information Complexity (ICOMP), and maximum log of marginallikelihood in Gaussian Process Regression (GPR).
arxiv-8400-133 | A Broadcast News Corpus for Evaluation and Tuning of German LVCSR Systems | http://arxiv.org/pdf/1412.4616v1.pdf | author:Felix Weninger, Björn Schuller, Florian Eyben, Martin Wöllmer, Gerhard Rigoll category:cs.CL cs.SD published:2014-12-15 summary:Transcription of broadcast news is an interesting and challenging applicationfor large-vocabulary continuous speech recognition (LVCSR). We present indetail the structure of a manually segmented and annotated corpus includingover 160 hours of German broadcast news, and propose it as an evaluationframework of LVCSR systems. We show our own experimental results on the corpus,achieved with a state-of-the-art LVCSR decoder, measuring the effect ofdifferent feature sets and decoding parameters, and thereby demonstrate thatreal-time decoding of our test set is feasible on a desktop PC at 9.2% worderror rate.
arxiv-8400-134 | Challenges of Big Data Analysis | http://arxiv.org/pdf/1308.1479v2.pdf | author:Jianqing Fan, Fang Han, Han Liu category:stat.ML published:2013-08-07 summary:Big Data bring new opportunities to modern society and challenges to datascientists. On one hand, Big Data hold great promises for discovering subtlepopulation patterns and heterogeneities that are not possible with small-scaledata. On the other hand, the massive sample size and high dimensionality of BigData introduce unique computational and statistical challenges, includingscalability and storage bottleneck, noise accumulation, spurious correlation,incidental endogeneity, and measurement errors. These challenges aredistinguished and require new computational and statistical paradigm. Thisarticle give overviews on the salient features of Big Data and how thesefeatures impact on paradigm change on statistical and computational methods aswell as computing architectures. We also provide various new perspectives onthe Big Data analysis and computation. In particular, we emphasis on theviability of the sparsest solution in high-confidence set and point out thatexogeneous assumptions in most statistical methods for Big Data can not bevalidated due to incidental endogeneity. They can lead to wrong statisticalinferences and consequently wrong scientific conclusions.
arxiv-8400-135 | CITlab ARGUS for Arabic Handwriting | http://arxiv.org/pdf/1412.6061v1.pdf | author:Gundram Leifert, Roger Labahn, Tobias Strauß category:cs.CV cs.NE 68T10, 68T05 published:2014-12-15 summary:In the recent years it turned out that multidimensional recurrent neuralnetworks (MDRNN) perform very well for offline handwriting recognition taskslike the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing anddictionary lookup, our ARGUS software completed this task with an error rate of26.27% in its primary setup.
arxiv-8400-136 | CITlab ARGUS for historical data tables | http://arxiv.org/pdf/1412.6012v1.pdf | author:Gundram Leifert, Tobias Grüning, Tobias Strauß, Roger Labahn, for the University o category:cs.CV cs.NE 68T05, 68T10 published:2014-12-15 summary:We describe CITlab's recognition system for the ANWRESH-2014 competitionattached to the 14. International Conference on Frontiers in HandwritingRecognition, ICFHR 2014. The task comprises word recognition from segmentedhistorical documents. The core components of our system are based onmulti-dimensional recurrent neural networks (MDRNN) and connectionist temporalclassification (CTC). The software modules behind that as well as the basicutility technologies are essentially powered by PLANET's ARGUS framework forintelligent text recognition and image processing.
arxiv-8400-137 | Automatic video scene segmentation based on spatial-temporal clues and rhythm | http://arxiv.org/pdf/1412.4470v1.pdf | author:Walid Mahdi, Liming Chen, Mohsen Ardebilian category:cs.CV published:2014-12-15 summary:With ever increasing computing power and data storage capacity, the potentialfor large digital video libraries is growing rapidly.However, the massive useof video for the moment is limited by its opaque characteristics. Indeed, auser who has to handle and retrieve sequentially needs too much time in orderto find out segments of interest within a video. Therefore, providing anenvironment both convenient and efficient for video storing and retrieval,especially for content-based searching as this exists in traditional textbaseddatabase systems, has been the focus of recent and important efforts of a largeresearch community In this paper, we propose a new automatic video scene segmentation methodthat explores two main video features; these are spatial-temporal relationshipand rhythm of shots. The experimental evidence we obtained from a 80minutevideo showed that our prototype provides very high accuracy for videosegmentation.
arxiv-8400-138 | Fixed Point Algorithm Based on Quasi-Newton Method for Convex Minimization Problem with Application to Image Deblurring | http://arxiv.org/pdf/1412.4438v1.pdf | author:Dai-Qiang Chen category:cs.CV published:2014-12-15 summary:Solving an optimization problem whose objective function is the sum of twoconvex functions has received considerable interests in the context of imageprocessing recently. In particular, we are interested in the scenario when anon-differentiable convex function such as the total variation (TV) norm isincluded in the objective function due to many variational models establishedin image processing have this nature. In this paper, we propose a fast fixedpoint algorithm based on the quasi-Newton method for solving this class ofproblem, and apply it in the field of TV-based image deblurring. The novelmethod is derived from the idea of the quasi-Newton method, and the fixed-pointalgorithms based on the proximity operator, which were widely investigated veryrecently. Utilizing the non-expansion property of the proximity operator wefurther investigate the global convergence of the proposed algorithm. Numericalexperiments on image deblurring problem with additive or multiplicative noiseare presented to demonstrate that the proposed algorithm is superior to therecently developed fixed-point algorithm in the computational efficiency.
arxiv-8400-139 | The Computational Theory of Intelligence: Applications to Genetic Programming and Turing Machines | http://arxiv.org/pdf/1412.6144v1.pdf | author:Daniel Kovach category:cs.NE published:2014-12-14 summary:In this paper, we continue the efforts of the Computational Theory ofIntelligence (CTI) by extending concepts to include computational processes interms of Genetic Algorithms (GA's) and Turing Machines (TM's). Active, Passive,and Hybrid Computational Intelligence processes are also introduced anddiscussed. We consider the ramifications of the assumptions of CTI with regardto the qualities of reproduction and virility. Applications to Biology,Computer Science and Cyber Security are also discussed.
arxiv-8400-140 | Sequence to Sequence Learning with Neural Networks | http://arxiv.org/pdf/1409.3215v3.pdf | author:Ilya Sutskever, Oriol Vinyals, Quoc V. Le category:cs.CL cs.LG published:2014-09-10 summary:Deep Neural Networks (DNNs) are powerful models that have achieved excellentperformance on difficult learning tasks. Although DNNs work well whenever largelabeled training sets are available, they cannot be used to map sequences tosequences. In this paper, we present a general end-to-end approach to sequencelearning that makes minimal assumptions on the sequence structure. Our methoduses a multilayered Long Short-Term Memory (LSTM) to map the input sequence toa vector of a fixed dimensionality, and then another deep LSTM to decode thetarget sequence from the vector. Our main result is that on an English toFrench translation task from the WMT'14 dataset, the translations produced bythe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM'sBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM didnot have difficulty on long sentences. For comparison, a phrase-based SMTsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTMto rerank the 1000 hypotheses produced by the aforementioned SMT system, itsBLEU score increases to 36.5, which is close to the previous best result onthis task. The LSTM also learned sensible phrase and sentence representationsthat are sensitive to word order and are relatively invariant to the active andthe passive voice. Finally, we found that reversing the order of the words inall source sentences (but not target sentences) improved the LSTM's performancemarkedly, because doing so introduced many short term dependencies between thesource and the target sentence which made the optimization problem easier.
arxiv-8400-141 | Tools for Terminology Processing | http://arxiv.org/pdf/1412.4401v1.pdf | author:C. Enguehard, B. Daille, E. Morin category:cs.CY cs.CL published:2014-12-14 summary:Automatic terminology processing appeared 10 years ago when electroniccorpora became widely available. Such processing may be statistically orlinguistically based and produces terminology resources that can be used in anumber of applications : indexing, information retrieval, technology watch,etc. We present the tools that have been developed in the IRIN Institute. Theyall take as input texts (or collection of texts) and reflect different statesof terminology processing: term acquisition, term recognition and termstructuring.
arxiv-8400-142 | Bach in 2014: Music Composition with Recurrent Neural Network | http://arxiv.org/pdf/1412.3191v2.pdf | author:I-Ting Liu, Bhiksha Ramakrishnan category:cs.AI cs.NE published:2014-12-10 summary:We propose a framework for computer music composition that uses resilientpropagation (RProp) and long short term memory (LSTM) recurrent neural network.In this paper, we show that LSTM network learns the structure andcharacteristics of music pieces properly by demonstrating its ability torecreate music. We also show that predicting existing music using RPropoutperforms Back propagation through time (BPTT).
arxiv-8400-143 | First order algorithms in variational image processing | http://arxiv.org/pdf/1412.4237v1.pdf | author:Martin Burger, Alex Sawatzky, Gabriele Steidl category:math.OC cs.CV stat.ML published:2014-12-13 summary:Variational methods in imaging are nowadays developing towards a quiteuniversal and flexible tool, allowing for highly successful approaches on taskslike denoising, deblurring, inpainting, segmentation, super-resolution,disparity, and optical flow estimation. The overall structure of suchapproaches is of the form ${\cal D}(Ku) + \alpha {\cal R} (u) \rightarrow\min_u$ ; where the functional ${\cal D}$ is a data fidelity term alsodepending on some input data $f$ and measuring the deviation of $Ku$ from suchand ${\cal R}$ is a regularization functional. Moreover $K$ is a (often linear)forward operator modeling the dependence of data on an underlying image, and$\alpha$ is a positive regularization parameter. While ${\cal D}$ is oftensmooth and (strictly) convex, the current practice almost exclusively usesnonsmooth regularization functionals. The majority of successful techniques isusing nonsmooth and convex functionals like the total variation andgeneralizations thereof or $\ell_1$-norms of coefficients arising from scalarproducts with some frame system. The efficient solution of such variationalproblems in imaging demands for appropriate algorithms. Taking into account thespecific structure as a sum of two very different terms to be minimized,splitting algorithms are a quite canonical choice. Consequently this field hasrevived the interest in techniques like operator splittings or augmentedLagrangians. Here we shall provide an overview of methods currently developedand recent results as well as some computational studies providing a comparisonof different methods and also illustrating their success in applications.
arxiv-8400-144 | Optimization of Reliability of Network of Given Connectivity using Genetic Algorithm | http://arxiv.org/pdf/1412.4218v1.pdf | author:Ho Tat Lam, Kwok Yip Szeto category:physics.soc-ph cs.NE cs.SI published:2014-12-13 summary:Reliability is one of the important measures of how well the system meets itsdesign objective, and mathematically is the probability that a system willperform satisfactorily for at least a given period of time. When the system isdescribed by a connected network of N components (nodes) and their L connection(links), the reliability of the system becomes a difficult network designproblem which solutions are of great practical interest in science andengineering. This paper discusses the numerical method of finding the mostreliable network for a given N and L using genetic algorithm. For a giventopology of the network, the reliability is numerically computed usingadjacency matrix. For a search in the space of all possible topologies of theconnected network with N nodes and L links, genetic operators such as mutationand crossover are applied to the adjacency matrix through a stringrepresentation. In the context of graphs, the mutation of strings in geneticalgorithm corresponds to the rewiring of graphs, while crossover corresponds tothe interchange of the sub-graphs. For small networks where the most reliablenetwork can be found by exhaustive search, genetic algorithm is very efficient.For larger networks, our results not only demonstrate the efficiency of ouralgorithm, but also suggest that the most reliable network will have highsymmetry.
arxiv-8400-145 | A Study of Sindhi Related and Arabic Script Adapted languages Recognition | http://arxiv.org/pdf/1412.4217v1.pdf | author:Dil Nawaz Hakro, A. Z. Talib, Zeeshan Bhatti, G. N. Moja category:cs.CV published:2014-12-13 summary:A large number of publications are available for the Optical CharacterRecognition (OCR). Significant researches, as well as articles are present forthe Latin, Chinese and Japanese scripts. Arabic script is also one of maturescript from OCR perspective. The adaptive languages which share Arabic scriptor its extended characters; still lacking the OCRs for their language. In thispaper we present the efforts of researchers on Arabic and its related andadapted languages. This survey is organized in different sections, in whichintroduction is followed by properties of Sindhi Language. OCR processtechniques and methods used by various researchers are presented. The lastsection is dedicated for future work and conclusion is also discussed.
arxiv-8400-146 | The application of the Bayes Ying Yang harmony based GMMs in on-line signature verification | http://arxiv.org/pdf/1412.4205v1.pdf | author:Xiaosha Zhao, Mandan Liu category:cs.CV published:2014-12-13 summary:In this contribution, a Bayes Ying Yang(BYY) harmony based approach foron-line signature verification is presented. In the proposed method, a simplebut effective Gaussian Mixture Models(GMMs) is used to represent for eachuser's signature model based on the prior information collected. Different fromthe early works, in this paper, we use the Bayes Ying Yang machine combinedwith the harmony function to achieve Automatic Model Selection(AMS) during theparameter learning for the GMMs, so that a better approximation of the usermodel is assured. Experiments on a database from the First InternationalSignature Verification Competition(SVC 2004) confirm that this combinedalgorithm yields quite satisfactory results.
arxiv-8400-147 | Descriptor Ensemble: An Unsupervised Approach to Descriptor Fusion in the Homography Space | http://arxiv.org/pdf/1412.4196v1.pdf | author:Yuan-Ting Hu, Yen-Yu Lin, Hsin-Yi Chen, Kuang-Jui Hsu, Bing-Yu Chen category:cs.CV published:2014-12-13 summary:With the aim to improve the performance of feature matching, we present anunsupervised approach to fuse various local descriptors in the space ofhomographies. Inspired by the observation that the homographies of correctfeature correspondences vary smoothly along the spatial domain, our approachstands on the unsupervised nature of feature matching, and can select a gooddescriptor for matching each feature point. Specifically, the homography spaceserves as the common domain, in which a correspondence obtained by anydescriptor is considered as a point, for integrating various heterogeneousdescriptors. Both geometric coherence and spatial continuity amongcorrespondences are considered via computing their geodesic distances in thespace. In this way, mutual verification across different descriptors isallowed, and correct correspondences will be highlighted with a high degree ofconsistency (i.e., short geodesic distances here). It follows that one-classSVM can be applied to identifying these correct correspondences, and boosts theperformance of feature matching. The proposed approach is comprehensivelycompared with the state-of-the-art approaches, and evaluated on four benchmarksof image matching. The promising results manifest its effectiveness.
arxiv-8400-148 | An Evaluation of Support Vector Machines as a Pattern Recognition Tool | http://arxiv.org/pdf/1412.4186v1.pdf | author:Eugene Borovikov category:cs.LG 62-07 published:2014-12-13 summary:The purpose of this report is in examining the generalization performance ofSupport Vector Machines (SVM) as a tool for pattern recognition and objectclassification. The work is motivated by the growing popularity of the methodthat is claimed to guarantee a good generalization performance for the task inhand. The method is implemented in MATLAB. SVMs based on various kernels aretested for classifying data from various domains.
arxiv-8400-149 | A survey of modern optical character recognition techniques | http://arxiv.org/pdf/1412.4183v1.pdf | author:Eugene Borovikov category:cs.CV 62-04 published:2014-12-13 summary:This report explores the latest advances in the field of digital documentrecognition. With the focus on printed document imagery, we discuss the majordevelopments in optical character recognition (OCR) and document imageenhancement/restoration in application to Latin and non-Latin scripts. Inaddition, we review and discuss the available technologies for hand-writtendocument recognition. In this report, we also provide some company-accumulatedbenchmark results on available OCR engines.
arxiv-8400-150 | The Statistics of Streaming Sparse Regression | http://arxiv.org/pdf/1412.4182v1.pdf | author:Jacob Steinhardt, Stefan Wager, Percy Liang category:math.ST cs.LG stat.ML stat.TH published:2014-12-13 summary:We present a sparse analogue to stochastic gradient descent that isguaranteed to perform well under similar conditions to the lasso. In the linearregression setup with irrepresentable noise features, our algorithm recoversthe support set of the optimal parameter vector with high probability, andachieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T),where k is the sparsity of the solution, d is the number of features, and T isthe number of training examples. Meanwhile, our algorithm does not require anymore computational resources than stochastic gradient descent. In ourexperiments, we find that our method substantially out-performs existingstreaming algorithms on both real and simulated data.
arxiv-8400-151 | Optimizing Over Radial Kernels on Compact Manifolds | http://arxiv.org/pdf/1412.4175v1.pdf | author:Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV published:2014-12-13 summary:We tackle the problem of optimizing over all possible positive definiteradial kernels on Riemannian manifolds for classification. Kernel methods onRiemannian manifolds have recently become increasingly popular in computervision. However, the number of known positive definite kernels on manifoldsremain very limited. Furthermore, most kernels typically depend on at least oneparameter that needs to be tuned for the problem at hand. A poor choice ofkernel, or of parameter value, may yield significant performance drop-off.Here, we show that positive definite radial kernels on the unit n-sphere, theGrassmann manifold and Kendall's shape manifold can be expressed in a simpleform whose parameters can be automatically optimized within a support vectormachine framework. We demonstrate the benefits of our kernel learning algorithmon object, face, action and shape recognition.
arxiv-8400-152 | Feature Weight Tuning for Recursive Neural Networks | http://arxiv.org/pdf/1412.3714v2.pdf | author:Jiwei Li category:cs.NE cs.AI cs.CL cs.LG published:2014-12-11 summary:This paper addresses how a recursive neural network model can automaticallyleave out useless information and emphasize important evidence, in other words,to perform "weight tuning" for higher-level representation acquisition. Wepropose two models, Weighted Neural Network (WNN) and Binary-Expectation NeuralNetwork (BENN), which automatically control how much one specific unitcontributes to the higher-level representation. The proposed model can beviewed as incorporating a more powerful compositional function for embeddingacquisition in recursive neural networks. Experimental results demonstrate thesignificant improvement over standard neural models.
arxiv-8400-153 | A Framework for Shape Analysis via Hilbert Space Embedding | http://arxiv.org/pdf/1412.4174v1.pdf | author:Sadeep Jayasumana, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV published:2014-12-13 summary:We propose a framework for 2D shape analysis using positive definite kernelsdefined on Kendall's shape manifold. Different representations of 2D shapes areknown to generate different nonlinear spaces. Due to the nonlinearity of thesespaces, most existing shape classification algorithms resort to nearestneighbor methods and to learning distances on shape spaces. Here, we propose tomap shapes on Kendall's shape manifold to a high dimensional Hilbert spacewhere Euclidean geometry applies. To this end, we introduce a kernel on thismanifold that permits such a mapping, and prove its positive definiteness. Thiskernel lets us extend kernel-based algorithms developed for Euclidean spaces,such as SVM, MKL and kernel PCA, to the shape manifold. We demonstrate thebenefits of our approach over the state-of-the-art methods on shapeclassification, clustering and retrieval.
arxiv-8400-154 | Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite Matrices | http://arxiv.org/pdf/1412.4172v1.pdf | author:Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV published:2014-12-13 summary:Symmetric Positive Definite (SPD) matrices have become popular to encodeimage information. Accounting for the geometry of the Riemannian manifold ofSPD matrices has proven key to the success of many algorithms. However, mostexisting methods only approximate the true shape of the manifold locally by itstangent plane. In this paper, inspired by kernel methods, we propose to map SPDmatrices to a high dimensional Hilbert space where Euclidean geometry applies.To encode the geometry of the manifold in the mapping, we introduce a family ofprovably positive definite kernels on the Riemannian manifold of SPD matrices.These kernels are derived from the Gaussian ker- nel, but exploit differentmetrics on the manifold. This lets us extend kernel-based algorithms developedfor Euclidean spaces, such as SVM and kernel PCA, to the Riemannian manifold ofSPD matrices. We demonstrate the benefits of our approach on the problems ofpedestrian detection, ob- ject categorization, texture analysis, 2D motionsegmentation and Diffusion Tensor Imaging (DTI) segmentation.
arxiv-8400-155 | Expanded Alternating Optimization of Nonconvex Functions with Applications to Matrix Factorization and Penalized Regression | http://arxiv.org/pdf/1412.4128v1.pdf | author:W. James Murdoch, Mu Zhu category:stat.CO stat.ML published:2014-12-12 summary:We propose a general technique for improving alternating optimization (AO) ofnonconvex functions. Starting from the solution given by AO, we conduct anothersequence of searches over subspaces that are both meaningful to theoptimization problem at hand and different from those used by AO. Todemonstrate the utility of our approach, we apply it to the matrixfactorization (MF) algorithm for recommender systems and the coordinate descentalgorithm for penalized regression (PR), and show meaningful improvements usingboth real-world (for MF) and simulated (for PR) data sets. Moreover, wedemonstrate for MF that, by constructing search spaces customized to the givendata set, we can significantly increase the convergence rate of our technique.
arxiv-8400-156 | Representing Data by a Mixture of Activated Simplices | http://arxiv.org/pdf/1412.4102v1.pdf | author:Chunyu Wang, John Flynn, Yizhou Wang, Alan L. Yuille category:cs.CV published:2014-12-12 summary:We present a new model which represents data as a mixture of simplices.Simplices are geometric structures that generalize triangles. We give a simplegeometric understanding that allows us to learn a simplicial structureefficiently. Our method requires that the data are unit normalized (and thuslie on the unit sphere). We show that under this restriction, building a modelwith simplices amounts to constructing a convex hull inside the sphere whoseboundary facets is close to the data. We call the boundary facets of the convexhull that are close to the data Activated Simplices. While the total number ofbases used to build the simplices is a parameter of the model, the dimensionsof the individual activated simplices are learned from the data. Simplices canhave different dimensions, which facilitates modeling of inhomogeneous datasources. The simplicial structure is bounded --- this is appropriate formodeling data with constraints, such as human elbows can not bend more than 180degrees. The simplices are easy to interpret and extremes within the data canbe discovered among the vertices. The method provides good reconstruction andregularization. It supports good nearest neighbor classification and it allowsrealistic generative models to be constructed. It achieves state-of-the-artresults on benchmark datasets, including 3D poses and digits.
arxiv-8400-157 | Multiscale Fields of Patterns | http://arxiv.org/pdf/1406.0924v3.pdf | author:Pedro F. Felzenszwalb, John G. Oberlin category:cs.CV published:2014-06-04 summary:We describe a framework for defining high-order image models that can be usedin a variety of applications. The approach involves modeling local patterns ina multiscale representation of an image. Local properties of a coarsened imagereflect non-local properties of the original image. In the case of binaryimages local properties are defined by the binary patterns observed over smallneighborhoods around each pixel. With the multiscale representation we capturethe frequency of patterns observed at different scales of resolution. Thisframework leads to expressive priors that depend on a relatively small numberof parameters. For inference and learning we use an MCMC method for blocksampling with very large blocks. We evaluate the approach with two exampleapplications. One involves contour detection. The other involves binarysegmentation.
arxiv-8400-158 | Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso | http://arxiv.org/pdf/1412.4080v1.pdf | author:Antoine Bonnefoy, Valentin Emiya, Liva Ralaivola, Rémi Gribonval category:stat.ML cs.LG published:2014-12-12 summary:Recent computational strategies based on screening tests have been proposedto accelerate algorithms addressing penalized sparse regression problems suchas the Lasso. Such approaches build upon the idea that it is worth dedicatingsome small computational effort to locate inactive atoms and remove them fromthe dictionary in a preprocessing stage so that the regression algorithmworking with a smaller dictionary will then converge faster to the solution ofthe initial problem. We believe that there is an even more efficient way toscreen the dictionary and obtain a greater acceleration: inside each iterationof the regression algorithm, one may take advantage of the algorithmcomputations to obtain a new screening test for free with increasing screeningeffects along the iterations. The dictionary is henceforth dynamically screenedinstead of being screened statically, once and for all, before the firstiteration. We formalize this dynamic screening principle in a generalalgorithmic scheme and apply it by embedding inside a number of first-orderalgorithms adapted existing screening tests to solve the Lasso or new screeningtests to solve the Group-Lasso. Computational gains are assessed in a large setof experiments on synthetic data as well as real-world sounds and images. Theyshow both the screening efficiency and the gain in terms running times.
arxiv-8400-159 | Computabilities of Validity and Satisfiability in Probability Logics over Finite and Countable Models | http://arxiv.org/pdf/1410.3059v2.pdf | author:Greg Yang category:cs.LO cs.LG math.LO math.PR published:2014-10-12 summary:The $\epsilon$-logic (which is called $\epsilon$E-logic in this paper) ofKuyper and Terwijn is a variant of first order logic with the same syntax, inwhich the models are equipped with probability measures and in which the$\forall x$ quantifier is interpreted as "there exists a set $A$ of measure$\ge 1 - \epsilon$ such that for each $x \in A$, ...." Previously, Kuyper andTerwijn proved that the general satisfiability and validity problems for thislogic are, i) for rational $\epsilon \in (0, 1)$, respectively$\Sigma^1_1$-complete and $\Pi^1_1$-hard, and ii) for $\epsilon = 0$,respectively decidable and $\Sigma^0_1$-complete. The adjective "general" heremeans "uniformly over all languages." We extend these results in the scenario of finite models. In particular, weshow that the problems of satisfiability by and validity over finite models in$\epsilon$E-logic are, i) for rational $\epsilon \in (0, 1)$, respectively$\Sigma^0_1$- and $\Pi^0_1$-complete, and ii) for $\epsilon = 0$, respectivelydecidable and $\Pi^0_1$-complete. Although partial results toward the countablecase are also achieved, the computability of $\epsilon$E-logic over countablemodels still remains largely unsolved. In addition, most of the results, ofthis paper and of Kuyper and Terwijn, do not apply to individual languages witha finite number of unary predicates. Reducing this requirement continues to bea major point of research. On the positive side, we derive the decidability of the correspondingproblems for monadic relational languages --- equality- and function-freelanguages with finitely many unary and zero other predicates. This result holdsfor all three of the unrestricted, the countable, and the finite model cases. Applications in computational learning theory, weighted graphs, and neuralnetworks are discussed in the context of these decidability and undecidabilityresults.
arxiv-8400-160 | Using Sentence Plausibility to Learn the Semantics of Transitive Verbs | http://arxiv.org/pdf/1411.7942v2.pdf | author:Tamara Polajnar, Laura Rimell, Stephen Clark category:cs.CL published:2014-11-28 summary:The functional approach to compositional distributional semantics considerstransitive verbs to be linear maps that transform the distributional vectorsrepresenting nouns into a vector representing a sentence. We conduct an initialinvestigation that uses a matrix consisting of the parameters of a logisticregression classifier trained on a plausibility task as a transitive verbfunction. We compare our method to a commonly used corpus-based method forconstructing a verb matrix and find that the plausibility training may be moreeffective for disambiguation tasks.
arxiv-8400-161 | An Automatic Seeded Region Growing for 2D Biomedical Image Segmentation | http://arxiv.org/pdf/1412.3958v1.pdf | author:Mohammed M. Abdelsamea category:cs.CV published:2014-12-12 summary:In this paper, an automatic seeded region growing algorithm is proposed forcellular image segmentation. First, the regions of interest (ROIs) extractedfrom the preprocessed image. Second, the initial seeds are automaticallyselected based on ROIs extracted from the image. Third, the most reprehensiveseeds are selected using a machine learning algorithm. Finally, the cellularimage is segmented into regions where each region corresponds to a seed. Theaim of the proposed is to automatically extract the Region of Interests (ROI)from the cellular images in terms of overcoming the explosion, undersegmentation and over segmentation problems. Experimental results show that theproposed algorithm can improve the segmented image and the segmented resultsare less noisy as compared to some existing algorithms.
arxiv-8400-162 | CITlab ARGUS for historical handwritten documents | http://arxiv.org/pdf/1412.3949v1.pdf | author:Tobias Strauß, Tobias Grüning, Gundram Leifert, Roger Labahn, for the University o category:cs.CV cs.NE 68T05, 68T10 published:2014-12-12 summary:We describe CITlab's recognition system for the HTRtS competition attached tothe 14. International Conference on Frontiers in Handwriting Recognition, ICFHR2014. The task comprises the recognition of historical handwritten documents.The core algorithms of our system are based on multi-dimensional recurrentneural networks (MDRNN) and connectionist temporal classification (CTC). Thesoftware modules behind that as well as the basic utility technologies areessentially powered by PLANET's ARGUS framework for intelligent textrecognition and image processing.
arxiv-8400-163 | Sparse Graph-based Transduction for Image Classification | http://arxiv.org/pdf/1408.6257v2.pdf | author:Sheng Huang, Dan Yang, Jia Zhou, Luwen Huangfu, Xiaohong Zhang category:cs.CV published:2014-08-26 summary:Motivated by the remarkable successes of Graph-based Transduction (GT) andSparse Representation (SR), we present a novel Classifier named SparseGraph-based Classifier (SGC) for image classification. In SGC, SR is leveragedto measure the correlation (similarity) of each two samples and a graph isconstructed for encoding these correlations. Then the Laplacian eigenmapping isadopted for deriving the graph Laplacian of the graph. Finally, SGC can beobtained by plugging the graph Laplacian into the conventional GT framework. Inthe image classification procedure, SGC utilizes the correlations, which areencoded in the learned graph Laplacian, to infer the labels of unlabeledimages. SGC inherits the merits of both GT and SR. Compared to SR, SGC improvesthe robustness and the discriminating power of GT. Compared to GT, SGCsufficiently exploits the whole data. Therefore it alleviates the undercompletedictionary issue suffered by SR. Four popular image databases are employed forevaluation. The results demonstrate that SGC can achieve a promisingperformance in comparison with the state-of-the-art classifiers, particularlyin the small training sample size case and the noisy sample case.
arxiv-8400-164 | Region segmentation for sparse decompositions: better brain parcellations from rest fMRI | http://arxiv.org/pdf/1412.3925v1.pdf | author:Alexandre Abraham, Elvis Dohmatob, Bertrand Thirion, Dimitris Samaras, Gael Varoquaux category:q-bio.NC cs.CV published:2014-12-12 summary:Functional Magnetic Resonance Images acquired during resting-state provideinformation about the functional organization of the brain through measuringcorrelations between brain areas. Independent components analysis is thereference approach to estimate spatial components from weakly structured datasuch as brain signal time courses; each of these components may be referred toas a brain network and the whole set of components can be conceptualized as abrain functional atlas. Recently, new methods using a sparsity prior haveemerged to deal with low signal-to-noise ratio data. However, even when usingsophisticated priors, the results may not be very sparse and most often do notseparate the spatial components into brain regions. This work presentspost-processing techniques that automatically sparsify brain maps and separateregions properly using geometric operations, and compares these techniquesaccording to faithfulness to data and stability metrics. In particular, amongthreshold-based approaches, hysteresis thresholding and random walkersegmentation, the latter improves significantly the stability of both dense andsparse models.
arxiv-8400-165 | Size sensitive packing number for Hamming cube and its consequences | http://arxiv.org/pdf/1412.3922v1.pdf | author:Kunal Dutta, Arijit Ghosh category:cs.DM cs.CG cs.LG math.CO published:2014-12-12 summary:We prove a size-sensitive version of Haussler's Packinglemma~\cite{Haussler92spherepacking} for set-systems with bounded primalshatter dimension, which have an additional {\em size-sensitive property}. Thisanswers a question asked by Ezra~\cite{Ezra-sizesendisc-soda-14}. We alsopartially address another point raised by Ezra regarding overcounting of setsin her chaining procedure. As a consequence of these improvements, we get animprovement on the size-sensitive discrepancy bounds for set systems with theabove property. Improved bounds on the discrepancy for these special setsystems also imply an improvement in the sizes of {\em relative $(\varepsilon,\delta)$-approximations} and $(\nu, \alpha)$-samples.
arxiv-8400-166 | Machine Learning for Neuroimaging with Scikit-Learn | http://arxiv.org/pdf/1412.3919v1.pdf | author:Alexandre Abraham, Fabian Pedregosa, Michael Eickenberg, Philippe Gervais, Andreas Muller, Jean Kossaifi, Alexandre Gramfort, Bertrand Thirion, Gäel Varoquaux category:cs.LG cs.CV stat.ML published:2014-12-12 summary:Statistical machine learning methods are increasingly used for neuroimagingdata analysis. Their main virtue is their ability to model high-dimensionaldatasets, e.g. multivariate analysis of activation images or resting-state timeseries. Supervised learning is typically used in decoding or encoding settingsto relate brain images to behavioral or clinical observations, whileunsupervised learning can uncover hidden structures in sets of images (e.g.resting state functional MRI) or find sub-populations in large cohorts. Byconsidering different functional neuroimaging applications, we illustrate howscikit-learn, a Python machine learning library, can be used to perform somekey analysis steps. Scikit-learn contains a very large set of statisticallearning algorithms, both supervised and unsupervised, and its application toneuroimaging data provides a versatile tool to study the brain.
arxiv-8400-167 | Edge Preserving Multi-Modal Registration Based On Gradient Intensity Self-Similarity | http://arxiv.org/pdf/1412.3914v1.pdf | author:Tamar Rott, Dorin Shriki, Tamir Bendory category:cs.CV published:2014-12-12 summary:Image registration is a challenging task in the world of medical imaging.Particularly, accurate edge registration plays a central role in a variety ofclinical conditions. The Modality Independent Neighbourhood Descriptor (MIND)demonstrates state of the art alignment, based on the image self-similarity.However, this method appears to be less accurate regarding edge registration.In this work, we propose a new registration method, incorporating gradientintensity and MIND self-similarity metric. Experimental results show thesuperiority of this method in edge registration tasks, while preserving theoriginal MIND performance for other image features and textures.
arxiv-8400-168 | Score Function Features for Discriminative Learning: Matrix and Tensor Framework | http://arxiv.org/pdf/1412.2863v2.pdf | author:Majid Janzamin, Hanie Sedghi, Anima Anandkumar category:cs.LG stat.ML published:2014-12-09 summary:Feature learning forms the cornerstone for tackling challenging learningproblems in domains such as speech, computer vision and natural languageprocessing. In this paper, we consider a novel class of matrix andtensor-valued features, which can be pre-trained using unlabeled samples. Wepresent efficient algorithms for extracting discriminative information, giventhese pre-trained features and labeled samples for any related task. Our classof features are based on higher-order score functions, which capture localvariations in the probability density function of the input. We establish atheoretical framework to characterize the nature of discriminative informationthat can be extracted from score-function features, when used in conjunctionwith labeled samples. We employ efficient spectral decomposition algorithms (onmatrices and tensors) for extracting discriminative components. The advantageof employing tensor-valued features is that we can extract richerdiscriminative information in the form of an overcomplete representations.Thus, we present a novel framework for employing generative models of the inputfor discriminative learning.
arxiv-8400-169 | Simulating a perceptron on a quantum computer | http://arxiv.org/pdf/1412.3635v1.pdf | author:Maria Schuld, Ilya Sinayskiy, Francesco Petruccione category:quant-ph cs.LG cs.NE published:2014-12-11 summary:Perceptrons are the basic computational unit of artificial neural networks,as they model the activation mechanism of an output neuron due to incomingsignals from its neighbours. As linear classifiers, they play an important rolein the foundations of machine learning. In the context of the emerging field ofquantum machine learning, several attempts have been made to develop acorresponding unit using quantum information theory. Based on the quantum phaseestimation algorithm, this paper introduces a quantum perceptron modelimitating the step-activation function of a classical perceptron. This schemerequires resources in $\mathcal{O}(n)$ (where $n$ is the size of the input) andpromises efficient applications for more complex structures such as trainablequantum neural networks.
arxiv-8400-170 | Efficient penalty search for multiple changepoint problems | http://arxiv.org/pdf/1412.3617v1.pdf | author:Kaylea Haynes, Idris A. Eckley, Paul Fearnhead category:stat.CO stat.ML published:2014-12-11 summary:In the multiple changepoint setting, various search methods have beenproposed which involve optimising either a constrained or penalised costfunction over possible numbers and locations of changepoints using dynamicprogramming. Such methods are typically computationally intensive. Recent workin the penalised optimisation setting has focussed on developing apruning-based approach which gives an improved computational cost that, undercertain conditions, is linear in the number of data points. Such an approachnaturally requires the specification of a penalty to avoid under/over-fitting.Work has been undertaken to identify the appropriate penalty choice for datagenerating processes with known distributional form, but in many applicationsthe model assumed for the data is not correct and these penalty choices are notalways appropriate. Consequently it is desirable to have an approach thatenables us to compare segmentations for different choices of penalty. To thisend we present a method to obtain optimal changepoint segmentations of datasequences for all penalty values across a continuous range. This permits anevaluation of the various segmentations to identify a suitably parsimoniouspenalty choice. The computational complexity of this approach can be linear inthe number of data points and linear in the difference between the number ofchangepoints in the optimal segmentations for the smallest and largest penaltyvalues. This can be orders of magnitude faster than alternative approaches thatfind optimal segmentations for a range of the number of changepoints.
arxiv-8400-171 | High-level numerical simulations of noise in CCD and CMOS photosensors: review and tutorial | http://arxiv.org/pdf/1412.4031v1.pdf | author:Mikhail Konnik, James Welsh category:astro-ph.IM cs.CV published:2014-12-11 summary:In many applications, such as development and testing of image processingalgorithms, it is often necessary to simulate images containing realistic noisefrom solid-state photosensors. A high-level model of CCD and CMOS photosensorsbased on a literature review is formulated in this paper. The model includesphoto-response non-uniformity, photon shot noise, dark current Fixed PatternNoise, dark current shot noise, offset Fixed Pattern Noise, source followernoise, sense node reset noise, and quantisation noise. The model also includesvoltage-to-voltage, voltage-to-electrons, and analogue-to-digital converternon-linearities. The formulated model can be used to create synthetic imagesfor testing and validation of image processing algorithms in the presence ofrealistic images noise. An example of the simulated CMOS photosensor and acomparison with a custom-made CMOS hardware sensor is presented. Procedures forcharacterisation from both light and dark noises are described. Experimentalresults that confirm the validity of the numerical model are provided. Thepaper addresses the issue of the lack of comprehensive high-level photosensormodels that enable engineers to simulate realistic effects of noise on theimages obtained from solid-state photosensors.
arxiv-8400-172 | Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling | http://arxiv.org/pdf/1412.3555v1.pdf | author:Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio category:cs.NE cs.LG published:2014-12-11 summary:In this paper we compare different types of recurrent units in recurrentneural networks (RNNs). Especially, we focus on more sophisticated units thatimplement a gating mechanism, such as a long short-term memory (LSTM) unit anda recently proposed gated recurrent unit (GRU). We evaluate these recurrentunits on the tasks of polyphonic music modeling and speech signal modeling. Ourexperiments revealed that these advanced recurrent units are indeed better thanmore traditional recurrent units such as tanh units. Also, we found GRU to becomparable to LSTM.
arxiv-8400-173 | Reinforcement Learning and Nonparametric Detection of Game-Theoretic Equilibrium Play in Social Networks | http://arxiv.org/pdf/1501.01209v1.pdf | author:Omid Namvar Gharehshiran, William Hoiles, Vikram Krishnamurthy category:cs.GT cs.LG cs.SI stat.ML published:2014-12-11 summary:This paper studies two important signal processing aspects of equilibriumbehavior in non-cooperative games arising in social networks, namely,reinforcement learning and detection of equilibrium play. The first part of thepaper presents a reinforcement learning (adaptive filtering) algorithm thatfacilitates learning an equilibrium by resorting to diffusion cooperationstrategies in a social network. Agents form homophilic social groups, withinwhich they exchange past experiences over an undirected graph. It is shownthat, if all agents follow the proposed algorithm, their global behavior isattracted to the correlated equilibria set of the game. The second part of thepaper provides a test to detect if the actions of agents are consistent withplay from the equilibrium of a concave potential game. The theory of revealedpreference from microeconomics is used to construct a non-parametric decisiontest and statistical test which only require the probe and associated actionsof agents. A stochastic gradient algorithm is given to optimize the probe inreal time to minimize the Type-II error probabilities of the detection testsubject to specified Type-I error probability. We provide a real-world exampleusing the energy market, and a numerical example to detect malicious agents inan online social network.
arxiv-8400-174 | The ROMES method for statistical modeling of reduced-order-model error | http://arxiv.org/pdf/1405.5170v3.pdf | author:Martin Drohmann, Kevin Carlberg category:cs.NA math.NA stat.ML published:2014-05-20 summary:This work presents a technique for statistically modeling errors introducedby reduced-order models. The method employs Gaussian-process regression toconstruct a mapping from a small number of computationally inexpensive `errorindicators' to a distribution over the true error. The variance of thisdistribution can be interpreted as the (epistemic) uncertainty introduced bythe reduced-order model. To model normed errors, the method employs existingrigorous error bounds and residual norms as indicators; numerical experimentsshow that the method leads to a near-optimal expected effectivity in contrastto typical error bounds. To model errors in general outputs, the method usesdual-weighted residuals---which are amenable to uncertainty control---asindicators. Experiments illustrate that correcting the reduced-order-modeloutput with this surrogate can improve prediction accuracy by an order ofmagnitude; this contrasts with existing `multifidelity correction' approaches,which often fail for reduced-order models and suffer from the curse ofdimensionality. The proposed error surrogates also lead to a notion of`probabilistic rigor', i.e., the surrogate bounds the error with specifiedprobability.
arxiv-8400-175 | Deep Domain Confusion: Maximizing for Domain Invariance | http://arxiv.org/pdf/1412.3474v1.pdf | author:Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell category:cs.CV published:2014-12-10 summary:Recent reports suggest that a generic supervised deep CNN model trained on alarge-scale dataset reduces, but does not remove, dataset bias on a standardbenchmark. Fine-tuning deep models in a new domain can require a significantamount of data, which for many applications is simply not available. We proposea new CNN architecture which introduces an adaptation layer and an additionaldomain confusion loss, to learn a representation that is both semanticallymeaningful and domain invariant. We additionally show that a domain confusionmetric can be used for model selection to determine the dimension of anadaptation layer and the best position for the layer in the CNN architecture.Our proposed adaptation method offers empirical performance which exceedspreviously published results on a standard benchmark visual domain adaptationtask.
arxiv-8400-176 | Complex support vector machines regression for robust channel estimation in LTE downlink system | http://arxiv.org/pdf/1412.8109v1.pdf | author:Anis Charrada, Abdelaziz Samet category:cs.IT cs.LG math.IT published:2014-12-10 summary:In this paper, the problem of channel estimation for LTE Downlink system inthe environment of high mobility presenting non-Gaussian impulse noiseinterfering with reference signals is faced. The estimation of the frequencyselective time varying multipath fading channel is performed by using a channelestimator based on a nonlinear complex Support Vector Machine Regression (SVR)which is applied to Long Term Evolution (LTE) downlink. The estimationalgorithm makes use of the pilot signals to estimate the total frequencyresponse of the highly selective fading multipath channel. Thus, the algorithmmaps trained data into a high dimensional feature space and uses the structuralrisk minimization principle to carry out the regression estimation for thefrequency response function of the fading channel. The obtained results showthe effectiveness of the proposed method which has better performance than theconventional Least Squares (LS) and Decision Feedback methods to track thevariations of the fading multipath channel.
arxiv-8400-177 | GP-select: Accelerating EM using adaptive subspace preselection | http://arxiv.org/pdf/1412.3411v1.pdf | author:Jacquelyn A. Shelton, Jan Gasthaus, Zhenwen Dai, Joerg Luecke, Arthur Gretton category:stat.ML cs.LG published:2014-12-10 summary:We propose a nonparametric procedure to achieve fast inference in generativegraphical models when the number of latent states is very large. The approachis based on iterative latent variable preselection, where we alternate betweenlearning a 'selection function' to reveal the relevant latent variables, anduse this to obtain a compact approximation of the posterior distribution forEM; this can make inference possible where the number of possible latent statesis e.g. exponential in the number of latent variables, whereas an exactapproach would be computationally unfeasible. We learn the selection functionentirely from the observed data and current EM state via Gaussian processregression: this is by contrast with earlier approaches, where selections werehand-designed for each problem setting. We show our approach to perform as wellas these bespoke selection functions on a wide variety of inference problems:in particular, for the challenging case of a hierarchical model for objectlocalization with occlusion, we achieve results that match a customizedstate-of-the-art selection method, at a far lower computational cost.
arxiv-8400-178 | Object Recognition Using Deep Neural Networks: A Survey | http://arxiv.org/pdf/1412.3684v1.pdf | author:Soren Goyal, Paul Benjamin category:cs.CV cs.LG cs.NE published:2014-12-10 summary:Recognition of objects using Deep Neural Networks is an active area ofresearch and many breakthroughs have been made in the last few years. The paperattempts to indicate how far this field has progressed. The paper brieflydescribes the history of research in Neural Networks and describe several ofthe recent advances in this field. The performances of recently developedNeural Network Algorithm over benchmark datasets have been tabulated. Finally,some the applications of this field have been provided.
arxiv-8400-179 | Inexact Coordinate Descent: Complexity and Preconditioning | http://arxiv.org/pdf/1304.5530v2.pdf | author:Rachael Tappenden, Peter Richtárik, Jacek Gondzio category:math.OC cs.AI stat.ML published:2013-04-19 summary:In this paper we consider the problem of minimizing a convex function using arandomized block coordinate descent method. One of the key steps at eachiteration of the algorithm is determining the update to a block of variables.Existing algorithms assume that in order to compute the update, a particularsubproblem is solved exactly. In his work we relax this requirement, and allowfor the subproblem to be solved inexactly, leading to an inexact blockcoordinate descent method. Our approach incorporates the best known results forexact updates as a special case. Moreover, these theoretical guarantees arecomplemented by practical considerations: the use of iterative techniques todetermine the update as well as the use of preconditioning for furtheracceleration.
arxiv-8400-180 | Candidate Constrained CRFs for Loss-Aware Structured Prediction | http://arxiv.org/pdf/1412.3369v1.pdf | author:Faruk Ahmed, Daniel Tarlow, Dhruv Batra category:cs.CV published:2014-12-10 summary:When evaluating computer vision systems, we are often concerned withperformance on a task-specific evaluation measure such as theIntersection-Over-Union score used in the PASCAL VOC image segmentationchallenge. Ideally, our systems would be tuned specifically to these evaluationmeasures. However, despite much work on loss-aware structured prediction, topperforming systems do not use these techniques. In this work, we seek toaddress this problem, incorporating loss-aware prediction in a manner that isamenable to the approaches taken by top performing systems. Our main idea is tosimultaneously leverage two systems: a highly tuned pipeline system as is foundon top of leaderboards, and a traditional CRF. We show how to combine highquality candidate solutions from the pipeline with the probabilistic approachof the CRF that is amenable to loss-aware prediction. The result is that we canuse loss-aware prediction methodology to improve performance of the highlytuned pipeline system.
arxiv-8400-181 | Neural Turing Machines | http://arxiv.org/pdf/1410.5401v2.pdf | author:Alex Graves, Greg Wayne, Ivo Danihelka category:cs.NE published:2014-10-20 summary:We extend the capabilities of neural networks by coupling them to externalmemory resources, which they can interact with by attentional processes. Thecombined system is analogous to a Turing Machine or Von Neumann architecturebut is differentiable end-to-end, allowing it to be efficiently trained withgradient descent. Preliminary results demonstrate that Neural Turing Machinescan infer simple algorithms such as copying, sorting, and associative recallfrom input and output examples.
arxiv-8400-182 | Deep Multi-Instance Transfer Learning | http://arxiv.org/pdf/1411.3128v2.pdf | author:Dimitrios Kotzias, Misha Denil, Phil Blunsom, Nando de Freitas category:cs.LG stat.ML published:2014-11-12 summary:We present a new approach for transferring knowledge from groups toindividuals that comprise them. We evaluate our method in text, by inferringthe ratings of individual sentences using full-review ratings. This approach,which combines ideas from transfer learning, deep learning and multi-instancelearning, reduces the need for laborious human labelling of fine-grained datawhen abundant labels are available at the group level.
arxiv-8400-183 | Statistical Patterns in Written Language | http://arxiv.org/pdf/1412.3336v1.pdf | author:Damián H. Zanette category:cs.CL published:2014-12-10 summary:Quantitative linguistics has been allowed, in the last few decades, withinthe admittedly blurry boundaries of the field of complex systems. A growinghost of applied mathematicians and statistical physicists devote their effortsto disclose regularities, correlations, patterns, and structural properties oflanguage streams, using techniques borrowed from statistics and informationtheory. Overall, results can still be categorized as modest, but the prospectsare promising: medium- and long-range features in the organization of humanlanguage -which are beyond the scope of traditional linguistics- have alreadyemerged from this kind of analysis and continue to be reported, contributing anew perspective to our understanding of this most complex communication system.This short book is intended to review some of these recent contributions.
arxiv-8400-184 | Convergence and rate of convergence of some greedy algorithms in convex optimization | http://arxiv.org/pdf/1412.3297v1.pdf | author:Vladimir Temlyakov category:stat.ML math.NA published:2014-12-10 summary:The paper gives a systematic study of the approximate versions of threegreedy-type algorithms that are widely used in convex optimization. Byapproximate version we mean the one where some of evaluations are made with anerror. Importance of such versions of greedy-type algorithms in convexoptimization and in approximation theory was emphasized in previous literature.
arxiv-8400-185 | Generalised Entropy MDPs and Minimax Regret | http://arxiv.org/pdf/1412.3276v1.pdf | author:Emmanouil G. Androulakis, Christos Dimitrakakis category:cs.LG stat.ML published:2014-12-10 summary:Bayesian methods suffer from the problem of how to specify prior beliefs. Oneinteresting idea is to consider worst-case priors. This requires solving astochastic zero-sum game. In this paper, we extend well-known results frombandit theory in order to discover minimax-Bayes policies and discuss when theyare practical.
arxiv-8400-186 | Object-centric Sampling for Fine-grained Image Classification | http://arxiv.org/pdf/1412.3161v1.pdf | author:Xiaoyu Wang, Tianbao Yang, Guobin Chen, Yuanqing Lin category:cs.CV published:2014-12-10 summary:This paper proposes to go beyond the state-of-the-art deep convolutionalneural network (CNN) by incorporating the information from object detection,focusing on dealing with fine-grained image classification. Unfortunately, CNNsuffers from over-fiting when it is trained on existing fine-grained imageclassification benchmarks, which typically only consist of less than a few tensof thousands training images. Therefore, we first construct a large-scalefine-grained car recognition dataset that consists of 333 car classes with morethan 150 thousand training images. With this large-scale dataset, we are ableto build a strong baseline for CNN with top-1 classification accuracy of 81.6%.One major challenge in fine-grained image classification is that many classesare very similar to each other while having large within-class variation. Onecontributing factor to the within-class variation is cluttered imagebackground. However, the existing CNN training takes uniform window samplingover the image, acting as blind on the location of the object of interest. Incontrast, this paper proposes an \emph{object-centric sampling} (OCS) schemethat samples image windows based on the object location information. Thechallenge in using the location information lies in how to design powerfulobject detector and how to handle the imperfectness of detection results. Tothat end, we design a saliency-aware object detection approach specific for thesetting of fine-grained image classification, and the uncertainty of detectionresults are naturally handled in our OCS scheme. Our framework is demonstratedto be very effective, improving top-1 accuracy to 89.3% (from 81.6%) on thelarge-scale fine-grained car classification dataset.
arxiv-8400-187 | Road Detection via On--line Label Transfer | http://arxiv.org/pdf/1412.3159v1.pdf | author:José M. Álvarez, Ferran Diego, Joan Serrat, Antonio M. López category:cs.CV published:2014-12-10 summary:Vision-based road detection is an essential functionality for supportingadvanced driver assistance systems (ADAS) such as road following and vehicleand pedestrian detection. The major challenges of road detection are dealingwith shadows and lighting variations and the presence of other objects in thescene. Current road detection algorithms characterize road areas at pixel leveland group pixels accordingly. However, these algorithms fail in presence ofstrong shadows and lighting variations. Therefore, we propose a road detectionalgorithm based on video alignment. The key idea of the algorithm is to exploitthe similarities occurred when a vehicle follows the same trajectory more thanonce. In this way, road areas are learned in a first ride and then, this roadknowledge is used to infer areas depicting drivable road surfaces in subsequentrides. Two different experiments are conducted to validate the proposal ondifferent video sequences taken at different scenarios and different daytime.The former aims to perform on-line road detection. The latter aims to performoff-line road detection and is applied to automatically generate theground-truth necessary to validate road detection algorithms. Qualitative andquantitative evaluations prove that the proposed algorithm is a valid roaddetection approach.
arxiv-8400-188 | Semi-Supervised Learning with Heterophily | http://arxiv.org/pdf/1412.3100v1.pdf | author:Wolfgang Gatterbauer category:cs.LG cs.DB published:2014-12-09 summary:We propose a novel linear semi-supervised learning formulation that isderived from a solid probabilistic framework: belief propagation. We show thatour formulation generalizes a number of label propagation algorithms describedin the literature by allowing them to propagate generalized assumptions aboutinfluences between classes of neighboring nodes. We call this formulationSemi-Supervised Learning with Heterophily (SSL-H). We also show how theaffinity matrix can be learned from observed data with a simple convexoptimization framework that is inspired by locally linear embedding. We callthis approach Linear Heterophily Estimation (LHE). Experiments on syntheticdata show that both approaches combined can learn heterophily of a graph with1M nodes, 10M edges and few labels in under 1min, and give better labelingaccuracies than a baseline method in the case of small fraction of explicitlylabeled nodes.
arxiv-8400-189 | Hierarchical Mixture-of-Experts Model for Large-Scale Gaussian Process Regression | http://arxiv.org/pdf/1412.3078v1.pdf | author:Jun Wei Ng, Marc Peter Deisenroth category:stat.ML cs.AI cs.LG stat.CO published:2014-12-09 summary:We propose a practical and scalable Gaussian process model for large-scalenonlinear probabilistic regression. Our mixture-of-experts model isconceptually simple and hierarchically recombines computations for an overallapproximation of a full Gaussian process. Closed-form and distributedcomputations allow for efficient and massive parallelisation while keeping thememory consumption small. Given sufficient computing resources, our model canhandle arbitrarily large data sets, without explicit sparse approximations. Weprovide strong experimental evidence that our model can be applied to largedata sets of sizes far beyond millions. Hence, our model has the potential tolay the foundation for general large-scale Gaussian process research.
arxiv-8400-190 | POPE: Post Optimization Posterior Evaluation of Likelihood Free Models | http://arxiv.org/pdf/1412.3051v1.pdf | author:Edward Meeds, Michael Chiang, Mary Lee, Olivier Cinquin, John Lowengrub, Max Welling category:stat.ML q-bio.QM published:2014-12-09 summary:In many domains, scientists build complex simulators of natural phenomenathat encode their hypotheses about the underlying processes. These simulatorscan be deterministic or stochastic, fast or slow, constrained or unconstrained,and so on. Optimizing the simulators with respect to a set of parameter valuesis common practice, resulting in a single parameter setting that minimizes anobjective subject to constraints. We propose a post optimization posterioranalysis that computes and visualizes all the models that can generate equallygood or better simulation results, subject to constraints. These optimizationposteriors are desirable for a number of reasons among which easyinterpretability, automatic parameter sensitivity and correlation analysis andposterior predictive analysis. We develop a new sampling framework based onapproximate Bayesian computation (ABC) with one-sided kernels. In collaborationwith two groups of scientists we applied POPE to two important biologicalsimulators: a fast and stochastic simulator of stem-cell cycling and a slow anddeterministic simulator of tumor growth patterns.
arxiv-8400-191 | A Unified Semantic Embedding: Relating Taxonomies and Attributes | http://arxiv.org/pdf/1411.5879v2.pdf | author:Sung Ju Hwang, Leonid Sigal category:cs.CV published:2014-11-18 summary:We propose a method that learns a discriminative yet semantic space forobject categorization, where we also embed auxiliary semantic entities such assupercategories and attributes. Contrary to prior work which only utilized themas side information, we explicitly embed the semantic entities into the samespace where we embed categories, which enables us to represent a category astheir linear combination. By exploiting such a unified model for semantics, weenforce each category to be represented by a supercategory + sparse combinationof attributes, with an additional exclusive regularization to learndiscriminative composition.
arxiv-8400-192 | Brain Tumor Detection Based on Bilateral Symmetry Information | http://arxiv.org/pdf/1412.3009v1.pdf | author:Narkhede Sachin, Deven Shah, Vaishali Khairnar, Sujata Kadu category:cs.CV published:2014-12-09 summary:Advances in computing technology have allowed researchers across many fieldsof endeavor to collect and maintain vast amounts of observational statisticaldata such as clinical data,biological patient data,data regarding access of websites,financial data,and the like.Brain Magnetic ResonanceImaging(MRI)segmentation is a complex problem in the field of medical imagingdespite various presented methods.MR image of human brain can be divided intoseveral sub regions especially soft tissues such as gray matter,white matterand cerebrospinal fluid.Although edge information is the main clue in imagesegmentation,it can not get a better result in analysis the content of imageswithout combining other information.The segmentation of brain tissue in themagnetic resonance imaging(MRI)is very important for detecting the existenceand outlines of tumors.In this paper,an algorithm about segmentation based onthe symmetry character of brain MRI image is presented.Our goal is to detectthe position and boundary of tumors automatically.Experiments were conducted onreal pictures,and the results show that the algorithm is flexible andconvenient.
arxiv-8400-193 | An Approach for Reducing Outliers of Non Local Means Image Denoising Filter | http://arxiv.org/pdf/1412.2444v2.pdf | author:Raka Kundu, Amlan Chakrabarti, Prasanna Lenka category:cs.CV published:2014-12-08 summary:We propose an adaptive approach for non local means (NLM) image filteringtermed as non local adaptive clipped means (NLACM), which reduces the effect ofoutliers and improves the denoising quality as compared to traditional NLM.Common method to neglect outliers from a data population is computation of meanin a range defined by mean and standard deviation. In NLACM we perform themedian within the defined range based on statistical estimation of theneighbourhood region of a pixel to be denoised. As parameters of the range areindependent of any additional input and is based on local intensity values,hence the approach is adaptive. Experimental results for NLACM show betterestimation of true intensity from noisy neighbourhood observation as comparedto NLM at high noise levels. We have verified the technique for speckle noisereduction and we have tested it on ultrasound (US) image of lumbar spine. Theseultrasound images act as guidance for injection therapy for treatment of lumbarradiculopathy. We believe that the proposed approach for image denoising isfirst of its kind and its efficiency can be well justified as it shows betterperformance in image restoration.
arxiv-8400-194 | Sparsity and adaptivity for the blind separation of partially correlated sources | http://arxiv.org/pdf/1412.4005v1.pdf | author:Jerome Bobin, Jeremy Rapin, Anthony Larue, Jean-Luc Starck category:stat.AP cs.LG stat.ML published:2014-12-09 summary:Blind source separation (BSS) is a very popular technique to analyzemultichannel data. In this context, the data are modeled as the linearcombination of sources to be retrieved. For that purpose, standard BSS methodsall rely on some discrimination principle, whether it is statisticalindependence or morphological diversity, to distinguish between the sources.However, dealing with real-world data reveals that such assumptions are rarelyvalid in practice: the signals of interest are more likely partiallycorrelated, which generally hampers the performances of standard BSS methods.In this article, we introduce a novel sparsity-enforcing BSS method coinedAdaptive Morphological Component Analysis (AMCA), which is designed to retrievesparse and partially correlated sources. More precisely, it makes profit of anadaptive re-weighting scheme to favor/penalize samples based on their level ofcorrelation. Extensive numerical experiments have been carried out which showthat the proposed method is robust to the partial correlation of sources whilestandard BSS techniques fail. The AMCA algorithm is evaluated in the field ofastrophysics for the separation of physical components from microwave data.
arxiv-8400-195 | Bayesian Fisher's Discriminant for Functional Data | http://arxiv.org/pdf/1412.2929v1.pdf | author:Yao-Hsiang Yang, Lu-Hung Chen, Chieh-Chih Wang, Chu-Song Chen category:cs.LG stat.ML published:2014-12-09 summary:We propose a Bayesian framework of Gaussian process in order to extendFisher's discriminant to classify functional data such as spectra and images.The probability structure for our extended Fisher's discriminant is explicitlyformulated, and we utilize the smoothness assumptions of functional data asprior probabilities. Existing methods which directly employ the smoothnessassumption of functional data can be shown as special cases within thisframework given corresponding priors while their estimates of the unknowns areone-step approximations to the proposed MAP estimates. Empirical results onvarious simulation studies and different real applications show that theproposed method significantly outperforms the other Fisher's discriminantmethods for functional data.
arxiv-8400-196 | Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition | http://arxiv.org/pdf/1406.2227v4.pdf | author:Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2014-06-09 summary:In this work we present a framework for the recognition of natural scenetext. Our framework does not require any human-labelled data, and performs wordrecognition on the whole image holistically, departing from the character basedrecognition systems of the past. The deep neural network models at the centreof this framework are trained solely on data produced by a synthetic textgeneration engine -- synthetic data that is highly realistic and sufficient toreplace real data, giving us infinite amounts of training data. This excess ofdata exposes new possibilities for word recognition models, and here weconsider three models, each one "reading" words in a different way: via 90k-waydictionary encoding, character sequence encoding, and bag-of-N-grams encoding.In the scenarios of language based and completely unconstrained textrecognition we greatly improve upon state-of-the-art performance on standarddatasets, using our fast, simple machinery and requiring zero data-acquisitioncosts.
arxiv-8400-197 | ROP: Matrix recovery via rank-one projections | http://arxiv.org/pdf/1310.5791v3.pdf | author:T. Tony Cai, Anru Zhang category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH published:2013-10-22 summary:Estimation of low-rank matrices is of significant interest in a range ofcontemporary applications. In this paper, we introduce a rank-one projectionmodel for low-rank matrix recovery and propose a constrained nuclear normminimization method for stable recovery of low-rank matrices in the noisy case.The procedure is adaptive to the rank and robust against small perturbations.Both upper and lower bounds for the estimation accuracy under the Frobeniusnorm loss are obtained. The proposed estimator is shown to be rate-optimalunder certain conditions. The estimator is easy to implement via convexprogramming and performs well numerically. The techniques and main resultsdeveloped in the paper also have implications to other related statisticalproblems. An application to estimation of spiked covariance matrices fromone-dimensional random projections is considered. The results demonstrate thatit is still possible to accurately estimate the covariance matrix of ahigh-dimensional distribution based only on one-dimensional projections.
arxiv-8400-198 | Cancer Detection with Multiple Radiologists via Soft Multiple Instance Logistic Regression and $L_1$ Regularization | http://arxiv.org/pdf/1412.2873v1.pdf | author:Inna Stainvas, Alexandra Manevitch, Isaac Leichter category:cs.CV published:2014-12-09 summary:This paper deals with the multiple annotation problem in medical applicationof cancer detection in digital images. The main assumption is that thoughimages are labeled by many experts, the number of images read by the sameexpert is not large. Thus differing with the existing work on modeling eachexpert and ground truth simultaneously, the multi annotation information isused in a soft manner. The multiple labels from different experts are used toestimate the probability of the findings to be marked as malignant. Thelearning algorithm minimizes the Kullback Leibler (KL) divergence between themodeled probabilities and desired ones constraining the model to be compact.The probabilities are modeled by logit regression and multiple instancelearning concept is used by us. Experiments on a real-life computer aided diagnosis (CAD) problem for CXR CADlung cancer detection demonstrate that the proposed algorithm leads to similarresults as learning with a binary RVMMIL classifier or a mixture of binaryRVMMIL models per annotator. However, this model achieves a smaller complexityand is more preferable in practice.
arxiv-8400-199 | Stochastic Coordinate Coding and Its Application for Drosophila Gene Expression Pattern Annotation | http://arxiv.org/pdf/1407.8147v2.pdf | author:Binbin Lin, Qingyang Li, Qian Sun, Ming-Jun Lai, Ian Davidson, Wei Fan, Jieping Ye category:cs.LG cs.CE published:2014-07-30 summary:\textit{Drosophila melanogaster} has been established as a model organism forinvestigating the fundamental principles of developmental gene interactions.The gene expression patterns of \textit{Drosophila melanogaster} can bedocumented as digital images, which are annotated with anatomical ontologyterms to facilitate pattern discovery and comparison. The automated annotationof gene expression pattern images has received increasing attention due to therecent expansion of the image database. The effectiveness of gene expressionpattern annotation relies on the quality of feature representation. Previousstudies have demonstrated that sparse coding is effective for extractingfeatures from gene expression images. However, solving sparse coding remains acomputationally challenging problem, especially when dealing with large-scaledata sets and learning large size dictionaries. In this paper, we propose anovel algorithm to solve the sparse coding problem, called StochasticCoordinate Coding (SCC). The proposed algorithm alternatively updates thesparse codes via just a few steps of coordinate descent and updates thedictionary via second order stochastic gradient descent. The computational costis further reduced by focusing on the non-zero components of the sparse codesand the corresponding columns of the dictionary only in the updating procedure.Thus, the proposed algorithm significantly improves the efficiency and thescalability, making sparse coding applicable for large-scale data sets andlarge dictionary sizes. Our experiments on Drosophila gene expression data setsdemonstrate the efficiency and the effectiveness of the proposed algorithm.
arxiv-8400-200 | Learning Multi-target Tracking with Quadratic Object Interactions | http://arxiv.org/pdf/1412.2066v2.pdf | author:Shaofei Wang, Charless C. Fowlkes category:cs.CV cs.LG published:2014-12-05 summary:We describe a model for multi-target tracking based on associatingcollections of candidate detections across frames of a video. In order to modelpairwise interactions between different tracks, such as suppression ofoverlapping tracks and contextual cues about co-occurence of different objects,we augment a standard min-cost flow objective with quadratic terms betweendetection variables. We learn the parameters of this model using structuredprediction and a loss function which approximates the multi-target trackingaccuracy. We evaluate two different approaches to finding an optimal set oftracks under model objective based on an LP relaxation and a novel greedyextension to dynamic programming that handles pairwise interactions. We findthe greedy algorithm achieves equivalent performance to the LP relaxation whilebeing 2-7x faster than a commercial solver. The resulting model with learnedparameters outperforms existing methods across several categories on the KITTItracking benchmark.
arxiv-8400-201 | Circumventing the Curse of Dimensionality in Prediction: Causal Rate-Distortion for Infinite-Order Markov Processes | http://arxiv.org/pdf/1412.2859v1.pdf | author:Sarah Marzen, James P. Crutchfield category:cs.LG nlin.CD q-bio.NC stat.ML published:2014-12-09 summary:Predictive rate-distortion analysis suffers from the curse of dimensionality:clustering arbitrarily long pasts to retain information about arbitrarily longfutures requires resources that typically grow exponentially with length. Thechallenge is compounded for infinite-order Markov processes, since conditioningon finite sequences cannot capture all of their past dependencies. Spectralarguments show that algorithms which cluster finite-length sequences faildramatically when the underlying process has long-range temporal correlationsand can fail even for processes generated by finite-memory hidden Markovmodels. We circumvent the curse of dimensionality in rate-distortion analysisof infinite-order processes by casting predictive rate-distortion objectivefunctions in terms of the forward- and reverse-time causal states ofcomputational mechanics. Examples demonstrate that the resulting causalrate-distortion theory substantially improves current predictiverate-distortion analyses.
arxiv-8400-202 | Fast Learning of Relational Dependency Networks | http://arxiv.org/pdf/1410.7835v2.pdf | author:Oliver Schulte, Zhensong Qian, Arthur E. Kirkpatrick, Xiaoqian Yin, Yan Sun category:cs.LG published:2014-10-28 summary:A Relational Dependency Network (RDN) is a directed graphical model widelyused for multi-relational data. These networks allow cyclic dependencies,necessary to represent relational autocorrelations. We describe an approach forlearning both the RDN's structure and its parameters, given an input relationaldatabase: First learn a Bayesian network (BN), then transform the Bayesiannetwork to an RDN. Thus fast Bayes net learning can provide fast RDN learning.The BN-to-RDN transform comprises a simple, local adjustment of the Bayes netstructure and a closed-form transform of the Bayes net parameters. This methodcan learn an RDN for a dataset with a million tuples in minutes. We empiricallycompare our approach to state-of-the art RDN learning methods that usefunctional gradient boosting, on five benchmark datasets. Learning RDNs via BNsscales much better to large datasets than learning RDNs with boosting, andprovides competitive accuracy in predictions.
arxiv-8400-203 | Zipf's Law and the Frequency of Characters or Words of Oracles | http://arxiv.org/pdf/1412.2821v1.pdf | author:Xiuli Wang category:cs.CL math.ST stat.TH published:2014-12-09 summary:The article discusses the frequency of characters of Oracle,concluding thatthe frequency and the rank of a word or character is fit to Zipf-Mandelboit Lawor Zipf's law with three parameters,and figuring out the parameters based onthe frequency,and pointing out that what some researchers of Oracle call theassembling on the two ends is just a description by their impression about theOracle data.
arxiv-8400-204 | What is a salient object? A dataset and a baseline model for salient object detection | http://arxiv.org/pdf/1412.5027v1.pdf | author:Ali Borji category:cs.CV published:2014-12-08 summary:Salient object detection or salient region detection models, diverging fromfixation prediction models, have traditionally been dealing with locating andsegmenting the most salient object or region in a scene. While the notion ofmost salient object is sensible when multiple objects exist in a scene, currentdatasets for evaluation of saliency detection approaches often have scenes withonly one single object. We introduce three main contributions in this paper:First, we take an indepth look at the problem of salient object detection bystudying the relationship between where people look in scenes and what theychoose as the most salient object when they are explicitly asked. Based on theagreement between fixations and saliency judgments, we then suggest that themost salient object is the one that attracts the highest fraction of fixations.Second, we provide two new less biased benchmark datasets containing sceneswith multiple objects that challenge existing saliency models. Indeed, weobserved a severe drop in performance of 8 state-of-the-art models on ourdatasets (40% to 70%). Third, we propose a very simple yet powerful model basedon superpixels to be used as a baseline for model evaluation and comparison.While on par with the best models on MSRA-5K dataset, our model wins over othermodels on our data highlighting a serious drawback of existing models, which isconvoluting the processes of locating the most salient object and itssegmentation. We also provide a review and statistical analysis of some labeledscene datasets that can be used for evaluating salient object detection models.We believe that our work can greatly help remedy the over-fitting of models toexisting biased datasets and opens new venues for future research in thisfast-evolving field.
arxiv-8400-205 | Unsupervised Induction of Semantic Roles within a Reconstruction-Error Minimization Framework | http://arxiv.org/pdf/1412.2812v1.pdf | author:Ivan Titov, Ehsan Khoddam category:cs.CL cs.AI cs.LG stat.ML published:2014-12-08 summary:We introduce a new approach to unsupervised estimation of feature-richsemantic role labeling models. Our model consists of two components: (1) anencoding component: a semantic role labeling model which predicts roles given arich set of syntactic and lexical features; (2) a reconstruction component: atensor factorization model which relies on roles to predict argument fillers.When the components are estimated jointly to minimize errors in argumentreconstruction, the induced roles largely correspond to roles defined inannotated resources. Our method performs on par with most accurate roleinduction methods on English and German, even though, unlike these previousapproaches, we do not incorporate any prior linguistic knowledge about thelanguages.
arxiv-8400-206 | Covariance Matrices for Mean Field Variational Bayes | http://arxiv.org/pdf/1410.6853v2.pdf | author:Ryan Giordano, Tamara Broderick category:stat.ML cs.LG stat.ME published:2014-10-24 summary:Mean Field Variational Bayes (MFVB) is a popular posterior approximationmethod due to its fast runtime on large-scale data sets. However, it is wellknown that a major failing of MFVB is its (sometimes severe) underestimates ofthe uncertainty of model variables and lack of information about model variablecovariance. We develop a fast, general methodology for exponential familiesthat augments MFVB to deliver accurate uncertainty estimates for modelvariables -- both for individual variables and coherently across variables.MFVB for exponential families defines a fixed-point equation in the means ofthe approximating posterior, and our approach yields a covariance estimate byperturbing this fixed point. Inspired by linear response theory, we call ourmethod linear response variational Bayes (LRVB). We demonstrate the accuracy ofour method on simulated data sets.
arxiv-8400-207 | First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs | http://arxiv.org/pdf/1408.2873v2.pdf | author:Awni Y. Hannun, Andrew L. Maas, Daniel Jurafsky, Andrew Y. Ng category:cs.CL cs.LG cs.NE published:2014-08-12 summary:We present a method to perform first-pass large vocabulary continuous speechrecognition using only a neural network and language model. Deep neural networkacoustic models are now commonplace in HMM-based speech recognition systems,but building such systems is a complex, domain-specific task. Recent workdemonstrated the feasibility of discarding the HMM sequence modeling frameworkby directly predicting transcript text from audio. This paper extends thisapproach in two ways. First, we demonstrate that a straightforward recurrentneural network architecture can achieve a high level of accuracy. Second, wepropose and evaluate a modified prefix-search decoding algorithm. This approachto decoding enables first-pass speech recognition with a language model,completely unaided by the cumbersome infrastructure of HMM-based systems.Experiments on the Wall Street Journal corpus demonstrate fairly competitiveword error rates, and the importance of bi-directional network recurrence.
arxiv-8400-208 | Image quality assessment measure based on natural image statistics in the Tetrolet domain | http://arxiv.org/pdf/1412.2697v1.pdf | author:Abdelkaher Ait Abdelouahad, Mohammed El Hassouni, Hocine Cherifi, Driss Aboutajdine category:cs.CV published:2014-12-08 summary:This paper deals with a reduced reference (RR) image quality measure based onnatural image statistics modeling. For this purpose, Tetrolet transform is usedsince it provides a convenient way to capture local geometric structures. Thistransform is applied to both reference and distorted images. Then, GaussianScale Mixture (GSM) is proposed to model subbands in order to take accountstatistical dependencies between tetrolet coefficients. In order to quantifythe visual degradation, a measure based on Kullback Leibler Divergence (KLD) isprovided. The proposed measure was tested on the Cornell VCL A-57 dataset andcompared with other measures according to FR-TV1 VQEG framework.
arxiv-8400-209 | A New Approach of Learning Hierarchy Construction Based on Fuzzy Logic | http://arxiv.org/pdf/1412.2689v1.pdf | author:Ali Aajli, Karim Afdel category:cs.CY cs.AI cs.LG published:2014-12-08 summary:In recent years, adaptive learning systems rely increasingly on learninghierarchy to customize the educational logic developed in their courses. Mostapproaches do not consider that the relationships of prerequisites between theskills are fuzzy relationships. In this article, we describe a new approach ofa practical application of fuzzy logic techniques to the construction oflearning hierarchies. For this, we use a learning hierarchy predefined by oneor more experts of a specific field. However, the relationships ofprerequisites between the skills in the learning hierarchy are not definitiveand they are fuzzy relationships. Indeed, we measure relevance degree of allrelationships existing in this learning hierarchy and we try to answer to thefollowing question: Is the relationships of prerequisites predefined in initiallearning hierarchy are correctly established or not?
arxiv-8400-210 | Real-Time System of Hand Detection And Gesture Recognition In Cyber Presence Interactive System For E-Learning | http://arxiv.org/pdf/1502.07243v1.pdf | author:Bousaaid Mourad, Ayaou Tarik, Afdel Karim, Estraillier Pascal category:cs.CV published:2014-12-08 summary:The development of technologies of multimedia, linked to that of Internet anddemocratization of high outflow, has made henceforth E-learning possible forlearners being in virtual classes and geographically distributed. The qualityand quantity of asynchronous and synchronous communications are the keyelements for E-learning success. It is important to have a propitioussupervision to reduce the feeling of isolation in E-learning. This feeling ofisolation is among the main causes of loss and high rates of stalling inE-learning. The researches to be conducted in this domain aim to bringsolutions of convergence coming from real time image for the capture andrecognition of hand gestures. These gestures will be analyzed by the system andtransformed as indicator of participation. This latter is displayed in thetable of performance of the tutor as a curve according to the time. In case ofisolation of learner, the indicator of participation will become red and thetutor will be informed of learners with difficulties to participate duringlearning session.
arxiv-8400-211 | When Computer Vision Gazes at Cognition | http://arxiv.org/pdf/1412.2672v1.pdf | author:Tao Gao, Daniel Harari, Joshua Tenenbaum, Shimon Ullman category:cs.AI cs.CV published:2014-12-08 summary:Joint attention is a core, early-developing form of social interaction. It isbased on our ability to discriminate the third party objects that other peopleare looking at. While it has been shown that people can accurately determinewhether another person is looking directly at them versus away, little is knownabout human ability to discriminate a third person gaze directed towardsobjects that are further away, especially in unconstraint cases where thelooker can move her head and eyes freely. In this paper we address thisquestion by jointly exploring human psychophysics and a cognitively motivatedcomputer vision model, which can detect the 3D direction of gaze from 2D faceimages. The synthesis of behavioral study and computer vision yields severalinteresting discoveries. (1) Human accuracy of discriminating targets8{\deg}-10{\deg} of visual angle apart is around 40% in a free looking gazetask; (2) The ability to interpret gaze of different lookers vary dramatically;(3) This variance can be captured by the computational model; (4) Humanoutperforms the current model significantly. These results collectively showthat the acuity of human joint attention is indeed highly impressive, given thecomputational challenge of the natural looking task. Moreover, the gap betweenhuman and model performance, as well as the variability of gaze interpretationacross different lookers, require further understanding of the underlyingmechanisms utilized by humans for this challenging task.
arxiv-8400-212 | Probabilistic low-rank matrix completion on finite alphabets | http://arxiv.org/pdf/1412.2632v1.pdf | author:Jean Lafond, Olga Klopp, Eric Moulines, Jospeh Salmon category:math.ST stat.ML stat.TH published:2014-12-08 summary:The task of reconstructing a matrix given a sample of observedentries isknown as the matrix completion problem. It arises ina wide range of problems,including recommender systems, collaborativefiltering, dimensionalityreduction, image processing, quantum physics or multi-class classificationtoname a few. Most works have focused on recovering an unknown real-valuedlow-rankmatrix from randomly sub-sampling its entries.Here, we investigate thecase where the observations take a finite number of values, corresponding forexamples to ratings in recommender systems or labels in multi-classclassification.We also consider a general sampling scheme (not necessarilyuniform) over the matrix entries.The performance of a nuclear-norm penalizedestimator is analyzed theoretically.More precisely, we derive bounds for theKullback-Leibler divergence between the true and estimated distributions.Inpractice, we have also proposed an efficient algorithm based on liftedcoordinate gradient descent in order to tacklepotentially high dimensionalsettings.
arxiv-8400-213 | Low Complexity Regularization of Linear Inverse Problems | http://arxiv.org/pdf/1407.1598v2.pdf | author:Samuel Vaiter, Gabriel Peyré, Jalal M. Fadili category:math.OC cs.IT math.IT stat.ML published:2014-07-07 summary:Inverse problems and regularization theory is a central theme in contemporarysignal processing, where the goal is to reconstruct an unknown signal frompartial indirect, and possibly noisy, measurements of it. A now standard methodfor recovering the unknown signal is to solve a convex optimization problemthat enforces some prior knowledge about its structure. This has provedefficient in many problems routinely encountered in imaging sciences,statistics and machine learning. This chapter delivers a review of recentadvances in the field where the regularization prior promotes solutionsconforming to some notion of simplicity/low-complexity. These priors encompassas popular examples sparsity and group sparsity (to capture the compressibilityof natural signals and images), total variation and analysis sparsity (topromote piecewise regularity), and low-rank (as natural extension of sparsityto matrix-valued data). Our aim is to provide a unified treatment of all theseregularizations under a single umbrella, namely the theory of partialsmoothness. This framework is very general and accommodates all low-complexityregularizers just mentioned, as well as many others. Partial smoothness turnsout to be the canonical way to encode low-dimensional models that can be linearspaces or more general smooth manifolds. This review is intended to serve as aone stop shop toward the understanding of the theoretical properties of theso-regularized solutions. It covers a large spectrum including: (i) recoveryguarantees and stability to noise, both in terms of $\ell^2$-stability andmodel (manifold) identification; (ii) sensitivity analysis to perturbations ofthe parameters involved (in particular the observations), with applications tounbiased risk estimation ; (iii) convergence properties of the forward-backwardproximal splitting scheme, that is particularly well suited to solve thecorresponding large-scale regularized optimization problem.
arxiv-8400-214 | Web image annotation by diffusion maps manifold learning algorithm | http://arxiv.org/pdf/1412.3352v1.pdf | author:Neda Pourali category:cs.CV cs.IR cs.LG 68T10 published:2014-12-08 summary:Automatic image annotation is one of the most challenging problems in machinevision areas. The goal of this task is to predict number of keywordsautomatically for images captured in real data. Many methods are based onvisual features in order to calculate similarities between image samples. Butthe computation cost of these approaches is very high. These methods requiremany training samples to be stored in memory. To lessen this burden, a numberof techniques have been developed to reduce the number of features in adataset. Manifold learning is a popular approach to nonlinear dimensionalityreduction. In this paper, we investigate Diffusion maps manifold learningmethod for web image auto-annotation task. Diffusion maps manifold learningmethod is used to reduce the dimension of some visual features. Extensiveexperiments and analysis on NUS-WIDE-LITE web image dataset with differentvisual features show how this manifold learning dimensionality reduction methodcan be applied effectively to image annotation.
arxiv-8400-215 | Optimization models of natural communication | http://arxiv.org/pdf/1412.2486v1.pdf | author:Ramon Ferrer-i-Cancho category:physics.soc-ph cs.CL published:2014-12-08 summary:A family of information theoretic models of communication was introduced morethan a decade ago to explain the origins of Zipf's law for word frequencies.The family is a based on a combination of two information theoretic principles:maximization of mutual information between forms and meanings and minimizationof form entropy. The family also sheds light on the origins of three otherpatterns: the principle of contrast, a related a vocabulary learning bias andthe meaning-frequency law. Here two important components of the family, namelythe information theoretic principles and the energy function that combines themlinearly, are reviewed from the perspective of psycholinguistics, languagelearning, information theory and synergetic linguistics. The minimization ofthis linear function resembles a sort of agnostic information theoretic modelselection that might be tuned by self-organization.
arxiv-8400-216 | Accurate Streaming Support Vector Machines | http://arxiv.org/pdf/1412.2485v1.pdf | author:Vikram Nathan, Sharath Raghvendra category:cs.LG published:2014-12-08 summary:A widely-used tool for binary classification is the Support Vector Machine(SVM), a supervised learning technique that finds the "maximum margin" linearseparator between the two classes. While SVMs have been well studied in thebatch (offline) setting, there is considerably less work on the streaming(online) setting, which requires only a single pass over the data usingsub-linear space. Existing streaming algorithms are not yet competitive withthe batch implementation. In this paper, we use the formulation of the SVM as aminimum enclosing ball (MEB) problem to provide a streaming SVM algorithm basedoff of the blurred ball cover originally proposed by Agarwal and Sharathkumar.Our implementation consistently outperforms existing streaming SVM approachesand provides higher accuracies than libSVM on several datasets, thus making itcompetitive with the standard SVM batch implementation.
arxiv-8400-217 | Double Ramp Loss Based Reject Option Classifier | http://arxiv.org/pdf/1311.6556v2.pdf | author:Naresh Manwani, Kalpit Desai, Sanand Sasidharan, Ramasubramanian Sundararajan category:cs.LG published:2013-11-26 summary:We consider the problem of learning reject option classifiers. The goodnessof a reject option classifier is quantified using $0-d-1$ loss function whereina loss $d \in (0,.5)$ is assigned for rejection. In this paper, we propose {\emdouble ramp loss} function which gives a continuous upper bound for $(0-d-1)$loss. Our approach is based on minimizing regularized risk under the doubleramp loss using {\em difference of convex (DC) programming}. We show theeffectiveness of our approach through experiments on synthetic and benchmarkdatasets. Our approach performs better than the state of the art reject optionclassification approaches.
arxiv-8400-218 | Weighted Polynomial Approximations: Limits for Learning and Pseudorandomness | http://arxiv.org/pdf/1412.2457v1.pdf | author:Mark Bun, Thomas Steinke category:cs.CC cs.LG published:2014-12-08 summary:Polynomial approximations to boolean functions have led to many positiveresults in computer science. In particular, polynomial approximations to thesign function underly algorithms for agnostically learning halfspaces, as wellas pseudorandom generators for halfspaces. In this work, we investigate thelimits of these techniques by proving inapproximability results for the signfunction. Firstly, the polynomial regression algorithm of Kalai et al. (SIAM J. Comput.2008) shows that halfspaces can be learned with respect to log-concavedistributions on $\mathbb{R}^n$ in the challenging agnostic learning model. Thepower of this algorithm relies on the fact that under log-concavedistributions, halfspaces can be approximated arbitrarily well by low-degreepolynomials. We ask whether this technique can be extended beyond log-concavedistributions, and establish a negative result. We show that polynomials of anydegree cannot approximate the sign function to within arbitrarily low error fora large class of non-log-concave distributions on the real line, includingthose with densities proportional to $\exp(-x^{0.99})$. Secondly, we investigate the derandomization of Chernoff-type concentrationinequalities. Chernoff-type tail bounds on sums of independent random variableshave pervasive applications in theoretical computer science. Schmidt et al.(SIAM J. Discrete Math. 1995) showed that these inequalities can be establishedfor sums of random variables with only $O(\log(1/\delta))$-wise independence,for a tail probability of $\delta$. We show that their results are tight up toconstant factors. These results rely on techniques from weighted approximation theory, whichstudies how well functions on the real line can be approximated by polynomialsunder various distributions. We believe that these techniques will have furtherapplications in other areas of computer science.
arxiv-8400-219 | Rediscovering the Alphabet - On the Innate Universal Grammar | http://arxiv.org/pdf/1412.2442v1.pdf | author:M. Yahia Kaadan, Asaad Kaadan category:cs.CL published:2014-12-08 summary:Universal Grammar (UG) theory has been one of the most important researchtopics in linguistics since introduced five decades ago. UG specifies therestricted set of languages learnable by human brain, and thus, manyresearchers believe in its biological roots. Numerous empirical studies ofneurobiological and cognitive functions of the human brain, and of many naturallanguages, have been conducted to unveil some aspects of UG. This, however,resulted in different and sometimes contradicting theories that do not indicatea universally unique grammar. In this research, we tackle the UG problem froman entirely different perspective. We search for the Unique Universal Grammar(UUG) that facilitates communication and knowledge transfer, the sole purposeof a language. We formulate this UG and show that it is unique, intrinsic, andcosmic, rather than humanistic. Initial analysis on a widespread naturallanguage already showed some positive results.
arxiv-8400-220 | Learning Word Representations from Relational Graphs | http://arxiv.org/pdf/1412.2378v1.pdf | author:Danushka Bollegala, Takanori Maehara, Yuichi Yoshida, Ken-ichi Kawarabayashi category:cs.CL published:2014-12-07 summary:Attributes of words and relations between two words are central to numeroustasks in Artificial Intelligence such as knowledge representation, similaritymeasurement, and analogy detection. Often when two words share one or moreattributes in common, they are connected by some semantic relations. On theother hand, if there are numerous semantic relations between two words, we canexpect some of the attributes of one of the words to be inherited by the other.Motivated by this close connection between attributes and relations, given arelational graph in which words are inter- connected via numerous semanticrelations, we propose a method to learn a latent representation for theindividual words. The proposed method considers not only the co-occurrences ofwords as done by existing approaches for word representation learning, but alsothe semantic relations in which two words co-occur. To evaluate the accuracy ofthe word representations learnt using the proposed method, we use the learntword representations to solve semantic word analogy problems. Our experimentalresults show that it is possible to learn better word representations by usingsemantic semantics between words.
arxiv-8400-221 | SimNets: A Generalization of Convolutional Networks | http://arxiv.org/pdf/1410.0781v3.pdf | author:Nadav Cohen, Amnon Shashua category:cs.NE cs.LG published:2014-10-03 summary:We present a deep layered architecture that generalizes classicalconvolutional neural networks (ConvNets). The architecture, called SimNets, isdriven by two operators, one being a similarity function whose family containsthe convolution operator used in ConvNets, and the other is a new softmax-min-mean operator called MEX that realizes classical operators like ReLUand max pooling, but has additional capabilities that make SimNets a powerfulgeneralization of ConvNets. Three interesting properties emerge from thearchitecture: (i) the basic input to hidden layer to output machinery containsas special cases kernel machines with the Exponential and Generalized Gaussiankernels, the output units being "neurons in feature space" (ii) in its generalform, the basic machinery has a higher abstraction level than kernel machines,and (iii) initializing networks using unsupervised learning is natural.Experiments demonstrate the capability of achieving state of the art accuracywith networks that are an order of magnitude smaller than comparable ConvNets.
arxiv-8400-222 | A Physically Inspired Clustering Algorithm: to Evolve Like Particles | http://arxiv.org/pdf/1412.5902v1.pdf | author:Teng Qiu, Kaifu Yang, Chaoyi Li, Yongjie Li category:cs.LG cs.CV published:2014-12-07 summary:Clustering analysis is a method to organize raw data into categories based ona measure of similarity. It has been successfully applied to diverse fieldsfrom science to business and engineering. By endowing data points with physicalmeaning like particles in the physical world and then leaning their evolvingtendency of moving from higher to lower potentials, data points in the proposedclustering algorithm sequentially hop to the locations of their transfer pointsand gather, after a few steps, at the locations of cluster centers with thelocally lowest potentials, where cluster members can be easily identified. Thewhole clustering process is simple and efficient, and can be performed eitherautomatically or interactively, with reliable performances on test data ofdiverse shapes, attributes, and dimensionalities.
arxiv-8400-223 | Bayesian Image Restoration for Poisson Corrupted Image using a Latent Variational Method with Gaussian MRF | http://arxiv.org/pdf/1412.2342v1.pdf | author:Hayaru Shouno category:cs.CV published:2014-12-07 summary:We treat an image restoration problem with a Poisson noise chan- nel using aBayesian framework. The Poisson randomness might be appeared in observation oflow contrast object in the field of imaging. The noise observation is oftenhard to treat in a theo- retical analysis. In our formulation, we interpret theobservation through the Poisson noise channel as a likelihood, and evaluate thebound of it with a Gaussian function using a latent variable method. We thenintroduce a Gaussian Markov random field (GMRF) as the prior for the Bayesianapproach, and derive the posterior as a Gaussian distribution. The latentparameters in the likelihood and the hyperparameter in the GMRF prior could betreated as hid- den parameters, so that, we propose an algorithm to infer themin the expectation maximization (EM) framework using loopy beliefpropagation(LBP). We confirm the ability of our algorithm in the computersimulation, and compare it with the results of other im- age restorationframeworks.
arxiv-8400-224 | Dictionary Learning over Distributed Models | http://arxiv.org/pdf/1402.1515v2.pdf | author:Jianshu Chen, Zaid J. Towfic, Ali H. Sayed category:cs.LG cs.DC published:2014-02-06 summary:In this paper, we consider learning dictionary models over a network ofagents, where each agent is only in charge of a portion of the dictionaryelements. This formulation is relevant in Big Data scenarios where largedictionary models may be spread over different spatial locations and it is notfeasible to aggregate all dictionaries in one location due to communication andprivacy considerations. We first show that the dual function of the inferenceproblem is an aggregation of individual cost functions associated withdifferent agents, which can then be minimized efficiently by means of diffusionstrategies. The collaborative inference step generates dual variables that areused by the agents to update their dictionaries without the need to share thesedictionaries or even the coefficient models for the training data. This is apowerful property that leads to an effective distributed procedure for learningdictionaries over large networks (e.g., hundreds of agents in our experiments).Furthermore, the proposed learning strategy operates in an online manner and isable to respond to streaming data, where each data sample is presented to thenetwork once.
arxiv-8400-225 | Iterative Bayesian Reconstruction of Non-IID Block-Sparse Signals | http://arxiv.org/pdf/1412.2316v1.pdf | author:Mehdi Korki, Jingxin Zhang, Cishen Zhang, Hadi Zayyani category:stat.ML cs.IT math.IT published:2014-12-07 summary:This paper presents a novel Block Iterative Bayesian Algorithm (Block-IBA)for reconstructing block-sparse signals with unknown block structures. Unlikethe existing algorithms for block sparse signal recovery which assume thecluster structure of the nonzero elements of the unknown signal to beindependent and identically distributed (i.i.d.), we use a more realisticBernoulli-Gaussian hidden Markov model (BGHMM) to characterize the non-i.i.d.block-sparse signals commonly encountered in practice. The Block-IBAiteratively estimates the amplitudes and positions of the block-sparse signalusing the steepest-ascent based Expectation-Maximization (EM), and optimallyselects the nonzero elements of the block-sparse signal by adaptivethresholding. The global convergence of Block-IBA is analyzed and proved, andthe effectiveness of Block-IBA is demonstrated by numerical experiments andsimulations on synthetic and real-life data.
arxiv-8400-226 | Selectable Factor Extraction in High Dimensions | http://arxiv.org/pdf/1403.6212v2.pdf | author:Yiyuan She category:stat.ME stat.ML published:2014-03-25 summary:This paper studies how to perform joint feature selection and extraction forboth supervised and unsupervised learning of high-dimensional data. We proposea novel selectable reduced rank regression (SEL-RRR) which can constructmultiple explanatory factors from a guaranteed small subset of input features.Sharp oracle inequalities are proved to reveal its power in predictivelearning. We develop a fast and simple-to-implement algorithm which provides acomputational framework for a wide family of sparsity-inducing penalties. Italso adapts to rank constrained variable screening in ultrahigh dimensions.Moreover, a predictive information criterion (PIC) is proposed for modelselection, with a theoretical guarantee of achieving the non-asymptotic optimalerror rate. Experiments demonstrate the efficacy and efficiency of simultaneousrank reduction and variable selection in various applications.
arxiv-8400-227 | Sparse Modeling for Image and Vision Processing | http://arxiv.org/pdf/1411.3230v2.pdf | author:Julien Mairal, Francis Bach, Jean Ponce category:cs.CV published:2014-11-12 summary:In recent years, a large amount of multi-disciplinary research has beenconducted on sparse models and their applications. In statistics and machinelearning, the sparsity principle is used to perform model selection---that is,automatically selecting a simple model among a large collection of them. Insignal processing, sparse coding consists of representing data with linearcombinations of a few dictionary elements. Subsequently, the correspondingtools have been widely adopted by several scientific communities such asneuroscience, bioinformatics, or computer vision. The goal of this monograph isto offer a self-contained view of sparse modeling for visual recognition andimage processing. More specifically, we focus on applications where thedictionary is learned and adapted to data, yielding a compact representationthat has been successful in various contexts.
arxiv-8400-228 | Smoothed Low Rank and Sparse Matrix Recovery by Iteratively Reweighted Least Squares Minimization | http://arxiv.org/pdf/1401.7413v2.pdf | author:Canyi Lu, Zhouchen Lin, Shuicheng Yan category:cs.LG cs.CV stat.ML published:2014-01-29 summary:This work presents a general framework for solving the low rank and/or sparsematrix minimization problems, which may involve multiple non-smooth terms. TheIteratively Reweighted Least Squares (IRLS) method is a fast solver, whichsmooths the objective function and minimizes it by alternately updating thevariables and their weights. However, the traditional IRLS can only solve asparse only or low rank only minimization problem with squared loss or anaffine constraint. This work generalizes IRLS to solve joint/mixed low rank andsparse minimization problems, which are essential formulations for many tasks.As a concrete example, we solve the Schatten-$p$ norm and $\ell_{2,q}$-normregularized Low-Rank Representation (LRR) problem by IRLS, and theoreticallyprove that the derived solution is a stationary point (globally optimal if$p,q\geq1$). Our convergence proof of IRLS is more general than previous onewhich depends on the special properties of the Schatten-$p$ norm and$\ell_{2,q}$-norm. Extensive experiments on both synthetic and real data setsdemonstrate that our IRLS is much more efficient.
arxiv-8400-229 | Generalized Singular Value Thresholding | http://arxiv.org/pdf/1412.2231v1.pdf | author:Canyi Lu, Changbo Zhu, Chunyan Xu, Shuicheng Yan, Zhouchen Lin category:cs.CV cs.LG cs.NA math.NA published:2014-12-06 summary:This work studies the Generalized Singular Value Thresholding (GSVT) operator${\Prox}_{g}^{\bm{\sigma}}(\cdot)$, \begin{equation*} {\Prox}_{g}^{\bm{\sigma}}(\B)=\arg\min\limits_{\X}\sum_{i=1}^{m}g(\sigma_{i}(\X))+ \frac{1}{2}\X-\B_{F}^{2}, \end{equation*} associated with a nonconvexfunction $g$ defined on the singular values of $\X$. We prove that GSVT can beobtained by performing the proximal operator of $g$ (denoted as$\Prox_g(\cdot)$) on the singular values since $\Prox_g(\cdot)$ is monotonewhen $g$ is lower bounded. If the nonconvex $g$ satisfies some conditions (manypopular nonconvex surrogate functions, e.g., $\ell_p$-norm, $0<p<1$, of$\ell_0$-norm are special cases), a general solver to find $\Prox_g(b)$ isproposed for any $b\geq0$. GSVT greatly generalizes the known Singular ValueThresholding (SVT) which is a basic subroutine in many convex low rankminimization methods. We are able to solve the nonconvex low rank minimizationproblem by using GSVT in place of SVT.
arxiv-8400-230 | Markov Chain Analysis of Evolution Strategies on a Linear Constraint Optimization Problem | http://arxiv.org/pdf/1404.3023v2.pdf | author:Alexandre Chotard, Anne Auger, Nikolaus Hansen category:cs.NE math.OC published:2014-04-11 summary:This paper analyses a $(1,\lambda)$-Evolution Strategy, a randomisedcomparison-based adaptive search algorithm, on a simple constraint optimisationproblem. The algorithm uses resampling to handle the constraint and optimizes alinear function with a linear constraint. Two cases are investigated: first thecase where the step-size is constant, and second the case where the step-sizeis adapted using path length control. We exhibit for each case a Markov chainwhose stability analysis would allow us to deduce the divergence of thealgorithm depending on its internal parameters. We show divergence at aconstant rate when the step-size is constant. We sketch that with step-sizeadaptation geometric divergence takes place. Our results complement previousstudies where stability was assumed.
arxiv-8400-231 | Relations among Some Low Rank Subspace Recovery Models | http://arxiv.org/pdf/1412.2196v1.pdf | author:Hongyang Zhang, Zhouchen Lin, Chao Zhang, Junbin Gao category:cs.LG math.OC published:2014-12-06 summary:Recovering intrinsic low dimensional subspaces from data distributed on themis a key preprocessing step to many applications. In recent years, there hasbeen a lot of work that models subspace recovery as low rank minimizationproblems. We find that some representative models, such as Robust PrincipalComponent Analysis (R-PCA), Robust Low Rank Representation (R-LRR), and RobustLatent Low Rank Representation (R-LatLRR), are actually deeply connected. Morespecifically, we discover that once a solution to one of the models isobtained, we can obtain the solutions to other models in closed-formformulations. Since R-PCA is the simplest, our discovery makes it the center oflow rank subspace recovery models. Our work has two important implications.First, R-PCA has a solid theoretical foundation. Under certain conditions, wecould find better solutions to these low rank models at overwhelmingprobabilities, although these models are non-convex. Second, we can obtainsignificantly faster algorithms for these models by solving R-PCA first. Thecomputation cost can be further cut by applying low complexity randomizedalgorithms, e.g., our novel $\ell_{2,1}$ filtering algorithm, to R-PCA.Experiments verify the advantages of our algorithms over other state-of-the-artones that are based on the alternating direction method.
arxiv-8400-232 | Using Artificial Neural Network Techniques for Prediction of Electric Energy Consumption | http://arxiv.org/pdf/1412.2186v1.pdf | author:Hasan M. H. Owda, Babatunji Omoniwa, Ahmad R. Shahid, Sheikh Ziauddin category:cs.NE cs.AI published:2014-12-06 summary:Due to imprecision and uncertainties in predicting real world problems,artificial neural network (ANN) techniques have become increasingly useful formodeling and optimization. This paper presents an artificial neural networkapproach for forecasting electric energy consumption. For effective planningand operation of power systems, optimal forecasting tools are needed for energyoperators to maximize profit and also to provide maximum satisfaction to energyconsumers. Monthly data for electric energy consumed in the Gaza strip wascollected from year 1994 to 2013. Data was trained and the proposed model wasvalidated using 2-Fold and K-Fold cross validation techniques. The model hasbeen tested with actual energy consumption data and yields satisfactoryperformance.
arxiv-8400-233 | Iterative Neural Autoregressive Distribution Estimator (NADE-k) | http://arxiv.org/pdf/1406.1485v3.pdf | author:Tapani Raiko, Li Yao, Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG published:2014-06-05 summary:Training of the neural autoregressive density estimator (NADE) can be viewedas doing one step of probabilistic inference on missing values in data. Wepropose a new model that extends this inference scheme to multiple steps,arguing that it is easier to learn to improve a reconstruction in $k$ stepsrather than to learn to reconstruct in a single inference step. The proposedmodel is an unsupervised building block for deep learning that combines thedesirable properties of NADE and multi-predictive training: (1) Its testlikelihood can be computed analytically, (2) it is easy to generate independentsamples from it, and (3) it uses an inference engine that is a superset ofvariational inference for Boltzmann machines. The proposed NADE-k iscompetitive with the state-of-the-art in density estimation on the two datasetstested.
arxiv-8400-234 | On the Information Theoretic Limits of Learning Ising Models | http://arxiv.org/pdf/1411.1434v2.pdf | author:Karthikeyan Shanmugam, Rashish Tandon, Alexandros G. Dimakis, Pradeep Ravikumar category:cs.LG published:2014-11-05 summary:We provide a general framework for computing lower-bounds on the samplecomplexity of recovering the underlying graphs of Ising models, given i.i.dsamples. While there have been recent results for specific graph classes, theseinvolve fairly extensive technical arguments that are specialized to eachspecific graph class. In contrast, we isolate two key graph-structuralingredients that can then be used to specify sample complexity lower-bounds.Presence of these structural properties makes the graph class hard to learn. Wederive corollaries of our main result that not only recover existing recentresults, but also provide lower bounds for novel graph classes not consideredpreviously. We also extend our framework to the random graph setting and derivecorollaries for Erd\H{o}s-R\'{e}nyi graphs in a certain dense setting.
arxiv-8400-235 | Consistent optimization of AMS by logistic loss minimization | http://arxiv.org/pdf/1412.2106v1.pdf | author:Wojciech Kotłowski category:cs.LG published:2014-12-05 summary:In this paper, we theoretically justify an approach popular amongparticipants of the Higgs Boson Machine Learning Challenge to optimizeapproximate median significance (AMS). The approach is based on the followingtwo-stage procedure. First, a real-valued function is learned by minimizing asurrogate loss for binary classification, such as logistic loss, on thetraining sample. Then, a threshold is tuned on a separate validation sample, bydirect optimization of AMS. We show that the regret of the resulting(thresholded) classifier measured with respect to the squared AMS, isupperbounded by the regret of the underlying real-valued function measured withrespect to the logistic loss. Hence, we prove that minimizing logisticsurrogate is a consistent method of optimizing AMS.
arxiv-8400-236 | "Mental Rotation" by Optimizing Transforming Distance | http://arxiv.org/pdf/1406.3010v2.pdf | author:Weiguang Ding, Graham W. Taylor category:cs.LG cs.CV published:2014-06-11 summary:The human visual system is able to recognize objects despite transformationsthat can drastically alter their appearance. To this end, much effort has beendevoted to the invariance properties of recognition systems. Invariance can beengineered (e.g. convolutional nets), or learned from data explicitly (e.g.temporal coherence) or implicitly (e.g. by data augmentation). One idea thathas not, to date, been explored is the integration of latent variables whichpermit a search over a learned space of transformations. Motivated by evidencethat people mentally simulate transformations in space while comparingexamples, so-called "mental rotation", we propose a transforming distance.Here, a trained relational model actively transforms pairs of examples so thatthey are maximally similar in some feature space yet respect the learnedtransformational constraints. We apply our method to nearest-neighbour problemson the Toronto Face Database and NORB.
arxiv-8400-237 | Multi-Target Shrinkage | http://arxiv.org/pdf/1412.2041v1.pdf | author:Daniel Bartz, Johannes Höhne, Klaus-Robert Müller category:stat.ME stat.ML published:2014-12-05 summary:Stein showed that the multivariate sample mean is outperformed by "shrinking"to a constant target vector. Ledoit and Wolf extended this approach to thesample covariance matrix and proposed a multiple of the identity as shrinkagetarget. In a general framework, independent of a specific estimator, we extendthe shrinkage concept by allowing simultaneous shrinkage to a set of targets.Application scenarios include settings with (A) additional data sets frompotentially similar distributions, (B) non-stationarity, (C) a natural groupingof the data or (D) multiple alternative estimators which could serve astargets. We show that this Multi-Target Shrinkage can be translated into a quadraticprogram and derive conditions under which the estimation of the shrinkageintensities yields optimal expected squared error in the limit. For the samplemean and the sample covariance as specific instances, we derive conditionsunder which the optimality of MTS is applicable. We consider two asymptoticsettings: the large dimensional limit (LDL), where the dimensionality and thenumber of observations go to infinity at the same rate, and the finiteobservations large dimensional limit (FOLDL), where only the dimensionalitygoes to infinity while the number of observations remains constant. We thenshow the effectiveness in extensive simulations and on real world data.
arxiv-8400-238 | CoMIC: Good features for detection and matching at object boundaries | http://arxiv.org/pdf/1412.1957v1.pdf | author:Swarna Kamlam Ravindran, Anurag Mittal category:cs.CV published:2014-12-05 summary:Feature or interest points typically use information aggregation in 2Dpatches which does not remain stable at object boundaries when there is objectmotion against a significantly varying background. Level or iso-intensitycurves are much more stable under such conditions, especially the longer ones.In this paper, we identify stable portions on long iso-curves and detectcorners on them. Further, the iso-curve associated with a corner is used todiscard portions from the background and improve matching. Such CoMIC (Cornerson Maximally-stable Iso-intensity Curves) points yield superior results at theobject boundary regions compared to state-of-the-art detectors while performingcomparably at the interior regions as well. This is illustrated in exhaustivematching experiments for both boundary and non-boundary regions in applicationssuch as stereo and point tracking for structure from motion in video sequences.
arxiv-8400-239 | A parallel sampling based clustering | http://arxiv.org/pdf/1412.1947v1.pdf | author:Aditya AV Sastry, Kalyan Netti category:cs.LG 68Q32 published:2014-12-05 summary:The problem of automatically clustering data is an age old problem. Peoplehave created numerous algorithms to tackle this problem. The execution time ofany of this algorithm grows with the number of input points and the number ofcluster centers required. To reduce the number of input points we could averagethe points locally and use the means or the local centers as the input forclustering. However since the required number of local centers is very high,running the clustering algorithm on the entire dataset to obtain theserepresentational points is very time consuming. To remedy this problem, in thispaper we are proposing two subclustering schemes where by we subdivide thedataset into smaller sets and run the clustering algorithm on the smallerdatasets to obtain the required number of datapoints to run our clusteringalgorithm with. As we are subdividing the given dataset, we could runclustering algorithm on each smaller piece of the dataset in parallel. We foundthat both parallel and serial execution of this method to be much faster thanthe original clustering algorithm and error in running the clustering algorithmon a reduced set to be very less.
arxiv-8400-240 | Near-optimal sample compression for nearest neighbors | http://arxiv.org/pdf/1404.3368v3.pdf | author:Lee-Ad Gottlieb, Aryeh Kontorovich, Pinhas Nisnevitch category:cs.LG cs.CC published:2014-04-13 summary:We present the first sample compression algorithm for nearest neighbors withnon-trivial performance guarantees. We complement these guarantees bydemonstrating almost matching hardness lower bounds, which show that our boundis nearly optimal. Our result yields new insight into margin-based nearestneighbor classification in metric spaces and allows us to significantly sharpenand simplify existing bounds. Some encouraging empirical results are alsopresented.
arxiv-8400-241 | Quantile universal threshold: model selection at the detection edge for high-dimensional linear regression | http://arxiv.org/pdf/1412.1927v1.pdf | author:Jairo Diaz Rodriguez, Sylvain Sardy category:stat.ML stat.ME published:2014-12-05 summary:To estimate a sparse linear model from data with Gaussian noise, consiliencefrom lasso and compressed sensing literatures is that thresholding estimatorslike lasso and the Dantzig selector have the ability in some situations toidentify with high probability part of the significant covariatesasymptotically, and are numerically tractable thanks to convexity. Yet, the selection of a threshold parameter $\lambda$ remains crucial inpractice. To that aim we propose Quantile Universal Thresholding, a selectionof $\lambda$ at the detection edge. We show with extensive simulations and realdata that an excellent compromise between high true positive rate and low falsediscovery rate is achieved, leading also to good predictive risk.
arxiv-8400-242 | Person Re-identification by Saliency Learning | http://arxiv.org/pdf/1412.1908v1.pdf | author:Rui Zhao, Wanli Ouyang, Xiaogang Wang category:cs.CV published:2014-12-05 summary:Human eyes can recognize person identities based on small salient regions,i.e. human saliency is distinctive and reliable in pedestrian matching acrossdisjoint camera views. However, such valuable information is often hidden whencomputing similarities of pedestrian images with existing approaches. Inspiredby our user study result of human perception on human saliency, we propose anovel perspective for person re-identification based on learning human saliencyand matching saliency distribution. The proposed saliency learning and matchingframework consists of four steps: (1) To handle misalignment caused by drasticviewpoint change and pose variations, we apply adjacency constrained patchmatching to build dense correspondence between image pairs. (2) We propose twoalternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate asaliency score for each image patch, through which distinctive features standout without using identity labels in the training procedure. (3) saliencymatching is proposed based on patch matching. Matching patches withinconsistent saliency brings penalty, and images of the same identity arerecognized by minimizing the saliency matching cost. (4) Furthermore, saliencymatching is tightly integrated with patch matching in a unified structuralRankSVM learning framework. The effectiveness of our approach is validated onthe VIPeR dataset and the CUHK01 dataset. Our approach outperforms thestate-of-the-art person re-identification methods on both datasets.
arxiv-8400-243 | Deep Structured learning for mass segmentation from Mammograms | http://arxiv.org/pdf/1410.7454v2.pdf | author:Neeraj Dhungel, Gustavo Carneiro, Andrew P. Bradley category:cs.CV published:2014-10-27 summary:In this paper, we present a novel method for the segmentation of breastmasses from mammograms exploring structured and deep learning. Specifically,using structured support vector machine (SSVM), we formulate a model thatcombines different types of potential functions, including one that classifiesimage regions using deep learning. Our main goal with this work is to show theaccuracy and efficiency improvements that these relatively new techniques canprovide for the segmentation of breast masses from mammograms. We also proposean easily reproducible quantitative analysis to as- sess the performance ofbreast mass segmentation methodologies based on widely accepted accuracy andrunning time measurements on public datasets, which will facilitate furthercomparisons for this segmentation problem. In particular, we use two publiclyavailable datasets (DDSM-BCRP and INbreast) and propose the computa- tion ofthe running time taken for the methodology to produce a mass segmentation givenan input image and the use of the Dice index to quantitatively measure thesegmentation accuracy. For both databases, we show that our proposedmethodology produces competitive results in terms of accuracy and running time.
arxiv-8400-244 | A higher homotopic extension of persistent (co)homology | http://arxiv.org/pdf/1412.1871v1.pdf | author:Estanislao Herscovich category:math.AT cs.CG cs.CV math.KT published:2014-12-05 summary:Our objective in this article is to show a possibly interesting structure ofhomotopic nature appearing in persistent (co)homology. Assuming that thefiltration of the (say) simplicial set embedded in a finite dimensional vectorspace induces a multiplicative filtration (which would not be a so harshhypothesis in our setting) on the dg algebra given by the complex of simplicialcochains, we may use a result by T. Kadeishvili to get a unique (up tononcanonical equivalence) A_infinity-algebra structure on the completepersistent cohomology of the filtered simplicial (or topological) set. We thenprovide a construction of a (pseudo)metric on the set of all (generalized)barcodes (that is, of all cohomological degrees) enriched with theA_infinity-algebra structure stated before, refining the usual bottleneckmetric, and which is also independent of the particular A_infinity-algebrastructure chosen (among those equivalent to each other). We think that thisdistance might deserve some attention for topological data analysis, for it inparticular can recognize different linking or foldings patterns, as in theBorromean rings. As an aside, we give a simple proof of a result relating thebarcode structure between persistent homology and cohomology. This result wasobserved in a recent article by V. de Silva, D. Morozov and M.Vejdemo-Johansson under some restricted assumptions, which we do not suppose.
arxiv-8400-245 | Analysis of a Reduced-Communication Diffusion LMS Algorithm | http://arxiv.org/pdf/1408.5845v2.pdf | author:Reza Arablouei, Stefan Werner, Kutluyıl Doğançay, Yih-Fang Huang category:cs.DC cs.LG cs.SY math.OC published:2014-08-25 summary:In diffusion-based algorithms for adaptive distributed estimation, each nodeof an adaptive network estimates a target parameter vector by creating anintermediate estimate and then combining the intermediate estimates availablewithin its closed neighborhood. We analyze the performance of areduced-communication diffusion least mean-square (RC-DLMS) algorithm, whichallows each node to receive the intermediate estimates of only a subset of itsneighbors at each iteration. This algorithm eases the usage of networkcommunication resources and delivers a trade-off between estimation performanceand communication cost. We show analytically that the RC-DLMS algorithm isstable and convergent in both mean and mean-square senses. We also calculateits theoretical steady-state mean-square deviation. Simulation resultsdemonstrate a good match between theory and experiment.
arxiv-8400-246 | Efficient Representations for Life-Long Learning and Autoencoding | http://arxiv.org/pdf/1411.1490v2.pdf | author:Maria-Florina Balcan, Avrim Blum, Santosh Vempala category:cs.LG published:2014-11-06 summary:It has been a long-standing goal in machine learning, as well as in AI moregenerally, to develop life-long learning systems that learn many differenttasks over time, and reuse insights from tasks learned, "learning to learn" asthey do so. In this work we pose and provide efficient algorithms for severalnatural theoretical formulations of this goal. Specifically, we consider theproblem of learning many different target functions over time, that sharecertain commonalities that are initially unknown to the learning algorithm. Ouraim is to learn new internal representations as the algorithm learns new targetfunctions, that capture this commonality and allow subsequent learning tasks tobe solved more efficiently and from less data. We develop efficient algorithmsfor two very different kinds of commonalities that target functions mightshare: one based on learning common low-dimensional and unions oflow-dimensional subspaces and one based on learning nonlinear Booleancombinations of features. Our algorithms for learning Boolean featurecombinations additionally have a dual interpretation, and can be viewed asgiving an efficient procedure for constructing near-optimal sparse Booleanautoencoders under a natural "anchor-set" assumption.
arxiv-8400-247 | Reading Text in the Wild with Convolutional Neural Networks | http://arxiv.org/pdf/1412.1842v1.pdf | author:Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2014-12-04 summary:In this work we present an end-to-end system for text spotting -- localisingand recognising text in natural scene images -- and text based image retrieval.This system is based on a region proposal mechanism for detection and deepconvolutional neural networks for recognition. Our pipeline uses a novelcombination of complementary proposal generation techniques to ensure highrecall, and a fast subsequent filtering stage for improving precision. For therecognition and ranking of proposals, we train very large convolutional neuralnetworks to perform word recognition on the whole proposal region at the sametime, departing from the character classifier based systems of the past. Thesenetworks are trained solely on data produced by a synthetic text generationengine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performancethroughout. We perform rigorous experiments across a number of standardend-to-end text spotting benchmarks and text-based image retrieval datasets,showing a large improvement over all previous methods. Finally, we demonstratea real-world application of our text spotting system to allow thousands ofhours of news footage to be instantly searchable via a text query.
arxiv-8400-248 | Primal-Dual Algorithms for Non-negative Matrix Factorization with the Kullback-Leibler Divergence | http://arxiv.org/pdf/1412.1788v1.pdf | author:Felipe Yanez, Francis Bach category:cs.LG math.OC published:2014-12-04 summary:Non-negative matrix factorization (NMF) approximates a given matrix as aproduct of two non-negative matrices. Multiplicative algorithms deliverreliable results, but they show slow convergence for high-dimensional data andmay be stuck away from local minima. Gradient descent methods have betterbehavior, but only apply to smooth losses such as the least-squares loss. Inthis article, we propose a first-order primal-dual algorithm for non-negativedecomposition problems (where one factor is fixed) with the KL divergence,based on the Chambolle-Pock algorithm. All required computations may beobtained in closed form and we provide an efficient heuristic way to selectstep-sizes. By using alternating optimization, our algorithm readily extends toNMF and, on synthetic examples, face recognition or music source separationdatasets, it is either faster than existing algorithms, or leads to improvedlocal optima, or both.
arxiv-8400-249 | Quantifying error in estimates of human brain fiber directions using Earth Mover's Distance | http://arxiv.org/pdf/1411.5271v2.pdf | author:Charles Zheng, Franco Pestilli, Ariel Rokem category:stat.ML published:2014-11-19 summary:Diffusion-weighted MR imaging (DWI) is the only method we currently have tomeasure connections between different parts of the human brain in vivo. Toelucidate the structure of these connections, algorithms for tracking bundlesof axonal fibers through the subcortical white matter rely on local estimatesof the fiber orientation distribution function (fODF) in different parts of thebrain. These functions describe the relative abundance of populations of axonalfibers crossing each other in each location. Multiple models exist forestimating fODFs. The quality of the resulting estimates can be quantified bymeans of a suitable measure of distance on the space of fODFs. However, thereare multiple distance metrics that can be applied for this purpose, includingsmoothed $L_p$ distances and the Wasserstein metrics. Here, we give fourreasons for the use of the Earth Mover's Distance (EMD) equipped with thearc-length, as a distance metric. (continued)
arxiv-8400-250 | Annotating Synapses in Large EM Datasets | http://arxiv.org/pdf/1409.1801v2.pdf | author:Stephen M. Plaza, Toufiq Parag, Gary B. Huang, Donald J. Olbris, Mathew A. Saunders, Patricia K. Rivlin category:q-bio.QM cs.CV q-bio.NC published:2014-09-05 summary:Reconstructing neuronal circuits at the level of synapses is a centralproblem in neuroscience and becoming a focus of the emerging field ofconnectomics. To date, electron microscopy (EM) is the most proven techniquefor identifying and quantifying synaptic connections. As advances in EM makeacquiring larger datasets possible, subsequent manual synapse identification({\em i.e.}, proofreading) for deciphering a connectome becomes a major timebottleneck. Here we introduce a large-scale, high-throughput, andsemi-automated methodology to efficiently identify synapses. We successfullyapplied our methodology to the Drosophila medulla optic lobe, annotating manymore synapses than previous connectome efforts. Our approaches are extensibleand will make the often complicated process of synapse identificationaccessible to a wider-community of potential proofreaders.
arxiv-8400-251 | Convolutional Neural Networks at Constrained Time Cost | http://arxiv.org/pdf/1412.1710v1.pdf | author:Kaiming He, Jian Sun category:cs.CV published:2014-12-04 summary:Though recent advanced convolutional neural networks (CNNs) have beenimproving the image recognition accuracy, the models are getting more complexand time-consuming. For real-world applications in industrial and commercialscenarios, engineers and developers are often faced with the requirement ofconstrained time budget. In this paper, we investigate the accuracy of CNNsunder constrained time cost. Under this constraint, the designs of the networkarchitectures should exhibit as trade-offs among the factors like depth,numbers of filters, filter sizes, etc. With a series of controlled comparisons,we progressively modify a baseline model while preserving its time complexity.This is also helpful for understanding the importance of the factors in networkdesigns. We present an architecture that achieves very competitive accuracy inthe ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than"AlexNet" (16.0% top-5 error, 10-view test).
arxiv-8400-252 | Deep Learning for Answer Sentence Selection | http://arxiv.org/pdf/1412.1632v1.pdf | author:Lei Yu, Karl Moritz Hermann, Phil Blunsom, Stephen Pulman category:cs.CL published:2014-12-04 summary:Answer sentence selection is the task of identifying sentences that containthe answer to a given question. This is an important problem in its own rightas well as in the larger context of open domain question answering. We proposea novel approach to solving this task via means of distributed representations,and learn to match questions with answers by considering their semanticencoding. This contrasts prior work on this task, which typically relies onclassifiers with large numbers of hand-crafted syntactic and semantic featuresand various external resources. Our approach does not require any featureengineering nor does it involve specialist linguistic data, making this modeleasily applicable to a wide range of domains and languages. Experimentalresults on a standard benchmark dataset from TREC demonstrate that---despiteits simplicity---our model matches state of the art performance on the answersentence selection task.
arxiv-8400-253 | Tight convex relaxations for sparse matrix factorization | http://arxiv.org/pdf/1407.5158v2.pdf | author:Emile Richard, Guillaume Obozinski, Jean-Philippe Vert category:stat.ML cs.LG math.ST stat.TH published:2014-07-19 summary:Based on a new atomic norm, we propose a new convex formulation for sparsematrix factorization problems in which the number of nonzero elements of thefactors is assumed fixed and known. The formulation counts sparse PCA withmultiple factors, subspace clustering and low-rank sparse bilinear regressionas potential applications. We compute slow rates and an upper bound on thestatistical dimension of the suggested norm for rank 1 matrices, showing thatits statistical dimension is an order of magnitude smaller than the usual$\ell\_1$-norm, trace norm and their combinations. Even though our convexformulation is in theory hard and does not lead to provably polynomial timealgorithmic schemes, we propose an active set algorithm leveraging thestructure of the convex problem to solve it and show promising numericalresults.
arxiv-8400-254 | End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results | http://arxiv.org/pdf/1412.1602v1.pdf | author:Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio category:cs.NE cs.LG stat.ML published:2014-12-04 summary:We replace the Hidden Markov Model (HMM) which is traditionally used in incontinuous speech recognition with a bi-directional recurrent neural networkencoder coupled to a recurrent neural network decoder that directly emits astream of phonemes. The alignment between the input and output sequences isestablished using an attention mechanism: the decoder emits each symbol basedon a context created with a subset of input symbols elected by the attentionmechanism. We report initial results demonstrating that this new approachachieves phoneme error rates that are comparable to the state-of-the-artHMM-based decoders, on the TIMIT dataset.
arxiv-8400-255 | LightLDA: Big Topic Models on Modest Compute Clusters | http://arxiv.org/pdf/1412.1576v1.pdf | author:Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang Wei, Xun Zheng, Eric P. Xing, Tie-Yan Liu, Wei-Ying Ma category:stat.ML cs.DC cs.IR cs.LG published:2014-12-04 summary:When building large-scale machine learning (ML) programs, such as big topicmodels or deep neural nets, one usually assumes such tasks can only beattempted with industrial-sized clusters with thousands of nodes, which are outof reach for most practitioners or academic researchers. We consider thischallenge in the context of topic modeling on web-scale corpora, and show thatwith a modest cluster of as few as 8 machines, we can train a topic model with1 million topics and a 1-million-word vocabulary (for a total of 1 trillionparameters), on a document collection with 200 billion tokens -- a scale notyet reported even with thousands of machines. Our major contributions include:1) a new, highly efficient O(1) Metropolis-Hastings sampling algorithm, whoserunning cost is (surprisingly) agnostic of model size, and empiricallyconverges nearly an order of magnitude faster than current state-of-the-artGibbs samplers; 2) a structure-aware model-parallel scheme, which leveragesdependencies within the topic model, yielding a sampling strategy that isfrugal on machine memory and network communication; 3) a differentialdata-structure for model storage, which uses separate data structures for high-and low-frequency words to allow extremely large models to fit in memory, whilemaintaining high inference speed; and 4) a bounded asynchronous data-parallelscheme, which allows efficient distributed processing of massive data via aparameter server. Our distribution strategy is an instance of themodel-and-data-parallel programming model underlying the Petuum framework forgeneral distributed ML, and was implemented on top of the Petuum open-sourcesystem. We provide experimental evidence showing how this development putsmassive models within reach on a small cluster while still enjoyingproportional time cost reductions with increasing cluster size, in comparisonwith alternative options.
arxiv-8400-256 | Metric Learning Driven Multi-Task Structured Output Optimization for Robust Keypoint Tracking | http://arxiv.org/pdf/1412.1574v1.pdf | author:Liming Zhao, Xi Li, Jun Xiao, Fei Wu, Yueting Zhuang category:cs.CV cs.LG published:2014-12-04 summary:As an important and challenging problem in computer vision and graphics,keypoint-based object tracking is typically formulated in a spatio-temporalstatistical learning framework. However, most existing keypoint trackers areincapable of effectively modeling and balancing the following three aspects ina simultaneous manner: temporal model coherence across frames, spatial modelconsistency within frames, and discriminative feature construction. To addressthis issue, we propose a robust keypoint tracker based on spatio-temporalmulti-task structured output optimization driven by discriminative metriclearning. Consequently, temporal model coherence is characterized by multi-taskstructured keypoint model learning over several adjacent frames, while spatialmodel consistency is modeled by solving a geometric verification basedstructured learning problem. Discriminative feature construction is enabled bymetric learning to ensure the intra-class compactness and inter-classseparability. Finally, the above three modules are simultaneously optimized ina joint learning scheme. Experimental results have demonstrated theeffectiveness of our tracker.
arxiv-8400-257 | On the String Kernel Pre-Image Problem with Applications in Drug Discovery | http://arxiv.org/pdf/1412.1463v2.pdf | author:Sébastien Giguère, Amélie Rolland, François Laviolette, Mario Marchand category:cs.LG cs.CE I.2.6; K.3.2 published:2014-12-03 summary:The pre-image problem has to be solved during inference by most structuredoutput predictors. For string kernels, this problem corresponds to finding thestring associated to a given input. An algorithm capable of solving or findinggood approximations to this problem would have many applications incomputational biology and other fields. This work uses a recent result oncombinatorial optimization of linear predictors based on string kernels todevelop, for the pre-image, a low complexity upper bound valid for many stringkernels. This upper bound is used with success in a branch and bound searchingalgorithm. Applications and results in the discovery of druggable peptides arepresented and discussed.
arxiv-8400-258 | Detection of cheating by decimation algorithm | http://arxiv.org/pdf/1410.3596v2.pdf | author:Shogo Yamanaka, Masayuki Ohzeki, Aurelien Decelle category:stat.ML cs.LG published:2014-10-14 summary:We expand the item response theory to study the case of "cheating students"for a set of exams, trying to detect them by applying a greedy algorithm ofinference. This extended model is closely related to the Boltzmann machinelearning. In this paper we aim to infer the correct biases and interactions ofour model by considering a relatively small number of sets of training data.Nevertheless, the greedy algorithm that we employed in the present studyexhibits good performance with a few number of training data. The key point isthe sparseness of the interactions in our problem in the context of theBoltzmann machine learning: the existence of cheating students is expected tobe very rare (possibly even in real world). We compare a standard approach toinfer the sparse interactions in the Boltzmann machine learning to our greedyalgorithm and we find the latter to be superior in several aspects.
arxiv-8400-259 | Context-Dependent Fine-Grained Entity Type Tagging | http://arxiv.org/pdf/1412.1820v1.pdf | author:Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, David Huynh category:cs.CL published:2014-12-03 summary:Entity type tagging is the task of assigning category labels to each mentionof an entity in a document. While standard systems focus on a small set oftypes, recent work (Ling and Weld, 2012) suggests that using a largefine-grained label set can lead to dramatic improvements in downstream tasks.In the absence of labeled training data, existing fine-grained tagging systemsobtain examples automatically, using resolved entities and their typesextracted from a knowledge base. However, since the appropriate type oftendepends on context (e.g. Washington could be tagged either as city orgovernment), this procedure can result in spurious labels, leading to poorergeneralization. We propose the task of context-dependent fine type tagging,where the set of acceptable labels for a mention is restricted to only thosededucible from the local context (e.g. sentence or document). We introduce newresources for this task: 11,304 mentions annotated with their context-dependentfine types, and we provide baseline experimental results on this data.
arxiv-8400-260 | Textural Approach for Mass Abnormality Segmentation in Mammographic Images | http://arxiv.org/pdf/1412.1506v1.pdf | author:Khamsa Djaroudib, Abdelmalik Taleb Ahmed, Abdelmadjid Zidani category:cs.CV 68U10 published:2014-12-03 summary:Mass abnormality segmentation is a vital step for the medical diagnosticprocess and is attracting more and more the interest of many research groups.Currently, most of the works achieved in this area have used the Gray LevelCo-occurrence Matrix (GLCM) as texture features with a region-based approach.These features come in previous phase for segmentation stage or are using asinputs to classification stage. The work discussed in this paper attempts toexperiment the GLCM method under a contour-based approach. Besides, weexperiment the proposed approach on various tissues densities to bring moresignificant results. At this end, we explored some challenging breast imagesfrom BIRADS medical Data Base. Our first experimentations showed promisingresults with regard to the edges mass segmentation methods. This paperdiscusses first the main works achieved in this area. Sections 2 and 3 presentmaterials and our methodology. The main results are showed and evaluated beforeconcluding our paper.
arxiv-8400-261 | Playing with Duality: An Overview of Recent Primal-Dual Approaches for Solving Large-Scale Optimization Problems | http://arxiv.org/pdf/1406.5429v2.pdf | author:Nikos Komodakis, Jean-Christophe Pesquet category:cs.NA cs.CV cs.LG math.OC published:2014-06-20 summary:Optimization methods are at the core of many problems in signal/imageprocessing, computer vision, and machine learning. For a long time, it has beenrecognized that looking at the dual of an optimization problem may drasticallysimplify its solution. Deriving efficient strategies which jointly brings intoplay the primal and the dual problems is however a more recent idea which hasgenerated many important new contributions in the last years. These noveldevelopments are grounded on recent advances in convex analysis, discreteoptimization, parallel processing, and non-smooth optimization with emphasis onsparsity issues. In this paper, we aim at presenting the principles ofprimal-dual approaches, while giving an overview of numerical methods whichhave been proposed in different contexts. We show the benefits which can bedrawn from primal-dual algorithms both for solving large-scale convexoptimization problems and discrete ones, and we provide various applicationexamples to illustrate their usefulness.
arxiv-8400-262 | Structure learning of antiferromagnetic Ising models | http://arxiv.org/pdf/1412.1443v1.pdf | author:Guy Bresler, David Gamarnik, Devavrat Shah category:stat.ML cs.IT cs.LG math.IT published:2014-12-03 summary:In this paper we investigate the computational complexity of learning thegraph structure underlying a discrete undirected graphical model from i.i.d.samples. We first observe that the notoriously difficult problem of learningparities with noise can be captured as a special case of learning graphicalmodels. This leads to an unconditional computational lower bound of $\Omega(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree$d$, for the class of so-called statistical algorithms recently introduced byFeldman et al (2013). The lower bound suggests that the $O(p^d)$ runtimerequired to exhaustively search over neighborhoods cannot be significantlyimproved without restricting the class of models. Aside from structural assumptions on the graph such as it being a tree,hypertree, tree-like, etc., many recent papers on structure learning assumethat the model has the correlation decay property. Indeed, focusing onferromagnetic Ising models, Bento and Montanari (2009) showed that all knownlow-complexity algorithms fail to learn simple graphs when the interactionstrength exceeds a number related to the correlation decay threshold. Oursecond set of results gives a class of repelling (antiferromagnetic) modelsthat have the opposite behavior: very strong interaction allows efficientlearning in time $O(p^2)$. We provide an algorithm whose performanceinterpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of therepulsion.
arxiv-8400-263 | Memory Bounded Deep Convolutional Networks | http://arxiv.org/pdf/1412.1442v1.pdf | author:Maxwell D. Collins, Pushmeet Kohli category:cs.CV published:2014-12-03 summary:In this work, we investigate the use of sparsity-inducing regularizers duringtraining of Convolution Neural Networks (CNNs). These regularizers encouragethat fewer connections in the convolution and fully connected layers takenon-zero values and in effect result in sparse connectivity between hiddenunits in the deep network. This in turn reduces the memory and runtime costinvolved in deploying the learned CNNs. We show that training with suchregularization can still be performed using stochastic gradient descentimplying that it can be used easily in existing codebases. Experimentalevaluation of our approach on MNIST, CIFAR, and ImageNet datasets shows thatour regularizers can result in dramatic reductions in memory requirements. Forinstance, when applied on AlexNet, our method can reduce the memory consumptionby a factor of four with minimal loss in accuracy.
arxiv-8400-264 | A note relating ridge regression and OLS p-values to preconditioned sparse penalized regression | http://arxiv.org/pdf/1411.7405v2.pdf | author:Karl Rohe category:stat.ML stat.ME published:2014-11-26 summary:When the design matrix has orthonormal columns, "soft thresholding" theordinary least squares (OLS) solution produces the Lasso solution [Tibshirani,1996]. If one uses the Puffer preconditioned Lasso [Jia and Rohe, 2012], thenthis result generalizes from orthonormal designs to full rank designs (Theorem1). Theorem 2 refines the Puffer preconditioner to make the Lasso select thesame model as removing the elements of the OLS solution with the largestp-values. Using a generalized Puffer preconditioner, Theorem 3 relates ridgeregression to the preconditioned Lasso; this result is for the high dimensionalsetting, p > n. Where the standard Lasso is akin to forward selection [Efron etal., 2004], Theorems 1, 2, and 3 suggest that the preconditioned Lasso is moreakin to backward elimination. These results hold for sparse penalties beyondl1; for a broad class of sparse and non-convex techniques (e.g. SCAD and MC+),the results hold for all local minima.
arxiv-8400-265 | Nested Variational Compression in Deep Gaussian Processes | http://arxiv.org/pdf/1412.1370v1.pdf | author:James Hensman, Neil D. Lawrence category:stat.ML published:2014-12-03 summary:Deep Gaussian processes provide a flexible approach to probabilisticmodelling of data using either supervised or unsupervised learning. Fortractable inference approximations to the marginal likelihood of the model mustbe made. The original approach to approximate inference in these models usedvariational compression to allow for approximate variational marginalization ofthe hidden variables leading to a lower bound on the marginal likelihood of themodel [Damianou and Lawrence, 2013]. In this paper we extend this idea with anested variational compression. The resulting lower bound on the likelihood canbe easily parallelized or adapted for stochastic variational inference.
arxiv-8400-266 | Curriculum Learning of Multiple Tasks | http://arxiv.org/pdf/1412.1353v1.pdf | author:Anastasia Pentina, Viktoriia Sharmanska, Christoph H. Lampert category:stat.ML cs.LG published:2014-12-03 summary:Sharing information between multiple tasks enables algorithms to achieve goodgeneralization performance even from small amounts of training data. However,in a realistic scenario of multi-task learning not all tasks are equallyrelated to each other, hence it could be advantageous to transfer informationonly between the most related tasks. In this work we propose an approach thatprocesses multiple tasks in a sequence with sharing between subsequent tasksinstead of solving all tasks jointly. Subsequently, we address the question ofcurriculum learning of tasks, i.e. finding the best order of tasks to belearned. Our approach is based on a generalization bound criterion for choosingthe task order that optimizes the average expected classification performanceover all tasks. Our experimental results show that learning multiple relatedtasks sequentially can be more effective than learning them jointly, the orderin which tasks are being solved affects the overall performance, and that ourmodel is able to automatically discover the favourable order of tasks.
arxiv-8400-267 | A perspective on the advancement of natural language processing tasks via topological analysis of complex networks | http://arxiv.org/pdf/1412.1342v1.pdf | author:Diego R. Amancio category:cs.CL published:2014-12-03 summary:Comment on "Approaching human language with complex networks" by Cong and Liu(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).
arxiv-8400-268 | The inductive theory of natural selection | http://arxiv.org/pdf/1412.1285v1.pdf | author:Steven A. Frank category:q-bio.PE cs.NE physics.bio-ph published:2014-12-03 summary:The theory of natural selection has two forms. Deductive theory describes howpopulations change over time. One starts with an initial population and somerules for change. From those assumptions, one calculates the future state ofthe population. Deductive theory predicts how populations adapt toenvironmental challenge. Inductive theory describes the causes of change inpopulations. One starts with a given amount of change. One then assignsdifferent parts of the total change to particular causes. Inductive theoryanalyzes alternative causal models for how populations have adapted toenvironmental challenge. This chapter emphasizes the inductive analysis ofcause.
arxiv-8400-269 | Deeply learned face representations are sparse, selective, and robust | http://arxiv.org/pdf/1412.1265v1.pdf | author:Yi Sun, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-12-03 summary:This paper designs a high-performance deep convolutional network (DeepID2+)for face recognition. It is learned with the identification-verificationsupervisory signal. By increasing the dimension of hidden representations andadding supervision to early convolutional layers, DeepID2+ achieves newstate-of-the-art on LFW and YouTube Faces benchmarks. Through empiricalstudies, we have discovered three properties of its deep neural activationscritical for the high performance: sparsity, selectiveness and robustness. (1)It is observed that neural activations are moderately sparse. Moderate sparsitymaximizes the discriminative power of the deep net as well as the distancebetween images. It is surprising that DeepID2+ still can achieve highrecognition accuracy even after the neural responses are binarized. (2) Itsneurons in higher layers are highly selective to identities andidentity-related attributes. We can identify different subsets of neurons whichare either constantly excited or inhibited when different identities orattributes are present. Although DeepID2+ is not taught to distinguishattributes during training, it has implicitly learned such high-level concepts.(3) It is much more robust to occlusions, although occlusion patterns are notincluded in the training set.
arxiv-8400-270 | Colorisation et texturation temps réel d'environnements urbains par système mobile avec scanner laser et caméra fish-eye | http://arxiv.org/pdf/1412.1219v1.pdf | author:Jean-Emmanuel Deschaud, Xavier Brun, François Goulette category:cs.RO cs.CV published:2014-12-03 summary:We present here a real time mobile mapping system mounted on a vehicle. Theterrestrial acquisition system is based on a geolocation system and twosensors, namely, a laser scanner and a camera with a fish-eye lens. We produce3D colored points cloud and textured models of the environment. Once the systemhas been calibrated, the data acquisition and processing are done "on the way".This article mainly presents our methods of colorization of point cloud,triangulation and texture mapping.
arxiv-8400-271 | Simple Two-Dimensional Object Tracking based on a Graph Algorithm | http://arxiv.org/pdf/1412.1216v1.pdf | author:Alexandra Heidsieck category:cs.CV published:2014-12-03 summary:The visual observation and tracking of cells and other micrometer-sizedobjects has many different biomedical applications. The automation of thosetasks based on computer methods helps in the evaluation of such measurements.In this work, we present a general purpose algorithm that excels at evaluatingdeterministic behavior of micrometer-sized objects. Our concrete application isthe tracking of fast moving objects over large distances along deterministictrajectories in a microscopic video. Thereby, we are able to determinecharacteristic properties of the objects. For this purpose, we use a set ofbasic algorithms, including blob recognition, feature-based shape recognitionand a graph algorithm, and combined them in a novel way. An evaluation of thealgorithms performance shows a high accuracy in the recognition of objects aswell as of complete trajectories. Moreover, a direct comparison to a similaralgorithm shows superior recognition rates.
arxiv-8400-272 | Mary Astell's words in A Serious Proposal to the Ladies (part I), a lexicographic inquiry with NooJ | http://arxiv.org/pdf/1412.1215v1.pdf | author:Hélène Pignot, Odile Piton category:cs.CL published:2014-12-03 summary:In the following article we elected to study with NooJ the lexis of a 17 thcentury text, Mary Astell's seminal essay, A Serious Proposal to the Ladies,part I, published in 1694. We first focused on the semantics to see how Astellbuilds her vindication of the female sex, which words she uses to sensitisewomen to their alienated condition and promote their education. Then we studiedthe morphology of the lexemes (which is different from contemporary English)used by the author, thanks to the NooJ tools we have devised for this purpose.NooJ has great functionalities for lexicographic work. Its commands and graphsprove to be most efficient in the spotting of archaic words or variants inspelling. Introduction In our previous articles, we have studied thesingularities of 17 th century English within the framework of a diachronicanalysis thanks to syntactical and morphological graphs and thanks to thedictionaries we have compiled from a corpus that may be expanded overtime. Ourearly work was based on a limited corpus of English travel literature to Greecein the 17 th century. This article deals with a late seventeenth century textwritten by a woman philosopher and essayist, Mary Astell (1666--1731),considered as one of the first English feminists. Astell wrote her essay at atime in English history when women were "the weaker vessel" and their mainbusiness in life was to charm and please men by their looks and submissiveness.In this essay we will see how NooJ can help us analyse Astell's rhetoric (whatpoint of view does she adopt, does she speak in her own name, in the name ofall women, what is her representation of men and women and their relationshipsin the text, what are the goals of education?). Then we will turn our attentionto the morphology of words in the text and use NooJ commands and graphs tocarry out a lexicographic inquiry into Astell's lexemes.
arxiv-8400-273 | Gradient Boundary Histograms for Action Recognition | http://arxiv.org/pdf/1412.1194v1.pdf | author:Feng Shi, Robert Laganiere, Emil Petriu category:cs.CV published:2014-12-03 summary:This paper introduces a high efficient local spatiotemporal descriptor,called gradient boundary histograms (GBH). The proposed GBH descriptor is builton simple spatio-temporal gradients, which are fast to compute. We demonstratethat it can better represent local structure and motion than othergradient-based descriptors, and significantly outperforms them on largerealistic datasets. A comprehensive evaluation shows that the recognitionaccuracy is preserved while the spatial resolution is greatly reduced, whichyields both high efficiency and low memory usage.
arxiv-8400-274 | Highly comparative fetal heart rate analysis | http://arxiv.org/pdf/1412.1138v1.pdf | author:B. D. Fulcher, A. E. Georgieva, C. W. G. Redman, Nick S. Jones category:cs.LG cs.AI q-bio.QM published:2014-12-03 summary:A database of fetal heart rate (FHR) time series measured from 7221 patientsduring labor is analyzed with the aim of learning the types of features ofthese recordings that are informative of low cord pH. Our 'highly comparative'analysis involves extracting over 9000 time-series analysis features from eachFHR time series, including measures of autocorrelation, entropy, distribution,and various model fits. This diverse collection of features was developed inprevious work, and is publicly available. We describe five features that mostaccurately classify a balanced training set of 59 'low pH' and 59 'normal pH'FHR recordings. We then describe five of the features with the strongest linearcorrelation to cord pH across the full dataset of FHR time series. The featuresidentified in this work may be used as part of a system for guidingintervention during labor in future. This work successfully demonstrates theutility of comparing across a large, interdisciplinary literature ontime-series analysis to automatically contribute new scientific results forspecific biomedical signal processing challenges.
arxiv-8400-275 | Detector Discovery in the Wild: Joint Multiple Instance and Representation Learning | http://arxiv.org/pdf/1412.1135v1.pdf | author:Judy Hoffman, Deepak Pathak, Trevor Darrell, Kate Saenko category:cs.CV published:2014-12-02 summary:We develop methods for detector learning which exploit joint training overboth weak and strong labels and which transfer learned perceptualrepresentations from strongly-labeled auxiliary tasks. Previous methods forweak-label learning often learn detector models independently using latentvariable optimization, but fail to share deep representation knowledge acrossclasses and usually require strong initialization. Other previous methodstransfer deep representations from domains with strong labels to those withonly weak labels, but do not optimize over individual latent boxes, and thusmay miss specific salient structures for a particular category. We propose amodel that subsumes these previous approaches, and simultaneously trains arepresentation and detectors for categories with either weak or strong labelspresent. We provide a novel formulation of a joint multiple instance learningmethod that includes examples from classification-style data when available,and also performs domain transfer learning to improve the underlying detectorrepresentation. Our model outperforms known methods on ImageNet-200 detectionwith weak labels.
arxiv-8400-276 | Spread Unary Coding | http://arxiv.org/pdf/1412.6122v1.pdf | author:Subhash Kak category:cs.NE cs.IT math.IT published:2014-12-02 summary:Unary coding is useful but it is redundant in its standard form. Unary codingcan also be seen as spatial coding where the value of the number is determinedby its place in an array. Motivated by biological finding that several neuronsin the vicinity represent the same number, we propose a variant of unarynumeration in its spatial form, where each number is represented by several 1s.We call this spread unary coding where the number of 1s used is the spread ofthe code. Spread unary coding is associated with saturation of the Hammingdistance between code words.
arxiv-8400-277 | Easy Hyperparameter Search Using Optunity | http://arxiv.org/pdf/1412.1114v1.pdf | author:Marc Claesen, Jaak Simm, Dusan Popovic, Yves Moreau, Bart De Moor category:cs.LG published:2014-12-02 summary:Optunity is a free software package dedicated to hyperparameter optimization.It contains various types of solvers, ranging from undirected methods to directsearch, particle swarm and evolutionary optimization. The design focuses onease of use, flexibility, code clarity and interoperability with existingsoftware in all machine learning environments. Optunity is written in Pythonand contains interfaces to environments such as R and MATLAB. Optunity uses aBSD license and is freely available online at http://www.optunity.net.
arxiv-8400-278 | Learning Theory and Algorithms for Revenue Optimization in Second-Price Auctions with Reserve | http://arxiv.org/pdf/1310.5665v3.pdf | author:Mehryar Mohri, Andres Muñoz Medina category:cs.LG published:2013-10-21 summary:Second-price auctions with reserve play a critical role for modern searchengine and popular online sites since the revenue of these companies oftendirectly de- pends on the outcome of such auctions. The choice of the reserveprice is the main mechanism through which the auction revenue can be influencedin these electronic markets. We cast the problem of selecting the reserve priceto optimize revenue as a learning problem and present a full theoreticalanalysis dealing with the complex properties of the corresponding lossfunction. We further give novel algorithms for solving this problem and reportthe results of several experiments in both synthetic and real datademonstrating their effectiveness.
arxiv-8400-279 | Fuzzy human motion analysis: A review | http://arxiv.org/pdf/1412.0439v2.pdf | author:Chern Hong Lim, Ekta Vats, Chee Seng Chan category:cs.CV cs.AI published:2014-12-01 summary:Human Motion Analysis (HMA) is currently one of the most popularly activeresearch domains as such significant research interests are motivated by anumber of real world applications such as video surveillance, sports analysis,healthcare monitoring and so on. However, most of these real world applicationsface high levels of uncertainties that can affect the operations of suchapplications. Hence, the fuzzy set theory has been applied and showed greatsuccess in the recent past. In this paper, we aim at reviewing the fuzzy setoriented approaches for HMA, individuating how the fuzzy set may improve theHMA, envisaging and delineating the future perspectives. To the best of ourknowledge, there is not found a single survey in the current literature thathas discussed and reviewed fuzzy approaches towards the HMA. For ease ofunderstanding, we conceptually classify the human motion into three broadlevels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA.
arxiv-8400-280 | Group Factor Analysis | http://arxiv.org/pdf/1411.5799v2.pdf | author:Arto Klami, Seppo Virtanen, Eemeli Leppäaho, Samuel Kaski category:stat.ML published:2014-11-21 summary:Factor analysis provides linear factors that describe relationships betweenindividual variables of a data set. We extend this classical formulation intolinear factors that describe relationships between groups of variables, whereeach group represents either a set of related variables or a data set. Themodel also naturally extends canonical correlation analysis to more than twosets, in a way that is more flexible than previous extensions. Our solution isformulated as variational inference of a latent variable model with structuralsparsity, and it consists of two hierarchical levels: The higher level modelsthe relationships between the groups, whereas the lower models the observedvariables given the higher level. We show that the resulting solution solvesthe group factor analysis problem accurately, outperforming alternative factoranalysis based solutions as well as more straightforward implementations ofgroup factor analysis. The method is demonstrated on two life science datasets, one on brain activation and the other on systems biology, illustratingits applicability to the analysis of different types of high-dimensional datasources.
arxiv-8400-281 | Learning interpretable models of phenotypes from whole genome sequences with the Set Covering Machine | http://arxiv.org/pdf/1412.1074v1.pdf | author:Alexandre Drouin, Sébastien Giguère, Vladana Sagatovich, Maxime Déraspe, François Laviolette, Mario Marchand, Jacques Corbeil category:q-bio.GN cs.CE cs.LG stat.ML published:2014-12-02 summary:The increased affordability of whole genome sequencing has motivated its usefor phenotypic studies. We address the problem of learning interpretable modelsfor discrete phenotypes from whole genomes. We propose a general approach thatrelies on the Set Covering Machine and a k-mer representation of the genomes.We show results for the problem of predicting the resistance of PseudomonasAeruginosa, an important human pathogen, against 4 antibiotics. Our resultsdemonstrate that extremely sparse models which are biologically relevant can belearnt using this approach.
arxiv-8400-282 | Convex Techniques for Model Selection | http://arxiv.org/pdf/1411.7596v2.pdf | author:Dustin Tran category:math.OC stat.ML published:2014-11-27 summary:We develop a robust convex algorithm to select the regularization parameterin model selection. In practice this would be automated in order to savepractitioners time from having to tune it manually. In particular, we implementand test the convex method for $K$-fold cross validation on ridge regression,although the same concept extends to more complex models. We then compare itsperformance with standard methods.
arxiv-8400-283 | Watsonsim: Overview of a Question Answering Engine | http://arxiv.org/pdf/1412.0879v1.pdf | author:Sean Gallagher, Wlodek Zadrozny, Walid Shalaby, Adarsh Avadhani category:cs.CL cs.IR published:2014-12-02 summary:The objective of the project is to design and run a system similar to Watson,designed to answer Jeopardy questions. In the course of a semester, wedeveloped an open source question answering system using the Indri, Lucene,Bing and Google search engines, Apache UIMA, Open- and CoreNLP, and Weka amongadditional modules. By the end of the semester, we achieved 18% accuracy onJeopardy questions, and work has not stopped since then.
arxiv-8400-284 | Hashing on Nonlinear Manifolds | http://arxiv.org/pdf/1412.0826v1.pdf | author:Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin Tang, Heng Tao Shen category:cs.CV published:2014-12-02 summary:Learning based hashing methods have attracted considerable attention due totheir ability to greatly increase the scale at which existing algorithms mayoperate. Most of these methods are designed to generate binary codes preservingthe Euclidean similarity in the original space. Manifold learning techniques,in contrast, are better able to model the intrinsic structure embedded in theoriginal high-dimensional data. The complexities of these models, and theproblems with out-of-sample data, have previously rendered them unsuitable forapplication to large-scale embedding, however. In this work, how to learncompact binary embeddings on their intrinsic manifolds is considered. In orderto address the above-mentioned difficulties, an efficient, inductive solutionto the out-of-sample data problem, and a process by which non-parametricmanifold learning may be used as the basis of a hashing method is proposed. Theproposed approach thus allows the development of a range of new hashingtechniques exploiting the flexibility of the wide variety of manifold learningapproaches available. It is particularly shown that hashing on the basis oft-SNE outperforms state-of-the-art hashing methods on large-scale benchmarkdatasets, and is very effective for image classification with very short codelengths. The proposed hashing framework is shown to be easily improved, forexample, by minimizing the quantization error with learned orthogonalrotations. In addition, a supervised inductive manifold hashing framework isdeveloped by incorporating the label information, which is shown to greatlyadvance the semantic retrieval performance.
arxiv-8400-285 | Analytical Comparison of Noise Reduction Filters for Image Restoration Using SNR Estimation | http://arxiv.org/pdf/1412.0801v1.pdf | author:Poorna Banerjee Dasgupta category:cs.CV published:2014-12-02 summary:Noise removal from images is a part of image restoration in which we try toreconstruct or recover an image that has been degraded by using aprioriknowledge of the degradation phenomenon. Noises present in images can be ofvarious types with their characteristic Probability Distribution Functions(PDF). Noise removal techniques depend on the kind of noise present in theimage rather than on the image itself. This paper explores the effects ofapplying noise reduction filters having similar properties on noisy images withemphasis on Signal-to-Noise-Ratio (SNR) value estimation for comparing theresults.
arxiv-8400-286 | Beta Process Non-negative Matrix Factorization with Stochastic Structured Mean-Field Variational Inference | http://arxiv.org/pdf/1411.1804v2.pdf | author:Dawen Liang, Matthew D. Hoffman category:stat.ML cs.LG published:2014-11-07 summary:Beta process is the standard nonparametric Bayesian prior for latent factormodel. In this paper, we derive a structured mean-field variational inferencealgorithm for a beta process non-negative matrix factorization (NMF) model withPoisson likelihood. Unlike the linear Gaussian model, which is well-studied inthe nonparametric Bayesian literature, NMF model with beta process prior doesnot enjoy the conjugacy. We leverage the recently developed stochasticstructured mean-field variational inference to relax the conjugacy constraintand restore the dependencies among the latent variables in the approximatingvariational distribution. Preliminary results on both synthetic and realexamples demonstrate that the proposed inference algorithm can reasonablyrecover the hidden structure of the data.
arxiv-8400-287 | Feedforward semantic segmentation with zoom-out features | http://arxiv.org/pdf/1412.0774v1.pdf | author:Mohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich category:cs.CV published:2014-12-02 summary:We introduce a purely feed-forward architecture for semantic segmentation. Wemap small image elements (superpixels) to rich feature representationsextracted from a sequence of nested regions of increasing extent. These regionsare obtained by "zooming out" from the superpixel all the way to scene-levelresolution. This approach exploits statistical structure in the image and inthe label space without setting up explicit structured prediction mechanisms,and thus avoids complex and expensive inference. Instead superpixels areclassified by a feedforward multilayer network. Our architecture achieves newstate of the art performance in semantic segmentation, obtaining 64.4% averageaccuracy on the PASCAL VOC 2012 test set.
arxiv-8400-288 | Tiered Clustering to Improve Lexical Entailment | http://arxiv.org/pdf/1412.0751v1.pdf | author:John Wieting category:cs.CL published:2014-12-02 summary:Many tasks in Natural Language Processing involve recognizing lexicalentailment. Two different approaches to this problem have been proposedrecently that are quite different from each other. The first is an asymmetricsimilarity measure designed to give high scores when the contexts of thenarrower term in the entailment are a subset of those of the broader term. Thesecond is a supervised approach where a classifier is learned to predictentailment given a concatenated latent vector representation of the word. Bothof these approaches are vector space models that use a single context vector asa representation of the word. In this work, I study the effects of clusteringwords into senses and using these multiple context vectors to infer entailmentusing extensions of these two algorithms. I find that this approach offers someimprovement to these entailment algorithms.
arxiv-8400-289 | Fast Sublinear Sparse Representation using Shallow Tree Matching Pursuit | http://arxiv.org/pdf/1412.0680v1.pdf | author:Ali Ayremlou, Thomas Goldstein, Ashok Veeraraghavan, Richard Baraniuk category:cs.CV published:2014-12-01 summary:Sparse approximations using highly over-complete dictionaries is astate-of-the-art tool for many imaging applications including denoising,super-resolution, compressive sensing, light-field analysis, and objectrecognition. Unfortunately, the applicability of such methods is severelyhampered by the computational burden of sparse approximation: these algorithmsare linear or super-linear in both the data dimensionality and size of thedictionary. We propose a framework for learning the hierarchical structure ofover-complete dictionaries that enables fast computation of sparserepresentations. Our method builds on tree-based strategies for nearestneighbor matching, and presents domain-specific enhancements that are highlyefficient for the analysis of image patches. Contrary to most popular methodsfor building spatial data structures, out methods rely on shallow, balancedtrees with relatively few layers. We show an extensive array of experiments onseveral applications such as image denoising/superresolution, compressivevideo/light-field sensing where we practically achieve 100-1000x speedup (witha less than 1dB loss in accuracy).
arxiv-8400-290 | Predictive Overlapping Co-Clustering | http://arxiv.org/pdf/1403.1942v2.pdf | author:Chandrima Sarkar, Jaideep Srivastava category:cs.LG published:2014-03-08 summary:In the past few years co-clustering has emerged as an important data miningtool for two way data analysis. Co-clustering is more advantageous overtraditional one dimensional clustering in many ways such as, ability to findhighly correlated sub-groups of rows and columns. However, one of theoverlooked benefits of co-clustering is that, it can be used to extractmeaningful knowledge for various other knowledge extraction purposes. Forexample, building predictive models with high dimensional data andheterogeneous population is a non-trivial task. Co-clusters extracted from suchdata, which shows similar pattern in both the dimension, can be used for a moreaccurate predictive model building. Several applications such as findingpatient-disease cohorts in health care analysis, finding user-genre groups inrecommendation systems and community detection problems can benefit fromco-clustering technique that utilizes the predictive power of the data togenerate co-clusters for improved data analysis. In this paper, we present the novel idea of Predictive OverlappingCo-Clustering (POCC) as an optimization problem for a more effective andimproved predictive analysis. Our algorithm generates optimal co-clusters bymaximizing predictive power of the co-clusters subject to the constraints onthe number of row and column clusters. In this paper precision, recall andf-measure have been used as evaluation measures of the resulting co-clusters.Results of our algorithm has been compared with two other well-known techniques- K-means and Spectral co-clustering, over four real data set namely, Leukemia,Internet-Ads, Ovarian cancer and MovieLens data set. The results demonstratethe effectiveness and utility of our algorithm POCC in practice.
arxiv-8400-291 | How to monitor and mitigate stair-casing in l1 trend filtering | http://arxiv.org/pdf/1412.0607v1.pdf | author:Cristian R. Rojas, Bo Wahlberg category:math.ST cs.SY stat.ML stat.TH published:2014-12-01 summary:In this paper we study the estimation of changing trends in time-series using$\ell_1$ trend filtering. This method generalizes 1D Total Variation (TV)denoising for detection of step changes in means to detecting changes intrends, and it relies on a convex optimization problem for which there are veryefficient numerical algorithms. It is known that TV denoising suffers from theso-called stair-case effect, which leads to detecting false change points. Theobjective of this paper is to show that $\ell_1$ trend filtering also suffersfrom a certain stair-case problem. The analysis is based on an interpretationof the dual variables of the optimization problem in the method as integratedrandom walk. We discuss consistency conditions for $\ell_1$ trend filtering,how to monitor their fulfillment, and how to modify the algorithm to avoid thestair-case false detection problem.
arxiv-8400-292 | Scalability and Optimization Strategies for GPU Enhanced Neural Networks (GeNN) | http://arxiv.org/pdf/1412.0595v1.pdf | author:Naresh Balaji, Esin Yavuz, Thomas Nowotny category:cs.DC cs.NE q-bio.NC published:2014-12-01 summary:Simulation of spiking neural networks has been traditionally done onhigh-performance supercomputers or large-scale clusters. Utilizing the parallelnature of neural network computation algorithms, GeNN (GPU Enhanced NeuralNetwork) provides a simulation environment that performs on General PurposeNVIDIA GPUs with a code generation based approach. GeNN allows the users todesign and simulate neural networks by specifying the populations of neurons atdifferent stages, their synapse connection densities and the model ofindividual neurons. In this report we describe work on how to scale synapticweights based on the configuration of the user-defined network to ensuresufficient spiking and subsequent effective learning. We also discussoptimization strategies particular to GPU computing: sparse representation ofsynapse connections and occupancy based block-size determination.
arxiv-8400-293 | Game-theoretical control with continuous action sets | http://arxiv.org/pdf/1412.0543v1.pdf | author:Steven Perkins, Panayotis Mertikopoulos, David S. Leslie category:math.OC cs.GT cs.MA stat.ML published:2014-12-01 summary:Motivated by the recent applications of game-theoretical learning techniquesto the design of distributed control systems, we study a class of controlproblems that can be formulated as potential games with continuous action sets,and we propose an actor-critic reinforcement learning algorithm that provablyconverges to equilibrium in this class of problems. The method employed is toanalyse the learning process under study through a mean-field dynamical systemthat evolves in an infinite-dimensional function space (the space ofprobability distributions over the players' continuous controls). To do so, weextend the theory of finite-dimensional two-timescale stochastic approximationto an infinite-dimensional, Banach space setting, and we prove that thecontinuous dynamics of the process converge to equilibrium in the case ofpotential games. These results combine to give a provably-convergent learningalgorithm in which players do not need to keep track of the controls selectedby the other agents.
arxiv-8400-294 | Study of the Influence of the Number Normalization Scheme Used in Two Chaotic Pseudo Random Number Generators Used as the Source of Randomness in Differential Evolution | http://arxiv.org/pdf/1412.6145v1.pdf | author:Lenka Skanderova, Tomas Fabian category:cs.NE cs.CR published:2014-12-01 summary:In many publications, authors showed that chaotic pseudo random numbergenerators (PRNGs) may improve performance of the evolutionary algorithms. Inthis paper, we use two chaotic maps Gingerbread man and Tinkerbell as thechaotic PRNGs instead of the classical PRNG in the differential evolution.Numbers generated by this maps are normalized to the unit interval by threedifferent methods -- operation modulo, straightforward number normalizationwhere we know minimal and maximal generated number and arctangent of the twovariables $x$ and $y$, where numbers $x$ and $y$ are generated by theGingerbread man map and Tinkerbell map. The first goal of this paper is to showwhether the differential evolution convergence speed might be affected by theway how we normalize number generated by the chaotic map. The second goal is tofind out the influence of the probability distribution function of the selectedchaotic PRNGs. The results mentioned below showed that the selectednormalization method may improve differential evolution convergence speed,especially in the case of arctangent and straightforward number normalization,where we know the minimal and maximal generated numbers.
arxiv-8400-295 | CAM: Causal additive models, high-dimensional order search and penalized regression | http://arxiv.org/pdf/1310.1533v2.pdf | author:Peter Bühlmann, Jonas Peters, Jan Ernest category:stat.ME cs.LG stat.ML published:2013-10-06 summary:We develop estimation for potentially high-dimensional additive structuralequation models. A key component of our approach is to decouple order searchamong the variables from feature or edge selection in a directed acyclic graphencoding the causal structure. We show that the former can be done withnonregularized (restricted) maximum likelihood estimation while the latter canbe efficiently addressed using sparse regression techniques. Thus, wesubstantially simplify the problem of structure search and estimation for animportant class of causal models. We establish consistency of the (restricted)maximum likelihood estimator for low- and high-dimensional scenarios, and wealso allow for misspecification of the error distribution. Furthermore, wedevelop an efficient computational algorithm which can deal with manyvariables, and the new method's accuracy and performance is illustrated onsimulated and real data.
arxiv-8400-296 | Efficiently learning Ising models on arbitrary graphs | http://arxiv.org/pdf/1411.6156v2.pdf | author:Guy Bresler category:cs.LG cs.IT math.IT stat.ML published:2014-11-22 summary:We consider the problem of reconstructing the graph underlying an Ising modelfrom i.i.d. samples. Over the last fifteen years this problem has been ofsignificant interest in the statistics, machine learning, and statisticalphysics communities, and much of the effort has been directed towards findingalgorithms with low computational cost for various restricted classes ofmodels. Nevertheless, for learning Ising models on general graphs with $p$nodes of degree at most $d$, it is not known whether or not it is possible toimprove upon the $p^{d}$ computation needed to exhaustively search over allpossible neighborhoods for each node. In this paper we show that a simple greedy procedure allows to learn thestructure of an Ising model on an arbitrary bounded-degree graph in time on theorder of $p^2$. We make no assumptions on the parameters except what isnecessary for identifiability of the model, and in particular the results holdat low-temperatures as well as for highly non-uniform models. The proof restson a new structural property of Ising models: we show that for any node thereexists at least one neighbor with which it has a high mutual information. Thisstructural property may be of independent interest.
arxiv-8400-297 | Seeding the Initial Population of Multi-Objective Evolutionary Algorithms: A Computational Study | http://arxiv.org/pdf/1412.0307v1.pdf | author:Tobias Friedrich, Markus Wagner category:cs.NE published:2014-11-30 summary:Most experimental studies initialize the population of evolutionaryalgorithms with random genotypes. In practice, however, optimizers aretypically seeded with good candidate solutions either previously known orcreated according to some problem-specific method. This "seeding" has beenstudied extensively for single-objective problems. For multi-objectiveproblems, however, very little literature is available on the approaches toseeding and their individual benefits and disadvantages. In this article, weare trying to narrow this gap via a comprehensive computational study on commonreal-valued test functions. We investigate the effect of two seeding techniquesfor five algorithms on 48 optimization problems with 2, 3, 4, 6, and 8objectives. We observe that some functions (e.g., DTLZ4 and the LZ family)benefit significantly from seeding, while others (e.g., WFG) profit less. Theadvantage of seeding also depends on the examined algorithm.
arxiv-8400-298 | Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection | http://arxiv.org/pdf/1412.0296v1.pdf | author:George Papandreou, Iasonas Kokkinos, Pierre-André Savalle category:cs.CV published:2014-11-30 summary:Deep Convolutional Neural Networks (DCNNs) commonly use generic `max-pooling'(MP) layers to extract deformation-invariant features, but we argue in favor ofa more refined treatment. First, we introduce epitomic convolution as abuilding block alternative to the common convolution-MP cascade of DCNNs; whilehaving identical complexity to MP, Epitomic Convolution allows for parametersharing across different filters, resulting in faster convergence and bettergeneralization. Second, we introduce a Multiple Instance Learning approach toexplicitly accommodate global translation and scaling when training a DCNNexclusively with class labels. For this we rely on a `patchwork' data structurethat efficiently lays out all image scales and positions as candidates to aDCNN. Factoring global and local deformations allows a DCNN to `focus itsresources' on the treatment of non-rigid deformations and yields a substantialclassification accuracy improvement. Third, further pursuing this idea, wedevelop an efficient DCNN sliding window object detector that employs explicitsearch over position, scale, and aspect ratio. We provide competitive imageclassification and localization results on the ImageNet dataset and objectdetection results on the Pascal VOC 2007 benchmark.
arxiv-8400-299 | A Clearer Picture of Blind Deconvolution | http://arxiv.org/pdf/1412.0251v1.pdf | author:Daniele Perrone, Paolo Favaro category:cs.CV published:2014-11-30 summary:Blind deconvolution is the problem of recovering a sharp image and a blurkernel from a noisy blurry image. Recently, there has been a significant efforton understanding the basic mechanisms to solve blind deconvolution. While thiseffort resulted in the deployment of effective algorithms, the theoreticalfindings generated contrasting views on why these approaches worked. On the onehand, one could observe experimentally that alternating energy minimizationalgorithms converge to the desired solution. On the other hand, it has beenshown that such alternating minimization algorithms should fail to converge andone should instead use a so-called Variational Bayes approach. To clarify thisconundrum, recent work showed that a good image and blur prior is instead whatmakes a blind deconvolution algorithm work. Unfortunately, this analysis didnot apply to algorithms based on total variation regularization. In thismanuscript, we provide both analysis and experiments to get a clearer pictureof blind deconvolution. Our analysis reveals the very reason why an algorithmbased on total variation works. We also introduce an implementation of thisalgorithm and show that, in spite of its extreme simplicity, it is very robustand achieves a performance comparable to the state of the art.
arxiv-8400-300 | Simple pairs of points in digital spaces. Topology-preserving transformations of digital spaces by contracting simple pairs of points | http://arxiv.org/pdf/1412.0218v1.pdf | author:Alexander V. Evako category:cs.DM cs.CV published:2014-11-30 summary:Transformations of digital spaces preserving local and global topology playan important role in thinning, skeletonization and simplification of digitalimages. In the present paper, we introduce and study contractions of simplepair of points based on the notions of a digital contractible space andcontractible transformations of digital spaces. We show that the contraction ofa simple pair of points preserves local and global topology of a digital space.Relying on the obtained results, we study properties if digital manifolds. Inparticular, we show that a digital n-manifold can be transformed to itscompressed form with the minimal number of points by sequential contractions ofsimple pairs. Key Words: Graph, digital space, contraction, splitting, simple pair,homotopy, thinning
